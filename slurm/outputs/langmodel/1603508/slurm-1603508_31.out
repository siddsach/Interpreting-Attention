Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 80, 'lr': 16.968474233996695, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 2.2130081039708775, 'dropout': 0.6466402571867684, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.415682315826416 and batch: 50, loss is 6.429890155792236 and perplexity is 620.1058292380128
At time: 2.368140697479248 and batch: 100, loss is 5.856617012023926 and perplexity is 349.53965314663543
At time: 3.3149173259735107 and batch: 150, loss is 5.801838207244873 and perplexity is 330.90727733781915
At time: 4.247862815856934 and batch: 200, loss is 5.726370887756348 and perplexity is 306.8536389315599
At time: 5.180911540985107 and batch: 250, loss is 5.717520627975464 and perplexity is 304.14988661556237
At time: 6.113255262374878 and batch: 300, loss is 5.697012100219727 and perplexity is 297.97574787377664
At time: 7.046269178390503 and batch: 350, loss is 5.712259998321533 and perplexity is 302.5540678887034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.538078571188039 and perplexity of 254.18912363307683
Finished 1 epochs...
Completing Train Step...
At time: 8.808274984359741 and batch: 50, loss is 5.317367753982544 and perplexity is 203.84660077195946
At time: 9.73573088645935 and batch: 100, loss is 5.1643181419372555 and perplexity is 174.91814855026078
At time: 10.661729335784912 and batch: 150, loss is 5.09604928970337 and perplexity is 163.375182613976
At time: 11.596118211746216 and batch: 200, loss is 5.064110641479492 and perplexity is 158.23964769566163
At time: 12.530500411987305 and batch: 250, loss is 5.069675235748291 and perplexity is 159.1226416052805
At time: 13.466002225875854 and batch: 300, loss is 5.037035636901855 and perplexity is 154.01278784209543
At time: 14.404130935668945 and batch: 350, loss is 5.079741525650024 and perplexity is 160.7325053291216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.207727366480334 and perplexity of 182.6784249697046
Finished 2 epochs...
Completing Train Step...
At time: 16.195217609405518 and batch: 50, loss is 4.995101470947265 and perplexity is 147.68793065739837
At time: 17.131953477859497 and batch: 100, loss is 4.931243476867675 and perplexity is 138.55169109047654
At time: 18.06772494316101 and batch: 150, loss is 4.880761404037475 and perplexity is 131.7309261699786
At time: 19.003881454467773 and batch: 200, loss is 4.897955026626587 and perplexity is 134.01544126414618
At time: 19.93818211555481 and batch: 250, loss is 4.8876986312866215 and perplexity is 132.64795066656652
At time: 20.8721706867218 and batch: 300, loss is 4.850041913986206 and perplexity is 127.74574406717439
At time: 21.806567192077637 and batch: 350, loss is 4.894604415893554 and perplexity is 133.56715911713107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.150015469255118 and perplexity of 172.43415772419974
Finished 3 epochs...
Completing Train Step...
At time: 23.58275580406189 and batch: 50, loss is 4.8645152759552 and perplexity is 129.6080992210643
At time: 24.530424118041992 and batch: 100, loss is 4.82642897605896 and perplexity is 124.76462672547197
At time: 25.465747594833374 and batch: 150, loss is 4.786533613204956 and perplexity is 119.88507951389906
At time: 26.40002751350403 and batch: 200, loss is 4.802145900726319 and perplexity is 121.77144678322017
At time: 27.33440613746643 and batch: 250, loss is 4.852886657714844 and perplexity is 128.10966535745243
At time: 28.269565105438232 and batch: 300, loss is 4.843779640197754 and perplexity is 126.948264865233
At time: 29.20494031906128 and batch: 350, loss is 4.84192234992981 and perplexity is 126.712703908617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.139178045864763 and perplexity of 170.57550544232458
Finished 4 epochs...
Completing Train Step...
At time: 30.986737966537476 and batch: 50, loss is 4.789257297515869 and perplexity is 120.2120537091896
At time: 31.954813480377197 and batch: 100, loss is 4.782465658187866 and perplexity is 119.39838300413544
At time: 32.89998173713684 and batch: 150, loss is 4.769271783828735 and perplexity is 117.83340252039925
At time: 33.85721206665039 and batch: 200, loss is 4.786060371398926 and perplexity is 119.82835830483619
At time: 34.79371166229248 and batch: 250, loss is 4.819377574920654 and perplexity is 123.88795580622049
At time: 35.72954607009888 and batch: 300, loss is 4.793866195678711 and perplexity is 120.76737755519544
At time: 36.6704785823822 and batch: 350, loss is 4.828416137695313 and perplexity is 125.01280070451385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1346409238618 and perplexity of 169.803336600159
Finished 5 epochs...
Completing Train Step...
At time: 38.450390100479126 and batch: 50, loss is 4.735874214172363 and perplexity is 113.96304329031571
At time: 39.386181116104126 and batch: 100, loss is 4.737921657562256 and perplexity is 114.19661520102996
At time: 40.321202993392944 and batch: 150, loss is 4.673348274230957 and perplexity is 107.05559449303371
At time: 41.2612566947937 and batch: 200, loss is 4.7371230316162105 and perplexity is 114.1054512289819
At time: 42.20162391662598 and batch: 250, loss is 4.78075249671936 and perplexity is 119.19400940744143
At time: 43.136688232421875 and batch: 300, loss is 4.7654596710205075 and perplexity is 117.385063400012
At time: 44.072479486465454 and batch: 350, loss is 4.807628984451294 and perplexity is 122.44096365239479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.209572627626616 and perplexity of 183.01582556993722
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 45.85101938247681 and batch: 50, loss is 4.710918626785278 and perplexity is 111.1542222657831
At time: 46.80010962486267 and batch: 100, loss is 4.654693078994751 and perplexity is 105.07696472160093
At time: 47.73626637458801 and batch: 150, loss is 4.6004522514343265 and perplexity is 99.52931774174093
At time: 48.67272996902466 and batch: 200, loss is 4.598380537033081 and perplexity is 99.3233348633502
At time: 49.608259439468384 and batch: 250, loss is 4.587739753723144 and perplexity is 98.27205989333645
At time: 50.54400849342346 and batch: 300, loss is 4.593092250823974 and perplexity is 98.79947103314707
At time: 51.480292081832886 and batch: 350, loss is 4.641930932998657 and perplexity is 103.74447793782466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.05670113399111 and perplexity of 157.07150287433794
Finished 7 epochs...
Completing Train Step...
At time: 53.26608681678772 and batch: 50, loss is 4.625353698730469 and perplexity is 102.03885771521148
At time: 54.20266246795654 and batch: 100, loss is 4.604009046554565 and perplexity is 99.88395344280248
At time: 55.13862609863281 and batch: 150, loss is 4.5654591464996335 and perplexity is 96.10671094760923
At time: 56.073774099349976 and batch: 200, loss is 4.569635076522827 and perplexity is 96.50888498817214
At time: 57.022512674331665 and batch: 250, loss is 4.558292093276978 and perplexity is 95.42037148881914
At time: 57.958356857299805 and batch: 300, loss is 4.5357574844360355 and perplexity is 93.29415741811253
At time: 58.89412474632263 and batch: 350, loss is 4.5920631694793705 and perplexity is 98.69785063744493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.015850198679957 and perplexity of 150.78427887993195
Finished 8 epochs...
Completing Train Step...
At time: 60.675318241119385 and batch: 50, loss is 4.543666591644287 and perplexity is 94.0349565793719
At time: 61.612091064453125 and batch: 100, loss is 4.548076667785645 and perplexity is 94.4505736757532
At time: 62.54973578453064 and batch: 150, loss is 4.507518711090088 and perplexity is 90.69649487271812
At time: 63.48539733886719 and batch: 200, loss is 4.542233095169068 and perplexity is 93.900254371205
At time: 64.42241048812866 and batch: 250, loss is 4.562166910171509 and perplexity is 95.79082521301545
At time: 65.35933804512024 and batch: 300, loss is 4.543983564376831 and perplexity is 94.06476782093927
At time: 66.29507374763489 and batch: 350, loss is 4.6091732406616215 and perplexity is 100.40110775987203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0631082469019395 and perplexity of 158.08110860349578
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 68.06324315071106 and batch: 50, loss is 4.509197721481323 and perplexity is 90.84890314178752
At time: 69.01107907295227 and batch: 100, loss is 4.451747102737427 and perplexity is 85.77667382907363
At time: 69.95910429954529 and batch: 150, loss is 4.3985440635681154 and perplexity is 81.3323676637653
At time: 70.89612698554993 and batch: 200, loss is 4.404756393432617 and perplexity is 81.83920384710447
At time: 71.83096575737 and batch: 250, loss is 4.40487566947937 and perplexity is 81.84896588598592
At time: 72.7660403251648 and batch: 300, loss is 4.386789455413818 and perplexity is 80.38193447992818
At time: 73.70210194587708 and batch: 350, loss is 4.464726572036743 and perplexity is 86.897266148617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.983973535998114 and perplexity of 146.05357931521465
Finished 10 epochs...
Completing Train Step...
At time: 75.48244714736938 and batch: 50, loss is 4.445650405883789 and perplexity is 85.25531036403274
At time: 76.4306869506836 and batch: 100, loss is 4.412691450119018 and perplexity is 82.49118590508812
At time: 77.36634159088135 and batch: 150, loss is 4.370133733749389 and perplexity is 79.05420320766493
At time: 78.30381441116333 and batch: 200, loss is 4.388236722946167 and perplexity is 80.4983528678442
At time: 79.24076819419861 and batch: 250, loss is 4.393063116073608 and perplexity is 80.88780864223182
At time: 80.18865489959717 and batch: 300, loss is 4.373291053771973 and perplexity is 79.30419707396618
At time: 81.12458610534668 and batch: 350, loss is 4.448657007217407 and perplexity is 85.51202481949923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.988966317012392 and perplexity of 146.78461628824834
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 82.90679097175598 and batch: 50, loss is 4.397698841094971 and perplexity is 81.26365276260199
At time: 83.8431351184845 and batch: 100, loss is 4.347662296295166 and perplexity is 77.29755277498725
At time: 84.77999258041382 and batch: 150, loss is 4.292614336013794 and perplexity is 73.15747693735833
At time: 85.71739101409912 and batch: 200, loss is 4.300153617858887 and perplexity is 73.71111617374916
At time: 86.66738271713257 and batch: 250, loss is 4.29935055732727 and perplexity is 73.65194544762598
At time: 87.61623930931091 and batch: 300, loss is 4.285898294448852 and perplexity is 72.66779448591082
At time: 88.55210661888123 and batch: 350, loss is 4.372052726745605 and perplexity is 79.20605332299633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.943106289567618 and perplexity of 140.20509145083878
Finished 12 epochs...
Completing Train Step...
At time: 90.33289933204651 and batch: 50, loss is 4.358626480102539 and perplexity is 78.14972047631933
At time: 91.26790857315063 and batch: 100, loss is 4.325371141433716 and perplexity is 75.59356355757497
At time: 92.21371936798096 and batch: 150, loss is 4.275313405990601 and perplexity is 71.90267050644552
At time: 93.15110683441162 and batch: 200, loss is 4.290305700302124 and perplexity is 72.98877778090133
At time: 94.08774542808533 and batch: 250, loss is 4.2986502265930175 and perplexity is 73.60038278415796
At time: 95.02429986000061 and batch: 300, loss is 4.284559364318848 and perplexity is 72.57056249434012
At time: 95.97022795677185 and batch: 350, loss is 4.364563961029052 and perplexity is 78.61511321407863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9404012745824355 and perplexity of 139.82634706303037
Finished 13 epochs...
Completing Train Step...
At time: 97.73892998695374 and batch: 50, loss is 4.341738185882568 and perplexity is 76.84098724529524
At time: 98.6855800151825 and batch: 100, loss is 4.31038818359375 and perplexity is 74.46939112617704
At time: 99.62110900878906 and batch: 150, loss is 4.263984441757202 and perplexity is 71.09268453817216
At time: 100.55781364440918 and batch: 200, loss is 4.282468185424805 and perplexity is 72.41896303176713
At time: 101.49470615386963 and batch: 250, loss is 4.294807300567627 and perplexity is 73.31808473036357
At time: 102.43203639984131 and batch: 300, loss is 4.28046591758728 and perplexity is 72.27410594100392
At time: 103.36742758750916 and batch: 350, loss is 4.356474599838257 and perplexity is 77.98173244505533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.940897974474677 and perplexity of 139.8958160457389
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 105.14822840690613 and batch: 50, loss is 4.3216092395782475 and perplexity is 75.30972221694049
At time: 106.08424425125122 and batch: 100, loss is 4.280283269882202 and perplexity is 72.26090644688254
At time: 107.02033424377441 and batch: 150, loss is 4.2235070419311525 and perplexity is 68.27249944465346
At time: 107.95628643035889 and batch: 200, loss is 4.239250135421753 and perplexity is 69.35582485824187
At time: 108.89255595207214 and batch: 250, loss is 4.2485799217224125 and perplexity is 70.00592782809348
At time: 109.82911729812622 and batch: 300, loss is 4.2344460105896 and perplexity is 69.02342989054054
At time: 110.7657482624054 and batch: 350, loss is 4.3173733329772945 and perplexity is 74.99139195210154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.924487409920528 and perplexity of 137.61878153803326
Finished 15 epochs...
Completing Train Step...
At time: 112.56004023551941 and batch: 50, loss is 4.307788133621216 and perplexity is 74.2760184860261
At time: 113.49542546272278 and batch: 100, loss is 4.270009899139405 and perplexity is 71.52234362536498
At time: 114.43160676956177 and batch: 150, loss is 4.21579571723938 and perplexity is 67.74805272222581
At time: 115.36809062957764 and batch: 200, loss is 4.234387292861938 and perplexity is 69.01937711056814
At time: 116.30454301834106 and batch: 250, loss is 4.245693521499634 and perplexity is 69.80415404245113
At time: 117.24129104614258 and batch: 300, loss is 4.233010311126709 and perplexity is 68.9244040919723
At time: 118.17896962165833 and batch: 350, loss is 4.314443435668945 and perplexity is 74.77199643478588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.923760249696929 and perplexity of 137.51874700905174
Finished 16 epochs...
Completing Train Step...
At time: 119.95001864433289 and batch: 50, loss is 4.301476135253906 and perplexity is 73.80866489781931
At time: 120.89690589904785 and batch: 100, loss is 4.263952655792236 and perplexity is 71.09042482450587
At time: 121.8327751159668 and batch: 150, loss is 4.210788087844849 and perplexity is 67.40964360289031
At time: 122.7691068649292 and batch: 200, loss is 4.230391683578492 and perplexity is 68.74415285709895
At time: 123.70610427856445 and batch: 250, loss is 4.242602033615112 and perplexity is 69.58868857199096
At time: 124.65111541748047 and batch: 300, loss is 4.230260181427002 and perplexity is 68.73511344745984
At time: 125.5872893333435 and batch: 350, loss is 4.3113196754455565 and perplexity is 74.53879107494544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.923890738651671 and perplexity of 137.5366928574484
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 127.36652731895447 and batch: 50, loss is 4.294524774551392 and perplexity is 73.29737338985072
At time: 128.31508946418762 and batch: 100, loss is 4.2510097885131835 and perplexity is 70.17623974111379
At time: 129.25099182128906 and batch: 150, loss is 4.1932437705993655 and perplexity is 66.23730147196989
At time: 130.18787813186646 and batch: 200, loss is 4.207419414520263 and perplexity is 67.18294458651094
At time: 131.12491655349731 and batch: 250, loss is 4.2196502780914305 and perplexity is 68.00969564939409
At time: 132.06092405319214 and batch: 300, loss is 4.2062953758239745 and perplexity is 67.1074707827496
At time: 132.99776124954224 and batch: 350, loss is 4.293094758987427 and perplexity is 73.19263191392506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.916591249663254 and perplexity of 136.53640053533312
Finished 18 epochs...
Completing Train Step...
At time: 134.78357672691345 and batch: 50, loss is 4.286964054107666 and perplexity is 72.74528217406042
At time: 135.7194139957428 and batch: 100, loss is 4.244458513259888 and perplexity is 69.71799854936282
At time: 136.65650153160095 and batch: 150, loss is 4.1877769947052 and perplexity is 65.87618495985943
At time: 137.59429287910461 and batch: 200, loss is 4.204087743759155 and perplexity is 66.95948558699452
At time: 138.53097653388977 and batch: 250, loss is 4.219816617965698 and perplexity is 68.02100931454811
At time: 139.46723318099976 and batch: 300, loss is 4.207196016311645 and perplexity is 67.16793771335733
At time: 140.40438604354858 and batch: 350, loss is 4.294016122817993 and perplexity is 73.26010003420451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.915717815530711 and perplexity of 136.4171970485463
Finished 19 epochs...
Completing Train Step...
At time: 142.18582010269165 and batch: 50, loss is 4.283196506500244 and perplexity is 72.47172650084548
At time: 143.12106323242188 and batch: 100, loss is 4.240305156707763 and perplexity is 69.42903534229495
At time: 144.0564296245575 and batch: 150, loss is 4.183648262023926 and perplexity is 65.60476050717027
At time: 144.9929928779602 and batch: 200, loss is 4.201402835845947 and perplexity is 66.77994666486798
At time: 145.92914485931396 and batch: 250, loss is 4.219105911254883 and perplexity is 67.9726835015258
At time: 146.86503529548645 and batch: 300, loss is 4.207049417495727 and perplexity is 67.15809169494587
At time: 147.8014259338379 and batch: 350, loss is 4.2941956710815425 and perplexity is 73.27325493888792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.915521555933459 and perplexity of 136.3904264914706
Finished 20 epochs...
Completing Train Step...
At time: 149.5738503932953 and batch: 50, loss is 4.280194416046142 and perplexity is 72.25448607338923
At time: 150.52209043502808 and batch: 100, loss is 4.236889610290527 and perplexity is 69.19230176678084
At time: 151.45779943466187 and batch: 150, loss is 4.18059006690979 and perplexity is 65.4044348227676
At time: 152.40637183189392 and batch: 200, loss is 4.19907940864563 and perplexity is 66.62496843039223
At time: 153.3427929878235 and batch: 250, loss is 4.218357067108155 and perplexity is 67.9218016090297
At time: 154.27984738349915 and batch: 300, loss is 4.206768035888672 and perplexity is 67.13919730157002
At time: 155.21645545959473 and batch: 350, loss is 4.293936653137207 and perplexity is 73.25427830876883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.915460520777209 and perplexity of 136.3821021345205
Finished 21 epochs...
Completing Train Step...
At time: 157.10669088363647 and batch: 50, loss is 4.27760365486145 and perplexity is 72.06753423380421
At time: 158.0464894771576 and batch: 100, loss is 4.233812074661255 and perplexity is 68.97968732489228
At time: 158.9817759990692 and batch: 150, loss is 4.177633142471313 and perplexity is 65.21132449807799
At time: 159.91744780540466 and batch: 200, loss is 4.196812429428101 and perplexity is 66.47410208164158
At time: 160.85369777679443 and batch: 250, loss is 4.217470693588257 and perplexity is 67.8616241964321
At time: 161.78975319862366 and batch: 300, loss is 4.206265659332275 and perplexity is 67.10547661378112
At time: 162.72567296028137 and batch: 350, loss is 4.293684349060059 and perplexity is 73.23579828707342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.915574172447467 and perplexity of 136.39760306905825
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 164.50990629196167 and batch: 50, loss is 4.273865976333618 and perplexity is 71.7986717323501
At time: 165.44694638252258 and batch: 100, loss is 4.227449598312378 and perplexity is 68.54219892636249
At time: 166.38309383392334 and batch: 150, loss is 4.168420686721801 and perplexity is 64.61332679973941
At time: 167.3200867176056 and batch: 200, loss is 4.185570449829101 and perplexity is 65.73098645390132
At time: 168.25691270828247 and batch: 250, loss is 4.206273469924927 and perplexity is 67.10600074937057
At time: 169.19381260871887 and batch: 300, loss is 4.1948276805877684 and perplexity is 66.34229852636668
At time: 170.1306653022766 and batch: 350, loss is 4.284643478393555 and perplexity is 72.57666695678732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.911667922447467 and perplexity of 135.86583920953044
Finished 23 epochs...
Completing Train Step...
At time: 171.9006049633026 and batch: 50, loss is 4.269345970153808 and perplexity is 71.47487362841153
At time: 172.849702835083 and batch: 100, loss is 4.224185209274292 and perplexity is 68.31881532739584
At time: 173.78495573997498 and batch: 150, loss is 4.1667483520507815 and perplexity is 64.50536199492757
At time: 174.72097754478455 and batch: 200, loss is 4.185136804580688 and perplexity is 65.70248870335573
At time: 175.65572595596313 and batch: 250, loss is 4.20659610748291 and perplexity is 67.12765515865337
At time: 176.60384607315063 and batch: 300, loss is 4.195360279083252 and perplexity is 66.37764174578601
At time: 177.539160490036 and batch: 350, loss is 4.284636468887329 and perplexity is 72.57615823197138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.911183324353448 and perplexity of 135.800014833279
Finished 24 epochs...
Completing Train Step...
At time: 179.32081484794617 and batch: 50, loss is 4.266174831390381 and perplexity is 71.24857588644797
At time: 180.27054286003113 and batch: 100, loss is 4.222227268218994 and perplexity is 68.18518197981642
At time: 181.20675611495972 and batch: 150, loss is 4.165661840438843 and perplexity is 64.43531423083331
At time: 182.1431188583374 and batch: 200, loss is 4.184703011512756 and perplexity is 65.67399360014626
At time: 183.0805389881134 and batch: 250, loss is 4.206619119644165 and perplexity is 67.12919992885271
At time: 184.01766300201416 and batch: 300, loss is 4.19553876876831 and perplexity is 66.38949052756736
At time: 184.95440793037415 and batch: 350, loss is 4.28451132774353 and perplexity is 72.56707653677644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.911039681270204 and perplexity of 135.78050950138032
Finished 25 epochs...
Completing Train Step...
At time: 186.74459838867188 and batch: 50, loss is 4.263616161346436 and perplexity is 71.06650731568294
At time: 187.6812663078308 and batch: 100, loss is 4.220510234832764 and perplexity is 68.06820620030933
At time: 188.62723803520203 and batch: 150, loss is 4.164678544998169 and perplexity is 64.37198642020792
At time: 189.56670308113098 and batch: 200, loss is 4.184280118942261 and perplexity is 65.64622642785727
At time: 190.5040099620819 and batch: 250, loss is 4.206507015228271 and perplexity is 67.12167487090929
At time: 191.44107031822205 and batch: 300, loss is 4.19553921699524 and perplexity is 66.38952028513151
At time: 192.3774013519287 and batch: 350, loss is 4.284222440719605 and perplexity is 72.54611587777771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.911018108499461 and perplexity of 135.7775803711723
Finished 26 epochs...
Completing Train Step...
At time: 194.1614227294922 and batch: 50, loss is 4.261265335083007 and perplexity is 70.89963852045672
At time: 195.09715151786804 and batch: 100, loss is 4.218885087966919 and perplexity is 67.95767520721472
At time: 196.03441786766052 and batch: 150, loss is 4.163657760620117 and perplexity is 64.30631002850625
At time: 196.9716591835022 and batch: 200, loss is 4.1837511205673215 and perplexity is 65.61150886433299
At time: 197.91681718826294 and batch: 250, loss is 4.206310005187988 and perplexity is 67.10845252954887
At time: 198.85379934310913 and batch: 300, loss is 4.195580205917358 and perplexity is 66.39224157577898
At time: 199.78927993774414 and batch: 350, loss is 4.284167222976684 and perplexity is 72.5421101555958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.910961808829472 and perplexity of 135.76993635338533
Finished 27 epochs...
Completing Train Step...
At time: 201.56981658935547 and batch: 50, loss is 4.259266796112061 and perplexity is 70.75808432774869
At time: 202.51887559890747 and batch: 100, loss is 4.217477416992187 and perplexity is 67.86208045907676
At time: 203.45545029640198 and batch: 150, loss is 4.162767286300659 and perplexity is 64.24907239894858
At time: 204.3926284313202 and batch: 200, loss is 4.183264803886414 and perplexity is 65.5796086505445
At time: 205.33008480072021 and batch: 250, loss is 4.206108427047729 and perplexity is 67.09492629583463
At time: 206.26674103736877 and batch: 300, loss is 4.195477933883667 and perplexity is 66.38545185341695
At time: 207.20404267311096 and batch: 350, loss is 4.283829565048218 and perplexity is 72.517619871856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9108907765355605 and perplexity of 135.7602926458733
Finished 28 epochs...
Completing Train Step...
At time: 208.98366618156433 and batch: 50, loss is 4.257248191833496 and perplexity is 70.61539582024936
At time: 209.9193549156189 and batch: 100, loss is 4.216116981506348 and perplexity is 67.7698212472595
At time: 210.8550946712494 and batch: 150, loss is 4.161801481246949 and perplexity is 64.18705027559062
At time: 211.79259705543518 and batch: 200, loss is 4.182710475921631 and perplexity is 65.54326611332296
At time: 212.72976779937744 and batch: 250, loss is 4.205813312530518 and perplexity is 67.07512853050117
At time: 213.66578483581543 and batch: 300, loss is 4.1953924369812015 and perplexity is 66.37977634553735
At time: 214.60166025161743 and batch: 350, loss is 4.283476037979126 and perplexity is 72.49198746138761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.910962861159752 and perplexity of 135.77007922827562
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 216.3981282711029 and batch: 50, loss is 4.254879579544068 and perplexity is 70.44833325721287
At time: 217.33568954467773 and batch: 100, loss is 4.2137463188171385 and perplexity is 67.60935214478482
At time: 218.27329683303833 and batch: 150, loss is 4.157187452316284 and perplexity is 63.89157156651124
At time: 219.21086525917053 and batch: 200, loss is 4.177189302444458 and perplexity is 65.18238752423017
At time: 220.15934920310974 and batch: 250, loss is 4.200427255630493 and perplexity is 66.71482923891355
At time: 221.0963020324707 and batch: 300, loss is 4.189380531311035 and perplexity is 65.98190457387749
At time: 222.03317785263062 and batch: 350, loss is 4.279218206405639 and perplexity is 72.18398496504452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.909589570144127 and perplexity of 135.58375536600656
Finished 30 epochs...
Completing Train Step...
At time: 223.8023121356964 and batch: 50, loss is 4.253350553512573 and perplexity is 70.34069823113572
At time: 224.75027799606323 and batch: 100, loss is 4.212922325134278 and perplexity is 67.55366541162417
At time: 225.68556213378906 and batch: 150, loss is 4.156447868347168 and perplexity is 63.844335853960004
At time: 226.62119555473328 and batch: 200, loss is 4.176585912704468 and perplexity is 65.14306900375881
At time: 227.55796360969543 and batch: 250, loss is 4.2003398036956785 and perplexity is 66.70899515312055
At time: 228.49334239959717 and batch: 300, loss is 4.189392738342285 and perplexity is 65.98271002196459
At time: 229.42912197113037 and batch: 350, loss is 4.279226589202881 and perplexity is 72.18459007129077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.909480653960129 and perplexity of 135.56898890492786
Finished 31 epochs...
Completing Train Step...
At time: 231.19809460639954 and batch: 50, loss is 4.252608165740967 and perplexity is 70.28849753589911
At time: 232.14828658103943 and batch: 100, loss is 4.212614183425903 and perplexity is 67.53285251658248
At time: 233.08428645133972 and batch: 150, loss is 4.155989451408386 and perplexity is 63.815075236253065
At time: 234.02040719985962 and batch: 200, loss is 4.176023769378662 and perplexity is 65.10645955313908
At time: 234.95750880241394 and batch: 250, loss is 4.200223684310913 and perplexity is 66.70124939537007
At time: 235.89430904388428 and batch: 300, loss is 4.189344215393066 and perplexity is 65.979508423953
At time: 236.8305697441101 and batch: 350, loss is 4.2791501331329345 and perplexity is 72.17907133219649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.90948381095097 and perplexity of 135.56941689565977
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 238.61332535743713 and batch: 50, loss is 4.251875658035278 and perplexity is 70.23702952249229
At time: 239.54979014396667 and batch: 100, loss is 4.2117036437988284 and perplexity is 67.4713891648919
At time: 240.487158536911 and batch: 150, loss is 4.154221243858338 and perplexity is 63.702336640405235
At time: 241.42448353767395 and batch: 200, loss is 4.173973975181579 and perplexity is 64.97314139422845
At time: 242.36137700080872 and batch: 250, loss is 4.197728805541992 and perplexity is 66.53504528016255
At time: 243.29950594902039 and batch: 300, loss is 4.186825761795044 and perplexity is 65.81355115911914
At time: 244.2365436553955 and batch: 350, loss is 4.277107372283935 and perplexity is 72.03177724567851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.90875349373653 and perplexity of 135.47044436182085
Finished 33 epochs...
Completing Train Step...
At time: 246.0300314426422 and batch: 50, loss is 4.250844869613648 and perplexity is 70.16466730716489
At time: 246.9669041633606 and batch: 100, loss is 4.211153039932251 and perplexity is 67.43424938272648
At time: 247.9036726951599 and batch: 150, loss is 4.154274735450745 and perplexity is 63.70574427097116
At time: 248.85325527191162 and batch: 200, loss is 4.174510140419006 and perplexity is 65.00798707469694
At time: 249.78953099250793 and batch: 250, loss is 4.197920360565186 and perplexity is 66.54779162307842
At time: 250.7267038822174 and batch: 300, loss is 4.187001276016235 and perplexity is 65.82510338705517
At time: 251.66329050064087 and batch: 350, loss is 4.2764847946166995 and perplexity is 71.9869458268022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908562495790679 and perplexity of 135.44457225606237
Finished 34 epochs...
Completing Train Step...
At time: 253.42975211143494 and batch: 50, loss is 4.250038270950317 and perplexity is 70.1080953987819
At time: 254.38008332252502 and batch: 100, loss is 4.210784397125244 and perplexity is 67.40939481325624
At time: 255.31742906570435 and batch: 150, loss is 4.1544106006622314 and perplexity is 63.71440025339958
At time: 256.253865480423 and batch: 200, loss is 4.175001888275147 and perplexity is 65.0399624742457
At time: 257.1920254230499 and batch: 250, loss is 4.1980531120300295 and perplexity is 66.5566265263087
At time: 258.1297583580017 and batch: 300, loss is 4.18712119102478 and perplexity is 65.83299727817885
At time: 259.0686798095703 and batch: 350, loss is 4.275884675979614 and perplexity is 71.94375807916695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908453579606681 and perplexity of 135.42982095345138
Finished 35 epochs...
Completing Train Step...
At time: 260.85509157180786 and batch: 50, loss is 4.249320793151855 and perplexity is 70.05781243745204
At time: 261.7916178703308 and batch: 100, loss is 4.210465517044067 and perplexity is 67.38790272684736
At time: 262.72709584236145 and batch: 150, loss is 4.154560060501098 and perplexity is 63.7239237090643
At time: 263.6797773838043 and batch: 200, loss is 4.175435824394226 and perplexity is 65.06819178756321
At time: 264.6220564842224 and batch: 250, loss is 4.19817271232605 and perplexity is 66.56458719458304
At time: 265.5706250667572 and batch: 300, loss is 4.187218732833863 and perplexity is 65.83941906102174
At time: 266.50672721862793 and batch: 350, loss is 4.275318431854248 and perplexity is 71.90303188037144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908380968817349 and perplexity of 135.41998764425944
Finished 36 epochs...
Completing Train Step...
At time: 268.29312324523926 and batch: 50, loss is 4.248663396835327 and perplexity is 70.01177182473435
At time: 269.23086762428284 and batch: 100, loss is 4.210190000534058 and perplexity is 67.36933880452219
At time: 270.1671631336212 and batch: 150, loss is 4.154678401947021 and perplexity is 63.73146533657065
At time: 271.10504698753357 and batch: 200, loss is 4.1758358335495 and perplexity is 65.09422486638316
At time: 272.0550494194031 and batch: 250, loss is 4.198246297836303 and perplexity is 66.56948556391885
At time: 272.99272894859314 and batch: 300, loss is 4.187289514541626 and perplexity is 65.84407945247432
At time: 273.9301519393921 and batch: 350, loss is 4.274767780303955 and perplexity is 71.86344926351063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908326773807921 and perplexity of 135.41264875561959
Finished 37 epochs...
Completing Train Step...
At time: 275.69938588142395 and batch: 50, loss is 4.2480495262146 and perplexity is 69.96880684372512
At time: 276.6525869369507 and batch: 100, loss is 4.209942388534546 and perplexity is 67.35265941292894
At time: 277.5910294055939 and batch: 150, loss is 4.154810700416565 and perplexity is 63.739897469663326
At time: 278.53923296928406 and batch: 200, loss is 4.176211123466492 and perplexity is 65.11865865722079
At time: 279.47860455513 and batch: 250, loss is 4.1983453559875485 and perplexity is 66.57608014070605
At time: 280.4174852371216 and batch: 300, loss is 4.18735297203064 and perplexity is 65.8482578849978
At time: 281.35619044303894 and batch: 350, loss is 4.274248170852661 and perplexity is 71.82611803574002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908286259092134 and perplexity of 135.40716266177586
Finished 38 epochs...
Completing Train Step...
At time: 283.1368820667267 and batch: 50, loss is 4.247476987838745 and perplexity is 69.92875848240016
At time: 284.08900690078735 and batch: 100, loss is 4.209707164764405 and perplexity is 67.33681832962553
At time: 285.0280544757843 and batch: 150, loss is 4.15490535736084 and perplexity is 63.745931179147945
At time: 285.9679834842682 and batch: 200, loss is 4.176549882888794 and perplexity is 65.14072195327232
At time: 286.9064407348633 and batch: 250, loss is 4.198418121337891 and perplexity is 66.58092474875959
At time: 287.8455195426941 and batch: 300, loss is 4.187397165298462 and perplexity is 65.8511679989974
At time: 288.7841794490814 and batch: 350, loss is 4.273732843399048 and perplexity is 71.78911360074932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908228907091864 and perplexity of 135.39939701283623
Finished 39 epochs...
Completing Train Step...
At time: 290.5987470149994 and batch: 50, loss is 4.246914920806884 and perplexity is 69.88946487653294
At time: 291.53917121887207 and batch: 100, loss is 4.209465131759644 and perplexity is 67.32052256928964
At time: 292.4858331680298 and batch: 150, loss is 4.154980850219727 and perplexity is 63.750743723389135
At time: 293.4414029121399 and batch: 200, loss is 4.176858897209168 and perplexity is 65.16085447965465
At time: 294.38093852996826 and batch: 250, loss is 4.198492622375488 and perplexity is 66.58588528151765
At time: 295.32071566581726 and batch: 300, loss is 4.187418909072876 and perplexity is 65.85259986750631
At time: 296.2732288837433 and batch: 350, loss is 4.27324875831604 and perplexity is 71.75436997184752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908185761550377 and perplexity of 135.39355525855882
Finished 40 epochs...
Completing Train Step...
At time: 298.07009387016296 and batch: 50, loss is 4.246380949020386 and perplexity is 69.85215583598941
At time: 299.0098204612732 and batch: 100, loss is 4.2092614841461184 and perplexity is 67.30681430140253
At time: 299.9486858844757 and batch: 150, loss is 4.155052056312561 and perplexity is 63.75528332638675
At time: 300.88813304901123 and batch: 200, loss is 4.1771495819091795 and perplexity is 65.1797984963261
At time: 301.8270719051361 and batch: 250, loss is 4.198556079864502 and perplexity is 66.59011078866999
At time: 302.7664749622345 and batch: 300, loss is 4.187442874908447 and perplexity is 65.85417809899842
At time: 303.70607590675354 and batch: 350, loss is 4.27276125907898 and perplexity is 71.71939829625569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9081531393116915 and perplexity of 135.38913848972553
Finished 41 epochs...
Completing Train Step...
At time: 305.4998035430908 and batch: 50, loss is 4.245861282348633 and perplexity is 69.81586542892556
At time: 306.45214319229126 and batch: 100, loss is 4.209043493270874 and perplexity is 67.2921436291375
At time: 307.39156556129456 and batch: 150, loss is 4.155128059387207 and perplexity is 63.76012910808927
At time: 308.33195400238037 and batch: 200, loss is 4.177406606674194 and perplexity is 65.19655347184788
At time: 309.271281003952 and batch: 250, loss is 4.198628349304199 and perplexity is 66.59492339256607
At time: 310.2092216014862 and batch: 300, loss is 4.187453632354736 and perplexity is 65.85488652559266
At time: 311.1455409526825 and batch: 350, loss is 4.272294244766235 and perplexity is 71.68591213061787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908146825330011 and perplexity of 135.38828364788407
Finished 42 epochs...
Completing Train Step...
At time: 312.9306516647339 and batch: 50, loss is 4.245361309051514 and perplexity is 69.78096808509284
At time: 313.86697030067444 and batch: 100, loss is 4.208862810134888 and perplexity is 67.27998617195617
At time: 314.80273389816284 and batch: 150, loss is 4.155177593231201 and perplexity is 63.76328747059985
At time: 315.7386498451233 and batch: 200, loss is 4.1776612281799315 and perplexity is 65.21315603005627
At time: 316.67415618896484 and batch: 250, loss is 4.19869137763977 and perplexity is 66.59912089202432
At time: 317.60999965667725 and batch: 300, loss is 4.187464666366577 and perplexity is 65.85561317319929
At time: 318.54512095451355 and batch: 350, loss is 4.271845350265503 and perplexity is 71.65373994038231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.90814103751347 and perplexity of 135.38750004760422
Finished 43 epochs...
Completing Train Step...
At time: 320.32144045829773 and batch: 50, loss is 4.244876956939697 and perplexity is 69.74717770971661
At time: 321.25645327568054 and batch: 100, loss is 4.208672285079956 and perplexity is 67.2671688699423
At time: 322.1922106742859 and batch: 150, loss is 4.155250310897827 and perplexity is 63.76792435667084
At time: 323.1276330947876 and batch: 200, loss is 4.177918615341187 and perplexity is 65.2299432194742
At time: 324.06395077705383 and batch: 250, loss is 4.198753852844238 and perplexity is 66.60328181569533
At time: 324.99990725517273 and batch: 300, loss is 4.187470941543579 and perplexity is 65.85602643012517
At time: 325.93609857559204 and batch: 350, loss is 4.27140396118164 and perplexity is 71.62211974067357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.90814208984375 and perplexity of 135.387642520045
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 327.69927430152893 and batch: 50, loss is 4.244346714019775 and perplexity is 69.71020456579167
At time: 328.64653277397156 and batch: 100, loss is 4.208006906509399 and perplexity is 67.22242562452331
At time: 329.5813400745392 and batch: 150, loss is 4.154540243148804 and perplexity is 63.72266088213157
At time: 330.51668643951416 and batch: 200, loss is 4.177256417274475 and perplexity is 65.18676237589604
At time: 331.45167803764343 and batch: 250, loss is 4.197788343429566 and perplexity is 66.5390067541359
At time: 332.38637256622314 and batch: 300, loss is 4.186261539459228 and perplexity is 65.77642815735406
At time: 333.3209273815155 and batch: 350, loss is 4.269984836578369 and perplexity is 71.52055111472502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.907994237439386 and perplexity of 135.3676266113138
Finished 45 epochs...
Completing Train Step...
At time: 335.09281730651855 and batch: 50, loss is 4.244020223617554 and perplexity is 69.6874485680736
At time: 336.04728150367737 and batch: 100, loss is 4.20795298576355 and perplexity is 67.21880103891687
At time: 336.989511013031 and batch: 150, loss is 4.154619579315185 and perplexity is 63.727716594305335
At time: 337.92666244506836 and batch: 200, loss is 4.177417821884156 and perplexity is 65.1972846689841
At time: 338.8631544113159 and batch: 250, loss is 4.197832794189453 and perplexity is 66.54196452928544
At time: 339.79892778396606 and batch: 300, loss is 4.186140546798706 and perplexity is 65.76847017375013
At time: 340.7352979183197 and batch: 350, loss is 4.269747142791748 and perplexity is 71.50355314434533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.907977926320043 and perplexity of 135.3654186318083
Finished 46 epochs...
Completing Train Step...
At time: 342.51805424690247 and batch: 50, loss is 4.243800897598266 and perplexity is 69.67216597338147
At time: 343.45446038246155 and batch: 100, loss is 4.207880258560181 and perplexity is 67.21391258126758
At time: 344.4039912223816 and batch: 150, loss is 4.154670453071594 and perplexity is 63.73095874510532
At time: 345.34090209007263 and batch: 200, loss is 4.177566690444946 and perplexity is 65.20699121740225
At time: 346.2779293060303 and batch: 250, loss is 4.197867650985717 and perplexity is 66.54428400941066
At time: 347.2144045829773 and batch: 300, loss is 4.186014947891235 and perplexity is 65.76021024448019
At time: 348.15105962753296 and batch: 350, loss is 4.269499349594116 and perplexity is 71.48583724529993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.907949513402478 and perplexity of 135.36157255996696
Finished 47 epochs...
Completing Train Step...
At time: 349.9339601993561 and batch: 50, loss is 4.243602886199951 and perplexity is 69.65837145615431
At time: 350.86938309669495 and batch: 100, loss is 4.207786598205566 and perplexity is 67.20761759718003
At time: 351.80755639076233 and batch: 150, loss is 4.154723353385926 and perplexity is 63.7343302220309
At time: 352.744647026062 and batch: 200, loss is 4.177706298828125 and perplexity is 65.21609529550607
At time: 353.68060636520386 and batch: 250, loss is 4.197881031036377 and perplexity is 66.54517438125839
At time: 354.61583375930786 and batch: 300, loss is 4.1858957004547115 and perplexity is 65.7523689755182
At time: 355.5508852005005 and batch: 350, loss is 4.2692578125 and perplexity is 71.46857284898127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.907954248888739 and perplexity of 135.3622135643518
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 357.3152873516083 and batch: 50, loss is 4.243373184204102 and perplexity is 69.6423726267551
At time: 358.2707781791687 and batch: 100, loss is 4.207502717971802 and perplexity is 67.18854139079285
At time: 359.2065382003784 and batch: 150, loss is 4.154462614059448 and perplexity is 63.71771434199615
At time: 360.14210653305054 and batch: 200, loss is 4.177371544837952 and perplexity is 65.19426760104014
At time: 361.0774829387665 and batch: 250, loss is 4.197427463531494 and perplexity is 66.51499849647003
At time: 362.0114862918854 and batch: 300, loss is 4.185080804824829 and perplexity is 65.69880948304768
At time: 362.94624614715576 and batch: 350, loss is 4.268642883300782 and perplexity is 71.42463824642937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.907969507677802 and perplexity of 135.36427904357393
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 364.72584295272827 and batch: 50, loss is 4.243307943344116 and perplexity is 69.63782924668213
At time: 365.6620898246765 and batch: 100, loss is 4.207358341217041 and perplexity is 67.17884162745679
At time: 366.59854912757874 and batch: 150, loss is 4.154448938369751 and perplexity is 63.716842964264956
At time: 367.53498554229736 and batch: 200, loss is 4.177124614715576 and perplexity is 65.17817115999311
At time: 368.48477435112 and batch: 250, loss is 4.197177667617797 and perplexity is 66.49838539667466
At time: 369.4195206165314 and batch: 300, loss is 4.184676389694214 and perplexity is 65.67224526227781
At time: 370.3554878234863 and batch: 350, loss is 4.268392839431763 and perplexity is 71.40678118615621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.907930045292296 and perplexity of 135.3589373516092
Finished Training.
Improved accuracyfrom -10000000 to -135.3589373516092
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f43fb50a898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'lr': 1.0239259495251651, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 4.640018865507108, 'dropout': 0.9235342072397739, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1730828285217285 and batch: 50, loss is 9.155524291992187 and perplexity is 9466.592415261985
At time: 2.1284127235412598 and batch: 100, loss is 8.30634370803833 and perplexity is 4049.479802558801
At time: 3.071455478668213 and batch: 150, loss is 7.704218826293945 and perplexity is 2217.684308763431
At time: 4.014978408813477 and batch: 200, loss is 7.2969582080841064 and perplexity is 1475.8040045294651
At time: 4.957780122756958 and batch: 250, loss is 7.083689994812012 and perplexity is 1192.3602138626288
At time: 5.9008941650390625 and batch: 300, loss is 6.913713245391846 and perplexity is 1005.9757503928272
At time: 6.844675064086914 and batch: 350, loss is 6.8068852043151855 and perplexity is 904.0504843160081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.410899195177802 and perplexity of 608.4405419585942
Finished 1 epochs...
Completing Train Step...
At time: 8.620124101638794 and batch: 50, loss is 5.971581106185913 and perplexity is 392.12517240762503
At time: 9.563485145568848 and batch: 100, loss is 5.688722505569458 and perplexity is 295.51585953331323
At time: 10.495002508163452 and batch: 150, loss is 5.54787576675415 and perplexity is 256.6917033018202
At time: 11.427022218704224 and batch: 200, loss is 5.424024276733398 and perplexity is 226.78995408923296
At time: 12.358258962631226 and batch: 250, loss is 5.367693901062012 and perplexity is 214.36794356466342
At time: 13.28964638710022 and batch: 300, loss is 5.313531818389893 and perplexity is 203.066156165221
At time: 14.220946311950684 and batch: 350, loss is 5.309623918533325 and perplexity is 202.27414252561152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.383454947636046 and perplexity of 217.77337277627248
Finished 2 epochs...
Completing Train Step...
At time: 15.99979043006897 and batch: 50, loss is 5.214548768997193 and perplexity is 183.92880787159007
At time: 16.932530403137207 and batch: 100, loss is 5.132997961044311 and perplexity is 169.52458508373135
At time: 17.864948987960815 and batch: 150, loss is 5.102116594314575 and perplexity is 164.3694428024471
At time: 18.797155618667603 and batch: 200, loss is 5.063859214782715 and perplexity is 158.19986702491323
At time: 19.741945028305054 and batch: 250, loss is 5.046605052947998 and perplexity is 155.49367459526974
At time: 20.67549705505371 and batch: 300, loss is 5.028360080718994 and perplexity is 152.68242043109657
At time: 21.608471393585205 and batch: 350, loss is 5.059894485473633 and perplexity is 157.57388911189872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.16864066288389 and perplexity of 175.67587237006978
Finished 3 epochs...
Completing Train Step...
At time: 23.38886833190918 and batch: 50, loss is 4.99105206489563 and perplexity is 147.09109149486417
At time: 24.322200775146484 and batch: 100, loss is 4.922460489273071 and perplexity is 137.34012169435536
At time: 25.255573272705078 and batch: 150, loss is 4.899132871627808 and perplexity is 134.173383679308
At time: 26.188644647598267 and batch: 200, loss is 4.88874418258667 and perplexity is 132.7867134329891
At time: 27.122189044952393 and batch: 250, loss is 4.882955303192139 and perplexity is 132.02024779273106
At time: 28.05538249015808 and batch: 300, loss is 4.868781099319458 and perplexity is 130.1621654129655
At time: 28.988559007644653 and batch: 350, loss is 4.914356555938721 and perplexity is 136.23162416564088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.048567147090517 and perplexity of 155.79906733006445
Finished 4 epochs...
Completing Train Step...
At time: 30.75526523590088 and batch: 50, loss is 4.851972560882569 and perplexity is 127.9926142243507
At time: 31.70099973678589 and batch: 100, loss is 4.7889629745483395 and perplexity is 120.17667774704356
At time: 32.64297938346863 and batch: 150, loss is 4.763058528900147 and perplexity is 117.10354330012409
At time: 33.57511377334595 and batch: 200, loss is 4.7717214202880855 and perplexity is 118.12240535075888
At time: 34.507319688797 and batch: 250, loss is 4.771352939605713 and perplexity is 118.07888754445958
At time: 35.439576864242554 and batch: 300, loss is 4.755491790771484 and perplexity is 116.22079542687423
At time: 36.37303900718689 and batch: 350, loss is 4.810861854553223 and perplexity is 122.83743991585453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.96728199925916 and perplexity of 143.63595370401205
Finished 5 epochs...
Completing Train Step...
At time: 38.15256357192993 and batch: 50, loss is 4.749786996841431 and perplexity is 115.55966733136574
At time: 39.089123249053955 and batch: 100, loss is 4.690399055480957 and perplexity is 108.8966269337681
At time: 40.02392387390137 and batch: 150, loss is 4.6604162693023685 and perplexity is 105.68006436847101
At time: 40.95921754837036 and batch: 200, loss is 4.683248167037964 and perplexity is 108.12069690373099
At time: 41.90979313850403 and batch: 250, loss is 4.685948219299316 and perplexity is 108.41302290605182
At time: 42.85625171661377 and batch: 300, loss is 4.66730920791626 and perplexity is 106.41102691068242
At time: 43.79067659378052 and batch: 350, loss is 4.729621362686157 and perplexity is 113.25267254161496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.908271000303071 and perplexity of 135.40509652820663
Finished 6 epochs...
Completing Train Step...
At time: 45.57885122299194 and batch: 50, loss is 4.668619966506958 and perplexity is 106.55059753006188
At time: 46.51210594177246 and batch: 100, loss is 4.611547441482544 and perplexity is 100.63976334837903
At time: 47.45824909210205 and batch: 150, loss is 4.577634172439575 and perplexity is 97.28396465147635
At time: 48.39536261558533 and batch: 200, loss is 4.6113204002380375 and perplexity is 100.61691656494082
At time: 49.33319306373596 and batch: 250, loss is 4.616516036987305 and perplexity is 101.14104592807733
At time: 50.265141010284424 and batch: 300, loss is 4.595135173797607 and perplexity is 99.00151705426211
At time: 51.19748616218567 and batch: 350, loss is 4.662049770355225 and perplexity is 105.85283393610099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.862447278252963 and perplexity of 129.34034692067763
Finished 7 epochs...
Completing Train Step...
At time: 52.96619939804077 and batch: 50, loss is 4.601042890548706 and perplexity is 99.58812101387355
At time: 53.91493058204651 and batch: 100, loss is 4.54579948425293 and perplexity is 94.23573708872983
At time: 54.85107374191284 and batch: 150, loss is 4.508361139297485 and perplexity is 90.77293235033403
At time: 55.78678631782532 and batch: 200, loss is 4.550592565536499 and perplexity is 94.68850083633363
At time: 56.72151207923889 and batch: 250, loss is 4.557936763763427 and perplexity is 95.38647183776514
At time: 57.65707349777222 and batch: 300, loss is 4.5339539432525635 and perplexity is 93.12604920364366
At time: 58.6016149520874 and batch: 350, loss is 4.604072065353393 and perplexity is 99.89024820791285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.825566785088901 and perplexity of 124.657102150946
Finished 8 epochs...
Completing Train Step...
At time: 60.370492696762085 and batch: 50, loss is 4.542960186004638 and perplexity is 93.96855321233787
At time: 61.31684947013855 and batch: 100, loss is 4.489476366043091 and perplexity is 89.07479107939234
At time: 62.24912881851196 and batch: 150, loss is 4.448936834335327 and perplexity is 85.53595675119755
At time: 63.18174695968628 and batch: 200, loss is 4.4981533908844 and perplexity is 89.85105822851834
At time: 64.11354851722717 and batch: 250, loss is 4.507079858779907 and perplexity is 90.65670123882062
At time: 65.04509139060974 and batch: 300, loss is 4.480669574737549 and perplexity is 88.2937721670062
At time: 65.97721815109253 and batch: 350, loss is 4.553137149810791 and perplexity is 94.92975051641659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.795048680798761 and perplexity of 120.91026764819637
Finished 9 epochs...
Completing Train Step...
At time: 67.75265574455261 and batch: 50, loss is 4.492112474441528 and perplexity is 89.30991164974517
At time: 68.68645644187927 and batch: 100, loss is 4.4400356483459475 and perplexity is 84.7779638141433
At time: 69.62184524536133 and batch: 150, loss is 4.396911725997925 and perplexity is 81.19971408152317
At time: 70.55871868133545 and batch: 200, loss is 4.45197681427002 and perplexity is 85.79637998355844
At time: 71.51476049423218 and batch: 250, loss is 4.461819496154785 and perplexity is 86.64501603474841
At time: 72.44744205474854 and batch: 300, loss is 4.433447303771973 and perplexity is 84.22125329036159
At time: 73.3802638053894 and batch: 350, loss is 4.507672710418701 and perplexity is 90.71046314756093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.770314709893588 and perplexity of 117.95635815287024
Finished 10 epochs...
Completing Train Step...
At time: 75.1604323387146 and batch: 50, loss is 4.4467665958404545 and perplexity is 85.35052461392272
At time: 76.09412932395935 and batch: 100, loss is 4.39598747253418 and perplexity is 81.12469963604876
At time: 77.02827954292297 and batch: 150, loss is 4.35053503036499 and perplexity is 77.51992734693187
At time: 77.97125172615051 and batch: 200, loss is 4.410585622787476 and perplexity is 82.31765648682024
At time: 78.90720891952515 and batch: 250, loss is 4.420869522094726 and perplexity is 83.16857083680846
At time: 79.85085535049438 and batch: 300, loss is 4.390944948196411 and perplexity is 80.71665601330095
At time: 80.7871618270874 and batch: 350, loss is 4.466666011810303 and perplexity is 87.06596169734968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.749474361025054 and perplexity of 115.5235448873084
Finished 11 epochs...
Completing Train Step...
At time: 82.55276250839233 and batch: 50, loss is 4.40560754776001 and perplexity is 81.9088912927876
At time: 83.49806427955627 and batch: 100, loss is 4.356208543777466 and perplexity is 77.96098769226334
At time: 84.4300889968872 and batch: 150, loss is 4.308632717132569 and perplexity is 74.33877728532129
At time: 85.36295080184937 and batch: 200, loss is 4.372975597381592 and perplexity is 79.27918400368901
At time: 86.2963387966156 and batch: 250, loss is 4.383551454544067 and perplexity is 80.12207863992927
At time: 87.2296621799469 and batch: 300, loss is 4.3521942615509035 and perplexity is 77.64865759500331
At time: 88.16230034828186 and batch: 350, loss is 4.4291634845733645 and perplexity is 83.8612363429001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.731228532462285 and perplexity of 113.43483515790469
Finished 12 epochs...
Completing Train Step...
At time: 89.94180202484131 and batch: 50, loss is 4.367955713272095 and perplexity is 78.88220890580999
At time: 90.87647247314453 and batch: 100, loss is 4.319847631454468 and perplexity is 75.17717278284145
At time: 91.82249784469604 and batch: 150, loss is 4.270306205749511 and perplexity is 71.54353930860695
At time: 92.77417850494385 and batch: 200, loss is 4.3384255695343015 and perplexity is 76.58686367406162
At time: 93.7151427268982 and batch: 250, loss is 4.349210929870606 and perplexity is 77.41735109842455
At time: 94.65522837638855 and batch: 300, loss is 4.316654272079468 and perplexity is 74.93748795692537
At time: 95.63426232337952 and batch: 350, loss is 4.394598579406738 and perplexity is 81.0121043077972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.715588799838362 and perplexity of 111.67454577404884
Finished 13 epochs...
Completing Train Step...
At time: 97.53072810173035 and batch: 50, loss is 4.3333719539642335 and perplexity is 76.20079943909198
At time: 98.46300506591797 and batch: 100, loss is 4.2862327289581295 and perplexity is 72.69210116837012
At time: 99.39608001708984 and batch: 150, loss is 4.234959850311279 and perplexity is 69.05890598427736
At time: 100.32897996902466 and batch: 200, loss is 4.306340074539184 and perplexity is 74.1685402590723
At time: 101.2617769241333 and batch: 250, loss is 4.317288703918457 and perplexity is 74.98504576971924
At time: 102.19741559028625 and batch: 300, loss is 4.283670778274536 and perplexity is 72.50610594711515
At time: 103.13034129142761 and batch: 350, loss is 4.362600564956665 and perplexity is 78.46091203809162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702007951407597 and perplexity of 110.1681628265264
Finished 14 epochs...
Completing Train Step...
At time: 104.91749000549316 and batch: 50, loss is 4.301281585693359 and perplexity is 73.79430685122
At time: 105.86706209182739 and batch: 100, loss is 4.254971809387207 and perplexity is 70.45483099557671
At time: 106.80207228660583 and batch: 150, loss is 4.202179832458496 and perplexity is 66.83185462075423
At time: 107.73735022544861 and batch: 200, loss is 4.276445055007935 and perplexity is 71.98408515058034
At time: 108.67285323143005 and batch: 250, loss is 4.287445688247681 and perplexity is 72.78032722424932
At time: 109.60895991325378 and batch: 300, loss is 4.25306339263916 and perplexity is 70.3205020347127
At time: 110.54493808746338 and batch: 350, loss is 4.332858257293701 and perplexity is 76.16166539450298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.690103465113147 and perplexity of 108.86444289663949
Finished 15 epochs...
Completing Train Step...
At time: 112.32284951210022 and batch: 50, loss is 4.271275901794434 and perplexity is 71.61294844315705
At time: 113.27702116966248 and batch: 100, loss is 4.225715465545655 and perplexity is 68.42344065436737
At time: 114.208731174469 and batch: 150, loss is 4.171682243347168 and perplexity is 64.8244108680492
At time: 115.1414110660553 and batch: 200, loss is 4.248498387336731 and perplexity is 70.00022017046231
At time: 116.07379460334778 and batch: 250, loss is 4.2594371509552005 and perplexity is 70.77013933689065
At time: 117.00537657737732 and batch: 300, loss is 4.224512796401978 and perplexity is 68.3411993580358
At time: 117.93908143043518 and batch: 350, loss is 4.305035104751587 and perplexity is 74.0718156798972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.679840087890625 and perplexity of 107.75284020963647
Finished 16 epochs...
Completing Train Step...
At time: 119.72293734550476 and batch: 50, loss is 4.243115348815918 and perplexity is 69.62441867325659
At time: 120.65529680252075 and batch: 100, loss is 4.198205852508545 and perplexity is 66.56679319370379
At time: 121.6062490940094 and batch: 150, loss is 4.143192920684815 and perplexity is 63.00366634715596
At time: 122.55022096633911 and batch: 200, loss is 4.222233629226684 and perplexity is 68.18561570766283
At time: 123.49294805526733 and batch: 250, loss is 4.2330140781402585 and perplexity is 68.92466373162547
At time: 124.42757821083069 and batch: 300, loss is 4.197723331451416 and perplexity is 66.53468106229512
At time: 125.36374640464783 and batch: 350, loss is 4.27881344795227 and perplexity is 72.15477379906403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.671112060546875 and perplexity of 106.81646278212418
Finished 17 epochs...
Completing Train Step...
At time: 127.15689396858215 and batch: 50, loss is 4.216551351547241 and perplexity is 67.79926482152635
At time: 128.09040546417236 and batch: 100, loss is 4.172167530059815 and perplexity is 64.85587692771142
At time: 129.02482318878174 and batch: 150, loss is 4.116405425071716 and perplexity is 61.33836018201257
At time: 129.95946550369263 and batch: 200, loss is 4.19744827747345 and perplexity is 66.51638295019585
At time: 130.89363884925842 and batch: 250, loss is 4.2080388879776 and perplexity is 67.22457553076919
At time: 131.82961869239807 and batch: 300, loss is 4.172482547760009 and perplexity is 64.87631089526705
At time: 132.7647557258606 and batch: 350, loss is 4.25402006149292 and perplexity is 70.38780765825365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.663857821760507 and perplexity of 106.04439442467249
Finished 18 epochs...
Completing Train Step...
At time: 134.54898881912231 and batch: 50, loss is 4.1913806915283205 and perplexity is 66.11401102746079
At time: 135.49609446525574 and batch: 100, loss is 4.147548861503601 and perplexity is 63.278705180684476
At time: 136.43046307563782 and batch: 150, loss is 4.090952525138855 and perplexity is 59.79682256251829
At time: 137.36305904388428 and batch: 200, loss is 4.17396152973175 and perplexity is 64.97233277928879
At time: 138.2958378791809 and batch: 250, loss is 4.1843463325500485 and perplexity is 65.65057324525435
At time: 139.2299358844757 and batch: 300, loss is 4.148540134429932 and perplexity is 63.34146274774299
At time: 140.1670241355896 and batch: 350, loss is 4.230575828552246 and perplexity is 68.75681291292966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.657583302464978 and perplexity of 105.38109992886325
Finished 19 epochs...
Completing Train Step...
At time: 141.97625088691711 and batch: 50, loss is 4.167443909645081 and perplexity is 64.55024479681347
At time: 142.91247820854187 and batch: 100, loss is 4.1240602350234985 and perplexity is 61.80969536103741
At time: 143.86304664611816 and batch: 150, loss is 4.066682887077332 and perplexity is 58.363044336752324
At time: 144.80095267295837 and batch: 200, loss is 4.151577744483948 and perplexity is 63.53416193614687
At time: 145.73867869377136 and batch: 250, loss is 4.161795325279236 and perplexity is 64.18665514339779
At time: 146.67688155174255 and batch: 300, loss is 4.125775508880615 and perplexity is 61.91580689475994
At time: 147.6156690120697 and batch: 350, loss is 4.208303985595703 and perplexity is 67.24239896799136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.652313758587015 and perplexity of 104.82725014835225
Finished 20 epochs...
Completing Train Step...
At time: 149.40967297554016 and batch: 50, loss is 4.144729056358337 and perplexity is 63.10052289995141
At time: 150.34589266777039 and batch: 100, loss is 4.1016690540313725 and perplexity is 60.441082890626674
At time: 151.28166818618774 and batch: 150, loss is 4.043569221496582 and perplexity is 57.029531005921925
At time: 152.2179434299469 and batch: 200, loss is 4.13023129940033 and perplexity is 62.19230631441877
At time: 153.15297412872314 and batch: 250, loss is 4.140248250961304 and perplexity is 62.81841424538227
At time: 154.09392786026 and batch: 300, loss is 4.104032607078552 and perplexity is 60.58410755287324
At time: 155.02948379516602 and batch: 350, loss is 4.187055382728577 and perplexity is 65.82866506334341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.647468829977101 and perplexity of 104.32059794296012
Finished 21 epochs...
Completing Train Step...
At time: 156.81952619552612 and batch: 50, loss is 4.123089370727539 and perplexity is 61.74971565545584
At time: 157.77675223350525 and batch: 100, loss is 4.080316891670227 and perplexity is 59.164215526626904
At time: 158.71475982666016 and batch: 150, loss is 4.021552958488464 and perplexity is 55.787674533312
At time: 159.6529679298401 and batch: 200, loss is 4.109814581871032 and perplexity is 60.935417990889476
At time: 160.59132313728333 and batch: 250, loss is 4.1196533203125 and perplexity is 61.53790462443442
At time: 161.52959322929382 and batch: 300, loss is 4.083301811218262 and perplexity is 59.3410797815645
At time: 162.46773743629456 and batch: 350, loss is 4.166740918159485 and perplexity is 64.50488247086084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.643584678912985 and perplexity of 103.91618688670118
Finished 22 epochs...
Completing Train Step...
At time: 164.2455506324768 and batch: 50, loss is 4.102435727119445 and perplexity is 60.487439210093555
At time: 165.1997742652893 and batch: 100, loss is 4.059901218414307 and perplexity is 57.96858456742306
At time: 166.13248825073242 and batch: 150, loss is 4.000400395393371 and perplexity is 54.620015257979645
At time: 167.07694673538208 and batch: 200, loss is 4.090253000259399 and perplexity is 59.75500782435338
At time: 168.00981783866882 and batch: 250, loss is 4.099919743537903 and perplexity is 60.3354450936812
At time: 168.94151210784912 and batch: 300, loss is 4.063484320640564 and perplexity is 58.176664495043916
At time: 169.87306356430054 and batch: 350, loss is 4.147253684997558 and perplexity is 63.26002955002161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.639673693426724 and perplexity of 103.51056589393245
Finished 23 epochs...
Completing Train Step...
At time: 171.6519536972046 and batch: 50, loss is 4.082672748565674 and perplexity is 59.3037622632688
At time: 172.58521580696106 and batch: 100, loss is 4.040284609794616 and perplexity is 56.842518440986645
At time: 173.51920890808105 and batch: 150, loss is 3.980170578956604 and perplexity is 53.52616388599191
At time: 174.45215225219727 and batch: 200, loss is 4.071417741775512 and perplexity is 58.64004012133263
At time: 175.3855242729187 and batch: 250, loss is 4.080966639518738 and perplexity is 59.20266983982491
At time: 176.31828260421753 and batch: 300, loss is 4.044555377960205 and perplexity is 57.085798786398485
At time: 177.26285362243652 and batch: 350, loss is 4.128514833450318 and perplexity is 62.08564690308841
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6371265279835665 and perplexity of 103.24724286351857
Finished 24 epochs...
Completing Train Step...
At time: 179.04101252555847 and batch: 50, loss is 4.0637255859375 and perplexity is 58.19070219861535
At time: 179.9746789932251 and batch: 100, loss is 4.021449484825134 and perplexity is 55.78190227690278
At time: 180.90946412086487 and batch: 150, loss is 3.960843286514282 and perplexity is 52.501581161936706
At time: 181.84541249275208 and batch: 200, loss is 4.053277859687805 and perplexity is 57.58590654285255
At time: 182.77943444252014 and batch: 250, loss is 4.06275580406189 and perplexity is 58.13429726495591
At time: 183.71276354789734 and batch: 300, loss is 4.026327247619629 and perplexity is 56.05465784211307
At time: 184.65742874145508 and batch: 350, loss is 4.110498685836792 and perplexity is 60.97711841407837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.635276005185884 and perplexity of 103.05635815952267
Finished 25 epochs...
Completing Train Step...
At time: 186.42885375022888 and batch: 50, loss is 4.04555995464325 and perplexity is 57.14317466320108
At time: 187.37358736991882 and batch: 100, loss is 4.003304195404053 and perplexity is 54.778851361399056
At time: 188.3066930770874 and batch: 150, loss is 3.9423361253738403 and perplexity is 51.53886201707477
At time: 189.2401077747345 and batch: 200, loss is 4.035804824829102 and perplexity is 56.588445701995056
At time: 190.17282128334045 and batch: 250, loss is 4.045249886512757 and perplexity is 57.125459132515495
At time: 191.11810040473938 and batch: 300, loss is 4.00873746395111 and perplexity is 55.07728958488354
At time: 192.051922082901 and batch: 350, loss is 4.093107738494873 and perplexity is 59.92583644944006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.63378958866514 and perplexity of 102.90328727791362
Finished 26 epochs...
Completing Train Step...
At time: 193.83497166633606 and batch: 50, loss is 4.028139095306397 and perplexity is 56.15631240774243
At time: 194.76939606666565 and batch: 100, loss is 3.9857822275161743 and perplexity is 53.827378270644545
At time: 195.70409178733826 and batch: 150, loss is 3.9244731855392456 and perplexity is 50.626400312563774
At time: 196.63942408561707 and batch: 200, loss is 4.018958916664124 and perplexity is 55.643146509204264
At time: 197.57462620735168 and batch: 250, loss is 4.028429088592529 and perplexity is 56.17259972280648
At time: 198.50856256484985 and batch: 300, loss is 3.99177827835083 and perplexity is 54.15109952170522
At time: 199.4439640045166 and batch: 350, loss is 4.0763012266159055 and perplexity is 58.92710824420834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.632954564587823 and perplexity of 102.8173964208591
Finished 27 epochs...
Completing Train Step...
At time: 201.24585556983948 and batch: 50, loss is 4.011333031654358 and perplexity is 55.22043210660338
At time: 202.17840838432312 and batch: 100, loss is 3.9688755083084106 and perplexity is 52.924983661392496
At time: 203.1114640235901 and batch: 150, loss is 3.9072058248519896 and perplexity is 49.75972017149905
At time: 204.04494976997375 and batch: 200, loss is 4.002786779403687 and perplexity is 54.75051523863682
At time: 204.97822427749634 and batch: 250, loss is 4.012133345603943 and perplexity is 55.26464347784921
At time: 205.91007471084595 and batch: 300, loss is 3.9753634929656982 and perplexity is 53.26947646697246
At time: 206.84254693984985 and batch: 350, loss is 4.059955592155457 and perplexity is 57.971736621928855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.632288965685614 and perplexity of 102.74898404480024
Finished 28 epochs...
Completing Train Step...
At time: 208.619713306427 and batch: 50, loss is 3.9951461267471315 and perplexity is 54.33377966222723
At time: 209.56749272346497 and batch: 100, loss is 3.9525452136993406 and perplexity is 52.06772180637224
At time: 210.50250792503357 and batch: 150, loss is 3.8905205965042113 and perplexity is 48.93635598935617
At time: 211.43753337860107 and batch: 200, loss is 3.987060604095459 and perplexity is 53.89623393270564
At time: 212.37343859672546 and batch: 250, loss is 3.9964011096954346 and perplexity is 54.40201043449286
At time: 213.30956435203552 and batch: 300, loss is 3.9593403720855713 and perplexity is 52.422735042404796
At time: 214.2447509765625 and batch: 350, loss is 4.044100112915039 and perplexity is 57.05981553271803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631689137425916 and perplexity of 102.68737078105059
Finished 29 epochs...
Completing Train Step...
At time: 216.02095794677734 and batch: 50, loss is 3.979481806755066 and perplexity is 53.48930924593506
At time: 216.9668264389038 and batch: 100, loss is 3.936723074913025 and perplexity is 51.25038216726839
At time: 217.90921211242676 and batch: 150, loss is 3.874431266784668 and perplexity is 48.15530298122612
At time: 218.84181594848633 and batch: 200, loss is 3.971818504333496 and perplexity is 53.08097110055555
At time: 219.77568697929382 and batch: 250, loss is 3.981178798675537 and perplexity is 53.580157233905915
At time: 220.7079839706421 and batch: 300, loss is 3.943746790885925 and perplexity is 51.611617416955944
At time: 221.6395161151886 and batch: 350, loss is 4.028702020645142 and perplexity is 56.187933118141764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631941696693158 and perplexity of 102.71330870346353
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 223.4211688041687 and batch: 50, loss is 3.969212670326233 and perplexity is 52.94283096422423
At time: 224.35360527038574 and batch: 100, loss is 3.92138614654541 and perplexity is 50.47035562262567
At time: 225.28650832176208 and batch: 150, loss is 3.8489780807495118 and perplexity is 46.94506464514041
At time: 226.21848845481873 and batch: 200, loss is 3.9388625383377076 and perplexity is 51.3601478633931
At time: 227.1523756980896 and batch: 250, loss is 3.9403005409240723 and perplexity is 51.43405701689759
At time: 228.08793354034424 and batch: 300, loss is 3.8907450246810913 and perplexity is 48.94733991901956
At time: 229.02428913116455 and batch: 350, loss is 3.965338625907898 and perplexity is 52.73812486172611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.610256852774785 and perplexity of 100.50996258396704
Finished 31 epochs...
Completing Train Step...
At time: 230.81642293930054 and batch: 50, loss is 3.952003026008606 and perplexity is 52.039498980250066
At time: 231.7629313468933 and batch: 100, loss is 3.905983452796936 and perplexity is 49.69893244027514
At time: 232.69891047477722 and batch: 150, loss is 3.8362084817886353 and perplexity is 46.34940624995638
At time: 233.63615012168884 and batch: 200, loss is 3.9292952966690065 and perplexity is 50.871115989873665
At time: 234.5725598335266 and batch: 250, loss is 3.933930959701538 and perplexity is 51.10748478144152
At time: 235.50844764709473 and batch: 300, loss is 3.8874290370941162 and perplexity is 48.78529995716323
At time: 236.44498658180237 and batch: 350, loss is 3.9659708452224733 and perplexity is 52.771477464847194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.609704905542834 and perplexity of 100.4545016954853
Finished 32 epochs...
Completing Train Step...
At time: 238.22218823432922 and batch: 50, loss is 3.943972544670105 and perplexity is 51.62327025018158
At time: 239.16709780693054 and batch: 100, loss is 3.898476758003235 and perplexity is 49.327254504635874
At time: 240.0993959903717 and batch: 150, loss is 3.8293427991867066 and perplexity is 46.03227584194822
At time: 241.04062509536743 and batch: 200, loss is 3.923647952079773 and perplexity is 50.584638946901315
At time: 241.97592520713806 and batch: 250, loss is 3.929860625267029 and perplexity is 50.899883017201525
At time: 242.91081857681274 and batch: 300, loss is 3.8847264575958254 and perplexity is 48.65363180764307
At time: 243.84485411643982 and batch: 350, loss is 3.9650341033935548 and perplexity is 52.7220673604009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.609770676185345 and perplexity of 100.4611088698816
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 245.62821626663208 and batch: 50, loss is 3.94278461933136 and perplexity is 51.56198206948123
At time: 246.56257271766663 and batch: 100, loss is 3.897992005348206 and perplexity is 49.303348781698915
At time: 247.49715423583984 and batch: 150, loss is 3.825529251098633 and perplexity is 45.85706384645116
At time: 248.43271040916443 and batch: 200, loss is 3.917335271835327 and perplexity is 50.266320075655166
At time: 249.36798882484436 and batch: 250, loss is 3.92043776512146 and perplexity is 50.422513164928
At time: 250.30274081230164 and batch: 300, loss is 3.8731625413894655 and perplexity is 48.09424586596663
At time: 251.2379994392395 and batch: 350, loss is 3.949571523666382 and perplexity is 51.91311852604221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.605434549265895 and perplexity of 100.02643982248543
Finished 34 epochs...
Completing Train Step...
At time: 253.01765537261963 and batch: 50, loss is 3.938320183753967 and perplexity is 51.33230000416747
At time: 253.9498815536499 and batch: 100, loss is 3.8928364896774292 and perplexity is 49.04981869515518
At time: 254.88206553459167 and batch: 150, loss is 3.82198974609375 and perplexity is 45.69503945169023
At time: 255.8145968914032 and batch: 200, loss is 3.9146313428878785 and perplexity is 50.13058710657092
At time: 256.74737334251404 and batch: 250, loss is 3.9186954212188723 and perplexity is 50.334736297512606
At time: 257.67978143692017 and batch: 300, loss is 3.872889127731323 and perplexity is 48.081098039748355
At time: 258.61278676986694 and batch: 350, loss is 3.9506162357330323 and perplexity is 51.967381126845446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.605180411503233 and perplexity of 100.00102255674224
Finished 35 epochs...
Completing Train Step...
At time: 260.38804292678833 and batch: 50, loss is 3.9361037969589234 and perplexity is 51.21865376081824
At time: 261.34913969039917 and batch: 100, loss is 3.8904007053375245 and perplexity is 48.93048930423209
At time: 262.28324699401855 and batch: 150, loss is 3.820003237724304 and perplexity is 45.60435597494141
At time: 263.2391209602356 and batch: 200, loss is 3.9131295251846314 and perplexity is 50.055356608766736
At time: 264.17456245422363 and batch: 250, loss is 3.917704939842224 and perplexity is 50.284905360992624
At time: 265.1098484992981 and batch: 300, loss is 3.8726610422134398 and perplexity is 48.070132688168044
At time: 266.05700755119324 and batch: 350, loss is 3.9509288263320923 and perplexity is 51.98362818084934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.605090963429418 and perplexity of 99.99207805793503
Finished 36 epochs...
Completing Train Step...
At time: 267.8241620063782 and batch: 50, loss is 3.9342331409454347 and perplexity is 51.12293083840203
At time: 268.7679419517517 and batch: 100, loss is 3.8885257863998413 and perplexity is 48.83883455266985
At time: 269.70017194747925 and batch: 150, loss is 3.8184322929382324 and perplexity is 45.532770292999324
At time: 270.63175320625305 and batch: 200, loss is 3.9118998050689697 and perplexity is 49.99384036148381
At time: 271.5630855560303 and batch: 250, loss is 3.916840977668762 and perplexity is 50.24147986655806
At time: 272.4950089454651 and batch: 300, loss is 3.8723450660705567 and perplexity is 48.05494607248384
At time: 273.426965713501 and batch: 350, loss is 3.9509498023986818 and perplexity is 51.98471860433198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.605067812163254 and perplexity of 99.98976314151838
Finished 37 epochs...
Completing Train Step...
At time: 275.207133769989 and batch: 50, loss is 3.932558312416077 and perplexity is 51.03738035652114
At time: 276.14028906822205 and batch: 100, loss is 3.886914963722229 and perplexity is 48.760227178691785
At time: 277.0737178325653 and batch: 150, loss is 3.817055459022522 and perplexity is 45.47012236837402
At time: 278.00690031051636 and batch: 200, loss is 3.910790719985962 and perplexity is 49.938423675487044
At time: 278.93967056274414 and batch: 250, loss is 3.9160191726684572 and perplexity is 50.200208128164284
At time: 279.8746898174286 and batch: 300, loss is 3.871964092254639 and perplexity is 48.03664188323467
At time: 280.8082399368286 and batch: 350, loss is 3.9508172082901 and perplexity is 51.977826193865376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.605080966291757 and perplexity of 99.99107842836243
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 282.58581161499023 and batch: 50, loss is 3.9328271484375 and perplexity is 51.0511028872727
At time: 283.51939702033997 and batch: 100, loss is 3.8875131225585937 and perplexity is 48.789402264239584
At time: 284.4521155357361 and batch: 150, loss is 3.8167591285705567 and perplexity is 45.456650182669705
At time: 285.3862683773041 and batch: 200, loss is 3.909036355018616 and perplexity is 49.850890259696186
At time: 286.31957030296326 and batch: 250, loss is 3.914448733329773 and perplexity is 50.12143361799451
At time: 287.26483058929443 and batch: 300, loss is 3.868541965484619 and perplexity is 47.87253536191766
At time: 288.19822692871094 and batch: 350, loss is 3.9459685039520265 and perplexity is 51.72641109382117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603929190800108 and perplexity of 99.87597745281074
Finished 39 epochs...
Completing Train Step...
At time: 289.9647810459137 and batch: 50, loss is 3.93133282661438 and perplexity is 50.97487308025107
At time: 290.91008973121643 and batch: 100, loss is 3.886141886711121 and perplexity is 48.722546334977196
At time: 291.84252738952637 and batch: 150, loss is 3.8158568048477175 and perplexity is 45.4156520684163
At time: 292.77484679222107 and batch: 200, loss is 3.90830659866333 and perplexity is 49.81452452638888
At time: 293.70843505859375 and batch: 250, loss is 3.9139288377761843 and perplexity is 50.0953824800392
At time: 294.6411895751953 and batch: 300, loss is 3.8687024450302125 and perplexity is 47.88021854111902
At time: 295.57341146469116 and batch: 350, loss is 3.946435260772705 and perplexity is 51.75056038449306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603905513368804 and perplexity of 99.87361267421169
Finished 40 epochs...
Completing Train Step...
At time: 297.3546829223633 and batch: 50, loss is 3.9306866788864134 and perplexity is 50.9419464207156
At time: 298.28947043418884 and batch: 100, loss is 3.8854549407958983 and perplexity is 48.68908807411669
At time: 299.22429966926575 and batch: 150, loss is 3.815327515602112 and perplexity is 45.39162041260385
At time: 300.1593906879425 and batch: 200, loss is 3.9079056930541993 and perplexity is 49.79455760678254
At time: 301.09541034698486 and batch: 250, loss is 3.9136641693115233 and perplexity is 50.082125566492465
At time: 302.0299446582794 and batch: 300, loss is 3.868777480125427 and perplexity is 47.88381137266869
At time: 302.96408200263977 and batch: 350, loss is 3.9466862535476683 and perplexity is 51.76355103146094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603891833075162 and perplexity of 99.87224638320887
Finished 41 epochs...
Completing Train Step...
At time: 304.74111771583557 and batch: 50, loss is 3.930177011489868 and perplexity is 50.915989586746285
At time: 305.6737959384918 and batch: 100, loss is 3.884929871559143 and perplexity is 48.663529642363635
At time: 306.60567688941956 and batch: 150, loss is 3.8149054765701296 and perplexity is 45.37246741900453
At time: 307.5380811691284 and batch: 200, loss is 3.9075921630859374 and perplexity is 49.77894796788913
At time: 308.48190784454346 and batch: 250, loss is 3.913468179702759 and perplexity is 50.07231095210923
At time: 309.41372537612915 and batch: 300, loss is 3.8687987899780274 and perplexity is 47.88483178050331
At time: 310.35883498191833 and batch: 350, loss is 3.946826639175415 and perplexity is 51.77081840017202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6038818359375 and perplexity of 99.87124795160395
Finished 42 epochs...
Completing Train Step...
At time: 312.1285367012024 and batch: 50, loss is 3.9297248983383177 and perplexity is 50.892975001220336
At time: 313.07670068740845 and batch: 100, loss is 3.8844764757156374 and perplexity is 48.64147080136476
At time: 314.01228618621826 and batch: 150, loss is 3.8145358514785768 and perplexity is 45.35569971565365
At time: 314.95852637290955 and batch: 200, loss is 3.907314476966858 and perplexity is 49.76512696405534
At time: 315.8926887512207 and batch: 250, loss is 3.91329607963562 and perplexity is 50.063694245521745
At time: 316.82754039764404 and batch: 300, loss is 3.8687884616851806 and perplexity is 47.88433721449179
At time: 317.7619001865387 and batch: 350, loss is 3.946905508041382 and perplexity is 51.77490166692861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603877100451239 and perplexity of 99.87077501380121
Finished 43 epochs...
Completing Train Step...
At time: 319.53503251075745 and batch: 50, loss is 3.9293065786361696 and perplexity is 50.871689919371335
At time: 320.4924898147583 and batch: 100, loss is 3.884064564704895 and perplexity is 48.62143896991217
At time: 321.4264168739319 and batch: 150, loss is 3.8141973924636843 and perplexity is 45.34035126776492
At time: 322.37292408943176 and batch: 200, loss is 3.9070550107955935 and perplexity is 49.75221627211582
At time: 323.33208203315735 and batch: 250, loss is 3.9131327962875364 and perplexity is 50.05552034525695
At time: 324.2813458442688 and batch: 300, loss is 3.868757972717285 and perplexity is 47.88287729272761
At time: 325.22975158691406 and batch: 350, loss is 3.946947145462036 and perplexity is 51.77705748516968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603876048120959 and perplexity of 99.87066991681588
Finished 44 epochs...
Completing Train Step...
At time: 327.0200836658478 and batch: 50, loss is 3.928911318778992 and perplexity is 50.85158635580698
At time: 327.9527847766876 and batch: 100, loss is 3.883680472373962 and perplexity is 48.60276743411132
At time: 328.8883607387543 and batch: 150, loss is 3.813879919052124 and perplexity is 45.32595919643749
At time: 329.8230490684509 and batch: 200, loss is 3.906806678771973 and perplexity is 49.73986273752196
At time: 330.75870728492737 and batch: 250, loss is 3.9129731178283693 and perplexity is 50.047528194999515
At time: 331.69453978538513 and batch: 300, loss is 3.8687134885787966 and perplexity is 47.880747311558444
At time: 332.6298928260803 and batch: 350, loss is 3.946964349746704 and perplexity is 51.77794828006865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603877626616379 and perplexity of 99.87082756233534
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 334.435396194458 and batch: 50, loss is 3.928894362449646 and perplexity is 50.850724106871276
At time: 335.3733217716217 and batch: 100, loss is 3.8837047624588013 and perplexity is 48.60394801379384
At time: 336.309757232666 and batch: 150, loss is 3.813781485557556 and perplexity is 45.321497823456994
At time: 337.2433702945709 and batch: 200, loss is 3.906361632347107 and perplexity is 49.71773111460244
At time: 338.17640924453735 and batch: 250, loss is 3.912703104019165 and perplexity is 50.03401649552513
At time: 339.11016297340393 and batch: 300, loss is 3.8673335886001587 and perplexity is 47.81472223384438
At time: 340.0532126426697 and batch: 350, loss is 3.9455685663223266 and perplexity is 51.7057278918465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603748189991918 and perplexity of 99.85790145610734
Finished 46 epochs...
Completing Train Step...
At time: 341.8326904773712 and batch: 50, loss is 3.9287034368515013 and perplexity is 50.84101632871625
At time: 342.7870194911957 and batch: 100, loss is 3.8835141468048096 and perplexity is 48.59468422339639
At time: 343.72083711624146 and batch: 150, loss is 3.813572254180908 and perplexity is 45.31201613604349
At time: 344.65409874916077 and batch: 200, loss is 3.906217670440674 and perplexity is 49.71057417042367
At time: 345.5863411426544 and batch: 250, loss is 3.912597622871399 and perplexity is 50.0287391283741
At time: 346.51956486701965 and batch: 300, loss is 3.867393045425415 and perplexity is 47.817565229945835
At time: 347.45233631134033 and batch: 350, loss is 3.9456652879714964 and perplexity is 51.71072919698307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603721355569774 and perplexity of 99.85522186297808
Finished 47 epochs...
Completing Train Step...
At time: 349.23466324806213 and batch: 50, loss is 3.928555951118469 and perplexity is 50.83351855707567
At time: 350.17032742500305 and batch: 100, loss is 3.8833675718307497 and perplexity is 48.58756198080091
At time: 351.10518503189087 and batch: 150, loss is 3.813432970046997 and perplexity is 45.30570533062796
At time: 352.04125142097473 and batch: 200, loss is 3.9061274671554567 and perplexity is 49.70609031555572
At time: 352.976277589798 and batch: 250, loss is 3.91253607749939 and perplexity is 50.02566018576161
At time: 353.911034822464 and batch: 300, loss is 3.8674306011199953 and perplexity is 47.81936108554328
At time: 354.846328496933 and batch: 350, loss is 3.9457328701019287 and perplexity is 51.714224036321426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603713463092673 and perplexity of 99.85443376103613
Finished 48 epochs...
Completing Train Step...
At time: 356.6324954032898 and batch: 50, loss is 3.9284243154525758 and perplexity is 50.82682749341168
At time: 357.56457591056824 and batch: 100, loss is 3.8832403564453126 and perplexity is 48.58138128852399
At time: 358.5098657608032 and batch: 150, loss is 3.8133216953277587 and perplexity is 45.300664231466044
At time: 359.44314098358154 and batch: 200, loss is 3.906055006980896 and perplexity is 49.70248873406192
At time: 360.376859664917 and batch: 250, loss is 3.912489581108093 and perplexity is 50.0233342271655
At time: 361.30971813201904 and batch: 300, loss is 3.867455520629883 and perplexity is 47.82055273543227
At time: 362.24272179603577 and batch: 350, loss is 3.9457819080352783 and perplexity is 51.71676005717304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603710306101832 and perplexity of 99.854118522001
Finished 49 epochs...
Completing Train Step...
At time: 364.0093595981598 and batch: 50, loss is 3.9283034324646 and perplexity is 50.82068376597849
At time: 364.9565122127533 and batch: 100, loss is 3.883125190734863 and perplexity is 48.57578670139176
At time: 365.89142632484436 and batch: 150, loss is 3.813224949836731 and perplexity is 45.29628180845434
At time: 366.8262586593628 and batch: 200, loss is 3.905990653038025 and perplexity is 49.699290285858865
At time: 367.762921333313 and batch: 250, loss is 3.9124497842788695 and perplexity is 50.02134349668871
At time: 368.69844150543213 and batch: 300, loss is 3.867472014427185 and perplexity is 47.82134148444068
At time: 369.63386392593384 and batch: 350, loss is 3.9458193349838258 and perplexity is 51.718695693913006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.603707149110991 and perplexity of 99.85380328396097
Finished Training.
Improved accuracyfrom -135.3589373516092 to -99.85380328396097
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f43f60ac390>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'lr': 21.64987812181001, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 6.560187749659181, 'dropout': 0.8202590420169474, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.15797758102417 and batch: 50, loss is 6.738095378875732 and perplexity is 843.9517957087546
At time: 2.111018180847168 and batch: 100, loss is 6.2698600959777835 and perplexity is 528.4034469386104
At time: 3.051286458969116 and batch: 150, loss is 6.184597425460815 and perplexity is 485.2175880380415
At time: 3.9951963424682617 and batch: 200, loss is 6.104060544967651 and perplexity is 447.6718762443517
At time: 4.939021587371826 and batch: 250, loss is 6.091324415206909 and perplexity is 442.00642368978924
At time: 5.87907338142395 and batch: 300, loss is 6.0652906036376955 and perplexity is 430.6478073091243
At time: 6.819509029388428 and batch: 350, loss is 6.060941066741943 and perplexity is 428.7787564815568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.905841169686153 and perplexity of 367.17595320451056
Finished 1 epochs...
Completing Train Step...
At time: 8.596890687942505 and batch: 50, loss is 5.5853941535949705 and perplexity is 266.5053060649092
At time: 9.54318904876709 and batch: 100, loss is 5.3629331874847415 and perplexity is 213.34982459452587
At time: 10.477112293243408 and batch: 150, loss is 5.2921816349029545 and perplexity is 198.77661070329094
At time: 11.412041187286377 and batch: 200, loss is 5.215127544403076 and perplexity is 184.03529215428907
At time: 12.34630036354065 and batch: 250, loss is 5.188831672668457 and perplexity is 179.2589973548805
At time: 13.287084817886353 and batch: 300, loss is 5.173671226501465 and perplexity is 176.5618476311422
At time: 14.225632667541504 and batch: 350, loss is 5.191911573410034 and perplexity is 179.81194835376596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.322860191608298 and perplexity of 204.9692958559337
Finished 2 epochs...
Completing Train Step...
At time: 16.00859022140503 and batch: 50, loss is 5.145276432037353 and perplexity is 171.6189190820149
At time: 16.944056510925293 and batch: 100, loss is 5.078613147735596 and perplexity is 160.55124060676494
At time: 17.897541046142578 and batch: 150, loss is 5.077101860046387 and perplexity is 160.30878474977183
At time: 18.83935284614563 and batch: 200, loss is 5.060846900939941 and perplexity is 157.72403641093032
At time: 19.784496307373047 and batch: 250, loss is 5.077165584564209 and perplexity is 160.31900067528173
At time: 20.721441984176636 and batch: 300, loss is 5.079662179946899 and perplexity is 160.71975240142288
At time: 21.65797209739685 and batch: 350, loss is 5.1038969612121585 and perplexity is 164.66234137348007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.308306463833513 and perplexity of 202.00783097111466
Finished 3 epochs...
Completing Train Step...
At time: 23.453589916229248 and batch: 50, loss is 5.051026573181153 and perplexity is 156.18271520456545
At time: 24.38837766647339 and batch: 100, loss is 5.019087190628052 and perplexity is 151.27315719727702
At time: 25.323403358459473 and batch: 150, loss is 5.012540073394775 and perplexity is 150.28598918144567
At time: 26.25901508331299 and batch: 200, loss is 5.036511125564576 and perplexity is 153.93202757047675
At time: 27.193549394607544 and batch: 250, loss is 5.062857522964477 and perplexity is 158.04147885377935
At time: 28.128700017929077 and batch: 300, loss is 5.0479842376708985 and perplexity is 155.70827704997546
At time: 29.06403613090515 and batch: 350, loss is 5.073571586608887 and perplexity is 159.74384868129513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.283717188341864 and perplexity of 197.101177515024
Finished 4 epochs...
Completing Train Step...
At time: 30.836655616760254 and batch: 50, loss is 4.977432556152344 and perplexity is 145.10136340405094
At time: 31.785919666290283 and batch: 100, loss is 4.943466348648071 and perplexity is 140.25558265650446
At time: 32.72222423553467 and batch: 150, loss is 4.98437162399292 and perplexity is 146.11173306612548
At time: 33.65856742858887 and batch: 200, loss is 4.955569763183593 and perplexity is 141.96346889251777
At time: 34.594728231430054 and batch: 250, loss is 5.018513259887695 and perplexity is 151.1863617918288
At time: 35.53148436546326 and batch: 300, loss is 4.980590963363648 and perplexity is 145.56037709109034
At time: 36.46743392944336 and batch: 350, loss is 5.0374338722229 and perplexity is 154.07413338827766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.270352593783675 and perplexity of 194.4845243917459
Finished 5 epochs...
Completing Train Step...
At time: 38.27720832824707 and batch: 50, loss is 4.962074575424194 and perplexity is 142.88992454600023
At time: 39.216508626937866 and batch: 100, loss is 4.934028987884521 and perplexity is 138.93816636809274
At time: 40.16373062133789 and batch: 150, loss is 4.925563497543335 and perplexity is 137.76695111293702
At time: 41.09983158111572 and batch: 200, loss is 4.960666255950928 and perplexity is 142.68883151757922
At time: 42.035489082336426 and batch: 250, loss is 5.004162044525146 and perplexity is 149.0321485160863
At time: 42.981446504592896 and batch: 300, loss is 4.967630386352539 and perplexity is 143.68600333424416
At time: 43.91644501686096 and batch: 350, loss is 4.9973116207122805 and perplexity is 148.01470407871906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.297030218716325 and perplexity of 199.74273603755196
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 45.70138955116272 and batch: 50, loss is 4.908659725189209 and perplexity is 135.45774209059425
At time: 46.63835263252258 and batch: 100, loss is 4.806079397201538 and perplexity is 122.25137762425364
At time: 47.57508969306946 and batch: 150, loss is 4.7390429878234865 and perplexity is 114.32473914258631
At time: 48.5109007358551 and batch: 200, loss is 4.7159897899627685 and perplexity is 111.71933514349907
At time: 49.447936058044434 and batch: 250, loss is 4.733620090484619 and perplexity is 113.70644580479197
At time: 50.38403868675232 and batch: 300, loss is 4.703830003738403 and perplexity is 110.36907796775137
At time: 51.3200306892395 and batch: 350, loss is 4.747418928146362 and perplexity is 115.28633785995835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.135492785223599 and perplexity of 169.9480471296167
Finished 7 epochs...
Completing Train Step...
At time: 53.09112906455994 and batch: 50, loss is 4.778955888748169 and perplexity is 118.9800567520945
At time: 54.038755655288696 and batch: 100, loss is 4.719731760025025 and perplexity is 112.13816869356319
At time: 54.97502374649048 and batch: 150, loss is 4.6700638389587406 and perplexity is 106.70455412265098
At time: 55.90950059890747 and batch: 200, loss is 4.666946020126343 and perplexity is 106.37238674223934
At time: 56.852985858917236 and batch: 250, loss is 4.693835020065308 and perplexity is 109.27143543288781
At time: 57.79103350639343 and batch: 300, loss is 4.665564527511597 and perplexity is 106.22553553582635
At time: 58.72545528411865 and batch: 350, loss is 4.721335411071777 and perplexity is 112.31814345495677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.118617090685614 and perplexity of 167.10411991536907
Finished 8 epochs...
Completing Train Step...
At time: 60.496476888656616 and batch: 50, loss is 4.731209030151367 and perplexity is 113.43262293805236
At time: 61.44446659088135 and batch: 100, loss is 4.680351152420044 and perplexity is 107.80792293842727
At time: 62.381096839904785 and batch: 150, loss is 4.635843257904053 and perplexity is 103.11483374246197
At time: 63.33494853973389 and batch: 200, loss is 4.6389635562896725 and perplexity is 103.43708529075451
At time: 64.28170943260193 and batch: 250, loss is 4.670770387649537 and perplexity is 106.77997272598854
At time: 65.23013949394226 and batch: 300, loss is 4.641667041778565 and perplexity is 103.71710429295489
At time: 66.16747856140137 and batch: 350, loss is 4.6997810649871825 and perplexity is 109.92310380134477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.113459093817349 and perplexity of 166.24441646804436
Finished 9 epochs...
Completing Train Step...
At time: 67.95000100135803 and batch: 50, loss is 4.697156286239624 and perplexity is 109.63495829914712
At time: 68.88578176498413 and batch: 100, loss is 4.650275316238403 and perplexity is 104.61378348602081
At time: 69.8216507434845 and batch: 150, loss is 4.607706832885742 and perplexity is 100.25398669084936
At time: 70.75769114494324 and batch: 200, loss is 4.613959293365479 and perplexity is 100.88278449881352
At time: 71.69446587562561 and batch: 250, loss is 4.647008018493652 and perplexity is 104.27253688786128
At time: 72.6305513381958 and batch: 300, loss is 4.616057815551758 and perplexity is 101.094711549333
At time: 73.56466746330261 and batch: 350, loss is 4.677785949707031 and perplexity is 107.53172816127011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.107287702889278 and perplexity of 165.22161647989526
Finished 10 epochs...
Completing Train Step...
At time: 75.3417809009552 and batch: 50, loss is 4.667156505584717 and perplexity is 106.3947789393544
At time: 76.2759428024292 and batch: 100, loss is 4.623998708724976 and perplexity is 101.9006897121068
At time: 77.20958209037781 and batch: 150, loss is 4.583293628692627 and perplexity is 97.8360999125079
At time: 78.14352059364319 and batch: 200, loss is 4.59032205581665 and perplexity is 98.52615597455468
At time: 79.07936406135559 and batch: 250, loss is 4.6249361419677735 and perplexity is 101.99625959430047
At time: 80.01666688919067 and batch: 300, loss is 4.593517570495606 and perplexity is 98.84150132924651
At time: 80.95986461639404 and batch: 350, loss is 4.656840410232544 and perplexity is 105.30284220040913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.104177014581088 and perplexity of 164.70846207474503
Finished 11 epochs...
Completing Train Step...
At time: 82.73841071128845 and batch: 50, loss is 4.641694498062134 and perplexity is 103.71995201827515
At time: 83.68500113487244 and batch: 100, loss is 4.602799024581909 and perplexity is 99.76316475764024
At time: 84.61948847770691 and batch: 150, loss is 4.563349103927612 and perplexity is 95.90413549263003
At time: 85.5545129776001 and batch: 200, loss is 4.572335968017578 and perplexity is 96.76989733915427
At time: 86.49285197257996 and batch: 250, loss is 4.606397972106934 and perplexity is 100.12285401567868
At time: 87.44951057434082 and batch: 300, loss is 4.57559455871582 and perplexity is 97.08574515638962
At time: 88.3828980922699 and batch: 350, loss is 4.642018241882324 and perplexity is 103.75353614780535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.103850792194235 and perplexity of 164.65473925038253
Finished 12 epochs...
Completing Train Step...
At time: 90.16947555541992 and batch: 50, loss is 4.623240613937378 and perplexity is 101.8234686045413
At time: 91.10345721244812 and batch: 100, loss is 4.583924198150635 and perplexity is 97.89781182378177
At time: 92.03728699684143 and batch: 150, loss is 4.544953212738037 and perplexity is 94.15602180389169
At time: 92.97103548049927 and batch: 200, loss is 4.555691776275634 and perplexity is 95.17257059462983
At time: 93.9053316116333 and batch: 250, loss is 4.586879749298095 and perplexity is 98.18758181793554
At time: 94.83886432647705 and batch: 300, loss is 4.554474544525147 and perplexity is 95.05679399769645
At time: 95.77244281768799 and batch: 350, loss is 4.6223952293396 and perplexity is 101.73742498758934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105376671100485 and perplexity of 164.90617422469577
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 97.565190076828 and batch: 50, loss is 4.591894750595093 and perplexity is 98.68122945525971
At time: 98.50103521347046 and batch: 100, loss is 4.528761405944824 and perplexity is 92.64374200158282
At time: 99.43806004524231 and batch: 150, loss is 4.465744047164917 and perplexity is 86.98572695133122
At time: 100.37422347068787 and batch: 200, loss is 4.458787279129028 and perplexity is 86.38268746150875
At time: 101.31107187271118 and batch: 250, loss is 4.469324970245362 and perplexity is 87.29777452436969
At time: 102.24763178825378 and batch: 300, loss is 4.4206218338012695 and perplexity is 83.14797350639276
At time: 103.18443155288696 and batch: 350, loss is 4.5134320640563965 and perplexity is 91.23440411586164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.045096035661368 and perplexity of 155.2592089032275
Finished 14 epochs...
Completing Train Step...
At time: 104.96370935440063 and batch: 50, loss is 4.5471847629547115 and perplexity is 94.3663703090841
At time: 105.91283416748047 and batch: 100, loss is 4.497983846664429 and perplexity is 89.83582579225968
At time: 106.84749960899353 and batch: 150, loss is 4.438976469039917 and perplexity is 84.68821628701609
At time: 107.78078389167786 and batch: 200, loss is 4.439963150024414 and perplexity is 84.77181777685513
At time: 108.71459555625916 and batch: 250, loss is 4.460532083511352 and perplexity is 86.53353991889233
At time: 109.64747929573059 and batch: 300, loss is 4.425986404418945 and perplexity is 83.59522526620236
At time: 110.58429074287415 and batch: 350, loss is 4.518584508895874 and perplexity is 91.70569746437663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.040612582502694 and perplexity of 154.56466964320785
Finished 15 epochs...
Completing Train Step...
At time: 112.36459612846375 and batch: 50, loss is 4.535400676727295 and perplexity is 93.26087528157981
At time: 113.31292152404785 and batch: 100, loss is 4.4862966632843015 and perplexity is 88.79200953933557
At time: 114.24840307235718 and batch: 150, loss is 4.428741703033447 and perplexity is 83.8258726798904
At time: 115.1826286315918 and batch: 200, loss is 4.433168048858643 and perplexity is 84.19773737519365
At time: 116.11822938919067 and batch: 250, loss is 4.459444131851196 and perplexity is 86.43944680413962
At time: 117.05304598808289 and batch: 300, loss is 4.428868827819824 and perplexity is 83.83652970341923
At time: 117.98953938484192 and batch: 350, loss is 4.519550294876098 and perplexity is 91.7943083239698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.039347155340787 and perplexity of 154.36920301244544
Finished 16 epochs...
Completing Train Step...
At time: 119.77052927017212 and batch: 50, loss is 4.528274145126343 and perplexity is 92.59861133212436
At time: 120.70811438560486 and batch: 100, loss is 4.479352254867553 and perplexity is 88.17753760239329
At time: 121.64688754081726 and batch: 150, loss is 4.422962913513183 and perplexity is 83.34285757081236
At time: 122.583913564682 and batch: 200, loss is 4.429354076385498 and perplexity is 83.87722113113892
At time: 123.52203822135925 and batch: 250, loss is 4.457965288162232 and perplexity is 86.31171084779183
At time: 124.46246242523193 and batch: 300, loss is 4.4295010757446285 and perplexity is 83.8895519351788
At time: 125.40081691741943 and batch: 350, loss is 4.518804836273193 and perplexity is 91.72590496623542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0387857371363145 and perplexity of 154.28256165495776
Finished 17 epochs...
Completing Train Step...
At time: 127.20100688934326 and batch: 50, loss is 4.522773876190185 and perplexity is 92.09069219311822
At time: 128.13969349861145 and batch: 100, loss is 4.473721504211426 and perplexity is 87.68242710469984
At time: 129.07704091072083 and batch: 150, loss is 4.418264017105103 and perplexity is 82.95215676691282
At time: 130.01422023773193 and batch: 200, loss is 4.425966405868531 and perplexity is 83.59355349959195
At time: 130.95176601409912 and batch: 250, loss is 4.456161012649536 and perplexity is 86.15612114700292
At time: 131.88924074172974 and batch: 300, loss is 4.428961553573608 and perplexity is 83.8443038692578
At time: 132.82776021957397 and batch: 350, loss is 4.517578134536743 and perplexity is 91.61345362556466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.038153286637931 and perplexity of 154.18501642146438
Finished 18 epochs...
Completing Train Step...
At time: 134.5998513698578 and batch: 50, loss is 4.518224849700927 and perplexity is 91.67272059762676
At time: 135.54803156852722 and batch: 100, loss is 4.469141302108764 and perplexity is 87.28174217715444
At time: 136.48317646980286 and batch: 150, loss is 4.414531259536743 and perplexity is 82.64309366360864
At time: 137.41708278656006 and batch: 200, loss is 4.42295126914978 and perplexity is 83.34188710194198
At time: 138.35195016860962 and batch: 250, loss is 4.453944349288941 and perplexity is 85.9653535418586
At time: 139.28757286071777 and batch: 300, loss is 4.427536706924439 and perplexity is 83.72492366327799
At time: 140.2244634628296 and batch: 350, loss is 4.515711936950684 and perplexity is 91.44264425115225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.037576083479257 and perplexity of 154.09604602243357
Finished 19 epochs...
Completing Train Step...
At time: 142.00616931915283 and batch: 50, loss is 4.51398307800293 and perplexity is 91.28468939760663
At time: 142.94070839881897 and batch: 100, loss is 4.4650797939300535 and perplexity is 86.92796558702722
At time: 143.87496733665466 and batch: 150, loss is 4.411289396286011 and perplexity is 82.37560986255016
At time: 144.80780243873596 and batch: 200, loss is 4.420590162277222 and perplexity is 83.14534012505214
At time: 145.74269318580627 and batch: 250, loss is 4.452228155136108 and perplexity is 85.81794683021221
At time: 146.67852425575256 and batch: 300, loss is 4.4268487930297855 and perplexity is 83.66734793080492
At time: 147.61682081222534 and batch: 350, loss is 4.514569768905639 and perplexity is 91.33826100788443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.037816014783136 and perplexity of 154.13302292345898
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 149.41771006584167 and batch: 50, loss is 4.509591264724731 and perplexity is 90.88466314988328
At time: 150.35620760917664 and batch: 100, loss is 4.455847721099854 and perplexity is 86.12913339003299
At time: 151.2921998500824 and batch: 150, loss is 4.394889078140259 and perplexity is 81.03564164011556
At time: 152.22782921791077 and batch: 200, loss is 4.398970775604248 and perplexity is 81.36708056965522
At time: 153.16339826583862 and batch: 250, loss is 4.4282527256011965 and perplexity is 83.78489373961622
At time: 154.09871578216553 and batch: 300, loss is 4.400424671173096 and perplexity is 81.48546584660664
At time: 155.03536128997803 and batch: 350, loss is 4.493675775527954 and perplexity is 89.44963912125306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.030475484913793 and perplexity of 153.00574732671188
Finished 21 epochs...
Completing Train Step...
At time: 156.81413531303406 and batch: 50, loss is 4.502544984817505 and perplexity is 90.24651529858424
At time: 157.76303696632385 and batch: 100, loss is 4.45119348526001 and perplexity is 85.72919950580726
At time: 158.69755387306213 and batch: 150, loss is 4.391435422897339 and perplexity is 80.75625520142508
At time: 159.6440098285675 and batch: 200, loss is 4.39703598022461 and perplexity is 81.20980411605511
At time: 160.5786111354828 and batch: 250, loss is 4.428417415618896 and perplexity is 83.79869341155208
At time: 161.5180368423462 and batch: 300, loss is 4.401536951065063 and perplexity is 81.57615091599564
At time: 162.45512676239014 and batch: 350, loss is 4.4942332363128665 and perplexity is 89.49951768866867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.029214793238147 and perplexity of 152.81297579299675
Finished 22 epochs...
Completing Train Step...
At time: 164.2259602546692 and batch: 50, loss is 4.500121784210205 and perplexity is 90.02809463332905
At time: 165.17445921897888 and batch: 100, loss is 4.449304561614991 and perplexity is 85.56741643982545
At time: 166.12049961090088 and batch: 150, loss is 4.3896675872802735 and perplexity is 80.61361753430143
At time: 167.06204199790955 and batch: 200, loss is 4.396170415878296 and perplexity is 81.13954221752496
At time: 168.00743889808655 and batch: 250, loss is 4.429032573699951 and perplexity is 83.8502587137661
At time: 168.95317459106445 and batch: 300, loss is 4.4022509765625 and perplexity is 81.63441916776553
At time: 169.89440417289734 and batch: 350, loss is 4.4944453620910645 and perplexity is 89.51850485726948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.028600758519666 and perplexity of 152.71917212272785
Finished 23 epochs...
Completing Train Step...
At time: 171.67842769622803 and batch: 50, loss is 4.498365249633789 and perplexity is 89.87009597793161
At time: 172.61414980888367 and batch: 100, loss is 4.4479392147064205 and perplexity is 85.45066695222775
At time: 173.54997086524963 and batch: 150, loss is 4.388329000473022 and perplexity is 80.50578139950076
At time: 174.48544311523438 and batch: 200, loss is 4.39553129196167 and perplexity is 81.0877005638947
At time: 175.4231767654419 and batch: 250, loss is 4.429459915161133 and perplexity is 83.88609906333357
At time: 176.3591890335083 and batch: 300, loss is 4.402672967910767 and perplexity is 81.66887545599155
At time: 177.29493165016174 and batch: 350, loss is 4.494492559432984 and perplexity is 89.52272999245811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.028204030004041 and perplexity of 152.6585960891786
Finished 24 epochs...
Completing Train Step...
At time: 179.0792772769928 and batch: 50, loss is 4.496853771209717 and perplexity is 89.73436187231887
At time: 180.01470923423767 and batch: 100, loss is 4.446834211349487 and perplexity is 85.35629582820029
At time: 180.95043516159058 and batch: 150, loss is 4.387246437072754 and perplexity is 80.41867594414354
At time: 181.8862361907959 and batch: 200, loss is 4.395015659332276 and perplexity is 81.04589987746648
At time: 182.82067108154297 and batch: 250, loss is 4.429863414764404 and perplexity is 83.91995390077301
At time: 183.76834535598755 and batch: 300, loss is 4.402986860275268 and perplexity is 81.69451471618811
At time: 184.70674514770508 and batch: 350, loss is 4.49444164276123 and perplexity is 89.5181719090428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0279541015625 and perplexity of 152.62044713162356
Finished 25 epochs...
Completing Train Step...
At time: 186.47719740867615 and batch: 50, loss is 4.495594663619995 and perplexity is 89.62144775665395
At time: 187.42663502693176 and batch: 100, loss is 4.445911836624146 and perplexity is 85.2776016366255
At time: 188.36478185653687 and batch: 150, loss is 4.3862808799743656 and perplexity is 80.34106459586654
At time: 189.30282044410706 and batch: 200, loss is 4.394577207565308 and perplexity is 81.01037294845126
At time: 190.24198150634766 and batch: 250, loss is 4.430173892974853 and perplexity is 83.94601326310382
At time: 191.1813826560974 and batch: 300, loss is 4.403185586929322 and perplexity is 81.7107512070105
At time: 192.12130999565125 and batch: 350, loss is 4.494338054656982 and perplexity is 89.50889937158958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027793621194774 and perplexity of 152.59595651132977
Finished 26 epochs...
Completing Train Step...
At time: 193.9260287284851 and batch: 50, loss is 4.49445686340332 and perplexity is 89.51953444346726
At time: 194.86418437957764 and batch: 100, loss is 4.445057010650634 and perplexity is 85.20473527625361
At time: 195.7995994091034 and batch: 150, loss is 4.385393133163452 and perplexity is 80.26977372079168
At time: 196.73568439483643 and batch: 200, loss is 4.394138088226319 and perplexity is 80.97480753633248
At time: 197.67129850387573 and batch: 250, loss is 4.430391445159912 and perplexity is 83.96427788839871
At time: 198.60671973228455 and batch: 300, loss is 4.403379421234131 and perplexity is 81.72659108877282
At time: 199.5416705608368 and batch: 350, loss is 4.494186983108521 and perplexity is 89.4953781449225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027675760203395 and perplexity of 152.57797246044478
Finished 27 epochs...
Completing Train Step...
At time: 201.33259272575378 and batch: 50, loss is 4.493430089950562 and perplexity is 89.42766533444976
At time: 202.26851105690002 and batch: 100, loss is 4.444289331436157 and perplexity is 85.1393504724955
At time: 203.20443177223206 and batch: 150, loss is 4.384618473052979 and perplexity is 80.20761600767229
At time: 204.14093565940857 and batch: 200, loss is 4.393732595443725 and perplexity is 80.94197949252185
At time: 205.07897877693176 and batch: 250, loss is 4.430543422698975 and perplexity is 83.9770395424392
At time: 206.0150945186615 and batch: 300, loss is 4.403490085601806 and perplexity is 81.73563581075287
At time: 206.96318650245667 and batch: 350, loss is 4.494025640487671 and perplexity is 89.4809398908428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0275726318359375 and perplexity of 152.56223815457557
Finished 28 epochs...
Completing Train Step...
At time: 208.73134779930115 and batch: 50, loss is 4.492481479644775 and perplexity is 89.34287355302833
At time: 209.67793345451355 and batch: 100, loss is 4.443589487075806 and perplexity is 85.07978702322646
At time: 210.6114945411682 and batch: 150, loss is 4.3838855743408205 and perplexity is 80.14885348531378
At time: 211.5460033416748 and batch: 200, loss is 4.393369102478028 and perplexity is 80.9125629990148
At time: 212.48177456855774 and batch: 250, loss is 4.430694360733032 and perplexity is 83.98971582833644
At time: 213.41647458076477 and batch: 300, loss is 4.403551063537598 and perplexity is 81.74062003306746
At time: 214.35117292404175 and batch: 350, loss is 4.4938990497589115 and perplexity is 89.46961315039727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027531064789871 and perplexity of 152.55589672479235
Finished 29 epochs...
Completing Train Step...
At time: 216.1220405101776 and batch: 50, loss is 4.4916043376922605 and perplexity is 89.26454152965097
At time: 217.0696702003479 and batch: 100, loss is 4.442944536209106 and perplexity is 85.02493243200675
At time: 218.00435757637024 and batch: 150, loss is 4.383186950683593 and perplexity is 80.09287915494164
At time: 218.9399697780609 and batch: 200, loss is 4.39302827835083 and perplexity is 80.88499074426304
At time: 219.87488865852356 and batch: 250, loss is 4.430817184448242 and perplexity is 84.00003239082021
At time: 220.8111288547516 and batch: 300, loss is 4.403611488342285 and perplexity is 81.74555934329494
At time: 221.74667143821716 and batch: 350, loss is 4.493741788864136 and perplexity is 89.45554418525602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027487393083243 and perplexity of 152.5492344939028
Finished 30 epochs...
Completing Train Step...
At time: 223.52897930145264 and batch: 50, loss is 4.490772008895874 and perplexity is 89.19027499261577
At time: 224.46452021598816 and batch: 100, loss is 4.442298707962036 and perplexity is 84.9700386568202
At time: 225.3993628025055 and batch: 150, loss is 4.382506713867188 and perplexity is 80.03841555598123
At time: 226.33560490608215 and batch: 200, loss is 4.392662019729614 and perplexity is 80.85537134358754
At time: 227.27158331871033 and batch: 250, loss is 4.43087308883667 and perplexity is 84.00472849252407
At time: 228.20809769630432 and batch: 300, loss is 4.403627452850341 and perplexity is 81.74686438135274
At time: 229.14455699920654 and batch: 350, loss is 4.493547859191895 and perplexity is 89.43819778293746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027410572972791 and perplexity of 152.53751609496965
Finished 31 epochs...
Completing Train Step...
At time: 230.92853426933289 and batch: 50, loss is 4.489911155700684 and perplexity is 89.11352829796667
At time: 231.86334443092346 and batch: 100, loss is 4.441657886505127 and perplexity is 84.91560547569253
At time: 232.79787468910217 and batch: 150, loss is 4.381849908828736 and perplexity is 79.98586318159572
At time: 233.73317313194275 and batch: 200, loss is 4.3922819232940675 and perplexity is 80.82464434512627
At time: 234.6682095527649 and batch: 250, loss is 4.4309235954284665 and perplexity is 84.00897139220135
At time: 235.60359525680542 and batch: 300, loss is 4.403635692596436 and perplexity is 81.74753795753429
At time: 236.54873991012573 and batch: 350, loss is 4.493322534561157 and perplexity is 89.41804742431948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027375846073546 and perplexity of 152.53221903199295
Finished 32 epochs...
Completing Train Step...
At time: 238.32142186164856 and batch: 50, loss is 4.489136323928833 and perplexity is 89.04450704832145
At time: 239.26978874206543 and batch: 100, loss is 4.4410128402709965 and perplexity is 84.86084864640338
At time: 240.20480370521545 and batch: 150, loss is 4.3812405872344975 and perplexity is 79.93714091319795
At time: 241.1412012577057 and batch: 200, loss is 4.391928453445434 and perplexity is 80.79608031888218
At time: 242.07776713371277 and batch: 250, loss is 4.430952739715576 and perplexity is 84.01141980946191
At time: 243.01316165924072 and batch: 300, loss is 4.403647079467773 and perplexity is 81.74846881153091
At time: 243.96520280838013 and batch: 350, loss is 4.4931309700012205 and perplexity is 89.40091973599567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027321651064116 and perplexity of 152.52395277094126
Finished 33 epochs...
Completing Train Step...
At time: 245.7711751461029 and batch: 50, loss is 4.488410530090332 and perplexity is 88.97990254136592
At time: 246.70541071891785 and batch: 100, loss is 4.440426759719848 and perplexity is 84.8111279250549
At time: 247.6398274898529 and batch: 150, loss is 4.38065203666687 and perplexity is 79.89010770560714
At time: 248.5747425556183 and batch: 200, loss is 4.391544408798218 and perplexity is 80.7650569742757
At time: 249.50985670089722 and batch: 250, loss is 4.43095666885376 and perplexity is 84.01174990258784
At time: 250.44503808021545 and batch: 300, loss is 4.403575601577759 and perplexity is 81.74262581229351
At time: 251.38088464736938 and batch: 350, loss is 4.492925662994384 and perplexity is 89.38256698479493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.0272979736328125 and perplexity of 152.5203414382811
Finished 34 epochs...
Completing Train Step...
At time: 253.17355346679688 and batch: 50, loss is 4.48769983291626 and perplexity is 88.9166872422086
At time: 254.1096568107605 and batch: 100, loss is 4.439842691421509 and perplexity is 84.76160689712651
At time: 255.05671215057373 and batch: 150, loss is 4.380068159103393 and perplexity is 79.84347527931261
At time: 255.99248719215393 and batch: 200, loss is 4.391195030212402 and perplexity is 80.736844321622
At time: 256.9288272857666 and batch: 250, loss is 4.430963516235352 and perplexity is 84.01232516506711
At time: 257.86425495147705 and batch: 300, loss is 4.403459367752075 and perplexity is 81.73312510633625
At time: 258.8013217449188 and batch: 350, loss is 4.492709045410156 and perplexity is 89.36320724596821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027261668238147 and perplexity of 152.51480422760656
Finished 35 epochs...
Completing Train Step...
At time: 260.57737612724304 and batch: 50, loss is 4.487011194229126 and perplexity is 88.85547684978293
At time: 261.5247540473938 and batch: 100, loss is 4.439301500320434 and perplexity is 84.7157470803427
At time: 262.4585177898407 and batch: 150, loss is 4.379479856491089 and perplexity is 79.79651696843315
At time: 263.39311170578003 and batch: 200, loss is 4.390831422805786 and perplexity is 80.70749314351612
At time: 264.3278956413269 and batch: 250, loss is 4.4309165668487545 and perplexity is 84.00838093052444
At time: 265.26274061203003 and batch: 300, loss is 4.403357353210449 and perplexity is 81.72478756432538
At time: 266.19727420806885 and batch: 350, loss is 4.492504205703735 and perplexity is 89.34490398751215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027226415173761 and perplexity of 152.50942770816326
Finished 36 epochs...
Completing Train Step...
At time: 267.965008020401 and batch: 50, loss is 4.486340436935425 and perplexity is 88.79589637485357
At time: 268.9124710559845 and batch: 100, loss is 4.438791522979736 and perplexity is 84.67255498336287
At time: 269.84688544273376 and batch: 150, loss is 4.3789955997467045 and perplexity is 79.75788432172764
At time: 270.7823734283447 and batch: 200, loss is 4.390489377975464 and perplexity is 80.67989228335297
At time: 271.71720004081726 and batch: 250, loss is 4.430904521942138 and perplexity is 84.0073690635151
At time: 272.6523370742798 and batch: 300, loss is 4.403313150405884 and perplexity is 81.72117517935192
At time: 273.5881645679474 and batch: 350, loss is 4.492347755432129 and perplexity is 89.3309270463933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027177481815733 and perplexity of 152.5019650923215
Finished 37 epochs...
Completing Train Step...
At time: 275.3732316493988 and batch: 50, loss is 4.485700826644898 and perplexity is 88.73911976516315
At time: 276.31400632858276 and batch: 100, loss is 4.438283891677856 and perplexity is 84.6295834518132
At time: 277.2551281452179 and batch: 150, loss is 4.378436717987061 and perplexity is 79.71332154881284
At time: 278.1900882720947 and batch: 200, loss is 4.390154886245727 and perplexity is 80.65291003954843
At time: 279.1377489566803 and batch: 250, loss is 4.430871458053589 and perplexity is 84.00459149914582
At time: 280.07390451431274 and batch: 300, loss is 4.403245792388916 and perplexity is 81.71567078843208
At time: 281.01081895828247 and batch: 350, loss is 4.492143659591675 and perplexity is 89.31269683617757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027131705448546 and perplexity of 152.49498426615028
Finished 38 epochs...
Completing Train Step...
At time: 282.7932438850403 and batch: 50, loss is 4.4850719547271725 and perplexity is 88.68333176833004
At time: 283.7278423309326 and batch: 100, loss is 4.437756471633911 and perplexity is 84.58495988190698
At time: 284.66243267059326 and batch: 150, loss is 4.377905416488647 and perplexity is 79.67098099042751
At time: 285.6029314994812 and batch: 200, loss is 4.389822263717651 and perplexity is 80.62608752584781
At time: 286.5405867099762 and batch: 250, loss is 4.430800323486328 and perplexity is 83.99861608141352
At time: 287.4763045310974 and batch: 300, loss is 4.40311222076416 and perplexity is 81.70475662244448
At time: 288.41176080703735 and batch: 350, loss is 4.491926527023315 and perplexity is 89.29330624616685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027061199319774 and perplexity of 152.48423281417865
Finished 39 epochs...
Completing Train Step...
At time: 290.18326711654663 and batch: 50, loss is 4.484456014633179 and perplexity is 88.62872496761038
At time: 291.13322496414185 and batch: 100, loss is 4.437295007705688 and perplexity is 84.54593597880566
At time: 292.07800483703613 and batch: 150, loss is 4.377372102737427 and perplexity is 79.6285026888301
At time: 293.0142116546631 and batch: 200, loss is 4.38949348449707 and perplexity is 80.59958370082477
At time: 293.9500091075897 and batch: 250, loss is 4.430728063583374 and perplexity is 83.99254656886107
At time: 294.88544392585754 and batch: 300, loss is 4.403027048110962 and perplexity is 81.69779790789435
At time: 295.82175374031067 and batch: 350, loss is 4.491708755493164 and perplexity is 89.27386282342219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.027004373484645 and perplexity of 152.4755680164995
Finished 40 epochs...
Completing Train Step...
At time: 297.60116481781006 and batch: 50, loss is 4.483854074478149 and perplexity is 88.57539183245146
At time: 298.53717017173767 and batch: 100, loss is 4.436789894104004 and perplexity is 84.50324146027442
At time: 299.4704201221466 and batch: 150, loss is 4.376865310668945 and perplexity is 79.58815781933578
At time: 300.4042139053345 and batch: 200, loss is 4.389142322540283 and perplexity is 80.57128516227208
At time: 301.33902978897095 and batch: 250, loss is 4.430626010894775 and perplexity is 83.98397534102727
At time: 302.27419686317444 and batch: 300, loss is 4.4029304313659665 and perplexity is 81.68990491389103
At time: 303.2229971885681 and batch: 350, loss is 4.491464614868164 and perplexity is 89.25207010710825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026903349777748 and perplexity of 152.46016514744784
Finished 41 epochs...
Completing Train Step...
At time: 305.02466559410095 and batch: 50, loss is 4.483208713531494 and perplexity is 88.51824717517651
At time: 305.96553564071655 and batch: 100, loss is 4.436300630569458 and perplexity is 84.4619072181702
At time: 306.90135645866394 and batch: 150, loss is 4.376282892227173 and perplexity is 79.54181770445376
At time: 307.8369462490082 and batch: 200, loss is 4.388791170120239 and perplexity is 80.5429973274628
At time: 308.77251172065735 and batch: 250, loss is 4.4305063819885255 and perplexity is 83.97392903084128
At time: 309.71004128456116 and batch: 300, loss is 4.402785081863403 and perplexity is 81.6780321897155
At time: 310.64761996269226 and batch: 350, loss is 4.491245822906494 and perplexity is 89.23254460769547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026893878805226 and perplexity of 152.4587212082508
Finished 42 epochs...
Completing Train Step...
At time: 312.4567904472351 and batch: 50, loss is 4.482648010253906 and perplexity is 88.4686286158035
At time: 313.40465927124023 and batch: 100, loss is 4.435840520858765 and perplexity is 84.42305441343724
At time: 314.3385808467865 and batch: 150, loss is 4.375780868530273 and perplexity is 79.50189584876891
At time: 315.2734339237213 and batch: 200, loss is 4.388473682403564 and perplexity is 80.51742997402224
At time: 316.2074592113495 and batch: 250, loss is 4.430405187606811 and perplexity is 83.96543177095762
At time: 317.1424572467804 and batch: 300, loss is 4.402688837051391 and perplexity is 81.6701714811442
At time: 318.0785961151123 and batch: 350, loss is 4.491039705276489 and perplexity is 89.21415410245042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026892300309806 and perplexity of 152.4584805530475
Finished 43 epochs...
Completing Train Step...
At time: 319.85178804397583 and batch: 50, loss is 4.4820871925354 and perplexity is 88.41902775116569
At time: 320.8004958629608 and batch: 100, loss is 4.435399646759033 and perplexity is 84.38584267877388
At time: 321.73487281799316 and batch: 150, loss is 4.375273857116699 and perplexity is 79.46159769684756
At time: 322.6707408428192 and batch: 200, loss is 4.388170528411865 and perplexity is 80.49302449322063
At time: 323.6061713695526 and batch: 250, loss is 4.4303224086761475 and perplexity is 83.95848148997527
At time: 324.54233145713806 and batch: 300, loss is 4.402552871704102 and perplexity is 81.65906792278227
At time: 325.4778127670288 and batch: 350, loss is 4.490832462310791 and perplexity is 89.19566701229779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026872832199623 and perplexity of 152.45551250344107
Finished 44 epochs...
Completing Train Step...
At time: 327.26238799095154 and batch: 50, loss is 4.481524391174316 and perplexity is 88.36927940253392
At time: 328.1986463069916 and batch: 100, loss is 4.434937219619751 and perplexity is 84.34682939604363
At time: 329.13366413116455 and batch: 150, loss is 4.374768533706665 and perplexity is 79.4214540349529
At time: 330.0691306591034 and batch: 200, loss is 4.3878428173065185 and perplexity is 80.46665035697598
At time: 331.0048267841339 and batch: 250, loss is 4.430208597183228 and perplexity is 83.94892659359245
At time: 331.94001364707947 and batch: 300, loss is 4.402447013854981 and perplexity is 81.6504241270056
At time: 332.8759081363678 and batch: 350, loss is 4.490627851486206 and perplexity is 89.17741848030826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026873358364763 and perplexity of 152.45559272023823
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 334.6574146747589 and batch: 50, loss is 4.480830698013306 and perplexity is 88.30799949495274
At time: 335.593309879303 and batch: 100, loss is 4.433408794403076 and perplexity is 84.21801004549769
At time: 336.5293574333191 and batch: 150, loss is 4.372129764556885 and perplexity is 79.21215541902747
At time: 337.46487379074097 and batch: 200, loss is 4.38503870010376 and perplexity is 80.24132850055197
At time: 338.40050530433655 and batch: 250, loss is 4.426231899261475 and perplexity is 83.61574998211816
At time: 339.3349184989929 and batch: 300, loss is 4.397766160964966 and perplexity is 81.26912360528743
At time: 340.2690906524658 and batch: 350, loss is 4.486915988922119 and perplexity is 88.84701773951268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.026014130691002 and perplexity of 152.3246549167012
Finished 46 epochs...
Completing Train Step...
At time: 342.0372242927551 and batch: 50, loss is 4.4797774600982665 and perplexity is 88.21503912497
At time: 342.9860010147095 and batch: 100, loss is 4.433038492202758 and perplexity is 84.18682970450223
At time: 343.92209696769714 and batch: 150, loss is 4.371843605041504 and perplexity is 79.18949134994469
At time: 344.8577916622162 and batch: 200, loss is 4.38511607170105 and perplexity is 80.24753714048983
At time: 345.7934019565582 and batch: 250, loss is 4.426181879043579 and perplexity is 83.61156760868703
At time: 346.7282135486603 and batch: 300, loss is 4.3977218341827395 and perplexity is 81.26552128638386
At time: 347.66396737098694 and batch: 350, loss is 4.4868621730804445 and perplexity is 88.84223649112737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.02569264379041 and perplexity of 152.2756924063328
Finished 47 epochs...
Completing Train Step...
At time: 349.44571018218994 and batch: 50, loss is 4.479277057647705 and perplexity is 88.17090714601159
At time: 350.3811204433441 and batch: 100, loss is 4.4328148078918455 and perplexity is 84.16800053748496
At time: 351.3296551704407 and batch: 150, loss is 4.371819515228271 and perplexity is 79.18758371286545
At time: 352.2647223472595 and batch: 200, loss is 4.385344581604004 and perplexity is 80.26587659270756
At time: 353.35759019851685 and batch: 250, loss is 4.426260042190552 and perplexity is 83.61810320735279
At time: 354.29142022132874 and batch: 300, loss is 4.397681350708008 and perplexity is 81.26223144229893
At time: 355.2262451648712 and batch: 350, loss is 4.486737699508667 and perplexity is 88.83117866884419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.025530584927263 and perplexity of 152.2510167802425
Finished 48 epochs...
Completing Train Step...
At time: 357.0141806602478 and batch: 50, loss is 4.4788514995574955 and perplexity is 88.13339328588398
At time: 357.9509060382843 and batch: 100, loss is 4.432627401351929 and perplexity is 84.15222838168125
At time: 358.8878707885742 and batch: 150, loss is 4.371853446960449 and perplexity is 79.19027073033511
At time: 359.8247010707855 and batch: 200, loss is 4.385589733123779 and perplexity is 80.28555630649764
At time: 360.76150822639465 and batch: 250, loss is 4.426345338821411 and perplexity is 83.6252358540263
At time: 361.70947766304016 and batch: 300, loss is 4.397636585235595 and perplexity is 81.25859378154053
At time: 362.64574670791626 and batch: 350, loss is 4.4865855884552 and perplexity is 88.81766749230187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.025431665880927 and perplexity of 152.2359569997207
Finished 49 epochs...
Completing Train Step...
At time: 364.43298530578613 and batch: 50, loss is 4.47847014427185 and perplexity is 88.09978955839935
At time: 365.36819338798523 and batch: 100, loss is 4.432457914352417 and perplexity is 84.13796688159421
At time: 366.30253863334656 and batch: 150, loss is 4.371910486221314 and perplexity is 79.19478781366958
At time: 367.23846673965454 and batch: 200, loss is 4.385827913284301 and perplexity is 80.3046810106585
At time: 368.1747553348541 and batch: 250, loss is 4.426425046920777 and perplexity is 83.63190172829376
At time: 369.1088044643402 and batch: 300, loss is 4.397586174011231 and perplexity is 81.254497539587
At time: 370.04295444488525 and batch: 350, loss is 4.486422672271728 and perplexity is 88.80319883551034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.025364842908136 and perplexity of 152.22578448039113
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f43f60ac390>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'lr': 2.350638840697703, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 5.744900982954643, 'dropout': 0.7136190258012192, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1871850490570068 and batch: 50, loss is 7.152272500991821 and perplexity is 1277.004654621486
At time: 2.1295828819274902 and batch: 100, loss is 6.087532091140747 and perplexity is 440.33336648450387
At time: 3.07187819480896 and batch: 150, loss is 5.939957494735718 and perplexity is 379.91878064676075
At time: 4.0197107791900635 and batch: 200, loss is 5.8243958854675295 and perplexity is 338.4566047513394
At time: 4.97102427482605 and batch: 250, loss is 5.7872881412506105 and perplexity is 326.12741261440027
At time: 5.913337230682373 and batch: 300, loss is 5.750175399780273 and perplexity is 314.2457740918057
At time: 6.868942022323608 and batch: 350, loss is 5.75081088066101 and perplexity is 314.4455347384067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.6287952291554415 and perplexity of 278.32659579228186
Finished 1 epochs...
Completing Train Step...
At time: 8.654696941375732 and batch: 50, loss is 5.3623952960968015 and perplexity is 213.23509641967277
At time: 9.588196992874146 and batch: 100, loss is 5.166380739212036 and perplexity is 175.27930658051488
At time: 10.521631002426147 and batch: 150, loss is 5.072119388580322 and perplexity is 159.5120373378913
At time: 11.455037355422974 and batch: 200, loss is 4.996665964126587 and perplexity is 147.91916825523677
At time: 12.388593196868896 and batch: 250, loss is 4.947252826690674 and perplexity is 140.78766406214288
At time: 13.322295665740967 and batch: 300, loss is 4.902501602172851 and perplexity is 134.6261398338145
At time: 14.255877256393433 and batch: 350, loss is 4.9237650203704835 and perplexity is 137.5194030676663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.997461253199084 and perplexity of 148.03685354407236
Finished 2 epochs...
Completing Train Step...
At time: 16.0368754863739 and batch: 50, loss is 4.798630809783935 and perplexity is 121.34416048864101
At time: 16.96969985961914 and batch: 100, loss is 4.707063522338867 and perplexity is 110.72653604649493
At time: 17.90279507637024 and batch: 150, loss is 4.653254499435425 and perplexity is 104.92591182485168
At time: 18.83583092689514 and batch: 200, loss is 4.665229187011719 and perplexity is 106.1899197836758
At time: 19.76766347885132 and batch: 250, loss is 4.654863634109497 and perplexity is 105.09488766375746
At time: 20.7008216381073 and batch: 300, loss is 4.618051624298095 and perplexity is 101.29647614259476
At time: 21.63338613510132 and batch: 350, loss is 4.6734361743927 and perplexity is 107.06500511069649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.834747314453125 and perplexity of 125.80678963271475
Finished 3 epochs...
Completing Train Step...
At time: 23.40244960784912 and batch: 50, loss is 4.578419284820557 and perplexity is 97.36037348743189
At time: 24.350547075271606 and batch: 100, loss is 4.504560012817382 and perplexity is 90.42854789259633
At time: 25.285593271255493 and batch: 150, loss is 4.447801332473755 and perplexity is 85.43888563572142
At time: 26.232994318008423 and batch: 200, loss is 4.491378049850464 and perplexity is 89.24434433447533
At time: 27.169079780578613 and batch: 250, loss is 4.491356582641601 and perplexity is 89.24242852805925
At time: 28.10386848449707 and batch: 300, loss is 4.45373462677002 and perplexity is 85.94732656177177
At time: 29.0390567779541 and batch: 350, loss is 4.522348318099976 and perplexity is 92.05151059163751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7515106201171875 and perplexity of 115.75902041917341
Finished 4 epochs...
Completing Train Step...
At time: 30.823484182357788 and batch: 50, loss is 4.436627063751221 and perplexity is 84.48948288784311
At time: 31.756487131118774 and batch: 100, loss is 4.371850452423096 and perplexity is 79.19003359246646
At time: 32.68887519836426 and batch: 150, loss is 4.310520915985108 and perplexity is 74.47927628256991
At time: 33.621620655059814 and batch: 200, loss is 4.371665525436401 and perplexity is 79.17539057216452
At time: 34.554396867752075 and batch: 250, loss is 4.376888694763184 and perplexity is 79.59001893807871
At time: 35.48714756965637 and batch: 300, loss is 4.336897344589233 and perplexity is 76.46991110620777
At time: 36.420424938201904 and batch: 350, loss is 4.412776317596435 and perplexity is 82.49818702102436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.699516033304149 and perplexity of 109.89397455639693
Finished 5 epochs...
Completing Train Step...
At time: 38.20413279533386 and batch: 50, loss is 4.331369829177857 and perplexity is 76.04838855351075
At time: 39.141807556152344 and batch: 100, loss is 4.271377611160278 and perplexity is 71.62023252115264
At time: 40.078125953674316 and batch: 150, loss is 4.207209548950195 and perplexity is 67.1688466789309
At time: 41.01382851600647 and batch: 200, loss is 4.27835376739502 and perplexity is 72.12161327464386
At time: 41.94882035255432 and batch: 250, loss is 4.287373204231262 and perplexity is 72.77505200500264
At time: 42.884204387664795 and batch: 300, loss is 4.245501861572266 and perplexity is 69.7907766653519
At time: 43.819648027420044 and batch: 350, loss is 4.3260273742675786 and perplexity is 75.6431868164352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.666801715719289 and perplexity of 106.35703784552696
Finished 6 epochs...
Completing Train Step...
At time: 45.588698625564575 and batch: 50, loss is 4.246523637771606 and perplexity is 69.86212366400723
At time: 46.53500533103943 and batch: 100, loss is 4.190070924758911 and perplexity is 66.02747377700327
At time: 47.4666645526886 and batch: 150, loss is 4.1238935899734495 and perplexity is 61.79939593945811
At time: 48.40326905250549 and batch: 200, loss is 4.200984663963318 and perplexity is 66.75202700686411
At time: 49.35058951377869 and batch: 250, loss is 4.213613653182984 and perplexity is 67.60038330214951
At time: 50.30814456939697 and batch: 300, loss is 4.170118541717529 and perplexity is 64.72312404295975
At time: 51.240081548690796 and batch: 350, loss is 4.253663845062256 and perplexity is 70.36273882987675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.644880623653017 and perplexity of 104.05094382241212
Finished 7 epochs...
Completing Train Step...
At time: 53.05661344528198 and batch: 50, loss is 4.175297937393188 and perplexity is 65.05922034827132
At time: 54.00397729873657 and batch: 100, loss is 4.121102261543274 and perplexity is 61.62713406019352
At time: 54.93691682815552 and batch: 150, loss is 4.053723959922791 and perplexity is 57.61160136009907
At time: 55.86967992782593 and batch: 200, loss is 4.13482506275177 and perplexity is 62.478660269490646
At time: 56.802995920181274 and batch: 250, loss is 4.150039935112 and perplexity is 63.43653359261319
At time: 57.73626351356506 and batch: 300, loss is 4.105710411071778 and perplexity is 60.68584113108813
At time: 58.67051863670349 and batch: 350, loss is 4.191677541732788 and perplexity is 66.13363989842549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.630815177128233 and perplexity of 102.5976653011383
Finished 8 epochs...
Completing Train Step...
At time: 60.49391579627991 and batch: 50, loss is 4.113467445373535 and perplexity is 61.158413793932894
At time: 61.431007623672485 and batch: 100, loss is 4.06090238571167 and perplexity is 58.026649880262475
At time: 62.371575355529785 and batch: 150, loss is 3.992790641784668 and perplexity is 54.20594787331148
At time: 63.30504536628723 and batch: 200, loss is 4.076958737373352 and perplexity is 58.9658661922717
At time: 64.23872089385986 and batch: 250, loss is 4.0942340755462645 and perplexity is 59.99337116565551
At time: 65.17180061340332 and batch: 300, loss is 4.04920416355133 and perplexity is 57.35179622874954
At time: 66.10611319541931 and batch: 350, loss is 4.136953630447388 and perplexity is 62.611791967055005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.623001624797952 and perplexity of 101.7991368090394
Finished 9 epochs...
Completing Train Step...
At time: 67.89606428146362 and batch: 50, loss is 4.058560013771057 and perplexity is 57.8908889471487
At time: 68.82888007164001 and batch: 100, loss is 4.007510104179382 and perplexity is 55.00973140288952
At time: 69.76184868812561 and batch: 150, loss is 3.9388173484802245 and perplexity is 51.357826958071925
At time: 70.69349884986877 and batch: 200, loss is 4.025419363975525 and perplexity is 56.003789829694604
At time: 71.62540531158447 and batch: 250, loss is 4.044121699333191 and perplexity is 57.06104726305005
At time: 72.55722308158875 and batch: 300, loss is 3.9987448835372925 and perplexity is 54.529665982924044
At time: 73.49030590057373 and batch: 350, loss is 4.08786678314209 and perplexity is 59.61258939006872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.618479235418912 and perplexity of 101.33980090470772
Finished 10 epochs...
Completing Train Step...
At time: 75.27607369422913 and batch: 50, loss is 4.009073629379272 and perplexity is 55.095807777932585
At time: 76.23209643363953 and batch: 100, loss is 3.9597833013534545 and perplexity is 52.44595974912904
At time: 77.16619729995728 and batch: 150, loss is 3.8900686073303223 and perplexity is 48.91424228419354
At time: 78.10004472732544 and batch: 200, loss is 3.978915877342224 and perplexity is 53.45904663661824
At time: 79.03439688682556 and batch: 250, loss is 3.998656458854675 and perplexity is 54.524844427691654
At time: 79.96873474121094 and batch: 300, loss is 3.9526805591583254 and perplexity is 52.07476941299843
At time: 80.90318417549133 and batch: 350, loss is 4.043067960739136 and perplexity is 57.000951503503046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.61626934183055 and perplexity of 101.11609799929087
Finished 11 epochs...
Completing Train Step...
At time: 82.68290972709656 and batch: 50, loss is 3.9641642904281618 and perplexity is 52.676228960955584
At time: 83.61675906181335 and batch: 100, loss is 3.9164918518066405 and perplexity is 50.22394232816763
At time: 84.55038809776306 and batch: 150, loss is 3.8453645277023316 and perplexity is 46.7757322937176
At time: 85.48352885246277 and batch: 200, loss is 3.9362731122970582 and perplexity is 51.22732659870009
At time: 86.41644215583801 and batch: 250, loss is 3.956782941818237 and perplexity is 52.28883884118819
At time: 87.34897661209106 and batch: 300, loss is 3.9102792358398437 and perplexity is 49.91288749472743
At time: 88.29531669616699 and batch: 350, loss is 4.00154818534851 and perplexity is 54.68274355541051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.616605035189925 and perplexity of 101.15004769994086
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 90.08344745635986 and batch: 50, loss is 3.9306368923187254 and perplexity is 50.939410259185884
At time: 91.02010226249695 and batch: 100, loss is 3.8765341091156005 and perplexity is 48.25667253553463
At time: 91.96284127235413 and batch: 150, loss is 3.78769389629364 and perplexity is 44.1544580171656
At time: 92.90739059448242 and batch: 200, loss is 3.864937672615051 and perplexity is 47.70029930511907
At time: 93.84649014472961 and batch: 250, loss is 3.865125241279602 and perplexity is 47.70924722570693
At time: 94.78506994247437 and batch: 300, loss is 3.7901189708709717 and perplexity is 44.26166581168675
At time: 95.72411370277405 and batch: 350, loss is 3.8550220012664793 and perplexity is 47.22965604209222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.58160295157597 and perplexity of 97.67083040525691
Finished 13 epochs...
Completing Train Step...
At time: 97.51027369499207 and batch: 50, loss is 3.8914201927185057 and perplexity is 48.980398757329006
At time: 98.45829653739929 and batch: 100, loss is 3.8383294582366942 and perplexity is 46.447816575093086
At time: 99.39518404006958 and batch: 150, loss is 3.7553497171401977 and perplexity is 42.74916731323973
At time: 100.3312361240387 and batch: 200, loss is 3.8381572771072388 and perplexity is 46.43981982603884
At time: 101.26677346229553 and batch: 250, loss is 3.8456966257095337 and perplexity is 46.79126900090972
At time: 102.2020251750946 and batch: 300, loss is 3.7791961002349854 and perplexity is 43.78083218429007
At time: 103.13715243339539 and batch: 350, loss is 3.8548604249954224 and perplexity is 47.222025466862185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.580719520305765 and perplexity of 97.58458304190818
Finished 14 epochs...
Completing Train Step...
At time: 104.92152047157288 and batch: 50, loss is 3.8734594631195067 and perplexity is 48.108528212918834
At time: 105.87027716636658 and batch: 100, loss is 3.820484733581543 and perplexity is 45.62631957068125
At time: 106.81140208244324 and batch: 150, loss is 3.738982014656067 and perplexity is 42.05515683355072
At time: 107.75377607345581 and batch: 200, loss is 3.8237600183486937 and perplexity is 45.77600375550619
At time: 108.69758749008179 and batch: 250, loss is 3.834620246887207 and perplexity is 46.27585093231293
At time: 109.63404202461243 and batch: 300, loss is 3.7719835948944094 and perplexity is 43.46619871009354
At time: 110.57063674926758 and batch: 350, loss is 3.8521068477630616 and perplexity is 47.0921748315931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.581152028050916 and perplexity of 97.6267982584271
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 112.37582397460938 and batch: 50, loss is 3.866618571281433 and perplexity is 47.7805460990773
At time: 113.31336283683777 and batch: 100, loss is 3.8138807916641237 and perplexity is 45.32599874843063
At time: 114.25011992454529 and batch: 150, loss is 3.728849005699158 and perplexity is 41.63116334546783
At time: 115.18942022323608 and batch: 200, loss is 3.809510741233826 and perplexity is 45.12835402134931
At time: 116.12714862823486 and batch: 250, loss is 3.814841179847717 and perplexity is 45.36955021184615
At time: 117.06405305862427 and batch: 300, loss is 3.745359263420105 and perplexity is 42.32421002821562
At time: 118.00105619430542 and batch: 350, loss is 3.8173653078079224 and perplexity is 45.48421341349574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5775172792632 and perplexity of 97.27259348445678
Finished 16 epochs...
Completing Train Step...
At time: 119.8009386062622 and batch: 50, loss is 3.858200216293335 and perplexity is 47.380000832120515
At time: 120.73729729652405 and batch: 100, loss is 3.8054810190200805 and perplexity is 44.94686521101952
At time: 121.67350506782532 and batch: 150, loss is 3.721887621879578 and perplexity is 41.34235924297456
At time: 122.62304210662842 and batch: 200, loss is 3.803792200088501 and perplexity is 44.87102215475667
At time: 123.56022953987122 and batch: 250, loss is 3.8112286949157714 and perplexity is 45.205949076571564
At time: 124.49708485603333 and batch: 300, loss is 3.744537787437439 and perplexity is 42.28945598295328
At time: 125.43362712860107 and batch: 350, loss is 3.819423322677612 and perplexity is 45.57791698964051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.577177376582704 and perplexity of 97.23953588769376
Finished 17 epochs...
Completing Train Step...
At time: 127.24437284469604 and batch: 50, loss is 3.853894519805908 and perplexity is 47.17643548874711
At time: 128.2081823348999 and batch: 100, loss is 3.8008515214920044 and perplexity is 44.739264723381126
At time: 129.16091966629028 and batch: 150, loss is 3.7178074502944947 and perplexity is 41.17401898563956
At time: 130.1084508895874 and batch: 200, loss is 3.8004250001907347 and perplexity is 44.72018654288787
At time: 131.04756474494934 and batch: 250, loss is 3.8091148471832277 and perplexity is 45.11049151054315
At time: 131.9838044643402 and batch: 300, loss is 3.7439945793151854 and perplexity is 42.26649024513003
At time: 132.91993045806885 and batch: 350, loss is 3.8203031492233275 and perplexity is 45.6180352968943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.577126338564116 and perplexity of 97.23457310110008
Finished 18 epochs...
Completing Train Step...
At time: 134.71004700660706 and batch: 50, loss is 3.8503680753707887 and perplexity is 47.01036340444119
At time: 135.64439749717712 and batch: 100, loss is 3.7972352790832518 and perplexity is 44.57776887700914
At time: 136.57831168174744 and batch: 150, loss is 3.714623155593872 and perplexity is 41.0431173005603
At time: 137.5121762752533 and batch: 200, loss is 3.7977589082717897 and perplexity is 44.60111721035398
At time: 138.44619393348694 and batch: 250, loss is 3.8073577451705933 and perplexity is 45.031297371584905
At time: 139.3802809715271 and batch: 300, loss is 3.7433330821990967 and perplexity is 42.23854032913928
At time: 140.31437921524048 and batch: 350, loss is 3.820585136413574 and perplexity is 45.63090081236227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.577193687702048 and perplexity of 97.24112198630398
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 142.11233067512512 and batch: 50, loss is 3.8487391757965086 and perplexity is 46.93385057627927
At time: 143.04829502105713 and batch: 100, loss is 3.7960095405578613 and perplexity is 44.52316166224108
At time: 143.98519015312195 and batch: 150, loss is 3.7125137710571288 and perplexity is 40.956632830156764
At time: 144.92298340797424 and batch: 200, loss is 3.7947751760482786 and perplexity is 44.46823775666444
At time: 145.88382482528687 and batch: 250, loss is 3.8031977224349975 and perplexity is 44.844355262017075
At time: 146.82128381729126 and batch: 300, loss is 3.736871919631958 and perplexity is 41.96651001587422
At time: 147.75874018669128 and batch: 350, loss is 3.8124445390701296 and perplexity is 45.26094589250206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576861677498653 and perplexity of 97.20884230050402
Finished 20 epochs...
Completing Train Step...
At time: 149.54120087623596 and batch: 50, loss is 3.847594919204712 and perplexity is 46.880176922451405
At time: 150.50293684005737 and batch: 100, loss is 3.7947204875946046 and perplexity is 44.46580592400111
At time: 151.43787908554077 and batch: 150, loss is 3.7112608909606934 and perplexity is 40.90535121163453
At time: 152.37429118156433 and batch: 200, loss is 3.7938557147979735 and perplexity is 44.42736972634214
At time: 153.3093843460083 and batch: 250, loss is 3.802557144165039 and perplexity is 44.81563814126968
At time: 154.24466156959534 and batch: 300, loss is 3.736919903755188 and perplexity is 41.96852379037657
At time: 155.18116092681885 and batch: 350, loss is 3.812906413078308 and perplexity is 45.28185557544319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576793802195582 and perplexity of 97.20224444479
Finished 21 epochs...
Completing Train Step...
At time: 156.96466517448425 and batch: 50, loss is 3.846745500564575 and perplexity is 46.84037293383875
At time: 157.91409873962402 and batch: 100, loss is 3.7937916040420534 and perplexity is 44.42452154538594
At time: 158.85043478012085 and batch: 150, loss is 3.7104813289642333 and perplexity is 40.87347538058562
At time: 159.78742265701294 and batch: 200, loss is 3.793237829208374 and perplexity is 44.399927173856376
At time: 160.72397828102112 and batch: 250, loss is 3.8021910858154295 and perplexity is 44.799236004987605
At time: 161.66047167778015 and batch: 300, loss is 3.736936106681824 and perplexity is 41.969203808797694
At time: 162.5988039970398 and batch: 350, loss is 3.813226203918457 and perplexity is 45.29633861372923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5767627584523165 and perplexity of 97.19922697010568
Finished 22 epochs...
Completing Train Step...
At time: 164.42557525634766 and batch: 50, loss is 3.845995283126831 and perplexity is 46.8052456474723
At time: 165.3633143901825 and batch: 100, loss is 3.7929939889907835 and perplexity is 44.38910200581261
At time: 166.30032563209534 and batch: 150, loss is 3.709824595451355 and perplexity is 40.84664121192878
At time: 167.2374930381775 and batch: 200, loss is 3.7927115964889526 and perplexity is 44.37656862599269
At time: 168.1838858127594 and batch: 250, loss is 3.801886868476868 and perplexity is 44.78560937347445
At time: 169.12988090515137 and batch: 300, loss is 3.7369200944900514 and perplexity is 41.968531795237986
At time: 170.08587431907654 and batch: 350, loss is 3.8134588098526 and perplexity is 45.30687603637174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5767538136449355 and perplexity of 97.19835754563128
Finished 23 epochs...
Completing Train Step...
At time: 171.88397121429443 and batch: 50, loss is 3.8453057765960694 and perplexity is 46.77298424842527
At time: 172.82009959220886 and batch: 100, loss is 3.792275424003601 and perplexity is 44.35721700839209
At time: 173.75624585151672 and batch: 150, loss is 3.709226870536804 and perplexity is 40.82223345209045
At time: 174.69261646270752 and batch: 200, loss is 3.7922305727005003 and perplexity is 44.35522757402204
At time: 175.6292748451233 and batch: 250, loss is 3.801604838371277 and perplexity is 44.77298026431211
At time: 176.5665500164032 and batch: 300, loss is 3.7368783521652222 and perplexity is 41.966779967714125
At time: 177.502338886261 and batch: 350, loss is 3.8136319637298586 and perplexity is 45.31472177686451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576758022966056 and perplexity of 97.19876668559164
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 179.301824092865 and batch: 50, loss is 3.8449477195739745 and perplexity is 46.75623985087424
At time: 180.2536904811859 and batch: 100, loss is 3.791988716125488 and perplexity is 44.34450126776323
At time: 181.19166564941406 and batch: 150, loss is 3.708546814918518 and perplexity is 40.79448150038526
At time: 182.1303837299347 and batch: 200, loss is 3.791574840545654 and perplexity is 44.32615195901591
At time: 183.06819915771484 and batch: 250, loss is 3.800799288749695 and perplexity is 44.73692792992481
At time: 184.00683283805847 and batch: 300, loss is 3.7354957246780396 and perplexity is 41.908795638782905
At time: 184.95490860939026 and batch: 350, loss is 3.8119927263259887 and perplexity is 45.24050103930636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576745395002694 and perplexity of 97.19753927087707
Finished 25 epochs...
Completing Train Step...
At time: 186.756267786026 and batch: 50, loss is 3.844811043739319 and perplexity is 46.749849839457276
At time: 187.6919710636139 and batch: 100, loss is 3.7918484830856323 and perplexity is 44.33828313955326
At time: 188.62805581092834 and batch: 150, loss is 3.708409323692322 and perplexity is 40.78887300267019
At time: 189.56357431411743 and batch: 200, loss is 3.7914636373519897 and perplexity is 44.32122302341698
At time: 190.50005769729614 and batch: 250, loss is 3.8007302474975586 and perplexity is 44.733839343024975
At time: 191.43301796913147 and batch: 300, loss is 3.7355037784576415 and perplexity is 41.90913316434553
At time: 192.36635065078735 and batch: 350, loss is 3.8120542907714845 and perplexity is 45.2432863314034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5767406595164335 and perplexity of 97.19707899435508
Finished 26 epochs...
Completing Train Step...
At time: 194.16662788391113 and batch: 50, loss is 3.844678773880005 and perplexity is 46.74366665232973
At time: 195.10526657104492 and batch: 100, loss is 3.791713161468506 and perplexity is 44.33228361731988
At time: 196.04413390159607 and batch: 150, loss is 3.708282804489136 and perplexity is 40.783712753401225
At time: 196.98342728614807 and batch: 200, loss is 3.7913641023635862 and perplexity is 44.31681173053995
At time: 197.92286229133606 and batch: 250, loss is 3.8006696605682375 and perplexity is 44.731129139164736
At time: 198.86113095283508 and batch: 300, loss is 3.735509295463562 and perplexity is 41.90936437791913
At time: 199.80018877983093 and batch: 350, loss is 3.8121075105667113 and perplexity is 45.24569423391081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576739607186153 and perplexity of 97.19697671097947
Finished 27 epochs...
Completing Train Step...
At time: 201.59136533737183 and batch: 50, loss is 3.84454993724823 and perplexity is 46.737644743690964
At time: 202.54028344154358 and batch: 100, loss is 3.791580944061279 and perplexity is 44.32642250520263
At time: 203.4756236076355 and batch: 150, loss is 3.7081636333465577 and perplexity is 40.778852801342595
At time: 204.41144180297852 and batch: 200, loss is 3.7912708377838134 and perplexity is 44.3126787344511
At time: 205.34684991836548 and batch: 250, loss is 3.8006140327453615 and perplexity is 44.728640913043876
At time: 206.28267526626587 and batch: 300, loss is 3.7355125284194948 and perplexity is 41.909499869266355
At time: 207.21861052513123 and batch: 350, loss is 3.8121544981002806 and perplexity is 45.24782026743564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576739081021013 and perplexity of 97.19692556933208
Finished 28 epochs...
Completing Train Step...
At time: 209.00657773017883 and batch: 50, loss is 3.844423604011536 and perplexity is 46.731740598707766
At time: 209.95747327804565 and batch: 100, loss is 3.7914517545700073 and perplexity is 44.32069636711567
At time: 210.89340925216675 and batch: 150, loss is 3.708049626350403 and perplexity is 40.77420399183152
At time: 211.82946372032166 and batch: 200, loss is 3.7911815214157105 and perplexity is 44.30872106367065
At time: 212.766432762146 and batch: 250, loss is 3.8005615186691286 and perplexity is 44.72629209145882
At time: 213.7032380104065 and batch: 300, loss is 3.735513896942139 and perplexity is 41.90955722340517
At time: 214.63964462280273 and batch: 350, loss is 3.8121968126296997 and perplexity is 45.24973494816662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Finished 29 epochs...
Completing Train Step...
At time: 216.4374816417694 and batch: 50, loss is 3.8442999935150146 and perplexity is 46.725964422054446
At time: 217.3750786781311 and batch: 100, loss is 3.7913249349594116 and perplexity is 44.31507599005562
At time: 218.32531833648682 and batch: 150, loss is 3.707939872741699 and perplexity is 40.769729121372464
At time: 219.26257967948914 and batch: 200, loss is 3.791095380783081 and perplexity is 44.30490444679248
At time: 220.19974732398987 and batch: 250, loss is 3.800511178970337 and perplexity is 44.72404064005603
At time: 221.13679218292236 and batch: 300, loss is 3.73551344871521 and perplexity is 41.90953843841726
At time: 222.07368159294128 and batch: 350, loss is 3.8122352027893065 and perplexity is 45.2514721260585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576739081021013 and perplexity of 97.19692556933208
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 223.8705313205719 and batch: 50, loss is 3.844232382774353 and perplexity is 46.722805351786484
At time: 224.81010460853577 and batch: 100, loss is 3.7912762641906737 and perplexity is 44.312919193727396
At time: 225.74827456474304 and batch: 150, loss is 3.707796673774719 and perplexity is 40.76389135626918
At time: 226.68221855163574 and batch: 200, loss is 3.7909631395339964 and perplexity is 44.29904589826729
At time: 227.61603784561157 and batch: 250, loss is 3.800342869758606 and perplexity is 44.71651380546606
At time: 228.55983209609985 and batch: 300, loss is 3.7352703762054444 and perplexity is 41.89935261972232
At time: 229.50582003593445 and batch: 350, loss is 3.81194176197052 and perplexity is 45.23819544508187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Finished 31 epochs...
Completing Train Step...
At time: 231.29341387748718 and batch: 50, loss is 3.844209837913513 and perplexity is 46.7217520045156
At time: 232.24621844291687 and batch: 100, loss is 3.791253514289856 and perplexity is 44.31191109067795
At time: 233.1909785270691 and batch: 150, loss is 3.7077778339385987 and perplexity is 40.7631233784707
At time: 234.1318335533142 and batch: 200, loss is 3.790947980880737 and perplexity is 44.298374389480394
At time: 235.0701174736023 and batch: 250, loss is 3.8003347015380857 and perplexity is 44.716148552612125
At time: 236.0094027519226 and batch: 300, loss is 3.7352708387374878 and perplexity is 41.899371999519985
At time: 236.94844245910645 and batch: 350, loss is 3.811949596405029 and perplexity is 45.238549862149725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738028690733 and perplexity of 97.19682328611802
Finished 32 epochs...
Completing Train Step...
At time: 238.74879789352417 and batch: 50, loss is 3.8441877031326293 and perplexity is 46.72071784021802
At time: 239.6846477985382 and batch: 100, loss is 3.7912310695648195 and perplexity is 44.31091653317902
At time: 240.6205883026123 and batch: 150, loss is 3.7077589988708497 and perplexity is 40.76235560951072
At time: 241.55603742599487 and batch: 200, loss is 3.7909326791763305 and perplexity is 44.297696554035824
At time: 242.5060260295868 and batch: 250, loss is 3.8003266763687136 and perplexity is 44.71578969938625
At time: 243.43956923484802 and batch: 300, loss is 3.735271105766296 and perplexity is 41.89938318786086
At time: 244.37542605400085 and batch: 350, loss is 3.8119572401046753 and perplexity is 45.23889565335886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738028690733 and perplexity of 97.19682328611802
Finished 33 epochs...
Completing Train Step...
At time: 246.16498684883118 and batch: 50, loss is 3.844165630340576 and perplexity is 46.719686594909824
At time: 247.10113763809204 and batch: 100, loss is 3.7912087202072144 and perplexity is 44.30992622372604
At time: 248.03903532028198 and batch: 150, loss is 3.707740292549133 and perplexity is 40.761593102904634
At time: 248.9758574962616 and batch: 200, loss is 3.7909176921844483 and perplexity is 44.297032669791996
At time: 249.92292618751526 and batch: 250, loss is 3.8003185510635378 and perplexity is 44.71542637142484
At time: 250.86153078079224 and batch: 300, loss is 3.7352715587615966 and perplexity is 41.899402168088834
At time: 251.79793882369995 and batch: 350, loss is 3.8119646596908567 and perplexity is 45.239231308489124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 253.57343816757202 and batch: 50, loss is 3.844153094291687 and perplexity is 46.71910091830563
At time: 254.5192358493805 and batch: 100, loss is 3.7912000179290772 and perplexity is 44.30954062810159
At time: 255.45271277427673 and batch: 150, loss is 3.707715005874634 and perplexity is 40.76056239079947
At time: 256.38745641708374 and batch: 200, loss is 3.7908941316604614 and perplexity is 44.29598902078573
At time: 257.32196378707886 and batch: 250, loss is 3.8002885723114015 and perplexity is 44.71408587883423
At time: 258.25585556030273 and batch: 300, loss is 3.7352295684814454 and perplexity is 41.89764283739128
At time: 259.18872356414795 and batch: 350, loss is 3.8119135761260985 and perplexity is 45.23692038631253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738028690733 and perplexity of 97.19682328611802
Finished 35 epochs...
Completing Train Step...
At time: 260.9562544822693 and batch: 50, loss is 3.8441492414474485 and perplexity is 46.718920917233596
At time: 261.9028391838074 and batch: 100, loss is 3.791196208000183 and perplexity is 44.30937181222405
At time: 262.83641386032104 and batch: 150, loss is 3.707711706161499 and perplexity is 40.76042789285825
At time: 263.77090096473694 and batch: 200, loss is 3.7908915758132933 and perplexity is 44.29587580715231
At time: 264.70489740371704 and batch: 250, loss is 3.8002871656417847 and perplexity is 44.71402298093242
At time: 265.6514382362366 and batch: 300, loss is 3.735229501724243 and perplexity is 41.89764004042195
At time: 266.5846104621887 and batch: 350, loss is 3.8119148778915406 and perplexity is 45.23697927421053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 268.36593985557556 and batch: 50, loss is 3.8441472959518435 and perplexity is 46.718830025866694
At time: 269.3004367351532 and batch: 100, loss is 3.7911949062347414 and perplexity is 44.309314131852624
At time: 270.2350971698761 and batch: 150, loss is 3.707707390785217 and perplexity is 40.76025199665403
At time: 271.16957330703735 and batch: 200, loss is 3.790887503623962 and perplexity is 44.295695426326716
At time: 272.1065549850464 and batch: 250, loss is 3.8002818393707276 and perplexity is 44.71378482256022
At time: 273.04584097862244 and batch: 300, loss is 3.7352220153808595 and perplexity is 41.89732638147573
At time: 273.98258805274963 and batch: 350, loss is 3.8119056701660154 and perplexity is 45.236562746439425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 275.7796516418457 and batch: 50, loss is 3.844147114753723 and perplexity is 46.718821560503265
At time: 276.7154026031494 and batch: 100, loss is 3.7911949253082273 and perplexity is 44.30931497698571
At time: 277.65130949020386 and batch: 150, loss is 3.707706985473633 and perplexity is 40.76023547605506
At time: 278.58732438087463 and batch: 200, loss is 3.790887060165405 and perplexity is 44.2956757830259
At time: 279.52328610420227 and batch: 250, loss is 3.8002812337875365 and perplexity is 44.71375774465192
At time: 280.45981311798096 and batch: 300, loss is 3.735220913887024 and perplexity is 41.89728023185441
At time: 281.4031193256378 and batch: 350, loss is 3.811904492378235 and perplexity is 45.23650946739996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 283.195054769516 and batch: 50, loss is 3.844147114753723 and perplexity is 46.718821560503265
At time: 284.1472089290619 and batch: 100, loss is 3.7911948490142824 and perplexity is 44.309311596453405
At time: 285.0853588581085 and batch: 150, loss is 3.707706861495972 and perplexity is 40.760230422696715
At time: 286.0245544910431 and batch: 200, loss is 3.790887017250061 and perplexity is 44.29567388206176
At time: 286.96362495422363 and batch: 250, loss is 3.800281186103821 and perplexity is 44.71375561253386
At time: 287.90269231796265 and batch: 300, loss is 3.735220956802368 and perplexity is 41.89728202989065
At time: 288.8497898578644 and batch: 350, loss is 3.811904468536377 and perplexity is 45.236508388877546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576738554855873 and perplexity of 97.19687442771159
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 290.64750599861145 and batch: 50, loss is 3.8441471242904663 and perplexity is 46.71882200604868
At time: 291.5829198360443 and batch: 100, loss is 3.7911948776245117 and perplexity is 44.309312864152986
At time: 292.5156042575836 and batch: 150, loss is 3.7077069234848024 and perplexity is 40.76023294937581
At time: 293.451815366745 and batch: 200, loss is 3.790887041091919 and perplexity is 44.295674938152935
At time: 294.39442133903503 and batch: 250, loss is 3.8002811908721923 and perplexity is 44.71375582574566
At time: 295.330096244812 and batch: 300, loss is 3.7352209377288816 and perplexity is 41.89728123076342
At time: 296.266019821167 and batch: 350, loss is 3.811904397010803 and perplexity is 45.23650515331044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.576739081021013 and perplexity of 97.19692556933208
Annealing...
Model not improving. Stopping early with 97.19682328611802loss at 39 epochs.
Finished Training.
Improved accuracyfrom -99.85380328396097 to -97.19682328611802
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f43f60acb00>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'lr': 26.567398669487652, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 3.7489425079864036, 'dropout': 0.6617483856703484, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1727707386016846 and batch: 50, loss is 6.466382827758789 and perplexity is 643.1531187924504
At time: 2.1336915493011475 and batch: 100, loss is 5.972359876632691 and perplexity is 392.4306668429129
At time: 3.086879014968872 and batch: 150, loss is 5.920521202087403 and perplexity is 372.6058662307388
At time: 4.0422186851501465 and batch: 200, loss is 5.839800043106079 and perplexity is 343.7106064879782
At time: 4.990560293197632 and batch: 250, loss is 5.827041463851929 and perplexity is 339.35320371740113
At time: 5.939493656158447 and batch: 300, loss is 5.806900796890258 and perplexity is 332.5867728047376
At time: 6.888231039047241 and batch: 350, loss is 5.826899013519287 and perplexity is 339.3048661835792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.654246363146552 and perplexity of 285.50123737409535
Finished 1 epochs...
Completing Train Step...
At time: 8.682732105255127 and batch: 50, loss is 5.48191541671753 and perplexity is 240.30655402559483
At time: 9.638387441635132 and batch: 100, loss is 5.430238971710205 and perplexity is 228.2037731543979
At time: 10.58991551399231 and batch: 150, loss is 5.376248378753662 and perplexity is 216.2096153950111
At time: 11.537745237350464 and batch: 200, loss is 5.310015611648559 and perplexity is 202.35338743345795
At time: 12.475737810134888 and batch: 250, loss is 5.312314176559449 and perplexity is 202.81904479622818
At time: 13.413236379623413 and batch: 300, loss is 5.259731884002686 and perplexity is 192.42989084800402
At time: 14.351027488708496 and batch: 350, loss is 5.34637752532959 and perplexity is 209.84675481025803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4895303660425645 and perplexity of 242.1434613586452
Finished 2 epochs...
Completing Train Step...
At time: 16.152287483215332 and batch: 50, loss is 5.243196144104004 and perplexity is 189.27408393291404
At time: 17.091511487960815 and batch: 100, loss is 5.175184822082519 and perplexity is 176.82929321466693
At time: 18.031818151474 and batch: 150, loss is 5.17026909828186 and perplexity is 175.9621822344958
At time: 18.980887413024902 and batch: 200, loss is 5.211572513580323 and perplexity is 183.38220258280973
At time: 19.94930863380432 and batch: 250, loss is 5.222929821014405 and perplexity is 185.47680262893653
At time: 20.890225887298584 and batch: 300, loss is 5.174900732040405 and perplexity is 176.77906490832925
At time: 21.832061767578125 and batch: 350, loss is 5.187562942504883 and perplexity is 179.0317102713218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.464837436018319 and perplexity of 236.23744828694683
Finished 3 epochs...
Completing Train Step...
At time: 23.651556491851807 and batch: 50, loss is 5.203003253936767 and perplexity is 181.81746676377983
At time: 24.599705934524536 and batch: 100, loss is 5.115547199249267 and perplexity is 166.59191501763868
At time: 25.541877508163452 and batch: 150, loss is 5.121634874343872 and perplexity is 167.60916567380718
At time: 26.48074245452881 and batch: 200, loss is 5.114382820129395 and perplexity is 166.39805175732175
At time: 27.419207096099854 and batch: 250, loss is 5.169364852905273 and perplexity is 175.8031411616703
At time: 28.355111122131348 and batch: 300, loss is 5.128444623947144 and perplexity is 168.75443720365874
At time: 29.29133439064026 and batch: 350, loss is 5.2013817882537845 and perplexity is 181.5228948645079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.453216552734375 and perplexity of 233.50805019730328
Finished 4 epochs...
Completing Train Step...
At time: 31.059160470962524 and batch: 50, loss is 5.1588725566864015 and perplexity is 173.9682057052792
At time: 32.00693416595459 and batch: 100, loss is 5.125274171829224 and perplexity is 168.22025658530924
At time: 32.94421625137329 and batch: 150, loss is 5.130346813201904 and perplexity is 169.0757455785867
At time: 33.881996870040894 and batch: 200, loss is 5.175053329467773 and perplexity is 176.8060429971879
At time: 34.819775342941284 and batch: 250, loss is 5.195909767150879 and perplexity is 180.5323104742484
At time: 35.758142709732056 and batch: 300, loss is 5.1471421432495115 and perplexity is 171.9394093016972
At time: 36.698261976242065 and batch: 350, loss is 5.188559427261352 and perplexity is 179.21020155868544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.451865886819774 and perplexity of 233.19287173143718
Finished 5 epochs...
Completing Train Step...
At time: 38.50078749656677 and batch: 50, loss is 5.155505542755127 and perplexity is 173.38343734712413
At time: 39.43850493431091 and batch: 100, loss is 5.121673107147217 and perplexity is 167.61557396457982
At time: 40.374627113342285 and batch: 150, loss is 5.042564697265625 and perplexity is 154.8666923121902
At time: 41.309162616729736 and batch: 200, loss is 5.12430287361145 and perplexity is 168.0569438754059
At time: 42.24529147148132 and batch: 250, loss is 5.191491479873657 and perplexity is 179.7364263807562
At time: 43.183218479156494 and batch: 300, loss is 5.10449875831604 and perplexity is 164.76146451665565
At time: 44.137824296951294 and batch: 350, loss is 5.151540098190307 and perplexity is 172.6972563434006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4312023294383085 and perplexity of 228.4237209500426
Finished 6 epochs...
Completing Train Step...
At time: 45.94146966934204 and batch: 50, loss is 5.131206102371216 and perplexity is 169.22109297433371
At time: 46.886037826538086 and batch: 100, loss is 5.11940203666687 and perplexity is 167.23533911600268
At time: 47.827664613723755 and batch: 150, loss is 5.086745748519897 and perplexity is 161.86226353928313
At time: 48.76947212219238 and batch: 200, loss is 5.114958801269531 and perplexity is 166.49392150382195
At time: 49.72730803489685 and batch: 250, loss is 5.151609573364258 and perplexity is 172.70925493212306
At time: 50.66992449760437 and batch: 300, loss is 5.081270923614502 and perplexity is 160.9785173728038
At time: 51.614264249801636 and batch: 350, loss is 5.167908849716187 and perplexity is 175.54735748360028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.453559612405711 and perplexity of 233.58817113460375
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 53.40862035751343 and batch: 50, loss is 5.070648813247681 and perplexity is 159.27763526571843
At time: 54.370218992233276 and batch: 100, loss is 4.980509090423584 and perplexity is 145.54846012290633
At time: 55.30972194671631 and batch: 150, loss is 4.941299629211426 and perplexity is 139.95201714886917
At time: 56.24855637550354 and batch: 200, loss is 4.920891304016113 and perplexity is 137.12477860099477
At time: 57.18657183647156 and batch: 250, loss is 4.951353940963745 and perplexity is 141.36623594447968
At time: 58.12252116203308 and batch: 300, loss is 4.893916091918945 and perplexity is 133.4752532734168
At time: 59.05821943283081 and batch: 350, loss is 4.922501888275146 and perplexity is 137.3458075560321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.253462692786908 and perplexity of 191.22728468943828
Finished 8 epochs...
Completing Train Step...
At time: 60.84093904495239 and batch: 50, loss is 4.945826482772827 and perplexity is 140.58699557895622
At time: 61.79242944717407 and batch: 100, loss is 4.90521993637085 and perplexity is 134.9925965238896
At time: 62.72751069068909 and batch: 150, loss is 4.871496829986572 and perplexity is 130.51613121762378
At time: 63.66248631477356 and batch: 200, loss is 4.8740174674987795 and perplexity is 130.84553004703596
At time: 64.60403370857239 and batch: 250, loss is 4.910066938400268 and perplexity is 135.6484941977689
At time: 65.53859281539917 and batch: 300, loss is 4.870804653167725 and perplexity is 130.4258222356069
At time: 66.47369885444641 and batch: 350, loss is 4.90866325378418 and perplexity is 135.45822006694505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.252545586947737 and perplexity of 191.05198942448288
Finished 9 epochs...
Completing Train Step...
At time: 68.26315140724182 and batch: 50, loss is 4.904379930496216 and perplexity is 134.879249562499
At time: 69.19855809211731 and batch: 100, loss is 4.874165887832642 and perplexity is 130.86495162553285
At time: 70.13468790054321 and batch: 150, loss is 4.838107357025146 and perplexity is 126.23021676996339
At time: 71.07020139694214 and batch: 200, loss is 4.8370457363128665 and perplexity is 126.09627926531876
At time: 72.00580883026123 and batch: 250, loss is 4.871157884597778 and perplexity is 130.47190087304344
At time: 72.94980525970459 and batch: 300, loss is 4.834585838317871 and perplexity is 125.78647647862508
At time: 73.88694477081299 and batch: 350, loss is 4.873445043563843 and perplexity is 130.7706523667903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.246534676387392 and perplexity of 189.9070375526819
Finished 10 epochs...
Completing Train Step...
At time: 75.67982220649719 and batch: 50, loss is 4.851292753219605 and perplexity is 127.90563343285226
At time: 76.61547112464905 and batch: 100, loss is 4.831781663894653 and perplexity is 125.4342433508733
At time: 77.55163431167603 and batch: 150, loss is 4.80807126045227 and perplexity is 122.49512832914004
At time: 78.48767447471619 and batch: 200, loss is 4.81347472190857 and perplexity is 123.15881752832212
At time: 79.43043661117554 and batch: 250, loss is 4.832394790649414 and perplexity is 125.51117402315161
At time: 80.36999273300171 and batch: 300, loss is 4.80775484085083 and perplexity is 122.45637460099869
At time: 81.30601072311401 and batch: 350, loss is 4.824413404464722 and perplexity is 124.51340794764332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.205867372710129 and perplexity of 182.3389600365076
Finished 11 epochs...
Completing Train Step...
At time: 83.08661937713623 and batch: 50, loss is 4.8071599864959715 and perplexity is 122.38355255469016
At time: 84.0348768234253 and batch: 100, loss is 4.808042192459107 and perplexity is 122.49156769333771
At time: 84.96998119354248 and batch: 150, loss is 4.796652421951294 and perplexity is 121.10433199308144
At time: 85.90598011016846 and batch: 200, loss is 4.814720830917358 and perplexity is 123.312382499832
At time: 86.84101176261902 and batch: 250, loss is 4.840885362625122 and perplexity is 126.58137254956598
At time: 87.77798652648926 and batch: 300, loss is 4.816626691818238 and perplexity is 123.54762284471512
At time: 88.71611404418945 and batch: 350, loss is 4.841080312728882 and perplexity is 126.60605200683177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.246263175175108 and perplexity of 189.85548456043176
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 90.50477147102356 and batch: 50, loss is 4.7711786365509035 and perplexity is 118.058307827258
At time: 91.44236063957214 and batch: 100, loss is 4.698072624206543 and perplexity is 109.73546701688241
At time: 92.39402365684509 and batch: 150, loss is 4.654805164337159 and perplexity is 105.08874296924301
At time: 93.33229446411133 and batch: 200, loss is 4.649815826416016 and perplexity is 104.56572555913723
At time: 94.27112984657288 and batch: 250, loss is 4.662704057693482 and perplexity is 105.9221147673778
At time: 95.20771408081055 and batch: 300, loss is 4.6326092338562015 and perplexity is 102.78189654408162
At time: 96.14558935165405 and batch: 350, loss is 4.684582529067993 and perplexity is 108.26506535480839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.144491787614493 and perplexity of 171.4843120705264
Finished 13 epochs...
Completing Train Step...
At time: 97.95028257369995 and batch: 50, loss is 4.69546332359314 and perplexity is 109.4495074349845
At time: 98.88794898986816 and batch: 100, loss is 4.660636940002441 and perplexity is 105.70338743552307
At time: 99.82645440101624 and batch: 150, loss is 4.630784902572632 and perplexity is 102.594559249433
At time: 100.76452255249023 and batch: 200, loss is 4.634261589050293 and perplexity is 102.9518691335677
At time: 101.70721650123596 and batch: 250, loss is 4.660743360519409 and perplexity is 105.7146370432431
At time: 102.64962768554688 and batch: 300, loss is 4.63275526046753 and perplexity is 102.79690653204202
At time: 103.58671283721924 and batch: 350, loss is 4.680308017730713 and perplexity is 107.80327277745629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.143690964271283 and perplexity of 171.3470384037407
Finished 14 epochs...
Completing Train Step...
At time: 105.38986992835999 and batch: 50, loss is 4.675306892395019 and perplexity is 107.26548100163937
At time: 106.3599066734314 and batch: 100, loss is 4.646554613113404 and perplexity is 104.22526987499285
At time: 107.29781770706177 and batch: 150, loss is 4.622990484237671 and perplexity is 101.7980027159364
At time: 108.23700761795044 and batch: 200, loss is 4.627691116333008 and perplexity is 102.27764410041013
At time: 109.18560934066772 and batch: 250, loss is 4.6572887134552 and perplexity is 105.35006038716428
At time: 110.13152742385864 and batch: 300, loss is 4.622871408462524 and perplexity is 101.78588176152496
At time: 111.06842684745789 and batch: 350, loss is 4.6691247081756595 and perplexity is 106.60439163138055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1416299754175645 and perplexity of 170.99425773086506
Finished 15 epochs...
Completing Train Step...
At time: 112.8564863204956 and batch: 50, loss is 4.662697114944458 and perplexity is 105.92137937927174
At time: 113.80750703811646 and batch: 100, loss is 4.640922775268555 and perplexity is 103.63993984473373
At time: 114.74548482894897 and batch: 150, loss is 4.619929800033569 and perplexity is 101.48690750197441
At time: 115.69746828079224 and batch: 200, loss is 4.624261922836304 and perplexity is 101.92751494182771
At time: 116.63565039634705 and batch: 250, loss is 4.6561237239837645 and perplexity is 105.22740013881995
At time: 117.57153749465942 and batch: 300, loss is 4.618595161437988 and perplexity is 101.35154950537171
At time: 118.50754237174988 and batch: 350, loss is 4.662903985977173 and perplexity is 105.94329371105258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1431384908741915 and perplexity of 171.252399868396
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 120.30656409263611 and batch: 50, loss is 4.644730014801025 and perplexity is 104.03527400925029
At time: 121.24569153785706 and batch: 100, loss is 4.605731401443482 and perplexity is 100.05613729662484
At time: 122.1933000087738 and batch: 150, loss is 4.565278968811035 and perplexity is 96.0893962224823
At time: 123.13320183753967 and batch: 200, loss is 4.563945589065551 and perplexity is 95.96135794859397
At time: 124.07183289527893 and batch: 250, loss is 4.589986114501953 and perplexity is 98.49306252722387
At time: 125.0097930431366 and batch: 300, loss is 4.545929908752441 and perplexity is 94.24802853911144
At time: 125.94724154472351 and batch: 350, loss is 4.607091064453125 and perplexity is 100.19227245339079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.118564474171605 and perplexity of 167.09532771041222
Finished 17 epochs...
Completing Train Step...
At time: 127.7525110244751 and batch: 50, loss is 4.62673433303833 and perplexity is 102.17983335841949
At time: 128.69287300109863 and batch: 100, loss is 4.590604724884034 and perplexity is 98.55401020775645
At time: 129.63660621643066 and batch: 150, loss is 4.552918443679809 and perplexity is 94.90899106815816
At time: 130.57555961608887 and batch: 200, loss is 4.556188192367554 and perplexity is 95.21982751876065
At time: 131.51473546028137 and batch: 250, loss is 4.587553052902222 and perplexity is 98.25371413171828
At time: 132.4538323879242 and batch: 300, loss is 4.550322437286377 and perplexity is 94.66292625166044
At time: 133.39168739318848 and batch: 350, loss is 4.610602827072143 and perplexity is 100.54474246377488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.11639719995959 and perplexity of 166.73357846222268
Finished 18 epochs...
Completing Train Step...
At time: 135.18202662467957 and batch: 50, loss is 4.620700540542603 and perplexity is 101.56515772415715
At time: 136.13349318504333 and batch: 100, loss is 4.584184322357178 and perplexity is 97.92328072680017
At time: 137.071683883667 and batch: 150, loss is 4.54830885887146 and perplexity is 94.47250680325065
At time: 138.01043701171875 and batch: 200, loss is 4.552991027832031 and perplexity is 94.9158802068312
At time: 138.94877696037292 and batch: 250, loss is 4.586774110794067 and perplexity is 98.17720997652073
At time: 139.90090799331665 and batch: 300, loss is 4.551603765487671 and perplexity is 94.78429827077338
At time: 140.83987259864807 and batch: 350, loss is 4.612116680145264 and perplexity is 100.69706770106312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.115919442012392 and perplexity of 166.6539391957086
Finished 19 epochs...
Completing Train Step...
At time: 142.63530015945435 and batch: 50, loss is 4.6170643615722655 and perplexity is 101.19651925741361
At time: 143.57290816307068 and batch: 100, loss is 4.57952772140503 and perplexity is 97.46835111940669
At time: 144.51206421852112 and batch: 150, loss is 4.544418487548828 and perplexity is 94.10568766598247
At time: 145.45052576065063 and batch: 200, loss is 4.549953737258911 and perplexity is 94.62803046158612
At time: 146.38921785354614 and batch: 250, loss is 4.5853409004211425 and perplexity is 98.03660216515289
At time: 147.32868647575378 and batch: 300, loss is 4.5515974998474125 and perplexity is 94.78370438831882
At time: 148.2667212486267 and batch: 350, loss is 4.612647972106934 and perplexity is 100.7505814581521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.115715289938039 and perplexity of 166.61991992099357
Finished 20 epochs...
Completing Train Step...
At time: 150.06122756004333 and batch: 50, loss is 4.614148845672608 and perplexity is 100.90190887584248
At time: 150.99984884262085 and batch: 100, loss is 4.575902767181397 and perplexity is 97.11567241661422
At time: 151.94347190856934 and batch: 150, loss is 4.541300716400147 and perplexity is 93.8127445701129
At time: 152.88778519630432 and batch: 200, loss is 4.547310676574707 and perplexity is 94.37825306846047
At time: 153.82651710510254 and batch: 250, loss is 4.583790636062622 and perplexity is 97.88473726077504
At time: 154.76477527618408 and batch: 300, loss is 4.550804815292358 and perplexity is 94.70860058052624
At time: 155.70249700546265 and batch: 350, loss is 4.6124873447418215 and perplexity is 100.73439945738984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1155984812769395 and perplexity of 166.6004584078939
Finished 21 epochs...
Completing Train Step...
At time: 157.4877142906189 and batch: 50, loss is 4.61148962020874 and perplexity is 100.63394439729814
At time: 158.4405403137207 and batch: 100, loss is 4.572618274688721 and perplexity is 96.79721998323983
At time: 159.3792176246643 and batch: 150, loss is 4.538437032699585 and perplexity is 93.54447884001208
At time: 160.31826210021973 and batch: 200, loss is 4.545059108734131 and perplexity is 94.1659930775585
At time: 161.2575716972351 and batch: 250, loss is 4.582504110336304 and perplexity is 97.7588870002325
At time: 162.19690871238708 and batch: 300, loss is 4.550437641143799 and perplexity is 94.6738324141234
At time: 163.13602328300476 and batch: 350, loss is 4.612577028274536 and perplexity is 100.74343407932145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.115606899919181 and perplexity of 166.60186096345424
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 164.92220950126648 and batch: 50, loss is 4.607434797286987 and perplexity is 100.22671774678227
At time: 165.8731029033661 and batch: 100, loss is 4.563100185394287 and perplexity is 95.88026614676794
At time: 166.81202054023743 and batch: 150, loss is 4.523247575759887 and perplexity is 92.13432584818956
At time: 167.7571141719818 and batch: 200, loss is 4.524788980484009 and perplexity is 92.27645164185053
At time: 168.70583605766296 and batch: 250, loss is 4.562285423278809 and perplexity is 95.80217835409694
At time: 169.6598391532898 and batch: 300, loss is 4.527711925506591 and perplexity is 92.5465652081187
At time: 170.60747456550598 and batch: 350, loss is 4.596012191772461 and perplexity is 99.08838124941903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.107061451879041 and perplexity of 165.18423915074234
Finished 23 epochs...
Completing Train Step...
At time: 172.4256944656372 and batch: 50, loss is 4.599566793441772 and perplexity is 99.44122771761646
At time: 173.36423468589783 and batch: 100, loss is 4.558821487426758 and perplexity is 95.47089984878413
At time: 174.31369400024414 and batch: 150, loss is 4.520941219329834 and perplexity is 91.92207610920075
At time: 175.2521710395813 and batch: 200, loss is 4.523781948089599 and perplexity is 92.1835730395358
At time: 176.19130635261536 and batch: 250, loss is 4.563008737564087 and perplexity is 95.87149850536683
At time: 177.12949204444885 and batch: 300, loss is 4.529028902053833 and perplexity is 92.6685271569108
At time: 178.0680055618286 and batch: 350, loss is 4.59683313369751 and perplexity is 99.16976045510239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.106144872205011 and perplexity of 165.03290400062295
Finished 24 epochs...
Completing Train Step...
At time: 179.87397742271423 and batch: 50, loss is 4.596710624694825 and perplexity is 99.15761201081456
At time: 180.81245040893555 and batch: 100, loss is 4.556944723129273 and perplexity is 95.29189150327764
At time: 181.75060749053955 and batch: 150, loss is 4.5194305896759035 and perplexity is 91.7833207255646
At time: 182.68983578681946 and batch: 200, loss is 4.523151540756226 and perplexity is 92.12547815272043
At time: 183.62824535369873 and batch: 250, loss is 4.56357967376709 and perplexity is 95.92625064320123
At time: 184.5669617652893 and batch: 300, loss is 4.529816370010376 and perplexity is 92.7415293923151
At time: 185.50532984733582 and batch: 350, loss is 4.597166585922241 and perplexity is 99.20283434632702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105800234038254 and perplexity of 164.9760371629378
Finished 25 epochs...
Completing Train Step...
At time: 187.28991675376892 and batch: 50, loss is 4.594561004638672 and perplexity is 98.94468975228504
At time: 188.2418191432953 and batch: 100, loss is 4.555600967407226 and perplexity is 95.1639284735872
At time: 189.18509483337402 and batch: 150, loss is 4.518354768753052 and perplexity is 91.68463140429607
At time: 190.12881302833557 and batch: 200, loss is 4.5226459980010985 and perplexity is 92.07891655510849
At time: 191.06670665740967 and batch: 250, loss is 4.564004287719727 and perplexity is 95.96699091648061
At time: 192.00513553619385 and batch: 300, loss is 4.53031494140625 and perplexity is 92.78777919453583
At time: 192.94317245483398 and batch: 350, loss is 4.5973395824432375 and perplexity is 99.21999757608863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105653433964171 and perplexity of 164.9518204460117
Finished 26 epochs...
Completing Train Step...
At time: 194.74164032936096 and batch: 50, loss is 4.592731876373291 and perplexity is 98.76387264280613
At time: 195.68073534965515 and batch: 100, loss is 4.554398498535156 and perplexity is 95.04956558454097
At time: 196.61950206756592 and batch: 150, loss is 4.517288932800293 and perplexity is 91.58696268649138
At time: 197.55758380889893 and batch: 200, loss is 4.522138128280639 and perplexity is 92.03216433452098
At time: 198.49614191055298 and batch: 250, loss is 4.564242925643921 and perplexity is 95.98989501276834
At time: 199.43617844581604 and batch: 300, loss is 4.5306235122680665 and perplexity is 92.81641521742188
At time: 200.37541222572327 and batch: 350, loss is 4.597442150115967 and perplexity is 99.23017486224958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105523997339709 and perplexity of 164.930471020903
Finished 27 epochs...
Completing Train Step...
At time: 202.17172265052795 and batch: 50, loss is 4.591119251251221 and perplexity is 98.6047318923034
At time: 203.11025714874268 and batch: 100, loss is 4.553357667922974 and perplexity is 94.9506865540934
At time: 204.04853534698486 and batch: 150, loss is 4.516300897598267 and perplexity is 91.4965162328085
At time: 204.98624992370605 and batch: 200, loss is 4.52168978691101 and perplexity is 91.99091175622307
At time: 205.92567992210388 and batch: 250, loss is 4.564415969848633 and perplexity is 96.00650694506908
At time: 206.86690497398376 and batch: 300, loss is 4.530790061950683 and perplexity is 92.83187504929728
At time: 207.81076622009277 and batch: 350, loss is 4.597472648620606 and perplexity is 99.23320128034833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105460857522899 and perplexity of 164.92005766992827
Finished 28 epochs...
Completing Train Step...
At time: 209.59812879562378 and batch: 50, loss is 4.589666128158569 and perplexity is 98.46155113417369
At time: 210.55639743804932 and batch: 100, loss is 4.552353258132935 and perplexity is 94.85536503389523
At time: 211.49906730651855 and batch: 150, loss is 4.515446271896362 and perplexity is 91.41835436273008
At time: 212.45082330703735 and batch: 200, loss is 4.521265420913696 and perplexity is 91.9518822232009
At time: 213.3903112411499 and batch: 250, loss is 4.5645095729827885 and perplexity is 96.01549387561435
At time: 214.32939267158508 and batch: 300, loss is 4.530947751998902 and perplexity is 92.84651486639639
At time: 215.27511835098267 and batch: 350, loss is 4.59746862411499 and perplexity is 99.23280191657612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105406136348329 and perplexity of 164.9110332975769
Finished 29 epochs...
Completing Train Step...
At time: 217.06684136390686 and batch: 50, loss is 4.5883390331268314 and perplexity is 98.33096996482162
At time: 218.0172188282013 and batch: 100, loss is 4.551434144973755 and perplexity is 94.76822227283763
At time: 218.96183109283447 and batch: 150, loss is 4.514549026489258 and perplexity is 91.33636645129191
At time: 219.90922021865845 and batch: 200, loss is 4.520831956863403 and perplexity is 91.91203302512166
At time: 220.8557252883911 and batch: 250, loss is 4.564562377929687 and perplexity is 96.02056410253525
At time: 221.79400277137756 and batch: 300, loss is 4.531009225845337 and perplexity is 92.85222267423202
At time: 222.73203897476196 and batch: 350, loss is 4.597443399429321 and perplexity is 99.23029883190961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.105451386550377 and perplexity of 164.91849572399036
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 224.5301113128662 and batch: 50, loss is 4.5867860317230225 and perplexity is 98.1783803470418
At time: 225.46958994865417 and batch: 100, loss is 4.549619140625 and perplexity is 94.59637353756553
At time: 226.40922284126282 and batch: 150, loss is 4.509777688980103 and perplexity is 90.90160783493677
At time: 227.35404086112976 and batch: 200, loss is 4.515740795135498 and perplexity is 91.4452831579568
At time: 228.2963809967041 and batch: 250, loss is 4.558414640426636 and perplexity is 95.4320656998959
At time: 229.23502159118652 and batch: 300, loss is 4.5245193672180175 and perplexity is 92.25157603989612
At time: 230.1733374595642 and batch: 350, loss is 4.592389535903931 and perplexity is 98.73006755904404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.103198873585668 and perplexity of 164.5474327432312
Finished 31 epochs...
Completing Train Step...
At time: 231.97240114212036 and batch: 50, loss is 4.585173978805542 and perplexity is 98.02023910284385
At time: 232.91090297698975 and batch: 100, loss is 4.548481426239014 and perplexity is 94.48881108180866
At time: 233.8559329509735 and batch: 150, loss is 4.509654130935669 and perplexity is 90.89037690388699
At time: 234.80104994773865 and batch: 200, loss is 4.516484727859497 and perplexity is 91.5133376073807
At time: 235.74606800079346 and batch: 250, loss is 4.558722381591797 and perplexity is 95.46143859438065
At time: 236.70189809799194 and batch: 300, loss is 4.524822463989258 and perplexity is 92.27954143263169
At time: 237.64025378227234 and batch: 350, loss is 4.591593093872071 and perplexity is 98.65146608834401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102749002390895 and perplexity of 164.47342424146095
Finished 32 epochs...
Completing Train Step...
At time: 239.42399096488953 and batch: 50, loss is 4.584119501113892 and perplexity is 97.9169334237191
At time: 240.374685049057 and batch: 100, loss is 4.5481030559539795 and perplexity is 94.45306608627561
At time: 241.31228518486023 and batch: 150, loss is 4.509870510101319 and perplexity is 90.9100458157017
At time: 242.25179362297058 and batch: 200, loss is 4.517289543151856 and perplexity is 91.58701858675427
At time: 243.1896674633026 and batch: 250, loss is 4.5590118789672855 and perplexity is 95.48907843095114
At time: 244.12749648094177 and batch: 300, loss is 4.524958152770996 and perplexity is 92.29206358072629
At time: 245.06498551368713 and batch: 350, loss is 4.5907649898529055 and perplexity is 98.5698062288752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102560109105603 and perplexity of 164.44235925008934
Finished 33 epochs...
Completing Train Step...
At time: 246.8540997505188 and batch: 50, loss is 4.58328535079956 and perplexity is 97.83529003908676
At time: 247.79314804077148 and batch: 100, loss is 4.54785719871521 and perplexity is 94.42984697066512
At time: 248.73218941688538 and batch: 150, loss is 4.510125780105591 and perplexity is 90.93325538571295
At time: 249.67087173461914 and batch: 200, loss is 4.518015041351318 and perplexity is 91.65348891296853
At time: 250.61077046394348 and batch: 250, loss is 4.559236354827881 and perplexity is 95.51051583000869
At time: 251.54903364181519 and batch: 300, loss is 4.5250352764129635 and perplexity is 92.29918175528061
At time: 252.48867273330688 and batch: 350, loss is 4.589969234466553 and perplexity is 98.49139997487372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102448562095905 and perplexity of 164.42401721966556
Finished 34 epochs...
Completing Train Step...
At time: 254.30290460586548 and batch: 50, loss is 4.582552852630616 and perplexity is 97.7636521088045
At time: 255.24190640449524 and batch: 100, loss is 4.547667026519775 and perplexity is 94.41189074679353
At time: 256.1803545951843 and batch: 150, loss is 4.510377855300903 and perplexity is 90.95618029310351
At time: 257.118038892746 and batch: 200, loss is 4.518674983978271 and perplexity is 91.71399492026188
At time: 258.0564136505127 and batch: 250, loss is 4.559415330886841 and perplexity is 95.52761145552938
At time: 258.99463176727295 and batch: 300, loss is 4.525067396163941 and perplexity is 92.30214642962603
At time: 259.9466700553894 and batch: 350, loss is 4.589210462570191 and perplexity is 98.41669581383253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102372794315733 and perplexity of 164.41155964882114
Finished 35 epochs...
Completing Train Step...
At time: 261.73046612739563 and batch: 50, loss is 4.581877450942994 and perplexity is 97.69764466646029
At time: 262.68351769447327 and batch: 100, loss is 4.54749285697937 and perplexity is 94.39544850308395
At time: 263.622451543808 and batch: 150, loss is 4.510624494552612 and perplexity is 90.97861642405087
At time: 264.5613214969635 and batch: 200, loss is 4.519279060363769 and perplexity is 91.7694139157809
At time: 265.5023715496063 and batch: 250, loss is 4.55955472946167 and perplexity is 95.54092879661071
At time: 266.44920659065247 and batch: 300, loss is 4.5250723171234135 and perplexity is 92.30260064586545
At time: 267.39801931381226 and batch: 350, loss is 4.588485536575317 and perplexity is 98.34537684631754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102319125471444 and perplexity of 164.40273610720382
Finished 36 epochs...
Completing Train Step...
At time: 269.1928286552429 and batch: 50, loss is 4.581259765625 and perplexity is 97.63731689945334
At time: 270.1467218399048 and batch: 100, loss is 4.547360563278199 and perplexity is 94.3829614058281
At time: 271.0849919319153 and batch: 150, loss is 4.510867443084717 and perplexity is 91.00072223054187
At time: 272.0204508304596 and batch: 200, loss is 4.5198523139953615 and perplexity is 91.82203614707876
At time: 272.955198764801 and batch: 250, loss is 4.559688625335693 and perplexity is 95.55372218924903
At time: 273.88982462882996 and batch: 300, loss is 4.52507453918457 and perplexity is 92.30280574811687
At time: 274.82387804985046 and batch: 350, loss is 4.5877836799621585 and perplexity is 98.2763767101378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102294921875 and perplexity of 164.39875701787906
Finished 37 epochs...
Completing Train Step...
At time: 276.6092176437378 and batch: 50, loss is 4.580687618255615 and perplexity is 97.58146994330349
At time: 277.54555344581604 and batch: 100, loss is 4.547244358062744 and perplexity is 94.37199425069534
At time: 278.4818618297577 and batch: 150, loss is 4.51110011100769 and perplexity is 91.0218976428964
At time: 279.41667675971985 and batch: 200, loss is 4.520390529632568 and perplexity is 91.87146950448293
At time: 280.3526029586792 and batch: 250, loss is 4.559805488586425 and perplexity is 95.5648895603586
At time: 281.2882194519043 and batch: 300, loss is 4.525066680908203 and perplexity is 92.30208041000985
At time: 282.2233989238739 and batch: 350, loss is 4.58710129737854 and perplexity is 98.20933749808643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1022796630859375 and perplexity of 164.39624851106203
Finished 38 epochs...
Completing Train Step...
At time: 284.00872898101807 and batch: 50, loss is 4.580144882202148 and perplexity is 97.52852333073245
At time: 284.94476222991943 and batch: 100, loss is 4.5471391773223875 and perplexity is 94.36206865647097
At time: 285.8842360973358 and batch: 150, loss is 4.5113208198547365 and perplexity is 91.04198919809158
At time: 286.82060289382935 and batch: 200, loss is 4.520897722244262 and perplexity is 91.91807785374716
At time: 287.7566168308258 and batch: 250, loss is 4.559911308288574 and perplexity is 95.57500274358529
At time: 288.6922299861908 and batch: 300, loss is 4.525049772262573 and perplexity is 92.30051972003587
At time: 289.62750124931335 and batch: 350, loss is 4.586436786651611 and perplexity is 98.14409801840209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102271770608836 and perplexity of 164.39495102255535
Finished 39 epochs...
Completing Train Step...
At time: 291.3974494934082 and batch: 50, loss is 4.579623365402222 and perplexity is 97.47767382793096
At time: 292.34597754478455 and batch: 100, loss is 4.547042961120606 and perplexity is 94.35298993339968
At time: 293.28097915649414 and batch: 150, loss is 4.511525468826294 and perplexity is 91.06062275415354
At time: 294.21549916267395 and batch: 200, loss is 4.521378145217896 and perplexity is 91.96224801937193
At time: 295.15055108070374 and batch: 250, loss is 4.559999275207519 and perplexity is 95.58341055190392
At time: 296.08596324920654 and batch: 300, loss is 4.525021600723266 and perplexity is 92.2979195089427
At time: 297.04557037353516 and batch: 350, loss is 4.585800495147705 and perplexity is 98.08166962610849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102259142645474 and perplexity of 164.3928750622445
Finished 40 epochs...
Completing Train Step...
At time: 298.8329048156738 and batch: 50, loss is 4.579129219055176 and perplexity is 97.42951749060931
At time: 299.7687692642212 and batch: 100, loss is 4.546965560913086 and perplexity is 94.34568727501602
At time: 300.7091484069824 and batch: 150, loss is 4.511721878051758 and perplexity is 91.07850965705782
At time: 301.66454696655273 and batch: 200, loss is 4.521839628219604 and perplexity is 92.00469682758013
At time: 302.6147451400757 and batch: 250, loss is 4.56008165359497 and perplexity is 95.59128488346518
At time: 303.55644512176514 and batch: 300, loss is 4.524987592697143 and perplexity is 92.2947806922577
At time: 304.4962260723114 and batch: 350, loss is 4.585183849334717 and perplexity is 98.02120661924863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102252302498653 and perplexity of 164.39175059468846
Finished 41 epochs...
Completing Train Step...
At time: 306.3061022758484 and batch: 50, loss is 4.578654460906982 and perplexity is 97.38327301164627
At time: 307.2454671859741 and batch: 100, loss is 4.5468861103057865 and perplexity is 94.33819175063186
At time: 308.1978266239166 and batch: 150, loss is 4.511905736923218 and perplexity is 91.0952567885648
At time: 309.1367917060852 and batch: 200, loss is 4.5222780227661135 and perplexity is 92.04504002739584
At time: 310.0772500038147 and batch: 250, loss is 4.560155439376831 and perplexity is 95.59833842138156
At time: 311.01626205444336 and batch: 300, loss is 4.524945812225342 and perplexity is 92.2909246533297
At time: 311.9555284976959 and batch: 350, loss is 4.58458517074585 and perplexity is 97.96254098427279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102245988516972 and perplexity of 164.3907126314636
Finished 42 epochs...
Completing Train Step...
At time: 313.75418162345886 and batch: 50, loss is 4.578195934295654 and perplexity is 97.33863042516133
At time: 314.70821356773376 and batch: 100, loss is 4.546805515289306 and perplexity is 94.33058886889431
At time: 315.6480655670166 and batch: 150, loss is 4.512077512741089 and perplexity is 91.11090609485156
At time: 316.5878748893738 and batch: 200, loss is 4.5227008819580075 and perplexity is 92.08397034908177
At time: 317.5280513763428 and batch: 250, loss is 4.560222272872925 and perplexity is 95.6047278060691
At time: 318.4685757160187 and batch: 300, loss is 4.524904098510742 and perplexity is 92.2870749363321
At time: 319.41403317451477 and batch: 350, loss is 4.584000854492188 and perplexity is 97.90531659952283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102241253030711 and perplexity of 164.38993416334574
Finished 43 epochs...
Completing Train Step...
At time: 321.20779752731323 and batch: 50, loss is 4.577752637863159 and perplexity is 97.295490120228
At time: 322.1600408554077 and batch: 100, loss is 4.546730442047119 and perplexity is 94.32350743156705
At time: 323.09866166114807 and batch: 150, loss is 4.512242403030395 and perplexity is 91.125930637183
At time: 324.0352854728699 and batch: 200, loss is 4.523104600906372 and perplexity is 92.12115389809892
At time: 324.97952485084534 and batch: 250, loss is 4.560285892486572 and perplexity is 95.61081033539702
At time: 325.9236946105957 and batch: 300, loss is 4.524865818023682 and perplexity is 92.28354220977187
At time: 326.862423658371 and batch: 350, loss is 4.583434314727783 and perplexity is 97.84986505375858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102238622205011 and perplexity of 164.38950168265094
Finished 44 epochs...
Completing Train Step...
At time: 328.66957807540894 and batch: 50, loss is 4.577322807312012 and perplexity is 97.25367853267805
At time: 329.60877776145935 and batch: 100, loss is 4.546660137176514 and perplexity is 94.3168762626865
At time: 330.5485954284668 and batch: 150, loss is 4.512404890060425 and perplexity is 91.14073862203105
At time: 331.4876525402069 and batch: 200, loss is 4.5234924411773685 and perplexity is 92.15688912072257
At time: 332.4403944015503 and batch: 250, loss is 4.56034517288208 and perplexity is 95.61647835004796
At time: 333.38014698028564 and batch: 300, loss is 4.5248206520080565 and perplexity is 92.27937422398882
At time: 334.32049775123596 and batch: 350, loss is 4.582879819869995 and perplexity is 97.79562284665329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102241253030711 and perplexity of 164.38993416334574
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 336.12741684913635 and batch: 50, loss is 4.5767442321777345 and perplexity is 97.19742624721808
At time: 337.0667064189911 and batch: 100, loss is 4.545873785018921 and perplexity is 94.24273913630482
At time: 338.0050163269043 and batch: 150, loss is 4.511281175613403 and perplexity is 91.03837997904321
At time: 338.9449670314789 and batch: 200, loss is 4.522150249481201 and perplexity is 92.03327988160386
At time: 339.88515424728394 and batch: 250, loss is 4.558700170516968 and perplexity is 95.45931831677164
At time: 340.823974609375 and batch: 300, loss is 4.5226091957092285 and perplexity is 92.07552790230184
At time: 341.763916015625 and batch: 350, loss is 4.581130123138427 and perplexity is 97.62465977537646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102142860149515 and perplexity of 164.37376015980018
Finished 46 epochs...
Completing Train Step...
At time: 343.5546088218689 and batch: 50, loss is 4.576531858444214 and perplexity is 97.17678625869063
At time: 344.5078384876251 and batch: 100, loss is 4.545745677947998 and perplexity is 94.23066674833396
At time: 345.44749331474304 and batch: 150, loss is 4.5111846828460695 and perplexity is 91.02959585763436
At time: 346.3885598182678 and batch: 200, loss is 4.5221906089782715 and perplexity is 92.03699437345065
At time: 347.32870531082153 and batch: 250, loss is 4.5588123226165775 and perplexity is 95.47002488011876
At time: 348.2666702270508 and batch: 300, loss is 4.522375001907348 and perplexity is 92.0539669091863
At time: 349.20567750930786 and batch: 350, loss is 4.581112794876098 and perplexity is 97.62296812431882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102071301690463 and perplexity of 164.3619982476518
Finished 47 epochs...
Completing Train Step...
At time: 351.0116765499115 and batch: 50, loss is 4.5763959884643555 and perplexity is 97.16358374763175
At time: 351.95154643058777 and batch: 100, loss is 4.545684471130371 and perplexity is 94.22489936560274
At time: 352.89229226112366 and batch: 150, loss is 4.511209745407104 and perplexity is 91.03187732102603
At time: 353.8320035934448 and batch: 200, loss is 4.522267379760742 and perplexity is 92.04406039675355
At time: 354.7715618610382 and batch: 250, loss is 4.558902416229248 and perplexity is 95.47862650703196
At time: 355.707560300827 and batch: 300, loss is 4.522172441482544 and perplexity is 92.03532230693729
At time: 356.6580104827881 and batch: 350, loss is 4.581055116653443 and perplexity is 97.61733756740884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102037100956358 and perplexity of 164.35637704277795
Finished 48 epochs...
Completing Train Step...
At time: 358.45143008232117 and batch: 50, loss is 4.576278343200683 and perplexity is 97.15215358456787
At time: 359.38811445236206 and batch: 100, loss is 4.545641326904297 and perplexity is 94.22083419293774
At time: 360.32537937164307 and batch: 150, loss is 4.511245203018189 and perplexity is 91.03510515115362
At time: 361.26180601119995 and batch: 200, loss is 4.522345046997071 and perplexity is 92.05120948216629
At time: 362.1994574069977 and batch: 250, loss is 4.55897702217102 and perplexity is 95.48575004560746
At time: 363.1339900493622 and batch: 300, loss is 4.521980791091919 and perplexity is 92.01768539158071
At time: 364.07054352760315 and batch: 350, loss is 4.580984630584717 and perplexity is 97.61045714753396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1020223683324355 and perplexity of 164.3539556599224
Finished 49 epochs...
Completing Train Step...
At time: 365.838849067688 and batch: 50, loss is 4.576168451309204 and perplexity is 97.14147793724348
At time: 366.7872235774994 and batch: 100, loss is 4.545604190826416 and perplexity is 94.21733526566975
At time: 367.723185300827 and batch: 150, loss is 4.5112849044799805 and perplexity is 91.03871944964848
At time: 368.66101336479187 and batch: 200, loss is 4.522422752380371 and perplexity is 92.05836263459798
At time: 369.6000392436981 and batch: 250, loss is 4.559042062759399 and perplexity is 95.49196069694227
At time: 370.5368182659149 and batch: 300, loss is 4.521797323226929 and perplexity is 92.00080465188458
At time: 371.47524094581604 and batch: 350, loss is 4.580907220840454 and perplexity is 97.60290143945531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102015002020474 and perplexity of 164.35274498187206
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f43f60acb00>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'lr': 1.618464462281116, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 8.0, 'dropout': 0.0, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1652765274047852 and batch: 50, loss is 6.959228105545044 and perplexity is 1052.8205771497076
At time: 2.126607894897461 and batch: 100, loss is 5.8711755752563475 and perplexity is 354.6656714815066
At time: 3.0746662616729736 and batch: 150, loss is 5.668674421310425 and perplexity is 289.6503254995238
At time: 4.020880699157715 and batch: 200, loss is 5.511148891448975 and perplexity is 247.43524006035884
At time: 4.961035251617432 and batch: 250, loss is 5.4202272319793705 and perplexity is 225.930455294143
At time: 5.896351099014282 and batch: 300, loss is 5.337832880020142 and perplexity is 208.06132750555528
At time: 6.8303611278533936 and batch: 350, loss is 5.31737208366394 and perplexity is 203.84748336470523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.367413224845097 and perplexity of 214.30778402430698
Finished 1 epochs...
Completing Train Step...
At time: 8.603919982910156 and batch: 50, loss is 5.195561771392822 and perplexity is 180.46949692606918
At time: 9.55216908454895 and batch: 100, loss is 5.102006025314331 and perplexity is 164.35126964219845
At time: 10.488674879074097 and batch: 150, loss is 5.05266206741333 and perplexity is 156.43836013036497
At time: 11.424219846725464 and batch: 200, loss is 5.01117018699646 and perplexity is 150.08025539713822
At time: 12.3598792552948 and batch: 250, loss is 4.985625038146972 and perplexity is 146.29498640259496
At time: 13.295111656188965 and batch: 300, loss is 4.949470310211182 and perplexity is 141.10020478597417
At time: 14.230148792266846 and batch: 350, loss is 4.983601388931274 and perplexity is 145.99923601654777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.090378333782327 and perplexity of 162.45131124763154
Finished 2 epochs...
Completing Train Step...
At time: 16.01513123512268 and batch: 50, loss is 4.904383153915405 and perplexity is 134.87968433556102
At time: 16.950408697128296 and batch: 100, loss is 4.827622451782227 and perplexity is 124.913619170339
At time: 17.887125492095947 and batch: 150, loss is 4.789582576751709 and perplexity is 120.2511625544617
At time: 18.823116064071655 and batch: 200, loss is 4.789649295806885 and perplexity is 120.25918586606203
At time: 19.759637594223022 and batch: 250, loss is 4.78251537322998 and perplexity is 119.40431904732895
At time: 20.695800065994263 and batch: 300, loss is 4.753478126525879 and perplexity is 115.98700123702773
At time: 21.63231372833252 and batch: 350, loss is 4.805337991714477 and perplexity is 122.16077337347694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.955330158102101 and perplexity of 141.92945779876302
Finished 3 epochs...
Completing Train Step...
At time: 23.41494369506836 and batch: 50, loss is 4.7357197189331055 and perplexity is 113.94543790268533
At time: 24.34847378730774 and batch: 100, loss is 4.6662483978271485 and perplexity is 106.29820487170156
At time: 25.283979177474976 and batch: 150, loss is 4.625935029983521 and perplexity is 102.09819333738184
At time: 26.220671892166138 and batch: 200, loss is 4.649427118301392 and perplexity is 104.5250879117042
At time: 27.156909227371216 and batch: 250, loss is 4.650122413635254 and perplexity is 104.59778898903176
At time: 28.092907905578613 and batch: 300, loss is 4.620564002990722 and perplexity is 101.5512912128364
At time: 29.031349182128906 and batch: 350, loss is 4.68363278388977 and perplexity is 108.16228994396857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.86993302970097 and perplexity of 130.31218955792394
Finished 4 epochs...
Completing Train Step...
At time: 30.807373523712158 and batch: 50, loss is 4.614881315231323 and perplexity is 100.97584352664529
At time: 31.758843898773193 and batch: 100, loss is 4.5518987846374515 and perplexity is 94.8122655791047
At time: 32.71424388885498 and batch: 150, loss is 4.506807146072387 and perplexity is 90.63198137523466
At time: 33.65390396118164 and batch: 200, loss is 4.54632646560669 and perplexity is 94.28541065240476
At time: 34.59188199043274 and batch: 250, loss is 4.552120952606201 and perplexity is 94.83333216763494
At time: 35.53006196022034 and batch: 300, loss is 4.520716857910156 and perplexity is 91.90145465512104
At time: 36.467657804489136 and batch: 350, loss is 4.590191621780395 and perplexity is 98.51330564843236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.808981007543103 and perplexity of 122.60661862207401
Finished 5 epochs...
Completing Train Step...
At time: 38.26239204406738 and batch: 50, loss is 4.520576248168945 and perplexity is 91.88853332381886
At time: 39.19962191581726 and batch: 100, loss is 4.462511396408081 and perplexity is 86.70498648768299
At time: 40.13608455657959 and batch: 150, loss is 4.413245162963867 and perplexity is 82.53687498245775
At time: 41.07263994216919 and batch: 200, loss is 4.464016370773315 and perplexity is 86.8355735100978
At time: 42.00958847999573 and batch: 250, loss is 4.473521280288696 and perplexity is 87.66487274265074
At time: 42.94634509086609 and batch: 300, loss is 4.44018934249878 and perplexity is 84.79099469282984
At time: 43.88254642486572 and batch: 350, loss is 4.5139795017242434 and perplexity is 91.28436293870128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.762987334152748 and perplexity of 117.09520643971318
Finished 6 epochs...
Completing Train Step...
At time: 45.68081188201904 and batch: 50, loss is 4.443093633651733 and perplexity is 85.03761037709299
At time: 46.620529651641846 and batch: 100, loss is 4.388506441116333 and perplexity is 80.5200676645871
At time: 47.57138466835022 and batch: 150, loss is 4.336148271560669 and perplexity is 76.41265100697682
At time: 48.511823415756226 and batch: 200, loss is 4.395294389724731 and perplexity is 81.06849298149235
At time: 49.45237731933594 and batch: 250, loss is 4.407236127853394 and perplexity is 82.04239516398432
At time: 50.392229318618774 and batch: 300, loss is 4.372150039672851 and perplexity is 79.21376147094594
At time: 51.34967303276062 and batch: 350, loss is 4.449464254379272 and perplexity is 85.5810820282079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.7283435689991915 and perplexity of 113.10805140927182
Finished 7 epochs...
Completing Train Step...
At time: 53.13063883781433 and batch: 50, loss is 4.377584056854248 and perplexity is 79.64538206655492
At time: 54.08049297332764 and batch: 100, loss is 4.325534229278564 and perplexity is 75.60589295430002
At time: 55.01648306846619 and batch: 150, loss is 4.270397882461548 and perplexity is 71.55009848571555
At time: 55.96578502655029 and batch: 200, loss is 4.3359599113464355 and perplexity is 76.39825925912191
At time: 56.90221571922302 and batch: 250, loss is 4.35003779411316 and perplexity is 77.4813912103917
At time: 57.838839530944824 and batch: 300, loss is 4.313090629577637 and perplexity is 74.67091281123642
At time: 58.77509641647339 and batch: 350, loss is 4.393519659042358 and perplexity is 80.92474583359093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702438880657327 and perplexity of 110.21564774086181
Finished 8 epochs...
Completing Train Step...
At time: 60.564692974090576 and batch: 50, loss is 4.320734119415283 and perplexity is 75.24384598957279
At time: 61.51707720756531 and batch: 100, loss is 4.27081449508667 and perplexity is 71.57991337027083
At time: 62.4532253742218 and batch: 150, loss is 4.213352518081665 and perplexity is 67.58273277389326
At time: 63.38931131362915 and batch: 200, loss is 4.283667058944702 and perplexity is 72.50583627349363
At time: 64.32614731788635 and batch: 250, loss is 4.299524631500244 and perplexity is 73.66476746507637
At time: 65.26477432250977 and batch: 300, loss is 4.261178340911865 and perplexity is 70.89347093344529
At time: 66.20189476013184 and batch: 350, loss is 4.343814010620117 and perplexity is 77.0006613378554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.682652966729526 and perplexity of 108.05636257938947
Finished 9 epochs...
Completing Train Step...
At time: 68.00534772872925 and batch: 50, loss is 4.270374212265015 and perplexity is 71.54840490086623
At time: 68.94358015060425 and batch: 100, loss is 4.2222176361083985 and perplexity is 68.18452521576565
At time: 69.88036847114563 and batch: 150, loss is 4.162768530845642 and perplexity is 64.24915235985904
At time: 70.81706047058105 and batch: 200, loss is 4.236665558815003 and perplexity is 69.17680086603976
At time: 71.75307321548462 and batch: 250, loss is 4.254201393127442 and perplexity is 70.40057235175313
At time: 72.68906903266907 and batch: 300, loss is 4.2148949146270756 and perplexity is 67.68705257802458
At time: 73.62625503540039 and batch: 350, loss is 4.298853187561035 and perplexity is 73.61532230511285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.667367869410022 and perplexity of 106.4172693235666
Finished 10 epochs...
Completing Train Step...
At time: 75.41008377075195 and batch: 50, loss is 4.225136857032776 and perplexity is 68.38386172058256
At time: 76.34678411483765 and batch: 100, loss is 4.178359603881836 and perplexity is 65.25871522053625
At time: 77.28321886062622 and batch: 150, loss is 4.1172072696685795 and perplexity is 61.3875637388718
At time: 78.22016048431396 and batch: 200, loss is 4.1941513299942015 and perplexity is 66.29744304410573
At time: 79.15562391281128 and batch: 250, loss is 4.213128700256347 and perplexity is 67.56760824624736
At time: 80.10450863838196 and batch: 300, loss is 4.173144102096558 and perplexity is 64.91924429990324
At time: 81.03990840911865 and batch: 350, loss is 4.257695159912109 and perplexity is 70.64696570287893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.654901964911099 and perplexity of 105.09891611225223
Finished 11 epochs...
Completing Train Step...
At time: 82.81592535972595 and batch: 50, loss is 4.183939256668091 and perplexity is 65.62385391901513
At time: 83.7669129371643 and batch: 100, loss is 4.1384699392318725 and perplexity is 62.70680279189713
At time: 84.7046468257904 and batch: 150, loss is 4.075642342567444 and perplexity is 58.888294900714875
At time: 85.6432158946991 and batch: 200, loss is 4.155342783927917 and perplexity is 63.77382144251516
At time: 86.58139657974243 and batch: 250, loss is 4.175470967292785 and perplexity is 65.07047851260747
At time: 87.52004671096802 and batch: 300, loss is 4.135017571449279 and perplexity is 62.49068911279257
At time: 88.4604127407074 and batch: 350, loss is 4.219888982772827 and perplexity is 68.02593181987379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.644951129781789 and perplexity of 104.05828031028673
Finished 12 epochs...
Completing Train Step...
At time: 90.24721121788025 and batch: 50, loss is 4.145996770858765 and perplexity is 63.18056707367615
At time: 91.18287920951843 and batch: 100, loss is 4.101776404380798 and perplexity is 60.44757161027158
At time: 92.1184515953064 and batch: 150, loss is 4.037359566688537 and perplexity is 56.676494556171846
At time: 93.05368232727051 and batch: 200, loss is 4.119747061729431 and perplexity is 61.54367354519806
At time: 93.98993492126465 and batch: 250, loss is 4.140636892318725 and perplexity is 62.84283282389306
At time: 94.92591285705566 and batch: 300, loss is 4.099829950332642 and perplexity is 60.33002762390499
At time: 95.86225032806396 and batch: 350, loss is 4.184940767288208 and perplexity is 65.6896098277723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.637260700094289 and perplexity of 103.26109669339783
Finished 13 epochs...
Completing Train Step...
At time: 97.65104508399963 and batch: 50, loss is 4.110863537788391 and perplexity is 60.99937009377219
At time: 98.58984971046448 and batch: 100, loss is 4.067851357460022 and perplexity is 58.43127968323054
At time: 99.52882766723633 and batch: 150, loss is 4.002266492843628 and perplexity is 54.722036690547185
At time: 100.46730971336365 and batch: 200, loss is 4.086778321266174 and perplexity is 59.54773865936968
At time: 101.40428757667542 and batch: 250, loss is 4.108151335716247 and perplexity is 60.834151630013736
At time: 102.34041166305542 and batch: 300, loss is 4.0670827293395995 and perplexity is 58.38638501441648
At time: 103.29167890548706 and batch: 350, loss is 4.152424211502075 and perplexity is 63.58796427649101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.631691242086476 and perplexity of 102.68758690333736
Finished 14 epochs...
Completing Train Step...
At time: 105.07324433326721 and batch: 50, loss is 4.078226819038391 and perplexity is 59.040687155572904
At time: 106.0219738483429 and batch: 100, loss is 4.036310482025146 and perplexity is 56.617067292524005
At time: 106.95691442489624 and batch: 150, loss is 3.969588050842285 and perplexity is 52.96270840200085
At time: 107.89202380180359 and batch: 200, loss is 4.055857911109924 and perplexity is 57.734672972918155
At time: 108.82716202735901 and batch: 250, loss is 4.077658410072327 and perplexity is 59.00713743551523
At time: 109.7633159160614 and batch: 300, loss is 4.036273460388184 and perplexity is 56.61497127481207
At time: 110.69855093955994 and batch: 350, loss is 4.121716065406799 and perplexity is 61.664972644724365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.628695783943965 and perplexity of 102.38045077138175
Finished 15 epochs...
Completing Train Step...
At time: 112.47132515907288 and batch: 50, loss is 4.047766666412354 and perplexity is 57.26941241319162
At time: 113.4202651977539 and batch: 100, loss is 4.00676908493042 and perplexity is 54.968983232493656
At time: 114.35616755485535 and batch: 150, loss is 3.9387067699432374 and perplexity is 51.35214819868431
At time: 115.29878616333008 and batch: 200, loss is 4.026894173622131 and perplexity is 56.08644569503393
At time: 116.23723602294922 and batch: 250, loss is 4.049019422531128 and perplexity is 57.34120197803027
At time: 117.18160700798035 and batch: 300, loss is 4.007347140312195 and perplexity is 55.00076753474031
At time: 118.12646150588989 and batch: 350, loss is 4.092809886932373 and perplexity is 59.90799010333243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.626042859307651 and perplexity of 102.10920311014182
Finished 16 epochs...
Completing Train Step...
At time: 119.9841775894165 and batch: 50, loss is 4.0191874504089355 and perplexity is 55.65586429901584
At time: 120.92113661766052 and batch: 100, loss is 3.979058337211609 and perplexity is 53.46666294791602
At time: 121.85840654373169 and batch: 150, loss is 3.909456272125244 and perplexity is 49.87182789702526
At time: 122.79532098770142 and batch: 200, loss is 3.9995502424240112 and perplexity is 54.573599622812225
At time: 123.73262476921082 and batch: 250, loss is 4.021907067298889 and perplexity is 55.807432938483544
At time: 124.6693844795227 and batch: 300, loss is 3.980010781288147 and perplexity is 53.517611213168266
At time: 125.60607957839966 and batch: 350, loss is 4.065489525794983 and perplexity is 58.29343768052575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.625152061725485 and perplexity of 102.01828497973193
Finished 17 epochs...
Completing Train Step...
At time: 127.39763617515564 and batch: 50, loss is 3.992174353599548 and perplexity is 54.17255167997066
At time: 128.33324909210205 and batch: 100, loss is 3.952850856781006 and perplexity is 52.083638377591456
At time: 129.26888418197632 and batch: 150, loss is 3.8817942333221436 and perplexity is 48.511177403652404
At time: 130.206768989563 and batch: 200, loss is 3.973550491333008 and perplexity is 53.17298631399779
At time: 131.14297747612 and batch: 250, loss is 3.9961978578567505 and perplexity is 54.39095424947697
At time: 132.07916688919067 and batch: 300, loss is 3.9542260694503786 and perplexity is 52.155313730085666
At time: 133.01657581329346 and batch: 350, loss is 4.039520130157471 and perplexity is 56.799080099093935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.625256768588362 and perplexity of 102.02896755356788
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 134.80810737609863 and batch: 50, loss is 3.978320779800415 and perplexity is 53.42724275352225
At time: 135.76131868362427 and batch: 100, loss is 3.9332475566864016 and perplexity is 51.07256970413929
At time: 136.70972847938538 and batch: 150, loss is 3.8484912395477293 and perplexity is 46.922215415875236
At time: 137.65033650398254 and batch: 200, loss is 3.9280715084075926 and perplexity is 50.80889859350493
At time: 138.5911145210266 and batch: 250, loss is 3.935884027481079 and perplexity is 51.207398700829955
At time: 139.5312089920044 and batch: 300, loss is 3.871103272438049 and perplexity is 47.99530878270753
At time: 140.4703631401062 and batch: 350, loss is 3.9320780181884767 and perplexity is 51.01287328311813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.591710057751886 and perplexity of 98.66300542139659
Finished 19 epochs...
Completing Train Step...
At time: 142.26061725616455 and batch: 50, loss is 3.951325387954712 and perplexity is 52.004246980832576
At time: 143.19864082336426 and batch: 100, loss is 3.9077076053619386 and perplexity is 49.78469489465226
At time: 144.1368854045868 and batch: 150, loss is 3.8263488817214966 and perplexity is 45.8946651077219
At time: 145.07484102249146 and batch: 200, loss is 3.9103456211090086 and perplexity is 49.91620108518414
At time: 146.01464676856995 and batch: 250, loss is 3.922957949638367 and perplexity is 50.54974746151968
At time: 146.95209550857544 and batch: 300, loss is 3.8645278215408325 and perplexity is 47.68075329195917
At time: 147.89671874046326 and batch: 350, loss is 3.9340668439865114 and perplexity is 51.11442995732728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.589931093413254 and perplexity of 98.48764348077658
Finished 20 epochs...
Completing Train Step...
At time: 149.69955444335938 and batch: 50, loss is 3.940578007698059 and perplexity is 51.448330238851966
At time: 150.63768601417542 and batch: 100, loss is 3.8964182043075564 and perplexity is 49.225816146552766
At time: 151.5896816253662 and batch: 150, loss is 3.815816254615784 and perplexity is 45.41381049052997
At time: 152.52816796302795 and batch: 200, loss is 3.9013617515563963 and perplexity is 49.469768793431356
At time: 153.46671295166016 and batch: 250, loss is 3.9164615058898926 and perplexity is 50.22241825971973
At time: 154.40538239479065 and batch: 300, loss is 3.8610947704315186 and perplexity is 47.51734348657756
At time: 155.34464573860168 and batch: 350, loss is 3.9345227241516114 and perplexity is 51.137737324374726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.589460175612877 and perplexity of 98.44127481511622
Finished 21 epochs...
Completing Train Step...
At time: 157.11885595321655 and batch: 50, loss is 3.932403964996338 and perplexity is 51.02950347645657
At time: 158.06706166267395 and batch: 100, loss is 3.888372149467468 and perplexity is 48.83133168032241
At time: 159.00260543823242 and batch: 150, loss is 3.8083444356918337 and perplexity is 45.07575125336596
At time: 159.93862962722778 and batch: 200, loss is 3.89492347240448 and perplexity is 49.15229171205166
At time: 160.8743085861206 and batch: 250, loss is 3.91167622089386 and perplexity is 49.98266377942603
At time: 161.81009769439697 and batch: 300, loss is 3.858184018135071 and perplexity is 47.37923336958425
At time: 162.7453601360321 and batch: 350, loss is 3.933930196762085 and perplexity is 51.10744578953991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.58941387308055 and perplexity of 98.43671684033045
Finished 22 epochs...
Completing Train Step...
At time: 164.5203995704651 and batch: 50, loss is 3.9254991102218626 and perplexity is 50.67836583803619
At time: 165.47010016441345 and batch: 100, loss is 3.88179313659668 and perplexity is 48.511124200238044
At time: 166.4060606956482 and batch: 150, loss is 3.802242660522461 and perplexity is 44.80154657204269
At time: 167.34144735336304 and batch: 200, loss is 3.889605941772461 and perplexity is 48.891616583469585
At time: 168.2771441936493 and batch: 250, loss is 3.907603211402893 and perplexity is 49.77949794452214
At time: 169.21333479881287 and batch: 300, loss is 3.8553847551345823 and perplexity is 47.24679189037038
At time: 170.14857268333435 and batch: 350, loss is 3.9327011108398438 and perplexity is 51.04466893437557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.589530155576509 and perplexity of 98.44816397299635
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 171.93735194206238 and batch: 50, loss is 3.925450339317322 and perplexity is 50.675894268564456
At time: 172.87425255775452 and batch: 100, loss is 3.8816721439361572 and perplexity is 48.5052550653244
At time: 173.8108172416687 and batch: 150, loss is 3.800017280578613 and perplexity is 44.7019569623079
At time: 174.74818897247314 and batch: 200, loss is 3.8847889566421507 and perplexity is 48.65667270825702
At time: 175.6980938911438 and batch: 250, loss is 3.899935746192932 and perplexity is 49.39927491207476
At time: 176.63437008857727 and batch: 300, loss is 3.841370711326599 and perplexity is 46.58929116173918
At time: 177.571839094162 and batch: 350, loss is 3.9131290912628174 and perplexity is 50.05533488866031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586835137728987 and perplexity of 98.18320161354832
Finished 24 epochs...
Completing Train Step...
At time: 179.35368061065674 and batch: 50, loss is 3.9211404943466186 and perplexity is 50.45795899148537
At time: 180.28794932365417 and batch: 100, loss is 3.877045922279358 and perplexity is 48.28137725733908
At time: 181.2235128879547 and batch: 150, loss is 3.796006464958191 and perplexity is 44.52302472703032
At time: 182.16119050979614 and batch: 200, loss is 3.881645679473877 and perplexity is 48.50397141681694
At time: 183.09750699996948 and batch: 250, loss is 3.897580485343933 and perplexity is 49.283063641554364
At time: 184.03380846977234 and batch: 300, loss is 3.840898804664612 and perplexity is 46.56731055166879
At time: 184.96948337554932 and batch: 350, loss is 3.914352307319641 and perplexity is 50.11660084113506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.58648997339709 and perplexity of 98.1493181223826
Finished 25 epochs...
Completing Train Step...
At time: 186.7446584701538 and batch: 50, loss is 3.9194845247268675 and perplexity is 50.37447128994808
At time: 187.7029139995575 and batch: 100, loss is 3.8748918199539184 and perplexity is 48.17748616650637
At time: 188.6409637928009 and batch: 150, loss is 3.794059796333313 and perplexity is 44.43643745741375
At time: 189.58240604400635 and batch: 200, loss is 3.8800563478469847 and perplexity is 48.42694374848319
At time: 190.5274875164032 and batch: 250, loss is 3.896542501449585 and perplexity is 49.23193515509358
At time: 191.47219395637512 and batch: 300, loss is 3.840834732055664 and perplexity is 46.56432695817439
At time: 192.41090631484985 and batch: 350, loss is 3.915126295089722 and perplexity is 50.155405492489734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586340542497306 and perplexity of 98.13465267722493
Finished 26 epochs...
Completing Train Step...
At time: 194.20221400260925 and batch: 50, loss is 3.9181795597076414 and perplexity is 50.3087772405936
At time: 195.13881134986877 and batch: 100, loss is 3.8733388566970826 and perplexity is 48.10272636532005
At time: 196.0740888118744 and batch: 150, loss is 3.7926758098602296 and perplexity is 44.37498056662309
At time: 197.01000547409058 and batch: 200, loss is 3.878928208351135 and perplexity is 48.3723422054468
At time: 197.94611477851868 and batch: 250, loss is 3.895847854614258 and perplexity is 49.197748222438044
At time: 198.88162326812744 and batch: 300, loss is 3.840758204460144 and perplexity is 46.56076363854315
At time: 199.83164405822754 and batch: 350, loss is 3.9155884838104247 and perplexity is 50.17859211307513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586265300882274 and perplexity of 98.12726914524482
Finished 27 epochs...
Completing Train Step...
At time: 201.62493014335632 and batch: 50, loss is 3.9170267677307127 and perplexity is 50.25081510138504
At time: 202.5639066696167 and batch: 100, loss is 3.8720571422576904 and perplexity is 48.04111190087285
At time: 203.50244283676147 and batch: 150, loss is 3.7915359258651735 and perplexity is 44.32442705453776
At time: 204.4413115978241 and batch: 200, loss is 3.878004469871521 and perplexity is 48.327679443134514
At time: 205.3841769695282 and batch: 250, loss is 3.89527681350708 and perplexity is 49.16966230569214
At time: 206.3220763206482 and batch: 300, loss is 3.840635800361633 and perplexity is 46.55506475903422
At time: 207.26101446151733 and batch: 350, loss is 3.9158624696731565 and perplexity is 50.19234222150755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586227943157327 and perplexity of 98.1236034021867
Finished 28 epochs...
Completing Train Step...
At time: 209.0478959083557 and batch: 50, loss is 3.9159659481048585 and perplexity is 50.19753631509778
At time: 210.00366592407227 and batch: 100, loss is 3.8709291076660155 and perplexity is 47.986950418582126
At time: 210.93852639198303 and batch: 150, loss is 3.79052960395813 and perplexity is 44.279844848363304
At time: 211.87360215187073 and batch: 200, loss is 3.8771905755996703 and perplexity is 48.28836182402681
At time: 212.8093454837799 and batch: 250, loss is 3.894759087562561 and perplexity is 49.144212484418084
At time: 213.74340724945068 and batch: 300, loss is 3.8404728174209595 and perplexity is 46.54747769597425
At time: 214.67890810966492 and batch: 350, loss is 3.9160164833068847 and perplexity is 50.20007312183515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586211632037985 and perplexity of 98.12200290943426
Finished 29 epochs...
Completing Train Step...
At time: 216.45236015319824 and batch: 50, loss is 3.914970455169678 and perplexity is 50.14758988711548
At time: 217.401061296463 and batch: 100, loss is 3.869900650978088 and perplexity is 47.937623288247316
At time: 218.33546900749207 and batch: 150, loss is 3.7896076822280884 and perplexity is 44.23904110901177
At time: 219.27657437324524 and batch: 200, loss is 3.8764437198638917 and perplexity is 48.25231084814199
At time: 220.21558690071106 and batch: 250, loss is 3.894268107414246 and perplexity is 49.1200895741033
At time: 221.15132665634155 and batch: 300, loss is 3.8402776193618773 and perplexity is 46.538392605397696
At time: 222.09697079658508 and batch: 350, loss is 3.9160892391204833 and perplexity is 50.20372560186581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586211105872845 and perplexity of 98.12195128107045
Finished 30 epochs...
Completing Train Step...
At time: 223.88374495506287 and batch: 50, loss is 3.914024300575256 and perplexity is 50.10016495374185
At time: 224.8299639225006 and batch: 100, loss is 3.8689424324035646 and perplexity is 47.891710567917734
At time: 225.76639032363892 and batch: 150, loss is 3.7887444925308227 and perplexity is 44.20087090094477
At time: 226.70308995246887 and batch: 200, loss is 3.8757418632507323 and perplexity is 48.218456526502486
At time: 227.63961386680603 and batch: 250, loss is 3.8937918519973755 and perplexity is 49.09670143517248
At time: 228.5767104625702 and batch: 300, loss is 3.8400572824478147 and perplexity is 46.528139609183846
At time: 229.5271441936493 and batch: 350, loss is 3.916103410720825 and perplexity is 50.20443707404204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586220050680226 and perplexity of 98.12282896694987
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 231.31961011886597 and batch: 50, loss is 3.913982620239258 and perplexity is 50.098076805550676
At time: 232.25552606582642 and batch: 100, loss is 3.8686701679229736 and perplexity is 47.87867313111136
At time: 233.1912271976471 and batch: 150, loss is 3.7882121562957765 and perplexity is 44.177347437495094
At time: 234.12686109542847 and batch: 200, loss is 3.874746627807617 and perplexity is 48.170491681670875
At time: 235.06279230117798 and batch: 250, loss is 3.892749128341675 and perplexity is 49.045533824644195
At time: 235.99834752082825 and batch: 300, loss is 3.837170295715332 and perplexity is 46.39400719985079
At time: 236.9341003894806 and batch: 350, loss is 3.9126147270202636 and perplexity is 50.02959483469371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5863626414331895 and perplexity of 98.13682137258527
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 238.71207284927368 and batch: 50, loss is 3.913846879005432 and perplexity is 50.091276892317524
At time: 239.66330528259277 and batch: 100, loss is 3.8683539485931395 and perplexity is 47.863535362734126
At time: 240.62077927589417 and batch: 150, loss is 3.7879312944412233 and perplexity is 44.16494144803072
At time: 241.56996989250183 and batch: 200, loss is 3.8744885301589966 and perplexity is 48.15806059532103
At time: 242.51270580291748 and batch: 250, loss is 3.8925005054473876 and perplexity is 49.03334149778145
At time: 243.45089673995972 and batch: 300, loss is 3.836798586845398 and perplexity is 46.37676534053736
At time: 244.38988709449768 and batch: 350, loss is 3.9121457052230837 and perplexity is 50.00613536614327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586379478717673 and perplexity of 98.13847374407568
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 246.17713570594788 and batch: 50, loss is 3.913832788467407 and perplexity is 50.09057108424838
At time: 247.11316585540771 and batch: 100, loss is 3.8683219385147094 and perplexity is 47.86200327173453
At time: 248.06600260734558 and batch: 150, loss is 3.787897744178772 and perplexity is 44.16345972751018
At time: 249.00531649589539 and batch: 200, loss is 3.8744558095932007 and perplexity is 48.1564848621103
At time: 249.94835352897644 and batch: 250, loss is 3.8924655294418335 and perplexity is 49.031626537348295
At time: 250.88661885261536 and batch: 300, loss is 3.836752133369446 and perplexity is 46.3746110286219
At time: 251.82206845283508 and batch: 350, loss is 3.9120873069763182 and perplexity is 50.00321518077805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586380531047952 and perplexity of 98.13857701821755
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 253.6114695072174 and batch: 50, loss is 3.913831124305725 and perplexity is 50.0904877255087
At time: 254.54991626739502 and batch: 100, loss is 3.868318133354187 and perplexity is 47.86182114947567
At time: 255.48764276504517 and batch: 150, loss is 3.787893772125244 and perplexity is 44.163284308232555
At time: 256.43037128448486 and batch: 200, loss is 3.87445182800293 and perplexity is 48.156293123100404
At time: 257.37362694740295 and batch: 250, loss is 3.8924612283706663 and perplexity is 49.031415649286636
At time: 258.3099925518036 and batch: 300, loss is 3.8367463397979735 and perplexity is 46.374342354776694
At time: 259.2488851547241 and batch: 350, loss is 3.912079653739929 and perplexity is 50.00283249581645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.586381057213092 and perplexity of 98.13862865532924
Annealing...
Model not improving. Stopping early with 98.12195128107045loss at 34 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f43f60acb00>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'batch_size': 80, 'lr': 16.968474233996695, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 2.2130081039708775, 'dropout': 0.6466402571867684, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}, 'best_accuracy': -135.3589373516092}, {'params': {'batch_size': 80, 'lr': 1.0239259495251651, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 4.640018865507108, 'dropout': 0.9235342072397739, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}, 'best_accuracy': -99.85380328396097}, {'params': {'batch_size': 80, 'lr': 21.64987812181001, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 6.560187749659181, 'dropout': 0.8202590420169474, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}, 'best_accuracy': -152.22578448039113}, {'params': {'batch_size': 80, 'lr': 2.350638840697703, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 5.744900982954643, 'dropout': 0.7136190258012192, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}, 'best_accuracy': -97.19682328611802}, {'params': {'batch_size': 80, 'lr': 26.567398669487652, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 3.7489425079864036, 'dropout': 0.6617483856703484, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}, 'best_accuracy': -164.35274498187206}, {'params': {'batch_size': 80, 'lr': 1.618464462281116, 'num_layers': 1, 'tune_wordvecs': True, 'data': 'ptb', 'anneal': 8.0, 'dropout': 0.0, 'seq_len': 35, 'wordvec_source': '', 'wordvec_dim': 200}, 'best_accuracy': -98.12195128107045}]
