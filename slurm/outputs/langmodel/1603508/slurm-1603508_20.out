Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 2.092748306085282, 'lr': 21.42281950332199, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.35146804005267573}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1617157459259033 and batch: 50, loss is 6.703449630737305 and perplexity is 815.213164631526
At time: 1.8230836391448975 and batch: 100, loss is 5.907035789489746 and perplexity is 367.6148509753998
At time: 2.4727091789245605 and batch: 150, loss is 5.802957792282104 and perplexity is 331.2779636429375
At time: 3.1230437755584717 and batch: 200, loss is 5.78006685256958 and perplexity is 323.7808352745464
At time: 3.768115997314453 and batch: 250, loss is 5.79930624961853 and perplexity is 330.0704939304544
At time: 4.402487516403198 and batch: 300, loss is 5.83778079032898 and perplexity is 343.0172681395853
At time: 5.039004325866699 and batch: 350, loss is 5.936730690002442 and perplexity is 378.69483270961376
At time: 5.694036483764648 and batch: 400, loss is 5.972045669555664 and perplexity is 392.3073817196918
At time: 6.347153663635254 and batch: 450, loss is 5.942429075241089 and perplexity is 380.8589418620997
At time: 7.006911039352417 and batch: 500, loss is 5.989805164337159 and perplexity is 399.33679729742
At time: 7.672598838806152 and batch: 550, loss is 6.06583342552185 and perplexity is 430.8816358211701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.07004108834774 and perplexity of 432.69846007402305
Finished 1 epochs...
Completing Train Step...
At time: 9.467242002487183 and batch: 50, loss is 6.027572431564331 and perplexity is 414.7070766305633
At time: 10.109122514724731 and batch: 100, loss is 6.050784978866577 and perplexity is 424.4460805120767
At time: 10.736562490463257 and batch: 150, loss is 6.027469673156738 and perplexity is 414.6644641811825
At time: 11.390026569366455 and batch: 200, loss is 5.825237503051758 and perplexity is 338.741575682849
At time: 12.028985023498535 and batch: 250, loss is 5.843430919647217 and perplexity is 344.9608456244723
At time: 12.667192935943604 and batch: 300, loss is 5.93454342842102 and perplexity is 377.8674332503344
At time: 13.300391674041748 and batch: 350, loss is 5.990273523330688 and perplexity is 399.52387408400955
At time: 13.939547061920166 and batch: 400, loss is 6.036961898803711 and perplexity is 418.61929321284947
At time: 14.592163324356079 and batch: 450, loss is 5.990147304534912 and perplexity is 399.47344984404924
At time: 15.23157024383545 and batch: 500, loss is 6.014713697433471 and perplexity is 409.4086074005335
At time: 15.86772346496582 and batch: 550, loss is 6.08791971206665 and perplexity is 440.5040819960416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.165835116771942 and perplexity of 476.19865841276305
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 17.60555601119995 and batch: 50, loss is 5.9756330871582035 and perplexity is 393.7172795605814
At time: 18.26331663131714 and batch: 100, loss is 5.848805732727051 and perplexity is 346.81993734927823
At time: 18.906651258468628 and batch: 150, loss is 5.7998676586151126 and perplexity is 330.25585050080815
At time: 19.540581464767456 and batch: 200, loss is 5.7308916664123535 and perplexity is 308.2439966948644
At time: 20.17483901977539 and batch: 250, loss is 5.69448184967041 and perplexity is 297.2227476154308
At time: 20.808069705963135 and batch: 300, loss is 5.683791437149048 and perplexity is 294.06223751091636
At time: 21.449676752090454 and batch: 350, loss is 5.729267539978028 and perplexity is 307.74377579256156
At time: 22.08592414855957 and batch: 400, loss is 5.683227214813233 and perplexity is 293.89636782648364
At time: 22.71785283088684 and batch: 450, loss is 5.617367248535157 and perplexity is 275.1639903655036
At time: 23.37260150909424 and batch: 500, loss is 5.657376909255982 and perplexity is 286.39641262460975
At time: 24.01023530960083 and batch: 550, loss is 5.67850417137146 and perplexity is 292.511555352862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.810157288896277 and perplexity of 333.6716043856731
Finished 3 epochs...
Completing Train Step...
At time: 25.797086000442505 and batch: 50, loss is 5.734400100708008 and perplexity is 309.32734982968526
At time: 26.424315690994263 and batch: 100, loss is 5.7131603336334225 and perplexity is 302.82659066230843
At time: 27.063769817352295 and batch: 150, loss is 5.706186037063599 and perplexity is 300.7219359837562
At time: 27.694995403289795 and batch: 200, loss is 5.633065204620362 and perplexity is 279.51758446417693
At time: 28.340043306350708 and batch: 250, loss is 5.603657751083374 and perplexity is 271.41737115536216
At time: 28.9798846244812 and batch: 300, loss is 5.6060182571411135 and perplexity is 272.05881026676315
At time: 29.61536717414856 and batch: 350, loss is 5.665452823638916 and perplexity is 288.7186901682127
At time: 30.249088525772095 and batch: 400, loss is 5.643643712997436 and perplexity is 282.49015855251787
At time: 30.888108253479004 and batch: 450, loss is 5.583316373825073 and perplexity is 265.9521416073655
At time: 31.54270601272583 and batch: 500, loss is 5.6244442653656 and perplexity is 277.11823752005745
At time: 32.19282412528992 and batch: 550, loss is 5.661688575744629 and perplexity is 287.6339243912391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.797587293259641 and perplexity of 329.5036045246929
Finished 4 epochs...
Completing Train Step...
At time: 33.918099880218506 and batch: 50, loss is 5.707721271514893 and perplexity is 301.183969234071
At time: 34.55519700050354 and batch: 100, loss is 5.695698099136353 and perplexity is 297.5844645479075
At time: 35.20920491218567 and batch: 150, loss is 5.684473819732666 and perplexity is 294.26296894032026
At time: 35.85246276855469 and batch: 200, loss is 5.612141056060791 and perplexity is 273.72968164118464
At time: 36.48646306991577 and batch: 250, loss is 5.590279884338379 and perplexity is 267.8105652079771
At time: 37.122615814208984 and batch: 300, loss is 5.589779424667358 and perplexity is 267.6765703529244
At time: 37.75658917427063 and batch: 350, loss is 5.638005495071411 and perplexity is 280.9018991604248
At time: 38.38829326629639 and batch: 400, loss is 5.611504802703857 and perplexity is 273.5555756059961
At time: 39.05509924888611 and batch: 450, loss is 5.54346625328064 and perplexity is 255.56230964581954
At time: 39.690314292907715 and batch: 500, loss is 5.577335014343261 and perplexity is 264.3661342121413
At time: 40.33874034881592 and batch: 550, loss is 5.609331216812134 and perplexity is 272.96162480164315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.735172352892287 and perplexity of 309.56632081230384
Finished 5 epochs...
Completing Train Step...
At time: 42.09891200065613 and batch: 50, loss is 5.651701765060425 and perplexity is 284.7756749957062
At time: 42.742380142211914 and batch: 100, loss is 5.644620132446289 and perplexity is 282.7661221436217
At time: 43.378960847854614 and batch: 150, loss is 5.636612968444824 and perplexity is 280.5110080127165
At time: 44.016817569732666 and batch: 200, loss is 5.5705170249938964 and perplexity is 262.56981931114797
At time: 44.658987522125244 and batch: 250, loss is 5.546409435272217 and perplexity is 256.3155840012954
At time: 45.303608894348145 and batch: 300, loss is 5.54689474105835 and perplexity is 256.4400056261124
At time: 45.93483924865723 and batch: 350, loss is 5.585676679611206 and perplexity is 266.58061138469236
At time: 46.59688687324524 and batch: 400, loss is 5.587793521881103 and perplexity is 267.1455181895615
At time: 47.24318313598633 and batch: 450, loss is 5.519221296310425 and perplexity is 249.44072113232153
At time: 47.88194799423218 and batch: 500, loss is 5.554596538543701 and perplexity is 258.4226798938042
At time: 48.51243448257446 and batch: 550, loss is 5.593601579666138 and perplexity is 268.70162941374923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.7297915194896945 and perplexity of 307.9050694795232
Finished 6 epochs...
Completing Train Step...
At time: 50.28466248512268 and batch: 50, loss is 5.628568868637085 and perplexity is 278.263600769848
At time: 50.92957353591919 and batch: 100, loss is 5.6246617603302 and perplexity is 277.17851589620096
At time: 51.573203325271606 and batch: 150, loss is 5.616574487686157 and perplexity is 274.9459375701669
At time: 52.2140896320343 and batch: 200, loss is 5.548542060852051 and perplexity is 256.8627924602289
At time: 52.855714559555054 and batch: 250, loss is 5.523985843658448 and perplexity is 250.6320290264682
At time: 53.49277329444885 and batch: 300, loss is 5.527312517166138 and perplexity is 251.4671883385797
At time: 54.127747535705566 and batch: 350, loss is 5.580429420471192 and perplexity is 265.18545740344115
At time: 54.778501749038696 and batch: 400, loss is 5.566888265609741 and perplexity is 261.6187432720165
At time: 55.41864562034607 and batch: 450, loss is 5.506957387924194 and perplexity is 246.40028490381863
At time: 56.07268810272217 and batch: 500, loss is 5.521257057189941 and perplexity is 249.94904002646737
At time: 56.709115743637085 and batch: 550, loss is 5.539676628112793 and perplexity is 254.59565706756433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.6694206075465425 and perplexity of 289.86653924352623
Finished 7 epochs...
Completing Train Step...
At time: 58.452922344207764 and batch: 50, loss is 5.54810435295105 and perplexity is 256.75038618884713
At time: 59.10793161392212 and batch: 100, loss is 5.5469945049285885 and perplexity is 256.465590349752
At time: 59.75268077850342 and batch: 150, loss is 5.539842700958252 and perplexity is 254.63794200386764
At time: 60.395756244659424 and batch: 200, loss is 5.475349979400635 and perplexity is 238.73400429536287
At time: 61.04751992225647 and batch: 250, loss is 5.456566867828369 and perplexity is 234.29168772576617
At time: 61.68335270881653 and batch: 300, loss is 5.452655544281006 and perplexity is 233.37708694639315
At time: 62.31490874290466 and batch: 350, loss is 5.501892204284668 and perplexity is 245.1553777153305
At time: 62.956846714019775 and batch: 400, loss is 5.494634418487549 and perplexity is 243.3825337398803
At time: 63.60839796066284 and batch: 450, loss is 5.430450210571289 and perplexity is 228.25198375133064
At time: 64.24898862838745 and batch: 500, loss is 5.44653504371643 and perplexity is 231.9530646639532
At time: 64.8829357624054 and batch: 550, loss is 5.474291172027588 and perplexity is 238.48136474333037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.6269148156998 and perplexity of 277.80371848332754
Finished 8 epochs...
Completing Train Step...
At time: 66.58169913291931 and batch: 50, loss is 5.518514852523804 and perplexity is 249.26456751338813
At time: 67.2301938533783 and batch: 100, loss is 5.515094776153564 and perplexity is 248.41351980972118
At time: 67.87126851081848 and batch: 150, loss is 5.513029155731201 and perplexity is 247.9009213691771
At time: 68.51497220993042 and batch: 200, loss is 5.453524389266968 and perplexity is 233.5799435709442
At time: 69.16605544090271 and batch: 250, loss is 5.423091030120849 and perplexity is 226.57840186337242
At time: 69.80108618736267 and batch: 300, loss is 5.415447950363159 and perplexity is 224.85324621581066
At time: 70.444256067276 and batch: 350, loss is 5.466137008666992 and perplexity is 236.54465558919998
At time: 71.09381651878357 and batch: 400, loss is 5.467602338790893 and perplexity is 236.89152567621005
At time: 71.73860168457031 and batch: 450, loss is 5.4090437412261965 and perplexity is 223.41784022795346
At time: 72.40814852714539 and batch: 500, loss is 5.434983263015747 and perplexity is 229.2890106370728
At time: 73.04933476448059 and batch: 550, loss is 5.470551614761352 and perplexity is 237.59121544242768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.628819891747008 and perplexity of 278.3334601320818
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 74.827157497406 and batch: 50, loss is 5.438294343948364 and perplexity is 230.0494633737771
At time: 75.49356245994568 and batch: 100, loss is 5.36889877319336 and perplexity is 214.62638518900891
At time: 76.14729523658752 and batch: 150, loss is 5.3263075733184815 and perplexity is 205.67712263114583
At time: 76.79364943504333 and batch: 200, loss is 5.262643728256226 and perplexity is 192.99103330310953
At time: 77.42697834968567 and batch: 250, loss is 5.240443153381348 and perplexity is 188.75373072822458
At time: 78.06654453277588 and batch: 300, loss is 5.227484426498413 and perplexity is 186.32350302077796
At time: 78.7131757736206 and batch: 350, loss is 5.255781917572022 and perplexity is 191.67129843249714
At time: 79.34906053543091 and batch: 400, loss is 5.25669041633606 and perplexity is 191.8455106940543
At time: 80.00393891334534 and batch: 450, loss is 5.202572603225708 and perplexity is 181.7391837999518
At time: 80.64371991157532 and batch: 500, loss is 5.217583475112915 and perplexity is 184.48782554785592
At time: 81.28521394729614 and batch: 550, loss is 5.268189144134522 and perplexity is 194.06422173144242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.440121265167885 and perplexity of 230.47012976512974
Finished 10 epochs...
Completing Train Step...
At time: 83.02216744422913 and batch: 50, loss is 5.315950651168823 and perplexity is 203.55793376416048
At time: 83.67058849334717 and batch: 100, loss is 5.30063346862793 and perplexity is 200.4637572632474
At time: 84.30938029289246 and batch: 150, loss is 5.288205032348633 and perplexity is 197.98772470803914
At time: 84.95613408088684 and batch: 200, loss is 5.235910568237305 and perplexity is 187.90012435562178
At time: 85.5911078453064 and batch: 250, loss is 5.204713382720947 and perplexity is 182.12866406504804
At time: 86.22789740562439 and batch: 300, loss is 5.191747522354126 and perplexity is 179.78245243325813
At time: 86.87534832954407 and batch: 350, loss is 5.2330675792694095 and perplexity is 187.36668501538963
At time: 87.52172374725342 and batch: 400, loss is 5.243151378631592 and perplexity is 189.26561117877634
At time: 88.17404747009277 and batch: 450, loss is 5.19386435508728 and perplexity is 180.16342489864735
At time: 88.84860372543335 and batch: 500, loss is 5.207425737380982 and perplexity is 182.62333215012634
At time: 89.48131370544434 and batch: 550, loss is 5.250938787460327 and perplexity is 190.74525368321716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.427783722573138 and perplexity of 227.64416330920216
Finished 11 epochs...
Completing Train Step...
At time: 91.20286703109741 and batch: 50, loss is 5.295115489959716 and perplexity is 199.3606487903905
At time: 91.86132097244263 and batch: 100, loss is 5.288207550048828 and perplexity is 197.98822318239974
At time: 92.50636863708496 and batch: 150, loss is 5.268954944610596 and perplexity is 194.21289312387918
At time: 93.14732241630554 and batch: 200, loss is 5.220375213623047 and perplexity is 185.00358691554604
At time: 93.78348731994629 and batch: 250, loss is 5.18946949005127 and perplexity is 179.3733683294395
At time: 94.42499208450317 and batch: 300, loss is 5.174061326980591 and perplexity is 176.63073792869386
At time: 95.0705840587616 and batch: 350, loss is 5.218421039581298 and perplexity is 184.64241072387262
At time: 95.70990991592407 and batch: 400, loss is 5.225892038345337 and perplexity is 186.02703978684391
At time: 96.34095621109009 and batch: 450, loss is 5.1740791130065915 and perplexity is 176.63387951552923
At time: 96.97461771965027 and batch: 500, loss is 5.186805582046508 and perplexity is 178.89617006604192
At time: 97.62829351425171 and batch: 550, loss is 5.228368215560913 and perplexity is 186.48824648334633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.413484289291057 and perplexity of 224.41214387876454
Finished 12 epochs...
Completing Train Step...
At time: 99.34032368659973 and batch: 50, loss is 5.275353698730469 and perplexity is 195.45959809942724
At time: 100.00432300567627 and batch: 100, loss is 5.270343399047851 and perplexity is 194.48273616614352
At time: 100.66628360748291 and batch: 150, loss is 5.249450902938843 and perplexity is 190.46165780399366
At time: 101.29942798614502 and batch: 200, loss is 5.197695112228393 and perplexity is 180.85491083688098
At time: 101.94063591957092 and batch: 250, loss is 5.1725434303283695 and perplexity is 176.36283409951295
At time: 102.57241439819336 and batch: 300, loss is 5.150905904769897 and perplexity is 172.58776760189417
At time: 103.22110176086426 and batch: 350, loss is 5.19196533203125 and perplexity is 179.82161505601957
At time: 103.86343026161194 and batch: 400, loss is 5.201932134628296 and perplexity is 181.62282282656108
At time: 104.51915550231934 and batch: 450, loss is 5.14913724899292 and perplexity is 172.28278903024722
At time: 105.1846969127655 and batch: 500, loss is 5.163457412719726 and perplexity is 174.767656165015
At time: 105.83679914474487 and batch: 550, loss is 5.203545217514038 and perplexity is 181.91603191542592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.397712545191988 and perplexity of 220.90053784044568
Finished 13 epochs...
Completing Train Step...
At time: 107.55033874511719 and batch: 50, loss is 5.251879558563233 and perplexity is 190.924785741944
At time: 108.20532512664795 and batch: 100, loss is 5.2506305122375485 and perplexity is 190.68646071031807
At time: 108.85718631744385 and batch: 150, loss is 5.230488929748535 and perplexity is 186.88415440888514
At time: 109.49775409698486 and batch: 200, loss is 5.179373416900635 and perplexity is 177.5715128193671
At time: 110.14641118049622 and batch: 250, loss is 5.152499990463257 and perplexity is 172.86310669182376
At time: 110.79639458656311 and batch: 300, loss is 5.135255403518677 and perplexity is 169.90770936035452
At time: 111.43641352653503 and batch: 350, loss is 5.179124822616577 and perplexity is 177.52737504269555
At time: 112.0716552734375 and batch: 400, loss is 5.187156000137329 and perplexity is 178.95886950527893
At time: 112.70777678489685 and batch: 450, loss is 5.1362619972229 and perplexity is 170.07882349765407
At time: 113.35600090026855 and batch: 500, loss is 5.148276605606079 and perplexity is 172.13457877443844
At time: 113.9995436668396 and batch: 550, loss is 5.190579996109009 and perplexity is 179.57267418626475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.387690442673703 and perplexity of 218.69770694054893
Finished 14 epochs...
Completing Train Step...
At time: 115.743971824646 and batch: 50, loss is 5.235337638854981 and perplexity is 187.7925016864747
At time: 116.39608311653137 and batch: 100, loss is 5.235391292572022 and perplexity is 187.80257772252855
At time: 117.03066945075989 and batch: 150, loss is 5.215621795654297 and perplexity is 184.12627430987504
At time: 117.68429446220398 and batch: 200, loss is 5.167555236816407 and perplexity is 175.48529264758176
At time: 118.31493520736694 and batch: 250, loss is 5.139198255538941 and perplexity is 170.57895275254666
At time: 118.95145344734192 and batch: 300, loss is 5.121765604019165 and perplexity is 167.63107859791538
At time: 119.58989000320435 and batch: 350, loss is 5.164234209060669 and perplexity is 174.90346778299386
At time: 120.2253828048706 and batch: 400, loss is 5.170636949539184 and perplexity is 176.02692205105365
At time: 120.864905834198 and batch: 450, loss is 5.1171285343170165 and perplexity is 166.85556105628288
At time: 121.5276107788086 and batch: 500, loss is 5.12975302696228 and perplexity is 168.97538052804984
At time: 122.17992663383484 and batch: 550, loss is 5.172119226455688 and perplexity is 176.28803616819863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.37462924389129 and perplexity of 215.85982612356423
Finished 15 epochs...
Completing Train Step...
At time: 123.93324422836304 and batch: 50, loss is 5.210884027481079 and perplexity is 183.25598993829394
At time: 124.61050510406494 and batch: 100, loss is 5.213480005264282 and perplexity is 183.73233644177049
At time: 125.2579288482666 and batch: 150, loss is 5.19406753540039 and perplexity is 180.20003427875596
At time: 125.90849351882935 and batch: 200, loss is 5.144230623245239 and perplexity is 171.4395323260189
At time: 126.54058694839478 and batch: 250, loss is 5.113685722351074 and perplexity is 166.28209646590855
At time: 127.17430138587952 and batch: 300, loss is 5.098109722137451 and perplexity is 163.71215317247783
At time: 127.81417179107666 and batch: 350, loss is 5.136244277954102 and perplexity is 170.07580985196356
At time: 128.44987535476685 and batch: 400, loss is 5.144696378707886 and perplexity is 171.51939982263107
At time: 129.08938884735107 and batch: 450, loss is 5.093051986694336 and perplexity is 162.88623082191927
At time: 129.72310662269592 and batch: 500, loss is 5.105139360427857 and perplexity is 164.86704487266002
At time: 130.36209416389465 and batch: 550, loss is 5.144943084716797 and perplexity is 171.56171990930724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.359421101022274 and perplexity of 212.60183583325343
Finished 16 epochs...
Completing Train Step...
At time: 132.10009717941284 and batch: 50, loss is 5.186487741470337 and perplexity is 178.839318639597
At time: 132.75308418273926 and batch: 100, loss is 5.1858828830718995 and perplexity is 178.7311788836637
At time: 133.39710354804993 and batch: 150, loss is 5.169946584701538 and perplexity is 175.90544119147236
At time: 134.03978157043457 and batch: 200, loss is 5.118687238693237 and perplexity is 167.11584234759457
At time: 134.674396276474 and batch: 250, loss is 5.090270957946777 and perplexity is 162.4338688388135
At time: 135.31969380378723 and batch: 300, loss is 5.06957633972168 and perplexity is 159.10690578639964
At time: 135.96813941001892 and batch: 350, loss is 5.111939754486084 and perplexity is 165.99202656900266
At time: 136.61564230918884 and batch: 400, loss is 5.119592380523682 and perplexity is 167.26717436517112
At time: 137.25966787338257 and batch: 450, loss is 5.07124680519104 and perplexity is 159.3729104924345
At time: 137.9255633354187 and batch: 500, loss is 5.080596151351929 and perplexity is 160.86993017434543
At time: 138.570951461792 and batch: 550, loss is 5.125626258850097 and perplexity is 168.27949518225955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.341693634682513 and perplexity of 208.8661538646509
Finished 17 epochs...
Completing Train Step...
At time: 140.26638746261597 and batch: 50, loss is 5.1636694717407225 and perplexity is 174.80472115292685
At time: 140.92221021652222 and batch: 100, loss is 5.167129535675048 and perplexity is 175.41060425680465
At time: 141.56102871894836 and batch: 150, loss is 5.1469320297241214 and perplexity is 171.90328630135372
At time: 142.20862913131714 and batch: 200, loss is 5.093232078552246 and perplexity is 162.91556794746538
At time: 142.84223580360413 and batch: 250, loss is 5.066625537872315 and perplexity is 158.63810484406534
At time: 143.4808487892151 and batch: 300, loss is 5.051489200592041 and perplexity is 156.2549863257379
At time: 144.1181080341339 and batch: 350, loss is 5.094245100021363 and perplexity is 163.08068853667967
At time: 144.7519519329071 and batch: 400, loss is 5.102386856079102 and perplexity is 164.4138715815229
At time: 145.41138815879822 and batch: 450, loss is 5.0508513927459715 and perplexity is 156.15535744489023
At time: 146.05059599876404 and batch: 500, loss is 5.060783653259278 and perplexity is 157.7140610469051
At time: 146.70219206809998 and batch: 550, loss is 5.104408655166626 and perplexity is 164.74661965859497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.321418924534575 and perplexity of 204.67409314276847
Finished 18 epochs...
Completing Train Step...
At time: 148.43558168411255 and batch: 50, loss is 5.143601160049439 and perplexity is 171.33165140721056
At time: 149.09299039840698 and batch: 100, loss is 5.146729898452759 and perplexity is 171.86854278303628
At time: 149.74249982833862 and batch: 150, loss is 5.125464925765991 and perplexity is 168.25234832221213
At time: 150.3812882900238 and batch: 200, loss is 5.076587858200074 and perplexity is 160.2264069114244
At time: 151.03214287757874 and batch: 250, loss is 5.051377820968628 and perplexity is 156.23758367337294
At time: 151.68314480781555 and batch: 300, loss is 5.037095041275024 and perplexity is 154.02193714696887
At time: 152.32963800430298 and batch: 350, loss is 5.0796322250366215 and perplexity is 160.7149381277658
At time: 152.97024703025818 and batch: 400, loss is 5.084863729476929 and perplexity is 161.55792215493602
At time: 153.6042914390564 and batch: 450, loss is 5.032638368606567 and perplexity is 153.33703910614884
At time: 154.2754156589508 and batch: 500, loss is 5.045927543640136 and perplexity is 155.38836186262444
At time: 154.91713690757751 and batch: 550, loss is 5.088126993179321 and perplexity is 162.08598940086537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.31666857130984 and perplexity of 203.70412457505844
Finished 19 epochs...
Completing Train Step...
At time: 156.6308033466339 and batch: 50, loss is 5.132912330627441 and perplexity is 169.51006924434844
At time: 157.28419375419617 and batch: 100, loss is 5.129460096359253 and perplexity is 168.92588971697543
At time: 157.92666363716125 and batch: 150, loss is 5.106387119293213 and perplexity is 165.07288758383245
At time: 158.56541538238525 and batch: 200, loss is 5.063154144287109 and perplexity is 158.0883642795324
At time: 159.21474814414978 and batch: 250, loss is 5.0380788421630855 and perplexity is 154.17353862612242
At time: 159.86489915847778 and batch: 300, loss is 5.022360849380493 and perplexity is 151.76918536283023
At time: 160.5111789703369 and batch: 350, loss is 5.064367837905884 and perplexity is 158.28035160178771
At time: 161.15058660507202 and batch: 400, loss is 5.072387113571167 and perplexity is 159.55474841378322
At time: 161.7927849292755 and batch: 450, loss is 5.019776067733765 and perplexity is 151.37740171368702
At time: 162.43055844306946 and batch: 500, loss is 5.0328594589233395 and perplexity is 153.37094418860238
At time: 163.06482028961182 and batch: 550, loss is 5.071631498336792 and perplexity is 159.43423195293602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.308723287379488 and perplexity of 202.09205014258222
Finished 20 epochs...
Completing Train Step...
At time: 164.7959382534027 and batch: 50, loss is 5.117900371551514 and perplexity is 166.98439610453678
At time: 165.45266675949097 and batch: 100, loss is 5.118086500167847 and perplexity is 167.01547957180455
At time: 166.0965120792389 and batch: 150, loss is 5.096831998825073 and perplexity is 163.5031079172879
At time: 166.7378249168396 and batch: 200, loss is 5.050204133987426 and perplexity is 156.05431722520137
At time: 167.371431350708 and batch: 250, loss is 5.025335865020752 and perplexity is 152.22137336266428
At time: 168.01440000534058 and batch: 300, loss is 5.0107721424102785 and perplexity is 150.02052865172706
At time: 168.65254664421082 and batch: 350, loss is 5.053580408096313 and perplexity is 156.58208982716468
At time: 169.29856824874878 and batch: 400, loss is 5.060251893997193 and perplexity is 157.63021742846843
At time: 169.93971991539001 and batch: 450, loss is 5.01105694770813 and perplexity is 150.06326137803808
At time: 170.6062831878662 and batch: 500, loss is 5.0249388217926025 and perplexity is 152.16094689394492
At time: 171.24153327941895 and batch: 550, loss is 5.063973550796509 and perplexity is 158.21795600118256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3022308349609375 and perplexity of 200.7842272058945
Finished 21 epochs...
Completing Train Step...
At time: 172.978698015213 and batch: 50, loss is 5.106360187530518 and perplexity is 165.06844193986137
At time: 173.62748742103577 and batch: 100, loss is 5.1075966167449955 and perplexity is 165.27266361066702
At time: 174.27574753761292 and batch: 150, loss is 5.0848689460754395 and perplexity is 161.55876493995032
At time: 174.9125280380249 and batch: 200, loss is 5.04070481300354 and perplexity is 154.57892587752175
At time: 175.55065631866455 and batch: 250, loss is 5.014673547744751 and perplexity is 150.60696275722222
At time: 176.2002866268158 and batch: 300, loss is 4.998890285491943 and perplexity is 148.2485542158415
At time: 176.84137272834778 and batch: 350, loss is 5.041255207061767 and perplexity is 154.66402861772482
At time: 177.4744997024536 and batch: 400, loss is 5.049149684906006 and perplexity is 155.8898526187666
At time: 178.11169505119324 and batch: 450, loss is 4.998215208053589 and perplexity is 148.14850873464476
At time: 178.74586653709412 and batch: 500, loss is 5.0083450508117675 and perplexity is 149.65685659825087
At time: 179.38233423233032 and batch: 550, loss is 5.051064805984497 and perplexity is 156.18868662175296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.29537282091506 and perplexity of 199.41195705792126
Finished 22 epochs...
Completing Train Step...
At time: 181.16690921783447 and batch: 50, loss is 5.098875732421875 and perplexity is 163.83760640859325
At time: 181.8303325176239 and batch: 100, loss is 5.099885282516479 and perplexity is 164.00309219868518
At time: 182.47220993041992 and batch: 150, loss is 5.075228300094604 and perplexity is 160.0087178152167
At time: 183.10869359970093 and batch: 200, loss is 5.032074823379516 and perplexity is 153.25065109369282
At time: 183.7432804107666 and batch: 250, loss is 5.008464212417603 and perplexity is 149.67469101217497
At time: 184.3793706893921 and batch: 300, loss is 4.9919784641265865 and perplexity is 147.22741970633078
At time: 185.01006507873535 and batch: 350, loss is 5.034997844696045 and perplexity is 153.69926134285973
At time: 185.64749956130981 and batch: 400, loss is 5.046226119995117 and perplexity is 155.43476408027357
At time: 186.2838535308838 and batch: 450, loss is 4.992143640518188 and perplexity is 147.25174020879385
At time: 186.94831824302673 and batch: 500, loss is 5.0056524753570555 and perplexity is 149.25443623626734
At time: 187.5923731327057 and batch: 550, loss is 5.047793312072754 and perplexity is 155.67855119184756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.288931988655253 and perplexity of 198.1317054606663
Finished 23 epochs...
Completing Train Step...
At time: 189.32021355628967 and batch: 50, loss is 5.089897994995117 and perplexity is 162.3732983196237
At time: 189.9912257194519 and batch: 100, loss is 5.092113151550293 and perplexity is 162.73337926637606
At time: 190.6326928138733 and batch: 150, loss is 5.066949443817139 and perplexity is 158.68949699196173
At time: 191.27284026145935 and batch: 200, loss is 5.0253373146057125 and perplexity is 152.2215940206377
At time: 191.9103376865387 and batch: 250, loss is 5.0015288257598876 and perplexity is 148.64023049538827
At time: 192.5487802028656 and batch: 300, loss is 4.9840436363220215 and perplexity is 146.06381807729855
At time: 193.19287061691284 and batch: 350, loss is 5.0279334545135494 and perplexity is 152.61729600231166
At time: 193.8461618423462 and batch: 400, loss is 5.038981370925903 and perplexity is 154.3127474897501
At time: 194.4834966659546 and batch: 450, loss is 4.983031978607178 and perplexity is 145.91612620828678
At time: 195.1200180053711 and batch: 500, loss is 4.9997353267669675 and perplexity is 148.37388330978717
At time: 195.7672712802887 and batch: 550, loss is 5.039757862091064 and perplexity is 154.43261650744142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.290690320603391 and perplexity of 198.4803932329044
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 197.5437090396881 and batch: 50, loss is 5.052953767776489 and perplexity is 156.483999913074
At time: 198.21603679656982 and batch: 100, loss is 5.024643669128418 and perplexity is 152.11604281219
At time: 198.85346865653992 and batch: 150, loss is 4.988394050598145 and perplexity is 146.70064041279676
At time: 199.49167394638062 and batch: 200, loss is 4.946285533905029 and perplexity is 140.65154701352057
At time: 200.12767338752747 and batch: 250, loss is 4.914852075576782 and perplexity is 136.2991463386397
At time: 200.77109217643738 and batch: 300, loss is 4.897803955078125 and perplexity is 133.99519687313085
At time: 201.4081621170044 and batch: 350, loss is 4.939448280334473 and perplexity is 139.69315683340724
At time: 202.0541594028473 and batch: 400, loss is 4.9483036708831785 and perplexity is 140.93568772257927
At time: 202.69835448265076 and batch: 450, loss is 4.886676092147827 and perplexity is 132.51238226913756
At time: 203.3592278957367 and batch: 500, loss is 4.897736978530884 and perplexity is 133.98622263803242
At time: 203.99989533424377 and batch: 550, loss is 4.946575899124145 and perplexity is 140.6923932606659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.214976209275266 and perplexity of 184.00744325714058
Finished 25 epochs...
Completing Train Step...
At time: 205.73601388931274 and batch: 50, loss is 5.0029027557373045 and perplexity is 148.8445921210504
At time: 206.39184617996216 and batch: 100, loss is 4.993596334457397 and perplexity is 147.46580736872016
At time: 207.02810263633728 and batch: 150, loss is 4.9685272789001464 and perplexity is 143.81493204878905
At time: 207.6666419506073 and batch: 200, loss is 4.9335713386535645 and perplexity is 138.87459597068508
At time: 208.31243252754211 and batch: 250, loss is 4.905634937286377 and perplexity is 135.04863020124583
At time: 208.94148087501526 and batch: 300, loss is 4.89166784286499 and perplexity is 133.17550474245368
At time: 209.56780791282654 and batch: 350, loss is 4.935403566360474 and perplexity is 139.12927910070326
At time: 210.19521260261536 and batch: 400, loss is 4.948696422576904 and perplexity is 140.99105132400115
At time: 210.8281910419464 and batch: 450, loss is 4.886207208633423 and perplexity is 132.4502639619004
At time: 211.4798002243042 and batch: 500, loss is 4.894471244812012 and perplexity is 133.54937301841616
At time: 212.12127351760864 and batch: 550, loss is 4.941832218170166 and perplexity is 140.02657390024504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.211501101230053 and perplexity of 183.3691072963138
Finished 26 epochs...
Completing Train Step...
At time: 213.81090092658997 and batch: 50, loss is 4.993881607055664 and perplexity is 147.50788132373174
At time: 214.4595823287964 and batch: 100, loss is 4.985276002883911 and perplexity is 146.24393320373295
At time: 215.10348558425903 and batch: 150, loss is 4.96204797744751 and perplexity is 142.8861240136622
At time: 215.74028134346008 and batch: 200, loss is 4.9284345817565915 and perplexity is 138.16305999026022
At time: 216.3869504928589 and batch: 250, loss is 4.901882972717285 and perplexity is 134.54288189378644
At time: 217.03177094459534 and batch: 300, loss is 4.889384117126465 and perplexity is 132.87171543229752
At time: 217.6807565689087 and batch: 350, loss is 4.934056081771851 and perplexity is 138.94193079411428
At time: 218.3172469139099 and batch: 400, loss is 4.946745586395264 and perplexity is 140.71626899459227
At time: 218.95926666259766 and batch: 450, loss is 4.883360385894775 and perplexity is 132.07373774470537
At time: 219.6291515827179 and batch: 500, loss is 4.891054620742798 and perplexity is 133.09386361144368
At time: 220.27697920799255 and batch: 550, loss is 4.936882200241089 and perplexity is 139.3351525347751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.209384025411403 and perplexity of 182.98131163462028
Finished 27 epochs...
Completing Train Step...
At time: 222.07471561431885 and batch: 50, loss is 4.985600519180298 and perplexity is 146.2913994446731
At time: 222.7298619747162 and batch: 100, loss is 4.978833971023559 and perplexity is 145.30485316604438
At time: 223.38495922088623 and batch: 150, loss is 4.95528998374939 and perplexity is 141.92375598919935
At time: 224.0358567237854 and batch: 200, loss is 4.9222690200805665 and perplexity is 137.31382780947087
At time: 224.6780605316162 and batch: 250, loss is 4.897482624053955 and perplexity is 133.95214697628995
At time: 225.30638933181763 and batch: 300, loss is 4.886221160888672 and perplexity is 132.45211195468278
At time: 225.93659162521362 and batch: 350, loss is 4.931816501617432 and perplexity is 138.6311073901715
At time: 226.57593774795532 and batch: 400, loss is 4.944319067001342 and perplexity is 140.37523217240346
At time: 227.2104377746582 and batch: 450, loss is 4.8804543018341064 and perplexity is 131.69047752355212
At time: 227.850346326828 and batch: 500, loss is 4.887919940948486 and perplexity is 132.67731018832868
At time: 228.48604822158813 and batch: 550, loss is 4.932675580978394 and perplexity is 138.7502536840511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.207709454475565 and perplexity of 182.6751528621904
Finished 28 epochs...
Completing Train Step...
At time: 230.18203854560852 and batch: 50, loss is 4.9810120677948 and perplexity is 145.62168611872917
At time: 230.8302230834961 and batch: 100, loss is 4.973944540023804 and perplexity is 144.5961291530665
At time: 231.46360731124878 and batch: 150, loss is 4.95153881072998 and perplexity is 141.39237270334607
At time: 232.10122323036194 and batch: 200, loss is 4.920366306304931 and perplexity is 137.05280730012015
At time: 232.73928666114807 and batch: 250, loss is 4.895044736862182 and perplexity is 133.62598448808214
At time: 233.39437127113342 and batch: 300, loss is 4.883961725234985 and perplexity is 132.15318276333335
At time: 234.03484439849854 and batch: 350, loss is 4.929914312362671 and perplexity is 138.36765543449584
At time: 234.67643237113953 and batch: 400, loss is 4.941771945953369 and perplexity is 140.01813444256055
At time: 235.31848430633545 and batch: 450, loss is 4.876573848724365 and perplexity is 131.18044901138265
At time: 235.9703130722046 and batch: 500, loss is 4.884142942428589 and perplexity is 132.17713336230418
At time: 236.6066312789917 and batch: 550, loss is 4.9282174396514895 and perplexity is 138.13306222957291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2054878397190825 and perplexity of 182.26976951649368
Finished 29 epochs...
Completing Train Step...
At time: 238.31271886825562 and batch: 50, loss is 4.975531454086304 and perplexity is 144.82577294809383
At time: 238.96220064163208 and batch: 100, loss is 4.968479700088501 and perplexity is 143.8080896680028
At time: 239.59216141700745 and batch: 150, loss is 4.946079845428467 and perplexity is 140.62261958621045
At time: 240.22414541244507 and batch: 200, loss is 4.913257904052735 and perplexity is 136.08203522295398
At time: 240.85705637931824 and batch: 250, loss is 4.888171129226684 and perplexity is 132.7106413594553
At time: 241.48303604125977 and batch: 300, loss is 4.877756252288818 and perplexity is 131.33564897829547
At time: 242.11759495735168 and batch: 350, loss is 4.926939687728882 and perplexity is 137.95667515719202
At time: 242.75566697120667 and batch: 400, loss is 4.938614664077758 and perplexity is 139.5767548709441
At time: 243.388418674469 and batch: 450, loss is 4.873422241210937 and perplexity is 130.76767052222203
At time: 244.01640248298645 and batch: 500, loss is 4.883160400390625 and perplexity is 132.04732755259326
At time: 244.64647507667542 and batch: 550, loss is 4.926130132675171 and perplexity is 137.84503682840912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.204888201774435 and perplexity of 182.16050640895597
Finished 30 epochs...
Completing Train Step...
At time: 246.35936522483826 and batch: 50, loss is 4.972116727828979 and perplexity is 144.33207597819717
At time: 247.0110743045807 and batch: 100, loss is 4.965619916915894 and perplexity is 143.39741721006328
At time: 247.63691926002502 and batch: 150, loss is 4.9439096355438235 and perplexity is 140.31776990072908
At time: 248.273535490036 and batch: 200, loss is 4.91239070892334 and perplexity is 135.96407669873167
At time: 248.92168307304382 and batch: 250, loss is 4.8891700172424315 and perplexity is 132.84327065855223
At time: 249.5682451725006 and batch: 300, loss is 4.879471521377564 and perplexity is 131.56111827221315
At time: 250.20591402053833 and batch: 350, loss is 4.925650644302368 and perplexity is 137.7789575793831
At time: 250.84542989730835 and batch: 400, loss is 4.934558086395263 and perplexity is 139.01169779597242
At time: 251.48067808151245 and batch: 450, loss is 4.868193416595459 and perplexity is 130.08569382974915
At time: 252.12708926200867 and batch: 500, loss is 4.876277589797974 and perplexity is 131.14159138863803
At time: 252.76928615570068 and batch: 550, loss is 4.918418531417847 and perplexity is 136.78611909246737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.201030000727227 and perplexity of 181.4590486051273
Finished 31 epochs...
Completing Train Step...
At time: 254.4867537021637 and batch: 50, loss is 4.967238798141479 and perplexity is 143.62974860430754
At time: 255.15086007118225 and batch: 100, loss is 4.9593099594116214 and perplexity is 142.49543433080865
At time: 255.8015992641449 and batch: 150, loss is 4.938945512771607 and perplexity is 139.62294129792298
At time: 256.4364218711853 and batch: 200, loss is 4.9068928050994876 and perplexity is 135.21861041028527
At time: 257.0772304534912 and batch: 250, loss is 4.882972736358642 and perplexity is 132.0225493437542
At time: 257.7290472984314 and batch: 300, loss is 4.871619815826416 and perplexity is 130.53218384073716
At time: 258.37221240997314 and batch: 350, loss is 4.919206314086914 and perplexity is 136.893919282443
At time: 259.0104537010193 and batch: 400, loss is 4.9280389785766605 and perplexity is 138.10841305434258
At time: 259.657753944397 and batch: 450, loss is 4.863651123046875 and perplexity is 129.4961463843315
At time: 260.30296874046326 and batch: 500, loss is 4.871918392181397 and perplexity is 130.5711634833062
At time: 260.94218921661377 and batch: 550, loss is 4.912864360809326 and perplexity is 136.02849159400049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.195707280585107 and perplexity of 180.49575880742515
Finished 32 epochs...
Completing Train Step...
At time: 262.66709446907043 and batch: 50, loss is 4.959091539382935 and perplexity is 142.4643138727438
At time: 263.32737708091736 and batch: 100, loss is 4.952404356002807 and perplexity is 141.51480718180161
At time: 263.96769547462463 and batch: 150, loss is 4.9318483543396 and perplexity is 138.63552323864712
At time: 264.60667610168457 and batch: 200, loss is 4.89985387802124 and perplexity is 134.27015843018387
At time: 265.24261474609375 and batch: 250, loss is 4.876261739730835 and perplexity is 131.13951280208275
At time: 265.8825192451477 and batch: 300, loss is 4.8666281986236575 and perplexity is 129.88224062973185
At time: 266.51464676856995 and batch: 350, loss is 4.9144856643676755 and perplexity is 136.2492139520813
At time: 267.15298986434937 and batch: 400, loss is 4.925022859573364 and perplexity is 137.69248919844108
At time: 267.7984082698822 and batch: 450, loss is 4.859811735153198 and perplexity is 128.9999136721358
At time: 268.46799659729004 and batch: 500, loss is 4.865792255401612 and perplexity is 129.77371181929504
At time: 269.11005663871765 and batch: 550, loss is 4.907120018005371 and perplexity is 135.24933731432856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.193765843168218 and perplexity of 180.1456775280939
Finished 33 epochs...
Completing Train Step...
At time: 270.83773732185364 and batch: 50, loss is 4.954104223251343 and perplexity is 141.75556814057285
At time: 271.4946584701538 and batch: 100, loss is 4.9460256481170655 and perplexity is 140.61499842483175
At time: 272.1401300430298 and batch: 150, loss is 4.9249913692474365 and perplexity is 137.6881532853485
At time: 272.7773063182831 and batch: 200, loss is 4.893893375396728 and perplexity is 133.47222121429945
At time: 273.4237713813782 and batch: 250, loss is 4.8711456775665285 and perplexity is 130.47030820819313
At time: 274.0586633682251 and batch: 300, loss is 4.8630254936218265 and perplexity is 129.41515512262308
At time: 274.69618582725525 and batch: 350, loss is 4.91011528968811 and perplexity is 135.65505313572248
At time: 275.34532713890076 and batch: 400, loss is 4.921391525268555 and perplexity is 137.19338848811856
At time: 275.9922676086426 and batch: 450, loss is 4.856560125350952 and perplexity is 128.58113750566986
At time: 276.6347620487213 and batch: 500, loss is 4.8628185367584225 and perplexity is 129.38837453935028
At time: 277.28031396865845 and batch: 550, loss is 4.902559909820557 and perplexity is 134.6339897962021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.192168540142952 and perplexity of 179.85815997984503
Finished 34 epochs...
Completing Train Step...
At time: 279.0647819042206 and batch: 50, loss is 4.950842266082764 and perplexity is 141.29392089500288
At time: 279.71960639953613 and batch: 100, loss is 4.9427111625671385 and perplexity is 140.1497035769447
At time: 280.3560879230499 and batch: 150, loss is 4.921724710464478 and perplexity is 137.23910691006907
At time: 280.99830389022827 and batch: 200, loss is 4.890239181518555 and perplexity is 132.98537789231557
At time: 281.6309549808502 and batch: 250, loss is 4.867326860427856 and perplexity is 129.97301609728706
At time: 282.2731931209564 and batch: 300, loss is 4.8600857639312744 and perplexity is 129.03526820470972
At time: 282.90460896492004 and batch: 350, loss is 4.907212524414063 and perplexity is 135.261849323513
At time: 283.5584089756012 and batch: 400, loss is 4.919340734481811 and perplexity is 136.91232185394477
At time: 284.19848895072937 and batch: 450, loss is 4.85391752243042 and perplexity is 128.2417971845116
At time: 284.8653829097748 and batch: 500, loss is 4.8585360145568846 and perplexity is 128.83545075201357
At time: 285.5036597251892 and batch: 550, loss is 4.89811954498291 and perplexity is 134.0374910780147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.192405863011137 and perplexity of 179.9008494996372
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 287.21903443336487 and batch: 50, loss is 4.932982940673828 and perplexity is 138.79290647430332
At time: 287.86702609062195 and batch: 100, loss is 4.913453607559204 and perplexity is 136.10866956054628
At time: 288.50489258766174 and batch: 150, loss is 4.88574402809143 and perplexity is 132.38892978234725
At time: 289.13657808303833 and batch: 200, loss is 4.852228784561158 and perplexity is 128.0254131645526
At time: 289.77923917770386 and batch: 250, loss is 4.825087022781372 and perplexity is 124.59731071595351
At time: 290.4184765815735 and batch: 300, loss is 4.810943384170532 and perplexity is 122.84745521358823
At time: 291.05747747421265 and batch: 350, loss is 4.860204601287842 and perplexity is 129.05060332606175
At time: 291.70822167396545 and batch: 400, loss is 4.8711998081207275 and perplexity is 130.47737082943303
At time: 292.3564932346344 and batch: 450, loss is 4.800494518280029 and perplexity is 121.57052150144608
At time: 292.9839859008789 and batch: 500, loss is 4.8078466796875 and perplexity is 122.46762136842194
At time: 293.61335372924805 and batch: 550, loss is 4.853516845703125 and perplexity is 128.19042397362563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.168436253324468 and perplexity of 175.6399662123035
Finished 36 epochs...
Completing Train Step...
At time: 295.3855323791504 and batch: 50, loss is 4.908240375518798 and perplexity is 135.40094983983204
At time: 296.04686093330383 and batch: 100, loss is 4.89877875328064 and perplexity is 134.12587883411058
At time: 296.7024326324463 and batch: 150, loss is 4.877478342056275 and perplexity is 131.29915452887204
At time: 297.35552430152893 and batch: 200, loss is 4.843571653366089 and perplexity is 126.92186404344513
At time: 297.99237537384033 and batch: 250, loss is 4.818311386108398 and perplexity is 123.75593824409648
At time: 298.63664960861206 and batch: 300, loss is 4.80468074798584 and perplexity is 122.08051035036526
At time: 299.2814025878906 and batch: 350, loss is 4.856775245666504 and perplexity is 128.60880089591606
At time: 299.92056465148926 and batch: 400, loss is 4.870300455093384 and perplexity is 130.36007836255118
At time: 300.5571529865265 and batch: 450, loss is 4.8014632415771485 and perplexity is 121.68834675868915
At time: 301.2330629825592 and batch: 500, loss is 4.808183631896973 and perplexity is 122.50889405710227
At time: 301.8759787082672 and batch: 550, loss is 4.853216028213501 and perplexity is 128.15186785156064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.165807521089595 and perplexity of 175.1788620965436
Finished 37 epochs...
Completing Train Step...
At time: 303.60342288017273 and batch: 50, loss is 4.903648738861084 and perplexity is 134.78066303068212
At time: 304.25632667541504 and batch: 100, loss is 4.895352506637574 and perplexity is 133.6671168566397
At time: 304.8907690048218 and batch: 150, loss is 4.874080715179443 and perplexity is 130.8538059850508
At time: 305.53718304634094 and batch: 200, loss is 4.840486078262329 and perplexity is 126.53084067585095
At time: 306.1862976551056 and batch: 250, loss is 4.8151882553100585 and perplexity is 123.37003518842117
At time: 306.82328820228577 and batch: 300, loss is 4.801460933685303 and perplexity is 121.68806591547003
At time: 307.4669256210327 and batch: 350, loss is 4.8548155117034915 and perplexity is 128.35700866433717
At time: 308.116819858551 and batch: 400, loss is 4.86914397239685 and perplexity is 130.2094063291989
At time: 308.7628927230835 and batch: 450, loss is 4.80055453300476 and perplexity is 121.57781774176816
At time: 309.4088683128357 and batch: 500, loss is 4.806900730133057 and perplexity is 122.35182795255442
At time: 310.0575952529907 and batch: 550, loss is 4.85199089050293 and perplexity is 127.99496030187974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.165472477040392 and perplexity of 175.12017929246323
Finished 38 epochs...
Completing Train Step...
At time: 311.78095626831055 and batch: 50, loss is 4.900196762084961 and perplexity is 134.31620542168147
At time: 312.43319940567017 and batch: 100, loss is 4.892737550735474 and perplexity is 133.31803984990378
At time: 313.0813250541687 and batch: 150, loss is 4.870814599990845 and perplexity is 130.42711956464305
At time: 313.7316155433655 and batch: 200, loss is 4.837475881576538 and perplexity is 126.15053064976293
At time: 314.3704426288605 and batch: 250, loss is 4.811627492904663 and perplexity is 122.93152498381897
At time: 315.008665561676 and batch: 300, loss is 4.798682613372803 and perplexity is 121.35044671446555
At time: 315.6476192474365 and batch: 350, loss is 4.853204383850097 and perplexity is 128.15037561332866
At time: 316.28396010398865 and batch: 400, loss is 4.867759428024292 and perplexity is 130.0292503741479
At time: 316.9254820346832 and batch: 450, loss is 4.79924162864685 and perplexity is 121.4183024321157
At time: 317.57526755332947 and batch: 500, loss is 4.805265693664551 and perplexity is 122.15194170704451
At time: 318.2184042930603 and batch: 550, loss is 4.849987554550171 and perplexity is 127.73880006930862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.164367026471077 and perplexity of 174.92669955141378
Finished 39 epochs...
Completing Train Step...
At time: 319.9339368343353 and batch: 50, loss is 4.897138214111328 and perplexity is 133.9060204687049
At time: 320.58218121528625 and batch: 100, loss is 4.889902257919312 and perplexity is 132.94057952738635
At time: 321.2225215435028 and batch: 150, loss is 4.868581857681274 and perplexity is 130.13623427326627
At time: 321.85951042175293 and batch: 200, loss is 4.834957675933838 and perplexity is 125.83325731905889
At time: 322.4864752292633 and batch: 250, loss is 4.808999004364014 and perplexity is 122.60882517128722
At time: 323.1255407333374 and batch: 300, loss is 4.795910882949829 and perplexity is 121.01456169586402
At time: 323.7635750770569 and batch: 350, loss is 4.851094141006469 and perplexity is 127.88023233449135
At time: 324.39827275276184 and batch: 400, loss is 4.866022825241089 and perplexity is 129.803637173007
At time: 325.03976702690125 and batch: 450, loss is 4.797685613632202 and perplexity is 121.22952064221762
At time: 325.67078042030334 and batch: 500, loss is 4.80365834236145 and perplexity is 121.95575833440311
At time: 326.3017029762268 and batch: 550, loss is 4.848187704086303 and perplexity is 127.50909610914938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.163454745678192 and perplexity of 174.76719005303875
Finished 40 epochs...
Completing Train Step...
At time: 328.00168919563293 and batch: 50, loss is 4.894622144699096 and perplexity is 133.56952712431274
At time: 328.6689648628235 and batch: 100, loss is 4.887238311767578 and perplexity is 132.5869042772138
At time: 329.31828141212463 and batch: 150, loss is 4.865864324569702 and perplexity is 129.783064839774
At time: 329.95616364479065 and batch: 200, loss is 4.832267723083496 and perplexity is 125.49522663699219
At time: 330.59558057785034 and batch: 250, loss is 4.806147518157959 and perplexity is 122.25970578867928
At time: 331.2280583381653 and batch: 300, loss is 4.793498439788818 and perplexity is 120.72297280634704
At time: 331.8659369945526 and batch: 350, loss is 4.848989858627319 and perplexity is 127.61141914357084
At time: 332.52323603630066 and batch: 400, loss is 4.864136581420898 and perplexity is 129.55902663463465
At time: 333.16271686553955 and batch: 450, loss is 4.796010103225708 and perplexity is 121.02656938975436
At time: 333.82070231437683 and batch: 500, loss is 4.801528377532959 and perplexity is 121.696273303615
At time: 334.45220041275024 and batch: 550, loss is 4.845712633132934 and perplexity is 127.19389228590387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1623548142453455 and perplexity of 174.57506380948666
Finished 41 epochs...
Completing Train Step...
At time: 336.1972279548645 and batch: 50, loss is 4.891950092315674 and perplexity is 133.21309876070768
At time: 336.84727907180786 and batch: 100, loss is 4.884846506118774 and perplexity is 132.27016111564794
At time: 337.4885821342468 and batch: 150, loss is 4.864329090118408 and perplexity is 129.58397027495957
At time: 338.132865190506 and batch: 200, loss is 4.829803075790405 and perplexity is 125.18630601293324
At time: 338.7775628566742 and batch: 250, loss is 4.803447370529175 and perplexity is 121.93003181849132
At time: 339.421293258667 and batch: 300, loss is 4.791191301345825 and perplexity is 120.44476924530228
At time: 340.0596008300781 and batch: 350, loss is 4.846665534973145 and perplexity is 127.31515334569448
At time: 340.69841718673706 and batch: 400, loss is 4.86202181816101 and perplexity is 129.2853294694784
At time: 341.33271646499634 and batch: 450, loss is 4.794249572753906 and perplexity is 120.81368587538854
At time: 341.97312021255493 and batch: 500, loss is 4.799427394866943 and perplexity is 121.4408599463558
At time: 342.6112837791443 and batch: 550, loss is 4.843852777481079 and perplexity is 126.95754985598339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.160821468272108 and perplexity of 174.30758495958227
Finished 42 epochs...
Completing Train Step...
At time: 344.3025369644165 and batch: 50, loss is 4.888964538574219 and perplexity is 132.81597700444414
At time: 344.95507764816284 and batch: 100, loss is 4.88228624343872 and perplexity is 131.93194790054253
At time: 345.582083940506 and batch: 150, loss is 4.862009105682373 and perplexity is 129.283685942936
At time: 346.2163348197937 and batch: 200, loss is 4.827963590621948 and perplexity is 124.95623932673023
At time: 346.861976146698 and batch: 250, loss is 4.800526065826416 and perplexity is 121.57435681360944
At time: 347.5053651332855 and batch: 300, loss is 4.788503999710083 and perplexity is 120.12153233195947
At time: 348.142706155777 and batch: 350, loss is 4.844135417938232 and perplexity is 126.99343826743335
At time: 348.77095460891724 and batch: 400, loss is 4.860210123062134 and perplexity is 129.05131591633295
At time: 349.4030041694641 and batch: 450, loss is 4.791962213516236 and perplexity is 120.53765738347354
At time: 350.07412242889404 and batch: 500, loss is 4.79656792640686 and perplexity is 121.0940996489162
At time: 350.7155375480652 and batch: 550, loss is 4.841485576629639 and perplexity is 126.65737126754702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.159575766705452 and perplexity of 174.0905849146274
Finished 43 epochs...
Completing Train Step...
At time: 352.45189118385315 and batch: 50, loss is 4.886232938766479 and perplexity is 132.4536719686595
At time: 353.11459493637085 and batch: 100, loss is 4.879810810089111 and perplexity is 131.60576304782737
At time: 353.768630027771 and batch: 150, loss is 4.859636764526368 and perplexity is 128.97734445091243
At time: 354.4115765094757 and batch: 200, loss is 4.825401420593262 and perplexity is 124.63648999642723
At time: 355.0553858280182 and batch: 250, loss is 4.797206935882568 and perplexity is 121.17150465467631
At time: 355.69608092308044 and batch: 300, loss is 4.7851176071167 and perplexity is 119.71544164388939
At time: 356.3382830619812 and batch: 350, loss is 4.841412830352783 and perplexity is 126.64815775047957
At time: 356.98509073257446 and batch: 400, loss is 4.8571913623809815 and perplexity is 128.66232830364993
At time: 357.61719012260437 and batch: 450, loss is 4.7895055103302 and perplexity is 120.24189558477208
At time: 358.2574973106384 and batch: 500, loss is 4.793951072692871 and perplexity is 120.77762836463316
At time: 358.8974928855896 and batch: 550, loss is 4.83914213180542 and perplexity is 126.36090421916244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.159492330348238 and perplexity of 174.0760600363567
Finished 44 epochs...
Completing Train Step...
At time: 360.6638562679291 and batch: 50, loss is 4.8829145336151125 and perplexity is 132.0148654927874
At time: 361.3373668193817 and batch: 100, loss is 4.876901235580444 and perplexity is 131.2234027970323
At time: 361.9821457862854 and batch: 150, loss is 4.85662184715271 and perplexity is 128.58907401007403
At time: 362.6297354698181 and batch: 200, loss is 4.822179307937622 and perplexity is 124.23554347896516
At time: 363.2767176628113 and batch: 250, loss is 4.793426713943481 and perplexity is 120.71431415959901
At time: 363.9283037185669 and batch: 300, loss is 4.781978054046631 and perplexity is 119.34017804976743
At time: 364.56666445732117 and batch: 350, loss is 4.838613052368164 and perplexity is 126.29406694573962
At time: 365.2306730747223 and batch: 400, loss is 4.854647836685181 and perplexity is 128.3354882048312
At time: 365.8725047111511 and batch: 450, loss is 4.786852817535401 and perplexity is 119.92335345871516
At time: 366.5324897766113 and batch: 500, loss is 4.7918096446990965 and perplexity is 120.51926849848671
At time: 367.1814637184143 and batch: 550, loss is 4.836929025650025 and perplexity is 126.08156334375302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1583872044340096 and perplexity of 173.88379033204845
Finished 45 epochs...
Completing Train Step...
At time: 368.89994978904724 and batch: 50, loss is 4.879710083007812 and perplexity is 131.59250745104345
At time: 369.552369594574 and batch: 100, loss is 4.874105319976807 and perplexity is 130.8570256560409
At time: 370.1876013278961 and batch: 150, loss is 4.853875169754028 and perplexity is 128.23636591619075
At time: 370.83184337615967 and batch: 200, loss is 4.81942928314209 and perplexity is 123.89436199769744
At time: 371.46862864494324 and batch: 250, loss is 4.790427236557007 and perplexity is 120.35277678669925
At time: 372.1172046661377 and batch: 300, loss is 4.779455547332764 and perplexity is 119.03952101352341
At time: 372.7606463432312 and batch: 350, loss is 4.836315593719482 and perplexity is 126.00424460426166
At time: 373.4055380821228 and batch: 400, loss is 4.852436132431031 and perplexity is 128.05196171355928
At time: 374.0448365211487 and batch: 450, loss is 4.784947271347046 and perplexity is 119.69505155862728
At time: 374.6775691509247 and batch: 500, loss is 4.789562177658081 and perplexity is 120.24870956475739
At time: 375.3116674423218 and batch: 550, loss is 4.834043531417847 and perplexity is 125.71828009785631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.156302269468916 and perplexity of 173.52163160760645
Finished 46 epochs...
Completing Train Step...
At time: 377.0661578178406 and batch: 50, loss is 4.877054958343506 and perplexity is 131.2435763716177
At time: 377.733758687973 and batch: 100, loss is 4.871898193359375 and perplexity is 130.56852612624962
At time: 378.3813695907593 and batch: 150, loss is 4.851403827667236 and perplexity is 127.91984126948398
At time: 379.0317361354828 and batch: 200, loss is 4.8173323917388915 and perplexity is 123.63484116371436
At time: 379.6852476596832 and batch: 250, loss is 4.787576932907104 and perplexity is 120.01022325046078
At time: 380.32665491104126 and batch: 300, loss is 4.776731996536255 and perplexity is 118.71575193215669
At time: 380.9582395553589 and batch: 350, loss is 4.833234872817993 and perplexity is 125.61665802381619
At time: 381.59811878204346 and batch: 400, loss is 4.849855880737305 and perplexity is 127.72198132177122
At time: 382.24330973625183 and batch: 450, loss is 4.781582517623901 and perplexity is 119.29298399675247
At time: 382.91193890571594 and batch: 500, loss is 4.785911226272583 and perplexity is 119.81048782187824
At time: 383.562744140625 and batch: 550, loss is 4.830086441040039 and perplexity is 125.22178448823342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.153882939764794 and perplexity of 173.10233298527484
Finished 47 epochs...
Completing Train Step...
At time: 385.2861635684967 and batch: 50, loss is 4.871395740509033 and perplexity is 130.50293807696366
At time: 385.94915795326233 and batch: 100, loss is 4.866798486709595 and perplexity is 129.9043599111575
At time: 386.5981481075287 and batch: 150, loss is 4.845722045898437 and perplexity is 127.19508953782008
At time: 387.2597997188568 and batch: 200, loss is 4.812090883255005 and perplexity is 122.98850346686925
At time: 387.89525294303894 and batch: 250, loss is 4.782434110641479 and perplexity is 119.39461633752397
At time: 388.5452616214752 and batch: 300, loss is 4.770148429870606 and perplexity is 117.93674599752562
At time: 389.1885426044464 and batch: 350, loss is 4.8273352527618405 and perplexity is 124.8777492524296
At time: 389.83204770088196 and batch: 400, loss is 4.844047679901123 and perplexity is 126.98229660121511
At time: 390.48067927360535 and batch: 450, loss is 4.776709899902344 and perplexity is 118.71312874262871
At time: 391.1327486038208 and batch: 500, loss is 4.781124773025513 and perplexity is 119.2383907735321
At time: 391.78384494781494 and batch: 550, loss is 4.825148096084595 and perplexity is 124.6049205176671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1518307949634305 and perplexity of 172.74746617611146
Finished 48 epochs...
Completing Train Step...
At time: 393.51657724380493 and batch: 50, loss is 4.866344146728515 and perplexity is 129.845352572445
At time: 394.172082901001 and batch: 100, loss is 4.862110071182251 and perplexity is 129.2967397938965
At time: 394.81263279914856 and batch: 150, loss is 4.841532011032104 and perplexity is 126.66325266344815
At time: 395.4638903141022 and batch: 200, loss is 4.807552194595337 and perplexity is 122.4315617894211
At time: 396.10050892829895 and batch: 250, loss is 4.778131437301636 and perplexity is 118.88200389765952
At time: 396.7335362434387 and batch: 300, loss is 4.765699005126953 and perplexity is 117.4131610114951
At time: 397.3707809448242 and batch: 350, loss is 4.8235195541381835 and perplexity is 124.40216132361337
At time: 398.0086340904236 and batch: 400, loss is 4.840324115753174 and perplexity is 126.51034908288925
At time: 398.6461431980133 and batch: 450, loss is 4.77312008857727 and perplexity is 118.28773500708652
At time: 399.30801153182983 and batch: 500, loss is 4.77792028427124 and perplexity is 118.85690425230337
At time: 399.9490442276001 and batch: 550, loss is 4.821969709396362 and perplexity is 124.20950661901858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.150021492166722 and perplexity of 172.4351962830168
Finished 49 epochs...
Completing Train Step...
At time: 401.722553730011 and batch: 50, loss is 4.862077350616455 and perplexity is 129.292509200629
At time: 402.39883732795715 and batch: 100, loss is 4.858190240859986 and perplexity is 128.79091054277558
At time: 403.0473253726959 and batch: 150, loss is 4.838229207992554 and perplexity is 126.2455989811425
At time: 403.69741129875183 and batch: 200, loss is 4.804435548782348 and perplexity is 122.05057997606744
At time: 404.34300446510315 and batch: 250, loss is 4.77507080078125 and perplexity is 118.51870554062292
At time: 404.9767279624939 and batch: 300, loss is 4.762812833786011 and perplexity is 117.07477506593928
At time: 405.61041045188904 and batch: 350, loss is 4.82074294090271 and perplexity is 124.05722373670561
At time: 406.2534017562866 and batch: 400, loss is 4.837473955154419 and perplexity is 126.15028763082437
At time: 406.8948450088501 and batch: 450, loss is 4.770055465698242 and perplexity is 117.92578261515148
At time: 407.5309736728668 and batch: 500, loss is 4.775350141525268 and perplexity is 118.55181726852081
At time: 408.1717176437378 and batch: 550, loss is 4.819065322875977 and perplexity is 123.8492775777013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.148280691593252 and perplexity of 172.13528211555618
Finished Training.
Improved accuracyfrom -10000000 to -172.13528211555618
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4194bc38d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 5.523306651105269, 'lr': 12.682961565888766, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.780798867653166}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.910088300704956 and batch: 50, loss is 6.73857741355896 and perplexity is 844.3587078102524
At time: 1.5650410652160645 and batch: 100, loss is 6.101301374435425 and perplexity is 446.4383756970483
At time: 2.2087271213531494 and batch: 150, loss is 6.023195886611939 and perplexity is 412.8960583597584
At time: 2.860039710998535 and batch: 200, loss is 5.932664957046509 and perplexity is 377.15828635821
At time: 3.5000357627868652 and batch: 250, loss is 5.880516366958618 and perplexity is 357.9940502891752
At time: 4.141089916229248 and batch: 300, loss is 5.874022426605225 and perplexity is 355.6767904966406
At time: 4.781678676605225 and batch: 350, loss is 5.940197687149048 and perplexity is 380.0100452156436
At time: 5.434030532836914 and batch: 400, loss is 5.944259443283081 and perplexity is 381.55669227301485
At time: 6.0894551277160645 and batch: 450, loss is 5.884051189422608 and perplexity is 359.2617349001589
At time: 6.732752799987793 and batch: 500, loss is 5.896815671920776 and perplexity is 363.8769175735367
At time: 7.382500171661377 and batch: 550, loss is 5.977195672988891 and perplexity is 394.3329775180941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.992805643284575 and perplexity of 400.5367983392829
Finished 1 epochs...
Completing Train Step...
At time: 9.099358081817627 and batch: 50, loss is 5.7921715927124025 and perplexity is 327.723935101465
At time: 9.774851083755493 and batch: 100, loss is 5.698218955993652 and perplexity is 298.3355787138768
At time: 10.427292585372925 and batch: 150, loss is 5.6566582298278805 and perplexity is 286.1906593587501
At time: 11.07601284980774 and batch: 200, loss is 5.535171947479248 and perplexity is 253.45136421372897
At time: 11.724640846252441 and batch: 250, loss is 5.487449607849121 and perplexity is 241.64014319073064
At time: 12.360465049743652 and batch: 300, loss is 5.457443218231202 and perplexity is 234.4970993338116
At time: 13.004935503005981 and batch: 350, loss is 5.475094318389893 and perplexity is 238.67297711999325
At time: 13.658387422561646 and batch: 400, loss is 5.476517391204834 and perplexity is 239.0128679327213
At time: 14.290058851242065 and batch: 450, loss is 5.403743753433227 and perplexity is 222.2368607548714
At time: 14.924874305725098 and batch: 500, loss is 5.421360950469971 and perplexity is 226.18674207998296
At time: 15.56816840171814 and batch: 550, loss is 5.456679430007934 and perplexity is 234.31806159311287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.510050672165891 and perplexity of 247.163651067841
Finished 2 epochs...
Completing Train Step...
At time: 17.268840551376343 and batch: 50, loss is 5.455930891036988 and perplexity is 234.14273102148317
At time: 17.934768676757812 and batch: 100, loss is 5.448247222900391 and perplexity is 232.35055005893201
At time: 18.585979461669922 and batch: 150, loss is 5.4250807952880855 and perplexity is 227.02968850337336
At time: 19.22502040863037 and batch: 200, loss is 5.377259931564331 and perplexity is 216.42843349353035
At time: 19.87676501274109 and batch: 250, loss is 5.341179723739624 and perplexity is 208.75884283909767
At time: 20.50661301612854 and batch: 300, loss is 5.331529130935669 and perplexity is 206.75388632286436
At time: 21.141992807388306 and batch: 350, loss is 5.351482248306274 and perplexity is 210.92070313336555
At time: 21.783353090286255 and batch: 400, loss is 5.344273271560669 and perplexity is 209.40564824838947
At time: 22.41959238052368 and batch: 450, loss is 5.286561193466187 and perplexity is 197.66253214323635
At time: 23.0501127243042 and batch: 500, loss is 5.307721900939941 and perplexity is 201.88977919662315
At time: 23.6951265335083 and batch: 550, loss is 5.359817657470703 and perplexity is 212.6861611809761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.480341485206117 and perplexity of 239.92862546267108
Finished 3 epochs...
Completing Train Step...
At time: 25.452888250350952 and batch: 50, loss is 5.363211374282837 and perplexity is 213.40918395521658
At time: 26.103354454040527 and batch: 100, loss is 5.358790845870971 and perplexity is 212.4678846471896
At time: 26.74106454849243 and batch: 150, loss is 5.331031560897827 and perplexity is 206.6510373732225
At time: 27.37608051300049 and batch: 200, loss is 5.269277362823487 and perplexity is 194.2755209934409
At time: 28.006474256515503 and batch: 250, loss is 5.25159517288208 and perplexity is 190.87049718651164
At time: 28.63516664505005 and batch: 300, loss is 5.246212978363037 and perplexity is 189.84595465953987
At time: 29.268639087677002 and batch: 350, loss is 5.297411203384399 and perplexity is 199.81884945564343
At time: 29.913639545440674 and batch: 400, loss is 5.303130197525024 and perplexity is 200.96488625011597
At time: 30.556843996047974 and batch: 450, loss is 5.241658124923706 and perplexity is 188.98320051097355
At time: 31.193681955337524 and batch: 500, loss is 5.253671598434448 and perplexity is 191.2672373221938
At time: 31.837857484817505 and batch: 550, loss is 5.3130559158325195 and perplexity is 202.9695394540794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4802914883228055 and perplexity of 239.9166300790491
Finished 4 epochs...
Completing Train Step...
At time: 33.57936978340149 and batch: 50, loss is 5.326135301589966 and perplexity is 205.64169332953523
At time: 34.2377347946167 and batch: 100, loss is 5.317266759872436 and perplexity is 203.82601450547932
At time: 34.88053774833679 and batch: 150, loss is 5.288374147415161 and perplexity is 198.02121024664967
At time: 35.52550482749939 and batch: 200, loss is 5.239035568237305 and perplexity is 188.4882306817656
At time: 36.1741669178009 and batch: 250, loss is 5.224093418121338 and perplexity is 185.69274851251643
At time: 36.828723669052124 and batch: 300, loss is 5.213889636993408 and perplexity is 183.80761445352198
At time: 37.47340774536133 and batch: 350, loss is 5.262744951248169 and perplexity is 193.01056942165417
At time: 38.11658692359924 and batch: 400, loss is 5.255090351104736 and perplexity is 191.53879081395414
At time: 38.74415183067322 and batch: 450, loss is 5.196937818527221 and perplexity is 180.7180023985702
At time: 39.389657497406006 and batch: 500, loss is 5.220628499984741 and perplexity is 185.05045173583562
At time: 40.033830881118774 and batch: 550, loss is 5.282272567749024 and perplexity is 196.8166466641589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.392532673287899 and perplexity of 219.75925973988038
Finished 5 epochs...
Completing Train Step...
At time: 41.797539949417114 and batch: 50, loss is 5.285632390975952 and perplexity is 197.47902792393546
At time: 42.453330993652344 and batch: 100, loss is 5.2749101924896244 and perplexity is 195.3729297682307
At time: 43.090075731277466 and batch: 150, loss is 5.246304121017456 and perplexity is 189.86325851232587
At time: 43.732014656066895 and batch: 200, loss is 5.201868371963501 and perplexity is 181.6112424405921
At time: 44.36771249771118 and batch: 250, loss is 5.181670398712158 and perplexity is 177.9798601582219
At time: 45.0137665271759 and batch: 300, loss is 5.166781072616577 and perplexity is 175.34949078966824
At time: 45.652709007263184 and batch: 350, loss is 5.208521327972412 and perplexity is 182.82352219774768
At time: 46.28792667388916 and batch: 400, loss is 5.21429762840271 and perplexity is 183.882621681295
At time: 46.91794991493225 and batch: 450, loss is 5.158962888717651 and perplexity is 173.9839213164743
At time: 47.55063271522522 and batch: 500, loss is 5.174127311706543 and perplexity is 176.6423932440629
At time: 48.198078870773315 and batch: 550, loss is 5.231257438659668 and perplexity is 187.02783174848437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.419897525868517 and perplexity of 225.85597692106674
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 49.95431137084961 and batch: 50, loss is 5.156053619384766 and perplexity is 173.47849080300998
At time: 50.608327865600586 and batch: 100, loss is 5.056662425994873 and perplexity is 157.06542306886507
At time: 51.241145849227905 and batch: 150, loss is 4.975385160446167 and perplexity is 144.80458740827623
At time: 51.87767577171326 and batch: 200, loss is 4.902606239318848 and perplexity is 134.6402274658952
At time: 52.51841425895691 and batch: 250, loss is 4.8681189060211185 and perplexity is 130.07600143108596
At time: 53.15872669219971 and batch: 300, loss is 4.855187797546387 and perplexity is 128.4048030575353
At time: 53.79482412338257 and batch: 350, loss is 4.887896385192871 and perplexity is 132.67418491084345
At time: 54.426778078079224 and batch: 400, loss is 4.854470796585083 and perplexity is 128.31276968825694
At time: 55.05657196044922 and batch: 450, loss is 4.780790967941284 and perplexity is 119.1985950348361
At time: 55.697376012802124 and batch: 500, loss is 4.779296951293945 and perplexity is 119.02064331403162
At time: 56.332491874694824 and batch: 550, loss is 4.836388854980469 and perplexity is 126.01347617226506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.097974736639794 and perplexity of 163.69005589744873
Finished 7 epochs...
Completing Train Step...
At time: 58.083057165145874 and batch: 50, loss is 4.933121757507324 and perplexity is 138.81217460343126
At time: 58.73342180252075 and batch: 100, loss is 4.917957954406738 and perplexity is 136.72313305664812
At time: 59.36278700828552 and batch: 150, loss is 4.8771022605895995 and perplexity is 131.24978463439666
At time: 59.99278497695923 and batch: 200, loss is 4.831841516494751 and perplexity is 125.44175114115724
At time: 60.63234210014343 and batch: 250, loss is 4.8024530029296875 and perplexity is 121.80884880566249
At time: 61.27058696746826 and batch: 300, loss is 4.793976669311523 and perplexity is 120.78071990309444
At time: 61.9100239276886 and batch: 350, loss is 4.834375801086426 and perplexity is 125.76005940973346
At time: 62.55290484428406 and batch: 400, loss is 4.817777509689331 and perplexity is 123.68988550051844
At time: 63.18608331680298 and batch: 450, loss is 4.7558306694030765 and perplexity is 116.26018684506906
At time: 63.82092022895813 and batch: 500, loss is 4.766966390609741 and perplexity is 117.56206308548147
At time: 64.45918536186218 and batch: 550, loss is 4.8218000125885006 and perplexity is 124.18843045056909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.075009285135472 and perplexity of 159.97367734975674
Finished 8 epochs...
Completing Train Step...
At time: 66.14215683937073 and batch: 50, loss is 4.886129217147827 and perplexity is 132.4399343718617
At time: 66.80000901222229 and batch: 100, loss is 4.871174268722534 and perplexity is 130.47403855845644
At time: 67.45108604431152 and batch: 150, loss is 4.830783367156982 and perplexity is 125.30908523780623
At time: 68.10353541374207 and batch: 200, loss is 4.788682231903076 and perplexity is 120.14294376413923
At time: 68.74761080741882 and batch: 250, loss is 4.759199457168579 and perplexity is 116.65250318434545
At time: 69.39510464668274 and batch: 300, loss is 4.75412145614624 and perplexity is 116.06164311759238
At time: 70.04207992553711 and batch: 350, loss is 4.7925216102600094 and perplexity is 120.6051046196393
At time: 70.68779134750366 and batch: 400, loss is 4.78591667175293 and perplexity is 119.81114024931139
At time: 71.32891988754272 and batch: 450, loss is 4.725490083694458 and perplexity is 112.78575929332213
At time: 71.96681714057922 and batch: 500, loss is 4.738901500701904 and perplexity is 114.30856480857634
At time: 72.60804319381714 and batch: 550, loss is 4.790559120178223 and perplexity is 120.36865039343662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.047261014897773 and perplexity of 155.59570598993253
Finished 9 epochs...
Completing Train Step...
At time: 74.32624506950378 and batch: 50, loss is 4.839761562347412 and perplexity is 126.43920026952857
At time: 74.99894499778748 and batch: 100, loss is 4.8231738185882564 and perplexity is 124.35915850818542
At time: 75.6457417011261 and batch: 150, loss is 4.784959564208984 and perplexity is 119.69652296241465
At time: 76.29390168190002 and batch: 200, loss is 4.743490610122681 and perplexity is 114.83434482849583
At time: 76.94153952598572 and batch: 250, loss is 4.713672389984131 and perplexity is 111.46073651258526
At time: 77.58757948875427 and batch: 300, loss is 4.711563682556152 and perplexity is 111.22594606875214
At time: 78.2282943725586 and batch: 350, loss is 4.748324251174926 and perplexity is 115.39075649566769
At time: 78.87015008926392 and batch: 400, loss is 4.747015342712403 and perplexity is 115.23981936098924
At time: 79.50507092475891 and batch: 450, loss is 4.683516359329223 and perplexity is 108.14969792991818
At time: 80.14989519119263 and batch: 500, loss is 4.703413295745849 and perplexity is 110.32309587205017
At time: 80.79687118530273 and batch: 550, loss is 4.754368181228638 and perplexity is 116.09028196886995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.024719562936337 and perplexity of 152.12758791602
Finished 10 epochs...
Completing Train Step...
At time: 82.53850436210632 and batch: 50, loss is 4.793983716964721 and perplexity is 120.78157112672086
At time: 83.20798206329346 and batch: 100, loss is 4.775517587661743 and perplexity is 118.57166997437754
At time: 83.8545970916748 and batch: 150, loss is 4.741048698425293 and perplexity is 114.55427159482143
At time: 84.49934554100037 and batch: 200, loss is 4.699226865768432 and perplexity is 109.86220138069123
At time: 85.13429665565491 and batch: 250, loss is 4.670159578323364 and perplexity is 106.7147704379091
At time: 85.77175974845886 and batch: 300, loss is 4.677986822128296 and perplexity is 107.55333048945198
At time: 86.41898274421692 and batch: 350, loss is 4.712294616699219 and perplexity is 111.30727462961875
At time: 87.06044864654541 and batch: 400, loss is 4.712938508987427 and perplexity is 111.37896760417394
At time: 87.70722675323486 and batch: 450, loss is 4.653045740127563 and perplexity is 104.90400985032244
At time: 88.36350774765015 and batch: 500, loss is 4.670535278320313 and perplexity is 106.75487070920119
At time: 89.01063466072083 and batch: 550, loss is 4.726196517944336 and perplexity is 112.8654631660435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.003924106029754 and perplexity of 148.996692249276
Finished 11 epochs...
Completing Train Step...
At time: 90.82044577598572 and batch: 50, loss is 4.767353792190551 and perplexity is 117.60761563755973
At time: 91.51219773292542 and batch: 100, loss is 4.751730461120605 and perplexity is 115.78447179590259
At time: 92.15772938728333 and batch: 150, loss is 4.719321165084839 and perplexity is 112.09213478018862
At time: 92.80629467964172 and batch: 200, loss is 4.682643747329712 and perplexity is 108.05536636917262
At time: 93.448157787323 and batch: 250, loss is 4.651120252609253 and perplexity is 104.70221282990389
At time: 94.10293650627136 and batch: 300, loss is 4.661774053573608 and perplexity is 105.82365255648604
At time: 94.7504358291626 and batch: 350, loss is 4.695203971862793 and perplexity is 109.42112519649605
At time: 95.39221429824829 and batch: 400, loss is 4.6970999050140385 and perplexity is 109.6287771200841
At time: 96.0333788394928 and batch: 450, loss is 4.640099086761475 and perplexity is 103.55460796567772
At time: 96.68291091918945 and batch: 500, loss is 4.6589805698394775 and perplexity is 105.52844842032954
At time: 97.33648657798767 and batch: 550, loss is 4.710400915145874 and perplexity is 111.09669132465105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.994151013962766 and perplexity of 147.54762631935998
Finished 12 epochs...
Completing Train Step...
At time: 99.11861181259155 and batch: 50, loss is 4.746336336135864 and perplexity is 115.1615973254077
At time: 99.78606915473938 and batch: 100, loss is 4.727568807601929 and perplexity is 113.02045359540443
At time: 100.44434022903442 and batch: 150, loss is 4.698554859161377 and perplexity is 109.78839805643597
At time: 101.09548163414001 and batch: 200, loss is 4.658757953643799 and perplexity is 105.5049586933002
At time: 101.74327087402344 and batch: 250, loss is 4.627085151672364 and perplexity is 102.21568623654521
At time: 102.381667137146 and batch: 300, loss is 4.633032941818238 and perplexity is 102.82545527944035
At time: 103.03928089141846 and batch: 350, loss is 4.669912233352661 and perplexity is 106.68837834025237
At time: 103.69449639320374 and batch: 400, loss is 4.675543231964111 and perplexity is 107.29083507516513
At time: 104.34285831451416 and batch: 450, loss is 4.6208005714416505 and perplexity is 101.5753178863528
At time: 105.001296043396 and batch: 500, loss is 4.635797033309936 and perplexity is 103.11006741128651
At time: 105.65209579467773 and batch: 550, loss is 4.686479892730713 and perplexity is 108.4706785555787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.985457724713265 and perplexity of 146.2705113336467
Finished 13 epochs...
Completing Train Step...
At time: 107.4621856212616 and batch: 50, loss is 4.722551784515381 and perplexity is 112.45484738655105
At time: 108.14130282402039 and batch: 100, loss is 4.70277738571167 and perplexity is 110.25296260996932
At time: 108.78816962242126 and batch: 150, loss is 4.67336064338684 and perplexity is 107.05691868855978
At time: 109.43479776382446 and batch: 200, loss is 4.636206254959107 and perplexity is 103.15227091782394
At time: 110.0817985534668 and batch: 250, loss is 4.6074422836303714 and perplexity is 100.22746808121623
At time: 110.7262499332428 and batch: 300, loss is 4.617497358322144 and perplexity is 101.2403465091951
At time: 111.36801028251648 and batch: 350, loss is 4.652243022918701 and perplexity is 104.8198353850027
At time: 112.01597452163696 and batch: 400, loss is 4.658700294494629 and perplexity is 105.49887554252507
At time: 112.66011428833008 and batch: 450, loss is 4.605067501068115 and perplexity is 99.98973203519397
At time: 113.30691361427307 and batch: 500, loss is 4.61936017036438 and perplexity is 101.4291140104335
At time: 113.9497606754303 and batch: 550, loss is 4.668234653472901 and perplexity is 106.50955010460886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.97235659335522 and perplexity of 144.36670041999403
Finished 14 epochs...
Completing Train Step...
At time: 115.70675659179688 and batch: 50, loss is 4.701347713470459 and perplexity is 110.09544963260946
At time: 116.37402534484863 and batch: 100, loss is 4.681789197921753 and perplexity is 107.96306716254811
At time: 117.01897668838501 and batch: 150, loss is 4.655396280288696 and perplexity is 105.15088096511006
At time: 117.66316032409668 and batch: 200, loss is 4.617844963073731 and perplexity is 101.2755442517909
At time: 118.30752277374268 and batch: 250, loss is 4.588545198440552 and perplexity is 98.35124448997296
At time: 118.96201753616333 and batch: 300, loss is 4.601875038146972 and perplexity is 99.67102752003187
At time: 119.60329914093018 and batch: 350, loss is 4.636465158462524 and perplexity is 103.1789808596496
At time: 120.24784517288208 and batch: 400, loss is 4.64639946937561 and perplexity is 104.20910123131624
At time: 120.89470934867859 and batch: 450, loss is 4.589673271179199 and perplexity is 98.4622544495766
At time: 121.54953956604004 and batch: 500, loss is 4.6040535736083985 and perplexity is 99.88840107999388
At time: 122.18878173828125 and batch: 550, loss is 4.653674592971802 and perplexity is 104.96999978209232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.961979967482547 and perplexity of 142.87640666381643
Finished 15 epochs...
Completing Train Step...
At time: 123.96511554718018 and batch: 50, loss is 4.684726676940918 and perplexity is 108.28067265854418
At time: 124.61834239959717 and batch: 100, loss is 4.664214305877685 and perplexity is 106.08220430591133
At time: 125.26284432411194 and batch: 150, loss is 4.638328685760498 and perplexity is 103.37143697497518
At time: 125.91133880615234 and batch: 200, loss is 4.600482635498047 and perplexity is 99.53234189281598
At time: 126.56491017341614 and batch: 250, loss is 4.571138830184936 and perplexity is 96.65411974868269
At time: 127.21191716194153 and batch: 300, loss is 4.58489351272583 and perplexity is 97.99275160548576
At time: 127.86190676689148 and batch: 350, loss is 4.618901662826538 and perplexity is 101.3826186571526
At time: 128.5141191482544 and batch: 400, loss is 4.628484020233154 and perplexity is 102.3587726026176
At time: 129.15858149528503 and batch: 450, loss is 4.5730733299255375 and perplexity is 96.84127808877803
At time: 129.81024432182312 and batch: 500, loss is 4.586494798660278 and perplexity is 98.14979171981688
At time: 130.45490288734436 and batch: 550, loss is 4.637897777557373 and perplexity is 103.32690297053688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.95452880859375 and perplexity of 141.8157682561489
Finished 16 epochs...
Completing Train Step...
At time: 132.1813521385193 and batch: 50, loss is 4.665641021728516 and perplexity is 106.23366148577409
At time: 132.85024094581604 and batch: 100, loss is 4.646604528427124 and perplexity is 104.23047244187907
At time: 133.49057340621948 and batch: 150, loss is 4.619761524200439 and perplexity is 101.46983114487213
At time: 134.1310679912567 and batch: 200, loss is 4.582060775756836 and perplexity is 97.71555671079254
At time: 134.7727289199829 and batch: 250, loss is 4.554772357940674 and perplexity is 95.08510740203256
At time: 135.4136519432068 and batch: 300, loss is 4.569602880477905 and perplexity is 96.50577783379504
At time: 136.0608537197113 and batch: 350, loss is 4.606281671524048 and perplexity is 100.11121034649234
At time: 136.7137303352356 and batch: 400, loss is 4.615716276168823 and perplexity is 101.06018961956728
At time: 137.3607132434845 and batch: 450, loss is 4.559274673461914 and perplexity is 95.51417573263187
At time: 138.00566172599792 and batch: 500, loss is 4.573000316619873 and perplexity is 96.83420764506135
At time: 138.65558314323425 and batch: 550, loss is 4.62607707977295 and perplexity is 102.11269739437121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.946427040911735 and perplexity of 140.67145160121314
Finished 17 epochs...
Completing Train Step...
At time: 140.4198820590973 and batch: 50, loss is 4.650149421691895 and perplexity is 104.60061401019027
At time: 141.0861942768097 and batch: 100, loss is 4.632234792709351 and perplexity is 102.7434179772931
At time: 141.73130774497986 and batch: 150, loss is 4.605428705215454 and perplexity is 100.02585526463378
At time: 142.3822934627533 and batch: 200, loss is 4.568219575881958 and perplexity is 96.37237323864969
At time: 143.04048371315002 and batch: 250, loss is 4.5396231842041015 and perplexity is 93.65550259680829
At time: 143.69278740882874 and batch: 300, loss is 4.5559054946899415 and perplexity is 95.19291289918796
At time: 144.3335976600647 and batch: 350, loss is 4.592729206085205 and perplexity is 98.76360891516582
At time: 144.98559308052063 and batch: 400, loss is 4.602128362655639 and perplexity is 99.69627983248648
At time: 145.62805223464966 and batch: 450, loss is 4.547424583435059 and perplexity is 94.38900401124444
At time: 146.27592825889587 and batch: 500, loss is 4.558650388717651 and perplexity is 95.4545662984273
At time: 146.92193460464478 and batch: 550, loss is 4.6119704246521 and perplexity is 100.68234127870264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9400693203540555 and perplexity of 139.77993881901642
Finished 18 epochs...
Completing Train Step...
At time: 148.64323568344116 and batch: 50, loss is 4.634956674575806 and perplexity is 103.02345436367337
At time: 149.30918979644775 and batch: 100, loss is 4.616430835723877 and perplexity is 101.13242895027379
At time: 149.95519495010376 and batch: 150, loss is 4.592386236190796 and perplexity is 98.72974177868082
At time: 150.60782837867737 and batch: 200, loss is 4.557771682739258 and perplexity is 95.3707266409546
At time: 151.25115656852722 and batch: 250, loss is 4.528065967559814 and perplexity is 92.57933638492875
At time: 151.90518951416016 and batch: 300, loss is 4.542939910888672 and perplexity is 93.96664800833845
At time: 152.5570034980774 and batch: 350, loss is 4.580785322189331 and perplexity is 97.59100450254913
At time: 153.2060432434082 and batch: 400, loss is 4.590626678466797 and perplexity is 98.5561738451259
At time: 153.85696363449097 and batch: 450, loss is 4.536417407989502 and perplexity is 93.35574474922363
At time: 154.51010060310364 and batch: 500, loss is 4.550200300216675 and perplexity is 94.65136510527525
At time: 155.16998672485352 and batch: 550, loss is 4.599999942779541 and perplexity is 99.48430994939578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.928272653133311 and perplexity of 138.14068924744842
Finished 19 epochs...
Completing Train Step...
At time: 156.95872354507446 and batch: 50, loss is 4.622100839614868 and perplexity is 101.70747894317248
At time: 157.6359794139862 and batch: 100, loss is 4.604154224395752 and perplexity is 99.89845543219081
At time: 158.28491520881653 and batch: 150, loss is 4.580673303604126 and perplexity is 97.58007310856709
At time: 158.94362878799438 and batch: 200, loss is 4.544140224456787 and perplexity is 94.07950516933404
At time: 159.58848190307617 and batch: 250, loss is 4.515410556793213 and perplexity is 91.41508940507875
At time: 160.22930240631104 and batch: 300, loss is 4.532839708328247 and perplexity is 93.02234269469388
At time: 160.86964201927185 and batch: 350, loss is 4.568836994171143 and perplexity is 96.43189367707221
At time: 161.51872539520264 and batch: 400, loss is 4.5800696468353275 and perplexity is 97.52118601252053
At time: 162.1743187904358 and batch: 450, loss is 4.525473928451538 and perplexity is 92.33967786071656
At time: 162.8164665699005 and batch: 500, loss is 4.537260046005249 and perplexity is 93.4344430011506
At time: 163.45669746398926 and batch: 550, loss is 4.591288919448853 and perplexity is 98.6214633988037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.924702421147773 and perplexity of 137.64837430241798
Finished 20 epochs...
Completing Train Step...
At time: 165.19281697273254 and batch: 50, loss is 4.610404586791992 and perplexity is 100.52481242139497
At time: 165.8665943145752 and batch: 100, loss is 4.591147871017456 and perplexity is 98.60755397706336
At time: 166.5193531513214 and batch: 150, loss is 4.569549856185913 and perplexity is 96.50066081891643
At time: 167.16257238388062 and batch: 200, loss is 4.5341292572021485 and perplexity is 93.14237693033611
At time: 167.8037497997284 and batch: 250, loss is 4.504449424743652 and perplexity is 90.41854812661227
At time: 168.45335698127747 and batch: 300, loss is 4.5246828842163085 and perplexity is 92.2666619740675
At time: 169.0999677181244 and batch: 350, loss is 4.560418224334716 and perplexity is 95.62346352782299
At time: 169.74452829360962 and batch: 400, loss is 4.569896507263183 and perplexity is 96.53411867571265
At time: 170.3882875442505 and batch: 450, loss is 4.515591106414795 and perplexity is 91.43159585494955
At time: 171.02613520622253 and batch: 500, loss is 4.525275297164917 and perplexity is 92.3213381331791
At time: 171.6684033870697 and batch: 550, loss is 4.576463117599487 and perplexity is 97.17010647390504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.921711698491523 and perplexity of 137.23732117013958
Finished 21 epochs...
Completing Train Step...
At time: 173.38508081436157 and batch: 50, loss is 4.597493152618409 and perplexity is 99.235235978549
At time: 174.04186964035034 and batch: 100, loss is 4.580563631057739 and perplexity is 97.56937184030106
At time: 174.6898946762085 and batch: 150, loss is 4.55888991355896 and perplexity is 95.47743277670759
At time: 175.34335446357727 and batch: 200, loss is 4.522886533737182 and perplexity is 92.10106748901866
At time: 175.99260830879211 and batch: 250, loss is 4.49541558265686 and perplexity is 89.60539969846589
At time: 176.6377956867218 and batch: 300, loss is 4.512389926910401 and perplexity is 91.13937487968872
At time: 177.27601265907288 and batch: 350, loss is 4.548888921737671 and perplexity is 94.52732269311919
At time: 177.93321633338928 and batch: 400, loss is 4.5600026416778565 and perplexity is 95.58373233116191
At time: 178.57718420028687 and batch: 450, loss is 4.505872087478638 and perplexity is 90.54727477117206
At time: 179.2245807647705 and batch: 500, loss is 4.517062244415283 and perplexity is 91.5662033388724
At time: 179.86605191230774 and batch: 550, loss is 4.568209381103515 and perplexity is 96.37139074866467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.923773258290392 and perplexity of 137.52053594616098
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 181.59489226341248 and batch: 50, loss is 4.578643188476563 and perplexity is 97.38217527166435
At time: 182.24786949157715 and batch: 100, loss is 4.543912658691406 and perplexity is 94.05809833055773
At time: 182.90421414375305 and batch: 150, loss is 4.5136439228057865 and perplexity is 91.25373497025197
At time: 183.551114320755 and batch: 200, loss is 4.472813758850098 and perplexity is 87.60286990252912
At time: 184.19542741775513 and batch: 250, loss is 4.437906198501587 and perplexity is 84.59762547116843
At time: 184.85874819755554 and batch: 300, loss is 4.440418663024903 and perplexity is 84.81044123800405
At time: 185.4959123134613 and batch: 350, loss is 4.476589336395263 and perplexity is 87.93424650641018
At time: 186.14548182487488 and batch: 400, loss is 4.48054196357727 and perplexity is 88.28250561517874
At time: 186.79596090316772 and batch: 450, loss is 4.414879179000854 and perplexity is 82.67185180693735
At time: 187.4440996646881 and batch: 500, loss is 4.421794519424439 and perplexity is 83.2455371340704
At time: 188.08893156051636 and batch: 550, loss is 4.474955186843872 and perplexity is 87.79066614477355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.868354635035738 and perplexity of 130.1066677330555
Finished 23 epochs...
Completing Train Step...
At time: 189.80160880088806 and batch: 50, loss is 4.540326976776123 and perplexity is 93.72143984421568
At time: 190.47274351119995 and batch: 100, loss is 4.518695077896118 and perplexity is 91.71583783225682
At time: 191.10939645767212 and batch: 150, loss is 4.495578327178955 and perplexity is 89.61998367311575
At time: 191.75124406814575 and batch: 200, loss is 4.456666946411133 and perplexity is 86.19972146596605
At time: 192.39403891563416 and batch: 250, loss is 4.4252400302886965 and perplexity is 83.53285523123704
At time: 193.03291630744934 and batch: 300, loss is 4.428677816390991 and perplexity is 83.82051749739804
At time: 193.6681706905365 and batch: 350, loss is 4.466968269348144 and perplexity is 87.09228201811939
At time: 194.3244309425354 and batch: 400, loss is 4.4734429931640625 and perplexity is 87.6580099804688
At time: 194.98271369934082 and batch: 450, loss is 4.4114031410217285 and perplexity is 82.384980187426
At time: 195.63906931877136 and batch: 500, loss is 4.422240543365478 and perplexity is 83.28267491817164
At time: 196.2975628376007 and batch: 550, loss is 4.47684419631958 and perplexity is 87.95666027788437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.865462607525765 and perplexity of 129.73093924118513
Finished 24 epochs...
Completing Train Step...
At time: 198.07705545425415 and batch: 50, loss is 4.53170823097229 and perplexity is 92.91714954339386
At time: 198.73342990875244 and batch: 100, loss is 4.510757837295532 and perplexity is 90.99074857116085
At time: 199.37544679641724 and batch: 150, loss is 4.488810644149781 and perplexity is 89.01551177478352
At time: 200.03222727775574 and batch: 200, loss is 4.449747352600098 and perplexity is 85.60531331002099
At time: 200.68102073669434 and batch: 250, loss is 4.418888959884644 and perplexity is 83.00401332033323
At time: 201.34304475784302 and batch: 300, loss is 4.42205662727356 and perplexity is 83.26735930251343
At time: 201.9879686832428 and batch: 350, loss is 4.4620728683471675 and perplexity is 86.66697225385037
At time: 202.63252997398376 and batch: 400, loss is 4.469040813446045 and perplexity is 87.27297179227278
At time: 203.27571082115173 and batch: 450, loss is 4.4095641613006595 and perplexity is 82.23361510086077
At time: 203.92062044143677 and batch: 500, loss is 4.421486406326294 and perplexity is 83.21989204471417
At time: 204.56134605407715 and batch: 550, loss is 4.476016550064087 and perplexity is 87.88389339413293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.863194242436835 and perplexity of 129.4369956194213
Finished 25 epochs...
Completing Train Step...
At time: 206.33117508888245 and batch: 50, loss is 4.526045131683349 and perplexity is 92.39243764998162
At time: 206.99833273887634 and batch: 100, loss is 4.505152540206909 and perplexity is 90.48214516137459
At time: 207.65592622756958 and batch: 150, loss is 4.48378402709961 and perplexity is 88.56918757574898
At time: 208.30034160614014 and batch: 200, loss is 4.44504228591919 and perplexity is 85.20348066864574
At time: 208.95141553878784 and batch: 250, loss is 4.4141374206542965 and perplexity is 82.61055200846337
At time: 209.60411596298218 and batch: 300, loss is 4.417041149139404 and perplexity is 82.85077923002147
At time: 210.25560069084167 and batch: 350, loss is 4.458521718978882 and perplexity is 86.35975070773512
At time: 210.90626883506775 and batch: 400, loss is 4.466189060211182 and perplexity is 87.02444534912047
At time: 211.55437922477722 and batch: 450, loss is 4.4075439929962155 and perplexity is 82.06765704611587
At time: 212.199298620224 and batch: 500, loss is 4.4198203372955325 and perplexity is 83.08135739602427
At time: 212.84438467025757 and batch: 550, loss is 4.474346523284912 and perplexity is 87.73724742413785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.861500516850898 and perplexity of 129.21795042176637
Finished 26 epochs...
Completing Train Step...
At time: 214.56168460845947 and batch: 50, loss is 4.52140853881836 and perplexity is 91.96504312567247
At time: 215.21909523010254 and batch: 100, loss is 4.500507164001465 and perplexity is 90.06279632788275
At time: 215.86644053459167 and batch: 150, loss is 4.479514932632446 and perplexity is 88.19188329395507
At time: 216.51713252067566 and batch: 200, loss is 4.441157083511353 and perplexity is 84.87309013304616
At time: 217.17382788658142 and batch: 250, loss is 4.40998085975647 and perplexity is 82.26788886170466
At time: 217.81944036483765 and batch: 300, loss is 4.412899303436279 and perplexity is 82.50833375378016
At time: 218.46303367614746 and batch: 350, loss is 4.45559268951416 and perplexity is 86.10717054129918
At time: 219.11328315734863 and batch: 400, loss is 4.463731136322021 and perplexity is 86.81080854501339
At time: 219.76171350479126 and batch: 450, loss is 4.405323801040649 and perplexity is 81.88565321061752
At time: 220.4191017150879 and batch: 500, loss is 4.417745952606201 and perplexity is 82.90919332926136
At time: 221.07214641571045 and batch: 550, loss is 4.472265663146973 and perplexity is 87.55486830189136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8600168431058846 and perplexity of 129.02637529397754
Finished 27 epochs...
Completing Train Step...
At time: 222.83911895751953 and batch: 50, loss is 4.517351360321045 and perplexity is 91.59268041197495
At time: 223.50166130065918 and batch: 100, loss is 4.496625909805298 and perplexity is 89.71391720396184
At time: 224.1645736694336 and batch: 150, loss is 4.476004734039306 and perplexity is 87.88285496200582
At time: 224.81511402130127 and batch: 200, loss is 4.4377930641174315 and perplexity is 84.5880551122883
At time: 225.46108150482178 and batch: 250, loss is 4.406385927200318 and perplexity is 81.97267230944409
At time: 226.10647010803223 and batch: 300, loss is 4.409242572784424 and perplexity is 82.2071739664063
At time: 226.75684881210327 and batch: 350, loss is 4.452945308685303 and perplexity is 85.87951354912288
At time: 227.40707778930664 and batch: 400, loss is 4.46130675315857 and perplexity is 86.60060079739365
At time: 228.05216526985168 and batch: 450, loss is 4.403027820587158 and perplexity is 81.69786101752288
At time: 228.70321416854858 and batch: 500, loss is 4.415599355697632 and perplexity is 82.73141159229864
At time: 229.35640907287598 and batch: 550, loss is 4.470130987167359 and perplexity is 87.36816637258852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.858756856715425 and perplexity of 128.86390619301906
Finished 28 epochs...
Completing Train Step...
At time: 231.1183717250824 and batch: 50, loss is 4.513735389709472 and perplexity is 91.26208204857436
At time: 231.77983593940735 and batch: 100, loss is 4.493172941207885 and perplexity is 89.40467207921857
At time: 232.41455173492432 and batch: 150, loss is 4.4728665447235105 and perplexity is 87.60749421857861
At time: 233.04823899269104 and batch: 200, loss is 4.434774055480957 and perplexity is 84.33306814096736
At time: 233.68365383148193 and batch: 250, loss is 4.4030836391448975 and perplexity is 81.70242140157113
At time: 234.32545232772827 and batch: 300, loss is 4.405852880477905 and perplexity is 81.92898868888187
At time: 234.97638082504272 and batch: 350, loss is 4.450342426300049 and perplexity is 85.65626994051944
At time: 235.62552046775818 and batch: 400, loss is 4.458785581588745 and perplexity is 86.38254082354146
At time: 236.27514672279358 and batch: 450, loss is 4.4006369972229 and perplexity is 81.50276917059453
At time: 236.92035961151123 and batch: 500, loss is 4.41325047492981 and perplexity is 82.53731341669115
At time: 237.5671579837799 and batch: 550, loss is 4.467844438552857 and perplexity is 87.16862303253099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8573760986328125 and perplexity of 128.6860990955285
Finished 29 epochs...
Completing Train Step...
At time: 239.35014605522156 and batch: 50, loss is 4.510188951492309 and perplexity is 90.93899994699849
At time: 240.03215837478638 and batch: 100, loss is 4.489746780395508 and perplexity is 89.09888143838135
At time: 240.68224096298218 and batch: 150, loss is 4.469792699813842 and perplexity is 87.33861582537477
At time: 241.32316541671753 and batch: 200, loss is 4.4318220329284665 and perplexity is 84.08448211819118
At time: 241.96224808692932 and batch: 250, loss is 4.3999564075469975 and perplexity is 81.44731809919332
At time: 242.62035059928894 and batch: 300, loss is 4.402453746795654 and perplexity is 81.65097387631793
At time: 243.27380657196045 and batch: 350, loss is 4.447695751190185 and perplexity is 85.42986536470347
At time: 243.93637132644653 and batch: 400, loss is 4.4561944103240965 and perplexity is 86.15899860914836
At time: 244.59994626045227 and batch: 450, loss is 4.398068513870239 and perplexity is 81.29369927599402
At time: 245.2497594356537 and batch: 500, loss is 4.4106956958770756 and perplexity is 82.3267179443005
At time: 245.90298008918762 and batch: 550, loss is 4.465409812927246 and perplexity is 86.95665820135169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.855227531270778 and perplexity of 128.40990515995253
Finished 30 epochs...
Completing Train Step...
At time: 247.63028645515442 and batch: 50, loss is 4.506374626159668 and perplexity is 90.59278971475844
At time: 248.30779480934143 and batch: 100, loss is 4.486115131378174 and perplexity is 88.775892419525
At time: 248.9556860923767 and batch: 150, loss is 4.466429452896119 and perplexity is 87.04536790390688
At time: 249.60240197181702 and batch: 200, loss is 4.428656091690064 and perplexity is 83.81869654150378
At time: 250.24176263809204 and batch: 250, loss is 4.396435174942017 and perplexity is 81.16102749083812
At time: 250.88717103004456 and batch: 300, loss is 4.398626356124878 and perplexity is 81.3390609876495
At time: 251.52731323242188 and batch: 350, loss is 4.4444430065155025 and perplexity is 85.15243527431299
At time: 252.17015194892883 and batch: 400, loss is 4.453194952011108 and perplexity is 85.90095547280903
At time: 252.81687760353088 and batch: 450, loss is 4.395385503768921 and perplexity is 81.07587979626042
At time: 253.47389435768127 and batch: 500, loss is 4.407934713363647 and perplexity is 82.0997288163701
At time: 254.1228370666504 and batch: 550, loss is 4.4627164936065675 and perplexity is 86.72277126124773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.853181554916057 and perplexity of 128.14745011024976
Finished 31 epochs...
Completing Train Step...
At time: 255.93482542037964 and batch: 50, loss is 4.5031554222106935 and perplexity is 90.30162196400082
At time: 256.5992615222931 and batch: 100, loss is 4.482507152557373 and perplexity is 88.45616800616818
At time: 257.2547779083252 and batch: 150, loss is 4.462900066375733 and perplexity is 86.73869266184093
At time: 257.9112091064453 and batch: 200, loss is 4.425534801483154 and perplexity is 83.55748194019391
At time: 258.55848956108093 and batch: 250, loss is 4.39278694152832 and perplexity is 80.86547257292958
At time: 259.2152347564697 and batch: 300, loss is 4.394927186965942 and perplexity is 81.03872987210106
At time: 259.86748909950256 and batch: 350, loss is 4.441300134658814 and perplexity is 84.88523219442546
At time: 260.51188826560974 and batch: 400, loss is 4.449829149246216 and perplexity is 85.61231582392678
At time: 261.1571445465088 and batch: 450, loss is 4.391916856765747 and perplexity is 80.7951433580516
At time: 261.8033490180969 and batch: 500, loss is 4.405064449310303 and perplexity is 81.86441877847922
At time: 262.4502332210541 and batch: 550, loss is 4.459720296859741 and perplexity is 86.46332165125185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850551524060838 and perplexity of 127.81086117610047
Finished 32 epochs...
Completing Train Step...
At time: 264.2144844532013 and batch: 50, loss is 4.499299726486206 and perplexity is 89.95411675396842
At time: 264.8918471336365 and batch: 100, loss is 4.478305025100708 and perplexity is 88.0852437950949
At time: 265.5342421531677 and batch: 150, loss is 4.458956270217896 and perplexity is 86.39728659944953
At time: 266.1852858066559 and batch: 200, loss is 4.4222704029083255 and perplexity is 83.28516173789924
At time: 266.8332316875458 and batch: 250, loss is 4.388945655822754 and perplexity is 80.55544103014991
At time: 267.48804807662964 and batch: 300, loss is 4.39117504119873 and perplexity is 80.73523048786656
At time: 268.14704036712646 and batch: 350, loss is 4.437615747451782 and perplexity is 84.57305757009843
At time: 268.7987606525421 and batch: 400, loss is 4.4464795112609865 and perplexity is 85.32602531131081
At time: 269.4420680999756 and batch: 450, loss is 4.3884296798706055 and perplexity is 80.51388708110476
At time: 270.0901565551758 and batch: 500, loss is 4.401734685897827 and perplexity is 81.5922829574352
At time: 270.74200344085693 and batch: 550, loss is 4.456168603897095 and perplexity is 86.1567751819297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.848360102227393 and perplexity of 127.53108033525606
Finished 33 epochs...
Completing Train Step...
At time: 272.56215047836304 and batch: 50, loss is 4.495417833328247 and perplexity is 89.60560137100207
At time: 273.2271981239319 and batch: 100, loss is 4.474431419372559 and perplexity is 87.74469628937035
At time: 273.8691849708557 and batch: 150, loss is 4.45513484954834 and perplexity is 86.06775626068668
At time: 274.51883459091187 and batch: 200, loss is 4.4187946796417235 and perplexity is 82.99618805068398
At time: 275.15725564956665 and batch: 250, loss is 4.385113897323609 and perplexity is 80.2473626522451
At time: 275.80458426475525 and batch: 300, loss is 4.38698823928833 and perplexity is 80.39791470055734
At time: 276.45324897766113 and batch: 350, loss is 4.433618412017823 and perplexity is 84.23566547426194
At time: 277.0940911769867 and batch: 400, loss is 4.443317527770996 and perplexity is 85.05665192953887
At time: 277.7449724674225 and batch: 450, loss is 4.385011444091797 and perplexity is 80.23914147174742
At time: 278.3847894668579 and batch: 500, loss is 4.398191556930542 and perplexity is 81.30370251693837
At time: 279.0380492210388 and batch: 550, loss is 4.452453022003174 and perplexity is 85.83724661291342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8454602829953455 and perplexity of 127.1617989392356
Finished 34 epochs...
Completing Train Step...
At time: 280.79818844795227 and batch: 50, loss is 4.490985240936279 and perplexity is 89.20929524472825
At time: 281.459246635437 and batch: 100, loss is 4.470278377532959 and perplexity is 87.38104454760816
At time: 282.11131596565247 and batch: 150, loss is 4.451066684722901 and perplexity is 85.718329686428
At time: 282.75747060775757 and batch: 200, loss is 4.414551296234131 and perplexity is 82.64474957485699
At time: 283.3929400444031 and batch: 250, loss is 4.381030626296997 and perplexity is 79.9203589979857
At time: 284.0452799797058 and batch: 300, loss is 4.382395029067993 and perplexity is 80.02947698077278
At time: 284.6846191883087 and batch: 350, loss is 4.429708213806152 and perplexity is 83.90693045416296
At time: 285.3246262073517 and batch: 400, loss is 4.439834537506104 and perplexity is 84.76091576097201
At time: 285.97492241859436 and batch: 450, loss is 4.381201171875 and perplexity is 79.93399022414486
At time: 286.6240656375885 and batch: 500, loss is 4.394331912994385 and perplexity is 80.99050398075315
At time: 287.2652111053467 and batch: 550, loss is 4.447903242111206 and perplexity is 85.44759312526176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.842700390105552 and perplexity of 126.81132984553025
Finished 35 epochs...
Completing Train Step...
At time: 289.01198172569275 and batch: 50, loss is 4.486311388015747 and perplexity is 88.79331698745645
At time: 289.68379068374634 and batch: 100, loss is 4.465912752151489 and perplexity is 87.00040311516467
At time: 290.33490109443665 and batch: 150, loss is 4.446681385040283 and perplexity is 85.3432521372765
At time: 290.99489998817444 and batch: 200, loss is 4.410208597183227 and perplexity is 82.28662647256546
At time: 291.6478819847107 and batch: 250, loss is 4.3767208480834965 and perplexity is 79.57666113872618
At time: 292.2953143119812 and batch: 300, loss is 4.3775701999664305 and perplexity is 79.64427843707688
At time: 292.9429466724396 and batch: 350, loss is 4.4254992580413814 and perplexity is 83.55451207247985
At time: 293.5927002429962 and batch: 400, loss is 4.436124677658081 and perplexity is 84.44704720706416
At time: 294.2313368320465 and batch: 450, loss is 4.377108898162842 and perplexity is 79.60754686061092
At time: 294.8785307407379 and batch: 500, loss is 4.390233011245727 and perplexity is 80.65921129428482
At time: 295.525114774704 and batch: 550, loss is 4.443865766525269 and perplexity is 85.10329606732901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.839680773146609 and perplexity of 126.42898576020521
Finished 36 epochs...
Completing Train Step...
At time: 297.2617359161377 and batch: 50, loss is 4.481777057647705 and perplexity is 88.3916101777168
At time: 297.92177534103394 and batch: 100, loss is 4.461686611175537 and perplexity is 86.63350297856375
At time: 298.5799686908722 and batch: 150, loss is 4.442438592910767 and perplexity is 84.98192551769846
At time: 299.238894701004 and batch: 200, loss is 4.405851020812988 and perplexity is 81.92883632855758
At time: 299.9024307727814 and batch: 250, loss is 4.372325344085693 and perplexity is 79.22764921014503
At time: 300.5557224750519 and batch: 300, loss is 4.3731388759613035 and perplexity is 79.29212965309871
At time: 301.20406770706177 and batch: 350, loss is 4.421307430267334 and perplexity is 83.2049990091971
At time: 301.84736728668213 and batch: 400, loss is 4.432199621200562 and perplexity is 84.11623742734308
At time: 302.4963548183441 and batch: 450, loss is 4.373037452697754 and perplexity is 79.28408799434804
At time: 303.1521518230438 and batch: 500, loss is 4.38640043258667 and perplexity is 80.35067015418763
At time: 303.80404710769653 and batch: 550, loss is 4.440298986434937 and perplexity is 84.80029202092713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.836515710708943 and perplexity of 126.02946271755337
Finished 37 epochs...
Completing Train Step...
At time: 305.61871433258057 and batch: 50, loss is 4.477372913360596 and perplexity is 88.00317675898866
At time: 306.2764301300049 and batch: 100, loss is 4.457659406661987 and perplexity is 86.28531372958992
At time: 306.9258680343628 and batch: 150, loss is 4.437850885391235 and perplexity is 84.59294624278779
At time: 307.5650668144226 and batch: 200, loss is 4.401462707519531 and perplexity is 81.57009463814335
At time: 308.203950881958 and batch: 250, loss is 4.367950325012207 and perplexity is 78.88178386911297
At time: 308.8509569168091 and batch: 300, loss is 4.3682623291015625 and perplexity is 78.90639914808996
At time: 309.4912450313568 and batch: 350, loss is 4.416878986358642 and perplexity is 82.83734500656821
At time: 310.14983201026917 and batch: 400, loss is 4.42850019454956 and perplexity is 83.80563046490069
At time: 310.8015048503876 and batch: 450, loss is 4.369122953414917 and perplexity is 78.97433714402825
At time: 311.44469022750854 and batch: 500, loss is 4.382188577651977 and perplexity is 80.0129564873254
At time: 312.09164214134216 and batch: 550, loss is 4.436207180023193 and perplexity is 84.45401457559372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.833103585750498 and perplexity of 125.6001672636912
Finished 38 epochs...
Completing Train Step...
At time: 313.83424735069275 and batch: 50, loss is 4.472890901565552 and perplexity is 87.60962808646393
At time: 314.49820256233215 and batch: 100, loss is 4.453676509857178 and perplexity is 85.94233171362895
At time: 315.1444683074951 and batch: 150, loss is 4.433562002182007 and perplexity is 84.23091388822205
At time: 315.794305562973 and batch: 200, loss is 4.397168035507202 and perplexity is 81.22052900781067
At time: 316.4518723487854 and batch: 250, loss is 4.36449933052063 and perplexity is 78.61003244353007
At time: 317.1097345352173 and batch: 300, loss is 4.364371976852417 and perplexity is 78.60002180499936
At time: 317.7496671676636 and batch: 350, loss is 4.412899684906006 and perplexity is 82.50836522821771
At time: 318.3969223499298 and batch: 400, loss is 4.424819498062134 and perplexity is 83.49773435888244
At time: 319.049045085907 and batch: 450, loss is 4.3654447937011716 and perplexity is 78.68439048068072
At time: 319.70008420944214 and batch: 500, loss is 4.37793833732605 and perplexity is 79.67360386901186
At time: 320.3525743484497 and batch: 550, loss is 4.432472677230835 and perplexity is 84.13920900933824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8303868719872005 and perplexity of 125.2594106392996
Finished 39 epochs...
Completing Train Step...
At time: 322.1749622821808 and batch: 50, loss is 4.468440389633178 and perplexity is 87.22058674997622
At time: 322.8481123447418 and batch: 100, loss is 4.449452304840088 and perplexity is 85.5800593798249
At time: 323.4922950267792 and batch: 150, loss is 4.429732103347778 and perplexity is 83.9089349762142
At time: 324.15407967567444 and batch: 200, loss is 4.393293180465698 and perplexity is 80.90642018759944
At time: 324.8000295162201 and batch: 250, loss is 4.361358270645142 and perplexity is 78.3635010124991
At time: 325.4472997188568 and batch: 300, loss is 4.361170024871826 and perplexity is 78.3487508030271
At time: 326.08575320243835 and batch: 350, loss is 4.409351444244384 and perplexity is 82.21612446867351
At time: 326.73349928855896 and batch: 400, loss is 4.422032403945923 and perplexity is 83.26534231441674
At time: 327.38475155830383 and batch: 450, loss is 4.362730751037597 and perplexity is 78.47112722165917
At time: 328.04479026794434 and batch: 500, loss is 4.375178298950195 and perplexity is 79.45400485504896
At time: 328.6977801322937 and batch: 550, loss is 4.429427938461304 and perplexity is 83.88341670561068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.828835669984209 and perplexity of 125.06525861405879
Finished 40 epochs...
Completing Train Step...
At time: 330.4852702617645 and batch: 50, loss is 4.465146026611328 and perplexity is 86.9337232499364
At time: 331.15989422798157 and batch: 100, loss is 4.445749216079712 and perplexity is 85.26373487416012
At time: 331.811484336853 and batch: 150, loss is 4.425924329757691 and perplexity is 83.59003628196534
At time: 332.4618489742279 and batch: 200, loss is 4.390164442062378 and perplexity is 80.65368074765149
At time: 333.11392617225647 and batch: 250, loss is 4.358580827713013 and perplexity is 78.14615283627502
At time: 333.76557517051697 and batch: 300, loss is 4.3573300075531005 and perplexity is 78.04846715923111
At time: 334.4141492843628 and batch: 350, loss is 4.40543851852417 and perplexity is 81.89504746552261
At time: 335.0639772415161 and batch: 400, loss is 4.417993021011353 and perplexity is 82.92968010214257
At time: 335.7096674442291 and batch: 450, loss is 4.358820962905884 and perplexity is 78.16492073108432
At time: 336.3756513595581 and batch: 500, loss is 4.371370077133179 and perplexity is 79.15200179262074
At time: 337.04103350639343 and batch: 550, loss is 4.4258772659301755 and perplexity is 83.58610230749045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.826418612865692 and perplexity of 124.76333377223177
Finished 41 epochs...
Completing Train Step...
At time: 338.8044910430908 and batch: 50, loss is 4.461340208053588 and perplexity is 86.60349805986542
At time: 339.4655051231384 and batch: 100, loss is 4.442134847640991 and perplexity is 84.95611657967589
At time: 340.1205654144287 and batch: 150, loss is 4.422313432693482 and perplexity is 83.28874555762049
At time: 340.7755048274994 and batch: 200, loss is 4.3865500164031985 and perplexity is 80.36269021307066
At time: 341.4180369377136 and batch: 250, loss is 4.355493202209472 and perplexity is 77.90523889917702
At time: 342.0648617744446 and batch: 300, loss is 4.353969411849976 and perplexity is 77.78661804679633
At time: 342.7088453769684 and batch: 350, loss is 4.401864433288575 and perplexity is 81.60287003006175
At time: 343.368195772171 and batch: 400, loss is 4.414799146652221 and perplexity is 82.66523564922828
At time: 344.0105309486389 and batch: 450, loss is 4.355450029373169 and perplexity is 77.90187558165337
At time: 344.6515166759491 and batch: 500, loss is 4.3675535202026365 and perplexity is 78.85048940719332
At time: 345.3081405162811 and batch: 550, loss is 4.421965398788452 and perplexity is 83.2597632939568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.824707355905087 and perplexity of 124.55001422322438
Finished 42 epochs...
Completing Train Step...
At time: 347.02482533454895 and batch: 50, loss is 4.457491016387939 and perplexity is 86.27078534521831
At time: 347.6945958137512 and batch: 100, loss is 4.438176383972168 and perplexity is 84.62048560851585
At time: 348.3274841308594 and batch: 150, loss is 4.418528337478637 and perplexity is 82.97408560996726
At time: 348.9760105609894 and batch: 200, loss is 4.3830875968933105 and perplexity is 80.08492201911528
At time: 349.62290239334106 and batch: 250, loss is 4.3522958564758305 and perplexity is 77.65654670528237
At time: 350.2688937187195 and batch: 300, loss is 4.349932012557983 and perplexity is 77.47319554181516
At time: 350.9149751663208 and batch: 350, loss is 4.398614625930787 and perplexity is 81.3381068702729
At time: 351.5753140449524 and batch: 400, loss is 4.4115721702575685 and perplexity is 82.39890683464388
At time: 352.21674942970276 and batch: 450, loss is 4.351958999633789 and perplexity is 77.63039197164316
At time: 352.8618197441101 and batch: 500, loss is 4.36433967590332 and perplexity is 78.5974829906993
At time: 353.5066041946411 and batch: 550, loss is 4.4185983848571775 and perplexity is 82.97989793071754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.82234029566988 and perplexity of 124.25554548737146
Finished 43 epochs...
Completing Train Step...
At time: 355.3059983253479 and batch: 50, loss is 4.452796716690063 and perplexity is 85.86675348889928
At time: 355.9797043800354 and batch: 100, loss is 4.433923196792603 and perplexity is 84.26134313547328
At time: 356.63322472572327 and batch: 150, loss is 4.414282064437867 and perplexity is 82.62250197549209
At time: 357.28259658813477 and batch: 200, loss is 4.379304895401001 and perplexity is 79.78255690410879
At time: 357.9416890144348 and batch: 250, loss is 4.348826236724854 and perplexity is 77.3875749018113
At time: 358.5971751213074 and batch: 300, loss is 4.34543441772461 and perplexity is 77.12553490221973
At time: 359.24920535087585 and batch: 350, loss is 4.394713678359985 and perplexity is 81.02142925283881
At time: 359.9075379371643 and batch: 400, loss is 4.407870559692383 and perplexity is 82.0944619863018
At time: 360.560928106308 and batch: 450, loss is 4.348269023895264 and perplexity is 77.34446556387864
At time: 361.2232301235199 and batch: 500, loss is 4.360887222290039 and perplexity is 78.32659670678566
At time: 361.8862569332123 and batch: 550, loss is 4.414913377761841 and perplexity is 82.67467913018282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8199122002784245 and perplexity of 123.95420715716487
Finished 44 epochs...
Completing Train Step...
At time: 363.6658787727356 and batch: 50, loss is 4.44867467880249 and perplexity is 85.51353596587361
At time: 364.31643748283386 and batch: 100, loss is 4.429458522796631 and perplexity is 83.88598226338836
At time: 364.9622266292572 and batch: 150, loss is 4.410071601867676 and perplexity is 82.27535436233698
At time: 365.60476088523865 and batch: 200, loss is 4.375559139251709 and perplexity is 79.48426990492398
At time: 366.24776697158813 and batch: 250, loss is 4.345712089538575 and perplexity is 77.14695346292673
At time: 366.90885949134827 and batch: 300, loss is 4.341731777191162 and perplexity is 76.84049479669864
At time: 367.54530477523804 and batch: 350, loss is 4.391640357971191 and perplexity is 80.77280668648079
At time: 368.18155455589294 and batch: 400, loss is 4.404895887374878 and perplexity is 81.85062071655418
At time: 368.83445167541504 and batch: 450, loss is 4.345024566650391 and perplexity is 77.09393139569832
At time: 369.4814248085022 and batch: 500, loss is 4.357851762771606 and perplexity is 78.08919997962454
At time: 370.1217975616455 and batch: 550, loss is 4.411756191253662 and perplexity is 82.41407135880925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.818009072161735 and perplexity of 123.71853075267103
Finished 45 epochs...
Completing Train Step...
At time: 371.87444496154785 and batch: 50, loss is 4.445460109710694 and perplexity is 85.23908814829612
At time: 372.5387144088745 and batch: 100, loss is 4.426218910217285 and perplexity is 83.61466390050029
At time: 373.17956018447876 and batch: 150, loss is 4.40716549873352 and perplexity is 82.03660078644988
At time: 373.82428646087646 and batch: 200, loss is 4.372716512680054 and perplexity is 79.25864664053641
At time: 374.4710168838501 and batch: 250, loss is 4.343102750778198 and perplexity is 76.9459133319816
At time: 375.1214144229889 and batch: 300, loss is 4.338738679885864 and perplexity is 76.61084756847856
At time: 375.7845838069916 and batch: 350, loss is 4.388797550201416 and perplexity is 80.54351119996326
At time: 376.4269895553589 and batch: 400, loss is 4.402387380599976 and perplexity is 81.64555519161905
At time: 377.07393288612366 and batch: 450, loss is 4.342649898529053 and perplexity is 76.91107609072357
At time: 377.71328830718994 and batch: 500, loss is 4.355733566284179 and perplexity is 77.92396677050343
At time: 378.35245752334595 and batch: 550, loss is 4.408933372497558 and perplexity is 82.18175941392938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8153839111328125 and perplexity of 123.39417561568973
Finished 46 epochs...
Completing Train Step...
At time: 380.0939087867737 and batch: 50, loss is 4.441789464950562 and perplexity is 84.92677927415353
At time: 380.74994683265686 and batch: 100, loss is 4.422037172317505 and perplexity is 83.26573935545542
At time: 381.38940477371216 and batch: 150, loss is 4.402904005050659 and perplexity is 81.68774617923015
At time: 382.04487466812134 and batch: 200, loss is 4.369499788284302 and perplexity is 79.00410303611152
At time: 382.69692635536194 and batch: 250, loss is 4.340324630737305 and perplexity is 76.73244500570169
At time: 383.361275434494 and batch: 300, loss is 4.336200857162476 and perplexity is 76.41666931786742
At time: 384.01061511039734 and batch: 350, loss is 4.386429386138916 and perplexity is 80.3529966251936
At time: 384.6568593978882 and batch: 400, loss is 4.400405015945434 and perplexity is 81.48386424696429
At time: 385.32201170921326 and batch: 450, loss is 4.340395469665527 and perplexity is 76.7378808423979
At time: 385.9783225059509 and batch: 500, loss is 4.354183597564697 and perplexity is 77.80328061355664
At time: 386.6241366863251 and batch: 550, loss is 4.4064780044555665 and perplexity is 81.98022047561763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8137502467378654 and perplexity of 123.19275551566228
Finished 47 epochs...
Completing Train Step...
At time: 388.3741216659546 and batch: 50, loss is 4.4389677906036376 and perplexity is 84.68748132891655
At time: 389.0514063835144 and batch: 100, loss is 4.418893547058105 and perplexity is 83.00439407501362
At time: 389.7067725658417 and batch: 150, loss is 4.399914064407349 and perplexity is 81.44386943704313
At time: 390.3653562068939 and batch: 200, loss is 4.366781387329102 and perplexity is 78.78962985108335
At time: 391.01945066452026 and batch: 250, loss is 4.337440452575684 and perplexity is 76.51145380571751
At time: 391.68541860580444 and batch: 300, loss is 4.333301725387574 and perplexity is 76.19544815331582
At time: 392.34303879737854 and batch: 350, loss is 4.383674068450928 and perplexity is 80.13190332332621
At time: 392.98938608169556 and batch: 400, loss is 4.397349414825439 and perplexity is 81.23526206808475
At time: 393.6318962574005 and batch: 450, loss is 4.337527208328247 and perplexity is 76.51809190241451
At time: 394.27712512016296 and batch: 500, loss is 4.351255474090576 and perplexity is 77.57579621496801
At time: 394.9324929714203 and batch: 550, loss is 4.403367738723755 and perplexity is 81.72563632260147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.812622719622673 and perplexity of 123.05393062254623
Finished 48 epochs...
Completing Train Step...
At time: 396.66000604629517 and batch: 50, loss is 4.4367914962768555 and perplexity is 84.50337684918219
At time: 397.3373610973358 and batch: 100, loss is 4.415826396942139 and perplexity is 82.75019716741586
At time: 397.9789400100708 and batch: 150, loss is 4.397438325881958 and perplexity is 81.24248510216086
At time: 398.62713146209717 and batch: 200, loss is 4.364371709823608 and perplexity is 78.60000081653196
At time: 399.2691099643707 and batch: 250, loss is 4.334776868820191 and perplexity is 76.30793031151576
At time: 399.91295647621155 and batch: 300, loss is 4.330186548233033 and perplexity is 75.95845516317773
At time: 400.5507752895355 and batch: 350, loss is 4.381667757034302 and perplexity is 79.97129493994382
At time: 401.18478512763977 and batch: 400, loss is 4.395187168121338 and perplexity is 81.05980115367439
At time: 401.81774640083313 and batch: 450, loss is 4.335311460494995 and perplexity is 76.34873480168042
At time: 402.457998752594 and batch: 500, loss is 4.349028949737549 and perplexity is 77.40326396039924
At time: 403.09643626213074 and batch: 550, loss is 4.4014256477355955 and perplexity is 81.56707172407505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.810694268409242 and perplexity of 122.81685578781807
Finished 49 epochs...
Completing Train Step...
At time: 404.9017324447632 and batch: 50, loss is 4.4342528820037845 and perplexity is 84.28912743396386
At time: 405.55936789512634 and batch: 100, loss is 4.413058586120606 and perplexity is 82.52147694937374
At time: 406.20282006263733 and batch: 150, loss is 4.3950395584106445 and perplexity is 81.04783682292464
At time: 406.8447768688202 and batch: 200, loss is 4.362108354568481 and perplexity is 78.42230226496531
At time: 407.4852650165558 and batch: 250, loss is 4.332736291885376 and perplexity is 76.15237687233515
At time: 408.12346720695496 and batch: 300, loss is 4.328815393447876 and perplexity is 75.85437573471714
At time: 408.7832190990448 and batch: 350, loss is 4.379179124832153 and perplexity is 79.77252323752604
At time: 409.43601965904236 and batch: 400, loss is 4.392737579345703 and perplexity is 80.86148097522282
At time: 410.08859872817993 and batch: 450, loss is 4.333110418319702 and perplexity is 76.18087281977107
At time: 410.7419924736023 and batch: 500, loss is 4.3465923309326175 and perplexity is 77.2148913012512
At time: 411.3821175098419 and batch: 550, loss is 4.39900686264038 and perplexity is 81.3700169194135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.809028787815825 and perplexity of 122.61247693970412
Finished Training.
Improved accuracyfrom -172.13528211555618 to -122.61247693970412
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f418b26d518>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 6.283449836288544, 'lr': 28.586967510418816, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.8765580558535844}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9066789150238037 and batch: 50, loss is 6.82207236289978 and perplexity is 917.885231727145
At time: 1.5495095252990723 and batch: 100, loss is 6.30442850112915 and perplexity is 546.9888953055417
At time: 2.1927802562713623 and batch: 150, loss is 6.224546203613281 and perplexity is 504.99382607839044
At time: 2.8444406986236572 and batch: 200, loss is 6.161513986587525 and perplexity is 474.1453814495745
At time: 3.511211633682251 and batch: 250, loss is 6.1292835712432865 and perplexity is 459.10712528183336
At time: 4.17616605758667 and batch: 300, loss is 6.112645177841187 and perplexity is 451.53151805038794
At time: 4.826691627502441 and batch: 350, loss is 6.188365669250488 and perplexity is 487.04945549473194
At time: 5.47992467880249 and batch: 400, loss is 6.178809804916382 and perplexity is 482.4174436598182
At time: 6.132848024368286 and batch: 450, loss is 6.127604665756226 and perplexity is 458.3369744961426
At time: 6.800488233566284 and batch: 500, loss is 6.13834324836731 and perplexity is 463.28538586967846
At time: 7.46100640296936 and batch: 550, loss is 6.205987930297852 and perplexity is 495.70843949883715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.104911641871675 and perplexity of 448.05305057738025
Finished 1 epochs...
Completing Train Step...
At time: 9.212497234344482 and batch: 50, loss is 6.145156297683716 and perplexity is 466.4525488188519
At time: 9.85611081123352 and batch: 100, loss is 6.399681777954101 and perplexity is 601.6535479825845
At time: 10.503889560699463 and batch: 150, loss is 6.458253421783447 and perplexity is 637.945860621072
At time: 11.145190477371216 and batch: 200, loss is 6.4692856884002685 and perplexity is 645.0228150884006
At time: 11.790704250335693 and batch: 250, loss is 6.512979888916016 and perplexity is 673.831371472663
At time: 12.424960851669312 and batch: 300, loss is 6.669099569320679 and perplexity is 787.6860283829102
At time: 13.06049633026123 and batch: 350, loss is 6.824649267196655 and perplexity is 920.2535843232373
At time: 13.700181245803833 and batch: 400, loss is 7.193350868225098 and perplexity is 1330.5542559788967
At time: 14.346680402755737 and batch: 450, loss is 6.5640112400054935 and perplexity is 709.1104094485128
At time: 14.995222091674805 and batch: 500, loss is 6.829919395446777 and perplexity is 925.1162408924176
At time: 15.63615345954895 and batch: 550, loss is 7.191932029724121 and perplexity is 1328.6677530108745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.756256752825798 and perplexity of 859.419148921458
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 17.367568731307983 and batch: 50, loss is 6.2829790115356445 and perplexity is 535.3811973227774
At time: 17.994075775146484 and batch: 100, loss is 6.09332540512085 and perplexity is 442.89175956416295
At time: 18.626807689666748 and batch: 150, loss is 6.054696788787842 and perplexity is 426.1096846310069
At time: 19.261335134506226 and batch: 200, loss is 5.971542568206787 and perplexity is 392.11006098709964
At time: 19.900442361831665 and batch: 250, loss is 5.9123758125305175 and perplexity is 369.5831735162351
At time: 20.539525270462036 and batch: 300, loss is 5.873547248840332 and perplexity is 355.50782094277486
At time: 21.173907995224 and batch: 350, loss is 5.940055208206177 and perplexity is 379.95590564308685
At time: 21.810265064239502 and batch: 400, loss is 5.922184581756592 and perplexity is 373.22616700807623
At time: 22.438931941986084 and batch: 450, loss is 5.840171661376953 and perplexity is 343.83835936542386
At time: 23.11512303352356 and batch: 500, loss is 5.846019992828369 and perplexity is 345.85513168489825
At time: 23.75629234313965 and batch: 550, loss is 5.904830284118653 and perplexity is 366.8049678761323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.926213690575133 and perplexity of 374.7329693457555
Finished 3 epochs...
Completing Train Step...
At time: 25.540498971939087 and batch: 50, loss is 5.927584085464478 and perplexity is 375.2468535235193
At time: 26.180256128311157 and batch: 100, loss is 5.887519836425781 and perplexity is 360.5100507722359
At time: 26.81814432144165 and batch: 150, loss is 5.836482524871826 and perplexity is 342.5722296207298
At time: 27.480963706970215 and batch: 200, loss is 5.736937246322632 and perplexity is 310.113154788065
At time: 28.13077664375305 and batch: 250, loss is 5.690173492431641 and perplexity is 295.94496039763675
At time: 28.773776054382324 and batch: 300, loss is 5.679467496871948 and perplexity is 292.7934749616931
At time: 29.415467023849487 and batch: 350, loss is 5.740600385665894 and perplexity is 311.2512256697346
At time: 30.06021809577942 and batch: 400, loss is 5.727604732513428 and perplexity is 307.23248235407016
At time: 30.701406240463257 and batch: 450, loss is 5.662307758331298 and perplexity is 287.812077457504
At time: 31.34755301475525 and batch: 500, loss is 5.6662804317474365 and perplexity is 288.95773500135414
At time: 31.99367618560791 and batch: 550, loss is 5.721365242004395 and perplexity is 305.32147624840076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.787102232588098 and perplexity of 326.0667883387761
Finished 4 epochs...
Completing Train Step...
At time: 33.694291830062866 and batch: 50, loss is 5.76779278755188 and perplexity is 319.83101799536706
At time: 34.36332821846008 and batch: 100, loss is 5.7378245449066165 and perplexity is 310.3884398631512
At time: 35.010966777801514 and batch: 150, loss is 5.714671068191528 and perplexity is 303.2844270054507
At time: 35.65226125717163 and batch: 200, loss is 5.644869441986084 and perplexity is 282.83662722383224
At time: 36.29695510864258 and batch: 250, loss is 5.6163741302490235 and perplexity is 274.890855624988
At time: 36.94159913063049 and batch: 300, loss is 5.595419120788574 and perplexity is 269.19044976585377
At time: 37.57700872421265 and batch: 350, loss is 5.661455402374267 and perplexity is 287.56686363835325
At time: 38.22928810119629 and batch: 400, loss is 5.657937669754029 and perplexity is 286.5570574570205
At time: 38.866185426712036 and batch: 450, loss is 5.592589330673218 and perplexity is 268.4297740758382
At time: 39.52928400039673 and batch: 500, loss is 5.6000596714019775 and perplexity is 270.4425446304755
At time: 40.16874885559082 and batch: 550, loss is 5.6517122364044186 and perplexity is 284.7786569953729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.734567845121343 and perplexity of 309.3792421167098
Finished 5 epochs...
Completing Train Step...
At time: 41.89064383506775 and batch: 50, loss is 5.698905906677246 and perplexity is 298.5405909522013
At time: 42.537682056427 and batch: 100, loss is 5.677635269165039 and perplexity is 292.25750180683957
At time: 43.19416427612305 and batch: 150, loss is 5.663493680953979 and perplexity is 288.1536027825191
At time: 43.83035349845886 and batch: 200, loss is 5.605840368270874 and perplexity is 272.0104183366866
At time: 44.46808958053589 and batch: 250, loss is 5.581416549682618 and perplexity is 265.4473589589605
At time: 45.121333360672 and batch: 300, loss is 5.559327926635742 and perplexity is 259.648274980817
At time: 45.763370752334595 and batch: 350, loss is 5.623416061401367 and perplexity is 276.8334498846558
At time: 46.4033043384552 and batch: 400, loss is 5.617610445022583 and perplexity is 275.2309174193091
At time: 47.05174374580383 and batch: 450, loss is 5.554455451965332 and perplexity is 258.38622249401027
At time: 47.694708585739136 and batch: 500, loss is 5.5632108783721925 and perplexity is 260.65843663628715
At time: 48.32861018180847 and batch: 550, loss is 5.62162449836731 and perplexity is 276.3379293199543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.7028425500748 and perplexity of 299.71815510280317
Finished 6 epochs...
Completing Train Step...
At time: 50.09592008590698 and batch: 50, loss is 5.659152555465698 and perplexity is 286.90540308892355
At time: 50.759050369262695 and batch: 100, loss is 5.64820987701416 and perplexity is 283.7830043763159
At time: 51.40172457695007 and batch: 150, loss is 5.632784385681152 and perplexity is 279.43910165286286
At time: 52.04770112037659 and batch: 200, loss is 5.5780142879486085 and perplexity is 264.5457721540082
At time: 52.68988656997681 and batch: 250, loss is 5.559673738479614 and perplexity is 259.73807995646354
At time: 53.33065104484558 and batch: 300, loss is 5.534696197509765 and perplexity is 253.3308134132325
At time: 53.971423864364624 and batch: 350, loss is 5.596327905654907 and perplexity is 269.435197167285
At time: 54.61757040023804 and batch: 400, loss is 5.594879179000855 and perplexity is 269.0451418256149
At time: 55.25827622413635 and batch: 450, loss is 5.532046003341675 and perplexity is 252.66032642171652
At time: 55.8982355594635 and batch: 500, loss is 5.540891313552857 and perplexity is 254.90509860432502
At time: 56.53602194786072 and batch: 550, loss is 5.599068250656128 and perplexity is 270.17455514828555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.680795872465093 and perplexity of 293.18267311109463
Finished 7 epochs...
Completing Train Step...
At time: 58.242945432662964 and batch: 50, loss is 5.631272430419922 and perplexity is 279.01692147212384
At time: 58.9011447429657 and batch: 100, loss is 5.625648012161255 and perplexity is 277.45201856411666
At time: 59.543285846710205 and batch: 150, loss is 5.614368028640747 and perplexity is 274.3399494090987
At time: 60.19469594955444 and batch: 200, loss is 5.556298418045044 and perplexity is 258.86285861472425
At time: 60.83613634109497 and batch: 250, loss is 5.537551794052124 and perplexity is 254.05525787635432
At time: 61.46946477890015 and batch: 300, loss is 5.5136680412292485 and perplexity is 248.0593522770083
At time: 62.09980750083923 and batch: 350, loss is 5.577527809143066 and perplexity is 264.41710754160044
At time: 62.731258392333984 and batch: 400, loss is 5.576611480712891 and perplexity is 264.17492560457816
At time: 63.36148428916931 and batch: 450, loss is 5.516629619598389 and perplexity is 248.79508842134157
At time: 63.99248909950256 and batch: 500, loss is 5.527576847076416 and perplexity is 251.53366742372938
At time: 64.63481163978577 and batch: 550, loss is 5.582856779098511 and perplexity is 265.8299394894403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.667675261801862 and perplexity of 289.36106315631764
Finished 8 epochs...
Completing Train Step...
At time: 66.37323546409607 and batch: 50, loss is 5.61218523979187 and perplexity is 273.7417763070184
At time: 67.01987218856812 and batch: 100, loss is 5.609946451187134 and perplexity is 273.1296118466806
At time: 67.6480553150177 and batch: 150, loss is 5.596006641387939 and perplexity is 269.3486511689844
At time: 68.27457427978516 and batch: 200, loss is 5.534415435791016 and perplexity is 253.25969780238412
At time: 68.90310978889465 and batch: 250, loss is 5.5055756092071535 and perplexity is 246.06004935352158
At time: 69.53149032592773 and batch: 300, loss is 5.46879548072815 and perplexity is 237.1743395750857
At time: 70.16214418411255 and batch: 350, loss is 5.520279541015625 and perplexity is 249.70483017607197
At time: 70.81211113929749 and batch: 400, loss is 5.517811393737793 and perplexity is 249.08928182368712
At time: 71.43959450721741 and batch: 450, loss is 5.4538838577270505 and perplexity is 233.6639232866911
At time: 72.06842112541199 and batch: 500, loss is 5.465883111953735 and perplexity is 236.48460530221513
At time: 72.70457696914673 and batch: 550, loss is 5.514685621261597 and perplexity is 248.31190099319585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.60324421334774 and perplexity of 271.30515303509554
Finished 9 epochs...
Completing Train Step...
At time: 74.46463751792908 and batch: 50, loss is 5.548878135681153 and perplexity is 256.9491320867826
At time: 75.1096363067627 and batch: 100, loss is 5.542079553604126 and perplexity is 255.20816707514825
At time: 75.73236656188965 and batch: 150, loss is 5.526709575653076 and perplexity is 251.3156140313704
At time: 76.36134266853333 and batch: 200, loss is 5.470109128952027 and perplexity is 237.48610795717423
At time: 77.0031590461731 and batch: 250, loss is 5.453450593948364 and perplexity is 233.56270710058226
At time: 77.63874650001526 and batch: 300, loss is 5.4264343738555905 and perplexity is 227.33719909684206
At time: 78.26912069320679 and batch: 350, loss is 5.484309024810791 and perplexity is 240.8824426891011
At time: 78.90187168121338 and batch: 400, loss is 5.4881926441192626 and perplexity is 241.8197573030798
At time: 79.5301251411438 and batch: 450, loss is 5.430967903137207 and perplexity is 228.37017869815762
At time: 80.16068482398987 and batch: 500, loss is 5.441343917846679 and perplexity is 230.75208701944086
At time: 80.81056952476501 and batch: 550, loss is 5.493240079879761 and perplexity is 243.04341255644755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.5897203810671545 and perplexity of 267.6607662310913
Finished 10 epochs...
Completing Train Step...
At time: 82.54354619979858 and batch: 50, loss is 5.528279466629028 and perplexity is 251.710461998991
At time: 83.19329118728638 and batch: 100, loss is 5.5226500415802 and perplexity is 250.2974577514713
At time: 83.82861614227295 and batch: 150, loss is 5.508384494781494 and perplexity is 246.75217547304516
At time: 84.46349048614502 and batch: 200, loss is 5.45121548652649 and perplexity is 233.0412523312673
At time: 85.09816884994507 and batch: 250, loss is 5.436663360595703 and perplexity is 229.6745623405043
At time: 85.73262596130371 and batch: 300, loss is 5.410261383056641 and perplexity is 223.69004882853935
At time: 86.37533068656921 and batch: 350, loss is 5.467839918136597 and perplexity is 236.9478128959585
At time: 87.0461208820343 and batch: 400, loss is 5.471324472427368 and perplexity is 237.7749106106263
At time: 87.68322157859802 and batch: 450, loss is 5.412840251922607 and perplexity is 224.26766060348095
At time: 88.3292248249054 and batch: 500, loss is 5.420205926895141 and perplexity is 225.9256418780383
At time: 88.96552515029907 and batch: 550, loss is 5.466664237976074 and perplexity is 236.6694017465588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.566737398188165 and perplexity of 261.5792765039827
Finished 11 epochs...
Completing Train Step...
At time: 90.70577478408813 and batch: 50, loss is 5.499948425292969 and perplexity is 244.67931267492955
At time: 91.3531801700592 and batch: 100, loss is 5.49534909248352 and perplexity is 243.5565350775274
At time: 91.99136257171631 and batch: 150, loss is 5.483134336471558 and perplexity is 240.59964702345258
At time: 92.63160157203674 and batch: 200, loss is 5.418000144958496 and perplexity is 225.4278483919321
At time: 93.286630153656 and batch: 250, loss is 5.398560819625854 and perplexity is 221.08800161824692
At time: 93.93133163452148 and batch: 300, loss is 5.373947362899781 and perplexity is 215.7126855831701
At time: 94.56174063682556 and batch: 350, loss is 5.431517200469971 and perplexity is 228.49565628729687
At time: 95.20176696777344 and batch: 400, loss is 5.435128946304321 and perplexity is 229.32241664746593
At time: 95.83227586746216 and batch: 450, loss is 5.372350511550903 and perplexity is 215.36849937047324
At time: 96.48012900352478 and batch: 500, loss is 5.389835290908813 and perplexity is 219.16728373676523
At time: 97.11060857772827 and batch: 550, loss is 5.44058837890625 and perplexity is 230.5778106766844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.5422304843334445 and perplexity of 255.24668873691098
Finished 12 epochs...
Completing Train Step...
At time: 98.80013465881348 and batch: 50, loss is 5.4687466812133785 and perplexity is 237.16276586479626
At time: 99.44905018806458 and batch: 100, loss is 5.471778612136841 and perplexity is 237.88291816285383
At time: 100.08143877983093 and batch: 150, loss is 5.453145151138306 and perplexity is 233.49137794504392
At time: 100.71325826644897 and batch: 200, loss is 5.399099464416504 and perplexity is 221.20712159739182
At time: 101.34410190582275 and batch: 250, loss is 5.3775733470916744 and perplexity is 216.49627615606244
At time: 101.97463154792786 and batch: 300, loss is 5.353138523101807 and perplexity is 211.27033524131818
At time: 102.61397528648376 and batch: 350, loss is 5.410621652603149 and perplexity is 223.77065205956217
At time: 103.26663661003113 and batch: 400, loss is 5.415985269546509 and perplexity is 224.9740966431597
At time: 103.89770483970642 and batch: 450, loss is 5.358001861572266 and perplexity is 212.30031693506322
At time: 104.5289511680603 and batch: 500, loss is 5.371120920181275 and perplexity is 215.10384686292105
At time: 105.1705060005188 and batch: 550, loss is 5.420376472473144 and perplexity is 225.96417578301825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.523219331781915 and perplexity of 250.4399902089996
Finished 13 epochs...
Completing Train Step...
At time: 106.87965106964111 and batch: 50, loss is 5.453094520568848 and perplexity is 233.47955644288226
At time: 107.52603363990784 and batch: 100, loss is 5.452815637588501 and perplexity is 233.41445204700747
At time: 108.15771245956421 and batch: 150, loss is 5.433763408660889 and perplexity is 229.00948196572224
At time: 108.79504728317261 and batch: 200, loss is 5.377792921066284 and perplexity is 216.54381832322647
At time: 109.43577337265015 and batch: 250, loss is 5.359478330612182 and perplexity is 212.614003297298
At time: 110.07839846611023 and batch: 300, loss is 5.33826512336731 and perplexity is 208.1512800694707
At time: 110.72110438346863 and batch: 350, loss is 5.39286054611206 and perplexity is 219.83132464242277
At time: 111.37133955955505 and batch: 400, loss is 5.396319398880005 and perplexity is 220.59300533946265
At time: 112.00930833816528 and batch: 450, loss is 5.339376773834228 and perplexity is 208.38280019803702
At time: 112.6455192565918 and batch: 500, loss is 5.35038119316101 and perplexity is 210.6885956129768
At time: 113.28050184249878 and batch: 550, loss is 5.402790460586548 and perplexity is 222.02510489395544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.5066515334109045 and perplexity of 246.32493378843176
Finished 14 epochs...
Completing Train Step...
At time: 114.98569416999817 and batch: 50, loss is 5.433729047775269 and perplexity is 229.00161313229728
At time: 115.64576482772827 and batch: 100, loss is 5.4316928005218506 and perplexity is 228.53578365947644
At time: 116.28064346313477 and batch: 150, loss is 5.416021203994751 and perplexity is 224.98218110844581
At time: 116.91522121429443 and batch: 200, loss is 5.362246150970459 and perplexity is 213.2032958157881
At time: 117.55227518081665 and batch: 250, loss is 5.3384514331817625 and perplexity is 208.19006430866852
At time: 118.1912841796875 and batch: 300, loss is 5.320013818740844 and perplexity is 204.38670634038044
At time: 118.83715391159058 and batch: 350, loss is 5.371036329269409 and perplexity is 215.0856518019484
At time: 119.49796462059021 and batch: 400, loss is 5.376810178756714 and perplexity is 216.33111608394805
At time: 120.1365270614624 and batch: 450, loss is 5.315532884597778 and perplexity is 203.47291182506146
At time: 120.77470564842224 and batch: 500, loss is 5.326037845611572 and perplexity is 205.62165329363972
At time: 121.41673183441162 and batch: 550, loss is 5.3803652572631835 and perplexity is 217.10155886586392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.493243927651263 and perplexity of 243.04434773376337
Finished 15 epochs...
Completing Train Step...
At time: 123.1173484325409 and batch: 50, loss is 5.41422924041748 and perplexity is 224.57938224259968
At time: 123.77548313140869 and batch: 100, loss is 5.411783256530762 and perplexity is 224.0307359558719
At time: 124.40867042541504 and batch: 150, loss is 5.392384195327759 and perplexity is 219.72663275552625
At time: 125.04286241531372 and batch: 200, loss is 5.340904760360718 and perplexity is 208.7014496931623
At time: 125.6959400177002 and batch: 250, loss is 5.315499496459961 and perplexity is 203.46611835685064
At time: 126.32821726799011 and batch: 300, loss is 5.295603151321411 and perplexity is 199.45789298503902
At time: 126.96254467964172 and batch: 350, loss is 5.34789288520813 and perplexity is 210.16498922207188
At time: 127.60343837738037 and batch: 400, loss is 5.353582458496094 and perplexity is 211.3641464424103
At time: 128.2452232837677 and batch: 450, loss is 5.2937700557708744 and perplexity is 199.0926025173428
At time: 128.88584208488464 and batch: 500, loss is 5.303026313781738 and perplexity is 200.9440103498154
At time: 129.5308873653412 and batch: 550, loss is 5.357154197692871 and perplexity is 212.12043387577035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4767209316821805 and perplexity of 239.06152167728746
Finished 16 epochs...
Completing Train Step...
At time: 131.2500159740448 and batch: 50, loss is 5.387029304504394 and perplexity is 218.55316532520465
At time: 131.89776992797852 and batch: 100, loss is 5.380037059783936 and perplexity is 217.030318372618
At time: 132.53168416023254 and batch: 150, loss is 5.35622462272644 and perplexity is 211.9233436498333
At time: 133.17646837234497 and batch: 200, loss is 5.2973674774169925 and perplexity is 199.81011237416493
At time: 133.8145260810852 and batch: 250, loss is 5.2716320133209225 and perplexity is 194.73351093710625
At time: 134.45549416542053 and batch: 300, loss is 5.254443292617798 and perplexity is 191.4148941023122
At time: 135.09908318519592 and batch: 350, loss is 5.306784524917602 and perplexity is 201.70062122837155
At time: 135.7509627342224 and batch: 400, loss is 5.317437677383423 and perplexity is 203.86085491788646
At time: 136.38553547859192 and batch: 450, loss is 5.257283353805542 and perplexity is 191.95929681639086
At time: 137.0223240852356 and batch: 500, loss is 5.268937749862671 and perplexity is 194.20955371084838
At time: 137.65788745880127 and batch: 550, loss is 5.322292776107788 and perplexity is 204.85302609008968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.461576421210107 and perplexity of 235.46832920576676
Finished 17 epochs...
Completing Train Step...
At time: 139.34756922721863 and batch: 50, loss is 5.3599396991729735 and perplexity is 212.71211934609303
At time: 139.99741220474243 and batch: 100, loss is 5.350014753341675 and perplexity is 210.6114050657732
At time: 140.6309278011322 and batch: 150, loss is 5.337841720581054 and perplexity is 208.06316689252526
At time: 141.26595306396484 and batch: 200, loss is 5.281595239639282 and perplexity is 196.68338235383473
At time: 141.91719722747803 and batch: 250, loss is 5.25868239402771 and perplexity is 192.22804354356137
At time: 142.55149936676025 and batch: 300, loss is 5.236475305557251 and perplexity is 188.00626853723722
At time: 143.18768572807312 and batch: 350, loss is 5.289700059890747 and perplexity is 198.28394318169322
At time: 143.82318329811096 and batch: 400, loss is 5.303215532302857 and perplexity is 200.98203627577266
At time: 144.45764112472534 and batch: 450, loss is 5.24234076499939 and perplexity is 189.11225206014143
At time: 145.09281182289124 and batch: 500, loss is 5.250775957107544 and perplexity is 190.71419709481455
At time: 145.72759866714478 and batch: 550, loss is 5.304537582397461 and perplexity is 201.24792031312674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4434544989403255 and perplexity of 231.23962232204028
Finished 18 epochs...
Completing Train Step...
At time: 147.42042422294617 and batch: 50, loss is 5.339887180328369 and perplexity is 208.48918728054463
At time: 148.06831860542297 and batch: 100, loss is 5.325695400238037 and perplexity is 205.5512511649003
At time: 148.70224118232727 and batch: 150, loss is 5.303483390808106 and perplexity is 201.03587823428273
At time: 149.3348047733307 and batch: 200, loss is 5.242736797332764 and perplexity is 189.18716145888743
At time: 149.97765517234802 and batch: 250, loss is 5.2140178203582765 and perplexity is 183.8311770421647
At time: 150.6155788898468 and batch: 300, loss is 5.184100494384766 and perplexity is 178.41289419016536
At time: 151.2525224685669 and batch: 350, loss is 5.228696479797363 and perplexity is 186.54947395402976
At time: 151.90212154388428 and batch: 400, loss is 5.235348014831543 and perplexity is 187.7944502271798
At time: 152.53680396080017 and batch: 450, loss is 5.175550088882447 and perplexity is 176.89389488243685
At time: 153.1711118221283 and batch: 500, loss is 5.182333097457886 and perplexity is 178.09784627862263
At time: 153.80545473098755 and batch: 550, loss is 5.233120756149292 and perplexity is 187.37664885601333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.382199226541722 and perplexity of 217.5000817828393
Finished 19 epochs...
Completing Train Step...
At time: 155.49202513694763 and batch: 50, loss is 5.2689011192321775 and perplexity is 194.20243982274204
At time: 156.13824009895325 and batch: 100, loss is 5.260294885635376 and perplexity is 192.53825969378588
At time: 156.77092289924622 and batch: 150, loss is 5.2424229145050045 and perplexity is 189.12778817628728
At time: 157.40493988990784 and batch: 200, loss is 5.186800994873047 and perplexity is 178.8953494401604
At time: 158.0373456478119 and batch: 250, loss is 5.167717981338501 and perplexity is 175.5138542417267
At time: 158.67131233215332 and batch: 300, loss is 5.1443401718139645 and perplexity is 171.45831431015918
At time: 159.3055510520935 and batch: 350, loss is 5.1928806972503665 and perplexity is 179.98629286675876
At time: 159.93953680992126 and batch: 400, loss is 5.208078641891479 and perplexity is 182.7426066806116
At time: 160.5728657245636 and batch: 450, loss is 5.147543659210205 and perplexity is 172.0084595802709
At time: 161.21004271507263 and batch: 500, loss is 5.15081262588501 and perplexity is 172.57166955820253
At time: 161.84609603881836 and batch: 550, loss is 5.202547769546509 and perplexity is 181.7346706034034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.356635235725565 and perplexity of 212.0103799974589
Finished 20 epochs...
Completing Train Step...
At time: 163.52466011047363 and batch: 50, loss is 5.237960777282715 and perplexity is 188.28575406589115
At time: 164.17414236068726 and batch: 100, loss is 5.225672121047974 and perplexity is 185.98613372115813
At time: 164.80767488479614 and batch: 150, loss is 5.20920280456543 and perplexity is 182.94815461099128
At time: 165.44072461128235 and batch: 200, loss is 5.15700668334961 and perplexity is 173.64390571424448
At time: 166.0743293762207 and batch: 250, loss is 5.137736415863037 and perplexity is 170.32977584425802
At time: 166.7052903175354 and batch: 300, loss is 5.116960411071777 and perplexity is 166.82751111586145
At time: 167.3382592201233 and batch: 350, loss is 5.169044046401978 and perplexity is 175.74675141626798
At time: 168.00571751594543 and batch: 400, loss is 5.179385881423951 and perplexity is 177.57372617742303
At time: 168.65442276000977 and batch: 450, loss is 5.122280778884888 and perplexity is 167.71746016521868
At time: 169.29594564437866 and batch: 500, loss is 5.125319242477417 and perplexity is 168.22783855217355
At time: 169.93458914756775 and batch: 550, loss is 5.1791832160949705 and perplexity is 177.53774178630644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.347611447598072 and perplexity of 210.10584921228926
Finished 21 epochs...
Completing Train Step...
At time: 171.62730526924133 and batch: 50, loss is 5.220441370010376 and perplexity is 185.01582648935783
At time: 172.2769706249237 and batch: 100, loss is 5.207206449508667 and perplexity is 182.58328945878284
At time: 172.9124596118927 and batch: 150, loss is 5.192387952804565 and perplexity is 179.89762746711386
At time: 173.54755997657776 and batch: 200, loss is 5.141242227554321 and perplexity is 170.9279679254988
At time: 174.1947741508484 and batch: 250, loss is 5.118529405593872 and perplexity is 167.0894680176697
At time: 174.8299684524536 and batch: 300, loss is 5.1022981262207034 and perplexity is 164.39928380917377
At time: 175.4651803970337 and batch: 350, loss is 5.154229755401611 and perplexity is 173.16237799279557
At time: 176.11224794387817 and batch: 400, loss is 5.169257040023804 and perplexity is 175.78418834014778
At time: 176.74994707107544 and batch: 450, loss is 5.110046930313111 and perplexity is 165.6781300178006
At time: 177.389422416687 and batch: 500, loss is 5.11545952796936 and perplexity is 166.57731033144205
At time: 178.0271565914154 and batch: 550, loss is 5.167029876708984 and perplexity is 175.39312388839986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.336160862699468 and perplexity of 207.71373603266363
Finished 22 epochs...
Completing Train Step...
At time: 179.71122407913208 and batch: 50, loss is 5.210425758361817 and perplexity is 183.17202861709518
At time: 180.36019897460938 and batch: 100, loss is 5.198156175613403 and perplexity is 180.93831564024003
At time: 180.99373579025269 and batch: 150, loss is 5.179747991561889 and perplexity is 177.63803906737542
At time: 181.64308214187622 and batch: 200, loss is 5.13000244140625 and perplexity is 169.01753068483933
At time: 182.27521443367004 and batch: 250, loss is 5.107676973342896 and perplexity is 165.2859448932529
At time: 182.90864396095276 and batch: 300, loss is 5.092800769805908 and perplexity is 162.84531618928298
At time: 183.54292178153992 and batch: 350, loss is 5.14375039100647 and perplexity is 171.35722130138217
At time: 184.1896526813507 and batch: 400, loss is 5.159575748443603 and perplexity is 174.09058173541146
At time: 184.82306694984436 and batch: 450, loss is 5.103787136077881 and perplexity is 164.6442583027343
At time: 185.45793104171753 and batch: 500, loss is 5.110614156723022 and perplexity is 165.77213368685287
At time: 186.09235310554504 and batch: 550, loss is 5.162172594070435 and perplexity is 174.54325560896018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.330751784304355 and perplexity of 206.59322933699355
Finished 23 epochs...
Completing Train Step...
At time: 187.78387594223022 and batch: 50, loss is 5.197498168945312 and perplexity is 180.8192961841279
At time: 188.4322657585144 and batch: 100, loss is 5.189963502883911 and perplexity is 179.46200296674988
At time: 189.09167790412903 and batch: 150, loss is 5.173397464752197 and perplexity is 176.51351836653456
At time: 189.73694396018982 and batch: 200, loss is 5.123545999526978 and perplexity is 167.92979405414314
At time: 190.37357139587402 and batch: 250, loss is 5.102722682952881 and perplexity is 164.46909545033262
At time: 191.02205634117126 and batch: 300, loss is 5.08591778755188 and perplexity is 161.72830336742715
At time: 191.6663908958435 and batch: 350, loss is 5.1359760951995845 and perplexity is 170.0302045683493
At time: 192.29899787902832 and batch: 400, loss is 5.1546499538421635 and perplexity is 173.23515584348928
At time: 192.9328899383545 and batch: 450, loss is 5.097712411880493 and perplexity is 163.64712157454957
At time: 193.56729340553284 and batch: 500, loss is 5.105524749755859 and perplexity is 164.93059511729967
At time: 194.21052742004395 and batch: 550, loss is 5.156258945465088 and perplexity is 173.5141141186195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3252576462765955 and perplexity of 205.46128998229221
Finished 24 epochs...
Completing Train Step...
At time: 195.90059566497803 and batch: 50, loss is 5.194229240417481 and perplexity is 180.22917588448712
At time: 196.54803919792175 and batch: 100, loss is 5.182925930023194 and perplexity is 178.20345978417728
At time: 197.18268251419067 and batch: 150, loss is 5.166792755126953 and perplexity is 175.3515393238798
At time: 197.81743574142456 and batch: 200, loss is 5.116238327026367 and perplexity is 166.70709111366267
At time: 198.45073318481445 and batch: 250, loss is 5.096677703857422 and perplexity is 163.4778821566949
At time: 199.08767127990723 and batch: 300, loss is 5.083195285797119 and perplexity is 161.28859660052984
At time: 199.72122859954834 and batch: 350, loss is 5.132682399749756 and perplexity is 169.47109812585404
At time: 200.37074756622314 and batch: 400, loss is 5.144584550857544 and perplexity is 171.50022024928208
At time: 201.00511002540588 and batch: 450, loss is 5.093923768997192 and perplexity is 163.02829407044436
At time: 201.64236855506897 and batch: 500, loss is 5.098622150421143 and perplexity is 163.79606540781268
At time: 202.27972221374512 and batch: 550, loss is 5.153194236755371 and perplexity is 172.98315793042966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.325069995636635 and perplexity of 205.42273865694358
Finished 25 epochs...
Completing Train Step...
At time: 203.95590162277222 and batch: 50, loss is 5.185464897155762 and perplexity is 178.65648737920154
At time: 204.61036729812622 and batch: 100, loss is 5.177577505111694 and perplexity is 177.25289623539055
At time: 205.25488209724426 and batch: 150, loss is 5.161893463134765 and perplexity is 174.49454198576316
At time: 205.88980674743652 and batch: 200, loss is 5.109032831192017 and perplexity is 165.51020113442303
At time: 206.5217764377594 and batch: 250, loss is 5.090386152267456 and perplexity is 162.4525813757579
At time: 207.15425491333008 and batch: 300, loss is 5.075578050613403 and perplexity is 160.06469073499323
At time: 207.78545212745667 and batch: 350, loss is 5.125100173950195 and perplexity is 168.19098916376225
At time: 208.41746377944946 and batch: 400, loss is 5.137503871917724 and perplexity is 170.29017129125702
At time: 209.05329489707947 and batch: 450, loss is 5.0848473072052 and perplexity is 161.55526902862363
At time: 209.6876449584961 and batch: 500, loss is 5.0899747943878175 and perplexity is 162.38576896918846
At time: 210.33415460586548 and batch: 550, loss is 5.140910053253174 and perplexity is 170.87119947623862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.327524388090093 and perplexity of 205.92754592062596
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 212.00516605377197 and batch: 50, loss is 5.147541542053222 and perplexity is 172.00809541174505
At time: 212.66287183761597 and batch: 100, loss is 5.112314481735229 and perplexity is 166.05423996029737
At time: 213.30484080314636 and batch: 150, loss is 5.070738153457642 and perplexity is 159.29186579876506
At time: 213.9491205215454 and batch: 200, loss is 5.020152912139893 and perplexity is 151.43445819076894
At time: 214.5927619934082 and batch: 250, loss is 4.983884372711182 and perplexity is 146.04055727856706
At time: 215.22716116905212 and batch: 300, loss is 4.962988958358765 and perplexity is 143.0206404075962
At time: 215.85961437225342 and batch: 350, loss is 5.007782039642334 and perplexity is 149.57262183118613
At time: 216.50679469108582 and batch: 400, loss is 5.009534645080566 and perplexity is 149.83499347144098
At time: 217.14022016525269 and batch: 450, loss is 4.939565534591675 and perplexity is 139.70953741107544
At time: 217.7733063697815 and batch: 500, loss is 4.935388879776001 and perplexity is 139.1272357817978
At time: 218.40718150138855 and batch: 550, loss is 4.9931081104278565 and perplexity is 147.3938285903415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.22484897045379 and perplexity of 185.8331021173443
Finished 27 epochs...
Completing Train Step...
At time: 220.0908043384552 and batch: 50, loss is 5.073455572128296 and perplexity is 159.72531715664613
At time: 220.74073719978333 and batch: 100, loss is 5.065797357559204 and perplexity is 158.50677827726915
At time: 221.38425421714783 and batch: 150, loss is 5.0358513069152835 and perplexity is 153.83049384856767
At time: 222.02086663246155 and batch: 200, loss is 4.990579528808594 and perplexity is 147.02160206547583
At time: 222.65757489204407 and batch: 250, loss is 4.959560737609864 and perplexity is 142.53117356021033
At time: 223.2940230369568 and batch: 300, loss is 4.941222686767578 and perplexity is 139.94124931290537
At time: 223.9290828704834 and batch: 350, loss is 4.9896706199646 and perplexity is 146.8880335411022
At time: 224.56582617759705 and batch: 400, loss is 4.997965612411499 and perplexity is 148.11153612677538
At time: 225.20118880271912 and batch: 450, loss is 4.940416641235352 and perplexity is 139.8284957424832
At time: 225.83669805526733 and batch: 500, loss is 4.941694250106812 and perplexity is 140.00725603768055
At time: 226.48124623298645 and batch: 550, loss is 4.997592906951905 and perplexity is 148.05634443439342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.218175685152095 and perplexity of 184.59711344775354
Finished 28 epochs...
Completing Train Step...
At time: 228.1682116985321 and batch: 50, loss is 5.060082216262817 and perplexity is 157.6034733593067
At time: 228.81809043884277 and batch: 100, loss is 5.052696943283081 and perplexity is 156.44381614937814
At time: 229.45531702041626 and batch: 150, loss is 5.023927392959595 and perplexity is 152.00712472819868
At time: 230.1053183078766 and batch: 200, loss is 4.979209384918213 and perplexity is 145.35941286747774
At time: 230.7461929321289 and batch: 250, loss is 4.949790582656861 and perplexity is 141.1454025310566
At time: 231.3834936618805 and batch: 300, loss is 4.9332677936553955 and perplexity is 138.83244767898063
At time: 232.02067732810974 and batch: 350, loss is 4.984017553329468 and perplexity is 146.06000834550414
At time: 232.67074012756348 and batch: 400, loss is 4.9969412708282475 and perplexity is 147.95989699975243
At time: 233.31710124015808 and batch: 450, loss is 4.942209014892578 and perplexity is 140.0793453957801
At time: 233.9543833732605 and batch: 500, loss is 4.943987264633178 and perplexity is 140.3286630642345
At time: 234.59203386306763 and batch: 550, loss is 4.998891859054566 and perplexity is 148.2487874944088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2156430508228055 and perplexity of 184.13018798645518
Finished 29 epochs...
Completing Train Step...
At time: 236.2750644683838 and batch: 50, loss is 5.053082227706909 and perplexity is 156.50410312801586
At time: 236.9249222278595 and batch: 100, loss is 5.04572039604187 and perplexity is 155.35617687030305
At time: 237.56103205680847 and batch: 150, loss is 5.017279014587403 and perplexity is 150.99987584369526
At time: 238.19699788093567 and batch: 200, loss is 4.973217964172363 and perplexity is 144.49110725521973
At time: 238.8295168876648 and batch: 250, loss is 4.945100898742676 and perplexity is 140.48502489873943
At time: 239.46484422683716 and batch: 300, loss is 4.929651985168457 and perplexity is 138.33136259618308
At time: 240.10044646263123 and batch: 350, loss is 4.981459655761719 and perplexity is 145.68687922189426
At time: 240.73473954200745 and batch: 400, loss is 4.995904788970948 and perplexity is 147.80661869971496
At time: 241.3787715435028 and batch: 450, loss is 4.942726497650146 and perplexity is 140.1518528007618
At time: 242.0162546634674 and batch: 500, loss is 4.944730920791626 and perplexity is 140.4330581509424
At time: 242.65148949623108 and batch: 550, loss is 4.9986624431610105 and perplexity is 148.2147807673483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.21290426051363 and perplexity of 183.6265839596786
Finished 30 epochs...
Completing Train Step...
At time: 244.33836150169373 and batch: 50, loss is 5.04821816444397 and perplexity is 155.74470564541204
At time: 244.98556685447693 and batch: 100, loss is 5.0406920433044435 and perplexity is 154.57695196375485
At time: 245.6218147277832 and batch: 150, loss is 5.013396863937378 and perplexity is 150.41480797314287
At time: 246.2649474143982 and batch: 200, loss is 4.969289379119873 and perplexity is 143.92457521433698
At time: 246.8996958732605 and batch: 250, loss is 4.941028995513916 and perplexity is 139.91414654175657
At time: 247.53663277626038 and batch: 300, loss is 4.926867866516114 and perplexity is 137.94676729727428
At time: 248.17710900306702 and batch: 350, loss is 4.979783420562744 and perplexity is 145.44287830550812
At time: 248.85162496566772 and batch: 400, loss is 4.995189247131347 and perplexity is 147.70089470934562
At time: 249.50394415855408 and batch: 450, loss is 4.942450618743896 and perplexity is 140.11319319383242
At time: 250.14329624176025 and batch: 500, loss is 4.944652967453003 and perplexity is 140.42211135188177
At time: 250.7749047279358 and batch: 550, loss is 4.997423124313355 and perplexity is 148.0312091714073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.211695894281915 and perplexity of 183.40482980347642
Finished 31 epochs...
Completing Train Step...
At time: 252.47964692115784 and batch: 50, loss is 5.044218988418579 and perplexity is 155.12309893830079
At time: 253.14123821258545 and batch: 100, loss is 5.037320079803467 and perplexity is 154.05660191736052
At time: 253.77885913848877 and batch: 150, loss is 5.010153665542602 and perplexity is 149.92777311161404
At time: 254.41585493087769 and batch: 200, loss is 4.965811996459961 and perplexity is 143.42496356604252
At time: 255.05272841453552 and batch: 250, loss is 4.937902669906617 and perplexity is 139.47741240487417
At time: 255.68907761573792 and batch: 300, loss is 4.923959617614746 and perplexity is 137.5461665685032
At time: 256.3268897533417 and batch: 350, loss is 4.977872514724732 and perplexity is 145.16521603797256
At time: 256.97342133522034 and batch: 400, loss is 4.994569387435913 and perplexity is 147.60936924713883
At time: 257.61535000801086 and batch: 450, loss is 4.941635274887085 and perplexity is 139.99899932246564
At time: 258.25389790534973 and batch: 500, loss is 4.944347848892212 and perplexity is 140.37927249516042
At time: 258.8917739391327 and batch: 550, loss is 4.9963255214691165 and perplexity is 147.86881883153728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.21119625010389 and perplexity of 183.3132155372167
Finished 32 epochs...
Completing Train Step...
At time: 260.5945529937744 and batch: 50, loss is 5.040992279052734 and perplexity is 154.6233684581929
At time: 261.2446258068085 and batch: 100, loss is 5.034640054702759 and perplexity is 153.6442791218066
At time: 261.88955783843994 and batch: 150, loss is 5.008105630874634 and perplexity is 149.62103005201837
At time: 262.52858448028564 and batch: 200, loss is 4.963352336883545 and perplexity is 143.07262048056973
At time: 263.1638603210449 and batch: 250, loss is 4.9349894523620605 and perplexity is 139.07167564666003
At time: 263.7995183467865 and batch: 300, loss is 4.922380132675171 and perplexity is 137.32908595282407
At time: 264.43602776527405 and batch: 350, loss is 4.976239995956421 and perplexity is 144.9284244342431
At time: 265.0851671695709 and batch: 400, loss is 4.993992767333984 and perplexity is 147.52427925225632
At time: 265.72524666786194 and batch: 450, loss is 4.941272630691528 and perplexity is 139.9482387025559
At time: 266.3773832321167 and batch: 500, loss is 4.9438371753692625 and perplexity is 140.30760281898677
At time: 267.021005153656 and batch: 550, loss is 4.995643873214721 and perplexity is 147.76805865470206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.210473243226397 and perplexity of 183.18072672258143
Finished 33 epochs...
Completing Train Step...
At time: 268.7881157398224 and batch: 50, loss is 5.038457536697388 and perplexity is 154.23193435890875
At time: 269.43645548820496 and batch: 100, loss is 5.0328630924224855 and perplexity is 153.37150146280953
At time: 270.083190202713 and batch: 150, loss is 5.0058225440979 and perplexity is 149.27982190889705
At time: 270.72158575057983 and batch: 200, loss is 4.960871543884277 and perplexity is 142.71812681979665
At time: 271.3569710254669 and batch: 250, loss is 4.932351751327515 and perplexity is 138.70532951212323
At time: 271.99146366119385 and batch: 300, loss is 4.920736360549927 and perplexity is 137.10353365842332
At time: 272.62582540512085 and batch: 350, loss is 4.974788799285888 and perplexity is 144.7182573210299
At time: 273.2602481842041 and batch: 400, loss is 4.992882156372071 and perplexity is 147.36052811930364
At time: 273.89274549484253 and batch: 450, loss is 4.939603242874146 and perplexity is 139.71480571710478
At time: 274.52761363983154 and batch: 500, loss is 4.942842350006104 and perplexity is 140.16809066367975
At time: 275.1632444858551 and batch: 550, loss is 4.994000959396362 and perplexity is 147.52548778530436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.208690562146775 and perplexity of 182.8544648037803
Finished 34 epochs...
Completing Train Step...
At time: 276.8505029678345 and batch: 50, loss is 5.035863838195801 and perplexity is 153.83242155371647
At time: 277.4977526664734 and batch: 100, loss is 5.030048561096192 and perplexity is 152.94043947067982
At time: 278.1314625740051 and batch: 150, loss is 5.002400035858154 and perplexity is 148.76978379107996
At time: 278.76548981666565 and batch: 200, loss is 4.958624086380005 and perplexity is 142.39773406409174
At time: 279.399329662323 and batch: 250, loss is 4.929780626296997 and perplexity is 138.34915884342013
At time: 280.03436517715454 and batch: 300, loss is 4.9185960006713865 and perplexity is 136.81039657710716
At time: 280.6724934577942 and batch: 350, loss is 4.973315477371216 and perplexity is 144.50519773228652
At time: 281.32212495803833 and batch: 400, loss is 4.991633186340332 and perplexity is 147.17659412375585
At time: 281.9577100276947 and batch: 450, loss is 4.938932189941406 and perplexity is 139.62108113757526
At time: 282.5952546596527 and batch: 500, loss is 4.942146425247192 and perplexity is 140.0705781535981
At time: 283.2384042739868 and batch: 550, loss is 4.992340812683105 and perplexity is 147.28077701572957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2077013380984045 and perplexity of 182.67367020776882
Finished 35 epochs...
Completing Train Step...
At time: 284.9298782348633 and batch: 50, loss is 5.0335417652130126 and perplexity is 153.47562585689457
At time: 285.5811228752136 and batch: 100, loss is 5.0284069061279295 and perplexity is 152.68957001526027
At time: 286.2167429924011 and batch: 150, loss is 5.0002907943725585 and perplexity is 148.45632308967538
At time: 286.85221099853516 and batch: 200, loss is 4.956508302688599 and perplexity is 142.09676976062153
At time: 287.48956966400146 and batch: 250, loss is 4.927920646667481 and perplexity is 138.09207138904137
At time: 288.1277222633362 and batch: 300, loss is 4.917134838104248 and perplexity is 136.61064032055555
At time: 288.76437997817993 and batch: 350, loss is 4.9718984031677245 and perplexity is 144.30056816619242
At time: 289.40217328071594 and batch: 400, loss is 4.99080340385437 and perplexity is 147.05452021800733
At time: 290.0454611778259 and batch: 450, loss is 4.937710952758789 and perplexity is 139.45067475629634
At time: 290.6829524040222 and batch: 500, loss is 4.940845775604248 and perplexity is 139.8885138327489
At time: 291.3199393749237 and batch: 550, loss is 4.991067848205566 and perplexity is 147.09341309747134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.206521541514295 and perplexity of 182.45827951923414
Finished 36 epochs...
Completing Train Step...
At time: 293.0292704105377 and batch: 50, loss is 5.031476860046387 and perplexity is 153.15904021626304
At time: 293.69112968444824 and batch: 100, loss is 5.026330137252808 and perplexity is 152.37279811356214
At time: 294.3374104499817 and batch: 150, loss is 4.998348808288574 and perplexity is 148.16830273243636
At time: 294.97509813308716 and batch: 200, loss is 4.954873170852661 and perplexity is 141.86461266405743
At time: 295.61170649528503 and batch: 250, loss is 4.926389827728271 and perplexity is 137.88083915120973
At time: 296.2489368915558 and batch: 300, loss is 4.915752763748169 and perplexity is 136.42196466972317
At time: 296.88561964035034 and batch: 350, loss is 4.970696258544922 and perplexity is 144.12720224044008
At time: 297.53539752960205 and batch: 400, loss is 4.989824304580688 and perplexity is 146.91060970690518
At time: 298.17090344429016 and batch: 450, loss is 4.936682796478271 and perplexity is 139.30737135099514
At time: 298.8079676628113 and batch: 500, loss is 4.93968879699707 and perplexity is 139.7267594061039
At time: 299.44402956962585 and batch: 550, loss is 4.989865064620972 and perplexity is 146.9165979113138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.205605364860372 and perplexity of 182.2911920557278
Finished 37 epochs...
Completing Train Step...
At time: 301.1430022716522 and batch: 50, loss is 5.029589710235595 and perplexity is 152.870278716301
At time: 301.8000159263611 and batch: 100, loss is 5.0245782661437985 and perplexity is 152.10609429431653
At time: 302.4333519935608 and batch: 150, loss is 4.996210193634033 and perplexity is 147.85176642411258
At time: 303.0675449371338 and batch: 200, loss is 4.953364763259888 and perplexity is 141.65078431586463
At time: 303.70231199264526 and batch: 250, loss is 4.925018672943115 and perplexity is 137.6919127321075
At time: 304.3365604877472 and batch: 300, loss is 4.914252080917358 and perplexity is 136.21739210725625
At time: 304.97143149375916 and batch: 350, loss is 4.969300737380982 and perplexity is 143.9262099565261
At time: 305.60634446144104 and batch: 400, loss is 4.988659954071045 and perplexity is 146.7396538092281
At time: 306.23935103416443 and batch: 450, loss is 4.936179351806641 and perplexity is 139.2372554483894
At time: 306.87312960624695 and batch: 500, loss is 4.938633031845093 and perplexity is 139.57931860784788
At time: 307.5072453022003 and batch: 550, loss is 4.98879602432251 and perplexity is 146.75962206933414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2052342840965755 and perplexity of 182.22355985022764
Finished 38 epochs...
Completing Train Step...
At time: 309.1955442428589 and batch: 50, loss is 5.027988739013672 and perplexity is 152.62573360646343
At time: 309.8481307029724 and batch: 100, loss is 5.022953662872315 and perplexity is 151.85918285678477
At time: 310.4870584011078 and batch: 150, loss is 4.9947357082366945 and perplexity is 147.63392179737818
At time: 311.12917852401733 and batch: 200, loss is 4.951763029098511 and perplexity is 141.4240790249133
At time: 311.7676010131836 and batch: 250, loss is 4.923944301605225 and perplexity is 137.54405992623916
At time: 312.4067475795746 and batch: 300, loss is 4.913069362640381 and perplexity is 136.05638054239793
At time: 313.04495120048523 and batch: 350, loss is 4.968200168609619 and perplexity is 143.7678963979265
At time: 313.7053802013397 and batch: 400, loss is 4.987664499282837 and perplexity is 146.59365379848495
At time: 314.3425965309143 and batch: 450, loss is 4.935326318740845 and perplexity is 139.1185321101671
At time: 314.98086380958557 and batch: 500, loss is 4.938034429550171 and perplexity is 139.4957911097757
At time: 315.61508083343506 and batch: 550, loss is 4.987370872497559 and perplexity is 146.55061629396923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.204348625020778 and perplexity of 182.06224334687937
Finished 39 epochs...
Completing Train Step...
At time: 317.30720615386963 and batch: 50, loss is 5.026379041671753 and perplexity is 152.38024999893042
At time: 317.9745309352875 and batch: 100, loss is 5.0214918422698975 and perplexity is 151.63735415100717
At time: 318.6115210056305 and batch: 150, loss is 4.992792825698853 and perplexity is 147.34736489206978
At time: 319.2467141151428 and batch: 200, loss is 4.9503728294372555 and perplexity is 141.22760791683604
At time: 319.88182282447815 and batch: 250, loss is 4.922481327056885 and perplexity is 137.34298358793788
At time: 320.51712346076965 and batch: 300, loss is 4.9112709045410154 and perplexity is 135.8119087448776
At time: 321.15063643455505 and batch: 350, loss is 4.966773481369018 and perplexity is 143.56293082018104
At time: 321.79549193382263 and batch: 400, loss is 4.9863423824310305 and perplexity is 146.39996792432265
At time: 322.4355866909027 and batch: 450, loss is 4.934327831268311 and perplexity is 138.97969332458672
At time: 323.07068705558777 and batch: 500, loss is 4.937074613571167 and perplexity is 139.36196505493163
At time: 323.70950865745544 and batch: 550, loss is 4.986305446624756 and perplexity is 146.39456062333088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.203246745657413 and perplexity of 181.8617432018599
Finished 40 epochs...
Completing Train Step...
At time: 325.3924813270569 and batch: 50, loss is 5.024718236923218 and perplexity is 152.12738619297642
At time: 326.0415802001953 and batch: 100, loss is 5.01942476272583 and perplexity is 151.3242314144199
At time: 326.6773874759674 and batch: 150, loss is 4.991114482879639 and perplexity is 147.10027291080073
At time: 327.3133053779602 and batch: 200, loss is 4.948878650665283 and perplexity is 141.01674619485496
At time: 327.9486918449402 and batch: 250, loss is 4.921011953353882 and perplexity is 137.141323612777
At time: 328.58355140686035 and batch: 300, loss is 4.909602279663086 and perplexity is 135.5854785812665
At time: 329.2186439037323 and batch: 350, loss is 4.965337257385254 and perplexity is 143.35689029134954
At time: 329.883234500885 and batch: 400, loss is 4.985079889297485 and perplexity is 146.21525559362578
At time: 330.5173444747925 and batch: 450, loss is 4.933008670806885 and perplexity is 138.79647768019788
At time: 331.1515872478485 and batch: 500, loss is 4.936247968673706 and perplexity is 139.24680980042817
At time: 331.78639578819275 and batch: 550, loss is 4.9847892379760745 and perplexity is 146.17276411177892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.201422833381815 and perplexity of 181.53034564787623
Finished 41 epochs...
Completing Train Step...
At time: 333.47393703460693 and batch: 50, loss is 5.023079814910889 and perplexity is 151.8783414106983
At time: 334.124929189682 and batch: 100, loss is 5.017809228897095 and perplexity is 151.07995936746832
At time: 334.76217913627625 and batch: 150, loss is 4.989222068786621 and perplexity is 146.8221615152181
At time: 335.4016525745392 and batch: 200, loss is 4.947152433395385 and perplexity is 140.7735306340743
At time: 336.0418903827667 and batch: 250, loss is 4.919476213455201 and perplexity is 136.9308718512928
At time: 336.6835067272186 and batch: 300, loss is 4.907877330780029 and perplexity is 135.35180215931157
At time: 337.3263039588928 and batch: 350, loss is 4.963904228210449 and perplexity is 143.15160281175747
At time: 337.96751976013184 and batch: 400, loss is 4.983772344589234 and perplexity is 146.0241975455991
At time: 338.60638093948364 and batch: 450, loss is 4.932236270904541 and perplexity is 138.68931268683517
At time: 339.24583101272583 and batch: 500, loss is 4.935264081954956 and perplexity is 139.10987408929742
At time: 339.88570952415466 and batch: 550, loss is 4.983489208221435 and perplexity is 145.9828586372457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.200498540350732 and perplexity of 181.36263593286395
Finished 42 epochs...
Completing Train Step...
At time: 341.5820245742798 and batch: 50, loss is 5.0219097995758055 and perplexity is 151.70074533750508
At time: 342.2350423336029 and batch: 100, loss is 5.01615758895874 and perplexity is 150.83063562589444
At time: 342.87325525283813 and batch: 150, loss is 4.987433204650879 and perplexity is 146.55975139415543
At time: 343.51294350624084 and batch: 200, loss is 4.945735206604004 and perplexity is 140.57416392223453
At time: 344.15071201324463 and batch: 250, loss is 4.9182743740081785 and perplexity is 136.76640178109284
At time: 344.78922510147095 and batch: 300, loss is 4.906496562957764 and perplexity is 135.16504171228408
At time: 345.427640914917 and batch: 350, loss is 4.962839813232422 and perplexity is 142.99931116672863
At time: 346.07789611816406 and batch: 400, loss is 4.982640352249145 and perplexity is 145.85899279540703
At time: 346.71596574783325 and batch: 450, loss is 4.9310660076141355 and perplexity is 138.527104607019
At time: 347.35369634628296 and batch: 500, loss is 4.934365310668945 and perplexity is 138.98490229780688
At time: 347.99085903167725 and batch: 550, loss is 4.982151327133178 and perplexity is 145.78768152246332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.199350235310007 and perplexity of 181.15449583085845
Finished 43 epochs...
Completing Train Step...
At time: 349.67564630508423 and batch: 50, loss is 5.020122518539429 and perplexity is 151.4298556222949
At time: 350.3235878944397 and batch: 100, loss is 5.01465856552124 and perplexity is 150.60470634694698
At time: 350.9579031467438 and batch: 150, loss is 4.985845642089844 and perplexity is 146.32726321348264
At time: 351.59190249443054 and batch: 200, loss is 4.944349708557129 and perplexity is 140.37953355381134
At time: 352.22661876678467 and batch: 250, loss is 4.916751317977905 and perplexity is 136.55825743610728
At time: 352.8619258403778 and batch: 300, loss is 4.9050743103027346 and perplexity is 134.97293951415438
At time: 353.49762082099915 and batch: 350, loss is 4.961590347290039 and perplexity is 142.82074997393636
At time: 354.132670879364 and batch: 400, loss is 4.98186243057251 and perplexity is 145.74557004590687
At time: 354.7650671005249 and batch: 450, loss is 4.930247230529785 and perplexity is 138.41372820953737
At time: 355.39727759361267 and batch: 500, loss is 4.933432960510254 and perplexity is 138.85538009150122
At time: 356.0303978919983 and batch: 550, loss is 4.98089451789856 and perplexity is 145.6045693106948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.198672680144615 and perplexity of 181.0317952393781
Finished 44 epochs...
Completing Train Step...
At time: 357.7012948989868 and batch: 50, loss is 5.018718585968018 and perplexity is 151.21740748202967
At time: 358.346230506897 and batch: 100, loss is 5.013165702819824 and perplexity is 150.38004193646796
At time: 358.9795227050781 and batch: 150, loss is 4.984050016403199 and perplexity is 146.06474997928774
At time: 359.613751411438 and batch: 200, loss is 4.942661209106445 and perplexity is 140.14270278909413
At time: 360.25557923316956 and batch: 250, loss is 4.915090265274048 and perplexity is 136.33161525777126
At time: 360.88676619529724 and batch: 300, loss is 4.904061517715454 and perplexity is 134.83630912233406
At time: 361.5344018936157 and batch: 350, loss is 4.960683183670044 and perplexity is 142.69124693448387
At time: 362.1983540058136 and batch: 400, loss is 4.980769901275635 and perplexity is 145.58642569150666
At time: 362.8306305408478 and batch: 450, loss is 4.929503717422485 and perplexity is 138.31085403727027
At time: 363.4636867046356 and batch: 500, loss is 4.932375421524048 and perplexity is 138.7086127333901
At time: 364.10405707359314 and batch: 550, loss is 4.9797399044036865 and perplexity is 145.4365493277894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.198142518388464 and perplexity of 180.93584454183755
Finished 45 epochs...
Completing Train Step...
At time: 365.78403210639954 and batch: 50, loss is 5.017395267486572 and perplexity is 151.01743103743763
At time: 366.4344832897186 and batch: 100, loss is 5.011898164749145 and perplexity is 150.1895502614728
At time: 367.06905150413513 and batch: 150, loss is 4.982864427566528 and perplexity is 145.89167985755594
At time: 367.7099606990814 and batch: 200, loss is 4.941512269973755 and perplexity is 139.9817798167512
At time: 368.35440325737 and batch: 250, loss is 4.913789415359497 and perplexity is 136.15438358860624
At time: 369.00012731552124 and batch: 300, loss is 4.902682027816772 and perplexity is 134.65043203318461
At time: 369.633665561676 and batch: 350, loss is 4.959349145889282 and perplexity is 142.50101833437085
At time: 370.27000737190247 and batch: 400, loss is 4.980031538009643 and perplexity is 145.47896969840997
At time: 370.9156095981598 and batch: 450, loss is 4.928649635314941 and perplexity is 138.1927756430575
At time: 371.5511817932129 and batch: 500, loss is 4.931664638519287 and perplexity is 138.61005603920614
At time: 372.1863660812378 and batch: 550, loss is 4.978474464416504 and perplexity is 145.25262450013173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.197100375561004 and perplexity of 180.74738176883827
Finished 46 epochs...
Completing Train Step...
At time: 373.878511428833 and batch: 50, loss is 5.015833511352539 and perplexity is 150.78176271429479
At time: 374.5310232639313 and batch: 100, loss is 5.010440397262573 and perplexity is 149.97076832362072
At time: 375.16389513015747 and batch: 150, loss is 4.981146755218506 and perplexity is 145.6413008493674
At time: 375.7979974746704 and batch: 200, loss is 4.9398580837249755 and perplexity is 139.75041529426198
At time: 376.4311215877533 and batch: 250, loss is 4.912343301773071 and perplexity is 135.9576311820995
At time: 377.0652813911438 and batch: 300, loss is 4.9014688205719 and perplexity is 134.48717220754313
At time: 377.6964614391327 and batch: 350, loss is 4.9578730010986325 and perplexity is 142.2908213772342
At time: 378.3420877456665 and batch: 400, loss is 4.978812789916992 and perplexity is 145.30177548105922
At time: 378.97831082344055 and batch: 450, loss is 4.92775447845459 and perplexity is 138.06912678270916
At time: 379.62896037101746 and batch: 500, loss is 4.930756750106812 and perplexity is 138.48427068364904
At time: 380.2684841156006 and batch: 550, loss is 4.977426300048828 and perplexity is 145.10045563774068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.196142643056017 and perplexity of 180.57435699507755
Finished 47 epochs...
Completing Train Step...
At time: 381.9599027633667 and batch: 50, loss is 5.014107122421264 and perplexity is 150.5216793152539
At time: 382.6097722053528 and batch: 100, loss is 5.009015150070191 and perplexity is 149.75717515481662
At time: 383.24397683143616 and batch: 150, loss is 4.979663734436035 and perplexity is 145.4254718524226
At time: 383.87518644332886 and batch: 200, loss is 4.938362121582031 and perplexity is 139.54151025949187
At time: 384.5088906288147 and batch: 250, loss is 4.910989274978638 and perplexity is 135.7736654819216
At time: 385.140887260437 and batch: 300, loss is 4.899807119369507 and perplexity is 134.26388028538767
At time: 385.77312874794006 and batch: 350, loss is 4.956588401794433 and perplexity is 142.10815204067043
At time: 386.4142019748688 and batch: 400, loss is 4.9779549503326415 and perplexity is 145.17718331406206
At time: 387.0538330078125 and batch: 450, loss is 4.927211828231812 and perplexity is 137.99422386518225
At time: 387.6859667301178 and batch: 500, loss is 4.9297243595123295 and perplexity is 138.34137460008972
At time: 388.3188798427582 and batch: 550, loss is 4.976200075149536 and perplexity is 144.92263889008166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.19541638962766 and perplexity of 180.44326185914446
Finished 48 epochs...
Completing Train Step...
At time: 390.00452399253845 and batch: 50, loss is 5.0127464866638185 and perplexity is 150.31701340555963
At time: 390.64934396743774 and batch: 100, loss is 5.0077614498138425 and perplexity is 149.5695421882604
At time: 391.28297424316406 and batch: 150, loss is 4.978272218704223 and perplexity is 145.22325075008672
At time: 391.915314912796 and batch: 200, loss is 4.936566534042359 and perplexity is 139.29117607813052
At time: 392.5498149394989 and batch: 250, loss is 4.90952585220337 and perplexity is 135.57511652354202
At time: 393.1811501979828 and batch: 300, loss is 4.898306503295898 and perplexity is 134.06255284390357
At time: 393.8136501312256 and batch: 350, loss is 4.955254316329956 and perplexity is 141.91869402534095
At time: 394.45784425735474 and batch: 400, loss is 4.976754999160766 and perplexity is 145.0030822600871
At time: 395.08938360214233 and batch: 450, loss is 4.926351594924927 and perplexity is 137.87556768097366
At time: 395.7236838340759 and batch: 500, loss is 4.928850545883178 and perplexity is 138.22054282140462
At time: 396.3556478023529 and batch: 550, loss is 4.974937791824341 and perplexity is 144.7398208679125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.195084916784408 and perplexity of 180.38345973002885
Finished 49 epochs...
Completing Train Step...
At time: 398.0234169960022 and batch: 50, loss is 5.011757650375366 and perplexity is 150.16844795349314
At time: 398.6705918312073 and batch: 100, loss is 5.006496515274048 and perplexity is 149.38046611788232
At time: 399.30598616600037 and batch: 150, loss is 4.977232322692871 and perplexity is 145.0723121646945
At time: 399.941593170166 and batch: 200, loss is 4.935613136291504 and perplexity is 139.1584394695902
At time: 400.5764744281769 and batch: 250, loss is 4.908705358505249 and perplexity is 135.4639236175898
At time: 401.2112948894501 and batch: 300, loss is 4.897527666091919 and perplexity is 133.95818058986146
At time: 401.84562373161316 and batch: 350, loss is 4.954162340164185 and perplexity is 141.76380677597106
At time: 402.4796907901764 and batch: 400, loss is 4.9758696937561036 and perplexity is 144.87476705512248
At time: 403.1136484146118 and batch: 450, loss is 4.925285348892212 and perplexity is 137.7286367501068
At time: 403.75035762786865 and batch: 500, loss is 4.928025341033935 and perplexity is 138.10652960780166
At time: 404.38393354415894 and batch: 550, loss is 4.973551988601685 and perplexity is 144.5393788763642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.194400219207115 and perplexity of 180.25999388537377
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f418b26d518>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 4.352298599715045, 'lr': 20.193303942065043, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.6225035402310415}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8731071949005127 and batch: 50, loss is 6.685930395126343 and perplexity is 801.0556298261145
At time: 1.5161561965942383 and batch: 100, loss is 5.985597133636475 and perplexity is 397.6599064729795
At time: 2.146123170852661 and batch: 150, loss is 5.9645107650756835 and perplexity is 389.36249173694966
At time: 2.7753639221191406 and batch: 200, loss is 5.93886022567749 and perplexity is 379.5021361514772
At time: 3.4056191444396973 and batch: 250, loss is 5.9516571998596195 and perplexity is 384.389822285022
At time: 4.03620457649231 and batch: 300, loss is 5.981506052017212 and perplexity is 396.0363706121686
At time: 4.666362762451172 and batch: 350, loss is 6.06943247795105 and perplexity is 432.4351954135215
At time: 5.314728498458862 and batch: 400, loss is 6.101155805587768 and perplexity is 446.373392906998
At time: 5.946554899215698 and batch: 450, loss is 6.054285259246826 and perplexity is 425.93436398535493
At time: 6.5780134201049805 and batch: 500, loss is 6.037454109191895 and perplexity is 418.8253926956552
At time: 7.209147691726685 and batch: 550, loss is 6.157993392944336 and perplexity is 472.47904320474163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.172176929230385 and perplexity of 479.2282172907898
Finished 1 epochs...
Completing Train Step...
At time: 8.873661279678345 and batch: 50, loss is 6.17852126121521 and perplexity is 482.2782652256108
At time: 9.508715629577637 and batch: 100, loss is 6.26233021736145 and perplexity is 524.4395755867436
At time: 10.134047508239746 and batch: 150, loss is 6.2867083168029785 and perplexity is 537.3815248396253
At time: 10.755794048309326 and batch: 200, loss is 6.146155309677124 and perplexity is 466.91877335265286
At time: 11.379014015197754 and batch: 250, loss is 6.147643041610718 and perplexity is 467.6139403049807
At time: 11.999719142913818 and batch: 300, loss is 6.216247577667236 and perplexity is 500.82041196295
At time: 12.621535062789917 and batch: 350, loss is 6.323005065917969 and perplexity is 557.2450369596733
At time: 13.243763208389282 and batch: 400, loss is 6.349195766448974 and perplexity is 572.0324762614813
At time: 13.87059235572815 and batch: 450, loss is 6.297788982391357 and perplexity is 543.3691821597954
At time: 14.50040078163147 and batch: 500, loss is 6.332214679718017 and perplexity is 562.4007531787109
At time: 15.127491474151611 and batch: 550, loss is 6.399170894622802 and perplexity is 601.3462517166571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.445188481756982 and perplexity of 629.6653462089333
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.79163694381714 and batch: 50, loss is 6.242422180175781 and perplexity is 514.102252335807
At time: 17.43822407722473 and batch: 100, loss is 6.081281967163086 and perplexity is 437.58981106442553
At time: 18.075027227401733 and batch: 150, loss is 5.98621413230896 and perplexity is 397.9053378149939
At time: 18.710768938064575 and batch: 200, loss is 5.9385834980010985 and perplexity is 379.39713193661066
At time: 19.343799114227295 and batch: 250, loss is 5.887496538162232 and perplexity is 360.50165161190404
At time: 19.97861337661743 and batch: 300, loss is 5.873690595626831 and perplexity is 355.5587854991979
At time: 20.612567901611328 and batch: 350, loss is 5.90883882522583 and perplexity is 368.2782715946377
At time: 21.247018098831177 and batch: 400, loss is 5.842466802597046 and perplexity is 344.6284232643464
At time: 21.880802392959595 and batch: 450, loss is 5.7243321514129635 and perplexity is 306.2286825428881
At time: 22.51592206954956 and batch: 500, loss is 5.712865219116211 and perplexity is 302.73723532488356
At time: 23.152063131332397 and batch: 550, loss is 5.758186407089234 and perplexity is 316.77330982116905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.845444699551197 and perplexity of 345.6562207743054
Finished 3 epochs...
Completing Train Step...
At time: 24.863372802734375 and batch: 50, loss is 5.802519130706787 and perplexity is 331.1326765977898
At time: 25.510664701461792 and batch: 100, loss is 5.780519304275512 and perplexity is 323.92736361181295
At time: 26.159001111984253 and batch: 150, loss is 5.755897703170777 and perplexity is 316.0491385286669
At time: 26.811214685440063 and batch: 200, loss is 5.704648265838623 and perplexity is 300.2598498263389
At time: 27.448864459991455 and batch: 250, loss is 5.6628717422485355 and perplexity is 287.97444462230766
At time: 28.09469199180603 and batch: 300, loss is 5.648872909545898 and perplexity is 283.9712241311866
At time: 28.72958779335022 and batch: 350, loss is 5.687889280319214 and perplexity is 295.26973081187526
At time: 29.363770484924316 and batch: 400, loss is 5.658966951370239 and perplexity is 286.8521572125801
At time: 29.997621536254883 and batch: 450, loss is 5.584746856689453 and perplexity is 266.3328538249113
At time: 30.632457733154297 and batch: 500, loss is 5.590888862609863 and perplexity is 267.973705692526
At time: 31.265973329544067 and batch: 550, loss is 5.651163396835327 and perplexity is 284.6224020833426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.7339535976978055 and perplexity of 309.1892650667736
Finished 4 epochs...
Completing Train Step...
At time: 32.94180154800415 and batch: 50, loss is 5.687927942276001 and perplexity is 295.2811467381281
At time: 33.58702611923218 and batch: 100, loss is 5.676325397491455 and perplexity is 291.87493259681594
At time: 34.21871876716614 and batch: 150, loss is 5.664324436187744 and perplexity is 288.3930873589181
At time: 34.85096573829651 and batch: 200, loss is 5.601582355499268 and perplexity is 270.85465687116687
At time: 35.48200988769531 and batch: 250, loss is 5.574461727142334 and perplexity is 263.60762461215427
At time: 36.11746430397034 and batch: 300, loss is 5.564672126770019 and perplexity is 261.0396017798561
At time: 36.749971866607666 and batch: 350, loss is 5.613442239761352 and perplexity is 274.0860860648243
At time: 37.38293099403381 and batch: 400, loss is 5.60128249168396 and perplexity is 270.77344953654256
At time: 38.01551175117493 and batch: 450, loss is 5.5330613994598385 and perplexity is 252.91700703057228
At time: 38.657578468322754 and batch: 500, loss is 5.544155759811401 and perplexity is 255.73858229092326
At time: 39.2940559387207 and batch: 550, loss is 5.599674730300904 and perplexity is 270.33846021403843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.674300822805851 and perplexity of 291.28460777690157
Finished 5 epochs...
Completing Train Step...
At time: 40.97810935974121 and batch: 50, loss is 5.624384870529175 and perplexity is 277.1017786164613
At time: 41.62380766868591 and batch: 100, loss is 5.621241731643677 and perplexity is 276.23217659673577
At time: 42.25684213638306 and batch: 150, loss is 5.603309717178345 and perplexity is 271.32292514394646
At time: 42.889774560928345 and batch: 200, loss is 5.540672798156738 and perplexity is 254.84940400101652
At time: 43.518625259399414 and batch: 250, loss is 5.516693544387818 and perplexity is 248.81099310332618
At time: 44.15596652030945 and batch: 300, loss is 5.509541349411011 and perplexity is 247.0377970490514
At time: 44.78808259963989 and batch: 350, loss is 5.559813289642334 and perplexity is 259.77432923677986
At time: 45.42175102233887 and batch: 400, loss is 5.560673761367798 and perplexity is 259.99795389964123
At time: 46.055354833602905 and batch: 450, loss is 5.49450722694397 and perplexity is 243.35157950833369
At time: 46.687753438949585 and batch: 500, loss is 5.506440181732177 and perplexity is 246.2728781013854
At time: 47.319738149642944 and batch: 550, loss is 5.563934373855591 and perplexity is 260.84709007462925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.646179848528923 and perplexity of 283.20750113542994
Finished 6 epochs...
Completing Train Step...
At time: 49.00685095787048 and batch: 50, loss is 5.588564901351929 and perplexity is 267.35166825775923
At time: 49.65750432014465 and batch: 100, loss is 5.588305330276489 and perplexity is 267.28228050362543
At time: 50.29303336143494 and batch: 150, loss is 5.573293800354004 and perplexity is 263.29992992317256
At time: 50.944011211395264 and batch: 200, loss is 5.5090326404571535 and perplexity is 246.91215866914519
At time: 51.59461164474487 and batch: 250, loss is 5.488822736740112 and perplexity is 241.97217416105823
At time: 52.23901152610779 and batch: 300, loss is 5.488072309494019 and perplexity is 241.79065976396402
At time: 52.87425780296326 and batch: 350, loss is 5.534805688858032 and perplexity is 253.35855246411606
At time: 53.50974369049072 and batch: 400, loss is 5.5355517864227295 and perplexity is 253.5476531981312
At time: 54.14398670196533 and batch: 450, loss is 5.469297561645508 and perplexity is 237.29345018415313
At time: 54.778308391571045 and batch: 500, loss is 5.47219744682312 and perplexity is 237.98257264817445
At time: 55.41225719451904 and batch: 550, loss is 5.514631900787354 and perplexity is 248.29856191840818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.593813632396942 and perplexity of 268.75861436973213
Finished 7 epochs...
Completing Train Step...
At time: 57.16992974281311 and batch: 50, loss is 5.5313153648376465 and perplexity is 252.47579048153338
At time: 57.82177495956421 and batch: 100, loss is 5.530353775024414 and perplexity is 252.23312902241034
At time: 58.47075152397156 and batch: 150, loss is 5.51120662689209 and perplexity is 247.449526255992
At time: 59.11227583885193 and batch: 200, loss is 5.449420728683472 and perplexity is 232.62337482253784
At time: 59.752026319503784 and batch: 250, loss is 5.43051290512085 and perplexity is 228.26629435523202
At time: 60.38806509971619 and batch: 300, loss is 5.424923973083496 and perplexity is 226.99408799866282
At time: 61.02055096626282 and batch: 350, loss is 5.469546852111816 and perplexity is 237.35261255300227
At time: 61.65413165092468 and batch: 400, loss is 5.469466390609742 and perplexity is 237.3335155735711
At time: 62.28670144081116 and batch: 450, loss is 5.4101187610626225 and perplexity is 223.65814798266854
At time: 62.91865301132202 and batch: 500, loss is 5.4170576858520505 and perplexity is 225.21549194771944
At time: 63.54930138587952 and batch: 550, loss is 5.4751544857025145 and perplexity is 238.68733786364137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.572944803440825 and perplexity of 263.2080550933375
Finished 8 epochs...
Completing Train Step...
At time: 65.24271488189697 and batch: 50, loss is 5.502779626846314 and perplexity is 245.37303068941998
At time: 65.88700294494629 and batch: 100, loss is 5.502976522445679 and perplexity is 245.42134831597355
At time: 66.52162408828735 and batch: 150, loss is 5.487786951065064 and perplexity is 241.72167260465963
At time: 67.15404963493347 and batch: 200, loss is 5.422686882019043 and perplexity is 226.48684913402613
At time: 67.78588604927063 and batch: 250, loss is 5.405655422210693 and perplexity is 222.66211036147092
At time: 68.41906428337097 and batch: 300, loss is 5.405154094696045 and perplexity is 222.55051169515545
At time: 69.05220890045166 and batch: 350, loss is 5.445297737121582 and perplexity is 231.66624508581856
At time: 69.68640565872192 and batch: 400, loss is 5.450568180084229 and perplexity is 232.89045203958642
At time: 70.33655762672424 and batch: 450, loss is 5.387690677642822 and perplexity is 218.69775832776247
At time: 70.9738781452179 and batch: 500, loss is 5.395612621307373 and perplexity is 220.43715023454368
At time: 71.60423851013184 and batch: 550, loss is 5.459410982131958 and perplexity is 234.9589885561418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.558647642744348 and perplexity of 259.47170050911336
Finished 9 epochs...
Completing Train Step...
At time: 73.2984709739685 and batch: 50, loss is 5.4836054611206055 and perplexity is 240.71302615347176
At time: 73.9474868774414 and batch: 100, loss is 5.480854959487915 and perplexity is 240.05185427601893
At time: 74.5827648639679 and batch: 150, loss is 5.465018692016602 and perplexity is 236.28027162234378
At time: 75.21760678291321 and batch: 200, loss is 5.398133840560913 and perplexity is 220.99362182057902
At time: 75.848140001297 and batch: 250, loss is 5.378653831481934 and perplexity is 216.7303234224375
At time: 76.47905206680298 and batch: 300, loss is 5.377241697311401 and perplexity is 216.42448711871253
At time: 77.11085987091064 and batch: 350, loss is 5.415918579101563 and perplexity is 224.95909352084126
At time: 77.74351167678833 and batch: 400, loss is 5.423075752258301 and perplexity is 226.57494025613536
At time: 78.37319302558899 and batch: 450, loss is 5.360897512435913 and perplexity is 212.91595543806346
At time: 79.002681016922 and batch: 500, loss is 5.368830480575562 and perplexity is 214.61172829180043
At time: 79.63711524009705 and batch: 550, loss is 5.428496799468994 and perplexity is 227.8065489923931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.543136921334774 and perplexity of 255.47815867065682
Finished 10 epochs...
Completing Train Step...
At time: 81.39466786384583 and batch: 50, loss is 5.457047815322876 and perplexity is 234.40439682736923
At time: 82.03431916236877 and batch: 100, loss is 5.4601803970336915 and perplexity is 235.1398390688509
At time: 82.66086292266846 and batch: 150, loss is 5.440564098358155 and perplexity is 230.57221218902993
At time: 83.28666257858276 and batch: 200, loss is 5.382669353485108 and perplexity is 217.6023584711036
At time: 83.91222167015076 and batch: 250, loss is 5.36318151473999 and perplexity is 213.40281174968035
At time: 84.5367386341095 and batch: 300, loss is 5.354924087524414 and perplexity is 211.64790902631975
At time: 85.16274309158325 and batch: 350, loss is 5.396574573516846 and perplexity is 220.64930226195773
At time: 85.78988862037659 and batch: 400, loss is 5.403930387496948 and perplexity is 222.27834159405103
At time: 86.41544961929321 and batch: 450, loss is 5.342458896636963 and perplexity is 209.0260523600899
At time: 87.04154348373413 and batch: 500, loss is 5.350690755844116 and perplexity is 210.75382703602082
At time: 87.66800427436829 and batch: 550, loss is 5.410270566940308 and perplexity is 223.69210318135868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.522349256150266 and perplexity of 250.222183244335
Finished 11 epochs...
Completing Train Step...
At time: 89.31937599182129 and batch: 50, loss is 5.43395655632019 and perplexity is 229.0537188831127
At time: 89.95757102966309 and batch: 100, loss is 5.4367670154571535 and perplexity is 229.69837045933477
At time: 90.58400654792786 and batch: 150, loss is 5.413332252502442 and perplexity is 224.37802757061553
At time: 91.21306991577148 and batch: 200, loss is 5.351715679168701 and perplexity is 210.96994428198002
At time: 91.840167760849 and batch: 250, loss is 5.331191110610962 and perplexity is 206.68401111736011
At time: 92.47147417068481 and batch: 300, loss is 5.315645446777344 and perplexity is 203.49581646857317
At time: 93.09788370132446 and batch: 350, loss is 5.357190761566162 and perplexity is 212.1281899622324
At time: 93.72487092018127 and batch: 400, loss is 5.358657655715942 and perplexity is 212.4395879011605
At time: 94.35085010528564 and batch: 450, loss is 5.292847080230713 and perplexity is 198.9089296907943
At time: 94.97743892669678 and batch: 500, loss is 5.296110076904297 and perplexity is 199.55902892573366
At time: 95.61062669754028 and batch: 550, loss is 5.356839818954468 and perplexity is 212.05375820263498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.479382454080785 and perplexity of 239.69863674379158
Finished 12 epochs...
Completing Train Step...
At time: 97.26953220367432 and batch: 50, loss is 5.379053726196289 and perplexity is 216.8170100648244
At time: 97.91018152236938 and batch: 100, loss is 5.383132371902466 and perplexity is 217.70313569979234
At time: 98.5368378162384 and batch: 150, loss is 5.358487815856933 and perplexity is 212.4035102553014
At time: 99.18309473991394 and batch: 200, loss is 5.301948022842407 and perplexity is 200.7274510221058
At time: 99.81226801872253 and batch: 250, loss is 5.282977123260498 and perplexity is 196.95536377853145
At time: 100.44379448890686 and batch: 300, loss is 5.2713573551177975 and perplexity is 194.68003312530007
At time: 101.0746054649353 and batch: 350, loss is 5.315398759841919 and perplexity is 203.44562290054006
At time: 101.70607089996338 and batch: 400, loss is 5.320767831802368 and perplexity is 204.54087470173786
At time: 102.3367063999176 and batch: 450, loss is 5.2581265354156494 and perplexity is 192.12122162177116
At time: 102.9691309928894 and batch: 500, loss is 5.265900478363037 and perplexity is 193.62058145550176
At time: 103.5986819267273 and batch: 550, loss is 5.328618307113647 and perplexity is 206.15293723803092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.455000207779255 and perplexity of 233.92491967421068
Finished 13 epochs...
Completing Train Step...
At time: 105.2792706489563 and batch: 50, loss is 5.348862342834472 and perplexity is 210.368834067168
At time: 105.92505359649658 and batch: 100, loss is 5.353662729263306 and perplexity is 211.38111348557598
At time: 106.5578293800354 and batch: 150, loss is 5.331989831924439 and perplexity is 206.84915998737586
At time: 107.19011998176575 and batch: 200, loss is 5.278867521286011 and perplexity is 196.14761652334388
At time: 107.82678890228271 and batch: 250, loss is 5.259388341903686 and perplexity is 192.36379443349242
At time: 108.4609887599945 and batch: 300, loss is 5.25010495185852 and perplexity is 190.5862697922462
At time: 109.09637451171875 and batch: 350, loss is 5.295404014587402 and perplexity is 199.41817754619024
At time: 109.7325210571289 and batch: 400, loss is 5.300365409851074 and perplexity is 200.4100283952406
At time: 110.36737751960754 and batch: 450, loss is 5.239419565200806 and perplexity is 188.56062348842255
At time: 111.00318789482117 and batch: 500, loss is 5.245235872268677 and perplexity is 189.66054561715652
At time: 111.63891863822937 and batch: 550, loss is 5.297920913696289 and perplexity is 199.92072514505992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4255945733252995 and perplexity of 227.14636134052608
Finished 14 epochs...
Completing Train Step...
At time: 113.30140686035156 and batch: 50, loss is 5.31759370803833 and perplexity is 203.8926659422721
At time: 113.94616055488586 and batch: 100, loss is 5.324352426528931 and perplexity is 205.2753865196568
At time: 114.57821035385132 and batch: 150, loss is 5.301632595062256 and perplexity is 200.66414599242182
At time: 115.20911860466003 and batch: 200, loss is 5.247789421081543 and perplexity is 190.14547195630223
At time: 115.83876085281372 and batch: 250, loss is 5.22501971244812 and perplexity is 185.86483434074677
At time: 116.46858549118042 and batch: 300, loss is 5.217417573928833 and perplexity is 184.45722133785637
At time: 117.09918904304504 and batch: 350, loss is 5.25909239768982 and perplexity is 192.30687390463842
At time: 117.73105454444885 and batch: 400, loss is 5.269316473007202 and perplexity is 194.28311929334288
At time: 118.362717628479 and batch: 450, loss is 5.211086931228638 and perplexity is 183.29317703798844
At time: 118.99423170089722 and batch: 500, loss is 5.219920606613159 and perplexity is 184.91950210230067
At time: 119.62775039672852 and batch: 550, loss is 5.277117719650269 and perplexity is 195.8046972109873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4124298095703125 and perplexity of 224.175630545128
Finished 15 epochs...
Completing Train Step...
At time: 121.30307102203369 and batch: 50, loss is 5.301892976760865 and perplexity is 200.7164020665726
At time: 121.94778299331665 and batch: 100, loss is 5.306056795120239 and perplexity is 201.5538910725858
At time: 122.57972145080566 and batch: 150, loss is 5.2843235492706295 and perplexity is 197.220728209855
At time: 123.21011686325073 and batch: 200, loss is 5.232319860458374 and perplexity is 187.22663978419396
At time: 123.84101057052612 and batch: 250, loss is 5.211819372177124 and perplexity is 183.42747764405678
At time: 124.47281575202942 and batch: 300, loss is 5.203255052566528 and perplexity is 181.86325391710687
At time: 125.10080742835999 and batch: 350, loss is 5.246892375946045 and perplexity is 189.97497936685366
At time: 125.73089098930359 and batch: 400, loss is 5.2561901092529295 and perplexity is 191.74955303233781
At time: 126.3608136177063 and batch: 450, loss is 5.19727876663208 and perplexity is 180.779628364033
At time: 126.99272608757019 and batch: 500, loss is 5.207030925750733 and perplexity is 182.55124456608328
At time: 127.62327146530151 and batch: 550, loss is 5.265053415298462 and perplexity is 193.45664205571688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.407972944543717 and perplexity of 223.17873318619095
Finished 16 epochs...
Completing Train Step...
At time: 129.29600286483765 and batch: 50, loss is 5.287526741027832 and perplexity is 197.85347688745736
At time: 129.94348311424255 and batch: 100, loss is 5.295311813354492 and perplexity is 199.39979179196348
At time: 130.58981466293335 and batch: 150, loss is 5.274868803024292 and perplexity is 195.36484355447035
At time: 131.22175335884094 and batch: 200, loss is 5.22147427558899 and perplexity is 185.20702909879202
At time: 131.85551238059998 and batch: 250, loss is 5.203607387542725 and perplexity is 181.92734199191895
At time: 132.48866319656372 and batch: 300, loss is 5.1923919296264645 and perplexity is 179.89834288936098
At time: 133.12236309051514 and batch: 350, loss is 5.2364722347259525 and perplexity is 188.00569120258993
At time: 133.75691938400269 and batch: 400, loss is 5.24692328453064 and perplexity is 189.98085131532065
At time: 134.39012742042542 and batch: 450, loss is 5.187193441390991 and perplexity is 178.9655700751452
At time: 135.02325916290283 and batch: 500, loss is 5.197369861602783 and perplexity is 180.79609722908646
At time: 135.65721774101257 and batch: 550, loss is 5.257039852142334 and perplexity is 191.91256009881297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.399921173745013 and perplexity of 221.3889642534126
Finished 17 epochs...
Completing Train Step...
At time: 137.33697986602783 and batch: 50, loss is 5.279634656906128 and perplexity is 196.29814607768944
At time: 137.9768271446228 and batch: 100, loss is 5.2862362289428715 and perplexity is 197.59830926834422
At time: 138.60457682609558 and batch: 150, loss is 5.264759473800659 and perplexity is 193.39978547725428
At time: 139.23195052146912 and batch: 200, loss is 5.214299936294555 and perplexity is 183.88304606298786
At time: 139.85979628562927 and batch: 250, loss is 5.195993728637696 and perplexity is 180.5474688718061
At time: 140.4888207912445 and batch: 300, loss is 5.183663568496704 and perplexity is 178.33495800533788
At time: 141.11659789085388 and batch: 350, loss is 5.230244846343994 and perplexity is 186.83854465474064
At time: 141.74770665168762 and batch: 400, loss is 5.2391955757141115 and perplexity is 188.5183926209683
At time: 142.3770625591278 and batch: 450, loss is 5.178325901031494 and perplexity is 177.38560123145064
At time: 143.0076038837433 and batch: 500, loss is 5.189209241867065 and perplexity is 179.32669280991013
At time: 143.6381368637085 and batch: 550, loss is 5.248725957870484 and perplexity is 190.32363360048473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.400378612761802 and perplexity of 221.49025936995156
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 145.30634832382202 and batch: 50, loss is 5.2369114017486575 and perplexity is 188.08827523501148
At time: 145.95407557487488 and batch: 100, loss is 5.214443950653076 and perplexity is 183.90952976888073
At time: 146.58615469932556 and batch: 150, loss is 5.172936315536499 and perplexity is 176.4321380616538
At time: 147.21819424629211 and batch: 200, loss is 5.118164310455322 and perplexity is 167.02847559988865
At time: 147.8501431941986 and batch: 250, loss is 5.086804304122925 and perplexity is 161.87174175923084
At time: 148.48466062545776 and batch: 300, loss is 5.069359788894653 and perplexity is 159.07245478469702
At time: 149.12286734580994 and batch: 350, loss is 5.1172910404205325 and perplexity is 166.88267830665978
At time: 149.75326347351074 and batch: 400, loss is 5.114432516098023 and perplexity is 166.40632127516082
At time: 150.38333702087402 and batch: 450, loss is 5.038263158798218 and perplexity is 154.201957992989
At time: 151.01344537734985 and batch: 500, loss is 5.0478697681427 and perplexity is 155.69045421707006
At time: 151.64302921295166 and batch: 550, loss is 5.116807508468628 and perplexity is 166.8020047051829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.306154940990692 and perplexity of 201.57367372544527
Finished 19 epochs...
Completing Train Step...
At time: 153.31558561325073 and batch: 50, loss is 5.166596260070801 and perplexity is 175.31708699828062
At time: 153.95881509780884 and batch: 100, loss is 5.172557220458985 and perplexity is 176.36526618280018
At time: 154.59309458732605 and batch: 150, loss is 5.144412479400635 and perplexity is 171.4707124953175
At time: 155.22535705566406 and batch: 200, loss is 5.092272253036499 and perplexity is 162.75927244863996
At time: 155.8531687259674 and batch: 250, loss is 5.067348184585572 and perplexity is 158.7527855809663
At time: 156.48088932037354 and batch: 300, loss is 5.052486391067505 and perplexity is 156.41088002478332
At time: 157.1106767654419 and batch: 350, loss is 5.103999090194702 and perplexity is 164.67915902963827
At time: 157.74065518379211 and batch: 400, loss is 5.109066534042358 and perplexity is 165.51577939396296
At time: 158.36818552017212 and batch: 450, loss is 5.041944341659546 and perplexity is 154.77064968479064
At time: 159.00048661231995 and batch: 500, loss is 5.05243106842041 and perplexity is 156.40222720021677
At time: 159.62679839134216 and batch: 550, loss is 5.117748146057129 and perplexity is 166.95897875691844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.300453023707613 and perplexity of 200.42758785993232
Finished 20 epochs...
Completing Train Step...
At time: 161.2781629562378 and batch: 50, loss is 5.15491473197937 and perplexity is 173.28103079842677
At time: 161.92097568511963 and batch: 100, loss is 5.162148513793945 and perplexity is 174.53905260971067
At time: 162.55167245864868 and batch: 150, loss is 5.135086708068847 and perplexity is 169.87904912038846
At time: 163.1821460723877 and batch: 200, loss is 5.08445966720581 and perplexity is 161.49265588070037
At time: 163.81374430656433 and batch: 250, loss is 5.059989290237427 and perplexity is 157.58882857539103
At time: 164.44482851028442 and batch: 300, loss is 5.0473396110534665 and perplexity is 155.60793569486344
At time: 165.07910656929016 and batch: 350, loss is 5.100980653762817 and perplexity is 164.18283489470875
At time: 165.7120177745819 and batch: 400, loss is 5.108351716995239 and perplexity is 165.3975081694647
At time: 166.34431838989258 and batch: 450, loss is 5.042212114334107 and perplexity is 154.81209858479397
At time: 166.9761095046997 and batch: 500, loss is 5.052268686294556 and perplexity is 156.3768323359675
At time: 167.60899114608765 and batch: 550, loss is 5.114989356994629 and perplexity is 166.49900892404239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.29676299399518 and perplexity of 199.6893669716979
Finished 21 epochs...
Completing Train Step...
At time: 169.28525257110596 and batch: 50, loss is 5.147559976577758 and perplexity is 172.01126632842744
At time: 169.94956231117249 and batch: 100, loss is 5.156042900085449 and perplexity is 173.47663124510868
At time: 170.58039212226868 and batch: 150, loss is 5.12826979637146 and perplexity is 168.724936853329
At time: 171.21271657943726 and batch: 200, loss is 5.077274684906006 and perplexity is 160.33649248721585
At time: 171.84624242782593 and batch: 250, loss is 5.054192590713501 and perplexity is 156.67797600765164
At time: 172.47960925102234 and batch: 300, loss is 5.0429168128967286 and perplexity is 154.9212328970233
At time: 173.11449551582336 and batch: 350, loss is 5.098434896469116 and perplexity is 163.76539681873547
At time: 173.74867057800293 and batch: 400, loss is 5.106302614212036 and perplexity is 165.05893867545217
At time: 174.38207364082336 and batch: 450, loss is 5.040409393310547 and perplexity is 154.53326696329916
At time: 175.01234006881714 and batch: 500, loss is 5.049886646270752 and perplexity is 156.0047797605328
At time: 175.64610385894775 and batch: 550, loss is 5.112067108154297 and perplexity is 166.0131676086462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.294816037441822 and perplexity of 199.3009586797642
Finished 22 epochs...
Completing Train Step...
At time: 177.32523679733276 and batch: 50, loss is 5.143085584640503 and perplexity is 171.24333978857834
At time: 177.97100186347961 and batch: 100, loss is 5.1514476299285885 and perplexity is 172.6812880665937
At time: 178.60530829429626 and batch: 150, loss is 5.12355715751648 and perplexity is 167.93166782347603
At time: 179.23884797096252 and batch: 200, loss is 5.073136339187622 and perplexity is 159.67433571186135
At time: 179.87208461761475 and batch: 250, loss is 5.050451154708862 and perplexity is 156.09287063677527
At time: 180.50481033325195 and batch: 300, loss is 5.040088500976562 and perplexity is 154.4836863780253
At time: 181.14098024368286 and batch: 350, loss is 5.095388460159302 and perplexity is 163.26725513129938
At time: 181.77506637573242 and batch: 400, loss is 5.103287420272827 and perplexity is 164.56200351836864
At time: 182.40851306915283 and batch: 450, loss is 5.037927045822143 and perplexity is 154.15013742324226
At time: 183.04138350486755 and batch: 500, loss is 5.046393165588379 and perplexity is 155.46073094141735
At time: 183.67497491836548 and batch: 550, loss is 5.107676954269409 and perplexity is 165.28594174067362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.292004849048371 and perplexity of 198.74147291631985
Finished 23 epochs...
Completing Train Step...
At time: 185.36585187911987 and batch: 50, loss is 5.137470226287842 and perplexity is 170.28444186756676
At time: 186.01546335220337 and batch: 100, loss is 5.14643669128418 and perplexity is 171.81815708132447
At time: 186.65212893486023 and batch: 150, loss is 5.117605075836182 and perplexity is 166.93509360760618
At time: 187.28730463981628 and batch: 200, loss is 5.068184413909912 and perplexity is 158.88559483734798
At time: 187.92190217971802 and batch: 250, loss is 5.045345554351806 and perplexity is 155.29795381129986
At time: 188.5581374168396 and batch: 300, loss is 5.03569673538208 and perplexity is 153.8067178708721
At time: 189.19525241851807 and batch: 350, loss is 5.091829442977906 and perplexity is 162.6872169602953
At time: 189.83157920837402 and batch: 400, loss is 5.100057344436646 and perplexity is 164.03131331346214
At time: 190.4908905029297 and batch: 450, loss is 5.034870882034301 and perplexity is 153.6797485142581
At time: 191.1359257698059 and batch: 500, loss is 5.042419948577881 and perplexity is 154.84427718402202
At time: 191.77354979515076 and batch: 550, loss is 5.104226865768433 and perplexity is 164.7166731918104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.289492343334442 and perplexity of 198.24276060114482
Finished 24 epochs...
Completing Train Step...
At time: 193.45734286308289 and batch: 50, loss is 5.132103843688965 and perplexity is 169.3730779527212
At time: 194.10527658462524 and batch: 100, loss is 5.1401708126068115 and perplexity is 170.7449312173579
At time: 194.73952412605286 and batch: 150, loss is 5.111829881668091 and perplexity is 165.9737895591733
At time: 195.37727093696594 and batch: 200, loss is 5.0631370353698735 and perplexity is 158.08565958192932
At time: 196.0142171382904 and batch: 250, loss is 5.0396081066131595 and perplexity is 154.4094911087731
At time: 196.65760588645935 and batch: 300, loss is 5.031763477325439 and perplexity is 153.20294453519983
At time: 197.29434251785278 and batch: 350, loss is 5.087586803436279 and perplexity is 161.99845585639025
At time: 197.93072247505188 and batch: 400, loss is 5.095496873855591 and perplexity is 163.28495649742916
At time: 198.5660367012024 and batch: 450, loss is 5.031115322113037 and perplexity is 153.10367742186767
At time: 199.21482968330383 and batch: 500, loss is 5.037389631271362 and perplexity is 154.0673171527887
At time: 199.8694086074829 and batch: 550, loss is 5.0991366481781 and perplexity is 163.88035979884154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.284568299638464 and perplexity of 197.2690039631422
Finished 25 epochs...
Completing Train Step...
At time: 201.65937876701355 and batch: 50, loss is 5.126897983551025 and perplexity is 168.4936365084889
At time: 202.3100028038025 and batch: 100, loss is 5.134955854415893 and perplexity is 169.85682128057942
At time: 202.94634532928467 and batch: 150, loss is 5.106417846679688 and perplexity is 165.0779599201751
At time: 203.5830135345459 and batch: 200, loss is 5.058391799926758 and perplexity is 157.33728292303823
At time: 204.2187569141388 and batch: 250, loss is 5.0351018142700195 and perplexity is 153.71524222032983
At time: 204.85391426086426 and batch: 300, loss is 5.026968727111816 and perplexity is 152.47013291242618
At time: 205.49799585342407 and batch: 350, loss is 5.083292226791382 and perplexity is 161.30423283533142
At time: 206.1373257637024 and batch: 400, loss is 5.0918153953552245 and perplexity is 162.6849316077083
At time: 206.77370023727417 and batch: 450, loss is 5.027619247436523 and perplexity is 152.5693501006964
At time: 207.40908312797546 and batch: 500, loss is 5.033487024307251 and perplexity is 153.46722469206867
At time: 208.04630303382874 and batch: 550, loss is 5.09487380027771 and perplexity is 163.18324964408697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.282392785904255 and perplexity of 196.84030902063364
Finished 26 epochs...
Completing Train Step...
At time: 209.75284099578857 and batch: 50, loss is 5.122568559646607 and perplexity is 167.76573296931755
At time: 210.41255688667297 and batch: 100, loss is 5.130551052093506 and perplexity is 169.11028094807313
At time: 211.05226397514343 and batch: 150, loss is 5.101086072921753 and perplexity is 164.20014382340534
At time: 211.68734574317932 and batch: 200, loss is 5.053921003341674 and perplexity is 156.63543002566186
At time: 212.32149481773376 and batch: 250, loss is 5.031226377487183 and perplexity is 153.1206813522183
At time: 212.9565966129303 and batch: 300, loss is 5.022200603485107 and perplexity is 151.74486692234692
At time: 213.59154629707336 and batch: 350, loss is 5.079843044281006 and perplexity is 160.7488235013021
At time: 214.22332572937012 and batch: 400, loss is 5.088120040893554 and perplexity is 162.0848625366654
At time: 214.85737109184265 and batch: 450, loss is 5.024806489944458 and perplexity is 152.1408124868681
At time: 215.4890055656433 and batch: 500, loss is 5.0292533588409425 and perplexity is 152.81886923116446
At time: 216.12239861488342 and batch: 550, loss is 5.091031293869019 and perplexity is 162.55742010856247
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2791761033078455 and perplexity of 196.20815349125832
Finished 27 epochs...
Completing Train Step...
At time: 217.81895542144775 and batch: 50, loss is 5.117834110260009 and perplexity is 166.97333186935913
At time: 218.48871088027954 and batch: 100, loss is 5.1265755558013915 and perplexity is 168.4393182417707
At time: 219.12487936019897 and batch: 150, loss is 5.096933965682983 and perplexity is 163.5197806654802
At time: 219.76040387153625 and batch: 200, loss is 5.049895105361938 and perplexity is 156.00609942477178
At time: 220.3994219303131 and batch: 250, loss is 5.027615737915039 and perplexity is 152.568814656224
At time: 221.03597974777222 and batch: 300, loss is 5.018271417617798 and perplexity is 151.1498029598292
At time: 221.66733193397522 and batch: 350, loss is 5.075655374526978 and perplexity is 160.07706804183067
At time: 222.30362725257874 and batch: 400, loss is 5.084646253585816 and perplexity is 161.52279102207464
At time: 222.95395517349243 and batch: 450, loss is 5.02179370880127 and perplexity is 151.68313530268227
At time: 223.59094405174255 and batch: 500, loss is 5.026004152297974 and perplexity is 152.32313496900989
At time: 224.22802996635437 and batch: 550, loss is 5.087456274032593 and perplexity is 161.9773116745479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.27853718209774 and perplexity of 196.08283197994612
Finished 28 epochs...
Completing Train Step...
At time: 225.91352152824402 and batch: 50, loss is 5.114126091003418 and perplexity is 166.35533801409545
At time: 226.568035364151 and batch: 100, loss is 5.122310304641724 and perplexity is 167.72241222327094
At time: 227.20061016082764 and batch: 150, loss is 5.093334054946899 and perplexity is 162.93218233684087
At time: 227.83377289772034 and batch: 200, loss is 5.046700706481934 and perplexity is 155.5085488261244
At time: 228.46718049049377 and batch: 250, loss is 5.023303909301758 and perplexity is 151.91238030892353
At time: 229.10303568840027 and batch: 300, loss is 5.014639616012573 and perplexity is 150.60185248879844
At time: 229.7354826927185 and batch: 350, loss is 5.071769952774048 and perplexity is 159.45630785801856
At time: 230.36726808547974 and batch: 400, loss is 5.0816162586212155 and perplexity is 161.03411849013497
At time: 230.99912309646606 and batch: 450, loss is 5.018280420303345 and perplexity is 151.151163720101
At time: 231.630961894989 and batch: 500, loss is 5.021912488937378 and perplexity is 151.70115331620872
At time: 232.263512134552 and batch: 550, loss is 5.082398796081543 and perplexity is 161.1601830389272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.276108762051197 and perplexity of 195.60723820443596
Finished 29 epochs...
Completing Train Step...
At time: 233.97282934188843 and batch: 50, loss is 5.1103769874572755 and perplexity is 165.73282229353651
At time: 234.6211211681366 and batch: 100, loss is 5.118973560333252 and perplexity is 167.16369808038706
At time: 235.25425958633423 and batch: 150, loss is 5.088207855224609 and perplexity is 162.09909653540856
At time: 235.8879747390747 and batch: 200, loss is 5.043451023101807 and perplexity is 155.00401551030987
At time: 236.5233588218689 and batch: 250, loss is 5.019395799636841 and perplexity is 151.31984866070854
At time: 237.15530943870544 and batch: 300, loss is 5.010665082931519 and perplexity is 150.00446839184323
At time: 237.7882423400879 and batch: 350, loss is 5.068584690093994 and perplexity is 158.94920568705982
At time: 238.42032480239868 and batch: 400, loss is 5.078305978775024 and perplexity is 160.5019318225209
At time: 239.05074191093445 and batch: 450, loss is 5.014881067276001 and perplexity is 150.6382198866568
At time: 239.682959318161 and batch: 500, loss is 5.019233160018921 and perplexity is 151.29524005955966
At time: 240.31540656089783 and batch: 550, loss is 5.079181632995605 and perplexity is 160.6425375685559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.272660925033245 and perplexity of 194.9339776408753
Finished 30 epochs...
Completing Train Step...
At time: 242.01140332221985 and batch: 50, loss is 5.106255970001221 and perplexity is 165.0512398110747
At time: 242.66004729270935 and batch: 100, loss is 5.114822940826416 and perplexity is 166.47130310237827
At time: 243.29642367362976 and batch: 150, loss is 5.085017108917237 and perplexity is 161.58270371905604
At time: 243.93938088417053 and batch: 200, loss is 5.041029977798462 and perplexity is 154.6291976751204
At time: 244.57474279403687 and batch: 250, loss is 5.016276626586914 and perplexity is 150.84859121568954
At time: 245.21381878852844 and batch: 300, loss is 5.006764230728149 and perplexity is 149.4204629308465
At time: 245.85163855552673 and batch: 350, loss is 5.065350990295411 and perplexity is 158.43604182874873
At time: 246.48947525024414 and batch: 400, loss is 5.075107040405274 and perplexity is 159.9893163841338
At time: 247.12539100646973 and batch: 450, loss is 5.011527681350708 and perplexity is 150.13391783256418
At time: 247.7623951435089 and batch: 500, loss is 5.015323114395142 and perplexity is 150.70482379773918
At time: 248.39914464950562 and batch: 550, loss is 5.075120687484741 and perplexity is 159.9914997859469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.26806186107879 and perplexity of 194.03952221624047
Finished 31 epochs...
Completing Train Step...
At time: 250.11110758781433 and batch: 50, loss is 5.101712045669555 and perplexity is 164.3029608155843
At time: 250.7817199230194 and batch: 100, loss is 5.109342260360718 and perplexity is 165.56142274269274
At time: 251.4186999797821 and batch: 150, loss is 5.079559907913208 and perplexity is 160.70331610599155
At time: 252.05476474761963 and batch: 200, loss is 5.035297508239746 and perplexity is 153.74532630982887
At time: 252.6893744468689 and batch: 250, loss is 5.010008335113525 and perplexity is 149.9059856272478
At time: 253.32531094551086 and batch: 300, loss is 5.000530614852905 and perplexity is 148.4919302258788
At time: 253.95917415618896 and batch: 350, loss is 5.05853684425354 and perplexity is 157.36010545841714
At time: 254.59334015846252 and batch: 400, loss is 5.068839111328125 and perplexity is 158.98965088497528
At time: 255.22935104370117 and batch: 450, loss is 5.0039864444732665 and perplexity is 149.00598076067195
At time: 255.8629298210144 and batch: 500, loss is 5.007324743270874 and perplexity is 149.50423845088054
At time: 256.4991223812103 and batch: 550, loss is 5.06681040763855 and perplexity is 158.6674349444647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.261546682804189 and perplexity of 192.7794294584805
Finished 32 epochs...
Completing Train Step...
At time: 258.19154715538025 and batch: 50, loss is 5.091805429458618 and perplexity is 162.6833103145793
At time: 258.8395872116089 and batch: 100, loss is 5.099836645126342 and perplexity is 163.99511571028563
At time: 259.4739010334015 and batch: 150, loss is 5.068674449920654 and perplexity is 158.9634735805423
At time: 260.1096074581146 and batch: 200, loss is 5.025217895507812 and perplexity is 152.2034169405657
At time: 260.7483220100403 and batch: 250, loss is 4.998427028656006 and perplexity is 148.1798929648081
At time: 261.386029958725 and batch: 300, loss is 4.988626117706299 and perplexity is 146.7346887567793
At time: 262.0200436115265 and batch: 350, loss is 5.046624689102173 and perplexity is 155.496727923015
At time: 262.65388655662537 and batch: 400, loss is 5.056431035995484 and perplexity is 157.02908390513872
At time: 263.2876329421997 and batch: 450, loss is 4.990109500885009 and perplexity is 146.95251404505632
At time: 263.9211587905884 and batch: 500, loss is 4.993102483749389 and perplexity is 147.3929992549932
At time: 264.5730838775635 and batch: 550, loss is 5.0525932788848875 and perplexity is 156.42759933589352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.24922115244764 and perplexity of 190.4179041709596
Finished 33 epochs...
Completing Train Step...
At time: 266.27275252342224 and batch: 50, loss is 5.07896318435669 and perplexity is 160.6074492575085
At time: 266.9246618747711 and batch: 100, loss is 5.087595443725586 and perplexity is 161.99985557596315
At time: 267.56576585769653 and batch: 150, loss is 5.055418367385864 and perplexity is 156.87014597038194
At time: 268.2034225463867 and batch: 200, loss is 5.012651357650757 and perplexity is 150.30271457655553
At time: 268.84972047805786 and batch: 250, loss is 4.986641426086425 and perplexity is 146.44375445259587
At time: 269.490829706192 and batch: 300, loss is 4.978410758972168 and perplexity is 145.24337141188622
At time: 270.12503576278687 and batch: 350, loss is 5.038381948471069 and perplexity is 154.22027668114595
At time: 270.76881861686707 and batch: 400, loss is 5.045596513748169 and perplexity is 155.3369321828344
At time: 271.4068171977997 and batch: 450, loss is 4.981200513839721 and perplexity is 145.64913053534772
At time: 272.05017256736755 and batch: 500, loss is 4.985024681091309 and perplexity is 146.20718353447285
At time: 272.68474102020264 and batch: 550, loss is 5.043572978973389 and perplexity is 155.02292031287254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.244494823699302 and perplexity of 189.5200500046259
Finished 34 epochs...
Completing Train Step...
At time: 274.3769886493683 and batch: 50, loss is 5.071500263214111 and perplexity is 159.41330995482664
At time: 275.02564668655396 and batch: 100, loss is 5.07918459892273 and perplexity is 160.64301402332197
At time: 275.6619873046875 and batch: 150, loss is 5.048225898742675 and perplexity is 155.74591022614558
At time: 276.2981159687042 and batch: 200, loss is 5.004317579269409 and perplexity is 149.05532999590866
At time: 276.9349822998047 and batch: 250, loss is 4.979392299652099 and perplexity is 145.38600367765164
At time: 277.5774278640747 and batch: 300, loss is 4.9724047565460205 and perplexity is 144.37365374837756
At time: 278.221816778183 and batch: 350, loss is 5.031382904052735 and perplexity is 153.14465068245482
At time: 278.8617534637451 and batch: 400, loss is 5.0380932903289795 and perplexity is 154.17576616707686
At time: 279.4955117702484 and batch: 450, loss is 4.974319524765015 and perplexity is 144.6503606624959
At time: 280.13036847114563 and batch: 500, loss is 4.97829080581665 and perplexity is 145.2259500560615
At time: 280.766863822937 and batch: 550, loss is 5.0371283912658695 and perplexity is 154.02707386281693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.240776548994348 and perplexity of 188.81667088538998
Finished 35 epochs...
Completing Train Step...
At time: 282.46272015571594 and batch: 50, loss is 5.065357303619384 and perplexity is 158.4370420899674
At time: 283.1097483634949 and batch: 100, loss is 5.07214319229126 and perplexity is 159.51583436151057
At time: 283.7448375225067 and batch: 150, loss is 5.0417414569854735 and perplexity is 154.73925227710745
At time: 284.3793671131134 and batch: 200, loss is 4.998066730499268 and perplexity is 148.12651363932096
At time: 285.04025077819824 and batch: 250, loss is 4.973263187408447 and perplexity is 144.49764175842975
At time: 285.68927025794983 and batch: 300, loss is 4.966895751953125 and perplexity is 143.58048541677198
At time: 286.33284759521484 and batch: 350, loss is 5.025523252487183 and perplexity is 152.24990041287768
At time: 286.98482608795166 and batch: 400, loss is 5.031148881912231 and perplexity is 153.10881563675605
At time: 287.6186316013336 and batch: 450, loss is 4.967264480590821 and perplexity is 143.63343741541863
At time: 288.25181007385254 and batch: 500, loss is 4.971341466903686 and perplexity is 144.22022432213623
At time: 288.88342094421387 and batch: 550, loss is 5.030177021026612 and perplexity is 152.9600874508547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.235555445894282 and perplexity of 187.83340867002278
Finished 36 epochs...
Completing Train Step...
At time: 290.5637640953064 and batch: 50, loss is 5.058228006362915 and perplexity is 157.3115141991761
At time: 291.2113914489746 and batch: 100, loss is 5.064749431610108 and perplexity is 158.34076191282537
At time: 291.84498405456543 and batch: 150, loss is 5.035325565338135 and perplexity is 153.7496400180907
At time: 292.4793071746826 and batch: 200, loss is 4.992098178863525 and perplexity is 147.24504605319677
At time: 293.11425852775574 and batch: 250, loss is 4.967944574356079 and perplexity is 143.73115484542856
At time: 293.7482042312622 and batch: 300, loss is 4.960584058761596 and perplexity is 142.67710337869613
At time: 294.3821039199829 and batch: 350, loss is 5.019772596359253 and perplexity is 151.3768762269452
At time: 295.0156238079071 and batch: 400, loss is 5.0257096767425535 and perplexity is 152.27828613300358
At time: 295.64795446395874 and batch: 450, loss is 4.961723728179932 and perplexity is 142.8398008031431
At time: 296.2800803184509 and batch: 500, loss is 4.965403633117676 and perplexity is 143.36640602574374
At time: 296.91639709472656 and batch: 550, loss is 5.024046697616577 and perplexity is 152.02526096792062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.23325364133145 and perplexity of 187.40155009059455
Finished 37 epochs...
Completing Train Step...
At time: 298.61223816871643 and batch: 50, loss is 5.052144317626953 and perplexity is 156.35738516702025
At time: 299.2626450061798 and batch: 100, loss is 5.058443832397461 and perplexity is 157.34546978359182
At time: 299.9009459018707 and batch: 150, loss is 5.029655027389526 and perplexity is 152.88026409393203
At time: 300.54976534843445 and batch: 200, loss is 4.986581230163575 and perplexity is 146.4349394009686
At time: 301.1968734264374 and batch: 250, loss is 4.961568403244018 and perplexity is 142.81761594321307
At time: 301.83715415000916 and batch: 300, loss is 4.955186595916748 and perplexity is 141.90908355815574
At time: 302.4867854118347 and batch: 350, loss is 5.0146260261535645 and perplexity is 150.5998058447635
At time: 303.1274025440216 and batch: 400, loss is 5.020989198684692 and perplexity is 151.56115376008879
At time: 303.7671389579773 and batch: 450, loss is 4.957076005935669 and perplexity is 142.1774614605156
At time: 304.41075682640076 and batch: 500, loss is 4.960300884246826 and perplexity is 142.63670657912004
At time: 305.0482015609741 and batch: 550, loss is 5.019029321670533 and perplexity is 151.26440343065747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.22890001662234 and perplexity of 186.58744750603657
Finished 38 epochs...
Completing Train Step...
At time: 306.74197721481323 and batch: 50, loss is 5.046939516067505 and perplexity is 155.5456901929031
At time: 307.395991563797 and batch: 100, loss is 5.053328504562378 and perplexity is 156.54265121295796
At time: 308.0442605018616 and batch: 150, loss is 5.025153760910034 and perplexity is 152.19365574865813
At time: 308.6795530319214 and batch: 200, loss is 4.982299156188965 and perplexity is 145.80923477082706
At time: 309.33517813682556 and batch: 250, loss is 4.958026695251465 and perplexity is 142.31269232515695
At time: 309.99096298217773 and batch: 300, loss is 4.951028871536255 and perplexity is 141.3202895713799
At time: 310.64321637153625 and batch: 350, loss is 5.01073691368103 and perplexity is 150.0152437122329
At time: 311.2847230434418 and batch: 400, loss is 5.015922269821167 and perplexity is 150.79514646660226
At time: 311.9183588027954 and batch: 450, loss is 4.953248920440674 and perplexity is 141.63437604007447
At time: 312.55193519592285 and batch: 500, loss is 4.955436191558838 and perplexity is 141.94450786767752
At time: 313.18604016304016 and batch: 550, loss is 5.0140361404418945 and perplexity is 150.51099536770639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.228568543779089 and perplexity of 186.52560908374187
Finished 39 epochs...
Completing Train Step...
At time: 314.88937044143677 and batch: 50, loss is 5.042202529907226 and perplexity is 154.81061480666546
At time: 315.55795192718506 and batch: 100, loss is 5.04801815032959 and perplexity is 155.71355762116903
At time: 316.2015058994293 and batch: 150, loss is 5.020999994277954 and perplexity is 151.56278996149098
At time: 316.8418288230896 and batch: 200, loss is 4.977396574020386 and perplexity is 145.09614244157683
At time: 317.47953248023987 and batch: 250, loss is 4.953356714248657 and perplexity is 141.64964417169935
At time: 318.1143653392792 and batch: 300, loss is 4.946163778305054 and perplexity is 140.63442294252337
At time: 318.7495355606079 and batch: 350, loss is 5.004848890304565 and perplexity is 149.13454577983094
At time: 319.38488388061523 and batch: 400, loss is 5.009408349990845 and perplexity is 149.81607124241458
At time: 320.01866340637207 and batch: 450, loss is 4.948462038040161 and perplexity is 140.95800907419917
At time: 320.6521246433258 and batch: 500, loss is 4.9504494380950925 and perplexity is 141.23842758876307
At time: 321.2850618362427 and batch: 550, loss is 5.009015951156616 and perplexity is 149.75729512330483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.223672745075632 and perplexity of 185.6146490067643
Finished 40 epochs...
Completing Train Step...
At time: 322.9660973548889 and batch: 50, loss is 5.03727349281311 and perplexity is 154.04942505110816
At time: 323.6127998828888 and batch: 100, loss is 5.042542543411255 and perplexity is 154.86326145604534
At time: 324.24634194374084 and batch: 150, loss is 5.01559947013855 and perplexity is 150.74647769673706
At time: 324.87942337989807 and batch: 200, loss is 4.972716455459595 and perplexity is 144.4186618735271
At time: 325.5128653049469 and batch: 250, loss is 4.94850788116455 and perplexity is 140.96447117786317
At time: 326.1460039615631 and batch: 300, loss is 4.941138381958008 and perplexity is 139.9294520898194
At time: 326.779109954834 and batch: 350, loss is 5.000258493423462 and perplexity is 148.45152788698536
At time: 327.4133424758911 and batch: 400, loss is 5.004516716003418 and perplexity is 149.0850153431342
At time: 328.04668951034546 and batch: 450, loss is 4.9441502571105955 and perplexity is 140.3515374448053
At time: 328.6824402809143 and batch: 500, loss is 4.945159139633179 and perplexity is 140.49320710995923
At time: 329.3170976638794 and batch: 550, loss is 5.003268613815307 and perplexity is 148.89905808024295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.220296332176695 and perplexity of 184.98899414058704
Finished 41 epochs...
Completing Train Step...
At time: 331.00954151153564 and batch: 50, loss is 5.031925754547119 and perplexity is 153.22780790071656
At time: 331.65998101234436 and batch: 100, loss is 5.036853513717651 and perplexity is 153.9847410968243
At time: 332.2965233325958 and batch: 150, loss is 5.011529321670532 and perplexity is 150.13416410040782
At time: 332.93213844299316 and batch: 200, loss is 4.9677708721160885 and perplexity is 143.70619059011176
At time: 333.56655859947205 and batch: 250, loss is 4.944425144195557 and perplexity is 140.39012357297452
At time: 334.20214343070984 and batch: 300, loss is 4.9383392429351805 and perplexity is 139.53831777507762
At time: 334.8385581970215 and batch: 350, loss is 4.997385272979736 and perplexity is 148.02560609876556
At time: 335.47513461112976 and batch: 400, loss is 5.000831899642944 and perplexity is 148.53667532606997
At time: 336.11162185668945 and batch: 450, loss is 4.941272239685059 and perplexity is 139.94818398189994
At time: 336.7475905418396 and batch: 500, loss is 4.9415993404388425 and perplexity is 139.99396862605863
At time: 337.3834228515625 and batch: 550, loss is 4.999777679443359 and perplexity is 148.38016747392663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.21823347375748 and perplexity of 184.60778136573694
Finished 42 epochs...
Completing Train Step...
At time: 339.0897738933563 and batch: 50, loss is 5.029079389572144 and perplexity is 152.79228575664652
At time: 339.75328254699707 and batch: 100, loss is 5.034131069183349 and perplexity is 153.5660963072415
At time: 340.38831329345703 and batch: 150, loss is 5.007212266921997 and perplexity is 149.48742370564634
At time: 341.0236961841583 and batch: 200, loss is 4.964330739974976 and perplexity is 143.2126716768365
At time: 341.65872049331665 and batch: 250, loss is 4.940159959793091 and perplexity is 139.7926089684583
At time: 342.29435181617737 and batch: 300, loss is 4.934989862442016 and perplexity is 139.07173267717832
At time: 342.929678440094 and batch: 350, loss is 4.993371610641479 and perplexity is 147.4326720130621
At time: 343.5651481151581 and batch: 400, loss is 4.997332639694214 and perplexity is 148.0178152298065
At time: 344.20395612716675 and batch: 450, loss is 4.937780647277832 and perplexity is 139.46039404269027
At time: 344.84728598594666 and batch: 500, loss is 4.938409175872803 and perplexity is 139.5480764407726
At time: 345.4824125766754 and batch: 550, loss is 4.99576696395874 and perplexity is 147.78624865447142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.216685518305352 and perplexity of 184.32223780543964
Finished 43 epochs...
Completing Train Step...
At time: 347.18631863594055 and batch: 50, loss is 5.025012836456299 and perplexity is 152.17220945204915
At time: 347.8431508541107 and batch: 100, loss is 5.030522756576538 and perplexity is 153.01298033375437
At time: 348.4774079322815 and batch: 150, loss is 5.003839721679688 and perplexity is 148.9841197907053
At time: 349.1147117614746 and batch: 200, loss is 4.960483264923096 and perplexity is 142.66272313051283
At time: 349.7492468357086 and batch: 250, loss is 4.935961580276489 and perplexity is 139.20693683960098
At time: 350.3841905593872 and batch: 300, loss is 4.933038129806518 and perplexity is 138.80056654580963
At time: 351.0181622505188 and batch: 350, loss is 4.989666919708252 and perplexity is 146.8874900187293
At time: 351.66724729537964 and batch: 400, loss is 4.994988355636597 and perplexity is 147.67122583604342
At time: 352.305340051651 and batch: 450, loss is 4.934699773788452 and perplexity is 139.03139539647375
At time: 352.9578323364258 and batch: 500, loss is 4.935915975570679 and perplexity is 139.20058849295805
At time: 353.5923070907593 and batch: 550, loss is 4.992447214126587 and perplexity is 147.29644873673323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.215212233523105 and perplexity of 184.05087860122777
Finished 44 epochs...
Completing Train Step...
At time: 355.277973651886 and batch: 50, loss is 5.022523584365845 and perplexity is 151.7938855287232
At time: 355.9271402359009 and batch: 100, loss is 5.026669683456421 and perplexity is 152.42454450334347
At time: 356.56352138519287 and batch: 150, loss is 5.000604333877564 and perplexity is 148.502877309644
At time: 357.20005464553833 and batch: 200, loss is 4.957443399429321 and perplexity is 142.22970613138864
At time: 357.83702087402344 and batch: 250, loss is 4.9328743267059325 and perplexity is 138.77783244465533
At time: 358.48253989219666 and batch: 300, loss is 4.93007942199707 and perplexity is 138.3905031536347
At time: 359.12344694137573 and batch: 350, loss is 4.986901588439942 and perplexity is 146.48185856082435
At time: 359.7648413181305 and batch: 400, loss is 4.991693801879883 and perplexity is 147.18551558280478
At time: 360.40945506095886 and batch: 450, loss is 4.931755542755127 and perplexity is 138.62265685315515
At time: 361.04475474357605 and batch: 500, loss is 4.9327193641662594 and perplexity is 138.75632874546636
At time: 361.6818413734436 and batch: 550, loss is 4.989320259094239 and perplexity is 146.83657873622656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2131172342503325 and perplexity of 183.66569576416813
Finished 45 epochs...
Completing Train Step...
At time: 363.3798089027405 and batch: 50, loss is 5.019623260498047 and perplexity is 151.35427191862613
At time: 364.0271165370941 and batch: 100, loss is 5.02419508934021 and perplexity is 152.04782193232012
At time: 364.6617217063904 and batch: 150, loss is 4.997998485565185 and perplexity is 148.116405100094
At time: 365.3095510005951 and batch: 200, loss is 4.954806518554688 and perplexity is 141.85515737673407
At time: 365.9467742443085 and batch: 250, loss is 4.931748657226563 and perplexity is 138.62170236617783
At time: 366.5840368270874 and batch: 300, loss is 4.92753002166748 and perplexity is 138.0381397078722
At time: 367.2207190990448 and batch: 350, loss is 4.984067420959473 and perplexity is 146.06729219357143
At time: 367.8558819293976 and batch: 400, loss is 4.98790581703186 and perplexity is 146.62903371777028
At time: 368.4897391796112 and batch: 450, loss is 4.92825457572937 and perplexity is 138.13819204497975
At time: 369.12472581863403 and batch: 500, loss is 4.929059925079346 and perplexity is 138.24948635753586
At time: 369.77978801727295 and batch: 550, loss is 4.984626312255859 and perplexity is 146.14895074886743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.207241951151097 and perplexity of 182.58977158050126
Finished 46 epochs...
Completing Train Step...
At time: 371.4596538543701 and batch: 50, loss is 5.013984756469727 and perplexity is 150.50326171360408
At time: 372.1066834926605 and batch: 100, loss is 5.019697113037109 and perplexity is 151.36545022867338
At time: 372.7406907081604 and batch: 150, loss is 4.993919563293457 and perplexity is 147.51348027420843
At time: 373.37474298477173 and batch: 200, loss is 4.950599927902221 and perplexity is 141.25968413189648
At time: 374.0148150920868 and batch: 250, loss is 4.927388553619385 and perplexity is 138.0186131029129
At time: 374.6520481109619 and batch: 300, loss is 4.923371868133545 and perplexity is 137.46534763343152
At time: 375.2929883003235 and batch: 350, loss is 4.980710897445679 and perplexity is 145.57783578822233
At time: 375.9342722892761 and batch: 400, loss is 4.984647073745728 and perplexity is 146.15198505032592
At time: 376.583411693573 and batch: 450, loss is 4.925313167572021 and perplexity is 137.7324682322463
At time: 377.2202560901642 and batch: 500, loss is 4.926358242034912 and perplexity is 137.87648415808223
At time: 377.8536078929901 and batch: 550, loss is 4.982044687271118 and perplexity is 145.77213557313942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.204562248067653 and perplexity of 182.10114019250744
Finished 47 epochs...
Completing Train Step...
At time: 379.5574953556061 and batch: 50, loss is 5.010679969787597 and perplexity is 150.0067015033973
At time: 380.2049355506897 and batch: 100, loss is 5.016310329437256 and perplexity is 150.8536753288576
At time: 380.8387875556946 and batch: 150, loss is 4.990229578018188 and perplexity is 146.97016074111738
At time: 381.49309182167053 and batch: 200, loss is 4.947445192337036 and perplexity is 140.81474937719858
At time: 382.1421389579773 and batch: 250, loss is 4.923856000900269 and perplexity is 137.5319152249859
At time: 382.7867383956909 and batch: 300, loss is 4.919844760894775 and perplexity is 136.98134667412432
At time: 383.43225049972534 and batch: 350, loss is 4.977174520492554 and perplexity is 145.06392690819166
At time: 384.0704915523529 and batch: 400, loss is 4.980911626815796 and perplexity is 145.60706046853076
At time: 384.70636773109436 and batch: 450, loss is 4.920407381057739 and perplexity is 137.05843682591652
At time: 385.33924102783203 and batch: 500, loss is 4.922803173065185 and perplexity is 137.3871939930633
At time: 385.97211742401123 and batch: 550, loss is 4.978118085861206 and perplexity is 145.2008688025174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.20147153164478 and perplexity of 181.53918607563975
Finished 48 epochs...
Completing Train Step...
At time: 387.653559923172 and batch: 50, loss is 5.006469993591309 and perplexity is 149.37650434908917
At time: 388.3024549484253 and batch: 100, loss is 5.01353777885437 and perplexity is 150.43600515678736
At time: 388.93804931640625 and batch: 150, loss is 4.986774177551269 and perplexity is 146.4631963659594
At time: 389.5744061470032 and batch: 200, loss is 4.943589706420898 and perplexity is 140.2728853399958
At time: 390.20989894866943 and batch: 250, loss is 4.920159187316894 and perplexity is 137.02442400082612
At time: 390.84549283981323 and batch: 300, loss is 4.91745283126831 and perplexity is 136.65408847805801
At time: 391.4807562828064 and batch: 350, loss is 4.973721466064453 and perplexity is 144.56387711945771
At time: 392.12487959861755 and batch: 400, loss is 4.978495140075683 and perplexity is 145.25562772493754
At time: 392.76660990715027 and batch: 450, loss is 4.918569946289063 and perplexity is 136.80683211316403
At time: 393.4025032520294 and batch: 500, loss is 4.9207549571990965 and perplexity is 137.1060833484465
At time: 394.0372860431671 and batch: 550, loss is 4.976805610656738 and perplexity is 145.01042126871837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.199159662774268 and perplexity of 181.11997604859317
Finished 49 epochs...
Completing Train Step...
At time: 395.7313060760498 and batch: 50, loss is 5.004107971191406 and perplexity is 149.02409006885705
At time: 396.37696647644043 and batch: 100, loss is 5.010658636093139 and perplexity is 150.00350134039653
At time: 397.01284193992615 and batch: 150, loss is 4.984769420623779 and perplexity is 146.1698673833194
At time: 397.6473536491394 and batch: 200, loss is 4.941534204483032 and perplexity is 139.98485028207372
At time: 398.2816913127899 and batch: 250, loss is 4.917606887817382 and perplexity is 136.6751425570633
At time: 398.9307658672333 and batch: 300, loss is 4.916064281463623 and perplexity is 136.46446914860013
At time: 399.5712606906891 and batch: 350, loss is 4.971555233001709 and perplexity is 144.25105701212007
At time: 400.2054400444031 and batch: 400, loss is 4.97586799621582 and perplexity is 144.87452112457808
At time: 400.8514552116394 and batch: 450, loss is 4.916489858627319 and perplexity is 136.52255767002308
At time: 401.497540473938 and batch: 500, loss is 4.918633279800415 and perplexity is 136.81549684459972
At time: 402.13927149772644 and batch: 550, loss is 4.9747504615783695 and perplexity is 144.71270926115872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.198012981008976 and perplexity of 180.91240810466013
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f418b26d518>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 3.860431593547222, 'lr': 27.04765542952505, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.545279976180985}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.859804630279541 and batch: 50, loss is 6.780724744796753 and perplexity is 880.7067805224901
At time: 1.516625165939331 and batch: 100, loss is 5.991989221572876 and perplexity is 400.20992485224656
At time: 2.1507081985473633 and batch: 150, loss is 6.022050285339356 and perplexity is 412.4233149492857
At time: 2.7846996784210205 and batch: 200, loss is 6.007960357666016 and perplexity is 406.65304706151244
At time: 3.418822765350342 and batch: 250, loss is 6.06809778213501 and perplexity is 431.85841096901123
At time: 4.050877571105957 and batch: 300, loss is 6.152715759277344 and perplexity is 469.99204041682225
At time: 4.685551643371582 and batch: 350, loss is 6.242480506896973 and perplexity is 514.1322391090495
At time: 5.326881170272827 and batch: 400, loss is 6.305397834777832 and perplexity is 547.5193671077641
At time: 5.965834856033325 and batch: 450, loss is 6.440288419723511 and perplexity is 626.5874939441192
At time: 6.596540927886963 and batch: 500, loss is 6.6185682678222655 and perplexity is 748.8721448582297
At time: 7.2264604568481445 and batch: 550, loss is 6.730141258239746 and perplexity is 837.2655282559052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 7.726120319772274 and perplexity of 2266.7906958859276
Finished 1 epochs...
Completing Train Step...
At time: 8.933922529220581 and batch: 50, loss is 6.8605921268463135 and perplexity is 953.931748891246
At time: 9.57351565361023 and batch: 100, loss is 6.621546468734741 and perplexity is 751.1057609912218
At time: 10.199829816818237 and batch: 150, loss is 6.99007661819458 and perplexity is 1085.8046653921056
At time: 10.839042901992798 and batch: 200, loss is 7.060778732299805 and perplexity is 1165.352310000156
At time: 11.465534448623657 and batch: 250, loss is 6.853460893630982 and perplexity is 947.1532374179305
At time: 12.092396020889282 and batch: 300, loss is 6.94343768119812 and perplexity is 1036.3266591686395
At time: 12.721436262130737 and batch: 350, loss is 6.738116493225098 and perplexity is 843.9696153899419
At time: 13.351402997970581 and batch: 400, loss is 6.698960933685303 and perplexity is 811.5621200494411
At time: 13.981105327606201 and batch: 450, loss is 6.738097248077392 and perplexity is 843.9533732263266
At time: 14.61432695388794 and batch: 500, loss is 6.662194623947143 and perplexity is 782.2658339939547
At time: 15.246895790100098 and batch: 550, loss is 6.571457223892212 and perplexity is 714.4101405012655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.290242945894282 and perplexity of 539.2843300847658
Finished 2 epochs...
Completing Train Step...
At time: 16.90945839881897 and batch: 50, loss is 6.663456573486328 and perplexity is 783.2536371504613
At time: 17.54699444770813 and batch: 100, loss is 6.5702393436431885 and perplexity is 713.5406041045255
At time: 18.17304491996765 and batch: 150, loss is 6.733972244262695 and perplexity is 840.4792326900542
At time: 18.797356367111206 and batch: 200, loss is 6.829515361785889 and perplexity is 924.742538290162
At time: 19.42106533050537 and batch: 250, loss is 6.479881362915039 and perplexity is 651.8936029266125
At time: 20.045575857162476 and batch: 300, loss is 6.761334342956543 and perplexity is 863.7940246236516
At time: 20.673500299453735 and batch: 350, loss is 6.549591960906983 and perplexity is 698.9589130601319
At time: 21.32353377342224 and batch: 400, loss is 6.715364980697632 and perplexity is 824.984815489175
At time: 21.952308654785156 and batch: 450, loss is 6.408348398208618 and perplexity is 606.8905114152917
At time: 22.578364372253418 and batch: 500, loss is 6.608636751174926 and perplexity is 741.4715192331131
At time: 23.20658779144287 and batch: 550, loss is 6.6834142303466795 and perplexity is 799.0425755140426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.94411906790226 and perplexity of 1037.0330390069273
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 24.89487361907959 and batch: 50, loss is 6.104902000427246 and perplexity is 448.0487307196165
At time: 25.539169788360596 and batch: 100, loss is 5.939452352523804 and perplexity is 379.7269160970786
At time: 26.16638994216919 and batch: 150, loss is 5.930876874923706 and perplexity is 376.48449894204435
At time: 26.806307554244995 and batch: 200, loss is 5.878027982711792 and perplexity is 357.1043309748404
At time: 27.43224287033081 and batch: 250, loss is 5.86200587272644 and perplexity is 351.4283580606575
At time: 28.063774347305298 and batch: 300, loss is 5.838556861877441 and perplexity is 343.28357740617565
At time: 28.705329418182373 and batch: 350, loss is 5.885403280258179 and perplexity is 359.7478179396475
At time: 29.340633153915405 and batch: 400, loss is 5.887218589782715 and perplexity is 360.40146468606724
At time: 29.97782254219055 and batch: 450, loss is 5.815767669677735 and perplexity is 335.54889036264314
At time: 30.612040042877197 and batch: 500, loss is 5.838218421936035 and perplexity is 343.16741619025896
At time: 31.24082899093628 and batch: 550, loss is 5.877633266448974 and perplexity is 356.96340390281523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.933287924908577 and perplexity of 377.3933170502068
Finished 4 epochs...
Completing Train Step...
At time: 32.8915741443634 and batch: 50, loss is 5.905230951309204 and perplexity is 366.9519640383948
At time: 33.530256271362305 and batch: 100, loss is 5.9012344932556156 and perplexity is 365.4883824256822
At time: 34.16855502128601 and batch: 150, loss is 5.893933296203613 and perplexity is 362.8295976920861
At time: 34.793676137924194 and batch: 200, loss is 5.81872449874878 and perplexity is 336.54251934803113
At time: 35.41901659965515 and batch: 250, loss is 5.8057333278656005 and perplexity is 332.19871461651087
At time: 36.04410982131958 and batch: 300, loss is 5.81112075805664 and perplexity is 333.99324160507797
At time: 36.669636726379395 and batch: 350, loss is 5.854461174011231 and perplexity is 348.7869139588533
At time: 37.29632544517517 and batch: 400, loss is 5.837686977386475 and perplexity is 342.98509018970884
At time: 37.92159676551819 and batch: 450, loss is 5.777886981964111 and perplexity is 323.07580366713864
At time: 38.54909873008728 and batch: 500, loss is 5.7883620452880855 and perplexity is 326.4778302833357
At time: 39.17451333999634 and batch: 550, loss is 5.824680662155151 and perplexity is 338.5530030274835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.9005185391040555 and perplexity of 365.2268031514948
Finished 5 epochs...
Completing Train Step...
At time: 40.919025897979736 and batch: 50, loss is 5.858461885452271 and perplexity is 350.1851047700839
At time: 41.561542987823486 and batch: 100, loss is 5.85637526512146 and perplexity is 349.45516323119733
At time: 42.190221548080444 and batch: 150, loss is 5.842973375320435 and perplexity is 344.803046849315
At time: 42.83102107048035 and batch: 200, loss is 5.766466732025147 and perplexity is 319.40718538123025
At time: 43.457979679107666 and batch: 250, loss is 5.762706861495972 and perplexity is 318.20851056127475
At time: 44.0833044052124 and batch: 300, loss is 5.760421991348267 and perplexity is 317.482275427187
At time: 44.70863366127014 and batch: 350, loss is 5.807438468933105 and perplexity is 332.7656434968714
At time: 45.33410978317261 and batch: 400, loss is 5.803028726577759 and perplexity is 331.30146344541566
At time: 45.961222648620605 and batch: 450, loss is 5.7403484725952145 and perplexity is 311.17282729292606
At time: 46.58984899520874 and batch: 500, loss is 5.736035785675049 and perplexity is 309.8337259486997
At time: 47.22682809829712 and batch: 550, loss is 5.765835771560669 and perplexity is 319.20571564158877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.846643975440492 and perplexity of 346.0710066174811
Finished 6 epochs...
Completing Train Step...
At time: 48.901222944259644 and batch: 50, loss is 5.808321504592896 and perplexity is 333.05961720196245
At time: 49.5422637462616 and batch: 100, loss is 5.810019578933716 and perplexity is 333.62565764526
At time: 50.180256605148315 and batch: 150, loss is 5.804354944229126 and perplexity is 331.7411327783329
At time: 50.804988861083984 and batch: 200, loss is 5.730000228881836 and perplexity is 307.96933886599044
At time: 51.43372678756714 and batch: 250, loss is 5.727456436157227 and perplexity is 307.1869242745625
At time: 52.073140382766724 and batch: 300, loss is 5.726972303390503 and perplexity is 307.03824101313546
At time: 52.714014291763306 and batch: 350, loss is 5.771666126251221 and perplexity is 321.07223412805416
At time: 53.35353374481201 and batch: 400, loss is 5.770136928558349 and perplexity is 320.58162642207145
At time: 53.988465785980225 and batch: 450, loss is 5.703849897384644 and perplexity is 300.0202275004218
At time: 54.61480975151062 and batch: 500, loss is 5.693331098556518 and perplexity is 296.8809149274336
At time: 55.24005341529846 and batch: 550, loss is 5.721466073989868 and perplexity is 305.352263971226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.7814272616771945 and perplexity of 324.2216094203272
Finished 7 epochs...
Completing Train Step...
At time: 56.89254665374756 and batch: 50, loss is 5.734911146163941 and perplexity is 309.48547056621203
At time: 57.53165626525879 and batch: 100, loss is 5.744744081497192 and perplexity is 312.5436318959342
At time: 58.15696597099304 and batch: 150, loss is 5.721854667663575 and perplexity is 305.4709449871098
At time: 58.795058488845825 and batch: 200, loss is 5.650010013580323 and perplexity is 284.2943126136022
At time: 59.418946743011475 and batch: 250, loss is 5.641783790588379 and perplexity is 281.9652370842111
At time: 60.04262065887451 and batch: 300, loss is 5.642129135131836 and perplexity is 282.0626290562078
At time: 60.66851758956909 and batch: 350, loss is 5.686976079940796 and perplexity is 295.00021346236514
At time: 61.29439902305603 and batch: 400, loss is 5.679839115142823 and perplexity is 292.90230258649626
At time: 61.92047357559204 and batch: 450, loss is 5.605950632095337 and perplexity is 272.0404128993334
At time: 62.55820274353027 and batch: 500, loss is 5.608149757385254 and perplexity is 272.6393221480895
At time: 63.18487286567688 and batch: 550, loss is 5.651147317886353 and perplexity is 284.6178256910545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.735072359125665 and perplexity of 309.5353676574535
Finished 8 epochs...
Completing Train Step...
At time: 64.84002327919006 and batch: 50, loss is 5.670930500030518 and perplexity is 290.304537134136
At time: 65.49210000038147 and batch: 100, loss is 5.675483846664429 and perplexity is 291.62940833096553
At time: 66.12651348114014 and batch: 150, loss is 5.654813079833985 and perplexity is 285.6630815451627
At time: 66.75534534454346 and batch: 200, loss is 5.584965076446533 and perplexity is 266.39097925740367
At time: 67.38263320922852 and batch: 250, loss is 5.576736736297607 and perplexity is 264.2080170617539
At time: 68.00878024101257 and batch: 300, loss is 5.571904935836792 and perplexity is 262.93449582108855
At time: 68.63701391220093 and batch: 350, loss is 5.613269929885864 and perplexity is 274.0388623941307
At time: 69.26547455787659 and batch: 400, loss is 5.610903024673462 and perplexity is 273.3910053928401
At time: 69.89374828338623 and batch: 450, loss is 5.542568397521973 and perplexity is 255.33295453371775
At time: 70.52174663543701 and batch: 500, loss is 5.549093770980835 and perplexity is 257.0045453641978
At time: 71.14876008033752 and batch: 550, loss is 5.592803621292115 and perplexity is 268.48730222190613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.691975695021609 and perplexity of 296.47879406547486
Finished 9 epochs...
Completing Train Step...
At time: 72.83627533912659 and batch: 50, loss is 5.624890270233155 and perplexity is 277.24186116920487
At time: 73.49030828475952 and batch: 100, loss is 5.624985065460205 and perplexity is 277.26814362008804
At time: 74.12162137031555 and batch: 150, loss is 5.603243570327759 and perplexity is 271.30497858051746
At time: 74.76022553443909 and batch: 200, loss is 5.53670973777771 and perplexity is 253.84141909718986
At time: 75.38846325874329 and batch: 250, loss is 5.535504350662231 and perplexity is 253.535626257635
At time: 76.01358318328857 and batch: 300, loss is 5.527514705657959 and perplexity is 251.51803725049152
At time: 76.64101600646973 and batch: 350, loss is 5.558858528137207 and perplexity is 259.52642507071346
At time: 77.26949048042297 and batch: 400, loss is 5.544066524505615 and perplexity is 255.71576239851632
At time: 77.89883303642273 and batch: 450, loss is 5.473728685379029 and perplexity is 238.34725987936562
At time: 78.5266923904419 and batch: 500, loss is 5.476275072097779 and perplexity is 238.95495756466676
At time: 79.15314078330994 and batch: 550, loss is 5.510878601074219 and perplexity is 247.368369734205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.61117878366024 and perplexity of 273.46640581517556
Finished 10 epochs...
Completing Train Step...
At time: 80.79906344413757 and batch: 50, loss is 5.538737716674805 and perplexity is 254.35672647796582
At time: 81.44638228416443 and batch: 100, loss is 5.53016435623169 and perplexity is 252.18535585233667
At time: 82.07173585891724 and batch: 150, loss is 5.510741271972656 and perplexity is 247.3344011907226
At time: 82.70586681365967 and batch: 200, loss is 5.455533218383789 and perplexity is 234.04963737203687
At time: 83.33110046386719 and batch: 250, loss is 5.448282270431519 and perplexity is 232.35869351477098
At time: 83.96202778816223 and batch: 300, loss is 5.443365774154663 and perplexity is 231.21910654625543
At time: 84.58860278129578 and batch: 350, loss is 5.485864753723145 and perplexity is 241.25748212387475
At time: 85.22354173660278 and batch: 400, loss is 5.492735214233399 and perplexity is 242.9207392562464
At time: 85.86138272285461 and batch: 450, loss is 5.4239301586151125 and perplexity is 226.76861004995416
At time: 86.50173807144165 and batch: 500, loss is 5.440609035491943 and perplexity is 230.58257367618293
At time: 87.1303060054779 and batch: 550, loss is 5.485027303695679 and perplexity is 241.05552561488884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.603647434965093 and perplexity of 271.4145711961001
Finished 11 epochs...
Completing Train Step...
At time: 88.79671263694763 and batch: 50, loss is 5.519079666137696 and perplexity is 249.40539530156263
At time: 89.43663740158081 and batch: 100, loss is 5.515005683898925 and perplexity is 248.39138907501177
At time: 90.07457566261292 and batch: 150, loss is 5.492739286422729 and perplexity is 242.92172847750314
At time: 90.7148790359497 and batch: 200, loss is 5.438777389526368 and perplexity is 230.16061459317314
At time: 91.34522271156311 and batch: 250, loss is 5.42692795753479 and perplexity is 227.4494367250433
At time: 91.96775388717651 and batch: 300, loss is 5.421090106964112 and perplexity is 226.12548916513066
At time: 92.59140729904175 and batch: 350, loss is 5.471825284957886 and perplexity is 237.8940210888232
At time: 93.21590447425842 and batch: 400, loss is 5.470766820907593 and perplexity is 237.6423520345407
At time: 93.8403971195221 and batch: 450, loss is 5.4125810241699215 and perplexity is 224.20953173645592
At time: 94.46253848075867 and batch: 500, loss is 5.4221782207489015 and perplexity is 226.3716733408912
At time: 95.08519792556763 and batch: 550, loss is 5.464741525650024 and perplexity is 236.2147917527921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.588754856840093 and perplexity of 267.4024579981568
Finished 12 epochs...
Completing Train Step...
At time: 96.72977352142334 and batch: 50, loss is 5.501911201477051 and perplexity is 245.1600350234424
At time: 97.36924171447754 and batch: 100, loss is 5.49766098022461 and perplexity is 244.1202618306352
At time: 97.99439001083374 and batch: 150, loss is 5.4735744667053225 and perplexity is 238.3105051152739
At time: 98.61932587623596 and batch: 200, loss is 5.422894306182862 and perplexity is 226.53383285193436
At time: 99.24585771560669 and batch: 250, loss is 5.410618858337402 and perplexity is 223.77002678576756
At time: 99.87316584587097 and batch: 300, loss is 5.3986739158630375 and perplexity is 221.1130072533113
At time: 100.50056457519531 and batch: 350, loss is 5.450145111083985 and perplexity is 232.7919441481625
At time: 101.12739086151123 and batch: 400, loss is 5.454826564788818 and perplexity is 233.88430377805557
At time: 101.75427341461182 and batch: 450, loss is 5.38986029624939 and perplexity is 219.17276415785798
At time: 102.38073134422302 and batch: 500, loss is 5.407703647613525 and perplexity is 223.11863993028766
At time: 103.00502562522888 and batch: 550, loss is 5.4495031356811525 and perplexity is 232.6425454063316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.5729123379321805 and perplexity of 263.19951004866004
Finished 13 epochs...
Completing Train Step...
At time: 104.64892601966858 and batch: 50, loss is 5.482512664794922 and perplexity is 240.45011952081185
At time: 105.28871202468872 and batch: 100, loss is 5.474809532165527 and perplexity is 238.60501602163285
At time: 105.913498878479 and batch: 150, loss is 5.452663564682007 and perplexity is 233.37895873172116
At time: 106.55152153968811 and batch: 200, loss is 5.397840452194214 and perplexity is 220.92879437309668
At time: 107.1760687828064 and batch: 250, loss is 5.390957136154174 and perplexity is 219.4132934785926
At time: 107.79904818534851 and batch: 300, loss is 5.379775438308716 and perplexity is 216.97354600731674
At time: 108.42357993125916 and batch: 350, loss is 5.430436000823975 and perplexity is 228.2487403713615
At time: 109.05302810668945 and batch: 400, loss is 5.4334555339813235 and perplexity is 228.9389865972709
At time: 109.67721629142761 and batch: 450, loss is 5.369007997512817 and perplexity is 214.6498288901566
At time: 110.29778790473938 and batch: 500, loss is 5.382290267944336 and perplexity is 217.51988419675956
At time: 110.92115020751953 and batch: 550, loss is 5.429435043334961 and perplexity is 228.02038739020801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.565007635887633 and perplexity of 261.1271976400947
Finished 14 epochs...
Completing Train Step...
At time: 112.56606960296631 and batch: 50, loss is 5.458270053863526 and perplexity is 234.6910700711253
At time: 113.20354700088501 and batch: 100, loss is 5.451465406417847 and perplexity is 233.0995012542107
At time: 113.82807087898254 and batch: 150, loss is 5.433946256637573 and perplexity is 229.05135971465532
At time: 114.45239973068237 and batch: 200, loss is 5.3788206672668455 and perplexity is 216.7664848124829
At time: 115.07695984840393 and batch: 250, loss is 5.3611552524566655 and perplexity is 212.970839473439
At time: 115.7012951374054 and batch: 300, loss is 5.353488130569458 and perplexity is 211.34420984101513
At time: 116.3274872303009 and batch: 350, loss is 5.4029765224456785 and perplexity is 222.06641914112927
At time: 116.95607376098633 and batch: 400, loss is 5.41056902885437 and perplexity is 223.75887671881807
At time: 117.58950734138489 and batch: 450, loss is 5.345808591842651 and perplexity is 209.72739992003744
At time: 118.21631574630737 and batch: 500, loss is 5.354961462020874 and perplexity is 211.65581940816884
At time: 118.84219408035278 and batch: 550, loss is 5.4021618366241455 and perplexity is 221.8855784521912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.548889809466423 and perplexity of 256.9521316732841
Finished 15 epochs...
Completing Train Step...
At time: 120.51976037025452 and batch: 50, loss is 5.435459632873535 and perplexity is 229.39826303066988
At time: 121.16660356521606 and batch: 100, loss is 5.422352352142334 and perplexity is 226.41109518799465
At time: 121.79575872421265 and batch: 150, loss is 5.404090747833252 and perplexity is 222.31398908180734
At time: 122.44130325317383 and batch: 200, loss is 5.350134115219117 and perplexity is 210.63654553886965
At time: 123.079026222229 and batch: 250, loss is 5.33862811088562 and perplexity is 208.2268501007154
At time: 123.70782327651978 and batch: 300, loss is 5.339430665969848 and perplexity is 208.39403069478115
At time: 124.33748602867126 and batch: 350, loss is 5.386730585098267 and perplexity is 218.48788900358883
At time: 124.96735787391663 and batch: 400, loss is 5.389125089645386 and perplexity is 219.01168611430765
At time: 125.61059784889221 and batch: 450, loss is 5.327624263763428 and perplexity is 205.94811410004635
At time: 126.24019694328308 and batch: 500, loss is 5.340382080078125 and perplexity is 208.5923940635557
At time: 126.8764500617981 and batch: 550, loss is 5.385448179244995 and perplexity is 218.20787843783054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.540397481715425 and perplexity of 254.7792494278332
Finished 16 epochs...
Completing Train Step...
At time: 128.55188393592834 and batch: 50, loss is 5.420156230926514 and perplexity is 225.91441456340587
At time: 129.20333123207092 and batch: 100, loss is 5.410020914077759 and perplexity is 223.6362647778685
At time: 129.8314254283905 and batch: 150, loss is 5.385884466171265 and perplexity is 218.3031004529518
At time: 130.45954489707947 and batch: 200, loss is 5.333961267471313 and perplexity is 207.2573520041368
At time: 131.08728528022766 and batch: 250, loss is 5.318686122894287 and perplexity is 204.1155230235988
At time: 131.71494364738464 and batch: 300, loss is 5.312796220779419 and perplexity is 202.91683611244792
At time: 132.34314036369324 and batch: 350, loss is 5.361907262802124 and perplexity is 213.13105598268973
At time: 132.97103834152222 and batch: 400, loss is 5.3665446949005124 and perplexity is 214.1217321040519
At time: 133.59736394882202 and batch: 450, loss is 5.301406688690186 and perplexity is 200.61881980312637
At time: 134.2342357635498 and batch: 500, loss is 5.317775573730469 and perplexity is 203.92975039517833
At time: 134.87151384353638 and batch: 550, loss is 5.358657655715942 and perplexity is 212.4395879011605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.519568909990027 and perplexity of 249.52744521160176
Finished 17 epochs...
Completing Train Step...
At time: 136.53211045265198 and batch: 50, loss is 5.394267854690551 and perplexity is 220.14091294345363
At time: 137.17078852653503 and batch: 100, loss is 5.385041561126709 and perplexity is 218.11916919751937
At time: 137.79665565490723 and batch: 150, loss is 5.363667421340942 and perplexity is 213.50653078141158
At time: 138.43422198295593 and batch: 200, loss is 5.313215065002441 and perplexity is 203.00184445839207
At time: 139.0593867301941 and batch: 250, loss is 5.302718267440796 and perplexity is 200.88211981575859
At time: 139.6844835281372 and batch: 300, loss is 5.304217977523804 and perplexity is 201.18361077434975
At time: 140.31199264526367 and batch: 350, loss is 5.349581413269043 and perplexity is 210.5201584760346
At time: 140.94987607002258 and batch: 400, loss is 5.354574394226074 and perplexity is 211.5739101101423
At time: 141.58239650726318 and batch: 450, loss is 5.288006658554077 and perplexity is 197.94845302717835
At time: 142.2094223499298 and batch: 500, loss is 5.30714108467102 and perplexity is 201.7725523752549
At time: 142.834614276886 and batch: 550, loss is 5.346403350830078 and perplexity is 209.85217427770678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.521021416846742 and perplexity of 249.89014888772988
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 144.485844373703 and batch: 50, loss is 5.327191123962402 and perplexity is 205.85892909106624
At time: 145.12743878364563 and batch: 100, loss is 5.273757429122925 and perplexity is 195.14784077404846
At time: 145.75216960906982 and batch: 150, loss is 5.22649920463562 and perplexity is 186.14002343094273
At time: 146.37832736968994 and batch: 200, loss is 5.1713455772399906 and perplexity is 176.15170381079085
At time: 147.00757145881653 and batch: 250, loss is 5.1460698699951175 and perplexity is 171.75514208178788
At time: 147.6363091468811 and batch: 300, loss is 5.136680669784546 and perplexity is 170.15004574272913
At time: 148.27781081199646 and batch: 350, loss is 5.179919919967651 and perplexity is 177.66858271782027
At time: 148.91623425483704 and batch: 400, loss is 5.162774562835693 and perplexity is 174.648356827675
At time: 149.54463124275208 and batch: 450, loss is 5.1017035865783695 and perplexity is 164.3015709677351
At time: 150.1791775226593 and batch: 500, loss is 5.10821943283081 and perplexity is 165.37563014538787
At time: 150.80692172050476 and batch: 550, loss is 5.160622472763062 and perplexity is 174.2729019839753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.375706774123172 and perplexity of 216.09254697141859
Finished 19 epochs...
Completing Train Step...
At time: 152.49045062065125 and batch: 50, loss is 5.225479669570923 and perplexity is 185.95034385902895
At time: 153.13283395767212 and batch: 100, loss is 5.214285049438477 and perplexity is 183.88030864292168
At time: 153.76334762573242 and batch: 150, loss is 5.18846471786499 and perplexity is 179.19322947239465
At time: 154.40621399879456 and batch: 200, loss is 5.142057037353515 and perplexity is 171.0672984649566
At time: 155.03462266921997 and batch: 250, loss is 5.122269163131714 and perplexity is 167.715512011913
At time: 155.66420888900757 and batch: 300, loss is 5.117051210403442 and perplexity is 166.8426596301012
At time: 156.29479813575745 and batch: 350, loss is 5.162709484100342 and perplexity is 174.63699130331239
At time: 156.92506766319275 and batch: 400, loss is 5.157819976806641 and perplexity is 173.78518661024188
At time: 157.55259108543396 and batch: 450, loss is 5.102781314849853 and perplexity is 164.47873886809535
At time: 158.18241667747498 and batch: 500, loss is 5.111674518585205 and perplexity is 165.94800536255713
At time: 158.8115758895874 and batch: 550, loss is 5.161503686904907 and perplexity is 174.42654141442117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.371326852352061 and perplexity of 215.14814822643137
Finished 20 epochs...
Completing Train Step...
At time: 160.47124862670898 and batch: 50, loss is 5.211142044067383 and perplexity is 183.30327912367238
At time: 161.1089425086975 and batch: 100, loss is 5.202645435333252 and perplexity is 181.75242072976218
At time: 161.73542094230652 and batch: 150, loss is 5.178038654327392 and perplexity is 177.33465511954347
At time: 162.36143016815186 and batch: 200, loss is 5.134855279922485 and perplexity is 169.8397388758687
At time: 162.98684978485107 and batch: 250, loss is 5.117352437973023 and perplexity is 166.8929248092127
At time: 163.61241173744202 and batch: 300, loss is 5.113118257522583 and perplexity is 166.1877639922682
At time: 164.24091124534607 and batch: 350, loss is 5.161472635269165 and perplexity is 174.421125269084
At time: 164.86960768699646 and batch: 400, loss is 5.156800146102905 and perplexity is 173.60804548341542
At time: 165.5065348148346 and batch: 450, loss is 5.102794971466064 and perplexity is 164.48098510644493
At time: 166.13729906082153 and batch: 500, loss is 5.110048112869262 and perplexity is 165.6783259416083
At time: 166.77303218841553 and batch: 550, loss is 5.158317012786865 and perplexity is 173.87158557073448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3688695380028255 and perplexity of 214.62011063746354
Finished 21 epochs...
Completing Train Step...
At time: 168.4568030834198 and batch: 50, loss is 5.2034937858581545 and perplexity is 181.90667591327215
At time: 169.1130256652832 and batch: 100, loss is 5.196479377746582 and perplexity is 180.63517288413925
At time: 169.74716472625732 and batch: 150, loss is 5.173373613357544 and perplexity is 176.5093083231543
At time: 170.3878309726715 and batch: 200, loss is 5.129856281280517 and perplexity is 168.99282886655774
At time: 171.01088786125183 and batch: 250, loss is 5.114251384735107 and perplexity is 166.37618260100243
At time: 171.6366732120514 and batch: 300, loss is 5.110588092803955 and perplexity is 165.76781307168312
At time: 172.2761197090149 and batch: 350, loss is 5.1592426776885985 and perplexity is 174.03260690930912
At time: 172.91248559951782 and batch: 400, loss is 5.155773544311524 and perplexity is 173.42991060536121
At time: 173.5444564819336 and batch: 450, loss is 5.099817323684692 and perplexity is 163.9919471188375
At time: 174.17038583755493 and batch: 500, loss is 5.10697811126709 and perplexity is 165.1704731688075
At time: 174.79744601249695 and batch: 550, loss is 5.155437088012695 and perplexity is 173.37156883481117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3672637939453125 and perplexity of 214.2757622118931
Finished 22 epochs...
Completing Train Step...
At time: 176.4779200553894 and batch: 50, loss is 5.198180952072144 and perplexity is 180.94279870648913
At time: 177.11873269081116 and batch: 100, loss is 5.190692634582519 and perplexity is 179.59290211736948
At time: 177.74741888046265 and batch: 150, loss is 5.169398441314697 and perplexity is 175.8090462087236
At time: 178.37710213661194 and batch: 200, loss is 5.1261998462677 and perplexity is 168.37604587081958
At time: 179.0058569908142 and batch: 250, loss is 5.110364465713501 and perplexity is 165.73074704259366
At time: 179.6348373889923 and batch: 300, loss is 5.10706470489502 and perplexity is 165.18477649858585
At time: 180.26909184455872 and batch: 350, loss is 5.15487380027771 and perplexity is 173.27393825612668
At time: 180.90033268928528 and batch: 400, loss is 5.15294397354126 and perplexity is 172.93987202599956
At time: 181.54246759414673 and batch: 450, loss is 5.096250896453857 and perplexity is 163.40812347409144
At time: 182.17985820770264 and batch: 500, loss is 5.103842477798462 and perplexity is 164.65337025140573
At time: 182.8108513355255 and batch: 550, loss is 5.152723979949951 and perplexity is 172.90183054706637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.364895759744847 and perplexity of 213.76895018957822
Finished 23 epochs...
Completing Train Step...
At time: 184.49704837799072 and batch: 50, loss is 5.193246021270752 and perplexity is 180.05205819497868
At time: 185.139719247818 and batch: 100, loss is 5.186981086730957 and perplexity is 178.92756993725007
At time: 185.76706218719482 and batch: 150, loss is 5.166188440322876 and perplexity is 175.24560380517374
At time: 186.40694975852966 and batch: 200, loss is 5.122744331359863 and perplexity is 167.79522403139862
At time: 187.0333273410797 and batch: 250, loss is 5.1066353893280025 and perplexity is 165.11387532317886
At time: 187.66070461273193 and batch: 300, loss is 5.10413556098938 and perplexity is 164.70163445892283
At time: 188.28823161125183 and batch: 350, loss is 5.152013654708862 and perplexity is 172.7790576221143
At time: 188.9168131351471 and batch: 400, loss is 5.1517111110687255 and perplexity is 172.72679232374733
At time: 189.5446572303772 and batch: 450, loss is 5.094347524642944 and perplexity is 163.0973928699432
At time: 190.17257452011108 and batch: 500, loss is 5.101104688644409 and perplexity is 164.2032005561945
At time: 190.81111860275269 and batch: 550, loss is 5.149947738647461 and perplexity is 172.4224790494225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.36390621104139 and perplexity of 213.55752002955185
Finished 24 epochs...
Completing Train Step...
At time: 192.49669814109802 and batch: 50, loss is 5.189389371871949 and perplexity is 179.35899783742678
At time: 193.1448519229889 and batch: 100, loss is 5.183402738571167 and perplexity is 178.28844897725753
At time: 193.77310872077942 and batch: 150, loss is 5.164110107421875 and perplexity is 174.88176332281867
At time: 194.39930319786072 and batch: 200, loss is 5.120605745315552 and perplexity is 167.4367629435429
At time: 195.02507328987122 and batch: 250, loss is 5.105371389389038 and perplexity is 164.90530324016785
At time: 195.65134239196777 and batch: 300, loss is 5.10159670829773 and perplexity is 164.28401163669588
At time: 196.27803540229797 and batch: 350, loss is 5.148844747543335 and perplexity is 172.23240343400653
At time: 196.90542197227478 and batch: 400, loss is 5.149423151016236 and perplexity is 172.33205207007921
At time: 197.5323519706726 and batch: 450, loss is 5.0914079952239994 and perplexity is 162.6186672441947
At time: 198.181494474411 and batch: 500, loss is 5.097844076156616 and perplexity is 163.66866947286448
At time: 198.80656671524048 and batch: 550, loss is 5.146976013183593 and perplexity is 171.91084736885955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.361337864652593 and perplexity of 213.00973409731202
Finished 25 epochs...
Completing Train Step...
At time: 200.47163343429565 and batch: 50, loss is 5.18640682220459 and perplexity is 178.82484767874354
At time: 201.1328957080841 and batch: 100, loss is 5.180057077407837 and perplexity is 177.6929529570689
At time: 201.77087998390198 and batch: 150, loss is 5.159775323867798 and perplexity is 174.1253294043847
At time: 202.41108417510986 and batch: 200, loss is 5.117762718200684 and perplexity is 166.96141172485142
At time: 203.0345058441162 and batch: 250, loss is 5.1014501953125 and perplexity is 164.25994365890423
At time: 203.66589450836182 and batch: 300, loss is 5.099281415939331 and perplexity is 163.90408610900033
At time: 204.29626607894897 and batch: 350, loss is 5.146462030410767 and perplexity is 171.82251085851695
At time: 204.926278591156 and batch: 400, loss is 5.147830934524536 and perplexity is 172.05788046292426
At time: 205.555593252182 and batch: 450, loss is 5.089827833175659 and perplexity is 162.3619063132288
At time: 206.1943576335907 and batch: 500, loss is 5.09488000869751 and perplexity is 163.18426275735007
At time: 206.82466506958008 and batch: 550, loss is 5.145245628356934 and perplexity is 171.61363266909845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.360588560713098 and perplexity of 212.85018484730796
Finished 26 epochs...
Completing Train Step...
At time: 208.54657363891602 and batch: 50, loss is 5.18294472694397 and perplexity is 178.20680949197492
At time: 209.20288681983948 and batch: 100, loss is 5.178288068771362 and perplexity is 177.37889046018384
At time: 209.8294439315796 and batch: 150, loss is 5.157024154663086 and perplexity is 173.6469395278567
At time: 210.45961213111877 and batch: 200, loss is 5.116343479156495 and perplexity is 166.72462164107063
At time: 211.0897035598755 and batch: 250, loss is 5.099774942398072 and perplexity is 163.9849970764001
At time: 211.7187144756317 and batch: 300, loss is 5.097092370986939 and perplexity is 163.54568511768642
At time: 212.35130381584167 and batch: 350, loss is 5.145566854476929 and perplexity is 171.6687683054882
At time: 212.99374341964722 and batch: 400, loss is 5.1464072036743165 and perplexity is 171.81309064923988
At time: 213.62257933616638 and batch: 450, loss is 5.087857179641723 and perplexity is 162.04226230601645
At time: 214.2516930103302 and batch: 500, loss is 5.093616504669189 and perplexity is 162.9782089862952
At time: 214.8825168609619 and batch: 550, loss is 5.143587808609009 and perplexity is 171.3293638981439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.359728874044215 and perplexity of 212.66727901304404
Finished 27 epochs...
Completing Train Step...
At time: 216.55890083312988 and batch: 50, loss is 5.180166778564453 and perplexity is 177.71244714877886
At time: 217.19980216026306 and batch: 100, loss is 5.174865493774414 and perplexity is 176.7728356303734
At time: 217.82954120635986 and batch: 150, loss is 5.154892320632935 and perplexity is 173.2771473807313
At time: 218.4710087776184 and batch: 200, loss is 5.113926248550415 and perplexity is 166.32209647692238
At time: 219.10005974769592 and batch: 250, loss is 5.097288236618042 and perplexity is 163.57772123380087
At time: 219.7279725074768 and batch: 300, loss is 5.095143041610718 and perplexity is 163.22719123492652
At time: 220.3660249710083 and batch: 350, loss is 5.142947759628296 and perplexity is 171.21973979955328
At time: 220.99710750579834 and batch: 400, loss is 5.144696245193481 and perplexity is 171.519376922322
At time: 221.6234848499298 and batch: 450, loss is 5.085741205215454 and perplexity is 161.69974752705502
At time: 222.25159668922424 and batch: 500, loss is 5.091006441116333 and perplexity is 162.55338015940535
At time: 222.87963247299194 and batch: 550, loss is 5.141367969512939 and perplexity is 170.9494620942994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.358623098819814 and perplexity of 212.432246775232
Finished 28 epochs...
Completing Train Step...
At time: 224.58432865142822 and batch: 50, loss is 5.177276020050049 and perplexity is 177.19946518977792
At time: 225.22488737106323 and batch: 100, loss is 5.172958698272705 and perplexity is 176.43608713985367
At time: 225.85387516021729 and batch: 150, loss is 5.152805585861206 and perplexity is 172.91594093424337
At time: 226.48054814338684 and batch: 200, loss is 5.112169103622437 and perplexity is 166.03010106294695
At time: 227.1065239906311 and batch: 250, loss is 5.095367336273194 and perplexity is 163.26380632882302
At time: 227.73693537712097 and batch: 300, loss is 5.093008184432984 and perplexity is 162.8790961929237
At time: 228.3634912967682 and batch: 350, loss is 5.141307287216186 and perplexity is 170.93908880305162
At time: 228.99100303649902 and batch: 400, loss is 5.143997831344604 and perplexity is 171.39962723641588
At time: 229.62635564804077 and batch: 450, loss is 5.084651699066162 and perplexity is 161.52367059365352
At time: 230.26123905181885 and batch: 500, loss is 5.0891279602050785 and perplexity is 162.2483133585043
At time: 230.89791464805603 and batch: 550, loss is 5.139328298568725 and perplexity is 170.60113679878913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.358093586373837 and perplexity of 212.3197910326228
Finished 29 epochs...
Completing Train Step...
At time: 232.58901834487915 and batch: 50, loss is 5.175024423599243 and perplexity is 176.8009323388181
At time: 233.2324550151825 and batch: 100, loss is 5.170321197509765 and perplexity is 175.97134996714453
At time: 233.8640513420105 and batch: 150, loss is 5.150169429779052 and perplexity is 172.4607078212481
At time: 234.51005291938782 and batch: 200, loss is 5.109298982620239 and perplexity is 165.55425777344902
At time: 235.14131021499634 and batch: 250, loss is 5.092809820175171 and perplexity is 162.84679000619653
At time: 235.78564310073853 and batch: 300, loss is 5.090405588150024 and perplexity is 162.45573881573614
At time: 236.43260097503662 and batch: 350, loss is 5.138960561752319 and perplexity is 170.53841201368076
At time: 237.07533526420593 and batch: 400, loss is 5.141716403961182 and perplexity is 171.00903715420043
At time: 237.72095441818237 and batch: 450, loss is 5.081506080627442 and perplexity is 161.0163770514066
At time: 238.35325169563293 and batch: 500, loss is 5.086953506469727 and perplexity is 161.8958952048128
At time: 239.00097155570984 and batch: 550, loss is 5.137394876480102 and perplexity is 170.27161145100172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.357384215009973 and perplexity of 212.16923086073353
Finished 30 epochs...
Completing Train Step...
At time: 240.6625247001648 and batch: 50, loss is 5.173575353622437 and perplexity is 176.544920949901
At time: 241.3030285835266 and batch: 100, loss is 5.169585437774658 and perplexity is 175.84192495200224
At time: 241.92809081077576 and batch: 150, loss is 5.148390588760376 and perplexity is 172.15420033493308
At time: 242.5532042980194 and batch: 200, loss is 5.107415561676025 and perplexity is 165.2427428658911
At time: 243.17766690254211 and batch: 250, loss is 5.091778116226196 and perplexity is 162.6788669681949
At time: 243.80341625213623 and batch: 300, loss is 5.08942177772522 and perplexity is 162.29599175961422
At time: 244.42819833755493 and batch: 350, loss is 5.138036479949951 and perplexity is 170.38089336180317
At time: 245.053120136261 and batch: 400, loss is 5.140472354888916 and perplexity is 170.79642579708138
At time: 245.6781792640686 and batch: 450, loss is 5.080860195159912 and perplexity is 160.91241249164815
At time: 246.30542278289795 and batch: 500, loss is 5.084977931976319 and perplexity is 161.57637352701866
At time: 246.9451129436493 and batch: 550, loss is 5.136664533615113 and perplexity is 170.1473001949133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3571835781665555 and perplexity of 212.12666616614908
Finished 31 epochs...
Completing Train Step...
At time: 248.66389203071594 and batch: 50, loss is 5.171270151138305 and perplexity is 176.1384178755266
At time: 249.30892324447632 and batch: 100, loss is 5.167599439620972 and perplexity is 175.49304976111853
At time: 249.93443989753723 and batch: 150, loss is 5.147272653579712 and perplexity is 171.96185063516074
At time: 250.57533407211304 and batch: 200, loss is 5.106371126174927 and perplexity is 165.07024757472647
At time: 251.20399236679077 and batch: 250, loss is 5.089970664978027 and perplexity is 162.38509841318879
At time: 251.82962226867676 and batch: 300, loss is 5.087633972167969 and perplexity is 162.0060972983058
At time: 252.45717787742615 and batch: 350, loss is 5.135828762054444 and perplexity is 170.0051553288782
At time: 253.0850911140442 and batch: 400, loss is 5.137800998687744 and perplexity is 170.34077657753926
At time: 253.71103692054749 and batch: 450, loss is 5.078695182800293 and perplexity is 160.56441197842506
At time: 254.33784770965576 and batch: 500, loss is 5.083600177764892 and perplexity is 161.35391428019636
At time: 254.96470427513123 and batch: 550, loss is 5.134339456558227 and perplexity is 169.75215416144013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.356464142495013 and perplexity of 211.97410955953623
Finished 32 epochs...
Completing Train Step...
At time: 256.6203134059906 and batch: 50, loss is 5.169545106887817 and perplexity is 175.8348332342337
At time: 257.28199577331543 and batch: 100, loss is 5.16498366355896 and perplexity is 175.03459910603272
At time: 257.91273260116577 and batch: 150, loss is 5.145643682479858 and perplexity is 171.68195778077646
At time: 258.5390512943268 and batch: 200, loss is 5.103860893249512 and perplexity is 164.65640244540526
At time: 259.16502952575684 and batch: 250, loss is 5.088731002807617 and perplexity is 162.18392047175297
At time: 259.7908296585083 and batch: 300, loss is 5.086168737411499 and perplexity is 161.76889415536684
At time: 260.4172639846802 and batch: 350, loss is 5.1347160911560055 and perplexity is 169.8161007372352
At time: 261.05237317085266 and batch: 400, loss is 5.137273416519165 and perplexity is 170.2509315236432
At time: 261.6869013309479 and batch: 450, loss is 5.078139715194702 and perplexity is 160.47524841498839
At time: 262.3149015903473 and batch: 500, loss is 5.08265043258667 and perplexity is 161.200741926987
At time: 262.94337368011475 and batch: 550, loss is 5.133820180892944 and perplexity is 169.66402888129164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.356118384827959 and perplexity of 211.90083055505798
Finished 33 epochs...
Completing Train Step...
At time: 264.59797835350037 and batch: 50, loss is 5.168566446304322 and perplexity is 175.66283479151807
At time: 265.23908257484436 and batch: 100, loss is 5.162898101806641 and perplexity is 174.6699340387409
At time: 265.86782932281494 and batch: 150, loss is 5.143301343917846 and perplexity is 171.28029111397325
At time: 266.50974583625793 and batch: 200, loss is 5.101906070709228 and perplexity is 164.33484279692627
At time: 267.14066648483276 and batch: 250, loss is 5.087076663970947 and perplexity is 161.91583512657434
At time: 267.77014684677124 and batch: 300, loss is 5.084571504592896 and perplexity is 161.51071780734807
At time: 268.40748715400696 and batch: 350, loss is 5.133275089263916 and perplexity is 169.57157164052447
At time: 269.04982256889343 and batch: 400, loss is 5.135389308929444 and perplexity is 169.93046244531544
At time: 269.68823742866516 and batch: 450, loss is 5.076369342803955 and perplexity is 160.19139879969444
At time: 270.32897686958313 and batch: 500, loss is 5.081242504119873 and perplexity is 160.9739425097019
At time: 270.97444558143616 and batch: 550, loss is 5.13186089515686 and perplexity is 169.33193401018715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.355672633394282 and perplexity of 211.806396504661
Finished 34 epochs...
Completing Train Step...
At time: 272.6371612548828 and batch: 50, loss is 5.166466426849365 and perplexity is 175.2943264936698
At time: 273.2763135433197 and batch: 100, loss is 5.16157826423645 and perplexity is 174.4395501655031
At time: 273.90276050567627 and batch: 150, loss is 5.14254301071167 and perplexity is 171.1504528182256
At time: 274.5292499065399 and batch: 200, loss is 5.101303758621216 and perplexity is 164.2358917373292
At time: 275.15501832962036 and batch: 250, loss is 5.0864474487304685 and perplexity is 161.8139872609127
At time: 275.7810447216034 and batch: 300, loss is 5.0834618759155275 and perplexity is 161.33160027851858
At time: 276.4120764732361 and batch: 350, loss is 5.131827774047852 and perplexity is 169.32632564162043
At time: 277.03924441337585 and batch: 400, loss is 5.1346525287628175 and perplexity is 169.80530716250684
At time: 277.66626596450806 and batch: 450, loss is 5.074407300949097 and perplexity is 159.87740470596773
At time: 278.2952973842621 and batch: 500, loss is 5.079591207504272 and perplexity is 160.70834613278674
At time: 278.92951226234436 and batch: 550, loss is 5.1308728694915775 and perplexity is 169.16471233668912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.354807752244016 and perplexity of 211.62328833963247
Finished 35 epochs...
Completing Train Step...
At time: 280.5881838798523 and batch: 50, loss is 5.164762344360351 and perplexity is 174.99586487530232
At time: 281.22780990600586 and batch: 100, loss is 5.160475368499756 and perplexity is 174.2472675826259
At time: 281.85418701171875 and batch: 150, loss is 5.141530952453613 and perplexity is 170.97732621096438
At time: 282.5029401779175 and batch: 200, loss is 5.100755863189697 and perplexity is 164.14593228899062
At time: 283.1297152042389 and batch: 250, loss is 5.085444555282593 and perplexity is 161.6517864219885
At time: 283.77495217323303 and batch: 300, loss is 5.08251350402832 and perplexity is 161.1786704529321
At time: 284.4168555736542 and batch: 350, loss is 5.130568399429321 and perplexity is 169.11321458635183
At time: 285.05842757225037 and batch: 400, loss is 5.1333787155151365 and perplexity is 169.58914461730234
At time: 285.6948502063751 and batch: 450, loss is 5.073278865814209 and perplexity is 159.69709517815934
At time: 286.3228883743286 and batch: 500, loss is 5.078323078155518 and perplexity is 160.50467632958774
At time: 286.9482214450836 and batch: 550, loss is 5.129406938552856 and perplexity is 168.9169102259016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.354755158120013 and perplexity of 211.61215849084857
Finished 36 epochs...
Completing Train Step...
At time: 288.6117014884949 and batch: 50, loss is 5.162890825271607 and perplexity is 174.66866305147067
At time: 289.2547001838684 and batch: 100, loss is 5.158743419647217 and perplexity is 173.94574141679922
At time: 289.89819526672363 and batch: 150, loss is 5.139626989364624 and perplexity is 170.65210139907035
At time: 290.527939081192 and batch: 200, loss is 5.099376001358032 and perplexity is 163.91958977881075
At time: 291.15826988220215 and batch: 250, loss is 5.084046144485473 and perplexity is 161.4258888041291
At time: 291.7895007133484 and batch: 300, loss is 5.081877746582031 and perplexity is 161.07623247931133
At time: 292.42001390457153 and batch: 350, loss is 5.129666290283203 and perplexity is 168.9607248003008
At time: 293.0509259700775 and batch: 400, loss is 5.132805080413818 and perplexity is 169.49189022807985
At time: 293.6819143295288 and batch: 450, loss is 5.072249956130982 and perplexity is 159.53286579363782
At time: 294.3130440711975 and batch: 500, loss is 5.077094039916992 and perplexity is 160.30753111923377
At time: 294.9421453475952 and batch: 550, loss is 5.128298864364624 and perplexity is 168.72984141992276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.356319346326463 and perplexity of 211.9434187426501
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 296.59448313713074 and batch: 50, loss is 5.152063636779785 and perplexity is 172.7876936930489
At time: 297.2329316139221 and batch: 100, loss is 5.1374405574798585 and perplexity is 170.27938980610307
At time: 297.85995221138 and batch: 150, loss is 5.108458642959595 and perplexity is 165.41519440306968
At time: 298.49983048439026 and batch: 200, loss is 5.0651505756378175 and perplexity is 158.40429210533645
At time: 299.12703037261963 and batch: 250, loss is 5.042911348342895 and perplexity is 154.92038632391925
At time: 299.7548084259033 and batch: 300, loss is 5.0323323631286625 and perplexity is 153.290124310675
At time: 300.3823838233948 and batch: 350, loss is 5.080662517547608 and perplexity is 160.88060685389155
At time: 301.0105359554291 and batch: 400, loss is 5.077571725845337 and perplexity is 160.38412606376417
At time: 301.6359078884125 and batch: 450, loss is 5.010943403244019 and perplexity is 150.04622349273927
At time: 302.2630777359009 and batch: 500, loss is 5.016777458190918 and perplexity is 150.92415987959782
At time: 302.8990578651428 and batch: 550, loss is 5.076889314651489 and perplexity is 160.27471547657348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.319735587911403 and perplexity of 204.32984756784705
Finished 38 epochs...
Completing Train Step...
At time: 304.5768702030182 and batch: 50, loss is 5.126718044281006 and perplexity is 168.46332061412443
At time: 305.2160713672638 and batch: 100, loss is 5.121899976730346 and perplexity is 167.65360515386809
At time: 305.8422462940216 and batch: 150, loss is 5.096949424743652 and perplexity is 163.52230854722936
At time: 306.4687502384186 and batch: 200, loss is 5.054143190383911 and perplexity is 156.67023625517214
At time: 307.0943126678467 and batch: 250, loss is 5.036826333999634 and perplexity is 153.9805558918589
At time: 307.72005915641785 and batch: 300, loss is 5.026746320724487 and perplexity is 152.43622635164715
At time: 308.3495502471924 and batch: 350, loss is 5.0776311302185055 and perplexity is 160.39365386523298
At time: 308.97799944877625 and batch: 400, loss is 5.077218275070191 and perplexity is 160.32744818709565
At time: 309.6045837402344 and batch: 450, loss is 5.013743009567261 and perplexity is 150.4668824137425
At time: 310.23219776153564 and batch: 500, loss is 5.0209047222137455 and perplexity is 151.5483509494623
At time: 310.8588070869446 and batch: 550, loss is 5.078719501495361 and perplexity is 160.56831674287798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.318150946434508 and perplexity of 204.00631442614286
Finished 39 epochs...
Completing Train Step...
At time: 312.5122404098511 and batch: 50, loss is 5.122929649353027 and perplexity is 167.8263223870321
At time: 313.15234446525574 and batch: 100, loss is 5.118483514785766 and perplexity is 167.08180032289613
At time: 313.7801878452301 and batch: 150, loss is 5.094391565322876 and perplexity is 163.10457594819303
At time: 314.44418835639954 and batch: 200, loss is 5.051657199859619 and perplexity is 156.28123925417069
At time: 315.07795548439026 and batch: 250, loss is 5.0352159595489505 and perplexity is 153.73278909095617
At time: 315.7060844898224 and batch: 300, loss is 5.025468692779541 and perplexity is 152.24159392942443
At time: 316.3322865962982 and batch: 350, loss is 5.07747410774231 and perplexity is 160.3684704337711
At time: 316.95824360847473 and batch: 400, loss is 5.077334928512573 and perplexity is 160.34615202674746
At time: 317.58339262008667 and batch: 450, loss is 5.0144995498657225 and perplexity is 150.58075974483594
At time: 318.2093768119812 and batch: 500, loss is 5.02158935546875 and perplexity is 151.65214151544586
At time: 318.8395025730133 and batch: 550, loss is 5.077945175170899 and perplexity is 160.44403259280378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.316540007895612 and perplexity of 203.6779373607056
Finished 40 epochs...
Completing Train Step...
At time: 320.514897108078 and batch: 50, loss is 5.119044322967529 and perplexity is 167.17552744255212
At time: 321.1763451099396 and batch: 100, loss is 5.115470685958862 and perplexity is 166.57916900969155
At time: 321.81049704551697 and batch: 150, loss is 5.092523193359375 and perplexity is 162.80012043800872
At time: 322.45239305496216 and batch: 200, loss is 5.0501158237457275 and perplexity is 156.04053663922147
At time: 323.08862495422363 and batch: 250, loss is 5.034111881256104 and perplexity is 153.56314972042776
At time: 323.7246062755585 and batch: 300, loss is 5.024686679840088 and perplexity is 152.1225855721512
At time: 324.3600754737854 and batch: 350, loss is 5.07743763923645 and perplexity is 160.36262214190728
At time: 324.9977025985718 and batch: 400, loss is 5.077315044403076 and perplexity is 160.3429637180016
At time: 325.6333763599396 and batch: 450, loss is 5.014836988449097 and perplexity is 150.63158007697587
At time: 326.2754409313202 and batch: 500, loss is 5.021873531341552 and perplexity is 151.69524351910803
At time: 326.9150550365448 and batch: 550, loss is 5.07739556312561 and perplexity is 160.3558748483944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.315948810983212 and perplexity of 203.5575591801244
Finished 41 epochs...
Completing Train Step...
At time: 328.60401248931885 and batch: 50, loss is 5.117187671661377 and perplexity is 166.865428742827
At time: 329.2708547115326 and batch: 100, loss is 5.113961801528931 and perplexity is 166.32800982796314
At time: 329.9130916595459 and batch: 150, loss is 5.091547012329102 and perplexity is 162.6412755919877
At time: 330.5588700771332 and batch: 200, loss is 5.049238243103027 and perplexity is 155.90365855435388
At time: 331.19791865348816 and batch: 250, loss is 5.0335220527648925 and perplexity is 153.4726005064008
At time: 331.83440923690796 and batch: 300, loss is 5.0239681625366215 and perplexity is 152.0133221207105
At time: 332.4659466743469 and batch: 350, loss is 5.077409172058106 and perplexity is 160.35805713551972
At time: 333.1003928184509 and batch: 400, loss is 5.077163000106811 and perplexity is 160.31858633818976
At time: 333.7318136692047 and batch: 450, loss is 5.01470832824707 and perplexity is 150.6122010341341
At time: 334.36553978919983 and batch: 500, loss is 5.021921272277832 and perplexity is 151.70248576493725
At time: 334.99850153923035 and batch: 550, loss is 5.077016563415527 and perplexity is 160.29511153368338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3154121561253325 and perplexity of 203.44834833401893
Finished 42 epochs...
Completing Train Step...
At time: 336.67278933525085 and batch: 50, loss is 5.1157217121124265 and perplexity is 166.62098998661187
At time: 337.31866478919983 and batch: 100, loss is 5.113004264831543 and perplexity is 166.16882088154136
At time: 337.95147013664246 and batch: 150, loss is 5.090600128173828 and perplexity is 162.4873460333672
At time: 338.5829975605011 and batch: 200, loss is 5.048313570022583 and perplexity is 155.75956526800428
At time: 339.21489667892456 and batch: 250, loss is 5.032762327194214 and perplexity is 153.35604772706697
At time: 339.85731077194214 and batch: 300, loss is 5.023299942016601 and perplexity is 151.91177763038755
At time: 340.50181245803833 and batch: 350, loss is 5.077091789245605 and perplexity is 160.30717032006638
At time: 341.13523864746094 and batch: 400, loss is 5.076953573226929 and perplexity is 160.28501483237636
At time: 341.76774740219116 and batch: 450, loss is 5.014579105377197 and perplexity is 150.5927397507271
At time: 342.41025471687317 and batch: 500, loss is 5.021998157501221 and perplexity is 151.7141498928377
At time: 343.0504298210144 and batch: 550, loss is 5.076419677734375 and perplexity is 160.19946222554123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.31504886708361 and perplexity of 203.3744512023345
Finished 43 epochs...
Completing Train Step...
At time: 344.7317383289337 and batch: 50, loss is 5.114572582244873 and perplexity is 166.42963079979282
At time: 345.3814585208893 and batch: 100, loss is 5.111647987365723 and perplexity is 165.94360261800963
At time: 346.0206232070923 and batch: 150, loss is 5.089537649154663 and perplexity is 162.31479831772268
At time: 346.6746926307678 and batch: 200, loss is 5.047047901153564 and perplexity is 155.5625499395768
At time: 347.3200652599335 and batch: 250, loss is 5.032040615081787 and perplexity is 153.24540873945946
At time: 347.95656633377075 and batch: 300, loss is 5.022638492584228 and perplexity is 151.8113288958447
At time: 348.59217262268066 and batch: 350, loss is 5.076747102737427 and perplexity is 160.2519241231491
At time: 349.2292687892914 and batch: 400, loss is 5.076445055007935 and perplexity is 160.2035277027034
At time: 349.876492023468 and batch: 450, loss is 5.014284200668335 and perplexity is 150.5483357904445
At time: 350.51795983314514 and batch: 500, loss is 5.022076606750488 and perplexity is 151.7260522208583
At time: 351.15127635002136 and batch: 550, loss is 5.075858135223388 and perplexity is 160.1095286703848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.314624218230552 and perplexity of 203.28810680921137
Finished 44 epochs...
Completing Train Step...
At time: 352.83340549468994 and batch: 50, loss is 5.1133822822570805 and perplexity is 166.23164746543927
At time: 353.4803490638733 and batch: 100, loss is 5.110728998184204 and perplexity is 165.79117229409846
At time: 354.1233401298523 and batch: 150, loss is 5.088739070892334 and perplexity is 162.18522899064166
At time: 354.76139068603516 and batch: 200, loss is 5.046370439529419 and perplexity is 155.4571979718254
At time: 355.39537525177 and batch: 250, loss is 5.031615934371948 and perplexity is 153.1803421877291
At time: 356.02990770339966 and batch: 300, loss is 5.02192889213562 and perplexity is 151.70364172070902
At time: 356.6642265319824 and batch: 350, loss is 5.076220407485962 and perplexity is 160.16754241934734
At time: 357.2979633808136 and batch: 400, loss is 5.076272573471069 and perplexity is 160.17589793491487
At time: 357.93027448654175 and batch: 450, loss is 5.013884544372559 and perplexity is 150.48818022180373
At time: 358.5646333694458 and batch: 500, loss is 5.021831216812134 and perplexity is 151.68882474206828
At time: 359.1976788043976 and batch: 550, loss is 5.075282583236694 and perplexity is 160.01740382693134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.314253137466755 and perplexity of 203.21268449801568
Finished 45 epochs...
Completing Train Step...
At time: 360.96205163002014 and batch: 50, loss is 5.112369594573974 and perplexity is 166.06339193304055
At time: 361.6238098144531 and batch: 100, loss is 5.109873857498169 and perplexity is 165.64945811869885
At time: 362.259348154068 and batch: 150, loss is 5.088098268508912 and perplexity is 162.08133360111032
At time: 362.90683341026306 and batch: 200, loss is 5.0458666038513185 and perplexity is 155.37889281719154
At time: 363.5493907928467 and batch: 250, loss is 5.031174402236939 and perplexity is 153.11272307330603
At time: 364.1833791732788 and batch: 300, loss is 5.021342048645019 and perplexity is 151.61464154320635
At time: 364.81761622428894 and batch: 350, loss is 5.075774307250977 and perplexity is 160.096107575772
At time: 365.45177578926086 and batch: 400, loss is 5.076027822494507 and perplexity is 160.13669952459384
At time: 366.08426451683044 and batch: 450, loss is 5.0135813331604 and perplexity is 150.44255743528285
At time: 366.7155420780182 and batch: 500, loss is 5.021550559997559 and perplexity is 151.6462582132821
At time: 367.3484992980957 and batch: 550, loss is 5.074837045669556 and perplexity is 159.94612594179733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3138417994722404 and perplexity of 203.12911258924333
Finished 46 epochs...
Completing Train Step...
At time: 369.03343987464905 and batch: 50, loss is 5.1114367008209225 and perplexity is 165.90854467134736
At time: 369.684690952301 and batch: 100, loss is 5.109179630279541 and perplexity is 165.53449966438288
At time: 370.3175718784332 and batch: 150, loss is 5.087526779174805 and perplexity is 161.98873231054478
At time: 370.95060420036316 and batch: 200, loss is 5.045273599624633 and perplexity is 155.28677979141943
At time: 371.5845341682434 and batch: 250, loss is 5.0305246734619145 and perplexity is 153.01327364237994
At time: 372.2181875705719 and batch: 300, loss is 5.0206955051422115 and perplexity is 151.51664776382034
At time: 372.85144114494324 and batch: 350, loss is 5.075315923690796 and perplexity is 160.02273896877662
At time: 373.4839286804199 and batch: 400, loss is 5.076080598831177 and perplexity is 160.1451511759828
At time: 374.116986989975 and batch: 450, loss is 5.013489313125611 and perplexity is 150.42871434284456
At time: 374.7573878765106 and batch: 500, loss is 5.021489725112915 and perplexity is 151.63703311126392
At time: 375.3918514251709 and batch: 550, loss is 5.074361419677734 and perplexity is 159.87006949565333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.313441499750665 and perplexity of 203.04781633454962
Finished 47 epochs...
Completing Train Step...
At time: 377.0967094898224 and batch: 50, loss is 5.110586395263672 and perplexity is 165.76753167438173
At time: 377.7470052242279 and batch: 100, loss is 5.108331670761109 and perplexity is 165.39419260552373
At time: 378.3825533390045 and batch: 150, loss is 5.086956100463867 and perplexity is 161.896315162361
At time: 379.0386915206909 and batch: 200, loss is 5.044758329391479 and perplexity is 155.20678574723533
At time: 379.6979546546936 and batch: 250, loss is 5.030196866989136 and perplexity is 152.9631231211407
At time: 380.3335418701172 and batch: 300, loss is 5.02027232170105 and perplexity is 151.4525419926335
At time: 380.97615242004395 and batch: 350, loss is 5.075127830505371 and perplexity is 159.99264261261212
At time: 381.62069368362427 and batch: 400, loss is 5.075806550979614 and perplexity is 160.10126975444408
At time: 382.26161909103394 and batch: 450, loss is 5.012965984344483 and perplexity is 150.3500112627106
At time: 382.905371427536 and batch: 500, loss is 5.021046724319458 and perplexity is 151.5698726624425
At time: 383.54427552223206 and batch: 550, loss is 5.07344162940979 and perplexity is 159.72309016703593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.313169114133145 and perplexity of 202.9925165614846
Finished 48 epochs...
Completing Train Step...
At time: 385.21487736701965 and batch: 50, loss is 5.109939632415771 and perplexity is 165.66035405649325
At time: 385.854505777359 and batch: 100, loss is 5.107851047515869 and perplexity is 165.31471941179183
At time: 386.4806110858917 and batch: 150, loss is 5.0864205646514895 and perplexity is 161.80963709937458
At time: 387.1083912849426 and batch: 200, loss is 5.044256820678711 and perplexity is 155.12896770674595
At time: 387.7373616695404 and batch: 250, loss is 5.029676198959351 and perplexity is 152.88350084338146
At time: 388.3649933338165 and batch: 300, loss is 5.019704732894898 and perplexity is 151.36660361627253
At time: 388.994238615036 and batch: 350, loss is 5.074855823516845 and perplexity is 159.9491294139241
At time: 389.62273955345154 and batch: 400, loss is 5.075787715911865 and perplexity is 160.0982542645801
At time: 390.25028109550476 and batch: 450, loss is 5.01272876739502 and perplexity is 150.31434992159163
At time: 390.87914514541626 and batch: 500, loss is 5.0207754611969 and perplexity is 151.52876292152877
At time: 391.5080373287201 and batch: 550, loss is 5.073124723434448 and perplexity is 159.67248098496162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.312942180227726 and perplexity of 202.94645590349057
Finished 49 epochs...
Completing Train Step...
At time: 393.1681442260742 and batch: 50, loss is 5.109234848022461 and perplexity is 165.54364035819168
At time: 393.8076333999634 and batch: 100, loss is 5.1070701694488525 and perplexity is 165.18567916215565
At time: 394.4334452152252 and batch: 150, loss is 5.0858385944366455 and perplexity is 161.71549610639204
At time: 395.0737473964691 and batch: 200, loss is 5.043714485168457 and perplexity is 155.04485856863732
At time: 395.70006132125854 and batch: 250, loss is 5.029204740524292 and perplexity is 152.81143961559914
At time: 396.3260223865509 and batch: 300, loss is 5.019246664047241 and perplexity is 151.29728316856122
At time: 396.9521589279175 and batch: 350, loss is 5.074651727676391 and perplexity is 159.91648779304933
At time: 397.57828283309937 and batch: 400, loss is 5.075500974655151 and perplexity is 160.0523540710087
At time: 398.20290350914 and batch: 450, loss is 5.012423276901245 and perplexity is 150.2684373298999
At time: 398.82943773269653 and batch: 500, loss is 5.020421810150147 and perplexity is 151.47518409057164
At time: 399.45553851127625 and batch: 550, loss is 5.072367525100708 and perplexity is 159.55162301091835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.312855821974734 and perplexity of 202.92893055884795
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f418b26d518>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 5.483130747189248, 'lr': 10.940649164380526, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.780772010508459}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8952600955963135 and batch: 50, loss is 6.743240127563476 and perplexity is 848.3049038242184
At time: 1.5462234020233154 and batch: 100, loss is 6.0742604637146 and perplexity is 434.5280344138436
At time: 2.1964540481567383 and batch: 150, loss is 5.994871835708619 and perplexity is 401.36524000277825
At time: 2.8439419269561768 and batch: 200, loss is 5.897767152786255 and perplexity is 364.2233042620179
At time: 3.4872968196868896 and batch: 250, loss is 5.837536067962646 and perplexity is 342.93333441268203
At time: 4.137305974960327 and batch: 300, loss is 5.825192813873291 and perplexity is 338.7264379383683
At time: 4.77291202545166 and batch: 350, loss is 5.874990034103393 and perplexity is 356.02111258350146
At time: 5.409075498580933 and batch: 400, loss is 5.875337429046631 and perplexity is 356.1448140030786
At time: 6.045595645904541 and batch: 450, loss is 5.802060441970825 and perplexity is 330.9808245979889
At time: 6.681407690048218 and batch: 500, loss is 5.817028312683106 and perplexity is 335.97216446695194
At time: 7.3197431564331055 and batch: 550, loss is 5.890880403518676 and perplexity is 361.72360696191834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.865664218334442 and perplexity of 352.71635899067303
Finished 1 epochs...
Completing Train Step...
At time: 9.009013652801514 and batch: 50, loss is 5.572924880981446 and perplexity is 263.20281139378557
At time: 9.634348630905151 and batch: 100, loss is 5.392372770309448 and perplexity is 219.72412238906415
At time: 10.260693788528442 and batch: 150, loss is 5.29757266998291 and perplexity is 199.8511161305083
At time: 10.885732173919678 and batch: 200, loss is 5.194895315170288 and perplexity is 180.3492619769805
At time: 11.521050691604614 and batch: 250, loss is 5.136646242141723 and perplexity is 170.1441879785631
At time: 12.184758424758911 and batch: 300, loss is 5.1214109230041505 and perplexity is 167.57163357944316
At time: 12.827969074249268 and batch: 350, loss is 5.13700026512146 and perplexity is 170.20443359451323
At time: 13.455042362213135 and batch: 400, loss is 5.130100908279419 and perplexity is 169.03417413199725
At time: 14.094499826431274 and batch: 450, loss is 5.037738370895386 and perplexity is 154.12105590091758
At time: 14.721688032150269 and batch: 500, loss is 5.059416065216064 and perplexity is 157.49852060163235
At time: 15.355718612670898 and batch: 550, loss is 5.1048907375335695 and perplexity is 164.82606024586056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.152221030377327 and perplexity of 172.8148915100952
Finished 2 epochs...
Completing Train Step...
At time: 17.047930479049683 and batch: 50, loss is 5.070272521972656 and perplexity is 159.21771175631534
At time: 17.67558264732361 and batch: 100, loss is 5.027383403778076 and perplexity is 152.53337182979203
At time: 18.303343772888184 and batch: 150, loss is 5.016222896575928 and perplexity is 150.84048633696582
At time: 18.94709038734436 and batch: 200, loss is 4.950680618286133 and perplexity is 141.2710828899192
At time: 19.574838638305664 and batch: 250, loss is 4.9247203540802005 and perplexity is 137.65084276354645
At time: 20.20101237297058 and batch: 300, loss is 4.923729391098022 and perplexity is 137.5145034386713
At time: 20.82750654220581 and batch: 350, loss is 4.938100271224975 and perplexity is 139.50497604866314
At time: 21.45248532295227 and batch: 400, loss is 4.935796632766723 and perplexity is 139.18397689567203
At time: 22.077184200286865 and batch: 450, loss is 4.854197521209716 and perplexity is 128.2777097586819
At time: 22.702812433242798 and batch: 500, loss is 4.891272706985474 and perplexity is 133.12289271739112
At time: 23.341339111328125 and batch: 550, loss is 4.938283014297485 and perplexity is 139.53047194614504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0650271151928195 and perplexity of 158.3847366481311
Finished 3 epochs...
Completing Train Step...
At time: 25.008302211761475 and batch: 50, loss is 4.923721237182617 and perplexity is 137.51338216161471
At time: 25.63283634185791 and batch: 100, loss is 4.894618301391602 and perplexity is 133.5690137765346
At time: 26.257218599319458 and batch: 150, loss is 4.887569608688355 and perplexity is 132.63083718735328
At time: 26.88140630722046 and batch: 200, loss is 4.834547281265259 and perplexity is 125.7816266163324
At time: 27.504637002944946 and batch: 250, loss is 4.809443206787109 and perplexity is 122.66330040664118
At time: 28.130972862243652 and batch: 300, loss is 4.801579236984253 and perplexity is 121.7024628666973
At time: 28.755839824676514 and batch: 350, loss is 4.833308000564575 and perplexity is 125.62584442283683
At time: 29.380199670791626 and batch: 400, loss is 4.838781414031982 and perplexity is 126.31533181502515
At time: 30.00472092628479 and batch: 450, loss is 4.76597469329834 and perplexity is 117.44553489349423
At time: 30.628692388534546 and batch: 500, loss is 4.792949514389038 and perplexity is 120.65672308497759
At time: 31.25240993499756 and batch: 550, loss is 4.842828235626221 and perplexity is 126.82754314239763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.03189314172623 and perplexity of 153.2228107911476
Finished 4 epochs...
Completing Train Step...
At time: 32.917255878448486 and batch: 50, loss is 4.8481954574584964 and perplexity is 127.51008473846214
At time: 33.55816197395325 and batch: 100, loss is 4.825568580627442 and perplexity is 124.65732597777826
At time: 34.19613289833069 and batch: 150, loss is 4.817914009094238 and perplexity is 123.70677024863511
At time: 34.82321214675903 and batch: 200, loss is 4.763680362701416 and perplexity is 117.17638488692975
At time: 35.45045757293701 and batch: 250, loss is 4.745302333831787 and perplexity is 115.0425815103494
At time: 36.07838010787964 and batch: 300, loss is 4.7294423866271975 and perplexity is 113.23240483838771
At time: 36.70665001869202 and batch: 350, loss is 4.759219961166382 and perplexity is 116.65489505153576
At time: 37.33548021316528 and batch: 400, loss is 4.766114578247071 and perplexity is 117.46196490525038
At time: 37.96237349510193 and batch: 450, loss is 4.702817401885986 and perplexity is 110.25737460001491
At time: 38.60208320617676 and batch: 500, loss is 4.722289514541626 and perplexity is 112.42535772397395
At time: 39.22796010971069 and batch: 550, loss is 4.767898035049439 and perplexity is 117.67164016338639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.010820104720745 and perplexity of 150.02772415545368
Finished 5 epochs...
Completing Train Step...
At time: 40.88780951499939 and batch: 50, loss is 4.775249719619751 and perplexity is 118.53991266687954
At time: 41.52753686904907 and batch: 100, loss is 4.766688556671142 and perplexity is 117.52940489145205
At time: 42.15167593955994 and batch: 150, loss is 4.75572099685669 and perplexity is 116.2474369935015
At time: 42.77650260925293 and batch: 200, loss is 4.711254730224609 and perplexity is 111.19158786118359
At time: 43.4016809463501 and batch: 250, loss is 4.691337556838989 and perplexity is 108.99887453828356
At time: 44.026495933532715 and batch: 300, loss is 4.6716953372955325 and perplexity is 106.87878451488078
At time: 44.65154051780701 and batch: 350, loss is 4.70376708984375 and perplexity is 110.3621344376317
At time: 45.27714705467224 and batch: 400, loss is 4.710460090637207 and perplexity is 111.10326572046537
At time: 45.90201473236084 and batch: 450, loss is 4.648759727478027 and perplexity is 104.45535210032925
At time: 46.5258002281189 and batch: 500, loss is 4.672074146270752 and perplexity is 106.9192788270358
At time: 47.145864963531494 and batch: 550, loss is 4.715340166091919 and perplexity is 111.64678316485451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.996798251537567 and perplexity of 147.93873739338636
Finished 6 epochs...
Completing Train Step...
At time: 48.80740547180176 and batch: 50, loss is 4.731417970657349 and perplexity is 113.45632608387123
At time: 49.44415068626404 and batch: 100, loss is 4.7172900390625 and perplexity is 111.86469258840643
At time: 50.06903433799744 and batch: 150, loss is 4.716485900878906 and perplexity is 111.77477407600011
At time: 50.69391703605652 and batch: 200, loss is 4.666520328521728 and perplexity is 106.32711454692078
At time: 51.31979155540466 and batch: 250, loss is 4.6386166858673095 and perplexity is 103.40121224730026
At time: 51.94620418548584 and batch: 300, loss is 4.627476444244385 and perplexity is 102.25569030144995
At time: 52.5728440284729 and batch: 350, loss is 4.654310474395752 and perplexity is 105.03676948158171
At time: 53.19867014884949 and batch: 400, loss is 4.664434823989868 and perplexity is 106.10559993282602
At time: 53.82393932342529 and batch: 450, loss is 4.606548080444336 and perplexity is 100.1378844188971
At time: 54.46253514289856 and batch: 500, loss is 4.63001784324646 and perplexity is 102.51589331052438
At time: 55.090166091918945 and batch: 550, loss is 4.675022659301757 and perplexity is 107.23499693466961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.993334506420379 and perplexity of 147.4272017400881
Finished 7 epochs...
Completing Train Step...
At time: 56.78022646903992 and batch: 50, loss is 4.684557790756226 and perplexity is 108.26238709299614
At time: 57.42437791824341 and batch: 100, loss is 4.678775663375855 and perplexity is 107.63820646528839
At time: 58.06671118736267 and batch: 150, loss is 4.682111043930053 and perplexity is 107.9978202370274
At time: 58.6973774433136 and batch: 200, loss is 4.641830272674561 and perplexity is 103.73403551063008
At time: 59.32783341407776 and batch: 250, loss is 4.611757154464722 and perplexity is 100.66087102647622
At time: 59.9589524269104 and batch: 300, loss is 4.59139762878418 and perplexity is 98.63218505530031
At time: 60.58753228187561 and batch: 350, loss is 4.616993045806884 and perplexity is 101.18930260752238
At time: 61.21823477745056 and batch: 400, loss is 4.632404136657715 and perplexity is 102.76081842665057
At time: 61.84944224357605 and batch: 450, loss is 4.573605375289917 and perplexity is 96.8928157508371
At time: 62.5038743019104 and batch: 500, loss is 4.594867095947266 and perplexity is 98.97498049748013
At time: 63.137818813323975 and batch: 550, loss is 4.639216318130493 and perplexity is 103.46323354334271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9953321091672205 and perplexity of 147.72199706719113
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 64.82676839828491 and batch: 50, loss is 4.5998964023590085 and perplexity is 99.4740098353555
At time: 65.4698178768158 and batch: 100, loss is 4.51854437828064 and perplexity is 91.70201733216044
At time: 66.09674763679504 and batch: 150, loss is 4.46947826385498 and perplexity is 87.31115774109279
At time: 66.73081541061401 and batch: 200, loss is 4.40882453918457 and perplexity is 82.17281578747624
At time: 67.36108565330505 and batch: 250, loss is 4.359429740905762 and perplexity is 78.2125203025035
At time: 67.98865270614624 and batch: 300, loss is 4.320888462066651 and perplexity is 75.25546022052448
At time: 68.61745715141296 and batch: 350, loss is 4.341901578903198 and perplexity is 76.85354355208796
At time: 69.24515771865845 and batch: 400, loss is 4.314988842010498 and perplexity is 74.81278867897512
At time: 69.87208199501038 and batch: 450, loss is 4.240428953170777 and perplexity is 69.43763094334227
At time: 70.52980756759644 and batch: 500, loss is 4.2391780614852905 and perplexity is 69.35082629106311
At time: 71.15709590911865 and batch: 550, loss is 4.2727413082122805 and perplexity is 71.71796744637393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.775783132999502 and perplexity of 118.60316030940028
Finished 9 epochs...
Completing Train Step...
At time: 72.81500720977783 and batch: 50, loss is 4.419491147994995 and perplexity is 83.05401240318183
At time: 73.45275521278381 and batch: 100, loss is 4.39744387626648 and perplexity is 81.2429360304441
At time: 74.07714462280273 and batch: 150, loss is 4.3746107482910155 and perplexity is 79.4089234764122
At time: 74.7013988494873 and batch: 200, loss is 4.334369592666626 and perplexity is 76.27685823905857
At time: 75.32540535926819 and batch: 250, loss is 4.292810306549073 and perplexity is 73.1718150521515
At time: 75.95062017440796 and batch: 300, loss is 4.263931465148926 and perplexity is 71.08891838863188
At time: 76.57541155815125 and batch: 350, loss is 4.291673107147217 and perplexity is 73.08865140362958
At time: 77.19973230361938 and batch: 400, loss is 4.278264756202698 and perplexity is 72.11519392955513
At time: 77.82581806182861 and batch: 450, loss is 4.214907646179199 and perplexity is 67.68791434474838
At time: 78.45900368690491 and batch: 500, loss is 4.2277425050735475 and perplexity is 68.56227834040794
At time: 79.0859763622284 and batch: 550, loss is 4.2734698295593265 and perplexity is 71.7702345531652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.762859587973737 and perplexity of 117.0802489299127
Finished 10 epochs...
Completing Train Step...
At time: 80.74057078361511 and batch: 50, loss is 4.380376396179199 and perplexity is 79.86808979201268
At time: 81.38152956962585 and batch: 100, loss is 4.361175584793091 and perplexity is 78.34918641712375
At time: 82.00829768180847 and batch: 150, loss is 4.339080753326416 and perplexity is 76.63705858748106
At time: 82.63367080688477 and batch: 200, loss is 4.3032027435302735 and perplexity is 73.93621363116175
At time: 83.2606725692749 and batch: 250, loss is 4.263547744750976 and perplexity is 71.06164535352258
At time: 83.88829159736633 and batch: 300, loss is 4.237605972290039 and perplexity is 69.24188626052842
At time: 84.51635479927063 and batch: 350, loss is 4.269835329055786 and perplexity is 71.50985905360577
At time: 85.14359211921692 and batch: 400, loss is 4.262625823020935 and perplexity is 70.99616226827632
At time: 85.76727223396301 and batch: 450, loss is 4.200940885543823 and perplexity is 66.74910477258953
At time: 86.40518975257874 and batch: 500, loss is 4.218644309043884 and perplexity is 67.94131440111403
At time: 87.03013348579407 and batch: 550, loss is 4.265774250030518 and perplexity is 71.22004075073436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.755768471575798 and perplexity of 116.25295593892393
Finished 11 epochs...
Completing Train Step...
At time: 88.68498921394348 and batch: 50, loss is 4.355568761825562 and perplexity is 77.91112561151566
At time: 89.32614731788635 and batch: 100, loss is 4.335704441070557 and perplexity is 76.3787442676086
At time: 89.95284056663513 and batch: 150, loss is 4.314021902084351 and perplexity is 74.74048416930717
At time: 90.57962393760681 and batch: 200, loss is 4.282736892700195 and perplexity is 72.4384251486991
At time: 91.20653939247131 and batch: 250, loss is 4.242452459335327 and perplexity is 69.57828067241306
At time: 91.83350276947021 and batch: 300, loss is 4.220034904479981 and perplexity is 68.03585900425392
At time: 92.46401381492615 and batch: 350, loss is 4.254127616882324 and perplexity is 70.39537865345903
At time: 93.09556317329407 and batch: 400, loss is 4.2483371114730835 and perplexity is 69.98893173479935
At time: 93.7247622013092 and batch: 450, loss is 4.18811755657196 and perplexity is 65.8986236970566
At time: 94.357994556427 and batch: 500, loss is 4.207422790527343 and perplexity is 67.18317139699039
At time: 94.98718523979187 and batch: 550, loss is 4.256214575767517 and perplexity is 70.54244432103977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.750674633269615 and perplexity of 115.66228783983075
Finished 12 epochs...
Completing Train Step...
At time: 96.64834022521973 and batch: 50, loss is 4.335423936843872 and perplexity is 76.35732271157164
At time: 97.28768491744995 and batch: 100, loss is 4.316082372665405 and perplexity is 74.89464350399795
At time: 97.91141676902771 and batch: 150, loss is 4.293496437072754 and perplexity is 73.22203769560599
At time: 98.53612184524536 and batch: 200, loss is 4.264511575698853 and perplexity is 71.13016978420004
At time: 99.16156435012817 and batch: 250, loss is 4.2252234268188475 and perplexity is 68.38978195311536
At time: 99.78662180900574 and batch: 300, loss is 4.204562721252441 and perplexity is 66.99129738995748
At time: 100.41172051429749 and batch: 350, loss is 4.240205130577087 and perplexity is 69.42209097184814
At time: 101.03655862808228 and batch: 400, loss is 4.235508842468262 and perplexity is 69.09682919085154
At time: 101.66187286376953 and batch: 450, loss is 4.176360301971435 and perplexity is 65.12837368598193
At time: 102.30120944976807 and batch: 500, loss is 4.196700344085693 and perplexity is 66.46665172669412
At time: 102.93568921089172 and batch: 550, loss is 4.246091904640198 and perplexity is 69.83196838057682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.747362177422706 and perplexity of 115.27979546250175
Finished 13 epochs...
Completing Train Step...
At time: 104.59190893173218 and batch: 50, loss is 4.318250980377197 and perplexity is 75.05723684233523
At time: 105.24395656585693 and batch: 100, loss is 4.298831272125244 and perplexity is 73.6137090109217
At time: 105.8744592666626 and batch: 150, loss is 4.2755210590362545 and perplexity is 71.91760286528805
At time: 106.50503492355347 and batch: 200, loss is 4.249171333312988 and perplexity is 70.04734239053903
At time: 107.14284253120422 and batch: 250, loss is 4.2102605676651 and perplexity is 67.37409303322596
At time: 107.77646136283875 and batch: 300, loss is 4.1905042886734005 and perplexity is 66.05609390252003
At time: 108.40836501121521 and batch: 350, loss is 4.227832851409912 and perplexity is 68.56847297089564
At time: 109.03738760948181 and batch: 400, loss is 4.221652040481567 and perplexity is 68.14597125049089
At time: 109.66549181938171 and batch: 450, loss is 4.1631404447555544 and perplexity is 64.27305195734264
At time: 110.29088997840881 and batch: 500, loss is 4.183126192092896 and perplexity is 65.57051917334017
At time: 110.91798448562622 and batch: 550, loss is 4.233742818832398 and perplexity is 68.97491024489449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.74366240805768 and perplexity of 114.85407482654225
Finished 14 epochs...
Completing Train Step...
At time: 112.60154128074646 and batch: 50, loss is 4.30107738494873 and perplexity is 73.77923953724233
At time: 113.24508690834045 and batch: 100, loss is 4.2806954765319825 and perplexity is 72.29069901296421
At time: 113.87484979629517 and batch: 150, loss is 4.257639980316162 and perplexity is 70.64306753940706
At time: 114.5060362815857 and batch: 200, loss is 4.233665885925293 and perplexity is 68.96960400864678
At time: 115.13659858703613 and batch: 250, loss is 4.196148772239685 and perplexity is 66.43000070166919
At time: 115.76842212677002 and batch: 300, loss is 4.177145795822144 and perplexity is 65.17955172040317
At time: 116.39818048477173 and batch: 350, loss is 4.2149556016922 and perplexity is 67.69116043123798
At time: 117.03042221069336 and batch: 400, loss is 4.208582425117493 and perplexity is 67.26112451624941
At time: 117.66035962104797 and batch: 450, loss is 4.151452050209046 and perplexity is 63.52617655759949
At time: 118.31871914863586 and batch: 500, loss is 4.172365870475769 and perplexity is 64.86874174508277
At time: 118.95304560661316 and batch: 550, loss is 4.223812131881714 and perplexity is 68.29333187585486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.740026271089595 and perplexity of 114.4372080308136
Finished 15 epochs...
Completing Train Step...
At time: 120.63235521316528 and batch: 50, loss is 4.286062140464782 and perplexity is 72.67970178997928
At time: 121.27509760856628 and batch: 100, loss is 4.262512216567993 and perplexity is 70.98809710424456
At time: 121.90390825271606 and batch: 150, loss is 4.240687351226807 and perplexity is 69.45557581055294
At time: 122.5379490852356 and batch: 200, loss is 4.217716460227966 and perplexity is 67.87830436940716
At time: 123.19273114204407 and batch: 250, loss is 4.181158113479614 and perplexity is 65.4415981418686
At time: 123.82704472541809 and batch: 300, loss is 4.162285094261169 and perplexity is 64.21809947574182
At time: 124.46655559539795 and batch: 350, loss is 4.198975777626037 and perplexity is 66.61806437472681
At time: 125.10395407676697 and batch: 400, loss is 4.192359580993652 and perplexity is 66.17876102274009
At time: 125.73290586471558 and batch: 450, loss is 4.13675591468811 and perplexity is 62.59941385278096
At time: 126.36705112457275 and batch: 500, loss is 4.157052016258239 and perplexity is 63.882918929868694
At time: 127.01592588424683 and batch: 550, loss is 4.208889966011047 and perplexity is 67.28181324373641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.735443439889462 and perplexity of 113.91396151440746
Finished 16 epochs...
Completing Train Step...
At time: 128.65998530387878 and batch: 50, loss is 4.2706026315689085 and perplexity is 71.56474980438307
At time: 129.29834723472595 and batch: 100, loss is 4.246656007766724 and perplexity is 69.87137192505818
At time: 129.92318630218506 and batch: 150, loss is 4.224593453407287 and perplexity is 68.34671177681987
At time: 130.54867243766785 and batch: 200, loss is 4.20387405872345 and perplexity is 66.94517887554558
At time: 131.17440390586853 and batch: 250, loss is 4.168131103515625 and perplexity is 64.59461857432773
At time: 131.80010604858398 and batch: 300, loss is 4.1503670167922975 and perplexity is 63.45728591428003
At time: 132.4271695613861 and batch: 350, loss is 4.185573544502258 and perplexity is 65.73118987013542
At time: 133.05496525764465 and batch: 400, loss is 4.178487272262573 and perplexity is 65.26704722689303
At time: 133.68459939956665 and batch: 450, loss is 4.124018650054932 and perplexity is 61.807125060242036
At time: 134.33719444274902 and batch: 500, loss is 4.144330124855042 and perplexity is 63.07535513393334
At time: 134.96638870239258 and batch: 550, loss is 4.196181440353394 and perplexity is 66.43217087993344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.734697382500831 and perplexity of 113.8290068562171
Finished 17 epochs...
Completing Train Step...
At time: 136.66547465324402 and batch: 50, loss is 4.256385703086853 and perplexity is 70.55451709339692
At time: 137.3126835823059 and batch: 100, loss is 4.231370162963867 and perplexity is 68.81145051284982
At time: 137.93927192687988 and batch: 150, loss is 4.21052677154541 and perplexity is 67.39203066565157
At time: 138.56478214263916 and batch: 200, loss is 4.190193920135498 and perplexity is 66.03559535045333
At time: 139.19013261795044 and batch: 250, loss is 4.1549955129623415 and perplexity is 63.751678490988965
At time: 139.81827402114868 and batch: 300, loss is 4.13926260471344 and perplexity is 62.75652801506009
At time: 140.4436366558075 and batch: 350, loss is 4.173264565467835 and perplexity is 64.92706516198663
At time: 141.06842231750488 and batch: 400, loss is 4.166018466949463 and perplexity is 64.45829767011647
At time: 141.69410943984985 and batch: 450, loss is 4.111520948410035 and perplexity is 61.03948491209447
At time: 142.31974267959595 and batch: 500, loss is 4.133294105529785 and perplexity is 62.38308129565965
At time: 142.946058511734 and batch: 550, loss is 4.185134510993958 and perplexity is 65.7023380091723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7322160436752 and perplexity of 113.54690865736933
Finished 18 epochs...
Completing Train Step...
At time: 144.6205472946167 and batch: 50, loss is 4.242533235549927 and perplexity is 69.58390116954232
At time: 145.2662205696106 and batch: 100, loss is 4.217329807281494 and perplexity is 67.85206409628759
At time: 145.8989176750183 and batch: 150, loss is 4.196674704551697 and perplexity is 66.46494757456448
At time: 146.5298945903778 and batch: 200, loss is 4.177832798957825 and perplexity is 65.2243456618447
At time: 147.15953636169434 and batch: 250, loss is 4.142472581863403 and perplexity is 62.95829870239448
At time: 147.78927755355835 and batch: 300, loss is 4.129105920791626 and perplexity is 62.12235579106048
At time: 148.41815447807312 and batch: 350, loss is 4.161868081092835 and perplexity is 64.19132526560213
At time: 149.0519027709961 and batch: 400, loss is 4.15371940612793 and perplexity is 63.67037642445581
At time: 149.68232369422913 and batch: 450, loss is 4.100824389457703 and perplexity is 60.39005200413975
At time: 150.33294987678528 and batch: 500, loss is 4.12227201461792 and perplexity is 61.69926476907647
At time: 150.96246886253357 and batch: 550, loss is 4.1745322942733765 and perplexity is 65.00942726812838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7312375332446805 and perplexity of 113.43585616476697
Finished 19 epochs...
Completing Train Step...
At time: 152.60945844650269 and batch: 50, loss is 4.230754599571228 and perplexity is 68.76910573720369
At time: 153.2479226589203 and batch: 100, loss is 4.204612627029419 and perplexity is 66.99464072612966
At time: 153.8873815536499 and batch: 150, loss is 4.184664163589478 and perplexity is 65.6714423514372
At time: 154.54246926307678 and batch: 200, loss is 4.16535744190216 and perplexity is 64.41570320040667
At time: 155.1783094406128 and batch: 250, loss is 4.129415807723999 and perplexity is 62.141609680438606
At time: 155.8030242919922 and batch: 300, loss is 4.117491354942322 and perplexity is 61.405005519080085
At time: 156.42926716804504 and batch: 350, loss is 4.149929332733154 and perplexity is 63.429517749083764
At time: 157.05568408966064 and batch: 400, loss is 4.141614470481873 and perplexity is 62.9042966429208
At time: 157.68186473846436 and batch: 450, loss is 4.088734679222107 and perplexity is 59.66434938061747
At time: 158.30820989608765 and batch: 500, loss is 4.111477684974671 and perplexity is 61.036844191408065
At time: 158.93450140953064 and batch: 550, loss is 4.163474373817444 and perplexity is 64.29451818118604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.727977022211602 and perplexity of 113.06659961385263
Finished 20 epochs...
Completing Train Step...
At time: 160.57306718826294 and batch: 50, loss is 4.2174458503723145 and perplexity is 67.85993831638943
At time: 161.21240735054016 and batch: 100, loss is 4.191592407226563 and perplexity is 66.12800988330554
At time: 161.83873438835144 and batch: 150, loss is 4.173252778053284 and perplexity is 64.92629984426455
At time: 162.4654791355133 and batch: 200, loss is 4.154258637428284 and perplexity is 63.704718742723514
At time: 163.09073042869568 and batch: 250, loss is 4.117143778800965 and perplexity is 61.383666312916034
At time: 163.72582697868347 and batch: 300, loss is 4.106499228477478 and perplexity is 60.73373006418071
At time: 164.36417961120605 and batch: 350, loss is 4.138780303001404 and perplexity is 62.72626773203675
At time: 165.00272417068481 and batch: 400, loss is 4.1302994441986085 and perplexity is 62.19654454099197
At time: 165.63063383102417 and batch: 450, loss is 4.076951823234558 and perplexity is 58.965458495498176
At time: 166.2720184326172 and batch: 500, loss is 4.100533270835877 and perplexity is 60.37247389421002
At time: 166.89971017837524 and batch: 550, loss is 4.15103750705719 and perplexity is 63.499847673750224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.726411860039893 and perplexity of 112.8897704684965
Finished 21 epochs...
Completing Train Step...
At time: 168.56517601013184 and batch: 50, loss is 4.2055659055709835 and perplexity is 67.05853572956292
At time: 169.2110104560852 and batch: 100, loss is 4.178715739250183 and perplexity is 65.2819602960703
At time: 169.84379148483276 and batch: 150, loss is 4.160563163757324 and perplexity is 64.10761552150022
At time: 170.47686004638672 and batch: 200, loss is 4.141878628730774 and perplexity is 62.920915526681355
At time: 171.10828351974487 and batch: 250, loss is 4.105688104629516 and perplexity is 60.6844874609747
At time: 171.74017357826233 and batch: 300, loss is 4.095575079917908 and perplexity is 60.07387650560034
At time: 172.37162351608276 and batch: 350, loss is 4.12790421962738 and perplexity is 62.04774812082108
At time: 173.00306248664856 and batch: 400, loss is 4.118871684074402 and perplexity is 61.48982316171815
At time: 173.63437294960022 and batch: 450, loss is 4.066619544029236 and perplexity is 58.35934756071166
At time: 174.26756405830383 and batch: 500, loss is 4.090576572418213 and perplexity is 59.77434600971549
At time: 174.89817094802856 and batch: 550, loss is 4.140640468597412 and perplexity is 62.84305756777856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.723102326088763 and perplexity of 112.5167755006022
Finished 22 epochs...
Completing Train Step...
At time: 176.5540370941162 and batch: 50, loss is 4.194747228622436 and perplexity is 66.3369613727607
At time: 177.1967511177063 and batch: 100, loss is 4.168025608062744 and perplexity is 64.58780449522105
At time: 177.82680916786194 and batch: 150, loss is 4.149605722427368 and perplexity is 63.408994624375545
At time: 178.45515894889832 and batch: 200, loss is 4.132221183776855 and perplexity is 62.31618502437245
At time: 179.08265495300293 and batch: 250, loss is 4.095629558563233 and perplexity is 60.0771493381607
At time: 179.71102499961853 and batch: 300, loss is 4.08591103553772 and perplexity is 59.496116144439696
At time: 180.3389856815338 and batch: 350, loss is 4.118885846138 and perplexity is 61.49069399067076
At time: 180.97513341903687 and batch: 400, loss is 4.109535989761352 and perplexity is 60.918444228725036
At time: 181.60792183876038 and batch: 450, loss is 4.057389607429505 and perplexity is 57.82317271908832
At time: 182.24827098846436 and batch: 500, loss is 4.081344237327576 and perplexity is 59.22502885932652
At time: 182.87782883644104 and batch: 550, loss is 4.129441800117493 and perplexity is 62.14322491060155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7218757791722075 and perplexity of 112.37885299805235
Finished 23 epochs...
Completing Train Step...
At time: 184.52097606658936 and batch: 50, loss is 4.18453851222992 and perplexity is 65.66319116381892
At time: 185.16051197052002 and batch: 100, loss is 4.158659520149231 and perplexity is 63.98569355378892
At time: 185.78863835334778 and batch: 150, loss is 4.139603247642517 and perplexity is 62.77790922405396
At time: 186.41670560836792 and batch: 200, loss is 4.123772578239441 and perplexity is 61.79191793986677
At time: 187.04401063919067 and batch: 250, loss is 4.085996489524842 and perplexity is 59.50120054202044
At time: 187.67216420173645 and batch: 300, loss is 4.076030368804932 and perplexity is 58.911149538029974
At time: 188.30065298080444 and batch: 350, loss is 4.109607262611389 and perplexity is 60.92278621459605
At time: 188.92895579338074 and batch: 400, loss is 4.101068658828735 and perplexity is 60.40480524596861
At time: 189.5569794178009 and batch: 450, loss is 4.048687858581543 and perplexity is 57.32219285417273
At time: 190.18575191497803 and batch: 500, loss is 4.072443928718567 and perplexity is 58.70024665113929
At time: 190.81316947937012 and batch: 550, loss is 4.1197192668914795 and perplexity is 61.541962972537604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.7212699727809175 and perplexity of 112.31079378809397
Finished 24 epochs...
Completing Train Step...
At time: 192.46078276634216 and batch: 50, loss is 4.175161552429199 and perplexity is 65.05034785389813
At time: 193.1008002758026 and batch: 100, loss is 4.148491334915161 and perplexity is 63.33837179051527
At time: 193.72805428504944 and batch: 150, loss is 4.1285303640365605 and perplexity is 62.086611137069625
At time: 194.35666275024414 and batch: 200, loss is 4.11319580078125 and perplexity is 61.1418026978124
At time: 194.9846386909485 and batch: 250, loss is 4.075882029533386 and perplexity is 58.902411349145915
At time: 195.61231994628906 and batch: 300, loss is 4.066619572639465 and perplexity is 58.35934923038598
At time: 196.24004101753235 and batch: 350, loss is 4.10058135509491 and perplexity is 60.37537692967781
At time: 196.86779928207397 and batch: 400, loss is 4.091014075279236 and perplexity is 59.8005031786114
At time: 197.49502539634705 and batch: 450, loss is 4.0400487995147705 and perplexity is 56.82911597108813
At time: 198.1460406780243 and batch: 500, loss is 4.063653483390808 and perplexity is 58.18650665204962
At time: 198.77475309371948 and batch: 550, loss is 4.111413283348083 and perplexity is 61.032913445934945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.723292573969415 and perplexity of 112.53818361503859
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 200.42955994606018 and batch: 50, loss is 4.160885696411133 and perplexity is 64.12829565569373
At time: 201.07355093955994 and batch: 100, loss is 4.133414263725281 and perplexity is 62.39057758449885
At time: 201.70536637306213 and batch: 150, loss is 4.1007809782028195 and perplexity is 60.3874304531026
At time: 202.3365843296051 and batch: 200, loss is 4.081434741020202 and perplexity is 59.2303891856952
At time: 202.96740698814392 and batch: 250, loss is 4.035590186119079 and perplexity is 56.576300934422655
At time: 203.59776616096497 and batch: 300, loss is 4.013823609352112 and perplexity is 55.35813429108297
At time: 204.22930097579956 and batch: 350, loss is 4.042606792449951 and perplexity is 56.97467053265723
At time: 204.86113476753235 and batch: 400, loss is 4.021695671081543 and perplexity is 55.795636705144034
At time: 205.49241948127747 and batch: 450, loss is 3.9614270162582397 and perplexity is 52.532236842911665
At time: 206.12401151657104 and batch: 500, loss is 3.976121129989624 and perplexity is 53.30985068715972
At time: 206.754065990448 and batch: 550, loss is 4.022098135948181 and perplexity is 55.81809700806783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.687208784387467 and perplexity of 108.54977074949876
Finished 26 epochs...
Completing Train Step...
At time: 208.4060082435608 and batch: 50, loss is 4.131822056770325 and perplexity is 62.291317914882875
At time: 209.04595398902893 and batch: 100, loss is 4.1090768480300905 and perplexity is 60.890480448936486
At time: 209.67304968833923 and batch: 150, loss is 4.079851717948913 and perplexity is 59.13670028850078
At time: 210.3273937702179 and batch: 200, loss is 4.062786178588867 and perplexity is 58.136063093554455
At time: 210.9630126953125 and batch: 250, loss is 4.020617995262146 and perplexity is 55.73553948511582
At time: 211.59176898002625 and batch: 300, loss is 4.00250834941864 and perplexity is 54.735273175519815
At time: 212.2197186946869 and batch: 350, loss is 4.034248423576355 and perplexity is 56.50043987816605
At time: 212.84828996658325 and batch: 400, loss is 4.016743335723877 and perplexity is 55.52000108404622
At time: 213.47753643989563 and batch: 450, loss is 3.9592521476745604 and perplexity is 52.4181102814935
At time: 214.13120126724243 and batch: 500, loss is 3.9785679960250855 and perplexity is 53.44045246753086
At time: 214.76706647872925 and batch: 550, loss is 4.026875538825989 and perplexity is 56.08540054529015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6841982577709445 and perplexity of 108.22347019014525
Finished 27 epochs...
Completing Train Step...
At time: 216.41855669021606 and batch: 50, loss is 4.122992315292358 and perplexity is 61.74372280075491
At time: 217.06003379821777 and batch: 100, loss is 4.100476450920105 and perplexity is 60.36904363278293
At time: 217.6882827281952 and batch: 150, loss is 4.07163969039917 and perplexity is 58.65305664197477
At time: 218.31790447235107 and batch: 200, loss is 4.055414633750916 and perplexity is 57.70908617100347
At time: 218.9466471672058 and batch: 250, loss is 4.014421167373658 and perplexity is 55.39122387377455
At time: 219.57623672485352 and batch: 300, loss is 3.9972436666488647 and perplexity is 54.447866542143416
At time: 220.20443964004517 and batch: 350, loss is 4.03092583656311 and perplexity is 56.313023775954164
At time: 220.83414793014526 and batch: 400, loss is 4.015336399078369 and perplexity is 55.441942884309434
At time: 221.47389435768127 and batch: 450, loss is 3.9587213373184205 and perplexity is 52.390293589054146
At time: 222.10786724090576 and batch: 500, loss is 3.979535083770752 and perplexity is 53.492159072617234
At time: 222.73810863494873 and batch: 550, loss is 4.028227620124817 and perplexity is 56.16128385514644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.682797046417885 and perplexity of 108.07193242805772
Finished 28 epochs...
Completing Train Step...
At time: 224.40821075439453 and batch: 50, loss is 4.116881446838379 and perplexity is 61.367565527229154
At time: 225.05133295059204 and batch: 100, loss is 4.094747595787048 and perplexity is 60.02418688763321
At time: 225.682701587677 and batch: 150, loss is 4.065893063545227 and perplexity is 58.316966030190684
At time: 226.31474900245667 and batch: 200, loss is 4.050260791778564 and perplexity is 57.41242778239185
At time: 226.94766235351562 and batch: 250, loss is 4.010075340270996 and perplexity is 55.151025500151704
At time: 227.57896995544434 and batch: 300, loss is 3.9933921003341677 and perplexity is 54.23856031062511
At time: 228.21102166175842 and batch: 350, loss is 4.0287453556060795 and perplexity is 56.19036807278757
At time: 228.84238719940186 and batch: 400, loss is 4.014276127815247 and perplexity is 55.383190537713915
At time: 229.47441267967224 and batch: 450, loss is 3.95810088634491 and perplexity is 52.35779806237735
At time: 230.11858558654785 and batch: 500, loss is 3.9795294761657716 and perplexity is 53.49185911056065
At time: 230.74985146522522 and batch: 550, loss is 4.02831160068512 and perplexity is 56.1660005092828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.681867883560505 and perplexity of 107.97156263968108
Finished 29 epochs...
Completing Train Step...
At time: 232.41635084152222 and batch: 50, loss is 4.112021384239196 and perplexity is 61.070038901856016
At time: 233.05852913856506 and batch: 100, loss is 4.090254116058349 and perplexity is 59.75507449896557
At time: 233.6892008781433 and batch: 150, loss is 4.061392307281494 and perplexity is 58.05508535267031
At time: 234.31907677650452 and batch: 200, loss is 4.046374621391297 and perplexity is 57.189746275073105
At time: 234.9481644630432 and batch: 250, loss is 4.006537585258484 and perplexity is 54.956259403746586
At time: 235.57867789268494 and batch: 300, loss is 3.99017991065979 and perplexity is 54.064615289009446
At time: 236.20836210250854 and batch: 350, loss is 4.026907262802124 and perplexity is 56.08717982542134
At time: 236.83802771568298 and batch: 400, loss is 4.013229222297668 and perplexity is 55.325239909667864
At time: 237.46752500534058 and batch: 450, loss is 3.957178134918213 and perplexity is 52.30950711321422
At time: 238.09772419929504 and batch: 500, loss is 3.978892788887024 and perplexity is 53.4578123640637
At time: 238.72699236869812 and batch: 550, loss is 4.027762174606323 and perplexity is 56.1351499196966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6811205275515295 and perplexity of 107.89089958930599
Finished 30 epochs...
Completing Train Step...
At time: 240.3892993927002 and batch: 50, loss is 4.10784688949585 and perplexity is 60.815633721475564
At time: 241.0301878452301 and batch: 100, loss is 4.086485133171082 and perplexity is 59.530282530394715
At time: 241.65814471244812 and batch: 150, loss is 4.057515931129456 and perplexity is 57.8304776175903
At time: 242.2967095375061 and batch: 200, loss is 4.042937903404236 and perplexity is 56.99353859372584
At time: 242.925057888031 and batch: 250, loss is 4.003453464508056 and perplexity is 54.78702876176154
At time: 243.55248284339905 and batch: 300, loss is 3.987284293174744 and perplexity is 53.90829128014917
At time: 244.17974996566772 and batch: 350, loss is 4.025167469978332 and perplexity is 55.98968458780395
At time: 244.80864691734314 and batch: 400, loss is 4.012079486846924 and perplexity is 55.26166707299782
At time: 245.43667101860046 and batch: 450, loss is 3.9560008335113523 and perplexity is 52.24795929417359
At time: 246.08677983283997 and batch: 500, loss is 3.9778195238113403 and perplexity is 53.40046873899046
At time: 246.7317087650299 and batch: 550, loss is 4.026820368766785 and perplexity is 56.08230639577459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.680401091879987 and perplexity of 107.81330694231242
Finished 31 epochs...
Completing Train Step...
At time: 248.37957525253296 and batch: 50, loss is 4.104115552902222 and perplexity is 60.589132959990906
At time: 249.01932668685913 and batch: 100, loss is 4.08309003829956 and perplexity is 59.32851427846384
At time: 249.6458559036255 and batch: 150, loss is 4.053953289985657 and perplexity is 57.624814947339274
At time: 250.27268528938293 and batch: 200, loss is 4.039783048629761 and perplexity is 56.814015589783196
At time: 250.89933228492737 and batch: 250, loss is 4.0006383037567135 and perplexity is 54.63301136229505
At time: 251.52635598182678 and batch: 300, loss is 3.9845293521881104 and perplexity is 53.75998150512329
At time: 252.1535668373108 and batch: 350, loss is 4.023492202758789 and perplexity is 55.895965428817625
At time: 252.7802016735077 and batch: 400, loss is 4.010853190422058 and perplexity is 55.193941422582725
At time: 253.4109845161438 and batch: 450, loss is 3.954717617034912 and perplexity is 52.180956850460895
At time: 254.04299306869507 and batch: 500, loss is 3.976534380912781 and perplexity is 53.33188558482686
At time: 254.67579579353333 and batch: 550, loss is 4.025577030181885 and perplexity is 56.01262043090424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.679729705161237 and perplexity of 107.74094681345967
Finished 32 epochs...
Completing Train Step...
At time: 256.3601999282837 and batch: 50, loss is 4.100702137947082 and perplexity is 60.382669680315104
At time: 257.0055389404297 and batch: 100, loss is 4.080030431747437 and perplexity is 59.14726977727009
At time: 257.6357192993164 and batch: 150, loss is 4.050682706832886 and perplexity is 57.43665606075667
At time: 258.2660143375397 and batch: 200, loss is 4.036970295906067 and perplexity is 56.65443634637512
At time: 258.89639616012573 and batch: 250, loss is 3.997934880256653 and perplexity is 54.48551465835855
At time: 259.5266652107239 and batch: 300, loss is 3.9819475984573365 and perplexity is 53.62136548551385
At time: 260.1566014289856 and batch: 350, loss is 4.021902875900269 and perplexity is 55.80719902777634
At time: 260.78642201423645 and batch: 400, loss is 4.009609127044678 and perplexity is 55.1253193553565
At time: 261.41626930236816 and batch: 450, loss is 3.953286108970642 and perplexity is 52.10631282943397
At time: 262.05918860435486 and batch: 500, loss is 3.9751160192489623 and perplexity is 53.25629530270573
At time: 262.68969202041626 and batch: 550, loss is 4.024155864715576 and perplexity is 55.93307376694471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.679188180477061 and perplexity of 107.68261822587405
Finished 33 epochs...
Completing Train Step...
At time: 264.3412621021271 and batch: 50, loss is 4.097577481269837 and perplexity is 60.19428903400163
At time: 264.98165583610535 and batch: 100, loss is 4.077196974754333 and perplexity is 58.979915739297816
At time: 265.60877752304077 and batch: 150, loss is 4.0476364898681645 and perplexity is 57.26195776421665
At time: 266.23720693588257 and batch: 200, loss is 4.034243512153625 and perplexity is 56.50016238130284
At time: 266.8646264076233 and batch: 250, loss is 3.9954331302642823 and perplexity is 54.34937588606887
At time: 267.49273777008057 and batch: 300, loss is 3.9794944524765015 and perplexity is 53.48998566111643
At time: 268.1207458972931 and batch: 350, loss is 4.020372161865234 and perplexity is 55.721839512139915
At time: 268.7486445903778 and batch: 400, loss is 4.008268823623657 and perplexity is 55.051484193053184
At time: 269.3751907348633 and batch: 450, loss is 3.951782851219177 and perplexity is 52.02804245578532
At time: 270.01242208480835 and batch: 500, loss is 3.9735856294631957 and perplexity is 53.17485474613977
At time: 270.6473560333252 and batch: 550, loss is 4.022634739875794 and perplexity is 55.8480572558286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6786934061253325 and perplexity of 107.62935280651723
Finished 34 epochs...
Completing Train Step...
At time: 272.29400992393494 and batch: 50, loss is 4.09461356639862 and perplexity is 60.016142421683256
At time: 272.9347994327545 and batch: 100, loss is 4.074386072158814 and perplexity is 58.81436172816839
At time: 273.5612781047821 and batch: 150, loss is 4.044805731773376 and perplexity is 57.10009222293609
At time: 274.20156931877136 and batch: 200, loss is 4.031593589782715 and perplexity is 56.35063953651113
At time: 274.84130001068115 and batch: 250, loss is 3.9930194854736327 and perplexity is 54.21835398186233
At time: 275.48825216293335 and batch: 300, loss is 3.977070770263672 and perplexity is 53.36049991384211
At time: 276.12239933013916 and batch: 350, loss is 4.018803963661194 and perplexity is 55.63452510453371
At time: 276.74996972084045 and batch: 400, loss is 4.006784048080444 and perplexity is 54.969805747790126
At time: 277.3768060207367 and batch: 450, loss is 3.9501304721832273 and perplexity is 51.94214339759594
At time: 278.0171823501587 and batch: 500, loss is 3.9718852043151855 and perplexity is 53.08451171843427
At time: 278.643404006958 and batch: 550, loss is 4.020973262786865 and perplexity is 55.75534403001508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.678187593500665 and perplexity of 107.57492628705363
Finished 35 epochs...
Completing Train Step...
At time: 280.29218316078186 and batch: 50, loss is 4.091840815544129 and perplexity is 59.849963104889135
At time: 280.93613386154175 and batch: 100, loss is 4.07166446685791 and perplexity is 58.654509875015535
At time: 281.58174204826355 and batch: 150, loss is 4.042149033546448 and perplexity is 56.94859583836377
At time: 282.2191219329834 and batch: 200, loss is 4.029122467041016 and perplexity is 56.21156209912476
At time: 282.85045766830444 and batch: 250, loss is 3.990652904510498 and perplexity is 54.090193568289564
At time: 283.48177909851074 and batch: 300, loss is 3.974665389060974 and perplexity is 53.23230181484291
At time: 284.1133568286896 and batch: 350, loss is 4.01700873374939 and perplexity is 55.53473793818989
At time: 284.7456555366516 and batch: 400, loss is 4.005286049842835 and perplexity is 54.8875227209597
At time: 285.38528084754944 and batch: 450, loss is 3.948470125198364 and perplexity is 51.85597297261693
At time: 286.01874709129333 and batch: 500, loss is 3.9700625562667846 and perplexity is 52.98784545779681
At time: 286.6511867046356 and batch: 550, loss is 4.019246425628662 and perplexity is 55.65914671263152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6777379462059505 and perplexity of 107.52656638573374
Finished 36 epochs...
Completing Train Step...
At time: 288.3224768638611 and batch: 50, loss is 4.089149270057678 and perplexity is 59.68909080150981
At time: 288.9662935733795 and batch: 100, loss is 4.068985333442688 and perplexity is 58.49757693366834
At time: 289.5969879627228 and batch: 150, loss is 4.039601864814759 and perplexity is 56.803722742169924
At time: 290.22937965393066 and batch: 200, loss is 4.026462407112121 and perplexity is 56.06223467324764
At time: 290.86081290245056 and batch: 250, loss is 3.9879560470581055 and perplexity is 53.944516550032624
At time: 291.4923903942108 and batch: 300, loss is 3.972003297805786 and perplexity is 53.09078102389473
At time: 292.1238920688629 and batch: 350, loss is 4.0146330547332765 and perplexity is 55.40296181746425
At time: 292.76763677597046 and batch: 400, loss is 4.0030221128463745 and perplexity is 54.763401382087636
At time: 293.40111017227173 and batch: 450, loss is 3.9460045385360716 and perplexity is 51.728275067112634
At time: 294.0442786216736 and batch: 500, loss is 3.9678790187835693 and perplexity is 52.87227073787153
At time: 294.67485761642456 and batch: 550, loss is 4.01632604598999 and perplexity is 55.49683799076507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6765974328873 and perplexity of 107.40400081178265
Finished 37 epochs...
Completing Train Step...
At time: 296.3568289279938 and batch: 50, loss is 4.086392650604248 and perplexity is 59.524777271636104
At time: 296.99914717674255 and batch: 100, loss is 4.065792317390442 and perplexity is 58.3110911160469
At time: 297.628294467926 and batch: 150, loss is 4.0364005708694455 and perplexity is 56.622168088442955
At time: 298.2740545272827 and batch: 200, loss is 4.023251094818115 and perplexity is 55.88249009227169
At time: 298.9045786857605 and batch: 250, loss is 3.9852967500686645 and perplexity is 53.80125263465223
At time: 299.535089969635 and batch: 300, loss is 3.9696076583862303 and perplexity is 52.963746880814284
At time: 300.1647799015045 and batch: 350, loss is 4.012825422286987 and perplexity is 55.30290408710759
At time: 300.7956449985504 and batch: 400, loss is 4.001271252632141 and perplexity is 54.66760221136271
At time: 301.427366733551 and batch: 450, loss is 3.9441644668579103 and perplexity is 51.63317885195951
At time: 302.06237721443176 and batch: 500, loss is 3.965866107940674 and perplexity is 52.76595061317975
At time: 302.70016980171204 and batch: 550, loss is 4.014579973220825 and perplexity is 55.40002102250831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.676153954039228 and perplexity of 107.35637996942295
Finished 38 epochs...
Completing Train Step...
At time: 304.3416516780853 and batch: 50, loss is 4.083740825653076 and perplexity is 59.36713709151517
At time: 304.98196029663086 and batch: 100, loss is 4.0633226585388185 and perplexity is 58.1672602933611
At time: 305.6088788509369 and batch: 150, loss is 4.033912529945374 and perplexity is 56.481464927224316
At time: 306.23499178886414 and batch: 200, loss is 4.020925378799438 and perplexity is 55.75267430574161
At time: 306.8625798225403 and batch: 250, loss is 3.9831266736984254 and perplexity is 53.68462639730974
At time: 307.4955544471741 and batch: 300, loss is 3.967283663749695 and perplexity is 52.840802333701276
At time: 308.1323914527893 and batch: 350, loss is 4.011109981536865 and perplexity is 55.20811655627756
At time: 308.76929354667664 and batch: 400, loss is 3.9996194410324097 and perplexity is 54.57737617062584
At time: 309.39913606643677 and batch: 450, loss is 3.9423509645462036 and perplexity is 51.539626816806134
At time: 310.0511305332184 and batch: 500, loss is 3.9638634586334227 and perplexity is 52.66038465981144
At time: 310.6812994480133 and batch: 550, loss is 4.0128532409667965 and perplexity is 55.304442562288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.675732227081948 and perplexity of 107.31111443547265
Finished 39 epochs...
Completing Train Step...
At time: 312.3403673171997 and batch: 50, loss is 4.081235775947571 and perplexity is 59.21860557931079
At time: 312.9857714176178 and batch: 100, loss is 4.060928049087525 and perplexity is 58.02813905909653
At time: 313.6186990737915 and batch: 150, loss is 4.0315066957473755 and perplexity is 56.34574321478149
At time: 314.25135922431946 and batch: 200, loss is 4.0186190509796145 and perplexity is 55.62423852639693
At time: 314.8912625312805 and batch: 250, loss is 3.9810181856155396 and perplexity is 53.57155225195216
At time: 315.52745246887207 and batch: 300, loss is 3.9650548934936523 and perplexity is 52.723163468852725
At time: 316.1590530872345 and batch: 350, loss is 4.009429130554199 and perplexity is 55.1153978842778
At time: 316.79203033447266 and batch: 400, loss is 3.998002038002014 and perplexity is 54.48917390554987
At time: 317.4239819049835 and batch: 450, loss is 3.9405607652664183 and perplexity is 51.44744315218258
At time: 318.05737829208374 and batch: 500, loss is 3.9618364906311037 and perplexity is 52.55375185226992
At time: 318.6898169517517 and batch: 550, loss is 4.011156659126282 and perplexity is 55.21069359821923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.675276086685505 and perplexity of 107.26217666328726
Finished 40 epochs...
Completing Train Step...
At time: 320.3482234477997 and batch: 50, loss is 4.078803753852844 and perplexity is 59.07475961131006
At time: 320.9920074939728 and batch: 100, loss is 4.0585572624206545 and perplexity is 57.89072966924721
At time: 321.6214792728424 and batch: 150, loss is 4.029134783744812 and perplexity is 56.21225444454877
At time: 322.25179958343506 and batch: 200, loss is 4.016368937492371 and perplexity is 55.499218384572856
At time: 322.8853931427002 and batch: 250, loss is 3.9790553855895996 and perplexity is 53.46650513476979
At time: 323.5208795070648 and batch: 300, loss is 3.9629573249816894 and perplexity is 52.61268892577629
At time: 324.15038108825684 and batch: 350, loss is 4.007717461585998 and perplexity is 55.02113926084291
At time: 324.78049898147583 and batch: 400, loss is 3.996494116783142 and perplexity is 54.40707044235345
At time: 325.40937209129333 and batch: 450, loss is 3.938703474998474 and perplexity is 51.351978996471274
At time: 326.05145621299744 and batch: 500, loss is 3.9598287534713745 and perplexity is 52.448343583250725
At time: 326.6812000274658 and batch: 550, loss is 4.009455118179321 and perplexity is 55.116830221187904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.674859554209608 and perplexity of 107.21750778693621
Finished 41 epochs...
Completing Train Step...
At time: 328.3382270336151 and batch: 50, loss is 4.076379685401917 and perplexity is 58.93173177496027
At time: 328.9803099632263 and batch: 100, loss is 4.056204071044922 and perplexity is 57.75466186303845
At time: 329.6078932285309 and batch: 150, loss is 4.02684609413147 and perplexity is 56.08374915211662
At time: 330.2350513935089 and batch: 200, loss is 4.01418028831482 and perplexity is 55.377882894745696
At time: 330.86192202568054 and batch: 250, loss is 3.977142276763916 and perplexity is 53.36431567286639
At time: 331.4920027256012 and batch: 300, loss is 3.9608375310897825 and perplexity is 52.50127899391977
At time: 332.12229895591736 and batch: 350, loss is 4.0060613489151 and perplexity is 54.930093466801694
At time: 332.7521622180939 and batch: 400, loss is 3.9949911499023436 and perplexity is 54.3253598369456
At time: 333.38177967071533 and batch: 450, loss is 3.9368566370010374 and perplexity is 51.2572277324659
At time: 334.019647359848 and batch: 500, loss is 3.957906494140625 and perplexity is 52.34762110379324
At time: 334.67313861846924 and batch: 550, loss is 4.007776727676392 and perplexity is 55.02440024528784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.674365429168052 and perplexity of 107.16454201837948
Finished 42 epochs...
Completing Train Step...
At time: 336.34989500045776 and batch: 50, loss is 4.074031982421875 and perplexity is 58.79353985291958
At time: 336.9956727027893 and batch: 100, loss is 4.053852872848511 and perplexity is 57.61902871891681
At time: 337.6288480758667 and batch: 150, loss is 4.024515953063965 and perplexity is 55.953218241775424
At time: 338.2616596221924 and batch: 200, loss is 4.011929292678833 and perplexity is 55.25336771615756
At time: 338.89499020576477 and batch: 250, loss is 3.9751839303970335 and perplexity is 53.25991212167153
At time: 339.5283770561218 and batch: 300, loss is 3.958764271736145 and perplexity is 52.3925429840917
At time: 340.1608159542084 and batch: 350, loss is 4.004412670135498 and perplexity is 54.839606000220854
At time: 340.7925536632538 and batch: 400, loss is 3.9936399269104004 and perplexity is 54.252003733076556
At time: 341.4248311519623 and batch: 450, loss is 3.935029273033142 and perplexity is 51.16364764988957
At time: 342.0694787502289 and batch: 500, loss is 3.955934133529663 and perplexity is 52.24447447246543
At time: 342.70076179504395 and batch: 550, loss is 4.00603084564209 and perplexity is 54.92841794471877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.673898575153757 and perplexity of 107.11452349833
Finished 43 epochs...
Completing Train Step...
At time: 344.3483102321625 and batch: 50, loss is 4.071709518432617 and perplexity is 58.65715241257388
At time: 344.9912910461426 and batch: 100, loss is 4.051581044197082 and perplexity is 57.48827673789052
At time: 345.6201412677765 and batch: 150, loss is 4.022253050804138 and perplexity is 55.82674473033863
At time: 346.25065994262695 and batch: 200, loss is 4.009637236595154 and perplexity is 55.12686892508217
At time: 346.8803725242615 and batch: 250, loss is 3.973253092765808 and perplexity is 53.15717509528769
At time: 347.51145601272583 and batch: 300, loss is 3.9567413091659547 and perplexity is 52.28666196345737
At time: 348.1415505409241 and batch: 350, loss is 4.002885217666626 and perplexity is 54.7559050495294
At time: 348.77177929878235 and batch: 400, loss is 3.992232627868652 and perplexity is 54.175708637809116
At time: 349.4021706581116 and batch: 450, loss is 3.9332779932022093 and perplexity is 51.07412419887101
At time: 350.03234243392944 and batch: 500, loss is 3.954026141166687 and perplexity is 52.14488745001509
At time: 350.66234707832336 and batch: 550, loss is 4.004352169036865 and perplexity is 54.836288244174234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.673469056474402 and perplexity of 107.06852568882361
Finished 44 epochs...
Completing Train Step...
At time: 352.3070306777954 and batch: 50, loss is 4.0694784879684445 and perplexity is 58.52643239299433
At time: 352.9482009410858 and batch: 100, loss is 4.049361910820007 and perplexity is 57.36084403157267
At time: 353.5771701335907 and batch: 150, loss is 4.020031752586365 and perplexity is 55.70287450904898
At time: 354.20591711997986 and batch: 200, loss is 4.007522473335266 and perplexity is 55.01041183104051
At time: 354.83476638793945 and batch: 250, loss is 3.9714113903045654 and perplexity is 53.05936549082307
At time: 355.4638876914978 and batch: 300, loss is 3.9547227096557616 and perplexity is 52.18122258896635
At time: 356.0923194885254 and batch: 350, loss is 4.0012350225448605 and perplexity is 54.66562163524162
At time: 356.72147846221924 and batch: 400, loss is 3.990896053314209 and perplexity is 54.10334713322017
At time: 357.34968996047974 and batch: 450, loss is 3.9314667558670044 and perplexity is 50.98170056409504
At time: 357.9913833141327 and batch: 500, loss is 3.952129373550415 and perplexity is 52.04607445841219
At time: 358.61973786354065 and batch: 550, loss is 4.002943091392517 and perplexity is 54.75907406946975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.673160634142287 and perplexity of 107.03550845632225
Finished 45 epochs...
Completing Train Step...
At time: 360.2732779979706 and batch: 50, loss is 4.0673104095458985 and perplexity is 58.39967995204126
At time: 360.9128315448761 and batch: 100, loss is 4.047123074531555 and perplexity is 57.23256614259651
At time: 361.53958344459534 and batch: 150, loss is 4.017809882164001 and perplexity is 55.579247332379566
At time: 362.1666660308838 and batch: 200, loss is 4.005448246002198 and perplexity is 54.896425988360356
At time: 362.800749540329 and batch: 250, loss is 3.9696055173873903 and perplexity is 52.96363348561504
At time: 363.4289171695709 and batch: 300, loss is 3.9526848220825195 and perplexity is 52.07499140426603
At time: 364.0570833683014 and batch: 350, loss is 3.9995646381378176 and perplexity is 54.57438525438862
At time: 364.6855351924896 and batch: 400, loss is 3.989541850090027 and perplexity is 54.03012979286175
At time: 365.31263160705566 and batch: 450, loss is 3.9296803712844848 and perplexity is 50.890708937433686
At time: 365.9406678676605 and batch: 500, loss is 3.950320439338684 and perplexity is 51.95201163611644
At time: 366.5687942504883 and batch: 550, loss is 4.0012633419036865 and perplexity is 54.66716975251689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.67281779837101 and perplexity of 106.99881914479018
Finished 46 epochs...
Completing Train Step...
At time: 368.22489857673645 and batch: 50, loss is 4.06516128540039 and perplexity is 58.27430655951444
At time: 368.8709704875946 and batch: 100, loss is 4.044888367652893 and perplexity is 57.104810934242174
At time: 369.5036518573761 and batch: 150, loss is 4.015542907714844 and perplexity is 55.453393306602834
At time: 370.1385476589203 and batch: 200, loss is 4.003399972915649 and perplexity is 54.784098194730866
At time: 370.7720229625702 and batch: 250, loss is 3.967789902687073 and perplexity is 52.867559177431524
At time: 371.4049537181854 and batch: 300, loss is 3.950691695213318 and perplexity is 51.9713027063754
At time: 372.038613319397 and batch: 350, loss is 3.997922992706299 and perplexity is 54.48486696290925
At time: 372.67216634750366 and batch: 400, loss is 3.9883034467697143 and perplexity is 53.963260115091025
At time: 373.30894589424133 and batch: 450, loss is 3.9279611492156983 and perplexity is 50.80329167390836
At time: 373.954044342041 and batch: 500, loss is 3.9484871292114256 and perplexity is 51.85685473975545
At time: 374.58638548851013 and batch: 550, loss is 3.999663782119751 and perplexity is 54.57979624448344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.672478858460772 and perplexity of 106.9625591199655
Finished 47 epochs...
Completing Train Step...
At time: 376.22777819633484 and batch: 50, loss is 4.063037972450257 and perplexity is 58.15070324043898
At time: 376.8771941661835 and batch: 100, loss is 4.042669439315796 and perplexity is 56.97823992900324
At time: 377.5131676197052 and batch: 150, loss is 4.013299832344055 and perplexity is 55.32914656534719
At time: 378.1436893939972 and batch: 200, loss is 4.001356778144836 and perplexity is 54.672277886011535
At time: 378.7744987010956 and batch: 250, loss is 3.9659849643707275 and perplexity is 52.77222255842114
At time: 379.40506863594055 and batch: 300, loss is 3.9487429666519165 and perplexity is 51.87012336197677
At time: 380.03491401672363 and batch: 350, loss is 3.9963013458251955 and perplexity is 54.396583350101096
At time: 380.6642949581146 and batch: 400, loss is 3.9870761394500733 and perplexity is 53.89707123631605
At time: 381.293687582016 and batch: 450, loss is 3.9264103841781615 and perplexity is 50.7245687615353
At time: 381.9247660636902 and batch: 500, loss is 3.946607747077942 and perplexity is 51.75948741732105
At time: 382.55519938468933 and batch: 550, loss is 3.9981327486038207 and perplexity is 54.496296683764186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6720405740940825 and perplexity of 106.91568937437081
Finished 48 epochs...
Completing Train Step...
At time: 384.2110800743103 and batch: 50, loss is 4.060951361656189 and perplexity is 58.02949185984136
At time: 384.85396027565 and batch: 100, loss is 4.0404880380630495 and perplexity is 56.85408299232492
At time: 385.4843502044678 and batch: 150, loss is 4.011147251129151 and perplexity is 55.21017417861558
At time: 386.11453914642334 and batch: 200, loss is 3.999427971839905 and perplexity is 54.5669272848332
At time: 386.7452323436737 and batch: 250, loss is 3.9642081928253172 and perplexity is 52.678541624445444
At time: 387.3750207424164 and batch: 300, loss is 3.9468728637695314 and perplexity is 51.773211540550186
At time: 388.0047969818115 and batch: 350, loss is 3.9947274208068846 and perplexity is 54.311034548016096
At time: 388.63555788993835 and batch: 400, loss is 3.9857310009002687 and perplexity is 53.824620946837456
At time: 389.26602697372437 and batch: 450, loss is 3.924614653587341 and perplexity is 50.63356283722034
At time: 389.9091897010803 and batch: 500, loss is 3.94468740940094 and perplexity is 51.66018709907827
At time: 390.53882360458374 and batch: 550, loss is 3.9963703298568727 and perplexity is 54.40033597516417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6717259833153255 and perplexity of 106.88205997441611
Finished 49 epochs...
Completing Train Step...
At time: 392.19930267333984 and batch: 50, loss is 4.058970508575439 and perplexity is 57.91465773443064
At time: 392.8464689254761 and batch: 100, loss is 4.038390069007874 and perplexity is 56.73492991898108
At time: 393.4789776802063 and batch: 150, loss is 4.008967566490173 and perplexity is 55.0899644672671
At time: 394.11468958854675 and batch: 200, loss is 3.9974946403503417 and perplexity is 54.461533239666124
At time: 394.7472016811371 and batch: 250, loss is 3.962477879524231 and perplexity is 52.587470057085255
At time: 395.380651473999 and batch: 300, loss is 3.9449692726135255 and perplexity is 51.67475025768971
At time: 396.01382279396057 and batch: 350, loss is 3.9931353521347046 and perplexity is 54.22463644546403
At time: 396.64670610427856 and batch: 400, loss is 3.9843298768997193 and perplexity is 53.749258786802876
At time: 397.279554605484 and batch: 450, loss is 3.9227419090270996 and perplexity is 50.53882784276171
At time: 397.91294145584106 and batch: 500, loss is 3.942632794380188 and perplexity is 51.554154268314
At time: 398.54609775543213 and batch: 550, loss is 3.994711675643921 and perplexity is 54.31017941865851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.671735073657746 and perplexity of 106.88303157335592
Annealing...
Finished Training.
Improved accuracyfrom -122.61247693970412 to -106.88205997441611
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f4192677ac8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 2.092748306085282, 'lr': 21.42281950332199, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.35146804005267573}, 'best_accuracy': -172.13528211555618}, {'params': {'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 5.523306651105269, 'lr': 12.682961565888766, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.780798867653166}, 'best_accuracy': -122.61247693970412}, {'params': {'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 6.283449836288544, 'lr': 28.586967510418816, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.8765580558535844}, 'best_accuracy': -180.25999388537377}, {'params': {'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 4.352298599715045, 'lr': 20.193303942065043, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.6225035402310415}, 'best_accuracy': -180.91240810466013}, {'params': {'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 3.860431593547222, 'lr': 27.04765542952505, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.545279976180985}, 'best_accuracy': -202.92893055884795}, {'params': {'tune_wordvecs': True, 'batch_size': 50, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 5.483130747189248, 'lr': 10.940649164380526, 'data': 'ptb', 'num_layers': 1, 'seq_len': 35, 'dropout': 0.780772010508459}, 'best_accuracy': -106.88205997441611}]
