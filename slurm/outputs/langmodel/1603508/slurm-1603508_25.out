Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'lr': 2.1086149332259407, 'data': 'ptb', 'num_layers': 1, 'anneal': 3.757685917505186, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.34514372824247175, 'seq_len': 20, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9651961326599121 and batch: 50, loss is 6.935203199386597 and perplexity is 1027.8280848424038
At time: 1.4290575981140137 and batch: 100, loss is 5.887215051651001 and perplexity is 360.40018954047116
At time: 1.8755760192871094 and batch: 150, loss is 5.711440238952637 and perplexity is 302.3061479881681
At time: 2.3222970962524414 and batch: 200, loss is 5.59493085861206 and perplexity is 269.05904633322785
At time: 2.772238254547119 and batch: 250, loss is 5.514950885772705 and perplexity is 248.37777806525344
At time: 3.221524715423584 and batch: 300, loss is 5.500034923553467 and perplexity is 244.7004779252213
At time: 3.67098331451416 and batch: 350, loss is 5.377311563491821 and perplexity is 216.43960839920373
At time: 4.118943214416504 and batch: 400, loss is 5.396176433563232 and perplexity is 220.56147044482827
At time: 4.5686728954315186 and batch: 450, loss is 5.323333053588867 and perplexity is 205.0662409621492
At time: 5.01644229888916 and batch: 500, loss is 5.311746768951416 and perplexity is 202.7039963699197
At time: 5.466234445571899 and batch: 550, loss is 5.311581964492798 and perplexity is 202.67059260015898
At time: 5.915698766708374 and batch: 600, loss is 5.328512344360352 and perplexity is 206.1310938625138
At time: 6.364022493362427 and batch: 650, loss is 5.257417602539062 and perplexity is 191.98506883876783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1870075300628065 and perplexity of 178.93230144091703
Finished 1 epochs...
Completing Train Step...
At time: 7.289194822311401 and batch: 50, loss is 5.087110147476197 and perplexity is 161.92125672705632
At time: 7.7302937507629395 and batch: 100, loss is 5.006392831802368 and perplexity is 149.36497863546597
At time: 8.186944007873535 and batch: 150, loss is 4.930491018295288 and perplexity is 138.44747589652195
At time: 8.627357244491577 and batch: 200, loss is 4.881843566894531 and perplexity is 131.87355764672918
At time: 9.06864309310913 and batch: 250, loss is 4.854519357681275 and perplexity is 128.3190008483031
At time: 9.505515336990356 and batch: 300, loss is 4.888642997741699 and perplexity is 132.77327810971317
At time: 9.94124150276184 and batch: 350, loss is 4.804096345901489 and perplexity is 122.00918708841928
At time: 10.381304264068604 and batch: 400, loss is 4.836995077133179 and perplexity is 126.08989149305054
At time: 10.822546005249023 and batch: 450, loss is 4.775301418304443 and perplexity is 118.54604118286471
At time: 11.259312391281128 and batch: 500, loss is 4.7664165115356445 and perplexity is 117.49743593726977
At time: 11.697925090789795 and batch: 550, loss is 4.775672597885132 and perplexity is 118.59005122003153
At time: 12.137617588043213 and batch: 600, loss is 4.819491682052612 and perplexity is 123.90209311210998
At time: 12.577868938446045 and batch: 650, loss is 4.728554401397705 and perplexity is 113.13190076506174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.847791484757965 and perplexity of 127.45858454822122
Finished 2 epochs...
Completing Train Step...
At time: 13.509559869766235 and batch: 50, loss is 4.707268600463867 and perplexity is 110.7492459654693
At time: 13.949665307998657 and batch: 100, loss is 4.663889617919922 and perplexity is 106.04776628274844
At time: 14.389858722686768 and batch: 150, loss is 4.609008798599243 and perplexity is 100.38459895205531
At time: 14.828481674194336 and batch: 200, loss is 4.565298337936401 and perplexity is 96.09125740806878
At time: 15.26676607131958 and batch: 250, loss is 4.552891941070556 and perplexity is 94.90647576558447
At time: 15.705342292785645 and batch: 300, loss is 4.62231671333313 and perplexity is 101.72943728485616
At time: 16.143824100494385 and batch: 350, loss is 4.563141651153565 and perplexity is 95.88424197723332
At time: 16.5964457988739 and batch: 400, loss is 4.606496829986572 and perplexity is 100.13275243799043
At time: 17.035223960876465 and batch: 450, loss is 4.554518184661865 and perplexity is 95.06094237969985
At time: 17.47464418411255 and batch: 500, loss is 4.541833772659301 and perplexity is 93.86276537156067
At time: 17.912925004959106 and batch: 550, loss is 4.562089233398438 and perplexity is 95.78338477980121
At time: 18.35141634941101 and batch: 600, loss is 4.623256196975708 and perplexity is 101.82505533591848
At time: 18.790887117385864 and batch: 650, loss is 4.530444345474243 and perplexity is 92.79978708754203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.722945867800245 and perplexity of 112.4991726955661
Finished 3 epochs...
Completing Train Step...
At time: 19.687581300735474 and batch: 50, loss is 4.524993314743042 and perplexity is 92.29530880873996
At time: 20.139506816864014 and batch: 100, loss is 4.491690969467163 and perplexity is 89.27227501029725
At time: 20.57397437095642 and batch: 150, loss is 4.439954357147217 and perplexity is 84.77107239194869
At time: 21.00835633277893 and batch: 200, loss is 4.392219276428222 and perplexity is 80.8195810930751
At time: 21.441946983337402 and batch: 250, loss is 4.3847331523895265 and perplexity is 80.21681469130152
At time: 21.87627387046814 and batch: 300, loss is 4.469131164550781 and perplexity is 87.2808573579172
At time: 22.31038236618042 and batch: 350, loss is 4.419605808258057 and perplexity is 83.06353594406775
At time: 22.745347023010254 and batch: 400, loss is 4.467274837493896 and perplexity is 87.11898583058698
At time: 23.17927885055542 and batch: 450, loss is 4.41608341217041 and perplexity is 82.77146796157228
At time: 23.612743139266968 and batch: 500, loss is 4.401040954589844 and perplexity is 81.53569946539707
At time: 24.04953122138977 and batch: 550, loss is 4.426375799179077 and perplexity is 83.62778314741581
At time: 24.490631580352783 and batch: 600, loss is 4.4945799827575685 and perplexity is 89.53055670925379
At time: 24.942007303237915 and batch: 650, loss is 4.400170507431031 and perplexity is 81.46475782740845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.654433007333793 and perplexity of 105.04964073410876
Finished 4 epochs...
Completing Train Step...
At time: 25.882707595825195 and batch: 50, loss is 4.40025634765625 and perplexity is 81.47175108071474
At time: 26.32556176185608 and batch: 100, loss is 4.373572196960449 and perplexity is 79.3264960432459
At time: 26.767613649368286 and batch: 150, loss is 4.3224876022338865 and perplexity is 75.37590052456459
At time: 27.21012234687805 and batch: 200, loss is 4.272551822662353 and perplexity is 71.70437921530008
At time: 27.66862678527832 and batch: 250, loss is 4.265947647094727 and perplexity is 71.23239116744546
At time: 28.111623764038086 and batch: 300, loss is 4.358030471801758 and perplexity is 78.10315647185669
At time: 28.55292820930481 and batch: 350, loss is 4.314814701080322 and perplexity is 74.79976184465093
At time: 28.995627403259277 and batch: 400, loss is 4.365629835128784 and perplexity is 78.69895169979893
At time: 29.437538146972656 and batch: 450, loss is 4.314680299758911 and perplexity is 74.78970933336832
At time: 29.88023853302002 and batch: 500, loss is 4.296531400680542 and perplexity is 73.4446014809584
At time: 30.324795722961426 and batch: 550, loss is 4.324943103790283 and perplexity is 75.5612135907668
At time: 30.76226019859314 and batch: 600, loss is 4.397935295104981 and perplexity is 81.28287015109053
At time: 31.199755668640137 and batch: 650, loss is 4.301392974853516 and perplexity is 73.80252719490461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.6117816999846815 and perplexity of 100.66334183021861
Finished 5 epochs...
Completing Train Step...
At time: 32.10178518295288 and batch: 50, loss is 4.303677358627319 and perplexity is 73.97131320310399
At time: 32.555827140808105 and batch: 100, loss is 4.281537952423096 and perplexity is 72.35162784596706
At time: 32.99538540840149 and batch: 150, loss is 4.2321266841888425 and perplexity is 68.86352753188548
At time: 33.43417239189148 and batch: 200, loss is 4.17975263595581 and perplexity is 65.34968605187673
At time: 33.872392416000366 and batch: 250, loss is 4.173304772377014 and perplexity is 64.92967573107995
At time: 34.31302571296692 and batch: 300, loss is 4.270377531051635 and perplexity is 71.54864235514917
At time: 34.75148892402649 and batch: 350, loss is 4.232175874710083 and perplexity is 68.86691504801539
At time: 35.19167470932007 and batch: 400, loss is 4.285091724395752 and perplexity is 72.6092064499075
At time: 35.63177514076233 and batch: 450, loss is 4.234522738456726 and perplexity is 69.02872611427918
At time: 36.07166624069214 and batch: 500, loss is 4.212250466346741 and perplexity is 67.50829413114049
At time: 36.512036085128784 and batch: 550, loss is 4.244226732254028 and perplexity is 69.70184111409819
At time: 36.95220971107483 and batch: 600, loss is 4.31947361946106 and perplexity is 75.14906087601302
At time: 37.39181041717529 and batch: 650, loss is 4.2207077646255495 and perplexity is 68.0816530270061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.582886340571385 and perplexity of 97.7962605447862
Finished 6 epochs...
Completing Train Step...
At time: 38.3089804649353 and batch: 50, loss is 4.224351954460144 and perplexity is 68.33020811077428
At time: 38.74900722503662 and batch: 100, loss is 4.206183137893677 and perplexity is 67.0999392017939
At time: 39.20512390136719 and batch: 150, loss is 4.15743809223175 and perplexity is 63.907587351621316
At time: 39.64447379112244 and batch: 200, loss is 4.103918843269348 and perplexity is 60.57721566605187
At time: 40.089539527893066 and batch: 250, loss is 4.097031731605529 and perplexity is 60.16144698355226
At time: 40.533568143844604 and batch: 300, loss is 4.197383089065552 and perplexity is 66.51204699442073
At time: 40.97768759727478 and batch: 350, loss is 4.1632539701461795 and perplexity is 64.28034899486444
At time: 41.42061185836792 and batch: 400, loss is 4.218187704086303 and perplexity is 67.91029914153343
At time: 41.86506938934326 and batch: 450, loss is 4.16725480556488 and perplexity is 64.5380392362415
At time: 42.310351610183716 and batch: 500, loss is 4.141669020652771 and perplexity is 62.90772817664745
At time: 42.753764152526855 and batch: 550, loss is 4.176918787956238 and perplexity is 65.16475712877428
At time: 43.1980082988739 and batch: 600, loss is 4.252990522384644 and perplexity is 70.3153779485307
At time: 43.642115354537964 and batch: 650, loss is 4.152720432281495 and perplexity is 63.606803142924775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.564123415479473 and perplexity of 95.97842393009923
Finished 7 epochs...
Completing Train Step...
At time: 44.550992488861084 and batch: 50, loss is 4.1575456762313845 and perplexity is 63.91446315533218
At time: 45.008800745010376 and batch: 100, loss is 4.1419857358932495 and perplexity is 62.92765516832927
At time: 45.4520800113678 and batch: 150, loss is 4.093199319839478 and perplexity is 59.9313247894296
At time: 45.8937132358551 and batch: 200, loss is 4.0396349906921385 and perplexity is 56.80560444649058
At time: 46.33649945259094 and batch: 250, loss is 4.032240481376648 and perplexity is 56.38710408457323
At time: 46.779526710510254 and batch: 300, loss is 4.134484071731567 and perplexity is 62.45735923932082
At time: 47.20338749885559 and batch: 350, loss is 4.1038317346572875 and perplexity is 60.571939098693285
At time: 47.627864599227905 and batch: 400, loss is 4.159778075218201 and perplexity is 64.05730511892813
At time: 48.07438373565674 and batch: 450, loss is 4.108862957954407 and perplexity is 60.87745797220375
At time: 48.51693415641785 and batch: 500, loss is 4.080738468170166 and perplexity is 59.189163027800866
At time: 48.95675230026245 and batch: 550, loss is 4.118122401237488 and perplexity is 61.44376714922281
At time: 49.40171456336975 and batch: 600, loss is 4.195433125495911 and perplexity is 66.38247729499204
At time: 49.84036207199097 and batch: 650, loss is 4.093623595237732 and perplexity is 59.95675757099279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.552531822054994 and perplexity of 94.87230429222924
Finished 8 epochs...
Completing Train Step...
At time: 50.75472903251648 and batch: 50, loss is 4.099726510047913 and perplexity is 60.32378739142115
At time: 51.19067859649658 and batch: 100, loss is 4.085565767288208 and perplexity is 59.47557757042854
At time: 51.63030743598938 and batch: 150, loss is 4.037607140541077 and perplexity is 56.69052791135178
At time: 52.08152222633362 and batch: 200, loss is 3.9837924814224244 and perplexity is 53.720381938061806
At time: 52.52184247970581 and batch: 250, loss is 3.9759064388275145 and perplexity is 53.29840676186257
At time: 52.962451219558716 and batch: 300, loss is 4.079305729866028 and perplexity is 59.104421167691605
At time: 53.402098655700684 and batch: 350, loss is 4.05142219543457 and perplexity is 57.47914552153219
At time: 53.84318709373474 and batch: 400, loss is 4.107429976463318 and perplexity is 60.790284175840554
At time: 54.282196283340454 and batch: 450, loss is 4.056149368286133 and perplexity is 57.751502610112304
At time: 54.722519636154175 and batch: 500, loss is 4.027182126045227 and perplexity is 56.102598248446505
At time: 55.16245627403259 and batch: 550, loss is 4.065751252174377 and perplexity is 58.308696607657076
At time: 55.60082697868347 and batch: 600, loss is 4.144077763557434 and perplexity is 63.05943936381136
At time: 56.04621386528015 and batch: 650, loss is 4.041168665885925 and perplexity is 56.89279263498911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.546058804381127 and perplexity of 94.26017748095833
Finished 9 epochs...
Completing Train Step...
At time: 56.969680070877075 and batch: 50, loss is 4.048113002777099 and perplexity is 57.289250328395326
At time: 57.427440881729126 and batch: 100, loss is 4.034775562286377 and perplexity is 56.53023129857471
At time: 57.87082839012146 and batch: 150, loss is 3.988229274749756 and perplexity is 53.95925769952021
At time: 58.3159441947937 and batch: 200, loss is 3.933700194358826 and perplexity is 51.09569230590044
At time: 58.75972938537598 and batch: 250, loss is 3.9257188844680786 and perplexity is 50.68950486167813
At time: 59.20405840873718 and batch: 300, loss is 4.029919023513794 and perplexity is 56.2563556206761
At time: 59.64629077911377 and batch: 350, loss is 4.00412013053894 and perplexity is 54.82356559034808
At time: 60.089494705200195 and batch: 400, loss is 4.060163202285767 and perplexity is 57.983773391157634
At time: 60.53255796432495 and batch: 450, loss is 4.008308424949646 and perplexity is 55.05366434799308
At time: 60.97606444358826 and batch: 500, loss is 3.979299559593201 and perplexity is 53.4795618593786
At time: 61.41396117210388 and batch: 550, loss is 4.018753657341003 and perplexity is 55.63172640669684
At time: 61.86519455909729 and batch: 600, loss is 4.097457771301269 and perplexity is 60.187083608842656
At time: 62.30523729324341 and batch: 650, loss is 3.994136929512024 and perplexity is 54.27897382162212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.542455935010723 and perplexity of 93.9211814206241
Finished 10 epochs...
Completing Train Step...
At time: 63.21633505821228 and batch: 50, loss is 4.001746363639832 and perplexity is 54.69358156198643
At time: 63.65149259567261 and batch: 100, loss is 3.988615756034851 and perplexity is 53.98011597318527
At time: 64.08623743057251 and batch: 150, loss is 3.943538508415222 and perplexity is 51.600868741182595
At time: 64.52168011665344 and batch: 200, loss is 3.8882169198989867 and perplexity is 48.823752202072086
At time: 64.95737385749817 and batch: 250, loss is 3.880543923377991 and perplexity is 48.45056129849822
At time: 65.39269971847534 and batch: 300, loss is 3.9851132345199587 and perplexity is 53.791380174156636
At time: 65.82827639579773 and batch: 350, loss is 3.9610042715072633 and perplexity is 52.51003380896797
At time: 66.26344037055969 and batch: 400, loss is 4.0171486234664915 and perplexity is 55.54250722037805
At time: 66.69764018058777 and batch: 450, loss is 3.9649532318115233 and perplexity is 52.71780381580761
At time: 67.13217663764954 and batch: 500, loss is 3.935799031257629 and perplexity is 51.20304645029275
At time: 67.56758117675781 and batch: 550, loss is 3.9761378145217896 and perplexity is 53.31074014449832
At time: 68.00139284133911 and batch: 600, loss is 4.055102839469909 and perplexity is 57.69109561279558
At time: 68.43656301498413 and batch: 650, loss is 3.9517241811752317 and perplexity is 52.024990057791086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5412696389591 and perplexity of 93.8098291553761
Finished 11 epochs...
Completing Train Step...
At time: 69.34015202522278 and batch: 50, loss is 3.9587997579574585 and perplexity is 52.394402230455825
At time: 69.79762697219849 and batch: 100, loss is 3.946638321876526 and perplexity is 51.76106997741675
At time: 70.24247741699219 and batch: 150, loss is 3.9026373195648194 and perplexity is 49.532911110486715
At time: 70.68785214424133 and batch: 200, loss is 3.846716284751892 and perplexity is 46.83900447426755
At time: 71.1333110332489 and batch: 250, loss is 3.839061403274536 and perplexity is 46.4818262690488
At time: 71.57731866836548 and batch: 300, loss is 3.944198570251465 and perplexity is 51.63493974860447
At time: 72.01976943016052 and batch: 350, loss is 3.921415343284607 and perplexity is 50.471829213947885
At time: 72.46508431434631 and batch: 400, loss is 3.9775934076309203 and perplexity is 53.38839539400733
At time: 72.91210889816284 and batch: 450, loss is 3.9250349950790406 and perplexity is 50.6548506983256
At time: 73.37140798568726 and batch: 500, loss is 3.8955548763275147 and perplexity is 49.18333646172178
At time: 73.81661009788513 and batch: 550, loss is 3.9369744539260862 and perplexity is 51.263267057184216
At time: 74.25876641273499 and batch: 600, loss is 4.015825867652893 and perplexity is 55.469086615521654
At time: 74.6983687877655 and batch: 650, loss is 3.9126334381103516 and perplexity is 50.03053095170759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.542168112362132 and perplexity of 93.89415266735908
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 75.61819553375244 and batch: 50, loss is 3.9250537967681884 and perplexity is 50.65580310403565
At time: 76.0581386089325 and batch: 100, loss is 3.910222096443176 and perplexity is 49.91003558392905
At time: 76.49971723556519 and batch: 150, loss is 3.859693627357483 and perplexity is 47.45081151114769
At time: 76.93941378593445 and batch: 200, loss is 3.796469988822937 and perplexity is 44.54366699524436
At time: 77.37997674942017 and batch: 250, loss is 3.7793609523773193 and perplexity is 43.78805014320046
At time: 77.82038831710815 and batch: 300, loss is 3.8768595933914183 and perplexity is 48.272381880081866
At time: 78.260915517807 and batch: 350, loss is 3.847151618003845 and perplexity is 46.85939948939561
At time: 78.70084571838379 and batch: 400, loss is 3.890171055793762 and perplexity is 48.91925372985895
At time: 79.1415388584137 and batch: 450, loss is 3.8327741527557375 and perplexity is 46.190500162495745
At time: 79.5818178653717 and batch: 500, loss is 3.790499458312988 and perplexity is 44.278510023993235
At time: 80.02282214164734 and batch: 550, loss is 3.8199534893035887 and perplexity is 45.60208728668622
At time: 80.4650559425354 and batch: 600, loss is 3.8900138902664185 and perplexity is 48.91156591369491
At time: 80.90968084335327 and batch: 650, loss is 3.777024335861206 and perplexity is 43.685853705467075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.510790058210785 and perplexity of 90.99368042359254
Finished 13 epochs...
Completing Train Step...
At time: 81.83390092849731 and batch: 50, loss is 3.8850130367279054 and perplexity is 48.66757692131289
At time: 82.29352045059204 and batch: 100, loss is 3.8702289772033693 and perplexity is 47.95336505122589
At time: 82.73803806304932 and batch: 150, loss is 3.822860989570618 and perplexity is 45.73486830453245
At time: 83.18325877189636 and batch: 200, loss is 3.761664390563965 and perplexity is 43.01996845449443
At time: 83.62714743614197 and batch: 250, loss is 3.747986788749695 and perplexity is 42.435564190996544
At time: 84.07254004478455 and batch: 300, loss is 3.8495181369781495 and perplexity is 46.97042446695431
At time: 84.5157105922699 and batch: 350, loss is 3.823846435546875 and perplexity is 45.77995976042565
At time: 84.97450351715088 and batch: 400, loss is 3.8703247261047364 and perplexity is 47.957956753068096
At time: 85.41938805580139 and batch: 450, loss is 3.8166256046295164 and perplexity is 45.45058103678798
At time: 85.8632698059082 and batch: 500, loss is 3.778122553825378 and perplexity is 43.73385664880935
At time: 86.30651831626892 and batch: 550, loss is 3.8116797733306886 and perplexity is 45.22634510418392
At time: 86.74472689628601 and batch: 600, loss is 3.8870988273620606 and perplexity is 48.769193235780556
At time: 87.18602681159973 and batch: 650, loss is 3.7770500564575196 and perplexity is 43.68697734612515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.511006673177083 and perplexity of 91.01339315156973
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 88.1055474281311 and batch: 50, loss is 3.872240252494812 and perplexity is 48.0499095257076
At time: 88.54585695266724 and batch: 100, loss is 3.8603263807296755 and perplexity is 47.48084567325124
At time: 88.98853754997253 and batch: 150, loss is 3.8123943853378295 and perplexity is 45.258675944061785
At time: 89.42822122573853 and batch: 200, loss is 3.74903329372406 and perplexity is 42.47999646525535
At time: 89.8685986995697 and batch: 250, loss is 3.7314187240600587 and perplexity is 41.738281282821866
At time: 90.31132483482361 and batch: 300, loss is 3.8312556028366087 and perplexity is 46.12041081276645
At time: 90.75279593467712 and batch: 350, loss is 3.8019838237762453 and perplexity is 44.78995178614562
At time: 91.19287705421448 and batch: 400, loss is 3.8454597425460815 and perplexity is 46.78018624979723
At time: 91.63347625732422 and batch: 450, loss is 3.788633289337158 and perplexity is 44.19595589622501
At time: 92.07344365119934 and batch: 500, loss is 3.7457991790771485 and perplexity is 42.342833206892635
At time: 92.51378726959229 and batch: 550, loss is 3.776149001121521 and perplexity is 43.64763069149245
At time: 92.95892906188965 and batch: 600, loss is 3.84709424495697 and perplexity is 46.85671109999345
At time: 93.39953303337097 and batch: 650, loss is 3.7349438667297363 and perplexity is 41.88567431723516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503692926145067 and perplexity of 90.35017248796673
Finished 15 epochs...
Completing Train Step...
At time: 94.30271410942078 and batch: 50, loss is 3.860265622138977 and perplexity is 47.47796089162148
At time: 94.76206660270691 and batch: 100, loss is 3.845924825668335 and perplexity is 46.80194798499352
At time: 95.20639419555664 and batch: 150, loss is 3.798552689552307 and perplexity is 44.63653479733481
At time: 95.65139412879944 and batch: 200, loss is 3.736302042007446 and perplexity is 41.94260105407556
At time: 96.09597873687744 and batch: 250, loss is 3.720047674179077 and perplexity is 41.26636140161921
At time: 96.55557751655579 and batch: 300, loss is 3.821680841445923 and perplexity is 45.680926221554365
At time: 96.9863829612732 and batch: 350, loss is 3.7941772985458373 and perplexity is 44.441659143905554
At time: 97.41426515579224 and batch: 400, loss is 3.8392166090011597 and perplexity is 46.489041074544765
At time: 97.86405158042908 and batch: 450, loss is 3.7841649055480957 and perplexity is 43.99891196516544
At time: 98.30981492996216 and batch: 500, loss is 3.7433249378204345 and perplexity is 42.23819632387356
At time: 98.75389242172241 and batch: 550, loss is 3.7757206678390505 and perplexity is 43.628938961997015
At time: 99.19414377212524 and batch: 600, loss is 3.8489026927948 and perplexity is 46.94152568613212
At time: 99.63339185714722 and batch: 650, loss is 3.737527189254761 and perplexity is 41.99401840678485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503511316636029 and perplexity of 90.33376552737478
Finished 16 epochs...
Completing Train Step...
At time: 100.55303859710693 and batch: 50, loss is 3.853096590042114 and perplexity is 47.13880702115454
At time: 100.99379825592041 and batch: 100, loss is 3.838267560005188 and perplexity is 46.4449416263678
At time: 101.43530511856079 and batch: 150, loss is 3.7911110973358153 and perplexity is 44.30560077263151
At time: 101.87600088119507 and batch: 200, loss is 3.729165802001953 and perplexity is 41.644354033366994
At time: 102.3158597946167 and batch: 250, loss is 3.7134025573730467 and perplexity is 40.993050706419986
At time: 102.75649523735046 and batch: 300, loss is 3.8158200693130495 and perplexity is 45.413983730799096
At time: 103.19784045219421 and batch: 350, loss is 3.7891277551651 and perplexity is 44.21781468991681
At time: 103.63706278800964 and batch: 400, loss is 3.8350114965438844 and perplexity is 46.2939598854325
At time: 104.07799506187439 and batch: 450, loss is 3.7808727264404296 and perplexity is 43.854297844842456
At time: 104.51823735237122 and batch: 500, loss is 3.741008939743042 and perplexity is 42.14048593460708
At time: 104.95872735977173 and batch: 550, loss is 3.774424982070923 and perplexity is 43.572446173060115
At time: 105.39893651008606 and batch: 600, loss is 3.8488058853149414 and perplexity is 46.9369816152833
At time: 105.84295082092285 and batch: 650, loss is 3.7376841831207277 and perplexity is 42.00061172762394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.503778794232537 and perplexity of 90.35793101758111
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 106.75306439399719 and batch: 50, loss is 3.8500027561187746 and perplexity is 46.99319275022162
At time: 107.20759558677673 and batch: 100, loss is 3.8364359092712403 and perplexity is 46.35994857750212
At time: 107.64691972732544 and batch: 150, loss is 3.7897412824630736 and perplexity is 44.244951850128864
At time: 108.10132360458374 and batch: 200, loss is 3.7262645959854126 and perplexity is 41.52371027371263
At time: 108.54067635536194 and batch: 250, loss is 3.708868999481201 and perplexity is 40.807626970087334
At time: 108.98072743415833 and batch: 300, loss is 3.8116403150558473 and perplexity is 45.22456058583597
At time: 109.42036128044128 and batch: 350, loss is 3.7827145528793333 and perplexity is 43.93514427976854
At time: 109.85958623886108 and batch: 400, loss is 3.8280778884887696 and perplexity is 45.9740859340625
At time: 110.29897022247314 and batch: 450, loss is 3.771461124420166 and perplexity is 43.443494836208274
At time: 110.73736381530762 and batch: 500, loss is 3.730158181190491 and perplexity is 41.68570153643728
At time: 111.17650723457336 and batch: 550, loss is 3.762812213897705 and perplexity is 43.06937612831917
At time: 111.61681890487671 and batch: 600, loss is 3.8345697784423827 and perplexity is 46.27351552101624
At time: 112.05152487754822 and batch: 650, loss is 3.7232335567474366 and perplexity is 41.398040829287694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.502689137178309 and perplexity of 90.25952548452081
Finished 18 epochs...
Completing Train Step...
At time: 112.98354363441467 and batch: 50, loss is 3.8469741535186768 and perplexity is 46.851084348032884
At time: 113.42484784126282 and batch: 100, loss is 3.832348594665527 and perplexity is 46.17084760341008
At time: 113.8665783405304 and batch: 150, loss is 3.7855387353897094 and perplexity is 44.05940052440171
At time: 114.30837416648865 and batch: 200, loss is 3.7223359966278076 and perplexity is 41.36090026924472
At time: 114.7502007484436 and batch: 250, loss is 3.7056399297714235 and perplexity is 40.67606881744507
At time: 115.190913438797 and batch: 300, loss is 3.8087316608428954 and perplexity is 45.09320909779434
At time: 115.63166570663452 and batch: 350, loss is 3.7803842639923095 and perplexity is 43.83288189802529
At time: 116.07305431365967 and batch: 400, loss is 3.8262275743484495 and perplexity is 45.889098084128136
At time: 116.51357579231262 and batch: 450, loss is 3.770279130935669 and perplexity is 43.39217524405478
At time: 116.95414853096008 and batch: 500, loss is 3.7299367666244505 and perplexity is 41.67647273665455
At time: 117.39581084251404 and batch: 550, loss is 3.7633761739730835 and perplexity is 43.09367238734308
At time: 117.83564472198486 and batch: 600, loss is 3.8358006572723387 and perplexity is 46.33050767968393
At time: 118.27626872062683 and batch: 650, loss is 3.724540739059448 and perplexity is 41.452191000373936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.502587711109834 and perplexity of 90.25037127995387
Finished 19 epochs...
Completing Train Step...
At time: 119.18049931526184 and batch: 50, loss is 3.8448133993148805 and perplexity is 46.74995996239077
At time: 119.63526439666748 and batch: 100, loss is 3.8298213624954225 and perplexity is 46.05431047224512
At time: 120.07555484771729 and batch: 150, loss is 3.783064560890198 and perplexity is 43.95052462368972
At time: 120.51647973060608 and batch: 200, loss is 3.7198982286453246 and perplexity is 41.26019478901135
At time: 120.96031022071838 and batch: 250, loss is 3.7034924697875975 and perplexity is 40.58881231082096
At time: 121.40426397323608 and batch: 300, loss is 3.80687469959259 and perplexity is 45.00955045531654
At time: 121.84898209571838 and batch: 350, loss is 3.7788525724411013 and perplexity is 43.76579483461705
At time: 122.29627656936646 and batch: 400, loss is 3.825018882751465 and perplexity is 45.833665823882804
At time: 122.74025082588196 and batch: 450, loss is 3.7694171714782714 and perplexity is 43.35478906322694
At time: 123.1863272190094 and batch: 500, loss is 3.7295836925506594 and perplexity is 41.66176045206014
At time: 123.63036751747131 and batch: 550, loss is 3.763503942489624 and perplexity is 43.099178753698936
At time: 124.07649302482605 and batch: 600, loss is 3.8362748575210572 and perplexity is 46.352482827847474
At time: 124.52056217193604 and batch: 650, loss is 3.725046925544739 and perplexity is 41.47317885067898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.502611646465227 and perplexity of 90.25253148051726
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 125.44513130187988 and batch: 50, loss is 3.843937954902649 and perplexity is 46.70905088059591
At time: 125.8883421421051 and batch: 100, loss is 3.8292582607269288 and perplexity is 46.02838450873445
At time: 126.33329176902771 and batch: 150, loss is 3.78299120426178 and perplexity is 43.947300679636406
At time: 126.77222228050232 and batch: 200, loss is 3.7189419317245482 and perplexity is 41.220756652073966
At time: 127.21355938911438 and batch: 250, loss is 3.7019659996032717 and perplexity is 40.52690196318268
At time: 127.65593647956848 and batch: 300, loss is 3.805079984664917 and perplexity is 44.92884358779955
At time: 128.0982940196991 and batch: 350, loss is 3.776840271949768 and perplexity is 43.67781345634209
At time: 128.53853845596313 and batch: 400, loss is 3.8233537006378175 and perplexity is 45.75740793260913
At time: 128.98033499717712 and batch: 450, loss is 3.7663247346878053 and perplexity is 43.22092420946524
At time: 129.42077684402466 and batch: 500, loss is 3.7258172702789305 and perplexity is 41.505139804512474
At time: 129.86115217208862 and batch: 550, loss is 3.7593881464004517 and perplexity is 42.92215586728367
At time: 130.30100798606873 and batch: 600, loss is 3.831406764984131 and perplexity is 46.12738300006165
At time: 130.75720071792603 and batch: 650, loss is 3.7203601694107054 and perplexity is 41.279258957891315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.502221799364277 and perplexity of 90.21735365019926
Finished 21 epochs...
Completing Train Step...
At time: 131.6564919948578 and batch: 50, loss is 3.8429124402999877 and perplexity is 46.66117461993951
At time: 132.1122124195099 and batch: 100, loss is 3.8282272577285767 and perplexity is 45.980953561222655
At time: 132.55217599868774 and batch: 150, loss is 3.7816965675354 and perplexity is 43.89044170395207
At time: 132.9929895401001 and batch: 200, loss is 3.717924995422363 and perplexity is 41.178859075425
At time: 133.43272948265076 and batch: 250, loss is 3.7011642026901246 and perplexity is 40.494420641740945
At time: 133.87312960624695 and batch: 300, loss is 3.804418320655823 and perplexity is 44.89912562176577
At time: 134.31480717658997 and batch: 350, loss is 3.776244044303894 and perplexity is 43.651779298361696
At time: 134.7599160671234 and batch: 400, loss is 3.8228271198272705 and perplexity is 45.733319302513245
At time: 135.20452332496643 and batch: 450, loss is 3.766127042770386 and perplexity is 43.212380626612095
At time: 135.6457507610321 and batch: 500, loss is 3.725873112678528 and perplexity is 41.507457615830255
At time: 136.086608171463 and batch: 550, loss is 3.7597554826736452 and perplexity is 42.937925628282684
At time: 136.5256326198578 and batch: 600, loss is 3.831820912361145 and perplexity is 46.14649049112518
At time: 136.9728856086731 and batch: 650, loss is 3.7206954288482668 and perplexity is 41.29310053916322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5020719042011335 and perplexity of 90.20383151873189
Finished 22 epochs...
Completing Train Step...
At time: 137.91474890708923 and batch: 50, loss is 3.842160773277283 and perplexity is 46.626114132293985
At time: 138.35960364341736 and batch: 100, loss is 3.8274356603622435 and perplexity is 45.94456956212096
At time: 138.80373668670654 and batch: 150, loss is 3.7808773946762084 and perplexity is 43.85450256752255
At time: 139.2481245994568 and batch: 200, loss is 3.717160491943359 and perplexity is 41.147389725147036
At time: 139.70006918907166 and batch: 250, loss is 3.7005472707748415 and perplexity is 40.46944604586091
At time: 140.14479637145996 and batch: 300, loss is 3.80390350818634 and perplexity is 44.876016940855735
At time: 140.58261847496033 and batch: 350, loss is 3.775791254043579 and perplexity is 43.632018671897185
At time: 141.02412247657776 and batch: 400, loss is 3.8224565315246584 and perplexity is 45.716374209360694
At time: 141.46816730499268 and batch: 450, loss is 3.7659382390975953 and perplexity is 43.20422274058342
At time: 141.91038298606873 and batch: 500, loss is 3.7258720207214355 and perplexity is 41.50741229149227
At time: 142.36442828178406 and batch: 550, loss is 3.759959683418274 and perplexity is 42.946694479941335
At time: 142.80408430099487 and batch: 600, loss is 3.832090253829956 and perplexity is 46.15892132864941
At time: 143.24674654006958 and batch: 650, loss is 3.7209196519851684 and perplexity is 41.302360445802364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.502015356924019 and perplexity of 90.19873088188916
Finished 23 epochs...
Completing Train Step...
At time: 144.14781427383423 and batch: 50, loss is 3.841523079872131 and perplexity is 46.596390445110885
At time: 144.6034803390503 and batch: 100, loss is 3.826743469238281 and perplexity is 45.9127781430142
At time: 145.04491138458252 and batch: 150, loss is 3.7802006340026857 and perplexity is 43.82483360535374
At time: 145.48562622070312 and batch: 200, loss is 3.716506824493408 and perplexity is 41.12050180466986
At time: 145.92563366889954 and batch: 250, loss is 3.6999979066848754 and perplexity is 40.447219691202044
At time: 146.36776876449585 and batch: 300, loss is 3.8034395217895507 and perplexity is 44.85519990923629
At time: 146.80856084823608 and batch: 350, loss is 3.775393552780151 and perplexity is 43.61466961304545
At time: 147.2488169670105 and batch: 400, loss is 3.822139263153076 and perplexity is 45.70187215040461
At time: 147.68887495994568 and batch: 450, loss is 3.765745072364807 and perplexity is 43.195877928029994
At time: 148.13270211219788 and batch: 500, loss is 3.725829701423645 and perplexity is 41.50565576411876
At time: 148.57574152946472 and batch: 550, loss is 3.760074806213379 and perplexity is 42.95163890805313
At time: 149.01936650276184 and batch: 600, loss is 3.832277545928955 and perplexity is 46.167567339552065
At time: 149.4652397632599 and batch: 650, loss is 3.7210811138153077 and perplexity is 41.30902973891266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5019980037913605 and perplexity of 90.19716566492727
Finished 24 epochs...
Completing Train Step...
At time: 150.38786840438843 and batch: 50, loss is 3.8409431886672976 and perplexity is 46.56937744117345
At time: 150.82862854003906 and batch: 100, loss is 3.8261127948760985 and perplexity is 45.88383125993081
At time: 151.27000737190247 and batch: 150, loss is 3.7795948457717894 and perplexity is 43.79829307671623
At time: 151.70915460586548 and batch: 200, loss is 3.715915479660034 and perplexity is 41.09619259665259
At time: 152.14793968200684 and batch: 250, loss is 3.699486722946167 and perplexity is 40.42654901392711
At time: 152.58714985847473 and batch: 300, loss is 3.8030044746398928 and perplexity is 44.83569002653559
At time: 153.02915954589844 and batch: 350, loss is 3.775024528503418 and perplexity is 43.59857771047051
At time: 153.46798849105835 and batch: 400, loss is 3.821847553253174 and perplexity is 45.68854240615797
At time: 153.92093706130981 and batch: 450, loss is 3.7655487632751465 and perplexity is 43.18739901682776
At time: 154.35621571540833 and batch: 500, loss is 3.725759358406067 and perplexity is 41.502736233731255
At time: 154.79029989242554 and batch: 550, loss is 3.760135545730591 and perplexity is 42.95424784909597
At time: 155.2255208492279 and batch: 600, loss is 3.832411379814148 and perplexity is 46.17374653794288
At time: 155.6619517803192 and batch: 650, loss is 3.7212002229690553 and perplexity is 41.31395031552404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.502000098134957 and perplexity of 90.1973545689814
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 156.56696009635925 and batch: 50, loss is 3.8406851053237916 and perplexity is 46.55736021132857
At time: 157.02374482154846 and batch: 100, loss is 3.8259168577194216 and perplexity is 45.87484179321033
At time: 157.46602129936218 and batch: 150, loss is 3.7794731998443605 and perplexity is 43.79296551677966
At time: 157.90859484672546 and batch: 200, loss is 3.7156595468521116 and perplexity is 41.08567607850482
At time: 158.34859466552734 and batch: 250, loss is 3.6990757846832274 and perplexity is 40.409939611051996
At time: 158.78967666625977 and batch: 300, loss is 3.8024127531051635 and perplexity is 44.80916763093262
At time: 159.23253870010376 and batch: 350, loss is 3.774434199333191 and perplexity is 43.57284779357507
At time: 159.6735475063324 and batch: 400, loss is 3.8213306283950805 and perplexity is 45.66493096605411
At time: 160.11372828483582 and batch: 450, loss is 3.7646603393554687 and perplexity is 43.14904733730984
At time: 160.55428099632263 and batch: 500, loss is 3.724603686332703 and perplexity is 41.45480038489381
At time: 160.99766564369202 and batch: 550, loss is 3.758729648590088 and perplexity is 42.89390102552568
At time: 161.43902134895325 and batch: 600, loss is 3.830907063484192 and perplexity is 46.10433883566952
At time: 161.8794026374817 and batch: 650, loss is 3.719845509529114 and perplexity is 41.258019645343715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501924103381587 and perplexity of 90.19050030371369
Finished 26 epochs...
Completing Train Step...
At time: 162.80859756469727 and batch: 50, loss is 3.840475778579712 and perplexity is 46.54761553064931
At time: 163.25188374519348 and batch: 100, loss is 3.8256989240646364 and perplexity is 45.8648452106111
At time: 163.6952784061432 and batch: 150, loss is 3.779227089881897 and perplexity is 43.78218895784382
At time: 164.14101719856262 and batch: 200, loss is 3.7154494285583497 and perplexity is 41.077044133245714
At time: 164.58494114875793 and batch: 250, loss is 3.6988981485366823 and perplexity is 40.40276198261937
At time: 165.02851271629333 and batch: 300, loss is 3.802272319793701 and perplexity is 44.80287537296999
At time: 165.48743653297424 and batch: 350, loss is 3.774304790496826 and perplexity is 43.567209446878806
At time: 165.93439269065857 and batch: 400, loss is 3.821225199699402 and perplexity is 45.66011682572284
At time: 166.37862968444824 and batch: 450, loss is 3.7646128797531127 and perplexity is 43.14699954927515
At time: 166.82156014442444 and batch: 500, loss is 3.7246088552474976 and perplexity is 41.45501466177862
At time: 167.26782155036926 and batch: 550, loss is 3.7588068532943724 and perplexity is 42.89721276430922
At time: 167.71412420272827 and batch: 600, loss is 3.8309860706329344 and perplexity is 46.107981551924
At time: 168.15826869010925 and batch: 650, loss is 3.719913291931152 and perplexity is 41.2608163077998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5018768310546875 and perplexity of 90.18623688967168
Finished 27 epochs...
Completing Train Step...
At time: 169.06679606437683 and batch: 50, loss is 3.8402821826934814 and perplexity is 46.53860497600004
At time: 169.5199625492096 and batch: 100, loss is 3.82550163269043 and perplexity is 45.855797364832036
At time: 169.9621021747589 and batch: 150, loss is 3.7790144634246827 and perplexity is 43.77288069574309
At time: 170.4046561717987 and batch: 200, loss is 3.715256814956665 and perplexity is 41.069132897758884
At time: 170.84782218933105 and batch: 250, loss is 3.6987370824813843 and perplexity is 40.396254993165314
At time: 171.28882765769958 and batch: 300, loss is 3.802144947052002 and perplexity is 44.79716907131908
At time: 171.72987866401672 and batch: 350, loss is 3.7741900205612184 and perplexity is 43.56220952798166
At time: 172.17441058158875 and batch: 400, loss is 3.821131176948547 and perplexity is 45.65582393775226
At time: 172.61485958099365 and batch: 450, loss is 3.7645678615570066 and perplexity is 43.14505719290907
At time: 173.05544543266296 and batch: 500, loss is 3.7246096420288084 and perplexity is 41.455047277822224
At time: 173.49508094787598 and batch: 550, loss is 3.75886607170105 and perplexity is 42.8997531441179
At time: 173.93903756141663 and batch: 600, loss is 3.8310515451431275 and perplexity is 46.111000548264684
At time: 174.38016867637634 and batch: 650, loss is 3.71996883392334 and perplexity is 41.26310807938102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5018454159007355 and perplexity of 90.1834037196579
Finished 28 epochs...
Completing Train Step...
At time: 175.3107945919037 and batch: 50, loss is 3.8401008319854735 and perplexity is 46.53016593227434
At time: 175.75244736671448 and batch: 100, loss is 3.8253154802322387 and perplexity is 45.847261989895415
At time: 176.1930115222931 and batch: 150, loss is 3.7788212728500366 and perplexity is 43.76442500457377
At time: 176.6322157382965 and batch: 200, loss is 3.715077004432678 and perplexity is 41.06174889933306
At time: 177.0885579586029 and batch: 250, loss is 3.698586583137512 and perplexity is 40.390175840759646
At time: 177.52792692184448 and batch: 300, loss is 3.8020243310928343 and perplexity is 44.79176614364976
At time: 177.96784162521362 and batch: 350, loss is 3.774082622528076 and perplexity is 43.55753128358064
At time: 178.40858626365662 and batch: 400, loss is 3.8210439252853394 and perplexity is 45.651840564959045
At time: 178.85402965545654 and batch: 450, loss is 3.7645226764678954 and perplexity is 43.143107723698904
At time: 179.30489253997803 and batch: 500, loss is 3.72460618019104 and perplexity is 41.45490376742227
At time: 179.75941061973572 and batch: 550, loss is 3.7589131116867067 and perplexity is 42.90177119535466
At time: 180.20284032821655 and batch: 600, loss is 3.831107578277588 and perplexity is 46.11358436454749
At time: 180.66043257713318 and batch: 650, loss is 3.7200158405303956 and perplexity is 41.26504776367703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501824173272825 and perplexity of 90.18148800751639
Finished 29 epochs...
Completing Train Step...
At time: 181.5618462562561 and batch: 50, loss is 3.839929013252258 and perplexity is 46.52217186489249
At time: 182.02096724510193 and batch: 100, loss is 3.825136842727661 and perplexity is 45.83907268090247
At time: 182.45922565460205 and batch: 150, loss is 3.7786415004730225 and perplexity is 43.75655807701137
At time: 182.89775252342224 and batch: 200, loss is 3.714906620979309 and perplexity is 41.05475325274235
At time: 183.34331560134888 and batch: 250, loss is 3.6984435987472533 and perplexity is 40.38440108895413
At time: 183.787451505661 and batch: 300, loss is 3.8019081115722657 and perplexity is 44.78656076855208
At time: 184.2319300174713 and batch: 350, loss is 3.7739801263809203 and perplexity is 43.55306703323259
At time: 184.67704892158508 and batch: 400, loss is 3.820961203575134 and perplexity is 45.648064322824254
At time: 185.12114930152893 and batch: 450, loss is 3.7644767904281617 and perplexity is 43.141128102762494
At time: 185.55959391593933 and batch: 500, loss is 3.724598832130432 and perplexity is 41.454599155396046
At time: 185.99812602996826 and batch: 550, loss is 3.758950796127319 and perplexity is 42.9033879550666
At time: 186.4406921863556 and batch: 600, loss is 3.8311563682556153 and perplexity is 46.11583430020212
At time: 186.88257336616516 and batch: 650, loss is 3.7200565433502195 and perplexity is 41.26672740166396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501810410443475 and perplexity of 90.18024686362727
Finished 30 epochs...
Completing Train Step...
At time: 187.8004322052002 and batch: 50, loss is 3.839764404296875 and perplexity is 46.51451452902997
At time: 188.2435598373413 and batch: 100, loss is 3.8249642038345337 and perplexity is 45.8311597571917
At time: 188.69871377944946 and batch: 150, loss is 3.7784710454940797 and perplexity is 43.74910018946083
At time: 189.1391942501068 and batch: 200, loss is 3.714743628501892 and perplexity is 41.048062182111856
At time: 189.58103609085083 and batch: 250, loss is 3.6983057975769045 and perplexity is 40.378836454635604
At time: 190.02276992797852 and batch: 300, loss is 3.801794619560242 and perplexity is 44.78147814008315
At time: 190.46309447288513 and batch: 350, loss is 3.7738810443878172 and perplexity is 43.54875192232361
At time: 190.90362167358398 and batch: 400, loss is 3.82088191986084 and perplexity is 45.644445318200354
At time: 191.34715962409973 and batch: 450, loss is 3.7644301748275755 and perplexity is 43.139117100038426
At time: 191.7868320941925 and batch: 500, loss is 3.7245880603790282 and perplexity is 41.4541526191644
At time: 192.2269766330719 and batch: 550, loss is 3.758981237411499 and perplexity is 42.904694009170484
At time: 192.66650485992432 and batch: 600, loss is 3.8311989974975584 and perplexity is 46.117800225162554
At time: 193.11059069633484 and batch: 650, loss is 3.7200922107696535 and perplexity is 41.26819930558821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501802631452972 and perplexity of 90.17954535507194
Finished 31 epochs...
Completing Train Step...
At time: 194.00810194015503 and batch: 50, loss is 3.839605631828308 and perplexity is 46.50712989098813
At time: 194.46052956581116 and batch: 100, loss is 3.8247966146469117 and perplexity is 45.82347959393435
At time: 194.89720487594604 and batch: 150, loss is 3.7783080244064333 and perplexity is 43.74196874486812
At time: 195.33739638328552 and batch: 200, loss is 3.7145864725112916 and perplexity is 41.04161174011347
At time: 195.77662205696106 and batch: 250, loss is 3.698171911239624 and perplexity is 40.373430642009346
At time: 196.22040700912476 and batch: 300, loss is 3.8016832208633424 and perplexity is 44.77648981962447
At time: 196.66029596328735 and batch: 350, loss is 3.7737846994400024 and perplexity is 43.544556422203115
At time: 197.100093126297 and batch: 400, loss is 3.8208048486709596 and perplexity is 45.64092758204805
At time: 197.53949284553528 and batch: 450, loss is 3.764382801055908 and perplexity is 43.13707348576224
At time: 197.98086714744568 and batch: 500, loss is 3.7245743608474733 and perplexity is 41.453584720582484
At time: 198.420565366745 and batch: 550, loss is 3.7590058279037475 and perplexity is 42.90574906968813
At time: 198.86060738563538 and batch: 600, loss is 3.831236834526062 and perplexity is 46.11954521869668
At time: 199.30123472213745 and batch: 650, loss is 3.720124087333679 and perplexity is 41.269514814952444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5017972459980085 and perplexity of 90.17905969849951
Finished 32 epochs...
Completing Train Step...
At time: 200.2271707057953 and batch: 50, loss is 3.8394516372680663 and perplexity is 46.49996859738674
At time: 200.67082619667053 and batch: 100, loss is 3.8246332693099974 and perplexity is 45.815995153512354
At time: 201.1134157180786 and batch: 150, loss is 3.778150758743286 and perplexity is 43.73509017604161
At time: 201.5511498451233 and batch: 200, loss is 3.7144339275360108 and perplexity is 41.035351525959335
At time: 201.99163627624512 and batch: 250, loss is 3.698040957450867 and perplexity is 40.368143934466396
At time: 202.43270349502563 and batch: 300, loss is 3.801573567390442 and perplexity is 44.771580191195284
At time: 202.87636995315552 and batch: 350, loss is 3.7736901521682737 and perplexity is 43.54043959781509
At time: 203.31651186943054 and batch: 400, loss is 3.820729579925537 and perplexity is 45.63749237597248
At time: 203.7572946548462 and batch: 450, loss is 3.764334511756897 and perplexity is 43.134990477016146
At time: 204.19956493377686 and batch: 500, loss is 3.7245582151412964 and perplexity is 41.45291542858672
At time: 204.64216566085815 and batch: 550, loss is 3.7590253829956053 and perplexity is 42.90658810375608
At time: 205.0828821659088 and batch: 600, loss is 3.83127076625824 and perplexity is 46.121110161303655
At time: 205.52288913726807 and batch: 650, loss is 3.720152678489685 and perplexity is 41.27069477495694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501795151654412 and perplexity of 90.17887083276108
Finished 33 epochs...
Completing Train Step...
At time: 206.42814135551453 and batch: 50, loss is 3.8393015003204347 and perplexity is 46.49298775809063
At time: 206.88249492645264 and batch: 100, loss is 3.824473624229431 and perplexity is 45.808681439089646
At time: 207.32333302497864 and batch: 150, loss is 3.7779978275299073 and perplexity is 43.728402227044874
At time: 207.7666413784027 and batch: 200, loss is 3.7142853498458863 and perplexity is 41.029255041128195
At time: 208.20706868171692 and batch: 250, loss is 3.697912588119507 and perplexity is 40.362962235414024
At time: 208.64738082885742 and batch: 300, loss is 3.801465401649475 and perplexity is 44.76673770195007
At time: 209.09058260917664 and batch: 350, loss is 3.7735972738265993 and perplexity is 43.53639582178198
At time: 209.53602576255798 and batch: 400, loss is 3.8206557607650757 and perplexity is 45.63412357894214
At time: 209.9793610572815 and batch: 450, loss is 3.7642858266830443 and perplexity is 43.13289049793836
At time: 210.4226496219635 and batch: 500, loss is 3.7245397663116453 and perplexity is 41.45215067786584
At time: 210.86716532707214 and batch: 550, loss is 3.7590406942367554 and perplexity is 42.90724506190287
At time: 211.32710528373718 and batch: 600, loss is 3.831301140785217 and perplexity is 46.122511089484625
At time: 211.7705409526825 and batch: 650, loss is 3.720178484916687 and perplexity is 41.27175983787164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.50179485246247 and perplexity of 90.1788438519736
Finished 34 epochs...
Completing Train Step...
At time: 212.69531273841858 and batch: 50, loss is 3.839154682159424 and perplexity is 46.48616224419504
At time: 213.14105796813965 and batch: 100, loss is 3.824317274093628 and perplexity is 45.801519805401476
At time: 213.58673858642578 and batch: 150, loss is 3.7778488969802857 and perplexity is 43.72189021699778
At time: 214.034245967865 and batch: 200, loss is 3.7141401672363283 and perplexity is 41.02329873919929
At time: 214.47927379608154 and batch: 250, loss is 3.6977862691879273 and perplexity is 40.357863951160745
At time: 214.92440605163574 and batch: 300, loss is 3.801358313560486 and perplexity is 44.76194397423946
At time: 215.36331915855408 and batch: 350, loss is 3.7735056495666504 and perplexity is 43.53240701447251
At time: 215.80239915847778 and batch: 400, loss is 3.820582890510559 and perplexity is 45.63079832989963
At time: 216.24466133117676 and batch: 450, loss is 3.76423641204834 and perplexity is 43.13075915457087
At time: 216.68482875823975 and batch: 500, loss is 3.724519577026367 and perplexity is 41.45131379701844
At time: 217.12751650810242 and batch: 550, loss is 3.7590526723861695 and perplexity is 42.90775901437327
At time: 217.57022309303284 and batch: 600, loss is 3.831328511238098 and perplexity is 46.12377350077746
At time: 218.0109143257141 and batch: 650, loss is 3.7202019786834715 and perplexity is 41.272729478362265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5017972459980085 and perplexity of 90.17905969849951
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 218.913822889328 and batch: 50, loss is 3.839087505340576 and perplexity is 46.483039556582334
At time: 219.36891102790833 and batch: 100, loss is 3.824264507293701 and perplexity is 45.79910306953183
At time: 219.80957627296448 and batch: 150, loss is 3.7777983570098876 and perplexity is 43.71968056979868
At time: 220.24957466125488 and batch: 200, loss is 3.7140716075897218 and perplexity is 41.02048629274637
At time: 220.693359375 and batch: 250, loss is 3.697667484283447 and perplexity is 40.35307033085679
At time: 221.13413977622986 and batch: 300, loss is 3.8011875581741332 and perplexity is 44.75430128373608
At time: 221.5749294757843 and batch: 350, loss is 3.773345980644226 and perplexity is 43.52545679683563
At time: 222.01811480522156 and batch: 400, loss is 3.8204218339920044 and perplexity is 45.623449784163235
At time: 222.45814728736877 and batch: 450, loss is 3.763989181518555 and perplexity is 43.120097232165726
At time: 222.92146110534668 and batch: 500, loss is 3.7241887187957765 and perplexity is 41.437601557208694
At time: 223.402658700943 and batch: 550, loss is 3.758643054962158 and perplexity is 42.890186847834194
At time: 223.86516976356506 and batch: 600, loss is 3.830908317565918 and perplexity is 46.1043966543146
At time: 224.32642364501953 and batch: 650, loss is 3.7198260498046873 and perplexity is 41.257216783462795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501788868623621 and perplexity of 90.17830423791894
Finished 36 epochs...
Completing Train Step...
At time: 225.2710781097412 and batch: 50, loss is 3.83904363155365 and perplexity is 46.4810002143463
At time: 225.71512627601624 and batch: 100, loss is 3.8242183256149294 and perplexity is 45.79698803890404
At time: 226.1596486568451 and batch: 150, loss is 3.7777527379989624 and perplexity is 43.717686166704816
At time: 226.60342478752136 and batch: 200, loss is 3.7140285062789915 and perplexity is 41.01871829412216
At time: 227.0471773147583 and batch: 250, loss is 3.697630705833435 and perplexity is 40.351586234768334
At time: 227.4926302433014 and batch: 300, loss is 3.8011574077606203 and perplexity is 44.752951943387586
At time: 227.93693494796753 and batch: 350, loss is 3.7733178520202637 and perplexity is 43.52423250284753
At time: 228.3824908733368 and batch: 400, loss is 3.8204010677337648 and perplexity is 45.62250236566044
At time: 228.82638597488403 and batch: 450, loss is 3.7639769172668456 and perplexity is 43.11956839968242
At time: 229.2644600868225 and batch: 500, loss is 3.7241868782043457 and perplexity is 41.43752528758455
At time: 229.70516085624695 and batch: 550, loss is 3.758652939796448 and perplexity is 42.890610812319245
At time: 230.1487419605255 and batch: 600, loss is 3.8309195852279663 and perplexity is 46.10491614600176
At time: 230.59255647659302 and batch: 650, loss is 3.7198358535766602 and perplexity is 41.25762126179108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501783183976715 and perplexity of 90.17779160755781
Finished 37 epochs...
Completing Train Step...
At time: 231.50066447257996 and batch: 50, loss is 3.8390002393722535 and perplexity is 46.47898334611199
At time: 231.95773720741272 and batch: 100, loss is 3.8241728782653808 and perplexity is 45.79490673447561
At time: 232.39981818199158 and batch: 150, loss is 3.7777077960968017 and perplexity is 43.7157214548797
At time: 232.84189248085022 and batch: 200, loss is 3.7139859437942504 and perplexity is 41.016972472704175
At time: 233.28345608711243 and batch: 250, loss is 3.6975943088531493 and perplexity is 40.35011758560701
At time: 233.7265317440033 and batch: 300, loss is 3.8011276626586916 and perplexity is 44.75162078206829
At time: 234.1689350605011 and batch: 350, loss is 3.7732907009124754 and perplexity is 43.52305078776195
At time: 234.6249475479126 and batch: 400, loss is 3.820380616188049 and perplexity is 45.62156932450875
At time: 235.06769061088562 and batch: 450, loss is 3.763964796066284 and perplexity is 43.11904574191335
At time: 235.51004242897034 and batch: 500, loss is 3.724184308052063 and perplexity is 41.43741878697121
At time: 235.9512495994568 and batch: 550, loss is 3.7586616897583007 and perplexity is 42.890986105169596
At time: 236.39332556724548 and batch: 600, loss is 3.8309303188323973 and perplexity is 46.10541102058989
At time: 236.83962965011597 and batch: 650, loss is 3.719845290184021 and perplexity is 41.25801059560057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.50177749932981 and perplexity of 90.17727898011088
Finished 38 epochs...
Completing Train Step...
At time: 237.7615909576416 and batch: 50, loss is 3.838957405090332 and perplexity is 46.476992494874565
At time: 238.20230340957642 and batch: 100, loss is 3.8241282320022583 and perplexity is 45.79286220866043
At time: 238.64514756202698 and batch: 150, loss is 3.7776640701293944 and perplexity is 43.713809984458926
At time: 239.08631491661072 and batch: 200, loss is 3.7139438152313233 and perplexity is 41.01524452299656
At time: 239.52644658088684 and batch: 250, loss is 3.6975583934783938 and perplexity is 40.34866842203627
At time: 239.97082662582397 and batch: 300, loss is 3.8010983276367187 and perplexity is 44.75030801154449
At time: 240.4115309715271 and batch: 350, loss is 3.773264112472534 and perplexity is 43.52189359312409
At time: 240.8530523777008 and batch: 400, loss is 3.8203604412078858 and perplexity is 45.62064891953721
At time: 241.29382133483887 and batch: 450, loss is 3.7639526748657226 and perplexity is 43.11852309047948
At time: 241.73617887496948 and batch: 500, loss is 3.724181489944458 and perplexity is 41.437302012030734
At time: 242.17729806900024 and batch: 550, loss is 3.7586696195602416 and perplexity is 42.89132622354299
At time: 242.62257313728333 and batch: 600, loss is 3.8309405755996706 and perplexity is 46.105883915485954
At time: 243.0628218650818 and batch: 650, loss is 3.719854307174683 and perplexity is 41.25838262037409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501772712258732 and perplexity of 90.17684729610002
Finished 39 epochs...
Completing Train Step...
At time: 243.98922061920166 and batch: 50, loss is 3.838914866447449 and perplexity is 46.47501546873884
At time: 244.44807076454163 and batch: 100, loss is 3.8240842533111574 and perplexity is 45.79084834280265
At time: 244.88989734649658 and batch: 150, loss is 3.7776210117340088 and perplexity is 43.71192777846748
At time: 245.3304135799408 and batch: 200, loss is 3.71390230178833 and perplexity is 41.01354187432284
At time: 245.77674341201782 and batch: 250, loss is 3.6975228071212767 and perplexity is 40.34723258546085
At time: 246.23724126815796 and batch: 300, loss is 3.8010691452026366 and perplexity is 44.749002107685605
At time: 246.67922496795654 and batch: 350, loss is 3.773238091468811 and perplexity is 43.520761124502904
At time: 247.12186002731323 and batch: 400, loss is 3.820340356826782 and perplexity is 45.61973266623934
At time: 247.5652859210968 and batch: 450, loss is 3.763940567970276 and perplexity is 43.11800106218868
At time: 248.00852608680725 and batch: 500, loss is 3.724178261756897 and perplexity is 41.43716824486372
At time: 248.45034623146057 and batch: 550, loss is 3.7586766386032107 and perplexity is 42.89162728066132
At time: 248.89320158958435 and batch: 600, loss is 3.830950298309326 and perplexity is 46.1063321917879
At time: 249.33737635612488 and batch: 650, loss is 3.7198628759384156 and perplexity is 41.25873615522144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501768523571538 and perplexity of 90.17646957428568
Finished 40 epochs...
Completing Train Step...
At time: 250.25792527198792 and batch: 50, loss is 3.838873019218445 and perplexity is 46.4730706588163
At time: 250.69977450370789 and batch: 100, loss is 3.824040689468384 and perplexity is 45.78885356093547
At time: 251.1410481929779 and batch: 150, loss is 3.7775783014297484 and perplexity is 43.71006086860068
At time: 251.58376288414001 and batch: 200, loss is 3.7138611459732056 and perplexity is 41.01185396330978
At time: 252.0261447429657 and batch: 250, loss is 3.69748761177063 and perplexity is 40.345812575451404
At time: 252.4686906337738 and batch: 300, loss is 3.801040391921997 and perplexity is 44.747715445567636
At time: 252.91191935539246 and batch: 350, loss is 3.7732123231887815 and perplexity is 43.51963968379202
At time: 253.35468244552612 and batch: 400, loss is 3.820320372581482 and perplexity is 45.61882099942071
At time: 253.7960810661316 and batch: 450, loss is 3.763928241729736 and perplexity is 43.11746958261158
At time: 254.23731970787048 and batch: 500, loss is 3.7241745948791505 and perplexity is 41.43701630011219
At time: 254.67899537086487 and batch: 550, loss is 3.7586831712722777 and perplexity is 42.89190747838331
At time: 255.12209677696228 and batch: 600, loss is 3.8309597110748292 and perplexity is 46.10676618192355
At time: 255.56508016586304 and batch: 650, loss is 3.719871153831482 and perplexity is 41.259077692041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501765531652114 and perplexity of 90.17619977395836
Finished 41 epochs...
Completing Train Step...
At time: 256.46926403045654 and batch: 50, loss is 3.8388316345214846 and perplexity is 46.47114742466677
At time: 256.9276237487793 and batch: 100, loss is 3.823997588157654 and perplexity is 45.78688004386107
At time: 257.3703451156616 and batch: 150, loss is 3.777536087036133 and perplexity is 43.70821571383254
At time: 257.8287854194641 and batch: 200, loss is 3.713820390701294 and perplexity is 41.01018254810963
At time: 258.27226066589355 and batch: 250, loss is 3.697452630996704 and perplexity is 40.34440127238722
At time: 258.7149991989136 and batch: 300, loss is 3.8010114336013796 and perplexity is 44.746419645639044
At time: 259.15788412094116 and batch: 350, loss is 3.7731869411468506 and perplexity is 43.518535080491354
At time: 259.60430788993835 and batch: 400, loss is 3.8203005504608156 and perplexity is 45.61791674660835
At time: 260.05068039894104 and batch: 450, loss is 3.7639159774780273 and perplexity is 43.11694078235424
At time: 260.4970235824585 and batch: 500, loss is 3.724170780181885 and perplexity is 41.43685823074091
At time: 260.9417917728424 and batch: 550, loss is 3.7586890029907227 and perplexity is 42.89215761264065
At time: 261.3878607749939 and batch: 600, loss is 3.8309687089920046 and perplexity is 46.107181048653345
At time: 261.83515453338623 and batch: 650, loss is 3.7198790740966796 and perplexity is 41.25940447617223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501762838924632 and perplexity of 90.17595695435395
Finished 42 epochs...
Completing Train Step...
At time: 262.76251745224 and batch: 50, loss is 3.8387906694412233 and perplexity is 46.46924376937464
At time: 263.2086749076843 and batch: 100, loss is 3.823954825401306 and perplexity is 45.78492211252939
At time: 263.6534113883972 and batch: 150, loss is 3.777494554519653 and perplexity is 43.70640043933982
At time: 264.0979266166687 and batch: 200, loss is 3.7137800884246825 and perplexity is 41.00852977769412
At time: 264.5421097278595 and batch: 250, loss is 3.697418155670166 and perplexity is 40.343010409954736
At time: 264.98952198028564 and batch: 300, loss is 3.8009829235076906 and perplexity is 44.74514393920804
At time: 265.43584418296814 and batch: 350, loss is 3.7731618547439574 and perplexity is 43.517443370680596
At time: 265.88246417045593 and batch: 400, loss is 3.8202807998657224 and perplexity is 45.61701577450309
At time: 266.3294622898102 and batch: 450, loss is 3.7639034843444823 and perplexity is 43.11640212001979
At time: 266.7743649482727 and batch: 500, loss is 3.7241667032241823 and perplexity is 41.43668929476695
At time: 267.21632647514343 and batch: 550, loss is 3.758694438934326 and perplexity is 42.892390772624175
At time: 267.6569654941559 and batch: 600, loss is 3.8309774255752562 and perplexity is 46.10758294748704
At time: 268.1008069515228 and batch: 650, loss is 3.7198866844177245 and perplexity is 41.25971847468123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501761342964921 and perplexity of 90.17582205485628
Finished 43 epochs...
Completing Train Step...
At time: 269.0040054321289 and batch: 50, loss is 3.8387499237060547 and perplexity is 46.46735038444848
At time: 269.460631608963 and batch: 100, loss is 3.8239123487472533 and perplexity is 45.782977363535494
At time: 269.9026036262512 and batch: 150, loss is 3.777453455924988 and perplexity is 43.7046042046155
At time: 270.34490966796875 and batch: 200, loss is 3.7137402057647706 and perplexity is 41.006894281061705
At time: 270.7865104675293 and batch: 250, loss is 3.6973835468292235 and perplexity is 40.3416142092849
At time: 271.22818517684937 and batch: 300, loss is 3.8009544944763185 and perplexity is 44.74387189618881
At time: 271.6714632511139 and batch: 350, loss is 3.773137049674988 and perplexity is 43.51636393088425
At time: 272.1134912967682 and batch: 400, loss is 3.820261163711548 and perplexity is 45.61612004054277
At time: 272.5551824569702 and batch: 450, loss is 3.7638910150527956 and perplexity is 43.1158644923772
At time: 272.99799942970276 and batch: 500, loss is 3.7241623115539553 and perplexity is 41.436507318891856
At time: 273.441002368927 and batch: 550, loss is 3.758699474334717 and perplexity is 42.892606753529215
At time: 273.8845422267914 and batch: 600, loss is 3.830985770225525 and perplexity is 46.107967700746784
At time: 274.3263370990753 and batch: 650, loss is 3.7198939657211305 and perplexity is 41.26001890030363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501759547813266 and perplexity of 90.1756601757254
Finished 44 epochs...
Completing Train Step...
At time: 275.24549627304077 and batch: 50, loss is 3.838709774017334 and perplexity is 46.46548477224699
At time: 275.68648648262024 and batch: 100, loss is 3.823870301246643 and perplexity is 45.78105234423827
At time: 276.1286211013794 and batch: 150, loss is 3.777412748336792 and perplexity is 43.7028251317964
At time: 276.57141041755676 and batch: 200, loss is 3.7137006759643554 and perplexity is 41.005273318753495
At time: 277.01469230651855 and batch: 250, loss is 3.6973494863510132 and perplexity is 40.340240178013374
At time: 277.45623445510864 and batch: 300, loss is 3.8009261465072632 and perplexity is 44.74260351627096
At time: 277.8972816467285 and batch: 350, loss is 3.773112096786499 and perplexity is 43.51527808545519
At time: 278.3407166004181 and batch: 400, loss is 3.820241651535034 and perplexity is 45.61522997944021
At time: 278.7849931716919 and batch: 450, loss is 3.763878331184387 and perplexity is 43.1153176198939
At time: 279.231472492218 and batch: 500, loss is 3.7241578197479246 and perplexity is 41.436321194556406
At time: 279.6775641441345 and batch: 550, loss is 3.758703989982605 and perplexity is 42.89280044187563
At time: 280.1233503818512 and batch: 600, loss is 3.830993800163269 and perplexity is 46.10833794634345
At time: 280.5824043750763 and batch: 650, loss is 3.719900965690613 and perplexity is 41.26030772018763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501757453469669 and perplexity of 90.17547131710673
Finished 45 epochs...
Completing Train Step...
At time: 281.4903938770294 and batch: 50, loss is 3.838669805526733 and perplexity is 46.46362765406896
At time: 281.9458749294281 and batch: 100, loss is 3.8238284826278686 and perplexity is 45.77913788389351
At time: 282.3870220184326 and batch: 150, loss is 3.7773723506927492 and perplexity is 43.70105967628343
At time: 282.8275263309479 and batch: 200, loss is 3.7136614418029783 and perplexity is 41.00366454280249
At time: 283.2687883377075 and batch: 250, loss is 3.6973155975341796 and perplexity is 40.33887311816711
At time: 283.71038150787354 and batch: 300, loss is 3.8008980083465578 and perplexity is 44.74134455941529
At time: 284.1506316661835 and batch: 350, loss is 3.7730876636505126 and perplexity is 43.51421488373697
At time: 284.58565497398376 and batch: 400, loss is 3.820222201347351 and perplexity is 45.61434276328419
At time: 285.0215997695923 and batch: 450, loss is 3.7638658237457276 and perplexity is 43.114778361075864
At time: 285.4591200351715 and batch: 500, loss is 3.724153118133545 and perplexity is 41.43612637741082
At time: 285.89686274528503 and batch: 550, loss is 3.75870831489563 and perplexity is 42.89298594990809
At time: 286.335036277771 and batch: 600, loss is 3.831001486778259 and perplexity is 46.10869236474722
At time: 286.77315497398376 and batch: 650, loss is 3.7199077463150023 and perplexity is 41.260587491784996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501757154277727 and perplexity of 90.17544433733634
Finished 46 epochs...
Completing Train Step...
At time: 287.69036388397217 and batch: 50, loss is 3.838630323410034 and perplexity is 46.46179320791381
At time: 288.13418769836426 and batch: 100, loss is 3.82378698348999 and perplexity is 45.77723812855788
At time: 288.5772681236267 and batch: 150, loss is 3.7773322439193726 and perplexity is 43.69930700293394
At time: 289.02023124694824 and batch: 200, loss is 3.713622555732727 and perplexity is 41.002070102423474
At time: 289.4625508785248 and batch: 250, loss is 3.6972820138931275 and perplexity is 40.33751841467992
At time: 289.904256105423 and batch: 300, loss is 3.8008699369430543 and perplexity is 44.740088624706885
At time: 290.34678053855896 and batch: 350, loss is 3.7730631923675535 and perplexity is 43.513150048100805
At time: 290.79017448425293 and batch: 400, loss is 3.820202975273132 and perplexity is 45.61346578697521
At time: 291.23274302482605 and batch: 450, loss is 3.7638531351089477 and perplexity is 43.114231296784155
At time: 291.6751034259796 and batch: 500, loss is 3.7241482496261598 and perplexity is 41.4359246458146
At time: 292.132848739624 and batch: 550, loss is 3.7587121438980104 and perplexity is 42.893150187567834
At time: 292.5746874809265 and batch: 600, loss is 3.8310089254379274 and perplexity is 46.10903535289315
At time: 293.0212516784668 and batch: 650, loss is 3.7199142932891847 and perplexity is 41.26085762467032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.50175505993413 and perplexity of 90.17525547916964
Finished 47 epochs...
Completing Train Step...
At time: 293.9335458278656 and batch: 50, loss is 3.838590841293335 and perplexity is 46.45995883418504
At time: 294.39400482177734 and batch: 100, loss is 3.8237457847595215 and perplexity is 45.77535220331176
At time: 294.83804416656494 and batch: 150, loss is 3.777292652130127 and perplexity is 43.69757690342999
At time: 295.2834119796753 and batch: 200, loss is 3.7135839796066286 and perplexity is 41.000488431904465
At time: 295.7294690608978 and batch: 250, loss is 3.697248411178589 and perplexity is 40.33616298733659
At time: 296.1749920845032 and batch: 300, loss is 3.8008419370651243 and perplexity is 44.73883592522461
At time: 296.62084221839905 and batch: 350, loss is 3.773039002418518 and perplexity is 43.51209747994961
At time: 297.0675575733185 and batch: 400, loss is 3.820183763504028 and perplexity is 45.612589480020205
At time: 297.51427125930786 and batch: 450, loss is 3.763840227127075 and perplexity is 43.113674782659864
At time: 297.9565875530243 and batch: 500, loss is 3.724143218994141 and perplexity is 41.43571619744966
At time: 298.3959701061249 and batch: 550, loss is 3.7587158250808717 and perplexity is 42.893308085387794
At time: 298.8387143611908 and batch: 600, loss is 3.831016206741333 and perplexity is 46.10937108799158
At time: 299.2806966304779 and batch: 650, loss is 3.719920597076416 and perplexity is 41.26111772515758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.50175505993413 and perplexity of 90.17525547916964
Finished 48 epochs...
Completing Train Step...
At time: 300.20188784599304 and batch: 50, loss is 3.8385518550872804 and perplexity is 46.45814757196399
At time: 300.6448423862457 and batch: 100, loss is 3.8237049436569213 and perplexity is 45.77348272563189
At time: 301.0881464481354 and batch: 150, loss is 3.777253260612488 and perplexity is 43.69585562346076
At time: 301.53099489212036 and batch: 200, loss is 3.713545727729797 and perplexity is 40.99892011626664
At time: 301.9738800525665 and batch: 250, loss is 3.6972150564193726 and perplexity is 40.33481760676998
At time: 302.41654419898987 and batch: 300, loss is 3.8008139944076538 and perplexity is 44.73758582072242
At time: 302.85991644859314 and batch: 350, loss is 3.7730148077011108 and perplexity is 43.51104472978283
At time: 303.30331325531006 and batch: 400, loss is 3.82016450881958 and perplexity is 45.61171123245812
At time: 303.76101565361023 and batch: 450, loss is 3.763827471733093 and perplexity is 43.11312485425928
At time: 304.2035286426544 and batch: 500, loss is 3.7241379261016845 and perplexity is 41.435496883240376
At time: 304.64638447761536 and batch: 550, loss is 3.758719081878662 and perplexity is 42.89344778044627
At time: 305.08868432044983 and batch: 600, loss is 3.8310231590270996 and perplexity is 46.10969165463025
At time: 305.53550028800964 and batch: 650, loss is 3.7199266719818116 and perplexity is 41.261368383305644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.501754162358303 and perplexity of 90.17517454007645
Finished 49 epochs...
Completing Train Step...
At time: 306.4512858390808 and batch: 50, loss is 3.8385131549835205 and perplexity is 46.45634967162216
At time: 306.9069902896881 and batch: 100, loss is 3.823663992881775 and perplexity is 45.77160830441289
At time: 307.34891176223755 and batch: 150, loss is 3.777214241027832 and perplexity is 43.6941506625868
At time: 307.78871178627014 and batch: 200, loss is 3.7135077333450317 and perplexity is 40.997362417112875
At time: 308.228022813797 and batch: 250, loss is 3.6971820259094237 and perplexity is 40.333485349178424
At time: 308.68004155158997 and batch: 300, loss is 3.80078622341156 and perplexity is 44.73634343065263
At time: 309.1295566558838 and batch: 350, loss is 3.772990641593933 and perplexity is 43.50999324991762
At time: 309.5762801170349 and batch: 400, loss is 3.8201454734802245 and perplexity is 45.61084300631975
At time: 310.02245926856995 and batch: 450, loss is 3.763814730644226 and perplexity is 43.112575549603555
At time: 310.46900725364685 and batch: 500, loss is 3.7241325616836547 and perplexity is 41.43527460651002
At time: 310.9139542579651 and batch: 550, loss is 3.7587222337722777 and perplexity is 42.893582976243536
At time: 311.35656476020813 and batch: 600, loss is 3.8310299253463747 and perplexity is 46.11000364858118
At time: 311.7959523200989 and batch: 650, loss is 3.7199325799942016 and perplexity is 41.26161215670138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5017538631663605 and perplexity of 90.17514756039486
Finished Training.
Improved accuracyfrom -10000000 to -90.17514756039486
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f93af975898>
SETTINGS FOR THIS RUN
{'lr': 22.230108533984744, 'data': 'ptb', 'num_layers': 1, 'anneal': 5.5056109879468655, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.91545527265737, 'seq_len': 20, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7277486324310303 and batch: 50, loss is 7.653942213058472 and perplexity is 2108.943126477451
At time: 1.1903409957885742 and batch: 100, loss is 6.923662014007569 and perplexity is 1016.0339206092791
At time: 1.6427197456359863 and batch: 150, loss is 6.783804025650024 and perplexity is 883.4229037560499
At time: 2.0966038703918457 and batch: 200, loss is 6.711984224319458 and perplexity is 822.2004520914646
At time: 2.5485105514526367 and batch: 250, loss is 6.633098754882813 and perplexity is 759.8330627547758
At time: 3.0013318061828613 and batch: 300, loss is 6.602181777954102 and perplexity is 736.700754576658
At time: 3.4527835845947266 and batch: 350, loss is 6.489323139190674 and perplexity is 658.0777853727355
At time: 3.90354323387146 and batch: 400, loss is 6.514915132522583 and perplexity is 675.1366619463544
At time: 4.355994462966919 and batch: 450, loss is 6.468075952529907 and perplexity is 644.2429796443465
At time: 4.822840213775635 and batch: 500, loss is 6.4520718955993654 and perplexity is 634.0145448636339
At time: 5.274855136871338 and batch: 550, loss is 6.421805667877197 and perplexity is 615.1128013870427
At time: 5.727210283279419 and batch: 600, loss is 6.4181771564483645 and perplexity is 612.8849019808116
At time: 6.178735971450806 and batch: 650, loss is 6.393255195617676 and perplexity is 597.799369780346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 6.119497261795343 and perplexity of 454.636074112567
Finished 1 epochs...
Completing Train Step...
At time: 7.1034815311431885 and batch: 50, loss is 5.784674777984619 and perplexity is 325.27623591563736
At time: 7.542815446853638 and batch: 100, loss is 5.467857398986816 and perplexity is 236.95195498138912
At time: 7.983498811721802 and batch: 150, loss is 5.339794788360596 and perplexity is 208.4699254441055
At time: 8.424019575119019 and batch: 200, loss is 5.294765472412109 and perplexity is 199.29088127565103
At time: 8.864779233932495 and batch: 250, loss is 5.237017612457276 and perplexity is 188.10825328492962
At time: 9.305404901504517 and batch: 300, loss is 5.269815454483032 and perplexity is 194.38008716150972
At time: 9.746603965759277 and batch: 350, loss is 5.199537334442138 and perplexity is 181.1883928507627
At time: 10.18721604347229 and batch: 400, loss is 5.236721839904785 and perplexity is 188.05262425388673
At time: 10.627468347549438 and batch: 450, loss is 5.188574504852295 and perplexity is 179.2129036371677
At time: 11.06778860092163 and batch: 500, loss is 5.193091459274292 and perplexity is 180.02423113995545
At time: 11.508228302001953 and batch: 550, loss is 5.20475209236145 and perplexity is 182.1357143366151
At time: 11.948840856552124 and batch: 600, loss is 5.245152587890625 and perplexity is 189.6447505143256
At time: 12.389522790908813 and batch: 650, loss is 5.220414342880249 and perplexity is 185.01082611011304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.282762116076899 and perplexity of 196.9130215125796
Finished 2 epochs...
Completing Train Step...
At time: 13.29248857498169 and batch: 50, loss is 5.173868732452393 and perplexity is 176.59672309069813
At time: 13.743643760681152 and batch: 100, loss is 5.176129322052002 and perplexity is 176.99638737450456
At time: 14.180109977722168 and batch: 150, loss is 5.165621500015259 and perplexity is 175.14627816715827
At time: 14.621111154556274 and batch: 200, loss is 5.105835790634155 and perplexity is 164.9819032535131
At time: 15.060330867767334 and batch: 250, loss is 5.112697324752808 and perplexity is 166.11782483734197
At time: 15.499477863311768 and batch: 300, loss is 5.136015548706054 and perplexity is 170.03691298845976
At time: 15.938929319381714 and batch: 350, loss is 5.099983282089234 and perplexity is 164.01916521921277
At time: 16.39388370513916 and batch: 400, loss is 5.182634353637695 and perplexity is 178.15150743789687
At time: 16.836931943893433 and batch: 450, loss is 5.142537488937378 and perplexity is 171.14950776666436
At time: 17.277278900146484 and batch: 500, loss is 5.116902837753296 and perplexity is 166.8179065789177
At time: 17.71696901321411 and batch: 550, loss is 5.109226350784302 and perplexity is 165.54223370043016
At time: 18.157716751098633 and batch: 600, loss is 5.1730337238311765 and perplexity is 176.44932485240307
At time: 18.598119497299194 and batch: 650, loss is 5.1268027400970455 and perplexity is 168.47758935677945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.265339570886948 and perplexity of 193.512008676327
Finished 3 epochs...
Completing Train Step...
At time: 19.528422355651855 and batch: 50, loss is 5.086465845108032 and perplexity is 161.8169640794987
At time: 19.972341060638428 and batch: 100, loss is 5.0902334690094 and perplexity is 162.42777947981912
At time: 20.415274143218994 and batch: 150, loss is 5.076860971450806 and perplexity is 160.27017284252494
At time: 20.85745072364807 and batch: 200, loss is 5.071680517196655 and perplexity is 159.44204742876073
At time: 21.295796632766724 and batch: 250, loss is 5.052942962646484 and perplexity is 156.48230909224606
At time: 21.734965562820435 and batch: 300, loss is 5.12846869468689 and perplexity is 168.7584992966863
At time: 22.176087617874146 and batch: 350, loss is 5.028870267868042 and perplexity is 152.7603369141901
At time: 22.616578340530396 and batch: 400, loss is 5.094000320434571 and perplexity is 163.04077459838388
At time: 23.056523084640503 and batch: 450, loss is 5.033717603683471 and perplexity is 153.50261514900635
At time: 23.497254848480225 and batch: 500, loss is 5.034164276123047 and perplexity is 153.57119585201085
At time: 23.937832593917847 and batch: 550, loss is 5.082434387207031 and perplexity is 161.16591901329963
At time: 24.37819266319275 and batch: 600, loss is 5.157315692901611 and perplexity is 173.69757163097046
At time: 24.819849491119385 and batch: 650, loss is 5.1471644878387455 and perplexity is 171.9432512600945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.290906120749081 and perplexity of 198.5232299525996
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 25.72638177871704 and batch: 50, loss is 5.034426374435425 and perplexity is 153.61145187856138
At time: 26.181954622268677 and batch: 100, loss is 4.936420364379883 and perplexity is 139.2708174218825
At time: 26.622786283493042 and batch: 150, loss is 4.877763519287109 and perplexity is 131.33660339770003
At time: 27.063456535339355 and batch: 200, loss is 4.853127813339233 and perplexity is 128.14056344926652
At time: 27.503471851348877 and batch: 250, loss is 4.841289710998535 and perplexity is 126.63256587093143
At time: 27.95804238319397 and batch: 300, loss is 4.90760612487793 and perplexity is 135.31509892899746
At time: 28.39906358718872 and batch: 350, loss is 4.821425895690918 and perplexity is 124.1419781500904
At time: 28.839406728744507 and batch: 400, loss is 4.86209641456604 and perplexity is 129.29497405000097
At time: 29.27962899208069 and batch: 450, loss is 4.819933071136474 and perplexity is 123.95679421482409
At time: 29.721293449401855 and batch: 500, loss is 4.7769840049743655 and perplexity is 118.7456730734119
At time: 30.162445545196533 and batch: 550, loss is 4.773228139877319 and perplexity is 118.3005168411683
At time: 30.60836935043335 and batch: 600, loss is 4.825384588241577 and perplexity is 124.63439208885124
At time: 31.051518440246582 and batch: 650, loss is 4.815403127670288 and perplexity is 123.39654684727311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.075662051930147 and perplexity of 160.07813694452085
Finished 5 epochs...
Completing Train Step...
At time: 31.982470989227295 and batch: 50, loss is 4.868962154388428 and perplexity is 130.18573406634934
At time: 32.42687106132507 and batch: 100, loss is 4.8182280731201175 and perplexity is 123.74562819655029
At time: 32.872952938079834 and batch: 150, loss is 4.780051364898681 and perplexity is 119.11046798480875
At time: 33.31678795814514 and batch: 200, loss is 4.761851606369018 and perplexity is 116.96229365107823
At time: 33.762362003326416 and batch: 250, loss is 4.752500658035278 and perplexity is 115.87368298952987
At time: 34.20631647109985 and batch: 300, loss is 4.824478063583374 and perplexity is 124.52145913515012
At time: 34.65059494972229 and batch: 350, loss is 4.751568622589112 and perplexity is 115.76573492323227
At time: 35.097583055496216 and batch: 400, loss is 4.8026793384552 and perplexity is 121.83642159570229
At time: 35.54174852371216 and batch: 450, loss is 4.763023710250854 and perplexity is 117.09946598390272
At time: 35.98322868347168 and batch: 500, loss is 4.7222058391571045 and perplexity is 112.41595088250243
At time: 36.421743869781494 and batch: 550, loss is 4.7339770793914795 and perplexity is 113.74704499088116
At time: 36.86277413368225 and batch: 600, loss is 4.794171905517578 and perplexity is 120.80430297467264
At time: 37.30457353591919 and batch: 650, loss is 4.768343353271485 and perplexity is 117.72405315833389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.059650495940564 and perplexity of 157.5354474221461
Finished 6 epochs...
Completing Train Step...
At time: 38.21045279502869 and batch: 50, loss is 4.801463470458985 and perplexity is 121.68837461094458
At time: 38.666146755218506 and batch: 100, loss is 4.756687669754029 and perplexity is 116.35986457181976
At time: 39.10620379447937 and batch: 150, loss is 4.728317050933838 and perplexity is 113.10505204234144
At time: 39.56194758415222 and batch: 200, loss is 4.711202688217163 and perplexity is 111.18580137831161
At time: 40.00335502624512 and batch: 250, loss is 4.697872972488403 and perplexity is 109.71356032927825
At time: 40.443581342697144 and batch: 300, loss is 4.7700497341156005 and perplexity is 117.92510671571986
At time: 40.88459658622742 and batch: 350, loss is 4.7026067638397215 and perplexity is 110.23415264784205
At time: 41.32536792755127 and batch: 400, loss is 4.75389404296875 and perplexity is 116.03525217148447
At time: 41.76528191566467 and batch: 450, loss is 4.719622402191162 and perplexity is 112.12590617685494
At time: 42.20577812194824 and batch: 500, loss is 4.684171953201294 and perplexity is 108.22062345577704
At time: 42.64527225494385 and batch: 550, loss is 4.698379783630371 and perplexity is 109.76917847683609
At time: 43.08467769622803 and batch: 600, loss is 4.7635560226440425 and perplexity is 117.16181607427234
At time: 43.52509307861328 and batch: 650, loss is 4.731567306518555 and perplexity is 113.47327044720528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0496682559742645 and perplexity of 155.97071355042763
Finished 7 epochs...
Completing Train Step...
At time: 44.45452117919922 and batch: 50, loss is 4.755919733047485 and perplexity is 116.27054186213007
At time: 44.89903688430786 and batch: 100, loss is 4.716994113922119 and perplexity is 111.83159391115547
At time: 45.34192728996277 and batch: 150, loss is 4.690206518173218 and perplexity is 108.87566228869935
At time: 45.78459882736206 and batch: 200, loss is 4.674025249481201 and perplexity is 107.12809301799042
At time: 46.22956442832947 and batch: 250, loss is 4.658773031234741 and perplexity is 105.50654946590221
At time: 46.67321062088013 and batch: 300, loss is 4.728936042785644 and perplexity is 113.17508482057721
At time: 47.11921572685242 and batch: 350, loss is 4.670563135147095 and perplexity is 106.75784460256412
At time: 47.56256866455078 and batch: 400, loss is 4.721605129241944 and perplexity is 112.3484417849073
At time: 48.0077543258667 and batch: 450, loss is 4.689344282150269 and perplexity is 108.78182623087976
At time: 48.45187759399414 and batch: 500, loss is 4.652113332748413 and perplexity is 104.80624216417465
At time: 48.897540807724 and batch: 550, loss is 4.665898399353027 and perplexity is 106.26100717214341
At time: 49.34106183052063 and batch: 600, loss is 4.733006610870361 and perplexity is 113.636710611028
At time: 49.78539419174194 and batch: 650, loss is 4.697243146896362 and perplexity is 109.64448167722392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.040998870251226 and perplexity of 154.62438761486465
Finished 8 epochs...
Completing Train Step...
At time: 50.68964648246765 and batch: 50, loss is 4.714557285308838 and perplexity is 111.55941124918132
At time: 51.14915919303894 and batch: 100, loss is 4.6823257732391355 and perplexity is 108.02101302434575
At time: 51.59197211265564 and batch: 150, loss is 4.656275749206543 and perplexity is 105.24339857382046
At time: 52.03084707260132 and batch: 200, loss is 4.640422372817993 and perplexity is 103.58809113855112
At time: 52.47120380401611 and batch: 250, loss is 4.6252256202697755 and perplexity is 102.02578957227611
At time: 52.91076612472534 and batch: 300, loss is 4.692792692184448 and perplexity is 109.15759810737025
At time: 53.35045266151428 and batch: 350, loss is 4.64011248588562 and perplexity is 103.55599551602162
At time: 53.79153347015381 and batch: 400, loss is 4.6900647926330565 and perplexity is 108.86023292004468
At time: 54.23156929016113 and batch: 450, loss is 4.658560533523559 and perplexity is 105.48413194754603
At time: 54.67186880111694 and batch: 500, loss is 4.625503587722778 and perplexity is 102.05415336306682
At time: 55.111767292022705 and batch: 550, loss is 4.640442361831665 and perplexity is 103.59016178301613
At time: 55.552048444747925 and batch: 600, loss is 4.711972284317016 and perplexity is 111.27140247232047
At time: 55.992321729660034 and batch: 650, loss is 4.6702807521820064 and perplexity is 106.72770226190086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.036624085669424 and perplexity of 153.94941673057286
Finished 9 epochs...
Completing Train Step...
At time: 56.9242844581604 and batch: 50, loss is 4.684972171783447 and perplexity is 108.30725826840899
At time: 57.361417293548584 and batch: 100, loss is 4.659157505035401 and perplexity is 105.54712176896383
At time: 57.799131870269775 and batch: 150, loss is 4.634287786483765 and perplexity is 102.95456624363861
At time: 58.23642659187317 and batch: 200, loss is 4.6150500679016115 and perplexity is 100.99288490772386
At time: 58.6731276512146 and batch: 250, loss is 4.598768272399902 and perplexity is 99.36185350006369
At time: 59.11052465438843 and batch: 300, loss is 4.665905656814576 and perplexity is 106.26177836011547
At time: 59.5468635559082 and batch: 350, loss is 4.616320905685424 and perplexity is 101.12131206952124
At time: 59.98307514190674 and batch: 400, loss is 4.667576551437378 and perplexity is 106.43947901237559
At time: 60.420451402664185 and batch: 450, loss is 4.633070201873779 and perplexity is 102.82928663299292
At time: 60.858736991882324 and batch: 500, loss is 4.600181350708008 and perplexity is 99.50235882903405
At time: 61.295475244522095 and batch: 550, loss is 4.617543153762817 and perplexity is 101.2449829616408
At time: 61.731618881225586 and batch: 600, loss is 4.693900680541992 and perplexity is 109.27861048298233
At time: 62.18662214279175 and batch: 650, loss is 4.647038288116455 and perplexity is 104.27569322599192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.039011038985907 and perplexity of 154.31732571751527
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 63.100900173187256 and batch: 50, loss is 4.648722877502442 and perplexity is 104.45150299407473
At time: 63.55980896949768 and batch: 100, loss is 4.596209564208984 and perplexity is 99.10794049482186
At time: 64.00498986244202 and batch: 150, loss is 4.5569336891174315 and perplexity is 95.29084005721928
At time: 64.4514741897583 and batch: 200, loss is 4.526302928924561 and perplexity is 92.4162592359542
At time: 64.89708638191223 and batch: 250, loss is 4.500848579406738 and perplexity is 90.09355040364997
At time: 65.34176778793335 and batch: 300, loss is 4.561984386444092 and perplexity is 95.7733427100795
At time: 65.78603076934814 and batch: 350, loss is 4.501052198410034 and perplexity is 90.11189703038407
At time: 66.23243284225464 and batch: 400, loss is 4.543986921310425 and perplexity is 94.06508359064831
At time: 66.67741584777832 and batch: 450, loss is 4.495839567184448 and perplexity is 89.64339905652707
At time: 67.1204559803009 and batch: 500, loss is 4.44803674697876 and perplexity is 85.45900155638834
At time: 67.56137204170227 and batch: 550, loss is 4.454389638900757 and perplexity is 86.00364154475922
At time: 68.00060558319092 and batch: 600, loss is 4.539626979827881 and perplexity is 93.65585807853569
At time: 68.4415991306305 and batch: 650, loss is 4.521914463043213 and perplexity is 92.01158224046955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9666891659007355 and perplexity of 143.55082675472804
Finished 11 epochs...
Completing Train Step...
At time: 69.36303877830505 and batch: 50, loss is 4.591880989074707 and perplexity is 98.67987146085295
At time: 69.80296993255615 and batch: 100, loss is 4.555701732635498 and perplexity is 95.17351817170902
At time: 70.24413919448853 and batch: 150, loss is 4.525846614837646 and perplexity is 92.37409801511596
At time: 70.68452024459839 and batch: 200, loss is 4.500971889495849 and perplexity is 90.10466053235999
At time: 71.12476849555969 and batch: 250, loss is 4.481720056533813 and perplexity is 88.38657190107301
At time: 71.56391716003418 and batch: 300, loss is 4.5443601894378665 and perplexity is 94.10020164207475
At time: 72.00274562835693 and batch: 350, loss is 4.486884956359863 and perplexity is 88.84426063168371
At time: 72.43931746482849 and batch: 400, loss is 4.53251235961914 and perplexity is 92.99189693435675
At time: 72.87614631652832 and batch: 450, loss is 4.488860483169556 and perplexity is 89.01994833119106
At time: 73.31278038024902 and batch: 500, loss is 4.448081455230713 and perplexity is 85.46282236437176
At time: 73.76542806625366 and batch: 550, loss is 4.4593984889984135 and perplexity is 86.43550155123151
At time: 74.20347499847412 and batch: 600, loss is 4.544987287521362 and perplexity is 94.15923020459584
At time: 74.654470205307 and batch: 650, loss is 4.51800066947937 and perplexity is 91.65217169022782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.962825101964614 and perplexity of 142.99720748103974
Finished 12 epochs...
Completing Train Step...
At time: 75.59217262268066 and batch: 50, loss is 4.575183210372924 and perplexity is 97.04581730869076
At time: 76.05126476287842 and batch: 100, loss is 4.540377531051636 and perplexity is 93.72617798347258
At time: 76.49605011940002 and batch: 150, loss is 4.513146295547485 and perplexity is 91.20833592114798
At time: 76.94994759559631 and batch: 200, loss is 4.4902958488464355 and perplexity is 89.14781625624506
At time: 77.3976948261261 and batch: 250, loss is 4.473402910232544 and perplexity is 87.65449646087424
At time: 77.84352850914001 and batch: 300, loss is 4.536182441711426 and perplexity is 93.33381187418681
At time: 78.28877329826355 and batch: 350, loss is 4.479932126998901 and perplexity is 88.22868412684868
At time: 78.73609709739685 and batch: 400, loss is 4.527347927093506 and perplexity is 92.51288453547399
At time: 79.18192863464355 and batch: 450, loss is 4.486598978042602 and perplexity is 88.81885673218534
At time: 79.62646627426147 and batch: 500, loss is 4.448662309646607 and perplexity is 85.51247824215868
At time: 80.07312989234924 and batch: 550, loss is 4.460977296829224 and perplexity is 86.5720743806981
At time: 80.51740002632141 and batch: 600, loss is 4.5456283664703365 and perplexity is 94.2196130579517
At time: 80.96044445037842 and batch: 650, loss is 4.513220415115357 and perplexity is 91.21509649413504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.961342605890012 and perplexity of 142.78537174389206
Finished 13 epochs...
Completing Train Step...
At time: 81.88083100318909 and batch: 50, loss is 4.563715238571167 and perplexity is 95.93925574807281
At time: 82.3190906047821 and batch: 100, loss is 4.530217590332032 and perplexity is 92.77874664422832
At time: 82.76005387306213 and batch: 150, loss is 4.504293766021728 and perplexity is 90.4044747863202
At time: 83.20035004615784 and batch: 200, loss is 4.48260311126709 and perplexity is 88.46465655318507
At time: 83.64453411102295 and batch: 250, loss is 4.467876434326172 and perplexity is 87.17141210465277
At time: 84.08464217185974 and batch: 300, loss is 4.530339794158936 and perplexity is 92.79008525492029
At time: 84.52516031265259 and batch: 350, loss is 4.474622268676757 and perplexity is 87.76144390168854
At time: 84.96722173690796 and batch: 400, loss is 4.523324632644654 and perplexity is 92.14142570586242
At time: 85.42219305038452 and batch: 450, loss is 4.484500484466553 and perplexity is 88.6326663598777
At time: 85.86213231086731 and batch: 500, loss is 4.448062267303467 and perplexity is 85.46118252568658
At time: 86.30214858055115 and batch: 550, loss is 4.4601847934722905 and perplexity is 86.50349290025089
At time: 86.74544906616211 and batch: 600, loss is 4.543890037536621 and perplexity is 94.055970651822
At time: 87.18562388420105 and batch: 650, loss is 4.508022470474243 and perplexity is 90.74219559323716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9605865478515625 and perplexity of 142.6774585152845
Finished 14 epochs...
Completing Train Step...
At time: 88.09251117706299 and batch: 50, loss is 4.554406805038452 and perplexity is 95.0503551173499
At time: 88.54948711395264 and batch: 100, loss is 4.521642637252808 and perplexity is 91.98657451842647
At time: 88.98994779586792 and batch: 150, loss is 4.4972413063049315 and perplexity is 89.7691438259696
At time: 89.42983055114746 and batch: 200, loss is 4.476713209152222 and perplexity is 87.94513983863561
At time: 89.87353563308716 and batch: 250, loss is 4.463251161575317 and perplexity is 86.76915154712405
At time: 90.31420087814331 and batch: 300, loss is 4.525546026229859 and perplexity is 92.34633558634191
At time: 90.75947618484497 and batch: 350, loss is 4.470073947906494 and perplexity is 87.36318309907799
At time: 91.20268058776855 and batch: 400, loss is 4.519732847213745 and perplexity is 91.81106711916907
At time: 91.64783644676208 and batch: 450, loss is 4.482632989883423 and perplexity is 88.46729979420525
At time: 92.09068369865417 and batch: 500, loss is 4.447225303649902 and perplexity is 85.38968454690045
At time: 92.5342652797699 and batch: 550, loss is 4.4588416576385494 and perplexity is 86.38738495102142
At time: 92.98028993606567 and batch: 600, loss is 4.541516427993774 and perplexity is 93.8329832495271
At time: 93.42645215988159 and batch: 650, loss is 4.5032628059387205 and perplexity is 90.3113194094788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.960392671472886 and perplexity of 142.64979940761924
Finished 15 epochs...
Completing Train Step...
At time: 94.3562581539154 and batch: 50, loss is 4.546882829666138 and perplexity is 94.33788226152727
At time: 94.80400538444519 and batch: 100, loss is 4.514454965591431 and perplexity is 91.32777567469365
At time: 95.24780631065369 and batch: 150, loss is 4.4912966537475585 and perplexity is 89.23708048826852
At time: 95.69201993942261 and batch: 200, loss is 4.471481990814209 and perplexity is 87.4862808525578
At time: 96.13750696182251 and batch: 250, loss is 4.459095249176025 and perplexity is 86.4092948387533
At time: 96.58514356613159 and batch: 300, loss is 4.520749073028565 and perplexity is 91.90441531905348
At time: 97.0444884300232 and batch: 350, loss is 4.466242733001709 and perplexity is 87.02911631929734
At time: 97.48381352424622 and batch: 400, loss is 4.516412982940674 and perplexity is 91.50677222592198
At time: 97.9271399974823 and batch: 450, loss is 4.480046138763428 and perplexity is 88.23874380825801
At time: 98.36922883987427 and batch: 500, loss is 4.445960578918457 and perplexity is 85.281758363886
At time: 98.8116500377655 and batch: 550, loss is 4.456622171401977 and perplexity is 86.19586195905359
At time: 99.25312829017639 and batch: 600, loss is 4.538894805908203 and perplexity is 93.58731079915286
At time: 99.69869470596313 and batch: 650, loss is 4.498441610336304 and perplexity is 89.87695878361657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9597634708180145 and perplexity of 142.560072291555
Finished 16 epochs...
Completing Train Step...
At time: 100.6030170917511 and batch: 50, loss is 4.539844779968262 and perplexity is 93.67625855910579
At time: 101.05631113052368 and batch: 100, loss is 4.5078861522674565 and perplexity is 90.72982662293109
At time: 101.49551701545715 and batch: 150, loss is 4.485391788482666 and perplexity is 88.71170022773279
At time: 101.93286108970642 and batch: 200, loss is 4.466764335632324 and perplexity is 87.07452277634381
At time: 102.37080907821655 and batch: 250, loss is 4.454996862411499 and perplexity is 86.05588083677263
At time: 102.81164813041687 and batch: 300, loss is 4.516385507583618 and perplexity is 91.50425807922079
At time: 103.2497045993805 and batch: 350, loss is 4.462357149124146 and perplexity is 86.69161351041288
At time: 103.68769264221191 and batch: 400, loss is 4.513029623031616 and perplexity is 91.19769503588934
At time: 104.12609958648682 and batch: 450, loss is 4.477458400726318 and perplexity is 88.0107002403217
At time: 104.56591391563416 and batch: 500, loss is 4.444054403305054 and perplexity is 85.11935119329426
At time: 105.00312185287476 and batch: 550, loss is 4.454794025421142 and perplexity is 86.03842729107338
At time: 105.44106316566467 and batch: 600, loss is 4.536067752838135 and perplexity is 93.32310813827446
At time: 105.88093304634094 and batch: 650, loss is 4.49445987701416 and perplexity is 89.51980422091316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.959859212239583 and perplexity of 142.57372184893867
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 106.80913424491882 and batch: 50, loss is 4.532772064208984 and perplexity is 93.01605049306818
At time: 107.25374579429626 and batch: 100, loss is 4.498395824432373 and perplexity is 89.87284378002144
At time: 107.70614695549011 and batch: 150, loss is 4.473871049880981 and perplexity is 87.69554061247916
At time: 108.15162563323975 and batch: 200, loss is 4.449387121200561 and perplexity is 85.5744811418905
At time: 108.61206293106079 and batch: 250, loss is 4.434662199020385 and perplexity is 84.32363547001842
At time: 109.06055927276611 and batch: 300, loss is 4.493148632049561 and perplexity is 89.402498753306
At time: 109.50893449783325 and batch: 350, loss is 4.435641536712646 and perplexity is 84.40625723528358
At time: 109.95308780670166 and batch: 400, loss is 4.486407575607299 and perplexity is 88.8018582135368
At time: 110.39775848388672 and batch: 450, loss is 4.446691560745239 and perplexity is 85.34412056944866
At time: 110.84591722488403 and batch: 500, loss is 4.405982103347778 and perplexity is 81.93957647200318
At time: 111.29385781288147 and batch: 550, loss is 4.415808515548706 and perplexity is 82.74871749181307
At time: 111.74012613296509 and batch: 600, loss is 4.496841373443604 and perplexity is 89.73324937358431
At time: 112.18638944625854 and batch: 650, loss is 4.468733797073364 and perplexity is 87.24618167375294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.950225830078125 and perplexity of 141.20684907478636
Finished 18 epochs...
Completing Train Step...
At time: 113.0984308719635 and batch: 50, loss is 4.525649108886719 and perplexity is 92.35585538261996
At time: 113.5581603050232 and batch: 100, loss is 4.491228895187378 and perplexity is 89.23103411702894
At time: 114.00073385238647 and batch: 150, loss is 4.467648057937622 and perplexity is 87.15150648544477
At time: 114.44055438041687 and batch: 200, loss is 4.442992668151856 and perplexity is 85.02902494567638
At time: 114.88342928886414 and batch: 250, loss is 4.430032320022583 and perplexity is 83.93412961939599
At time: 115.32846736907959 and batch: 300, loss is 4.488692874908447 and perplexity is 89.0050291027752
At time: 115.77609777450562 and batch: 350, loss is 4.432156896591186 and perplexity is 84.11264367072832
At time: 116.21581077575684 and batch: 400, loss is 4.483557395935058 and perplexity is 88.54911731198486
At time: 116.65853524208069 and batch: 450, loss is 4.445223951339722 and perplexity is 85.21896060083387
At time: 117.10045623779297 and batch: 500, loss is 4.406155471801758 and perplexity is 81.95378344118042
At time: 117.54516196250916 and batch: 550, loss is 4.4175502300262455 and perplexity is 82.89296771595217
At time: 117.98679447174072 and batch: 600, loss is 4.499546642303467 and perplexity is 89.97633059058366
At time: 118.42746543884277 and batch: 650, loss is 4.469436321258545 and perplexity is 87.30749576123638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9487678677428 and perplexity of 141.00112481289966
Finished 19 epochs...
Completing Train Step...
At time: 119.35415053367615 and batch: 50, loss is 4.522885751724243 and perplexity is 92.10099546482037
At time: 119.79681825637817 and batch: 100, loss is 4.488215427398682 and perplexity is 88.96254401627994
At time: 120.25416350364685 and batch: 150, loss is 4.464799165725708 and perplexity is 86.90357457070074
At time: 120.6994137763977 and batch: 200, loss is 4.439776430130005 and perplexity is 84.75599066965482
At time: 121.1419768333435 and batch: 250, loss is 4.427803459167481 and perplexity is 83.74726045352568
At time: 121.58350872993469 and batch: 300, loss is 4.486123962402344 and perplexity is 88.77667640503833
At time: 122.02763676643372 and batch: 350, loss is 4.430181941986084 and perplexity is 83.94668894822665
At time: 122.4723379611969 and batch: 400, loss is 4.482036962509155 and perplexity is 88.41458657262237
At time: 122.91445684432983 and batch: 450, loss is 4.444502534866333 and perplexity is 85.15750440923124
At time: 123.355797290802 and batch: 500, loss is 4.406515378952026 and perplexity is 81.98328450233541
At time: 123.80281805992126 and batch: 550, loss is 4.418570337295532 and perplexity is 82.9775705795534
At time: 124.24410796165466 and batch: 600, loss is 4.50095944404602 and perplexity is 90.10353914630608
At time: 124.68614435195923 and batch: 650, loss is 4.469646587371826 and perplexity is 87.3258554991781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.948088103649663 and perplexity of 140.90530988062469
Finished 20 epochs...
Completing Train Step...
At time: 125.59438753128052 and batch: 50, loss is 4.520861310958862 and perplexity is 91.91473105931192
At time: 126.04965209960938 and batch: 100, loss is 4.48603856086731 and perplexity is 88.76909506433181
At time: 126.49049305915833 and batch: 150, loss is 4.462737236022949 and perplexity is 86.72457011973529
At time: 126.93292951583862 and batch: 200, loss is 4.437428865432739 and perplexity is 84.55725386310341
At time: 127.37772512435913 and batch: 250, loss is 4.426198272705078 and perplexity is 83.61293831965929
At time: 127.82007431983948 and batch: 300, loss is 4.484104824066162 and perplexity is 88.59760486029896
At time: 128.26149249076843 and batch: 350, loss is 4.428654413223267 and perplexity is 83.81855585472277
At time: 128.71007537841797 and batch: 400, loss is 4.480987844467163 and perplexity is 88.32187787436105
At time: 129.1557650566101 and batch: 450, loss is 4.443993711471558 and perplexity is 85.1141853005697
At time: 129.60154390335083 and batch: 500, loss is 4.40662260055542 and perplexity is 81.9920753528274
At time: 130.04739379882812 and batch: 550, loss is 4.4191861248016355 and perplexity is 83.02868286634181
At time: 130.4960377216339 and batch: 600, loss is 4.5018024921417235 and perplexity is 90.17953279204772
At time: 130.94157361984253 and batch: 650, loss is 4.46946530342102 and perplexity is 87.31002615793184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.947722191904106 and perplexity of 140.85376040458246
Finished 21 epochs...
Completing Train Step...
At time: 131.8852186203003 and batch: 50, loss is 4.519364051818847 and perplexity is 91.77721386326104
At time: 132.33358120918274 and batch: 100, loss is 4.484404382705688 and perplexity is 88.62414901384292
At time: 132.77849912643433 and batch: 150, loss is 4.461132936477661 and perplexity is 86.58554947652155
At time: 133.22413158416748 and batch: 200, loss is 4.435612640380859 and perplexity is 84.40381823930879
At time: 133.67195773124695 and batch: 250, loss is 4.4249423122406 and perplexity is 83.50798969426891
At time: 134.1166250705719 and batch: 300, loss is 4.4823571395874025 and perplexity is 88.4428994289476
At time: 134.55924797058105 and batch: 350, loss is 4.427343969345093 and perplexity is 83.70878827915756
At time: 134.9988613128662 and batch: 400, loss is 4.480063400268555 and perplexity is 88.24026695493248
At time: 135.44082951545715 and batch: 450, loss is 4.443475503921508 and perplexity is 85.07008991339869
At time: 135.88326382637024 and batch: 500, loss is 4.406651697158813 and perplexity is 81.99446107843342
At time: 136.3249635696411 and batch: 550, loss is 4.41954496383667 and perplexity is 83.0584821450342
At time: 136.76949644088745 and batch: 600, loss is 4.502343997955323 and perplexity is 90.22837875731577
At time: 137.2123703956604 and batch: 650, loss is 4.469509572982788 and perplexity is 87.31389142008386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.947455013499541 and perplexity of 140.81613234852642
Finished 22 epochs...
Completing Train Step...
At time: 138.11827206611633 and batch: 50, loss is 4.517920217514038 and perplexity is 91.6447983894906
At time: 138.57850456237793 and batch: 100, loss is 4.48292067527771 and perplexity is 88.49275420548369
At time: 139.02042651176453 and batch: 150, loss is 4.459795989990234 and perplexity is 86.46986657844036
At time: 139.4614896774292 and batch: 200, loss is 4.433942852020263 and perplexity is 84.26299932763196
At time: 139.90394043922424 and batch: 250, loss is 4.4237494468688965 and perplexity is 83.40843529440502
At time: 140.3467402458191 and batch: 300, loss is 4.480657482147217 and perplexity is 88.29270447304613
At time: 140.78942322731018 and batch: 350, loss is 4.426184387207031 and perplexity is 83.61177732042806
At time: 141.23209595680237 and batch: 400, loss is 4.479068908691406 and perplexity is 88.15255637362657
At time: 141.67702984809875 and batch: 450, loss is 4.442992811203003 and perplexity is 85.0290371091768
At time: 142.11891555786133 and batch: 500, loss is 4.406541385650635 and perplexity is 81.9854166446313
At time: 142.55947041511536 and batch: 550, loss is 4.419640941619873 and perplexity is 83.06645429659528
At time: 143.01631259918213 and batch: 600, loss is 4.50270793914795 and perplexity is 90.26122255733287
At time: 143.46451878547668 and batch: 650, loss is 4.469256448745727 and perplexity is 87.29179295487994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.947371838139553 and perplexity of 140.80442040310564
Finished 23 epochs...
Completing Train Step...
At time: 144.39252614974976 and batch: 50, loss is 4.516718435287475 and perplexity is 91.53472745352423
At time: 144.8387472629547 and batch: 100, loss is 4.481664962768555 and perplexity is 88.38170248616714
At time: 145.28279328346252 and batch: 150, loss is 4.4584959125518795 and perplexity is 86.35752209988885
At time: 145.72399520874023 and batch: 200, loss is 4.432529106140136 and perplexity is 84.14395702709362
At time: 146.16599345207214 and batch: 250, loss is 4.4226624965667725 and perplexity is 83.31782372451987
At time: 146.6083323955536 and batch: 300, loss is 4.479435901641846 and perplexity is 88.18491367746697
At time: 147.05083966255188 and batch: 350, loss is 4.4251330375671385 and perplexity is 83.52391830181806
At time: 147.49247002601624 and batch: 400, loss is 4.478296670913696 and perplexity is 88.08450791756911
At time: 147.9333837032318 and batch: 450, loss is 4.442469263076783 and perplexity is 84.98453196743233
At time: 148.3787875175476 and batch: 500, loss is 4.406409540176392 and perplexity is 81.97460795104709
At time: 148.81908750534058 and batch: 550, loss is 4.419728956222534 and perplexity is 83.07376567931416
At time: 149.25447964668274 and batch: 600, loss is 4.502918567657471 and perplexity is 90.28023614643932
At time: 149.69387459754944 and batch: 650, loss is 4.469063510894776 and perplexity is 87.27495268855614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.94727310479856 and perplexity of 140.79051899853002
Finished 24 epochs...
Completing Train Step...
At time: 150.60510993003845 and batch: 50, loss is 4.5156114959716795 and perplexity is 91.43346012368002
At time: 151.06220293045044 and batch: 100, loss is 4.480451164245605 and perplexity is 88.27448998658353
At time: 151.50805187225342 and batch: 150, loss is 4.457274570465088 and perplexity is 86.25211440616634
At time: 151.94975185394287 and batch: 200, loss is 4.431179599761963 and perplexity is 84.03048080606443
At time: 152.39095520973206 and batch: 250, loss is 4.421719989776611 and perplexity is 83.23933310469953
At time: 152.83412194252014 and batch: 300, loss is 4.478185253143311 and perplexity is 88.0746942848081
At time: 153.27322626113892 and batch: 350, loss is 4.424096326828003 and perplexity is 83.43737302769881
At time: 153.70953226089478 and batch: 400, loss is 4.47754997253418 and perplexity is 88.01875990826743
At time: 154.14297819137573 and batch: 450, loss is 4.441974630355835 and perplexity is 84.9425062316557
At time: 154.59930849075317 and batch: 500, loss is 4.406189985275269 and perplexity is 81.95661199972577
At time: 155.0436429977417 and batch: 550, loss is 4.419915466308594 and perplexity is 83.08926121949268
At time: 155.4857382774353 and batch: 600, loss is 4.502985563278198 and perplexity is 90.28628472951141
At time: 155.92909383773804 and batch: 650, loss is 4.468728246688843 and perplexity is 87.24569742524056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.947291355507047 and perplexity of 140.79308854869794
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 156.85170435905457 and batch: 50, loss is 4.5146880531311036 and perplexity is 91.3490655223322
At time: 157.29879426956177 and batch: 100, loss is 4.478895750045776 and perplexity is 88.13729331785923
At time: 157.75387454032898 and batch: 150, loss is 4.455443429946899 and perplexity is 86.09431918140423
At time: 158.19951486587524 and batch: 200, loss is 4.428522777557373 and perplexity is 83.80752306947853
At time: 158.64654636383057 and batch: 250, loss is 4.417640295028686 and perplexity is 82.90043380750356
At time: 159.09090662002563 and batch: 300, loss is 4.472384405136109 and perplexity is 87.56526535838339
At time: 159.54296565055847 and batch: 350, loss is 4.41861662864685 and perplexity is 82.98141181233188
At time: 160.01354002952576 and batch: 400, loss is 4.4718101978302 and perplexity is 87.5149991762564
At time: 160.47302985191345 and batch: 450, loss is 4.435133934020996 and perplexity is 84.36342326415806
At time: 160.9190332889557 and batch: 500, loss is 4.39702073097229 and perplexity is 81.20856573670353
At time: 161.36495232582092 and batch: 550, loss is 4.410802392959595 and perplexity is 82.3355024335499
At time: 161.81136107444763 and batch: 600, loss is 4.49482647895813 and perplexity is 89.55262837149543
At time: 162.25252032279968 and batch: 650, loss is 4.463197088241577 and perplexity is 86.76445977668504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945487527286305 and perplexity of 140.5393509206364
Finished 26 epochs...
Completing Train Step...
At time: 163.16095185279846 and batch: 50, loss is 4.512433910369873 and perplexity is 91.14338359284801
At time: 163.6184799671173 and batch: 100, loss is 4.477634620666504 and perplexity is 88.02621084725273
At time: 164.0620436668396 and batch: 150, loss is 4.4541668033599855 and perplexity is 85.98447901191325
At time: 164.5061321258545 and batch: 200, loss is 4.427712545394898 and perplexity is 83.73964702022256
At time: 164.94918036460876 and batch: 250, loss is 4.416917610168457 and perplexity is 82.8405445622164
At time: 165.39273023605347 and batch: 300, loss is 4.4719227600097655 and perplexity is 87.52485060974738
At time: 165.83612418174744 and batch: 350, loss is 4.418373394012451 and perplexity is 82.96123031348702
At time: 166.29224514961243 and batch: 400, loss is 4.47153486251831 and perplexity is 87.4909065235944
At time: 166.73456859588623 and batch: 450, loss is 4.434952602386475 and perplexity is 84.34812689362361
At time: 167.17940044403076 and batch: 500, loss is 4.397167291641235 and perplexity is 81.22046859064578
At time: 167.62172174453735 and batch: 550, loss is 4.4112644958496094 and perplexity is 82.37355869945327
At time: 168.06540989875793 and batch: 600, loss is 4.495199098587036 and perplexity is 89.58600365640375
At time: 168.50821042060852 and batch: 650, loss is 4.46302417755127 and perplexity is 86.74945857102271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945160809685202 and perplexity of 140.49344174111988
Finished 27 epochs...
Completing Train Step...
At time: 169.43125319480896 and batch: 50, loss is 4.511385231018067 and perplexity is 91.04785350738919
At time: 169.87427735328674 and batch: 100, loss is 4.4769370555877686 and perplexity is 87.96482824822009
At time: 170.31298089027405 and batch: 150, loss is 4.453512306213379 and perplexity is 85.92822082816613
At time: 170.75039649009705 and batch: 200, loss is 4.427262744903564 and perplexity is 83.7019893557017
At time: 171.1891553401947 and batch: 250, loss is 4.416489582061768 and perplexity is 82.80509406821551
At time: 171.62922072410583 and batch: 300, loss is 4.471659746170044 and perplexity is 87.50183338977384
At time: 172.07067728042603 and batch: 350, loss is 4.418376903533936 and perplexity is 82.96152146821814
At time: 172.51346516609192 and batch: 400, loss is 4.471409940719605 and perplexity is 87.47997768482043
At time: 172.95144152641296 and batch: 450, loss is 4.43486873626709 and perplexity is 84.34105324016772
At time: 173.38998746871948 and batch: 500, loss is 4.397340211868286 and perplexity is 81.23451446688873
At time: 173.8312714099884 and batch: 550, loss is 4.411528177261353 and perplexity is 82.39528193958292
At time: 174.26922345161438 and batch: 600, loss is 4.495428619384765 and perplexity is 89.60656786729513
At time: 174.7099573612213 and batch: 650, loss is 4.462854681015014 and perplexity is 86.73475608431839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944989073510263 and perplexity of 140.46931600652127
Finished 28 epochs...
Completing Train Step...
At time: 175.61290335655212 and batch: 50, loss is 4.510571155548096 and perplexity is 90.97376384463556
At time: 176.06603908538818 and batch: 100, loss is 4.47634913444519 and perplexity is 87.9131270654881
At time: 176.50391268730164 and batch: 150, loss is 4.453033723831177 and perplexity is 85.8871069345211
At time: 176.94741702079773 and batch: 200, loss is 4.426959295272827 and perplexity is 83.67659387125977
At time: 177.38626194000244 and batch: 250, loss is 4.416170320510864 and perplexity is 82.77866180508774
At time: 177.83779168128967 and batch: 300, loss is 4.471513957977295 and perplexity is 87.48907758546711
At time: 178.27642059326172 and batch: 350, loss is 4.41842866897583 and perplexity is 82.96581611919342
At time: 178.7177517414093 and batch: 400, loss is 4.471358518600464 and perplexity is 87.4754793946423
At time: 179.15798330307007 and batch: 450, loss is 4.4348420238494874 and perplexity is 84.3388003168232
At time: 179.59618163108826 and batch: 500, loss is 4.39744194984436 and perplexity is 81.24277952240581
At time: 180.03466129302979 and batch: 550, loss is 4.411743316650391 and perplexity is 82.41301031716678
At time: 180.47939705848694 and batch: 600, loss is 4.495586338043213 and perplexity is 89.62070160951542
At time: 180.91752767562866 and batch: 650, loss is 4.462639217376709 and perplexity is 86.71606991137263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944861019358916 and perplexity of 140.451329479119
Finished 29 epochs...
Completing Train Step...
At time: 181.82941508293152 and batch: 50, loss is 4.509890623092652 and perplexity is 90.91187430705341
At time: 182.27281618118286 and batch: 100, loss is 4.475844650268555 and perplexity is 87.86878746921779
At time: 182.71077513694763 and batch: 150, loss is 4.452636423110962 and perplexity is 85.85299070272937
At time: 183.14965510368347 and batch: 200, loss is 4.426708526611328 and perplexity is 83.65561303459404
At time: 183.59373140335083 and batch: 250, loss is 4.415888767242432 and perplexity is 82.75535848301726
At time: 184.033207654953 and batch: 300, loss is 4.471395492553711 and perplexity is 87.4787137687211
At time: 184.47229194641113 and batch: 350, loss is 4.418484115600586 and perplexity is 82.97041642120183
At time: 184.91250848770142 and batch: 400, loss is 4.4713169193267825 and perplexity is 87.47184055392161
At time: 185.3561463356018 and batch: 450, loss is 4.4348111343383785 and perplexity is 84.33619517274991
At time: 185.79354572296143 and batch: 500, loss is 4.397509431838989 and perplexity is 81.2482621322038
At time: 186.2315375804901 and batch: 550, loss is 4.411896781921387 and perplexity is 82.42565882265704
At time: 186.67177414894104 and batch: 600, loss is 4.495689973831177 and perplexity is 89.62999000284125
At time: 187.11378836631775 and batch: 650, loss is 4.462426300048828 and perplexity is 86.69760852292728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944757498946845 and perplexity of 140.43679065216108
Finished 30 epochs...
Completing Train Step...
At time: 188.00820350646973 and batch: 50, loss is 4.509260950088501 and perplexity is 90.85464757300106
At time: 188.46349930763245 and batch: 100, loss is 4.475386800765992 and perplexity is 87.82856599697732
At time: 188.89820790290833 and batch: 150, loss is 4.452276773452759 and perplexity is 85.82211925575285
At time: 189.34754180908203 and batch: 200, loss is 4.426458806991577 and perplexity is 83.6347251948772
At time: 189.78276324272156 and batch: 250, loss is 4.415580377578736 and perplexity is 82.72984152063157
At time: 190.2215874195099 and batch: 300, loss is 4.47127067565918 and perplexity is 87.46779562872925
At time: 190.65627646446228 and batch: 350, loss is 4.418521814346313 and perplexity is 82.97354436079267
At time: 191.0907576084137 and batch: 400, loss is 4.471253519058227 and perplexity is 87.46629499153642
At time: 191.52476739883423 and batch: 450, loss is 4.434742317199707 and perplexity is 84.33039159680675
At time: 191.96528697013855 and batch: 500, loss is 4.397539148330688 and perplexity is 81.25067658138535
At time: 192.4011561870575 and batch: 550, loss is 4.412029151916504 and perplexity is 82.43657022886838
At time: 192.8367383480072 and batch: 600, loss is 4.495754041671753 and perplexity is 89.63573258670709
At time: 193.2708854675293 and batch: 650, loss is 4.462224817276001 and perplexity is 86.68014220800443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9446740243949145 and perplexity of 140.4250682432551
Finished 31 epochs...
Completing Train Step...
At time: 194.18665552139282 and batch: 50, loss is 4.508682699203491 and perplexity is 90.80212597942722
At time: 194.62818598747253 and batch: 100, loss is 4.474976224899292 and perplexity is 87.79251310910142
At time: 195.0667963027954 and batch: 150, loss is 4.451948366165161 and perplexity is 85.79393927386114
At time: 195.5061182975769 and batch: 200, loss is 4.426272773742676 and perplexity is 83.61916780236929
At time: 195.94606971740723 and batch: 250, loss is 4.4153420352935795 and perplexity is 82.71012585078516
At time: 196.38785099983215 and batch: 300, loss is 4.4711677360534665 and perplexity is 87.45879219174769
At time: 196.8260452747345 and batch: 350, loss is 4.418573875427246 and perplexity is 82.97786416564678
At time: 197.26770401000977 and batch: 400, loss is 4.471228971481323 and perplexity is 87.46414793228622
At time: 197.7063274383545 and batch: 450, loss is 4.434698438644409 and perplexity is 84.32669138223636
At time: 198.14617586135864 and batch: 500, loss is 4.397588758468628 and perplexity is 81.2547075386456
At time: 198.58992552757263 and batch: 550, loss is 4.412142276763916 and perplexity is 82.44589638079651
At time: 199.02799153327942 and batch: 600, loss is 4.495818328857422 and perplexity is 89.64149520091962
At time: 199.46608924865723 and batch: 650, loss is 4.46201644897461 and perplexity is 86.66208269558852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944622264188879 and perplexity of 140.41780000089472
Finished 32 epochs...
Completing Train Step...
At time: 200.37317657470703 and batch: 50, loss is 4.508143377304077 and perplexity is 90.7531676077209
At time: 200.82532143592834 and batch: 100, loss is 4.47459508895874 and perplexity is 87.75905860280649
At time: 201.26669192314148 and batch: 150, loss is 4.45166205406189 and perplexity is 85.76937894678694
At time: 201.70783257484436 and batch: 200, loss is 4.426096801757812 and perplexity is 83.60445446604392
At time: 202.14602375030518 and batch: 250, loss is 4.415131273269654 and perplexity is 82.69269553415062
At time: 202.5841100215912 and batch: 300, loss is 4.471071748733521 and perplexity is 87.45039765957029
At time: 203.0266706943512 and batch: 350, loss is 4.418621063232422 and perplexity is 82.98177980131936
At time: 203.46569561958313 and batch: 400, loss is 4.471206111907959 and perplexity is 87.4621485620323
At time: 203.90473747253418 and batch: 450, loss is 4.434651575088501 and perplexity is 84.32273962621763
At time: 204.34443831443787 and batch: 500, loss is 4.397631502151489 and perplexity is 81.25818073832373
At time: 204.78856348991394 and batch: 550, loss is 4.412235488891602 and perplexity is 82.45358169639387
At time: 205.22726702690125 and batch: 600, loss is 4.495868530273437 and perplexity is 89.64599544387077
At time: 205.66515016555786 and batch: 650, loss is 4.461805696487427 and perplexity is 86.64382037059879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9445639217601105 and perplexity of 140.4096079243755
Finished 33 epochs...
Completing Train Step...
At time: 206.5847806930542 and batch: 50, loss is 4.507624111175537 and perplexity is 90.70605479482707
At time: 207.02306652069092 and batch: 100, loss is 4.474230117797852 and perplexity is 87.72703492152533
At time: 207.4618022441864 and batch: 150, loss is 4.451394243240356 and perplexity is 85.7464120544772
At time: 207.9015474319458 and batch: 200, loss is 4.42593487739563 and perplexity is 83.59091796405318
At time: 208.3456633090973 and batch: 250, loss is 4.4149273490905765 and perplexity is 82.67583421337207
At time: 208.78502082824707 and batch: 300, loss is 4.4709735774993895 and perplexity is 87.44181296749858
At time: 209.22385334968567 and batch: 350, loss is 4.418661403656006 and perplexity is 82.98512738898738
At time: 209.66371893882751 and batch: 400, loss is 4.471173858642578 and perplexity is 87.4593276676357
At time: 210.1082649230957 and batch: 450, loss is 4.43459267616272 and perplexity is 84.31777325369318
At time: 210.5463285446167 and batch: 500, loss is 4.397663974761963 and perplexity is 81.26081944641733
At time: 210.9844172000885 and batch: 550, loss is 4.412311019897461 and perplexity is 82.45980973355812
At time: 211.42588019371033 and batch: 600, loss is 4.495904769897461 and perplexity is 89.64924423990809
At time: 211.88468265533447 and batch: 650, loss is 4.461603403091431 and perplexity is 86.62629467066039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9445351993336395 and perplexity of 140.40557507765294
Finished 34 epochs...
Completing Train Step...
At time: 212.77986073493958 and batch: 50, loss is 4.507142505645752 and perplexity is 90.66238077492179
At time: 213.23549914360046 and batch: 100, loss is 4.473891878128052 and perplexity is 87.69736717588806
At time: 213.67837953567505 and batch: 150, loss is 4.451136283874511 and perplexity is 85.72429581706808
At time: 214.11694025993347 and batch: 200, loss is 4.425773668289184 and perplexity is 83.57744343299898
At time: 214.5562722682953 and batch: 250, loss is 4.41473123550415 and perplexity is 82.65962194878833
At time: 214.99668335914612 and batch: 300, loss is 4.470877513885498 and perplexity is 87.43341339439286
At time: 215.439293384552 and batch: 350, loss is 4.418691101074219 and perplexity is 82.98759186961513
At time: 215.8775508403778 and batch: 400, loss is 4.471130752563477 and perplexity is 87.45555772019355
At time: 216.31478095054626 and batch: 450, loss is 4.434524507522583 and perplexity is 84.31202562165743
At time: 216.75416803359985 and batch: 500, loss is 4.397692060470581 and perplexity is 81.26310174616417
At time: 217.19689989089966 and batch: 550, loss is 4.412380018234253 and perplexity is 82.46549951957252
At time: 217.6372673511505 and batch: 600, loss is 4.495927324295044 and perplexity is 89.65126624740822
At time: 218.07541179656982 and batch: 650, loss is 4.461388750076294 and perplexity is 86.60770207086938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9444750617532165 and perplexity of 140.39713167997516
Finished 35 epochs...
Completing Train Step...
At time: 218.98723077774048 and batch: 50, loss is 4.506651477813721 and perplexity is 90.61787395058208
At time: 219.42875957489014 and batch: 100, loss is 4.473549995422363 and perplexity is 87.66739008733225
At time: 219.87108182907104 and batch: 150, loss is 4.450881576538086 and perplexity is 85.70246399049476
At time: 220.3086290359497 and batch: 200, loss is 4.42561182975769 and perplexity is 83.56391847674678
At time: 220.747572183609 and batch: 250, loss is 4.414531679153442 and perplexity is 82.64312834203805
At time: 221.1875250339508 and batch: 300, loss is 4.4707784080505375 and perplexity is 87.42474866232477
At time: 221.6311068534851 and batch: 350, loss is 4.418710441589355 and perplexity is 82.98919690791287
At time: 222.06877255439758 and batch: 400, loss is 4.471082067489624 and perplexity is 87.45130004355056
At time: 222.50697541236877 and batch: 450, loss is 4.434446887969971 and perplexity is 84.30548161392363
At time: 222.94577264785767 and batch: 500, loss is 4.397722873687744 and perplexity is 81.26560576234387
At time: 223.40093183517456 and batch: 550, loss is 4.412435283660889 and perplexity is 82.47005713652437
At time: 223.84457921981812 and batch: 600, loss is 4.495935144424439 and perplexity is 89.65196733465196
At time: 224.28300046920776 and batch: 650, loss is 4.46118016242981 and perplexity is 86.5896386580949
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944434072457108 and perplexity of 140.39137701831254
Finished 36 epochs...
Completing Train Step...
At time: 225.17923164367676 and batch: 50, loss is 4.506175308227539 and perplexity is 90.57473474664594
At time: 225.63492369651794 and batch: 100, loss is 4.473221797943115 and perplexity is 87.63862259186125
At time: 226.07837438583374 and batch: 150, loss is 4.4506430912017825 and perplexity is 85.68202764652904
At time: 226.51624178886414 and batch: 200, loss is 4.425457038879395 and perplexity is 83.55098454546507
At time: 226.95416975021362 and batch: 250, loss is 4.4143595790863035 and perplexity is 82.62890667791162
At time: 227.3928723335266 and batch: 300, loss is 4.4706712913513185 and perplexity is 87.41538451335533
At time: 227.83450078964233 and batch: 350, loss is 4.418733739852906 and perplexity is 82.99113043461809
At time: 228.27797031402588 and batch: 400, loss is 4.47104284286499 and perplexity is 87.44786986640678
At time: 228.71614050865173 and batch: 450, loss is 4.434367647171021 and perplexity is 84.29880144487913
At time: 229.15384602546692 and batch: 500, loss is 4.397742385864258 and perplexity is 81.267191446658
At time: 229.5927107334137 and batch: 550, loss is 4.412467527389526 and perplexity is 82.47271632153817
At time: 230.0342879295349 and batch: 600, loss is 4.495960083007812 and perplexity is 89.65420315559292
At time: 230.47775769233704 and batch: 650, loss is 4.460966596603393 and perplexity is 86.5711480449076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944385304170496 and perplexity of 140.3845305383472
Finished 37 epochs...
Completing Train Step...
At time: 231.383727312088 and batch: 50, loss is 4.505701265335083 and perplexity is 90.53180861262139
At time: 231.8191659450531 and batch: 100, loss is 4.472898778915405 and perplexity is 87.6103182208731
At time: 232.2561275959015 and batch: 150, loss is 4.450404081344605 and perplexity is 85.66155124446696
At time: 232.69494342803955 and batch: 200, loss is 4.425294008255005 and perplexity is 83.53736428657606
At time: 233.13484263420105 and batch: 250, loss is 4.414180917739868 and perplexity is 82.61414540486378
At time: 233.5720453262329 and batch: 300, loss is 4.470538940429687 and perplexity is 87.40381577223391
At time: 234.00786590576172 and batch: 350, loss is 4.41876748085022 and perplexity is 82.99393068536855
At time: 234.44854378700256 and batch: 400, loss is 4.471004552841187 and perplexity is 87.44452154949204
At time: 234.903174161911 and batch: 450, loss is 4.434271259307861 and perplexity is 84.29067645512245
At time: 235.34767174720764 and batch: 500, loss is 4.397752456665039 and perplexity is 81.26800987647421
At time: 235.78748893737793 and batch: 550, loss is 4.412500553131103 and perplexity is 82.47544008913154
At time: 236.22978353500366 and batch: 600, loss is 4.49597692489624 and perplexity is 89.6557131143948
At time: 236.672283411026 and batch: 650, loss is 4.460763778686523 and perplexity is 86.55359164543646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944366155886183 and perplexity of 140.38184244117951
Finished 38 epochs...
Completing Train Step...
At time: 237.5730118751526 and batch: 50, loss is 4.5052710628509525 and perplexity is 90.4928699800122
At time: 238.0270426273346 and batch: 100, loss is 4.472598810195922 and perplexity is 87.58404180715105
At time: 238.46645164489746 and batch: 150, loss is 4.45017255783081 and perplexity is 85.64172087681256
At time: 238.90534949302673 and batch: 200, loss is 4.425148162841797 and perplexity is 83.52518163357702
At time: 239.3450973033905 and batch: 250, loss is 4.4140167331695555 and perplexity is 82.60058255033492
At time: 239.78501224517822 and batch: 300, loss is 4.470438280105591 and perplexity is 87.39501811860578
At time: 240.22416257858276 and batch: 350, loss is 4.41878493309021 and perplexity is 82.99537912800385
At time: 240.67987966537476 and batch: 400, loss is 4.470971345901489 and perplexity is 87.4416178327501
At time: 241.13533782958984 and batch: 450, loss is 4.434189014434814 and perplexity is 84.28374426421084
At time: 241.5816764831543 and batch: 500, loss is 4.397766389846802 and perplexity is 81.26914220631579
At time: 242.02529430389404 and batch: 550, loss is 4.412538366317749 and perplexity is 82.47855880730529
At time: 242.47685289382935 and batch: 600, loss is 4.495983657836914 and perplexity is 89.65631676302439
At time: 242.9191460609436 and batch: 650, loss is 4.4605639553070064 and perplexity is 86.53629794214521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944328158509498 and perplexity of 140.37650840077293
Finished 39 epochs...
Completing Train Step...
At time: 243.82811307907104 and batch: 50, loss is 4.504844589233398 and perplexity is 90.45428538662934
At time: 244.27491545677185 and batch: 100, loss is 4.472295932769775 and perplexity is 87.55751859484081
At time: 244.71723675727844 and batch: 150, loss is 4.449947023391724 and perplexity is 85.62240789728509
At time: 245.15881180763245 and batch: 200, loss is 4.425015811920166 and perplexity is 83.51412773032169
At time: 245.60193300247192 and batch: 250, loss is 4.4138539695739745 and perplexity is 82.58713927658941
At time: 246.0432186126709 and batch: 300, loss is 4.470334768295288 and perplexity is 87.38597217025819
At time: 246.49944472312927 and batch: 350, loss is 4.418793106079102 and perplexity is 82.9960574510875
At time: 246.9419069290161 and batch: 400, loss is 4.470924282073975 and perplexity is 87.43750259237109
At time: 247.38425493240356 and batch: 450, loss is 4.434108982086181 and perplexity is 84.27699910812476
At time: 247.8326256275177 and batch: 500, loss is 4.397780380249023 and perplexity is 81.27027920225696
At time: 248.2792489528656 and batch: 550, loss is 4.4125669479370115 and perplexity is 82.4809162117595
At time: 248.73748230934143 and batch: 600, loss is 4.495975046157837 and perplexity is 89.65554467492174
At time: 249.19181180000305 and batch: 650, loss is 4.460371885299683 and perplexity is 86.51967851086393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944297940123315 and perplexity of 140.37226651332293
Finished 40 epochs...
Completing Train Step...
At time: 250.12984490394592 and batch: 50, loss is 4.504415235519409 and perplexity is 90.41545683943932
At time: 250.6095323562622 and batch: 100, loss is 4.472003746032715 and perplexity is 87.53193918634142
At time: 251.05533981323242 and batch: 150, loss is 4.449732398986816 and perplexity is 85.60403321084195
At time: 251.50140500068665 and batch: 200, loss is 4.424872913360596 and perplexity is 83.50219453440383
At time: 251.94577503204346 and batch: 250, loss is 4.413695878982544 and perplexity is 82.57408405887738
At time: 252.39246439933777 and batch: 300, loss is 4.4702303504943846 and perplexity is 87.37684799558578
At time: 252.84368324279785 and batch: 350, loss is 4.418799533843994 and perplexity is 82.99659093194629
At time: 253.29475784301758 and batch: 400, loss is 4.470866813659668 and perplexity is 87.43247784212971
At time: 253.74474096298218 and batch: 450, loss is 4.434021272659302 and perplexity is 84.26960754499346
At time: 254.20181608200073 and batch: 500, loss is 4.397769393920899 and perplexity is 81.2693863452075
At time: 254.67404675483704 and batch: 550, loss is 4.412586936950683 and perplexity is 82.4825649403995
At time: 255.1336932182312 and batch: 600, loss is 4.495973110198975 and perplexity is 89.65537110564347
At time: 255.59185934066772 and batch: 650, loss is 4.460183753967285 and perplexity is 86.50340297948375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944280287798713 and perplexity of 140.3697886383795
Finished 41 epochs...
Completing Train Step...
At time: 256.57275390625 and batch: 50, loss is 4.5040246200561525 and perplexity is 90.38014606079746
At time: 257.02240681648254 and batch: 100, loss is 4.471712074279785 and perplexity is 87.50641231511622
At time: 257.48994398117065 and batch: 150, loss is 4.449524297714233 and perplexity is 85.58622075605408
At time: 257.9587936401367 and batch: 200, loss is 4.424732084274292 and perplexity is 83.49043582464681
At time: 258.43327617645264 and batch: 250, loss is 4.413529090881347 and perplexity is 82.56031283265939
At time: 258.892861366272 and batch: 300, loss is 4.470117044448853 and perplexity is 87.36694823133051
At time: 259.3505871295929 and batch: 350, loss is 4.418808250427246 and perplexity is 82.99731438179383
At time: 259.8021242618561 and batch: 400, loss is 4.470794639587402 and perplexity is 87.42616771187222
At time: 260.2548451423645 and batch: 450, loss is 4.43393840789795 and perplexity is 84.26262485338853
At time: 260.70937991142273 and batch: 500, loss is 4.397759828567505 and perplexity is 81.2686089785249
At time: 261.1566741466522 and batch: 550, loss is 4.412604608535767 and perplexity is 82.48402255094284
At time: 261.6249084472656 and batch: 600, loss is 4.495957155227661 and perplexity is 89.65394066818067
At time: 262.08309268951416 and batch: 650, loss is 4.460013465881348 and perplexity is 86.48867373470637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944280886182598 and perplexity of 140.36987263342417
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 263.04230093955994 and batch: 50, loss is 4.5037260913848876 and perplexity is 90.35316902279521
At time: 263.5233054161072 and batch: 100, loss is 4.471164894104004 and perplexity is 87.45854363863343
At time: 263.992151260376 and batch: 150, loss is 4.449121923446655 and perplexity is 85.55178999065413
At time: 264.4587802886963 and batch: 200, loss is 4.4241557216644285 and perplexity is 83.44232892399741
At time: 264.90980219841003 and batch: 250, loss is 4.413040237426758 and perplexity is 82.51996280194867
At time: 265.36013865470886 and batch: 300, loss is 4.469234018325806 and perplexity is 87.28983498526739
At time: 265.8063049316406 and batch: 350, loss is 4.418027725219726 and perplexity is 82.93255816098129
At time: 266.25270318984985 and batch: 400, loss is 4.469865026473999 and perplexity is 87.34493296420618
At time: 266.69932770729065 and batch: 450, loss is 4.432376699447632 and perplexity is 84.13113390209874
At time: 267.1451222896576 and batch: 500, loss is 4.395979557037354 and perplexity is 81.12405749628772
At time: 267.5918655395508 and batch: 550, loss is 4.410739936828613 and perplexity is 82.330360237208
At time: 268.03842329978943 and batch: 600, loss is 4.49408067703247 and perplexity is 89.48586474812215
At time: 268.4853570461273 and batch: 650, loss is 4.458722820281983 and perplexity is 86.37711951252399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944182152841606 and perplexity of 140.35601413108373
Finished 43 epochs...
Completing Train Step...
At time: 269.4260857105255 and batch: 50, loss is 4.50347333908081 and perplexity is 90.33033493694947
At time: 269.87163519859314 and batch: 100, loss is 4.471007461547852 and perplexity is 87.44477590032459
At time: 270.3327076435089 and batch: 150, loss is 4.448999080657959 and perplexity is 85.54128121567058
At time: 270.7781980037689 and batch: 200, loss is 4.4240732097625735 and perplexity is 83.43544422278148
At time: 271.22375679016113 and batch: 250, loss is 4.412922668457031 and perplexity is 82.51026158523236
At time: 271.66887068748474 and batch: 300, loss is 4.469121961593628 and perplexity is 87.28005411962276
At time: 272.1096615791321 and batch: 350, loss is 4.4180095005035405 and perplexity is 82.93104675241877
At time: 272.55026412010193 and batch: 400, loss is 4.4699015617370605 and perplexity is 87.34812419260491
At time: 272.993013381958 and batch: 450, loss is 4.432383718490601 and perplexity is 84.13172442421508
At time: 273.4359347820282 and batch: 500, loss is 4.395938549041748 and perplexity is 81.12073082950484
At time: 273.8783187866211 and batch: 550, loss is 4.410805654525757 and perplexity is 82.33577097667651
At time: 274.3208749294281 and batch: 600, loss is 4.494132509231568 and perplexity is 89.49050311748758
At time: 274.76355242729187 and batch: 650, loss is 4.458669500350952 and perplexity is 86.37251401325251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.94411423627068 and perplexity of 140.34648195559518
Finished 44 epochs...
Completing Train Step...
At time: 275.66884088516235 and batch: 50, loss is 4.503335361480713 and perplexity is 90.31787223392557
At time: 276.11951899528503 and batch: 100, loss is 4.470897512435913 and perplexity is 87.43516195340283
At time: 276.55650067329407 and batch: 150, loss is 4.448909730911255 and perplexity is 85.53363846530517
At time: 276.99360036849976 and batch: 200, loss is 4.424016590118408 and perplexity is 83.4307202713543
At time: 277.4302752017975 and batch: 250, loss is 4.412893362045288 and perplexity is 82.50784354096561
At time: 277.86696124076843 and batch: 300, loss is 4.469050903320312 and perplexity is 87.2738523700275
At time: 278.3030815124512 and batch: 350, loss is 4.418015985488892 and perplexity is 82.93158456078596
At time: 278.7397382259369 and batch: 400, loss is 4.469945983886719 and perplexity is 87.35200447023468
At time: 279.17523074150085 and batch: 450, loss is 4.432387981414795 and perplexity is 84.13208307214306
At time: 279.6125671863556 and batch: 500, loss is 4.39590518951416 and perplexity is 81.11802472538417
At time: 280.04987025260925 and batch: 550, loss is 4.410860042572022 and perplexity is 82.34024918017691
At time: 280.486798286438 and batch: 600, loss is 4.494151678085327 and perplexity is 89.49221856429621
At time: 280.925523519516 and batch: 650, loss is 4.458617572784424 and perplexity is 86.36802901523339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944066664751838 and perplexity of 140.33980561908737
Finished 45 epochs...
Completing Train Step...
At time: 281.8538405895233 and batch: 50, loss is 4.5032197380065915 and perplexity is 90.30742997145961
At time: 282.2986834049225 and batch: 100, loss is 4.470796670913696 and perplexity is 87.42634530312584
At time: 282.74438214302063 and batch: 150, loss is 4.448841857910156 and perplexity is 85.527833237579
At time: 283.1889281272888 and batch: 200, loss is 4.42397292137146 and perplexity is 83.42707703589141
At time: 283.6335406303406 and batch: 250, loss is 4.412885236740112 and perplexity is 82.507173142281
At time: 284.0786747932434 and batch: 300, loss is 4.468995180130005 and perplexity is 87.2689893280364
At time: 284.52524733543396 and batch: 350, loss is 4.418030748367309 and perplexity is 82.93280887872301
At time: 284.9698169231415 and batch: 400, loss is 4.4699916076660156 and perplexity is 87.35598988972201
At time: 285.4166557788849 and batch: 450, loss is 4.4323869228363035 and perplexity is 84.13199401177661
At time: 285.86107110977173 and batch: 500, loss is 4.395874156951904 and perplexity is 81.11550746429054
At time: 286.3059697151184 and batch: 550, loss is 4.410902538299561 and perplexity is 82.34374836332118
At time: 286.7438759803772 and batch: 600, loss is 4.494154977798462 and perplexity is 89.49251386343246
At time: 287.184130191803 and batch: 650, loss is 4.45856556892395 and perplexity is 86.36353766108796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9440331552542895 and perplexity of 140.335102981507
Finished 46 epochs...
Completing Train Step...
At time: 288.1005735397339 and batch: 50, loss is 4.503116426467895 and perplexity is 90.29810065383492
At time: 288.55731987953186 and batch: 100, loss is 4.470700616836548 and perplexity is 87.417948049511
At time: 288.99794340133667 and batch: 150, loss is 4.448784627914429 and perplexity is 85.52293862010904
At time: 289.43856024742126 and batch: 200, loss is 4.42393648147583 and perplexity is 83.42403701730083
At time: 289.8797233104706 and batch: 250, loss is 4.412884750366211 and perplexity is 82.5071330129551
At time: 290.3210744857788 and batch: 300, loss is 4.468946886062622 and perplexity is 87.26477485535322
At time: 290.762455701828 and batch: 350, loss is 4.418048877716064 and perplexity is 82.93431241016737
At time: 291.2038357257843 and batch: 400, loss is 4.470036468505859 and perplexity is 87.3599088406969
At time: 291.64546513557434 and batch: 450, loss is 4.432382230758667 and perplexity is 84.13159925885509
At time: 292.08603835105896 and batch: 500, loss is 4.395843410491944 and perplexity is 81.11301348792881
At time: 292.5262908935547 and batch: 550, loss is 4.410936260223389 and perplexity is 82.34652519975111
At time: 292.9835247993469 and batch: 600, loss is 4.49414924621582 and perplexity is 89.49200093116343
At time: 293.42474031448364 and batch: 650, loss is 4.458513689041138 and perplexity is 86.35905724709757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944005629595588 and perplexity of 140.33124021852126
Finished 47 epochs...
Completing Train Step...
At time: 294.35234689712524 and batch: 50, loss is 4.503020124435425 and perplexity is 90.28940518191628
At time: 294.80040764808655 and batch: 100, loss is 4.470607471466065 and perplexity is 87.40980585156294
At time: 295.24675583839417 and batch: 150, loss is 4.448733463287353 and perplexity is 85.51856298278803
At time: 295.6946611404419 and batch: 200, loss is 4.423905220031738 and perplexity is 83.42142910219549
At time: 296.14282298088074 and batch: 250, loss is 4.412887077331543 and perplexity is 82.50732500441667
At time: 296.5889239311218 and batch: 300, loss is 4.468902368545532 and perplexity is 87.26089013071699
At time: 297.0361578464508 and batch: 350, loss is 4.41806851387024 and perplexity is 82.93594093710124
At time: 297.4831557273865 and batch: 400, loss is 4.4700800228118895 and perplexity is 87.36371382376242
At time: 297.93069767951965 and batch: 450, loss is 4.432375030517578 and perplexity is 84.13099349323808
At time: 298.37872314453125 and batch: 500, loss is 4.395812168121338 and perplexity is 81.11047936468665
At time: 298.82427287101746 and batch: 550, loss is 4.410963821411133 and perplexity is 82.34879479906846
At time: 299.27150440216064 and batch: 600, loss is 4.49413803100586 and perplexity is 89.49099726521135
At time: 299.7182228565216 and batch: 650, loss is 4.458460903167724 and perplexity is 86.35449882914469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.943980796664369 and perplexity of 140.32775542575402
Finished 48 epochs...
Completing Train Step...
At time: 300.6213529109955 and batch: 50, loss is 4.502927827835083 and perplexity is 90.28107216133174
At time: 301.0783727169037 and batch: 100, loss is 4.470516548156739 and perplexity is 87.40185862404695
At time: 301.52104091644287 and batch: 150, loss is 4.448686580657959 and perplexity is 85.51455374167602
At time: 301.96457600593567 and batch: 200, loss is 4.423879270553589 and perplexity is 83.41926438773055
At time: 302.4076805114746 and batch: 250, loss is 4.412889785766602 and perplexity is 82.50754847045091
At time: 302.84980964660645 and batch: 300, loss is 4.468859853744507 and perplexity is 87.25718033019713
At time: 303.2918105125427 and batch: 350, loss is 4.418088302612305 and perplexity is 82.93758215128312
At time: 303.73505902290344 and batch: 400, loss is 4.470121660232544 and perplexity is 87.36735149919602
At time: 304.1779577732086 and batch: 450, loss is 4.432365341186523 and perplexity is 84.13017832413938
At time: 304.63477396965027 and batch: 500, loss is 4.395779991149903 and perplexity is 81.10786951709778
At time: 305.07761335372925 and batch: 550, loss is 4.410986661911011 and perplexity is 82.3506757081864
At time: 305.51985144615173 and batch: 600, loss is 4.494122991561889 and perplexity is 89.48965138049284
At time: 305.96259331703186 and batch: 650, loss is 4.458407802581787 and perplexity is 86.34991347640208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.943957459692862 and perplexity of 140.3244806391359
Finished 49 epochs...
Completing Train Step...
At time: 306.88553619384766 and batch: 50, loss is 4.50283821105957 and perplexity is 90.27298182527517
At time: 307.32772302627563 and batch: 100, loss is 4.470427379608155 and perplexity is 87.39406547462698
At time: 307.77257776260376 and batch: 150, loss is 4.448644647598266 and perplexity is 85.51096792997191
At time: 308.2196834087372 and batch: 200, loss is 4.423860988616943 and perplexity is 83.4177393359645
At time: 308.666086435318 and batch: 250, loss is 4.412891416549683 and perplexity is 82.50768302247472
At time: 309.11282205581665 and batch: 300, loss is 4.468817281723022 and perplexity is 87.25346569471174
At time: 309.5596706867218 and batch: 350, loss is 4.418107776641846 and perplexity is 82.93919729593463
At time: 310.0050904750824 and batch: 400, loss is 4.470159320831299 and perplexity is 87.37064186792337
At time: 310.45282340049744 and batch: 450, loss is 4.432352361679077 and perplexity is 84.1290863629499
At time: 310.8997411727905 and batch: 500, loss is 4.395746068954468 and perplexity is 81.10511820676223
At time: 311.3473300933838 and batch: 550, loss is 4.411005411148071 and perplexity is 82.35221973500198
At time: 311.79298186302185 and batch: 600, loss is 4.494104814529419 and perplexity is 89.48802473897776
At time: 312.24127554893494 and batch: 650, loss is 4.458357639312744 and perplexity is 86.34558199110221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9439416025199145 and perplexity of 140.32225550721992
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f93af975898>
SETTINGS FOR THIS RUN
{'lr': 21.899240546890415, 'data': 'ptb', 'num_layers': 1, 'anneal': 6.033026529213739, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.715177067076766, 'seq_len': 20, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7253055572509766 and batch: 50, loss is 6.600277919769287 and perplexity is 735.2995151191559
At time: 1.1746277809143066 and batch: 100, loss is 6.106792440414429 and perplexity is 448.8965410711811
At time: 1.623716115951538 and batch: 150, loss is 6.026568765640259 and perplexity is 414.29105807602394
At time: 2.087916612625122 and batch: 200, loss is 5.9848541736602785 and perplexity is 397.36457080320844
At time: 2.537285089492798 and batch: 250, loss is 5.95060393333435 and perplexity is 383.9851704930575
At time: 2.9871935844421387 and batch: 300, loss is 5.945286531448364 and perplexity is 381.94878595895653
At time: 3.43623685836792 and batch: 350, loss is 5.862357978820801 and perplexity is 351.55211991463455
At time: 3.8900222778320312 and batch: 400, loss is 5.893222169876099 and perplexity is 362.57167173260535
At time: 4.34417724609375 and batch: 450, loss is 5.858523321151734 and perplexity is 350.2066192978102
At time: 4.7978596687316895 and batch: 500, loss is 5.864120092391968 and perplexity is 352.1721407892748
At time: 5.2513017654418945 and batch: 550, loss is 5.853954839706421 and perplexity is 348.61035588172473
At time: 5.702938556671143 and batch: 600, loss is 5.879429779052734 and perplexity is 357.60526954423227
At time: 6.154330253601074 and batch: 650, loss is 5.848386154174805 and perplexity is 346.6744496659653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.631405699486826 and perplexity of 279.05410827476896
Finished 1 epochs...
Completing Train Step...
At time: 7.074984073638916 and batch: 50, loss is 5.516983995437622 and perplexity is 248.8832710135652
At time: 7.510802507400513 and batch: 100, loss is 5.406598424911499 and perplexity is 222.8721803656323
At time: 7.970713376998901 and batch: 150, loss is 5.346975870132447 and perplexity is 209.972353097189
At time: 8.409065008163452 and batch: 200, loss is 5.295242490768433 and perplexity is 199.38596936184564
At time: 8.848977088928223 and batch: 250, loss is 5.271766414642334 and perplexity is 194.75968513718348
At time: 9.308371782302856 and batch: 300, loss is 5.30820297241211 and perplexity is 201.9869259753154
At time: 9.768057823181152 and batch: 350, loss is 5.232211303710938 and perplexity is 187.20631617229807
At time: 10.204632043838501 and batch: 400, loss is 5.253794946670532 and perplexity is 191.29083125364312
At time: 10.640695810317993 and batch: 450, loss is 5.232643632888794 and perplexity is 187.28726842280744
At time: 11.07904863357544 and batch: 500, loss is 5.22787766456604 and perplexity is 186.39678692312503
At time: 11.530506372451782 and batch: 550, loss is 5.235534944534302 and perplexity is 187.82955786916878
At time: 11.977450132369995 and batch: 600, loss is 5.275753974914551 and perplexity is 195.53785158195453
At time: 12.416650772094727 and batch: 650, loss is 5.22649845123291 and perplexity is 186.13988319259747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.358326033049939 and perplexity of 212.36914979873728
Finished 2 epochs...
Completing Train Step...
At time: 13.354772329330444 and batch: 50, loss is 5.231815376281738 and perplexity is 187.13221072796318
At time: 13.795152425765991 and batch: 100, loss is 5.171198558807373 and perplexity is 176.12580816700884
At time: 14.22908353805542 and batch: 150, loss is 5.1268998146057125 and perplexity is 168.4939450298343
At time: 14.676748752593994 and batch: 200, loss is 5.117255096435547 and perplexity is 166.87667998597877
At time: 15.119802236557007 and batch: 250, loss is 5.109774341583252 and perplexity is 165.63297418153937
At time: 15.606120824813843 and batch: 300, loss is 5.120449275970459 and perplexity is 167.41056627244205
At time: 16.050644159317017 and batch: 350, loss is 5.145217819213867 and perplexity is 171.60886030739334
At time: 16.49353814125061 and batch: 400, loss is 5.154751520156861 and perplexity is 173.2527515933969
At time: 16.936705827713013 and batch: 450, loss is 5.10129599571228 and perplexity is 164.23461679400626
At time: 17.38056206703186 and batch: 500, loss is 5.121019163131714 and perplexity is 167.50599859506409
At time: 17.825100421905518 and batch: 550, loss is 5.13022521018982 and perplexity is 169.05518670868943
At time: 18.25405216217041 and batch: 600, loss is 5.198228168487549 and perplexity is 180.9513423785366
At time: 18.679194688796997 and batch: 650, loss is 5.163195381164551 and perplexity is 174.72186752357217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.312705245672488 and perplexity of 202.89837657127651
Finished 3 epochs...
Completing Train Step...
At time: 19.667328596115112 and batch: 50, loss is 5.156248168945313 and perplexity is 173.51224425041275
At time: 20.115076303482056 and batch: 100, loss is 5.126151924133301 and perplexity is 168.36797712465702
At time: 20.560723066329956 and batch: 150, loss is 5.056233396530152 and perplexity is 156.99805182763671
At time: 21.008265256881714 and batch: 200, loss is 4.994369440078735 and perplexity is 147.5798580943
At time: 21.454627990722656 and batch: 250, loss is 5.042634363174439 and perplexity is 154.87748161687404
At time: 21.90165400505066 and batch: 300, loss is 5.134924573898315 and perplexity is 169.8515081543947
At time: 22.349401473999023 and batch: 350, loss is 5.126021604537964 and perplexity is 168.34603690766167
At time: 22.79573917388916 and batch: 400, loss is 5.155478477478027 and perplexity is 173.3787447398515
At time: 23.237382888793945 and batch: 450, loss is 5.109444904327392 and perplexity is 165.57841749604088
At time: 23.67801856994629 and batch: 500, loss is 5.061592102050781 and perplexity is 157.8416163429767
At time: 24.12286877632141 and batch: 550, loss is 5.083188514709473 and perplexity is 161.28750450500328
At time: 24.5676851272583 and batch: 600, loss is 5.162914619445801 and perplexity is 174.6728191975115
At time: 25.010661602020264 and batch: 650, loss is 5.12370044708252 and perplexity is 167.95573240334326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.317710128484988 and perplexity of 203.91640459931713
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 25.937695264816284 and batch: 50, loss is 5.038797187805176 and perplexity is 154.28432830359696
At time: 26.39600682258606 and batch: 100, loss is 4.945352239608765 and perplexity is 140.52033896431797
At time: 26.837936639785767 and batch: 150, loss is 4.898913230895996 and perplexity is 134.14391697528967
At time: 27.284024715423584 and batch: 200, loss is 4.8625507640838626 and perplexity is 129.35373250654482
At time: 27.72864079475403 and batch: 250, loss is 4.836913232803345 and perplexity is 126.0795721726771
At time: 28.170761108398438 and batch: 300, loss is 4.881211442947388 and perplexity is 131.7902235544582
At time: 28.6135892868042 and batch: 350, loss is 4.823010540008545 and perplexity is 124.33885497902085
At time: 29.058987140655518 and batch: 400, loss is 4.847943124771118 and perplexity is 127.47791383516814
At time: 29.500093698501587 and batch: 450, loss is 4.827361288070679 and perplexity is 124.88100052552223
At time: 29.942821502685547 and batch: 500, loss is 4.785432815551758 and perplexity is 119.75318290877229
At time: 30.394874811172485 and batch: 550, loss is 4.785087852478028 and perplexity is 119.71187960717363
At time: 30.842280387878418 and batch: 600, loss is 4.842109317779541 and perplexity is 126.736397325285
At time: 31.287986993789673 and batch: 650, loss is 4.781195220947265 and perplexity is 119.24679116624681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0687704647288605 and perplexity of 158.97873716072309
Finished 5 epochs...
Completing Train Step...
At time: 32.21729755401611 and batch: 50, loss is 4.8512105464935305 and perplexity is 127.8951191616591
At time: 32.661951780319214 and batch: 100, loss is 4.821384716033935 and perplexity is 124.13686613126929
At time: 33.10717439651489 and batch: 150, loss is 4.805158777236938 and perplexity is 122.13888235595329
At time: 33.55598282814026 and batch: 200, loss is 4.779230461120606 and perplexity is 119.01272987391259
At time: 34.003275871276855 and batch: 250, loss is 4.762476072311402 and perplexity is 117.03535542992842
At time: 34.44705080986023 and batch: 300, loss is 4.810947999954224 and perplexity is 122.84802225217729
At time: 34.89313220977783 and batch: 350, loss is 4.758109741210937 and perplexity is 116.52545432628294
At time: 35.3410427570343 and batch: 400, loss is 4.790020685195923 and perplexity is 120.3038571463334
At time: 35.78660249710083 and batch: 450, loss is 4.766944913864136 and perplexity is 117.55953826207237
At time: 36.22927951812744 and batch: 500, loss is 4.735019044876099 and perplexity is 113.86562725432324
At time: 36.668726682662964 and batch: 550, loss is 4.744729480743408 and perplexity is 114.97669788485702
At time: 37.1126914024353 and batch: 600, loss is 4.8148377418518065 and perplexity is 123.32679990845901
At time: 37.58283758163452 and batch: 650, loss is 4.767532253265381 and perplexity is 117.62860589197057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.056557449640012 and perplexity of 157.04893577867952
Finished 6 epochs...
Completing Train Step...
At time: 38.496015310287476 and batch: 50, loss is 4.801542634963989 and perplexity is 121.69800839220721
At time: 38.95441198348999 and batch: 100, loss is 4.770412473678589 and perplexity is 117.96789057662568
At time: 39.39571928977966 and batch: 150, loss is 4.752788515090942 and perplexity is 115.90704284794913
At time: 39.8378164768219 and batch: 200, loss is 4.729031114578247 and perplexity is 113.18584509026006
At time: 40.28230547904968 and batch: 250, loss is 4.714212417602539 and perplexity is 111.52094464423392
At time: 40.72319030761719 and batch: 300, loss is 4.766543836593628 and perplexity is 117.51239725756864
At time: 41.16877293586731 and batch: 350, loss is 4.720485401153565 and perplexity is 112.22271248340685
At time: 41.620347023010254 and batch: 400, loss is 4.759416399002075 and perplexity is 116.67781273751929
At time: 42.068119764328 and batch: 450, loss is 4.73501298904419 and perplexity is 113.8649377053123
At time: 42.51084804534912 and batch: 500, loss is 4.702462892532349 and perplexity is 110.21829425699458
At time: 42.957889795303345 and batch: 550, loss is 4.710910873413086 and perplexity is 111.15336044906813
At time: 43.40924000740051 and batch: 600, loss is 4.782873115539551 and perplexity is 119.44704266576495
At time: 43.85776972770691 and batch: 650, loss is 4.7298742866516115 and perplexity is 113.28132047937476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.042275821461397 and perplexity of 154.82196153303127
Finished 7 epochs...
Completing Train Step...
At time: 44.77568864822388 and batch: 50, loss is 4.751407804489136 and perplexity is 115.74711919461284
At time: 45.22532606124878 and batch: 100, loss is 4.723911876678467 and perplexity is 112.60790040269266
At time: 45.67154860496521 and batch: 150, loss is 4.715171556472779 and perplexity is 111.6279600301922
At time: 46.11665201187134 and batch: 200, loss is 4.693504972457886 and perplexity is 109.2353766079554
At time: 46.56323170661926 and batch: 250, loss is 4.676256561279297 and perplexity is 107.3673960764319
At time: 47.008179903030396 and batch: 300, loss is 4.731636037826538 and perplexity is 113.48106988153388
At time: 47.455626249313354 and batch: 350, loss is 4.685340013504028 and perplexity is 108.34710552493424
At time: 47.901320934295654 and batch: 400, loss is 4.727521762847901 and perplexity is 113.01513670103184
At time: 48.35011291503906 and batch: 450, loss is 4.706826620101928 and perplexity is 110.70030778930612
At time: 48.79562425613403 and batch: 500, loss is 4.674296779632568 and perplexity is 107.15718547486411
At time: 49.25576734542847 and batch: 550, loss is 4.684462604522705 and perplexity is 108.25208249457249
At time: 49.7030565738678 and batch: 600, loss is 4.7598654079437255 and perplexity is 116.73021388214154
At time: 50.15037393569946 and batch: 650, loss is 4.713338441848755 and perplexity is 111.42352062189377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.038896149280024 and perplexity of 154.29959726377885
Finished 8 epochs...
Completing Train Step...
At time: 51.0735080242157 and batch: 50, loss is 4.721158800125122 and perplexity is 112.29830859289302
At time: 51.525071144104004 and batch: 100, loss is 4.694240293502808 and perplexity is 109.31572921808728
At time: 51.96279001235962 and batch: 150, loss is 4.68592661857605 and perplexity is 108.41068113163769
At time: 52.39989519119263 and batch: 200, loss is 4.6648776149749756 and perplexity is 106.15259293921767
At time: 52.83953547477722 and batch: 250, loss is 4.646729698181153 and perplexity is 104.2435197610247
At time: 53.278685092926025 and batch: 300, loss is 4.701755590438843 and perplexity is 110.14036419002258
At time: 53.71961307525635 and batch: 350, loss is 4.657407131195068 and perplexity is 105.36253644188882
At time: 54.161094665527344 and batch: 400, loss is 4.702295951843261 and perplexity is 110.19989587476329
At time: 54.60271692276001 and batch: 450, loss is 4.6766695880889895 and perplexity is 107.41175084872282
At time: 55.04013252258301 and batch: 500, loss is 4.64754150390625 and perplexity is 104.32817960619512
At time: 55.476722955703735 and batch: 550, loss is 4.659203329086304 and perplexity is 105.55195847646237
At time: 55.91466569900513 and batch: 600, loss is 4.735132331848145 and perplexity is 113.87852747715479
At time: 56.35521197319031 and batch: 650, loss is 4.692351217269898 and perplexity is 109.10941840192143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.037197935814951 and perplexity of 154.03778597869535
Finished 9 epochs...
Completing Train Step...
At time: 57.27451515197754 and batch: 50, loss is 4.7029172229766845 and perplexity is 110.26838116073931
At time: 57.71906638145447 and batch: 100, loss is 4.674784393310547 and perplexity is 107.20944952549274
At time: 58.161439180374146 and batch: 150, loss is 4.663948097229004 and perplexity is 106.05396806418655
At time: 58.60284662246704 and batch: 200, loss is 4.644878101348877 and perplexity is 104.05068137461676
At time: 59.04505252838135 and batch: 250, loss is 4.624316005706787 and perplexity is 101.93302762348648
At time: 59.487979888916016 and batch: 300, loss is 4.67670503616333 and perplexity is 107.41555845593776
At time: 59.93457579612732 and batch: 350, loss is 4.633053903579712 and perplexity is 102.82761070469806
At time: 60.38050580024719 and batch: 400, loss is 4.680306844711303 and perplexity is 107.80314632219907
At time: 60.84287667274475 and batch: 450, loss is 4.659897975921631 and perplexity is 105.62530528249458
At time: 61.28792667388916 and batch: 500, loss is 4.628248405456543 and perplexity is 102.33465820424253
At time: 61.734370946884155 and batch: 550, loss is 4.637824811935425 and perplexity is 103.31936393384626
At time: 62.17968487739563 and batch: 600, loss is 4.715786113739013 and perplexity is 111.69658288832196
At time: 62.62911343574524 and batch: 650, loss is 4.669402456283569 and perplexity is 106.63400491177694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.033938538794424 and perplexity of 153.5365330130014
Finished 10 epochs...
Completing Train Step...
At time: 63.538888692855835 and batch: 50, loss is 4.684002676010132 and perplexity is 108.20230572304355
At time: 63.99770140647888 and batch: 100, loss is 4.653194694519043 and perplexity is 104.91963692710546
At time: 64.44644093513489 and batch: 150, loss is 4.644116916656494 and perplexity is 103.97150972466908
At time: 64.89190912246704 and batch: 200, loss is 4.625266904830933 and perplexity is 102.03000174917366
At time: 65.3382637500763 and batch: 250, loss is 4.601020355224609 and perplexity is 99.58587678857751
At time: 65.78065061569214 and batch: 300, loss is 4.657379846572876 and perplexity is 105.35966170410707
At time: 66.21850943565369 and batch: 350, loss is 4.613170766830445 and perplexity is 100.80326710121723
At time: 66.65762448310852 and batch: 400, loss is 4.6602766227722165 and perplexity is 105.66530754456932
At time: 67.09927701950073 and batch: 450, loss is 4.641342973709106 and perplexity is 103.68349833679962
At time: 67.54455018043518 and batch: 500, loss is 4.610807075500488 and perplexity is 100.56528066677787
At time: 67.9858512878418 and batch: 550, loss is 4.619939441680908 and perplexity is 101.48788600766329
At time: 68.42771434783936 and batch: 600, loss is 4.696573362350464 and perplexity is 109.57106808624239
At time: 68.87053155899048 and batch: 650, loss is 4.645324640274048 and perplexity is 104.09715442928275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0347146426930145 and perplexity of 153.65573956720098
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 69.79036140441895 and batch: 50, loss is 4.6484103107452395 and perplexity is 104.41886002831772
At time: 70.23312783241272 and batch: 100, loss is 4.599348363876342 and perplexity is 99.41950918553141
At time: 70.67796635627747 and batch: 150, loss is 4.575121765136719 and perplexity is 97.03985448871876
At time: 71.11965441703796 and batch: 200, loss is 4.546991662979126 and perplexity is 94.34814992451585
At time: 71.56103849411011 and batch: 250, loss is 4.517649307250976 and perplexity is 91.61997423576149
At time: 72.00325775146484 and batch: 300, loss is 4.566346206665039 and perplexity is 96.1920012057215
At time: 72.46391415596008 and batch: 350, loss is 4.508215923309326 and perplexity is 90.75975162631372
At time: 72.90627455711365 and batch: 400, loss is 4.5458589458465575 and perplexity is 94.24134066243086
At time: 73.3485004901886 and batch: 450, loss is 4.521655855178833 and perplexity is 91.98779039819941
At time: 73.79224801063538 and batch: 500, loss is 4.471796426773071 and perplexity is 87.51379401050137
At time: 74.23561096191406 and batch: 550, loss is 4.467733411788941 and perplexity is 87.15894551964209
At time: 74.68223094940186 and batch: 600, loss is 4.54486424446106 and perplexity is 94.14764527749283
At time: 75.12889504432678 and batch: 650, loss is 4.530499715805053 and perplexity is 92.80492558471101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.959282669366575 and perplexity of 142.4915456770478
Finished 12 epochs...
Completing Train Step...
At time: 76.04484724998474 and batch: 50, loss is 4.594217643737793 and perplexity is 98.91072184643362
At time: 76.50443005561829 and batch: 100, loss is 4.561116275787353 and perplexity is 95.69023692836794
At time: 76.95201992988586 and batch: 150, loss is 4.545591650009155 and perplexity is 94.2161537106942
At time: 77.39878511428833 and batch: 200, loss is 4.524449272155762 and perplexity is 92.24510988655577
At time: 77.84549307823181 and batch: 250, loss is 4.49891131401062 and perplexity is 89.91918423734263
At time: 78.29087090492249 and batch: 300, loss is 4.548328142166138 and perplexity is 94.47432856200304
At time: 78.73958492279053 and batch: 350, loss is 4.493121738433838 and perplexity is 89.4000944291905
At time: 79.18528032302856 and batch: 400, loss is 4.53453911781311 and perplexity is 93.18056014621476
At time: 79.6299831867218 and batch: 450, loss is 4.514699878692627 and perplexity is 91.350145782714
At time: 80.07031106948853 and batch: 500, loss is 4.470701084136963 and perplexity is 87.41798889996396
At time: 80.51163530349731 and batch: 550, loss is 4.472718057632446 and perplexity is 87.59448660236207
At time: 80.95262837409973 and batch: 600, loss is 4.552511672973633 and perplexity is 94.87039272170944
At time: 81.39415669441223 and batch: 650, loss is 4.528334083557129 and perplexity is 92.60416171391957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.955360263001685 and perplexity of 141.9337306351544
Finished 13 epochs...
Completing Train Step...
At time: 82.33041477203369 and batch: 50, loss is 4.579650382995606 and perplexity is 97.4803074756637
At time: 82.77124786376953 and batch: 100, loss is 4.548275003433227 and perplexity is 94.46930844927306
At time: 83.21372294425964 and batch: 150, loss is 4.535242052078247 and perplexity is 93.24608298121112
At time: 83.65847778320312 and batch: 200, loss is 4.515761728286743 and perplexity is 91.44719741593546
At time: 84.11413908004761 and batch: 250, loss is 4.491391525268555 and perplexity is 89.24554694743031
At time: 84.55501127243042 and batch: 300, loss is 4.540796375274658 and perplexity is 93.76544287402916
At time: 84.9971444606781 and batch: 350, loss is 4.486001901626587 and perplexity is 88.76584091635473
At time: 85.43951940536499 and batch: 400, loss is 4.530375804901123 and perplexity is 92.79342675492254
At time: 85.8812689781189 and batch: 450, loss is 4.512197103500366 and perplexity is 91.12180276884762
At time: 86.32241415977478 and batch: 500, loss is 4.471146240234375 and perplexity is 87.45691221357866
At time: 86.76707482337952 and batch: 550, loss is 4.474813928604126 and perplexity is 87.77826586564922
At time: 87.20684218406677 and batch: 600, loss is 4.554351968765259 and perplexity is 95.04514305301596
At time: 87.64974403381348 and batch: 650, loss is 4.524808349609375 and perplexity is 92.27823897332024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9540863037109375 and perplexity of 141.7530279687013
Finished 14 epochs...
Completing Train Step...
At time: 88.56585645675659 and batch: 50, loss is 4.570037240982056 and perplexity is 96.5477052372532
At time: 89.02549242973328 and batch: 100, loss is 4.539503135681152 and perplexity is 93.64426006689355
At time: 89.47033166885376 and batch: 150, loss is 4.528233690261841 and perplexity is 92.5948653436221
At time: 89.9181809425354 and batch: 200, loss is 4.509813976287842 and perplexity is 90.90490646940317
At time: 90.36573052406311 and batch: 250, loss is 4.485903654098511 and perplexity is 88.75712032030266
At time: 90.8105878829956 and batch: 300, loss is 4.5356346797943115 and perplexity is 93.28270116599072
At time: 91.25521993637085 and batch: 350, loss is 4.480894136428833 and perplexity is 88.31360179221761
At time: 91.70380568504333 and batch: 400, loss is 4.527134113311767 and perplexity is 92.49310612029615
At time: 92.14914989471436 and batch: 450, loss is 4.510499181747437 and perplexity is 90.96721635271818
At time: 92.59582781791687 and batch: 500, loss is 4.471116991043091 and perplexity is 87.4543542070342
At time: 93.04576587677002 and batch: 550, loss is 4.475495128631592 and perplexity is 87.83808079342
At time: 93.49146866798401 and batch: 600, loss is 4.55405707359314 and perplexity is 95.01711883150317
At time: 93.94146227836609 and batch: 650, loss is 4.5207860565185545 and perplexity is 91.90781432793065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.953394272748162 and perplexity of 141.6549644196995
Finished 15 epochs...
Completing Train Step...
At time: 94.88351941108704 and batch: 50, loss is 4.562468452453613 and perplexity is 95.81971455251458
At time: 95.32991123199463 and batch: 100, loss is 4.532868728637696 and perplexity is 93.0250422710356
At time: 95.8024890422821 and batch: 150, loss is 4.522745018005371 and perplexity is 92.0880346612492
At time: 96.24590539932251 and batch: 200, loss is 4.505253133773803 and perplexity is 90.49124754090934
At time: 96.68251895904541 and batch: 250, loss is 4.48183536529541 and perplexity is 88.39676423484211
At time: 97.1259069442749 and batch: 300, loss is 4.531322736740112 and perplexity is 92.88133742130117
At time: 97.55977702140808 and batch: 350, loss is 4.4766410541534425 and perplexity is 87.93879438610871
At time: 97.99322867393494 and batch: 400, loss is 4.524651346206665 and perplexity is 92.26375211307821
At time: 98.42804718017578 and batch: 450, loss is 4.5087400150299075 and perplexity is 90.80733052746817
At time: 98.87486529350281 and batch: 500, loss is 4.4707486534118654 and perplexity is 87.4221474092172
At time: 99.31742215156555 and batch: 550, loss is 4.474749231338501 and perplexity is 87.77258703557078
At time: 99.75867676734924 and batch: 600, loss is 4.552927198410035 and perplexity is 94.90982197440809
At time: 100.19645190238953 and batch: 650, loss is 4.5172710132598874 and perplexity is 91.58532150491757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.953106150907629 and perplexity of 141.61415640975196
Finished 16 epochs...
Completing Train Step...
At time: 101.0960841178894 and batch: 50, loss is 4.556426830291748 and perplexity is 95.24255329224978
At time: 101.55215311050415 and batch: 100, loss is 4.526922283172607 and perplexity is 92.47351536778449
At time: 101.99471426010132 and batch: 150, loss is 4.517858180999756 and perplexity is 91.63911324199162
At time: 102.44288372993469 and batch: 200, loss is 4.501247501373291 and perplexity is 90.12949786959088
At time: 102.88620948791504 and batch: 250, loss is 4.477994060516357 and perplexity is 88.05785666230558
At time: 103.32859444618225 and batch: 300, loss is 4.5276812553405765 and perplexity is 92.5437268331266
At time: 103.77008891105652 and batch: 350, loss is 4.472794647216797 and perplexity is 87.60119568460202
At time: 104.21177983283997 and batch: 400, loss is 4.522080583572388 and perplexity is 92.026868522849
At time: 104.65853762626648 and batch: 450, loss is 4.5066468334198 and perplexity is 90.6174530864565
At time: 105.10000801086426 and batch: 500, loss is 4.469749584197998 and perplexity is 87.33485024834461
At time: 105.54183292388916 and batch: 550, loss is 4.473848476409912 and perplexity is 87.69356104207321
At time: 105.9842050075531 and batch: 600, loss is 4.5511924743652346 and perplexity is 94.7453223461236
At time: 106.43227577209473 and batch: 650, loss is 4.513745555877685 and perplexity is 91.26300983896796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.953100765452666 and perplexity of 141.61339375514413
Finished 17 epochs...
Completing Train Step...
At time: 107.36305379867554 and batch: 50, loss is 4.551026935577393 and perplexity is 94.72963961839443
At time: 107.80726671218872 and batch: 100, loss is 4.522053136825561 and perplexity is 92.02434271935013
At time: 108.25233459472656 and batch: 150, loss is 4.513694553375244 and perplexity is 91.25835531578305
At time: 108.69928550720215 and batch: 200, loss is 4.497753791809082 and perplexity is 89.81516100146092
At time: 109.14468169212341 and batch: 250, loss is 4.474669017791748 and perplexity is 87.76554676742317
At time: 109.59049367904663 and batch: 300, loss is 4.524300928115845 and perplexity is 92.23142688921327
At time: 110.03673195838928 and batch: 350, loss is 4.469139575958252 and perplexity is 87.2815915158605
At time: 110.48077917098999 and batch: 400, loss is 4.519580116271973 and perplexity is 91.79704579919482
At time: 110.92778658866882 and batch: 450, loss is 4.504648561477661 and perplexity is 90.43655557389268
At time: 111.37383341789246 and batch: 500, loss is 4.468490037918091 and perplexity is 87.2249172100165
At time: 111.81992602348328 and batch: 550, loss is 4.4723640823364255 and perplexity is 87.56348580511913
At time: 112.26267313957214 and batch: 600, loss is 4.549210367202758 and perplexity is 94.55771295647024
At time: 112.70110726356506 and batch: 650, loss is 4.51038519859314 and perplexity is 90.95684821336849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9528165331073835 and perplexity of 141.57314836790508
Finished 18 epochs...
Completing Train Step...
At time: 113.60536026954651 and batch: 50, loss is 4.545871801376343 and perplexity is 94.24255219258019
At time: 114.06367826461792 and batch: 100, loss is 4.517293701171875 and perplexity is 91.58739940820277
At time: 114.5071427822113 and batch: 150, loss is 4.509789876937866 and perplexity is 90.90271574664523
At time: 114.9497458934784 and batch: 200, loss is 4.494793424606323 and perplexity is 89.54966831633364
At time: 115.39208197593689 and batch: 250, loss is 4.471672496795654 and perplexity is 87.5029491000046
At time: 115.83562517166138 and batch: 300, loss is 4.5208679294586185 and perplexity is 91.91533939895018
At time: 116.27851819992065 and batch: 350, loss is 4.465917863845825 and perplexity is 87.00084783576916
At time: 116.72213768959045 and batch: 400, loss is 4.517273244857788 and perplexity is 91.58552588675678
At time: 117.1648223400116 and batch: 450, loss is 4.502665748596192 and perplexity is 90.2574144668841
At time: 117.6080732345581 and batch: 500, loss is 4.466859788894653 and perplexity is 87.08283472030332
At time: 118.04971814155579 and batch: 550, loss is 4.470850257873535 and perplexity is 87.43103034070784
At time: 118.50557851791382 and batch: 600, loss is 4.547214813232422 and perplexity is 94.36920608732618
At time: 118.94725131988525 and batch: 650, loss is 4.507345418930054 and perplexity is 90.68077924295115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.952694163602941 and perplexity of 141.5558251918328
Finished 19 epochs...
Completing Train Step...
At time: 119.86697554588318 and batch: 50, loss is 4.541230802536011 and perplexity is 93.80618598790537
At time: 120.31206154823303 and batch: 100, loss is 4.5130774974823 and perplexity is 91.20206117995535
At time: 120.7591187953949 and batch: 150, loss is 4.506354103088379 and perplexity is 90.59093049155537
At time: 121.20621061325073 and batch: 200, loss is 4.491752080917358 and perplexity is 89.2777307351873
At time: 121.6513340473175 and batch: 250, loss is 4.468613090515137 and perplexity is 87.23565112301071
At time: 122.09801650047302 and batch: 300, loss is 4.517803068161011 and perplexity is 91.63406288949169
At time: 122.54442501068115 and batch: 350, loss is 4.462827606201172 and perplexity is 86.73240778873371
At time: 122.98905110359192 and batch: 400, loss is 4.514906148910523 and perplexity is 91.36899054067874
At time: 123.43589758872986 and batch: 450, loss is 4.500580654144287 and perplexity is 90.06941529885948
At time: 123.88153004646301 and batch: 500, loss is 4.465395936965942 and perplexity is 86.95545160249878
At time: 124.32680916786194 and batch: 550, loss is 4.469030017852783 and perplexity is 87.27202963385213
At time: 124.76830244064331 and batch: 600, loss is 4.545074224472046 and perplexity is 94.16741647678818
At time: 125.20785927772522 and batch: 650, loss is 4.503873243331909 and perplexity is 90.36646564582445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.95256072399663 and perplexity of 141.5369372984727
Finished 20 epochs...
Completing Train Step...
At time: 126.110755443573 and batch: 50, loss is 4.537050561904907 and perplexity is 93.41487202089371
At time: 126.56770706176758 and batch: 100, loss is 4.509359865188599 and perplexity is 90.86363491404468
At time: 127.01203465461731 and batch: 150, loss is 4.503025608062744 and perplexity is 90.28990029672269
At time: 127.45408177375793 and batch: 200, loss is 4.488929662704468 and perplexity is 89.02610690283562
At time: 127.89628481864929 and batch: 250, loss is 4.465855770111084 and perplexity is 86.99544579591951
At time: 128.33951544761658 and batch: 300, loss is 4.514794883728027 and perplexity is 91.35882491882214
At time: 128.78176283836365 and batch: 350, loss is 4.459759273529053 and perplexity is 86.466691769225
At time: 129.22369027137756 and batch: 400, loss is 4.512538156509399 and perplexity is 91.15288543398742
At time: 129.66539597511292 and batch: 450, loss is 4.49853819847107 and perplexity is 89.88564025068051
At time: 130.13711380958557 and batch: 500, loss is 4.463844347000122 and perplexity is 86.82063701184843
At time: 130.57987666130066 and batch: 550, loss is 4.467329568862915 and perplexity is 87.12375410243489
At time: 131.02285051345825 and batch: 600, loss is 4.5428675842285156 and perplexity is 93.95985196029264
At time: 131.46814894676208 and batch: 650, loss is 4.50074670791626 and perplexity is 90.08437290685848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.952645993700214 and perplexity of 141.54900662572902
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 132.4015657901764 and batch: 50, loss is 4.53262077331543 and perplexity is 93.00197907613965
At time: 132.84898710250854 and batch: 100, loss is 4.502457132339478 and perplexity is 90.23858726683564
At time: 133.29663944244385 and batch: 150, loss is 4.493586521148682 and perplexity is 89.44165570551986
At time: 133.74104070663452 and batch: 200, loss is 4.475178146362305 and perplexity is 87.81024209166083
At time: 134.18752479553223 and batch: 250, loss is 4.448276977539063 and perplexity is 85.479533886363
At time: 134.63357257843018 and batch: 300, loss is 4.495867404937744 and perplexity is 89.64589456208913
At time: 135.08047890663147 and batch: 350, loss is 4.437824287414551 and perplexity is 84.59069627149844
At time: 135.524888753891 and batch: 400, loss is 4.489511661529541 and perplexity is 89.07793507295798
At time: 135.9707374572754 and batch: 450, loss is 4.472518405914307 and perplexity is 87.57699995828973
At time: 136.41484308242798 and batch: 500, loss is 4.432901411056519 and perplexity is 84.17529006833941
At time: 136.86072444915771 and batch: 550, loss is 4.434318876266479 and perplexity is 84.29469021633588
At time: 137.30320024490356 and batch: 600, loss is 4.509307365417481 and perplexity is 90.85886471922691
At time: 137.74652910232544 and batch: 650, loss is 4.478929643630981 and perplexity is 88.14028065734558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.945537791532629 and perplexity of 140.54641520272813
Finished 22 epochs...
Completing Train Step...
At time: 138.64416551589966 and batch: 50, loss is 4.527235660552979 and perplexity is 92.50249901695703
At time: 139.09720373153687 and batch: 100, loss is 4.497060308456421 and perplexity is 89.75289727441424
At time: 139.53563690185547 and batch: 150, loss is 4.488573894500733 and perplexity is 88.99443987809153
At time: 139.97362542152405 and batch: 200, loss is 4.470090475082397 and perplexity is 87.36462697770412
At time: 140.41391038894653 and batch: 250, loss is 4.444419546127319 and perplexity is 85.15043758856005
At time: 140.85348534584045 and batch: 300, loss is 4.492469244003296 and perplexity is 89.34178039234656
At time: 141.29491448402405 and batch: 350, loss is 4.435423927307129 and perplexity is 84.38789163816047
At time: 141.75311708450317 and batch: 400, loss is 4.487362585067749 and perplexity is 88.8867053366902
At time: 142.1953043937683 and batch: 450, loss is 4.47128493309021 and perplexity is 87.46904270368283
At time: 142.63531279563904 and batch: 500, loss is 4.432642393112182 and perplexity is 84.15348998117074
At time: 143.07533025741577 and batch: 550, loss is 4.4355503273010255 and perplexity is 84.3985589413078
At time: 143.51601195335388 and batch: 600, loss is 4.511771831512451 and perplexity is 91.08305945744817
At time: 143.95772194862366 and batch: 650, loss is 4.479768218994141 and perplexity is 88.21422392437469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.944327260933671 and perplexity of 140.37638240226886
Finished 23 epochs...
Completing Train Step...
At time: 144.87934851646423 and batch: 50, loss is 4.52524995803833 and perplexity is 92.31899882074364
At time: 145.32316303253174 and batch: 100, loss is 4.494700975418091 and perplexity is 89.54138990486346
At time: 145.76789927482605 and batch: 150, loss is 4.486082382202149 and perplexity is 88.77298512980329
At time: 146.21235585212708 and batch: 200, loss is 4.467438478469848 and perplexity is 87.13324323296806
At time: 146.660058259964 and batch: 250, loss is 4.442490758895874 and perplexity is 84.98635879919166
At time: 147.10771369934082 and batch: 300, loss is 4.490497808456421 and perplexity is 89.16582233263615
At time: 147.55623388290405 and batch: 350, loss is 4.43408447265625 and perplexity is 84.27493355223315
At time: 148.0047426223755 and batch: 400, loss is 4.486230096817017 and perplexity is 88.78609916565594
At time: 148.45207047462463 and batch: 450, loss is 4.4707016754150395 and perplexity is 87.41804058831956
At time: 148.89977622032166 and batch: 500, loss is 4.432629880905151 and perplexity is 84.15243704186899
At time: 149.34630417823792 and batch: 550, loss is 4.436328439712525 and perplexity is 84.46425606409124
At time: 149.7952754497528 and batch: 600, loss is 4.513192024230957 and perplexity is 91.21250685363621
At time: 150.2438097000122 and batch: 650, loss is 4.480224227905273 and perplexity is 88.254459569782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.943730073816636 and perplexity of 140.29257646156702
Finished 24 epochs...
Completing Train Step...
At time: 151.1551549434662 and batch: 50, loss is 4.523985433578491 and perplexity is 92.20233296758536
At time: 151.61798810958862 and batch: 100, loss is 4.493086156845092 and perplexity is 89.3969134883885
At time: 152.0655837059021 and batch: 150, loss is 4.484324741363525 and perplexity is 88.61709114872006
At time: 152.51039099693298 and batch: 200, loss is 4.465476551055908 and perplexity is 86.96246171965062
At time: 152.95031714439392 and batch: 250, loss is 4.4411202335357665 and perplexity is 84.86996261937156
At time: 153.40845489501953 and batch: 300, loss is 4.48896481513977 and perplexity is 89.02923644230387
At time: 153.85150384902954 and batch: 350, loss is 4.433109188079834 and perplexity is 84.19278157665036
At time: 154.29600429534912 and batch: 400, loss is 4.4854051303863525 and perplexity is 88.71288381858876
At time: 154.74086499214172 and batch: 450, loss is 4.470317850112915 and perplexity is 87.3844937709501
At time: 155.1855432987213 and batch: 500, loss is 4.432624340057373 and perplexity is 84.15197076731698
At time: 155.62900638580322 and batch: 550, loss is 4.436831741333008 and perplexity is 84.50677776076304
At time: 156.07341694831848 and batch: 600, loss is 4.5142151927948 and perplexity is 91.30588038356255
At time: 156.51733827590942 and batch: 650, loss is 4.480536231994629 and perplexity is 88.28199961815208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.943412032781863 and perplexity of 140.24796475993014
Finished 25 epochs...
Completing Train Step...
At time: 157.44051027297974 and batch: 50, loss is 4.52299843788147 and perplexity is 92.11137455685507
At time: 157.88450813293457 and batch: 100, loss is 4.491822843551636 and perplexity is 89.28404848612416
At time: 158.32833075523376 and batch: 150, loss is 4.482855176925659 and perplexity is 88.48695826572906
At time: 158.77172303199768 and batch: 200, loss is 4.463887920379639 and perplexity is 86.82442016283659
At time: 159.2166063785553 and batch: 250, loss is 4.44000205039978 and perplexity is 84.77511549652797
At time: 159.66001296043396 and batch: 300, loss is 4.4877417945861815 and perplexity is 88.92041841317173
At time: 160.10558152198792 and batch: 350, loss is 4.432309656143189 and perplexity is 84.12549366194767
At time: 160.55371642112732 and batch: 400, loss is 4.484682455062866 and perplexity is 88.64879636657699
At time: 161.00301957130432 and batch: 450, loss is 4.46994381904602 and perplexity is 87.35181536726496
At time: 161.4506320953369 and batch: 500, loss is 4.432536849975586 and perplexity is 84.14460862657383
At time: 161.89911913871765 and batch: 550, loss is 4.43713306427002 and perplexity is 84.53224542803876
At time: 162.34660458564758 and batch: 600, loss is 4.514956092834472 and perplexity is 91.37355398055074
At time: 162.79559230804443 and batch: 650, loss is 4.480742607116699 and perplexity is 88.30022070672494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.943217857211244 and perplexity of 140.2207346751389
Finished 26 epochs...
Completing Train Step...
At time: 163.71503353118896 and batch: 50, loss is 4.522179489135742 and perplexity is 92.03597094225654
At time: 164.17759609222412 and batch: 100, loss is 4.490807676315308 and perplexity is 89.1934562362964
At time: 164.63348937034607 and batch: 150, loss is 4.481670875549316 and perplexity is 88.38222506934218
At time: 165.1071469783783 and batch: 200, loss is 4.462516279220581 and perplexity is 86.70540985290845
At time: 165.56077313423157 and batch: 250, loss is 4.439112567901612 and perplexity is 84.69974304122294
At time: 166.009033203125 and batch: 300, loss is 4.486679487228393 and perplexity is 88.82600775388133
At time: 166.45681619644165 and batch: 350, loss is 4.4315567970275875 and perplexity is 84.06218285224028
At time: 166.9093029499054 and batch: 400, loss is 4.484053783416748 and perplexity is 88.59308289641329
At time: 167.36096167564392 and batch: 450, loss is 4.469595174789429 and perplexity is 87.3213659668471
At time: 167.81378817558289 and batch: 500, loss is 4.432419729232788 and perplexity is 84.13475412460365
At time: 168.26956009864807 and batch: 550, loss is 4.4373222351074215 and perplexity is 84.54823797630803
At time: 168.72372198104858 and batch: 600, loss is 4.515464248657227 and perplexity is 91.41999778339675
At time: 169.17978882789612 and batch: 650, loss is 4.48084508895874 and perplexity is 88.30927033969921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.943074843462775 and perplexity of 140.20068261615424
Finished 27 epochs...
Completing Train Step...
At time: 170.1151089668274 and batch: 50, loss is 4.521477508544922 and perplexity is 91.97138614828584
At time: 170.56026601791382 and batch: 100, loss is 4.489877290725708 and perplexity is 89.11051052165963
At time: 171.00721168518066 and batch: 150, loss is 4.480557279586792 and perplexity is 88.28385776123007
At time: 171.4563705921173 and batch: 200, loss is 4.461294574737549 and perplexity is 86.59954614523848
At time: 171.90579533576965 and batch: 250, loss is 4.438306798934937 and perplexity is 84.63152210564249
At time: 172.35620307922363 and batch: 300, loss is 4.48569432258606 and perplexity is 88.7385426025839
At time: 172.79972124099731 and batch: 350, loss is 4.43080813407898 and perplexity is 83.99927216294923
At time: 173.24304819107056 and batch: 400, loss is 4.483390369415283 and perplexity is 88.53432849618768
At time: 173.68893098831177 and batch: 450, loss is 4.46914454460144 and perplexity is 87.28202518802304
At time: 174.13622879981995 and batch: 500, loss is 4.432205266952515 and perplexity is 84.1167123280954
At time: 174.58402705192566 and batch: 550, loss is 4.4373892974853515 and perplexity is 84.55390817232279
At time: 175.03270316123962 and batch: 600, loss is 4.515759353637695 and perplexity is 91.446980261193
At time: 175.47939085960388 and batch: 650, loss is 4.480817499160767 and perplexity is 88.30683393838143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942934821633732 and perplexity of 140.1810528344722
Finished 28 epochs...
Completing Train Step...
At time: 176.39142537117004 and batch: 50, loss is 4.520868911743164 and perplexity is 91.91542968601189
At time: 176.85402607917786 and batch: 100, loss is 4.489001617431641 and perplexity is 89.03251298254018
At time: 177.29977226257324 and batch: 150, loss is 4.479558982849121 and perplexity is 88.19576825108913
At time: 177.74608755111694 and batch: 200, loss is 4.460140790939331 and perplexity is 86.49968661119722
At time: 178.19056296348572 and batch: 250, loss is 4.437545890808106 and perplexity is 84.56714978650247
At time: 178.63578915596008 and batch: 300, loss is 4.484770097732544 and perplexity is 88.6565661242305
At time: 179.08101963996887 and batch: 350, loss is 4.430151395797729 and perplexity is 83.94412473601794
At time: 179.53024291992188 and batch: 400, loss is 4.4828152751922605 and perplexity is 88.48342755315237
At time: 179.96945309638977 and batch: 450, loss is 4.468768978118897 and perplexity is 87.24925113963616
At time: 180.40961718559265 and batch: 500, loss is 4.431972179412842 and perplexity is 84.09710805541808
At time: 180.8543779850006 and batch: 550, loss is 4.437398900985718 and perplexity is 84.55472018971001
At time: 181.29722476005554 and batch: 600, loss is 4.516084270477295 and perplexity is 91.47669775260567
At time: 181.74997329711914 and batch: 650, loss is 4.4808320045471195 and perplexity is 88.30811487241546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942839678596048 and perplexity of 140.167716217733
Finished 29 epochs...
Completing Train Step...
At time: 182.67338848114014 and batch: 50, loss is 4.520295791625976 and perplexity is 91.86276619687295
At time: 183.11729192733765 and batch: 100, loss is 4.488257598876953 and perplexity is 88.96629577737997
At time: 183.56127285957336 and batch: 150, loss is 4.478607940673828 and perplexity is 88.11193022886664
At time: 184.00120306015015 and batch: 200, loss is 4.459118633270264 and perplexity is 86.41131546547209
At time: 184.4492151737213 and batch: 250, loss is 4.436848058700561 and perplexity is 84.5081567001668
At time: 184.89670515060425 and batch: 300, loss is 4.483953638076782 and perplexity is 88.58421115624715
At time: 185.33687090873718 and batch: 350, loss is 4.429536037445068 and perplexity is 83.89248490783413
At time: 185.77707743644714 and batch: 400, loss is 4.482259883880615 and perplexity is 88.43429827051594
At time: 186.2210659980774 and batch: 450, loss is 4.468406753540039 and perplexity is 87.2176530395362
At time: 186.663019657135 and batch: 500, loss is 4.431727676391602 and perplexity is 84.07654857195129
At time: 187.10693049430847 and batch: 550, loss is 4.437397890090942 and perplexity is 84.5546347138283
At time: 187.55084013938904 and batch: 600, loss is 4.51636570930481 and perplexity is 91.50244647034062
At time: 188.01368618011475 and batch: 650, loss is 4.48083046913147 and perplexity is 88.30797928285799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942749921013327 and perplexity of 140.15513566695813
Finished 30 epochs...
Completing Train Step...
At time: 188.9318253993988 and batch: 50, loss is 4.519763584136963 and perplexity is 91.81388915225978
At time: 189.3983018398285 and batch: 100, loss is 4.487583093643188 and perplexity is 88.90630777863332
At time: 189.85279536247253 and batch: 150, loss is 4.4777241134643555 and perplexity is 88.03408891165546
At time: 190.30361032485962 and batch: 200, loss is 4.458154935836792 and perplexity is 86.3280812152934
At time: 190.75521039962769 and batch: 250, loss is 4.436207571029663 and perplexity is 84.45404759766627
At time: 191.2066764831543 and batch: 300, loss is 4.483137502670288 and perplexity is 88.51194393899482
At time: 191.6584701538086 and batch: 350, loss is 4.4289625453948975 and perplexity is 83.84438702786458
At time: 192.1099534034729 and batch: 400, loss is 4.481697015762329 and perplexity is 88.38453542972857
At time: 192.56277775764465 and batch: 450, loss is 4.468049392700196 and perplexity is 87.18649043427584
At time: 193.01432299613953 and batch: 500, loss is 4.431479320526123 and perplexity is 84.05567026069711
At time: 193.4631142616272 and batch: 550, loss is 4.437367973327636 and perplexity is 84.55210515067353
At time: 193.90549302101135 and batch: 600, loss is 4.516563053131104 and perplexity is 91.52050569512195
At time: 194.34884786605835 and batch: 650, loss is 4.480795440673828 and perplexity is 88.30488604472222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9427050422219665 and perplexity of 140.14884581500795
Finished 31 epochs...
Completing Train Step...
At time: 195.28800010681152 and batch: 50, loss is 4.5192350578308105 and perplexity is 91.7653759179622
At time: 195.7291271686554 and batch: 100, loss is 4.486957340240479 and perplexity is 88.85069175679075
At time: 196.17019748687744 and batch: 150, loss is 4.47686990737915 and perplexity is 87.95892176588887
At time: 196.612637758255 and batch: 200, loss is 4.4572611331939695 and perplexity is 86.25095542090739
At time: 197.05497217178345 and batch: 250, loss is 4.4355943775177 and perplexity is 84.40227679800175
At time: 197.49632334709167 and batch: 300, loss is 4.482349185943604 and perplexity is 88.44219598842648
At time: 197.938782453537 and batch: 350, loss is 4.428313074111938 and perplexity is 83.789950185749
At time: 198.38044548034668 and batch: 400, loss is 4.481128311157226 and perplexity is 88.33428502758173
At time: 198.82366466522217 and batch: 450, loss is 4.467648973464966 and perplexity is 87.15158627506852
At time: 199.26499915122986 and batch: 500, loss is 4.431170797348022 and perplexity is 84.02974113824416
At time: 199.72126030921936 and batch: 550, loss is 4.437198810577392 and perplexity is 84.53780329373214
At time: 200.16353034973145 and batch: 600, loss is 4.516644926071167 and perplexity is 91.52799905474679
At time: 200.60543608665466 and batch: 650, loss is 4.480641527175903 and perplexity is 88.29129577671692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942644904641544 and perplexity of 140.14041785594276
Finished 32 epochs...
Completing Train Step...
At time: 201.51840615272522 and batch: 50, loss is 4.51876235961914 and perplexity is 91.72200883945028
At time: 201.97884702682495 and batch: 100, loss is 4.486277227401733 and perplexity is 88.79028380503581
At time: 202.42494320869446 and batch: 150, loss is 4.475977840423584 and perplexity is 87.880491506057
At time: 202.87237405776978 and batch: 200, loss is 4.456432085037232 and perplexity is 86.17947885814907
At time: 203.31888389587402 and batch: 250, loss is 4.434938049316406 and perplexity is 84.34689937835482
At time: 203.76545000076294 and batch: 300, loss is 4.481564826965332 and perplexity is 88.37285275649329
At time: 204.21107959747314 and batch: 350, loss is 4.427694940567017 and perplexity is 83.73817281112662
At time: 204.65841460227966 and batch: 400, loss is 4.480587711334229 and perplexity is 88.28654443417206
At time: 205.10472083091736 and batch: 450, loss is 4.46722677230835 and perplexity is 87.11479854100067
At time: 205.55178904533386 and batch: 500, loss is 4.430877599716187 and perplexity is 84.00510742858782
At time: 205.9975414276123 and batch: 550, loss is 4.437045373916626 and perplexity is 84.52483309056402
At time: 206.44507145881653 and batch: 600, loss is 4.51676236152649 and perplexity is 91.53874831815052
At time: 206.8903615474701 and batch: 650, loss is 4.480561008453369 and perplexity is 88.28418696057037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9425997266582415 and perplexity of 140.13408673749947
Finished 33 epochs...
Completing Train Step...
At time: 207.80142545700073 and batch: 50, loss is 4.518270978927612 and perplexity is 91.67694948687296
At time: 208.2431263923645 and batch: 100, loss is 4.485635223388672 and perplexity is 88.73329838090494
At time: 208.6852080821991 and batch: 150, loss is 4.475045566558838 and perplexity is 87.79860099872586
At time: 209.127512216568 and batch: 200, loss is 4.455668907165528 and perplexity is 86.11373367771402
At time: 209.57019591331482 and batch: 250, loss is 4.43431622505188 and perplexity is 84.29446673331881
At time: 210.01250672340393 and batch: 300, loss is 4.480688152313232 and perplexity is 88.29541246647732
At time: 210.45555019378662 and batch: 350, loss is 4.427035512924195 and perplexity is 83.6829717477694
At time: 210.89756512641907 and batch: 400, loss is 4.480094556808472 and perplexity is 88.24301625916132
At time: 211.3673141002655 and batch: 450, loss is 4.466786947250366 and perplexity is 87.07649169445415
At time: 211.80875277519226 and batch: 500, loss is 4.4304562854766845 and perplexity is 83.9697223352825
At time: 212.25053548812866 and batch: 550, loss is 4.436721820831298 and perplexity is 84.49748924386229
At time: 212.6921763420105 and batch: 600, loss is 4.516817150115966 and perplexity is 91.54376373444589
At time: 213.13445734977722 and batch: 650, loss is 4.480403623580933 and perplexity is 88.2702934584096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942547068876379 and perplexity of 140.12670778160992
Finished 34 epochs...
Completing Train Step...
At time: 214.0459258556366 and batch: 50, loss is 4.517742338180542 and perplexity is 91.62849812361793
At time: 214.5025806427002 and batch: 100, loss is 4.485083074569702 and perplexity is 88.6843179184972
At time: 214.9439001083374 and batch: 150, loss is 4.4743264865875245 and perplexity is 87.73548947707339
At time: 215.3907778263092 and batch: 200, loss is 4.454890489578247 and perplexity is 86.04672731576164
At time: 215.83609199523926 and batch: 250, loss is 4.433703927993775 and perplexity is 84.24286927742068
At time: 216.283189535141 and batch: 300, loss is 4.479961280822754 and perplexity is 88.23125636785963
At time: 216.72964715957642 and batch: 350, loss is 4.4264639949798585 and perplexity is 83.63515909197699
At time: 217.1737585067749 and batch: 400, loss is 4.47955849647522 and perplexity is 88.1957253549797
At time: 217.61896133422852 and batch: 450, loss is 4.4663433074951175 and perplexity is 87.03786966875738
At time: 218.06573843955994 and batch: 500, loss is 4.430115175247193 and perplexity is 83.94108428866936
At time: 218.51185011863708 and batch: 550, loss is 4.436487531661987 and perplexity is 84.4776947162107
At time: 218.95833587646484 and batch: 600, loss is 4.516880445480346 and perplexity is 91.54955821370808
At time: 219.40363955497742 and batch: 650, loss is 4.480304250717163 and perplexity is 88.26152222238149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942453421798407 and perplexity of 140.11358593930112
Finished 35 epochs...
Completing Train Step...
At time: 220.33124494552612 and batch: 50, loss is 4.5172695064544675 and perplexity is 91.5851835037627
At time: 220.77502989768982 and batch: 100, loss is 4.484556617736817 and perplexity is 88.6376417409336
At time: 221.21365094184875 and batch: 150, loss is 4.47361590385437 and perplexity is 87.67316829796529
At time: 221.65593433380127 and batch: 200, loss is 4.454117641448975 and perplexity is 85.98025195451373
At time: 222.09864830970764 and batch: 250, loss is 4.433145179748535 and perplexity is 84.19581186988417
At time: 222.53977012634277 and batch: 300, loss is 4.479205923080444 and perplexity is 88.16463536975948
At time: 222.99644470214844 and batch: 350, loss is 4.425888872146606 and perplexity is 83.58707243151414
At time: 223.43762636184692 and batch: 400, loss is 4.4789814949035645 and perplexity is 88.14485096155053
At time: 223.88015747070312 and batch: 450, loss is 4.4659195899963375 and perplexity is 87.00099801245685
At time: 224.32285618782043 and batch: 500, loss is 4.429796772003174 and perplexity is 83.91436142967366
At time: 224.76489353179932 and batch: 550, loss is 4.43626277923584 and perplexity is 84.45871028284654
At time: 225.20716905593872 and batch: 600, loss is 4.516901588439941 and perplexity is 91.5514938627809
At time: 225.6494414806366 and batch: 650, loss is 4.480191612243653 and perplexity is 88.2515811391334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942381316540288 and perplexity of 140.1034833772493
Finished 36 epochs...
Completing Train Step...
At time: 226.55176448822021 and batch: 50, loss is 4.516789541244507 and perplexity is 91.54123634932934
At time: 227.0049865245819 and batch: 100, loss is 4.484064750671386 and perplexity is 88.59405452464067
At time: 227.44215202331543 and batch: 150, loss is 4.472954950332642 and perplexity is 87.61523955482998
At time: 227.8792004585266 and batch: 200, loss is 4.453382034301757 and perplexity is 85.91702752367902
At time: 228.3169505596161 and batch: 250, loss is 4.43260048866272 and perplexity is 84.14996364938776
At time: 228.7599060535431 and batch: 300, loss is 4.4785104084014895 and perplexity is 88.10333689116197
At time: 229.2015335559845 and batch: 350, loss is 4.425342788696289 and perplexity is 83.54143937546121
At time: 229.64439582824707 and batch: 400, loss is 4.4784297752380375 and perplexity is 88.09623312680111
At time: 230.08552765846252 and batch: 450, loss is 4.465490379333496 and perplexity is 86.96366426902557
At time: 230.5276882648468 and batch: 500, loss is 4.429436092376709 and perplexity is 83.88410068668294
At time: 230.97284388542175 and batch: 550, loss is 4.435998849868774 and perplexity is 84.43642209028208
At time: 231.41588926315308 and batch: 600, loss is 4.516895084381104 and perplexity is 91.55089840841462
At time: 231.85719895362854 and batch: 650, loss is 4.48006501197815 and perplexity is 88.24040917273202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942327162798713 and perplexity of 140.09589645484863
Finished 37 epochs...
Completing Train Step...
At time: 232.79974746704102 and batch: 50, loss is 4.516332273483276 and perplexity is 91.49938706201773
At time: 233.24570989608765 and batch: 100, loss is 4.483555450439453 and perplexity is 88.54894504023383
At time: 233.6916105747223 and batch: 150, loss is 4.472285556793213 and perplexity is 87.55661010479326
At time: 234.13669633865356 and batch: 200, loss is 4.452652406692505 and perplexity is 85.85436295197366
At time: 234.59779596328735 and batch: 250, loss is 4.4320664215087895 and perplexity is 84.10503391661337
At time: 235.03717398643494 and batch: 300, loss is 4.477832183837891 and perplexity is 88.04360330263063
At time: 235.47719550132751 and batch: 350, loss is 4.424842710494995 and perplexity is 83.49967256693047
At time: 235.9198215007782 and batch: 400, loss is 4.477901515960693 and perplexity is 88.04970776416194
At time: 236.36231994628906 and batch: 450, loss is 4.465058650970459 and perplexity is 86.92612769199249
At time: 236.80451464653015 and batch: 500, loss is 4.429095249176026 and perplexity is 83.85551423334435
At time: 237.24792575836182 and batch: 550, loss is 4.435691461563111 and perplexity is 84.41047131024696
At time: 237.68905234336853 and batch: 600, loss is 4.516837291717529 and perplexity is 91.54560759102962
At time: 238.13185906410217 and batch: 650, loss is 4.479946527481079 and perplexity is 88.22995467159026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942254159964767 and perplexity of 140.08566943068863
Finished 38 epochs...
Completing Train Step...
At time: 239.03671002388 and batch: 50, loss is 4.515876245498657 and perplexity is 91.45767029367184
At time: 239.49311709403992 and batch: 100, loss is 4.483047075271607 and perplexity is 88.50394039602746
At time: 239.93560242652893 and batch: 150, loss is 4.471602735519409 and perplexity is 87.49684499551768
At time: 240.3782720565796 and batch: 200, loss is 4.451924295425415 and perplexity is 85.7918741751314
At time: 240.8196907043457 and batch: 250, loss is 4.4315172290802005 and perplexity is 84.0588567500159
At time: 241.26272130012512 and batch: 300, loss is 4.477133464813233 and perplexity is 87.98210704880677
At time: 241.70413851737976 and batch: 350, loss is 4.424295539855957 and perplexity is 83.45399649517485
At time: 242.14678835868835 and batch: 400, loss is 4.477387094497681 and perplexity is 88.00442475295117
At time: 242.5897126197815 and batch: 450, loss is 4.464629335403442 and perplexity is 86.88881696180557
At time: 243.03206706047058 and batch: 500, loss is 4.428743476867676 and perplexity is 83.82602137322449
At time: 243.47673654556274 and batch: 550, loss is 4.43540249824524 and perplexity is 84.38608330418336
At time: 243.92178463935852 and batch: 600, loss is 4.516772394180298 and perplexity is 91.53966669932933
At time: 244.36736130714417 and batch: 650, loss is 4.479803056716919 and perplexity is 88.21729716058478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942223044002757 and perplexity of 140.0813105981355
Finished 39 epochs...
Completing Train Step...
At time: 245.2957899570465 and batch: 50, loss is 4.515475301742554 and perplexity is 91.4210082620173
At time: 245.74045538902283 and batch: 100, loss is 4.482568740844727 and perplexity is 88.4616160378273
At time: 246.20180487632751 and batch: 150, loss is 4.470945339202881 and perplexity is 87.43934379451957
At time: 246.64667344093323 and batch: 200, loss is 4.451237144470215 and perplexity is 85.73294245665576
At time: 247.0913372039795 and batch: 250, loss is 4.430970592498779 and perplexity is 84.01291966051456
At time: 247.53813815116882 and batch: 300, loss is 4.476453857421875 and perplexity is 87.92233407192823
At time: 247.98439121246338 and batch: 350, loss is 4.423773469924927 and perplexity is 83.41043904398752
At time: 248.43075799942017 and batch: 400, loss is 4.476855001449585 and perplexity is 87.957610666168
At time: 248.87591242790222 and batch: 450, loss is 4.464196147918702 and perplexity is 86.85118596496068
At time: 249.32111859321594 and batch: 500, loss is 4.428366451263428 and perplexity is 83.79442277397935
At time: 249.76983523368835 and batch: 550, loss is 4.4351738166809085 and perplexity is 84.3667879689734
At time: 250.2146279811859 and batch: 600, loss is 4.516692981719971 and perplexity is 91.53239759781168
At time: 250.66290426254272 and batch: 650, loss is 4.479668350219726 and perplexity is 88.20541451784506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942179361979167 and perplexity of 140.07519169666537
Finished 40 epochs...
Completing Train Step...
At time: 251.56838536262512 and batch: 50, loss is 4.515038938522339 and perplexity is 91.38112419905869
At time: 252.02888560295105 and batch: 100, loss is 4.482095975875854 and perplexity is 88.41980436899969
At time: 252.46999669075012 and batch: 150, loss is 4.470283136367798 and perplexity is 87.38146038055666
At time: 252.9136745929718 and batch: 200, loss is 4.450506753921509 and perplexity is 85.67034678819772
At time: 253.3571548461914 and batch: 250, loss is 4.430438613891601 and perplexity is 83.968238470301
At time: 253.8017864227295 and batch: 300, loss is 4.475823402404785 and perplexity is 87.86692046502702
At time: 254.24558115005493 and batch: 350, loss is 4.423265724182129 and perplexity is 83.36809849868348
At time: 254.6889045238495 and batch: 400, loss is 4.476334800720215 and perplexity is 87.91186695193413
At time: 255.1316556930542 and batch: 450, loss is 4.463787117004395 and perplexity is 86.8156684093411
At time: 255.57470893859863 and batch: 500, loss is 4.427986288070679 and perplexity is 83.7625732730706
At time: 256.0176498889923 and batch: 550, loss is 4.434916152954101 and perplexity is 84.34505250830671
At time: 256.4604663848877 and batch: 600, loss is 4.516644096374511 and perplexity is 91.52792311430359
At time: 256.9036319255829 and batch: 650, loss is 4.479474649429322 and perplexity is 88.18833071396155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942141663794424 and perplexity of 140.06991121574382
Finished 41 epochs...
Completing Train Step...
At time: 257.84214425086975 and batch: 50, loss is 4.514616460800171 and perplexity is 91.34252586390039
At time: 258.28580713272095 and batch: 100, loss is 4.481614103317261 and perplexity is 88.37720755558009
At time: 258.7292437553406 and batch: 150, loss is 4.469644975662232 and perplexity is 87.32571475537239
At time: 259.1720323562622 and batch: 200, loss is 4.449848766326904 and perplexity is 85.61399530410743
At time: 259.61546874046326 and batch: 250, loss is 4.429877433776856 and perplexity is 83.9211303838982
At time: 260.0583758354187 and batch: 300, loss is 4.475164194107055 and perplexity is 87.80901694929638
At time: 260.5026059150696 and batch: 350, loss is 4.422754697799682 and perplexity is 83.32550608474673
At time: 260.94587111473083 and batch: 400, loss is 4.475786762237549 and perplexity is 87.86370106534669
At time: 261.3895728588104 and batch: 450, loss is 4.463324604034423 and perplexity is 86.77552432100146
At time: 261.8326871395111 and batch: 500, loss is 4.427639780044555 and perplexity is 83.73355389715638
At time: 262.2763600349426 and batch: 550, loss is 4.434677352905274 and perplexity is 84.32491331036579
At time: 262.720374584198 and batch: 600, loss is 4.516616258621216 and perplexity is 91.52537521802432
At time: 263.1664056777954 and batch: 650, loss is 4.479327058792114 and perplexity is 88.17531590249298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942061779545803 and perplexity of 140.0587222830476
Finished 42 epochs...
Completing Train Step...
At time: 264.0802569389343 and batch: 50, loss is 4.514228687286377 and perplexity is 91.30711251830986
At time: 264.5399925708771 and batch: 100, loss is 4.481113977432251 and perplexity is 88.33301887730863
At time: 264.98387360572815 and batch: 150, loss is 4.469069662094117 and perplexity is 87.27548953583873
At time: 265.42578768730164 and batch: 200, loss is 4.449130392074585 and perplexity is 85.55251450000007
At time: 265.8660161495209 and batch: 250, loss is 4.429428606033325 and perplexity is 83.88347270385144
At time: 266.31744623184204 and batch: 300, loss is 4.474532585144043 and perplexity is 87.75357349829142
At time: 266.78404664993286 and batch: 350, loss is 4.4222624969482425 and perplexity is 83.28450329133787
At time: 267.255398273468 and batch: 400, loss is 4.47529221534729 and perplexity is 87.82025908815093
At time: 267.73714661598206 and batch: 450, loss is 4.462888193130493 and perplexity is 86.73766279818513
At time: 268.1976583003998 and batch: 500, loss is 4.427202215194702 and perplexity is 83.69692305198731
At time: 268.647043466568 and batch: 550, loss is 4.434411153793335 and perplexity is 84.30246908077781
At time: 269.10821056365967 and batch: 600, loss is 4.516516065597534 and perplexity is 91.51620547331747
At time: 269.56411147117615 and batch: 650, loss is 4.4791021728515625 and perplexity is 88.15548874315108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.942044127221201 and perplexity of 140.05624994283983
Finished 43 epochs...
Completing Train Step...
At time: 270.49225306510925 and batch: 50, loss is 4.513832445144653 and perplexity is 91.27093995951036
At time: 270.93612146377563 and batch: 100, loss is 4.480675096511841 and perplexity is 88.29425970663353
At time: 271.37441992759705 and batch: 150, loss is 4.4684985160827635 and perplexity is 87.22565672036299
At time: 271.81215381622314 and batch: 200, loss is 4.448485708236694 and perplexity is 85.49737795134156
At time: 272.2560660839081 and batch: 250, loss is 4.428899078369141 and perplexity is 83.8390658428552
At time: 272.6950068473816 and batch: 300, loss is 4.473876867294312 and perplexity is 87.69605077517006
At time: 273.1367621421814 and batch: 350, loss is 4.421761827468872 and perplexity is 83.24281571915373
At time: 273.5753571987152 and batch: 400, loss is 4.4748074913024904 and perplexity is 87.77770081229349
At time: 274.01434683799744 and batch: 450, loss is 4.46246446609497 and perplexity is 86.70091749099929
At time: 274.4541482925415 and batch: 500, loss is 4.426848983764648 and perplexity is 83.66736388908656
At time: 274.8960440158844 and batch: 550, loss is 4.4341842079162594 and perplexity is 84.28333915380375
At time: 275.33515453338623 and batch: 600, loss is 4.516400737762451 and perplexity is 91.50565171604794
At time: 275.7753105163574 and batch: 650, loss is 4.4788919448852536 and perplexity is 88.13695794194821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.941992666207108 and perplexity of 140.04904269163558
Finished 44 epochs...
Completing Train Step...
At time: 276.6859040260315 and batch: 50, loss is 4.513421621322632 and perplexity is 91.23345138424382
At time: 277.14389276504517 and batch: 100, loss is 4.480179376602173 and perplexity is 88.25050133103267
At time: 277.5869629383087 and batch: 150, loss is 4.46786247253418 and perplexity is 87.17019504402552
At time: 278.0335612297058 and batch: 200, loss is 4.447833414077759 and perplexity is 85.44162669618547
At time: 278.47645020484924 and batch: 250, loss is 4.428405675888062 and perplexity is 83.79770964322174
At time: 278.92042565345764 and batch: 300, loss is 4.473258352279663 and perplexity is 87.64182622211901
At time: 279.3648850917816 and batch: 350, loss is 4.421292562484741 and perplexity is 83.20376194455736
At time: 279.81173968315125 and batch: 400, loss is 4.474334173202514 and perplexity is 87.73616386859383
At time: 280.25381422042847 and batch: 450, loss is 4.4620708656311034 and perplexity is 86.66679868468663
At time: 280.71245217323303 and batch: 500, loss is 4.426513881683349 and perplexity is 83.63933147843245
At time: 281.159588098526 and batch: 550, loss is 4.433919143676758 and perplexity is 84.26100161518043
At time: 281.60432863235474 and batch: 600, loss is 4.51630669593811 and perplexity is 91.49704676224215
At time: 282.0471918582916 and batch: 650, loss is 4.478713665008545 and perplexity is 88.12124629652958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9419911702473955 and perplexity of 140.04883318406664
Finished 45 epochs...
Completing Train Step...
At time: 282.9725248813629 and batch: 50, loss is 4.513032159805298 and perplexity is 91.19792638409538
At time: 283.41542506217957 and batch: 100, loss is 4.4797704219818115 and perplexity is 88.21441825943643
At time: 283.85906744003296 and batch: 150, loss is 4.4673286724090575 and perplexity is 87.12367600004441
At time: 284.3049945831299 and batch: 200, loss is 4.447206134796143 and perplexity is 85.38804774021273
At time: 284.75365948677063 and batch: 250, loss is 4.4279030418396 and perplexity is 83.75560064476667
At time: 285.2008681297302 and batch: 300, loss is 4.472584028244018 and perplexity is 87.58274715362626
At time: 285.6473071575165 and batch: 350, loss is 4.420801143646241 and perplexity is 83.16288409339943
At time: 286.0963296890259 and batch: 400, loss is 4.473830223083496 and perplexity is 87.69196035748791
At time: 286.5418713092804 and batch: 450, loss is 4.461688137054443 and perplexity is 86.63363517089934
At time: 286.989625453949 and batch: 500, loss is 4.426171894073486 and perplexity is 83.61073275385307
At time: 287.4391212463379 and batch: 550, loss is 4.433664121627808 and perplexity is 84.2395159416786
At time: 287.88682746887207 and batch: 600, loss is 4.516174230575562 and perplexity is 91.4849273754879
At time: 288.334353685379 and batch: 650, loss is 4.478537454605102 and perplexity is 88.10571978417445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.941932827818627 and perplexity of 140.04066263333974
Finished 46 epochs...
Completing Train Step...
At time: 289.25118613243103 and batch: 50, loss is 4.512638988494873 and perplexity is 91.16207702380275
At time: 289.7130250930786 and batch: 100, loss is 4.479337768554688 and perplexity is 88.17626024424798
At time: 290.1611216068268 and batch: 150, loss is 4.466754789352417 and perplexity is 87.07369154254422
At time: 290.6106426715851 and batch: 200, loss is 4.446625108718872 and perplexity is 85.3384494681285
At time: 291.0528244972229 and batch: 250, loss is 4.427407426834106 and perplexity is 83.71410039721519
At time: 291.49464321136475 and batch: 300, loss is 4.471983814239502 and perplexity is 87.53019453521715
At time: 291.93880105018616 and batch: 350, loss is 4.420312662124633 and perplexity is 83.12227048154328
At time: 292.40184593200684 and batch: 400, loss is 4.473377532958985 and perplexity is 87.65227205696365
At time: 292.84502720832825 and batch: 450, loss is 4.461336994171143 and perplexity is 86.60321972685055
At time: 293.2890610694885 and batch: 500, loss is 4.425874099731446 and perplexity is 83.58583765769845
At time: 293.73387908935547 and batch: 550, loss is 4.433391056060791 and perplexity is 84.21651617085843
At time: 294.1814455986023 and batch: 600, loss is 4.516050081253052 and perplexity is 91.4735702887363
At time: 294.6249532699585 and batch: 650, loss is 4.4783561897277835 and perplexity is 88.08975075904212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.941944197112439 and perplexity of 140.04225480582966
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 295.5504946708679 and batch: 50, loss is 4.512281045913697 and perplexity is 91.12945207392514
At time: 295.9927294254303 and batch: 100, loss is 4.47838828086853 and perplexity is 88.09257770499177
At time: 296.4350173473358 and batch: 150, loss is 4.465384168624878 and perplexity is 86.9544282871083
At time: 296.8766887187958 and batch: 200, loss is 4.44498836517334 and perplexity is 85.19888655727918
At time: 297.3219630718231 and batch: 250, loss is 4.424444427490235 and perplexity is 83.46642268831431
At time: 297.76410818099976 and batch: 300, loss is 4.46792293548584 and perplexity is 87.17546577065487
At time: 298.20718336105347 and batch: 350, loss is 4.4163232517242434 and perplexity is 82.7913222143407
At time: 298.6515784263611 and batch: 400, loss is 4.4694019794464115 and perplexity is 87.30449751510199
At time: 299.09506464004517 and batch: 450, loss is 4.4559112071990965 and perplexity is 86.13460156631783
At time: 299.53573179244995 and batch: 500, loss is 4.419048662185669 and perplexity is 83.01727031081259
At time: 299.98153805732727 and batch: 550, loss is 4.42656699180603 and perplexity is 83.64377369155042
At time: 300.43174624443054 and batch: 600, loss is 4.509731636047364 and perplexity is 90.89742164569914
At time: 300.87658190727234 and batch: 650, loss is 4.4740258312225345 and perplexity is 87.70911529643021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.94005599676394 and perplexity of 139.7780764608119
Finished 48 epochs...
Completing Train Step...
At time: 301.8077609539032 and batch: 50, loss is 4.510465221405029 and perplexity is 90.96412712735885
At time: 302.27136731147766 and batch: 100, loss is 4.477751474380494 and perplexity is 88.0364976379318
At time: 302.7185146808624 and batch: 150, loss is 4.464619064331055 and perplexity is 86.88792452506006
At time: 303.16449999809265 and batch: 200, loss is 4.444306306838989 and perplexity is 85.14079575953375
At time: 303.6141803264618 and batch: 250, loss is 4.423981971740723 and perplexity is 83.42783208516184
At time: 304.0724115371704 and batch: 300, loss is 4.467617082595825 and perplexity is 87.1488069795525
At time: 304.51713967323303 and batch: 350, loss is 4.4162470149993895 and perplexity is 82.7850107156758
At time: 304.96392154693604 and batch: 400, loss is 4.46932692527771 and perplexity is 87.29794519450967
At time: 305.41328525543213 and batch: 450, loss is 4.455876264572144 and perplexity is 86.13159184965157
At time: 305.8590157032013 and batch: 500, loss is 4.419127550125122 and perplexity is 83.02381963053418
At time: 306.3039770126343 and batch: 550, loss is 4.426858005523681 and perplexity is 83.66811871928745
At time: 306.74573040008545 and batch: 600, loss is 4.5099482917785645 and perplexity is 90.91711722655326
At time: 307.18758177757263 and batch: 650, loss is 4.473741846084595 and perplexity is 87.68421074765241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.939722397748162 and perplexity of 139.73145440904491
Finished 49 epochs...
Completing Train Step...
At time: 308.1079535484314 and batch: 50, loss is 4.509736852645874 and perplexity is 90.8978958222903
At time: 308.55291986465454 and batch: 100, loss is 4.477304420471191 and perplexity is 87.99714937355452
At time: 308.99442958831787 and batch: 150, loss is 4.464174671173096 and perplexity is 86.84932070416407
At time: 309.436163187027 and batch: 200, loss is 4.443954563140869 and perplexity is 85.11085328751902
At time: 309.88136172294617 and batch: 250, loss is 4.423721885681152 and perplexity is 83.40613649053942
At time: 310.3245508670807 and batch: 300, loss is 4.467478141784668 and perplexity is 87.13669929476468
At time: 310.7658271789551 and batch: 350, loss is 4.416342620849609 and perplexity is 82.79292582537009
At time: 311.2066705226898 and batch: 400, loss is 4.469389820098877 and perplexity is 87.30343595582936
At time: 311.65263628959656 and batch: 450, loss is 4.455883636474609 and perplexity is 86.13222680568629
At time: 312.0946452617645 and batch: 500, loss is 4.419230003356933 and perplexity is 83.03232612492413
At time: 312.5361728668213 and batch: 550, loss is 4.427058200836182 and perplexity is 83.68487036120496
At time: 312.9785408973694 and batch: 600, loss is 4.510041532516479 and perplexity is 90.92559480087402
At time: 313.4222049713135 and batch: 650, loss is 4.4735016250610355 and perplexity is 87.66314968655274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.939521939146752 and perplexity of 139.70344684439354
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f93af975898>
SETTINGS FOR THIS RUN
{'lr': 23.86549757773326, 'data': 'ptb', 'num_layers': 1, 'anneal': 3.477683734440822, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.2140254870211421, 'seq_len': 20, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7279036045074463 and batch: 50, loss is 6.212283935546875 and perplexity is 498.839267950008
At time: 1.190786600112915 and batch: 100, loss is 5.520576295852661 and perplexity is 249.77894228827802
At time: 1.6376903057098389 and batch: 150, loss is 5.409124584197998 and perplexity is 223.43590272021385
At time: 2.0871822834014893 and batch: 200, loss is 5.36939866065979 and perplexity is 214.7337010496223
At time: 2.5351879596710205 and batch: 250, loss is 5.336931428909302 and perplexity is 207.8738549021411
At time: 2.982909679412842 and batch: 300, loss is 5.36127049446106 and perplexity is 212.9953840741148
At time: 3.4288318157196045 and batch: 350, loss is 5.285730504989624 and perplexity is 197.49840433451442
At time: 3.8911261558532715 and batch: 400, loss is 5.32257963180542 and perplexity is 204.91179777689436
At time: 4.339200735092163 and batch: 450, loss is 5.276522827148438 and perplexity is 195.68824910527988
At time: 4.779906988143921 and batch: 500, loss is 5.277124271392823 and perplexity is 195.80598007715676
At time: 5.224039077758789 and batch: 550, loss is 5.29125675201416 and perplexity is 198.5928506087416
At time: 5.672717332839966 and batch: 600, loss is 5.328753480911255 and perplexity is 206.18080559693888
At time: 6.1165547370910645 and batch: 650, loss is 5.279826965332031 and perplexity is 196.33589949520305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.229876948337929 and perplexity of 186.76981976935542
Finished 1 epochs...
Completing Train Step...
At time: 7.041021823883057 and batch: 50, loss is 5.1703583526611325 and perplexity is 175.97788833075504
At time: 7.486008405685425 and batch: 100, loss is 5.150320978164673 and perplexity is 172.48684594364644
At time: 7.928843021392822 and batch: 150, loss is 5.12997859954834 and perplexity is 169.0135010409257
At time: 8.370038747787476 and batch: 200, loss is 5.117082977294922 and perplexity is 166.8479597869482
At time: 8.81186032295227 and batch: 250, loss is 5.09401382446289 and perplexity is 163.04297632048733
At time: 9.2545804977417 and batch: 300, loss is 5.158675880432129 and perplexity is 173.93399365467778
At time: 9.696709871292114 and batch: 350, loss is 5.138564472198486 and perplexity is 170.47087690601313
At time: 10.136989116668701 and batch: 400, loss is 5.179006290435791 and perplexity is 177.5063335828522
At time: 10.580568313598633 and batch: 450, loss is 5.136193256378174 and perplexity is 170.06713253748472
At time: 11.021981716156006 and batch: 500, loss is 5.1243148517608645 and perplexity is 168.0589568986459
At time: 11.463598251342773 and batch: 550, loss is 5.137339267730713 and perplexity is 170.2621431228958
At time: 11.905518531799316 and batch: 600, loss is 5.205080156326294 and perplexity is 182.19547630353796
At time: 12.349852323532104 and batch: 650, loss is 5.169567089080811 and perplexity is 175.8386985119564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.312282786649816 and perplexity of 202.8126784246607
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 13.25755262374878 and batch: 50, loss is 5.092006883621216 and perplexity is 162.71608684600085
At time: 13.714598178863525 and batch: 100, loss is 5.014377403259277 and perplexity is 150.56236793930833
At time: 14.156487464904785 and batch: 150, loss is 4.9454085159301755 and perplexity is 140.52824715459815
At time: 14.59728217124939 and batch: 200, loss is 4.92154746055603 and perplexity is 137.21478344666332
At time: 15.037283897399902 and batch: 250, loss is 4.8994903659820555 and perplexity is 134.22135848131313
At time: 15.496453762054443 and batch: 300, loss is 4.937931299209595 and perplexity is 139.48140560313348
At time: 15.937805891036987 and batch: 350, loss is 4.898709993362427 and perplexity is 134.1166566667131
At time: 16.379366397857666 and batch: 400, loss is 4.916912937164307 and perplexity is 136.5803296541695
At time: 16.819870710372925 and batch: 450, loss is 4.876738405227661 and perplexity is 131.2020373835775
At time: 17.264508485794067 and batch: 500, loss is 4.842053108215332 and perplexity is 126.72927372783091
At time: 17.708807706832886 and batch: 550, loss is 4.842148685455323 and perplexity is 126.74138674089482
At time: 18.154000759124756 and batch: 600, loss is 4.900429735183716 and perplexity is 134.34750112965904
At time: 18.601564407348633 and batch: 650, loss is 4.850949287414551 and perplexity is 127.86170976506506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.064681707643995 and perplexity of 158.33003881154093
Finished 3 epochs...
Completing Train Step...
At time: 19.531648635864258 and batch: 50, loss is 4.884579801559449 and perplexity is 132.2348887644732
At time: 19.978203296661377 and batch: 100, loss is 4.860310802459717 and perplexity is 129.06430937915266
At time: 20.426384687423706 and batch: 150, loss is 4.807325916290283 and perplexity is 122.40386131726049
At time: 20.870782613754272 and batch: 200, loss is 4.788829746246338 and perplexity is 120.16066787883463
At time: 21.317086935043335 and batch: 250, loss is 4.7736560726165775 and perplexity is 118.35115233891675
At time: 21.763614654541016 and batch: 300, loss is 4.823896465301513 and perplexity is 124.44905872448037
At time: 22.2100772857666 and batch: 350, loss is 4.791203107833862 and perplexity is 120.44619128342411
At time: 22.65585231781006 and batch: 400, loss is 4.820880861282348 and perplexity is 124.07433493606474
At time: 23.094897031784058 and batch: 450, loss is 4.788257236480713 and perplexity is 120.0918944116492
At time: 23.540656089782715 and batch: 500, loss is 4.755671920776368 and perplexity is 116.2417321649328
At time: 23.986153602600098 and batch: 550, loss is 4.770203399658203 and perplexity is 117.94322913358971
At time: 24.43138813972473 and batch: 600, loss is 4.836408042907715 and perplexity is 126.01589413287564
At time: 24.876993894577026 and batch: 650, loss is 4.776981477737427 and perplexity is 118.74537297533979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.032353419883578 and perplexity of 153.29335213723726
Finished 4 epochs...
Completing Train Step...
At time: 25.79402184486389 and batch: 50, loss is 4.7940096759796145 and perplexity is 120.78470653802049
At time: 26.254905700683594 and batch: 100, loss is 4.785161991119384 and perplexity is 119.72075521229056
At time: 26.696998119354248 and batch: 150, loss is 4.742059364318847 and perplexity is 114.67010621527349
At time: 27.152964115142822 and batch: 200, loss is 4.718227548599243 and perplexity is 111.96961598018811
At time: 27.594226360321045 and batch: 250, loss is 4.703646450042725 and perplexity is 110.34882117476324
At time: 28.035864114761353 and batch: 300, loss is 4.7594821166992185 and perplexity is 116.68548078664067
At time: 28.478526830673218 and batch: 350, loss is 4.7186549949645995 and perplexity is 112.0174872160332
At time: 28.91893744468689 and batch: 400, loss is 4.750063095092774 and perplexity is 115.5915775584179
At time: 29.364872932434082 and batch: 450, loss is 4.721319932937622 and perplexity is 112.31640499311837
At time: 29.812620401382446 and batch: 500, loss is 4.694868564605713 and perplexity is 109.3844307111696
At time: 30.28533935546875 and batch: 550, loss is 4.717407207489014 and perplexity is 111.87780036631334
At time: 30.74867296218872 and batch: 600, loss is 4.790028886795044 and perplexity is 120.30484383438859
At time: 31.212012767791748 and batch: 650, loss is 4.73681637763977 and perplexity is 114.0704657031285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.029833625344669 and perplexity of 152.90757063485455
Finished 5 epochs...
Completing Train Step...
At time: 32.15008735656738 and batch: 50, loss is 4.741764078140259 and perplexity is 114.63625071658723
At time: 32.589890003204346 and batch: 100, loss is 4.731266765594483 and perplexity is 113.43917220986215
At time: 33.03194451332092 and batch: 150, loss is 4.6865302848815915 and perplexity is 108.47614476410423
At time: 33.471734046936035 and batch: 200, loss is 4.660244998931884 and perplexity is 105.66196605459054
At time: 33.911439180374146 and batch: 250, loss is 4.645605535507202 and perplexity is 104.12639893087513
At time: 34.35321593284607 and batch: 300, loss is 4.708978519439698 and perplexity is 110.93878020061683
At time: 34.79533267021179 and batch: 350, loss is 4.674077768325805 and perplexity is 107.13371940940485
At time: 35.235918283462524 and batch: 400, loss is 4.7133126544952395 and perplexity is 111.42064734122485
At time: 35.67594289779663 and batch: 450, loss is 4.6850800132751464 and perplexity is 108.31893891452033
At time: 36.115716218948364 and batch: 500, loss is 4.657814416885376 and perplexity is 105.40545783531749
At time: 36.55665135383606 and batch: 550, loss is 4.671725130081176 and perplexity is 106.88196877903151
At time: 36.996941328048706 and batch: 600, loss is 4.747766361236573 and perplexity is 115.32639910747861
At time: 37.4370973110199 and batch: 650, loss is 4.697011642456054 and perplexity is 109.61910143079363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.030351227405024 and perplexity of 152.9867363948718
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 38.34200119972229 and batch: 50, loss is 4.686251163482666 and perplexity is 108.44587097605522
At time: 38.802489042282104 and batch: 100, loss is 4.632172174453736 and perplexity is 102.73698456511003
At time: 39.25026774406433 and batch: 150, loss is 4.578167200088501 and perplexity is 97.33583351697435
At time: 39.69507932662964 and batch: 200, loss is 4.54410719871521 and perplexity is 94.07639817521431
At time: 40.14061903953552 and batch: 250, loss is 4.525522031784058 and perplexity is 92.34411981378099
At time: 40.58648729324341 and batch: 300, loss is 4.577682294845581 and perplexity is 97.28864630256643
At time: 41.03287625312805 and batch: 350, loss is 4.531704397201538 and perplexity is 92.91679332102638
At time: 41.477410316467285 and batch: 400, loss is 4.5583228015899655 and perplexity is 95.42330173244339
At time: 41.92346167564392 and batch: 450, loss is 4.526521682739258 and perplexity is 92.43647785657167
At time: 42.36844348907471 and batch: 500, loss is 4.492303714752198 and perplexity is 89.32699293825831
At time: 42.81365084648132 and batch: 550, loss is 4.499656715393066 and perplexity is 89.98623510838303
At time: 43.25868535041809 and batch: 600, loss is 4.576835231781006 and perplexity is 97.20627157690022
At time: 43.70415449142456 and batch: 650, loss is 4.5498592567443845 and perplexity is 94.61909037891792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9425701066559435 and perplexity of 140.1299360270006
Finished 7 epochs...
Completing Train Step...
At time: 44.62771201133728 and batch: 50, loss is 4.60999716758728 and perplexity is 100.48386502420884
At time: 45.067749977111816 and batch: 100, loss is 4.579185924530029 and perplexity is 97.43504243430915
At time: 45.5047652721405 and batch: 150, loss is 4.541101484298706 and perplexity is 93.79405592162146
At time: 45.93883180618286 and batch: 200, loss is 4.510851011276245 and perplexity is 90.99922693638867
At time: 46.378416776657104 and batch: 250, loss is 4.497178688049316 and perplexity is 89.76352281476593
At time: 46.81574726104736 and batch: 300, loss is 4.551883316040039 and perplexity is 94.81079897768184
At time: 47.252676010131836 and batch: 350, loss is 4.5103258037567135 and perplexity is 90.95144600668021
At time: 47.68933987617493 and batch: 400, loss is 4.541480226516724 and perplexity is 93.82958641842292
At time: 48.127556562423706 and batch: 450, loss is 4.513335590362549 and perplexity is 91.22560282044414
At time: 48.564980268478394 and batch: 500, loss is 4.481994171142578 and perplexity is 88.41080327258443
At time: 49.00268316268921 and batch: 550, loss is 4.495568008422851 and perplexity is 89.61905891113341
At time: 49.44021248817444 and batch: 600, loss is 4.574729042053223 and perplexity is 97.00175218015973
At time: 49.89298868179321 and batch: 650, loss is 4.53625638961792 and perplexity is 93.34071396937462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.940997553806679 and perplexity of 139.9097474712802
Finished 8 epochs...
Completing Train Step...
At time: 50.79805874824524 and batch: 50, loss is 4.58278790473938 and perplexity is 97.78663436230093
At time: 51.252644062042236 and batch: 100, loss is 4.555577363967895 and perplexity is 95.16168230408374
At time: 51.69326639175415 and batch: 150, loss is 4.519495334625244 and perplexity is 91.78926342439306
At time: 52.13534760475159 and batch: 200, loss is 4.488741807937622 and perplexity is 89.00938449502135
At time: 52.57644987106323 and batch: 250, loss is 4.476844348907471 and perplexity is 87.95667369900666
At time: 53.0176465511322 and batch: 300, loss is 4.532549781799316 and perplexity is 92.99537695899339
At time: 53.45801544189453 and batch: 350, loss is 4.49392427444458 and perplexity is 89.47187002173015
At time: 53.899107456207275 and batch: 400, loss is 4.526219844818115 and perplexity is 92.40858123259899
At time: 54.340238094329834 and batch: 450, loss is 4.499072675704956 and perplexity is 89.93369491996718
At time: 54.78130340576172 and batch: 500, loss is 4.467961254119873 and perplexity is 87.17880627942591
At time: 55.2257444858551 and batch: 550, loss is 4.483179740905761 and perplexity is 88.51568260628196
At time: 55.6709680557251 and batch: 600, loss is 4.5624019813537595 and perplexity is 95.81334552238118
At time: 56.11498427391052 and batch: 650, loss is 4.520179758071899 and perplexity is 91.85210765201016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.93772050446155 and perplexity of 139.45200675405286
Finished 9 epochs...
Completing Train Step...
At time: 57.04691171646118 and batch: 50, loss is 4.560999851226807 and perplexity is 95.67909688308502
At time: 57.491822719573975 and batch: 100, loss is 4.535962514877319 and perplexity is 93.31328752143689
At time: 57.9365508556366 and batch: 150, loss is 4.501637620925903 and perplexity is 90.16466600842023
At time: 58.38077974319458 and batch: 200, loss is 4.472216024398803 and perplexity is 87.55052229569874
At time: 58.829850912094116 and batch: 250, loss is 4.461255779266358 and perplexity is 86.59618654021
At time: 59.272376537323 and batch: 300, loss is 4.516522607803345 and perplexity is 91.51680419312719
At time: 59.718279123306274 and batch: 350, loss is 4.479379577636719 and perplexity is 88.17994688981283
At time: 60.162800312042236 and batch: 400, loss is 4.5128161811828615 and perplexity is 91.17823170847691
At time: 60.60521674156189 and batch: 450, loss is 4.485885334014893 and perplexity is 88.75549429733117
At time: 61.04906415939331 and batch: 500, loss is 4.45588189125061 and perplexity is 86.13207648578815
At time: 61.50308918952942 and batch: 550, loss is 4.471447229385376 and perplexity is 87.48323975728877
At time: 61.94181203842163 and batch: 600, loss is 4.550510015487671 and perplexity is 94.68068461858509
At time: 62.38356399536133 and batch: 650, loss is 4.505996904373169 and perplexity is 90.55857730617625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.937979604683671 and perplexity of 139.4881434812928
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 63.288267374038696 and batch: 50, loss is 4.540085124969482 and perplexity is 93.69877588543851
At time: 63.74685335159302 and batch: 100, loss is 4.506116285324096 and perplexity is 90.56938892058726
At time: 64.18859219551086 and batch: 150, loss is 4.462432918548584 and perplexity is 86.69818233292696
At time: 64.63104677200317 and batch: 200, loss is 4.42744857788086 and perplexity is 83.71754539095662
At time: 65.07265424728394 and batch: 250, loss is 4.412821674346924 and perplexity is 82.50192895556921
At time: 65.51438522338867 and batch: 300, loss is 4.463774347305298 and perplexity is 86.81455980645694
At time: 65.9561505317688 and batch: 350, loss is 4.422043266296387 and perplexity is 83.26624677665879
At time: 66.39684200286865 and batch: 400, loss is 4.449994850158691 and perplexity is 85.62650303816265
At time: 66.83859491348267 and batch: 450, loss is 4.41648983001709 and perplexity is 82.80511460018182
At time: 67.28090000152588 and batch: 500, loss is 4.382429218292236 and perplexity is 80.0322131732812
At time: 67.72350597381592 and batch: 550, loss is 4.394779386520386 and perplexity is 81.02675319681943
At time: 68.16608881950378 and batch: 600, loss is 4.479100637435913 and perplexity is 88.15535338793796
At time: 68.60774683952332 and batch: 650, loss is 4.452005863189697 and perplexity is 85.79887231190877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.906463922238817 and perplexity of 135.16062990014893
Finished 11 epochs...
Completing Train Step...
At time: 69.52662897109985 and batch: 50, loss is 4.5116027736663815 and perplexity is 91.06766245313172
At time: 69.96818232536316 and batch: 100, loss is 4.485269975662232 and perplexity is 88.70089466346403
At time: 70.41264581680298 and batch: 150, loss is 4.4457165813446045 and perplexity is 85.2609523601618
At time: 70.85729765892029 and batch: 200, loss is 4.413339014053345 and perplexity is 82.54462152160181
At time: 71.30039715766907 and batch: 250, loss is 4.401410884857178 and perplexity is 81.56586756820073
At time: 71.74593353271484 and batch: 300, loss is 4.454295520782471 and perplexity is 85.9955474247589
At time: 72.1916298866272 and batch: 350, loss is 4.4134695339202885 and perplexity is 82.55539593774212
At time: 72.6367118358612 and batch: 400, loss is 4.443828458786011 and perplexity is 85.10012111497485
At time: 73.09716868400574 and batch: 450, loss is 4.413701391220092 and perplexity is 82.57453922809853
At time: 73.54229378700256 and batch: 500, loss is 4.383913440704346 and perplexity is 80.15108697352058
At time: 73.98728442192078 and batch: 550, loss is 4.3975683689117435 and perplexity is 81.25305080805416
At time: 74.43215727806091 and batch: 600, loss is 4.479963607788086 and perplexity is 88.23146167917331
At time: 74.87775921821594 and batch: 650, loss is 4.4472143268585205 and perplexity is 85.38874724729132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904384538239124 and perplexity of 134.87987105283645
Finished 12 epochs...
Completing Train Step...
At time: 75.79083180427551 and batch: 50, loss is 4.5015805721282955 and perplexity is 90.15952236935836
At time: 76.25108170509338 and batch: 100, loss is 4.476123104095459 and perplexity is 87.8932582761908
At time: 76.69546604156494 and batch: 150, loss is 4.4371451568603515 and perplexity is 84.53326764803319
At time: 77.13995790481567 and batch: 200, loss is 4.405648937225342 and perplexity is 81.91228152816372
At time: 77.58401846885681 and batch: 250, loss is 4.3953994846344 and perplexity is 81.07701331515318
At time: 78.0236279964447 and batch: 300, loss is 4.448914403915405 and perplexity is 85.53403816528665
At time: 78.46306538581848 and batch: 350, loss is 4.408667411804199 and perplexity is 82.1599052025236
At time: 78.90912103652954 and batch: 400, loss is 4.440020446777344 and perplexity is 84.7766750659058
At time: 79.35078883171082 and batch: 450, loss is 4.411714019775391 and perplexity is 82.41059590887265
At time: 79.79125595092773 and batch: 500, loss is 4.383737201690674 and perplexity is 80.13696246968851
At time: 80.23215937614441 and batch: 550, loss is 4.39716944694519 and perplexity is 81.2206436456316
At time: 80.67330551147461 and batch: 600, loss is 4.4783941268920895 and perplexity is 88.09309269778177
At time: 81.11327481269836 and batch: 650, loss is 4.442975797653198 and perplexity is 85.02759047572532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.903825049306832 and perplexity of 134.804428364436
Finished 13 epochs...
Completing Train Step...
At time: 82.03248834609985 and batch: 50, loss is 4.494277772903442 and perplexity is 89.50350378080749
At time: 82.47335433959961 and batch: 100, loss is 4.4695376110076905 and perplexity is 87.31633956346624
At time: 82.91433453559875 and batch: 150, loss is 4.4308473587036135 and perplexity is 84.0025670674896
At time: 83.3544557094574 and batch: 200, loss is 4.399877986907959 and perplexity is 81.44093119889568
At time: 83.796213388443 and batch: 250, loss is 4.390524702072144 and perplexity is 80.68274227800454
At time: 84.23643159866333 and batch: 300, loss is 4.444921398162842 and perplexity is 85.19318123358507
At time: 84.69338750839233 and batch: 350, loss is 4.4045201587677 and perplexity is 81.8198728736195
At time: 85.1343240737915 and batch: 400, loss is 4.436801500320435 and perplexity is 84.50422222887539
At time: 85.57559633255005 and batch: 450, loss is 4.409356985092163 and perplexity is 82.21658001696622
At time: 86.01589965820312 and batch: 500, loss is 4.382721900939941 and perplexity is 80.05564064157397
At time: 86.4558653831482 and batch: 550, loss is 4.395634880065918 and perplexity is 81.09610072014458
At time: 86.8966932296753 and batch: 600, loss is 4.476141662597656 and perplexity is 87.89488945855375
At time: 87.33744287490845 and batch: 650, loss is 4.438495531082153 and perplexity is 84.64749630190643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.903549792719822 and perplexity of 134.76732766391095
Finished 14 epochs...
Completing Train Step...
At time: 88.23867845535278 and batch: 50, loss is 4.487854156494141 and perplexity is 88.93041024238116
At time: 88.69029426574707 and batch: 100, loss is 4.4638792991638185 and perplexity is 86.82367163399852
At time: 89.12715148925781 and batch: 150, loss is 4.4250871944427494 and perplexity is 83.52008939220715
At time: 89.56510329246521 and batch: 200, loss is 4.394702138900757 and perplexity is 81.02049431475369
At time: 90.00334405899048 and batch: 250, loss is 4.386327438354492 and perplexity is 80.34480523277007
At time: 90.4395341873169 and batch: 300, loss is 4.441085538864136 and perplexity is 84.86701813496643
At time: 90.87972974777222 and batch: 350, loss is 4.400423183441162 and perplexity is 81.48534461816712
At time: 91.32033395767212 and batch: 400, loss is 4.433057107925415 and perplexity is 84.1883969177627
At time: 91.76238179206848 and batch: 450, loss is 4.406731367111206 and perplexity is 82.00099383347268
At time: 92.20471978187561 and batch: 500, loss is 4.3810921478271485 and perplexity is 79.92527597200983
At time: 92.64638423919678 and batch: 550, loss is 4.393491430282593 and perplexity is 80.92246146062439
At time: 93.0875518321991 and batch: 600, loss is 4.473448667526245 and perplexity is 87.65850738517689
At time: 93.52886080741882 and batch: 650, loss is 4.434185514450073 and perplexity is 84.28344927290821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.903569240196078 and perplexity of 134.76994857380075
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 94.45864748954773 and batch: 50, loss is 4.482983627319336 and perplexity is 88.49832518038029
At time: 94.90435576438904 and batch: 100, loss is 4.4554245662689205 and perplexity is 86.09269514120912
At time: 95.34895634651184 and batch: 150, loss is 4.414850625991821 and perplexity is 82.66949131050575
At time: 95.79548454284668 and batch: 200, loss is 4.380890865325927 and perplexity is 79.90919003151487
At time: 96.25441360473633 and batch: 250, loss is 4.3708558273315425 and perplexity is 79.11130835559253
At time: 96.69521641731262 and batch: 300, loss is 4.423407354354858 and perplexity is 83.37990677305818
At time: 97.13527417182922 and batch: 350, loss is 4.379278182983398 and perplexity is 79.78042574759569
At time: 97.57611298561096 and batch: 400, loss is 4.41176362991333 and perplexity is 82.41468441131808
At time: 98.01746726036072 and batch: 450, loss is 4.384464349746704 and perplexity is 80.19525509728143
At time: 98.46110391616821 and batch: 500, loss is 4.354238862991333 and perplexity is 77.80758056387162
At time: 98.90204334259033 and batch: 550, loss is 4.366141004562378 and perplexity is 78.73919048190528
At time: 99.34360575675964 and batch: 600, loss is 4.447561464309692 and perplexity is 85.41839402482519
At time: 99.78515243530273 and batch: 650, loss is 4.417060785293579 and perplexity is 82.85240611666877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.896899354224112 and perplexity of 133.87403951139416
Finished 16 epochs...
Completing Train Step...
At time: 100.70606064796448 and batch: 50, loss is 4.476773662567139 and perplexity is 87.95045658337025
At time: 101.1619725227356 and batch: 100, loss is 4.448679656982422 and perplexity is 85.51396166870182
At time: 101.60291934013367 and batch: 150, loss is 4.409276142120361 and perplexity is 82.20993365296576
At time: 102.04410982131958 and batch: 200, loss is 4.3759091091156 and perplexity is 79.51209187218518
At time: 102.48456335067749 and batch: 250, loss is 4.36701376914978 and perplexity is 78.80794125625516
At time: 102.92540240287781 and batch: 300, loss is 4.419574613571167 and perplexity is 83.06094484348652
At time: 103.3673415184021 and batch: 350, loss is 4.3767628002166745 and perplexity is 79.57999961943985
At time: 103.80756735801697 and batch: 400, loss is 4.410297088623047 and perplexity is 82.29390845681178
At time: 104.24783492088318 and batch: 450, loss is 4.384066934585571 and perplexity is 80.16339061918761
At time: 104.69300556182861 and batch: 500, loss is 4.355273942947388 and perplexity is 77.88815932647918
At time: 105.13691067695618 and batch: 550, loss is 4.367632713317871 and perplexity is 78.85673407035114
At time: 105.58050441741943 and batch: 600, loss is 4.449115066528321 and perplexity is 85.55120337102801
At time: 106.02430725097656 and batch: 650, loss is 4.416739931106568 and perplexity is 82.82582683952648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.896058325674019 and perplexity of 133.76149495527514
Finished 17 epochs...
Completing Train Step...
At time: 106.95203018188477 and batch: 50, loss is 4.47333981513977 and perplexity is 87.64896606676022
At time: 107.3988299369812 and batch: 100, loss is 4.445147018432618 and perplexity is 85.2124047106395
At time: 107.85981822013855 and batch: 150, loss is 4.40641297340393 and perplexity is 81.97488938901171
At time: 108.3057291507721 and batch: 200, loss is 4.3730302429199215 and perplexity is 79.28351637574858
At time: 108.75205945968628 and batch: 250, loss is 4.364805355072021 and perplexity is 78.63409272477408
At time: 109.19707942008972 and batch: 300, loss is 4.417223491668701 and perplexity is 82.86588782808865
At time: 109.64282488822937 and batch: 350, loss is 4.375121231079102 and perplexity is 79.44947071352016
At time: 110.08978867530823 and batch: 400, loss is 4.4094036674499515 and perplexity is 82.22041817035702
At time: 110.53041076660156 and batch: 450, loss is 4.3834288215637205 and perplexity is 80.11225363308135
At time: 110.97316312789917 and batch: 500, loss is 4.355151920318604 and perplexity is 77.87865578836212
At time: 111.41470861434937 and batch: 550, loss is 4.368227281570435 and perplexity is 78.90363372207058
At time: 111.85578870773315 and batch: 600, loss is 4.449707260131836 and perplexity is 85.6018812505144
At time: 112.30047798156738 and batch: 650, loss is 4.416199445724487 and perplexity is 82.78107278640624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8955942789713545 and perplexity of 133.69943777442631
Finished 18 epochs...
Completing Train Step...
At time: 113.20683360099792 and batch: 50, loss is 4.470681848526001 and perplexity is 87.41630737771102
At time: 113.65909671783447 and batch: 100, loss is 4.442402782440186 and perplexity is 84.97888232944412
At time: 114.0955765247345 and batch: 150, loss is 4.404257049560547 and perplexity is 81.79834814353987
At time: 114.53267908096313 and batch: 200, loss is 4.3708119964599605 and perplexity is 79.10784091398637
At time: 114.96909356117249 and batch: 250, loss is 4.363135156631469 and perplexity is 78.50286780206659
At time: 115.41575717926025 and batch: 300, loss is 4.415262842178345 and perplexity is 82.70357603761109
At time: 115.85776543617249 and batch: 350, loss is 4.373793020248413 and perplexity is 79.34401511516315
At time: 116.2998595237732 and batch: 400, loss is 4.40863639831543 and perplexity is 82.15735717673813
At time: 116.74050283432007 and batch: 450, loss is 4.382872381210327 and perplexity is 80.06768834247153
At time: 117.182302236557 and batch: 500, loss is 4.354951610565186 and perplexity is 77.8630574963218
At time: 117.62327313423157 and batch: 550, loss is 4.368462047576904 and perplexity is 78.92215978761558
At time: 118.06426310539246 and batch: 600, loss is 4.4498403453826905 and perplexity is 85.61327435646459
At time: 118.50742959976196 and batch: 650, loss is 4.41550274848938 and perplexity is 82.72341952764172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.895479987649357 and perplexity of 133.68415796212435
Finished 19 epochs...
Completing Train Step...
At time: 119.43665027618408 and batch: 50, loss is 4.468466339111328 and perplexity is 87.22285010805263
At time: 119.88101005554199 and batch: 100, loss is 4.440177497863769 and perplexity is 84.78999038039338
At time: 120.32407426834106 and batch: 150, loss is 4.4023779773712155 and perplexity is 81.6447874633959
At time: 120.76880836486816 and batch: 200, loss is 4.368879241943359 and perplexity is 78.95509253726858
At time: 121.21341848373413 and batch: 250, loss is 4.361539611816406 and perplexity is 78.37771283011195
At time: 121.65796399116516 and batch: 300, loss is 4.4134026145935055 and perplexity is 82.54987157106918
At time: 122.10263872146606 and batch: 350, loss is 4.372553949356079 and perplexity is 79.24576313870563
At time: 122.54706716537476 and batch: 400, loss is 4.4078088474273684 and perplexity is 82.08939590742867
At time: 122.99239730834961 and batch: 450, loss is 4.382175102233886 and perplexity is 80.01187828654868
At time: 123.4371132850647 and batch: 500, loss is 4.354621162414551 and perplexity is 77.8373320436676
At time: 123.88237714767456 and batch: 550, loss is 4.368545513153077 and perplexity is 78.92874734606838
At time: 124.32617330551147 and batch: 600, loss is 4.44980188369751 and perplexity is 85.6099815889821
At time: 124.76497673988342 and batch: 650, loss is 4.414877548217773 and perplexity is 82.67171698719008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8953279981426165 and perplexity of 133.66384091692464
Finished 20 epochs...
Completing Train Step...
At time: 125.6699116230011 and batch: 50, loss is 4.466440868377686 and perplexity is 87.04636157437129
At time: 126.12591075897217 and batch: 100, loss is 4.438159990310669 and perplexity is 84.61909838028978
At time: 126.5662133693695 and batch: 150, loss is 4.400672559738159 and perplexity is 81.50566766560539
At time: 127.00784540176392 and batch: 200, loss is 4.36714165687561 and perplexity is 78.81802046912982
At time: 127.44906234741211 and batch: 250, loss is 4.360166025161743 and perplexity is 78.2701281550978
At time: 127.89185070991516 and batch: 300, loss is 4.411666755676269 and perplexity is 82.40670093834581
At time: 128.3335783481598 and batch: 350, loss is 4.371353759765625 and perplexity is 79.15071025085216
At time: 128.77534174919128 and batch: 400, loss is 4.406982336044312 and perplexity is 82.02157611805764
At time: 129.21575784683228 and batch: 450, loss is 4.381556053161621 and perplexity is 79.96236233550822
At time: 129.65809869766235 and batch: 500, loss is 4.354242906570435 and perplexity is 77.80789518561446
At time: 130.0994782447815 and batch: 550, loss is 4.3683423328399655 and perplexity is 78.91271220753653
At time: 130.5549259185791 and batch: 600, loss is 4.449612102508545 and perplexity is 85.5937359664944
At time: 130.99519419670105 and batch: 650, loss is 4.414199609756469 and perplexity is 82.61568964427362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.895313337737439 and perplexity of 133.6618813652231
Finished 21 epochs...
Completing Train Step...
At time: 131.91292071342468 and batch: 50, loss is 4.464651441574096 and perplexity is 86.89073776205194
At time: 132.34981298446655 and batch: 100, loss is 4.4364151096344 and perplexity is 84.47157689180875
At time: 132.7904543876648 and batch: 150, loss is 4.39900426864624 and perplexity is 81.36980584634017
At time: 133.23052835464478 and batch: 200, loss is 4.365604858398438 and perplexity is 78.69698608185125
At time: 133.67096209526062 and batch: 250, loss is 4.358897218704223 and perplexity is 78.17088148678504
At time: 134.11065483093262 and batch: 300, loss is 4.410056791305542 and perplexity is 82.27413582711272
At time: 134.55196285247803 and batch: 350, loss is 4.370184717178344 and perplexity is 79.0582337647627
At time: 134.9945888519287 and batch: 400, loss is 4.406052274703979 and perplexity is 81.94532648495152
At time: 135.4347894191742 and batch: 450, loss is 4.380731563568116 and perplexity is 79.89646137095346
At time: 135.87550473213196 and batch: 500, loss is 4.353749341964722 and perplexity is 77.76950143818263
At time: 136.31569743156433 and batch: 550, loss is 4.36814058303833 and perplexity is 78.89679318938552
At time: 136.755934715271 and batch: 600, loss is 4.449309425354004 and perplexity is 85.56783261841915
At time: 137.19763135910034 and batch: 650, loss is 4.413357839584351 and perplexity is 82.54617548256068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.895138011259191 and perplexity of 133.63844895250864
Finished 22 epochs...
Completing Train Step...
At time: 138.11276173591614 and batch: 50, loss is 4.462863540649414 and perplexity is 86.73552452595108
At time: 138.57306170463562 and batch: 100, loss is 4.43464301109314 and perplexity is 84.32201748975885
At time: 139.01180124282837 and batch: 150, loss is 4.397468738555908 and perplexity is 81.24495594094337
At time: 139.45177698135376 and batch: 200, loss is 4.364024515151978 and perplexity is 78.57271605189484
At time: 139.8933174610138 and batch: 250, loss is 4.3574989318847654 and perplexity is 78.06165255801945
At time: 140.334641456604 and batch: 300, loss is 4.408525972366333 and perplexity is 82.14828537348707
At time: 140.77579164505005 and batch: 350, loss is 4.3689957618713375 and perplexity is 78.96429291496781
At time: 141.21662998199463 and batch: 400, loss is 4.405085935592651 and perplexity is 81.86617775939226
At time: 141.65772342681885 and batch: 450, loss is 4.3799956512451175 and perplexity is 79.83768620980187
At time: 142.11431860923767 and batch: 500, loss is 4.353226041793823 and perplexity is 77.72881529125158
At time: 142.5566246509552 and batch: 550, loss is 4.367933015823365 and perplexity is 78.88041850123649
At time: 142.99900650978088 and batch: 600, loss is 4.44892991065979 and perplexity is 85.53536453003643
At time: 143.43993496894836 and batch: 650, loss is 4.412547521591186 and perplexity is 82.47931392452053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.895081164790135 and perplexity of 133.63085229447913
Finished 23 epochs...
Completing Train Step...
At time: 144.36234307289124 and batch: 50, loss is 4.461265287399292 and perplexity is 86.59700991217758
At time: 144.803551197052 and batch: 100, loss is 4.433008623123169 and perplexity is 84.18431515893919
At time: 145.24549889564514 and batch: 150, loss is 4.3959069919586184 and perplexity is 81.11817093625011
At time: 145.68641328811646 and batch: 200, loss is 4.3627254676818845 and perplexity is 78.47071263187608
At time: 146.12767124176025 and batch: 250, loss is 4.356251983642578 and perplexity is 77.96437438061082
At time: 146.5685532093048 and batch: 300, loss is 4.407065238952637 and perplexity is 82.02837622713373
At time: 147.0102846622467 and batch: 350, loss is 4.36792025566101 and perplexity is 78.87941198071154
At time: 147.45536541938782 and batch: 400, loss is 4.40427225112915 and perplexity is 81.79959161619213
At time: 147.9013123512268 and batch: 450, loss is 4.3791838645935055 and perplexity is 79.77290134114472
At time: 148.34643840789795 and batch: 500, loss is 4.352684659957886 and perplexity is 77.6867457114047
At time: 148.79029941558838 and batch: 550, loss is 4.367622880935669 and perplexity is 78.85595872461434
At time: 149.23403549194336 and batch: 600, loss is 4.448359651565552 and perplexity is 85.4866011157459
At time: 149.68071150779724 and batch: 650, loss is 4.41182620048523 and perplexity is 82.41984130658788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.894992603975184 and perplexity of 133.61901836131648
Finished 24 epochs...
Completing Train Step...
At time: 150.59535479545593 and batch: 50, loss is 4.4596858406066895 and perplexity is 86.46034250048682
At time: 151.05471992492676 and batch: 100, loss is 4.431415119171143 and perplexity is 84.05027394599978
At time: 151.49795484542847 and batch: 150, loss is 4.394589252471924 and perplexity is 81.01134871670487
At time: 151.94309639930725 and batch: 200, loss is 4.3612206935882565 and perplexity is 78.35272073423904
At time: 152.38824081420898 and batch: 250, loss is 4.355080661773681 and perplexity is 77.87310646639078
At time: 152.83406019210815 and batch: 300, loss is 4.405696277618408 and perplexity is 81.91615937955697
At time: 153.27932357788086 and batch: 350, loss is 4.366890258789063 and perplexity is 78.79820826007851
At time: 153.73988270759583 and batch: 400, loss is 4.403432950973511 and perplexity is 81.73096600898732
At time: 154.17871642112732 and batch: 450, loss is 4.378464422225952 and perplexity is 79.71552997630712
At time: 154.61918425559998 and batch: 500, loss is 4.352140350341797 and perplexity is 77.64447157482458
At time: 155.0608856678009 and batch: 550, loss is 4.367232627868653 and perplexity is 78.82519094886943
At time: 155.5028989315033 and batch: 600, loss is 4.447849245071411 and perplexity is 85.44297933274207
At time: 155.94506883621216 and batch: 650, loss is 4.411101713180542 and perplexity is 82.36015080302374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.89505573347503 and perplexity of 133.62745392937939
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 156.86542439460754 and batch: 50, loss is 4.457958993911743 and perplexity is 86.31116758197335
At time: 157.30678176879883 and batch: 100, loss is 4.42917103767395 and perplexity is 83.86186975764556
At time: 157.7482750415802 and batch: 150, loss is 4.391589765548706 and perplexity is 80.76872029789061
At time: 158.1889204978943 and batch: 200, loss is 4.35749927520752 and perplexity is 78.0616793583656
At time: 158.63191962242126 and batch: 250, loss is 4.350388622283935 and perplexity is 77.50857863391738
At time: 159.07230496406555 and batch: 300, loss is 4.399239864349365 and perplexity is 81.38897848136823
At time: 159.5130259990692 and batch: 350, loss is 4.360237483978271 and perplexity is 78.27572144566784
At time: 159.9544768333435 and batch: 400, loss is 4.396297235488891 and perplexity is 81.14983295519274
At time: 160.3953800201416 and batch: 450, loss is 4.370635385513306 and perplexity is 79.0938708369853
At time: 160.83564710617065 and batch: 500, loss is 4.342340326309204 and perplexity is 76.88727024317184
At time: 161.2766695022583 and batch: 550, loss is 4.35774416923523 and perplexity is 78.08079853842443
At time: 161.71785283088684 and batch: 600, loss is 4.439159078598022 and perplexity is 84.70368257687217
At time: 162.1593143939972 and batch: 650, loss is 4.4055929946899415 and perplexity is 81.9076992756272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.893868539847579 and perplexity of 133.46890639955228
Finished 26 epochs...
Completing Train Step...
At time: 163.06657123565674 and batch: 50, loss is 4.455344982147217 and perplexity is 86.08584380231375
At time: 163.52622437477112 and batch: 100, loss is 4.427538623809815 and perplexity is 83.72508415451357
At time: 163.96991539001465 and batch: 150, loss is 4.3900211143493655 and perplexity is 80.64212166843186
At time: 164.4127869606018 and batch: 200, loss is 4.356392154693603 and perplexity is 77.97530349486507
At time: 164.85851001739502 and batch: 250, loss is 4.349413986206055 and perplexity is 77.4330727781781
At time: 165.31841945648193 and batch: 300, loss is 4.3987338161468506 and perplexity is 81.34780215458504
At time: 165.7616958618164 and batch: 350, loss is 4.359841165542602 and perplexity is 78.24470548069894
At time: 166.2060830593109 and batch: 400, loss is 4.3959447860717775 and perplexity is 81.12123678351685
At time: 166.65032720565796 and batch: 450, loss is 4.370556850433349 and perplexity is 79.08765943742462
At time: 167.09509825706482 and batch: 500, loss is 4.342433004379273 and perplexity is 76.89439633720207
At time: 167.5392029285431 and batch: 550, loss is 4.358266792297363 and perplexity is 78.12161602960246
At time: 167.9849500656128 and batch: 600, loss is 4.4395177459716795 and perplexity is 84.73406847313015
At time: 168.4306287765503 and batch: 650, loss is 4.405321025848389 and perplexity is 81.88542596250186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.893522673962163 and perplexity of 133.42275204013416
Finished 27 epochs...
Completing Train Step...
At time: 169.36853098869324 and batch: 50, loss is 4.453962154388428 and perplexity is 85.96688417715738
At time: 169.81077456474304 and batch: 100, loss is 4.42647156715393 and perplexity is 83.63579239435788
At time: 170.2527515888214 and batch: 150, loss is 4.389123868942261 and perplexity is 80.56979834588216
At time: 170.69420719146729 and batch: 200, loss is 4.355823650360107 and perplexity is 77.93098679523403
At time: 171.13566064834595 and batch: 250, loss is 4.348836841583252 and perplexity is 77.38839559043659
At time: 171.57636046409607 and batch: 300, loss is 4.398460988998413 and perplexity is 81.32561129296393
At time: 172.01904368400574 and batch: 350, loss is 4.359653177261353 and perplexity is 78.2299977754802
At time: 172.45925664901733 and batch: 400, loss is 4.395741987228393 and perplexity is 81.1047871585621
At time: 172.90060186386108 and batch: 450, loss is 4.370508184432984 and perplexity is 79.0838106510148
At time: 173.34034419059753 and batch: 500, loss is 4.34247841835022 and perplexity is 76.89788849637915
At time: 173.7811427116394 and batch: 550, loss is 4.358545455932617 and perplexity is 78.14338871660436
At time: 174.220929145813 and batch: 600, loss is 4.439674139022827 and perplexity is 84.74732132893509
At time: 174.66190481185913 and batch: 650, loss is 4.405058917999267 and perplexity is 81.86396596216852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.893322813744638 and perplexity of 133.39608880443743
Finished 28 epochs...
Completing Train Step...
At time: 175.5629734992981 and batch: 50, loss is 4.452854290008545 and perplexity is 85.8716972651583
At time: 176.0145719051361 and batch: 100, loss is 4.425579605102539 and perplexity is 83.56122570167787
At time: 176.45383048057556 and batch: 150, loss is 4.388401403427124 and perplexity is 80.51161046691465
At time: 176.91117930412292 and batch: 200, loss is 4.355374021530151 and perplexity is 77.89595465314471
At time: 177.35322308540344 and batch: 250, loss is 4.348368377685547 and perplexity is 77.35215041144286
At time: 177.79213786125183 and batch: 300, loss is 4.398232946395874 and perplexity is 81.30706770335617
At time: 178.23303508758545 and batch: 350, loss is 4.359504518508911 and perplexity is 78.21836906598348
At time: 178.6742901802063 and batch: 400, loss is 4.395542850494385 and perplexity is 81.08863782415112
At time: 179.11433100700378 and batch: 450, loss is 4.370407962799073 and perplexity is 79.07588513945593
At time: 179.5544991493225 and batch: 500, loss is 4.342485294342041 and perplexity is 76.89841724744937
At time: 179.99385333061218 and batch: 550, loss is 4.358702383041382 and perplexity is 78.15565249489919
At time: 180.43518090248108 and batch: 600, loss is 4.439720878601074 and perplexity is 84.751282475562
At time: 180.87597346305847 and batch: 650, loss is 4.404789209365845 and perplexity is 81.84188952101952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.893205231311274 and perplexity of 133.3804046898208
Finished 29 epochs...
Completing Train Step...
At time: 181.80057859420776 and batch: 50, loss is 4.45187294960022 and perplexity is 85.78746923364548
At time: 182.2416274547577 and batch: 100, loss is 4.424781255722046 and perplexity is 83.49454127118443
At time: 182.68292117118835 and batch: 150, loss is 4.387811317443847 and perplexity is 80.46411570846087
At time: 183.12454676628113 and batch: 200, loss is 4.3549927234649655 and perplexity is 77.86625873820691
At time: 183.56608128547668 and batch: 250, loss is 4.347945699691772 and perplexity is 77.31946226845884
At time: 184.007253408432 and batch: 300, loss is 4.3980078125 and perplexity is 81.28876478682282
At time: 184.44781684875488 and batch: 350, loss is 4.359378633499145 and perplexity is 78.2085231655684
At time: 184.8890266418457 and batch: 400, loss is 4.395344581604004 and perplexity is 81.07256206362146
At time: 185.33023881912231 and batch: 450, loss is 4.370269832611084 and perplexity is 79.06496312692342
At time: 185.77257585525513 and batch: 500, loss is 4.342464122772217 and perplexity is 76.89678920447341
At time: 186.21581506729126 and batch: 550, loss is 4.358799848556519 and perplexity is 78.16327034706337
At time: 186.65649700164795 and batch: 600, loss is 4.439702053070068 and perplexity is 84.74968700268381
At time: 187.0975296497345 and batch: 650, loss is 4.404512720108032 and perplexity is 81.81926424569482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.893115174536612 and perplexity of 133.36839342162742
Finished 30 epochs...
Completing Train Step...
At time: 188.00316548347473 and batch: 50, loss is 4.45098464012146 and perplexity is 85.71129724872344
At time: 188.45844745635986 and batch: 100, loss is 4.4240759658813475 and perplexity is 83.43567418109262
At time: 188.9035792350769 and batch: 150, loss is 4.38728931427002 and perplexity is 80.42212414549991
At time: 189.348863363266 and batch: 200, loss is 4.354664115905762 and perplexity is 77.84067550063148
At time: 189.79363226890564 and batch: 250, loss is 4.347568492889405 and perplexity is 77.29030234134297
At time: 190.23776173591614 and batch: 300, loss is 4.397805776596069 and perplexity is 81.27234319668094
At time: 190.68199491500854 and batch: 350, loss is 4.359250192642212 and perplexity is 78.198478640911
At time: 191.12559509277344 and batch: 400, loss is 4.395150804519654 and perplexity is 81.05685358094512
At time: 191.56897163391113 and batch: 450, loss is 4.370138034820557 and perplexity is 79.05454322615026
At time: 192.01257157325745 and batch: 500, loss is 4.342425212860108 and perplexity is 76.89379721537337
At time: 192.45746779441833 and batch: 550, loss is 4.358865642547608 and perplexity is 78.1684131897583
At time: 192.90129709243774 and batch: 600, loss is 4.4396642971038816 and perplexity is 84.74648725677216
At time: 193.34595227241516 and batch: 650, loss is 4.4042445659637455 and perplexity is 81.79732701231632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.893044864430147 and perplexity of 133.3590166053332
Finished 31 epochs...
Completing Train Step...
At time: 194.2744038105011 and batch: 50, loss is 4.450178632736206 and perplexity is 85.64224114374508
At time: 194.7163290977478 and batch: 100, loss is 4.4234106159210205 and perplexity is 83.3801787225842
At time: 195.15518760681152 and batch: 150, loss is 4.386774482727051 and perplexity is 80.38073095541151
At time: 195.59634923934937 and batch: 200, loss is 4.354353437423706 and perplexity is 77.81649583397028
At time: 196.03787684440613 and batch: 250, loss is 4.347223472595215 and perplexity is 77.2636402182424
At time: 196.4783353805542 and batch: 300, loss is 4.397582740783691 and perplexity is 81.2542185748872
At time: 196.9256715774536 and batch: 350, loss is 4.35910852432251 and perplexity is 78.18740117851996
At time: 197.36956596374512 and batch: 400, loss is 4.3949644660949705 and perplexity is 81.04175098168005
At time: 197.8065791130066 and batch: 450, loss is 4.369987497329712 and perplexity is 79.04264344927682
At time: 198.244295835495 and batch: 500, loss is 4.342360277175903 and perplexity is 76.88880422615343
At time: 198.68129348754883 and batch: 550, loss is 4.358899078369141 and perplexity is 78.17102685856605
At time: 199.1216492652893 and batch: 600, loss is 4.439600286483764 and perplexity is 84.7410627551848
At time: 199.59507012367249 and batch: 650, loss is 4.403977460861206 and perplexity is 81.77548144655819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892985325233609 and perplexity of 133.35107675300208
Finished 32 epochs...
Completing Train Step...
At time: 200.5454876422882 and batch: 50, loss is 4.449426307678222 and perplexity is 85.57783457008821
At time: 201.00611972808838 and batch: 100, loss is 4.422784509658814 and perplexity is 83.32799021002421
At time: 201.4534239768982 and batch: 150, loss is 4.386300468444825 and perplexity is 80.34263836985089
At time: 201.90964221954346 and batch: 200, loss is 4.354057893753052 and perplexity is 77.79350105930152
At time: 202.365558385849 and batch: 250, loss is 4.34689962387085 and perplexity is 77.23862253810846
At time: 202.81800770759583 and batch: 300, loss is 4.397349796295166 and perplexity is 81.2352930568839
At time: 203.27137207984924 and batch: 350, loss is 4.358961048126221 and perplexity is 78.17587124821249
At time: 203.72444987297058 and batch: 400, loss is 4.394776430130005 and perplexity is 81.02651365045973
At time: 204.1700575351715 and batch: 450, loss is 4.369829254150391 and perplexity is 79.03013647967289
At time: 204.61447024345398 and batch: 500, loss is 4.342285451889038 and perplexity is 76.88305121455853
At time: 205.05911588668823 and batch: 550, loss is 4.35890679359436 and perplexity is 78.17162996797047
At time: 205.50539016723633 and batch: 600, loss is 4.439515199661255 and perplexity is 84.73385271416296
At time: 205.95172023773193 and batch: 650, loss is 4.403709163665772 and perplexity is 81.75354425720553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892925486845129 and perplexity of 133.34309747820285
Finished 33 epochs...
Completing Train Step...
At time: 206.88320541381836 and batch: 50, loss is 4.448717765808105 and perplexity is 85.51722056745658
At time: 207.332622051239 and batch: 100, loss is 4.422194128036499 and perplexity is 83.27880941512701
At time: 207.77654504776 and batch: 150, loss is 4.385849094390869 and perplexity is 80.30638197067888
At time: 208.22079515457153 and batch: 200, loss is 4.353775415420532 and perplexity is 77.77152918427683
At time: 208.6663839817047 and batch: 250, loss is 4.346595582962036 and perplexity is 77.21514240675754
At time: 209.10563015937805 and batch: 300, loss is 4.397110481262207 and perplexity is 81.21585455610403
At time: 209.54661679267883 and batch: 350, loss is 4.358823776245117 and perplexity is 78.16514063583183
At time: 209.98786640167236 and batch: 400, loss is 4.394584617614746 and perplexity is 81.01097324154391
At time: 210.43083429336548 and batch: 450, loss is 4.369659557342529 and perplexity is 79.01672645563872
At time: 210.87296986579895 and batch: 500, loss is 4.342209024429321 and perplexity is 76.8771754627962
At time: 211.32840418815613 and batch: 550, loss is 4.358898057937622 and perplexity is 78.17094709042709
At time: 211.76904439926147 and batch: 600, loss is 4.439423713684082 and perplexity is 84.7261011094348
At time: 212.21282744407654 and batch: 650, loss is 4.40343864440918 and perplexity is 81.7314313403091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892897961186428 and perplexity of 133.3394271721256
Finished 34 epochs...
Completing Train Step...
At time: 213.1183521747589 and batch: 50, loss is 4.448042039871216 and perplexity is 85.45945388289003
At time: 213.57420468330383 and batch: 100, loss is 4.421633539199829 and perplexity is 83.23213732738566
At time: 214.0174422264099 and batch: 150, loss is 4.385413064956665 and perplexity is 80.27137365726743
At time: 214.4582965373993 and batch: 200, loss is 4.353496456146241 and perplexity is 77.74983712067686
At time: 214.8989133834839 and batch: 250, loss is 4.346284580230713 and perplexity is 77.19113202041096
At time: 215.3424952030182 and batch: 300, loss is 4.396884326934814 and perplexity is 81.19748931591086
At time: 215.78351712226868 and batch: 350, loss is 4.358692770004272 and perplexity is 78.15490118532263
At time: 216.22469830513 and batch: 400, loss is 4.39438681602478 and perplexity is 80.9949507269237
At time: 216.66531562805176 and batch: 450, loss is 4.369493141174316 and perplexity is 79.00357788889464
At time: 217.10981845855713 and batch: 500, loss is 4.342133369445801 and perplexity is 76.87135954135795
At time: 217.55454683303833 and batch: 550, loss is 4.358859090805054 and perplexity is 78.16790105211695
At time: 217.99893641471863 and batch: 600, loss is 4.439309186935425 and perplexity is 84.71639826017683
At time: 218.44464993476868 and batch: 650, loss is 4.4031775951385494 and perplexity is 81.71009819439237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892842012293198 and perplexity of 133.33196718744236
Finished 35 epochs...
Completing Train Step...
At time: 219.3683545589447 and batch: 50, loss is 4.44738389968872 and perplexity is 85.40322808657301
At time: 219.81027483940125 and batch: 100, loss is 4.421096239089966 and perplexity is 83.18742870290555
At time: 220.25362515449524 and batch: 150, loss is 4.385005979537964 and perplexity is 80.2387030018374
At time: 220.69406604766846 and batch: 200, loss is 4.353233327865601 and perplexity is 77.7293816310422
At time: 221.13512635231018 and batch: 250, loss is 4.345942153930664 and perplexity is 77.16470427171294
At time: 221.5770812034607 and batch: 300, loss is 4.396658887863159 and perplexity is 81.17918629248459
At time: 222.0197627544403 and batch: 350, loss is 4.358572540283203 and perplexity is 78.14550520820207
At time: 222.46080613136292 and batch: 400, loss is 4.394188795089722 and perplexity is 80.97891361893973
At time: 222.91715502738953 and batch: 450, loss is 4.369313926696777 and perplexity is 78.98942057259549
At time: 223.35249853134155 and batch: 500, loss is 4.342058973312378 and perplexity is 76.86564082216506
At time: 223.7887406349182 and batch: 550, loss is 4.358827018737793 and perplexity is 78.16539408613878
At time: 224.22527170181274 and batch: 600, loss is 4.439179162979126 and perplexity is 84.70538381499806
At time: 224.66201186180115 and batch: 650, loss is 4.40291953086853 and perplexity is 81.68901445814515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892809400371477 and perplexity of 133.32761904666648
Finished 36 epochs...
Completing Train Step...
At time: 225.5690941810608 and batch: 50, loss is 4.446752481460571 and perplexity is 85.3493199526966
At time: 226.02556252479553 and batch: 100, loss is 4.420577697753906 and perplexity is 83.14430376448045
At time: 226.46797585487366 and batch: 150, loss is 4.384579029083252 and perplexity is 80.20445236328837
At time: 226.91113662719727 and batch: 200, loss is 4.35298487663269 and perplexity is 77.71007206918414
At time: 227.3526201248169 and batch: 250, loss is 4.345646886825562 and perplexity is 77.14192343624775
At time: 227.79318928718567 and batch: 300, loss is 4.396426496505737 and perplexity is 81.16032314308896
At time: 228.23659944534302 and batch: 350, loss is 4.358424634933471 and perplexity is 78.13394792463743
At time: 228.67787981033325 and batch: 400, loss is 4.393993463516235 and perplexity is 80.96309742507461
At time: 229.1187243461609 and batch: 450, loss is 4.369114952087402 and perplexity is 78.97370524701955
At time: 229.56068301200867 and batch: 500, loss is 4.341974306106567 and perplexity is 76.85913309863307
At time: 230.00387001037598 and batch: 550, loss is 4.358790903091431 and perplexity is 78.1625711433847
At time: 230.44441533088684 and batch: 600, loss is 4.439059991836547 and perplexity is 84.69528997908516
At time: 230.88402438163757 and batch: 650, loss is 4.4026471138000485 and perplexity is 81.66676400713823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892800723805147 and perplexity of 133.3264622257548
Finished 37 epochs...
Completing Train Step...
At time: 231.81512761116028 and batch: 50, loss is 4.4461577606201175 and perplexity is 85.29857602413377
At time: 232.25869035720825 and batch: 100, loss is 4.420082511901856 and perplexity is 83.10314207376227
At time: 232.70302987098694 and batch: 150, loss is 4.384196786880493 and perplexity is 80.17380069529975
At time: 233.14983487129211 and batch: 200, loss is 4.352738761901856 and perplexity is 77.69094882906579
At time: 233.59621739387512 and batch: 250, loss is 4.345341053009033 and perplexity is 77.11833443472993
At time: 234.03993344306946 and batch: 300, loss is 4.396187314987182 and perplexity is 81.14091341506986
At time: 234.50013709068298 and batch: 350, loss is 4.358270378112793 and perplexity is 78.12189615980085
At time: 234.94824600219727 and batch: 400, loss is 4.393797397613525 and perplexity is 80.94722487837522
At time: 235.39336705207825 and batch: 450, loss is 4.368921852111816 and perplexity is 78.95845689873961
At time: 235.8360824584961 and batch: 500, loss is 4.341877412796021 and perplexity is 76.85168632355858
At time: 236.28299164772034 and batch: 550, loss is 4.358739871978759 and perplexity is 78.15858252218273
At time: 236.7272868156433 and batch: 600, loss is 4.438927373886108 and perplexity is 84.68405860807344
At time: 237.1702115535736 and batch: 650, loss is 4.40239483833313 and perplexity is 81.6461640846534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892779181985294 and perplexity of 133.32359016205882
Finished 38 epochs...
Completing Train Step...
At time: 238.07389211654663 and batch: 50, loss is 4.4455805587768555 and perplexity is 85.24935573521229
At time: 238.54238653182983 and batch: 100, loss is 4.4195918941497805 and perplexity is 83.06238019707543
At time: 238.9841480255127 and batch: 150, loss is 4.383836250305176 and perplexity is 80.1449003179015
At time: 239.4284336566925 and batch: 200, loss is 4.3524946689605715 and perplexity is 77.67198733113298
At time: 239.87014651298523 and batch: 250, loss is 4.345046091079712 and perplexity is 77.09559081643468
At time: 240.31165027618408 and batch: 300, loss is 4.395944128036499 and perplexity is 81.12118340289875
At time: 240.75252318382263 and batch: 350, loss is 4.358109912872314 and perplexity is 78.10936131667715
At time: 241.19715571403503 and batch: 400, loss is 4.393597631454468 and perplexity is 80.93105597722833
At time: 241.6376621723175 and batch: 450, loss is 4.368729724884033 and perplexity is 78.94328828650399
At time: 242.0795955657959 and batch: 500, loss is 4.341781072616577 and perplexity is 76.84428277494294
At time: 242.5212914943695 and batch: 550, loss is 4.358711318969727 and perplexity is 78.15635089133004
At time: 242.9647581577301 and batch: 600, loss is 4.438791084289551 and perplexity is 84.67251783835216
At time: 243.4062740802765 and batch: 650, loss is 4.402152080535888 and perplexity is 81.62634624727156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892762128044577 and perplexity of 133.32131648884356
Finished 39 epochs...
Completing Train Step...
At time: 244.3358085155487 and batch: 50, loss is 4.4450109100341795 and perplexity is 85.2008073759725
At time: 244.77664828300476 and batch: 100, loss is 4.419089450836181 and perplexity is 83.02065654229722
At time: 245.217538356781 and batch: 150, loss is 4.383433637619018 and perplexity is 80.11263945905394
At time: 245.65926861763 and batch: 200, loss is 4.352260417938233 and perplexity is 77.6537947195957
At time: 246.1166045665741 and batch: 250, loss is 4.34475606918335 and perplexity is 77.0732346490307
At time: 246.55745911598206 and batch: 300, loss is 4.395684118270874 and perplexity is 81.10009384487931
At time: 247.00145030021667 and batch: 350, loss is 4.3579471397399905 and perplexity is 78.09664824597306
At time: 247.44770646095276 and batch: 400, loss is 4.393401670455932 and perplexity is 80.91519820049012
At time: 247.89164447784424 and batch: 450, loss is 4.368533239364624 and perplexity is 78.92777859726577
At time: 248.33617234230042 and batch: 500, loss is 4.34167552947998 and perplexity is 76.83617281629212
At time: 248.78037428855896 and batch: 550, loss is 4.358670291900634 and perplexity is 78.1531444310983
At time: 249.22738242149353 and batch: 600, loss is 4.438641109466553 and perplexity is 84.65982004467506
At time: 249.67313146591187 and batch: 650, loss is 4.401901969909668 and perplexity is 81.60593318356405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892748066023285 and perplexity of 133.31944173483384
Finished 40 epochs...
Completing Train Step...
At time: 250.58876514434814 and batch: 50, loss is 4.444449701309204 and perplexity is 85.15300535420863
At time: 251.04993844032288 and batch: 100, loss is 4.418597574234009 and perplexity is 82.97983066531702
At time: 251.49441695213318 and batch: 150, loss is 4.383063259124756 and perplexity is 80.08297295453653
At time: 251.93836092948914 and batch: 200, loss is 4.352046756744385 and perplexity is 77.63720488947367
At time: 252.38641047477722 and batch: 250, loss is 4.344462833404541 and perplexity is 77.05063733237586
At time: 252.8296468257904 and batch: 300, loss is 4.395438480377197 and perplexity is 81.08017503515762
At time: 253.2679316997528 and batch: 350, loss is 4.357782230377198 and perplexity is 78.08377043933918
At time: 253.70901441574097 and batch: 400, loss is 4.393204536437988 and perplexity is 80.89924863450877
At time: 254.1529257297516 and batch: 450, loss is 4.3683287715911865 and perplexity is 78.91164205987074
At time: 254.59365558624268 and batch: 500, loss is 4.341551628112793 and perplexity is 76.82665329918349
At time: 255.0334029197693 and batch: 550, loss is 4.358606204986573 and perplexity is 78.14813599773672
At time: 255.47675371170044 and batch: 600, loss is 4.438504314422607 and perplexity is 84.64823979295019
At time: 255.9202561378479 and batch: 650, loss is 4.401658163070679 and perplexity is 81.58603952415548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892709171070772 and perplexity of 133.31425638232116
Finished 41 epochs...
Completing Train Step...
At time: 256.84093737602234 and batch: 50, loss is 4.443903312683106 and perplexity is 85.10649142910201
At time: 257.28498458862305 and batch: 100, loss is 4.41811954498291 and perplexity is 82.94017335843932
At time: 257.74011301994324 and batch: 150, loss is 4.3827081966400145 and perplexity is 80.05454354258129
At time: 258.1801302433014 and batch: 200, loss is 4.351852025985718 and perplexity is 77.62208800957325
At time: 258.62280893325806 and batch: 250, loss is 4.34415644645691 and perplexity is 77.02703363890699
At time: 259.06505036354065 and batch: 300, loss is 4.3952007007598874 and perplexity is 81.06089811408665
At time: 259.50602984428406 and batch: 350, loss is 4.3576137065887455 and perplexity is 78.07061257526581
At time: 259.94693207740784 and batch: 400, loss is 4.3930079364776615 and perplexity is 80.88334540877477
At time: 260.3899209499359 and batch: 450, loss is 4.36813289642334 and perplexity is 78.89618674244309
At time: 260.8297371864319 and batch: 500, loss is 4.341426706314087 and perplexity is 76.8170565748973
At time: 261.27338337898254 and batch: 550, loss is 4.358545207977295 and perplexity is 78.14336934053765
At time: 261.718195438385 and batch: 600, loss is 4.438385829925537 and perplexity is 84.6382108829774
At time: 262.16396617889404 and batch: 650, loss is 4.401423177719116 and perplexity is 81.56687025231253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892680149452359 and perplexity of 133.31038744298522
Finished 42 epochs...
Completing Train Step...
At time: 263.0735173225403 and batch: 50, loss is 4.443371124267578 and perplexity is 85.0612107902617
At time: 263.52956104278564 and batch: 100, loss is 4.41763976097107 and perplexity is 82.90038953390727
At time: 263.9721305370331 and batch: 150, loss is 4.382311544418335 and perplexity is 80.02279602680714
At time: 264.41434621810913 and batch: 200, loss is 4.351644525527954 and perplexity is 77.6059830617285
At time: 264.85478019714355 and batch: 250, loss is 4.343836698532105 and perplexity is 77.00240834190068
At time: 265.29832792282104 and batch: 300, loss is 4.3949463844299315 and perplexity is 81.04028562513271
At time: 265.738107919693 and batch: 350, loss is 4.357423391342163 and perplexity is 78.05575596114763
At time: 266.1788966655731 and batch: 400, loss is 4.392816228866577 and perplexity is 80.86784094205956
At time: 266.62098503112793 and batch: 450, loss is 4.367913541793823 and perplexity is 78.87888239659345
At time: 267.0640218257904 and batch: 500, loss is 4.34128966331482 and perplexity is 76.80653005637872
At time: 267.5052282810211 and batch: 550, loss is 4.358480291366577 and perplexity is 78.138296702501
At time: 267.9396011829376 and batch: 600, loss is 4.4382505702972415 and perplexity is 84.62676352423394
At time: 268.3776741027832 and batch: 650, loss is 4.401193876266479 and perplexity is 81.54816899467133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892651127833946 and perplexity of 133.3065186159304
Finished 43 epochs...
Completing Train Step...
At time: 269.30139660835266 and batch: 50, loss is 4.442848405838013 and perplexity is 85.01675934654702
At time: 269.74363231658936 and batch: 100, loss is 4.417180261611938 and perplexity is 82.86230560848448
At time: 270.1883111000061 and batch: 150, loss is 4.381931104660034 and perplexity is 79.99235796391993
At time: 270.6290514469147 and batch: 200, loss is 4.35142707824707 and perplexity is 77.5891096863327
At time: 271.06997537612915 and batch: 250, loss is 4.343516635894775 and perplexity is 76.97776669165219
At time: 271.51116585731506 and batch: 300, loss is 4.394688768386841 and perplexity is 81.01941103634894
At time: 271.95452523231506 and batch: 350, loss is 4.357224769592285 and perplexity is 78.04025392988187
At time: 272.3965148925781 and batch: 400, loss is 4.392627229690552 and perplexity is 80.85255843099107
At time: 272.8375737667084 and batch: 450, loss is 4.367691373825073 and perplexity is 78.86135998204584
At time: 273.2815029621124 and batch: 500, loss is 4.341158351898193 and perplexity is 76.7964451442574
At time: 273.72311639785767 and batch: 550, loss is 4.358411903381348 and perplexity is 78.13295316453923
At time: 274.1636760234833 and batch: 600, loss is 4.438086576461792 and perplexity is 84.6128863946149
At time: 274.60518884658813 and batch: 650, loss is 4.400965232849121 and perplexity is 81.52952567405056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8926044538909315 and perplexity of 133.30029682027606
Finished 44 epochs...
Completing Train Step...
At time: 275.51196098327637 and batch: 50, loss is 4.442332277297973 and perplexity is 84.97289109246941
At time: 275.9665615558624 and batch: 100, loss is 4.416744451522828 and perplexity is 82.82620124758712
At time: 276.40760231018066 and batch: 150, loss is 4.381571083068848 and perplexity is 79.96356417142752
At time: 276.85426449775696 and batch: 200, loss is 4.351205787658691 and perplexity is 77.5719418462092
At time: 277.2990093231201 and batch: 250, loss is 4.343228387832641 and perplexity is 76.95558119719243
At time: 277.7432372570038 and batch: 300, loss is 4.394457006454468 and perplexity is 81.00063599684123
At time: 278.1882264614105 and batch: 350, loss is 4.357049016952515 and perplexity is 78.02653935446705
At time: 278.6326262950897 and batch: 400, loss is 4.392440433502197 and perplexity is 80.83745689175633
At time: 279.0780746936798 and batch: 450, loss is 4.367500629425049 and perplexity is 78.84631905378299
At time: 279.522828578949 and batch: 500, loss is 4.3410396385192875 and perplexity is 76.78732891988598
At time: 279.97032928466797 and batch: 550, loss is 4.358344211578369 and perplexity is 78.12766438307304
At time: 280.4294607639313 and batch: 600, loss is 4.437916288375854 and perplexity is 84.59847905487905
At time: 280.8736524581909 and batch: 650, loss is 4.400735836029053 and perplexity is 81.51082520511538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892574534696691 and perplexity of 133.29630864246502
Finished 45 epochs...
Completing Train Step...
At time: 281.8073482513428 and batch: 50, loss is 4.441813240051269 and perplexity is 84.92879844088641
At time: 282.2567799091339 and batch: 100, loss is 4.416294221878052 and perplexity is 82.78891882987591
At time: 282.7009651660919 and batch: 150, loss is 4.381193704605103 and perplexity is 79.93339333769443
At time: 283.14725136756897 and batch: 200, loss is 4.3509922695159915 and perplexity is 77.55538059738703
At time: 283.60988664627075 and batch: 250, loss is 4.342944622039795 and perplexity is 76.93374693373511
At time: 284.0740442276001 and batch: 300, loss is 4.394224042892456 and perplexity is 80.98176799801762
At time: 284.52110862731934 and batch: 350, loss is 4.356876745223999 and perplexity is 78.01309874541403
At time: 284.96223735809326 and batch: 400, loss is 4.392242908477783 and perplexity is 80.82149104798896
At time: 285.40323138237 and batch: 450, loss is 4.367316379547119 and perplexity is 78.83179296737802
At time: 285.84524965286255 and batch: 500, loss is 4.3409278774261475 and perplexity is 76.7787475636064
At time: 286.28666377067566 and batch: 550, loss is 4.358275527954102 and perplexity is 78.12229847620476
At time: 286.7275724411011 and batch: 600, loss is 4.437774715423584 and perplexity is 84.58650304620109
At time: 287.17047142982483 and batch: 650, loss is 4.400515632629395 and perplexity is 81.49287822036229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8925832112630205 and perplexity of 133.2974652017459
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 288.0761389732361 and batch: 50, loss is 4.441387777328491 and perplexity is 84.89267208881249
At time: 288.5334770679474 and batch: 100, loss is 4.415659809112549 and perplexity is 82.73641313982914
At time: 288.9745306968689 and batch: 150, loss is 4.380503978729248 and perplexity is 79.87828021662226
At time: 289.4181272983551 and batch: 200, loss is 4.349990015029907 and perplexity is 77.47768930898799
At time: 289.85941195487976 and batch: 250, loss is 4.341586685180664 and perplexity is 76.82934666359296
At time: 290.300350189209 and batch: 300, loss is 4.392446660995484 and perplexity is 80.83796030804393
At time: 290.7407319545746 and batch: 350, loss is 4.3553133296966555 and perplexity is 77.89122714829684
At time: 291.1799530982971 and batch: 400, loss is 4.390529479980469 and perplexity is 80.68312777367149
At time: 291.62098121643066 and batch: 450, loss is 4.364519281387329 and perplexity is 78.61160079745353
At time: 292.0760796070099 and batch: 500, loss is 4.338054523468018 and perplexity is 76.5584516909696
At time: 292.517053604126 and batch: 550, loss is 4.355248737335205 and perplexity is 77.88619613248339
At time: 292.95698285102844 and batch: 600, loss is 4.434999723434448 and perplexity is 84.35210155940187
At time: 293.39875531196594 and batch: 650, loss is 4.398351354598999 and perplexity is 81.31669569714977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892178404564951 and perplexity of 133.24351641514832
Finished 47 epochs...
Completing Train Step...
At time: 294.318053483963 and batch: 50, loss is 4.440833616256714 and perplexity is 84.84564090729026
At time: 294.7623586654663 and batch: 100, loss is 4.4152758693695064 and perplexity is 82.7046534399236
At time: 295.2079510688782 and batch: 150, loss is 4.380155162811279 and perplexity is 79.85042225991458
At time: 295.6529538631439 and batch: 200, loss is 4.349677858352661 and perplexity is 77.45350790532223
At time: 296.09995794296265 and batch: 250, loss is 4.341399154663086 and perplexity is 76.81494016731904
At time: 296.54568696022034 and batch: 300, loss is 4.392322864532471 and perplexity is 80.8279534738987
At time: 296.98983812332153 and batch: 350, loss is 4.355391225814819 and perplexity is 77.89729480885123
At time: 297.43991136550903 and batch: 400, loss is 4.390580749511718 and perplexity is 80.68726446585444
At time: 297.8924744129181 and batch: 450, loss is 4.364503536224365 and perplexity is 78.6103630547324
At time: 298.34283423423767 and batch: 500, loss is 4.3380270767211915 and perplexity is 76.55635043936502
At time: 298.79316329956055 and batch: 550, loss is 4.355372018814087 and perplexity is 77.89579864982122
At time: 299.2418746948242 and batch: 600, loss is 4.435030040740966 and perplexity is 84.35465892668638
At time: 299.6886887550354 and batch: 650, loss is 4.398140354156494 and perplexity is 81.29953964840523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.892050350413603 and perplexity of 133.2264551221392
Finished 48 epochs...
Completing Train Step...
At time: 300.59575295448303 and batch: 50, loss is 4.440500087738037 and perplexity is 84.81734718500624
At time: 301.049845457077 and batch: 100, loss is 4.414990968704224 and perplexity is 82.68109418531962
At time: 301.490886926651 and batch: 150, loss is 4.379958791732788 and perplexity is 79.83474348585668
At time: 301.9320559501648 and batch: 200, loss is 4.349475898742676 and perplexity is 77.43786700454199
At time: 302.3732478618622 and batch: 250, loss is 4.341300888061523 and perplexity is 76.80739219506243
At time: 302.81412053108215 and batch: 300, loss is 4.392264213562012 and perplexity is 80.82321297500604
At time: 303.2560420036316 and batch: 350, loss is 4.3554968166351316 and perplexity is 77.90552048238037
At time: 303.7131714820862 and batch: 400, loss is 4.390662879943847 and perplexity is 80.69389161789402
At time: 304.15531945228577 and batch: 450, loss is 4.364476118087769 and perplexity is 78.60820773460782
At time: 304.59781789779663 and batch: 500, loss is 4.337989044189453 and perplexity is 76.55343886290484
At time: 305.03935718536377 and batch: 550, loss is 4.355429067611694 and perplexity is 77.9002426382337
At time: 305.4802451133728 and batch: 600, loss is 4.435006341934204 and perplexity is 84.35265984561299
At time: 305.9208323955536 and batch: 650, loss is 4.3979487991333 and perplexity is 81.2839678046823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.891984528186274 and perplexity of 133.21768614872377
Finished 49 epochs...
Completing Train Step...
At time: 306.8373408317566 and batch: 50, loss is 4.440222339630127 and perplexity is 84.79379259857996
At time: 307.2747120857239 and batch: 100, loss is 4.414740705490113 and perplexity is 82.66040473795435
At time: 307.7115886211395 and batch: 150, loss is 4.379799690246582 and perplexity is 79.82204266990334
At time: 308.14968514442444 and batch: 200, loss is 4.349312973022461 and perplexity is 77.42525141201853
At time: 308.59175753593445 and batch: 250, loss is 4.341226234436035 and perplexity is 76.80165845879552
At time: 309.0310263633728 and batch: 300, loss is 4.392212390899658 and perplexity is 80.81902460945678
At time: 309.47276544570923 and batch: 350, loss is 4.3555956554412845 and perplexity is 77.91322095156389
At time: 309.91394567489624 and batch: 400, loss is 4.390744638442993 and perplexity is 80.70048929906753
At time: 310.3556549549103 and batch: 450, loss is 4.364438905715942 and perplexity is 78.60528259117906
At time: 310.7979521751404 and batch: 500, loss is 4.337944078445434 and perplexity is 76.54999665796039
At time: 311.23964524269104 and batch: 550, loss is 4.355458526611328 and perplexity is 77.90253753525562
At time: 311.68091201782227 and batch: 600, loss is 4.434958515167236 and perplexity is 84.34862562708
At time: 312.1218774318695 and batch: 650, loss is 4.397765741348267 and perplexity is 81.26908950341323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.891937555051317 and perplexity of 133.21142864334215
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f93af975898>
SETTINGS FOR THIS RUN
{'lr': 26.64396341882142, 'data': 'ptb', 'num_layers': 1, 'anneal': 6.563235363974766, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.011307254269460665, 'seq_len': 20, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7301614284515381 and batch: 50, loss is 6.287605581283569 and perplexity is 537.8639145776147
At time: 1.1931102275848389 and batch: 100, loss is 5.382487697601318 and perplexity is 217.56283331245837
At time: 1.641162633895874 and batch: 150, loss is 5.26891095161438 and perplexity is 194.20434930474244
At time: 2.089461088180542 and batch: 200, loss is 5.231106586456299 and perplexity is 186.9996203159057
At time: 2.5390772819519043 and batch: 250, loss is 5.202513723373413 and perplexity is 181.72848333867745
At time: 2.9882020950317383 and batch: 300, loss is 5.240114698410034 and perplexity is 188.6917438075263
At time: 3.439042091369629 and batch: 350, loss is 5.183550329208374 and perplexity is 178.31476462497253
At time: 3.891482353210449 and batch: 400, loss is 5.229566917419434 and perplexity is 186.71192432573767
At time: 4.345027923583984 and batch: 450, loss is 5.19580756187439 and perplexity is 180.5138600624219
At time: 4.813395023345947 and batch: 500, loss is 5.166078224182129 and perplexity is 175.22628997540681
At time: 5.266294002532959 and batch: 550, loss is 5.19192868232727 and perplexity is 179.81502476782532
At time: 5.720865726470947 and batch: 600, loss is 5.2511128902435305 and perplexity is 190.77846585384805
At time: 6.173365831375122 and batch: 650, loss is 5.20044093132019 and perplexity is 181.3521881081788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.291742063036152 and perplexity of 198.68925329878266
Finished 1 epochs...
Completing Train Step...
At time: 7.106165885925293 and batch: 50, loss is 5.197647314071656 and perplexity is 180.8462665120991
At time: 7.551513910293579 and batch: 100, loss is 5.171511297225952 and perplexity is 176.1808980876442
At time: 7.996414661407471 and batch: 150, loss is 5.124598083496093 and perplexity is 168.1065632701284
At time: 8.440410137176514 and batch: 200, loss is 5.100190458297729 and perplexity is 164.05314960824035
At time: 8.886309623718262 and batch: 250, loss is 5.119694862365723 and perplexity is 167.28431709170715
At time: 9.329174518585205 and batch: 300, loss is 5.149880714416504 and perplexity is 172.4109229526382
At time: 9.768694162368774 and batch: 350, loss is 5.11963490486145 and perplexity is 167.27428744222976
At time: 10.20883846282959 and batch: 400, loss is 5.174990139007568 and perplexity is 176.79487089495277
At time: 10.649909496307373 and batch: 450, loss is 5.144137411117554 and perplexity is 171.4235528271946
At time: 11.090757131576538 and batch: 500, loss is 5.1156300449371335 and perplexity is 166.6057170111412
At time: 11.532610893249512 and batch: 550, loss is 5.13433578491211 and perplexity is 169.75153089274653
At time: 11.9740469455719 and batch: 600, loss is 5.2131796646118165 and perplexity is 183.67716243787746
At time: 12.415586233139038 and batch: 650, loss is 5.180338373184204 and perplexity is 177.7429442650682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.356486600988052 and perplexity of 211.9788702320587
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 13.323119401931763 and batch: 50, loss is 5.0894593334198 and perplexity is 162.30208701276732
At time: 13.780256748199463 and batch: 100, loss is 4.981801586151123 and perplexity is 145.73670251080054
At time: 14.2223801612854 and batch: 150, loss is 4.922563009262085 and perplexity is 137.35420252389352
At time: 14.663285970687866 and batch: 200, loss is 4.88600872039795 and perplexity is 132.42397675165589
At time: 15.104068517684937 and batch: 250, loss is 4.864118938446045 and perplexity is 129.5567408481498
At time: 15.546619653701782 and batch: 300, loss is 4.898136730194092 and perplexity is 134.03979456039806
At time: 15.98823857307434 and batch: 350, loss is 4.84657639503479 and perplexity is 127.30380498657722
At time: 16.44643235206604 and batch: 400, loss is 4.865845413208008 and perplexity is 129.78061048850066
At time: 16.888031482696533 and batch: 450, loss is 4.836267518997192 and perplexity is 125.99818713080177
At time: 17.32997226715088 and batch: 500, loss is 4.789194355010986 and perplexity is 120.20448749953263
At time: 17.774837970733643 and batch: 550, loss is 4.777739658355713 and perplexity is 118.83543755393539
At time: 18.21999764442444 and batch: 600, loss is 4.824646120071411 and perplexity is 124.54238753278484
At time: 18.66393494606018 and batch: 650, loss is 4.760321254730225 and perplexity is 116.783437104923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.055248784083946 and perplexity of 156.8435456686031
Finished 3 epochs...
Completing Train Step...
At time: 19.594914197921753 and batch: 50, loss is 4.8743455028533935 and perplexity is 130.88845904761462
At time: 20.039490699768066 and batch: 100, loss is 4.824694585800171 and perplexity is 124.54842371663098
At time: 20.48602056503296 and batch: 150, loss is 4.795512962341308 and perplexity is 120.9664170873354
At time: 20.931834936141968 and batch: 200, loss is 4.76796329498291 and perplexity is 117.67931965737382
At time: 21.37699794769287 and batch: 250, loss is 4.752280445098877 and perplexity is 115.84816891491484
At time: 21.82180953025818 and batch: 300, loss is 4.796163520812988 and perplexity is 121.04513841839832
At time: 22.266058921813965 and batch: 350, loss is 4.750313711166382 and perplexity is 115.62055029609301
At time: 22.7096586227417 and batch: 400, loss is 4.7802778911590575 and perplexity is 119.13745268994978
At time: 23.149476289749146 and batch: 450, loss is 4.756765594482422 and perplexity is 116.36893223595546
At time: 23.588621854782104 and batch: 500, loss is 4.713195180892944 and perplexity is 111.40755912518657
At time: 24.030831575393677 and batch: 550, loss is 4.708918991088868 and perplexity is 110.93217639454718
At time: 24.471783876419067 and batch: 600, loss is 4.769201879501343 and perplexity is 117.82516574354823
At time: 24.914337158203125 and batch: 650, loss is 4.7037030220031735 and perplexity is 110.35506400049307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.017392027611826 and perplexity of 151.01694176066917
Finished 4 epochs...
Completing Train Step...
At time: 25.824249505996704 and batch: 50, loss is 4.784942693710327 and perplexity is 119.69450363941833
At time: 26.280715942382812 and batch: 100, loss is 4.747531232833862 and perplexity is 115.29928578314733
At time: 26.722153186798096 and batch: 150, loss is 4.723437757492065 and perplexity is 112.55452349108016
At time: 27.163445472717285 and batch: 200, loss is 4.695124635696411 and perplexity is 109.41244448825334
At time: 27.60432004928589 and batch: 250, loss is 4.682915544509887 and perplexity is 108.08473950464149
At time: 28.05977439880371 and batch: 300, loss is 4.724648065567017 and perplexity is 112.69083161050338
At time: 28.500086069107056 and batch: 350, loss is 4.6882781887054445 and perplexity is 108.6659164353338
At time: 28.941826105117798 and batch: 400, loss is 4.719429779052734 and perplexity is 112.104310212916
At time: 29.382890224456787 and batch: 450, loss is 4.6984624099731445 and perplexity is 109.77824867731643
At time: 29.823647022247314 and batch: 500, loss is 4.661729459762573 and perplexity is 105.81893358174023
At time: 30.264080286026 and batch: 550, loss is 4.66659646987915 and perplexity is 106.33521074597701
At time: 30.709745407104492 and batch: 600, loss is 4.73189980506897 and perplexity is 113.51100641837006
At time: 31.15665888786316 and batch: 650, loss is 4.668648099899292 and perplexity is 106.55359520199276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.010904648724725 and perplexity of 150.04040863615262
Finished 5 epochs...
Completing Train Step...
At time: 32.08919024467468 and batch: 50, loss is 4.732458753585815 and perplexity is 113.57447096210721
At time: 32.53500485420227 and batch: 100, loss is 4.698586406707764 and perplexity is 109.79186166565029
At time: 32.980767011642456 and batch: 150, loss is 4.6784711360931395 and perplexity is 107.60543268526555
At time: 33.42589735984802 and batch: 200, loss is 4.649588203430175 and perplexity is 104.54192670515475
At time: 33.86976408958435 and batch: 250, loss is 4.636595125198364 and perplexity is 103.19239156645325
At time: 34.3152961730957 and batch: 300, loss is 4.6783775806427 and perplexity is 107.59536608144116
At time: 34.76117300987244 and batch: 350, loss is 4.645023536682129 and perplexity is 104.06581512060158
At time: 35.206013441085815 and batch: 400, loss is 4.68116042137146 and perplexity is 107.89520385528255
At time: 35.64827275276184 and batch: 450, loss is 4.660555820465088 and perplexity is 105.69481317341243
At time: 36.08574724197388 and batch: 500, loss is 4.622926998138428 and perplexity is 101.79154016297652
At time: 36.52749228477478 and batch: 550, loss is 4.628055620193481 and perplexity is 102.3149314918111
At time: 36.968833684921265 and batch: 600, loss is 4.695623226165772 and perplexity is 109.4670100921193
At time: 37.40964150428772 and batch: 650, loss is 4.634625196456909 and perplexity is 102.98931000218568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.006693222943475 and perplexity of 149.4098532914767
Finished 6 epochs...
Completing Train Step...
At time: 38.31374263763428 and batch: 50, loss is 4.690926523208618 and perplexity is 108.95408154152041
At time: 38.76581859588623 and batch: 100, loss is 4.660208044052124 and perplexity is 105.65806140148814
At time: 39.20296096801758 and batch: 150, loss is 4.64108268737793 and perplexity is 103.6565144513346
At time: 39.65467643737793 and batch: 200, loss is 4.611449451446533 and perplexity is 100.62990213750251
At time: 40.0911705493927 and batch: 250, loss is 4.599556255340576 and perplexity is 99.44017980141724
At time: 40.52793550491333 and batch: 300, loss is 4.639635467529297 and perplexity is 103.50660918528284
At time: 40.9653115272522 and batch: 350, loss is 4.605286741256714 and perplexity is 100.0116562061452
At time: 41.402769327163696 and batch: 400, loss is 4.643745670318603 and perplexity is 103.93291784642878
At time: 41.84049391746521 and batch: 450, loss is 4.619726438522338 and perplexity is 101.46627106949386
At time: 42.278255224227905 and batch: 500, loss is 4.5875553798675535 and perplexity is 98.25394276497077
At time: 42.71741771697998 and batch: 550, loss is 4.594561557769776 and perplexity is 98.94474448168562
At time: 43.15847706794739 and batch: 600, loss is 4.664740295410156 and perplexity is 106.13801711214681
At time: 43.59775924682617 and batch: 650, loss is 4.605584363937378 and perplexity is 100.04142637328167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.998744889801624 and perplexity of 148.2270010818668
Finished 7 epochs...
Completing Train Step...
At time: 44.53123068809509 and batch: 50, loss is 4.655450210571289 and perplexity is 105.15655193475258
At time: 44.98232054710388 and batch: 100, loss is 4.624109916687011 and perplexity is 101.91202251027671
At time: 45.42664551734924 and batch: 150, loss is 4.607893180847168 and perplexity is 100.27267055769022
At time: 45.874483585357666 and batch: 200, loss is 4.579754467010498 and perplexity is 97.4904541454826
At time: 46.32046294212341 and batch: 250, loss is 4.566988887786866 and perplexity is 96.25384185874871
At time: 46.76522612571716 and batch: 300, loss is 4.609675378799438 and perplexity is 100.45153564497988
At time: 47.211647272109985 and batch: 350, loss is 4.575727481842041 and perplexity is 97.0986509548678
At time: 47.66386556625366 and batch: 400, loss is 4.617056283950806 and perplexity is 101.19570183353945
At time: 48.11892604827881 and batch: 450, loss is 4.5939075756073 and perplexity is 98.88005753808304
At time: 48.595865964889526 and batch: 500, loss is 4.561155929565429 and perplexity is 95.69403148302088
At time: 49.06069016456604 and batch: 550, loss is 4.5654207038879395 and perplexity is 96.10301642565307
At time: 49.50660800933838 and batch: 600, loss is 4.639048614501953 and perplexity is 103.44588383850193
At time: 49.948050022125244 and batch: 650, loss is 4.581128702163697 and perplexity is 97.62452105330038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0001678466796875 and perplexity of 148.43807184925632
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 50.857017040252686 and batch: 50, loss is 4.613868999481201 and perplexity is 100.87367581157991
At time: 51.31251382827759 and batch: 100, loss is 4.559042692184448 and perplexity is 95.49202080199322
At time: 51.75324988365173 and batch: 150, loss is 4.5282453918457035 and perplexity is 92.59594885654359
At time: 52.194541931152344 and batch: 200, loss is 4.487275352478028 and perplexity is 88.87895185737483
At time: 52.63387727737427 and batch: 250, loss is 4.463756952285767 and perplexity is 86.81304967862788
At time: 53.07499718666077 and batch: 300, loss is 4.503390111923218 and perplexity is 90.32281731276788
At time: 53.515201807022095 and batch: 350, loss is 4.457374067306518 and perplexity is 86.26069664606229
At time: 53.95516562461853 and batch: 400, loss is 4.48925859451294 and perplexity is 89.0553952378488
At time: 54.39603662490845 and batch: 450, loss is 4.453546743392945 and perplexity is 85.93118000468911
At time: 54.8396999835968 and batch: 500, loss is 4.403413553237915 and perplexity is 81.72938062869508
At time: 55.283761501312256 and batch: 550, loss is 4.394394636154175 and perplexity is 80.99558412039532
At time: 55.728899002075195 and batch: 600, loss is 4.467869338989257 and perplexity is 87.17079359630884
At time: 56.17186737060547 and batch: 650, loss is 4.447513389587402 and perplexity is 85.41428765796098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.926354501761642 and perplexity of 137.87596846331843
Finished 9 epochs...
Completing Train Step...
At time: 57.10546898841858 and batch: 50, loss is 4.554724826812744 and perplexity is 95.08058800703517
At time: 57.55123710632324 and batch: 100, loss is 4.51446138381958 and perplexity is 91.32836183907543
At time: 57.998059034347534 and batch: 150, loss is 4.493478422164917 and perplexity is 89.43198767599331
At time: 58.442760944366455 and batch: 200, loss is 4.4593540287017825 and perplexity is 86.43165868862113
At time: 58.88699531555176 and batch: 250, loss is 4.440942068099975 and perplexity is 84.85484307242623
At time: 59.33211421966553 and batch: 300, loss is 4.4823912525177 and perplexity is 88.44591652687186
At time: 59.778231620788574 and batch: 350, loss is 4.438591213226318 and perplexity is 84.65559594334016
At time: 60.22266745567322 and batch: 400, loss is 4.4733704662323 and perplexity is 87.65165264450235
At time: 60.663416624069214 and batch: 450, loss is 4.443378009796143 and perplexity is 85.0617964836747
At time: 61.10337495803833 and batch: 500, loss is 4.401086139678955 and perplexity is 81.53938374647983
At time: 61.54596400260925 and batch: 550, loss is 4.397846269607544 and perplexity is 81.27563422523795
At time: 61.98806190490723 and batch: 600, loss is 4.474740028381348 and perplexity is 87.77177927192994
At time: 62.44545030593872 and batch: 650, loss is 4.445707845687866 and perplexity is 85.26020755300199
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.921284395105698 and perplexity of 137.17869172531437
Finished 10 epochs...
Completing Train Step...
At time: 63.35138702392578 and batch: 50, loss is 4.537691707611084 and perplexity is 93.47478376901067
At time: 63.807623624801636 and batch: 100, loss is 4.499161949157715 and perplexity is 89.94172396981725
At time: 64.24915027618408 and batch: 150, loss is 4.4799119567871095 and perplexity is 88.22690455355101
At time: 64.69263339042664 and batch: 200, loss is 4.447707681655884 and perplexity is 85.43088458886176
At time: 65.13479614257812 and batch: 250, loss is 4.430862445831298 and perplexity is 84.00383443450525
At time: 65.57565522193909 and batch: 300, loss is 4.472621488571167 and perplexity is 87.58602809343937
At time: 66.01579427719116 and batch: 350, loss is 4.429316110610962 and perplexity is 83.87403672792233
At time: 66.45750427246094 and batch: 400, loss is 4.466177263259888 and perplexity is 87.02341873203278
At time: 66.89801168441772 and batch: 450, loss is 4.438772077560425 and perplexity is 84.67090850603535
At time: 67.33826732635498 and batch: 500, loss is 4.400709733963013 and perplexity is 81.50869763194004
At time: 67.77825975418091 and batch: 550, loss is 4.399297046661377 and perplexity is 81.39363262439612
At time: 68.22109603881836 and batch: 600, loss is 4.475961532592773 and perplexity is 87.87905837755557
At time: 68.66705393791199 and batch: 650, loss is 4.441885471343994 and perplexity is 84.93493317934474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.919334262025123 and perplexity of 136.91143569773922
Finished 11 epochs...
Completing Train Step...
At time: 69.61125874519348 and batch: 50, loss is 4.526115036010742 and perplexity is 92.39889650694008
At time: 70.05476331710815 and batch: 100, loss is 4.488422174453735 and perplexity is 88.98093866171916
At time: 70.50156283378601 and batch: 150, loss is 4.47004469871521 and perplexity is 87.36062783399423
At time: 70.9471275806427 and batch: 200, loss is 4.4393407917022705 and perplexity is 84.71907574450223
At time: 71.39283061027527 and batch: 250, loss is 4.423590908050537 and perplexity is 83.39521286779386
At time: 71.83727979660034 and batch: 300, loss is 4.46562273979187 and perplexity is 86.9751755812946
At time: 72.28019571304321 and batch: 350, loss is 4.422721796035766 and perplexity is 83.32276457371788
At time: 72.72350788116455 and batch: 400, loss is 4.461302947998047 and perplexity is 86.60027126883321
At time: 73.16803526878357 and batch: 450, loss is 4.435315961837769 and perplexity is 84.37878115164925
At time: 73.61349487304688 and batch: 500, loss is 4.399637804031372 and perplexity is 81.42137283065505
At time: 74.07289052009583 and batch: 550, loss is 4.399366617202759 and perplexity is 81.39929542046247
At time: 74.5186059474945 and batch: 600, loss is 4.47498387336731 and perplexity is 87.79318458989806
At time: 74.96463894844055 and batch: 650, loss is 4.437781066894531 and perplexity is 84.5870402966239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.918103087182138 and perplexity of 136.7429775044151
Finished 12 epochs...
Completing Train Step...
At time: 75.87120413780212 and batch: 50, loss is 4.516836004257202 and perplexity is 91.54548972976762
At time: 76.32582974433899 and batch: 100, loss is 4.4797311210632325 and perplexity is 88.21095141989234
At time: 76.76687693595886 and batch: 150, loss is 4.4622091293334964 and perplexity is 86.67878238558365
At time: 77.20754218101501 and batch: 200, loss is 4.43269910812378 and perplexity is 84.15826288267728
At time: 77.64866495132446 and batch: 250, loss is 4.417703294754029 and perplexity is 82.90565667658213
At time: 78.08876657485962 and batch: 300, loss is 4.459762897491455 and perplexity is 86.46700512183278
At time: 78.53051090240479 and batch: 350, loss is 4.416821012496948 and perplexity is 82.83254274498985
At time: 78.97200083732605 and batch: 400, loss is 4.456875190734864 and perplexity is 86.21767393785382
At time: 79.41285967826843 and batch: 450, loss is 4.431958475112915 and perplexity is 84.09595557132332
At time: 79.85383176803589 and batch: 500, loss is 4.398163337707519 and perplexity is 81.3014082219962
At time: 80.29440999031067 and batch: 550, loss is 4.398310995101928 and perplexity is 81.31341386243504
At time: 80.73679065704346 and batch: 600, loss is 4.472883806228638 and perplexity is 87.60900646884109
At time: 81.17856407165527 and batch: 650, loss is 4.4336416625976565 and perplexity is 84.23762402509554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.917498121074602 and perplexity of 136.66027765540645
Finished 13 epochs...
Completing Train Step...
At time: 82.09520959854126 and batch: 50, loss is 4.509111728668213 and perplexity is 90.84109112493185
At time: 82.53299403190613 and batch: 100, loss is 4.4723684597015385 and perplexity is 87.56386910330599
At time: 82.97117519378662 and batch: 150, loss is 4.455394315719604 and perplexity is 86.09009082928011
At time: 83.40907168388367 and batch: 200, loss is 4.427130289077759 and perplexity is 83.69090327380432
At time: 83.84676098823547 and batch: 250, loss is 4.412609672546386 and perplexity is 82.48444025196662
At time: 84.28455686569214 and batch: 300, loss is 4.454566354751587 and perplexity is 86.01884109441039
At time: 84.72184705734253 and batch: 350, loss is 4.411545009613037 and perplexity is 82.39666885761814
At time: 85.15899801254272 and batch: 400, loss is 4.452727584838867 and perplexity is 85.86081756645746
At time: 85.61094212532043 and batch: 450, loss is 4.428543186187744 and perplexity is 83.80923348369267
At time: 86.05297183990479 and batch: 500, loss is 4.396334009170532 and perplexity is 81.15281718818534
At time: 86.49609804153442 and batch: 550, loss is 4.396430130004883 and perplexity is 81.16061803958954
At time: 86.93758201599121 and batch: 600, loss is 4.470438146591187 and perplexity is 87.39500645011279
At time: 87.38174176216125 and batch: 650, loss is 4.429769878387451 and perplexity is 83.9121046994297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.917001163258272 and perplexity of 136.5923801347429
Finished 14 epochs...
Completing Train Step...
At time: 88.29805850982666 and batch: 50, loss is 4.50188193321228 and perplexity is 90.18669703523882
At time: 88.75980854034424 and batch: 100, loss is 4.465528526306152 and perplexity is 86.96698173282373
At time: 89.2072331905365 and batch: 150, loss is 4.4490140914917 and perplexity is 85.54256527125821
At time: 89.65041327476501 and batch: 200, loss is 4.421832132339477 and perplexity is 83.24866830027175
At time: 90.09723472595215 and batch: 250, loss is 4.407869987487793 and perplexity is 82.09441501148724
At time: 90.54334354400635 and batch: 300, loss is 4.449735078811646 and perplexity is 85.60426261496305
At time: 90.98943400382996 and batch: 350, loss is 4.406736478805542 and perplexity is 82.00141299855976
At time: 91.43661308288574 and batch: 400, loss is 4.448870334625244 and perplexity is 85.5302688239968
At time: 91.88230967521667 and batch: 450, loss is 4.425285568237305 and perplexity is 83.53665923271822
At time: 92.32800054550171 and batch: 500, loss is 4.393670873641968 and perplexity is 80.93698376188408
At time: 92.77327108383179 and batch: 550, loss is 4.393853693008423 and perplexity is 80.95178196263585
At time: 93.21735382080078 and batch: 600, loss is 4.467336597442627 and perplexity is 87.12436646083741
At time: 93.65594506263733 and batch: 650, loss is 4.425811061859131 and perplexity is 83.58056875040894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.916802200616575 and perplexity of 136.56520605735835
Finished 15 epochs...
Completing Train Step...
At time: 94.57882905006409 and batch: 50, loss is 4.49527798652649 and perplexity is 89.59307119040403
At time: 95.02063393592834 and batch: 100, loss is 4.45931718826294 and perplexity is 86.42847456703772
At time: 95.46203708648682 and batch: 150, loss is 4.443289155960083 and perplexity is 85.05423875252669
At time: 95.90193486213684 and batch: 200, loss is 4.416928691864014 and perplexity is 82.84146258099759
At time: 96.34297037124634 and batch: 250, loss is 4.403304767608643 and perplexity is 81.72049013018139
At time: 96.78458738327026 and batch: 300, loss is 4.445252704620361 and perplexity is 85.22141096075157
At time: 97.24087166786194 and batch: 350, loss is 4.402467384338379 and perplexity is 81.65208740255552
At time: 97.68174409866333 and batch: 400, loss is 4.445529298782349 and perplexity is 85.24498596570372
At time: 98.12294626235962 and batch: 450, loss is 4.423118181228638 and perplexity is 83.35579903057628
At time: 98.56452441215515 and batch: 500, loss is 4.392139301300049 and perplexity is 80.81311779517326
At time: 99.00606799125671 and batch: 550, loss is 4.391958560943603 and perplexity is 80.79851292334209
At time: 99.44696021080017 and batch: 600, loss is 4.464751834869385 and perplexity is 86.89946144743836
At time: 99.8889753818512 and batch: 650, loss is 4.422099838256836 and perplexity is 83.27095744472275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.916689704446232 and perplexity of 136.5498438587853
Finished 16 epochs...
Completing Train Step...
At time: 100.7989399433136 and batch: 50, loss is 4.489396066665649 and perplexity is 89.06763871629192
At time: 101.256422996521 and batch: 100, loss is 4.453697156906128 and perplexity is 85.94410618747752
At time: 101.69856262207031 and batch: 150, loss is 4.438639650344848 and perplexity is 84.65969651578428
At time: 102.1411280632019 and batch: 200, loss is 4.412416210174561 and perplexity is 82.46848416001822
At time: 102.58293747901917 and batch: 250, loss is 4.399024715423584 and perplexity is 81.37146961365214
At time: 103.02482914924622 and batch: 300, loss is 4.440970554351806 and perplexity is 84.85726030328384
At time: 103.46741700172424 and batch: 350, loss is 4.397848596572876 and perplexity is 81.2758233510412
At time: 103.9086697101593 and batch: 400, loss is 4.4417369937896725 and perplexity is 84.92232318436356
At time: 104.34921503067017 and batch: 450, loss is 4.419772176742554 and perplexity is 83.07735624825939
At time: 104.79479455947876 and batch: 500, loss is 4.390171585083007 and perplexity is 80.65425686061448
At time: 105.23984050750732 and batch: 550, loss is 4.389592971801758 and perplexity is 80.60760273505556
At time: 105.6844482421875 and batch: 600, loss is 4.461405582427979 and perplexity is 86.60915989443853
At time: 106.1286346912384 and batch: 650, loss is 4.4184403991699215 and perplexity is 82.96678933002744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.916658588484222 and perplexity of 136.54559504513466
Finished 17 epochs...
Completing Train Step...
At time: 107.06081318855286 and batch: 50, loss is 4.484173355102539 and perplexity is 88.60367675403471
At time: 107.50563383102417 and batch: 100, loss is 4.448655891418457 and perplexity is 85.51192940532495
At time: 107.94956231117249 and batch: 150, loss is 4.433720121383667 and perplexity is 84.2442334660939
At time: 108.39382600784302 and batch: 200, loss is 4.408395729064941 and perplexity is 82.13758680631982
At time: 108.85291004180908 and batch: 250, loss is 4.394954233169556 and perplexity is 81.04092169172982
At time: 109.29801774024963 and batch: 300, loss is 4.436658344268799 and perplexity is 84.4921258039334
At time: 109.74354648590088 and batch: 350, loss is 4.393898401260376 and perplexity is 80.95540125620543
At time: 110.18779063224792 and batch: 400, loss is 4.438074703216553 and perplexity is 84.61188177102841
At time: 110.62965559959412 and batch: 450, loss is 4.416641702651978 and perplexity is 82.81769138612897
At time: 111.06781649589539 and batch: 500, loss is 4.388155975341797 and perplexity is 80.49185308111903
At time: 111.50930571556091 and batch: 550, loss is 4.387172889709473 and perplexity is 80.41276158006488
At time: 111.95007348060608 and batch: 600, loss is 4.458526744842529 and perplexity is 86.36018474115745
At time: 112.39123558998108 and batch: 650, loss is 4.415203275680542 and perplexity is 82.69864982195094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9168743058746935 and perplexity of 136.57505348181246
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 113.29699659347534 and batch: 50, loss is 4.478565349578857 and perplexity is 88.10817752519465
At time: 113.75612735748291 and batch: 100, loss is 4.440491981506348 and perplexity is 84.81665963872538
At time: 114.19704556465149 and batch: 150, loss is 4.423539381027222 and perplexity is 83.39091587142275
At time: 114.63845920562744 and batch: 200, loss is 4.393969650268555 and perplexity is 80.96116945373838
At time: 115.07908082008362 and batch: 250, loss is 4.3764248466491695 and perplexity is 79.55310981867439
At time: 115.52106094360352 and batch: 300, loss is 4.416441125869751 and perplexity is 82.80108174588906
At time: 115.96165657043457 and batch: 350, loss is 4.369951744079589 and perplexity is 79.03981746839455
At time: 116.40285921096802 and batch: 400, loss is 4.413465318679809 and perplexity is 82.55504794762881
At time: 116.8440158367157 and batch: 450, loss is 4.388762998580932 and perplexity is 80.5407283392083
At time: 117.28541898727417 and batch: 500, loss is 4.355369987487793 and perplexity is 77.89564041819796
At time: 117.72835659980774 and batch: 550, loss is 4.3509493160247805 and perplexity is 77.5520493945721
At time: 118.17114543914795 and batch: 600, loss is 4.4204967594146725 and perplexity is 83.13757447494946
At time: 118.61181569099426 and batch: 650, loss is 4.391131820678711 and perplexity is 80.73174114462718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.90912852567785 and perplexity of 135.52125963364077
Finished 19 epochs...
Completing Train Step...
At time: 119.54396724700928 and batch: 50, loss is 4.472464056015014 and perplexity is 87.5722402865067
At time: 119.98810625076294 and batch: 100, loss is 4.434447202682495 and perplexity is 84.30550814591875
At time: 120.44833731651306 and batch: 150, loss is 4.418026294708252 and perplexity is 82.93243952509007
At time: 120.89162468910217 and batch: 200, loss is 4.388766279220581 and perplexity is 80.54099256474845
At time: 121.3356704711914 and batch: 250, loss is 4.372345199584961 and perplexity is 79.22922233029341
At time: 121.77922701835632 and batch: 300, loss is 4.412789793014526 and perplexity is 82.49929872607653
At time: 122.22324848175049 and batch: 350, loss is 4.367007608413696 and perplexity is 78.80745574282334
At time: 122.66797232627869 and batch: 400, loss is 4.411124124526977 and perplexity is 82.36199662557948
At time: 123.11229944229126 and batch: 450, loss is 4.387419109344482 and perplexity is 80.43256321854729
At time: 123.55650520324707 and batch: 500, loss is 4.354955682754516 and perplexity is 77.86337457007939
At time: 124.00135040283203 and batch: 550, loss is 4.351989898681641 and perplexity is 77.6327907138987
At time: 124.44041204452515 and batch: 600, loss is 4.422785491943359 and perplexity is 83.32807206186139
At time: 124.88087487220764 and batch: 650, loss is 4.3920205879211425 and perplexity is 80.80352476632152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.907803703756893 and perplexity of 135.34183697590038
Finished 20 epochs...
Completing Train Step...
At time: 125.78915643692017 and batch: 50, loss is 4.470056962966919 and perplexity is 87.36169925329354
At time: 126.24702191352844 and batch: 100, loss is 4.431663579940796 and perplexity is 84.07115973629631
At time: 126.68934512138367 and batch: 150, loss is 4.415230989456177 and perplexity is 82.70094174553614
At time: 127.13212609291077 and batch: 200, loss is 4.385928802490234 and perplexity is 80.31278329486797
At time: 127.57543778419495 and batch: 250, loss is 4.370099697113037 and perplexity is 79.05151251428958
At time: 128.02328658103943 and batch: 300, loss is 4.410637903213501 and perplexity is 82.32196020146976
At time: 128.46681141853333 and batch: 350, loss is 4.365346946716309 and perplexity is 78.67669182696757
At time: 128.90928268432617 and batch: 400, loss is 4.409796867370606 and perplexity is 82.25275358898317
At time: 129.35038018226624 and batch: 450, loss is 4.386757936477661 and perplexity is 80.37940096679415
At time: 129.7909710407257 and batch: 500, loss is 4.354958782196045 and perplexity is 77.86361590343006
At time: 130.23321986198425 and batch: 550, loss is 4.352747974395752 and perplexity is 77.69166455975446
At time: 130.67166781425476 and batch: 600, loss is 4.424169082641601 and perplexity is 83.44344380249731
At time: 131.11400771141052 and batch: 650, loss is 4.39250244140625 and perplexity is 80.84246960843953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.907124538047641 and perplexity of 135.24994864843174
Finished 21 epochs...
Completing Train Step...
At time: 132.0202088356018 and batch: 50, loss is 4.468425750732422 and perplexity is 87.21930994580839
At time: 132.4585165977478 and batch: 100, loss is 4.429679565429687 and perplexity is 83.90452669126341
At time: 132.89785408973694 and batch: 150, loss is 4.41322340965271 and perplexity is 82.53507955166269
At time: 133.35302066802979 and batch: 200, loss is 4.383849563598633 and perplexity is 80.14596731758114
At time: 133.81853127479553 and batch: 250, loss is 4.3684945392608645 and perplexity is 78.92472414314875
At time: 134.28121256828308 and batch: 300, loss is 4.408989601135254 and perplexity is 82.18638051222246
At time: 134.72839403152466 and batch: 350, loss is 4.364160518646241 and perplexity is 78.58340294254239
At time: 135.17221999168396 and batch: 400, loss is 4.408803386688232 and perplexity is 82.17107764567432
At time: 135.6208713054657 and batch: 450, loss is 4.386184663772583 and perplexity is 80.33333485565302
At time: 136.07788252830505 and batch: 500, loss is 4.354950332641602 and perplexity is 77.8629579933479
At time: 136.52885174751282 and batch: 550, loss is 4.3532442951202395 and perplexity is 77.73023411363815
At time: 136.97276139259338 and batch: 600, loss is 4.425050077438354 and perplexity is 83.51698943421302
At time: 137.41448760032654 and batch: 650, loss is 4.392676448822021 and perplexity is 80.85653802162929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.906654507506127 and perplexity of 135.1863919797867
Finished 22 epochs...
Completing Train Step...
At time: 138.33468914031982 and batch: 50, loss is 4.467044897079468 and perplexity is 87.09895595780742
At time: 138.79232478141785 and batch: 100, loss is 4.427998075485229 and perplexity is 83.7635606230647
At time: 139.23502802848816 and batch: 150, loss is 4.4114847755432125 and perplexity is 82.39170592038333
At time: 139.67977285385132 and batch: 200, loss is 4.381976957321167 and perplexity is 79.99602591049478
At time: 140.1213562488556 and batch: 250, loss is 4.367078065872192 and perplexity is 78.81300851148019
At time: 140.56153225898743 and batch: 300, loss is 4.407545957565308 and perplexity is 82.06781827385674
At time: 141.0043556690216 and batch: 350, loss is 4.363058815002441 and perplexity is 78.49687499400848
At time: 141.44632267951965 and batch: 400, loss is 4.407942333221436 and perplexity is 82.1003544070116
At time: 141.88686299324036 and batch: 450, loss is 4.385731506347656 and perplexity is 80.29693945553979
At time: 142.32800006866455 and batch: 500, loss is 4.3548667430877686 and perplexity is 77.85644973544407
At time: 142.7707998752594 and batch: 550, loss is 4.353476619720459 and perplexity is 77.74829485709981
At time: 143.2390034198761 and batch: 600, loss is 4.425661764144897 and perplexity is 83.56809129399123
At time: 143.68086767196655 and batch: 650, loss is 4.392768716812133 and perplexity is 80.86399883607187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.906325097177543 and perplexity of 135.14186751979562
Finished 23 epochs...
Completing Train Step...
At time: 144.62180948257446 and batch: 50, loss is 4.465873174667358 and perplexity is 86.99695992622783
At time: 145.06426858901978 and batch: 100, loss is 4.426546497344971 and perplexity is 83.64205947505364
At time: 145.50982904434204 and batch: 150, loss is 4.410018863677979 and perplexity is 82.27101542350613
At time: 145.95709204673767 and batch: 200, loss is 4.3804345798492434 and perplexity is 79.87273694578916
At time: 146.40411067008972 and batch: 250, loss is 4.365870971679687 and perplexity is 78.71793118180294
At time: 146.84875774383545 and batch: 300, loss is 4.406264410018921 and perplexity is 81.96271182655067
At time: 147.2971704006195 and batch: 350, loss is 4.362110433578491 and perplexity is 78.42246530588618
At time: 147.74874114990234 and batch: 400, loss is 4.4071785259246825 and perplexity is 82.03766949989178
At time: 148.19364833831787 and batch: 450, loss is 4.385250663757324 and perplexity is 80.25833854840006
At time: 148.639062166214 and batch: 500, loss is 4.354762439727783 and perplexity is 77.84832946963299
At time: 149.0867030620575 and batch: 550, loss is 4.35362681388855 and perplexity is 77.75997307454449
At time: 149.53218722343445 and batch: 600, loss is 4.42606071472168 and perplexity is 83.60143748351206
At time: 149.98026537895203 and batch: 650, loss is 4.392771348953247 and perplexity is 80.86421168180797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9060312906901045 and perplexity of 135.1021677946977
Finished 24 epochs...
Completing Train Step...
At time: 150.89532804489136 and batch: 50, loss is 4.464822425842285 and perplexity is 86.90559598148523
At time: 151.35027241706848 and batch: 100, loss is 4.425225811004639 and perplexity is 83.53166746228497
At time: 151.79131364822388 and batch: 150, loss is 4.408669910430908 and perplexity is 82.16011048971363
At time: 152.23467922210693 and batch: 200, loss is 4.379050846099854 and perplexity is 79.76229077569045
At time: 152.67634797096252 and batch: 250, loss is 4.36484073638916 and perplexity is 78.63687495176586
At time: 153.11771821975708 and batch: 300, loss is 4.405088090896607 and perplexity is 81.86635420607918
At time: 153.55936431884766 and batch: 350, loss is 4.361152391433716 and perplexity is 78.34736925735959
At time: 154.00548386573792 and batch: 400, loss is 4.4064341735839845 and perplexity is 81.97662728984851
At time: 154.44602513313293 and batch: 450, loss is 4.384816007614136 and perplexity is 80.22346134885156
At time: 154.91543459892273 and batch: 500, loss is 4.354585208892822 and perplexity is 77.8345335677665
At time: 155.3586573600769 and batch: 550, loss is 4.353667993545532 and perplexity is 77.76317526949482
At time: 155.8012409210205 and batch: 600, loss is 4.426351289749146 and perplexity is 83.62573350324239
At time: 156.24139857292175 and batch: 650, loss is 4.392663221359253 and perplexity is 80.8554685018566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905782662185968 and perplexity of 135.068581720214
Finished 25 epochs...
Completing Train Step...
At time: 157.17072224617004 and batch: 50, loss is 4.463808736801147 and perplexity is 86.81754536673672
At time: 157.61133933067322 and batch: 100, loss is 4.424043102264404 and perplexity is 83.43293222811242
At time: 158.0545961856842 and batch: 150, loss is 4.407493934631348 and perplexity is 82.06354897621812
At time: 158.50087523460388 and batch: 200, loss is 4.377782535552979 and perplexity is 79.66119154721805
At time: 158.95058155059814 and batch: 250, loss is 4.363854351043702 and perplexity is 78.55934693323738
At time: 159.39367961883545 and batch: 300, loss is 4.404045028686523 and perplexity is 81.78100702467734
At time: 159.84136867523193 and batch: 350, loss is 4.360294342041016 and perplexity is 78.28017217807796
At time: 160.2901303768158 and batch: 400, loss is 4.405676670074463 and perplexity is 81.91455322060861
At time: 160.73566246032715 and batch: 450, loss is 4.3843115711212155 and perplexity is 80.1830039123204
At time: 161.17935585975647 and batch: 500, loss is 4.354285326004028 and perplexity is 77.8111958224624
At time: 161.62399768829346 and batch: 550, loss is 4.353639240264893 and perplexity is 77.76093935523801
At time: 162.07077717781067 and batch: 600, loss is 4.426489782333374 and perplexity is 83.63731584919924
At time: 162.51389074325562 and batch: 650, loss is 4.392537937164307 and perplexity is 80.8453392241108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905644136316636 and perplexity of 135.04987252339558
Finished 26 epochs...
Completing Train Step...
At time: 163.43882584571838 and batch: 50, loss is 4.462843618392944 and perplexity is 86.73379657579885
At time: 163.8947012424469 and batch: 100, loss is 4.423002662658692 and perplexity is 83.34617044402653
At time: 164.33244943618774 and batch: 150, loss is 4.406315832138062 and perplexity is 81.96692663124946
At time: 164.77340865135193 and batch: 200, loss is 4.376570310592651 and perplexity is 79.56468276944797
At time: 165.21699047088623 and batch: 250, loss is 4.362895574569702 and perplexity is 78.48406217597922
At time: 165.65830731391907 and batch: 300, loss is 4.4030826377868655 and perplexity is 81.70233958823619
At time: 166.09926581382751 and batch: 350, loss is 4.359502859115601 and perplexity is 78.21823927105284
At time: 166.55781388282776 and batch: 400, loss is 4.404958696365356 and perplexity is 81.85576183286385
At time: 166.99979376792908 and batch: 450, loss is 4.383791675567627 and perplexity is 80.141327959623
At time: 167.44029092788696 and batch: 500, loss is 4.353963012695313 and perplexity is 77.78612027978939
At time: 167.88097643852234 and batch: 550, loss is 4.353549938201905 and perplexity is 77.75399545299071
At time: 168.32724285125732 and batch: 600, loss is 4.426615943908692 and perplexity is 83.64786833036692
At time: 168.768239736557 and batch: 650, loss is 4.392467947006225 and perplexity is 80.8396810440491
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905565448835785 and perplexity of 135.03924620722196
Finished 27 epochs...
Completing Train Step...
At time: 169.7086479663849 and batch: 50, loss is 4.461993293762207 and perplexity is 86.66007603988882
At time: 170.14758610725403 and batch: 100, loss is 4.422064657211304 and perplexity is 83.26802793690928
At time: 170.5845286846161 and batch: 150, loss is 4.40531328201294 and perplexity is 81.8847918576927
At time: 171.02067518234253 and batch: 200, loss is 4.375469646453857 and perplexity is 79.47715695350871
At time: 171.46140146255493 and batch: 250, loss is 4.362104148864746 and perplexity is 78.4219724446893
At time: 171.89939999580383 and batch: 300, loss is 4.402151498794556 and perplexity is 81.62629876186594
At time: 172.34047198295593 and batch: 350, loss is 4.35879753112793 and perplexity is 78.16308920947598
At time: 172.78082609176636 and batch: 400, loss is 4.404267702102661 and perplexity is 81.79921950852946
At time: 173.224351644516 and batch: 450, loss is 4.383342027664185 and perplexity is 80.10530067992924
At time: 173.66505479812622 and batch: 500, loss is 4.3536997890472415 and perplexity is 77.76564782797497
At time: 174.10791182518005 and batch: 550, loss is 4.353457746505737 and perplexity is 77.74682751068353
At time: 174.54969477653503 and batch: 600, loss is 4.426691703796386 and perplexity is 83.6542057235346
At time: 174.99555087089539 and batch: 650, loss is 4.392391881942749 and perplexity is 80.83353220243806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905457141352635 and perplexity of 135.02462123834985
Finished 28 epochs...
Completing Train Step...
At time: 175.92949748039246 and batch: 50, loss is 4.461195507049561 and perplexity is 86.5909673533686
At time: 176.38456988334656 and batch: 100, loss is 4.421190958023072 and perplexity is 83.19530850057723
At time: 176.82426071166992 and batch: 150, loss is 4.404344348907471 and perplexity is 81.80548939762113
At time: 177.26362109184265 and batch: 200, loss is 4.374471263885498 and perplexity is 79.39784794237842
At time: 177.70157480239868 and batch: 250, loss is 4.36126298904419 and perplexity is 78.3560347683699
At time: 178.1549391746521 and batch: 300, loss is 4.401283082962036 and perplexity is 81.5554439618391
At time: 178.593266248703 and batch: 350, loss is 4.358116407394409 and perplexity is 78.10986860129728
At time: 179.0446915626526 and batch: 400, loss is 4.403599405288697 and perplexity is 81.74457161328816
At time: 179.48696208000183 and batch: 450, loss is 4.38290041923523 and perplexity is 80.06993331378331
At time: 179.9258017539978 and batch: 500, loss is 4.353481140136719 and perplexity is 77.74864631255042
At time: 180.36401510238647 and batch: 550, loss is 4.353313980102539 and perplexity is 77.73565093235973
At time: 180.80729866027832 and batch: 600, loss is 4.426763982772827 and perplexity is 83.66025238241977
At time: 181.24884963035583 and batch: 650, loss is 4.39226357460022 and perplexity is 80.82316133207755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9053793514476105 and perplexity of 135.0141180944124
Finished 29 epochs...
Completing Train Step...
At time: 182.16929721832275 and batch: 50, loss is 4.460433425903321 and perplexity is 86.52500314794506
At time: 182.61366271972656 and batch: 100, loss is 4.42035439491272 and perplexity is 83.1257394780275
At time: 183.05512762069702 and batch: 150, loss is 4.403435621261597 and perplexity is 81.73118425450348
At time: 183.49626302719116 and batch: 200, loss is 4.3734588718414305 and perplexity is 79.31750686799977
At time: 183.93912720680237 and batch: 250, loss is 4.360467548370361 and perplexity is 78.29373197364879
At time: 184.3823115825653 and batch: 300, loss is 4.40037013053894 and perplexity is 81.48102169881933
At time: 184.8233287334442 and batch: 350, loss is 4.357462730407715 and perplexity is 78.05882666204693
At time: 185.26664400100708 and batch: 400, loss is 4.403025217056275 and perplexity is 81.6976483148955
At time: 185.71476364135742 and batch: 450, loss is 4.382508916854858 and perplexity is 80.0385918798181
At time: 186.15883255004883 and batch: 500, loss is 4.3532453060150145 and perplexity is 77.73031269076539
At time: 186.60264110565186 and batch: 550, loss is 4.3531498527526855 and perplexity is 77.72289343293902
At time: 187.04650235176086 and batch: 600, loss is 4.426773290634156 and perplexity is 83.6610310840717
At time: 187.49365782737732 and batch: 650, loss is 4.392108783721924 and perplexity is 80.81065161216857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905301860734528 and perplexity of 135.00365615948164
Finished 30 epochs...
Completing Train Step...
At time: 188.40649843215942 and batch: 50, loss is 4.459691209793091 and perplexity is 86.46080672342828
At time: 188.86861085891724 and batch: 100, loss is 4.419516592025757 and perplexity is 83.0561256589131
At time: 189.31358003616333 and batch: 150, loss is 4.402504777908325 and perplexity is 81.65514072268398
At time: 189.77350974082947 and batch: 200, loss is 4.372424507141114 and perplexity is 79.23550605546244
At time: 190.2183768749237 and batch: 250, loss is 4.35965274810791 and perplexity is 78.22996420281454
At time: 190.6583731174469 and batch: 300, loss is 4.399604825973511 and perplexity is 81.4186877561852
At time: 191.09979438781738 and batch: 350, loss is 4.3568186187744145 and perplexity is 78.00856425275116
At time: 191.54134106636047 and batch: 400, loss is 4.4023696804046635 and perplexity is 81.64411006213537
At time: 191.98502326011658 and batch: 450, loss is 4.381955957412719 and perplexity is 79.99434601891332
At time: 192.42888164520264 and batch: 500, loss is 4.352875909805298 and perplexity is 77.70160471051331
At time: 192.87120246887207 and batch: 550, loss is 4.352831439971924 and perplexity is 77.69814940992784
At time: 193.31332302093506 and batch: 600, loss is 4.426694812774659 and perplexity is 83.65446580304686
At time: 193.7578046321869 and batch: 650, loss is 4.391940746307373 and perplexity is 80.7970735400475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905125337488511 and perplexity of 134.97982697913625
Finished 31 epochs...
Completing Train Step...
At time: 194.6814911365509 and batch: 50, loss is 4.458879652023316 and perplexity is 86.39066724891826
At time: 195.1256947517395 and batch: 100, loss is 4.4185019207000735 and perplexity is 82.9718937308725
At time: 195.56849312782288 and batch: 150, loss is 4.401614265441895 and perplexity is 81.58245816908554
At time: 196.00934314727783 and batch: 200, loss is 4.371526517868042 and perplexity is 79.16438535856891
At time: 196.45096850395203 and batch: 250, loss is 4.358851127624511 and perplexity is 78.16727858948663
At time: 196.89404845237732 and batch: 300, loss is 4.398836450576782 and perplexity is 81.35615166835183
At time: 197.33801817893982 and batch: 350, loss is 4.356221227645874 and perplexity is 77.96197654544342
At time: 197.7822825908661 and batch: 400, loss is 4.401776609420776 and perplexity is 81.59570366508581
At time: 198.22647190093994 and batch: 450, loss is 4.38156286239624 and perplexity is 79.96290681984787
At time: 198.6733853816986 and batch: 500, loss is 4.352603883743286 and perplexity is 77.68047072359984
At time: 199.11829280853271 and batch: 550, loss is 4.352663803100586 and perplexity is 77.68512542693237
At time: 199.56341934204102 and batch: 600, loss is 4.426654319763184 and perplexity is 83.65107845038573
At time: 200.0088987350464 and batch: 650, loss is 4.391778545379639 and perplexity is 80.78396924255442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.90506041283701 and perplexity of 134.97106374538794
Finished 32 epochs...
Completing Train Step...
At time: 200.93945908546448 and batch: 50, loss is 4.458255338668823 and perplexity is 86.33674923427081
At time: 201.3986213207245 and batch: 100, loss is 4.417725706100464 and perplexity is 82.90751472479597
At time: 201.8491096496582 and batch: 150, loss is 4.400759773254395 and perplexity is 81.51277637145874
At time: 202.29456305503845 and batch: 200, loss is 4.370584535598755 and perplexity is 79.08984902266705
At time: 202.74064540863037 and batch: 250, loss is 4.358122243881225 and perplexity is 78.11032448984601
At time: 203.18757319450378 and batch: 300, loss is 4.398072204589844 and perplexity is 81.29399930879734
At time: 203.63189792633057 and batch: 350, loss is 4.355579795837403 and perplexity is 77.91198528854105
At time: 204.07073545455933 and batch: 400, loss is 4.401168432235718 and perplexity is 81.54609410694775
At time: 204.511248588562 and batch: 450, loss is 4.381086359024048 and perplexity is 79.92481330166359
At time: 204.9554889202118 and batch: 500, loss is 4.352270202636719 and perplexity is 77.65455454228064
At time: 205.3965244293213 and batch: 550, loss is 4.3524976825714115 and perplexity is 77.67222140462867
At time: 205.8376235961914 and batch: 600, loss is 4.426605606079102 and perplexity is 83.64700359742831
At time: 206.27846002578735 and batch: 650, loss is 4.391612224578857 and perplexity is 80.77053430538541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.905002369600184 and perplexity of 134.9632298153259
Finished 33 epochs...
Completing Train Step...
At time: 207.20076990127563 and batch: 50, loss is 4.45764277458191 and perplexity is 86.28387863727677
At time: 207.64125680923462 and batch: 100, loss is 4.416982660293579 and perplexity is 82.8459335252798
At time: 208.0842101573944 and batch: 150, loss is 4.399994277954102 and perplexity is 81.4504026006926
At time: 208.52580881118774 and batch: 200, loss is 4.369727087020874 and perplexity is 79.02206260993253
At time: 208.96821975708008 and batch: 250, loss is 4.357423610687256 and perplexity is 78.05577308229657
At time: 209.4091558456421 and batch: 300, loss is 4.397263927459717 and perplexity is 81.22831777635562
At time: 209.85420727729797 and batch: 350, loss is 4.354970684051514 and perplexity is 77.86454263044773
At time: 210.29481196403503 and batch: 400, loss is 4.4005840873718265 and perplexity is 81.49845698529516
At time: 210.73567819595337 and batch: 450, loss is 4.380631790161133 and perplexity is 79.88849022645736
At time: 211.17656350135803 and batch: 500, loss is 4.351932611465454 and perplexity is 77.62834347482008
At time: 211.61883282661438 and batch: 550, loss is 4.3522917175292966 and perplexity is 77.6562252896527
At time: 212.05913829803467 and batch: 600, loss is 4.426569986343384 and perplexity is 83.64402416633014
At time: 212.51675295829773 and batch: 650, loss is 4.391460008621216 and perplexity is 80.75824067682389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9049826229319855 and perplexity of 134.96056476752074
Finished 34 epochs...
Completing Train Step...
At time: 213.42786049842834 and batch: 50, loss is 4.457020587921143 and perplexity is 86.23021065643164
At time: 213.88544917106628 and batch: 100, loss is 4.416256380081177 and perplexity is 82.78578600770226
At time: 214.327374458313 and batch: 150, loss is 4.399214382171631 and perplexity is 81.38690453937735
At time: 214.7722942829132 and batch: 200, loss is 4.368824996948242 and perplexity is 78.95080973482078
At time: 215.21456503868103 and batch: 250, loss is 4.356709403991699 and perplexity is 78.00004502957738
At time: 215.65583109855652 and batch: 300, loss is 4.396518659591675 and perplexity is 81.16780347362544
At time: 216.0969262123108 and batch: 350, loss is 4.354405765533447 and perplexity is 77.8205679306456
At time: 216.538428068161 and batch: 400, loss is 4.3999901866912845 and perplexity is 81.45006936637066
At time: 216.97957801818848 and batch: 450, loss is 4.380139837265014 and perplexity is 79.84919851795124
At time: 217.41963744163513 and batch: 500, loss is 4.351594944000244 and perplexity is 77.60213533391774
At time: 217.8640172481537 and batch: 550, loss is 4.352058773040771 and perplexity is 77.63813780674333
At time: 218.30572843551636 and batch: 600, loss is 4.42649076461792 and perplexity is 83.63739800488243
At time: 218.74717235565186 and batch: 650, loss is 4.391272697448731 and perplexity is 80.74311517270702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904957790000766 and perplexity of 134.9572133427115
Finished 35 epochs...
Completing Train Step...
At time: 219.66857814788818 and batch: 50, loss is 4.456421270370483 and perplexity is 86.17854686084426
At time: 220.11124229431152 and batch: 100, loss is 4.415580348968506 and perplexity is 82.72983915371182
At time: 220.57746863365173 and batch: 150, loss is 4.398456993103028 and perplexity is 81.32528632497834
At time: 221.0211741924286 and batch: 200, loss is 4.36799033164978 and perplexity is 78.88493972717858
At time: 221.4735984802246 and batch: 250, loss is 4.35606611251831 and perplexity is 77.94988440136794
At time: 221.9368007183075 and batch: 300, loss is 4.39568712234497 and perplexity is 81.10033747593636
At time: 222.40914630889893 and batch: 350, loss is 4.353798398971557 and perplexity is 77.77331667072745
At time: 222.8678798675537 and batch: 400, loss is 4.399402322769165 and perplexity is 81.40220188029863
At time: 223.31751465797424 and batch: 450, loss is 4.37967303276062 and perplexity is 79.81193325088476
At time: 223.76658940315247 and batch: 500, loss is 4.351284170150757 and perplexity is 77.57802236662555
At time: 224.23039507865906 and batch: 550, loss is 4.351834392547607 and perplexity is 77.62071927735607
At time: 224.67853474617004 and batch: 600, loss is 4.426372175216675 and perplexity is 83.62748008402305
At time: 225.30520796775818 and batch: 650, loss is 4.3910426902771 and perplexity is 80.72454581278235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904935051413143 and perplexity of 134.9541446411797
Finished 36 epochs...
Completing Train Step...
At time: 226.2470383644104 and batch: 50, loss is 4.455814294815063 and perplexity is 86.12625446120774
At time: 226.69849109649658 and batch: 100, loss is 4.414877786636352 and perplexity is 82.67173669766572
At time: 227.1482653617859 and batch: 150, loss is 4.397723121643066 and perplexity is 81.2656259125858
At time: 227.60024237632751 and batch: 200, loss is 4.3671316814422605 and perplexity is 78.81723422914143
At time: 228.0493426322937 and batch: 250, loss is 4.355339260101318 and perplexity is 77.89324692552316
At time: 228.49202919006348 and batch: 300, loss is 4.394966335296631 and perplexity is 81.0419024651971
At time: 228.93635988235474 and batch: 350, loss is 4.353280191421509 and perplexity is 77.73302439161964
At time: 229.38966155052185 and batch: 400, loss is 4.398861560821533 and perplexity is 81.35819456688097
At time: 229.85279822349548 and batch: 450, loss is 4.379224452972412 and perplexity is 79.77613925960134
At time: 230.30642318725586 and batch: 500, loss is 4.350938425064087 and perplexity is 77.55120478284978
At time: 230.7598216533661 and batch: 550, loss is 4.351602993011475 and perplexity is 77.60275995689038
At time: 231.2105929851532 and batch: 600, loss is 4.426188220977783 and perplexity is 83.61209786942896
At time: 231.65986704826355 and batch: 650, loss is 4.390795383453369 and perplexity is 80.70458455014025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904904533835018 and perplexity of 134.9500262303696
Finished 37 epochs...
Completing Train Step...
At time: 232.5984354019165 and batch: 50, loss is 4.455201997756958 and perplexity is 86.07353575037833
At time: 233.05127453804016 and batch: 100, loss is 4.414198141098023 and perplexity is 82.6155683101323
At time: 233.5057351589203 and batch: 150, loss is 4.396984252929688 and perplexity is 81.20560346121223
At time: 233.95714712142944 and batch: 200, loss is 4.3662878513336185 and perplexity is 78.75075392680414
At time: 234.40432715415955 and batch: 250, loss is 4.354657068252563 and perplexity is 77.84012690847894
At time: 234.8502480983734 and batch: 300, loss is 4.394179210662842 and perplexity is 80.97813748618275
At time: 235.2963764667511 and batch: 350, loss is 4.352722330093384 and perplexity is 77.689672236763
At time: 235.74030113220215 and batch: 400, loss is 4.3982979106903075 and perplexity is 81.31234993121824
At time: 236.19402432441711 and batch: 450, loss is 4.378753566741944 and perplexity is 79.73858261725088
At time: 236.63491535186768 and batch: 500, loss is 4.350614213943482 and perplexity is 77.52606589521649
At time: 237.07989645004272 and batch: 550, loss is 4.35137414932251 and perplexity is 77.5850030868793
At time: 237.5234682559967 and batch: 600, loss is 4.426005754470825 and perplexity is 83.59684285379832
At time: 237.96611642837524 and batch: 650, loss is 4.390610103607178 and perplexity is 80.68963300228086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904875811408548 and perplexity of 134.9461501938289
Finished 38 epochs...
Completing Train Step...
At time: 238.89339232444763 and batch: 50, loss is 4.454637899398803 and perplexity is 86.0249955022053
At time: 239.3365113735199 and batch: 100, loss is 4.41349419593811 and perplexity is 82.55743194549395
At time: 239.77900338172913 and batch: 150, loss is 4.396275882720947 and perplexity is 81.14810020014058
At time: 240.22210669517517 and batch: 200, loss is 4.365464925765991 and perplexity is 78.68597457587566
At time: 240.66567850112915 and batch: 250, loss is 4.353991012573243 and perplexity is 77.78829831215404
At time: 241.10947704315186 and batch: 300, loss is 4.393407888412476 and perplexity is 80.91570132924045
At time: 241.5513973236084 and batch: 350, loss is 4.352146959304809 and perplexity is 77.64498472596104
At time: 241.99453783035278 and batch: 400, loss is 4.397727346420288 and perplexity is 81.26596924247629
At time: 242.43803596496582 and batch: 450, loss is 4.378352966308594 and perplexity is 79.70664570389746
At time: 242.88049006462097 and batch: 500, loss is 4.350321846008301 and perplexity is 77.50340307251078
At time: 243.3229010105133 and batch: 550, loss is 4.351149711608887 and perplexity is 77.5675920400959
At time: 243.76615524291992 and batch: 600, loss is 4.4258425998687745 and perplexity is 83.58320475675922
At time: 244.20875358581543 and batch: 650, loss is 4.390403985977173 and perplexity is 80.67300316027116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904857261508119 and perplexity of 134.9436469793968
Finished 39 epochs...
Completing Train Step...
At time: 245.13883018493652 and batch: 50, loss is 4.454078340530396 and perplexity is 85.97687291803231
At time: 245.58540391921997 and batch: 100, loss is 4.4128339481353756 and perplexity is 82.50294157300635
At time: 246.0320565700531 and batch: 150, loss is 4.395597076416015 and perplexity is 81.0930350494916
At time: 246.4773452281952 and batch: 200, loss is 4.364706153869629 and perplexity is 78.62629251512824
At time: 246.92386293411255 and batch: 250, loss is 4.35331503868103 and perplexity is 77.73573322169139
At time: 247.37269234657288 and batch: 300, loss is 4.392676916122436 and perplexity is 80.85657580593191
At time: 247.8182179927826 and batch: 350, loss is 4.351580810546875 and perplexity is 77.60103855550734
At time: 248.26335859298706 and batch: 400, loss is 4.397149095535278 and perplexity is 81.21899070783931
At time: 248.7096972465515 and batch: 450, loss is 4.3779127979278565 and perplexity is 79.67156907910089
At time: 249.15593123435974 and batch: 500, loss is 4.350021448135376 and perplexity is 77.48012471164344
At time: 249.6027479171753 and batch: 550, loss is 4.350915899276734 and perplexity is 77.54945790057684
At time: 250.04975008964539 and batch: 600, loss is 4.4256762027740475 and perplexity is 83.56929791138114
At time: 250.4968454837799 and batch: 650, loss is 4.390201377868652 and perplexity is 80.65665981139558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904809390797334 and perplexity of 134.9371872857163
Finished 40 epochs...
Completing Train Step...
At time: 251.4173243045807 and batch: 50, loss is 4.453507661819458 and perplexity is 85.92782174458642
At time: 251.8754916191101 and batch: 100, loss is 4.412214279174805 and perplexity is 82.45183289782301
At time: 252.31744027137756 and batch: 150, loss is 4.394969367980957 and perplexity is 81.0421482400772
At time: 252.7599754333496 and batch: 200, loss is 4.363933248519897 and perplexity is 78.56554531195701
At time: 253.20168614387512 and batch: 250, loss is 4.352701911926269 and perplexity is 77.68808597224657
At time: 253.64474892616272 and batch: 300, loss is 4.391948719024658 and perplexity is 80.79771771484018
At time: 254.0875473022461 and batch: 350, loss is 4.35099081993103 and perplexity is 77.55526817435513
At time: 254.52933764457703 and batch: 400, loss is 4.3965985965728756 and perplexity is 81.17429204214064
At time: 254.97197246551514 and batch: 450, loss is 4.377502231597901 and perplexity is 79.6388653293709
At time: 255.41438579559326 and batch: 500, loss is 4.349694147109985 and perplexity is 77.45476953699159
At time: 255.8566071987152 and batch: 550, loss is 4.350683012008667 and perplexity is 77.53139972202788
At time: 256.314035654068 and batch: 600, loss is 4.425512218475342 and perplexity is 83.55559498223316
At time: 256.75806999206543 and batch: 650, loss is 4.390007944107055 and perplexity is 80.64105959914295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.904818665747549 and perplexity of 134.93843882721453
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 257.68276286125183 and batch: 50, loss is 4.453128242492676 and perplexity is 85.89522525256884
At time: 258.12084770202637 and batch: 100, loss is 4.411195611953735 and perplexity is 82.36788468323942
At time: 258.5594274997711 and batch: 150, loss is 4.393673067092895 and perplexity is 80.9371612933809
At time: 259.0014138221741 and batch: 200, loss is 4.36226453781128 and perplexity is 78.43455147097623
At time: 259.44378542900085 and batch: 250, loss is 4.349911499023437 and perplexity is 77.4716063090425
At time: 259.8859317302704 and batch: 300, loss is 4.387910852432251 and perplexity is 80.47212510188564
At time: 260.32796239852905 and batch: 350, loss is 4.346830720901489 and perplexity is 77.23330075101184
At time: 260.770788192749 and batch: 400, loss is 4.3924971961975094 and perplexity is 80.84204557392344
At time: 261.2163772583008 and batch: 450, loss is 4.37188497543335 and perplexity is 79.1927675179996
At time: 261.65912556648254 and batch: 500, loss is 4.3431856822967525 and perplexity is 76.95229483803112
At time: 262.09939193725586 and batch: 550, loss is 4.343795547485351 and perplexity is 76.99923967739225
At time: 262.54408740997314 and batch: 600, loss is 4.4189411163330075 and perplexity is 83.00834262776762
At time: 262.98620104789734 and batch: 650, loss is 4.385524578094483 and perplexity is 80.28032546913396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.903388528262868 and perplexity of 134.74559623630057
Finished 42 epochs...
Completing Train Step...
At time: 263.90528440475464 and batch: 50, loss is 4.451427021026611 and perplexity is 85.74922267810642
At time: 264.3666157722473 and batch: 100, loss is 4.410530796051026 and perplexity is 82.3131434020828
At time: 264.8140661716461 and batch: 150, loss is 4.39292537689209 and perplexity is 80.87666798894448
At time: 265.2576959133148 and batch: 200, loss is 4.361656713485718 and perplexity is 78.38689152853114
At time: 265.6964735984802 and batch: 250, loss is 4.34942045211792 and perplexity is 77.43357345522082
At time: 266.1393904685974 and batch: 300, loss is 4.387610750198364 and perplexity is 80.44797886072864
At time: 266.58158564567566 and batch: 350, loss is 4.346710929870605 and perplexity is 77.2240494484189
At time: 267.0241184234619 and batch: 400, loss is 4.39231969833374 and perplexity is 80.82769755694015
At time: 267.4675211906433 and batch: 450, loss is 4.371800479888916 and perplexity is 79.18607636468322
At time: 267.9249551296234 and batch: 500, loss is 4.343206787109375 and perplexity is 76.95391891893249
At time: 268.3663067817688 and batch: 550, loss is 4.344064626693726 and perplexity is 77.01996135961197
At time: 268.8086004257202 and batch: 600, loss is 4.419170703887939 and perplexity is 83.02740249806126
At time: 269.25174283981323 and batch: 650, loss is 4.385384464263916 and perplexity is 80.26907787320364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9031108781403185 and perplexity of 134.7081892982534
Finished 43 epochs...
Completing Train Step...
At time: 270.1802382469177 and batch: 50, loss is 4.450787105560303 and perplexity is 85.69436797734835
At time: 270.62179040908813 and batch: 100, loss is 4.410087041854858 and perplexity is 82.27662470256074
At time: 271.06414794921875 and batch: 150, loss is 4.3925043392181395 and perplexity is 80.84262303238515
At time: 271.5057544708252 and batch: 200, loss is 4.361384744644165 and perplexity is 78.36557563521002
At time: 271.9484202861786 and batch: 250, loss is 4.349168205261231 and perplexity is 77.41404354299755
At time: 272.39069652557373 and batch: 300, loss is 4.38746826171875 and perplexity is 80.43651676716027
At time: 272.83505511283875 and batch: 350, loss is 4.3467395210266115 and perplexity is 77.22625740482798
At time: 273.27730321884155 and batch: 400, loss is 4.3922856998443605 and perplexity is 80.82494958403693
At time: 273.719535112381 and batch: 450, loss is 4.371741132736206 and perplexity is 79.181377035964
At time: 274.16169118881226 and batch: 500, loss is 4.343256235122681 and perplexity is 76.95772423142088
At time: 274.6071228981018 and batch: 550, loss is 4.344235782623291 and perplexity is 77.03314491088285
At time: 275.0545997619629 and batch: 600, loss is 4.419291067123413 and perplexity is 83.03739654630482
At time: 275.5003137588501 and batch: 650, loss is 4.385252170562744 and perplexity is 80.2584594821907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902950212067249 and perplexity of 134.68654800102615
Finished 44 epochs...
Completing Train Step...
At time: 276.4180552959442 and batch: 50, loss is 4.450284948348999 and perplexity is 85.65134673511797
At time: 276.8806381225586 and batch: 100, loss is 4.409718618392945 and perplexity is 82.24631764691127
At time: 277.32752990722656 and batch: 150, loss is 4.392181758880615 and perplexity is 80.8165489974726
At time: 277.7739369869232 and batch: 200, loss is 4.361187801361084 and perplexity is 78.35014358113348
At time: 278.2205991744995 and batch: 250, loss is 4.348992156982422 and perplexity is 77.40041613345234
At time: 278.66760444641113 and batch: 300, loss is 4.387374086380005 and perplexity is 80.42894198763055
At time: 279.11114740371704 and batch: 350, loss is 4.34679856300354 and perplexity is 77.23081713034226
At time: 279.56671619415283 and batch: 400, loss is 4.392279329299927 and perplexity is 80.82443468674438
At time: 280.0092046260834 and batch: 450, loss is 4.371682376861572 and perplexity is 79.17672480157592
At time: 280.452344417572 and batch: 500, loss is 4.34329888343811 and perplexity is 76.96100641870801
At time: 280.8960540294647 and batch: 550, loss is 4.34435996055603 and perplexity is 77.04271132152655
At time: 281.3413600921631 and batch: 600, loss is 4.41936448097229 and perplexity is 83.04349286496041
At time: 281.7841169834137 and batch: 650, loss is 4.385119819641114 and perplexity is 80.24783790401294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902843998927696 and perplexity of 134.67224327959556
Finished 45 epochs...
Completing Train Step...
At time: 282.7123339176178 and batch: 50, loss is 4.449856405258179 and perplexity is 85.61464930603161
At time: 283.1538646221161 and batch: 100, loss is 4.409408512115479 and perplexity is 82.22081650174746
At time: 283.595618724823 and batch: 150, loss is 4.391912298202515 and perplexity is 80.79477504912118
At time: 284.03767919540405 and batch: 200, loss is 4.361036014556885 and perplexity is 78.33825196574871
At time: 284.4799792766571 and batch: 250, loss is 4.348847122192383 and perplexity is 77.3891911943725
At time: 284.9206805229187 and batch: 300, loss is 4.387292804718018 and perplexity is 80.42240485523205
At time: 285.3666388988495 and batch: 350, loss is 4.346873073577881 and perplexity is 77.23657185727478
At time: 285.8120265007019 and batch: 400, loss is 4.392284469604492 and perplexity is 80.82485015002277
At time: 286.2582643032074 and batch: 450, loss is 4.371622228622437 and perplexity is 79.17196260421893
At time: 286.7034316062927 and batch: 500, loss is 4.343337373733521 and perplexity is 76.96396872758987
At time: 287.1503689289093 and batch: 550, loss is 4.344453506469726 and perplexity is 77.0499186894554
At time: 287.5964095592499 and batch: 600, loss is 4.41940902709961 and perplexity is 83.04719221336182
At time: 288.0421848297119 and batch: 650, loss is 4.38498779296875 and perplexity is 80.23724374838105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9027668074065565 and perplexity of 134.6618481254956
Finished 46 epochs...
Completing Train Step...
At time: 288.9616038799286 and batch: 50, loss is 4.449474086761475 and perplexity is 85.58192349825255
At time: 289.42275953292847 and batch: 100, loss is 4.409132776260376 and perplexity is 82.1981483999499
At time: 289.86781787872314 and batch: 150, loss is 4.391675319671631 and perplexity is 80.77563069051753
At time: 290.31043434143066 and batch: 200, loss is 4.360908498764038 and perplexity is 78.32826323831287
At time: 290.7502474784851 and batch: 250, loss is 4.348723649978638 and perplexity is 77.37963636950563
At time: 291.2072148323059 and batch: 300, loss is 4.3872225379943846 and perplexity is 80.41675403487079
At time: 291.65005350112915 and batch: 350, loss is 4.3469476318359375 and perplexity is 77.24233069621253
At time: 292.09242939949036 and batch: 400, loss is 4.392294549942017 and perplexity is 80.82566489589911
At time: 292.5353674888611 and batch: 450, loss is 4.371558828353882 and perplexity is 79.16694323964401
At time: 292.9788553714752 and batch: 500, loss is 4.343367834091186 and perplexity is 76.9663131133099
At time: 293.4215226173401 and batch: 550, loss is 4.344524841308594 and perplexity is 77.05541522903495
At time: 293.86455273628235 and batch: 600, loss is 4.419433555603027 and perplexity is 83.04922926168264
At time: 294.30709505081177 and batch: 650, loss is 4.384857969284058 and perplexity is 80.22682772988566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902706370634191 and perplexity of 134.65370984396236
Finished 47 epochs...
Completing Train Step...
At time: 295.2303349971771 and batch: 50, loss is 4.449123229980469 and perplexity is 85.55190176703358
At time: 295.67248797416687 and batch: 100, loss is 4.4088797187805175 and perplexity is 82.17735017535095
At time: 296.1147651672363 and batch: 150, loss is 4.391460390090942 and perplexity is 80.75827148365371
At time: 296.5565810203552 and batch: 200, loss is 4.360794124603271 and perplexity is 78.31930502124467
At time: 296.999370098114 and batch: 250, loss is 4.348612546920776 and perplexity is 77.37103973285394
At time: 297.4465353488922 and batch: 300, loss is 4.387162284851074 and perplexity is 80.4119088186366
At time: 297.8924117088318 and batch: 350, loss is 4.347018508911133 and perplexity is 77.24780560071389
At time: 298.3392128944397 and batch: 400, loss is 4.39230525970459 and perplexity is 80.82653052421527
At time: 298.7860209941864 and batch: 450, loss is 4.371492309570312 and perplexity is 79.16167732602383
At time: 299.23078417778015 and batch: 500, loss is 4.343390340805054 and perplexity is 76.96804539159046
At time: 299.675749540329 and batch: 550, loss is 4.34458023071289 and perplexity is 77.05968340078692
At time: 300.1231427192688 and batch: 600, loss is 4.4194371700286865 and perplexity is 83.04952943749034
At time: 300.5704278945923 and batch: 650, loss is 4.384732112884522 and perplexity is 80.21673130556452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902654311236213 and perplexity of 134.64670003535713
Finished 48 epochs...
Completing Train Step...
At time: 301.4873080253601 and batch: 50, loss is 4.448783340454102 and perplexity is 85.52282851278942
At time: 301.9441487789154 and batch: 100, loss is 4.40864330291748 and perplexity is 82.1579244425533
At time: 302.38944840431213 and batch: 150, loss is 4.39126030921936 and perplexity is 80.74211491467192
At time: 302.84538888931274 and batch: 200, loss is 4.360680255889893 and perplexity is 78.31038741047729
At time: 303.2809064388275 and batch: 250, loss is 4.348516044616699 and perplexity is 77.36357360950569
At time: 303.7201237678528 and batch: 300, loss is 4.387104825973511 and perplexity is 80.40728857335147
At time: 304.16287636756897 and batch: 350, loss is 4.347082843780518 and perplexity is 77.25277548806429
At time: 304.6168894767761 and batch: 400, loss is 4.392312355041504 and perplexity is 80.82710401771553
At time: 305.05561089515686 and batch: 450, loss is 4.371420431137085 and perplexity is 79.15598751317583
At time: 305.48987007141113 and batch: 500, loss is 4.343403978347778 and perplexity is 76.9690950537553
At time: 305.9236180782318 and batch: 550, loss is 4.344622478485108 and perplexity is 77.06293906951029
At time: 306.3571593761444 and batch: 600, loss is 4.419433679580688 and perplexity is 83.04923955793247
At time: 306.7995653152466 and batch: 650, loss is 4.384609632492065 and perplexity is 80.20690693049157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902614518707874 and perplexity of 134.64134220933164
Finished 49 epochs...
Completing Train Step...
At time: 307.7238259315491 and batch: 50, loss is 4.448475131988525 and perplexity is 85.49647371463628
At time: 308.1648123264313 and batch: 100, loss is 4.408423013687134 and perplexity is 82.13982792991762
At time: 308.6075699329376 and batch: 150, loss is 4.391075735092163 and perplexity is 80.72721338454424
At time: 309.05060482025146 and batch: 200, loss is 4.360586824417115 and perplexity is 78.30307109743998
At time: 309.4928960800171 and batch: 250, loss is 4.348424234390259 and perplexity is 77.35647116833775
At time: 309.93500232696533 and batch: 300, loss is 4.387047872543335 and perplexity is 80.40270923286192
At time: 310.3765251636505 and batch: 350, loss is 4.3471444225311275 and perplexity is 77.25753276393202
At time: 310.8219075202942 and batch: 400, loss is 4.392318868637085 and perplexity is 80.82763049449771
At time: 311.26690006256104 and batch: 450, loss is 4.371345558166504 and perplexity is 79.15006109111864
At time: 311.71359395980835 and batch: 500, loss is 4.343412752151489 and perplexity is 76.96977036844966
At time: 312.15946435928345 and batch: 550, loss is 4.344651966094971 and perplexity is 77.06521150489667
At time: 312.6037492752075 and batch: 600, loss is 4.419428882598877 and perplexity is 83.04884117319635
At time: 313.04948258399963 and batch: 650, loss is 4.384488496780396 and perplexity is 80.19719159818845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.902580111634498 and perplexity of 134.6367096744872
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f93af975898>
SETTINGS FOR THIS RUN
{'lr': 0.0, 'data': 'ptb', 'num_layers': 1, 'anneal': 7.599300554206457, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.0, 'seq_len': 20, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7344036102294922 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 1.1931540966033936 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 1.6345343589782715 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 2.075352668762207 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 2.515947103500366 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 2.956833600997925 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 3.3979721069335938 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 3.8412551879882812 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 4.283356666564941 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 4.725013256072998 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 5.166638374328613 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 5.608709812164307 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 6.050455331802368 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 1 epochs...
Completing Train Step...
At time: 6.986251354217529 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 7.43192458152771 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 7.878146409988403 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 8.32223916053772 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 8.768938302993774 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 9.216060876846313 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 9.665797710418701 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 10.111897706985474 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 10.557072877883911 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 11.00119400024414 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 11.447645664215088 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 11.89284372329712 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 12.351968050003052 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 2 epochs...
Completing Train Step...
At time: 13.265868663787842 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 13.719751834869385 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 14.161505222320557 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 14.620240688323975 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 15.062318563461304 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 15.50563383102417 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 15.947957515716553 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 16.391786575317383 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 16.834121465682983 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 17.275519132614136 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 17.71609091758728 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 18.15814781188965 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 18.599754095077515 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 3 epochs...
Completing Train Step...
At time: 19.530879020690918 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 19.97291898727417 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 20.423361778259277 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 20.864577054977417 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 21.310853958129883 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 21.755249738693237 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 22.202174425125122 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 22.647827863693237 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 23.094014883041382 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 23.539491415023804 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 23.99861168861389 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 24.44471573829651 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 24.889081239700317 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 4 epochs...
Completing Train Step...
At time: 25.81045913696289 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 26.269223928451538 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 26.711523294448853 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 27.150329113006592 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 27.58854842185974 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 28.030377864837646 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 28.471433877944946 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 28.91358780860901 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 29.355008602142334 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 29.796406030654907 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 30.237228631973267 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 30.679153203964233 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 31.120444297790527 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 5 epochs...
Completing Train Step...
At time: 32.05547785758972 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 32.499244928359985 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 32.93659329414368 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 33.3723030090332 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 33.807846784591675 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 34.24469065666199 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 34.68138241767883 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 35.12047553062439 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 35.57585620880127 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 36.01701498031616 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 36.4594361782074 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 36.901362895965576 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 37.34266686439514 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 6 epochs...
Completing Train Step...
At time: 38.26267170906067 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 38.722466230392456 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 39.16907024383545 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 39.61442732810974 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 40.05833053588867 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 40.505953788757324 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 40.94574522972107 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 41.38685917854309 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 41.83024764060974 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 42.27271819114685 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 42.71438002586365 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 43.15632343292236 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 43.59798812866211 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 7 epochs...
Completing Train Step...
At time: 44.52568697929382 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 44.96850275993347 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 45.41011714935303 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 45.850369691848755 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 46.29289364814758 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 46.73414468765259 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 47.190444469451904 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 47.632638454437256 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 48.07457160949707 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 48.52076029777527 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 48.96702837944031 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 49.414613008499146 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 49.8586790561676 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 8 epochs...
Completing Train Step...
At time: 50.778632402420044 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 51.239402532577515 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 51.68462610244751 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 52.13204216957092 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 52.577895164489746 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 53.0247745513916 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 53.46690583229065 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 53.90638256072998 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 54.34795379638672 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 54.790733098983765 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 55.233710289001465 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 55.676594734191895 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 56.118576526641846 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 9 epochs...
Completing Train Step...
At time: 57.04891896247864 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 57.49060845375061 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 57.9320752620697 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 58.372882604599 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 58.82923889160156 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 59.27103924751282 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 59.712403297424316 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 60.154295682907104 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 60.598029375076294 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 61.042977809906006 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 61.487975120544434 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 61.932656049728394 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 62.3788902759552 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 10 epochs...
Completing Train Step...
At time: 63.299585819244385 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 63.75948905944824 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 64.20515513420105 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 64.6509919166565 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 65.09763526916504 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 65.54225301742554 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 65.98610019683838 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 66.43208050727844 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 66.87597703933716 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 67.31982707977295 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 67.76109313964844 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 68.19894218444824 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 68.63613486289978 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 11 epochs...
Completing Train Step...
At time: 69.57766675949097 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 70.0294394493103 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 70.49789619445801 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 70.97384095191956 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 71.43926334381104 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 71.89337110519409 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 72.34064865112305 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 72.79843664169312 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 73.2585437297821 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 73.72744274139404 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 74.17662525177002 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 74.6181468963623 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 75.0625159740448 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 12 epochs...
Completing Train Step...
At time: 75.97579336166382 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 76.4268810749054 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 76.86543893814087 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 77.30379986763 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 77.74073505401611 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 78.17759227752686 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 78.61668586730957 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 79.05327916145325 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 79.48920392990112 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 79.92691493034363 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 80.36702871322632 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 80.80655026435852 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 81.24631118774414 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 13 epochs...
Completing Train Step...
At time: 82.1969861984253 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 82.63958978652954 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 83.08701634407043 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 83.53734230995178 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 83.98672080039978 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 84.4329400062561 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 84.88305020332336 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 85.33019351959229 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 85.77755331993103 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 86.22328448295593 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 86.67209434509277 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 87.1202597618103 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 87.56590008735657 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 14 epochs...
Completing Train Step...
At time: 88.50521755218506 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 88.9644730091095 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 89.40596055984497 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 89.84627509117126 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 90.28910255432129 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 90.73235654830933 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 91.17623901367188 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 91.61887907981873 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 92.05999088287354 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 92.50365161895752 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 92.94977498054504 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 93.4193468093872 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 93.86135053634644 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 15 epochs...
Completing Train Step...
At time: 94.7978413105011 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 95.24013137817383 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 95.6825659275055 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 96.1267580986023 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 96.56953620910645 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 97.0124351978302 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 97.45672845840454 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 97.90047359466553 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 98.34571599960327 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 98.79155588150024 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 99.24073338508606 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 99.68652129173279 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 100.13253736495972 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 16 epochs...
Completing Train Step...
At time: 101.05841445922852 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 101.51612257957458 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 101.96379566192627 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 102.41174411773682 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 102.85889101028442 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 103.30308389663696 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 103.74215602874756 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 104.1856586933136 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 104.62731599807739 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 105.09568548202515 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 105.54060292243958 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 105.98248934745789 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 106.42396593093872 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 17 epochs...
Completing Train Step...
At time: 107.35945105552673 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 107.80188179016113 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 108.24298238754272 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 108.68708324432373 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 109.12865781784058 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 109.56946516036987 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 110.01158571243286 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 110.45846271514893 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 110.90409588813782 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 111.34848165512085 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 111.79697132110596 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 112.24164581298828 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 112.68857049942017 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 18 epochs...
Completing Train Step...
At time: 113.61463665962219 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 114.0748131275177 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 114.51996111869812 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 114.96904587745667 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 115.41651225090027 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 115.85647773742676 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 116.29574680328369 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 116.75455713272095 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 117.19652724266052 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 117.64041924476624 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 118.08534526824951 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 118.52767872810364 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 118.9693169593811 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 19 epochs...
Completing Train Step...
At time: 119.91190671920776 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 120.3486590385437 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 120.78557324409485 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 121.2239511013031 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 121.6618824005127 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 122.0983989238739 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 122.53605842590332 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 122.97785878181458 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 123.4195761680603 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 123.85883688926697 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 124.30064034461975 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 124.74405837059021 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 125.18509221076965 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 20 epochs...
Completing Train Step...
At time: 126.11002588272095 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 126.57086896896362 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 127.01653528213501 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 127.46467995643616 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 127.91084265708923 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 128.36661553382874 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 128.80741214752197 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 129.25308537483215 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 129.69529581069946 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 130.13872480392456 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 130.58243823051453 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 131.02779579162598 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 131.46894574165344 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 21 epochs...
Completing Train Step...
At time: 132.40058541297913 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 132.8438937664032 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 133.286315202713 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 133.7276713848114 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 134.17343997955322 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 134.6145887374878 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 135.05623865127563 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 135.50503730773926 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 135.95056867599487 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 136.39715099334717 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 136.84218287467957 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 137.29057908058167 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 137.73617720603943 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 22 epochs...
Completing Train Step...
At time: 138.65975975990295 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 139.12106561660767 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 139.56808924674988 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 140.02905201911926 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 140.47870683670044 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 140.91967606544495 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 141.3594732284546 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 141.80380368232727 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 142.24772548675537 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 142.69045209884644 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 143.13265085220337 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 143.5771462917328 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 144.02128171920776 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 23 epochs...
Completing Train Step...
At time: 144.96737837791443 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 145.4108521938324 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 145.85379004478455 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 146.29587936401367 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 146.74050569534302 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 147.18167996406555 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 147.62474656105042 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 148.06879019737244 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 148.51625561714172 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 148.96070456504822 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 149.4071867465973 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 149.85521745681763 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 150.3018352985382 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 24 epochs...
Completing Train Step...
At time: 151.22429656982422 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 151.6830117702484 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 152.13082671165466 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 152.5734043121338 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 153.02688074111938 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 153.4903781414032 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 153.95479202270508 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 154.41012144088745 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 154.852942943573 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 155.29572415351868 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 155.73834085464478 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 156.17943120002747 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 156.6214518547058 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 25 epochs...
Completing Train Step...
At time: 157.55030250549316 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 157.99134516716003 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 158.43355584144592 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 158.8749372959137 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 159.31638932228088 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 159.7580282688141 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 160.19950127601624 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 160.64065194129944 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 161.08204793930054 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 161.5230085849762 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 161.9666588306427 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 162.41242861747742 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 162.87380981445312 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 26 epochs...
Completing Train Step...
At time: 163.7919750213623 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 164.24763751029968 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 164.70410299301147 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 165.1393322944641 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 165.5740931034088 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 166.01048612594604 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 166.44583344459534 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 166.88187217712402 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 167.31669998168945 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 167.75314164161682 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 168.18812131881714 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 168.62435221672058 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 169.06040048599243 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 27 epochs...
Completing Train Step...
At time: 169.98124074935913 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 170.42094922065735 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 170.86247563362122 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 171.30281782150269 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 171.74333667755127 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 172.1836302280426 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 172.62463307380676 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 173.06458187103271 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 173.50697088241577 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 173.9486207962036 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 174.4026665687561 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 174.8439769744873 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 175.28408908843994 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 28 epochs...
Completing Train Step...
At time: 176.1838629245758 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 176.63803601264954 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 177.0771312713623 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 177.5171639919281 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 177.95545554161072 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 178.39384865760803 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 178.8331151008606 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 179.27216482162476 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 179.71228861808777 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 180.1519296169281 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 180.59040546417236 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 181.0327968597412 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 181.47293210029602 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 29 epochs...
Completing Train Step...
At time: 182.4172112941742 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 182.86049628257751 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 183.30526852607727 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 183.7489151954651 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 184.19242906570435 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 184.63724374771118 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 185.0823040008545 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 185.52819871902466 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 185.9822645187378 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 186.42257738113403 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 186.86376070976257 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 187.30252289772034 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 187.74236607551575 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 30 epochs...
Completing Train Step...
At time: 188.65063095092773 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 189.10424876213074 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 189.54275107383728 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 189.98225951194763 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 190.42061066627502 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 190.86165928840637 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 191.3015899658203 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 191.74190735816956 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 192.1806721687317 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 192.61954641342163 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 193.05796217918396 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 193.51423501968384 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 193.9542737007141 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 31 epochs...
Completing Train Step...
At time: 194.888610124588 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 195.32963609695435 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 195.76946234703064 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 196.21066999435425 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 196.6539797782898 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 197.09429025650024 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 197.54807305335999 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 197.98847270011902 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 198.42915868759155 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 198.86906743049622 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 199.30773901939392 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 199.74706387519836 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 200.18665957450867 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 32 epochs...
Completing Train Step...
At time: 201.09787392616272 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 201.55398893356323 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 201.9940803050995 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 202.43494844436646 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 202.8746693134308 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 203.3130979537964 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 203.76136898994446 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 204.20086669921875 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 204.63994646072388 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 205.08030652999878 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 205.52007961273193 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 205.95924973487854 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 206.3981580734253 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 33 epochs...
Completing Train Step...
At time: 207.32760953903198 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 207.76181936264038 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 208.19645714759827 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 208.6331331729889 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 209.08193159103394 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 209.51879906654358 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 209.95382642745972 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 210.38839960098267 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 210.82546377182007 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 211.2632565498352 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 211.70172023773193 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 212.1596405506134 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 212.59855675697327 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 34 epochs...
Completing Train Step...
At time: 213.52887725830078 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 213.99611806869507 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 214.43642807006836 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 214.87729215621948 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 215.31743550300598 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 215.75750637054443 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 216.1981692314148 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 216.6396324634552 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 217.08079051971436 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 217.52170515060425 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 217.96446561813354 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 218.40524649620056 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 218.8449535369873 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 35 epochs...
Completing Train Step...
At time: 219.78148293495178 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 220.22300672531128 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 220.67705631256104 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 221.11640048027039 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 221.55887150764465 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 221.9981837272644 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 222.4399573802948 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 222.8809003829956 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 223.32257604599 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 223.76340889930725 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 224.20315885543823 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 224.64354372024536 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 225.0841715335846 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 36 epochs...
Completing Train Step...
At time: 226.0118272304535 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 226.46901154518127 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 226.91081428527832 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 227.35357427597046 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 227.79682159423828 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 228.23996138572693 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 228.68199110031128 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 229.12570786476135 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 229.57032108306885 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 230.01244354248047 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 230.45374965667725 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 230.8955852985382 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 231.3369379043579 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 37 epochs...
Completing Train Step...
At time: 232.27837324142456 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 232.71962308883667 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 233.16229677200317 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 233.6041865348816 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 234.04527711868286 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 234.49156713485718 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 234.93322563171387 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 235.37713742256165 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 235.82101249694824 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 236.27904725074768 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 236.71737837791443 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 237.15541243553162 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 237.6021101474762 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 38 epochs...
Completing Train Step...
At time: 238.52906441688538 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 238.9840452671051 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 239.42483711242676 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 239.86771297454834 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 240.3100357055664 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 240.7516541481018 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 241.1926019191742 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 241.63711619377136 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 242.0785734653473 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 242.52064037322998 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 242.96195149421692 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 243.41696047782898 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 243.86013650894165 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 39 epochs...
Completing Train Step...
At time: 244.78261017799377 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 245.2268192768097 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 245.66834497451782 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 246.1113851070404 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 246.55372738838196 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 246.99588012695312 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 247.43688082695007 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 247.87738251686096 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 248.31863141059875 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 248.7588279247284 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 249.199068069458 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 249.64050006866455 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 250.08327913284302 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 40 epochs...
Completing Train Step...
At time: 250.99494338035583 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 251.4466381072998 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 251.88259625434875 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 252.3214659690857 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 252.75768065452576 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 253.1936182975769 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 253.62975072860718 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 254.06569457054138 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 254.50168085098267 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 254.95194268226624 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 255.38736510276794 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 255.82471752166748 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 256.2636835575104 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 41 epochs...
Completing Train Step...
At time: 257.1865348815918 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 257.6282513141632 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 258.07031536102295 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 258.5116357803345 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 258.9543128013611 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 259.39734268188477 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 259.8440251350403 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 260.2908937931061 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 260.73453736305237 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 261.17561769485474 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 261.6196537017822 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 262.0622224807739 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 262.5046718120575 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 42 epochs...
Completing Train Step...
At time: 263.42844223976135 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 263.88409399986267 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 264.32558155059814 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 264.766330242157 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 265.20690083503723 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 265.64775228500366 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 266.089882850647 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 266.55675649642944 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 266.9998905658722 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 267.44095945358276 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 267.8819811344147 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 268.3226544857025 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 268.7642469406128 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 43 epochs...
Completing Train Step...
At time: 269.6915898323059 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 270.1323912143707 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 270.57461977005005 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 271.0146794319153 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 271.4575979709625 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 271.8990993499756 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 272.3433096408844 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 272.787446975708 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 273.2304470539093 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 273.67198872566223 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 274.1140465736389 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 274.55594086647034 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 274.99731826782227 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 44 epochs...
Completing Train Step...
At time: 275.9199171066284 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 276.3757948875427 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 276.8173429965973 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 277.2593629360199 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 277.70266580581665 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 278.16053581237793 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 278.60250210762024 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 279.04425287246704 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 279.4859404563904 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 279.92732310295105 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 280.3683497905731 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 280.81147480010986 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 281.2522015571594 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 45 epochs...
Completing Train Step...
At time: 282.17450737953186 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 282.6157350540161 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 283.0582411289215 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 283.50142455101013 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 283.9425530433655 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 284.3832790851593 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 284.82444977760315 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 285.26681637763977 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 285.70733761787415 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 286.14766454696655 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 286.588748216629 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 287.029554605484 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 287.4698748588562 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 46 epochs...
Completing Train Step...
At time: 288.37690353393555 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 288.8354160785675 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 289.27655720710754 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 289.73262214660645 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 290.1734027862549 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 290.61447644233704 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 291.05524611473083 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 291.4947507381439 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 291.9348473548889 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 292.378121137619 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 292.81966495513916 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 293.2609586715698 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 293.70358753204346 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 47 epochs...
Completing Train Step...
At time: 294.62868452072144 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 295.0645933151245 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 295.4998278617859 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 295.93738865852356 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 296.3741412162781 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 296.8109710216522 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 297.2470347881317 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 297.68335914611816 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 298.1199927330017 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 298.55658745765686 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 298.9934723377228 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 299.43045568466187 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 299.87066102027893 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 48 epochs...
Completing Train Step...
At time: 300.79946660995483 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 301.2552411556244 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 301.6994092464447 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 302.1435670852661 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 302.5852861404419 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 303.0267722606659 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 303.46921825408936 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 303.91129517555237 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 304.35334730148315 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 304.79490327835083 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 305.239310503006 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 305.68205642700195 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 306.12447595596313 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished 49 epochs...
Completing Train Step...
At time: 307.04855465888977 and batch: 50, loss is 9.18266538619995 and perplexity is 9727.044582158038
At time: 307.48921370506287 and batch: 100, loss is 9.182850952148437 and perplexity is 9728.849757896289
At time: 307.93066215515137 and batch: 150, loss is 9.183238620758056 and perplexity is 9732.622058709188
At time: 308.3747708797455 and batch: 200, loss is 9.182711143493652 and perplexity is 9727.48967557689
At time: 308.81533765792847 and batch: 250, loss is 9.183520755767823 and perplexity is 9735.36835952437
At time: 309.2569534778595 and batch: 300, loss is 9.18353063583374 and perplexity is 9735.464546080657
At time: 309.6990158557892 and batch: 350, loss is 9.182820148468018 and perplexity is 9728.550078133148
At time: 310.13999223709106 and batch: 400, loss is 9.181647701263428 and perplexity is 9717.150550767168
At time: 310.58279752731323 and batch: 450, loss is 9.183087482452393 and perplexity is 9731.151197856088
At time: 311.0246124267578 and batch: 500, loss is 9.18246774673462 and perplexity is 9725.122324230846
At time: 311.4671049118042 and batch: 550, loss is 9.183024845123292 and perplexity is 9730.541683625344
At time: 311.9070944786072 and batch: 600, loss is 9.182000102996826 and perplexity is 9720.575494907629
At time: 312.36308097839355 and batch: 650, loss is 9.182598953247071 and perplexity is 9726.398407327546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.360929601332721 and perplexity of 11625.190313004821
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f93af975898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'lr': 2.1086149332259407, 'data': 'ptb', 'num_layers': 1, 'anneal': 3.757685917505186, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.34514372824247175, 'seq_len': 20, 'wordvec_source': ''}, 'best_accuracy': -90.17514756039486}, {'params': {'lr': 22.230108533984744, 'data': 'ptb', 'num_layers': 1, 'anneal': 5.5056109879468655, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.91545527265737, 'seq_len': 20, 'wordvec_source': ''}, 'best_accuracy': -140.32225550721992}, {'params': {'lr': 21.899240546890415, 'data': 'ptb', 'num_layers': 1, 'anneal': 6.033026529213739, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.715177067076766, 'seq_len': 20, 'wordvec_source': ''}, 'best_accuracy': -139.70344684439354}, {'params': {'lr': 23.86549757773326, 'data': 'ptb', 'num_layers': 1, 'anneal': 3.477683734440822, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.2140254870211421, 'seq_len': 20, 'wordvec_source': ''}, 'best_accuracy': -133.21142864334215}, {'params': {'lr': 26.64396341882142, 'data': 'ptb', 'num_layers': 1, 'anneal': 6.563235363974766, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.011307254269460665, 'seq_len': 20, 'wordvec_source': ''}, 'best_accuracy': -134.6367096744872}, {'params': {'lr': 0.0, 'data': 'ptb', 'num_layers': 1, 'anneal': 7.599300554206457, 'wordvec_dim': 200, 'batch_size': 80, 'tune_wordvecs': True, 'dropout': 0.0, 'seq_len': 20, 'wordvec_source': ''}, 'best_accuracy': -11625.190313004821}]
