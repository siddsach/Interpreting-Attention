Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 26.31202854294525, 'anneal': 6.999923892627626, 'dropout': 0.9965728473943058, 'seq_len': 35, 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1538739204406738 and batch: 50, loss is 10.779582462310792 and perplexity is 48030.065688366725
At time: 1.8227615356445312 and batch: 100, loss is 9.890534954071045 and perplexity is 19742.618508646792
At time: 2.4678544998168945 and batch: 150, loss is 9.278983745574951 and perplexity is 10710.541748347388
At time: 3.1125335693359375 and batch: 200, loss is 8.895717945098877 and perplexity is 7300.644749596545
At time: 3.758741855621338 and batch: 250, loss is 8.713931064605713 and perplexity is 6087.124395883538
At time: 4.4043848514556885 and batch: 300, loss is 8.565793361663818 and perplexity is 5249.002626660717
At time: 5.049005508422852 and batch: 350, loss is 8.571861171722412 and perplexity is 5280.949403063131
At time: 5.712078094482422 and batch: 400, loss is 8.45255389213562 and perplexity is 4687.027626257347
At time: 6.375546216964722 and batch: 450, loss is 8.346238737106324 and perplexity is 4214.299806179978
At time: 7.028164863586426 and batch: 500, loss is 8.333694257736205 and perplexity is 4161.763816458966
At time: 7.669378280639648 and batch: 550, loss is 8.381249656677246 and perplexity is 4364.45959583631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.926696128033577 and perplexity of 1019.1213648390765
Finished 1 epochs...
Completing Train Step...
At time: 9.371976613998413 and batch: 50, loss is 5.913281402587891 and perplexity is 369.9180159556522
At time: 10.030943393707275 and batch: 100, loss is 5.542432079315185 and perplexity is 255.29815037549608
At time: 10.66623067855835 and batch: 150, loss is 5.5248332214355464 and perplexity is 250.8444990465674
At time: 11.345686674118042 and batch: 200, loss is 5.518330802917481 and perplexity is 249.218694689434
At time: 11.991796731948853 and batch: 250, loss is 5.558856220245361 and perplexity is 259.5258261124845
At time: 12.631268739700317 and batch: 300, loss is 5.632499370574951 and perplexity is 279.35946863647405
At time: 13.281237840652466 and batch: 350, loss is 5.676970081329346 and perplexity is 292.0631603157114
At time: 13.927141427993774 and batch: 400, loss is 5.746045265197754 and perplexity is 312.9505732707984
At time: 14.571961641311646 and batch: 450, loss is 5.648456802368164 and perplexity is 283.8530862472711
At time: 15.206751585006714 and batch: 500, loss is 5.7239885425567625 and perplexity is 306.1234777312312
At time: 15.85401701927185 and batch: 550, loss is 5.763149890899658 and perplexity is 318.3495175207649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.8794782922623 and perplexity of 357.62261854444
Finished 2 epochs...
Completing Train Step...
At time: 17.57535719871521 and batch: 50, loss is 5.808951368331909 and perplexity is 333.2694654587338
At time: 18.24154806137085 and batch: 100, loss is 5.835357837677002 and perplexity is 342.18715960302217
At time: 18.88149070739746 and batch: 150, loss is 5.823854293823242 and perplexity is 338.2733491115932
At time: 19.52364420890808 and batch: 200, loss is 5.80400821685791 and perplexity is 331.6261289860738
At time: 20.167670011520386 and batch: 250, loss is 5.805273199081421 and perplexity is 332.04589558676184
At time: 20.82531476020813 and batch: 300, loss is 5.8162970542907715 and perplexity is 335.7265718088849
At time: 21.474939346313477 and batch: 350, loss is 5.859503936767578 and perplexity is 350.550205813084
At time: 22.116841793060303 and batch: 400, loss is 5.859700384140015 and perplexity is 350.61907724450754
At time: 22.768853425979614 and batch: 450, loss is 5.839773664474487 and perplexity is 343.70153999209685
At time: 23.44652009010315 and batch: 500, loss is 5.865766944885254 and perplexity is 352.75259418677376
At time: 24.099050283432007 and batch: 550, loss is 5.920987195968628 and perplexity is 372.77953874653315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.0876594705784575 and perplexity of 440.3894594736027
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 25.82190489768982 and batch: 50, loss is 5.74825122833252 and perplexity is 313.6416927101754
At time: 26.461217641830444 and batch: 100, loss is 5.537061748504638 and perplexity is 253.93078972843833
At time: 27.095645427703857 and batch: 150, loss is 5.431419515609742 and perplexity is 228.47333681120466
At time: 27.730124473571777 and batch: 200, loss is 5.334879503250122 and perplexity is 207.44775052215604
At time: 28.373037576675415 and batch: 250, loss is 5.286469249725342 and perplexity is 197.6443591460657
At time: 29.02066993713379 and batch: 300, loss is 5.280966444015503 and perplexity is 196.55974757831706
At time: 29.671048641204834 and batch: 350, loss is 5.318488693237304 and perplexity is 204.07522854369694
At time: 30.313644886016846 and batch: 400, loss is 5.276866397857666 and perplexity is 195.75549340673734
At time: 30.95704460144043 and batch: 450, loss is 5.172170896530151 and perplexity is 176.29714521948495
At time: 31.599749326705933 and batch: 500, loss is 5.169500436782837 and perplexity is 175.82697884920336
At time: 32.24463176727295 and batch: 550, loss is 5.208354053497314 and perplexity is 182.7929430466635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.407966451441988 and perplexity of 223.17728406867724
Finished 4 epochs...
Completing Train Step...
At time: 34.008854389190674 and batch: 50, loss is 5.308801822662353 and perplexity is 202.10792212215208
At time: 34.651668548583984 and batch: 100, loss is 5.305673065185547 and perplexity is 201.47656364850025
At time: 35.28758764266968 and batch: 150, loss is 5.270621719360352 and perplexity is 194.5368721952781
At time: 35.94091534614563 and batch: 200, loss is 5.203391761779785 and perplexity is 181.88811799900597
At time: 36.578301429748535 and batch: 250, loss is 5.175132331848144 and perplexity is 176.82001164721967
At time: 37.210620164871216 and batch: 300, loss is 5.178925609588623 and perplexity is 177.49201279919518
At time: 37.86049771308899 and batch: 350, loss is 5.224092712402344 and perplexity is 185.69261746566292
At time: 38.49883699417114 and batch: 400, loss is 5.204234561920166 and perplexity is 182.04147794720433
At time: 39.1530327796936 and batch: 450, loss is 5.1331399631500245 and perplexity is 169.54865964106187
At time: 39.7897846698761 and batch: 500, loss is 5.139333515167237 and perplexity is 170.60202675874666
At time: 40.432509422302246 and batch: 550, loss is 5.187207555770874 and perplexity is 178.9680960810136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.383366686232547 and perplexity of 217.75415264095713
Finished 5 epochs...
Completing Train Step...
At time: 42.135030031204224 and batch: 50, loss is 5.252737741470337 and perplexity is 191.08870445566924
At time: 42.772905111312866 and batch: 100, loss is 5.251691303253174 and perplexity is 190.888846520187
At time: 43.41802930831909 and batch: 150, loss is 5.221415042877197 and perplexity is 185.19605910910983
At time: 44.0552191734314 and batch: 200, loss is 5.165057373046875 and perplexity is 175.04750129220116
At time: 44.695860147476196 and batch: 250, loss is 5.138880958557129 and perplexity is 170.52483715149017
At time: 45.33026957511902 and batch: 300, loss is 5.137937335968018 and perplexity is 170.3640019589442
At time: 45.97838544845581 and batch: 350, loss is 5.186053066253662 and perplexity is 178.76159851274755
At time: 46.62529730796814 and batch: 400, loss is 5.175533142089844 and perplexity is 176.8908971236888
At time: 47.2686505317688 and batch: 450, loss is 5.123031673431396 and perplexity is 167.8434455863741
At time: 47.9314980506897 and batch: 500, loss is 5.126660451889038 and perplexity is 168.45361868791272
At time: 48.57731771469116 and batch: 550, loss is 5.171894035339355 and perplexity is 176.248342138077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3683789841672205 and perplexity of 214.51485373819096
Finished 6 epochs...
Completing Train Step...
At time: 50.29835510253906 and batch: 50, loss is 5.22105489730835 and perplexity is 185.12937357802375
At time: 50.940513134002686 and batch: 100, loss is 5.222873649597168 and perplexity is 185.46638442667367
At time: 51.588982343673706 and batch: 150, loss is 5.190514249801636 and perplexity is 179.56086833413167
At time: 52.235127449035645 and batch: 200, loss is 5.139115161895752 and perplexity is 170.56477931478065
At time: 52.87076258659363 and batch: 250, loss is 5.118205099105835 and perplexity is 167.03528860495135
At time: 53.51841473579407 and batch: 300, loss is 5.116962938308716 and perplexity is 166.82793272904271
At time: 54.168771266937256 and batch: 350, loss is 5.164478187561035 and perplexity is 174.94614567480474
At time: 54.82699775695801 and batch: 400, loss is 5.158600778579712 and perplexity is 173.92093138006265
At time: 55.482001304626465 and batch: 450, loss is 5.099668970108032 and perplexity is 163.96762013148143
At time: 56.15704798698425 and batch: 500, loss is 5.108538188934326 and perplexity is 165.42835303929522
At time: 56.79848575592041 and batch: 550, loss is 5.152828283309937 and perplexity is 172.91986572948878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.353556856195977 and perplexity of 211.3587351033709
Finished 7 epochs...
Completing Train Step...
At time: 58.54511618614197 and batch: 50, loss is 5.1925144958496094 and perplexity is 179.9203937011137
At time: 59.20871067047119 and batch: 100, loss is 5.1953572750091555 and perplexity is 180.4325953398347
At time: 59.855639934539795 and batch: 150, loss is 5.162492504119873 and perplexity is 174.59910268303503
At time: 60.500847578048706 and batch: 200, loss is 5.114309711456299 and perplexity is 166.38588706122985
At time: 61.14913368225098 and batch: 250, loss is 5.093968133926392 and perplexity is 163.0355269696107
At time: 61.79704689979553 and batch: 300, loss is 5.094629459381103 and perplexity is 163.1433821733829
At time: 62.43846154212952 and batch: 350, loss is 5.140776443481445 and perplexity is 170.84837093937327
At time: 63.08106827735901 and batch: 400, loss is 5.136113376617431 and perplexity is 170.0535481581931
At time: 63.72754263877869 and batch: 450, loss is 5.078601331710815 and perplexity is 160.54934354053523
At time: 64.3767442703247 and batch: 500, loss is 5.088920564651489 and perplexity is 162.21466726888164
At time: 65.02502369880676 and batch: 550, loss is 5.1288618469238285 and perplexity is 168.8248601222859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.335466100814495 and perplexity of 207.56947456537011
Finished 8 epochs...
Completing Train Step...
At time: 66.74003791809082 and batch: 50, loss is 5.160474786758423 and perplexity is 174.2471662158176
At time: 67.40062022209167 and batch: 100, loss is 5.16233564376831 and perplexity is 174.57171715431357
At time: 68.03992819786072 and batch: 150, loss is 5.132285022735596 and perplexity is 169.40376758558494
At time: 68.68115258216858 and batch: 200, loss is 5.085260066986084 and perplexity is 161.62196631009928
At time: 69.32714891433716 and batch: 250, loss is 5.065898551940918 and perplexity is 158.52281908430038
At time: 69.97041821479797 and batch: 300, loss is 5.06470030784607 and perplexity is 158.332983809646
At time: 70.62090945243835 and batch: 350, loss is 5.1117727565765385 and perplexity is 165.96430856205345
At time: 71.27972912788391 and batch: 400, loss is 5.1080005073547365 and perplexity is 165.33942916962982
At time: 71.92434597015381 and batch: 450, loss is 5.048495283126831 and perplexity is 155.78787139384457
At time: 72.59053349494934 and batch: 500, loss is 5.062316617965698 and perplexity is 157.95601654343204
At time: 73.22925972938538 and batch: 550, loss is 5.102538032531738 and perplexity is 164.43872896627107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.317241912192487 and perplexity of 203.8209499648269
Finished 9 epochs...
Completing Train Step...
At time: 74.9307279586792 and batch: 50, loss is 5.135819711685181 and perplexity is 170.00361672640835
At time: 75.60208415985107 and batch: 100, loss is 5.137072982788086 and perplexity is 170.21681091379304
At time: 76.2465591430664 and batch: 150, loss is 5.1089231300354 and perplexity is 165.49204546979368
At time: 76.8971803188324 and batch: 200, loss is 5.062608251571655 and perplexity is 158.00208854386446
At time: 77.53665447235107 and batch: 250, loss is 5.042092781066895 and perplexity is 154.79362545351742
At time: 78.18262839317322 and batch: 300, loss is 5.040375890731812 and perplexity is 154.52808978708038
At time: 78.82847785949707 and batch: 350, loss is 5.0919426155090335 and perplexity is 162.70562972631086
At time: 79.47364950180054 and batch: 400, loss is 5.091855669021607 and perplexity is 162.69148365830597
At time: 80.12083172798157 and batch: 450, loss is 5.02867000579834 and perplexity is 152.72974787596553
At time: 80.7680447101593 and batch: 500, loss is 5.045065069198609 and perplexity is 155.25440114915688
At time: 81.41775751113892 and batch: 550, loss is 5.083554630279541 and perplexity is 161.34656518251703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.297639562728557 and perplexity of 199.86448516752782
Finished 10 epochs...
Completing Train Step...
At time: 83.14776182174683 and batch: 50, loss is 5.116305227279663 and perplexity is 166.71824423335354
At time: 83.81503033638 and batch: 100, loss is 5.118654928207397 and perplexity is 167.11044284076763
At time: 84.46097993850708 and batch: 150, loss is 5.090806875228882 and perplexity is 162.52094328659
At time: 85.11413240432739 and batch: 200, loss is 5.041081218719483 and perplexity is 154.63712122062915
At time: 85.77458071708679 and batch: 250, loss is 5.025878095626831 and perplexity is 152.30393483185583
At time: 86.43304109573364 and batch: 300, loss is 5.0236310958862305 and perplexity is 151.96209213385356
At time: 87.07822060585022 and batch: 350, loss is 5.072703561782837 and perplexity is 159.605247218289
At time: 87.73568677902222 and batch: 400, loss is 5.0782396030426025 and perplexity is 160.49127874279802
At time: 88.37722873687744 and batch: 450, loss is 5.011281690597534 and perplexity is 150.0969908190777
At time: 89.04793357849121 and batch: 500, loss is 5.027732467651367 and perplexity is 152.58662501320953
At time: 89.68655347824097 and batch: 550, loss is 5.068591861724854 and perplexity is 158.95034561617604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2896536969124 and perplexity of 198.27475036063382
Finished 11 epochs...
Completing Train Step...
At time: 91.48472094535828 and batch: 50, loss is 5.099298143386841 and perplexity is 163.9068278289283
At time: 92.14019823074341 and batch: 100, loss is 5.101597909927368 and perplexity is 164.28420904535201
At time: 92.79386496543884 and batch: 150, loss is 5.073584699630738 and perplexity is 159.74594341960756
At time: 93.44875073432922 and batch: 200, loss is 5.026912479400635 and perplexity is 152.46155705761473
At time: 94.08784294128418 and batch: 250, loss is 5.010872192382813 and perplexity is 150.03553895237582
At time: 94.73256278038025 and batch: 300, loss is 5.011297264099121 and perplexity is 150.0993283730043
At time: 95.38303184509277 and batch: 350, loss is 5.059274415969849 and perplexity is 157.47621263489975
At time: 96.03168272972107 and batch: 400, loss is 5.063594455718994 and perplexity is 158.15798772044633
At time: 96.67713975906372 and batch: 450, loss is 5.00007399559021 and perplexity is 148.42414142819612
At time: 97.32118082046509 and batch: 500, loss is 5.0152583503723145 and perplexity is 150.6950638631403
At time: 97.96509909629822 and batch: 550, loss is 5.055691356658936 and perplexity is 156.9129756832571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.283976778070977 and perplexity of 197.15234959788927
Finished 12 epochs...
Completing Train Step...
At time: 99.70501160621643 and batch: 50, loss is 5.086709928512573 and perplexity is 161.856465735657
At time: 100.36731266975403 and batch: 100, loss is 5.087953786849976 and perplexity is 162.05791751278753
At time: 101.01889610290527 and batch: 150, loss is 5.060224752426148 and perplexity is 157.62593915478305
At time: 101.65891003608704 and batch: 200, loss is 5.011218709945679 and perplexity is 150.08753791043264
At time: 102.29467988014221 and batch: 250, loss is 4.9953092765808105 and perplexity is 147.71862423043385
At time: 102.94109892845154 and batch: 300, loss is 4.9937951946258545 and perplexity is 147.49513536000322
At time: 103.58445334434509 and batch: 350, loss is 5.041539173126221 and perplexity is 154.7079541896347
At time: 104.2255802154541 and batch: 400, loss is 5.044684724807739 and perplexity is 155.19536223679333
At time: 104.87495875358582 and batch: 450, loss is 4.978513736724853 and perplexity is 145.2583290180037
At time: 105.54342031478882 and batch: 500, loss is 4.985323524475097 and perplexity is 146.25088311327426
At time: 106.19734120368958 and batch: 550, loss is 5.023043966293335 and perplexity is 151.8728968797123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.25098110767121 and perplexity of 190.7533262333928
Finished 13 epochs...
Completing Train Step...
At time: 107.8986189365387 and batch: 50, loss is 5.046888589859009 and perplexity is 155.53776904235238
At time: 108.55109310150146 and batch: 100, loss is 5.051763143539429 and perplexity is 156.2977971408287
At time: 109.19344711303711 and batch: 150, loss is 5.022698669433594 and perplexity is 151.8204646981948
At time: 109.85138654708862 and batch: 200, loss is 4.9760614776611325 and perplexity is 144.902554368183
At time: 110.49209094047546 and batch: 250, loss is 4.961326513290405 and perplexity is 142.78307397456348
At time: 111.1362235546112 and batch: 300, loss is 4.958982152938843 and perplexity is 142.44873106032898
At time: 111.79829549789429 and batch: 350, loss is 5.0115205192565915 and perplexity is 150.13284256316513
At time: 112.44946575164795 and batch: 400, loss is 5.0160050868988035 and perplexity is 150.80763539709565
At time: 113.10286903381348 and batch: 450, loss is 4.9540415859222415 and perplexity is 141.74668922847786
At time: 113.75217938423157 and batch: 500, loss is 4.966897420883178 and perplexity is 143.58072504275918
At time: 114.41387462615967 and batch: 550, loss is 5.007312288284302 and perplexity is 149.50237638919418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.251226871571642 and perplexity of 190.80021227608074
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 116.14239764213562 and batch: 50, loss is 5.019667377471924 and perplexity is 151.3609493583796
At time: 116.79947137832642 and batch: 100, loss is 4.9964045143127445 and perplexity is 147.8804998713739
At time: 117.45021033287048 and batch: 150, loss is 4.955470724105835 and perplexity is 141.94940965770158
At time: 118.10319709777832 and batch: 200, loss is 4.897836971282959 and perplexity is 133.99962095903044
At time: 118.75082612037659 and batch: 250, loss is 4.87128098487854 and perplexity is 130.48796298927766
At time: 119.40291857719421 and batch: 300, loss is 4.854510955810547 and perplexity is 128.31792273317512
At time: 120.04633808135986 and batch: 350, loss is 4.905832796096802 and perplexity is 135.0753534061906
At time: 120.69882535934448 and batch: 400, loss is 4.8967764282226565 and perplexity is 133.85758392244895
At time: 121.35662317276001 and batch: 450, loss is 4.812119235992432 and perplexity is 122.99199057704892
At time: 122.01942038536072 and batch: 500, loss is 4.826484222412109 and perplexity is 124.77151970650485
At time: 122.6596941947937 and batch: 550, loss is 4.877666158676147 and perplexity is 131.32381700820704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.151531787628823 and perplexity of 172.69582113819726
Finished 15 epochs...
Completing Train Step...
At time: 124.39620018005371 and batch: 50, loss is 4.951082096099854 and perplexity is 141.3278114822935
At time: 125.05443978309631 and batch: 100, loss is 4.9530063724517825 and perplexity is 141.60002707281288
At time: 125.69502592086792 and batch: 150, loss is 4.920111818313599 and perplexity is 137.01793344416083
At time: 126.33240532875061 and batch: 200, loss is 4.868177127838135 and perplexity is 130.0835749127083
At time: 126.97704267501831 and batch: 250, loss is 4.850527114868164 and perplexity is 127.80774145423938
At time: 127.62469434738159 and batch: 300, loss is 4.8348906707763675 and perplexity is 125.82482612430711
At time: 128.27574825286865 and batch: 350, loss is 4.889774141311645 and perplexity is 132.92354872230382
At time: 128.93379187583923 and batch: 400, loss is 4.886114549636841 and perplexity is 132.43799182191552
At time: 129.58168649673462 and batch: 450, loss is 4.810387907028198 and perplexity is 122.77923520930383
At time: 130.234778881073 and batch: 500, loss is 4.829355506896973 and perplexity is 125.13028905313763
At time: 130.88950991630554 and batch: 550, loss is 4.879893093109131 and perplexity is 131.6165924129933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.144944535925033 and perplexity of 171.56196888126885
Finished 16 epochs...
Completing Train Step...
At time: 132.63541769981384 and batch: 50, loss is 4.937618045806885 and perplexity is 139.4377194209934
At time: 133.30181908607483 and batch: 100, loss is 4.94067265510559 and perplexity is 139.8642983596356
At time: 133.95217609405518 and batch: 150, loss is 4.909487590789795 and perplexity is 135.5699293271737
At time: 134.60616827011108 and batch: 200, loss is 4.857956895828247 and perplexity is 128.76086132972094
At time: 135.25794529914856 and batch: 250, loss is 4.842583227157593 and perplexity is 126.79647312663637
At time: 135.91275072097778 and batch: 300, loss is 4.827707147598266 and perplexity is 124.92419927928826
At time: 136.56512570381165 and batch: 350, loss is 4.884882593154908 and perplexity is 132.27493443985855
At time: 137.20841884613037 and batch: 400, loss is 4.884302501678467 and perplexity is 132.19822512920294
At time: 137.85373282432556 and batch: 450, loss is 4.810538053512573 and perplexity is 122.79767146385959
At time: 138.51119828224182 and batch: 500, loss is 4.830128211975097 and perplexity is 125.22701522850689
At time: 139.16539764404297 and batch: 550, loss is 4.878902139663697 and perplexity is 131.48623109892972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.141490530460439 and perplexity of 170.9704151063326
Finished 17 epochs...
Completing Train Step...
At time: 140.88392639160156 and batch: 50, loss is 4.929808664321899 and perplexity is 138.35303793496172
At time: 141.54416847229004 and batch: 100, loss is 4.9331601619720455 and perplexity is 138.81750571306225
At time: 142.19394707679749 and batch: 150, loss is 4.902580604553223 and perplexity is 134.63677603945888
At time: 142.83267831802368 and batch: 200, loss is 4.851659660339355 and perplexity is 127.95257153084957
At time: 143.48196482658386 and batch: 250, loss is 4.837136726379395 and perplexity is 126.1077532961616
At time: 144.13382267951965 and batch: 300, loss is 4.823144235610962 and perplexity is 124.35547964843899
At time: 144.78323340415955 and batch: 350, loss is 4.8816830348968505 and perplexity is 131.8523894202132
At time: 145.4222400188446 and batch: 400, loss is 4.8825903987884525 and perplexity is 131.9720818114595
At time: 146.07911276817322 and batch: 450, loss is 4.809334964752197 and perplexity is 122.65002379995255
At time: 146.72138118743896 and batch: 500, loss is 4.829374914169311 and perplexity is 125.13271751429991
At time: 147.36984610557556 and batch: 550, loss is 4.876361446380615 and perplexity is 131.15258893543594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.138647850523603 and perplexity of 170.48509107477935
Finished 18 epochs...
Completing Train Step...
At time: 149.1470947265625 and batch: 50, loss is 4.923278427124023 and perplexity is 137.45250333267595
At time: 149.80928897857666 and batch: 100, loss is 4.927137966156006 and perplexity is 137.98403170177016
At time: 150.44929766654968 and batch: 150, loss is 4.897137470245362 and perplexity is 133.90592086061065
At time: 151.0968747138977 and batch: 200, loss is 4.846101970672607 and perplexity is 127.24342328450076
At time: 151.74221205711365 and batch: 250, loss is 4.832065658569336 and perplexity is 125.46987106680913
At time: 152.38608598709106 and batch: 300, loss is 4.819039678573608 and perplexity is 123.84610159010211
At time: 153.03362703323364 and batch: 350, loss is 4.878912115097046 and perplexity is 131.48754273760653
At time: 153.67962837219238 and batch: 400, loss is 4.880983819961548 and perplexity is 131.76022848418154
At time: 154.32424640655518 and batch: 450, loss is 4.807359533309937 and perplexity is 122.40797623943736
At time: 154.98851704597473 and batch: 500, loss is 4.827677888870239 and perplexity is 124.92054420958915
At time: 155.63028597831726 and batch: 550, loss is 4.8734024810791015 and perplexity is 130.76508656134246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.136206119618517 and perplexity of 170.069320165955
Finished 19 epochs...
Completing Train Step...
At time: 157.35223484039307 and batch: 50, loss is 4.917701272964478 and perplexity is 136.68804326930606
At time: 158.01863884925842 and batch: 100, loss is 4.922494735717773 and perplexity is 137.34482518577684
At time: 158.66444540023804 and batch: 150, loss is 4.892809429168701 and perplexity is 133.32762288613188
At time: 159.31061601638794 and batch: 200, loss is 4.841866092681885 and perplexity is 126.70557560112933
At time: 159.9624924659729 and batch: 250, loss is 4.828417587280273 and perplexity is 125.01298192132097
At time: 160.61275458335876 and batch: 300, loss is 4.816168909072876 and perplexity is 123.49107781840252
At time: 161.27772283554077 and batch: 350, loss is 4.876869735717773 and perplexity is 131.2192693429669
At time: 161.92950820922852 and batch: 400, loss is 4.879134569168091 and perplexity is 131.51679593040544
At time: 162.5802822113037 and batch: 450, loss is 4.805777053833008 and perplexity is 122.21442131796391
At time: 163.23436331748962 and batch: 500, loss is 4.8259154796600345 and perplexity is 124.70057698499811
At time: 163.88983130455017 and batch: 550, loss is 4.871183948516846 and perplexity is 130.47530152642537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.134880552900598 and perplexity of 169.8440312861128
Finished 20 epochs...
Completing Train Step...
At time: 165.62906503677368 and batch: 50, loss is 4.913467178344726 and perplexity is 136.11051667464193
At time: 166.30250000953674 and batch: 100, loss is 4.9189128494262695 and perplexity is 136.85375164905523
At time: 166.94387650489807 and batch: 150, loss is 4.88929256439209 and perplexity is 132.85955122027053
At time: 167.58684587478638 and batch: 200, loss is 4.838533868789673 and perplexity is 126.28406692550064
At time: 168.24438500404358 and batch: 250, loss is 4.825130844116211 and perplexity is 124.60277085606087
At time: 168.89714741706848 and batch: 300, loss is 4.813530902862549 and perplexity is 123.16573690254857
At time: 169.5440149307251 and batch: 350, loss is 4.874593276977539 and perplexity is 130.9208938390009
At time: 170.19235825538635 and batch: 400, loss is 4.877062606811523 and perplexity is 131.2445801877529
At time: 170.84286403656006 and batch: 450, loss is 4.8029940319061275 and perplexity is 121.87476875314606
At time: 171.49913930892944 and batch: 500, loss is 4.822909469604492 and perplexity is 124.32628863576211
At time: 172.1414852142334 and batch: 550, loss is 4.867787599563599 and perplexity is 130.03291354988423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1316823756441154 and perplexity of 169.30170765391298
Finished 21 epochs...
Completing Train Step...
At time: 173.83896589279175 and batch: 50, loss is 4.908276634216309 and perplexity is 135.4058593909213
At time: 174.49850964546204 and batch: 100, loss is 4.914776849746704 and perplexity is 136.28889350786113
At time: 175.15230059623718 and batch: 150, loss is 4.885136022567749 and perplexity is 132.30846104696198
At time: 175.79661583900452 and batch: 200, loss is 4.834683132171631 and perplexity is 125.79871532504522
At time: 176.44139885902405 and batch: 250, loss is 4.820896892547608 and perplexity is 124.0763240205837
At time: 177.088134765625 and batch: 300, loss is 4.810453205108643 and perplexity is 122.78725271944266
At time: 177.73316979408264 and batch: 350, loss is 4.872084712982177 and perplexity is 130.59288198984527
At time: 178.38889718055725 and batch: 400, loss is 4.874231443405152 and perplexity is 130.87353083356254
At time: 179.03700399398804 and batch: 450, loss is 4.799774599075318 and perplexity is 121.48303204473913
At time: 179.69300436973572 and batch: 500, loss is 4.819649019241333 and perplexity is 123.92158905279494
At time: 180.35133266448975 and batch: 550, loss is 4.864536628723145 and perplexity is 129.61086674227775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.130326616003158 and perplexity of 169.0723307566351
Finished 22 epochs...
Completing Train Step...
At time: 182.0782744884491 and batch: 50, loss is 4.904330492019653 and perplexity is 134.87258150271148
At time: 182.73307061195374 and batch: 100, loss is 4.911264953613281 and perplexity is 135.81110054042796
At time: 183.37054300308228 and batch: 150, loss is 4.881426181793213 and perplexity is 131.8185270737791
At time: 184.0161292552948 and batch: 200, loss is 4.831214523315429 and perplexity is 125.36312467049582
At time: 184.66587138175964 and batch: 250, loss is 4.817093677520752 and perplexity is 123.60533129162434
At time: 185.31141996383667 and batch: 300, loss is 4.807348594665528 and perplexity is 122.40663726943573
At time: 185.9476673603058 and batch: 350, loss is 4.869294509887696 and perplexity is 130.2290092019588
At time: 186.59427523612976 and batch: 400, loss is 4.871598443984985 and perplexity is 130.52939415741295
At time: 187.23787999153137 and batch: 450, loss is 4.7969190216064455 and perplexity is 121.13662267037857
At time: 187.88668990135193 and batch: 500, loss is 4.817034893035888 and perplexity is 123.59806542946001
At time: 188.5449149608612 and batch: 550, loss is 4.861831102371216 and perplexity is 129.26067506682645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.12902280117603 and perplexity of 168.8520353883286
Finished 23 epochs...
Completing Train Step...
At time: 190.22337651252747 and batch: 50, loss is 4.900966272354126 and perplexity is 134.4196028986768
At time: 190.88884329795837 and batch: 100, loss is 4.9078298187255855 and perplexity is 135.34537146988717
At time: 191.53443264961243 and batch: 150, loss is 4.8784495449066165 and perplexity is 131.42673458502688
At time: 192.17999029159546 and batch: 200, loss is 4.828414993286133 and perplexity is 125.01265763879898
At time: 192.82497382164001 and batch: 250, loss is 4.8137743949890135 and perplexity is 123.19573044118157
At time: 193.4643590450287 and batch: 300, loss is 4.804525718688965 and perplexity is 122.06158576165429
At time: 194.10585594177246 and batch: 350, loss is 4.867049570083618 and perplexity is 129.93698083125835
At time: 194.74904012680054 and batch: 400, loss is 4.869203996658325 and perplexity is 130.21722228722214
At time: 195.39338493347168 and batch: 450, loss is 4.7943729972839355 and perplexity is 120.82859816804122
At time: 196.03424787521362 and batch: 500, loss is 4.814532508850098 and perplexity is 123.28916224355278
At time: 196.68307733535767 and batch: 550, loss is 4.859387788772583 and perplexity is 128.9452362166084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.126641780772108 and perplexity of 168.45047350006297
Finished 24 epochs...
Completing Train Step...
At time: 198.42286944389343 and batch: 50, loss is 4.89779767036438 and perplexity is 133.99435475432153
At time: 199.0793867111206 and batch: 100, loss is 4.905402584075928 and perplexity is 135.0172548636732
At time: 199.72136521339417 and batch: 150, loss is 4.876133880615234 and perplexity is 131.12274649184118
At time: 200.37278985977173 and batch: 200, loss is 4.82565221786499 and perplexity is 124.66775240817601
At time: 201.0253505706787 and batch: 250, loss is 4.810940933227539 and perplexity is 122.8471541218477
At time: 201.66783595085144 and batch: 300, loss is 4.80194356918335 and perplexity is 121.74681107087787
At time: 202.31782793998718 and batch: 350, loss is 4.864744663238525 and perplexity is 129.63783308099588
At time: 202.95673894882202 and batch: 400, loss is 4.866896667480469 and perplexity is 129.91711464747817
At time: 203.59655499458313 and batch: 450, loss is 4.792002515792847 and perplexity is 120.54251542337785
At time: 204.24583888053894 and batch: 500, loss is 4.812590589523316 and perplexity is 123.04997695104531
At time: 204.8908383846283 and batch: 550, loss is 4.856701898574829 and perplexity is 128.59936816034264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1249659112159245 and perplexity of 168.16840889755545
Finished 25 epochs...
Completing Train Step...
At time: 206.61786651611328 and batch: 50, loss is 4.895341501235962 and perplexity is 133.66564580443125
At time: 207.2861306667328 and batch: 100, loss is 4.902950506210328 and perplexity is 134.68658761814788
At time: 207.9300196170807 and batch: 150, loss is 4.873602838516235 and perplexity is 130.7912889437859
At time: 208.57721304893494 and batch: 200, loss is 4.823018941879273 and perplexity is 124.3398996623955
At time: 209.22209286689758 and batch: 250, loss is 4.8083450222015385 and perplexity is 122.52866740039619
At time: 209.87158751487732 and batch: 300, loss is 4.799473066329956 and perplexity is 121.44640645475664
At time: 210.53113102912903 and batch: 350, loss is 4.862408313751221 and perplexity is 129.33530733668775
At time: 211.17796158790588 and batch: 400, loss is 4.864644393920899 and perplexity is 129.62483503559773
At time: 211.8217751979828 and batch: 450, loss is 4.789866018295288 and perplexity is 120.28525156048534
At time: 212.46575665473938 and batch: 500, loss is 4.810151405334473 and perplexity is 122.75020114566053
At time: 213.116361618042 and batch: 550, loss is 4.854566402435303 and perplexity is 128.32503772613578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.123819554105718 and perplexity of 167.97573830205675
Finished 26 epochs...
Completing Train Step...
At time: 214.86570477485657 and batch: 50, loss is 4.892827787399292 and perplexity is 133.33007056784442
At time: 215.54337549209595 and batch: 100, loss is 4.901033935546875 and perplexity is 134.42869846589096
At time: 216.19004249572754 and batch: 150, loss is 4.871166934967041 and perplexity is 130.4730816972682
At time: 216.8474531173706 and batch: 200, loss is 4.820667142868042 and perplexity is 124.04782079932664
At time: 217.50042271614075 and batch: 250, loss is 4.805889701843261 and perplexity is 122.22818930480322
At time: 218.15090441703796 and batch: 300, loss is 4.797226982116699 and perplexity is 121.1739337113753
At time: 218.79807329177856 and batch: 350, loss is 4.860226240158081 and perplexity is 129.05339586553495
At time: 219.44686007499695 and batch: 400, loss is 4.862712860107422 and perplexity is 129.37470193170097
At time: 220.08856010437012 and batch: 450, loss is 4.787899694442749 and perplexity is 120.04896418611581
At time: 220.75197649002075 and batch: 500, loss is 4.808529472351074 and perplexity is 122.55126991587517
At time: 221.3999445438385 and batch: 550, loss is 4.852512712478638 and perplexity is 128.0617683143737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.123124142910572 and perplexity of 167.85896669997416
Finished 27 epochs...
Completing Train Step...
At time: 223.1616644859314 and batch: 50, loss is 4.890512924194336 and perplexity is 133.02178664858744
At time: 223.8145866394043 and batch: 100, loss is 4.898763437271118 and perplexity is 134.12382457660476
At time: 224.4637577533722 and batch: 150, loss is 4.868916082382202 and perplexity is 130.17973628656483
At time: 225.11192798614502 and batch: 200, loss is 4.818259963989258 and perplexity is 123.74957461511279
At time: 225.75560331344604 and batch: 250, loss is 4.803343744277954 and perplexity is 121.91739732102684
At time: 226.39440298080444 and batch: 300, loss is 4.794957399368286 and perplexity is 120.89923128972131
At time: 227.04461312294006 and batch: 350, loss is 4.858114271163941 and perplexity is 128.78112670809045
At time: 227.6902952194214 and batch: 400, loss is 4.861064653396607 and perplexity is 129.1616413119722
At time: 228.3366575241089 and batch: 450, loss is 4.78610330581665 and perplexity is 119.83350317607209
At time: 228.98546075820923 and batch: 500, loss is 4.806613616943359 and perplexity is 122.3167041714573
At time: 229.63189315795898 and batch: 550, loss is 4.8505395317077635 and perplexity is 127.80932843231716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.12195733252992 and perplexity of 167.6632213362124
Finished 28 epochs...
Completing Train Step...
At time: 231.3743977546692 and batch: 50, loss is 4.888153762817383 and perplexity is 132.7083366721509
At time: 232.0372748374939 and batch: 100, loss is 4.896381425857544 and perplexity is 133.80472030152632
At time: 232.6841435432434 and batch: 150, loss is 4.866450300216675 and perplexity is 129.85913684115303
At time: 233.33218479156494 and batch: 200, loss is 4.815561971664429 and perplexity is 123.41614920445458
At time: 233.98382925987244 and batch: 250, loss is 4.800680904388428 and perplexity is 121.59318266964263
At time: 234.6363410949707 and batch: 300, loss is 4.79261734008789 and perplexity is 120.61665067818836
At time: 235.27861762046814 and batch: 350, loss is 4.855995836257935 and perplexity is 128.50860103990073
At time: 235.91919493675232 and batch: 400, loss is 4.859023971557617 and perplexity is 128.8983322526371
At time: 236.562997341156 and batch: 450, loss is 4.783997888565064 and perplexity is 119.58146906275324
At time: 237.22338032722473 and batch: 500, loss is 4.804418964385986 and perplexity is 122.04855585765733
At time: 237.87408876419067 and batch: 550, loss is 4.848213624954224 and perplexity is 127.5124012984248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.120519435152095 and perplexity of 167.422312072802
Finished 29 epochs...
Completing Train Step...
At time: 239.6370129585266 and batch: 50, loss is 4.885655546188355 and perplexity is 132.37721627611828
At time: 240.2989866733551 and batch: 100, loss is 4.8938623142242434 and perplexity is 133.4680754750005
At time: 240.9469292163849 and batch: 150, loss is 4.863664073944092 and perplexity is 129.49782348647327
At time: 241.60179090499878 and batch: 200, loss is 4.812989645004272 and perplexity is 123.09909051764642
At time: 242.24858355522156 and batch: 250, loss is 4.798149528503418 and perplexity is 121.28577386701882
At time: 242.89384055137634 and batch: 300, loss is 4.790303497314453 and perplexity is 120.3378853466072
At time: 243.53448796272278 and batch: 350, loss is 4.853863153457642 and perplexity is 128.23482499926845
At time: 244.1703908443451 and batch: 400, loss is 4.856994123458862 and perplexity is 128.63695358722106
At time: 244.8078863620758 and batch: 450, loss is 4.782032489776611 and perplexity is 119.34667459629611
At time: 245.4584150314331 and batch: 500, loss is 4.802310247421264 and perplexity is 121.79146116264123
At time: 246.1082308292389 and batch: 550, loss is 4.846017942428589 and perplexity is 127.23273169228348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.119309445644947 and perplexity of 167.21985534189253
Finished 30 epochs...
Completing Train Step...
At time: 247.85394310951233 and batch: 50, loss is 4.883402471542358 and perplexity is 132.0792962704533
At time: 248.5213062763214 and batch: 100, loss is 4.891587247848511 and perplexity is 133.16477189296722
At time: 249.16040635108948 and batch: 150, loss is 4.86118405342102 and perplexity is 129.17706413582258
At time: 249.79937720298767 and batch: 200, loss is 4.810636024475098 and perplexity is 122.80970265927297
At time: 250.4387617111206 and batch: 250, loss is 4.795530967712402 and perplexity is 120.9685951521733
At time: 251.0892927646637 and batch: 300, loss is 4.787587385177613 and perplexity is 120.01147763633358
At time: 251.73414826393127 and batch: 350, loss is 4.851595983505249 and perplexity is 127.94442417558041
At time: 252.3762228488922 and batch: 400, loss is 4.854747495651245 and perplexity is 128.34827862422446
At time: 253.02073097229004 and batch: 450, loss is 4.779696102142334 and perplexity is 119.06815998731554
At time: 253.68613719940186 and batch: 500, loss is 4.800028257369995 and perplexity is 121.51385113207314
At time: 254.32841300964355 and batch: 550, loss is 4.842914094924927 and perplexity is 126.83843293379671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.11753910145861 and perplexity of 166.92408053200492
Finished 31 epochs...
Completing Train Step...
At time: 256.05046820640564 and batch: 50, loss is 4.880389223098755 and perplexity is 131.68190755268148
At time: 256.7181987762451 and batch: 100, loss is 4.888515901565552 and perplexity is 132.75640420609795
At time: 257.3660554885864 and batch: 150, loss is 4.858504772186279 and perplexity is 128.83142568999065
At time: 258.00922298431396 and batch: 200, loss is 4.807266206741333 and perplexity is 122.39655285610607
At time: 258.64894795417786 and batch: 250, loss is 4.792274112701416 and perplexity is 120.57525884422267
At time: 259.28977608680725 and batch: 300, loss is 4.784938631057739 and perplexity is 119.69401736322106
At time: 259.93073177337646 and batch: 350, loss is 4.84941460609436 and perplexity is 127.6656332834618
At time: 260.58471488952637 and batch: 400, loss is 4.852350263595581 and perplexity is 128.04096651280923
At time: 261.23310136795044 and batch: 450, loss is 4.777140426635742 and perplexity is 118.76424892166996
At time: 261.878737449646 and batch: 500, loss is 4.797184286117553 and perplexity is 121.16876017965039
At time: 262.5208959579468 and batch: 550, loss is 4.8401586627960205 and perplexity is 126.4894193030177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.116498257251496 and perplexity of 166.75042895755016
Finished 32 epochs...
Completing Train Step...
At time: 264.2392852306366 and batch: 50, loss is 4.877599420547486 and perplexity is 131.3150529948614
At time: 264.9025135040283 and batch: 100, loss is 4.88632230758667 and perplexity is 132.46550972600855
At time: 265.5579710006714 and batch: 150, loss is 4.856275663375855 and perplexity is 128.54456626314533
At time: 266.1982305049896 and batch: 200, loss is 4.804771900177002 and perplexity is 122.09163876356381
At time: 266.84908986091614 and batch: 250, loss is 4.789559593200684 and perplexity is 120.24839878749202
At time: 267.49587988853455 and batch: 300, loss is 4.782155723571777 and perplexity is 119.36138304621745
At time: 268.13353395462036 and batch: 350, loss is 4.84701358795166 and perplexity is 127.35947347645597
At time: 268.77104926109314 and batch: 400, loss is 4.850251970291137 and perplexity is 127.77258068464553
At time: 269.415265083313 and batch: 450, loss is 4.7751253223419186 and perplexity is 118.52516754157503
At time: 270.0893747806549 and batch: 500, loss is 4.79490219116211 and perplexity is 120.89255684427746
At time: 270.7361350059509 and batch: 550, loss is 4.837559194564819 and perplexity is 126.16104106526554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.114924004737367 and perplexity of 166.48812819352136
Finished 33 epochs...
Completing Train Step...
At time: 272.46460223197937 and batch: 50, loss is 4.875109977722168 and perplexity is 130.98855824195337
At time: 273.11988496780396 and batch: 100, loss is 4.884423303604126 and perplexity is 132.2141958939974
At time: 273.76632499694824 and batch: 150, loss is 4.853741683959961 and perplexity is 128.2192493254944
At time: 274.4144730567932 and batch: 200, loss is 4.8021554756164555 and perplexity is 121.77261273702685
At time: 275.06365275382996 and batch: 250, loss is 4.786712369918823 and perplexity is 119.90651169227054
At time: 275.7114222049713 and batch: 300, loss is 4.779723434448242 and perplexity is 119.07141443916389
At time: 276.368882894516 and batch: 350, loss is 4.84497299194336 and perplexity is 127.09984922745404
At time: 277.00916504859924 and batch: 400, loss is 4.848117399215698 and perplexity is 127.50013191376344
At time: 277.6580903530121 and batch: 450, loss is 4.773595943450927 and perplexity is 118.34403619681886
At time: 278.31722617149353 and batch: 500, loss is 4.79279863357544 and perplexity is 120.63851967373967
At time: 278.98094272613525 and batch: 550, loss is 4.834977054595948 and perplexity is 125.83569582286202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.114023411527593 and perplexity of 166.33825761210582
Finished 34 epochs...
Completing Train Step...
At time: 280.7209610939026 and batch: 50, loss is 4.8728796482086185 and perplexity is 130.69673614525576
At time: 281.3953080177307 and batch: 100, loss is 4.88272271156311 and perplexity is 131.98954455902813
At time: 282.04296016693115 and batch: 150, loss is 4.851014585494995 and perplexity is 127.87005916187184
At time: 282.68634581565857 and batch: 200, loss is 4.799890069961548 and perplexity is 121.49706060804104
At time: 283.33015036582947 and batch: 250, loss is 4.784366903305053 and perplexity is 119.62560453028523
At time: 283.98468112945557 and batch: 300, loss is 4.777713127136231 and perplexity is 118.83228474668353
At time: 284.63540291786194 and batch: 350, loss is 4.84305006980896 and perplexity is 126.85568094762762
At time: 285.2875199317932 and batch: 400, loss is 4.846148748397827 and perplexity is 127.24937558160754
At time: 285.9451231956482 and batch: 450, loss is 4.7716114044189455 and perplexity is 118.10941072648986
At time: 286.6074159145355 and batch: 500, loss is 4.790982446670532 and perplexity is 120.41961641886495
At time: 287.2552435398102 and batch: 550, loss is 4.832916307449341 and perplexity is 125.57664728020097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1121839158078455 and perplexity of 166.032560349801
Finished 35 epochs...
Completing Train Step...
At time: 288.95784425735474 and batch: 50, loss is 4.8706807518005375 and perplexity is 130.4096632989932
At time: 289.6142301559448 and batch: 100, loss is 4.880671138763428 and perplexity is 131.71903597847114
At time: 290.2565989494324 and batch: 150, loss is 4.848651809692383 and perplexity is 127.56828752995243
At time: 290.90071415901184 and batch: 200, loss is 4.79775113105774 and perplexity is 121.23746354850577
At time: 291.55047726631165 and batch: 250, loss is 4.781946334838867 and perplexity is 119.33639273389927
At time: 292.1940498352051 and batch: 300, loss is 4.775566930770874 and perplexity is 118.57752081357705
At time: 292.8401584625244 and batch: 350, loss is 4.840762434005737 and perplexity is 126.56581303257315
At time: 293.47846484184265 and batch: 400, loss is 4.84375337600708 and perplexity is 126.94493071558342
At time: 294.1140332221985 and batch: 450, loss is 4.7692266464233395 and perplexity is 117.82808394637476
At time: 294.75263261795044 and batch: 500, loss is 4.788075647354126 and perplexity is 120.07008900930471
At time: 295.3939607143402 and batch: 550, loss is 4.830204448699951 and perplexity is 125.23656248993248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.110545381586602 and perplexity of 165.76073307778768
Finished 36 epochs...
Completing Train Step...
At time: 297.10739159584045 and batch: 50, loss is 4.868725109100342 and perplexity is 130.15487780882256
At time: 297.7727460861206 and batch: 100, loss is 4.878799667358399 and perplexity is 131.47275809203092
At time: 298.42447304725647 and batch: 150, loss is 4.8465962600708 and perplexity is 127.30633390636594
At time: 299.06571221351624 and batch: 200, loss is 4.7949137878417964 and perplexity is 120.89395880466473
At time: 299.7356894016266 and batch: 250, loss is 4.7787685966491695 and perplexity is 118.9577748142069
At time: 300.39072704315186 and batch: 300, loss is 4.772461051940918 and perplexity is 118.20980473835742
At time: 301.0360264778137 and batch: 350, loss is 4.837390689849854 and perplexity is 126.13978412599866
At time: 301.68892335891724 and batch: 400, loss is 4.839803390502929 and perplexity is 126.4444890986711
At time: 302.3331561088562 and batch: 450, loss is 4.76514307975769 and perplexity is 117.34790619668763
At time: 303.0022897720337 and batch: 500, loss is 4.784458017349243 and perplexity is 119.63650459946975
At time: 303.651083946228 and batch: 550, loss is 4.825952482223511 and perplexity is 124.70519131138396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.107596214781416 and perplexity of 165.27259717708887
Finished 37 epochs...
Completing Train Step...
At time: 305.381418466568 and batch: 50, loss is 4.86466835975647 and perplexity is 129.62794164030618
At time: 306.03814697265625 and batch: 100, loss is 4.875169897079468 and perplexity is 130.9964072273276
At time: 306.6811294555664 and batch: 150, loss is 4.843206701278686 and perplexity is 126.87555209556045
At time: 307.32517433166504 and batch: 200, loss is 4.791289472579956 and perplexity is 120.45659403736147
At time: 307.97522497177124 and batch: 250, loss is 4.77567120552063 and perplexity is 118.58988609956889
At time: 308.6257758140564 and batch: 300, loss is 4.7694950199127195 and perplexity is 117.85971012403442
At time: 309.27025413513184 and batch: 350, loss is 4.83498836517334 and perplexity is 125.83711910528736
At time: 309.9169921875 and batch: 400, loss is 4.837044382095337 and perplexity is 126.09610850364264
At time: 310.563529253006 and batch: 450, loss is 4.762173643112183 and perplexity is 116.9999658727929
At time: 311.2031843662262 and batch: 500, loss is 4.781322164535522 and perplexity is 119.26192974265787
At time: 311.85426115989685 and batch: 550, loss is 4.823267021179199 and perplexity is 124.3707496441194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.10580768991024 and perplexity of 164.9772672077371
Finished 38 epochs...
Completing Train Step...
At time: 313.6044170856476 and batch: 50, loss is 4.8614974784851075 and perplexity is 129.2175578109628
At time: 314.2773804664612 and batch: 100, loss is 4.872772750854492 and perplexity is 130.6827657566809
At time: 314.9241614341736 and batch: 150, loss is 4.841027040481567 and perplexity is 126.59930759756423
At time: 315.56856203079224 and batch: 200, loss is 4.789030847549438 and perplexity is 120.18483477564219
At time: 316.21229815483093 and batch: 250, loss is 4.773503408432007 and perplexity is 118.33308573584866
At time: 316.8573668003082 and batch: 300, loss is 4.767100505828857 and perplexity is 117.57783100466725
At time: 317.50027084350586 and batch: 350, loss is 4.832949991226196 and perplexity is 125.58087724720649
At time: 318.1449062824249 and batch: 400, loss is 4.83460485458374 and perplexity is 125.78886849044808
At time: 318.80272364616394 and batch: 450, loss is 4.759953908920288 and perplexity is 116.74054507721813
At time: 319.4685962200165 and batch: 500, loss is 4.7786281967163085 and perplexity is 118.94107432301104
At time: 320.1269826889038 and batch: 550, loss is 4.820820417404175 and perplexity is 124.06683562872527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.103881186627327 and perplexity of 164.65974391389454
Finished 39 epochs...
Completing Train Step...
At time: 321.8411617279053 and batch: 50, loss is 4.858508825302124 and perplexity is 128.8319478597416
At time: 322.50327825546265 and batch: 100, loss is 4.870353698730469 and perplexity is 130.36701939203508
At time: 323.13838624954224 and batch: 150, loss is 4.838901453018188 and perplexity is 126.33049548949636
At time: 323.78422474861145 and batch: 200, loss is 4.78704984664917 and perplexity is 119.9469841786608
At time: 324.43330430984497 and batch: 250, loss is 4.771318407058716 and perplexity is 118.07481005012963
At time: 325.0786101818085 and batch: 300, loss is 4.764704074859619 and perplexity is 117.29640119738954
At time: 325.7274806499481 and batch: 350, loss is 4.831100387573242 and perplexity is 125.34881707373826
At time: 326.3756625652313 and batch: 400, loss is 4.832329406738281 and perplexity is 125.50296787998084
At time: 327.01893186569214 and batch: 450, loss is 4.7580556964874265 and perplexity is 116.5191569104945
At time: 327.6686532497406 and batch: 500, loss is 4.776033401489258 and perplexity is 118.63284665783505
At time: 328.31136775016785 and batch: 550, loss is 4.818699693679809 and perplexity is 123.8040029432631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.10244523718002 and perplexity of 164.42347052454772
Finished 40 epochs...
Completing Train Step...
At time: 330.027957201004 and batch: 50, loss is 4.855892734527588 and perplexity is 128.49535226376665
At time: 330.69435119628906 and batch: 100, loss is 4.867976608276368 and perplexity is 130.0574932263052
At time: 331.34893798828125 and batch: 150, loss is 4.836819458007812 and perplexity is 126.06774964091223
At time: 332.00478768348694 and batch: 200, loss is 4.784649209976196 and perplexity is 119.65938040383654
At time: 332.65354108810425 and batch: 250, loss is 4.769226236343384 and perplexity is 117.82803562744922
At time: 333.2915518283844 and batch: 300, loss is 4.762600231170654 and perplexity is 117.04988730826285
At time: 333.9442355632782 and batch: 350, loss is 4.829298362731934 and perplexity is 125.12313879154841
At time: 334.59866738319397 and batch: 400, loss is 4.830636653900147 and perplexity is 125.29070208235915
At time: 335.24668407440186 and batch: 450, loss is 4.7563141918182374 and perplexity is 116.3164148440529
At time: 335.9184522628784 and batch: 500, loss is 4.773564567565918 and perplexity is 118.34032310619868
At time: 336.57118463516235 and batch: 550, loss is 4.816546325683594 and perplexity is 123.53769419879141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.100940785509475 and perplexity of 164.17628934233346
Finished 41 epochs...
Completing Train Step...
At time: 338.29875802993774 and batch: 50, loss is 4.853541669845581 and perplexity is 128.19360623047
At time: 338.9660222530365 and batch: 100, loss is 4.866218366622925 and perplexity is 129.82902163736287
At time: 339.60818219184875 and batch: 150, loss is 4.834973154067993 and perplexity is 125.83520499816998
At time: 340.2525975704193 and batch: 200, loss is 4.783000154495239 and perplexity is 119.4622180572378
At time: 340.9054741859436 and batch: 250, loss is 4.767019186019898 and perplexity is 117.56826998666742
At time: 341.5590646266937 and batch: 300, loss is 4.760636787414551 and perplexity is 116.82029191045967
At time: 342.22297859191895 and batch: 350, loss is 4.827068452835083 and perplexity is 124.84443632221668
At time: 342.8713803291321 and batch: 400, loss is 4.828419990539551 and perplexity is 125.01328236029063
At time: 343.511892080307 and batch: 450, loss is 4.7539326858520505 and perplexity is 116.0397361948301
At time: 344.16024804115295 and batch: 500, loss is 4.770929594039917 and perplexity is 118.0289099506463
At time: 344.80123114585876 and batch: 550, loss is 4.8138489913940425 and perplexity is 123.20492074256487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.09895714293135 and perplexity of 163.85094505454776
Finished 42 epochs...
Completing Train Step...
At time: 346.5038392543793 and batch: 50, loss is 4.8503039360046385 and perplexity is 127.77922065049049
At time: 347.1617958545685 and batch: 100, loss is 4.863292379379272 and perplexity is 129.44969879372482
At time: 347.8098473548889 and batch: 150, loss is 4.831510038375854 and perplexity is 125.40017683632473
At time: 348.450386762619 and batch: 200, loss is 4.779350156784058 and perplexity is 119.02697603415886
At time: 349.09627985954285 and batch: 250, loss is 4.763330793380737 and perplexity is 117.13543077623254
At time: 349.7360272407532 and batch: 300, loss is 4.757068157196045 and perplexity is 116.40414646286155
At time: 350.37861680984497 and batch: 350, loss is 4.823418788909912 and perplexity is 124.38962654297573
At time: 351.0188410282135 and batch: 400, loss is 4.8248913288116455 and perplexity is 124.572930159244
At time: 351.6669580936432 and batch: 450, loss is 4.750238065719604 and perplexity is 115.61180445870475
At time: 352.3264191150665 and batch: 500, loss is 4.76696548461914 and perplexity is 117.56195657540555
At time: 352.96420526504517 and batch: 550, loss is 4.810541391372681 and perplexity is 122.79808134599251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.095377495948305 and perplexity of 163.2654650444787
Finished 43 epochs...
Completing Train Step...
At time: 354.6770303249359 and batch: 50, loss is 4.8471950244903566 and perplexity is 127.38258323490761
At time: 355.34397196769714 and batch: 100, loss is 4.860234479904175 and perplexity is 129.05445923713046
At time: 356.0069215297699 and batch: 150, loss is 4.828612098693847 and perplexity is 125.03730073821666
At time: 356.64773964881897 and batch: 200, loss is 4.776123085021973 and perplexity is 118.64348654772367
At time: 357.2836375236511 and batch: 250, loss is 4.760425205230713 and perplexity is 116.79557743264442
At time: 357.9307699203491 and batch: 300, loss is 4.7541846179962155 and perplexity is 116.06897401719701
At time: 358.57925391197205 and batch: 350, loss is 4.820663814544678 and perplexity is 124.04740792875346
At time: 359.227530002594 and batch: 400, loss is 4.822478981018066 and perplexity is 124.27277910595899
At time: 359.87655448913574 and batch: 450, loss is 4.746606073379517 and perplexity is 115.1926648870957
At time: 360.52329897880554 and batch: 500, loss is 4.763527307510376 and perplexity is 117.1584518053661
At time: 361.1661386489868 and batch: 550, loss is 4.8066170787811275 and perplexity is 122.31712761277646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.09313055809508 and perplexity of 162.89902952403622
Finished 44 epochs...
Completing Train Step...
At time: 362.87691950798035 and batch: 50, loss is 4.844037084579468 and perplexity is 126.98095119006571
At time: 363.5443322658539 and batch: 100, loss is 4.857074241638184 and perplexity is 128.64726015860236
At time: 364.19292521476746 and batch: 150, loss is 4.825570421218872 and perplexity is 124.65755542119534
At time: 364.8383011817932 and batch: 200, loss is 4.772828330993653 and perplexity is 118.25322869733277
At time: 365.479150056839 and batch: 250, loss is 4.757800407409668 and perplexity is 116.48941463898572
At time: 366.1236763000488 and batch: 300, loss is 4.751428565979004 and perplexity is 115.74952230220117
At time: 366.77688932418823 and batch: 350, loss is 4.818159198760986 and perplexity is 123.73710558921086
At time: 367.43559288978577 and batch: 400, loss is 4.8202947235107425 and perplexity is 124.00163159104163
At time: 368.0987877845764 and batch: 450, loss is 4.744082365036011 and perplexity is 114.90231872622755
At time: 368.7651798725128 and batch: 500, loss is 4.760345697402954 and perplexity is 116.78629163914243
At time: 369.4086558818817 and batch: 550, loss is 4.803607339859009 and perplexity is 121.94953844415731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.092038418384308 and perplexity of 162.7212181401575
Finished 45 epochs...
Completing Train Step...
At time: 371.1255021095276 and batch: 50, loss is 4.841058874130249 and perplexity is 126.60333777959309
At time: 371.7809410095215 and batch: 100, loss is 4.854655313491821 and perplexity is 128.33644774804876
At time: 372.426203250885 and batch: 150, loss is 4.822910022735596 and perplexity is 124.32635740451836
At time: 373.07224202156067 and batch: 200, loss is 4.769922285079956 and perplexity is 117.9100782322939
At time: 373.7216229438782 and batch: 250, loss is 4.7549365139007564 and perplexity is 116.15627862127882
At time: 374.3655278682709 and batch: 300, loss is 4.749029302597046 and perplexity is 115.47214159960473
At time: 375.010128736496 and batch: 350, loss is 4.815872478485107 and perplexity is 123.45447671073258
At time: 375.6471583843231 and batch: 400, loss is 4.817397155761719 and perplexity is 123.64284851268098
At time: 376.2993083000183 and batch: 450, loss is 4.7408780765533445 and perplexity is 114.53472779790926
At time: 376.95782232284546 and batch: 500, loss is 4.757522096633911 and perplexity is 116.45699889066533
At time: 377.61123728752136 and batch: 550, loss is 4.801196594238281 and perplexity is 121.65590321053985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.09016256129488 and perplexity of 162.4162625055277
Finished 46 epochs...
Completing Train Step...
At time: 379.32319831848145 and batch: 50, loss is 4.83856388092041 and perplexity is 126.28785703630157
At time: 380.01565504074097 and batch: 100, loss is 4.852431812286377 and perplexity is 128.05140851175645
At time: 380.6602454185486 and batch: 150, loss is 4.820646934509277 and perplexity is 124.04531402178893
At time: 381.3272819519043 and batch: 200, loss is 4.767410459518433 and perplexity is 117.61428033570998
At time: 381.9673318862915 and batch: 250, loss is 4.752303037643433 and perplexity is 115.8507862493988
At time: 382.6079132556915 and batch: 300, loss is 4.746434774398804 and perplexity is 115.17293419098725
At time: 383.2478201389313 and batch: 350, loss is 4.813293199539185 and perplexity is 123.13646347689534
At time: 383.88773703575134 and batch: 400, loss is 4.814780578613282 and perplexity is 123.31975035066904
At time: 384.53178691864014 and batch: 450, loss is 4.7378722667694095 and perplexity is 114.19097507895096
At time: 385.20189905166626 and batch: 500, loss is 4.754840393066406 and perplexity is 116.14511411944189
At time: 385.841814994812 and batch: 550, loss is 4.798635215759277 and perplexity is 121.34469512919782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0887016134059175 and perplexity of 162.17915405340642
Finished 47 epochs...
Completing Train Step...
At time: 387.58431029319763 and batch: 50, loss is 4.835118980407715 and perplexity is 125.85355642355108
At time: 388.25507521629333 and batch: 100, loss is 4.849152517318726 and perplexity is 127.63217793827071
At time: 388.90114665031433 and batch: 150, loss is 4.817380504608154 and perplexity is 123.64078973376384
At time: 389.5464165210724 and batch: 200, loss is 4.763266487121582 and perplexity is 117.1278984770544
At time: 390.2012138366699 and batch: 250, loss is 4.7491210842132565 and perplexity is 115.48274030576374
At time: 390.8665101528168 and batch: 300, loss is 4.742782773971558 and perplexity is 114.75308968889097
At time: 391.51943469047546 and batch: 350, loss is 4.810351066589355 and perplexity is 122.77471205171602
At time: 392.17206597328186 and batch: 400, loss is 4.811436004638672 and perplexity is 122.90798729293189
At time: 392.8217396736145 and batch: 450, loss is 4.734605808258056 and perplexity is 113.81858352836994
At time: 393.46969628334045 and batch: 500, loss is 4.752360382080078 and perplexity is 115.8574298379558
At time: 394.11292004585266 and batch: 550, loss is 4.7957226848602295 and perplexity is 120.99178912948311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.085930032933012 and perplexity of 161.73028380426732
Finished 48 epochs...
Completing Train Step...
At time: 395.8430438041687 and batch: 50, loss is 4.832009992599487 and perplexity is 125.46288685914256
At time: 396.5102162361145 and batch: 100, loss is 4.846463928222656 and perplexity is 127.28948833854678
At time: 397.16358733177185 and batch: 150, loss is 4.814583597183227 and perplexity is 123.2954610422409
At time: 397.80816626548767 and batch: 200, loss is 4.760642318725586 and perplexity is 116.8209380816165
At time: 398.468355178833 and batch: 250, loss is 4.746227169036866 and perplexity is 115.14902615410325
At time: 399.1140766143799 and batch: 300, loss is 4.740802373886108 and perplexity is 114.52605754170868
At time: 399.7579870223999 and batch: 350, loss is 4.807881565093994 and perplexity is 122.47189377569767
At time: 400.398725271225 and batch: 400, loss is 4.808836288452149 and perplexity is 122.58887638753302
At time: 401.0505750179291 and batch: 450, loss is 4.732241630554199 and perplexity is 113.54981400555117
At time: 401.7099633216858 and batch: 500, loss is 4.749873085021973 and perplexity is 115.56961608109903
At time: 402.3616633415222 and batch: 550, loss is 4.792814483642578 and perplexity is 120.64043181752979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.084838217877327 and perplexity of 161.55380060648739
Finished 49 epochs...
Completing Train Step...
At time: 404.14673686027527 and batch: 50, loss is 4.829403085708618 and perplexity is 125.13624274522527
At time: 404.82107973098755 and batch: 100, loss is 4.844496622085571 and perplexity is 127.03931710933554
At time: 405.4735538959503 and batch: 150, loss is 4.812485990524292 and perplexity is 123.0371067197447
At time: 406.12100076675415 and batch: 200, loss is 4.758302764892578 and perplexity is 116.5479486693825
At time: 406.76038670539856 and batch: 250, loss is 4.74395055770874 and perplexity is 114.88717475676407
At time: 407.4159803390503 and batch: 300, loss is 4.737870941162109 and perplexity is 114.19082370666109
At time: 408.06512808799744 and batch: 350, loss is 4.805221948623657 and perplexity is 122.14659828223432
At time: 408.71564960479736 and batch: 400, loss is 4.806117467880249 and perplexity is 122.2560319057684
At time: 409.3722651004791 and batch: 450, loss is 4.729835824966431 and perplexity is 113.27696357267713
At time: 410.0328314304352 and batch: 500, loss is 4.747678413391113 and perplexity is 115.31625684515409
At time: 410.67591857910156 and batch: 550, loss is 4.790798473358154 and perplexity is 120.39746446090409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.083696405938331 and perplexity of 161.3694418197261
Finished Training.
Improved accuracyfrom -10000000 to -161.3694418197261
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fec840548d0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 25.983096356513364, 'anneal': 3.490448677857382, 'dropout': 0.34412375796989636, 'seq_len': 35, 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9001216888427734 and batch: 50, loss is 6.79730408668518 and perplexity is 895.4300330061074
At time: 1.5641918182373047 and batch: 100, loss is 5.8956448841094975 and perplexity is 363.4511442074052
At time: 2.2072668075561523 and batch: 150, loss is 5.878533039093018 and perplexity is 357.2847343490654
At time: 2.8448054790496826 and batch: 200, loss is 5.851041212081909 and perplexity is 347.59611339860624
At time: 3.4874322414398193 and batch: 250, loss is 5.907473001480103 and perplexity is 367.7756117367848
At time: 4.134461164474487 and batch: 300, loss is 6.011670131683349 and perplexity is 408.16443969877025
At time: 4.7859392166137695 and batch: 350, loss is 6.093439073562622 and perplexity is 442.9421052416494
At time: 5.434508800506592 and batch: 400, loss is 6.149486989974975 and perplexity is 468.47699173190176
At time: 6.087505340576172 and batch: 450, loss is 6.164470329284668 and perplexity is 475.5491917351126
At time: 6.736072778701782 and batch: 500, loss is 6.18731915473938 and perplexity is 486.54001778540146
At time: 7.3776350021362305 and batch: 550, loss is 6.204366693496704 and perplexity is 494.90542984437036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.284691343916223 and perplexity of 536.2987332219304
Finished 1 epochs...
Completing Train Step...
At time: 9.121215581893921 and batch: 50, loss is 6.19999831199646 and perplexity is 492.7482093318324
At time: 9.775415658950806 and batch: 100, loss is 6.34397066116333 and perplexity is 569.0513415002133
At time: 10.422119617462158 and batch: 150, loss is 6.364241361618042 and perplexity is 580.7041167362822
At time: 11.06644344329834 and batch: 200, loss is 6.37876781463623 and perplexity is 589.201255025474
At time: 11.707695960998535 and batch: 250, loss is 6.181833009719849 and perplexity is 483.8780972076754
At time: 12.348352193832397 and batch: 300, loss is 6.224282369613648 and perplexity is 504.86060911182136
At time: 12.991694688796997 and batch: 350, loss is 6.337562341690063 and perplexity is 565.4163382779038
At time: 13.642633199691772 and batch: 400, loss is 6.492844142913818 and perplexity is 660.3989637459381
At time: 14.29261827468872 and batch: 450, loss is 6.549348793029785 and perplexity is 698.7889693682554
At time: 14.943666696548462 and batch: 500, loss is 6.321725625991821 and perplexity is 556.5325313120213
At time: 15.586641550064087 and batch: 550, loss is 6.192420215606689 and perplexity is 489.0282288932232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.968668836228391 and perplexity of 390.9848593041769
Finished 2 epochs...
Completing Train Step...
At time: 17.32339310646057 and batch: 50, loss is 6.232600107192993 and perplexity is 509.07742002429865
At time: 17.990798234939575 and batch: 100, loss is 6.203213548660278 and perplexity is 494.33506112550134
At time: 18.633631706237793 and batch: 150, loss is 6.20099250793457 and perplexity is 493.23834120324767
At time: 19.273937702178955 and batch: 200, loss is 6.189428024291992 and perplexity is 487.5671498781685
At time: 19.921855449676514 and batch: 250, loss is 6.038286724090576 and perplexity is 419.17425817270237
At time: 20.565823793411255 and batch: 300, loss is 6.077783374786377 and perplexity is 436.0615376490278
At time: 21.213484048843384 and batch: 350, loss is 6.110993766784668 and perplexity is 450.7864692693325
At time: 21.861393213272095 and batch: 400, loss is 6.174847440719605 and perplexity is 480.50971211275
At time: 22.50701928138733 and batch: 450, loss is 6.015143089294433 and perplexity is 409.584441872603
At time: 23.146064519882202 and batch: 500, loss is 6.161393737792968 and perplexity is 474.08836946689047
At time: 23.789114236831665 and batch: 550, loss is 6.306830596923828 and perplexity is 548.3043943755399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.451621197639628 and perplexity of 633.7288601854063
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 25.532285690307617 and batch: 50, loss is 5.742817430496216 and perplexity is 311.9420491008292
At time: 26.189265251159668 and batch: 100, loss is 5.572786903381347 and perplexity is 263.1664978068187
At time: 26.836522817611694 and batch: 150, loss is 5.564751319885254 and perplexity is 261.0602751377036
At time: 27.479983806610107 and batch: 200, loss is 5.500107908248902 and perplexity is 244.71833796682233
At time: 28.133466005325317 and batch: 250, loss is 5.479512586593628 and perplexity is 239.7298313593889
At time: 28.7812283039093 and batch: 300, loss is 5.479425220489502 and perplexity is 239.708888012863
At time: 29.422168254852295 and batch: 350, loss is 5.542733678817749 and perplexity is 255.37515978307044
At time: 30.068400382995605 and batch: 400, loss is 5.545896806716919 and perplexity is 256.18422298634874
At time: 30.713622570037842 and batch: 450, loss is 5.476851463317871 and perplexity is 239.0927288054571
At time: 31.35081720352173 and batch: 500, loss is 5.500316734313965 and perplexity is 244.76944687063866
At time: 32.004554748535156 and batch: 550, loss is 5.539638175964355 and perplexity is 254.5858675057832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.645524045254322 and perplexity of 283.02183361613174
Finished 4 epochs...
Completing Train Step...
At time: 33.78456950187683 and batch: 50, loss is 5.568517208099365 and perplexity is 262.04525244416055
At time: 34.456666469573975 and batch: 100, loss is 5.563730411529541 and perplexity is 260.7938925207271
At time: 35.10221552848816 and batch: 150, loss is 5.541685733795166 and perplexity is 255.107680831581
At time: 35.753944873809814 and batch: 200, loss is 5.4572007274627685 and perplexity is 234.44024284586413
At time: 36.40523409843445 and batch: 250, loss is 5.428738374710083 and perplexity is 227.86158806215806
At time: 37.06119441986084 and batch: 300, loss is 5.419509286880493 and perplexity is 225.76830784452568
At time: 37.71472644805908 and batch: 350, loss is 5.473166790008545 and perplexity is 238.2133712766876
At time: 38.36788272857666 and batch: 400, loss is 5.475116910934449 and perplexity is 238.67836941077573
At time: 39.02250123023987 and batch: 450, loss is 5.4024800777435305 and perplexity is 221.9562028042436
At time: 39.66935992240906 and batch: 500, loss is 5.415317363739014 and perplexity is 224.82388530657195
At time: 40.312243938446045 and batch: 550, loss is 5.45223952293396 and perplexity is 233.28001728923675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.55267788501496 and perplexity of 257.9273316598953
Finished 5 epochs...
Completing Train Step...
At time: 42.06315326690674 and batch: 50, loss is 5.486647787094117 and perplexity is 241.4464687651415
At time: 42.73159456253052 and batch: 100, loss is 5.471312294006347 and perplexity is 237.77201490528927
At time: 43.37406778335571 and batch: 150, loss is 5.444232482910156 and perplexity is 231.41959303948562
At time: 44.01624870300293 and batch: 200, loss is 5.369296970367432 and perplexity is 214.71186582701728
At time: 44.6619610786438 and batch: 250, loss is 5.350544509887695 and perplexity is 210.72300739469537
At time: 45.30384612083435 and batch: 300, loss is 5.353305912017822 and perplexity is 211.3057025136825
At time: 45.96790909767151 and batch: 350, loss is 5.4037958431243895 and perplexity is 222.24843730581983
At time: 46.61613059043884 and batch: 400, loss is 5.40967755317688 and perplexity is 223.55949001000616
At time: 47.26114749908447 and batch: 450, loss is 5.326574172973633 and perplexity is 205.73196339105078
At time: 47.89640712738037 and batch: 500, loss is 5.346459951400757 and perplexity is 209.86405236667915
At time: 48.53244614601135 and batch: 550, loss is 5.390138025283814 and perplexity is 219.23364325157414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.515758757895612 and perplexity of 248.5785166227265
Finished 6 epochs...
Completing Train Step...
At time: 50.23858332633972 and batch: 50, loss is 5.432168807983398 and perplexity is 228.64459429301118
At time: 50.89426565170288 and batch: 100, loss is 5.415221557617188 and perplexity is 224.80234683380212
At time: 51.52777576446533 and batch: 150, loss is 5.389890060424805 and perplexity is 219.17928775154078
At time: 52.17736482620239 and batch: 200, loss is 5.32046893119812 and perplexity is 204.47974644678737
At time: 52.82505798339844 and batch: 250, loss is 5.304095268249512 and perplexity is 201.158925194078
At time: 53.47784757614136 and batch: 300, loss is 5.302832326889038 and perplexity is 200.90503362624986
At time: 54.12892198562622 and batch: 350, loss is 5.350170011520386 and perplexity is 210.64410674747552
At time: 54.77956414222717 and batch: 400, loss is 5.357289733886719 and perplexity is 212.14918582043575
At time: 55.42686152458191 and batch: 450, loss is 5.288582487106323 and perplexity is 198.06247022233205
At time: 56.07367992401123 and batch: 500, loss is 5.303902158737182 and perplexity is 201.12008324262894
At time: 56.73263931274414 and batch: 550, loss is 5.349269285202026 and perplexity is 210.4544594796852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.485952174409907 and perplexity of 241.2785739405657
Finished 7 epochs...
Completing Train Step...
At time: 58.503706216812134 and batch: 50, loss is 5.388462419509888 and perplexity is 218.8666016874622
At time: 59.16816973686218 and batch: 100, loss is 5.383602170944214 and perplexity is 217.80543645284595
At time: 59.80401349067688 and batch: 150, loss is 5.354924221038818 and perplexity is 211.64793728436607
At time: 60.44437265396118 and batch: 200, loss is 5.291210889816284 and perplexity is 198.58374291298134
At time: 61.08838391304016 and batch: 250, loss is 5.271624994277954 and perplexity is 194.73214409902258
At time: 61.72869348526001 and batch: 300, loss is 5.277637147903443 and perplexity is 195.90643012200968
At time: 62.36529183387756 and batch: 350, loss is 5.328092241287232 and perplexity is 206.0445157436568
At time: 63.00956082344055 and batch: 400, loss is 5.3359948348999025 and perplexity is 207.67925264081066
At time: 63.64722681045532 and batch: 450, loss is 5.262760953903198 and perplexity is 193.01365812792736
At time: 64.28516244888306 and batch: 500, loss is 5.2849994659423825 and perplexity is 197.35407804968034
At time: 64.92863941192627 and batch: 550, loss is 5.329541168212891 and perplexity is 206.34327557871902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.469035047165891 and perplexity of 237.2311653932489
Finished 8 epochs...
Completing Train Step...
At time: 66.661780834198 and batch: 50, loss is 5.3719784450531005 and perplexity is 215.28838287243218
At time: 67.32701563835144 and batch: 100, loss is 5.365754022598266 and perplexity is 213.95249889399065
At time: 67.97898173332214 and batch: 150, loss is 5.337507200241089 and perplexity is 207.99357717143963
At time: 68.62860584259033 and batch: 200, loss is 5.276225948333741 and perplexity is 195.63016203267262
At time: 69.27748107910156 and batch: 250, loss is 5.260594959259033 and perplexity is 192.59604401640686
At time: 69.94034790992737 and batch: 300, loss is 5.259082584381104 and perplexity is 192.30498674717626
At time: 70.61614561080933 and batch: 350, loss is 5.309988822937012 and perplexity is 202.3479667195386
At time: 71.28521251678467 and batch: 400, loss is 5.317463312149048 and perplexity is 203.86608091010564
At time: 71.94394946098328 and batch: 450, loss is 5.247446966171265 and perplexity is 190.08036685418088
At time: 72.60557341575623 and batch: 500, loss is 5.26465145111084 and perplexity is 193.37889504055755
At time: 73.26591420173645 and batch: 550, loss is 5.308305921554566 and perplexity is 202.0077214265505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.459414867644615 and perplexity of 234.95990149403923
Finished 9 epochs...
Completing Train Step...
At time: 75.04565334320068 and batch: 50, loss is 5.349706535339355 and perplexity is 210.546500842077
At time: 75.7071270942688 and batch: 100, loss is 5.345040912628174 and perplexity is 209.56645833807005
At time: 76.35663080215454 and batch: 150, loss is 5.3191911983489994 and perplexity is 204.2186428036465
At time: 77.00884652137756 and batch: 200, loss is 5.257788343429565 and perplexity is 192.0562587498425
At time: 77.65626645088196 and batch: 250, loss is 5.241722059249878 and perplexity is 188.99528341080796
At time: 78.29497742652893 and batch: 300, loss is 5.243398323059082 and perplexity is 189.31235503810262
At time: 78.94562149047852 and batch: 350, loss is 5.295073223114014 and perplexity is 199.35222262268394
At time: 79.59340691566467 and batch: 400, loss is 5.301380205154419 and perplexity is 200.61350677779072
At time: 80.23849296569824 and batch: 450, loss is 5.22829342842102 and perplexity is 186.4743000822805
At time: 80.88207626342773 and batch: 500, loss is 5.251771068572998 and perplexity is 190.9040734373624
At time: 81.5280532836914 and batch: 550, loss is 5.302011404037476 and perplexity is 200.74017377102186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.462172487948803 and perplexity of 235.60872588356074
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 83.2464861869812 and batch: 50, loss is 5.280407466888428 and perplexity is 196.4499058776763
At time: 83.90868473052979 and batch: 100, loss is 5.2206737995147705 and perplexity is 185.05883462419996
At time: 84.55380153656006 and batch: 150, loss is 5.1743270301818844 and perplexity is 176.6776755166659
At time: 85.20023155212402 and batch: 200, loss is 5.110400619506836 and perplexity is 165.7367389460859
At time: 85.85530424118042 and batch: 250, loss is 5.0852244853973385 and perplexity is 161.61621564607128
At time: 86.50780081748962 and batch: 300, loss is 5.074346437454223 and perplexity is 159.86767430448216
At time: 87.16276979446411 and batch: 350, loss is 5.121462182998657 and perplexity is 167.58022352061823
At time: 87.8188464641571 and batch: 400, loss is 5.111308259963989 and perplexity is 165.88723660413973
At time: 88.45831823348999 and batch: 450, loss is 5.036156692504883 and perplexity is 153.87747863850373
At time: 89.10382008552551 and batch: 500, loss is 5.0539197254180905 and perplexity is 156.63522985767966
At time: 89.75391483306885 and batch: 550, loss is 5.1110132122039795 and perplexity is 165.8382991663612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.30842525401014 and perplexity of 202.03182894236915
Finished 11 epochs...
Completing Train Step...
At time: 91.51715230941772 and batch: 50, loss is 5.1729975509643555 and perplexity is 176.44294228991302
At time: 92.17505240440369 and batch: 100, loss is 5.15736291885376 and perplexity is 173.70577485787774
At time: 92.82007598876953 and batch: 150, loss is 5.131407585144043 and perplexity is 169.25519154438882
At time: 93.47808074951172 and batch: 200, loss is 5.073036470413208 and perplexity is 159.65839002789969
At time: 94.12658023834229 and batch: 250, loss is 5.055814514160156 and perplexity is 156.93230188331012
At time: 94.77479290962219 and batch: 300, loss is 5.04940544128418 and perplexity is 155.9297275417821
At time: 95.42761063575745 and batch: 350, loss is 5.097818126678467 and perplexity is 163.664422411407
At time: 96.08068704605103 and batch: 400, loss is 5.105190372467041 and perplexity is 164.87545529132765
At time: 96.71999144554138 and batch: 450, loss is 5.035913429260254 and perplexity is 153.84005045640987
At time: 97.3650119304657 and batch: 500, loss is 5.052655897140503 and perplexity is 156.43739486598034
At time: 98.01175808906555 and batch: 550, loss is 5.106040296554565 and perplexity is 165.01564647971608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.299331340383976 and perplexity of 200.20289761624093
Finished 12 epochs...
Completing Train Step...
At time: 99.8289270401001 and batch: 50, loss is 5.157115869522094 and perplexity is 173.66286626278074
At time: 100.50051188468933 and batch: 100, loss is 5.146136074066162 and perplexity is 171.76651334782449
At time: 101.14796566963196 and batch: 150, loss is 5.122070293426514 and perplexity is 167.68216179376574
At time: 101.79237127304077 and batch: 200, loss is 5.065250539779663 and perplexity is 158.42012764594142
At time: 102.43376064300537 and batch: 250, loss is 5.049411926269531 and perplexity is 155.9307387470599
At time: 103.070383310318 and batch: 300, loss is 5.045255784988403 and perplexity is 155.28401343856694
At time: 103.7178692817688 and batch: 350, loss is 5.094648694992065 and perplexity is 163.14652036619583
At time: 104.36214780807495 and batch: 400, loss is 5.104005336761475 and perplexity is 164.680187712214
At time: 105.00639986991882 and batch: 450, loss is 5.03368971824646 and perplexity is 153.49833472118166
At time: 105.65339803695679 and batch: 500, loss is 5.048624792098999 and perplexity is 155.80804862748317
At time: 106.30216860771179 and batch: 550, loss is 5.102951946258545 and perplexity is 164.50680650155016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.296971097905585 and perplexity of 199.73092743412846
Finished 13 epochs...
Completing Train Step...
At time: 108.0631332397461 and batch: 50, loss is 5.151402521133423 and perplexity is 172.67349879742494
At time: 108.7341423034668 and batch: 100, loss is 5.139824180603028 and perplexity is 170.6857558163505
At time: 109.3889422416687 and batch: 150, loss is 5.117815532684326 and perplexity is 166.97022993846252
At time: 110.04187345504761 and batch: 200, loss is 5.061604318618774 and perplexity is 157.84354463759337
At time: 110.69110655784607 and batch: 250, loss is 5.044646415710449 and perplexity is 155.18941695644233
At time: 111.33401226997375 and batch: 300, loss is 5.0425965690612795 and perplexity is 154.87162827041973
At time: 111.98035597801208 and batch: 350, loss is 5.090572662353516 and perplexity is 162.48288324640535
At time: 112.6281304359436 and batch: 400, loss is 5.100651245117188 and perplexity is 164.12876055619682
At time: 113.26843762397766 and batch: 450, loss is 5.030084314346314 and perplexity is 152.94590768621867
At time: 113.91103529930115 and batch: 500, loss is 5.045630140304565 and perplexity is 155.34215571676913
At time: 114.55919790267944 and batch: 550, loss is 5.0985806369781494 and perplexity is 163.78926581032715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2961412795046545 and perplexity of 199.56525578350758
Finished 14 epochs...
Completing Train Step...
At time: 116.29132962226868 and batch: 50, loss is 5.144813709259033 and perplexity is 171.53952546899814
At time: 116.95856094360352 and batch: 100, loss is 5.133604412078857 and perplexity is 169.62742462415733
At time: 117.59758305549622 and batch: 150, loss is 5.111260185241699 and perplexity is 165.87926181300296
At time: 118.24612545967102 and batch: 200, loss is 5.055766820907593 and perplexity is 156.92481744988098
At time: 118.8901879787445 and batch: 250, loss is 5.0415576076507564 and perplexity is 154.7108061834995
At time: 119.53413009643555 and batch: 300, loss is 5.038138065338135 and perplexity is 154.18266954296672
At time: 120.17558312416077 and batch: 350, loss is 5.087355136871338 and perplexity is 161.96093057744028
At time: 120.82127261161804 and batch: 400, loss is 5.097157592773438 and perplexity is 163.5563522073035
At time: 121.46418642997742 and batch: 450, loss is 5.027278757095337 and perplexity is 152.51741055358482
At time: 122.11827969551086 and batch: 500, loss is 5.042379550933838 and perplexity is 154.83802196637907
At time: 122.76755619049072 and batch: 550, loss is 5.094934511184692 and perplexity is 163.19315694791487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.293932650951629 and perplexity of 199.1249766469035
Finished 15 epochs...
Completing Train Step...
At time: 124.49615263938904 and batch: 50, loss is 5.140630893707275 and perplexity is 170.8235058071668
At time: 125.1563994884491 and batch: 100, loss is 5.129784202575683 and perplexity is 168.98064852130383
At time: 125.80178356170654 and batch: 150, loss is 5.107027378082275 and perplexity is 165.178610792439
At time: 126.44705605506897 and batch: 200, loss is 5.052227411270142 and perplexity is 156.37037801159713
At time: 127.0956175327301 and batch: 250, loss is 5.037386083602906 and perplexity is 154.06677057399693
At time: 127.74047112464905 and batch: 300, loss is 5.035194005966186 and perplexity is 153.72941414249354
At time: 128.38774728775024 and batch: 350, loss is 5.082782144546509 and perplexity is 161.22197539095365
At time: 129.03281092643738 and batch: 400, loss is 5.092601184844971 and perplexity is 162.8128179563996
At time: 129.67151951789856 and batch: 450, loss is 5.020360527038574 and perplexity is 151.46590150440025
At time: 130.31146121025085 and batch: 500, loss is 5.035956773757935 and perplexity is 153.84671872063535
At time: 130.95403623580933 and batch: 550, loss is 5.089050045013428 and perplexity is 162.23567224254788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.288097625083112 and perplexity of 197.96646052990562
Finished 16 epochs...
Completing Train Step...
At time: 132.66802740097046 and batch: 50, loss is 5.133421745300293 and perplexity is 169.59644215876475
At time: 133.329407453537 and batch: 100, loss is 5.124644222259522 and perplexity is 168.11431967801607
At time: 133.97423911094666 and batch: 150, loss is 5.101241798400879 and perplexity is 164.2257159605398
At time: 134.628319978714 and batch: 200, loss is 5.046634464263916 and perplexity is 155.49824793611015
At time: 135.27199697494507 and batch: 250, loss is 5.030760374069214 and perplexity is 153.04934321454533
At time: 135.91836762428284 and batch: 300, loss is 5.029481620788574 and perplexity is 152.8537559453932
At time: 136.56944608688354 and batch: 350, loss is 5.078584203720093 and perplexity is 160.54659367641844
At time: 137.21329069137573 and batch: 400, loss is 5.087120323181153 and perplexity is 161.922904398374
At time: 137.85642862319946 and batch: 450, loss is 5.015550546646118 and perplexity is 150.73910283297977
At time: 138.51174521446228 and batch: 500, loss is 5.029581432342529 and perplexity is 152.86901327771835
At time: 139.1558895111084 and batch: 550, loss is 5.083106145858765 and perplexity is 161.2742199857464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.286370135368185 and perplexity of 197.62477072322622
Finished 17 epochs...
Completing Train Step...
At time: 140.8821804523468 and batch: 50, loss is 5.125495872497559 and perplexity is 168.25755526303976
At time: 141.54376196861267 and batch: 100, loss is 5.116138572692871 and perplexity is 166.69046218831707
At time: 142.18154764175415 and batch: 150, loss is 5.092362413406372 and perplexity is 162.77394754639036
At time: 142.83036398887634 and batch: 200, loss is 5.038283987045288 and perplexity is 154.20516978291664
At time: 143.47833037376404 and batch: 250, loss is 5.021838045120239 and perplexity is 151.68986052363596
At time: 144.1246371269226 and batch: 300, loss is 5.018054571151733 and perplexity is 151.11703021266666
At time: 144.78113675117493 and batch: 350, loss is 5.068090772628784 and perplexity is 158.87071728328206
At time: 145.42735767364502 and batch: 400, loss is 5.077053918838501 and perplexity is 160.30109953721683
At time: 146.0766954421997 and batch: 450, loss is 5.0027447700500485 and perplexity is 148.8210786633136
At time: 146.7262635231018 and batch: 500, loss is 5.018225622177124 and perplexity is 151.1428811464899
At time: 147.3738968372345 and batch: 550, loss is 5.0706409740447995 and perplexity is 159.27638666091514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.274304329080785 and perplexity of 195.25459630958235
Finished 18 epochs...
Completing Train Step...
At time: 149.10438656806946 and batch: 50, loss is 5.114123048782349 and perplexity is 166.35483192515105
At time: 149.76142168045044 and batch: 100, loss is 5.104969873428344 and perplexity is 164.83910441974473
At time: 150.40624928474426 and batch: 150, loss is 5.081128396987915 and perplexity is 160.95557528273784
At time: 151.05357837677002 and batch: 200, loss is 5.028208446502686 and perplexity is 152.65927030715022
At time: 151.69513750076294 and batch: 250, loss is 5.012290410995483 and perplexity is 150.24847310417846
At time: 152.34592580795288 and batch: 300, loss is 5.0091660213470455 and perplexity is 149.77977091553316
At time: 153.0027620792389 and batch: 350, loss is 5.058183174133301 and perplexity is 157.3044617313407
At time: 153.6442859172821 and batch: 400, loss is 5.069548301696777 and perplexity is 159.10244480555184
At time: 154.2946298122406 and batch: 450, loss is 4.995840654373169 and perplexity is 147.7971394856517
At time: 154.9502308368683 and batch: 500, loss is 5.009285726547241 and perplexity is 149.79770140616097
At time: 155.5993459224701 and batch: 550, loss is 5.062158145904541 and perplexity is 157.93098691121932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.270576639378325 and perplexity of 194.5281026742423
Finished 19 epochs...
Completing Train Step...
At time: 157.32119274139404 and batch: 50, loss is 5.1070112609863285 and perplexity is 165.17594861437385
At time: 157.9747633934021 and batch: 100, loss is 5.09835527420044 and perplexity is 163.75235796540855
At time: 158.62002444267273 and batch: 150, loss is 5.07343020439148 and perplexity is 159.7212653382306
At time: 159.2614505290985 and batch: 200, loss is 5.021313457489014 and perplexity is 151.6103067673057
At time: 159.92033100128174 and batch: 250, loss is 5.005490980148315 and perplexity is 149.23033430615746
At time: 160.57624530792236 and batch: 300, loss is 5.004736518859863 and perplexity is 149.11778825701782
At time: 161.21606922149658 and batch: 350, loss is 5.054663028717041 and perplexity is 156.7517006219539
At time: 161.85639643669128 and batch: 400, loss is 5.065008134841919 and perplexity is 158.38173047878138
At time: 162.50698828697205 and batch: 450, loss is 4.987822580337524 and perplexity is 146.61682930964426
At time: 163.14914798736572 and batch: 500, loss is 5.003161926269531 and perplexity is 148.88317325253965
At time: 163.78620266914368 and batch: 550, loss is 5.05677300453186 and perplexity is 157.0827920938622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.270320811170213 and perplexity of 194.47834326350954
Finished 20 epochs...
Completing Train Step...
At time: 165.51317167282104 and batch: 50, loss is 5.102667436599732 and perplexity is 164.4600093835916
At time: 166.17419958114624 and batch: 100, loss is 5.09269232749939 and perplexity is 162.82765782506385
At time: 166.824054479599 and batch: 150, loss is 5.070113153457641 and perplexity is 159.19233948785273
At time: 167.46962189674377 and batch: 200, loss is 5.018096265792846 and perplexity is 151.12333111436345
At time: 168.11596870422363 and batch: 250, loss is 5.002669734954834 and perplexity is 148.80991227844677
At time: 168.76118636131287 and batch: 300, loss is 5.0009418392181395 and perplexity is 148.55300628274907
At time: 169.39768719673157 and batch: 350, loss is 5.050366039276123 and perplexity is 156.07958528994556
At time: 170.04279232025146 and batch: 400, loss is 5.060769786834717 and perplexity is 157.71187413193783
At time: 170.6800467967987 and batch: 450, loss is 4.984798746109009 and perplexity is 146.17415394845884
At time: 171.3250811100006 and batch: 500, loss is 4.99959566116333 and perplexity is 148.3531620288694
At time: 171.97102236747742 and batch: 550, loss is 5.0519772720336915 and perplexity is 156.33126853624893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.268760843479887 and perplexity of 194.1751998399985
Finished 21 epochs...
Completing Train Step...
At time: 173.70654201507568 and batch: 50, loss is 5.097782011032105 and perplexity is 163.65851167174083
At time: 174.36406922340393 and batch: 100, loss is 5.087836284637451 and perplexity is 162.03887646762692
At time: 175.03140234947205 and batch: 150, loss is 5.0647557258605955 and perplexity is 158.34175855237984
At time: 175.67385721206665 and batch: 200, loss is 5.014755191802979 and perplexity is 150.6192594228271
At time: 176.31231999397278 and batch: 250, loss is 4.998537826538086 and perplexity is 148.1963118926898
At time: 176.95224809646606 and batch: 300, loss is 4.997307176589966 and perplexity is 148.01404628473145
At time: 177.5960500240326 and batch: 350, loss is 5.045073747634888 and perplexity is 155.2557485204308
At time: 178.24136757850647 and batch: 400, loss is 5.0582402896881105 and perplexity is 157.31344651952958
At time: 178.89036631584167 and batch: 450, loss is 4.979941368103027 and perplexity is 145.4658524646964
At time: 179.53677344322205 and batch: 500, loss is 4.995407304763794 and perplexity is 147.73310552853604
At time: 180.1805920600891 and batch: 550, loss is 5.049465942382812 and perplexity is 155.93916174699484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.268399502368683 and perplexity of 194.10504903246868
Finished 22 epochs...
Completing Train Step...
At time: 181.90352249145508 and batch: 50, loss is 5.095183725357056 and perplexity is 163.2338320636562
At time: 182.5619616508484 and batch: 100, loss is 5.085037078857422 and perplexity is 161.58593054820395
At time: 183.20729660987854 and batch: 150, loss is 5.062412586212158 and perplexity is 157.97117603276067
At time: 183.85118293762207 and batch: 200, loss is 5.010401868820191 and perplexity is 149.96499029482428
At time: 184.4933488368988 and batch: 250, loss is 4.995667104721069 and perplexity is 147.77149156917048
At time: 185.13718724250793 and batch: 300, loss is 4.994264755249024 and perplexity is 147.5644095306158
At time: 185.779705286026 and batch: 350, loss is 5.043327198028565 and perplexity is 154.98482331499852
At time: 186.42256355285645 and batch: 400, loss is 5.055811328887939 and perplexity is 156.93180201200514
At time: 187.06463408470154 and batch: 450, loss is 4.977329330444336 and perplexity is 145.0863859861212
At time: 187.7114613056183 and batch: 500, loss is 4.9917844581604 and perplexity is 147.19885947903836
At time: 188.3655138015747 and batch: 550, loss is 5.045254077911377 and perplexity is 155.2837483570213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.26755085397274 and perplexity of 193.94039197182016
Finished 23 epochs...
Completing Train Step...
At time: 190.10518407821655 and batch: 50, loss is 5.0909934329986575 and perplexity is 162.55126565966057
At time: 190.76357579231262 and batch: 100, loss is 5.080743255615235 and perplexity is 160.89359656758273
At time: 191.40651535987854 and batch: 150, loss is 5.057898349761963 and perplexity is 157.25966396696978
At time: 192.06316256523132 and batch: 200, loss is 5.007791357040405 and perplexity is 149.57401546533686
At time: 192.70948815345764 and batch: 250, loss is 4.992086992263794 and perplexity is 147.2433988910173
At time: 193.36092686653137 and batch: 300, loss is 4.989434213638305 and perplexity is 146.8533123850283
At time: 194.0100610256195 and batch: 350, loss is 5.037442569732666 and perplexity is 154.07547345538507
At time: 194.6594603061676 and batch: 400, loss is 5.051058721542359 and perplexity is 156.18773630361758
At time: 195.31341075897217 and batch: 450, loss is 4.971813192367554 and perplexity is 144.2882727231736
At time: 195.95256328582764 and batch: 500, loss is 4.985669174194336 and perplexity is 146.30144342753658
At time: 196.59548020362854 and batch: 550, loss is 5.040391407012939 and perplexity is 154.5304875069654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.261345071995512 and perplexity of 192.7405669594926
Finished 24 epochs...
Completing Train Step...
At time: 198.3555624485016 and batch: 50, loss is 5.081145782470703 and perplexity is 160.95837359744644
At time: 199.01118540763855 and batch: 100, loss is 5.073760967254639 and perplexity is 159.77410393930305
At time: 199.6491084098816 and batch: 150, loss is 5.048552579879761 and perplexity is 155.79679778874555
At time: 200.28804421424866 and batch: 200, loss is 4.997763042449951 and perplexity is 148.0815362172402
At time: 200.93600702285767 and batch: 250, loss is 4.980967273712158 and perplexity is 145.6151632749803
At time: 201.5859091281891 and batch: 300, loss is 4.981079320907593 and perplexity is 145.63147995974035
At time: 202.2408082485199 and batch: 350, loss is 5.030903425216675 and perplexity is 153.07123866475763
At time: 202.89761114120483 and batch: 400, loss is 5.040718879699707 and perplexity is 154.5811003075994
At time: 203.54540967941284 and batch: 450, loss is 4.963149423599243 and perplexity is 143.04359209047698
At time: 204.19000029563904 and batch: 500, loss is 4.9762570190429685 and perplexity is 144.93089158435464
At time: 204.8377549648285 and batch: 550, loss is 5.032228803634643 and perplexity is 153.2742504849201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2551327969165555 and perplexity of 191.54692100597018
Finished 25 epochs...
Completing Train Step...
At time: 206.55554103851318 and batch: 50, loss is 5.073985500335693 and perplexity is 159.80998253894873
At time: 207.22207117080688 and batch: 100, loss is 5.067581262588501 and perplexity is 158.78979167567476
At time: 207.87176942825317 and batch: 150, loss is 5.0419156455993654 and perplexity is 154.76620844063652
At time: 208.51889419555664 and batch: 200, loss is 4.98992802619934 and perplexity is 146.92584830341343
At time: 209.16356444358826 and batch: 250, loss is 4.974810314178467 and perplexity is 144.72137095228493
At time: 209.80613684654236 and batch: 300, loss is 4.973726692199707 and perplexity is 144.56463263180663
At time: 210.4460310935974 and batch: 350, loss is 5.020295972824097 and perplexity is 151.45612405769967
At time: 211.0993528366089 and batch: 400, loss is 5.033584518432617 and perplexity is 153.48218757429706
At time: 211.75043153762817 and batch: 450, loss is 4.954743041992187 and perplexity is 141.84615318475144
At time: 212.3944640159607 and batch: 500, loss is 4.96937665939331 and perplexity is 143.9371375388296
At time: 213.04279232025146 and batch: 550, loss is 5.021808195114136 and perplexity is 151.6853326479524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.247626122007978 and perplexity of 190.1144239120167
Finished 26 epochs...
Completing Train Step...
At time: 214.84463691711426 and batch: 50, loss is 5.065052556991577 and perplexity is 158.38876629198765
At time: 215.50989603996277 and batch: 100, loss is 5.058955249786377 and perplexity is 157.42595957309092
At time: 216.1494755744934 and batch: 150, loss is 5.033125505447388 and perplexity is 153.41175342353066
At time: 216.80249190330505 and batch: 200, loss is 4.985371580123902 and perplexity is 146.25791146322507
At time: 217.44943141937256 and batch: 250, loss is 4.967634992599487 and perplexity is 143.68666518898286
At time: 218.09359431266785 and batch: 300, loss is 4.969309644699097 and perplexity is 143.9274919587729
At time: 218.7344434261322 and batch: 350, loss is 5.01369909286499 and perplexity is 150.46027454956484
At time: 219.39008045196533 and batch: 400, loss is 5.027995595932007 and perplexity is 152.62678015224256
At time: 220.04372358322144 and batch: 450, loss is 4.949195194244385 and perplexity is 141.06139120608177
At time: 220.69757175445557 and batch: 500, loss is 4.963277530670166 and perplexity is 143.06191815989845
At time: 221.35151886940002 and batch: 550, loss is 5.015595293045044 and perplexity is 150.74584801591902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.24387830369016 and perplexity of 189.40324311225737
Finished 27 epochs...
Completing Train Step...
At time: 223.08196330070496 and batch: 50, loss is 5.059301013946533 and perplexity is 157.48040123923568
At time: 223.74215936660767 and batch: 100, loss is 5.052846260070801 and perplexity is 156.46717758154782
At time: 224.38112902641296 and batch: 150, loss is 5.028967761993409 and perplexity is 152.77523087565334
At time: 225.03104424476624 and batch: 200, loss is 4.979266109466553 and perplexity is 145.36765854838373
At time: 225.6746163368225 and batch: 250, loss is 4.962396192550659 and perplexity is 142.93588778381957
At time: 226.31280398368835 and batch: 300, loss is 4.965556364059449 and perplexity is 143.38830418417524
At time: 226.95849871635437 and batch: 350, loss is 5.010381870269775 and perplexity is 149.9619912423937
At time: 227.6039595603943 and batch: 400, loss is 5.024151935577392 and perplexity is 152.04126063824884
At time: 228.25152492523193 and batch: 450, loss is 4.94370005607605 and perplexity is 140.2883652586064
At time: 228.90033197402954 and batch: 500, loss is 4.955916872024536 and perplexity is 142.01275422085305
At time: 229.54481744766235 and batch: 550, loss is 5.007732353210449 and perplexity is 149.56519028592476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2405570821559175 and perplexity of 188.77523643449805
Finished 28 epochs...
Completing Train Step...
At time: 231.3287537097931 and batch: 50, loss is 5.054703989028931 and perplexity is 156.75812135199726
At time: 231.9956178665161 and batch: 100, loss is 5.047027807235718 and perplexity is 155.55942410988357
At time: 232.64018535614014 and batch: 150, loss is 5.022986516952515 and perplexity is 151.8641721325161
At time: 233.29266571998596 and batch: 200, loss is 4.974286365509033 and perplexity is 144.6455642436821
At time: 233.93679666519165 and batch: 250, loss is 4.956547708511352 and perplexity is 142.10236931107124
At time: 234.5754268169403 and batch: 300, loss is 4.9594596576690675 and perplexity is 142.51676724573224
At time: 235.21875476837158 and batch: 350, loss is 5.004419775009155 and perplexity is 149.07056359401187
At time: 235.86159825325012 and batch: 400, loss is 5.018151082992554 and perplexity is 151.13161549924692
At time: 236.5041148662567 and batch: 450, loss is 4.938935737609864 and perplexity is 139.62157646775947
At time: 237.15404605865479 and batch: 500, loss is 4.950274105072022 and perplexity is 141.21366599910576
At time: 237.79228520393372 and batch: 550, loss is 5.00465497970581 and perplexity is 149.10562981441043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.236496296334774 and perplexity of 188.01021497641236
Finished 29 epochs...
Completing Train Step...
At time: 239.52662348747253 and batch: 50, loss is 5.050020608901978 and perplexity is 156.02567997120562
At time: 240.17901611328125 and batch: 100, loss is 5.041392011642456 and perplexity is 154.68518881268028
At time: 240.83188152313232 and batch: 150, loss is 5.0181890487670895 and perplexity is 151.13735343700813
At time: 241.4825792312622 and batch: 200, loss is 4.969821977615356 and perplexity is 144.00124964308944
At time: 242.14365410804749 and batch: 250, loss is 4.9528974914550785 and perplexity is 141.58461036004064
At time: 242.78485870361328 and batch: 300, loss is 4.954790687561035 and perplexity is 141.85291168641382
At time: 243.4353952407837 and batch: 350, loss is 5.00096869468689 and perplexity is 148.55699579693692
At time: 244.09264254570007 and batch: 400, loss is 5.014835653305053 and perplexity is 150.63137896225325
At time: 244.73108386993408 and batch: 450, loss is 4.93331974029541 and perplexity is 138.83965974548224
At time: 245.3803060054779 and batch: 500, loss is 4.9467356300354 and perplexity is 140.714867979754
At time: 246.02733969688416 and batch: 550, loss is 4.999899559020996 and perplexity is 148.3982530881791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.231884895487035 and perplexity of 187.14522046274632
Finished 30 epochs...
Completing Train Step...
At time: 247.7378740310669 and batch: 50, loss is 5.045159368515015 and perplexity is 155.2690422233651
At time: 248.39193964004517 and batch: 100, loss is 5.038421831130981 and perplexity is 154.22642751864748
At time: 249.03815031051636 and batch: 150, loss is 5.013534088134765 and perplexity is 150.43544994069848
At time: 249.67936062812805 and batch: 200, loss is 4.965947532653809 and perplexity is 143.44440415712913
At time: 250.32673358917236 and batch: 250, loss is 4.950050296783448 and perplexity is 141.18206474663828
At time: 250.96688079833984 and batch: 300, loss is 4.951421031951904 and perplexity is 141.3757206631075
At time: 251.61638402938843 and batch: 350, loss is 4.998504638671875 and perplexity is 148.19139365493115
At time: 252.27163410186768 and batch: 400, loss is 5.0106940269470215 and perplexity is 150.008810186336
At time: 252.92816925048828 and batch: 450, loss is 4.930646991729736 and perplexity is 138.46907170897092
At time: 253.5786383152008 and batch: 500, loss is 4.943384580612182 and perplexity is 140.24411470185063
At time: 254.22904205322266 and batch: 550, loss is 4.996070394515991 and perplexity is 147.83109832229042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.23609404868268 and perplexity of 187.93460351715376
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 255.95081853866577 and batch: 50, loss is 5.029781932830811 and perplexity is 152.89966666243026
At time: 256.6024935245514 and batch: 100, loss is 5.010053062438965 and perplexity is 149.91269067100052
At time: 257.2507848739624 and batch: 150, loss is 4.9754085350036625 and perplexity is 144.80797219098903
At time: 257.90039348602295 and batch: 200, loss is 4.924425945281983 and perplexity is 137.6103231093188
At time: 258.54195189476013 and batch: 250, loss is 4.8998360919952395 and perplexity is 134.2677703188925
At time: 259.1881308555603 and batch: 300, loss is 4.896570138931274 and perplexity is 133.82997338429254
At time: 259.8332221508026 and batch: 350, loss is 4.941519260406494 and perplexity is 139.98275835338796
At time: 260.4887933731079 and batch: 400, loss is 4.951795930862427 and perplexity is 141.42873220312188
At time: 261.1388370990753 and batch: 450, loss is 4.861473207473755 and perplexity is 129.21442160820976
At time: 261.78505516052246 and batch: 500, loss is 4.874785766601563 and perplexity is 130.94609717824574
At time: 262.4313852787018 and batch: 550, loss is 4.936726961135864 and perplexity is 139.3135239492138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.196704621010638 and perplexity of 180.67586432264724
Finished 32 epochs...
Completing Train Step...
At time: 264.17660093307495 and batch: 50, loss is 4.996083908081054 and perplexity is 147.83309606095426
At time: 264.84366631507874 and batch: 100, loss is 4.9913934707641605 and perplexity is 147.141317830004
At time: 265.49425292015076 and batch: 150, loss is 4.962144412994385 and perplexity is 142.89990397960094
At time: 266.14885330200195 and batch: 200, loss is 4.912039079666138 and perplexity is 135.91627615596312
At time: 266.80267786979675 and batch: 250, loss is 4.8927567672729495 and perplexity is 133.32060178562847
At time: 267.45521879196167 and batch: 300, loss is 4.889407081604004 and perplexity is 132.87476679685852
At time: 268.10791015625 and batch: 350, loss is 4.93662350654602 and perplexity is 139.2991120712342
At time: 268.7546362876892 and batch: 400, loss is 4.950322704315186 and perplexity is 141.22052904316575
At time: 269.39905190467834 and batch: 450, loss is 4.864099149703979 and perplexity is 129.554177108589
At time: 270.04675221443176 and batch: 500, loss is 4.878547430038452 and perplexity is 131.43959993792157
At time: 270.69270730018616 and batch: 550, loss is 4.937720241546631 and perplexity is 139.4519700900446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.192917844082447 and perplexity of 179.99297891153861
Finished 33 epochs...
Completing Train Step...
At time: 272.4234449863434 and batch: 50, loss is 4.989908714294433 and perplexity is 146.92301091280038
At time: 273.0936942100525 and batch: 100, loss is 4.9855985355377195 and perplexity is 146.29110925511213
At time: 273.7468559741974 and batch: 150, loss is 4.957173776626587 and perplexity is 142.1913629287226
At time: 274.39635133743286 and batch: 200, loss is 4.907864923477173 and perplexity is 135.3501228189281
At time: 275.04442286491394 and batch: 250, loss is 4.888740205764771 and perplexity is 132.78618536492922
At time: 275.69012546539307 and batch: 300, loss is 4.885428819656372 and perplexity is 132.34720625112146
At time: 276.3317942619324 and batch: 350, loss is 4.934856252670288 and perplexity is 139.05315257599065
At time: 276.9790425300598 and batch: 400, loss is 4.949977655410766 and perplexity is 141.17180946014062
At time: 277.6219460964203 and batch: 450, loss is 4.86496000289917 and perplexity is 129.66575225392293
At time: 278.2739534378052 and batch: 500, loss is 4.87963303565979 and perplexity is 131.58236898790057
At time: 278.922180891037 and batch: 550, loss is 4.938779296875 and perplexity is 139.5997356741735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.191318917781748 and perplexity of 179.70541336294025
Finished 34 epochs...
Completing Train Step...
At time: 280.71183943748474 and batch: 50, loss is 4.986504955291748 and perplexity is 146.42377052069273
At time: 281.37934947013855 and batch: 100, loss is 4.9822374534606935 and perplexity is 145.8002382207932
At time: 282.03069257736206 and batch: 150, loss is 4.954084625244141 and perplexity is 141.7527900411502
At time: 282.6797049045563 and batch: 200, loss is 4.904585628509522 and perplexity is 134.9069968098492
At time: 283.3329403400421 and batch: 250, loss is 4.885401439666748 and perplexity is 132.34358263559506
At time: 283.9831187725067 and batch: 300, loss is 4.883121852874756 and perplexity is 132.042237554243
At time: 284.6313416957855 and batch: 350, loss is 4.933500738143921 and perplexity is 138.86479169953031
At time: 285.2778444290161 and batch: 400, loss is 4.9486181640625 and perplexity is 140.98001800551148
At time: 285.9217507839203 and batch: 450, loss is 4.864427070617676 and perplexity is 129.59666759909913
At time: 286.57168531417847 and batch: 500, loss is 4.879467334747314 and perplexity is 131.5605674756087
At time: 287.23238921165466 and batch: 550, loss is 4.938318300247192 and perplexity is 139.53539549822634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.19285745823637 and perplexity of 179.98211021138022
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 288.97999477386475 and batch: 50, loss is 4.98090482711792 and perplexity is 145.6060703878771
At time: 289.6565625667572 and batch: 100, loss is 4.972706136703491 and perplexity is 144.417171660267
At time: 290.3150990009308 and batch: 150, loss is 4.941685047149658 and perplexity is 140.00596756283093
At time: 290.9565086364746 and batch: 200, loss is 4.8916401672363286 and perplexity is 133.17181907763938
At time: 291.5983283519745 and batch: 250, loss is 4.868277521133423 and perplexity is 130.09663508702258
At time: 292.24367809295654 and batch: 300, loss is 4.865800399780273 and perplexity is 129.7747687498484
At time: 292.89123606681824 and batch: 350, loss is 4.91711537361145 and perplexity is 136.6079812896093
At time: 293.5287353992462 and batch: 400, loss is 4.929400129318237 and perplexity is 138.29652742014997
At time: 294.1754846572876 and batch: 450, loss is 4.841022901535034 and perplexity is 126.59878361088334
At time: 294.83076214790344 and batch: 500, loss is 4.856760921478272 and perplexity is 128.60695869243781
At time: 295.47845673561096 and batch: 550, loss is 4.918295068740845 and perplexity is 136.76923215450225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.180490372028757 and perplexity of 177.76996304058318
Finished 36 epochs...
Completing Train Step...
At time: 297.1929979324341 and batch: 50, loss is 4.971076164245606 and perplexity is 144.1819673883321
At time: 297.8476490974426 and batch: 100, loss is 4.96676999092102 and perplexity is 143.5624297221111
At time: 298.49885082244873 and batch: 150, loss is 4.937417640686035 and perplexity is 139.4097781878584
At time: 299.1445343494415 and batch: 200, loss is 4.888010377883911 and perplexity is 132.68930966025206
At time: 299.7947196960449 and batch: 250, loss is 4.866909236907959 and perplexity is 129.91874764149333
At time: 300.44623613357544 and batch: 300, loss is 4.86448371887207 and perplexity is 129.6040092320373
At time: 301.0972123146057 and batch: 350, loss is 4.916339855194092 and perplexity is 136.50208035353197
At time: 301.74091148376465 and batch: 400, loss is 4.929103546142578 and perplexity is 138.2555170786543
At time: 302.3901081085205 and batch: 450, loss is 4.8419262981414795 and perplexity is 126.71320419818089
At time: 303.0355067253113 and batch: 500, loss is 4.858689527511597 and perplexity is 128.8552301808907
At time: 303.67790031433105 and batch: 550, loss is 4.919709129333496 and perplexity is 136.96276894010083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.179408945935838 and perplexity of 177.57782187597374
Finished 37 epochs...
Completing Train Step...
At time: 305.4480218887329 and batch: 50, loss is 4.968164129257202 and perplexity is 143.76271518940624
At time: 306.1115849018097 and batch: 100, loss is 4.9644601535797115 and perplexity is 143.23120654422814
At time: 306.754762172699 and batch: 150, loss is 4.9359852313995365 and perplexity is 139.21022927892804
At time: 307.39910197257996 and batch: 200, loss is 4.886586675643921 and perplexity is 132.50053400491387
At time: 308.05341243743896 and batch: 250, loss is 4.86653434753418 and perplexity is 129.8700516119302
At time: 308.70728754997253 and batch: 300, loss is 4.864136533737183 and perplexity is 129.559020456779
At time: 309.3553137779236 and batch: 350, loss is 4.916129493713379 and perplexity is 136.4733685938184
At time: 310.0050411224365 and batch: 400, loss is 4.928853693008423 and perplexity is 138.2209778194488
At time: 310.65402126312256 and batch: 450, loss is 4.8423333263397215 and perplexity is 126.76479054321548
At time: 311.2932207584381 and batch: 500, loss is 4.859644889831543 and perplexity is 128.97839243545437
At time: 311.9364216327667 and batch: 550, loss is 4.920273599624633 and perplexity is 137.0401021782638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.178813528507314 and perplexity of 177.47212041727772
Finished 38 epochs...
Completing Train Step...
At time: 313.6916275024414 and batch: 50, loss is 4.966174297332763 and perplexity is 143.47693596977925
At time: 314.34527015686035 and batch: 100, loss is 4.96283127784729 and perplexity is 142.99809061774315
At time: 314.9864823818207 and batch: 150, loss is 4.934976968765259 and perplexity is 139.06993954277115
At time: 315.6238377094269 and batch: 200, loss is 4.885625972747802 and perplexity is 132.37330148426943
At time: 316.27029943466187 and batch: 250, loss is 4.8662854766845705 and perplexity is 129.8377347633743
At time: 316.931205034256 and batch: 300, loss is 4.863896446228027 and perplexity is 129.52791868799144
At time: 317.5673131942749 and batch: 350, loss is 4.915958318710327 and perplexity is 136.4500097638138
At time: 318.22283482551575 and batch: 400, loss is 4.928524961471558 and perplexity is 138.17554769255
At time: 318.878648519516 and batch: 450, loss is 4.842558784484863 and perplexity is 126.7933739198171
At time: 319.5262494087219 and batch: 500, loss is 4.860224046707153 and perplexity is 129.05311279355453
At time: 320.17385601997375 and batch: 550, loss is 4.92057207107544 and perplexity is 137.08101084110984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.178414202750997 and perplexity of 177.40126537662982
Finished 39 epochs...
Completing Train Step...
At time: 321.9639015197754 and batch: 50, loss is 4.964571399688721 and perplexity is 143.24714134497074
At time: 322.6319274902344 and batch: 100, loss is 4.96147274017334 and perplexity is 142.80395422499637
At time: 323.27953696250916 and batch: 150, loss is 4.934184551239014 and perplexity is 138.95978173655598
At time: 323.93221402168274 and batch: 200, loss is 4.884882583618164 and perplexity is 132.2749331783864
At time: 324.5807840824127 and batch: 250, loss is 4.866079559326172 and perplexity is 129.81100167250958
At time: 325.22789478302 and batch: 300, loss is 4.863700857162476 and perplexity is 129.5025869208015
At time: 325.86585092544556 and batch: 350, loss is 4.91578875541687 and perplexity is 136.42687481224075
At time: 326.5130834579468 and batch: 400, loss is 4.928228693008423 and perplexity is 138.134616698973
At time: 327.1679949760437 and batch: 450, loss is 4.842717189788818 and perplexity is 126.8134602536012
At time: 327.8158483505249 and batch: 500, loss is 4.860616655349731 and perplexity is 129.10379010851383
At time: 328.46404910087585 and batch: 550, loss is 4.920811147689819 and perplexity is 137.11378762300237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.178121688518118 and perplexity of 177.34938057046844
Finished 40 epochs...
Completing Train Step...
At time: 330.18769669532776 and batch: 50, loss is 4.963252115249634 and perplexity is 143.05828222729082
At time: 330.8550913333893 and batch: 100, loss is 4.9603665256500244 and perplexity is 142.64606975999027
At time: 331.5042142868042 and batch: 150, loss is 4.933501529693603 and perplexity is 138.86490161795564
At time: 332.15296363830566 and batch: 200, loss is 4.884273290634155 and perplexity is 132.19436353739164
At time: 332.81034326553345 and batch: 250, loss is 4.865894317626953 and perplexity is 129.7869574890439
At time: 333.4644396305084 and batch: 300, loss is 4.863518695831299 and perplexity is 129.47899870567275
At time: 334.1032838821411 and batch: 350, loss is 4.915599851608277 and perplexity is 136.40110569001345
At time: 334.75774240493774 and batch: 400, loss is 4.927955169677734 and perplexity is 138.09683882533048
At time: 335.41158962249756 and batch: 450, loss is 4.842818422317505 and perplexity is 126.82629855066989
At time: 336.0548405647278 and batch: 500, loss is 4.860922641754151 and perplexity is 129.1433001575067
At time: 336.7008891105652 and batch: 550, loss is 4.920926809310913 and perplexity is 137.12964734311595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.177852549451463 and perplexity of 177.301655346359
Finished 41 epochs...
Completing Train Step...
At time: 338.4538686275482 and batch: 50, loss is 4.962087841033935 and perplexity is 142.89182008054792
At time: 339.12465357780457 and batch: 100, loss is 4.959383459091186 and perplexity is 142.50590808447546
At time: 339.7809147834778 and batch: 150, loss is 4.932884359359742 and perplexity is 138.77922476158895
At time: 340.4316625595093 and batch: 200, loss is 4.883716249465943 and perplexity is 132.12074634050393
At time: 341.0885012149811 and batch: 250, loss is 4.865690879821777 and perplexity is 129.7605566008323
At time: 341.7382972240448 and batch: 300, loss is 4.863322076797485 and perplexity is 129.45354317265117
At time: 342.3853132724762 and batch: 350, loss is 4.915388841629028 and perplexity is 136.37232673196274
At time: 343.0315980911255 and batch: 400, loss is 4.927688541412354 and perplexity is 138.06002321299954
At time: 343.67721819877625 and batch: 450, loss is 4.842884483337403 and perplexity is 126.83467710204681
At time: 344.31759119033813 and batch: 500, loss is 4.86116810798645 and perplexity is 129.17500436782043
At time: 344.96359062194824 and batch: 550, loss is 4.920995569229126 and perplexity is 137.13907669062866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.17758860486619 and perplexity of 177.25486370994167
Finished 42 epochs...
Completing Train Step...
At time: 346.66085147857666 and batch: 50, loss is 4.961036243438721 and perplexity is 142.74163436748307
At time: 347.3164315223694 and batch: 100, loss is 4.958479642868042 and perplexity is 142.3771671207045
At time: 347.9549124240875 and batch: 150, loss is 4.932302856445313 and perplexity is 138.69854769717534
At time: 348.5925757884979 and batch: 200, loss is 4.883185997009277 and perplexity is 132.0507075609389
At time: 349.24360847473145 and batch: 250, loss is 4.865457830429077 and perplexity is 129.73031950542523
At time: 349.89100337028503 and batch: 300, loss is 4.8631015872955325 and perplexity is 129.42500317189197
At time: 350.53751969337463 and batch: 350, loss is 4.91515622138977 and perplexity is 136.3406074581058
At time: 351.1877770423889 and batch: 400, loss is 4.927430076599121 and perplexity is 138.0243441659726
At time: 351.83933568000793 and batch: 450, loss is 4.842939176559448 and perplexity is 126.84161428891144
At time: 352.48260164260864 and batch: 500, loss is 4.861349611282349 and perplexity is 129.19845218472457
At time: 353.1319410800934 and batch: 550, loss is 4.921063871383667 and perplexity is 137.14844390493522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.177339269759807 and perplexity of 177.21067335897357
Finished 43 epochs...
Completing Train Step...
At time: 354.91657733917236 and batch: 50, loss is 4.960074291229248 and perplexity is 142.6043897588801
At time: 355.57404112815857 and batch: 100, loss is 4.95765308380127 and perplexity is 142.25953260495461
At time: 356.2117865085602 and batch: 150, loss is 4.931758050918579 and perplexity is 138.62300454187275
At time: 356.8634145259857 and batch: 200, loss is 4.882684688568116 and perplexity is 131.98452601664667
At time: 357.5103690624237 and batch: 250, loss is 4.8652109527587895 and perplexity is 129.69829593949441
At time: 358.1653687953949 and batch: 300, loss is 4.862867193222046 and perplexity is 129.3946702732522
At time: 358.80265974998474 and batch: 350, loss is 4.914913473129272 and perplexity is 136.30751502953734
At time: 359.4530439376831 and batch: 400, loss is 4.927190523147583 and perplexity is 137.9912839179378
At time: 360.11119771003723 and batch: 450, loss is 4.84299036026001 and perplexity is 126.84810667826677
At time: 360.7667934894562 and batch: 500, loss is 4.861492614746094 and perplexity is 129.21692933201405
At time: 361.4178500175476 and batch: 550, loss is 4.921055450439453 and perplexity is 137.1472889904028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.177095129134807 and perplexity of 177.16741431528288
Finished 44 epochs...
Completing Train Step...
At time: 363.2135684490204 and batch: 50, loss is 4.959181327819824 and perplexity is 142.47710609508712
At time: 363.8762595653534 and batch: 100, loss is 4.9568740749359135 and perplexity is 142.14875432210835
At time: 364.51513051986694 and batch: 150, loss is 4.931245174407959 and perplexity is 138.55192628775313
At time: 365.1724364757538 and batch: 200, loss is 4.882196884155274 and perplexity is 131.9201590829424
At time: 365.8242754936218 and batch: 250, loss is 4.8649492263793945 and perplexity is 129.66435491590886
At time: 366.4773232936859 and batch: 300, loss is 4.862610626220703 and perplexity is 129.36147612915352
At time: 367.1148991584778 and batch: 350, loss is 4.91465350151062 and perplexity is 136.27208355000977
At time: 367.7556664943695 and batch: 400, loss is 4.9269787788391115 and perplexity is 137.9620681421957
At time: 368.4049310684204 and batch: 450, loss is 4.843041324615479 and perplexity is 126.85457157500437
At time: 369.05276918411255 and batch: 500, loss is 4.861618356704712 and perplexity is 129.2331783433647
At time: 369.6933810710907 and batch: 550, loss is 4.921041860580444 and perplexity is 137.1454251907464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.17686170212766 and perplexity of 177.12606348238353
Finished 45 epochs...
Completing Train Step...
At time: 371.41803073883057 and batch: 50, loss is 4.9583480930328365 and perplexity is 142.35843865972274
At time: 372.07843136787415 and batch: 100, loss is 4.956128568649292 and perplexity is 142.0428210240019
At time: 372.72664046287537 and batch: 150, loss is 4.930749101638794 and perplexity is 138.48321149518432
At time: 373.3727037906647 and batch: 200, loss is 4.881717338562011 and perplexity is 131.8569125180095
At time: 374.02164912223816 and batch: 250, loss is 4.864654493331909 and perplexity is 129.62614417669468
At time: 374.67715096473694 and batch: 300, loss is 4.862327842712403 and perplexity is 129.32490000890036
At time: 375.3271508216858 and batch: 350, loss is 4.914374837875366 and perplexity is 136.2341147663294
At time: 375.9768257141113 and batch: 400, loss is 4.926789579391479 and perplexity is 137.93596826422808
At time: 376.63011264801025 and batch: 450, loss is 4.84309160232544 and perplexity is 126.86094969269843
At time: 377.2718276977539 and batch: 500, loss is 4.861724004745484 and perplexity is 129.2468322967034
At time: 377.92074966430664 and batch: 550, loss is 4.921065998077393 and perplexity is 137.14873557798055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.176637690118018 and perplexity of 177.08638956082632
Finished 46 epochs...
Completing Train Step...
At time: 379.65292954444885 and batch: 50, loss is 4.957567720413208 and perplexity is 142.24738936756876
At time: 380.30759716033936 and batch: 100, loss is 4.95543867111206 and perplexity is 141.94485982707573
At time: 380.94724774360657 and batch: 150, loss is 4.930276756286621 and perplexity is 138.4178150399523
At time: 381.5952389240265 and batch: 200, loss is 4.8812594223022465 and perplexity is 131.79654691605526
At time: 382.2415955066681 and batch: 250, loss is 4.864350786209107 and perplexity is 129.58678177103081
At time: 382.8908932209015 and batch: 300, loss is 4.862065076828003 and perplexity is 129.29092230146128
At time: 383.5481057167053 and batch: 350, loss is 4.914116296768189 and perplexity is 136.19889720025284
At time: 384.2111475467682 and batch: 400, loss is 4.926612615585327 and perplexity is 137.91156074996582
At time: 384.8510437011719 and batch: 450, loss is 4.843120317459107 and perplexity is 126.86459257412868
At time: 385.48813581466675 and batch: 500, loss is 4.861823215484619 and perplexity is 129.25965560656093
At time: 386.1360728740692 and batch: 550, loss is 4.921053419113159 and perplexity is 137.1470103997915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.176416924659242 and perplexity of 177.0472993178384
Finished 47 epochs...
Completing Train Step...
At time: 387.8682973384857 and batch: 50, loss is 4.956843547821045 and perplexity is 142.14441499699058
At time: 388.52934098243713 and batch: 100, loss is 4.954783916473389 and perplexity is 141.85195119116773
At time: 389.18198680877686 and batch: 150, loss is 4.9298289775848385 and perplexity is 138.35584836514414
At time: 389.827410697937 and batch: 200, loss is 4.880817928314209 and perplexity is 131.73837237574756
At time: 390.481232881546 and batch: 250, loss is 4.864044208526611 and perplexity is 129.54705944509382
At time: 391.1306800842285 and batch: 300, loss is 4.861818199157715 and perplexity is 129.2590071994992
At time: 391.7709758281708 and batch: 350, loss is 4.9138689136505125 and perplexity is 136.16520805967352
At time: 392.42456221580505 and batch: 400, loss is 4.926435480117798 and perplexity is 137.88713388467062
At time: 393.07680559158325 and batch: 450, loss is 4.843120098114014 and perplexity is 126.86456474700586
At time: 393.72008061408997 and batch: 500, loss is 4.861897230148315 and perplexity is 129.26922307056213
At time: 394.3712434768677 and batch: 550, loss is 4.92104941368103 and perplexity is 137.1464610678498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.176196483855552 and perplexity of 177.00827517030086
Finished 48 epochs...
Completing Train Step...
At time: 396.0935637950897 and batch: 50, loss is 4.956164569854736 and perplexity is 142.0479348288345
At time: 396.7595889568329 and batch: 100, loss is 4.954153509140014 and perplexity is 141.76255486189476
At time: 397.41053676605225 and batch: 150, loss is 4.9293918800354 and perplexity is 138.29538657768543
At time: 398.0743064880371 and batch: 200, loss is 4.880380783081055 and perplexity is 131.68079615974108
At time: 398.7331326007843 and batch: 250, loss is 4.86372389793396 and perplexity is 129.50557079468865
At time: 399.37798738479614 and batch: 300, loss is 4.8615671730041505 and perplexity is 129.22656388033954
At time: 400.0218098163605 and batch: 350, loss is 4.913611507415771 and perplexity is 136.13016279679218
At time: 400.6684539318085 and batch: 400, loss is 4.926253499984742 and perplexity is 137.86204344874923
At time: 401.31444907188416 and batch: 450, loss is 4.843091278076172 and perplexity is 126.8609085581351
At time: 401.9599268436432 and batch: 500, loss is 4.8619499778747555 and perplexity is 129.27604190801526
At time: 402.6060883998871 and batch: 550, loss is 4.921053915023804 and perplexity is 137.14707841247076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.17596890063996 and perplexity of 176.9679956414974
Finished 49 epochs...
Completing Train Step...
At time: 404.34589099884033 and batch: 50, loss is 4.955505104064941 and perplexity is 141.95428995649254
At time: 405.01180696487427 and batch: 100, loss is 4.953549709320068 and perplexity is 141.67698449308043
At time: 405.6644711494446 and batch: 150, loss is 4.928971691131592 and perplexity is 138.2372885977157
At time: 406.31566286087036 and batch: 200, loss is 4.879948568344116 and perplexity is 131.62389407691424
At time: 406.9602584838867 and batch: 250, loss is 4.863395156860352 and perplexity is 129.463003991419
At time: 407.60685181617737 and batch: 300, loss is 4.861314687728882 and perplexity is 129.19394019445969
At time: 408.24574160575867 and batch: 350, loss is 4.91335244178772 and perplexity is 136.09490071846398
At time: 408.887478351593 and batch: 400, loss is 4.926073846817016 and perplexity is 137.83727832056803
At time: 409.5285429954529 and batch: 450, loss is 4.84303837776184 and perplexity is 126.85419775369937
At time: 410.1753749847412 and batch: 500, loss is 4.861979818344116 and perplexity is 129.27989962334058
At time: 410.8218502998352 and batch: 550, loss is 4.921044864654541 and perplexity is 137.1458371863846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1757497584566154 and perplexity of 176.9292187375333
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fec840548d0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 4.940696979226176, 'anneal': 6.0578549803630555, 'dropout': 0.4929960016803847, 'seq_len': 35, 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.945195198059082 and batch: 50, loss is 6.732200393676758 and perplexity is 838.9913476134288
At time: 1.6088664531707764 and batch: 100, loss is 5.976384239196777 and perplexity is 394.0131321989906
At time: 2.2549188137054443 and batch: 150, loss is 5.711531896591186 and perplexity is 302.3338579257059
At time: 2.898905038833618 and batch: 200, loss is 5.51894642829895 and perplexity is 249.37216727938517
At time: 3.5456244945526123 and batch: 250, loss is 5.392964630126953 and perplexity is 219.8542067601015
At time: 4.205683469772339 and batch: 300, loss is 5.344573554992675 and perplexity is 209.46853873714028
At time: 4.859798431396484 and batch: 350, loss is 5.343214817047119 and perplexity is 209.1841191547547
At time: 5.511968612670898 and batch: 400, loss is 5.30702877998352 and perplexity is 201.74989364417902
At time: 6.170274496078491 and batch: 450, loss is 5.191138391494751 and perplexity is 179.67297474001268
At time: 6.827685832977295 and batch: 500, loss is 5.194174613952637 and perplexity is 180.2193308706484
At time: 7.486351251602173 and batch: 550, loss is 5.211376705169678 and perplexity is 183.3462983204744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.039990364237035 and perplexity of 154.4685265966119
Finished 1 epochs...
Completing Train Step...
At time: 9.225880861282349 and batch: 50, loss is 4.924525861740112 and perplexity is 137.6240733323313
At time: 9.869846105575562 and batch: 100, loss is 4.835950078964234 and perplexity is 125.95819660975823
At time: 10.508603811264038 and batch: 150, loss is 4.763642902374268 and perplexity is 117.17199550343217
At time: 11.156111001968384 and batch: 200, loss is 4.656896848678588 and perplexity is 105.30878549690064
At time: 11.79572319984436 and batch: 250, loss is 4.6051484489440915 and perplexity is 99.9978263192248
At time: 12.458792448043823 and batch: 300, loss is 4.566707105636596 and perplexity is 96.2267230651935
At time: 13.094457387924194 and batch: 350, loss is 4.588155736923218 and perplexity is 98.31294792306582
At time: 13.731330394744873 and batch: 400, loss is 4.562306842803955 and perplexity is 95.80423041324227
At time: 14.369704961776733 and batch: 450, loss is 4.472287263870239 and perplexity is 87.55675957079846
At time: 15.027971029281616 and batch: 500, loss is 4.4925250148773195 and perplexity is 89.34676320047238
At time: 15.672318935394287 and batch: 550, loss is 4.524153881072998 and perplexity is 92.21786552773513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6317736037234045 and perplexity of 102.69604476938402
Finished 2 epochs...
Completing Train Step...
At time: 17.410121202468872 and batch: 50, loss is 4.434430446624756 and perplexity is 84.30409552979147
At time: 18.045660495758057 and batch: 100, loss is 4.393176126480102 and perplexity is 80.89695032290969
At time: 18.683529138565063 and batch: 150, loss is 4.3703350162506105 and perplexity is 79.07011703695257
At time: 19.32272958755493 and batch: 200, loss is 4.314876976013184 and perplexity is 74.80442013984391
At time: 19.96019411087036 and batch: 250, loss is 4.28903865814209 and perplexity is 72.89635648544687
At time: 20.60221242904663 and batch: 300, loss is 4.256635141372681 and perplexity is 70.57211828631257
At time: 21.24703884124756 and batch: 350, loss is 4.292671403884888 and perplexity is 73.1616519979515
At time: 21.90130114555359 and batch: 400, loss is 4.282331099510193 and perplexity is 72.409036092422
At time: 22.540300369262695 and batch: 450, loss is 4.207235040664673 and perplexity is 67.17055894981652
At time: 23.207809448242188 and batch: 500, loss is 4.239974994659423 and perplexity is 69.40611629348031
At time: 23.869508504867554 and batch: 550, loss is 4.2917477416992185 and perplexity is 73.09410654595203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5138114766871675 and perplexity of 91.26902616875081
Finished 3 epochs...
Completing Train Step...
At time: 25.625145196914673 and batch: 50, loss is 4.219310889244079 and perplexity is 67.98661783357711
At time: 26.27313256263733 and batch: 100, loss is 4.1849205732345585 and perplexity is 65.68828330166123
At time: 26.92272114753723 and batch: 150, loss is 4.179555563926697 and perplexity is 65.33680872556667
At time: 27.565643072128296 and batch: 200, loss is 4.140482182502747 and perplexity is 62.83311117282811
At time: 28.20636534690857 and batch: 250, loss is 4.1206869745254515 and perplexity is 61.60154642494685
At time: 28.854084730148315 and batch: 300, loss is 4.088845090866089 and perplexity is 59.67093738320913
At time: 29.503884315490723 and batch: 350, loss is 4.126345744132996 and perplexity is 61.951123538971714
At time: 30.16219401359558 and batch: 400, loss is 4.12188337802887 and perplexity is 61.67529083614524
At time: 30.805933713912964 and batch: 450, loss is 4.051509733200073 and perplexity is 57.48417733772793
At time: 31.45438861846924 and batch: 500, loss is 4.087797203063965 and perplexity is 59.6084416857422
At time: 32.106088638305664 and batch: 550, loss is 4.1483129167556765 and perplexity is 63.32707208286214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4699109665890955 and perplexity of 87.34894569265151
Finished 4 epochs...
Completing Train Step...
At time: 33.81966233253479 and batch: 50, loss is 4.0780564880371095 and perplexity is 59.03063155262813
At time: 34.48002552986145 and batch: 100, loss is 4.052809548377991 and perplexity is 57.558944725287446
At time: 35.13341236114502 and batch: 150, loss is 4.052208170890808 and perplexity is 57.52434047789425
At time: 35.7831449508667 and batch: 200, loss is 4.023500962257385 and perplexity is 55.89645505159273
At time: 36.4209668636322 and batch: 250, loss is 4.004625687599182 and perplexity is 54.851289038301864
At time: 37.06496524810791 and batch: 300, loss is 3.9728593111038206 and perplexity is 53.13624689537322
At time: 37.721452713012695 and batch: 350, loss is 4.012075252532959 and perplexity is 55.261433078244636
At time: 38.37119197845459 and batch: 400, loss is 4.009026594161988 and perplexity is 55.09321639558588
At time: 39.01818823814392 and batch: 450, loss is 3.944026255607605 and perplexity is 51.62604305888793
At time: 39.67685294151306 and batch: 500, loss is 3.9794303703308107 and perplexity is 53.48655801788883
At time: 40.31648850440979 and batch: 550, loss is 4.044822015762329 and perplexity is 57.10102204777766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4522692091921545 and perplexity of 85.82147007733245
Finished 5 epochs...
Completing Train Step...
At time: 42.02613878250122 and batch: 50, loss is 3.9777357244491576 and perplexity is 53.39599400126258
At time: 42.6933228969574 and batch: 100, loss is 3.9552398490905762 and perplexity is 52.208214535622425
At time: 43.33929681777954 and batch: 150, loss is 3.9573959875106812 and perplexity is 52.320904116337836
At time: 43.9862859249115 and batch: 200, loss is 3.936311612129211 and perplexity is 51.22929888014179
At time: 44.623149156570435 and batch: 250, loss is 3.9190204524993897 and perplexity is 50.35109932040898
At time: 45.267149925231934 and batch: 300, loss is 3.8866411066055297 and perplexity is 48.74687567175307
At time: 45.90705227851868 and batch: 350, loss is 3.927843451499939 and perplexity is 50.79731259439419
At time: 46.5521981716156 and batch: 400, loss is 3.9251104640960692 and perplexity is 50.65867371437335
At time: 47.190731048583984 and batch: 450, loss is 3.8618661308288575 and perplexity is 47.55401062350044
At time: 47.826162576675415 and batch: 500, loss is 3.8969964361190796 and perplexity is 49.25428831035902
At time: 48.48393774032593 and batch: 550, loss is 3.9646008253097533 and perplexity is 52.69922899212044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.447404252721908 and perplexity of 85.4049663189977
Finished 6 epochs...
Completing Train Step...
At time: 50.23551821708679 and batch: 50, loss is 3.8977048635482787 and perplexity is 49.289193761733515
At time: 50.886356830596924 and batch: 100, loss is 3.8826938343048094 and perplexity is 48.5548377420122
At time: 51.54519081115723 and batch: 150, loss is 3.8815329885482788 and perplexity is 48.49850576735312
At time: 52.201740026474 and batch: 200, loss is 3.8660991096496584 and perplexity is 47.755732384078
At time: 52.85438632965088 and batch: 250, loss is 3.8493155336380003 and perplexity is 46.96090906602763
At time: 53.50275278091431 and batch: 300, loss is 3.8182048273086546 and perplexity is 45.522414330595616
At time: 54.14714479446411 and batch: 350, loss is 3.863328070640564 and perplexity is 47.623582567443414
At time: 54.78638935089111 and batch: 400, loss is 3.858016996383667 and perplexity is 47.371320667861724
At time: 55.422752380371094 and batch: 450, loss is 3.7947106313705445 and perplexity is 44.46536766121472
At time: 56.078457832336426 and batch: 500, loss is 3.8310660791397093 and perplexity is 46.111670730259235
At time: 56.72108244895935 and batch: 550, loss is 3.897757344245911 and perplexity is 49.291780560885776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.451857221887467 and perplexity of 85.78612000358179
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 58.384299993515015 and batch: 50, loss is 3.8561727714538576 and perplexity is 47.284037806680956
At time: 59.054364919662476 and batch: 100, loss is 3.822940664291382 and perplexity is 45.738512362561515
At time: 59.6937460899353 and batch: 150, loss is 3.8064484786987305 and perplexity is 44.9903705322216
At time: 60.36002826690674 and batch: 200, loss is 3.781192831993103 and perplexity is 43.8683380961525
At time: 61.000142097473145 and batch: 250, loss is 3.7396328401565553 and perplexity is 42.08253631070955
At time: 61.634974241256714 and batch: 300, loss is 3.6932987451553343 and perplexity is 40.177162820353985
At time: 62.28539562225342 and batch: 350, loss is 3.7273684120178223 and perplexity is 41.56957011659646
At time: 62.92925548553467 and batch: 400, loss is 3.700231409072876 and perplexity is 40.45666531633302
At time: 63.57279396057129 and batch: 450, loss is 3.6177559518814086 and perplexity is 37.25387446431237
At time: 64.221670627594 and batch: 500, loss is 3.625623970031738 and perplexity is 37.548144768733046
At time: 64.86946892738342 and batch: 550, loss is 3.666942391395569 and perplexity is 39.13207219079532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.367791358460772 and perplexity of 78.86924530059314
Finished 8 epochs...
Completing Train Step...
At time: 66.55987763404846 and batch: 50, loss is 3.766262860298157 and perplexity is 43.218250023892395
At time: 67.20833158493042 and batch: 100, loss is 3.7450019550323486 and perplexity is 42.30908993439641
At time: 67.85339546203613 and batch: 150, loss is 3.7398766136169432 and perplexity is 42.092796166697404
At time: 68.5009253025055 and batch: 200, loss is 3.7222207307815554 and perplexity is 41.35613304482779
At time: 69.16832756996155 and batch: 250, loss is 3.687977032661438 and perplexity is 39.9639194242914
At time: 69.80635523796082 and batch: 300, loss is 3.647674379348755 and perplexity is 38.38529253584418
At time: 70.45302772521973 and batch: 350, loss is 3.6850460243225096 and perplexity is 39.846956336828846
At time: 71.10766673088074 and batch: 400, loss is 3.6651968288421632 and perplexity is 39.06382429375782
At time: 71.75345706939697 and batch: 450, loss is 3.5911727380752563 and perplexity is 36.27659393330695
At time: 72.40944647789001 and batch: 500, loss is 3.60955349445343 and perplexity is 36.94955095156679
At time: 73.04670071601868 and batch: 550, loss is 3.661379089355469 and perplexity is 38.91497310756962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363387087558178 and perplexity of 78.52264759376726
Finished 9 epochs...
Completing Train Step...
At time: 74.73444867134094 and batch: 50, loss is 3.732598180770874 and perplexity is 41.78753882163589
At time: 75.38469743728638 and batch: 100, loss is 3.7113967323303223 and perplexity is 40.910908227996075
At time: 76.03153443336487 and batch: 150, loss is 3.708322286605835 and perplexity is 40.78532301249559
At time: 76.67078971862793 and batch: 200, loss is 3.693066339492798 and perplexity is 40.16782650515817
At time: 77.31892490386963 and batch: 250, loss is 3.660492196083069 and perplexity is 38.88047498006478
At time: 77.97913193702698 and batch: 300, loss is 3.6226120615005493 and perplexity is 37.43522333105929
At time: 78.62588405609131 and batch: 350, loss is 3.661599011421204 and perplexity is 38.9235323099877
At time: 79.2646758556366 and batch: 400, loss is 3.6450884628295896 and perplexity is 38.286159603767594
At time: 79.93286347389221 and batch: 450, loss is 3.5753746795654298 and perplexity is 35.7079973664749
At time: 80.58869433403015 and batch: 500, loss is 3.598126516342163 and perplexity is 36.5297324382724
At time: 81.2246253490448 and batch: 550, loss is 3.653988037109375 and perplexity is 38.62841081114618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.362574151221742 and perplexity of 78.45883961972952
Finished 10 epochs...
Completing Train Step...
At time: 82.99690771102905 and batch: 50, loss is 3.707569818496704 and perplexity is 40.75464490120579
At time: 83.65865778923035 and batch: 100, loss is 3.6866246175765993 and perplexity is 39.90990814787753
At time: 84.31095504760742 and batch: 150, loss is 3.685078110694885 and perplexity is 39.848234901620046
At time: 84.95135307312012 and batch: 200, loss is 3.6715351247787478 and perplexity is 39.312208707959044
At time: 85.58667016029358 and batch: 250, loss is 3.6400336742401125 and perplexity is 38.09311946065268
At time: 86.23361086845398 and batch: 300, loss is 3.603592972755432 and perplexity is 36.72996741770769
At time: 86.88479828834534 and batch: 350, loss is 3.6438016223907472 and perplexity is 38.23692311189036
At time: 87.52745079994202 and batch: 400, loss is 3.629302492141724 and perplexity is 37.68652080306814
At time: 88.16625332832336 and batch: 450, loss is 3.5623865365982055 and perplexity is 35.24721561870675
At time: 88.84355330467224 and batch: 500, loss is 3.5874707174301146 and perplexity is 36.14254551173643
At time: 89.50816297531128 and batch: 550, loss is 3.6454858684539797 and perplexity is 38.30137776262111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363098469186337 and perplexity of 78.49998778524191
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 91.24086689949036 and batch: 50, loss is 3.7025783681869506 and perplexity is 40.551726964988596
At time: 91.91520118713379 and batch: 100, loss is 3.681767325401306 and perplexity is 39.716524104927124
At time: 92.58427214622498 and batch: 150, loss is 3.678479595184326 and perplexity is 39.58616130479864
At time: 93.2347104549408 and batch: 200, loss is 3.661445736885071 and perplexity is 38.917566780821765
At time: 93.88151860237122 and batch: 250, loss is 3.624248003959656 and perplexity is 37.49651532378968
At time: 94.526526927948 and batch: 300, loss is 3.582548508644104 and perplexity is 35.96508147249815
At time: 95.17342686653137 and batch: 350, loss is 3.6168666887283325 and perplexity is 37.220760692052856
At time: 95.81050753593445 and batch: 400, loss is 3.5979511833190916 and perplexity is 36.52332813131166
At time: 96.46899366378784 and batch: 450, loss is 3.527460651397705 and perplexity is 34.03742489286141
At time: 97.11627268791199 and batch: 500, loss is 3.544079976081848 and perplexity is 34.6078306590109
At time: 97.78137636184692 and batch: 550, loss is 3.5976074028015135 and perplexity is 36.51077428067128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.350774237450133 and perplexity of 77.5384728808161
Finished 12 epochs...
Completing Train Step...
At time: 99.49491906166077 and batch: 50, loss is 3.6848552370071412 and perplexity is 39.83935476816824
At time: 100.14776945114136 and batch: 100, loss is 3.6631739044189455 and perplexity is 38.98488100460465
At time: 100.78795766830444 and batch: 150, loss is 3.6626380252838135 and perplexity is 38.96399541686384
At time: 101.43097567558289 and batch: 200, loss is 3.6483261156082154 and perplexity is 38.41031777686246
At time: 102.07392859458923 and batch: 250, loss is 3.613755292892456 and perplexity is 37.10513214859299
At time: 102.71309757232666 and batch: 300, loss is 3.574048624038696 and perplexity is 35.66067796023254
At time: 103.3516457080841 and batch: 350, loss is 3.609905128479004 and perplexity is 36.96254595551991
At time: 103.98995304107666 and batch: 400, loss is 3.592735962867737 and perplexity is 36.33334675147935
At time: 104.63476920127869 and batch: 450, loss is 3.524353036880493 and perplexity is 33.93181388128502
At time: 105.31450200080872 and batch: 500, loss is 3.543749523162842 and perplexity is 34.59639628971356
At time: 105.97879648208618 and batch: 550, loss is 3.6000109481811524 and perplexity is 36.598635129971925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.350162911922373 and perplexity of 77.49108611880499
Finished 13 epochs...
Completing Train Step...
At time: 107.67574667930603 and batch: 50, loss is 3.6784718561172487 and perplexity is 39.58585494602643
At time: 108.33553910255432 and batch: 100, loss is 3.6561080026626587 and perplexity is 38.71038857576005
At time: 108.9719295501709 and batch: 150, loss is 3.655680856704712 and perplexity is 38.693857120672824
At time: 109.6075451374054 and batch: 200, loss is 3.6420420360565187 and perplexity is 38.16970110332763
At time: 110.24662470817566 and batch: 250, loss is 3.6083180904388428 and perplexity is 36.90393151300491
At time: 110.89432621002197 and batch: 300, loss is 3.569222173690796 and perplexity is 35.488978152290635
At time: 111.53635263442993 and batch: 350, loss is 3.6058259296417234 and perplexity is 36.81207548916154
At time: 112.17652320861816 and batch: 400, loss is 3.5895944738388064 and perplexity is 36.21938503973239
At time: 112.81319403648376 and batch: 450, loss is 3.5224291324615478 and perplexity is 33.86659507211036
At time: 113.45164966583252 and batch: 500, loss is 3.5431724834442138 and perplexity is 34.57643855368997
At time: 114.09274172782898 and batch: 550, loss is 3.600757999420166 and perplexity is 36.62598640082033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.35005577574385 and perplexity of 77.48278446468028
Finished 14 epochs...
Completing Train Step...
At time: 115.8501296043396 and batch: 50, loss is 3.673123822212219 and perplexity is 39.37471355053232
At time: 116.52035117149353 and batch: 100, loss is 3.650559620857239 and perplexity is 38.496203300413924
At time: 117.16525721549988 and batch: 150, loss is 3.650329566001892 and perplexity is 38.487348080564466
At time: 117.80868887901306 and batch: 200, loss is 3.637223539352417 and perplexity is 37.9862229238644
At time: 118.46142387390137 and batch: 250, loss is 3.6040688514709474 and perplexity is 36.74745058702749
At time: 119.11561727523804 and batch: 300, loss is 3.565347957611084 and perplexity is 35.351752175664714
At time: 119.76233196258545 and batch: 350, loss is 3.6024757385253907 and perplexity is 36.68895435570478
At time: 120.41432523727417 and batch: 400, loss is 3.5869098138809203 and perplexity is 36.1222787140727
At time: 121.06991052627563 and batch: 450, loss is 3.5206226396560667 and perplexity is 33.80547053890408
At time: 121.74000382423401 and batch: 500, loss is 3.542232646942139 and perplexity is 34.543957620399674
At time: 122.38148140907288 and batch: 550, loss is 3.6006953573226927 and perplexity is 36.62369214406944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.350128823138298 and perplexity of 77.48844458692606
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 124.10731387138367 and batch: 50, loss is 3.672509160041809 and perplexity is 39.350518840190404
At time: 124.77401089668274 and batch: 100, loss is 3.650219187736511 and perplexity is 38.48310014828823
At time: 125.42155432701111 and batch: 150, loss is 3.65063530921936 and perplexity is 38.499117125259545
At time: 126.06718158721924 and batch: 200, loss is 3.636415343284607 and perplexity is 37.95553501046225
At time: 126.71349453926086 and batch: 250, loss is 3.6032228326797484 and perplexity is 36.7163747005477
At time: 127.35646653175354 and batch: 300, loss is 3.56286940574646 and perplexity is 35.264239521518846
At time: 127.99063038825989 and batch: 350, loss is 3.598363604545593 and perplexity is 36.53839423367215
At time: 128.63052129745483 and batch: 400, loss is 3.5805572843551636 and perplexity is 35.89353818172449
At time: 129.26742029190063 and batch: 450, loss is 3.5120703840255736 and perplexity is 33.51759028058021
At time: 129.91734623908997 and batch: 500, loss is 3.5316911268234255 and perplexity is 34.18172439496457
At time: 130.56706404685974 and batch: 550, loss is 3.5876992559432983 and perplexity is 36.15080641928216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3487165735123 and perplexity of 77.37908879705111
Finished 16 epochs...
Completing Train Step...
At time: 132.33176803588867 and batch: 50, loss is 3.6686733675003054 and perplexity is 39.19986753181625
At time: 132.99399065971375 and batch: 100, loss is 3.6470130586624148 and perplexity is 38.35991593979782
At time: 133.6363136768341 and batch: 150, loss is 3.646825222969055 and perplexity is 38.35271125505971
At time: 134.27383279800415 and batch: 200, loss is 3.6335723924636842 and perplexity is 37.847782531120096
At time: 134.91182351112366 and batch: 250, loss is 3.6004875230789186 and perplexity is 36.61608127763519
At time: 135.55132222175598 and batch: 300, loss is 3.5604727125167845 and perplexity is 35.17982315787984
At time: 136.187757730484 and batch: 350, loss is 3.596584887504578 and perplexity is 36.47346053565494
At time: 136.83266806602478 and batch: 400, loss is 3.5794695663452147 and perplexity is 35.854517359476226
At time: 137.4768190383911 and batch: 450, loss is 3.5118588542938234 and perplexity is 33.51050106351577
At time: 138.1377387046814 and batch: 500, loss is 3.5318407154083253 and perplexity is 34.18683797320318
At time: 138.7828071117401 and batch: 550, loss is 3.5892056608200074 and perplexity is 36.20530520868423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348421137383643 and perplexity of 77.35623159520576
Finished 17 epochs...
Completing Train Step...
At time: 140.47682118415833 and batch: 50, loss is 3.6670814895629884 and perplexity is 39.13751576891148
At time: 141.13164496421814 and batch: 100, loss is 3.645380039215088 and perplexity is 38.29732457144083
At time: 141.76879954338074 and batch: 150, loss is 3.6450378370285033 and perplexity is 38.28422138532948
At time: 142.4098072052002 and batch: 200, loss is 3.6319925451278685 and perplexity is 37.78803602033158
At time: 143.05588483810425 and batch: 250, loss is 3.599137692451477 and perplexity is 36.56668911269779
At time: 143.69874930381775 and batch: 300, loss is 3.559260745048523 and perplexity is 35.13721218344651
At time: 144.34365224838257 and batch: 350, loss is 3.595621037483215 and perplexity is 36.43832252654418
At time: 144.98353219032288 and batch: 400, loss is 3.5789274168014527 and perplexity is 35.83508411758558
At time: 145.62417888641357 and batch: 450, loss is 3.5117662286758424 and perplexity is 33.50739727639323
At time: 146.27756881713867 and batch: 500, loss is 3.5320478868484497 and perplexity is 34.193921243359696
At time: 146.93989300727844 and batch: 550, loss is 3.5899627017974853 and perplexity is 36.23272448577794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348296794485538 and perplexity of 77.34661349516634
Finished 18 epochs...
Completing Train Step...
At time: 148.63393783569336 and batch: 50, loss is 3.665889859199524 and perplexity is 39.09090609303858
At time: 149.28774333000183 and batch: 100, loss is 3.6441263914108277 and perplexity is 38.24934329667676
At time: 149.9239673614502 and batch: 150, loss is 3.643741602897644 and perplexity is 38.23462822001718
At time: 150.57590126991272 and batch: 200, loss is 3.6308004426956177 and perplexity is 37.74301565045955
At time: 151.2224736213684 and batch: 250, loss is 3.598146605491638 and perplexity is 36.53046629689891
At time: 151.86813473701477 and batch: 300, loss is 3.558373327255249 and perplexity is 35.10604462751613
At time: 152.50588083267212 and batch: 350, loss is 3.594901342391968 and perplexity is 36.4121074792405
At time: 153.1478123664856 and batch: 400, loss is 3.578489980697632 and perplexity is 35.81941198603708
At time: 153.7853422164917 and batch: 450, loss is 3.511629705429077 and perplexity is 33.50282304997762
At time: 154.44719767570496 and batch: 500, loss is 3.532151918411255 and perplexity is 34.19747867546448
At time: 155.09749841690063 and batch: 550, loss is 3.590372347831726 and perplexity is 36.24757011819289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348253290703956 and perplexity of 77.34324869817793
Finished 19 epochs...
Completing Train Step...
At time: 156.76466512680054 and batch: 50, loss is 3.6648526906967165 and perplexity is 39.05038325463109
At time: 157.4138925075531 and batch: 100, loss is 3.6430405664443968 and perplexity is 38.20783374490784
At time: 158.07033395767212 and batch: 150, loss is 3.642653059959412 and perplexity is 38.19303082985248
At time: 158.73116660118103 and batch: 200, loss is 3.6297940540313722 and perplexity is 37.70505061434978
At time: 159.37092542648315 and batch: 250, loss is 3.5973087644577024 and perplexity is 36.499872391450936
At time: 160.0189220905304 and batch: 300, loss is 3.55761926651001 and perplexity is 35.07958251561569
At time: 160.66738939285278 and batch: 350, loss is 3.594286870956421 and perplexity is 36.389740152032054
At time: 161.30742287635803 and batch: 400, loss is 3.578080997467041 and perplexity is 35.80476544250467
At time: 161.93899011611938 and batch: 450, loss is 3.5114523696899416 and perplexity is 33.496882328855584
At time: 162.57985663414001 and batch: 500, loss is 3.5321718788146974 and perplexity is 34.19816127774804
At time: 163.21797490119934 and batch: 550, loss is 3.590602388381958 and perplexity is 36.25590948832733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348250368808178 and perplexity of 77.34302270959628
Finished 20 epochs...
Completing Train Step...
At time: 164.90861892700195 and batch: 50, loss is 3.6638962268829345 and perplexity is 39.01305083253518
At time: 165.5748155117035 and batch: 100, loss is 3.642047758102417 and perplexity is 38.16991951273415
At time: 166.221449136734 and batch: 150, loss is 3.6416767501831053 and perplexity is 38.15576079697568
At time: 166.85960578918457 and batch: 200, loss is 3.6288952732086184 and perplexity is 37.671177262576386
At time: 167.50542569160461 and batch: 250, loss is 3.596554899215698 and perplexity is 36.47236677538403
At time: 168.1468117237091 and batch: 300, loss is 3.5569350624084475 and perplexity is 35.055589130499676
At time: 168.78295636177063 and batch: 350, loss is 3.5937253522872923 and perplexity is 36.36931236939935
At time: 169.41793203353882 and batch: 400, loss is 3.5776807403564455 and perplexity is 35.79043719822522
At time: 170.0849485397339 and batch: 450, loss is 3.5112459516525267 and perplexity is 33.48996868172103
At time: 170.7502601146698 and batch: 500, loss is 3.5321328973770143 and perplexity is 34.19682821023793
At time: 171.38812279701233 and batch: 550, loss is 3.590728554725647 and perplexity is 36.26048405243654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348268549493018 and perplexity of 77.3444288714992
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 173.1041362285614 and batch: 50, loss is 3.663650994300842 and perplexity is 39.00348473435164
At time: 173.7759313583374 and batch: 100, loss is 3.6418579149246217 and perplexity is 38.16267390170433
At time: 174.42702388763428 and batch: 150, loss is 3.641510305404663 and perplexity is 38.14941049832522
At time: 175.06860423088074 and batch: 200, loss is 3.6283398199081422 and perplexity is 37.65025849307069
At time: 175.70782947540283 and batch: 250, loss is 3.5958020734786986 and perplexity is 36.44491977168792
At time: 176.3563413619995 and batch: 300, loss is 3.5563271331787107 and perplexity is 35.034284289775464
At time: 176.9992814064026 and batch: 350, loss is 3.592430896759033 and perplexity is 36.32226436928524
At time: 177.63977026939392 and batch: 400, loss is 3.5759795331954956 and perplexity is 35.7296020114697
At time: 178.2850170135498 and batch: 450, loss is 3.5090105152130127 and perplexity is 33.41518760068967
At time: 178.93029308319092 and batch: 500, loss is 3.5297337436676024 and perplexity is 34.11488310174863
At time: 179.56909680366516 and batch: 550, loss is 3.5873345851898195 and perplexity is 36.13762568092734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348263679666722 and perplexity of 77.34405221848276
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 181.28208327293396 and batch: 50, loss is 3.6634341287612915 and perplexity is 38.99502713970392
At time: 181.93770241737366 and batch: 100, loss is 3.641744074821472 and perplexity is 38.15832970624742
At time: 182.5935664176941 and batch: 150, loss is 3.6413160705566407 and perplexity is 38.14200127296311
At time: 183.23131322860718 and batch: 200, loss is 3.628170189857483 and perplexity is 37.64387241946548
At time: 183.87449622154236 and batch: 250, loss is 3.595540623664856 and perplexity is 36.435392499704115
At time: 184.5115659236908 and batch: 300, loss is 3.5561329126358032 and perplexity is 35.02748057279251
At time: 185.15084552764893 and batch: 350, loss is 3.5921816205978394 and perplexity is 36.31321122307105
At time: 185.78638863563538 and batch: 400, loss is 3.575674467086792 and perplexity is 35.7187037832431
At time: 186.4291603565216 and batch: 450, loss is 3.5086362409591674 and perplexity is 33.40268349641144
At time: 187.09199929237366 and batch: 500, loss is 3.5293413019180297 and perplexity is 34.10149762401966
At time: 187.73095846176147 and batch: 550, loss is 3.5867933225631714 and perplexity is 36.118071027309455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3482630303565495 and perplexity of 77.34400199821916
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 189.4435749053955 and batch: 50, loss is 3.663397374153137 and perplexity is 38.99359391910033
At time: 190.1034698486328 and batch: 100, loss is 3.6417205715179444 and perplexity is 38.15743286998157
At time: 190.75141024589539 and batch: 150, loss is 3.6412820196151734 and perplexity is 38.14070252402225
At time: 191.38689017295837 and batch: 200, loss is 3.628140196800232 and perplexity is 37.64274338157658
At time: 192.02463388442993 and batch: 250, loss is 3.5954965209960936 and perplexity is 36.433785637091205
At time: 192.66276574134827 and batch: 300, loss is 3.556099143028259 and perplexity is 35.02629772849252
At time: 193.30064702033997 and batch: 350, loss is 3.5921397590637207 and perplexity is 36.311691128157456
At time: 193.93754410743713 and batch: 400, loss is 3.5756229162216187 and perplexity is 35.71686250062046
At time: 194.58243584632874 and batch: 450, loss is 3.508573656082153 and perplexity is 33.40059305898844
At time: 195.23644042015076 and batch: 500, loss is 3.529276103973389 and perplexity is 34.099274348942664
At time: 195.8840000629425 and batch: 550, loss is 3.5867039728164674 and perplexity is 36.11484403097952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348263679666722 and perplexity of 77.34405221848276
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 197.64045310020447 and batch: 50, loss is 3.663391532897949 and perplexity is 38.99336614823279
At time: 198.29935240745544 and batch: 100, loss is 3.64171676158905 and perplexity is 38.15728749315249
At time: 198.94697332382202 and batch: 150, loss is 3.641276478767395 and perplexity is 38.140491192780885
At time: 199.59395718574524 and batch: 200, loss is 3.6281354904174803 and perplexity is 37.6425662208353
At time: 200.25032663345337 and batch: 250, loss is 3.59548933506012 and perplexity is 36.433523827181006
At time: 200.89809775352478 and batch: 300, loss is 3.556093621253967 and perplexity is 35.026104321716154
At time: 201.55094718933105 and batch: 350, loss is 3.5921327447891236 and perplexity is 36.31143642887806
At time: 202.19724988937378 and batch: 400, loss is 3.5756142568588256 and perplexity is 35.71655321668934
At time: 202.83872842788696 and batch: 450, loss is 3.508562893867493 and perplexity is 33.400233596570466
At time: 203.49531269073486 and batch: 500, loss is 3.529264979362488 and perplexity is 34.09889500989353
At time: 204.13802075386047 and batch: 550, loss is 3.586688656806946 and perplexity is 36.11429089992036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.348264328976895 and perplexity of 77.34410243877898
Annealing...
Model not improving. Stopping early with 77.34302270959628loss at 24 epochs.
Finished Training.
Improved accuracyfrom -161.3694418197261 to -77.34302270959628
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fec82176cc0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 20.377194193934663, 'anneal': 3.5122697591932353, 'dropout': 0.9600069746160612, 'seq_len': 35, 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8843667507171631 and batch: 50, loss is 7.724636344909668 and perplexity is 2263.429330182409
At time: 1.5401782989501953 and batch: 100, loss is 6.997238883972168 and perplexity is 1093.6094034282992
At time: 2.1778082847595215 and batch: 150, loss is 6.813411359786987 and perplexity is 909.9697523539577
At time: 2.816129207611084 and batch: 200, loss is 6.677673263549805 and perplexity is 794.4684411585765
At time: 3.4541635513305664 and batch: 250, loss is 6.588692283630371 and perplexity is 726.8297609676607
At time: 4.0978264808654785 and batch: 300, loss is 6.541428050994873 and perplexity is 693.2759048094048
At time: 4.74418568611145 and batch: 350, loss is 6.5646812915802 and perplexity is 709.5857092149002
At time: 5.389540433883667 and batch: 400, loss is 6.533965167999267 and perplexity is 688.1213257817326
At time: 6.035054922103882 and batch: 450, loss is 6.476518144607544 and perplexity is 649.7048251585895
At time: 6.685889482498169 and batch: 500, loss is 6.478295955657959 and perplexity is 650.8609049174563
At time: 7.326108455657959 and batch: 550, loss is 6.518429126739502 and perplexity is 677.5132615050649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.413357998462433 and perplexity of 609.9384182986532
Finished 1 epochs...
Completing Train Step...
At time: 9.01813554763794 and batch: 50, loss is 6.241901407241821 and perplexity is 513.8345914988114
At time: 9.660735845565796 and batch: 100, loss is 6.155565309524536 and perplexity is 471.3332163183093
At time: 10.294443607330322 and batch: 150, loss is 6.162036123275757 and perplexity is 474.39301479238605
At time: 10.92745304107666 and batch: 200, loss is 5.924160575866699 and perplexity is 373.9643888366124
At time: 11.560243844985962 and batch: 250, loss is 5.86565899848938 and perplexity is 352.71451787073346
At time: 12.192973136901855 and batch: 300, loss is 5.858449325561524 and perplexity is 350.1807065110477
At time: 12.825223684310913 and batch: 350, loss is 5.8998399925231935 and perplexity is 364.97906381382285
At time: 13.456926345825195 and batch: 400, loss is 5.924946060180664 and perplexity is 374.2582473935598
At time: 14.09241247177124 and batch: 450, loss is 5.855750989913941 and perplexity is 349.2370751172366
At time: 14.7420654296875 and batch: 500, loss is 5.876009950637817 and perplexity is 356.38440963767283
At time: 15.382035732269287 and batch: 550, loss is 5.964869947433471 and perplexity is 389.5023689939836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.994350352185838 and perplexity of 401.15598920866245
Finished 2 epochs...
Completing Train Step...
At time: 17.175029516220093 and batch: 50, loss is 5.991546869277954 and perplexity is 400.032930223414
At time: 17.843147039413452 and batch: 100, loss is 6.005541086196899 and perplexity is 405.67043203244
At time: 18.4966037273407 and batch: 150, loss is 5.992061624526977 and perplexity is 400.2389022820797
At time: 19.143545866012573 and batch: 200, loss is 5.9513078212738035 and perplexity is 384.255548170126
At time: 19.79588532447815 and batch: 250, loss is 5.911043262481689 and perplexity is 369.0910134271222
At time: 20.44186568260193 and batch: 300, loss is 5.9200202751159665 and perplexity is 372.41926464341446
At time: 21.087927103042603 and batch: 350, loss is 5.951373796463013 and perplexity is 384.28090033891925
At time: 21.73642373085022 and batch: 400, loss is 5.945412063598633 and perplexity is 381.99673582091236
At time: 22.37584948539734 and batch: 450, loss is 5.921191272735595 and perplexity is 372.85562215275576
At time: 23.011852025985718 and batch: 500, loss is 5.878884267807007 and perplexity is 357.41024504702904
At time: 23.64643406867981 and batch: 550, loss is 5.969240083694458 and perplexity is 391.2082722202807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.0522214199634305 and perplexity of 425.05621040844136
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 25.34015130996704 and batch: 50, loss is 5.831034488677979 and perplexity is 340.7109584548004
At time: 25.990837335586548 and batch: 100, loss is 5.685711307525635 and perplexity is 294.62734117890597
At time: 26.627747535705566 and batch: 150, loss is 5.57202109336853 and perplexity is 262.96503941703196
At time: 27.280354976654053 and batch: 200, loss is 5.487713937759399 and perplexity is 241.70402435057855
At time: 27.942474126815796 and batch: 250, loss is 5.434452610015869 and perplexity is 229.1673700130747
At time: 28.590901851654053 and batch: 300, loss is 5.401090049743653 and perplexity is 221.64789179770764
At time: 29.23346495628357 and batch: 350, loss is 5.435042600631714 and perplexity is 229.3026165039974
At time: 29.872339248657227 and batch: 400, loss is 5.399613666534424 and perplexity is 221.32089601683293
At time: 30.509713172912598 and batch: 450, loss is 5.314702730178833 and perplexity is 203.30406798110255
At time: 31.153789520263672 and batch: 500, loss is 5.317130832672119 and perplexity is 203.79831088885607
At time: 31.79880976676941 and batch: 550, loss is 5.356830787658692 and perplexity is 212.05184309107221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.467458522066157 and perplexity of 236.85745916259543
Finished 4 epochs...
Completing Train Step...
At time: 33.49932384490967 and batch: 50, loss is 5.39971432685852 and perplexity is 221.34317537126003
At time: 34.14570879936218 and batch: 100, loss is 5.398947057723999 and perplexity is 221.17341072057738
At time: 34.77703332901001 and batch: 150, loss is 5.353230829238892 and perplexity is 211.28983768992896
At time: 35.40847945213318 and batch: 200, loss is 5.296800918579102 and perplexity is 199.6969402514598
At time: 36.04015517234802 and batch: 250, loss is 5.259439506530762 and perplexity is 192.37363690708852
At time: 36.676172971725464 and batch: 300, loss is 5.245452070236206 and perplexity is 189.70155427447392
At time: 37.311951637268066 and batch: 350, loss is 5.292154912948608 and perplexity is 198.7712990747435
At time: 37.94778060913086 and batch: 400, loss is 5.288311290740967 and perplexity is 198.00876368313249
At time: 38.58465933799744 and batch: 450, loss is 5.226806383132935 and perplexity is 186.19721042648845
At time: 39.21884107589722 and batch: 500, loss is 5.23463996887207 and perplexity is 187.66153018778644
At time: 39.85360074043274 and batch: 550, loss is 5.279192266464233 and perplexity is 196.2113248599573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.415585781665558 and perplexity of 224.88424016750452
Finished 5 epochs...
Completing Train Step...
At time: 41.55456042289734 and batch: 50, loss is 5.315420866012573 and perplexity is 203.45012035390928
At time: 42.21525692939758 and batch: 100, loss is 5.315728349685669 and perplexity is 203.5126875629121
At time: 42.858402729034424 and batch: 150, loss is 5.274665908813477 and perplexity is 195.32520917964484
At time: 43.50285291671753 and batch: 200, loss is 5.220156288146972 and perplexity is 184.96308935034114
At time: 44.140419006347656 and batch: 250, loss is 5.195081462860108 and perplexity is 180.38283670028986
At time: 44.79146695137024 and batch: 300, loss is 5.184792861938477 and perplexity is 178.53646426228423
At time: 45.44043207168579 and batch: 350, loss is 5.232953214645386 and perplexity is 187.3452581201661
At time: 46.07954168319702 and batch: 400, loss is 5.229803905487061 and perplexity is 186.75617806748377
At time: 46.71541094779968 and batch: 450, loss is 5.166646242141724 and perplexity is 175.32584992834987
At time: 47.35077357292175 and batch: 500, loss is 5.1738028240203855 and perplexity is 176.58508426113434
At time: 47.990270376205444 and batch: 550, loss is 5.220547285079956 and perplexity is 185.0354234912844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3603479912940495 and perplexity of 212.79898576070903
Finished 6 epochs...
Completing Train Step...
At time: 49.69658446311951 and batch: 50, loss is 5.244529209136963 and perplexity is 189.52656684653238
At time: 50.34860444068909 and batch: 100, loss is 5.247558660507202 and perplexity is 190.10159894026123
At time: 50.986268520355225 and batch: 150, loss is 5.216717138290405 and perplexity is 184.32806616398972
At time: 51.624467849731445 and batch: 200, loss is 5.157663850784302 and perplexity is 173.75805633824086
At time: 52.26115965843201 and batch: 250, loss is 5.135501174926758 and perplexity is 169.94947294926564
At time: 52.89702033996582 and batch: 300, loss is 5.127741460800171 and perplexity is 168.6358170120901
At time: 53.53279137611389 and batch: 350, loss is 5.179337377548218 and perplexity is 177.56511337235403
At time: 54.16919660568237 and batch: 400, loss is 5.177686586380005 and perplexity is 177.2722322607035
At time: 54.803985595703125 and batch: 450, loss is 5.11301212310791 and perplexity is 166.17012668719016
At time: 55.438785791397095 and batch: 500, loss is 5.12335844039917 and perplexity is 167.89830024200575
At time: 56.07333040237427 and batch: 550, loss is 5.171908950805664 and perplexity is 176.25097098389122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.328981440118018 and perplexity of 206.22781176737374
Finished 7 epochs...
Completing Train Step...
At time: 57.75382328033447 and batch: 50, loss is 5.200151023864746 and perplexity is 181.29962037704183
At time: 58.42303228378296 and batch: 100, loss is 5.20160361289978 and perplexity is 181.56316558275512
At time: 59.068652391433716 and batch: 150, loss is 5.169522218704223 and perplexity is 175.83080874034533
At time: 59.71560478210449 and batch: 200, loss is 5.114670705795288 and perplexity is 166.44596226729894
At time: 60.362135887145996 and batch: 250, loss is 5.096561431884766 and perplexity is 163.45887536585084
At time: 61.00729155540466 and batch: 300, loss is 5.087299375534058 and perplexity is 161.9518996711542
At time: 61.653159379959106 and batch: 350, loss is 5.14178092956543 and perplexity is 171.020071971665
At time: 62.30167078971863 and batch: 400, loss is 5.1448421287536625 and perplexity is 171.5444006048951
At time: 62.95163822174072 and batch: 450, loss is 5.081589708328247 and perplexity is 161.02984304386854
At time: 63.604044675827026 and batch: 500, loss is 5.092103719711304 and perplexity is 162.731844398583
At time: 64.2547607421875 and batch: 550, loss is 5.136806430816651 and perplexity is 170.17144533368352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.314458319481383 and perplexity of 203.25438436390382
Finished 8 epochs...
Completing Train Step...
At time: 66.01099514961243 and batch: 50, loss is 5.163670349121094 and perplexity is 174.8048745232252
At time: 66.6736090183258 and batch: 100, loss is 5.164491004943848 and perplexity is 174.9483880408961
At time: 67.3177855014801 and batch: 150, loss is 5.140825901031494 and perplexity is 170.85682089018505
At time: 67.96319532394409 and batch: 200, loss is 5.082415370941162 and perplexity is 161.16285426847477
At time: 68.60407209396362 and batch: 250, loss is 5.065335550308228 and perplexity is 158.4335955971785
At time: 69.24114847183228 and batch: 300, loss is 5.057298526763916 and perplexity is 157.1653642882426
At time: 69.88452053070068 and batch: 350, loss is 5.109337730407715 and perplexity is 165.56067275892732
At time: 70.52261662483215 and batch: 400, loss is 5.113444871902466 and perplexity is 166.24205217091603
At time: 71.1699948310852 and batch: 450, loss is 5.053494882583618 and perplexity is 156.56869863630575
At time: 71.81636357307434 and batch: 500, loss is 5.063785047531128 and perplexity is 158.1881342106752
At time: 72.44958066940308 and batch: 550, loss is 5.108601303100586 and perplexity is 165.43879424136338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.286515905501995 and perplexity of 197.65358061225837
Finished 9 epochs...
Completing Train Step...
At time: 74.127281665802 and batch: 50, loss is 5.130543079376221 and perplexity is 169.10893268498774
At time: 74.77495527267456 and batch: 100, loss is 5.132033023834229 and perplexity is 169.3610834006756
At time: 75.40755343437195 and batch: 150, loss is 5.1079631042480464 and perplexity is 165.33324507697336
At time: 76.04126930236816 and batch: 200, loss is 5.050148544311523 and perplexity is 156.0456424573995
At time: 76.67498469352722 and batch: 250, loss is 5.036016359329223 and perplexity is 153.8558860383784
At time: 77.30816531181335 and batch: 300, loss is 5.028158922195434 and perplexity is 152.65171014975004
At time: 77.94263482093811 and batch: 350, loss is 5.082548170089722 and perplexity is 161.1842579794669
At time: 78.5870361328125 and batch: 400, loss is 5.0856131649017335 and perplexity is 161.67904476607544
At time: 79.23932576179504 and batch: 450, loss is 5.027399663925171 and perplexity is 152.53585206501927
At time: 79.88696837425232 and batch: 500, loss is 5.032895431518555 and perplexity is 153.37646143872982
At time: 80.5384476184845 and batch: 550, loss is 5.07983627319336 and perplexity is 160.74773506061413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2761288906665555 and perplexity of 195.61117554692163
Finished 10 epochs...
Completing Train Step...
At time: 82.25444412231445 and batch: 50, loss is 5.102196054458618 and perplexity is 164.38250414096956
At time: 82.92324995994568 and batch: 100, loss is 5.1065041446685795 and perplexity is 165.09220643084035
At time: 83.58425903320312 and batch: 150, loss is 5.080023708343506 and perplexity is 160.77786766033688
At time: 84.23196530342102 and batch: 200, loss is 5.01801420211792 and perplexity is 151.11092988729715
At time: 84.87641406059265 and batch: 250, loss is 5.004519538879395 and perplexity is 149.08543619223644
At time: 85.52818703651428 and batch: 300, loss is 4.99743239402771 and perplexity is 148.032581384792
At time: 86.16645455360413 and batch: 350, loss is 5.0519869518280025 and perplexity is 156.33278179809676
At time: 86.80338287353516 and batch: 400, loss is 5.056875057220459 and perplexity is 157.098823633145
At time: 87.4499282836914 and batch: 450, loss is 4.997915258407593 and perplexity is 148.10407830567385
At time: 88.0854377746582 and batch: 500, loss is 5.000514621734619 and perplexity is 148.48955539586476
At time: 88.72176456451416 and batch: 550, loss is 5.049355192184448 and perplexity is 155.92189241020753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.252079415828623 and perplexity of 190.96294726081902
Finished 11 epochs...
Completing Train Step...
At time: 90.49238395690918 and batch: 50, loss is 5.072762041091919 and perplexity is 159.61458109578896
At time: 91.14657044410706 and batch: 100, loss is 5.076183185577393 and perplexity is 160.16158078868006
At time: 91.80387592315674 and batch: 150, loss is 5.056675615310669 and perplexity is 157.067494667992
At time: 92.44588565826416 and batch: 200, loss is 4.991786651611328 and perplexity is 147.19918235286733
At time: 93.08041048049927 and batch: 250, loss is 4.9764610958099365 and perplexity is 144.96047163034038
At time: 93.71384286880493 and batch: 300, loss is 4.968562288284302 and perplexity is 143.8199670091273
At time: 94.35810542106628 and batch: 350, loss is 5.029729881286621 and perplexity is 152.8917082058012
At time: 94.99555540084839 and batch: 400, loss is 5.035786294937134 and perplexity is 153.82049334894253
At time: 95.63175344467163 and batch: 450, loss is 4.9781326484680175 and perplexity is 145.20298332107478
At time: 96.27370548248291 and batch: 500, loss is 4.978751707077026 and perplexity is 145.29290030702398
At time: 96.91212129592896 and batch: 550, loss is 5.026808834075927 and perplexity is 152.44575594889773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.247302765541888 and perplexity of 190.05295912180193
Finished 12 epochs...
Completing Train Step...
At time: 98.58933544158936 and batch: 50, loss is 5.050431251525879 and perplexity is 156.08976392272544
At time: 99.24817657470703 and batch: 100, loss is 5.0570916748046875 and perplexity is 157.13285768685967
At time: 99.88156962394714 and batch: 150, loss is 5.030745544433594 and perplexity is 153.04707356538268
At time: 100.51334357261658 and batch: 200, loss is 4.970500965118408 and perplexity is 144.0990578935531
At time: 101.14519143104553 and batch: 250, loss is 4.956105556488037 and perplexity is 142.03955234930922
At time: 101.77858543395996 and batch: 300, loss is 4.9510142993927 and perplexity is 141.3182302468375
At time: 102.41996121406555 and batch: 350, loss is 5.008168029785156 and perplexity is 149.6303665325749
At time: 103.06249499320984 and batch: 400, loss is 5.009166259765625 and perplexity is 149.7798066258176
At time: 103.69617009162903 and batch: 450, loss is 4.953277721405029 and perplexity is 141.63845530543333
At time: 104.33057236671448 and batch: 500, loss is 4.952497577667236 and perplexity is 141.52800004258907
At time: 104.96352767944336 and batch: 550, loss is 5.004169492721558 and perplexity is 149.03325854093393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.22853835085605 and perplexity of 186.51997741540066
Finished 13 epochs...
Completing Train Step...
At time: 106.61697340011597 and batch: 50, loss is 5.024103574752807 and perplexity is 152.03390797530528
At time: 107.28535270690918 and batch: 100, loss is 5.029382915496826 and perplexity is 152.83866921540013
At time: 107.93546891212463 and batch: 150, loss is 5.006919479370117 and perplexity is 149.44366205556844
At time: 108.58264636993408 and batch: 200, loss is 4.944946966171265 and perplexity is 140.46340134193954
At time: 109.23388433456421 and batch: 250, loss is 4.933645143508911 and perplexity is 138.88484596839714
At time: 109.88767313957214 and batch: 300, loss is 4.924342832565308 and perplexity is 137.59888641679652
At time: 110.54038882255554 and batch: 350, loss is 4.986536998748779 and perplexity is 146.42846251966526
At time: 111.1921603679657 and batch: 400, loss is 4.9919132804870605 and perplexity is 147.2178232000473
At time: 111.84653759002686 and batch: 450, loss is 4.932238893508911 and perplexity is 138.68967641450965
At time: 112.49717998504639 and batch: 500, loss is 4.938079957962036 and perplexity is 139.502142276185
At time: 113.14441251754761 and batch: 550, loss is 4.986443109512329 and perplexity is 146.41471510850187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2222608201047205 and perplexity of 185.3527599762026
Finished 14 epochs...
Completing Train Step...
At time: 114.90828895568848 and batch: 50, loss is 5.004990730285645 and perplexity is 149.15570052124127
At time: 115.57358694076538 and batch: 100, loss is 5.013917264938354 and perplexity is 150.49310436076598
At time: 116.21930122375488 and batch: 150, loss is 4.992821865081787 and perplexity is 147.35164383075173
At time: 116.86552453041077 and batch: 200, loss is 4.930898656845093 and perplexity is 138.503923929236
At time: 117.51055598258972 and batch: 250, loss is 4.920473327636719 and perplexity is 137.0674756589823
At time: 118.15933394432068 and batch: 300, loss is 4.908206090927124 and perplexity is 135.39630775313097
At time: 118.80812644958496 and batch: 350, loss is 4.972768898010254 and perplexity is 144.42623575511368
At time: 119.45690393447876 and batch: 400, loss is 4.978064260482788 and perplexity is 145.19305352113955
At time: 120.10571718215942 and batch: 450, loss is 4.918528089523315 and perplexity is 136.80110594147993
At time: 120.75120425224304 and batch: 500, loss is 4.923524389266968 and perplexity is 137.4863156030475
At time: 121.39553451538086 and batch: 550, loss is 4.974312181472778 and perplexity is 144.6492984565255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.213287678170712 and perplexity of 183.69700313339294
Finished 15 epochs...
Completing Train Step...
At time: 123.14473104476929 and batch: 50, loss is 4.991158180236816 and perplexity is 147.10670094440997
At time: 123.8130555152893 and batch: 100, loss is 4.999428844451904 and perplexity is 148.32841630630756
At time: 124.4580090045929 and batch: 150, loss is 4.976803970336914 and perplexity is 145.01018340544476
At time: 125.0967333316803 and batch: 200, loss is 4.917101707458496 and perplexity is 136.6061143967989
At time: 125.72899436950684 and batch: 250, loss is 4.905102691650391 and perplexity is 134.9767702824358
At time: 126.36085319519043 and batch: 300, loss is 4.89370379447937 and perplexity is 133.4469198265657
At time: 126.99342727661133 and batch: 350, loss is 4.9556259918212895 and perplexity is 141.97145152940053
At time: 127.62631583213806 and batch: 400, loss is 4.962655324935913 and perplexity is 142.97293190081726
At time: 128.25819635391235 and batch: 450, loss is 4.904917345046997 and perplexity is 134.95175511483677
At time: 128.88911390304565 and batch: 500, loss is 4.9067041015625 and perplexity is 135.19309658758434
At time: 129.52090430259705 and batch: 550, loss is 4.957953586578369 and perplexity is 142.30228841336907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.210202156229222 and perplexity of 183.1310755396331
Finished 16 epochs...
Completing Train Step...
At time: 131.31388664245605 and batch: 50, loss is 4.974108333587647 and perplexity is 144.61981500812072
At time: 131.97817730903625 and batch: 100, loss is 4.98215392112732 and perplexity is 145.78805969534554
At time: 132.62524724006653 and batch: 150, loss is 4.958774528503418 and perplexity is 142.4191582930864
At time: 133.27785873413086 and batch: 200, loss is 4.896798620223999 and perplexity is 133.8605545230927
At time: 133.93258905410767 and batch: 250, loss is 4.883546571731568 and perplexity is 132.09833029340678
At time: 134.58087301254272 and batch: 300, loss is 4.8739291763305665 and perplexity is 130.83397805230973
At time: 135.22542595863342 and batch: 350, loss is 4.938646202087402 and perplexity is 139.58115691340075
At time: 135.87434196472168 and batch: 400, loss is 4.94069766998291 and perplexity is 139.8677970916605
At time: 136.52106618881226 and batch: 450, loss is 4.884084186553955 and perplexity is 132.16936740737682
At time: 137.17627954483032 and batch: 500, loss is 4.886356477737427 and perplexity is 132.47003616978017
At time: 137.8274176120758 and batch: 550, loss is 4.933646955490112 and perplexity is 138.8850976253552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.197412044443983 and perplexity of 180.80372388300123
Finished 17 epochs...
Completing Train Step...
At time: 139.51950025558472 and batch: 50, loss is 4.951175794601441 and perplexity is 141.34105430686867
At time: 140.16871452331543 and batch: 100, loss is 4.955767307281494 and perplexity is 141.99151570806526
At time: 140.8042893409729 and batch: 150, loss is 4.927523279190064 and perplexity is 138.0372089919703
At time: 141.44121861457825 and batch: 200, loss is 4.859134664535523 and perplexity is 128.9126011826018
At time: 142.07838892936707 and batch: 250, loss is 4.846396694183349 and perplexity is 127.28093043977877
At time: 142.71570229530334 and batch: 300, loss is 4.841793003082276 and perplexity is 126.69631507976844
At time: 143.35369157791138 and batch: 350, loss is 4.904387941360474 and perplexity is 134.88033006618636
At time: 143.99340105056763 and batch: 400, loss is 4.912032642364502 and perplexity is 135.9154012247124
At time: 144.63140964508057 and batch: 450, loss is 4.8514424991607665 and perplexity is 127.9247882164503
At time: 145.28050112724304 and batch: 500, loss is 4.855692930221558 and perplexity is 128.46968090378903
At time: 145.92974996566772 and batch: 550, loss is 4.9046438026428225 and perplexity is 134.9148451357474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1713243849734045 and perplexity of 176.14797079647977
Finished 18 epochs...
Completing Train Step...
At time: 147.67931151390076 and batch: 50, loss is 4.923727817535401 and perplexity is 137.51428705115904
At time: 148.3428213596344 and batch: 100, loss is 4.933708744049072 and perplexity is 138.89367940052352
At time: 148.99011754989624 and batch: 150, loss is 4.907169370651245 and perplexity is 135.25601239169274
At time: 149.6451120376587 and batch: 200, loss is 4.846008920669556 and perplexity is 127.23158383441488
At time: 150.29634308815002 and batch: 250, loss is 4.830110654830933 and perplexity is 125.22481661904796
At time: 150.94143748283386 and batch: 300, loss is 4.82003345489502 and perplexity is 123.96923808829558
At time: 151.58646416664124 and batch: 350, loss is 4.884447603225708 and perplexity is 132.21740868796016
At time: 152.22186088562012 and batch: 400, loss is 4.891767377853394 and perplexity is 133.18876102449485
At time: 152.85571789741516 and batch: 450, loss is 4.829671401977539 and perplexity is 125.16982333989387
At time: 153.48886156082153 and batch: 500, loss is 4.835066413879394 and perplexity is 125.84694091289187
At time: 154.12819361686707 and batch: 550, loss is 4.886727638244629 and perplexity is 132.51921294126728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1669158935546875 and perplexity of 175.37313316622158
Finished 19 epochs...
Completing Train Step...
At time: 155.83085703849792 and batch: 50, loss is 4.904574327468872 and perplexity is 134.905472229009
At time: 156.48012447357178 and batch: 100, loss is 4.913964815139771 and perplexity is 136.17826713209385
At time: 157.11651635169983 and batch: 150, loss is 4.8900314044952395 and perplexity is 132.95774945672906
At time: 157.7534728050232 and batch: 200, loss is 4.827476549148559 and perplexity is 124.89539527381072
At time: 158.41408705711365 and batch: 250, loss is 4.813193187713623 and perplexity is 123.12414899019723
At time: 159.05586004257202 and batch: 300, loss is 4.805361289978027 and perplexity is 122.16361954052566
At time: 159.6909122467041 and batch: 350, loss is 4.866477594375611 and perplexity is 129.8626812854443
At time: 160.33462834358215 and batch: 400, loss is 4.873416500091553 and perplexity is 130.76691977156904
At time: 160.98245000839233 and batch: 450, loss is 4.811850757598877 and perplexity is 122.95897431727316
At time: 161.62063312530518 and batch: 500, loss is 4.814383897781372 and perplexity is 123.270841470758
At time: 162.25510239601135 and batch: 550, loss is 4.86434266090393 and perplexity is 129.5857288431598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.137126516788564 and perplexity of 170.22592354454545
Finished 20 epochs...
Completing Train Step...
At time: 163.9291455745697 and batch: 50, loss is 4.875348215103149 and perplexity is 131.0197683305647
At time: 164.58244490623474 and batch: 100, loss is 4.885545864105224 and perplexity is 132.36269766350847
At time: 165.22988367080688 and batch: 150, loss is 4.860158643722534 and perplexity is 129.04467261081302
At time: 165.86966705322266 and batch: 200, loss is 4.808561687469482 and perplexity is 122.55521798314
At time: 166.51228857040405 and batch: 250, loss is 4.788945178985596 and perplexity is 120.1745391544632
At time: 167.15562558174133 and batch: 300, loss is 4.785226793289184 and perplexity is 119.72851362837679
At time: 167.79051303863525 and batch: 350, loss is 4.844755325317383 and perplexity is 127.07218684280873
At time: 168.4271969795227 and batch: 400, loss is 4.849130611419678 and perplexity is 127.62938207128867
At time: 169.07742714881897 and batch: 450, loss is 4.788428678512573 and perplexity is 120.11248497503063
At time: 169.72048950195312 and batch: 500, loss is 4.792765684127808 and perplexity is 120.63454476663918
At time: 170.36445355415344 and batch: 550, loss is 4.841833200454712 and perplexity is 126.7014080410931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.135476294984209 and perplexity of 169.94524466874236
Finished 21 epochs...
Completing Train Step...
At time: 172.12757444381714 and batch: 50, loss is 4.858717956542969 and perplexity is 128.85889346234342
At time: 172.77857184410095 and batch: 100, loss is 4.869276485443115 and perplexity is 130.22666191755397
At time: 173.41336703300476 and batch: 150, loss is 4.843722372055054 and perplexity is 126.94099498205351
At time: 174.04755806922913 and batch: 200, loss is 4.7893344593048095 and perplexity is 120.221329844183
At time: 174.68109798431396 and batch: 250, loss is 4.771442861557007 and perplexity is 118.08950590583893
At time: 175.31532096862793 and batch: 300, loss is 4.768467006683349 and perplexity is 117.7386110392127
At time: 175.95040011405945 and batch: 350, loss is 4.8321604156494145 and perplexity is 125.48176078873801
At time: 176.58632636070251 and batch: 400, loss is 4.837964248657227 and perplexity is 126.21215346219539
At time: 177.22165942192078 and batch: 450, loss is 4.777324113845825 and perplexity is 118.78606639894645
At time: 177.8582980632782 and batch: 500, loss is 4.781589231491089 and perplexity is 119.29378491669206
At time: 178.49428510665894 and batch: 550, loss is 4.82676025390625 and perplexity is 124.80596532933534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.132523232317985 and perplexity of 169.44412599300549
Finished 22 epochs...
Completing Train Step...
At time: 180.17333149909973 and batch: 50, loss is 4.84159372329712 and perplexity is 126.6710695808616
At time: 180.82644939422607 and batch: 100, loss is 4.849973335266113 and perplexity is 127.7369837279388
At time: 181.4589340686798 and batch: 150, loss is 4.825917205810547 and perplexity is 124.70079223714873
At time: 182.0917842388153 and batch: 200, loss is 4.77277060508728 and perplexity is 118.24640261954742
At time: 182.72402834892273 and batch: 250, loss is 4.752697944641113 and perplexity is 115.8965455703188
At time: 183.36144137382507 and batch: 300, loss is 4.7516077041625975 and perplexity is 115.7702593187185
At time: 183.9975368976593 and batch: 350, loss is 4.81514539718628 and perplexity is 123.36474789348506
At time: 184.62814021110535 and batch: 400, loss is 4.819934949874878 and perplexity is 123.95702709743257
At time: 185.25975275039673 and batch: 450, loss is 4.762421073913575 and perplexity is 117.02891884989307
At time: 185.89229226112366 and batch: 500, loss is 4.765379838943481 and perplexity is 117.37569268063588
At time: 186.52373552322388 and batch: 550, loss is 4.811715497970581 and perplexity is 122.94234405683812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.133054368039395 and perplexity of 169.53414772586615
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 188.18604850769043 and batch: 50, loss is 4.788258152008057 and perplexity is 120.09200435911264
At time: 188.83272099494934 and batch: 100, loss is 4.7612991046905515 and perplexity is 116.89768963607546
At time: 189.46650314331055 and batch: 150, loss is 4.7134495449066165 and perplexity is 111.43590080347782
At time: 190.10186004638672 and batch: 200, loss is 4.653718566894531 and perplexity is 104.97461582624364
At time: 190.73643112182617 and batch: 250, loss is 4.621914081573486 and perplexity is 101.68848602720625
At time: 191.37103366851807 and batch: 300, loss is 4.61306960105896 and perplexity is 100.79306977675158
At time: 192.00573706626892 and batch: 350, loss is 4.667958698272705 and perplexity is 106.48016229543288
At time: 192.63920783996582 and batch: 400, loss is 4.659124307632446 and perplexity is 105.54361793679116
At time: 193.27140307426453 and batch: 450, loss is 4.592933406829834 and perplexity is 98.78377857690843
At time: 193.91239094734192 and batch: 500, loss is 4.588591232299804 and perplexity is 98.35577208152958
At time: 194.55447268486023 and batch: 550, loss is 4.650998754501343 and perplexity is 104.68949248191576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.016275608793218 and perplexity of 150.8484376830224
Finished 24 epochs...
Completing Train Step...
At time: 196.25213193893433 and batch: 50, loss is 4.707151708602905 and perplexity is 110.73630103660139
At time: 196.90350008010864 and batch: 100, loss is 4.708090200424194 and perplexity is 110.84027493113749
At time: 197.54043674468994 and batch: 150, loss is 4.675761785507202 and perplexity is 107.31428642990697
At time: 198.18768525123596 and batch: 200, loss is 4.6246876049041745 and perplexity is 101.97091289337061
At time: 198.8478479385376 and batch: 250, loss is 4.597940254211426 and perplexity is 99.27961413067041
At time: 199.4916558265686 and batch: 300, loss is 4.591999149322509 and perplexity is 98.69153218782148
At time: 200.14279675483704 and batch: 350, loss is 4.64746826171875 and perplexity is 104.32053866192595
At time: 200.7875838279724 and batch: 400, loss is 4.646238088607788 and perplexity is 104.1922852434705
At time: 201.42344450950623 and batch: 450, loss is 4.586520357131958 and perplexity is 98.15230031054664
At time: 202.05922770500183 and batch: 500, loss is 4.587681550979614 and perplexity is 98.26634035628625
At time: 202.6972086429596 and batch: 550, loss is 4.647612905502319 and perplexity is 104.33562907068259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.00899586779006 and perplexity of 149.75428752285384
Finished 25 epochs...
Completing Train Step...
At time: 204.38846921920776 and batch: 50, loss is 4.688085126876831 and perplexity is 108.64493921981376
At time: 205.0439577102661 and batch: 100, loss is 4.6914710521698 and perplexity is 109.01342635037595
At time: 205.6877477169037 and batch: 150, loss is 4.661361160278321 and perplexity is 105.77996769907563
At time: 206.33373498916626 and batch: 200, loss is 4.611172742843628 and perplexity is 100.6020608300139
At time: 206.97145199775696 and batch: 250, loss is 4.587049512863159 and perplexity is 98.20425190681681
At time: 207.62392711639404 and batch: 300, loss is 4.582270679473877 and perplexity is 97.73606972216218
At time: 208.2614483833313 and batch: 350, loss is 4.638534202575683 and perplexity is 103.39268372669102
At time: 208.89800190925598 and batch: 400, loss is 4.63758355140686 and perplexity is 103.29444005618744
At time: 209.53637671470642 and batch: 450, loss is 4.578839845657349 and perplexity is 97.40132805891695
At time: 210.17278027534485 and batch: 500, loss is 4.582467889785766 and perplexity is 97.75534618365097
At time: 210.8055284023285 and batch: 550, loss is 4.639153509140015 and perplexity is 103.45673532616763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.004036761344747 and perplexity of 149.0134784640847
Finished 26 epochs...
Completing Train Step...
At time: 212.47355580329895 and batch: 50, loss is 4.673863248825073 and perplexity is 107.11073960230208
At time: 213.11905193328857 and batch: 100, loss is 4.679354915618896 and perplexity is 107.70057419940146
At time: 213.75040459632874 and batch: 150, loss is 4.649466323852539 and perplexity is 104.52918595571697
At time: 214.39442920684814 and batch: 200, loss is 4.5994377613067625 and perplexity is 99.42839743147348
At time: 215.0488624572754 and batch: 250, loss is 4.5755314445495605 and perplexity is 97.07961786388991
At time: 215.68676280975342 and batch: 300, loss is 4.570912218093872 and perplexity is 96.63221923805041
At time: 216.33450269699097 and batch: 350, loss is 4.626636667251587 and perplexity is 102.16985437191596
At time: 216.9728033542633 and batch: 400, loss is 4.624966611862183 and perplexity is 101.99936745690833
At time: 217.60726737976074 and batch: 450, loss is 4.567753944396973 and perplexity is 96.32750967314192
At time: 218.2429039478302 and batch: 500, loss is 4.572521190643311 and perplexity is 96.78782297369663
At time: 218.87856698036194 and batch: 550, loss is 4.627609901428222 and perplexity is 102.26933796857827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.995907722635472 and perplexity of 147.8070523153848
Finished 27 epochs...
Completing Train Step...
At time: 220.56695652008057 and batch: 50, loss is 4.659847688674927 and perplexity is 105.61999381026047
At time: 221.22913002967834 and batch: 100, loss is 4.66622748374939 and perplexity is 106.29598176602646
At time: 221.87035036087036 and batch: 150, loss is 4.6361305713653564 and perplexity is 103.14446427867836
At time: 222.51974320411682 and batch: 200, loss is 4.587117671966553 and perplexity is 98.21094564869331
At time: 223.1617774963379 and batch: 250, loss is 4.563534803390503 and perplexity is 95.92194649277546
At time: 223.79926991462708 and batch: 300, loss is 4.560474262237549 and perplexity is 95.62882221632411
At time: 224.43722939491272 and batch: 350, loss is 4.616266021728515 and perplexity is 101.11576228408552
At time: 225.0749955177307 and batch: 400, loss is 4.614209175109863 and perplexity is 100.90799641485003
At time: 225.71113848686218 and batch: 450, loss is 4.558362684249878 and perplexity is 95.42710754342653
At time: 226.36197304725647 and batch: 500, loss is 4.56357045173645 and perplexity is 95.92536601245772
At time: 227.0136218070984 and batch: 550, loss is 4.617928190231323 and perplexity is 101.28397347823801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.989663306702959 and perplexity of 146.88695931440907
Finished 28 epochs...
Completing Train Step...
At time: 228.77072668075562 and batch: 50, loss is 4.647756795883179 and perplexity is 104.35064304424414
At time: 229.41992378234863 and batch: 100, loss is 4.655902662277222 and perplexity is 105.20414096112167
At time: 230.05584812164307 and batch: 150, loss is 4.625891084671021 and perplexity is 102.09370669895402
At time: 230.69148349761963 and batch: 200, loss is 4.576811075210571 and perplexity is 97.20392343511583
At time: 231.32814955711365 and batch: 250, loss is 4.554130048751831 and perplexity is 95.02405297383525
At time: 231.9641375541687 and batch: 300, loss is 4.552434005737305 and perplexity is 94.8630246866285
At time: 232.60033702850342 and batch: 350, loss is 4.606602277755737 and perplexity is 100.14331177007463
At time: 233.23623704910278 and batch: 400, loss is 4.6047717952728275 and perplexity is 99.96016886317798
At time: 233.87984824180603 and batch: 450, loss is 4.54953709602356 and perplexity is 94.58861273417126
At time: 234.52063274383545 and batch: 500, loss is 4.553875246047974 and perplexity is 94.99984367263474
At time: 235.1564166545868 and batch: 550, loss is 4.610080499649047 and perplexity is 100.49223890075702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.986129436087101 and perplexity of 146.36879590550325
Finished 29 epochs...
Completing Train Step...
At time: 236.83260822296143 and batch: 50, loss is 4.638307857513428 and perplexity is 103.36928395156781
At time: 237.48238277435303 and batch: 100, loss is 4.645953731536865 and perplexity is 104.16266164246574
At time: 238.13207507133484 and batch: 150, loss is 4.6163849353790285 and perplexity is 101.12778704344298
At time: 238.77027440071106 and batch: 200, loss is 4.567392177581787 and perplexity is 96.29266787943509
At time: 239.4054000377655 and batch: 250, loss is 4.546652307510376 and perplexity is 94.3161377959252
At time: 240.03969287872314 and batch: 300, loss is 4.545524921417236 and perplexity is 94.20986700917479
At time: 240.6740324497223 and batch: 350, loss is 4.598288955688477 and perplexity is 99.31423911529966
At time: 241.30843901634216 and batch: 400, loss is 4.597326126098633 and perplexity is 99.21866244659256
At time: 241.94243597984314 and batch: 450, loss is 4.542598915100098 and perplexity is 93.93461123961325
At time: 242.59008359909058 and batch: 500, loss is 4.5458887004852295 and perplexity is 94.2441448211884
At time: 243.22563362121582 and batch: 550, loss is 4.601084613800049 and perplexity is 99.59227624076155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.983345194065825 and perplexity of 145.96183655290065
Finished 30 epochs...
Completing Train Step...
At time: 244.91384148597717 and batch: 50, loss is 4.629324588775635 and perplexity is 102.44484833812398
At time: 245.58411979675293 and batch: 100, loss is 4.636852245330811 and perplexity is 103.21892781919652
At time: 246.22049593925476 and batch: 150, loss is 4.607016410827637 and perplexity is 100.18479301619323
At time: 246.8545105457306 and batch: 200, loss is 4.559282503128052 and perplexity is 95.51492357966696
At time: 247.48864150047302 and batch: 250, loss is 4.538205699920654 and perplexity is 93.52284143858496
At time: 248.12272238731384 and batch: 300, loss is 4.537768630981446 and perplexity is 93.48197444098382
At time: 248.771564245224 and batch: 350, loss is 4.590472974777222 and perplexity is 98.541026561702
At time: 249.40696215629578 and batch: 400, loss is 4.5891712951660155 and perplexity is 98.41284116281885
At time: 250.0400836467743 and batch: 450, loss is 4.533487024307251 and perplexity is 93.08257703674737
At time: 250.6749086380005 and batch: 500, loss is 4.538039512634278 and perplexity is 93.50730042274743
At time: 251.30951356887817 and batch: 550, loss is 4.593349180221558 and perplexity is 98.82485878301114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.980238569543717 and perplexity of 145.5090915506611
Finished 31 epochs...
Completing Train Step...
At time: 252.98056769371033 and batch: 50, loss is 4.621089563369751 and perplexity is 101.60467657532398
At time: 253.63307404518127 and batch: 100, loss is 4.628644161224365 and perplexity is 102.37516575049376
At time: 254.28545331954956 and batch: 150, loss is 4.598761262893677 and perplexity is 99.361157024974
At time: 254.92487812042236 and batch: 200, loss is 4.5517755889892575 and perplexity is 94.80058584005124
At time: 255.56108450889587 and batch: 250, loss is 4.529693975448608 and perplexity is 92.730179028093
At time: 256.19729018211365 and batch: 300, loss is 4.52970630645752 and perplexity is 92.731322491807
At time: 256.8345718383789 and batch: 350, loss is 4.581501808166504 and perplexity is 97.66095214403323
At time: 257.4739999771118 and batch: 400, loss is 4.580381212234497 and perplexity is 97.55157497359654
At time: 258.11045026779175 and batch: 450, loss is 4.525394630432129 and perplexity is 92.33235579746577
At time: 258.75768661499023 and batch: 500, loss is 4.529170169830322 and perplexity is 92.68161915840997
At time: 259.4030930995941 and batch: 550, loss is 4.584518003463745 and perplexity is 97.95596132761807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9765612013796545 and perplexity of 144.97498370729727
Finished 32 epochs...
Completing Train Step...
At time: 261.0951073169708 and batch: 50, loss is 4.610779838562012 and perplexity is 100.56254161371739
At time: 261.7520787715912 and batch: 100, loss is 4.6181902408599855 and perplexity is 101.31051848507737
At time: 262.4006586074829 and batch: 150, loss is 4.587775192260742 and perplexity is 98.27554257313597
At time: 263.0384509563446 and batch: 200, loss is 4.54214786529541 and perplexity is 93.89225160543076
At time: 263.6823925971985 and batch: 250, loss is 4.5202433872222905 and perplexity is 91.85795230952502
At time: 264.33393597602844 and batch: 300, loss is 4.519958448410034 and perplexity is 91.83178214232291
At time: 264.97138690948486 and batch: 350, loss is 4.571625366210937 and perplexity is 96.70115690170871
At time: 265.60690569877625 and batch: 400, loss is 4.570951461791992 and perplexity is 96.63601151810197
At time: 266.25061440467834 and batch: 450, loss is 4.51535961151123 and perplexity is 91.4104323561999
At time: 266.8994688987732 and batch: 500, loss is 4.5204175758361815 and perplexity is 91.8739543125556
At time: 267.5471992492676 and batch: 550, loss is 4.576269760131836 and perplexity is 97.1513197245235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.971465739798038 and perplexity of 144.2381481005337
Finished 33 epochs...
Completing Train Step...
At time: 269.2448182106018 and batch: 50, loss is 4.6009206867218015 and perplexity is 99.57595170795442
At time: 269.8926167488098 and batch: 100, loss is 4.607306318283081 and perplexity is 100.2138415450999
At time: 270.5295784473419 and batch: 150, loss is 4.578076734542846 and perplexity is 97.32702837597043
At time: 271.17359352111816 and batch: 200, loss is 4.532823963165283 and perplexity is 93.02087805427941
At time: 271.8251643180847 and batch: 250, loss is 4.5106895065307615 and perplexity is 90.98453131614122
At time: 272.477557182312 and batch: 300, loss is 4.510378303527832 and perplexity is 90.95622106212205
At time: 273.1127059459686 and batch: 350, loss is 4.560671377182007 and perplexity is 95.64767394422144
At time: 273.75589323043823 and batch: 400, loss is 4.560053672790527 and perplexity is 95.58861019983645
At time: 274.404283285141 and batch: 450, loss is 4.504170331954956 and perplexity is 90.39331648301446
At time: 275.04091596603394 and batch: 500, loss is 4.511823387145996 and perplexity is 91.08775542333439
At time: 275.67544317245483 and batch: 550, loss is 4.567248935699463 and perplexity is 96.27887572426472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.966159901720412 and perplexity of 143.47487054629426
Finished 34 epochs...
Completing Train Step...
At time: 277.33855962753296 and batch: 50, loss is 4.5904559135437015 and perplexity is 98.53934534457836
At time: 277.9902012348175 and batch: 100, loss is 4.596518011093139 and perplexity is 99.13851474528624
At time: 278.64145827293396 and batch: 150, loss is 4.568512077331543 and perplexity is 96.40056642059422
At time: 279.28257274627686 and batch: 200, loss is 4.5226488780975345 and perplexity is 92.0791817516498
At time: 279.92602491378784 and batch: 250, loss is 4.50087236404419 and perplexity is 90.09569327156662
At time: 280.5644543170929 and batch: 300, loss is 4.500004386901855 and perplexity is 90.01752619770829
At time: 281.20957946777344 and batch: 350, loss is 4.549240159988403 and perplexity is 94.56053013610902
At time: 281.85422921180725 and batch: 400, loss is 4.5501432132720945 and perplexity is 94.64596190226867
At time: 282.4944393634796 and batch: 450, loss is 4.493538255691528 and perplexity is 89.43733886729657
At time: 283.14936995506287 and batch: 500, loss is 4.5002969551086425 and perplexity is 90.04386631688023
At time: 283.8009729385376 and batch: 550, loss is 4.556134328842163 and perplexity is 95.21469878129064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.962562074052527 and perplexity of 142.95960017022475
Finished 35 epochs...
Completing Train Step...
At time: 285.54946851730347 and batch: 50, loss is 4.579192533493042 and perplexity is 97.43568638102863
At time: 286.2137060165405 and batch: 100, loss is 4.583537044525147 and perplexity is 97.85991766691025
At time: 286.8582718372345 and batch: 150, loss is 4.556332216262818 and perplexity is 95.23354243684055
At time: 287.5025746822357 and batch: 200, loss is 4.510379724502563 and perplexity is 90.95635030870561
At time: 288.14837312698364 and batch: 250, loss is 4.489562864303589 and perplexity is 89.0824962271111
At time: 288.7941806316376 and batch: 300, loss is 4.4888413143157955 and perplexity is 89.01824193717461
At time: 289.4406416416168 and batch: 350, loss is 4.537465028762817 and perplexity is 93.4535974140239
At time: 290.0882523059845 and batch: 400, loss is 4.538996858596802 and perplexity is 93.59686212319947
At time: 290.7334625720978 and batch: 450, loss is 4.4809592342376705 and perplexity is 88.31935100131327
At time: 291.37911343574524 and batch: 500, loss is 4.489665098190308 and perplexity is 89.09160394248966
At time: 292.0260729789734 and batch: 550, loss is 4.54531928062439 and perplexity is 94.19049560927233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.95550991626496 and perplexity of 141.95497307057298
Finished 36 epochs...
Completing Train Step...
At time: 293.78372740745544 and batch: 50, loss is 4.567036790847778 and perplexity is 96.25845282283711
At time: 294.4522178173065 and batch: 100, loss is 4.571301469802856 and perplexity is 96.66984081618855
At time: 295.10146498680115 and batch: 150, loss is 4.5445146656036375 and perplexity is 94.1147390032317
At time: 295.75090503692627 and batch: 200, loss is 4.497908239364624 and perplexity is 89.82903380481093
At time: 296.39479327201843 and batch: 250, loss is 4.477942066192627 and perplexity is 88.05327827262548
At time: 297.03903698921204 and batch: 300, loss is 4.478350210189819 and perplexity is 88.089224024608
At time: 297.68403673171997 and batch: 350, loss is 4.526518392562866 and perplexity is 92.43617372475481
At time: 298.3287432193756 and batch: 400, loss is 4.527725820541382 and perplexity is 92.5478511547961
At time: 298.97233986854553 and batch: 450, loss is 4.470243988037109 and perplexity is 87.37803960920876
At time: 299.61671352386475 and batch: 500, loss is 4.480901613235473 and perplexity is 88.31426209841042
At time: 300.26680064201355 and batch: 550, loss is 4.534942979812622 and perplexity is 93.21819983365918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.946852339074967 and perplexity of 140.73129163522458
Finished 37 epochs...
Completing Train Step...
At time: 302.02470445632935 and batch: 50, loss is 4.555919809341431 and perplexity is 95.19427556231334
At time: 302.6947808265686 and batch: 100, loss is 4.560106830596924 and perplexity is 95.59369161572846
At time: 303.34612369537354 and batch: 150, loss is 4.533748064041138 and perplexity is 93.10687845956697
At time: 303.98989033699036 and batch: 200, loss is 4.486952533721924 and perplexity is 88.85026469531856
At time: 304.63475704193115 and batch: 250, loss is 4.466636753082275 and perplexity is 87.06341429532294
At time: 305.2805736064911 and batch: 300, loss is 4.467534666061401 and perplexity is 87.14162477287245
At time: 305.9261746406555 and batch: 350, loss is 4.517157201766968 and perplexity is 91.57489863587996
At time: 306.571613073349 and batch: 400, loss is 4.517548723220825 and perplexity is 91.61075919296135
At time: 307.2164556980133 and batch: 450, loss is 4.4596044921875 and perplexity is 86.45330937437257
At time: 307.86642050743103 and batch: 500, loss is 4.470382175445557 and perplexity is 87.39011498837105
At time: 308.50905656814575 and batch: 550, loss is 4.524251947402954 and perplexity is 92.22690943880815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.941210482982879 and perplexity of 139.9395415104492
Finished 38 epochs...
Completing Train Step...
At time: 310.19305515289307 and batch: 50, loss is 4.545571880340576 and perplexity is 94.21429110697217
At time: 310.84239649772644 and batch: 100, loss is 4.548424835205078 and perplexity is 94.4834640135937
At time: 311.4854145050049 and batch: 150, loss is 4.522882509231567 and perplexity is 92.10069682850127
At time: 312.13103008270264 and batch: 200, loss is 4.476339292526245 and perplexity is 87.91226183587511
At time: 312.76856112480164 and batch: 250, loss is 4.456638307571411 and perplexity is 86.19725284130836
At time: 313.40783286094666 and batch: 300, loss is 4.458594226837159 and perplexity is 86.36601269531914
At time: 314.04587626457214 and batch: 350, loss is 4.508054084777832 and perplexity is 90.74506438990426
At time: 314.6834146976471 and batch: 400, loss is 4.508240070343017 and perplexity is 90.76194323155437
At time: 315.3208429813385 and batch: 450, loss is 4.4500953960418705 and perplexity is 85.63511286336816
At time: 315.9585642814636 and batch: 500, loss is 4.461076164245606 and perplexity is 86.58063396114777
At time: 316.5945293903351 and batch: 550, loss is 4.515455274581909 and perplexity is 91.41917737713231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9353037083402596 and perplexity of 139.11538661998776
Finished 39 epochs...
Completing Train Step...
At time: 318.25943064689636 and batch: 50, loss is 4.535249147415161 and perplexity is 93.24674459593301
At time: 318.90448927879333 and batch: 100, loss is 4.5373868656158445 and perplexity is 93.446293072223
At time: 319.53703260421753 and batch: 150, loss is 4.512006721496582 and perplexity is 91.10445646871214
At time: 320.1724753379822 and batch: 200, loss is 4.466976871490479 and perplexity is 87.09303120154783
At time: 320.80765414237976 and batch: 250, loss is 4.446541986465454 and perplexity is 85.33135623871219
At time: 321.443510055542 and batch: 300, loss is 4.450550012588501 and perplexity is 85.67405285335963
At time: 322.0782334804535 and batch: 350, loss is 4.499266948699951 and perplexity is 89.95116830547877
At time: 322.7130732536316 and batch: 400, loss is 4.499096183776856 and perplexity is 89.93580911258377
At time: 323.35864305496216 and batch: 450, loss is 4.440780611038208 and perplexity is 84.84114376474176
At time: 324.0235593318939 and batch: 500, loss is 4.452500667572021 and perplexity is 85.84133647478772
At time: 324.6766345500946 and batch: 550, loss is 4.5065787887573245 and perplexity is 90.61128726222496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.92942517869016 and perplexity of 138.2999917046898
Finished 40 epochs...
Completing Train Step...
At time: 326.42463779449463 and batch: 50, loss is 4.526007966995239 and perplexity is 92.38900397765869
At time: 327.1015555858612 and batch: 100, loss is 4.528813571929931 and perplexity is 92.64857497970583
At time: 327.7593801021576 and batch: 150, loss is 4.502980070114136 and perplexity is 90.28578877349901
At time: 328.41014289855957 and batch: 200, loss is 4.459038848876953 and perplexity is 86.40442146611292
At time: 329.05584692955017 and batch: 250, loss is 4.437154026031494 and perplexity is 84.53401739137597
At time: 329.7062666416168 and batch: 300, loss is 4.440826787948608 and perplexity is 84.84506155709073
At time: 330.3562982082367 and batch: 350, loss is 4.490462684631348 and perplexity is 89.16269054289056
At time: 331.00676131248474 and batch: 400, loss is 4.490658140182495 and perplexity is 89.18011958895876
At time: 331.65206384658813 and batch: 450, loss is 4.430363025665283 and perplexity is 83.96189169996155
At time: 332.29777574539185 and batch: 500, loss is 4.441981210708618 and perplexity is 84.94306518515201
At time: 332.9430170059204 and batch: 550, loss is 4.498026781082153 and perplexity is 89.8396829239321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9262354424659245 and perplexity of 137.85955402478197
Finished 41 epochs...
Completing Train Step...
At time: 334.708936214447 and batch: 50, loss is 4.517032661437988 and perplexity is 91.56349457802489
At time: 335.3743591308594 and batch: 100, loss is 4.519828100204467 and perplexity is 91.8198128144138
At time: 336.0212154388428 and batch: 150, loss is 4.493803434371948 and perplexity is 89.4610588876805
At time: 336.6669387817383 and batch: 200, loss is 4.4489034652709964 and perplexity is 85.53310254397553
At time: 337.31194019317627 and batch: 250, loss is 4.428369512557984 and perplexity is 83.79467929378222
At time: 337.96182465553284 and batch: 300, loss is 4.432171001434326 and perplexity is 84.11383007474036
At time: 338.60513067245483 and batch: 350, loss is 4.481348705291748 and perplexity is 88.35375553139261
At time: 339.2537591457367 and batch: 400, loss is 4.479794082641601 and perplexity is 88.216505495468
At time: 339.89987325668335 and batch: 450, loss is 4.42113862991333 and perplexity is 83.19095516124604
At time: 340.54090785980225 and batch: 500, loss is 4.434052381515503 and perplexity is 84.27222911687356
At time: 341.1818413734436 and batch: 550, loss is 4.489849281311035 and perplexity is 89.10801462337321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9205075527759305 and perplexity of 137.0721668926967
Finished 42 epochs...
Completing Train Step...
At time: 342.9046251773834 and batch: 50, loss is 4.508852090835571 and perplexity is 90.8175084025333
At time: 343.56440567970276 and batch: 100, loss is 4.509837055206299 and perplexity is 90.90700448053676
At time: 344.20518732070923 and batch: 150, loss is 4.483821306228638 and perplexity is 88.5724894194651
At time: 344.8500289916992 and batch: 200, loss is 4.439613332748413 and perplexity is 84.74216831672786
At time: 345.4973347187042 and batch: 250, loss is 4.41870156288147 and perplexity is 82.98846007434696
At time: 346.14801359176636 and batch: 300, loss is 4.423031253814697 and perplexity is 83.3485534414543
At time: 346.80011558532715 and batch: 350, loss is 4.472394180297852 and perplexity is 87.56612132719896
At time: 347.45317482948303 and batch: 400, loss is 4.469935321807862 and perplexity is 87.35107312123975
At time: 348.10213017463684 and batch: 450, loss is 4.410095148086548 and perplexity is 82.27729165864645
At time: 348.75008511543274 and batch: 500, loss is 4.423681802749634 and perplexity is 83.40279339508328
At time: 349.39680099487305 and batch: 550, loss is 4.479450960159301 and perplexity is 88.18624162152727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.913932313310339 and perplexity of 136.1738411612097
Finished 43 epochs...
Completing Train Step...
At time: 351.15481185913086 and batch: 50, loss is 4.497379808425904 and perplexity is 89.78157790384056
At time: 351.8236041069031 and batch: 100, loss is 4.497968235015869 and perplexity is 89.83442331786684
At time: 352.4738233089447 and batch: 150, loss is 4.4736293506622316 and perplexity is 87.67434723014046
At time: 353.12433099746704 and batch: 200, loss is 4.430730504989624 and perplexity is 83.9927516290395
At time: 353.77511858940125 and batch: 250, loss is 4.407395763397217 and perplexity is 82.05549309177246
At time: 354.4221353530884 and batch: 300, loss is 4.412951965332031 and perplexity is 82.51267891346212
At time: 355.06721091270447 and batch: 350, loss is 4.459113740921021 and perplexity is 86.4108927121724
At time: 355.7124433517456 and batch: 400, loss is 4.458050041198731 and perplexity is 86.31902633737221
At time: 356.35856914520264 and batch: 450, loss is 4.3993607521057125 and perplexity is 81.39881800709536
At time: 357.003977060318 and batch: 500, loss is 4.414128532409668 and perplexity is 82.60981774893139
At time: 357.6500117778778 and batch: 550, loss is 4.470035591125488 and perplexity is 87.35983219286129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.906764902967088 and perplexity of 135.2013167676441
Finished 44 epochs...
Completing Train Step...
At time: 359.4157040119171 and batch: 50, loss is 4.488612909317016 and perplexity is 88.99791204754621
At time: 360.0872132778168 and batch: 100, loss is 4.487491836547852 and perplexity is 88.89819481741662
At time: 360.73406076431274 and batch: 150, loss is 4.464479551315308 and perplexity is 86.87580337422638
At time: 361.38104605674744 and batch: 200, loss is 4.424231538772583 and perplexity is 83.44865551990316
At time: 362.02710485458374 and batch: 250, loss is 4.398045930862427 and perplexity is 81.2918634404776
At time: 362.67337441444397 and batch: 300, loss is 4.404320411682129 and perplexity is 81.80353122462388
At time: 363.3199985027313 and batch: 350, loss is 4.44997392654419 and perplexity is 85.62471144096538
At time: 363.9671115875244 and batch: 400, loss is 4.45001184463501 and perplexity is 85.62795822810585
At time: 364.61182022094727 and batch: 450, loss is 4.390010013580322 and perplexity is 80.64122648383265
At time: 365.2628185749054 and batch: 500, loss is 4.403815097808838 and perplexity is 81.76220520759615
At time: 365.91290640830994 and batch: 550, loss is 4.458609628677368 and perplexity is 86.36734290108998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.901420106279089 and perplexity of 134.48062091962896
Finished 45 epochs...
Completing Train Step...
At time: 367.69438910484314 and batch: 50, loss is 4.478069562911987 and perplexity is 88.06450549243576
At time: 368.3605411052704 and batch: 100, loss is 4.476247730255127 and perplexity is 87.90421275802385
At time: 369.00904393196106 and batch: 150, loss is 4.453784847259522 and perplexity is 85.9516429869687
At time: 369.65612053871155 and batch: 200, loss is 4.415207347869873 and perplexity is 82.69898658719607
At time: 370.3033514022827 and batch: 250, loss is 4.384818153381348 and perplexity is 80.22363348990923
At time: 370.9509816169739 and batch: 300, loss is 4.393975667953491 and perplexity is 80.96165665401412
At time: 371.5966658592224 and batch: 350, loss is 4.440509071350098 and perplexity is 84.81810915457197
At time: 372.2467894554138 and batch: 400, loss is 4.440650367736817 and perplexity is 84.83009449364644
At time: 372.89667773246765 and batch: 450, loss is 4.37914270401001 and perplexity is 79.76961790955285
At time: 373.54427003860474 and batch: 500, loss is 4.394368648529053 and perplexity is 80.99347926486887
At time: 374.19280338287354 and batch: 550, loss is 4.449084720611572 and perplexity is 85.54860728072336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8973729559715755 and perplexity of 133.9374575066981
Finished 46 epochs...
Completing Train Step...
At time: 375.88859486579895 and batch: 50, loss is 4.469278717041016 and perplexity is 87.29373681594488
At time: 376.5532395839691 and batch: 100, loss is 4.465656757354736 and perplexity is 86.97813431512189
At time: 377.1959581375122 and batch: 150, loss is 4.444657239913941 and perplexity is 85.17067972412255
At time: 377.8320167064667 and batch: 200, loss is 4.4060201072692875 and perplexity is 81.94269055640926
At time: 378.46724915504456 and batch: 250, loss is 4.376552448272705 and perplexity is 79.56326157232093
At time: 379.10335087776184 and batch: 300, loss is 4.3849951267242435 and perplexity is 80.23783219086592
At time: 379.7388708591461 and batch: 350, loss is 4.432326278686523 and perplexity is 84.12689205323399
At time: 380.3858206272125 and batch: 400, loss is 4.43130256652832 and perplexity is 84.04081439789573
At time: 381.0370419025421 and batch: 450, loss is 4.370664901733399 and perplexity is 79.09620542353885
At time: 381.6750543117523 and batch: 500, loss is 4.385667591094971 and perplexity is 80.29180742037404
At time: 382.310818195343 and batch: 550, loss is 4.441047353744507 and perplexity is 84.86377753959805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.891957059819648 and perplexity of 133.21402692673615
Finished 47 epochs...
Completing Train Step...
At time: 383.9940438270569 and batch: 50, loss is 4.462165279388428 and perplexity is 86.67498160906995
At time: 384.64500665664673 and batch: 100, loss is 4.457657318115235 and perplexity is 86.28513351886633
At time: 385.2898666858673 and batch: 150, loss is 4.435913295745849 and perplexity is 84.42919851525497
At time: 385.93016719818115 and batch: 200, loss is 4.398650388717652 and perplexity is 81.34101579966831
At time: 386.57525539398193 and batch: 250, loss is 4.369567747116089 and perplexity is 79.00947224510074
At time: 387.22965478897095 and batch: 300, loss is 4.377165575027465 and perplexity is 79.61205889463014
At time: 387.8706512451172 and batch: 350, loss is 4.424269456863403 and perplexity is 83.4518197935932
At time: 388.5201473236084 and batch: 400, loss is 4.423051013946533 and perplexity is 83.35020043613099
At time: 389.15254378318787 and batch: 450, loss is 4.362612953186035 and perplexity is 78.46188403588721
At time: 389.78536462783813 and batch: 500, loss is 4.377374773025513 and perplexity is 79.628715320156
At time: 390.41751408576965 and batch: 550, loss is 4.4333553123474125 and perplexity is 84.2135060136401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.888261510970745 and perplexity of 132.7226365207404
Finished 48 epochs...
Completing Train Step...
At time: 392.09082889556885 and batch: 50, loss is 4.45417441368103 and perplexity is 85.98513338389336
At time: 392.73939514160156 and batch: 100, loss is 4.4480416870117185 and perplexity is 85.45942372771542
At time: 393.37510776519775 and batch: 150, loss is 4.428102493286133 and perplexity is 83.772307486517
At time: 394.0107374191284 and batch: 200, loss is 4.39058931350708 and perplexity is 80.68795547417196
At time: 394.6455748081207 and batch: 250, loss is 4.3599159526824955 and perplexity is 78.25055739725497
At time: 395.2810215950012 and batch: 300, loss is 4.368066883087158 and perplexity is 78.89097871384573
At time: 395.9171588420868 and batch: 350, loss is 4.414926815032959 and perplexity is 82.6757900597248
At time: 396.55239939689636 and batch: 400, loss is 4.414601993560791 and perplexity is 82.64893954893228
At time: 397.1881990432739 and batch: 450, loss is 4.3534226226806645 and perplexity is 77.74409679267102
At time: 397.82459783554077 and batch: 500, loss is 4.368815469741821 and perplexity is 78.9500575577424
At time: 398.4605724811554 and batch: 550, loss is 4.425191974639892 and perplexity is 83.52884110213398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.883718612346243 and perplexity of 132.12105852638155
Finished 49 epochs...
Completing Train Step...
At time: 400.1385397911072 and batch: 50, loss is 4.446096487045288 and perplexity is 85.29334963557432
At time: 400.7864351272583 and batch: 100, loss is 4.438228454589844 and perplexity is 84.62489196418932
At time: 401.42152523994446 and batch: 150, loss is 4.419157409667969 and perplexity is 83.02629872084584
At time: 402.0578067302704 and batch: 200, loss is 4.382096662521362 and perplexity is 80.00560242395899
At time: 402.6924283504486 and batch: 250, loss is 4.351011886596679 and perplexity is 77.5569020224689
At time: 403.3273937702179 and batch: 300, loss is 4.36044716835022 and perplexity is 78.29213636207356
At time: 403.961962223053 and batch: 350, loss is 4.40847580909729 and perplexity is 82.14416465030175
At time: 404.59701657295227 and batch: 400, loss is 4.408379745483399 and perplexity is 82.13627396399536
At time: 405.23187351226807 and batch: 450, loss is 4.3461779117584225 and perplexity is 77.182898599415
At time: 405.8667950630188 and batch: 500, loss is 4.361588802337646 and perplexity is 78.3815683654868
At time: 406.50085735321045 and batch: 550, loss is 4.4168134212493895 and perplexity is 82.83191394503865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.881388887445977 and perplexity of 131.81361107952833
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fec82176cc0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 17.724973681306913, 'anneal': 7.237538899170577, 'dropout': 0.6970116747311355, 'seq_len': 35, 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8585512638092041 and batch: 50, loss is 6.672137928009033 and perplexity is 790.0829405664731
At time: 1.5048787593841553 and batch: 100, loss is 6.02805606842041 and perplexity is 414.9076927660667
At time: 2.137394428253174 and batch: 150, loss is 5.979335956573486 and perplexity is 395.1778657444183
At time: 2.7744524478912354 and batch: 200, loss is 5.929041442871093 and perplexity is 375.7941209900976
At time: 3.4099228382110596 and batch: 250, loss is 5.923756589889527 and perplexity is 373.8133429798189
At time: 4.055029392242432 and batch: 300, loss is 5.948515605926514 and perplexity is 383.18412045581033
At time: 4.707171201705933 and batch: 350, loss is 6.031032829284668 and perplexity is 416.1446138439269
At time: 5.363606929779053 and batch: 400, loss is 6.053518190383911 and perplexity is 425.6077682738107
At time: 6.015440225601196 and batch: 450, loss is 5.993818140029907 and perplexity is 400.94254591837614
At time: 6.667027711868286 and batch: 500, loss is 6.023460445404052 and perplexity is 413.005308093078
At time: 7.318702220916748 and batch: 550, loss is 6.109803171157837 and perplexity is 450.2500842425804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.116605068774933 and perplexity of 453.32307846585803
Finished 1 epochs...
Completing Train Step...
At time: 9.084394216537476 and batch: 50, loss is 6.074325370788574 and perplexity is 434.55623927245466
At time: 9.747314453125 and batch: 100, loss is 6.16642807006836 and perplexity is 476.4811057077705
At time: 10.390568733215332 and batch: 150, loss is 6.191125602722168 and perplexity is 488.39553628157404
At time: 11.032214641571045 and batch: 200, loss is 6.138646221160888 and perplexity is 463.4257700024721
At time: 11.674555540084839 and batch: 250, loss is 6.121658592224121 and perplexity is 455.61975554094204
At time: 12.31287670135498 and batch: 300, loss is 6.1315453910827635 and perplexity is 460.1467181289061
At time: 12.948848962783813 and batch: 350, loss is 6.191475486755371 and perplexity is 488.5664479795001
At time: 13.592927932739258 and batch: 400, loss is 6.204059505462647 and perplexity is 494.7534241666903
At time: 14.235487699508667 and batch: 450, loss is 6.131235580444336 and perplexity is 460.00418186115854
At time: 14.874265909194946 and batch: 500, loss is 6.155416278839112 and perplexity is 471.2629784399477
At time: 15.503431797027588 and batch: 550, loss is 6.235488586425781 and perplexity is 510.5500053220927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 6.34112094311004 and perplexity of 567.432014028322
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 17.193921089172363 and batch: 50, loss is 6.126210031509399 and perplexity is 457.69820758156436
At time: 17.84518599510193 and batch: 100, loss is 5.982004146575928 and perplexity is 396.23368330953514
At time: 18.495185375213623 and batch: 150, loss is 5.891858768463135 and perplexity is 362.07767783557534
At time: 19.1319317817688 and batch: 200, loss is 5.835297431945801 and perplexity is 342.16649016172136
At time: 19.766716957092285 and batch: 250, loss is 5.783316488265991 and perplexity is 324.8347164722214
At time: 20.42189383506775 and batch: 300, loss is 5.764790267944336 and perplexity is 318.8721593090908
At time: 21.057918787002563 and batch: 350, loss is 5.811164083480835 and perplexity is 334.00771231742135
At time: 21.712044954299927 and batch: 400, loss is 5.762770433425903 and perplexity is 318.2287403334276
At time: 22.365739107131958 and batch: 450, loss is 5.661054725646973 and perplexity is 287.45166536872625
At time: 23.014731407165527 and batch: 500, loss is 5.64746542930603 and perplexity is 283.5718213862013
At time: 23.654868125915527 and batch: 550, loss is 5.698811559677124 and perplexity is 298.5124258716967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.774210828415891 and perplexity of 321.8903077726888
Finished 3 epochs...
Completing Train Step...
At time: 25.34511685371399 and batch: 50, loss is 5.7562940979003905 and perplexity is 316.1744435749679
At time: 25.998294353485107 and batch: 100, loss is 5.753901309967041 and perplexity is 315.4188095780309
At time: 26.632895469665527 and batch: 150, loss is 5.721972484588623 and perplexity is 305.50693675471774
At time: 27.26774311065674 and batch: 200, loss is 5.667574319839478 and perplexity is 289.3318559569058
At time: 27.902896881103516 and batch: 250, loss is 5.639910211563111 and perplexity is 281.4374475122827
At time: 28.537251949310303 and batch: 300, loss is 5.6267946910858155 and perplexity is 277.770349423144
At time: 29.17085599899292 and batch: 350, loss is 5.674293270111084 and perplexity is 291.2824078014767
At time: 29.806520700454712 and batch: 400, loss is 5.638835487365722 and perplexity is 281.1351423538221
At time: 30.441912174224854 and batch: 450, loss is 5.556389694213867 and perplexity is 258.8864877030792
At time: 31.0762996673584 and batch: 500, loss is 5.5603728771209715 and perplexity is 259.9197363789058
At time: 31.712506771087646 and batch: 550, loss is 5.621130390167236 and perplexity is 276.2014222105043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.705873529961768 and perplexity of 300.62797292591114
Finished 4 epochs...
Completing Train Step...
At time: 33.39791226387024 and batch: 50, loss is 5.663230819702148 and perplexity is 288.077868320036
At time: 34.05150866508484 and batch: 100, loss is 5.657229480743408 and perplexity is 286.3541927397739
At time: 34.705193519592285 and batch: 150, loss is 5.623823852539062 and perplexity is 276.94636313308257
At time: 35.35277485847473 and batch: 200, loss is 5.559852895736694 and perplexity is 259.78461808712507
At time: 35.985931158065796 and batch: 250, loss is 5.518797369003296 and perplexity is 249.3349988099965
At time: 36.63207244873047 and batch: 300, loss is 5.509044771194458 and perplexity is 246.9151539138465
At time: 37.26861238479614 and batch: 350, loss is 5.553365306854248 and perplexity is 258.1046974962178
At time: 37.905669927597046 and batch: 400, loss is 5.523068475723266 and perplexity is 250.40221266870577
At time: 38.543192863464355 and batch: 450, loss is 5.450891828536987 and perplexity is 232.96583887280906
At time: 39.17707967758179 and batch: 500, loss is 5.459087686538696 and perplexity is 234.88303962818276
At time: 39.810917139053345 and batch: 550, loss is 5.513347482681274 and perplexity is 247.97984747488368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.612786475648272 and perplexity of 273.9064091649586
Finished 5 epochs...
Completing Train Step...
At time: 41.53480553627014 and batch: 50, loss is 5.554957122802734 and perplexity is 258.5158798465591
At time: 42.184011697769165 and batch: 100, loss is 5.546274747848511 and perplexity is 256.28106384039836
At time: 42.81973385810852 and batch: 150, loss is 5.521989784240723 and perplexity is 250.13225156323912
At time: 43.475040435791016 and batch: 200, loss is 5.470616827011108 and perplexity is 237.60670980531452
At time: 44.12901329994202 and batch: 250, loss is 5.44598572731018 and perplexity is 231.82568402939262
At time: 44.78433799743652 and batch: 300, loss is 5.4414173603057865 and perplexity is 230.7690346424856
At time: 45.43006443977356 and batch: 350, loss is 5.489334411621094 and perplexity is 242.09601692538686
At time: 46.06706666946411 and batch: 400, loss is 5.462316694259644 and perplexity is 235.64270459863604
At time: 46.703036069869995 and batch: 450, loss is 5.401719408035278 and perplexity is 221.787431641905
At time: 47.33962440490723 and batch: 500, loss is 5.411772890090942 and perplexity is 224.0284135667674
At time: 47.97593569755554 and batch: 550, loss is 5.4692653560638425 and perplexity is 237.28580813362356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.573630474983378 and perplexity of 263.38859125366633
Finished 6 epochs...
Completing Train Step...
At time: 49.67096185684204 and batch: 50, loss is 5.501743068695069 and perplexity is 245.1188190496983
At time: 50.32181239128113 and batch: 100, loss is 5.492559251785278 and perplexity is 242.87799808879805
At time: 50.958463191986084 and batch: 150, loss is 5.471764669418335 and perplexity is 237.87960145141057
At time: 51.59400939941406 and batch: 200, loss is 5.420930337905884 and perplexity is 226.08936419458854
At time: 52.22991466522217 and batch: 250, loss is 5.391491212844849 and perplexity is 219.53050830233002
At time: 52.887856245040894 and batch: 300, loss is 5.397097215652466 and perplexity is 220.76465302547405
At time: 53.53261876106262 and batch: 350, loss is 5.4428483009338375 and perplexity is 231.09948780289204
At time: 54.18273162841797 and batch: 400, loss is 5.433761005401611 and perplexity is 229.00893159722136
At time: 54.83405923843384 and batch: 450, loss is 5.374867382049561 and perplexity is 215.91123670616977
At time: 55.48616099357605 and batch: 500, loss is 5.3863817405700685 and perplexity is 218.41168399165588
At time: 56.12117791175842 and batch: 550, loss is 5.442333431243896 and perplexity is 230.9805323071788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.55357068650266 and perplexity of 258.15771239214695
Finished 7 epochs...
Completing Train Step...
At time: 57.78421354293823 and batch: 50, loss is 5.474847602844238 and perplexity is 238.61410004945304
At time: 58.447235107421875 and batch: 100, loss is 5.469085273742675 and perplexity is 237.24308100183086
At time: 59.08188533782959 and batch: 150, loss is 5.445863380432129 and perplexity is 231.79732261569998
At time: 59.71647906303406 and batch: 200, loss is 5.393349876403809 and perplexity is 219.93892109159952
At time: 60.35065674781799 and batch: 250, loss is 5.368160552978516 and perplexity is 214.46800212080768
At time: 60.983484983444214 and batch: 300, loss is 5.376774406433105 and perplexity is 216.3233775556702
At time: 61.61740064620972 and batch: 350, loss is 5.426763916015625 and perplexity is 227.41212863403138
At time: 62.25139260292053 and batch: 400, loss is 5.417691926956177 and perplexity is 225.35837817736902
At time: 62.88389539718628 and batch: 450, loss is 5.35993655204773 and perplexity is 212.71144991546595
At time: 63.51704931259155 and batch: 500, loss is 5.368150682449341 and perplexity is 214.46588521858305
At time: 64.15018963813782 and batch: 550, loss is 5.428545665740967 and perplexity is 227.81768132116883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.543153803399268 and perplexity of 255.48247170581467
Finished 8 epochs...
Completing Train Step...
At time: 65.83720517158508 and batch: 50, loss is 5.452978992462159 and perplexity is 233.45258454981166
At time: 66.49258017539978 and batch: 100, loss is 5.448854207992554 and perplexity is 232.49162619019648
At time: 67.13504528999329 and batch: 150, loss is 5.424143705368042 and perplexity is 226.81704092124028
At time: 67.76752614974976 and batch: 200, loss is 5.368708658218384 and perplexity is 214.58558537760993
At time: 68.4000723361969 and batch: 250, loss is 5.34600172996521 and perplexity is 209.76791018821982
At time: 69.05622506141663 and batch: 300, loss is 5.352467432022094 and perplexity is 211.12860116748553
At time: 69.68846940994263 and batch: 350, loss is 5.400010185241699 and perplexity is 221.40867129353418
At time: 70.32946562767029 and batch: 400, loss is 5.384415616989136 and perplexity is 217.98268150356026
At time: 70.96210837364197 and batch: 450, loss is 5.322932586669922 and perplexity is 204.98413515787794
At time: 71.60034942626953 and batch: 500, loss is 5.32717794418335 and perplexity is 205.85621593374435
At time: 72.24298620223999 and batch: 550, loss is 5.384778709411621 and perplexity is 218.06184373418046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.495082774060838 and perplexity of 243.49168012168957
Finished 9 epochs...
Completing Train Step...
At time: 73.93982219696045 and batch: 50, loss is 5.400078392028808 and perplexity is 221.42377338266783
At time: 74.5880479812622 and batch: 100, loss is 5.404077787399292 and perplexity is 222.31110781470457
At time: 75.22357058525085 and batch: 150, loss is 5.376050901412964 and perplexity is 216.1669231106478
At time: 75.86065316200256 and batch: 200, loss is 5.32316478729248 and perplexity is 205.03173812818193
At time: 76.49825239181519 and batch: 250, loss is 5.303443279266357 and perplexity is 201.02781453698475
At time: 77.13452100753784 and batch: 300, loss is 5.311400480270386 and perplexity is 202.6338144226856
At time: 77.76984906196594 and batch: 350, loss is 5.35832200050354 and perplexity is 212.36829341201368
At time: 78.40553545951843 and batch: 400, loss is 5.344630107879639 and perplexity is 209.4803851227044
At time: 79.04080843925476 and batch: 450, loss is 5.28268292427063 and perplexity is 196.8974282321663
At time: 79.67721605300903 and batch: 500, loss is 5.292294969558716 and perplexity is 198.79914025870403
At time: 80.3122296333313 and batch: 550, loss is 5.342963218688965 and perplexity is 209.13149539412683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.46675402052859 and perplexity of 236.69065148346363
Finished 10 epochs...
Completing Train Step...
At time: 81.99458646774292 and batch: 50, loss is 5.363155498504638 and perplexity is 213.39725988412476
At time: 82.64906215667725 and batch: 100, loss is 5.364201221466065 and perplexity is 213.62053101822053
At time: 83.28308701515198 and batch: 150, loss is 5.335714683532715 and perplexity is 207.621079163317
At time: 83.91835165023804 and batch: 200, loss is 5.278962621688843 and perplexity is 196.1662711277059
At time: 84.55180549621582 and batch: 250, loss is 5.25664852142334 and perplexity is 191.83747351148787
At time: 85.19800996780396 and batch: 300, loss is 5.2640519714355465 and perplexity is 193.2630030642629
At time: 85.83146500587463 and batch: 350, loss is 5.313730583190918 and perplexity is 203.10652258092455
At time: 86.46412086486816 and batch: 400, loss is 5.311674633026123 and perplexity is 202.68937465696277
At time: 87.10634469985962 and batch: 450, loss is 5.249439783096314 and perplexity is 190.45953991212633
At time: 87.74550890922546 and batch: 500, loss is 5.259322528839111 and perplexity is 192.3511347992567
At time: 88.37898969650269 and batch: 550, loss is 5.316793537139892 and perplexity is 203.72958222070574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4439872579371675 and perplexity of 231.36285013370667
Finished 11 epochs...
Completing Train Step...
At time: 90.08466362953186 and batch: 50, loss is 5.336640586853028 and perplexity is 207.81340523381476
At time: 90.74260187149048 and batch: 100, loss is 5.334032917022705 and perplexity is 207.27220243243758
At time: 91.38465714454651 and batch: 150, loss is 5.307787342071533 and perplexity is 201.90299152454077
At time: 92.0207211971283 and batch: 200, loss is 5.252382955551147 and perplexity is 191.02092089904886
At time: 92.65469288825989 and batch: 250, loss is 5.228582677841186 and perplexity is 186.52824546691463
At time: 93.29062604904175 and batch: 300, loss is 5.235158281326294 and perplexity is 187.75882270784578
At time: 93.92460083961487 and batch: 350, loss is 5.2876122856140135 and perplexity is 197.8704029052166
At time: 94.56734132766724 and batch: 400, loss is 5.288083620071411 and perplexity is 197.9636880267241
At time: 95.21276307106018 and batch: 450, loss is 5.226567945480347 and perplexity is 186.15281929318496
At time: 95.8539605140686 and batch: 500, loss is 5.233686752319336 and perplexity is 187.48273334049549
At time: 96.4866406917572 and batch: 550, loss is 5.288247842788696 and perplexity is 197.99620083109298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.420106928399268 and perplexity of 225.90327668639165
Finished 12 epochs...
Completing Train Step...
At time: 98.18325209617615 and batch: 50, loss is 5.312918109893799 and perplexity is 202.94157097331887
At time: 98.83236455917358 and batch: 100, loss is 5.31473087310791 and perplexity is 203.30978963358044
At time: 99.46838569641113 and batch: 150, loss is 5.288291254043579 and perplexity is 198.00479628120146
At time: 100.1044111251831 and batch: 200, loss is 5.236768589019776 and perplexity is 188.0614157531381
At time: 100.7435712814331 and batch: 250, loss is 5.212658805847168 and perplexity is 183.58151748887323
At time: 101.39373707771301 and batch: 300, loss is 5.221463947296143 and perplexity is 185.20511623623653
At time: 102.0294930934906 and batch: 350, loss is 5.270673751831055 and perplexity is 194.5469946927282
At time: 102.67600440979004 and batch: 400, loss is 5.269477758407593 and perplexity is 194.3144568511046
At time: 103.33032393455505 and batch: 450, loss is 5.208980054855346 and perplexity is 182.90740750096336
At time: 103.97709608078003 and batch: 500, loss is 5.217597951889038 and perplexity is 184.49049635613622
At time: 104.62404489517212 and batch: 550, loss is 5.270685634613037 and perplexity is 194.54930646598658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.415468581179355 and perplexity of 224.8578851696569
Finished 13 epochs...
Completing Train Step...
At time: 106.32057905197144 and batch: 50, loss is 5.300842876434326 and perplexity is 200.50574033455592
At time: 106.96871376037598 and batch: 100, loss is 5.298802604675293 and perplexity is 200.0970711748642
At time: 107.60448932647705 and batch: 150, loss is 5.271815185546875 and perplexity is 194.76918397482726
At time: 108.24001836776733 and batch: 200, loss is 5.223888664245606 and perplexity is 185.65473109480268
At time: 108.87609267234802 and batch: 250, loss is 5.200551700592041 and perplexity is 181.37227747062562
At time: 109.51205515861511 and batch: 300, loss is 5.207664670944214 and perplexity is 182.66697220693587
At time: 110.1485743522644 and batch: 350, loss is 5.257418251037597 and perplexity is 191.98519334084406
At time: 110.78344082832336 and batch: 400, loss is 5.256160526275635 and perplexity is 191.74388059356846
At time: 111.42161250114441 and batch: 450, loss is 5.195522623062134 and perplexity is 180.4624319848156
At time: 112.06075501441956 and batch: 500, loss is 5.210085287094116 and perplexity is 183.10967441981322
At time: 112.69563388824463 and batch: 550, loss is 5.26430004119873 and perplexity is 193.31095171872502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.4105516798952795 and perplexity of 223.75499476889368
Finished 14 epochs...
Completing Train Step...
At time: 114.37128353118896 and batch: 50, loss is 5.289982624053955 and perplexity is 198.33997903466505
At time: 115.01164770126343 and batch: 100, loss is 5.286403598785401 and perplexity is 197.6313840340325
At time: 115.64074182510376 and batch: 150, loss is 5.261909408569336 and perplexity is 192.84936820806175
At time: 116.26926732063293 and batch: 200, loss is 5.213937654495239 and perplexity is 183.8164406478897
At time: 116.89703941345215 and batch: 250, loss is 5.1907086849212645 and perplexity is 179.5957846674176
At time: 117.53896141052246 and batch: 300, loss is 5.196503820419312 and perplexity is 180.63958814451382
At time: 118.17087721824646 and batch: 350, loss is 5.2469589900970455 and perplexity is 189.98763481032665
At time: 118.80341243743896 and batch: 400, loss is 5.246706314086914 and perplexity is 189.9396355571738
At time: 119.43603253364563 and batch: 450, loss is 5.183597354888916 and perplexity is 178.32315019529682
At time: 120.06819534301758 and batch: 500, loss is 5.197953634262085 and perplexity is 180.90167186035063
At time: 120.70081758499146 and batch: 550, loss is 5.25344521522522 and perplexity is 191.2239425319802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.40446179978391 and perplexity of 222.39649442705263
Finished 15 epochs...
Completing Train Step...
At time: 122.36304235458374 and batch: 50, loss is 5.280581617355347 and perplexity is 196.48412069968828
At time: 123.00704097747803 and batch: 100, loss is 5.2746946144104 and perplexity is 195.33081618684443
At time: 123.63759136199951 and batch: 150, loss is 5.249856100082398 and perplexity is 190.538847961252
At time: 124.28075981140137 and batch: 200, loss is 5.2015211868286135 and perplexity is 181.54820066110585
At time: 124.92384099960327 and batch: 250, loss is 5.179435777664184 and perplexity is 177.58258665977414
At time: 125.55668020248413 and batch: 300, loss is 5.1841607189178465 and perplexity is 178.42363934697127
At time: 126.19037795066833 and batch: 350, loss is 5.236026763916016 and perplexity is 187.92195880661097
At time: 126.82253551483154 and batch: 400, loss is 5.236209964752197 and perplexity is 187.9563894203635
At time: 127.4540205001831 and batch: 450, loss is 5.173736629486084 and perplexity is 176.57339568058134
At time: 128.08593201637268 and batch: 500, loss is 5.188671140670777 and perplexity is 179.23022285960826
At time: 128.71757578849792 and batch: 550, loss is 5.241964197158813 and perplexity is 189.04105187444804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.392895637674535 and perplexity of 219.839039002439
Finished 16 epochs...
Completing Train Step...
At time: 130.40702772140503 and batch: 50, loss is 5.268513917922974 and perplexity is 194.12725893979953
At time: 131.05283975601196 and batch: 100, loss is 5.267290239334106 and perplexity is 193.88985485229264
At time: 131.68815088272095 and batch: 150, loss is 5.240300970077515 and perplexity is 188.7268950070198
At time: 132.32278776168823 and batch: 200, loss is 5.189771003723145 and perplexity is 179.42746000664144
At time: 132.95900201797485 and batch: 250, loss is 5.164836168289185 and perplexity is 175.00878423445005
At time: 133.6066870689392 and batch: 300, loss is 5.170928258895874 and perplexity is 176.07820781012444
At time: 134.2429482936859 and batch: 350, loss is 5.222685956954956 and perplexity is 185.43157701758867
At time: 134.8785915374756 and batch: 400, loss is 5.226548709869385 and perplexity is 186.14923856441246
At time: 135.51756834983826 and batch: 450, loss is 5.162563276290894 and perplexity is 174.61145987785773
At time: 136.1597604751587 and batch: 500, loss is 5.173402557373047 and perplexity is 176.51441728524733
At time: 136.79559469223022 and batch: 550, loss is 5.22544264793396 and perplexity is 185.94345980033577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.378381932035405 and perplexity of 216.67140257808848
Finished 17 epochs...
Completing Train Step...
At time: 138.4695053100586 and batch: 50, loss is 5.249048347473145 and perplexity is 190.3850018528099
At time: 139.11519598960876 and batch: 100, loss is 5.244176759719848 and perplexity is 189.4597800886879
At time: 139.75029134750366 and batch: 150, loss is 5.217453727722168 and perplexity is 184.46389028666894
At time: 140.38632321357727 and batch: 200, loss is 5.167724885940552 and perplexity is 175.5150660992284
At time: 141.0199522972107 and batch: 250, loss is 5.142904577255249 and perplexity is 171.21234628451214
At time: 141.6626172065735 and batch: 300, loss is 5.147928915023804 and perplexity is 172.07473960587512
At time: 142.314026594162 and batch: 350, loss is 5.197236061096191 and perplexity is 180.77190823797315
At time: 142.95385456085205 and batch: 400, loss is 5.194754152297974 and perplexity is 180.32380515396153
At time: 143.58815598487854 and batch: 450, loss is 5.123136968612671 and perplexity is 167.86111962288197
At time: 144.22238421440125 and batch: 500, loss is 5.138152799606323 and perplexity is 170.40071316146515
At time: 144.85709810256958 and batch: 550, loss is 5.188380737304687 and perplexity is 179.17818135646445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.349490226583278 and perplexity of 210.50096271570865
Finished 18 epochs...
Completing Train Step...
At time: 146.54645371437073 and batch: 50, loss is 5.210995044708252 and perplexity is 183.2763356395005
At time: 147.19425988197327 and batch: 100, loss is 5.20522099494934 and perplexity is 182.2211382706015
At time: 147.83452701568604 and batch: 150, loss is 5.177310094833374 and perplexity is 177.2055033260332
At time: 148.46904706954956 and batch: 200, loss is 5.1296666240692135 and perplexity is 168.96078119703645
At time: 149.10251235961914 and batch: 250, loss is 5.107452478408813 and perplexity is 165.24884320067616
At time: 149.7496898174286 and batch: 300, loss is 5.110630531311035 and perplexity is 165.77484815947008
At time: 150.38755416870117 and batch: 350, loss is 5.161050186157227 and perplexity is 174.34745678130002
At time: 151.03982090950012 and batch: 400, loss is 5.16590428352356 and perplexity is 175.1958136497428
At time: 151.69085836410522 and batch: 450, loss is 5.099694185256958 and perplexity is 163.971754651568
At time: 152.33753776550293 and batch: 500, loss is 5.111547060012818 and perplexity is 165.92685521461559
At time: 152.9894495010376 and batch: 550, loss is 5.167294616699219 and perplexity is 175.43956360925972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.328782426550033 and perplexity of 206.18677371843583
Finished 19 epochs...
Completing Train Step...
At time: 154.71451473236084 and batch: 50, loss is 5.187049522399902 and perplexity is 178.93981538419933
At time: 155.37566590309143 and batch: 100, loss is 5.182645092010498 and perplexity is 178.15342050547068
At time: 156.02613520622253 and batch: 150, loss is 5.158656873703003 and perplexity is 173.93068776979175
At time: 156.66506481170654 and batch: 200, loss is 5.10955843925476 and perplexity is 165.59721749685318
At time: 157.33205366134644 and batch: 250, loss is 5.089010753631592 and perplexity is 162.22929790403145
At time: 157.99657273292542 and batch: 300, loss is 5.093382005691528 and perplexity is 162.93999524362383
At time: 158.64334630966187 and batch: 350, loss is 5.142473001480102 and perplexity is 171.13847112596082
At time: 159.29296374320984 and batch: 400, loss is 5.146236085891724 and perplexity is 171.7836928894591
At time: 159.93037152290344 and batch: 450, loss is 5.08560658454895 and perplexity is 161.67798086442357
At time: 160.56755566596985 and batch: 500, loss is 5.101813640594482 and perplexity is 164.31965401051121
At time: 161.20507216453552 and batch: 550, loss is 5.154762830734253 and perplexity is 173.25471119313426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.316627015458777 and perplexity of 203.69565965268134
Finished 20 epochs...
Completing Train Step...
At time: 162.88314747810364 and batch: 50, loss is 5.171327590942383 and perplexity is 176.14853552231497
At time: 163.5351583957672 and batch: 100, loss is 5.164038772583008 and perplexity is 174.86928860535753
At time: 164.17244577407837 and batch: 150, loss is 5.137142000198364 and perplexity is 170.22855924268296
At time: 164.81807160377502 and batch: 200, loss is 5.0911617183685305 and perplexity is 162.57862296137768
At time: 165.4662048816681 and batch: 250, loss is 5.069183464050293 and perplexity is 159.04440883153697
At time: 166.11473393440247 and batch: 300, loss is 5.071765613555908 and perplexity is 159.45561594381624
At time: 166.74955987930298 and batch: 350, loss is 5.124282007217407 and perplexity is 168.0534371695796
At time: 167.39304041862488 and batch: 400, loss is 5.126540241241455 and perplexity is 168.43336998640157
At time: 168.02848315238953 and batch: 450, loss is 5.063173036575318 and perplexity is 158.09135095868533
At time: 168.66372299194336 and batch: 500, loss is 5.080087614059448 and perplexity is 160.788142613388
At time: 169.30006051063538 and batch: 550, loss is 5.132130184173584 and perplexity is 169.37753938043375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3050527369722404 and perplexity of 201.35162080853988
Finished 21 epochs...
Completing Train Step...
At time: 170.97414779663086 and batch: 50, loss is 5.153141136169434 and perplexity is 172.97397266725986
At time: 171.63745188713074 and batch: 100, loss is 5.143852367401123 and perplexity is 171.37469658402625
At time: 172.28476738929749 and batch: 150, loss is 5.123216028213501 and perplexity is 167.87439118060942
At time: 172.92676091194153 and batch: 200, loss is 5.075517921447754 and perplexity is 160.0550664680419
At time: 173.56185007095337 and batch: 250, loss is 5.052040605545044 and perplexity is 156.341169857959
At time: 174.19636368751526 and batch: 300, loss is 5.057136068344116 and perplexity is 157.13983352541283
At time: 174.83652663230896 and batch: 350, loss is 5.109897661209106 and perplexity is 165.65340123744707
At time: 175.47118139266968 and batch: 400, loss is 5.111078519821167 and perplexity is 165.849130024183
At time: 176.1057379245758 and batch: 450, loss is 5.04264494895935 and perplexity is 154.87912112525981
At time: 176.75244545936584 and batch: 500, loss is 5.0628563594818115 and perplexity is 158.0412949753652
At time: 177.3892560005188 and batch: 550, loss is 5.112044038772583 and perplexity is 166.00933783168867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.291519489694149 and perplexity of 198.6450352887137
Finished 22 epochs...
Completing Train Step...
At time: 179.06428360939026 and batch: 50, loss is 5.128849754333496 and perplexity is 168.8228186047581
At time: 179.71368432044983 and batch: 100, loss is 5.121748847961426 and perplexity is 167.62826978541582
At time: 180.3476047515869 and batch: 150, loss is 5.102318553924561 and perplexity is 164.4026421433592
At time: 180.98749566078186 and batch: 200, loss is 5.053397645950318 and perplexity is 156.5534751633217
At time: 181.63844299316406 and batch: 250, loss is 5.030325584411621 and perplexity is 152.9828134072984
At time: 182.30608916282654 and batch: 300, loss is 5.032958946228027 and perplexity is 153.38620340949393
At time: 182.94988441467285 and batch: 350, loss is 5.085360288619995 and perplexity is 161.6381651393618
At time: 183.58469319343567 and batch: 400, loss is 5.086144104003906 and perplexity is 161.76490928534193
At time: 184.21947693824768 and batch: 450, loss is 5.020325059890747 and perplexity is 151.46052953614566
At time: 184.85390305519104 and batch: 500, loss is 5.039823961257935 and perplexity is 154.44282471210343
At time: 185.4872751235962 and batch: 550, loss is 5.086702919006347 and perplexity is 161.85533120572893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.274380623026097 and perplexity of 195.26949362135534
Finished 23 epochs...
Completing Train Step...
At time: 187.23685574531555 and batch: 50, loss is 5.107554521560669 and perplexity is 165.26570657385807
At time: 187.8918216228485 and batch: 100, loss is 5.098484897613526 and perplexity is 163.77358548071066
At time: 188.53622031211853 and batch: 150, loss is 5.082538080215454 and perplexity is 161.18263165877468
At time: 189.18452525138855 and batch: 200, loss is 5.033578014373779 and perplexity is 153.4811893203648
At time: 189.83237648010254 and batch: 250, loss is 5.014564771652221 and perplexity is 150.59058121128214
At time: 190.46980929374695 and batch: 300, loss is 5.015473213195801 and perplexity is 150.72744610879312
At time: 191.109277009964 and batch: 350, loss is 5.066253309249878 and perplexity is 158.57906618945344
At time: 191.7438826560974 and batch: 400, loss is 5.065940570831299 and perplexity is 158.5294801772033
At time: 192.3832631111145 and batch: 450, loss is 4.998788833618164 and perplexity is 148.2335148851283
At time: 193.0241198539734 and batch: 500, loss is 5.022205581665039 and perplexity is 151.74562233747855
At time: 193.66980409622192 and batch: 550, loss is 5.070996541976928 and perplexity is 159.3330303060922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2594851230053195 and perplexity of 192.38241251435667
Finished 24 epochs...
Completing Train Step...
At time: 195.35225653648376 and batch: 50, loss is 5.091148080825806 and perplexity is 162.5764058035793
At time: 196.0036425590515 and batch: 100, loss is 5.081498317718506 and perplexity is 161.01512710078597
At time: 196.64124727249146 and batch: 150, loss is 5.066757307052613 and perplexity is 158.65900983439113
At time: 197.28929615020752 and batch: 200, loss is 5.0187180137634275 and perplexity is 151.21732095475974
At time: 197.92441511154175 and batch: 250, loss is 4.999328660964966 and perplexity is 148.3135569926913
At time: 198.57208824157715 and batch: 300, loss is 4.999755430221557 and perplexity is 148.37686616739538
At time: 199.20761060714722 and batch: 350, loss is 5.04995285987854 and perplexity is 156.0151097418218
At time: 199.85082077980042 and batch: 400, loss is 5.052254476547241 and perplexity is 156.3746102764816
At time: 200.48541116714478 and batch: 450, loss is 4.984666185379028 and perplexity is 146.15477828016188
At time: 201.12227129936218 and batch: 500, loss is 5.004109697341919 and perplexity is 149.02434730708862
At time: 201.75861835479736 and batch: 550, loss is 5.051003208160401 and perplexity is 156.1790660348152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.251475882022939 and perplexity of 190.84772943893668
Finished 25 epochs...
Completing Train Step...
At time: 203.44604563713074 and batch: 50, loss is 5.077599763870239 and perplexity is 160.38862298092678
At time: 204.10739135742188 and batch: 100, loss is 5.065762300491333 and perplexity is 158.50122159178608
At time: 204.75577640533447 and batch: 150, loss is 5.05340838432312 and perplexity is 156.55515630192787
At time: 205.40459299087524 and batch: 200, loss is 5.006700983047486 and perplexity is 149.4110127319772
At time: 206.05401158332825 and batch: 250, loss is 4.985486316680908 and perplexity is 146.27469355516263
At time: 206.70043444633484 and batch: 300, loss is 4.986677780151367 and perplexity is 146.44907837512804
At time: 207.34997129440308 and batch: 350, loss is 5.037099533081054 and perplexity is 154.02262898518873
At time: 207.99845027923584 and batch: 400, loss is 5.039428167343139 and perplexity is 154.381709277255
At time: 208.6367015838623 and batch: 450, loss is 4.970793237686157 and perplexity is 144.14118025051883
At time: 209.27202939987183 and batch: 500, loss is 4.994162054061889 and perplexity is 147.5492552687719
At time: 209.90718388557434 and batch: 550, loss is 5.037346868515015 and perplexity is 154.06072895051003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.245588911340592 and perplexity of 189.7275150208726
Finished 26 epochs...
Completing Train Step...
At time: 211.59425616264343 and batch: 50, loss is 5.066137742996216 and perplexity is 158.56074085977997
At time: 212.243567943573 and batch: 100, loss is 5.054812936782837 and perplexity is 156.77520072758807
At time: 212.8801712989807 and batch: 150, loss is 5.041224098205566 and perplexity is 154.6592172715372
At time: 213.51975655555725 and batch: 200, loss is 4.994133186340332 and perplexity is 147.54499591943397
At time: 214.15940952301025 and batch: 250, loss is 4.9727842807769775 and perplexity is 144.4284574472949
At time: 214.80968356132507 and batch: 300, loss is 4.972909669876099 and perplexity is 144.44656833689206
At time: 215.44705891609192 and batch: 350, loss is 5.026462812423706 and perplexity is 152.39301554173773
At time: 216.09528470039368 and batch: 400, loss is 5.026354236602783 and perplexity is 152.37647024319836
At time: 216.73458790779114 and batch: 450, loss is 4.957049922943115 and perplexity is 142.17375309520992
At time: 217.37762451171875 and batch: 500, loss is 4.982649183273315 and perplexity is 145.86028088538535
At time: 218.0212950706482 and batch: 550, loss is 5.024942932128906 and perplexity is 152.16157232789428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.234283447265625 and perplexity of 187.5946367227673
Finished 27 epochs...
Completing Train Step...
At time: 219.72055172920227 and batch: 50, loss is 5.053357372283935 and perplexity is 156.54717030785267
At time: 220.37012124061584 and batch: 100, loss is 5.04199478149414 and perplexity is 154.77845648764605
At time: 221.0050654411316 and batch: 150, loss is 5.02910361289978 and perplexity is 152.79598693907198
At time: 221.63985657691956 and batch: 200, loss is 4.983554248809814 and perplexity is 145.99235375704546
At time: 222.28743362426758 and batch: 250, loss is 4.961348161697388 and perplexity is 142.78616503411723
At time: 222.92448449134827 and batch: 300, loss is 4.961662139892578 and perplexity is 142.8310038153442
At time: 223.56077885627747 and batch: 350, loss is 5.013836536407471 and perplexity is 150.48095576391867
At time: 224.19831323623657 and batch: 400, loss is 5.0153035640716555 and perplexity is 150.7018774984836
At time: 224.83614349365234 and batch: 450, loss is 4.94640851020813 and perplexity is 140.66884488439
At time: 225.4728491306305 and batch: 500, loss is 4.971278085708618 and perplexity is 144.21108376163858
At time: 226.1079831123352 and batch: 550, loss is 5.01163911819458 and perplexity is 150.15064921475442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.228320831948138 and perplexity of 186.47941020583696
Finished 28 epochs...
Completing Train Step...
At time: 227.7890899181366 and batch: 50, loss is 5.044304866790771 and perplexity is 155.13642122956716
At time: 228.44686913490295 and batch: 100, loss is 5.031865463256836 and perplexity is 153.21856987695998
At time: 229.09274220466614 and batch: 150, loss is 5.017226819992065 and perplexity is 150.9919946719587
At time: 229.72894525527954 and batch: 200, loss is 4.974953069686889 and perplexity is 144.74203219989317
At time: 230.36693024635315 and batch: 250, loss is 4.950840969085693 and perplexity is 141.29373763732022
At time: 231.01857924461365 and batch: 300, loss is 4.952212762832642 and perplexity is 141.48769650846114
At time: 231.6795151233673 and batch: 350, loss is 5.001881141662597 and perplexity is 148.69260803855312
At time: 232.33196687698364 and batch: 400, loss is 5.004245090484619 and perplexity is 149.0445255477762
At time: 232.9680745601654 and batch: 450, loss is 4.9368491458892825 and perplexity is 139.33054697774142
At time: 233.60254096984863 and batch: 500, loss is 4.962323551177978 and perplexity is 142.92550510183634
At time: 234.23578333854675 and batch: 550, loss is 5.003341608047485 and perplexity is 148.90992724934802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.219504173765791 and perplexity of 184.8425115793103
Finished 29 epochs...
Completing Train Step...
At time: 235.90843391418457 and batch: 50, loss is 5.031975336074829 and perplexity is 153.23540535786515
At time: 236.55708146095276 and batch: 100, loss is 5.019968070983887 and perplexity is 151.4064694572724
At time: 237.19312953948975 and batch: 150, loss is 5.007168712615967 and perplexity is 149.48091302643167
At time: 237.82849502563477 and batch: 200, loss is 4.9618093872070315 and perplexity is 142.8520368455673
At time: 238.46328806877136 and batch: 250, loss is 4.940187931060791 and perplexity is 139.79651919963308
At time: 239.10860919952393 and batch: 300, loss is 4.943006277084351 and perplexity is 140.19106989265313
At time: 239.7523205280304 and batch: 350, loss is 4.991607646942139 and perplexity is 147.17283537011224
At time: 240.39823246002197 and batch: 400, loss is 4.997219743728638 and perplexity is 148.00110555887863
At time: 241.03103232383728 and batch: 450, loss is 4.927587251663208 and perplexity is 138.04603985607858
At time: 241.66584515571594 and batch: 500, loss is 4.952336215972901 and perplexity is 141.5051646871317
At time: 242.30335330963135 and batch: 550, loss is 4.993806352615357 and perplexity is 147.49678111835684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.215564808946975 and perplexity of 184.11578185873864
Finished 30 epochs...
Completing Train Step...
At time: 243.9877119064331 and batch: 50, loss is 5.023885354995728 and perplexity is 152.00073479249266
At time: 244.64026260375977 and batch: 100, loss is 5.013521375656128 and perplexity is 150.43353754541053
At time: 245.27887201309204 and batch: 150, loss is 4.999591941833496 and perplexity is 148.35261025555408
At time: 245.91540217399597 and batch: 200, loss is 4.9530983161926265 and perplexity is 141.61304690754255
At time: 246.55740356445312 and batch: 250, loss is 4.933369855880738 and perplexity is 138.8466179506529
At time: 247.20818400382996 and batch: 300, loss is 4.934197807312012 and perplexity is 138.96162380977583
At time: 247.8448464870453 and batch: 350, loss is 4.984188756942749 and perplexity is 146.08501648736976
At time: 248.481924533844 and batch: 400, loss is 4.990016193389892 and perplexity is 146.9388029137552
At time: 249.12476682662964 and batch: 450, loss is 4.920767021179199 and perplexity is 137.10773740348523
At time: 249.76087307929993 and batch: 500, loss is 4.9457440853118895 and perplexity is 140.5754120447131
At time: 250.41387510299683 and batch: 550, loss is 4.988691272735596 and perplexity is 146.74424957118833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.215476502763464 and perplexity of 184.09952401456331
Finished 31 epochs...
Completing Train Step...
At time: 252.1070375442505 and batch: 50, loss is 5.01656590461731 and perplexity is 150.89223471129225
At time: 252.7564799785614 and batch: 100, loss is 5.004151258468628 and perplexity is 149.03054105557845
At time: 253.39071249961853 and batch: 150, loss is 4.992507886886597 and perplexity is 147.30538588993588
At time: 254.0259029865265 and batch: 200, loss is 4.945155305862427 and perplexity is 140.49266849224335
At time: 254.65940761566162 and batch: 250, loss is 4.924482717514038 and perplexity is 137.61813577628473
At time: 255.29353213310242 and batch: 300, loss is 4.925282936096192 and perplexity is 137.72830443940111
At time: 255.92738437652588 and batch: 350, loss is 4.974825859069824 and perplexity is 144.72362064775905
At time: 256.5628204345703 and batch: 400, loss is 4.979595394134521 and perplexity is 145.415533771418
At time: 257.1962294578552 and batch: 450, loss is 4.910405216217041 and perplexity is 135.69438883636602
At time: 257.83224534988403 and batch: 500, loss is 4.937587118148803 and perplexity is 139.43340700557027
At time: 258.46792221069336 and batch: 550, loss is 4.978371229171753 and perplexity is 145.2376300838802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.205495631441157 and perplexity of 182.27118971741325
Finished 32 epochs...
Completing Train Step...
At time: 260.123051404953 and batch: 50, loss is 5.009193429946899 and perplexity is 149.78387622560052
At time: 260.7679228782654 and batch: 100, loss is 4.994742593765259 and perplexity is 147.63493833846348
At time: 261.40016508102417 and batch: 150, loss is 4.983249979019165 and perplexity is 145.94793945143982
At time: 262.0330159664154 and batch: 200, loss is 4.938829326629639 and perplexity is 139.60671998940725
At time: 262.66535902023315 and batch: 250, loss is 4.9169024467468265 and perplexity is 136.57889687700703
At time: 263.3113944530487 and batch: 300, loss is 4.919537286758423 and perplexity is 136.93923492732765
At time: 263.951642036438 and batch: 350, loss is 4.964543943405151 and perplexity is 143.24320836483022
At time: 264.59009051322937 and batch: 400, loss is 4.9732492065429685 and perplexity is 144.49562157046034
At time: 265.22728204727173 and batch: 450, loss is 4.904777793884278 and perplexity is 134.9329237544991
At time: 265.87196707725525 and batch: 500, loss is 4.933337526321411 and perplexity is 138.84212917324106
At time: 266.5031747817993 and batch: 550, loss is 4.973588762283325 and perplexity is 144.54469421919924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2082506545046545 and perplexity of 182.77404341760536
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 268.1585884094238 and batch: 50, loss is 4.996649990081787 and perplexity is 147.9168054066885
At time: 268.8026599884033 and batch: 100, loss is 4.958947467803955 and perplexity is 142.44379029256368
At time: 269.4338858127594 and batch: 150, loss is 4.939725790023804 and perplexity is 139.73192841745808
At time: 270.06426310539246 and batch: 200, loss is 4.889298582077027 and perplexity is 132.86035072959623
At time: 270.695702791214 and batch: 250, loss is 4.8554693126678465 and perplexity is 128.44095603983087
At time: 271.3324496746063 and batch: 300, loss is 4.849744243621826 and perplexity is 127.70772360405324
At time: 271.9675257205963 and batch: 350, loss is 4.895416526794434 and perplexity is 133.6756745203565
At time: 272.60245013237 and batch: 400, loss is 4.893629951477051 and perplexity is 133.43706606917547
At time: 273.2372236251831 and batch: 450, loss is 4.814672613143921 and perplexity is 123.30643679465612
At time: 273.875919342041 and batch: 500, loss is 4.833027153015137 and perplexity is 125.59056766621183
At time: 274.51000690460205 and batch: 550, loss is 4.875208759307862 and perplexity is 131.00149813854546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.138233590633311 and perplexity of 170.41448056621476
Finished 34 epochs...
Completing Train Step...
At time: 276.18343806266785 and batch: 50, loss is 4.952468452453613 and perplexity is 141.52387806938114
At time: 276.83468770980835 and batch: 100, loss is 4.931714735031128 and perplexity is 138.6170000934548
At time: 277.46913027763367 and batch: 150, loss is 4.918106927871704 and perplexity is 136.7435026927505
At time: 278.10489892959595 and batch: 200, loss is 4.870871353149414 and perplexity is 130.43452192569313
At time: 278.738644361496 and batch: 250, loss is 4.84150318145752 and perplexity is 126.65960106839559
At time: 279.3879625797272 and batch: 300, loss is 4.838719968795776 and perplexity is 126.30757057807332
At time: 280.02333641052246 and batch: 350, loss is 4.8861338233947755 and perplexity is 132.44054442431027
At time: 280.65922927856445 and batch: 400, loss is 4.886549911499023 and perplexity is 132.4956628256257
At time: 281.29484009742737 and batch: 450, loss is 4.810787143707276 and perplexity is 122.82826296961743
At time: 281.9308741092682 and batch: 500, loss is 4.833590383529663 and perplexity is 125.66132403045934
At time: 282.5672240257263 and batch: 550, loss is 4.878875255584717 and perplexity is 131.48269626022375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.13473283483627 and perplexity of 169.81894410753387
Finished 35 epochs...
Completing Train Step...
At time: 284.2435564994812 and batch: 50, loss is 4.943219623565674 and perplexity is 140.22098235487348
At time: 284.89286279678345 and batch: 100, loss is 4.923648099899292 and perplexity is 137.50332517419747
At time: 285.52862644195557 and batch: 150, loss is 4.91094820022583 and perplexity is 135.76808872670682
At time: 286.1645493507385 and batch: 200, loss is 4.864334878921508 and perplexity is 129.5847204132196
At time: 286.80003571510315 and batch: 250, loss is 4.836322269439697 and perplexity is 126.00508577615223
At time: 287.43504786491394 and batch: 300, loss is 4.834125385284424 and perplexity is 125.72857104635298
At time: 288.06945538520813 and batch: 350, loss is 4.882359676361084 and perplexity is 131.94163640475318
At time: 288.7162699699402 and batch: 400, loss is 4.884313306808472 and perplexity is 132.19965355592905
At time: 289.3579728603363 and batch: 450, loss is 4.809943675994873 and perplexity is 122.72470497568375
At time: 290.01240634918213 and batch: 500, loss is 4.8343652153015135 and perplexity is 125.75872814784019
At time: 290.6535007953644 and batch: 550, loss is 4.879808187484741 and perplexity is 131.60541789843066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1329098965259305 and perplexity of 169.509656640135
Finished 36 epochs...
Completing Train Step...
At time: 292.3474361896515 and batch: 50, loss is 4.937619562149048 and perplexity is 139.43793085644677
At time: 292.9961543083191 and batch: 100, loss is 4.9185851955413815 and perplexity is 136.80891833097243
At time: 293.6317858695984 and batch: 150, loss is 4.906716260910034 and perplexity is 135.19474045742413
At time: 294.2669837474823 and batch: 200, loss is 4.860463380813599 and perplexity is 129.08400330141757
At time: 294.92553329467773 and batch: 250, loss is 4.8330189228057865 and perplexity is 125.589534033801
At time: 295.5825090408325 and batch: 300, loss is 4.830942583084107 and perplexity is 125.32903802834832
At time: 296.2173635959625 and batch: 350, loss is 4.88023681640625 and perplexity is 131.6618398779515
At time: 296.8733916282654 and batch: 400, loss is 4.882948999404907 and perplexity is 132.01941556780176
At time: 297.51165223121643 and batch: 450, loss is 4.8092066764831545 and perplexity is 122.63429024993776
At time: 298.1476242542267 and batch: 500, loss is 4.834459295272827 and perplexity is 125.77056008194208
At time: 298.7828645706177 and batch: 550, loss is 4.879572410583496 and perplexity is 131.57439203854574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.131607380319149 and perplexity of 169.28901129341995
Finished 37 epochs...
Completing Train Step...
At time: 300.45878648757935 and batch: 50, loss is 4.933415975570679 and perplexity is 138.85302166128955
At time: 301.10958766937256 and batch: 100, loss is 4.91467396736145 and perplexity is 136.27487250268305
At time: 301.7454721927643 and batch: 150, loss is 4.903448657989502 and perplexity is 134.75369869576434
At time: 302.38209223747253 and batch: 200, loss is 4.857574481964111 and perplexity is 128.71163080500793
At time: 303.0194790363312 and batch: 250, loss is 4.830605812072754 and perplexity is 125.28683794774044
At time: 303.6702446937561 and batch: 300, loss is 4.828599939346313 and perplexity is 125.03578037546558
At time: 304.32217168807983 and batch: 350, loss is 4.878613719940185 and perplexity is 131.44831334488754
At time: 304.97487688064575 and batch: 400, loss is 4.881659870147705 and perplexity is 131.84933512806413
At time: 305.61929512023926 and batch: 450, loss is 4.808513240814209 and perplexity is 122.54928073656346
At time: 306.25680017471313 and batch: 500, loss is 4.8340108108520505 and perplexity is 125.71416659189904
At time: 306.89981055259705 and batch: 550, loss is 4.878847227096558 and perplexity is 131.4790110506742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.130545433531416 and perplexity of 169.10933079412877
Finished 38 epochs...
Completing Train Step...
At time: 308.5799775123596 and batch: 50, loss is 4.929923572540283 and perplexity is 138.36893674949357
At time: 309.2300617694855 and batch: 100, loss is 4.911156024932861 and perplexity is 135.7963076221609
At time: 309.8642773628235 and batch: 150, loss is 4.900481204986573 and perplexity is 134.3544161470123
At time: 310.5164375305176 and batch: 200, loss is 4.854876403808594 and perplexity is 128.36482483076836
At time: 311.15442848205566 and batch: 250, loss is 4.828637409210205 and perplexity is 125.04046553691346
At time: 311.80371356010437 and batch: 300, loss is 4.826396808624268 and perplexity is 124.76061343203894
At time: 312.43911480903625 and batch: 350, loss is 4.8769215965271 and perplexity is 131.22607465693733
At time: 313.07464814186096 and batch: 400, loss is 4.880173645019531 and perplexity is 131.65352287964936
At time: 313.71010398864746 and batch: 450, loss is 4.807489519119263 and perplexity is 122.4238885734632
At time: 314.3452425003052 and batch: 500, loss is 4.833073959350586 and perplexity is 125.59644623802741
At time: 314.97901916503906 and batch: 550, loss is 4.877970790863037 and perplexity is 131.36382856386734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.129142274247839 and perplexity of 168.87220986480702
Finished 39 epochs...
Completing Train Step...
At time: 316.6537215709686 and batch: 50, loss is 4.926631145477295 and perplexity is 137.91411625996415
At time: 317.30143785476685 and batch: 100, loss is 4.908523836135864 and perplexity is 135.4393361168668
At time: 317.93473196029663 and batch: 150, loss is 4.8982306003570555 and perplexity is 134.05237748832855
At time: 318.57022047042847 and batch: 200, loss is 4.852621784210205 and perplexity is 128.07573699497158
At time: 319.20438599586487 and batch: 250, loss is 4.826570329666137 and perplexity is 124.78226390201766
At time: 319.8541600704193 and batch: 300, loss is 4.824617309570312 and perplexity is 124.53879945587941
At time: 320.50161623954773 and batch: 350, loss is 4.875412330627442 and perplexity is 131.0281690010077
At time: 321.1472508907318 and batch: 400, loss is 4.878726568222046 and perplexity is 131.46314789821315
At time: 321.7839548587799 and batch: 450, loss is 4.806552610397339 and perplexity is 122.30924227942967
At time: 322.4197416305542 and batch: 500, loss is 4.831993646621704 and perplexity is 125.46083606234257
At time: 323.0550808906555 and batch: 550, loss is 4.8767846584320065 and perplexity is 131.20810603856947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.127238821476064 and perplexity of 168.55107531804504
Finished 40 epochs...
Completing Train Step...
At time: 324.78451585769653 and batch: 50, loss is 4.923868589401245 and perplexity is 137.53364655653235
At time: 325.44802236557007 and batch: 100, loss is 4.906087656021118 and perplexity is 135.1097830877233
At time: 326.0923686027527 and batch: 150, loss is 4.896273097991943 and perplexity is 133.7902263068347
At time: 326.7320861816406 and batch: 200, loss is 4.850453319549561 and perplexity is 127.79831018923467
At time: 327.3723990917206 and batch: 250, loss is 4.8246669006347656 and perplexity is 124.54497562065016
At time: 328.05389738082886 and batch: 300, loss is 4.822713203430176 and perplexity is 124.30188998511787
At time: 328.6990361213684 and batch: 350, loss is 4.873706712722778 and perplexity is 130.8048754907813
At time: 329.34040117263794 and batch: 400, loss is 4.87726731300354 and perplexity is 131.2714495160511
At time: 329.9769790172577 and batch: 450, loss is 4.805424976348877 and perplexity is 122.17139994585433
At time: 330.63129138946533 and batch: 500, loss is 4.830891838073731 and perplexity is 125.32267836637509
At time: 331.2795743942261 and batch: 550, loss is 4.875397653579712 and perplexity is 131.02624590842998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.12524965976147 and perplexity of 168.21613320951863
Finished 41 epochs...
Completing Train Step...
At time: 332.96586561203003 and batch: 50, loss is 4.921342239379883 and perplexity is 137.18662695667248
At time: 333.6247730255127 and batch: 100, loss is 4.904109449386596 and perplexity is 134.84277220685294
At time: 334.2711491584778 and batch: 150, loss is 4.8942275238037105 and perplexity is 133.51682819665558
At time: 334.908620595932 and batch: 200, loss is 4.848366756439209 and perplexity is 127.53192895690111
At time: 335.5456268787384 and batch: 250, loss is 4.822672281265259 and perplexity is 124.2968033867544
At time: 336.19201612472534 and batch: 300, loss is 4.820967769622802 and perplexity is 124.08511849919097
At time: 336.82931089401245 and batch: 350, loss is 4.8721952152252195 and perplexity is 130.60731359357808
At time: 337.46517539024353 and batch: 400, loss is 4.875885515213013 and perplexity is 131.09018418201134
At time: 338.101544380188 and batch: 450, loss is 4.804227647781372 and perplexity is 122.02520817582337
At time: 338.74440002441406 and batch: 500, loss is 4.829723482131958 and perplexity is 125.17634237337671
At time: 339.3817298412323 and batch: 550, loss is 4.873992080688477 and perplexity is 130.84220833854965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.123964025619182 and perplexity of 168.0000077642753
Finished 42 epochs...
Completing Train Step...
At time: 341.060644865036 and batch: 50, loss is 4.919118318557739 and perplexity is 136.88187375955883
At time: 341.71286940574646 and batch: 100, loss is 4.9020360088348385 and perplexity is 134.56347338965702
At time: 342.3480269908905 and batch: 150, loss is 4.892385864257813 and perplexity is 133.27116194170674
At time: 342.98498582839966 and batch: 200, loss is 4.846354341506958 and perplexity is 127.27553986587444
At time: 343.6158447265625 and batch: 250, loss is 4.820798931121826 and perplexity is 124.06416992230298
At time: 344.2591233253479 and batch: 300, loss is 4.819298543930054 and perplexity is 123.87816520523518
At time: 344.88986563682556 and batch: 350, loss is 4.8706653118133545 and perplexity is 130.40764979100763
At time: 345.5214867591858 and batch: 400, loss is 4.87446177482605 and perplexity is 130.90367859173207
At time: 346.15174651145935 and batch: 450, loss is 4.803007574081421 and perplexity is 121.87641921380371
At time: 346.7827980518341 and batch: 500, loss is 4.828500232696533 and perplexity is 125.0233140981974
At time: 347.43212389945984 and batch: 550, loss is 4.872428874969483 and perplexity is 130.63783483072464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.12227257261885 and perplexity of 167.71608383676858
Finished 43 epochs...
Completing Train Step...
At time: 349.11985445022583 and batch: 50, loss is 4.917033491134643 and perplexity is 136.59679594769773
At time: 349.7692618370056 and batch: 100, loss is 4.90009090423584 and perplexity is 134.3019877496128
At time: 350.40550899505615 and batch: 150, loss is 4.890477199554443 and perplexity is 133.01703457804493
At time: 351.0418725013733 and batch: 200, loss is 4.844173717498779 and perplexity is 126.9983021534531
At time: 351.67604517936707 and batch: 250, loss is 4.819020862579346 and perplexity is 123.84377132448839
At time: 352.30952525138855 and batch: 300, loss is 4.81741250038147 and perplexity is 123.64474577973272
At time: 352.9427583217621 and batch: 350, loss is 4.869083518981934 and perplexity is 130.2015349638599
At time: 353.57583808898926 and batch: 400, loss is 4.872925853729248 and perplexity is 130.70277519551152
At time: 354.21025586128235 and batch: 450, loss is 4.80151439666748 and perplexity is 121.69457189628228
At time: 354.8470723628998 and batch: 500, loss is 4.827100114822388 and perplexity is 124.84838920775236
At time: 355.482558965683 and batch: 550, loss is 4.870866546630859 and perplexity is 130.43389499125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.120328213306183 and perplexity of 167.3903003300105
Finished 44 epochs...
Completing Train Step...
At time: 357.1593813896179 and batch: 50, loss is 4.915126123428345 and perplexity is 136.3365039455158
At time: 357.81054639816284 and batch: 100, loss is 4.898063039779663 and perplexity is 134.02991747631458
At time: 358.44856214523315 and batch: 150, loss is 4.888402690887451 and perplexity is 132.74137561427017
At time: 359.09097623825073 and batch: 200, loss is 4.842096128463745 and perplexity is 126.73472576994119
At time: 359.7306435108185 and batch: 250, loss is 4.817123994827271 and perplexity is 123.60907872914626
At time: 360.3820323944092 and batch: 300, loss is 4.815554552078247 and perplexity is 123.41523351109637
At time: 361.02074098587036 and batch: 350, loss is 4.867410707473755 and perplexity is 129.9839144076225
At time: 361.67191767692566 and batch: 400, loss is 4.8714532470703125 and perplexity is 130.51044306796032
At time: 362.30904841423035 and batch: 450, loss is 4.800082759857178 and perplexity is 121.52047411967004
At time: 362.94562911987305 and batch: 500, loss is 4.825651111602784 and perplexity is 124.66761449302948
At time: 363.58237957954407 and batch: 550, loss is 4.869230995178222 and perplexity is 130.2207380069484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.11884908473238 and perplexity of 167.14289157360258
Finished 45 epochs...
Completing Train Step...
At time: 365.284143447876 and batch: 50, loss is 4.913330545425415 and perplexity is 136.09192076783518
At time: 365.94006395339966 and batch: 100, loss is 4.896176300048828 and perplexity is 133.7772763148957
At time: 366.58443546295166 and batch: 150, loss is 4.8864071083068845 and perplexity is 132.4767433729409
At time: 367.2187190055847 and batch: 200, loss is 4.840161199569702 and perplexity is 126.4897401784546
At time: 367.8661024570465 and batch: 250, loss is 4.815173597335815 and perplexity is 123.36822684687634
At time: 368.5030839443207 and batch: 300, loss is 4.8137115955352785 and perplexity is 123.1879940595302
At time: 369.1362466812134 and batch: 350, loss is 4.865348310470581 and perplexity is 129.71611222422194
At time: 369.76970744132996 and batch: 400, loss is 4.86985447883606 and perplexity is 130.30195382474392
At time: 370.4057331085205 and batch: 450, loss is 4.798455677032471 and perplexity is 121.32291101273438
At time: 371.0406515598297 and batch: 500, loss is 4.824177598953247 and perplexity is 124.48405046126126
At time: 371.6838788986206 and batch: 550, loss is 4.86737018585205 and perplexity is 129.9786473553307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.117961153070977 and perplexity of 166.994545978318
Finished 46 epochs...
Completing Train Step...
At time: 373.35204339027405 and batch: 50, loss is 4.911541872024536 and perplexity is 135.84871434235717
At time: 374.0030653476715 and batch: 100, loss is 4.894201583862305 and perplexity is 133.51336482287556
At time: 374.6380105018616 and batch: 150, loss is 4.884362907409668 and perplexity is 132.20621090084612
At time: 375.2792851924896 and batch: 200, loss is 4.838356008529663 and perplexity is 126.26160800585603
At time: 375.93786001205444 and batch: 250, loss is 4.813349370956421 and perplexity is 123.1433804208277
At time: 376.5910978317261 and batch: 300, loss is 4.812053833007813 and perplexity is 122.98394679682741
At time: 377.22599267959595 and batch: 350, loss is 4.86353310585022 and perplexity is 129.48086451393704
At time: 377.860693693161 and batch: 400, loss is 4.868181772232056 and perplexity is 130.0841790734758
At time: 378.50396156311035 and batch: 450, loss is 4.796991834640503 and perplexity is 121.14544331653576
At time: 379.14200711250305 and batch: 500, loss is 4.822707433700561 and perplexity is 124.301172798891
At time: 379.77688336372375 and batch: 550, loss is 4.865704708099365 and perplexity is 129.76235097823547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.116744670462101 and perplexity of 166.79152352903404
Finished 47 epochs...
Completing Train Step...
At time: 381.4757697582245 and batch: 50, loss is 4.909834384918213 and perplexity is 135.61695233583472
At time: 382.1251792907715 and batch: 100, loss is 4.892235765457153 and perplexity is 133.25115960133795
At time: 382.7621991634369 and batch: 150, loss is 4.882333927154541 and perplexity is 131.9382390560455
At time: 383.3990230560303 and batch: 200, loss is 4.836559228897094 and perplexity is 126.03494741076567
At time: 384.0370080471039 and batch: 250, loss is 4.811623792648316 and perplexity is 122.93107010650492
At time: 384.6738784313202 and batch: 300, loss is 4.810387802124024 and perplexity is 122.77922232925023
At time: 385.3173429965973 and batch: 350, loss is 4.861805114746094 and perplexity is 129.25731593250794
At time: 385.9701347351074 and batch: 400, loss is 4.866361093521118 and perplexity is 129.847553053351
At time: 386.6107656955719 and batch: 450, loss is 4.795377531051636 and perplexity is 120.950035558773
At time: 387.24737000465393 and batch: 500, loss is 4.820988130569458 and perplexity is 124.08764501539055
At time: 387.88338446617126 and batch: 550, loss is 4.863666076660156 and perplexity is 129.4980828341043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1154327392578125 and perplexity of 166.5728479996955
Finished 48 epochs...
Completing Train Step...
At time: 389.5604336261749 and batch: 50, loss is 4.907794723510742 and perplexity is 135.34062157834705
At time: 390.21945786476135 and batch: 100, loss is 4.890142469406128 and perplexity is 132.97251721739926
At time: 390.87731432914734 and batch: 150, loss is 4.879932374954223 and perplexity is 131.62176265713572
At time: 391.52145171165466 and batch: 200, loss is 4.834415998458862 and perplexity is 125.76511473528384
At time: 392.15405225753784 and batch: 250, loss is 4.8095560550689695 and perplexity is 122.67714353041107
At time: 392.7998585700989 and batch: 300, loss is 4.808376760482788 and perplexity is 122.5325563114166
At time: 393.43290615081787 and batch: 350, loss is 4.859828462600708 and perplexity is 129.00207152946817
At time: 394.0682418346405 and batch: 400, loss is 4.864419717788696 and perplexity is 129.59571470046913
At time: 394.7089433670044 and batch: 450, loss is 4.793465166091919 and perplexity is 120.71895597356894
At time: 395.34519505500793 and batch: 500, loss is 4.819216279983521 and perplexity is 123.8679749176285
At time: 395.9805431365967 and batch: 550, loss is 4.8617497825622555 and perplexity is 129.25016404080702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.114116912192487 and perplexity of 166.35381107690822
Finished 49 epochs...
Completing Train Step...
At time: 397.7191560268402 and batch: 50, loss is 4.905961389541626 and perplexity is 135.09272432806534
At time: 398.36851692199707 and batch: 100, loss is 4.8883398914337155 and perplexity is 132.73303979013878
At time: 399.0078008174896 and batch: 150, loss is 4.878093395233154 and perplexity is 131.37993533068448
At time: 399.64079880714417 and batch: 200, loss is 4.832542629241943 and perplexity is 125.5297307901352
At time: 400.2708911895752 and batch: 250, loss is 4.807738466262817 and perplexity is 122.45436944473192
At time: 400.9024157524109 and batch: 300, loss is 4.806388702392578 and perplexity is 122.28919645844398
At time: 401.5345573425293 and batch: 350, loss is 4.858161182403564 and perplexity is 128.78716813208868
At time: 402.16787815093994 and batch: 400, loss is 4.862819662094116 and perplexity is 129.38852014478846
At time: 402.80121064186096 and batch: 450, loss is 4.792026796340942 and perplexity is 120.54544229725406
At time: 403.43083214759827 and batch: 500, loss is 4.817770977020263 and perplexity is 123.6890774780687
At time: 404.06004309654236 and batch: 550, loss is 4.860290851593017 and perplexity is 129.06173446000665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.113086456948138 and perplexity of 166.18247920993755
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fec82176cc0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 3.3079619350335183, 'anneal': 5.793669854009312, 'dropout': 0.4746858094026927, 'seq_len': 35, 'batch_size': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8805570602416992 and batch: 50, loss is 6.798534984588623 and perplexity is 896.5328945721626
At time: 1.5138535499572754 and batch: 100, loss is 6.050059680938721 and perplexity is 424.13834226384193
At time: 2.1468992233276367 and batch: 150, loss is 5.847179641723633 and perplexity is 346.25643484668996
At time: 2.789548635482788 and batch: 200, loss is 5.6246026992797855 and perplexity is 277.1621459253183
At time: 3.434515953063965 and batch: 250, loss is 5.498733568191528 and perplexity is 244.3822427596355
At time: 4.066398859024048 and batch: 300, loss is 5.43585132598877 and perplexity is 229.48813435078551
At time: 4.698376417160034 and batch: 350, loss is 5.428982086181641 and perplexity is 227.91712731259764
At time: 5.331268787384033 and batch: 400, loss is 5.386702823638916 and perplexity is 218.48182354513298
At time: 5.967123031616211 and batch: 450, loss is 5.264659509658814 and perplexity is 193.3804533999394
At time: 6.602068185806274 and batch: 500, loss is 5.259090309143066 and perplexity is 192.3064722631607
At time: 7.237835645675659 and batch: 550, loss is 5.276965494155884 and perplexity is 195.77489301268847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.131136630443817 and perplexity of 169.20933726719872
Finished 1 epochs...
Completing Train Step...
At time: 8.909938335418701 and batch: 50, loss is 5.023813486099243 and perplexity is 151.98981105996128
At time: 9.541455030441284 and batch: 100, loss is 4.941928062438965 and perplexity is 140.03999528800418
At time: 10.172643184661865 and batch: 150, loss is 4.870314512252808 and perplexity is 130.36191086783512
At time: 10.805673837661743 and batch: 200, loss is 4.762873783111572 and perplexity is 117.08191091198002
At time: 11.434253215789795 and batch: 250, loss is 4.709067211151123 and perplexity is 110.94861998724362
At time: 12.062530517578125 and batch: 300, loss is 4.676024513244629 and perplexity is 107.34248457362933
At time: 12.690271615982056 and batch: 350, loss is 4.695931921005249 and perplexity is 109.50080720945718
At time: 13.319528341293335 and batch: 400, loss is 4.666534595489502 and perplexity is 106.32863152325876
At time: 13.948221683502197 and batch: 450, loss is 4.5760955810546875 and perplexity is 97.13439947093002
At time: 14.576671361923218 and batch: 500, loss is 4.589942073822021 and perplexity is 98.48872492129783
At time: 15.21410608291626 and batch: 550, loss is 4.6172847652435305 and perplexity is 101.21882579990881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.71913991075881 and perplexity of 112.07181943702341
Finished 2 epochs...
Completing Train Step...
At time: 16.876834869384766 and batch: 50, loss is 4.5568882179260255 and perplexity is 95.28650716770338
At time: 17.50199556350708 and batch: 100, loss is 4.5232186698913575 and perplexity is 92.13166266397052
At time: 18.126728296279907 and batch: 150, loss is 4.49859845161438 and perplexity is 89.89105630620962
At time: 18.765967845916748 and batch: 200, loss is 4.432393341064453 and perplexity is 84.13253399184174
At time: 19.39603018760681 and batch: 250, loss is 4.411699934005737 and perplexity is 82.40943510037711
At time: 20.024535417556763 and batch: 300, loss is 4.380119590759278 and perplexity is 79.8475818670612
At time: 20.654220819473267 and batch: 350, loss is 4.41980131149292 and perplexity is 83.07977672155454
At time: 21.280927181243896 and batch: 400, loss is 4.398176803588867 and perplexity is 81.30250302448394
At time: 21.90644097328186 and batch: 450, loss is 4.330537767410278 and perplexity is 75.98513791477772
At time: 22.531533002853394 and batch: 500, loss is 4.355198602676392 and perplexity is 77.88229143249526
At time: 23.166423320770264 and batch: 550, loss is 4.398433933258056 and perplexity is 81.32341099810589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.596413794984209 and perplexity of 99.12818345338724
Finished 3 epochs...
Completing Train Step...
At time: 24.82224726676941 and batch: 50, loss is 4.351304717063904 and perplexity is 77.5796163718891
At time: 25.44915008544922 and batch: 100, loss is 4.323964023590088 and perplexity is 75.48726930723623
At time: 26.077395915985107 and batch: 150, loss is 4.313640279769897 and perplexity is 74.71196697450016
At time: 26.704780101776123 and batch: 200, loss is 4.262690992355346 and perplexity is 71.00078919168224
At time: 27.33235216140747 and batch: 250, loss is 4.249383435249329 and perplexity is 70.06220114322498
At time: 27.975412845611572 and batch: 300, loss is 4.215518951416016 and perplexity is 67.72930497112088
At time: 28.606056451797485 and batch: 350, loss is 4.261197838783264 and perplexity is 70.89485321870036
At time: 29.231841564178467 and batch: 400, loss is 4.24340678691864 and perplexity is 69.64471283884075
At time: 29.857542753219604 and batch: 450, loss is 4.184434604644776 and perplexity is 65.65636861465568
At time: 30.484199285507202 and batch: 500, loss is 4.210261144638062 and perplexity is 67.3741319062672
At time: 31.111109972000122 and batch: 550, loss is 4.261920328140259 and perplexity is 70.94609250330659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.530097474443152 and perplexity of 92.76760311187704
Finished 4 epochs...
Completing Train Step...
At time: 32.77948045730591 and batch: 50, loss is 4.216297459602356 and perplexity is 67.78205331934238
At time: 33.41268467903137 and batch: 100, loss is 4.191994972229004 and perplexity is 66.1546360647905
At time: 34.04452037811279 and batch: 150, loss is 4.18875150680542 and perplexity is 65.94041338982719
At time: 34.67650055885315 and batch: 200, loss is 4.1468470096588135 and perplexity is 63.23430848649095
At time: 35.31213402748108 and batch: 250, loss is 4.136462736129761 and perplexity is 62.58106373693577
At time: 35.94375419616699 and batch: 300, loss is 4.100746874809265 and perplexity is 60.38537107191226
At time: 36.57376980781555 and batch: 350, loss is 4.149781455993653 and perplexity is 63.420138692300135
At time: 37.2048819065094 and batch: 400, loss is 4.1353796863555905 and perplexity is 62.51332202043527
At time: 37.83400082588196 and batch: 450, loss is 4.078938231468201 and perplexity is 59.08270437829507
At time: 38.46351432800293 and batch: 500, loss is 4.105512075424194 and perplexity is 60.67380615900968
At time: 39.09614896774292 and batch: 550, loss is 4.1623337888717655 and perplexity is 64.22122662722612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.492533257667055 and perplexity of 89.34749967009024
Finished 5 epochs...
Completing Train Step...
At time: 40.77878737449646 and batch: 50, loss is 4.116864552497864 and perplexity is 61.36652877143824
At time: 41.42670822143555 and batch: 100, loss is 4.094106492996215 and perplexity is 59.98571754662065
At time: 42.07034611701965 and batch: 150, loss is 4.09499990940094 and perplexity is 60.039333717943144
At time: 42.705315828323364 and batch: 200, loss is 4.059541444778443 and perplexity is 57.9477327501793
At time: 43.34470844268799 and batch: 250, loss is 4.050475392341614 and perplexity is 57.424749843833375
At time: 43.972097396850586 and batch: 300, loss is 4.013418207168579 and perplexity is 55.33569653103079
At time: 44.599735260009766 and batch: 350, loss is 4.06418598651886 and perplexity is 58.21749939998751
At time: 45.227314710617065 and batch: 400, loss is 4.052699317932129 and perplexity is 57.5526003268264
At time: 45.855884075164795 and batch: 450, loss is 3.9965753841400145 and perplexity is 54.411492140830916
At time: 46.4845712184906 and batch: 500, loss is 4.023043217658997 and perplexity is 55.87087460632579
At time: 47.13017201423645 and batch: 550, loss is 4.082681126594544 and perplexity is 59.30425911398245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.465787846991357 and perplexity of 86.9895369945134
Finished 6 epochs...
Completing Train Step...
At time: 48.768089294433594 and batch: 50, loss is 4.0381246852874755 and perplexity is 56.719875389904466
At time: 49.40994334220886 and batch: 100, loss is 4.0170535516738894 and perplexity is 55.537226945657565
At time: 50.037458658218384 and batch: 150, loss is 4.020976982116699 and perplexity is 55.7555514029152
At time: 50.66367268562317 and batch: 200, loss is 3.990378885269165 and perplexity is 54.075373845021815
At time: 51.28970646858215 and batch: 250, loss is 3.9801491117477417 and perplexity is 53.5250148409856
At time: 51.91559720039368 and batch: 300, loss is 3.942288398742676 and perplexity is 51.53640229951413
At time: 52.5434136390686 and batch: 350, loss is 3.995907006263733 and perplexity is 54.37513685415658
At time: 53.172602891922 and batch: 400, loss is 3.986323757171631 and perplexity is 53.85653528623534
At time: 53.8013231754303 and batch: 450, loss is 3.9284938907623292 and perplexity is 50.83036390870049
At time: 54.42907953262329 and batch: 500, loss is 3.955193519592285 and perplexity is 52.20579581126589
At time: 55.05828070640564 and batch: 550, loss is 4.016454658508301 and perplexity is 55.503976037868625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.454079810609209 and perplexity of 85.97699931090453
Finished 7 epochs...
Completing Train Step...
At time: 56.71388506889343 and batch: 50, loss is 3.9734698867797853 and perplexity is 53.168700501922736
At time: 57.360989570617676 and batch: 100, loss is 3.9535672569274904 and perplexity is 52.120964472368094
At time: 57.98855471611023 and batch: 150, loss is 3.9592669200897217 and perplexity is 52.41888462930003
At time: 58.61747407913208 and batch: 200, loss is 3.93258590221405 and perplexity is 51.03878848695919
At time: 59.24591660499573 and batch: 250, loss is 3.920631160736084 and perplexity is 50.43226560086119
At time: 59.87443232536316 and batch: 300, loss is 3.8840204858779908 and perplexity is 48.61929584115364
At time: 60.50367569923401 and batch: 350, loss is 3.9399281120300294 and perplexity is 51.4149050545201
At time: 61.13170576095581 and batch: 400, loss is 3.930506863594055 and perplexity is 50.93278710324402
At time: 61.760642290115356 and batch: 450, loss is 3.870353627204895 and perplexity is 47.959342810808806
At time: 62.38938355445862 and batch: 500, loss is 3.8985201930999756 and perplexity is 49.3293970852385
At time: 63.041364908218384 and batch: 550, loss is 3.9604639625549316 and perplexity is 52.4816698309594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4498482562126 and perplexity of 85.6139516311949
Finished 8 epochs...
Completing Train Step...
At time: 64.7153148651123 and batch: 50, loss is 3.9184115028381346 and perplexity is 50.32044736922883
At time: 65.35847616195679 and batch: 100, loss is 3.899260392189026 and perplexity is 49.36592417701668
At time: 65.98725914955139 and batch: 150, loss is 3.9069139194488525 and perplexity is 49.745197160098265
At time: 66.61828970909119 and batch: 200, loss is 3.882768430709839 and perplexity is 48.55845989345259
At time: 67.24940037727356 and batch: 250, loss is 3.869542145729065 and perplexity is 47.920440478920476
At time: 67.88078594207764 and batch: 300, loss is 3.8351450157165528 and perplexity is 46.30014142932408
At time: 68.51269292831421 and batch: 350, loss is 3.891319351196289 and perplexity is 48.97545974839232
At time: 69.1437451839447 and batch: 400, loss is 3.8806090545654297 and perplexity is 48.45371704385528
At time: 69.77518439292908 and batch: 450, loss is 3.819913558959961 and perplexity is 45.600266416024965
At time: 70.40771389007568 and batch: 500, loss is 3.849845175743103 and perplexity is 46.985788128680866
At time: 71.04232215881348 and batch: 550, loss is 3.912806544303894 and perplexity is 50.03919229612611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.448657421355552 and perplexity of 85.51206023329792
Finished 9 epochs...
Completing Train Step...
At time: 72.70758605003357 and batch: 50, loss is 3.8701081228256227 and perplexity is 47.94757002731582
At time: 73.34625887870789 and batch: 100, loss is 3.8521226596832276 and perplexity is 47.09291945518893
At time: 73.9759168624878 and batch: 150, loss is 3.861200661659241 and perplexity is 47.52237542283171
At time: 74.60620450973511 and batch: 200, loss is 3.8395482301712036 and perplexity is 46.50446038128309
At time: 75.23584294319153 and batch: 250, loss is 3.826123957633972 and perplexity is 45.88434345288821
At time: 75.86533951759338 and batch: 300, loss is 3.7923253202438354 and perplexity is 44.35943032196562
At time: 76.49598240852356 and batch: 350, loss is 3.84859055519104 and perplexity is 46.92687575730339
At time: 77.1254518032074 and batch: 400, loss is 3.836290807723999 and perplexity is 46.353222165251715
At time: 77.75256490707397 and batch: 450, loss is 3.7753617000579833 and perplexity is 43.613280389217216
At time: 78.37978267669678 and batch: 500, loss is 3.806564474105835 and perplexity is 44.9955895112502
At time: 79.01339983940125 and batch: 550, loss is 3.8711204814910887 and perplexity is 47.996134743629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.450104409075798 and perplexity of 85.63588469902405
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 80.66423010826111 and batch: 50, loss is 3.8529645919799806 and perplexity is 47.13258520061963
At time: 81.30574202537537 and batch: 100, loss is 3.822488021850586 and perplexity is 45.71781385555212
At time: 81.93403768539429 and batch: 150, loss is 3.814286470413208 and perplexity is 45.344390273177034
At time: 82.563729763031 and batch: 200, loss is 3.786587142944336 and perplexity is 44.10561695535822
At time: 83.19299674034119 and batch: 250, loss is 3.759964427947998 and perplexity is 42.94689824229323
At time: 83.81921744346619 and batch: 300, loss is 3.711047120094299 and perplexity is 40.89660777384529
At time: 84.4530816078186 and batch: 350, loss is 3.754003448486328 and perplexity is 42.69165417206305
At time: 85.08395481109619 and batch: 400, loss is 3.726648283004761 and perplexity is 41.53964543920242
At time: 85.71435308456421 and batch: 450, loss is 3.6494701433181764 and perplexity is 38.454285390036105
At time: 86.34354400634766 and batch: 500, loss is 3.667703237533569 and perplexity is 39.161857006187674
At time: 86.97267413139343 and batch: 550, loss is 3.711101198196411 and perplexity is 40.89881944457745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.38500424648853 and perplexity of 80.23856394431908
Finished 11 epochs...
Completing Train Step...
At time: 88.63040113449097 and batch: 50, loss is 3.7904730653762817 and perplexity is 44.277341399502504
At time: 89.2948865890503 and batch: 100, loss is 3.767318301200867 and perplexity is 43.26388841278575
At time: 89.9446017742157 and batch: 150, loss is 3.7670288133621215 and perplexity is 43.25136585588482
At time: 90.5825777053833 and batch: 200, loss is 3.745259656906128 and perplexity is 42.31999447114984
At time: 91.21677899360657 and batch: 250, loss is 3.724452772140503 and perplexity is 41.448544739225646
At time: 91.84881377220154 and batch: 300, loss is 3.6787445402145384 and perplexity is 39.59665085101674
At time: 92.4872407913208 and batch: 350, loss is 3.7267039442062377 and perplexity is 41.54195765012611
At time: 93.11988520622253 and batch: 400, loss is 3.7049245834350586 and perplexity is 40.64698174554431
At time: 93.75172805786133 and batch: 450, loss is 3.6329309272766115 and perplexity is 37.82351228131087
At time: 94.3839464187622 and batch: 500, loss is 3.657524275779724 and perplexity is 38.76525190000613
At time: 95.01673221588135 and batch: 550, loss is 3.709092264175415 and perplexity is 40.81673888959151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.381992421251662 and perplexity of 79.99726297304426
Finished 12 epochs...
Completing Train Step...
At time: 96.68084168434143 and batch: 50, loss is 3.7677513933181763 and perplexity is 43.282629719884724
At time: 97.33151650428772 and batch: 100, loss is 3.7448615503311156 and perplexity is 42.30314995627492
At time: 97.96492004394531 and batch: 150, loss is 3.745867910385132 and perplexity is 42.34574358521857
At time: 98.59929656982422 and batch: 200, loss is 3.725309443473816 and perplexity is 41.48406773290398
At time: 99.23531103134155 and batch: 250, loss is 3.7061008262634276 and perplexity is 40.69482059585404
At time: 99.87063837051392 and batch: 300, loss is 3.6616619396209718 and perplexity is 38.925981774873975
At time: 100.50401759147644 and batch: 350, loss is 3.711661334037781 and perplexity is 40.921734756462676
At time: 101.13647842407227 and batch: 400, loss is 3.6924271249771117 and perplexity is 40.14215885183477
At time: 101.7693829536438 and batch: 450, loss is 3.6226066541671753 and perplexity is 37.4350209068741
At time: 102.40147137641907 and batch: 500, loss is 3.649957175254822 and perplexity is 38.473018416542736
At time: 103.04115200042725 and batch: 550, loss is 3.705072865486145 and perplexity is 40.65300941025421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.381462584150598 and perplexity of 79.95488868186465
Finished 13 epochs...
Completing Train Step...
At time: 104.72203612327576 and batch: 50, loss is 3.750766968727112 and perplexity is 42.55370684972023
At time: 105.36487293243408 and batch: 100, loss is 3.7281838178634645 and perplexity is 41.60348001035431
At time: 105.99666500091553 and batch: 150, loss is 3.7303571939468383 and perplexity is 41.69399834835864
At time: 106.62874627113342 and batch: 200, loss is 3.7107241106033326 and perplexity is 40.883399914632776
At time: 107.27556300163269 and batch: 250, loss is 3.692333526611328 and perplexity is 40.13840178719751
At time: 107.9161286354065 and batch: 300, loss is 3.648697881698608 and perplexity is 38.42460008520758
At time: 108.55949306488037 and batch: 350, loss is 3.700159163475037 and perplexity is 40.45374260593841
At time: 109.19851207733154 and batch: 400, loss is 3.6823809719085694 and perplexity is 39.740903490622166
At time: 109.83080720901489 and batch: 450, loss is 3.613856692314148 and perplexity is 37.108894778295685
At time: 110.46241569519043 and batch: 500, loss is 3.6427323770523072 and perplexity is 38.19606031016997
At time: 111.0949592590332 and batch: 550, loss is 3.699920516014099 and perplexity is 40.44408957486157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.381711919256982 and perplexity of 79.97482672806417
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 112.77516722679138 and batch: 50, loss is 3.7494504737854 and perplexity is 42.49772196989293
At time: 113.42060399055481 and batch: 100, loss is 3.730471477508545 and perplexity is 41.69876355927913
At time: 114.05401968955994 and batch: 150, loss is 3.7265347719192503 and perplexity is 41.53493049656108
At time: 114.68631219863892 and batch: 200, loss is 3.7076158905029297 and perplexity is 40.75652259271358
At time: 115.31788969039917 and batch: 250, loss is 3.683639421463013 and perplexity is 39.79094689485854
At time: 115.9494206905365 and batch: 300, loss is 3.6361288452148437 and perplexity is 37.94466238051304
At time: 116.59123063087463 and batch: 350, loss is 3.6863414573669435 and perplexity is 39.898608849750346
At time: 117.2299132347107 and batch: 400, loss is 3.6622973251342774 and perplexity is 38.950722638945884
At time: 117.8732602596283 and batch: 450, loss is 3.5871324062347414 and perplexity is 36.13032015206515
At time: 118.5176157951355 and batch: 500, loss is 3.612453594207764 and perplexity is 37.05686386907338
At time: 119.17335486412048 and batch: 550, loss is 3.6667616415023803 and perplexity is 39.12499971212071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.367572540932513 and perplexity of 78.85198921531665
Finished 15 epochs...
Completing Train Step...
At time: 120.87445759773254 and batch: 50, loss is 3.7347345972061157 and perplexity is 41.87690983922531
At time: 121.52051615715027 and batch: 100, loss is 3.7154761600494384 and perplexity is 41.0781421985613
At time: 122.1552882194519 and batch: 150, loss is 3.714613342285156 and perplexity is 41.042714533755806
At time: 122.7906653881073 and batch: 200, loss is 3.6979176950454713 and perplexity is 40.36316836660021
At time: 123.42698240280151 and batch: 250, loss is 3.6751645851135253 and perplexity is 39.45515005319483
At time: 124.07659459114075 and batch: 300, loss is 3.6290381956100464 and perplexity is 37.676561702464745
At time: 124.71982645988464 and batch: 350, loss is 3.6801664447784423 and perplexity is 39.65299355705967
At time: 125.36609268188477 and batch: 400, loss is 3.6581371068954467 and perplexity is 38.78901573344383
At time: 126.00453972816467 and batch: 450, loss is 3.585249309539795 and perplexity is 36.06234728542597
At time: 126.6400318145752 and batch: 500, loss is 3.612637815475464 and perplexity is 37.063691160359156
At time: 127.27446866035461 and batch: 550, loss is 3.6693708515167236 and perplexity is 39.22721835013591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.366705387196642 and perplexity of 78.78364205631772
Finished 16 epochs...
Completing Train Step...
At time: 128.94419074058533 and batch: 50, loss is 3.729953932762146 and perplexity is 41.677188166864774
At time: 129.5904505252838 and batch: 100, loss is 3.7101552295684814 and perplexity is 40.86014873798488
At time: 130.22716975212097 and batch: 150, loss is 3.7094022750854494 and perplexity is 40.82939448554437
At time: 130.85963940620422 and batch: 200, loss is 3.6931187534332275 and perplexity is 40.16993191439971
At time: 131.49261856079102 and batch: 250, loss is 3.670747661590576 and perplexity is 39.28126397627341
At time: 132.12496495246887 and batch: 300, loss is 3.62531711101532 and perplexity is 37.536624549592936
At time: 132.75918841362 and batch: 350, loss is 3.67703031539917 and perplexity is 39.52883143499306
At time: 133.3979423046112 and batch: 400, loss is 3.6559144163131716 and perplexity is 38.702895498250605
At time: 134.0322642326355 and batch: 450, loss is 3.5841340398788453 and perplexity is 36.02215046291071
At time: 134.66405153274536 and batch: 500, loss is 3.612512707710266 and perplexity is 37.05905449483558
At time: 135.2961564064026 and batch: 550, loss is 3.6703204917907715 and perplexity is 39.264487789999755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.366409951067985 and perplexity of 78.76036995998538
Finished 17 epochs...
Completing Train Step...
At time: 136.98894453048706 and batch: 50, loss is 3.726097640991211 and perplexity is 41.51677826158778
At time: 137.63370895385742 and batch: 100, loss is 3.706153783798218 and perplexity is 40.69697575029686
At time: 138.26579427719116 and batch: 150, loss is 3.7054877710342407 and perplexity is 40.66988006902825
At time: 138.90874004364014 and batch: 200, loss is 3.689530415534973 and perplexity is 40.02604693369617
At time: 139.55391454696655 and batch: 250, loss is 3.6673926448822023 and perplexity is 39.14969550992112
At time: 140.20576333999634 and batch: 300, loss is 3.6224092626571656 and perplexity is 37.42763228082006
At time: 140.85461735725403 and batch: 350, loss is 3.6746260833740236 and perplexity is 39.43390910591571
At time: 141.49121499061584 and batch: 400, loss is 3.654102487564087 and perplexity is 38.63283210333293
At time: 142.13098335266113 and batch: 450, loss is 3.583015604019165 and perplexity is 35.981884519724964
At time: 142.76989245414734 and batch: 500, loss is 3.6120301342010497 and perplexity is 37.04117509127
At time: 143.40207529067993 and batch: 550, loss is 3.670492811203003 and perplexity is 39.27125440645023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.366309632646277 and perplexity of 78.75246924027833
Finished 18 epochs...
Completing Train Step...
At time: 145.07742404937744 and batch: 50, loss is 3.7226859998703 and perplexity is 41.375379252149045
At time: 145.73519325256348 and batch: 100, loss is 3.7027190637588503 and perplexity is 40.55743281478994
At time: 146.37593507766724 and batch: 150, loss is 3.702157664299011 and perplexity is 40.53467028394935
At time: 147.0129098892212 and batch: 200, loss is 3.6864953517913817 and perplexity is 39.90474949568863
At time: 147.64850759506226 and batch: 250, loss is 3.6645263767242433 and perplexity is 39.03764264777675
At time: 148.2850742340088 and batch: 300, loss is 3.619877953529358 and perplexity is 37.3330111817452
At time: 148.9202103614807 and batch: 350, loss is 3.672517805099487 and perplexity is 39.35085902916592
At time: 149.55753302574158 and batch: 400, loss is 3.6524341821670534 and perplexity is 38.56843447342521
At time: 150.1993625164032 and batch: 450, loss is 3.5818380451202394 and perplexity is 35.939538668668746
At time: 150.84131932258606 and batch: 500, loss is 3.6113206481933595 and perplexity is 37.01490421634542
At time: 151.47733116149902 and batch: 550, loss is 3.6702584981918336 and perplexity is 39.26205371854041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3663015162691154 and perplexity of 78.7518300581295
Finished 19 epochs...
Completing Train Step...
At time: 153.14823174476624 and batch: 50, loss is 3.7195493364334107 and perplexity is 41.24580193930999
At time: 153.82393717765808 and batch: 100, loss is 3.6996187496185304 and perplexity is 40.431886749022496
At time: 154.4653959274292 and batch: 150, loss is 3.699168853759766 and perplexity is 40.413700701832475
At time: 155.09940218925476 and batch: 200, loss is 3.683783383369446 and perplexity is 39.79667568778639
At time: 155.73335671424866 and batch: 250, loss is 3.6619434356689453 and perplexity is 38.9369408272997
At time: 156.36719274520874 and batch: 300, loss is 3.6175666856765747 and perplexity is 37.24682423208354
At time: 157.00230288505554 and batch: 350, loss is 3.670571699142456 and perplexity is 39.27435255699186
At time: 157.6364459991455 and batch: 400, loss is 3.6508393621444704 and perplexity is 38.506973784282984
At time: 158.27864742279053 and batch: 450, loss is 3.5806133460998537 and perplexity is 35.895550492504356
At time: 158.91813731193542 and batch: 500, loss is 3.610473146438599 and perplexity is 36.98354730946344
At time: 159.55617117881775 and batch: 550, loss is 3.669791831970215 and perplexity is 39.24373571880689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.366349565221908 and perplexity of 78.755614092003
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 161.2993528842926 and batch: 50, loss is 3.7197303199768066 and perplexity is 41.25326742623994
At time: 161.94366097450256 and batch: 100, loss is 3.702536735534668 and perplexity is 40.55003872418362
At time: 162.58759188652039 and batch: 150, loss is 3.700783133506775 and perplexity is 40.47899240573613
At time: 163.21935653686523 and batch: 200, loss is 3.6835505390167236 and perplexity is 39.78741033532972
At time: 163.86179089546204 and batch: 250, loss is 3.6621259546279905 and perplexity is 38.94404820580383
At time: 164.4948284626007 and batch: 300, loss is 3.614366374015808 and perplexity is 37.12781332374168
At time: 165.12693238258362 and batch: 350, loss is 3.6683026790618896 and perplexity is 39.185339287027325
At time: 165.75989174842834 and batch: 400, loss is 3.647606453895569 and perplexity is 38.382685286003316
At time: 166.39212465286255 and batch: 450, loss is 3.575413479804993 and perplexity is 35.709382872205836
At time: 167.024747133255 and batch: 500, loss is 3.6019293785095217 and perplexity is 36.66891445302006
At time: 167.65608167648315 and batch: 550, loss is 3.661034893989563 and perplexity is 38.901581059034385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.364301640936669 and perplexity of 78.59449359490606
Finished 21 epochs...
Completing Train Step...
At time: 169.34957098960876 and batch: 50, loss is 3.7168202924728395 and perplexity is 41.13339378578984
At time: 170.0027437210083 and batch: 100, loss is 3.6990110540390013 and perplexity is 40.40732393428597
At time: 170.6342101097107 and batch: 150, loss is 3.697832102775574 and perplexity is 40.35971373924627
At time: 171.26952362060547 and batch: 200, loss is 3.6814593267440796 and perplexity is 39.70429335245746
At time: 171.91378712654114 and batch: 250, loss is 3.6598329544067383 and perplexity is 38.85485179742532
At time: 172.55074501037598 and batch: 300, loss is 3.6131839036941527 and perplexity is 37.083936732874136
At time: 173.18327832221985 and batch: 350, loss is 3.666923723220825 and perplexity is 39.13134167325231
At time: 173.81575727462769 and batch: 400, loss is 3.646724987030029 and perplexity is 38.34886712769595
At time: 174.44697165489197 and batch: 450, loss is 3.574976978302002 and perplexity is 35.69379907433428
At time: 175.07919931411743 and batch: 500, loss is 3.602305226325989 and perplexity is 36.682698974728375
At time: 175.71108150482178 and batch: 550, loss is 3.662257571220398 and perplexity is 38.949174226050495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3639841282621346 and perplexity of 78.56954280834613
Finished 22 epochs...
Completing Train Step...
At time: 177.3848855495453 and batch: 50, loss is 3.7157443618774413 and perplexity is 41.08916090894301
At time: 178.03566336631775 and batch: 100, loss is 3.69768780708313 and perplexity is 40.353890426555
At time: 178.6721510887146 and batch: 150, loss is 3.696619334220886 and perplexity is 40.31079641623834
At time: 179.31010484695435 and batch: 200, loss is 3.6804486179351805 and perplexity is 39.66418414619347
At time: 179.95264625549316 and batch: 250, loss is 3.658790488243103 and perplexity is 38.81436803427633
At time: 180.59372925758362 and batch: 300, loss is 3.612473430633545 and perplexity is 37.05759895209389
At time: 181.23397135734558 and batch: 350, loss is 3.666297450065613 and perplexity is 39.106842436842705
At time: 181.866539478302 and batch: 400, loss is 3.6462823390960692 and perplexity is 38.33189583732259
At time: 182.49854516983032 and batch: 450, loss is 3.574847731590271 and perplexity is 35.68918606628922
At time: 183.131365776062 and batch: 500, loss is 3.60253812789917 and perplexity is 36.6912434279978
At time: 183.76514339447021 and batch: 550, loss is 3.6628928422927856 and perplexity is 38.9739253707404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363861408639462 and perplexity of 78.55990137530773
Finished 23 epochs...
Completing Train Step...
At time: 185.4289972782135 and batch: 50, loss is 3.714919490814209 and perplexity is 41.05528162403859
At time: 186.07521104812622 and batch: 100, loss is 3.696751298904419 and perplexity is 40.31611636874557
At time: 186.70975041389465 and batch: 150, loss is 3.6957328653335573 and perplexity is 40.27507798337066
At time: 187.36839866638184 and batch: 200, loss is 3.6796769952774047 and perplexity is 39.63359016802542
At time: 188.02367734909058 and batch: 250, loss is 3.658044309616089 and perplexity is 38.78541638532225
At time: 188.65657472610474 and batch: 300, loss is 3.6118874740600586 and perplexity is 37.03589116892052
At time: 189.30299305915833 and batch: 350, loss is 3.665821361541748 and perplexity is 39.08822854923465
At time: 189.94479727745056 and batch: 400, loss is 3.645953502655029 and perplexity is 38.31929298536918
At time: 190.57655477523804 and batch: 450, loss is 3.5747442054748535 and perplexity is 35.68549149473898
At time: 191.20934128761292 and batch: 500, loss is 3.602645449638367 and perplexity is 36.69518140736741
At time: 191.84095644950867 and batch: 550, loss is 3.6632548332214356 and perplexity is 38.9880361320082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363800048828125 and perplexity of 78.55508110246774
Finished 24 epochs...
Completing Train Step...
At time: 193.51576447486877 and batch: 50, loss is 3.7141926193237307 and perplexity is 41.02545055328337
At time: 194.1697871685028 and batch: 100, loss is 3.695969829559326 and perplexity is 40.284622866896065
At time: 194.81355261802673 and batch: 150, loss is 3.694984459877014 and perplexity is 40.24494717168352
At time: 195.45077919960022 and batch: 200, loss is 3.6790115213394166 and perplexity is 39.60722382073122
At time: 196.09259295463562 and batch: 250, loss is 3.6574199342727662 and perplexity is 38.76120728621951
At time: 196.73575592041016 and batch: 300, loss is 3.611363592147827 and perplexity is 37.016493816838334
At time: 197.37436747550964 and batch: 350, loss is 3.6654016828536986 and perplexity is 39.071827494586216
At time: 198.02540230751038 and batch: 400, loss is 3.6456660985946656 and perplexity is 38.30828144743108
At time: 198.66760110855103 and batch: 450, loss is 3.5746307611465453 and perplexity is 35.681443407746606
At time: 199.31005692481995 and batch: 500, loss is 3.602676281929016 and perplexity is 36.69631282130793
At time: 199.95816946029663 and batch: 550, loss is 3.6634722423553465 and perplexity is 38.99651340866184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3637708298703455 and perplexity of 78.55278583840241
Finished 25 epochs...
Completing Train Step...
At time: 201.62477087974548 and batch: 50, loss is 3.7135203742980956 and perplexity is 40.997880666122505
At time: 202.27349042892456 and batch: 100, loss is 3.6952719831466676 and perplexity is 40.25652019415826
At time: 202.90725111961365 and batch: 150, loss is 3.694313826560974 and perplexity is 40.21796661734844
At time: 203.54186606407166 and batch: 200, loss is 3.6784082555770876 and perplexity is 39.58333734433039
At time: 204.1767876148224 and batch: 250, loss is 3.6568598890304567 and perplexity is 38.73950533409744
At time: 204.81253933906555 and batch: 300, loss is 3.610877528190613 and perplexity is 36.998505805387694
At time: 205.44888758659363 and batch: 350, loss is 3.6650123310089113 and perplexity is 39.05661776763176
At time: 206.08501410484314 and batch: 400, loss is 3.6453959560394287 and perplexity is 38.29793414808
At time: 206.72576642036438 and batch: 450, loss is 3.574502902030945 and perplexity is 35.67688150159601
At time: 207.36174702644348 and batch: 500, loss is 3.602657561302185 and perplexity is 36.695625849759814
At time: 207.99813985824585 and batch: 550, loss is 3.663602328300476 and perplexity is 39.00158663693606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3637565450465425 and perplexity of 78.55166373371203
Finished 26 epochs...
Completing Train Step...
At time: 209.67443656921387 and batch: 50, loss is 3.712884268760681 and perplexity is 40.97180997994153
At time: 210.32180190086365 and batch: 100, loss is 3.6946269273757935 and perplexity is 40.23056086699874
At time: 210.95826363563538 and batch: 150, loss is 3.6936939096450807 and perplexity is 40.1930425457445
At time: 211.59357929229736 and batch: 200, loss is 3.677846574783325 and perplexity is 39.56111038680298
At time: 212.2290325164795 and batch: 250, loss is 3.656339392662048 and perplexity is 38.719346808931775
At time: 212.87506246566772 and batch: 300, loss is 3.6104170894622802 and perplexity is 36.98147418173492
At time: 213.50976610183716 and batch: 350, loss is 3.6646422863006594 and perplexity is 39.04216774664641
At time: 214.14912724494934 and batch: 400, loss is 3.645134057998657 and perplexity is 38.2879053074852
At time: 214.80674862861633 and batch: 450, loss is 3.5743617105484007 and perplexity is 35.671844585397444
At time: 215.45173335075378 and batch: 500, loss is 3.6026046228408815 and perplexity is 36.69368329120925
At time: 216.09103631973267 and batch: 550, loss is 3.6636748838424684 and perplexity is 39.004416520853695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363752973840592 and perplexity of 78.55138321004398
Finished 27 epochs...
Completing Train Step...
At time: 217.76429963111877 and batch: 50, loss is 3.7122747135162353 and perplexity is 40.946843008441085
At time: 218.41503763198853 and batch: 100, loss is 3.694018716812134 and perplexity is 40.20609965443548
At time: 219.0579171180725 and batch: 150, loss is 3.6931096839904787 and perplexity is 40.16956759715407
At time: 219.6985218524933 and batch: 200, loss is 3.677314963340759 and perplexity is 39.54008483704733
At time: 220.33646726608276 and batch: 250, loss is 3.655845732688904 and perplexity is 38.700237334405344
At time: 220.96975779533386 and batch: 300, loss is 3.609975275993347 and perplexity is 36.965138877184934
At time: 221.60303592681885 and batch: 350, loss is 3.6642860269546507 and perplexity is 39.028261086834085
At time: 222.235506772995 and batch: 400, loss is 3.6448761415481568 and perplexity is 38.278031500214595
At time: 222.87195754051208 and batch: 450, loss is 3.5742092752456665 and perplexity is 35.666407351392714
At time: 223.50460577011108 and batch: 500, loss is 3.602527184486389 and perplexity is 36.690841902772554
At time: 224.13629269599915 and batch: 550, loss is 3.6637073135375977 and perplexity is 39.00568144270057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3637565450465425 and perplexity of 78.55166373371203
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 225.82697224617004 and batch: 50, loss is 3.712178955078125 and perplexity is 40.94292219043774
At time: 226.47670793533325 and batch: 100, loss is 3.694434041976929 and perplexity is 40.22280172755577
At time: 227.11270403862 and batch: 150, loss is 3.693364019393921 and perplexity is 40.17978543965947
At time: 227.7445206642151 and batch: 200, loss is 3.6771690368652346 and perplexity is 39.534315312798526
At time: 228.37681412696838 and batch: 250, loss is 3.6560167121887206 and perplexity is 38.70685484734096
At time: 229.00979948043823 and batch: 300, loss is 3.6091030168533327 and perplexity is 36.932909755054
At time: 229.6424515247345 and batch: 350, loss is 3.663422050476074 and perplexity is 38.99455614948846
At time: 230.27567839622498 and batch: 400, loss is 3.6437007665634153 and perplexity is 38.23306688983979
At time: 230.9132444858551 and batch: 450, loss is 3.573031768798828 and perplexity is 35.62443464321779
At time: 231.5503876209259 and batch: 500, loss is 3.5999729204177857 and perplexity is 36.597243392198166
At time: 232.18911266326904 and batch: 550, loss is 3.661125102043152 and perplexity is 38.905090453228674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363607528361868 and perplexity of 78.53995909732149
Finished 29 epochs...
Completing Train Step...
At time: 233.8716700077057 and batch: 50, loss is 3.7118919134140014 and perplexity is 40.93117155246014
At time: 234.5195848941803 and batch: 100, loss is 3.694054446220398 and perplexity is 40.20753622024841
At time: 235.1561369895935 and batch: 150, loss is 3.692976756095886 and perplexity is 40.16422829598558
At time: 235.8087661266327 and batch: 200, loss is 3.676894268989563 and perplexity is 39.52345404519594
At time: 236.4599711894989 and batch: 250, loss is 3.6556813859939576 and perplexity is 38.69387760092069
At time: 237.10013580322266 and batch: 300, loss is 3.6089546489715576 and perplexity is 36.927430503948344
At time: 237.73627591133118 and batch: 350, loss is 3.663246445655823 and perplexity is 38.987709118668455
At time: 238.3709499835968 and batch: 400, loss is 3.6436740016937255 and perplexity is 38.23204360048081
At time: 239.00604271888733 and batch: 450, loss is 3.5729936027526854 and perplexity is 35.62307502534718
At time: 239.64179158210754 and batch: 500, loss is 3.6000800466537477 and perplexity is 36.60116412713239
At time: 240.286208152771 and batch: 550, loss is 3.6613397550582887 and perplexity is 38.91344244455662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3635312344165555 and perplexity of 78.53396720255282
Finished 30 epochs...
Completing Train Step...
At time: 241.99070763587952 and batch: 50, loss is 3.7116839027404787 and perplexity is 40.92265831735002
At time: 242.63723134994507 and batch: 100, loss is 3.6937730836868288 and perplexity is 40.19622491735195
At time: 243.2702672481537 and batch: 150, loss is 3.692706437110901 and perplexity is 40.153372609875035
At time: 243.90310192108154 and batch: 200, loss is 3.676693487167358 and perplexity is 39.51551925068083
At time: 244.5347385406494 and batch: 250, loss is 3.6554519081115724 and perplexity is 38.68499923056134
At time: 245.1659152507782 and batch: 300, loss is 3.608844289779663 and perplexity is 36.92335544742329
At time: 245.7980306148529 and batch: 350, loss is 3.663133306503296 and perplexity is 38.98329833182084
At time: 246.42894840240479 and batch: 400, loss is 3.6436486768722536 and perplexity is 38.23107539306201
At time: 247.0598282814026 and batch: 450, loss is 3.572983202934265 and perplexity is 35.622704553761764
At time: 247.69843292236328 and batch: 500, loss is 3.6001640796661376 and perplexity is 36.604239962445014
At time: 248.34235501289368 and batch: 550, loss is 3.6615022468566893 and perplexity is 38.91976607355641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.36348253615359 and perplexity of 78.53014282788725
Finished 31 epochs...
Completing Train Step...
At time: 250.047682762146 and batch: 50, loss is 3.7115064334869383 and perplexity is 40.91539644812387
At time: 250.69312834739685 and batch: 100, loss is 3.693545970916748 and perplexity is 40.18709687795057
At time: 251.3249146938324 and batch: 150, loss is 3.692493748664856 and perplexity is 40.1448333595833
At time: 251.95697712898254 and batch: 200, loss is 3.6765296506881713 and perplexity is 39.50904569745012
At time: 252.58887648582458 and batch: 250, loss is 3.6552728414535522 and perplexity is 38.67807265721129
At time: 253.22207736968994 and batch: 300, loss is 3.608748197555542 and perplexity is 36.91980757054076
At time: 253.8551094532013 and batch: 350, loss is 3.6630441522598267 and perplexity is 38.9798229602745
At time: 254.48720979690552 and batch: 400, loss is 3.643619656562805 and perplexity is 38.229965931522095
At time: 255.1176459789276 and batch: 450, loss is 3.5729783010482787 and perplexity is 35.62252993575349
At time: 255.74905562400818 and batch: 500, loss is 3.6002259159088137 and perplexity is 36.60650350109395
At time: 256.38273572921753 and batch: 550, loss is 3.6616268062591555 and perplexity is 38.92461419829615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363452992540725 and perplexity of 78.52782279802047
Finished 32 epochs...
Completing Train Step...
At time: 258.05068373680115 and batch: 50, loss is 3.711346664428711 and perplexity is 40.908859955944855
At time: 258.698703289032 and batch: 100, loss is 3.693352723121643 and perplexity is 40.17933156042664
At time: 259.33876848220825 and batch: 150, loss is 3.6923147535324095 and perplexity is 40.13764827288601
At time: 259.9705264568329 and batch: 200, loss is 3.6763880157470705 and perplexity is 39.50345023235585
At time: 260.60347962379456 and batch: 250, loss is 3.6551228094100954 and perplexity is 38.672270142226026
At time: 261.2354791164398 and batch: 300, loss is 3.608659100532532 and perplexity is 36.91651827213165
At time: 261.86727833747864 and batch: 350, loss is 3.66296648979187 and perplexity is 38.976795808572454
At time: 262.49953627586365 and batch: 400, loss is 3.643587384223938 and perplexity is 38.22873218101479
At time: 263.13117361068726 and batch: 450, loss is 3.572973051071167 and perplexity is 35.62234291877758
At time: 263.76847982406616 and batch: 500, loss is 3.600270986557007 and perplexity is 36.60815341711594
At time: 264.40878415107727 and batch: 550, loss is 3.6617240619659426 and perplexity is 38.92840002325497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363433188580452 and perplexity of 78.52626765153656
Finished 33 epochs...
Completing Train Step...
At time: 266.10302805900574 and batch: 50, loss is 3.7111989068984985 and perplexity is 40.90281581037897
At time: 266.7530417442322 and batch: 100, loss is 3.693182168006897 and perplexity is 40.172479354277954
At time: 267.40346479415894 and batch: 150, loss is 3.692156949043274 and perplexity is 40.13131487153793
At time: 268.04880714416504 and batch: 200, loss is 3.6762606477737427 and perplexity is 39.49841907837105
At time: 268.6833436489105 and batch: 250, loss is 3.654991068840027 and perplexity is 38.66717577088671
At time: 269.3234202861786 and batch: 300, loss is 3.608573970794678 and perplexity is 36.913375712373195
At time: 269.957129240036 and batch: 350, loss is 3.662895278930664 and perplexity is 38.97402033619893
At time: 270.5916073322296 and batch: 400, loss is 3.6435526466369628 and perplexity is 38.227404230170734
At time: 271.2252719402313 and batch: 450, loss is 3.5729657220840454 and perplexity is 35.622081844041794
At time: 271.85971188545227 and batch: 500, loss is 3.600303568840027 and perplexity is 36.60934621376333
At time: 272.49376606941223 and batch: 550, loss is 3.6618014001846313 and perplexity is 38.931410792791446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363420202376995 and perplexity of 78.52524790006944
Finished 34 epochs...
Completing Train Step...
At time: 274.1658420562744 and batch: 50, loss is 3.7110601043701172 and perplexity is 40.89713879012807
At time: 274.81223011016846 and batch: 100, loss is 3.6930275058746336 and perplexity is 40.16626667340833
At time: 275.44456219673157 and batch: 150, loss is 3.692013397216797 and perplexity is 40.125554361464964
At time: 276.08842420578003 and batch: 200, loss is 3.6761429262161256 and perplexity is 39.493769536634744
At time: 276.7272353172302 and batch: 250, loss is 3.65487135887146 and perplexity is 38.662547201539084
At time: 277.3670823574066 and batch: 300, loss is 3.6084911727905276 and perplexity is 36.910319485064264
At time: 278.00088381767273 and batch: 350, loss is 3.6628278875350953 and perplexity is 38.97139391107777
At time: 278.63571524620056 and batch: 400, loss is 3.6435161018371582 and perplexity is 38.22600724286256
At time: 279.2815442085266 and batch: 450, loss is 3.5729561948776247 and perplexity is 35.6217424667316
At time: 279.9203004837036 and batch: 500, loss is 3.6003266906738283 and perplexity is 36.61019269876817
At time: 280.55274629592896 and batch: 550, loss is 3.661863932609558 and perplexity is 38.93384534443255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363410787379488 and perplexity of 78.52450858853655
Finished 35 epochs...
Completing Train Step...
At time: 282.2322380542755 and batch: 50, loss is 3.710928177833557 and perplexity is 40.89174372813602
At time: 282.892945766449 and batch: 100, loss is 3.6928844833374024 and perplexity is 40.16052240282746
At time: 283.5272912979126 and batch: 150, loss is 3.691879906654358 and perplexity is 40.120198336142444
At time: 284.16342401504517 and batch: 200, loss is 3.676031537055969 and perplexity is 39.48937060381591
At time: 284.79460525512695 and batch: 250, loss is 3.6547603511810305 and perplexity is 38.65825559967293
At time: 285.42719292640686 and batch: 300, loss is 3.608410201072693 and perplexity is 36.90733091408623
At time: 286.06023836135864 and batch: 350, loss is 3.6627629709243776 and perplexity is 38.968864102384316
At time: 286.7146067619324 and batch: 400, loss is 3.6434781455993654 and perplexity is 38.224556354977075
At time: 287.34952211380005 and batch: 450, loss is 3.572944440841675 and perplexity is 35.62132376995074
At time: 287.98788261413574 and batch: 500, loss is 3.6003426837921144 and perplexity is 36.61077821459258
At time: 288.61987042427063 and batch: 550, loss is 3.661915078163147 and perplexity is 38.93583668842981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363402995657413 and perplexity of 78.52389674977321
Finished 36 epochs...
Completing Train Step...
At time: 290.28850746154785 and batch: 50, loss is 3.7108017015457153 and perplexity is 40.88657221922942
At time: 290.95307970046997 and batch: 100, loss is 3.6927501010894774 and perplexity is 40.15512590415406
At time: 291.6015191078186 and batch: 150, loss is 3.6917536783218385 and perplexity is 40.11513435002211
At time: 292.2459261417389 and batch: 200, loss is 3.6759250783920288 and perplexity is 39.48516684194899
At time: 292.88072180747986 and batch: 250, loss is 3.654655628204346 and perplexity is 38.65420740404634
At time: 293.516774892807 and batch: 300, loss is 3.608330421447754 and perplexity is 36.90438657851897
At time: 294.15216159820557 and batch: 350, loss is 3.66269998550415 and perplexity is 38.966409709399336
At time: 294.78797125816345 and batch: 400, loss is 3.6434392404556273 and perplexity is 38.223069252045924
At time: 295.42308831214905 and batch: 450, loss is 3.572930679321289 and perplexity is 35.62083356975047
At time: 296.059294462204 and batch: 500, loss is 3.6003531074523925 and perplexity is 36.61115983489614
At time: 296.6941604614258 and batch: 550, loss is 3.6619574499130247 and perplexity is 38.93748650291577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363399424451463 and perplexity of 78.52361632526662
Finished 37 epochs...
Completing Train Step...
At time: 298.37097239494324 and batch: 50, loss is 3.710679306983948 and perplexity is 40.88156823137717
At time: 299.02522468566895 and batch: 100, loss is 3.692622284889221 and perplexity is 40.14999375653297
At time: 299.65919303894043 and batch: 150, loss is 3.691633253097534 and perplexity is 40.11030376683787
At time: 300.29088020324707 and batch: 200, loss is 3.6758223009109496 and perplexity is 39.48110886449891
At time: 300.9225778579712 and batch: 250, loss is 3.6545555210113525 and perplexity is 38.65033803352488
At time: 301.5543444156647 and batch: 300, loss is 3.608251690864563 and perplexity is 36.90148118901433
At time: 302.1866853237152 and batch: 350, loss is 3.6626380920410155 and perplexity is 38.96399801799124
At time: 302.81887793540955 and batch: 400, loss is 3.6433995246887205 and perplexity is 38.22155122368208
At time: 303.46518754959106 and batch: 450, loss is 3.5729150676727297 and perplexity is 35.62027747415618
At time: 304.0988311767578 and batch: 500, loss is 3.6003590869903563 and perplexity is 36.611378753370786
At time: 304.73677039146423 and batch: 550, loss is 3.6619927072525025 and perplexity is 38.9388593592973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363395853245512 and perplexity of 78.52333590176147
Finished 38 epochs...
Completing Train Step...
At time: 306.4055118560791 and batch: 50, loss is 3.710560512542725 and perplexity is 40.876712016774164
At time: 307.05258202552795 and batch: 100, loss is 3.692499885559082 and perplexity is 40.1450797249353
At time: 307.69908475875854 and batch: 150, loss is 3.6915173387527465 and perplexity is 40.105654676710834
At time: 308.3320209980011 and batch: 200, loss is 3.6757223653793334 and perplexity is 39.47716349604026
At time: 308.97076869010925 and batch: 250, loss is 3.6544592142105103 and perplexity is 38.646615922352595
At time: 309.60519456863403 and batch: 300, loss is 3.608173704147339 and perplexity is 36.89860347584886
At time: 310.23754382133484 and batch: 350, loss is 3.6625772428512575 and perplexity is 38.961627162415155
At time: 310.86989855766296 and batch: 400, loss is 3.643359313011169 and perplexity is 38.22001430189007
At time: 311.50203800201416 and batch: 450, loss is 3.5728981733322143 and perplexity is 35.61967569814259
At time: 312.1353569030762 and batch: 500, loss is 3.6003613805770875 and perplexity is 36.611462724839605
At time: 312.7811722755432 and batch: 550, loss is 3.6620221948623657 and perplexity is 38.94000759011981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363392931349734 and perplexity of 78.52310646509304
Finished 39 epochs...
Completing Train Step...
At time: 314.4592709541321 and batch: 50, loss is 3.710444622039795 and perplexity is 40.87197506854938
At time: 315.11149406433105 and batch: 100, loss is 3.692381558418274 and perplexity is 40.140329753464755
At time: 315.7620289325714 and batch: 150, loss is 3.6914047622680664 and perplexity is 40.101139977220804
At time: 316.40218591690063 and batch: 200, loss is 3.675624632835388 and perplexity is 39.47330548095394
At time: 317.03757786750793 and batch: 250, loss is 3.654365744590759 and perplexity is 38.64300380667183
At time: 317.67576789855957 and batch: 300, loss is 3.6080965328216554 and perplexity is 36.89575607157315
At time: 318.31915187835693 and batch: 350, loss is 3.6625170803070066 and perplexity is 38.95928320230692
At time: 318.9632225036621 and batch: 400, loss is 3.6433184719085694 and perplexity is 38.218453386239574
At time: 319.6178729534149 and batch: 450, loss is 3.5728798723220825 and perplexity is 35.619023828061714
At time: 320.25687193870544 and batch: 500, loss is 3.600360713005066 and perplexity is 36.611438284059574
At time: 320.89281702041626 and batch: 550, loss is 3.6620471048355103 and perplexity is 38.9409775967445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363391632729388 and perplexity of 78.52300449345553
Finished 40 epochs...
Completing Train Step...
At time: 322.5667107105255 and batch: 50, loss is 3.7103310680389403 and perplexity is 40.86733415575961
At time: 323.2141728401184 and batch: 100, loss is 3.6922668409347534 and perplexity is 40.13572521996305
At time: 323.85648798942566 and batch: 150, loss is 3.691295208930969 and perplexity is 40.09674700415172
At time: 324.4905219078064 and batch: 200, loss is 3.6755288362503054 and perplexity is 39.46952425420413
At time: 325.1337652206421 and batch: 250, loss is 3.654274754524231 and perplexity is 38.63948783714619
At time: 325.768917798996 and batch: 300, loss is 3.6080200052261353 and perplexity is 36.89293263611284
At time: 326.40333914756775 and batch: 350, loss is 3.662457437515259 and perplexity is 38.95695963118507
At time: 327.03774905204773 and batch: 400, loss is 3.6432773447036744 and perplexity is 38.21688160039819
At time: 327.6734781265259 and batch: 450, loss is 3.5728603410720825 and perplexity is 35.618328150796316
At time: 328.3099670410156 and batch: 500, loss is 3.6003574466705324 and perplexity is 36.61131869904969
At time: 328.9444193840027 and batch: 550, loss is 3.6620680904388427 and perplexity is 38.941794805228504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363390658764129 and perplexity of 78.5229280148144
Finished 41 epochs...
Completing Train Step...
At time: 330.6163511276245 and batch: 50, loss is 3.710219564437866 and perplexity is 40.86277755487836
At time: 331.2615976333618 and batch: 100, loss is 3.6921550273895263 and perplexity is 40.13123775312041
At time: 331.8932147026062 and batch: 150, loss is 3.6911882257461546 and perplexity is 40.092457555910016
At time: 332.5371804237366 and batch: 200, loss is 3.675434556007385 and perplexity is 39.46580323328163
At time: 333.1712062358856 and batch: 250, loss is 3.654185581207275 and perplexity is 38.636042379474006
At time: 333.8028528690338 and batch: 300, loss is 3.60794415473938 and perplexity is 36.89013439533986
At time: 334.4360797405243 and batch: 350, loss is 3.662398352622986 and perplexity is 38.95465793142049
At time: 335.07788610458374 and batch: 400, loss is 3.643235836029053 and perplexity is 38.21529530121761
At time: 335.7215006351471 and batch: 450, loss is 3.572839937210083 and perplexity is 35.61760140675829
At time: 336.3558065891266 and batch: 500, loss is 3.6003519439697267 and perplexity is 36.611117238471074
At time: 336.98906683921814 and batch: 550, loss is 3.662085843086243 and perplexity is 38.942486131317224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363390658764129 and perplexity of 78.5229280148144
Finished 42 epochs...
Completing Train Step...
At time: 338.7350685596466 and batch: 50, loss is 3.710109844207764 and perplexity is 40.85829432747729
At time: 339.38191413879395 and batch: 100, loss is 3.6920455169677733 and perplexity is 40.1268432049774
At time: 340.0235230922699 and batch: 150, loss is 3.6910833168029784 and perplexity is 40.08825171917729
At time: 340.6567165851593 and batch: 200, loss is 3.675341649055481 and perplexity is 39.46213675612202
At time: 341.28815841674805 and batch: 250, loss is 3.6540980672836305 and perplexity is 38.63266133575763
At time: 341.9175102710724 and batch: 300, loss is 3.607868571281433 and perplexity is 36.88734621678955
At time: 342.5487427711487 and batch: 350, loss is 3.6623396396636965 and perplexity is 38.95237085531639
At time: 343.1847360134125 and batch: 400, loss is 3.643194055557251 and perplexity is 38.21369868150387
At time: 343.81967544555664 and batch: 450, loss is 3.5728184366226197 and perplexity is 35.616835615636525
At time: 344.4513349533081 and batch: 500, loss is 3.6003447914123536 and perplexity is 36.61085537629103
At time: 345.0853989124298 and batch: 550, loss is 3.6621005392074584 and perplexity is 38.94305843901918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363390983419215 and perplexity of 78.52295350768647
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 346.7376620769501 and batch: 50, loss is 3.710083074569702 and perplexity is 40.857200580366
At time: 347.3827724456787 and batch: 100, loss is 3.6920928144454956 and perplexity is 40.12874114833357
At time: 348.01419019699097 and batch: 150, loss is 3.6910735416412352 and perplexity is 40.087859851948025
At time: 348.64666748046875 and batch: 200, loss is 3.675286011695862 and perplexity is 39.45994124810469
At time: 349.282306432724 and batch: 250, loss is 3.6540825748443604 and perplexity is 38.63206282623425
At time: 349.91771960258484 and batch: 300, loss is 3.6076901865005495 and perplexity is 36.88076666248092
At time: 350.55338525772095 and batch: 350, loss is 3.6621181535720826 and perplexity is 38.94374440229149
At time: 351.1904766559601 and batch: 400, loss is 3.6429055452346804 and perplexity is 38.20267522523758
At time: 351.82585644721985 and batch: 450, loss is 3.5725460195541383 and perplexity is 35.60713430315108
At time: 352.4613859653473 and batch: 500, loss is 3.5998368501663207 and perplexity is 36.59226393487255
At time: 353.1087830066681 and batch: 550, loss is 3.6615760374069213 and perplexity is 38.922638090472425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363386113592919 and perplexity of 78.52257111547374
Finished 44 epochs...
Completing Train Step...
At time: 354.80288553237915 and batch: 50, loss is 3.7100582695007325 and perplexity is 40.85618712725714
At time: 355.448219537735 and batch: 100, loss is 3.6920629739761353 and perplexity is 40.127543705729074
At time: 356.08254313468933 and batch: 150, loss is 3.6910466718673707 and perplexity is 40.08678271469037
At time: 356.71702575683594 and batch: 200, loss is 3.675263104438782 and perplexity is 39.45903733943923
At time: 357.35115909576416 and batch: 250, loss is 3.654059052467346 and perplexity is 38.631154118975125
At time: 357.9859652519226 and batch: 300, loss is 3.6076756286621094 and perplexity is 36.880229762146364
At time: 358.6210308074951 and batch: 350, loss is 3.662106237411499 and perplexity is 38.943280345144366
At time: 359.25463604927063 and batch: 400, loss is 3.642902135848999 and perplexity is 38.20254497780571
At time: 359.88796496391296 and batch: 450, loss is 3.5725451135635375 and perplexity is 35.60710204343669
At time: 360.52038860321045 and batch: 500, loss is 3.599841284751892 and perplexity is 36.59242620675801
At time: 361.1583261489868 and batch: 550, loss is 3.6615874099731447 and perplexity is 38.92308074326874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363382217731882 and perplexity of 78.52226520304427
Finished 45 epochs...
Completing Train Step...
At time: 362.87932109832764 and batch: 50, loss is 3.7100337839126585 and perplexity is 40.855186751736305
At time: 363.54033374786377 and batch: 100, loss is 3.692034134864807 and perplexity is 40.12638647971557
At time: 364.1880929470062 and batch: 150, loss is 3.691020622253418 and perplexity is 40.08573848307702
At time: 364.84506487846375 and batch: 200, loss is 3.675240888595581 and perplexity is 39.45816073339015
At time: 365.48505878448486 and batch: 250, loss is 3.654036350250244 and perplexity is 38.630277116082404
At time: 366.1286919116974 and batch: 300, loss is 3.607661123275757 and perplexity is 36.87969480404479
At time: 366.77247285842896 and batch: 350, loss is 3.6620944499969483 and perplexity is 38.94282130726042
At time: 367.4180121421814 and batch: 400, loss is 3.6428983402252197 and perplexity is 38.20239997559275
At time: 368.07177662849426 and batch: 450, loss is 3.572544188499451 and perplexity is 35.6070691046006
At time: 368.7217547893524 and batch: 500, loss is 3.599845209121704 and perplexity is 36.59256980925255
At time: 369.3581688404083 and batch: 550, loss is 3.661598300933838 and perplexity is 38.92350465531958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363377347905585 and perplexity of 78.52188281418337
Finished 46 epochs...
Completing Train Step...
At time: 371.041166305542 and batch: 50, loss is 3.710009870529175 and perplexity is 40.85420977766964
At time: 371.6901898384094 and batch: 100, loss is 3.6920059013366697 and perplexity is 40.12525358624672
At time: 372.3252320289612 and batch: 150, loss is 3.690995149612427 and perplexity is 40.0847174064566
At time: 372.9603953361511 and batch: 200, loss is 3.675219135284424 and perplexity is 39.45730239707789
At time: 373.60049533843994 and batch: 250, loss is 3.6540143156051634 and perplexity is 38.629425921014715
At time: 374.2363166809082 and batch: 300, loss is 3.6076467847824096 and perplexity is 36.879166008577265
At time: 374.87573409080505 and batch: 350, loss is 3.6620828771591185 and perplexity is 38.9423706309126
At time: 375.52686858177185 and batch: 400, loss is 3.642894253730774 and perplexity is 38.2022438620164
At time: 376.16289710998535 and batch: 450, loss is 3.5725429344177244 and perplexity is 35.6070244504539
At time: 376.7983546257019 and batch: 500, loss is 3.5998490190505983 and perplexity is 36.59270922460716
At time: 377.4335823059082 and batch: 550, loss is 3.6616083765029908 and perplexity is 38.923896833758114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3633737766996346 and perplexity of 78.52160239686894
Finished 47 epochs...
Completing Train Step...
At time: 379.120876789093 and batch: 50, loss is 3.709986333847046 and perplexity is 40.8532482164365
At time: 379.78376054763794 and batch: 100, loss is 3.691978712081909 and perplexity is 40.124162625335906
At time: 380.4247763156891 and batch: 150, loss is 3.6909702491760252 and perplexity is 40.08371929192693
At time: 381.0604717731476 and batch: 200, loss is 3.6751980209350585 and perplexity is 39.45646929060535
At time: 381.70814871788025 and batch: 250, loss is 3.6539928483963013 and perplexity is 38.62859666396119
At time: 382.361115694046 and batch: 300, loss is 3.607632527351379 and perplexity is 36.878640210159716
At time: 383.01310420036316 and batch: 350, loss is 3.6620716094970702 and perplexity is 38.941931843913025
At time: 383.6628658771515 and batch: 400, loss is 3.642889714241028 and perplexity is 38.20207044371574
At time: 384.29779076576233 and batch: 450, loss is 3.5725415468215944 and perplexity is 35.60697504231885
At time: 384.93223214149475 and batch: 500, loss is 3.5998523664474487 and perplexity is 36.59283171513178
At time: 385.58863282203674 and batch: 550, loss is 3.661618070602417 and perplexity is 38.92427416771303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.3633715041140295 and perplexity of 78.52142395000841
Finished 48 epochs...
Completing Train Step...
At time: 387.2654321193695 and batch: 50, loss is 3.709963068962097 and perplexity is 40.85229778137288
At time: 387.9114990234375 and batch: 100, loss is 3.691952028274536 and perplexity is 40.123091974194
At time: 388.54300832748413 and batch: 150, loss is 3.690946021080017 and perplexity is 40.082748151492055
At time: 389.17678356170654 and batch: 200, loss is 3.675177159309387 and perplexity is 39.4556461730985
At time: 389.8100309371948 and batch: 250, loss is 3.653972001075745 and perplexity is 38.62779136961803
At time: 390.4441885948181 and batch: 300, loss is 3.607618308067322 and perplexity is 36.87811582602711
At time: 391.0775854587555 and batch: 350, loss is 3.6620602416992187 and perplexity is 38.94148916242004
At time: 391.7115843296051 and batch: 400, loss is 3.6428850793838503 and perplexity is 38.201893382985666
At time: 392.3451008796692 and batch: 450, loss is 3.572539973258972 and perplexity is 35.60691901255791
At time: 392.9782612323761 and batch: 500, loss is 3.5998555183410645 and perplexity is 36.59294705202621
At time: 393.6098976135254 and batch: 550, loss is 3.6616271686553956 and perplexity is 38.92462830443254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363368582218252 and perplexity of 78.52119451892649
Finished 49 epochs...
Completing Train Step...
At time: 395.2810823917389 and batch: 50, loss is 3.7099399948120118 and perplexity is 40.85135516019768
At time: 395.94019412994385 and batch: 100, loss is 3.6919260311126707 and perplexity is 40.12204890123593
At time: 396.5737347602844 and batch: 150, loss is 3.6909221315383913 and perplexity is 40.08179060444934
At time: 397.20606684684753 and batch: 200, loss is 3.675156798362732 and perplexity is 39.454842826969994
At time: 397.83781385421753 and batch: 250, loss is 3.653951573371887 and perplexity is 38.62700230059472
At time: 398.46936225891113 and batch: 300, loss is 3.607604146003723 and perplexity is 36.87759355950358
At time: 399.102196931839 and batch: 350, loss is 3.662049078941345 and perplexity is 38.94105447043147
At time: 399.73636078834534 and batch: 400, loss is 3.642880096435547 and perplexity is 38.20170302540012
At time: 400.36888694763184 and batch: 450, loss is 3.5725382566452026 and perplexity is 35.606857889282914
At time: 401.004674911499 and batch: 500, loss is 3.5998582458496093 and perplexity is 36.59304685973809
At time: 401.6407399177551 and batch: 550, loss is 3.6616357421875 and perplexity is 38.92496202741354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.363365335667387 and perplexity of 78.52093959628836
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fec82176cc0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -161.3694418197261, 'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 26.31202854294525, 'anneal': 6.999923892627626, 'dropout': 0.9965728473943058, 'seq_len': 35, 'batch_size': 50}}, {'best_accuracy': -176.9292187375333, 'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 25.983096356513364, 'anneal': 3.490448677857382, 'dropout': 0.34412375796989636, 'seq_len': 35, 'batch_size': 50}}, {'best_accuracy': -77.34302270959628, 'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 4.940696979226176, 'anneal': 6.0578549803630555, 'dropout': 0.4929960016803847, 'seq_len': 35, 'batch_size': 50}}, {'best_accuracy': -131.81361107952833, 'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 20.377194193934663, 'anneal': 3.5122697591932353, 'dropout': 0.9600069746160612, 'seq_len': 35, 'batch_size': 50}}, {'best_accuracy': -166.18247920993755, 'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 17.724973681306913, 'anneal': 7.237538899170577, 'dropout': 0.6970116747311355, 'seq_len': 35, 'batch_size': 50}}, {'best_accuracy': -78.52093959628836, 'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 3.3079619350335183, 'anneal': 5.793669854009312, 'dropout': 0.4746858094026927, 'seq_len': 35, 'batch_size': 50}}]
