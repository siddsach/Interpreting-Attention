Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.04511195580211125, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 19.292207257283696, 'seq_len': 20, 'anneal': 3.5932680391524086, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.0551421642303467 and batch: 50, loss is 6.634765138626099 and perplexity is 761.1002917698362
At time: 1.628258466720581 and batch: 100, loss is 6.029680414199829 and perplexity is 415.5821939889611
At time: 2.187981605529785 and batch: 150, loss is 5.677879962921143 and perplexity is 292.3290241428838
At time: 2.7476515769958496 and batch: 200, loss is 5.602862567901611 and perplexity is 271.2016304146534
At time: 3.306989908218384 and batch: 250, loss is 5.55991870880127 and perplexity is 259.80171587159373
At time: 3.8661112785339355 and batch: 300, loss is 5.614005641937256 and perplexity is 274.2405502707472
At time: 4.425364255905151 and batch: 350, loss is 5.561653003692627 and perplexity is 260.25267959920694
At time: 4.984711170196533 and batch: 400, loss is 5.636996698379517 and perplexity is 280.6186691385791
At time: 5.543935775756836 and batch: 450, loss is 5.627578058242798 and perplexity is 277.98803084317126
At time: 6.1040120124816895 and batch: 500, loss is 5.653011274337769 and perplexity is 285.1488356594254
At time: 6.662572860717773 and batch: 550, loss is 5.682802705764771 and perplexity is 293.77163263651283
At time: 7.2202394008636475 and batch: 600, loss is 5.748673429489136 and perplexity is 313.7741405534495
At time: 7.777209281921387 and batch: 650, loss is 5.775446214675903 and perplexity is 322.28821236847153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.8935995662913605 and perplexity of 362.70853080523
Finished 1 epochs...
Completing Train Step...
At time: 8.864285230636597 and batch: 50, loss is 5.710486822128296 and perplexity is 302.0180615756133
At time: 9.414255857467651 and batch: 100, loss is 5.689635906219483 and perplexity is 295.78590722359644
At time: 9.977173566818237 and batch: 150, loss is 5.6534150695800784 and perplexity is 285.26400065258815
At time: 10.52656364440918 and batch: 200, loss is 5.659210481643677 and perplexity is 286.92202290372245
At time: 11.076525688171387 and batch: 250, loss is 5.648769121170044 and perplexity is 283.9417527484648
At time: 11.630590915679932 and batch: 300, loss is 5.690035171508789 and perplexity is 295.90402784858054
At time: 12.18061351776123 and batch: 350, loss is 5.638962888717652 and perplexity is 281.1709616326967
At time: 12.73065447807312 and batch: 400, loss is 5.694074869155884 and perplexity is 297.1018083603068
At time: 13.281553268432617 and batch: 450, loss is 5.695025444030762 and perplexity is 297.38436014665336
At time: 13.832091569900513 and batch: 500, loss is 5.695640687942505 and perplexity is 297.56738035894375
At time: 14.38181447982788 and batch: 550, loss is 5.714329690933227 and perplexity is 303.1809102694093
At time: 14.932501792907715 and batch: 600, loss is 5.768855743408203 and perplexity is 320.17116499757606
At time: 15.48262071609497 and batch: 650, loss is 5.76133451461792 and perplexity is 317.77211761497455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.825224034926471 and perplexity of 338.73701349958986
Finished 2 epochs...
Completing Train Step...
At time: 16.55825448036194 and batch: 50, loss is 5.753007640838623 and perplexity is 315.1370554416272
At time: 17.107600450515747 and batch: 100, loss is 5.716106319427491 and perplexity is 303.7200288784454
At time: 17.657572031021118 and batch: 150, loss is 5.719784917831421 and perplexity is 304.83935039686565
At time: 18.207821130752563 and batch: 200, loss is 5.700223350524903 and perplexity is 298.934160613066
At time: 18.758413076400757 and batch: 250, loss is 5.688136301040649 and perplexity is 295.34267756310703
At time: 19.309183835983276 and batch: 300, loss is 5.720540313720703 and perplexity is 305.06971178511566
At time: 19.859397411346436 and batch: 350, loss is 5.646768198013306 and perplexity is 283.37417514917354
At time: 20.432706356048584 and batch: 400, loss is 5.711472244262695 and perplexity is 302.31582354500074
At time: 20.98340368270874 and batch: 450, loss is 5.713202180862427 and perplexity is 302.83926338115384
At time: 21.53337550163269 and batch: 500, loss is 5.7089541625976565 and perplexity is 301.55552526100297
At time: 22.08665919303894 and batch: 550, loss is 5.725192718505859 and perplexity is 306.4923262952834
At time: 22.636240005493164 and batch: 600, loss is 5.750989961624145 and perplexity is 314.50185099005495
At time: 23.186145067214966 and batch: 650, loss is 5.743168487548828 and perplexity is 312.05157778145525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.8260234757965685 and perplexity of 339.00792198562584
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 24.246504068374634 and batch: 50, loss is 5.576116781234742 and perplexity is 264.04427072685604
At time: 24.814295768737793 and batch: 100, loss is 5.425133571624756 and perplexity is 227.04167061483125
At time: 25.368640899658203 and batch: 150, loss is 5.353524465560913 and perplexity is 211.35188917058704
At time: 25.922834873199463 and batch: 200, loss is 5.319713554382324 and perplexity is 204.32534550980733
At time: 26.4769287109375 and batch: 250, loss is 5.294833173751831 and perplexity is 199.3043739920401
At time: 27.03197693824768 and batch: 300, loss is 5.328616819381714 and perplexity is 206.1526305379512
At time: 27.586533546447754 and batch: 350, loss is 5.243893375396729 and perplexity is 189.40609776387296
At time: 28.14349913597107 and batch: 400, loss is 5.266709651947021 and perplexity is 193.77731752012068
At time: 28.710288763046265 and batch: 450, loss is 5.227678365707398 and perplexity is 186.35964195783424
At time: 29.285797357559204 and batch: 500, loss is 5.21486722946167 and perplexity is 183.9873912529259
At time: 29.871618270874023 and batch: 550, loss is 5.2323416900634765 and perplexity is 187.2307269124151
At time: 30.462881326675415 and batch: 600, loss is 5.2844100856781 and perplexity is 197.237795721623
At time: 31.05810046195984 and batch: 650, loss is 5.257479591369629 and perplexity is 191.99697013754135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.422917384727328 and perplexity of 226.53906098339763
Finished 4 epochs...
Completing Train Step...
At time: 32.19893717765808 and batch: 50, loss is 5.284061670303345 and perplexity is 197.1690870113912
At time: 32.791800022125244 and batch: 100, loss is 5.242880668640137 and perplexity is 189.21438202122658
At time: 33.383986711502075 and batch: 150, loss is 5.211964025497436 and perplexity is 183.45401295689877
At time: 33.975836992263794 and batch: 200, loss is 5.199654407501221 and perplexity is 181.2096063719257
At time: 34.58326029777527 and batch: 250, loss is 5.18541353225708 and perplexity is 178.64731094250382
At time: 35.17718458175659 and batch: 300, loss is 5.228789529800415 and perplexity is 186.56683319077655
At time: 35.770244121551514 and batch: 350, loss is 5.161169948577881 and perplexity is 174.36833830514504
At time: 36.36482620239258 and batch: 400, loss is 5.213158226013183 and perplexity is 183.6732246991238
At time: 36.95995330810547 and batch: 450, loss is 5.186014442443848 and perplexity is 178.7546941921009
At time: 37.553608655929565 and batch: 500, loss is 5.179505977630615 and perplexity is 177.59505338897324
At time: 38.14722299575806 and batch: 550, loss is 5.205756778717041 and perplexity is 182.31879555787734
At time: 38.73997902870178 and batch: 600, loss is 5.265838031768799 and perplexity is 193.6084908871267
At time: 39.33324384689331 and batch: 650, loss is 5.226965589523315 and perplexity is 186.22685657212463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.395790249693627 and perplexity of 220.47630960761543
Finished 5 epochs...
Completing Train Step...
At time: 40.440192222595215 and batch: 50, loss is 5.244807434082031 and perplexity is 189.57930520140502
At time: 41.041412115097046 and batch: 100, loss is 5.204145278930664 and perplexity is 182.02522546538572
At time: 41.63286828994751 and batch: 150, loss is 5.181657857894898 and perplexity is 177.97762815931523
At time: 42.223416805267334 and batch: 200, loss is 5.167464094161987 and perplexity is 175.4692991810529
At time: 42.81427526473999 and batch: 250, loss is 5.156724233627319 and perplexity is 173.59486696711426
At time: 43.40583848953247 and batch: 300, loss is 5.200098762512207 and perplexity is 181.29014566124903
At time: 43.99672794342041 and batch: 350, loss is 5.137171545028687 and perplexity is 170.2335886908786
At time: 44.58919405937195 and batch: 400, loss is 5.191316699981689 and perplexity is 179.70501481270583
At time: 45.1821403503418 and batch: 450, loss is 5.1623127651214595 and perplexity is 174.5677232353346
At time: 45.77328157424927 and batch: 500, loss is 5.152872619628906 and perplexity is 172.92753252976945
At time: 46.364466428756714 and batch: 550, loss is 5.179814920425415 and perplexity is 177.64992857732042
At time: 46.975379943847656 and batch: 600, loss is 5.240901165008545 and perplexity is 188.84020193247179
At time: 47.56565260887146 and batch: 650, loss is 5.19858338356018 and perplexity is 181.01563044013062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.376672482958027 and perplexity of 216.30133024887712
Finished 6 epochs...
Completing Train Step...
At time: 48.68701124191284 and batch: 50, loss is 5.214281549453736 and perplexity is 183.87966506577348
At time: 49.265594482421875 and batch: 100, loss is 5.173864688873291 and perplexity is 176.59600900932293
At time: 49.85849690437317 and batch: 150, loss is 5.149800529479981 and perplexity is 172.3970987479792
At time: 50.438623666763306 and batch: 200, loss is 5.138981065750122 and perplexity is 170.54190876875458
At time: 51.023940563201904 and batch: 250, loss is 5.134745664596558 and perplexity is 169.8211228578555
At time: 51.61019945144653 and batch: 300, loss is 5.1761860847473145 and perplexity is 177.00643445165935
At time: 52.2192656993866 and batch: 350, loss is 5.112631626129151 and perplexity is 166.10691148338535
At time: 52.82730674743652 and batch: 400, loss is 5.169615621566773 and perplexity is 175.84723260821224
At time: 53.41843891143799 and batch: 450, loss is 5.140126180648804 and perplexity is 170.73731070681822
At time: 54.01082229614258 and batch: 500, loss is 5.13254373550415 and perplexity is 169.44760017308104
At time: 54.602901458740234 and batch: 550, loss is 5.157444105148316 and perplexity is 173.71987795856285
At time: 55.19519829750061 and batch: 600, loss is 5.218671960830688 and perplexity is 184.68874724142714
At time: 55.78674578666687 and batch: 650, loss is 5.178727045059204 and perplexity is 177.45677268003274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.362800149356618 and perplexity of 213.32144282119717
Finished 7 epochs...
Completing Train Step...
At time: 56.885918378829956 and batch: 50, loss is 5.191561727523804 and perplexity is 179.74905288584117
At time: 57.479907274246216 and batch: 100, loss is 5.148132925033569 and perplexity is 172.1098481563086
At time: 58.06502723693848 and batch: 150, loss is 5.126181755065918 and perplexity is 168.37299977335235
At time: 58.64806365966797 and batch: 200, loss is 5.111320114135742 and perplexity is 165.8892030715894
At time: 59.232078075408936 and batch: 250, loss is 5.102337589263916 and perplexity is 164.40577163322868
At time: 59.816211462020874 and batch: 300, loss is 5.149430179595948 and perplexity is 172.3332633239008
At time: 60.4052939414978 and batch: 350, loss is 5.087071676254272 and perplexity is 161.91502753827749
At time: 60.997467279434204 and batch: 400, loss is 5.141649684906006 and perplexity is 170.9976279734241
At time: 61.5893669128418 and batch: 450, loss is 5.103979291915894 and perplexity is 164.67589869800847
At time: 62.18175387382507 and batch: 500, loss is 5.085697641372681 and perplexity is 161.69270341811264
At time: 62.77351713180542 and batch: 550, loss is 5.109364700317383 and perplexity is 165.5651379755292
At time: 63.36650228500366 and batch: 600, loss is 5.171502103805542 and perplexity is 176.17927839002522
At time: 63.96002173423767 and batch: 650, loss is 5.133110857009887 and perplexity is 169.54372480583163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.329623951631434 and perplexity of 206.36035808752933
Finished 8 epochs...
Completing Train Step...
At time: 65.08078980445862 and batch: 50, loss is 5.1442616844177245 and perplexity is 171.4448575216063
At time: 65.66826677322388 and batch: 100, loss is 5.108079347610474 and perplexity is 165.35246508637974
At time: 66.25589418411255 and batch: 150, loss is 5.0835818099975585 and perplexity is 161.35095059625868
At time: 66.84420251846313 and batch: 200, loss is 5.071686029434204 and perplexity is 159.44292631362376
At time: 67.4328670501709 and batch: 250, loss is 5.062906541824341 and perplexity is 158.0492260567619
At time: 68.02198314666748 and batch: 300, loss is 5.106690673828125 and perplexity is 165.1230038135691
At time: 68.61088037490845 and batch: 350, loss is 5.048462905883789 and perplexity is 155.78282749372366
At time: 69.20034050941467 and batch: 400, loss is 5.1059604740142825 and perplexity is 165.0024750373237
At time: 69.78943967819214 and batch: 450, loss is 5.073748579025269 and perplexity is 159.77212463331608
At time: 70.37869501113892 and batch: 500, loss is 5.062296466827393 and perplexity is 157.95283358196676
At time: 70.97095251083374 and batch: 550, loss is 5.093522825241089 and perplexity is 162.9629419959972
At time: 71.56130528450012 and batch: 600, loss is 5.15222713470459 and perplexity is 172.8159464319686
At time: 72.1514835357666 and batch: 650, loss is 5.107876672744751 and perplexity is 165.31895569359193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.332295735677083 and perplexity of 206.91244560080713
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 73.26803278923035 and batch: 50, loss is 5.095137367248535 and perplexity is 163.2262650273532
At time: 73.86869931221008 and batch: 100, loss is 5.016831617355347 and perplexity is 150.93233402733952
At time: 74.4577329158783 and batch: 150, loss is 4.969278335571289 and perplexity is 143.92298578507464
At time: 75.04670906066895 and batch: 200, loss is 4.952569427490235 and perplexity is 141.53816916966213
At time: 75.63542199134827 and batch: 250, loss is 4.9413354873657225 and perplexity is 139.95703565987094
At time: 76.22481322288513 and batch: 300, loss is 4.979483938217163 and perplexity is 145.3993272528763
At time: 76.81501388549805 and batch: 350, loss is 4.908041391372681 and perplexity is 135.3740098778462
At time: 77.40443253517151 and batch: 400, loss is 4.950519924163818 and perplexity is 141.24838328114149
At time: 77.99300026893616 and batch: 450, loss is 4.911416292190552 and perplexity is 135.8316555545054
At time: 78.58194231987 and batch: 500, loss is 4.903884515762329 and perplexity is 134.81244494433264
At time: 79.17255139350891 and batch: 550, loss is 4.930812530517578 and perplexity is 138.49199560860015
At time: 79.77786302566528 and batch: 600, loss is 5.010157499313355 and perplexity is 149.92834790142737
At time: 80.37105059623718 and batch: 650, loss is 4.973893623352051 and perplexity is 144.58876698685182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.177805582682292 and perplexity of 177.2933282559886
Finished 10 epochs...
Completing Train Step...
At time: 81.48967599868774 and batch: 50, loss is 5.003371353149414 and perplexity is 148.91435665618855
At time: 82.07582592964172 and batch: 100, loss is 4.963433609008789 and perplexity is 143.08424876902197
At time: 82.6656563282013 and batch: 150, loss is 4.931581268310547 and perplexity is 138.59850057159824
At time: 83.2548451423645 and batch: 200, loss is 4.921088361740113 and perplexity is 137.15180276034206
At time: 83.84404134750366 and batch: 250, loss is 4.912087306976319 and perplexity is 135.9228311904365
At time: 84.43338251113892 and batch: 300, loss is 4.95271183013916 and perplexity is 141.55832601503593
At time: 85.02329707145691 and batch: 350, loss is 4.8863355159759525 and perplexity is 132.46725939358268
At time: 85.61297559738159 and batch: 400, loss is 4.938958053588867 and perplexity is 139.62469229469457
At time: 86.20529198646545 and batch: 450, loss is 4.905089359283448 and perplexity is 134.97497073460178
At time: 86.79794597625732 and batch: 500, loss is 4.9005810832977295 and perplexity is 134.36783590935397
At time: 87.39184761047363 and batch: 550, loss is 4.925880384445191 and perplexity is 137.81061457307723
At time: 87.98618388175964 and batch: 600, loss is 5.002269315719604 and perplexity is 148.75033785534612
At time: 88.58216905593872 and batch: 650, loss is 4.955513696670533 and perplexity is 141.9555097189586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.166936537798713 and perplexity of 175.3767536493491
Finished 11 epochs...
Completing Train Step...
At time: 89.68791794776917 and batch: 50, loss is 4.980679941177368 and perplexity is 145.57332931142983
At time: 90.29161858558655 and batch: 100, loss is 4.94613091468811 and perplexity is 140.62980126266172
At time: 90.8825786113739 and batch: 150, loss is 4.917901010513305 and perplexity is 136.71534773079506
At time: 91.47360515594482 and batch: 200, loss is 4.907089118957519 and perplexity is 135.24515830314698
At time: 92.06454873085022 and batch: 250, loss is 4.8975927734375 and perplexity is 133.96690253534686
At time: 92.65473103523254 and batch: 300, loss is 4.940319805145264 and perplexity is 139.81495595325333
At time: 93.24646401405334 and batch: 350, loss is 4.876962213516236 and perplexity is 131.2314047732319
At time: 93.83976125717163 and batch: 400, loss is 4.929901189804077 and perplexity is 138.36583970874347
At time: 94.43298602104187 and batch: 450, loss is 4.897036657333374 and perplexity is 133.89242209523272
At time: 95.03979659080505 and batch: 500, loss is 4.893088521957398 and perplexity is 133.36483885732648
At time: 95.63319373130798 and batch: 550, loss is 4.917526168823242 and perplexity is 136.6641107222774
At time: 96.22535800933838 and batch: 600, loss is 4.993295574188233 and perplexity is 147.42146218177288
At time: 96.81819796562195 and batch: 650, loss is 4.943259754180908 and perplexity is 140.22660962207627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.16152894263174 and perplexity of 174.43094673913325
Finished 12 epochs...
Completing Train Step...
At time: 97.94827699661255 and batch: 50, loss is 4.968197946548462 and perplexity is 143.7675769372232
At time: 98.5397777557373 and batch: 100, loss is 4.935622491836548 and perplexity is 139.15974137872888
At time: 99.13087391853333 and batch: 150, loss is 4.9073289775848385 and perplexity is 135.27760191195333
At time: 99.72134852409363 and batch: 200, loss is 4.897186880111694 and perplexity is 133.9125372977188
At time: 100.31202960014343 and batch: 250, loss is 4.8892060661315915 and perplexity is 132.84805959720927
At time: 100.9053361415863 and batch: 300, loss is 4.931236486434937 and perplexity is 138.5507225575844
At time: 101.49851822853088 and batch: 350, loss is 4.868664445877076 and perplexity is 130.14698243389276
At time: 102.0918173789978 and batch: 400, loss is 4.922422199249268 and perplexity is 137.33486303850424
At time: 102.68457961082458 and batch: 450, loss is 4.890524835586548 and perplexity is 133.02337113269752
At time: 103.2773220539093 and batch: 500, loss is 4.887256727218628 and perplexity is 132.5893459473416
At time: 103.87024974822998 and batch: 550, loss is 4.910846805572509 and perplexity is 135.7543232663022
At time: 104.46231651306152 and batch: 600, loss is 4.986685428619385 and perplexity is 146.45019849050385
At time: 105.05530142784119 and batch: 650, loss is 4.935286054611206 and perplexity is 139.11293073632297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.156379250919118 and perplexity of 173.53499006861927
Finished 13 epochs...
Completing Train Step...
At time: 106.17788672447205 and batch: 50, loss is 4.9586708354949955 and perplexity is 142.40439118774202
At time: 106.7804160118103 and batch: 100, loss is 4.927334337234497 and perplexity is 138.01113043550671
At time: 107.37176561355591 and batch: 150, loss is 4.898166437149047 and perplexity is 134.04377653368317
At time: 107.96431469917297 and batch: 200, loss is 4.890378942489624 and perplexity is 133.00396535673858
At time: 108.55634140968323 and batch: 250, loss is 4.88177110671997 and perplexity is 131.8640024119133
At time: 109.14889860153198 and batch: 300, loss is 4.925692672729492 and perplexity is 137.7847483339483
At time: 109.74176859855652 and batch: 350, loss is 4.861841402053833 and perplexity is 129.26200641761073
At time: 110.34769296646118 and batch: 400, loss is 4.91705020904541 and perplexity is 136.59907957983214
At time: 110.9398832321167 and batch: 450, loss is 4.884776430130005 and perplexity is 132.26089247808136
At time: 111.5321946144104 and batch: 500, loss is 4.882206392288208 and perplexity is 131.9214134033148
At time: 112.12511444091797 and batch: 550, loss is 4.904960269927979 and perplexity is 134.95754802719244
At time: 112.71953463554382 and batch: 600, loss is 4.980612049102783 and perplexity is 145.563446371589
At time: 113.31471753120422 and batch: 650, loss is 4.9269569396972654 and perplexity is 137.9590552019203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.152333577473958 and perplexity of 172.83434241894037
Finished 14 epochs...
Completing Train Step...
At time: 114.43593740463257 and batch: 50, loss is 4.950028781890869 and perplexity is 141.17902726235693
At time: 115.02611017227173 and batch: 100, loss is 4.91856086730957 and perplexity is 136.80559005237916
At time: 115.6161241531372 and batch: 150, loss is 4.891646909713745 and perplexity is 133.17271698864909
At time: 116.20538663864136 and batch: 200, loss is 4.883004379272461 and perplexity is 132.02672698800134
At time: 116.79476881027222 and batch: 250, loss is 4.8744857883453365 and perplexity is 130.9068220874857
At time: 117.38363456726074 and batch: 300, loss is 4.918878393173218 and perplexity is 136.84903626279507
At time: 117.97445726394653 and batch: 350, loss is 4.855324497222901 and perplexity is 128.42235715236825
At time: 118.56384944915771 and batch: 400, loss is 4.910182819366455 and perplexity is 135.66421418714458
At time: 119.15249347686768 and batch: 450, loss is 4.8771920490264895 and perplexity is 131.26156987648244
At time: 119.74213862419128 and batch: 500, loss is 4.875769834518433 and perplexity is 131.07502045554466
At time: 120.33181667327881 and batch: 550, loss is 4.897959270477295 and perplexity is 134.01601000687833
At time: 120.92054438591003 and batch: 600, loss is 4.972499599456787 and perplexity is 144.38734721529096
At time: 121.51591229438782 and batch: 650, loss is 4.918813953399658 and perplexity is 136.84021802601205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.147550695082721 and perplexity of 172.00966981412165
Finished 15 epochs...
Completing Train Step...
At time: 122.63603043556213 and batch: 50, loss is 4.941721982955933 and perplexity is 140.01113889162917
At time: 123.23847579956055 and batch: 100, loss is 4.908443288803101 and perplexity is 135.4284272789361
At time: 123.8276629447937 and batch: 150, loss is 4.884682445526123 and perplexity is 132.24846257461172
At time: 124.41354179382324 and batch: 200, loss is 4.876947317123413 and perplexity is 131.22944991323598
At time: 124.99424052238464 and batch: 250, loss is 4.867725114822388 and perplexity is 130.02478873077348
At time: 125.59239721298218 and batch: 300, loss is 4.912031860351562 and perplexity is 135.9152949371515
At time: 126.17826414108276 and batch: 350, loss is 4.847688264846802 and perplexity is 127.445428963418
At time: 126.7650203704834 and batch: 400, loss is 4.903030576705933 and perplexity is 134.6973724717384
At time: 127.35356736183167 and batch: 450, loss is 4.870339097976685 and perplexity is 130.36511594917928
At time: 127.9425413608551 and batch: 500, loss is 4.869798793792724 and perplexity is 130.29469815681642
At time: 128.53158259391785 and batch: 550, loss is 4.89173773765564 and perplexity is 133.1848133417848
At time: 129.12049651145935 and batch: 600, loss is 4.967071256637573 and perplexity is 143.60568667596283
At time: 129.70998358726501 and batch: 650, loss is 4.910636463165283 and perplexity is 135.72577137809674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.1433452531403185 and perplexity of 171.2878120634992
Finished 16 epochs...
Completing Train Step...
At time: 130.83517599105835 and batch: 50, loss is 4.932583456039429 and perplexity is 138.73747191416797
At time: 131.41636896133423 and batch: 100, loss is 4.901853904724121 and perplexity is 134.53897105905568
At time: 131.99945306777954 and batch: 150, loss is 4.8776334953308105 and perplexity is 131.31952760307468
At time: 132.5843641757965 and batch: 200, loss is 4.869998340606689 and perplexity is 130.3207006429805
At time: 133.17011547088623 and batch: 250, loss is 4.861350469589233 and perplexity is 129.1985630766931
At time: 133.75464725494385 and batch: 300, loss is 4.906028299331665 and perplexity is 135.10176365629238
At time: 134.33967185020447 and batch: 350, loss is 4.841747055053711 and perplexity is 126.690493767604
At time: 134.9260904788971 and batch: 400, loss is 4.898780393600464 and perplexity is 134.12609884362897
At time: 135.5165798664093 and batch: 450, loss is 4.865706901550293 and perplexity is 129.76263560589686
At time: 136.1080551147461 and batch: 500, loss is 4.865666723251342 and perplexity is 129.75742206866704
At time: 136.6999180316925 and batch: 550, loss is 4.886319894790649 and perplexity is 132.46519011413938
At time: 137.29060292243958 and batch: 600, loss is 4.960705995559692 and perplexity is 142.69450202859
At time: 137.88194704055786 and batch: 650, loss is 4.903720912933349 and perplexity is 134.79039105104687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.141226375804228 and perplexity of 170.92525843954243
Finished 17 epochs...
Completing Train Step...
At time: 138.98872780799866 and batch: 50, loss is 4.92687614440918 and perplexity is 137.9479092105891
At time: 139.58853340148926 and batch: 100, loss is 4.895593881607056 and perplexity is 133.69938464705353
At time: 140.17656421661377 and batch: 150, loss is 4.872610454559326 and perplexity is 130.66155814896123
At time: 140.77803468704224 and batch: 200, loss is 4.865466833114624 and perplexity is 129.73148743195492
At time: 141.3667016029358 and batch: 250, loss is 4.856525297164917 and perplexity is 128.57665933587597
At time: 141.95797729492188 and batch: 300, loss is 4.900296821594238 and perplexity is 134.32964570768695
At time: 142.55113554000854 and batch: 350, loss is 4.8370445728302 and perplexity is 126.09613255456885
At time: 143.14423155784607 and batch: 400, loss is 4.894559364318848 and perplexity is 133.5611418418287
At time: 143.73724842071533 and batch: 450, loss is 4.860364532470703 and perplexity is 129.07124419221566
At time: 144.33002042770386 and batch: 500, loss is 4.861337289810181 and perplexity is 129.19686027939917
At time: 144.92306566238403 and batch: 550, loss is 4.881106853485107 and perplexity is 131.77644040664146
At time: 145.5160276889801 and batch: 600, loss is 4.956692123413086 and perplexity is 142.12289249266243
At time: 146.1089403629303 and batch: 650, loss is 4.898503303527832 and perplexity is 134.08893898171075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.14102113013174 and perplexity of 170.89018036986297
Finished 18 epochs...
Completing Train Step...
At time: 147.2274878025055 and batch: 50, loss is 4.921241016387939 and perplexity is 137.17274121862704
At time: 147.8154730796814 and batch: 100, loss is 4.8909494590759275 and perplexity is 133.07986797481118
At time: 148.4042580127716 and batch: 150, loss is 4.867546319961548 and perplexity is 130.00154304493307
At time: 148.99199795722961 and batch: 200, loss is 4.861229019165039 and perplexity is 129.1828728092163
At time: 149.58079290390015 and batch: 250, loss is 4.8512679481506344 and perplexity is 127.90246076414255
At time: 150.16934537887573 and batch: 300, loss is 4.8946068000793455 and perplexity is 133.5674775664336
At time: 150.7599561214447 and batch: 350, loss is 4.832745990753174 and perplexity is 125.55526130176679
At time: 151.35080575942993 and batch: 400, loss is 4.890746898651123 and perplexity is 133.05291399021877
At time: 151.94306254386902 and batch: 450, loss is 4.85628231048584 and perplexity is 128.5454207158551
At time: 152.5344488620758 and batch: 500, loss is 4.857414140701294 and perplexity is 128.69099467404124
At time: 153.12833452224731 and batch: 550, loss is 4.877020349502564 and perplexity is 131.2390342621574
At time: 153.72119760513306 and batch: 600, loss is 4.95311710357666 and perplexity is 141.6157074712314
At time: 154.31565046310425 and batch: 650, loss is 4.8933032035827635 and perplexity is 133.39347291119162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.140986423866422 and perplexity of 170.88424951284227
Finished 19 epochs...
Completing Train Step...
At time: 155.42128229141235 and batch: 50, loss is 4.917075424194336 and perplexity is 136.60252398939238
At time: 156.02278065681458 and batch: 100, loss is 4.885424852371216 and perplexity is 132.34668119305616
At time: 156.61116194725037 and batch: 150, loss is 4.863945779800415 and perplexity is 129.5343089205694
At time: 157.19914150238037 and batch: 200, loss is 4.8580021572113035 and perplexity is 128.76668935627956
At time: 157.78654217720032 and batch: 250, loss is 4.846438827514649 and perplexity is 127.28629332236636
At time: 158.37422013282776 and batch: 300, loss is 4.889817428588867 and perplexity is 132.92930274534382
At time: 158.96468663215637 and batch: 350, loss is 4.828882808685303 and perplexity is 125.07115416685505
At time: 159.5577850341797 and batch: 400, loss is 4.886557760238648 and perplexity is 132.49670275366563
At time: 160.1502764225006 and batch: 450, loss is 4.851571063995362 and perplexity is 127.94123590296236
At time: 160.7431218624115 and batch: 500, loss is 4.852636823654175 and perplexity is 128.07766319732642
At time: 161.33573627471924 and batch: 550, loss is 4.871585664749145 and perplexity is 130.5277261021592
At time: 161.92901420593262 and batch: 600, loss is 4.947972269058227 and perplexity is 140.88898911689623
At time: 162.52174282073975 and batch: 650, loss is 4.888260841369629 and perplexity is 132.72254764954462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.136445886948529 and perplexity of 170.110102121663
Finished 20 epochs...
Completing Train Step...
At time: 163.64358758926392 and batch: 50, loss is 4.911196327209472 and perplexity is 135.8017806328001
At time: 164.23359441757202 and batch: 100, loss is 4.880865850448608 and perplexity is 131.74468571100715
At time: 164.8237497806549 and batch: 150, loss is 4.859989595413208 and perplexity is 129.0228596708529
At time: 165.4138321876526 and batch: 200, loss is 4.85343002319336 and perplexity is 128.17929464243355
At time: 166.00678825378418 and batch: 250, loss is 4.843351545333863 and perplexity is 126.89393059600486
At time: 166.5967254638672 and batch: 300, loss is 4.886155605316162 and perplexity is 132.4434292652559
At time: 167.18677854537964 and batch: 350, loss is 4.824393062591553 and perplexity is 124.51087513745213
At time: 167.77713108062744 and batch: 400, loss is 4.880772848129272 and perplexity is 131.73243371941643
At time: 168.3672375679016 and batch: 450, loss is 4.847153844833374 and perplexity is 127.37733777182844
At time: 168.95886373519897 and batch: 500, loss is 4.848002586364746 and perplexity is 127.4854941004423
At time: 169.55177783966064 and batch: 550, loss is 4.86695725440979 and perplexity is 129.92498616497411
At time: 170.14392709732056 and batch: 600, loss is 4.942854700088501 and perplexity is 140.16982176183842
At time: 170.75024247169495 and batch: 650, loss is 4.8834794235229495 and perplexity is 132.0894604249673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.132313447840073 and perplexity of 169.40858297381604
Finished 21 epochs...
Completing Train Step...
At time: 171.85727453231812 and batch: 50, loss is 4.905367212295532 and perplexity is 135.01247914744786
At time: 172.46963715553284 and batch: 100, loss is 4.875891933441162 and perplexity is 131.09102555142155
At time: 173.05829691886902 and batch: 150, loss is 4.854533920288086 and perplexity is 128.3208695210652
At time: 173.64730834960938 and batch: 200, loss is 4.846307544708252 and perplexity is 127.26958391741596
At time: 174.23537921905518 and batch: 250, loss is 4.834796867370605 and perplexity is 125.81302388064121
At time: 174.82355570793152 and batch: 300, loss is 4.880123691558838 and perplexity is 131.64694649482703
At time: 175.41215801239014 and batch: 350, loss is 4.819000597000122 and perplexity is 123.84126158416002
At time: 176.00076150894165 and batch: 400, loss is 4.875139112472534 and perplexity is 130.99237461649287
At time: 176.58935570716858 and batch: 450, loss is 4.8421561145782475 and perplexity is 126.74232832173413
At time: 177.17858600616455 and batch: 500, loss is 4.841949853897095 and perplexity is 126.71618905860741
At time: 177.77097702026367 and batch: 550, loss is 4.859813318252564 and perplexity is 129.0001178919789
At time: 178.36521100997925 and batch: 600, loss is 4.9359274578094485 and perplexity is 139.20218683652826
At time: 178.95894408226013 and batch: 650, loss is 4.87732889175415 and perplexity is 131.27953329679502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.127349853515625 and perplexity of 168.56979092670463
Finished 22 epochs...
Completing Train Step...
At time: 180.08351159095764 and batch: 50, loss is 4.89787257194519 and perplexity is 134.0043915191924
At time: 180.67494821548462 and batch: 100, loss is 4.869769868850708 and perplexity is 130.2909294447323
At time: 181.26729321479797 and batch: 150, loss is 4.848914623260498 and perplexity is 127.6018186127953
At time: 181.85881423950195 and batch: 200, loss is 4.839563808441162 and perplexity is 126.41419889591323
At time: 182.4498472213745 and batch: 250, loss is 4.828924150466919 and perplexity is 125.07632493808077
At time: 183.04109930992126 and batch: 300, loss is 4.8727708911895755 and perplexity is 130.68252273075225
At time: 183.63277053833008 and batch: 350, loss is 4.812134809494019 and perplexity is 122.99390600792431
At time: 184.22393941879272 and batch: 400, loss is 4.869811239242554 and perplexity is 130.2963197430361
At time: 184.81515169143677 and batch: 450, loss is 4.8366810131073 and perplexity is 126.05029741196927
At time: 185.40638375282288 and batch: 500, loss is 4.835277509689331 and perplexity is 125.8735094789698
At time: 186.0120050907135 and batch: 550, loss is 4.853061990737915 and perplexity is 128.1321291816317
At time: 186.60278129577637 and batch: 600, loss is 4.93021068572998 and perplexity is 138.40866999997587
At time: 187.19401049613953 and batch: 650, loss is 4.8700466728210445 and perplexity is 130.32699948323614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.122504140816483 and perplexity of 167.7549260451533
Finished 23 epochs...
Completing Train Step...
At time: 188.29585671424866 and batch: 50, loss is 4.889607000350952 and perplexity is 132.90133360925103
At time: 188.89324855804443 and batch: 100, loss is 4.859638938903808 and perplexity is 128.97762489664547
At time: 189.4775161743164 and batch: 150, loss is 4.836781921386719 and perplexity is 126.06301757237537
At time: 190.0655882358551 and batch: 200, loss is 4.826323213577271 and perplexity is 124.75143200668789
At time: 190.6572892665863 and batch: 250, loss is 4.813327941894531 and perplexity is 123.1407416019812
At time: 191.24454832077026 and batch: 300, loss is 4.8561944580078125 and perplexity is 128.5341281781521
At time: 191.83277082443237 and batch: 350, loss is 4.794742765426636 and perplexity is 120.87328499574443
At time: 192.42036724090576 and batch: 400, loss is 4.8497757148742675 and perplexity is 127.71174278930557
At time: 193.006760597229 and batch: 450, loss is 4.815205526351929 and perplexity is 123.37216593586453
At time: 193.59207844734192 and batch: 500, loss is 4.814630851745606 and perplexity is 123.30128745295688
At time: 194.17758917808533 and batch: 550, loss is 4.833594923019409 and perplexity is 125.66189447004598
At time: 194.76294207572937 and batch: 600, loss is 4.9078067016601565 and perplexity is 135.3422427182433
At time: 195.34934639930725 and batch: 650, loss is 4.84721263885498 and perplexity is 127.38482701793679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.105794270833333 and perplexity of 164.9750533799544
Finished 24 epochs...
Completing Train Step...
At time: 196.46759295463562 and batch: 50, loss is 4.870465784072876 and perplexity is 130.38163244298138
At time: 197.05588722229004 and batch: 100, loss is 4.841032905578613 and perplexity is 126.60005011696676
At time: 197.64406871795654 and batch: 150, loss is 4.819079780578614 and perplexity is 123.85106816667218
At time: 198.2318980693817 and batch: 200, loss is 4.811241865158081 and perplexity is 122.88412831617796
At time: 198.8209888935089 and batch: 250, loss is 4.799968090057373 and perplexity is 121.50654019014625
At time: 199.41175413131714 and batch: 300, loss is 4.843471927642822 and perplexity is 126.90920729986696
At time: 200.00228881835938 and batch: 350, loss is 4.78253776550293 and perplexity is 119.40699281136814
At time: 200.59308648109436 and batch: 400, loss is 4.837904815673828 and perplexity is 126.20465252027783
At time: 201.1971824169159 and batch: 450, loss is 4.803586435317993 and perplexity is 121.94698917167486
At time: 201.7881190776825 and batch: 500, loss is 4.802953414916992 and perplexity is 121.86981866751717
At time: 202.37865829467773 and batch: 550, loss is 4.821246166229248 and perplexity is 124.11966818412587
At time: 202.9692029953003 and batch: 600, loss is 4.89780026435852 and perplexity is 133.99470233534342
At time: 203.55955600738525 and batch: 650, loss is 4.83600884437561 and perplexity is 125.96559881249281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.099011589499081 and perplexity of 163.85986641899206
Finished 25 epochs...
Completing Train Step...
At time: 204.6637933254242 and batch: 50, loss is 4.859847927093506 and perplexity is 129.00458251379783
At time: 205.26633620262146 and batch: 100, loss is 4.830758819580078 and perplexity is 125.3060092411538
At time: 205.85552668571472 and batch: 150, loss is 4.810301733016968 and perplexity is 122.76865528597388
At time: 206.4441225528717 and batch: 200, loss is 4.802728719711304 and perplexity is 121.84243817979188
At time: 207.03237104415894 and batch: 250, loss is 4.7934049701690675 and perplexity is 120.71168940331955
At time: 207.62365746498108 and batch: 300, loss is 4.83628553390503 and perplexity is 126.00045699697627
At time: 208.2150375843048 and batch: 350, loss is 4.776175241470337 and perplexity is 118.64967473197906
At time: 208.8063395023346 and batch: 400, loss is 4.830302686691284 and perplexity is 125.24886608260313
At time: 209.39684224128723 and batch: 450, loss is 4.797120895385742 and perplexity is 121.16107944671614
At time: 209.98790526390076 and batch: 500, loss is 4.7962540340423585 and perplexity is 121.056095100631
At time: 210.5786325931549 and batch: 550, loss is 4.8146156978607175 and perplexity is 123.29941897359764
At time: 211.16993856430054 and batch: 600, loss is 4.891940059661866 and perplexity is 133.2117622865093
At time: 211.76138949394226 and batch: 650, loss is 4.829480257034302 and perplexity is 125.14590004759759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.094493192784927 and perplexity of 163.1211526946075
Finished 26 epochs...
Completing Train Step...
At time: 212.87990593910217 and batch: 50, loss is 4.852076930999756 and perplexity is 128.00597352564017
At time: 213.46898436546326 and batch: 100, loss is 4.8243626117706295 and perplexity is 124.50708373681623
At time: 214.05932569503784 and batch: 150, loss is 4.804244585037232 and perplexity is 122.02727496549844
At time: 214.6500997543335 and batch: 200, loss is 4.798181619644165 and perplexity is 121.28966612831177
At time: 215.24065494537354 and batch: 250, loss is 4.786485776901245 and perplexity is 119.87934479199016
At time: 215.83282160758972 and batch: 300, loss is 4.829004888534546 and perplexity is 125.08642376653664
At time: 216.44019508361816 and batch: 350, loss is 4.7687030792236325 and perplexity is 117.76640917326911
At time: 217.03333854675293 and batch: 400, loss is 4.823770961761475 and perplexity is 124.43344090712213
At time: 217.62670135498047 and batch: 450, loss is 4.790610437393188 and perplexity is 120.37482753583949
At time: 218.22097778320312 and batch: 500, loss is 4.7906018733978275 and perplexity is 120.37379665078916
At time: 218.81464552879333 and batch: 550, loss is 4.80732310295105 and perplexity is 122.40351695415951
At time: 219.4078471660614 and batch: 600, loss is 4.884441404342652 and perplexity is 132.21658909024595
At time: 220.00119853019714 and batch: 650, loss is 4.819778470993042 and perplexity is 123.9376319579411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.086893717447917 and perplexity of 161.88621589696453
Finished 27 epochs...
Completing Train Step...
At time: 221.1056125164032 and batch: 50, loss is 4.842686986923217 and perplexity is 126.8096301815447
At time: 221.70744824409485 and batch: 100, loss is 4.8159604835510255 and perplexity is 123.4653418081782
At time: 222.2949502468109 and batch: 150, loss is 4.796359367370606 and perplexity is 121.0688470136215
At time: 222.88260459899902 and batch: 200, loss is 4.790903692245483 and perplexity is 120.41013321463632
At time: 223.46923351287842 and batch: 250, loss is 4.776863117218017 and perplexity is 118.73131904304871
At time: 224.0573809146881 and batch: 300, loss is 4.820842752456665 and perplexity is 124.06960669895722
At time: 224.64827013015747 and batch: 350, loss is 4.75970079421997 and perplexity is 116.71100006843176
At time: 225.24394631385803 and batch: 400, loss is 4.81221570968628 and perplexity is 123.0038566410659
At time: 225.83579301834106 and batch: 450, loss is 4.7802064418792725 and perplexity is 119.12894070885073
At time: 226.42661881446838 and batch: 500, loss is 4.778176021575928 and perplexity is 118.88730428368572
At time: 227.01706290245056 and batch: 550, loss is 4.795354566574097 and perplexity is 120.94725803629034
At time: 227.60650086402893 and batch: 600, loss is 4.872307920455933 and perplexity is 130.62203455053836
At time: 228.19635105133057 and batch: 650, loss is 4.803659830093384 and perplexity is 121.9559397720143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.07650816674326 and perplexity of 160.21363874440124
Finished 28 epochs...
Completing Train Step...
At time: 229.31126523017883 and batch: 50, loss is 4.825407667160034 and perplexity is 124.6372685490159
At time: 229.8976502418518 and batch: 100, loss is 4.801837720870972 and perplexity is 121.73392505838196
At time: 230.48399376869202 and batch: 150, loss is 4.782301445007324 and perplexity is 119.37877782566913
At time: 231.07033109664917 and batch: 200, loss is 4.7723370456695555 and perplexity is 118.19514689008466
At time: 231.6706817150116 and batch: 250, loss is 4.7581010437011715 and perplexity is 116.52444084941335
At time: 232.25640153884888 and batch: 300, loss is 4.802649250030518 and perplexity is 121.83275578485704
At time: 232.842768907547 and batch: 350, loss is 4.737523803710937 and perplexity is 114.1511906746252
At time: 233.42868065834045 and batch: 400, loss is 4.790024976730347 and perplexity is 120.30437343558549
At time: 234.01544189453125 and batch: 450, loss is 4.763556880950928 and perplexity is 117.1619166351089
At time: 234.60378909111023 and batch: 500, loss is 4.763493528366089 and perplexity is 117.15449435995806
At time: 235.19224762916565 and batch: 550, loss is 4.780564594268799 and perplexity is 119.17161466506087
At time: 235.77988696098328 and batch: 600, loss is 4.856562719345093 and perplexity is 128.58147104481984
At time: 236.36988306045532 and batch: 650, loss is 4.798038578033447 and perplexity is 121.27231789989416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.078862807329963 and perplexity of 160.59132876835878
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 237.4865357875824 and batch: 50, loss is 4.824354228973388 and perplexity is 124.50604002355279
At time: 238.0909309387207 and batch: 100, loss is 4.780227060317993 and perplexity is 119.13139698693683
At time: 238.67952752113342 and batch: 150, loss is 4.755534992218018 and perplexity is 116.22581644181076
At time: 239.2681336402893 and batch: 200, loss is 4.733938465118408 and perplexity is 113.74265281622567
At time: 239.85875535011292 and batch: 250, loss is 4.715830097198486 and perplexity is 111.70149579849127
At time: 240.4500274658203 and batch: 300, loss is 4.759765539169312 and perplexity is 116.71855676084498
At time: 241.04146575927734 and batch: 350, loss is 4.69088490486145 and perplexity is 108.94954714708715
At time: 241.63281559944153 and batch: 400, loss is 4.743868322372436 and perplexity is 114.87772735977117
At time: 242.224130153656 and batch: 450, loss is 4.712459869384766 and perplexity is 111.32566997557052
At time: 242.81539583206177 and batch: 500, loss is 4.706940803527832 and perplexity is 110.71294865137284
At time: 243.4063756465912 and batch: 550, loss is 4.724651527404785 and perplexity is 112.69122172855566
At time: 243.9970030784607 and batch: 600, loss is 4.800723218917847 and perplexity is 121.59832793680687
At time: 244.5881621837616 and batch: 650, loss is 4.748148880004883 and perplexity is 115.37052205800919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.04414577110141 and perplexity of 155.11174165694547
Finished 30 epochs...
Completing Train Step...
At time: 245.71003651618958 and batch: 50, loss is 4.788310766220093 and perplexity is 120.0983230715194
At time: 246.29856705665588 and batch: 100, loss is 4.7624251079559325 and perplexity is 117.02939095046105
At time: 246.9014241695404 and batch: 150, loss is 4.739911060333252 and perplexity is 114.42402439298569
At time: 247.49097514152527 and batch: 200, loss is 4.71841402053833 and perplexity is 111.99049711841145
At time: 248.0881073474884 and batch: 250, loss is 4.705344705581665 and perplexity is 110.53638088862233
At time: 248.67746901512146 and batch: 300, loss is 4.751821994781494 and perplexity is 115.79507045753994
At time: 249.2668173313141 and batch: 350, loss is 4.684475517272949 and perplexity is 108.25348033570212
At time: 249.85618376731873 and batch: 400, loss is 4.739423198699951 and perplexity is 114.36821491631457
At time: 250.44526481628418 and batch: 450, loss is 4.709621238708496 and perplexity is 111.01010561096123
At time: 251.03421449661255 and batch: 500, loss is 4.706903352737426 and perplexity is 110.70880244157759
At time: 251.6231770515442 and batch: 550, loss is 4.724429817199707 and perplexity is 112.66623970416289
At time: 252.21198511123657 and batch: 600, loss is 4.800768852233887 and perplexity is 121.60387699834558
At time: 252.80130338668823 and batch: 650, loss is 4.744386167526245 and perplexity is 114.93723163983675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.040988697725184 and perplexity of 154.62281470225517
Finished 31 epochs...
Completing Train Step...
At time: 253.90221405029297 and batch: 50, loss is 4.780552463531494 and perplexity is 119.17016903427752
At time: 254.5001299381256 and batch: 100, loss is 4.753471202850342 and perplexity is 115.98619818344471
At time: 255.08459901809692 and batch: 150, loss is 4.7352212619781495 and perplexity is 113.88865515972948
At time: 255.66927003860474 and batch: 200, loss is 4.712006425857544 and perplexity is 111.2752015142697
At time: 256.2624955177307 and batch: 250, loss is 4.699180288314819 and perplexity is 109.85708439827151
At time: 256.850430727005 and batch: 300, loss is 4.748500871658325 and perplexity is 115.41113866676116
At time: 257.43770480155945 and batch: 350, loss is 4.681477165222168 and perplexity is 107.92938441057917
At time: 258.02677869796753 and batch: 400, loss is 4.736499929428101 and perplexity is 114.03437401912808
At time: 258.6181871891022 and batch: 450, loss is 4.706540307998657 and perplexity is 110.66861748822505
At time: 259.21089267730713 and batch: 500, loss is 4.706049280166626 and perplexity is 110.61428945628538
At time: 259.803537607193 and batch: 550, loss is 4.723329610824585 and perplexity is 112.54235175263315
At time: 260.39629101753235 and batch: 600, loss is 4.798127698898315 and perplexity is 121.28312627536873
At time: 260.9918749332428 and batch: 650, loss is 4.73961971282959 and perplexity is 114.39069209499651
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.038228951248468 and perplexity of 154.1966832120586
Finished 32 epochs...
Completing Train Step...
At time: 262.11880826950073 and batch: 50, loss is 4.774464044570923 and perplexity is 118.44681539209886
At time: 262.70932507514954 and batch: 100, loss is 4.746507339477539 and perplexity is 115.18129202726483
At time: 263.30083417892456 and batch: 150, loss is 4.7266273689270015 and perplexity is 112.91410183901519
At time: 263.8918704986572 and batch: 200, loss is 4.706288757324219 and perplexity is 110.64078222399328
At time: 264.48261070251465 and batch: 250, loss is 4.695175743103027 and perplexity is 109.41803641743594
At time: 265.07354044914246 and batch: 300, loss is 4.745183572769165 and perplexity is 115.02891974238162
At time: 265.6646189689636 and batch: 350, loss is 4.677254791259766 and perplexity is 107.47462694174823
At time: 266.25574254989624 and batch: 400, loss is 4.731456899642945 and perplexity is 113.46074290952583
At time: 266.8483040332794 and batch: 450, loss is 4.703708810806274 and perplexity is 110.35570282607877
At time: 267.4421863555908 and batch: 500, loss is 4.70741455078125 and perplexity is 110.76541103268752
At time: 268.03517484664917 and batch: 550, loss is 4.72216290473938 and perplexity is 112.41112447271867
At time: 268.62830328941345 and batch: 600, loss is 4.79591178894043 and perplexity is 121.01467133396915
At time: 269.22274565696716 and batch: 650, loss is 4.73661361694336 and perplexity is 114.04733904073098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.036515778186274 and perplexity of 153.9327437596344
Finished 33 epochs...
Completing Train Step...
At time: 270.33197951316833 and batch: 50, loss is 4.771044473648072 and perplexity is 118.04246984443513
At time: 270.9366557598114 and batch: 100, loss is 4.743094978332519 and perplexity is 114.78892169708917
At time: 271.5279417037964 and batch: 150, loss is 4.723152351379395 and perplexity is 112.52240432578819
At time: 272.1192088127136 and batch: 200, loss is 4.702430381774902 and perplexity is 110.21471103501071
At time: 272.71040320396423 and batch: 250, loss is 4.692853689193726 and perplexity is 109.16425659746662
At time: 273.30136132240295 and batch: 300, loss is 4.743448419570923 and perplexity is 114.82950000633032
At time: 273.89221000671387 and batch: 350, loss is 4.6747689533233645 and perplexity is 107.2077942257452
At time: 274.48348212242126 and batch: 400, loss is 4.7283560657501225 and perplexity is 113.10946490125058
At time: 275.0748391151428 and batch: 450, loss is 4.702933053970337 and perplexity is 110.2701268325994
At time: 275.6666188240051 and batch: 500, loss is 4.706773414611816 and perplexity is 110.69441808185837
At time: 276.2569546699524 and batch: 550, loss is 4.720137910842896 and perplexity is 112.18372295281625
At time: 276.8611783981323 and batch: 600, loss is 4.793057146072388 and perplexity is 120.66971027009447
At time: 277.45245909690857 and batch: 650, loss is 4.733407306671142 and perplexity is 113.68225348759478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.033975040211397 and perplexity of 153.54213741629712
Finished 34 epochs...
Completing Train Step...
At time: 278.5707275867462 and batch: 50, loss is 4.7676856803894045 and perplexity is 117.64665469522802
At time: 279.15753865242004 and batch: 100, loss is 4.739702405929566 and perplexity is 114.40015180705525
At time: 279.74517726898193 and batch: 150, loss is 4.719590702056885 and perplexity is 112.12235182691012
At time: 280.33282566070557 and batch: 200, loss is 4.698563833236694 and perplexity is 109.78938331020989
At time: 280.9240291118622 and batch: 250, loss is 4.68972409248352 and perplexity is 108.82315053975523
At time: 281.51439666748047 and batch: 300, loss is 4.741112613677979 and perplexity is 114.5615935940279
At time: 282.10478377342224 and batch: 350, loss is 4.672142601013183 and perplexity is 106.92659820924916
At time: 282.69546937942505 and batch: 400, loss is 4.724226951599121 and perplexity is 112.64338591798148
At time: 283.286758184433 and batch: 450, loss is 4.700037412643432 and perplexity is 109.95128594343066
At time: 283.87706184387207 and batch: 500, loss is 4.704969882965088 and perplexity is 110.4949571168789
At time: 284.46625328063965 and batch: 550, loss is 4.717890777587891 and perplexity is 111.931914208162
At time: 285.0561189651489 and batch: 600, loss is 4.790603256225586 and perplexity is 120.3739631071316
At time: 285.6469211578369 and batch: 650, loss is 4.7304919147491455 and perplexity is 113.35130781667677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.032371969784007 and perplexity of 153.29619574003004
Finished 35 epochs...
Completing Train Step...
At time: 286.7566599845886 and batch: 50, loss is 4.764276933670044 and perplexity is 117.24630977186638
At time: 287.3594925403595 and batch: 100, loss is 4.73697452545166 and perplexity is 114.08850712424874
At time: 287.9488971233368 and batch: 150, loss is 4.7166570281982425 and perplexity is 111.79390343018802
At time: 288.538378238678 and batch: 200, loss is 4.694721040725708 and perplexity is 109.36829508576322
At time: 289.12823843955994 and batch: 250, loss is 4.686581230163574 and perplexity is 108.48167125266066
At time: 289.71814918518066 and batch: 300, loss is 4.739127559661865 and perplexity is 114.33440820480718
At time: 290.3071403503418 and batch: 350, loss is 4.668639965057373 and perplexity is 106.55272840886558
At time: 290.89772510528564 and batch: 400, loss is 4.7206997966766355 and perplexity is 112.24677510991769
At time: 291.4870011806488 and batch: 450, loss is 4.698491945266723 and perplexity is 109.78149105800178
At time: 292.09059500694275 and batch: 500, loss is 4.705530281066895 and perplexity is 110.55689563459933
At time: 292.6784658432007 and batch: 550, loss is 4.716370134353638 and perplexity is 111.7618350477603
At time: 293.2669532299042 and batch: 600, loss is 4.78785397529602 and perplexity is 120.04347577537122
At time: 293.8565547466278 and batch: 650, loss is 4.728738336563111 and perplexity is 113.15271161380637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.029323802274816 and perplexity of 152.8296346962475
Finished 36 epochs...
Completing Train Step...
At time: 294.9757812023163 and batch: 50, loss is 4.7614333152771 and perplexity is 116.9133795964235
At time: 295.5623302459717 and batch: 100, loss is 4.733953075408936 and perplexity is 113.74431464156855
At time: 296.14924120903015 and batch: 150, loss is 4.715350103378296 and perplexity is 111.64789263642443
At time: 296.73585200309753 and batch: 200, loss is 4.6903534030914305 and perplexity is 108.89165565601343
At time: 297.32363772392273 and batch: 250, loss is 4.681739702224731 and perplexity is 107.95772358752936
At time: 297.91137957572937 and batch: 300, loss is 4.733671808242798 and perplexity is 113.71232659932896
At time: 298.5013210773468 and batch: 350, loss is 4.6652148342132564 and perplexity is 106.18839567209605
At time: 299.0895571708679 and batch: 400, loss is 4.720336380004883 and perplexity is 112.20599017190442
At time: 299.68075680732727 and batch: 450, loss is 4.699470529556274 and perplexity is 109.88897408245678
At time: 300.2731876373291 and batch: 500, loss is 4.702974185943604 and perplexity is 110.27466255378931
At time: 300.8637144565582 and batch: 550, loss is 4.712713069915772 and perplexity is 111.3538612631968
At time: 301.45480728149414 and batch: 600, loss is 4.785793170928955 and perplexity is 119.79634438843857
At time: 302.05461168289185 and batch: 650, loss is 4.7263838577270505 and perplexity is 112.88660933808677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.028590183632047 and perplexity of 152.71755714318454
Finished 37 epochs...
Completing Train Step...
At time: 303.19209146499634 and batch: 50, loss is 4.7602000904083255 and perplexity is 116.76928797615922
At time: 303.8053550720215 and batch: 100, loss is 4.731218862533569 and perplexity is 113.43373825643833
At time: 304.4024405479431 and batch: 150, loss is 4.710822706222534 and perplexity is 111.14356080156698
At time: 305.00383734703064 and batch: 200, loss is 4.687132892608642 and perplexity is 108.54153302689723
At time: 305.60547399520874 and batch: 250, loss is 4.683381242752075 and perplexity is 108.13508610008672
At time: 306.2068700790405 and batch: 300, loss is 4.741380596160889 and perplexity is 114.592298208291
At time: 306.8076956272125 and batch: 350, loss is 4.666175947189331 and perplexity is 106.29050377793773
At time: 307.42195224761963 and batch: 400, loss is 4.717747716903687 and perplexity is 111.9159022972959
At time: 308.01860642433167 and batch: 450, loss is 4.697363986968994 and perplexity is 109.6577319249178
At time: 308.6195538043976 and batch: 500, loss is 4.700157585144043 and perplexity is 109.96449985836658
At time: 309.2230336666107 and batch: 550, loss is 4.70901650428772 and perplexity is 110.9429942733575
At time: 309.82485580444336 and batch: 600, loss is 4.783429021835327 and perplexity is 119.5134624886831
At time: 310.4270179271698 and batch: 650, loss is 4.724087457656861 and perplexity is 112.6276739438983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.028173708448223 and perplexity of 152.6539673132125
Finished 38 epochs...
Completing Train Step...
At time: 311.566632270813 and batch: 50, loss is 4.757693862915039 and perplexity is 116.47700399432799
At time: 312.16720628738403 and batch: 100, loss is 4.7294176483154295 and perplexity is 113.22960369450252
At time: 312.76608753204346 and batch: 150, loss is 4.709253721237182 and perplexity is 110.96931495375556
At time: 313.36681485176086 and batch: 200, loss is 4.685784282684327 and perplexity is 108.39525149878777
At time: 313.96705174446106 and batch: 250, loss is 4.684292421340943 and perplexity is 108.23366137826775
At time: 314.5657720565796 and batch: 300, loss is 4.7425080585479735 and perplexity is 114.72156957497842
At time: 315.15991473197937 and batch: 350, loss is 4.6667059135437015 and perplexity is 106.34684909797222
At time: 315.7593026161194 and batch: 400, loss is 4.715863990783691 and perplexity is 111.70528182681733
At time: 316.3585534095764 and batch: 450, loss is 4.696594314575195 and perplexity is 109.57336386793575
At time: 316.95888471603394 and batch: 500, loss is 4.707866325378418 and perplexity is 110.81546333696542
At time: 317.5595397949219 and batch: 550, loss is 4.716043815612793 and perplexity is 111.72537101624503
At time: 318.1574716567993 and batch: 600, loss is 4.782376909255982 and perplexity is 119.38778699537428
At time: 318.7512900829315 and batch: 650, loss is 4.721347980499267 and perplexity is 112.31955523858937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0269512101715685 and perplexity of 152.46746212561047
Finished 39 epochs...
Completing Train Step...
At time: 319.8717951774597 and batch: 50, loss is 4.754867830276489 and perplexity is 116.14830086105559
At time: 320.47887659072876 and batch: 100, loss is 4.7263673973083495 and perplexity is 112.8847511925243
At time: 321.0706663131714 and batch: 150, loss is 4.70603645324707 and perplexity is 110.61287062479242
At time: 321.66414189338684 and batch: 200, loss is 4.683081998825073 and perplexity is 108.10273217337487
At time: 322.25257110595703 and batch: 250, loss is 4.680512561798095 and perplexity is 107.8253255526526
At time: 322.8577790260315 and batch: 300, loss is 4.7376218509674075 and perplexity is 114.16238343439439
At time: 323.4504017829895 and batch: 350, loss is 4.6629616355896 and perplexity is 105.94940147692772
At time: 324.0444040298462 and batch: 400, loss is 4.711703596115112 and perplexity is 111.24150917543473
At time: 324.63782024383545 and batch: 450, loss is 4.6913568305969235 and perplexity is 109.000975376452
At time: 325.2292239665985 and batch: 500, loss is 4.697096824645996 and perplexity is 109.62843942362265
At time: 325.8226990699768 and batch: 550, loss is 4.706332387924195 and perplexity is 110.64560965301453
At time: 326.4195508956909 and batch: 600, loss is 4.7802417278289795 and perplexity is 119.13314436082574
At time: 327.0204360485077 and batch: 650, loss is 4.72030104637146 and perplexity is 112.20202559662158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.026604745902267 and perplexity of 152.41464674760184
Finished 40 epochs...
Completing Train Step...
At time: 328.1638538837433 and batch: 50, loss is 4.7548387908935545 and perplexity is 116.1449280350423
At time: 328.7628240585327 and batch: 100, loss is 4.725230770111084 and perplexity is 112.75651620565499
At time: 329.35941886901855 and batch: 150, loss is 4.704164562225341 and perplexity is 110.40600905694119
At time: 329.95473980903625 and batch: 200, loss is 4.681463193893433 and perplexity is 107.92787650420317
At time: 330.55017161369324 and batch: 250, loss is 4.6825539493560795 and perplexity is 108.0456636518813
At time: 331.14159059524536 and batch: 300, loss is 4.745147180557251 and perplexity is 115.02473366172892
At time: 331.73350977897644 and batch: 350, loss is 4.668876686096191 and perplexity is 106.57795466709867
At time: 332.32404828071594 and batch: 400, loss is 4.717230625152588 and perplexity is 111.85804646707663
At time: 332.91519474983215 and batch: 450, loss is 4.694239559173584 and perplexity is 109.31564894438225
At time: 333.5064823627472 and batch: 500, loss is 4.693307390213013 and perplexity is 109.21379576909118
At time: 334.0975675582886 and batch: 550, loss is 4.702371377944946 and perplexity is 110.20820813679197
At time: 334.6895000934601 and batch: 600, loss is 4.77762804031372 and perplexity is 118.82217411530962
At time: 335.280797958374 and batch: 650, loss is 4.718431549072266 and perplexity is 111.99246016484526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.025816075942096 and perplexity of 152.2944892827416
Finished 41 epochs...
Completing Train Step...
At time: 336.38521218299866 and batch: 50, loss is 4.752027368545532 and perplexity is 115.81885416920841
At time: 336.9855773448944 and batch: 100, loss is 4.724028043746948 and perplexity is 112.62098249220954
At time: 337.57260298728943 and batch: 150, loss is 4.704673719406128 and perplexity is 110.4622373825677
At time: 338.17353081703186 and batch: 200, loss is 4.679547824859619 and perplexity is 107.7213526395078
At time: 338.7610020637512 and batch: 250, loss is 4.675662307739258 and perplexity is 107.30361157518858
At time: 339.34854888916016 and batch: 300, loss is 4.728324384689331 and perplexity is 113.10588153017973
At time: 339.9364342689514 and batch: 350, loss is 4.661280136108399 and perplexity is 105.77139731220738
At time: 340.5249562263489 and batch: 400, loss is 4.7135427284240725 and perplexity is 111.44628527650892
At time: 341.11536693573 and batch: 450, loss is 4.6931978034973145 and perplexity is 109.20182804366773
At time: 341.70582151412964 and batch: 500, loss is 4.697127141952515 and perplexity is 109.6317631130062
At time: 342.2953553199768 and batch: 550, loss is 4.705114755630493 and perplexity is 110.51096597542528
At time: 342.8847849369049 and batch: 600, loss is 4.77997576713562 and perplexity is 119.10146384022282
At time: 343.47356367111206 and batch: 650, loss is 4.71702127456665 and perplexity is 111.83463137057448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.021288105085785 and perplexity of 151.60646313039774
Finished 42 epochs...
Completing Train Step...
At time: 344.588698387146 and batch: 50, loss is 4.752196130752563 and perplexity is 115.83840166404663
At time: 345.1754536628723 and batch: 100, loss is 4.722086925506591 and perplexity is 112.40258388618201
At time: 345.7624590396881 and batch: 150, loss is 4.698234930038452 and perplexity is 109.75327916861542
At time: 346.3535795211792 and batch: 200, loss is 4.675652894973755 and perplexity is 107.30260155620874
At time: 346.94309973716736 and batch: 250, loss is 4.674466514587403 and perplexity is 107.17537533858506
At time: 347.5321681499481 and batch: 300, loss is 4.729118061065674 and perplexity is 113.19568662975452
At time: 348.1211304664612 and batch: 350, loss is 4.659617300033569 and perplexity is 105.595662966269
At time: 348.7103350162506 and batch: 400, loss is 4.710413694381714 and perplexity is 111.09811106454212
At time: 349.299751996994 and batch: 450, loss is 4.6895473861694335 and perplexity is 108.80392250084401
At time: 349.88856172561646 and batch: 500, loss is 4.6919192123413085 and perplexity is 109.06229277540068
At time: 350.4779074192047 and batch: 550, loss is 4.700654182434082 and perplexity is 110.0191214923519
At time: 351.06835079193115 and batch: 600, loss is 4.776566247940064 and perplexity is 118.69607659355307
At time: 351.6606571674347 and batch: 650, loss is 4.717374172210693 and perplexity is 111.87410451308742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.024885589001226 and perplexity of 152.15284715759094
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 352.7687449455261 and batch: 50, loss is 4.746383028030396 and perplexity is 115.16697456409982
At time: 353.37273383140564 and batch: 100, loss is 4.71575140953064 and perplexity is 111.69270661409668
At time: 353.9615788459778 and batch: 150, loss is 4.690621757507325 and perplexity is 108.92088113388037
At time: 354.5509285926819 and batch: 200, loss is 4.66591893196106 and perplexity is 106.26318901015215
At time: 355.13991928100586 and batch: 250, loss is 4.662575635910034 and perplexity is 105.90851293389741
At time: 355.7289454936981 and batch: 300, loss is 4.716414117813111 and perplexity is 111.76675082800845
At time: 356.31806468963623 and batch: 350, loss is 4.644583415985108 and perplexity is 104.02002367913434
At time: 356.9076087474823 and batch: 400, loss is 4.697240734100342 and perplexity is 109.64421712777408
At time: 357.4962317943573 and batch: 450, loss is 4.67250018119812 and perplexity is 106.96483987883563
At time: 358.08876180648804 and batch: 500, loss is 4.670857362747192 and perplexity is 106.78926032843326
At time: 358.68067598342896 and batch: 550, loss is 4.682063102722168 and perplexity is 107.99264281518317
At time: 359.27242946624756 and batch: 600, loss is 4.758003787994385 and perplexity is 116.51310873362603
At time: 359.86453557014465 and batch: 650, loss is 4.705205993652344 and perplexity is 110.5210492373351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.013684141869638 and perplexity of 150.45802503551567
Finished 44 epochs...
Completing Train Step...
At time: 360.9849338531494 and batch: 50, loss is 4.741754026412964 and perplexity is 114.63509843004816
At time: 361.57372665405273 and batch: 100, loss is 4.712561054229736 and perplexity is 111.3369350161443
At time: 362.1622974872589 and batch: 150, loss is 4.687752294540405 and perplexity is 108.60878468788482
At time: 362.7519657611847 and batch: 200, loss is 4.663454484939575 and perplexity is 106.00163144027044
At time: 363.3425543308258 and batch: 250, loss is 4.660734739303589 and perplexity is 105.71372565847041
At time: 363.9313871860504 and batch: 300, loss is 4.7140345287323 and perplexity is 111.50110807378798
At time: 364.52062249183655 and batch: 350, loss is 4.643273496627808 and perplexity is 103.8838550410086
At time: 365.10986709594727 and batch: 400, loss is 4.695798664093018 and perplexity is 109.48621644218355
At time: 365.6990821361542 and batch: 450, loss is 4.671656446456909 and perplexity is 106.87462799014584
At time: 366.2884724140167 and batch: 500, loss is 4.670738067626953 and perplexity is 106.77652165062828
At time: 366.8780381679535 and batch: 550, loss is 4.6826272964477536 and perplexity is 108.05358877771707
At time: 367.4671063423157 and batch: 600, loss is 4.758633975982666 and perplexity is 116.58655703591542
At time: 368.0785665512085 and batch: 650, loss is 4.7051207637786865 and perplexity is 110.51162994368043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.012779684627757 and perplexity of 150.3220037071835
Finished 45 epochs...
Completing Train Step...
At time: 369.1854462623596 and batch: 50, loss is 4.739952812194824 and perplexity is 114.428801908747
At time: 369.78768014907837 and batch: 100, loss is 4.710991411209107 and perplexity is 111.16231285623773
At time: 370.3749191761017 and batch: 150, loss is 4.6862695598602295 and perplexity is 108.4478660055935
At time: 370.9631860256195 and batch: 200, loss is 4.662241220474243 and perplexity is 105.87310141380011
At time: 371.55158615112305 and batch: 250, loss is 4.659777326583862 and perplexity is 105.6125624280846
At time: 372.1404597759247 and batch: 300, loss is 4.71285641670227 and perplexity is 111.3698246254941
At time: 372.7292568683624 and batch: 350, loss is 4.642698087692261 and perplexity is 103.82409653699632
At time: 373.31809186935425 and batch: 400, loss is 4.695090179443359 and perplexity is 109.40867461032737
At time: 373.909161567688 and batch: 450, loss is 4.671199522018433 and perplexity is 106.82580551570433
At time: 374.5007276535034 and batch: 500, loss is 4.670508699417114 and perplexity is 106.75203331953418
At time: 375.09494161605835 and batch: 550, loss is 4.682737674713135 and perplexity is 108.06551620366673
At time: 375.68876338005066 and batch: 600, loss is 4.7587823390960695 and perplexity is 116.60385546369085
At time: 376.28207659721375 and batch: 650, loss is 4.7049292469024655 and perplexity is 110.49046712811038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.012255201152727 and perplexity of 150.24318297219904
Finished 46 epochs...
Completing Train Step...
At time: 377.40238213539124 and batch: 50, loss is 4.7387135791778565 and perplexity is 114.28708578711424
At time: 377.9899914264679 and batch: 100, loss is 4.709825229644776 and perplexity is 111.0327529761914
At time: 378.5779182910919 and batch: 150, loss is 4.685128393173218 and perplexity is 108.32417950051274
At time: 379.1676585674286 and batch: 200, loss is 4.661368942260742 and perplexity is 105.78079088012775
At time: 379.7684438228607 and batch: 250, loss is 4.6591042137146 and perplexity is 105.54149717331046
At time: 380.3711590766907 and batch: 300, loss is 4.711976556777954 and perplexity is 111.27187787605662
At time: 380.96474170684814 and batch: 350, loss is 4.642224550247192 and perplexity is 103.77494357838737
At time: 381.5625171661377 and batch: 400, loss is 4.6945199775695805 and perplexity is 109.3463073617077
At time: 382.1617097854614 and batch: 450, loss is 4.670824880599976 and perplexity is 106.78579164029367
At time: 382.76108145713806 and batch: 500, loss is 4.670177211761475 and perplexity is 106.71665220280003
At time: 383.3740677833557 and batch: 550, loss is 4.682636241912842 and perplexity is 108.05455537164649
At time: 383.9730632305145 and batch: 600, loss is 4.758748006820679 and perplexity is 116.59985225673346
At time: 384.5720615386963 and batch: 650, loss is 4.704725770950318 and perplexity is 110.46798726224183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.011866550819547 and perplexity of 150.18480225465635
Finished 47 epochs...
Completing Train Step...
At time: 385.6810779571533 and batch: 50, loss is 4.737706022262573 and perplexity is 114.17199303448784
At time: 386.2859444618225 and batch: 100, loss is 4.708880300521851 and perplexity is 110.92788444877151
At time: 386.87615942955017 and batch: 150, loss is 4.684158020019531 and perplexity is 108.21911560866451
At time: 387.4679892063141 and batch: 200, loss is 4.660657939910888 and perplexity is 105.7056072202893
At time: 388.05922770500183 and batch: 250, loss is 4.6585554504394535 and perplexity is 105.48359576419426
At time: 388.6503667831421 and batch: 300, loss is 4.711227645874024 and perplexity is 111.18857635001841
At time: 389.2412271499634 and batch: 350, loss is 4.641805477142334 and perplexity is 103.73146340189813
At time: 389.83297657966614 and batch: 400, loss is 4.693991641998291 and perplexity is 109.28855107663284
At time: 390.42497754096985 and batch: 450, loss is 4.670446367263794 and perplexity is 106.7453794428034
At time: 391.0158052444458 and batch: 500, loss is 4.669793434143067 and perplexity is 106.67570459806285
At time: 391.60724401474 and batch: 550, loss is 4.682428560256958 and perplexity is 108.03211675278648
At time: 392.19835925102234 and batch: 600, loss is 4.758616199493408 and perplexity is 116.58448455465748
At time: 392.7894432544708 and batch: 650, loss is 4.7045121765136715 and perplexity is 110.44439443447321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.011537140490962 and perplexity of 150.1353379770664
Finished 48 epochs...
Completing Train Step...
At time: 393.90609312057495 and batch: 50, loss is 4.736831026077271 and perplexity is 114.07213666945448
At time: 394.49290800094604 and batch: 100, loss is 4.708053245544433 and perplexity is 110.83617891778906
At time: 395.0804862976074 and batch: 150, loss is 4.683287572860718 and perplexity is 108.12495757269626
At time: 395.66813921928406 and batch: 200, loss is 4.660033082962036 and perplexity is 105.63957696896415
At time: 396.2557909488678 and batch: 250, loss is 4.658077716827393 and perplexity is 105.43321474028913
At time: 396.84319519996643 and batch: 300, loss is 4.710560703277588 and perplexity is 111.11444467574711
At time: 397.43106031417847 and batch: 350, loss is 4.641409492492675 and perplexity is 103.69039546637698
At time: 398.0185444355011 and batch: 400, loss is 4.693475999832153 and perplexity is 109.23221181811857
At time: 398.6215682029724 and batch: 450, loss is 4.670060453414917 and perplexity is 106.7041928703183
At time: 399.2119333744049 and batch: 500, loss is 4.669380683898925 and perplexity is 106.63168326047995
At time: 399.80288100242615 and batch: 550, loss is 4.682159080505371 and perplexity is 108.00300820705849
At time: 400.39562702178955 and batch: 600, loss is 4.7584207820892335 and perplexity is 116.56170414323566
At time: 400.9889621734619 and batch: 650, loss is 4.704287481307984 and perplexity is 110.41958089639459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.0112355550130205 and perplexity of 150.0900661664102
Finished 49 epochs...
Completing Train Step...
At time: 402.0948715209961 and batch: 50, loss is 4.7360398483276365 and perplexity is 113.9819210260792
At time: 402.69716930389404 and batch: 100, loss is 4.707295684814453 and perplexity is 110.75224557749529
At time: 403.2854964733124 and batch: 150, loss is 4.682476453781128 and perplexity is 108.03729091548473
At time: 403.87482714653015 and batch: 200, loss is 4.659456806182861 and perplexity is 105.57871687160984
At time: 404.46383118629456 and batch: 250, loss is 4.6576399326324465 and perplexity is 105.38706784718072
At time: 405.0524866580963 and batch: 300, loss is 4.709944696426391 and perplexity is 111.046018494222
At time: 405.6413733959198 and batch: 350, loss is 4.641014928817749 and perplexity is 103.64949107311175
At time: 406.23270630836487 and batch: 400, loss is 4.692953119277954 and perplexity is 109.17511134833028
At time: 406.82217955589294 and batch: 450, loss is 4.669654874801636 and perplexity is 106.66092470665674
At time: 407.4116563796997 and batch: 500, loss is 4.668938293457031 and perplexity is 106.58452085587007
At time: 408.0006263256073 and batch: 550, loss is 4.681827821731567 and perplexity is 107.9672371880518
At time: 408.5898246765137 and batch: 600, loss is 4.758147392272949 and perplexity is 116.5298417159842
At time: 409.17905473709106 and batch: 650, loss is 4.703989782333374 and perplexity is 110.3867139928494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.010872934378829 and perplexity of 150.03565027818937
Finished Training.
Improved accuracyfrom -10000000 to -150.03565027818937
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f28e8edc8d0>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.3839706694510512, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 7.233346149393008, 'seq_len': 20, 'anneal': 4.453353708609926, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8083972930908203 and batch: 50, loss is 6.6719234180450435 and perplexity is 789.913478079689
At time: 1.3794727325439453 and batch: 100, loss is 5.792938098907471 and perplexity is 327.9752338267035
At time: 1.9363315105438232 and batch: 150, loss is 5.499717226028443 and perplexity is 244.62274953672429
At time: 2.4934821128845215 and batch: 200, loss is 5.362801141738892 and perplexity is 213.32165451772076
At time: 3.05033278465271 and batch: 250, loss is 5.267099056243897 and perplexity is 193.8527899338874
At time: 3.6072545051574707 and batch: 300, loss is 5.257904167175293 and perplexity is 192.07850471340188
At time: 4.164409399032593 and batch: 350, loss is 5.145613918304443 and perplexity is 171.6768478849158
At time: 4.721270561218262 and batch: 400, loss is 5.1575972270965575 and perplexity is 173.74648032137503
At time: 5.278654336929321 and batch: 450, loss is 5.09687370300293 and perplexity is 163.50992682216807
At time: 5.848875522613525 and batch: 500, loss is 5.082015590667725 and perplexity is 161.0984374156788
At time: 6.405436754226685 and batch: 550, loss is 5.081394166946411 and perplexity is 160.99835812424558
At time: 6.96228551864624 and batch: 600, loss is 5.108953123092651 and perplexity is 165.49700915662575
At time: 7.519086599349976 and batch: 650, loss is 5.038471145629883 and perplexity is 154.23403330517414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.9144541422526045 and perplexity of 136.24491915637165
Finished 1 epochs...
Completing Train Step...
At time: 8.583484411239624 and batch: 50, loss is 4.803553657531738 and perplexity is 121.94299208483753
At time: 9.133015632629395 and batch: 100, loss is 4.716783428192139 and perplexity is 111.80803507200018
At time: 9.682743072509766 and batch: 150, loss is 4.623526411056519 and perplexity is 101.85257361739568
At time: 10.231438159942627 and batch: 200, loss is 4.550953855514527 and perplexity is 94.72271702333076
At time: 10.78060269355774 and batch: 250, loss is 4.514086351394654 and perplexity is 91.29411716390348
At time: 11.329979658126831 and batch: 300, loss is 4.579514408111573 and perplexity is 97.46705350328327
At time: 11.879201889038086 and batch: 350, loss is 4.514020681381226 and perplexity is 91.28812207485437
At time: 12.429269075393677 and batch: 400, loss is 4.5443073177337645 and perplexity is 94.09522653557994
At time: 12.980870485305786 and batch: 450, loss is 4.495370979309082 and perplexity is 89.60140308679215
At time: 13.541150093078613 and batch: 500, loss is 4.461236791610718 and perplexity is 86.59454229725046
At time: 14.112529993057251 and batch: 550, loss is 4.471565132141113 and perplexity is 87.49355488041574
At time: 14.693888664245605 and batch: 600, loss is 4.537273769378662 and perplexity is 93.43572524569986
At time: 15.2818603515625 and batch: 650, loss is 4.430230236053466 and perplexity is 83.95074317317598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.6013761034198835 and perplexity of 99.62131058690109
Finished 2 epochs...
Completing Train Step...
At time: 16.385950803756714 and batch: 50, loss is 4.391591348648071 and perplexity is 80.76884816290168
At time: 16.986427307128906 and batch: 100, loss is 4.345157299041748 and perplexity is 77.10416493671775
At time: 17.578588724136353 and batch: 150, loss is 4.280626831054687 and perplexity is 72.28573675374673
At time: 18.176089763641357 and batch: 200, loss is 4.226138000488281 and perplexity is 68.45235805781505
At time: 18.77414083480835 and batch: 250, loss is 4.194287009239197 and perplexity is 66.30643884138078
At time: 19.373116493225098 and batch: 300, loss is 4.290067148208618 and perplexity is 72.97136823178404
At time: 19.97160267829895 and batch: 350, loss is 4.242410678863525 and perplexity is 69.57537371974672
At time: 20.58853840827942 and batch: 400, loss is 4.289821872711181 and perplexity is 72.95347233794418
At time: 21.187636613845825 and batch: 450, loss is 4.246231470108032 and perplexity is 69.84171519205698
At time: 21.78615379333496 and batch: 500, loss is 4.214372935295105 and perplexity is 67.65173055502184
At time: 22.385384559631348 and batch: 550, loss is 4.231235103607178 and perplexity is 68.80215751017828
At time: 22.985034704208374 and batch: 600, loss is 4.320255327224731 and perplexity is 75.20782844687305
At time: 23.586705923080444 and batch: 650, loss is 4.206352257728577 and perplexity is 67.11128809206767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.519215901692708 and perplexity of 91.76361806458499
Finished 3 epochs...
Completing Train Step...
At time: 24.720435857772827 and batch: 50, loss is 4.1916065549850465 and perplexity is 66.12894545303648
At time: 25.32087254524231 and batch: 100, loss is 4.159217391014099 and perplexity is 64.02139926665338
At time: 25.920753240585327 and batch: 150, loss is 4.10460955619812 and perplexity is 60.6190715856498
At time: 26.520981788635254 and batch: 200, loss is 4.052959814071655 and perplexity is 57.567594509909824
At time: 27.121004343032837 and batch: 250, loss is 4.023706827163696 and perplexity is 55.90796335461227
At time: 27.7200345993042 and batch: 300, loss is 4.133176288604736 and perplexity is 62.37573194579373
At time: 28.319491624832153 and batch: 350, loss is 4.087983469963074 and perplexity is 59.61954579946805
At time: 28.91924023628235 and batch: 400, loss is 4.142093663215637 and perplexity is 62.93444714816922
At time: 29.51881742477417 and batch: 450, loss is 4.099129929542541 and perplexity is 60.28781012858712
At time: 30.118040323257446 and batch: 500, loss is 4.068950061798096 and perplexity is 58.495513664312874
At time: 30.717406749725342 and batch: 550, loss is 4.087146201133728 and perplexity is 59.569649103539355
At time: 31.31745743751526 and batch: 600, loss is 4.1845735788345335 and perplexity is 65.66549378935355
At time: 31.917532920837402 and batch: 650, loss is 4.0676066684722905 and perplexity is 58.41698394162961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.494834750306373 and perplexity of 89.55336909553414
Finished 4 epochs...
Completing Train Step...
At time: 33.030426263809204 and batch: 50, loss is 4.060826630592346 and perplexity is 58.02225423097538
At time: 33.638171434402466 and batch: 100, loss is 4.038236675262451 and perplexity is 56.72622780302664
At time: 34.233296155929565 and batch: 150, loss is 3.993455934524536 and perplexity is 54.24202269571738
At time: 34.82854080200195 and batch: 200, loss is 3.9424327182769776 and perplexity is 51.543840545822796
At time: 35.42356634140015 and batch: 250, loss is 3.9075926637649534 and perplexity is 49.778972891170056
At time: 36.03232383728027 and batch: 300, loss is 4.022178387641906 and perplexity is 55.822576684641646
At time: 36.627209186553955 and batch: 350, loss is 3.9825990200042725 and perplexity is 53.656306977952156
At time: 37.22174286842346 and batch: 400, loss is 4.037661304473877 and perplexity is 56.69359857645488
At time: 37.81660056114197 and batch: 450, loss is 3.9969667911529543 and perplexity is 54.432793348888524
At time: 38.41135287284851 and batch: 500, loss is 3.965959825515747 and perplexity is 52.770895941846135
At time: 39.00579881668091 and batch: 550, loss is 3.9879690647125243 and perplexity is 53.94521878567759
At time: 39.600412130355835 and batch: 600, loss is 4.089491238594055 and perplexity is 59.7095060830219
At time: 40.19523549079895 and batch: 650, loss is 3.9713661336898802 and perplexity is 53.05696425789986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.49126449285769 and perplexity of 89.23421059050023
Finished 5 epochs...
Completing Train Step...
At time: 41.31659698486328 and batch: 50, loss is 3.9700439977645874 and perplexity is 52.98686209187539
At time: 41.905861377716064 and batch: 100, loss is 3.9495309591293335 and perplexity is 51.91101273713293
At time: 42.49523639678955 and batch: 150, loss is 3.9112046813964843 and perplexity is 49.959100535206744
At time: 43.08469104766846 and batch: 200, loss is 3.859381070137024 and perplexity is 47.43598273493451
At time: 43.67454123497009 and batch: 250, loss is 3.826881971359253 and perplexity is 45.91913760055572
At time: 44.26412487030029 and batch: 300, loss is 3.9401676416397096 and perplexity is 51.427221921727856
At time: 44.85325908660889 and batch: 350, loss is 3.900830445289612 and perplexity is 49.44349217633882
At time: 45.44266939163208 and batch: 400, loss is 3.9579631233215333 and perplexity is 52.3505855906363
At time: 46.03187036514282 and batch: 450, loss is 3.9185563707351685 and perplexity is 50.32773771467277
At time: 46.620909214019775 and batch: 500, loss is 3.885387649536133 and perplexity is 48.685811834275746
At time: 47.21054744720459 and batch: 550, loss is 3.912017650604248 and perplexity is 49.99973225952194
At time: 47.79948401451111 and batch: 600, loss is 4.010956659317016 and perplexity is 55.199652574168304
At time: 48.38906693458557 and batch: 650, loss is 3.8964385986328125 and perplexity is 49.22682008409554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.497794357000613 and perplexity of 89.81880444456733
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 49.49199342727661 and batch: 50, loss is 3.8934741497039793 and perplexity is 49.08110577804753
At time: 50.091989040374756 and batch: 100, loss is 3.84906898021698 and perplexity is 46.94933212046959
At time: 50.67885375022888 and batch: 150, loss is 3.788469500541687 and perplexity is 44.18871768662906
At time: 51.278297662734985 and batch: 200, loss is 3.7238163566589355 and perplexity is 41.422174635726265
At time: 51.86429977416992 and batch: 250, loss is 3.670860743522644 and perplexity is 39.28570622866243
At time: 52.450268268585205 and batch: 300, loss is 3.772860312461853 and perplexity is 43.50432299976796
At time: 53.036309242248535 and batch: 350, loss is 3.7225972557067872 and perplexity is 41.371707591648914
At time: 53.62225127220154 and batch: 400, loss is 3.757919988632202 and perplexity is 42.859185607166346
At time: 54.20756530761719 and batch: 450, loss is 3.700163941383362 and perplexity is 40.45393589067373
At time: 54.79361152648926 and batch: 500, loss is 3.6402724742889405 and perplexity is 38.102217185665246
At time: 55.38100504875183 and batch: 550, loss is 3.649087429046631 and perplexity is 38.43957120205972
At time: 55.981268644332886 and batch: 600, loss is 3.7251492881774904 and perplexity is 41.47742437174236
At time: 56.5696165561676 and batch: 650, loss is 3.596639995574951 and perplexity is 36.475470573069025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.384225882735907 and perplexity of 80.17613345454942
Finished 7 epochs...
Completing Train Step...
At time: 57.6832115650177 and batch: 50, loss is 3.779046115875244 and perplexity is 43.77426623661301
At time: 58.269445180892944 and batch: 100, loss is 3.7495621967315675 and perplexity is 42.50247020583532
At time: 58.85600280761719 and batch: 150, loss is 3.6995609188079834 and perplexity is 40.4295486078488
At time: 59.44434714317322 and batch: 200, loss is 3.6427907037734983 and perplexity is 38.198288226103166
At time: 60.03383183479309 and batch: 250, loss is 3.5940463066101076 and perplexity is 36.380987130854486
At time: 60.62317204475403 and batch: 300, loss is 3.7049809551239012 and perplexity is 40.6492731491362
At time: 61.213932037353516 and batch: 350, loss is 3.6628187084198 and perplexity is 38.97103618980162
At time: 61.80570936203003 and batch: 400, loss is 3.706648087501526 and perplexity is 40.71709738881418
At time: 62.40110993385315 and batch: 450, loss is 3.6572735786437987 and perplexity is 38.755534780459286
At time: 62.99922704696655 and batch: 500, loss is 3.605512957572937 and perplexity is 36.8005561404506
At time: 63.59562134742737 and batch: 550, loss is 3.6246450996398925 and perplexity is 37.511407984758655
At time: 64.19459557533264 and batch: 600, loss is 3.7125442218780518 and perplexity is 40.957880012237446
At time: 64.79616355895996 and batch: 650, loss is 3.5884451055526734 and perplexity is 36.17777954182463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.379754459156709 and perplexity of 79.81843231356831
Finished 8 epochs...
Completing Train Step...
At time: 65.9198362827301 and batch: 50, loss is 3.7298238849639893 and perplexity is 41.6717684927263
At time: 66.53091359138489 and batch: 100, loss is 3.7032672548294068 and perplexity is 40.57967213244445
At time: 67.12325549125671 and batch: 150, loss is 3.6548745965957643 and perplexity is 38.66267238041047
At time: 67.71952843666077 and batch: 200, loss is 3.601080646514893 and perplexity is 36.637805575532724
At time: 68.31786179542542 and batch: 250, loss is 3.5526443004608153 and perplexity is 34.90549617944371
At time: 68.91581058502197 and batch: 300, loss is 3.667498507499695 and perplexity is 39.15384021854282
At time: 69.51467299461365 and batch: 350, loss is 3.629113583564758 and perplexity is 37.679402168459184
At time: 70.11452150344849 and batch: 400, loss is 3.676052141189575 and perplexity is 39.49018425646612
At time: 70.70752906799316 and batch: 450, loss is 3.630270857810974 and perplexity is 37.72303281164192
At time: 71.30318284034729 and batch: 500, loss is 3.581742191314697 and perplexity is 35.936093892218025
At time: 71.90069961547852 and batch: 550, loss is 3.6051564979553223 and perplexity is 36.78744056600597
At time: 72.49959826469421 and batch: 600, loss is 3.6983121633529663 and perplexity is 40.37909349808425
At time: 73.09729290008545 and batch: 650, loss is 3.574412007331848 and perplexity is 35.6736388095612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.381951425589767 and perplexity of 79.9939834995214
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 74.23189377784729 and batch: 50, loss is 3.710163869857788 and perplexity is 40.86050178301629
At time: 74.82624530792236 and batch: 100, loss is 3.6840407180786134 and perplexity is 39.806918071554236
At time: 75.42440629005432 and batch: 150, loss is 3.6274782705307005 and perplexity is 37.617834905571854
At time: 76.02203273773193 and batch: 200, loss is 3.568202223777771 and perplexity is 35.45279962538991
At time: 76.62014675140381 and batch: 250, loss is 3.511914920806885 and perplexity is 33.51237993313169
At time: 77.21801424026489 and batch: 300, loss is 3.623415722846985 and perplexity is 37.465320665457874
At time: 77.81231832504272 and batch: 350, loss is 3.58321382522583 and perplexity is 35.98901759923301
At time: 78.40829658508301 and batch: 400, loss is 3.624586296081543 and perplexity is 37.50920224534376
At time: 79.00824880599976 and batch: 450, loss is 3.57471164226532 and perplexity is 35.6843294795217
At time: 79.60885095596313 and batch: 500, loss is 3.51529670715332 and perplexity is 33.62590349004102
At time: 80.20870637893677 and batch: 550, loss is 3.5387238597869874 and perplexity is 34.42296262259687
At time: 80.80765414237976 and batch: 600, loss is 3.6218904638290406 and perplexity is 37.40821990504759
At time: 81.42375063896179 and batch: 650, loss is 3.49853777885437 and perplexity is 33.067065229261964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.362400728113511 and perplexity of 78.44523422367358
Finished 10 epochs...
Completing Train Step...
At time: 82.54236817359924 and batch: 50, loss is 3.6812838172912596 and perplexity is 39.697325485137746
At time: 83.14932012557983 and batch: 100, loss is 3.657060341835022 and perplexity is 38.7472715549435
At time: 83.74217462539673 and batch: 150, loss is 3.604462833404541 and perplexity is 36.76193127104104
At time: 84.33698463439941 and batch: 200, loss is 3.5482860612869263 and perplexity is 34.75370069938425
At time: 84.93510270118713 and batch: 250, loss is 3.4935813617706297 and perplexity is 32.90357655596653
At time: 85.52972459793091 and batch: 300, loss is 3.607934904098511 and perplexity is 36.88979313953338
At time: 86.13339948654175 and batch: 350, loss is 3.5701092529296874 and perplexity is 35.52047365545715
At time: 86.73647046089172 and batch: 400, loss is 3.6141601181030274 and perplexity is 37.12015628239721
At time: 87.33939242362976 and batch: 450, loss is 3.5664555644989013 and perplexity is 35.39092971252051
At time: 87.94234299659729 and batch: 500, loss is 3.510285530090332 and perplexity is 33.457819634472195
At time: 88.53944444656372 and batch: 550, loss is 3.5370331811904907 and perplexity is 34.36481362594592
At time: 89.13339018821716 and batch: 600, loss is 3.623206434249878 and perplexity is 37.45748042152108
At time: 89.72649025917053 and batch: 650, loss is 3.5008396816253664 and perplexity is 33.14327007276289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3614238664215685 and perplexity of 78.36864149578668
Finished 11 epochs...
Completing Train Step...
At time: 90.85417175292969 and batch: 50, loss is 3.668075189590454 and perplexity is 39.176426048777266
At time: 91.44515419006348 and batch: 100, loss is 3.6442854738235475 and perplexity is 38.255428578511165
At time: 92.0362389087677 and batch: 150, loss is 3.591902174949646 and perplexity is 36.30306507193775
At time: 92.6272325515747 and batch: 200, loss is 3.5366781616210936 and perplexity is 34.35261561000533
At time: 93.21855878829956 and batch: 250, loss is 3.4825409269332885 and perplexity is 32.542304731749105
At time: 93.81093001365662 and batch: 300, loss is 3.598044981956482 and perplexity is 36.526754130397855
At time: 94.40424251556396 and batch: 350, loss is 3.561434383392334 and perplexity is 35.2136708417751
At time: 94.99810791015625 and batch: 400, loss is 3.606940870285034 and perplexity is 36.853141657204844
At time: 95.59106016159058 and batch: 450, loss is 3.560367455482483 and perplexity is 35.176120428899544
At time: 96.18403601646423 and batch: 500, loss is 3.506008758544922 and perplexity is 33.31503373246807
At time: 96.7909152507782 and batch: 550, loss is 3.534446368217468 and perplexity is 34.27603315898162
At time: 97.38500571250916 and batch: 600, loss is 3.622088761329651 and perplexity is 37.41563859708678
At time: 97.98385119438171 and batch: 650, loss is 3.499828429222107 and perplexity is 33.10977080221364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.361382278741575 and perplexity of 78.3653823935723
Finished 12 epochs...
Completing Train Step...
At time: 99.0932548046112 and batch: 50, loss is 3.6570856618881225 and perplexity is 38.74825265033741
At time: 99.6948127746582 and batch: 100, loss is 3.633949112892151 and perplexity is 37.862043249955136
At time: 100.28385043144226 and batch: 150, loss is 3.5817115449905397 and perplexity is 35.93499259991102
At time: 100.87240219116211 and batch: 200, loss is 3.5272897243499757 and perplexity is 34.03160747350348
At time: 101.46117067337036 and batch: 250, loss is 3.4734915494918823 and perplexity is 32.249145587884094
At time: 102.05022644996643 and batch: 300, loss is 3.5898464822769167 and perplexity is 36.22851378059712
At time: 102.63993000984192 and batch: 350, loss is 3.554140648841858 and perplexity is 34.957766059363465
At time: 103.2292013168335 and batch: 400, loss is 3.6006368589401245 and perplexity is 36.62154977997836
At time: 103.8178219795227 and batch: 450, loss is 3.554819951057434 and perplexity is 34.98152101478262
At time: 104.40807509422302 and batch: 500, loss is 3.501714687347412 and perplexity is 33.172283315249324
At time: 104.99949026107788 and batch: 550, loss is 3.5313023042678835 and perplexity is 34.16843635304511
At time: 105.59151220321655 and batch: 600, loss is 3.619952893257141 and perplexity is 37.335809012273515
At time: 106.183189868927 and batch: 650, loss is 3.497628173828125 and perplexity is 33.0370009359153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.361775117761948 and perplexity of 78.3961734211856
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 107.2995331287384 and batch: 50, loss is 3.6546341800689697 and perplexity is 38.65337835226392
At time: 107.88779139518738 and batch: 100, loss is 3.6331630611419676 and perplexity is 37.83229341857832
At time: 108.47686195373535 and batch: 150, loss is 3.58102192401886 and perplexity is 35.910219618362554
At time: 109.06577706336975 and batch: 200, loss is 3.522125315666199 and perplexity is 33.85630739458991
At time: 109.65728902816772 and batch: 250, loss is 3.468503518104553 and perplexity is 32.08868635800613
At time: 110.24789118766785 and batch: 300, loss is 3.5809902286529542 and perplexity is 35.90908144884944
At time: 110.83706855773926 and batch: 350, loss is 3.544491996765137 and perplexity is 34.62209273898037
At time: 111.42572259902954 and batch: 400, loss is 3.5882710552215578 and perplexity is 36.17148333526066
At time: 112.02819395065308 and batch: 450, loss is 3.5407831859588623 and perplexity is 34.493923771436734
At time: 112.61935424804688 and batch: 500, loss is 3.484695649147034 and perplexity is 32.61249995708805
At time: 113.21215891838074 and batch: 550, loss is 3.5139529800415037 and perplexity is 33.580749795777855
At time: 113.80800437927246 and batch: 600, loss is 3.597552103996277 and perplexity is 36.50875533429846
At time: 114.40500807762146 and batch: 650, loss is 3.474585909843445 and perplexity is 32.284457092418336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.358419979319853 and perplexity of 78.13358416401219
Finished 14 epochs...
Completing Train Step...
At time: 115.51074981689453 and batch: 50, loss is 3.648305640220642 and perplexity is 38.4095313187707
At time: 116.1120092868805 and batch: 100, loss is 3.6239689636230468 and perplexity is 37.486053743201424
At time: 116.70037579536438 and batch: 150, loss is 3.5723932933807374 and perplexity is 35.601696577035305
At time: 117.28918838500977 and batch: 200, loss is 3.5158871459960936 and perplexity is 33.645763392036706
At time: 117.87765073776245 and batch: 250, loss is 3.4625401878356934 and perplexity is 31.897900348734467
At time: 118.466392993927 and batch: 300, loss is 3.5762629461288453 and perplexity is 35.73972967787122
At time: 119.05522060394287 and batch: 350, loss is 3.5400875329971315 and perplexity is 34.46993631564311
At time: 119.64469242095947 and batch: 400, loss is 3.585014944076538 and perplexity is 36.05389650702227
At time: 120.23502612113953 and batch: 450, loss is 3.5388827991485594 and perplexity is 34.42843422111419
At time: 120.8276035785675 and batch: 500, loss is 3.483896164894104 and perplexity is 32.58643719669516
At time: 121.41898488998413 and batch: 550, loss is 3.5146710968017576 and perplexity is 33.60487335575777
At time: 122.00934743881226 and batch: 600, loss is 3.5993007707595823 and perplexity is 36.57265283275478
At time: 122.59943008422852 and batch: 650, loss is 3.4768426179885865 and perplexity is 32.357395959613285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.358027738683364 and perplexity of 78.10294300697484
Finished 15 epochs...
Completing Train Step...
At time: 123.71352458000183 and batch: 50, loss is 3.645158109664917 and perplexity is 38.28882620647998
At time: 124.29879522323608 and batch: 100, loss is 3.6203315782547 and perplexity is 37.34995020037708
At time: 124.88439345359802 and batch: 150, loss is 3.5688125562667845 and perplexity is 35.47444422535717
At time: 125.47144031524658 and batch: 200, loss is 3.5126581716537477 and perplexity is 33.53729729667608
At time: 126.05969905853271 and batch: 250, loss is 3.459620294570923 and perplexity is 31.80489772928357
At time: 126.65022659301758 and batch: 300, loss is 3.573755521774292 and perplexity is 35.65022726640804
At time: 127.25541520118713 and batch: 350, loss is 3.5379178667068483 and perplexity is 34.39522913092772
At time: 127.84544396400452 and batch: 400, loss is 3.5832421827316283 and perplexity is 35.99003817247864
At time: 128.4358630180359 and batch: 450, loss is 3.537684831619263 and perplexity is 34.38721476954464
At time: 129.0264127254486 and batch: 500, loss is 3.483233861923218 and perplexity is 32.56486224789419
At time: 129.6169879436493 and batch: 550, loss is 3.514677047729492 and perplexity is 33.60507333652568
At time: 130.20771265029907 and batch: 600, loss is 3.599701552391052 and perplexity is 36.58731341787464
At time: 130.79803848266602 and batch: 650, loss is 3.477224817276001 and perplexity is 32.36976529691702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357977175245098 and perplexity of 78.09899395347745
Finished 16 epochs...
Completing Train Step...
At time: 131.9014275074005 and batch: 50, loss is 3.6423640060424805 and perplexity is 38.18199258009315
At time: 132.5038378238678 and batch: 100, loss is 3.6174666261672974 and perplexity is 37.24309751957836
At time: 133.0944380760193 and batch: 150, loss is 3.5660229110717774 and perplexity is 35.375621017409784
At time: 133.68651032447815 and batch: 200, loss is 3.5100584506988524 and perplexity is 33.45022291571114
At time: 134.27867794036865 and batch: 250, loss is 3.4572852420806885 and perplexity is 31.730718263817046
At time: 134.87119579315186 and batch: 300, loss is 3.571679129600525 and perplexity is 35.576280211629935
At time: 135.46480917930603 and batch: 350, loss is 3.536141004562378 and perplexity is 34.33416781517024
At time: 136.05790448188782 and batch: 400, loss is 3.581728343963623 and perplexity is 35.93559627595501
At time: 136.65056777000427 and batch: 450, loss is 3.5365404081344605 and perplexity is 34.34788374335313
At time: 137.24313187599182 and batch: 500, loss is 3.4824796676635743 and perplexity is 32.540311274985896
At time: 137.83530688285828 and batch: 550, loss is 3.5143588972091675 and perplexity is 33.59438356552638
At time: 138.42783284187317 and batch: 600, loss is 3.5996383953094484 and perplexity is 36.58500274290396
At time: 139.02041149139404 and batch: 650, loss is 3.4771001672744752 and perplexity is 32.365730657087525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.358032525754442 and perplexity of 78.10331689220934
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 140.14322566986084 and batch: 50, loss is 3.64182596206665 and perplexity is 38.16145451468689
At time: 140.73817110061646 and batch: 100, loss is 3.6171399259567263 and perplexity is 37.230932179094246
At time: 141.33510446548462 and batch: 150, loss is 3.565986042022705 and perplexity is 35.37431677594575
At time: 141.9344277381897 and batch: 200, loss is 3.508954782485962 and perplexity is 33.413325333054885
At time: 142.54742050170898 and batch: 250, loss is 3.456293888092041 and perplexity is 31.69927747675334
At time: 143.1413254737854 and batch: 300, loss is 3.570330901145935 and perplexity is 35.528347577671326
At time: 143.74148559570312 and batch: 350, loss is 3.5337897062301638 and perplexity is 34.25353277931107
At time: 144.34132528305054 and batch: 400, loss is 3.57799533367157 and perplexity is 35.80169840176997
At time: 144.94211554527283 and batch: 450, loss is 3.5316176319122317 and perplexity is 34.179212304479776
At time: 145.54242277145386 and batch: 500, loss is 3.475998783111572 and perplexity is 32.33010317729351
At time: 146.1422824859619 and batch: 550, loss is 3.5068094825744627 and perplexity is 33.34172056349018
At time: 146.73610043525696 and batch: 600, loss is 3.5915889739990234 and perplexity is 36.29169669783243
At time: 147.33573722839355 and batch: 650, loss is 3.4692592287063597 and perplexity is 32.11294528369407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357749789368873 and perplexity of 78.08123736418028
Finished 18 epochs...
Completing Train Step...
At time: 148.45462584495544 and batch: 50, loss is 3.6402161645889284 and perplexity is 38.10007172165151
At time: 149.05464148521423 and batch: 100, loss is 3.6153605604171752 and perplexity is 37.16474364563071
At time: 149.63736820220947 and batch: 150, loss is 3.5639093685150147 and perplexity is 35.30093209383017
At time: 150.2249150276184 and batch: 200, loss is 3.5072756958007814 and perplexity is 33.35726853865931
At time: 150.8150815963745 and batch: 250, loss is 3.4547568368911743 and perplexity is 31.650591490253262
At time: 151.40764021873474 and batch: 300, loss is 3.568958921432495 and perplexity is 35.47963682826352
At time: 152.0000879764557 and batch: 350, loss is 3.532614302635193 and perplexity is 34.21329470636228
At time: 152.59302854537964 and batch: 400, loss is 3.577202310562134 and perplexity is 35.773318082192276
At time: 153.18523287773132 and batch: 450, loss is 3.5313475942611694 and perplexity is 34.169983876341526
At time: 153.77814412117004 and batch: 500, loss is 3.47618586063385 and perplexity is 32.33615197867065
At time: 154.37133884429932 and batch: 550, loss is 3.5076283025741577 and perplexity is 33.369032611411356
At time: 154.96769404411316 and batch: 600, loss is 3.5925561237335204 and perplexity is 36.32681318136978
At time: 155.56543064117432 and batch: 650, loss is 3.470094709396362 and perplexity is 32.13978624036689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357546638039982 and perplexity of 78.06537666816361
Finished 19 epochs...
Completing Train Step...
At time: 156.68994235992432 and batch: 50, loss is 3.639197030067444 and perplexity is 38.061262402606395
At time: 157.27843046188354 and batch: 100, loss is 3.614159550666809 and perplexity is 37.12013521908208
At time: 157.88078999519348 and batch: 150, loss is 3.5626552629470827 and perplexity is 35.25668874705064
At time: 158.46931052207947 and batch: 200, loss is 3.506209659576416 and perplexity is 33.32172742947198
At time: 159.0583951473236 and batch: 250, loss is 3.453796043395996 and perplexity is 31.62019641186589
At time: 159.64805936813354 and batch: 300, loss is 3.5681268215179442 and perplexity is 35.4501265049619
At time: 160.23709106445312 and batch: 350, loss is 3.5318970823287965 and perplexity is 34.188765034291116
At time: 160.8279275894165 and batch: 400, loss is 3.576711163520813 and perplexity is 35.75575243686844
At time: 161.41901659965515 and batch: 450, loss is 3.531156396865845 and perplexity is 34.16345128895283
At time: 162.0102550983429 and batch: 500, loss is 3.476286849975586 and perplexity is 32.339417750274535
At time: 162.6014211177826 and batch: 550, loss is 3.5080798959732054 and perplexity is 33.384105249367025
At time: 163.19302082061768 and batch: 600, loss is 3.5931082010269164 and perplexity is 36.34687392710023
At time: 163.7839949131012 and batch: 650, loss is 3.4705473709106447 and perplexity is 32.15433797793248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357426362879136 and perplexity of 78.05598790705703
Finished 20 epochs...
Completing Train Step...
At time: 164.88905835151672 and batch: 50, loss is 3.6383648824691774 and perplexity is 38.02960298899051
At time: 165.49354600906372 and batch: 100, loss is 3.613209342956543 and perplexity is 37.0848801328708
At time: 166.08532524108887 and batch: 150, loss is 3.561714735031128 and perplexity is 35.223544436078036
At time: 166.68029499053955 and batch: 200, loss is 3.5053830432891844 and perplexity is 33.29419452800024
At time: 167.2752697467804 and batch: 250, loss is 3.4530626344680786 and perplexity is 31.59701437950878
At time: 167.86860704421997 and batch: 300, loss is 3.5674913454055788 and perplexity is 35.42760595278201
At time: 168.45971488952637 and batch: 350, loss is 3.531347851753235 and perplexity is 34.16999267484238
At time: 169.05823230743408 and batch: 400, loss is 3.5763159132003786 and perplexity is 35.741622756824626
At time: 169.65681314468384 and batch: 450, loss is 3.530967884063721 and perplexity is 34.15701164801657
At time: 170.25638508796692 and batch: 500, loss is 3.476295666694641 and perplexity is 32.3397028790922
At time: 170.85630559921265 and batch: 550, loss is 3.508324189186096 and perplexity is 33.39226175594691
At time: 171.4545955657959 and batch: 600, loss is 3.5934278202056884 and perplexity is 36.3584929418268
At time: 172.04825949668884 and batch: 650, loss is 3.4707908391952516 and perplexity is 32.1621674925232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357366225298713 and perplexity of 78.05129394994981
Finished 21 epochs...
Completing Train Step...
At time: 173.1898331642151 and batch: 50, loss is 3.637624969482422 and perplexity is 38.0014747993475
At time: 173.78732466697693 and batch: 100, loss is 3.6123971462249758 and perplexity is 37.054772142896944
At time: 174.38507795333862 and batch: 150, loss is 3.560928153991699 and perplexity is 35.19584915760155
At time: 174.98272347450256 and batch: 200, loss is 3.504675703048706 and perplexity is 33.270652531518564
At time: 175.57457065582275 and batch: 250, loss is 3.4524409770965576 and perplexity is 31.577377966794447
At time: 176.17201709747314 and batch: 300, loss is 3.5669460010528566 and perplexity is 35.408290975080895
At time: 176.76957654953003 and batch: 350, loss is 3.530876727104187 and perplexity is 34.153898140599075
At time: 177.3664526939392 and batch: 400, loss is 3.575959424972534 and perplexity is 35.728883559889546
At time: 177.96420907974243 and batch: 450, loss is 3.530768103599548 and perplexity is 34.15018842597071
At time: 178.56149792671204 and batch: 500, loss is 3.4762408208847044 and perplexity is 32.337929230533724
At time: 179.1538302898407 and batch: 550, loss is 3.5084472465515137 and perplexity is 33.396371172545734
At time: 179.75275945663452 and batch: 600, loss is 3.5936121559143066 and perplexity is 36.36519572814998
At time: 180.35232067108154 and batch: 650, loss is 3.4709158754348755 and perplexity is 32.16618918042775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357342888327206 and perplexity of 78.04947249038055
Finished 22 epochs...
Completing Train Step...
At time: 181.47105026245117 and batch: 50, loss is 3.6369401836395263 and perplexity is 37.97546083540942
At time: 182.07026076316833 and batch: 100, loss is 3.61166934967041 and perplexity is 37.02781361875158
At time: 182.65592193603516 and batch: 150, loss is 3.560228705406189 and perplexity is 35.17124007808866
At time: 183.24130296707153 and batch: 200, loss is 3.504036965370178 and perplexity is 33.24940809768993
At time: 183.82670783996582 and batch: 250, loss is 3.4518815374374388 and perplexity is 31.55971726973662
At time: 184.41505360603333 and batch: 300, loss is 3.5664490461349487 and perplexity is 35.390699022311885
At time: 185.00556802749634 and batch: 350, loss is 3.5304479598999023 and perplexity is 34.13925720817803
At time: 185.59606766700745 and batch: 400, loss is 3.5756218671798705 and perplexity is 35.71682503216024
At time: 186.18626856803894 and batch: 450, loss is 3.53055597782135 and perplexity is 34.14294505895484
At time: 186.7765896320343 and batch: 500, loss is 3.476143617630005 and perplexity is 32.334786031329344
At time: 187.37502479553223 and batch: 550, loss is 3.5084954833984376 and perplexity is 33.397982147043656
At time: 187.9883098602295 and batch: 600, loss is 3.593713626861572 and perplexity is 36.368885926228785
At time: 188.58806490898132 and batch: 650, loss is 3.4709698295593263 and perplexity is 32.16792472582139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357343187519148 and perplexity of 78.04949584215731
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 189.71805143356323 and batch: 50, loss is 3.636760015487671 and perplexity is 37.96861948313025
At time: 190.31683444976807 and batch: 100, loss is 3.6114829111099245 and perplexity is 37.02091084997371
At time: 190.91634130477905 and batch: 150, loss is 3.5599867248535157 and perplexity is 35.16273035161204
At time: 191.51517629623413 and batch: 200, loss is 3.503536248207092 and perplexity is 33.232763715804566
At time: 192.11411905288696 and batch: 250, loss is 3.4513560342788696 and perplexity is 31.543136895528892
At time: 192.71323108673096 and batch: 300, loss is 3.5659240818023683 and perplexity is 35.37212504338487
At time: 193.3127200603485 and batch: 350, loss is 3.529680666923523 and perplexity is 34.113072442881304
At time: 193.91184997558594 and batch: 400, loss is 3.5745579528808595 and perplexity is 35.67884559830707
At time: 194.5108904838562 and batch: 450, loss is 3.5290966033935547 and perplexity is 34.09315405873936
At time: 195.11116123199463 and batch: 500, loss is 3.4742504215240477 and perplexity is 32.27362785080936
At time: 195.71029376983643 and batch: 550, loss is 3.5061512184143067 and perplexity is 33.31978012589941
At time: 196.30970525741577 and batch: 600, loss is 3.591394543647766 and perplexity is 36.28464117642173
At time: 196.90812397003174 and batch: 650, loss is 3.46891263961792 and perplexity is 32.10181721580592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.35732463761872 and perplexity of 78.04804804520926
Finished 24 epochs...
Completing Train Step...
At time: 198.01616549491882 and batch: 50, loss is 3.63653564453125 and perplexity is 37.96010138330568
At time: 198.62088871002197 and batch: 100, loss is 3.611247410774231 and perplexity is 37.0121934395577
At time: 199.21192955970764 and batch: 150, loss is 3.559737243652344 and perplexity is 35.153959005595894
At time: 199.80313920974731 and batch: 200, loss is 3.5033266830444334 and perplexity is 33.22580001597079
At time: 200.39321184158325 and batch: 250, loss is 3.4511752939224243 and perplexity is 31.537436292902985
At time: 200.9833583831787 and batch: 300, loss is 3.565756878852844 and perplexity is 35.366211214165304
At time: 201.5739004611969 and batch: 350, loss is 3.5295385217666624 and perplexity is 34.10822377946309
At time: 202.16438484191895 and batch: 400, loss is 3.5744540071487427 and perplexity is 35.67513712732348
At time: 202.75495672225952 and batch: 450, loss is 3.5290742254257204 and perplexity is 34.09239113177087
At time: 203.35954022407532 and batch: 500, loss is 3.4742726135253905 and perplexity is 32.27434407514916
At time: 203.95004439353943 and batch: 550, loss is 3.506274447441101 and perplexity is 33.323886342974696
At time: 204.54036402702332 and batch: 600, loss is 3.5915120315551756 and perplexity is 36.28890443342031
At time: 205.13113975524902 and batch: 650, loss is 3.4689959287643433 and perplexity is 32.10449106011003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.35730728448606 and perplexity of 78.04669367882899
Finished 25 epochs...
Completing Train Step...
At time: 206.24230694770813 and batch: 50, loss is 3.636328449249268 and perplexity is 37.95223704415061
At time: 206.82491779327393 and batch: 100, loss is 3.6110277938842774 and perplexity is 37.00406582925705
At time: 207.40781784057617 and batch: 150, loss is 3.559513416290283 and perplexity is 35.1460914682036
At time: 207.99018335342407 and batch: 200, loss is 3.503131875991821 and perplexity is 33.219328026214086
At time: 208.57793283462524 and batch: 250, loss is 3.4510073471069336 and perplexity is 31.53214012565851
At time: 209.16731119155884 and batch: 300, loss is 3.5656055212020874 and perplexity is 35.36085867260403
At time: 209.7581148147583 and batch: 350, loss is 3.529411110877991 and perplexity is 34.1038782971971
At time: 210.34844970703125 and batch: 400, loss is 3.574361810684204 and perplexity is 35.67184815742643
At time: 210.9395501613617 and batch: 450, loss is 3.529049153327942 and perplexity is 34.09153637472224
At time: 211.53217267990112 and batch: 500, loss is 3.4742889070510863 and perplexity is 32.274869942287765
At time: 212.12497663497925 and batch: 550, loss is 3.5063723802566527 and perplexity is 33.32715000479658
At time: 212.71776032447815 and batch: 600, loss is 3.5916117286682128 and perplexity is 36.29252251278064
At time: 213.3108093738556 and batch: 650, loss is 3.4690650844573976 and perplexity is 32.10671134521124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357290230545343 and perplexity of 78.04536268649116
Finished 26 epochs...
Completing Train Step...
At time: 214.415513753891 and batch: 50, loss is 3.636133909225464 and perplexity is 37.94485453317284
At time: 215.01626181602478 and batch: 100, loss is 3.610820059776306 and perplexity is 36.99637962102214
At time: 215.60248851776123 and batch: 150, loss is 3.559305844306946 and perplexity is 35.13879688139311
At time: 216.18864250183105 and batch: 200, loss is 3.502948799133301 and perplexity is 33.213246892671286
At time: 216.77406311035156 and batch: 250, loss is 3.4508493328094483 and perplexity is 31.52715799032302
At time: 217.36014461517334 and batch: 300, loss is 3.56546462059021 and perplexity is 35.355876656973315
At time: 217.94903254508972 and batch: 350, loss is 3.5292932844161986 and perplexity is 34.099860194608
At time: 218.556458234787 and batch: 400, loss is 3.57427622795105 and perplexity is 35.66879539379825
At time: 219.14505219459534 and batch: 450, loss is 3.5290206384658815 and perplexity is 34.09056427312482
At time: 219.7399399280548 and batch: 500, loss is 3.474299726486206 and perplexity is 32.27521914003816
At time: 220.3385043144226 and batch: 550, loss is 3.5064523315429685 and perplexity is 33.32981465982858
At time: 220.93067693710327 and batch: 600, loss is 3.591696147918701 and perplexity is 36.29558642965447
At time: 221.52207779884338 and batch: 650, loss is 3.4691224813461305 and perplexity is 32.10855422343713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357276766907935 and perplexity of 78.0443119191002
Finished 27 epochs...
Completing Train Step...
At time: 222.637193441391 and batch: 50, loss is 3.6359487915039064 and perplexity is 37.93783091827468
At time: 223.22302532196045 and batch: 100, loss is 3.6106222963333128 and perplexity is 36.98906381303353
At time: 223.80873584747314 and batch: 150, loss is 3.5591106605529785 and perplexity is 35.131939028400424
At time: 224.39399075508118 and batch: 200, loss is 3.5027751970291137 and perplexity is 33.20748150358015
At time: 224.9804949760437 and batch: 250, loss is 3.4506995487213135 and perplexity is 31.5224360773545
At time: 225.5663001537323 and batch: 300, loss is 3.565331473350525 and perplexity is 35.3511694329737
At time: 226.1541337966919 and batch: 350, loss is 3.5291821908950807 and perplexity is 34.09607213148788
At time: 226.74452471733093 and batch: 400, loss is 3.5741947650909425 and perplexity is 35.66588983005826
At time: 227.33590459823608 and batch: 450, loss is 3.5289892292022706 and perplexity is 34.08949353042063
At time: 227.926411151886 and batch: 500, loss is 3.474305443763733 and perplexity is 32.27540366695072
At time: 228.52205657958984 and batch: 550, loss is 3.506518216133118 and perplexity is 33.33201065334747
At time: 229.11830878257751 and batch: 600, loss is 3.5917678022384645 and perplexity is 36.298187258389696
At time: 229.71767401695251 and batch: 650, loss is 3.4691702127456665 and perplexity is 32.110086846244116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357265696806066 and perplexity of 78.043447965399
Finished 28 epochs...
Completing Train Step...
At time: 230.88858437538147 and batch: 50, loss is 3.63577118396759 and perplexity is 37.931093471920434
At time: 231.51858282089233 and batch: 100, loss is 3.61043270111084 and perplexity is 36.98205152801971
At time: 232.13106727600098 and batch: 150, loss is 3.5589254570007323 and perplexity is 35.125433070976875
At time: 232.74274921417236 and batch: 200, loss is 3.5026094579696654 and perplexity is 33.201978182899296
At time: 233.33100390434265 and batch: 250, loss is 3.450556263923645 and perplexity is 31.517919715049892
At time: 233.93296790122986 and batch: 300, loss is 3.56520450592041 and perplexity is 35.346681270770475
At time: 234.52091360092163 and batch: 350, loss is 3.529076352119446 and perplexity is 34.09246363592228
At time: 235.10995721817017 and batch: 400, loss is 3.5741161584854124 and perplexity is 35.66308636571236
At time: 235.6982889175415 and batch: 450, loss is 3.5289553117752077 and perplexity is 34.088337322118115
At time: 236.2870557308197 and batch: 500, loss is 3.4743062543869017 and perplexity is 32.27542983015132
At time: 236.8769543170929 and batch: 550, loss is 3.5065723705291747 and perplexity is 33.333815777131015
At time: 237.46773409843445 and batch: 600, loss is 3.5918288326263426 and perplexity is 36.300402618438795
At time: 238.05788898468018 and batch: 650, loss is 3.469209632873535 and perplexity is 32.11135265492248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3572558234719665 and perplexity of 78.04267742016688
Finished 29 epochs...
Completing Train Step...
At time: 239.17457032203674 and batch: 50, loss is 3.635600004196167 and perplexity is 37.92460099171658
At time: 239.76342344284058 and batch: 100, loss is 3.6102502870559694 and perplexity is 36.975306097292446
At time: 240.35230422019958 and batch: 150, loss is 3.5587484216690064 and perplexity is 35.11921517869069
At time: 240.94205927848816 and batch: 200, loss is 3.502450089454651 and perplexity is 33.19668725455564
At time: 241.53167605400085 and batch: 250, loss is 3.4504184532165527 and perplexity is 31.51357650752501
At time: 242.1205403804779 and batch: 300, loss is 3.5650826930999755 and perplexity is 35.34237585406468
At time: 242.718496799469 and batch: 350, loss is 3.5289745044708254 and perplexity is 34.08899157547887
At time: 243.3095886707306 and batch: 400, loss is 3.57403977394104 and perplexity is 35.66036236114665
At time: 243.90100193023682 and batch: 450, loss is 3.528918995857239 and perplexity is 34.08709939533459
At time: 244.49213886260986 and batch: 500, loss is 3.474302792549133 and perplexity is 32.27531809804274
At time: 245.08389973640442 and batch: 550, loss is 3.5066168451309205 and perplexity is 33.33529831827984
At time: 245.67520427703857 and batch: 600, loss is 3.591880717277527 and perplexity is 36.302286101028
At time: 246.26678776741028 and batch: 650, loss is 3.469242100715637 and perplexity is 32.1123952581756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357248343673407 and perplexity of 78.04209367884388
Finished 30 epochs...
Completing Train Step...
At time: 247.37347030639648 and batch: 50, loss is 3.635433974266052 and perplexity is 37.91830489554896
At time: 247.9780626296997 and batch: 100, loss is 3.6100740146636965 and perplexity is 36.96878894604545
At time: 248.56943941116333 and batch: 150, loss is 3.558578119277954 and perplexity is 35.113234801624586
At time: 249.1740620136261 and batch: 200, loss is 3.5022960710525513 and perplexity is 33.19157474754988
At time: 249.7658143043518 and batch: 250, loss is 3.4502851819992064 and perplexity is 31.509376934668225
At time: 250.3626194000244 and batch: 300, loss is 3.56496452331543 and perplexity is 35.33819969987716
At time: 250.95947074890137 and batch: 350, loss is 3.528875699043274 and perplexity is 34.08562356448309
At time: 251.558034658432 and batch: 400, loss is 3.5739648723602295 and perplexity is 35.65769144366275
At time: 252.159814119339 and batch: 450, loss is 3.5288807773590087 and perplexity is 34.08579666248109
At time: 252.7645354270935 and batch: 500, loss is 3.4742955207824706 and perplexity is 32.2750834003139
At time: 253.3725974559784 and batch: 550, loss is 3.5066533946990965 and perplexity is 33.33651673130449
At time: 253.98078727722168 and batch: 600, loss is 3.5919248962402346 and perplexity is 36.30388993379944
At time: 254.58565211296082 and batch: 650, loss is 3.4692685651779174 and perplexity is 32.11324510669398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3572429582184435 and perplexity of 78.04167338779483
Finished 31 epochs...
Completing Train Step...
At time: 255.73472571372986 and batch: 50, loss is 3.6352724123001097 and perplexity is 37.912179234515065
At time: 256.3397340774536 and batch: 100, loss is 3.60990327835083 and perplexity is 36.96247757013552
At time: 256.9440383911133 and batch: 150, loss is 3.5584136915206908 and perplexity is 35.107461685819146
At time: 257.54777240753174 and batch: 200, loss is 3.50214663028717 and perplexity is 33.186614943823066
At time: 258.149600982666 and batch: 250, loss is 3.450155668258667 and perplexity is 31.50529630165408
At time: 258.7498071193695 and batch: 300, loss is 3.5648497343063354 and perplexity is 35.3341434957587
At time: 259.3547511100769 and batch: 350, loss is 3.528779411315918 and perplexity is 34.08234169525895
At time: 259.9587998390198 and batch: 400, loss is 3.5738911533355715 and perplexity is 35.65506289031635
At time: 260.56399488449097 and batch: 450, loss is 3.5288408851623534 and perplexity is 34.08443693229896
At time: 261.1687693595886 and batch: 500, loss is 3.474284496307373 and perplexity is 32.27472758642201
At time: 261.771249294281 and batch: 550, loss is 3.506683030128479 and perplexity is 33.33750468793113
At time: 262.37141585350037 and batch: 600, loss is 3.591962432861328 and perplexity is 36.30525268473648
At time: 262.9759166240692 and batch: 650, loss is 3.4692899894714357 and perplexity is 32.113933117653026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357237273571538 and perplexity of 78.0412297496987
Finished 32 epochs...
Completing Train Step...
At time: 264.10367608070374 and batch: 50, loss is 3.6351147270202637 and perplexity is 37.90620151323461
At time: 264.71957325935364 and batch: 100, loss is 3.6097371625900267 and perplexity is 36.95633803000409
At time: 265.31814646720886 and batch: 150, loss is 3.558254246711731 and perplexity is 35.10186442953617
At time: 265.9129710197449 and batch: 200, loss is 3.5020010471343994 and perplexity is 33.181783883458785
At time: 266.51504278182983 and batch: 250, loss is 3.4500295639038088 and perplexity is 31.50132359708228
At time: 267.11605286598206 and batch: 300, loss is 3.5647378969192505 and perplexity is 35.330192038439705
At time: 267.7164249420166 and batch: 350, loss is 3.528685245513916 and perplexity is 34.07913245532181
At time: 268.3127760887146 and batch: 400, loss is 3.57381826877594 and perplexity is 35.65246428145933
At time: 268.90440249443054 and batch: 450, loss is 3.528799514770508 and perplexity is 34.083026874954754
At time: 269.4954297542572 and batch: 500, loss is 3.4742703819274903 and perplexity is 32.27427205187105
At time: 270.08635544776917 and batch: 550, loss is 3.5067068338394165 and perplexity is 33.33829825370096
At time: 270.67689085006714 and batch: 600, loss is 3.59199435710907 and perplexity is 36.3064117211181
At time: 271.26763582229614 and batch: 650, loss is 3.4693069410324098 and perplexity is 32.11447750356247
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3572336832682295 and perplexity of 78.04094955851629
Finished 33 epochs...
Completing Train Step...
At time: 272.38412976264954 and batch: 50, loss is 3.634960331916809 and perplexity is 37.90034943310829
At time: 272.9654817581177 and batch: 100, loss is 3.609575228691101 and perplexity is 36.95035403061613
At time: 273.5475628376007 and batch: 150, loss is 3.558099136352539 and perplexity is 35.09642018897617
At time: 274.1294274330139 and batch: 200, loss is 3.50185911655426 and perplexity is 33.177074707818065
At time: 274.7120578289032 and batch: 250, loss is 3.4499062061309815 and perplexity is 31.497437903632406
At time: 275.2955114841461 and batch: 300, loss is 3.564628281593323 and perplexity is 35.32631952017188
At time: 275.88093280792236 and batch: 350, loss is 3.528592882156372 and perplexity is 34.07598493758594
At time: 276.4674882888794 and batch: 400, loss is 3.573746151924133 and perplexity is 35.64989323068536
At time: 277.0566051006317 and batch: 450, loss is 3.528756604194641 and perplexity is 34.08156438402265
At time: 277.64525628089905 and batch: 500, loss is 3.474253659248352 and perplexity is 32.2737323440878
At time: 278.2344272136688 and batch: 550, loss is 3.506725368499756 and perplexity is 33.33891617346184
At time: 278.8233554363251 and batch: 600, loss is 3.592021164894104 and perplexity is 36.30738502864493
At time: 279.42960691452026 and batch: 650, loss is 3.469320044517517 and perplexity is 32.11489831789722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357233384076287 and perplexity of 78.0409262092965
Finished 34 epochs...
Completing Train Step...
At time: 280.53407168388367 and batch: 50, loss is 3.6348088598251342 and perplexity is 37.89460902267142
At time: 281.1362979412079 and batch: 100, loss is 3.609417085647583 and perplexity is 36.94451105119611
At time: 281.72723484039307 and batch: 150, loss is 3.5579478263854982 and perplexity is 35.09111015253498
At time: 282.31625843048096 and batch: 200, loss is 3.5017200326919555 and perplexity is 33.17246063300685
At time: 282.90480279922485 and batch: 250, loss is 3.449785342216492 and perplexity is 31.493631230040346
At time: 283.4929711818695 and batch: 300, loss is 3.5645207357406616 and perplexity is 35.322520525304434
At time: 284.0818634033203 and batch: 350, loss is 3.528501868247986 and perplexity is 34.072883690145126
At time: 284.67068815231323 and batch: 400, loss is 3.5736745834350585 and perplexity is 35.64734191298923
At time: 285.25988960266113 and batch: 450, loss is 3.528712592124939 and perplexity is 34.080064416843996
At time: 285.8483338356018 and batch: 500, loss is 3.474234256744385 and perplexity is 32.27310615894276
At time: 286.4365828037262 and batch: 550, loss is 3.5067396926879884 and perplexity is 33.33939372979287
At time: 287.0262975692749 and batch: 600, loss is 3.59204363822937 and perplexity is 36.308200985849915
At time: 287.6171672344208 and batch: 650, loss is 3.469329833984375 and perplexity is 32.1152127071688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357232785692402 and perplexity of 78.04087951087782
Finished 35 epochs...
Completing Train Step...
At time: 288.7422664165497 and batch: 50, loss is 3.6346598958969114 and perplexity is 37.888964513277514
At time: 289.33123302459717 and batch: 100, loss is 3.609262366294861 and perplexity is 36.938795462527125
At time: 289.9193880558014 and batch: 150, loss is 3.557799906730652 and perplexity is 35.0859198715148
At time: 290.5073583126068 and batch: 200, loss is 3.5015835905075074 and perplexity is 33.167934818777574
At time: 291.09707260131836 and batch: 250, loss is 3.449666700363159 and perplexity is 31.489894988904705
At time: 291.68738436698914 and batch: 300, loss is 3.5644149017333984 and perplexity is 35.31878239922452
At time: 292.2782213687897 and batch: 350, loss is 3.5284122705459593 and perplexity is 34.06983097482525
At time: 292.8685073852539 and batch: 400, loss is 3.5736035394668577 and perplexity is 35.64480947432224
At time: 293.45840287208557 and batch: 450, loss is 3.528667440414429 and perplexity is 34.07852567837988
At time: 294.0484080314636 and batch: 500, loss is 3.47421266078949 and perplexity is 32.27240919792362
At time: 294.6526005268097 and batch: 550, loss is 3.506750111579895 and perplexity is 33.339741091141924
At time: 295.2431449890137 and batch: 600, loss is 3.592062306404114 and perplexity is 36.30887880001731
At time: 295.8337173461914 and batch: 650, loss is 3.46933669090271 and perplexity is 32.115432919314635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357232187308517 and perplexity of 78.04083281248714
Finished 36 epochs...
Completing Train Step...
At time: 296.9360992908478 and batch: 50, loss is 3.634513249397278 and perplexity is 37.883408636641796
At time: 297.53832745552063 and batch: 100, loss is 3.609110703468323 and perplexity is 36.933193645202834
At time: 298.1259286403656 and batch: 150, loss is 3.557654871940613 and perplexity is 35.080831561492815
At time: 298.7145583629608 and batch: 200, loss is 3.5014496564865114 and perplexity is 33.163492801374424
At time: 299.3030047416687 and batch: 250, loss is 3.449549984931946 and perplexity is 31.486219846709293
At time: 299.8916871547699 and batch: 300, loss is 3.564310598373413 and perplexity is 35.31509872366289
At time: 300.47983169555664 and batch: 350, loss is 3.528323850631714 and perplexity is 34.06681865646853
At time: 301.07036542892456 and batch: 400, loss is 3.573532919883728 and perplexity is 35.642292341616894
At time: 301.6582796573639 and batch: 450, loss is 3.528621220588684 and perplexity is 34.07695061126133
At time: 302.24625182151794 and batch: 500, loss is 3.474189019203186 and perplexity is 32.27164623599519
At time: 302.8342890739441 and batch: 550, loss is 3.5067571592330933 and perplexity is 33.33997605890284
At time: 303.4245114326477 and batch: 600, loss is 3.592077512741089 and perplexity is 36.309430929261445
At time: 304.0173439979553 and batch: 650, loss is 3.469340834617615 and perplexity is 32.11556599678842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3572345808440565 and perplexity of 78.04101960621756
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 305.14124870300293 and batch: 50, loss is 3.634474081993103 and perplexity is 37.881924870922006
At time: 305.73510789871216 and batch: 100, loss is 3.6090668725967405 and perplexity is 36.93157486661154
At time: 306.3290944099426 and batch: 150, loss is 3.557584114074707 and perplexity is 35.078349404534414
At time: 306.9228343963623 and batch: 200, loss is 3.5013216638565066 and perplexity is 33.159248390343095
At time: 307.5168447494507 and batch: 250, loss is 3.449403176307678 and perplexity is 31.481597737381257
At time: 308.1110463142395 and batch: 300, loss is 3.5641655254364015 and perplexity is 35.309975830175794
At time: 308.7048680782318 and batch: 350, loss is 3.528132982254028 and perplexity is 34.06031699855885
At time: 309.29886960983276 and batch: 400, loss is 3.5732717180252074 and perplexity is 35.63298372438231
At time: 309.9067416191101 and batch: 450, loss is 3.5282697439193726 and perplexity is 34.06497546277613
At time: 310.50103068351746 and batch: 500, loss is 3.473732600212097 and perplexity is 32.256920204655195
At time: 311.09524273872375 and batch: 550, loss is 3.5061785411834716 and perplexity is 33.3206905269977
At time: 311.68920850753784 and batch: 600, loss is 3.5915263509750366 and perplexity is 36.289424073199655
At time: 312.283264875412 and batch: 650, loss is 3.468859519958496 and perplexity is 32.100112023498546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357233982460172 and perplexity of 78.04097290774305
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 313.3851215839386 and batch: 50, loss is 3.6344610166549685 and perplexity is 37.88142993399765
At time: 313.98719239234924 and batch: 100, loss is 3.6090519428253174 and perplexity is 36.93102349075645
At time: 314.5749671459198 and batch: 150, loss is 3.5575622940063476 and perplexity is 35.07758400090309
At time: 315.16282081604004 and batch: 200, loss is 3.501290326118469 and perplexity is 33.15820927078543
At time: 315.7499759197235 and batch: 250, loss is 3.4493676662445067 and perplexity is 31.480479843705226
At time: 316.33801436424255 and batch: 300, loss is 3.564130392074585 and perplexity is 35.308735293811445
At time: 316.92688512802124 and batch: 350, loss is 3.5280878591537475 and perplexity is 34.05878012613381
At time: 317.51772594451904 and batch: 400, loss is 3.5732121229171754 and perplexity is 35.63086023614314
At time: 318.1098988056183 and batch: 450, loss is 3.528191690444946 and perplexity is 34.062316676850095
At time: 318.70255994796753 and batch: 500, loss is 3.4736313438415527 and perplexity is 32.25365415134748
At time: 319.2952437400818 and batch: 550, loss is 3.506049518585205 and perplexity is 33.31639168225989
At time: 319.8880650997162 and batch: 600, loss is 3.5914028024673463 and perplexity is 36.284940845964194
At time: 320.48312854766846 and batch: 650, loss is 3.468751139640808 and perplexity is 32.09663319168148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357234880035999 and perplexity of 78.04104295546529
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 321.6067638397217 and batch: 50, loss is 3.634457745552063 and perplexity is 37.8813060201448
At time: 322.2008090019226 and batch: 100, loss is 3.6090485000610353 and perplexity is 36.93089634616674
At time: 322.7946584224701 and batch: 150, loss is 3.557557291984558 and perplexity is 35.077408542502404
At time: 323.3885841369629 and batch: 200, loss is 3.5012832498550415 and perplexity is 33.157974635392016
At time: 323.98382019996643 and batch: 250, loss is 3.449359431266785 and perplexity is 31.480220603722454
At time: 324.5777850151062 and batch: 300, loss is 3.5641223859786986 and perplexity is 35.30845260982266
At time: 325.185747385025 and batch: 350, loss is 3.528077630996704 and perplexity is 34.058431769363494
At time: 325.77934169769287 and batch: 400, loss is 3.573198504447937 and perplexity is 35.63037500167316
At time: 326.37366461753845 and batch: 450, loss is 3.5281738662719726 and perplexity is 34.061709549636554
At time: 326.9677164554596 and batch: 500, loss is 3.4736080741882325 and perplexity is 32.25290362872931
At time: 327.56167364120483 and batch: 550, loss is 3.506019892692566 and perplexity is 33.315404669037434
At time: 328.1556305885315 and batch: 600, loss is 3.5913744878768923 and perplexity is 36.283913467269464
At time: 328.7500972747803 and batch: 650, loss is 3.4687262868881223 and perplexity is 32.09583551190703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.357234880035999 and perplexity of 78.04104295546529
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 329.85722041130066 and batch: 50, loss is 3.634457478523254 and perplexity is 37.881295904746125
At time: 330.46355056762695 and batch: 100, loss is 3.609048104286194 and perplexity is 36.93088172984999
At time: 331.06009101867676 and batch: 150, loss is 3.5575566196441653 and perplexity is 35.077384958551704
At time: 331.6579966545105 and batch: 200, loss is 3.501281933784485 and perplexity is 33.1579309971866
At time: 332.25600838661194 and batch: 250, loss is 3.4493581771850588 and perplexity is 31.48018112497782
At time: 332.8530776500702 and batch: 300, loss is 3.5641210174560545 and perplexity is 35.3084042894388
At time: 333.4482569694519 and batch: 350, loss is 3.5280756044387815 and perplexity is 34.05836274804871
At time: 334.03804636001587 and batch: 400, loss is 3.5731956911087037 and perplexity is 35.630274761482276
At time: 334.62889194488525 and batch: 450, loss is 3.5281701707839965 and perplexity is 34.061583675231056
At time: 335.21996545791626 and batch: 500, loss is 3.4736031818389894 and perplexity is 32.25274583664664
At time: 335.81127738952637 and batch: 550, loss is 3.5060135412216185 and perplexity is 33.31519306788457
At time: 336.40209555625916 and batch: 600, loss is 3.5913684701919557 and perplexity is 36.283695122766915
At time: 336.992960691452 and batch: 650, loss is 3.4687211608886717 and perplexity is 32.0956709890935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3572345808440565 and perplexity of 78.04101960621756
Annealing...
Model not improving. Stopping early with 78.04083281248714loss at 40 epochs.
Finished Training.
Improved accuracyfrom -150.03565027818937 to -78.04083281248714
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f28e0412ac8>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.3609633842882489, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 10.525727233259758, 'seq_len': 20, 'anneal': 6.03673053876303, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.825411319732666 and batch: 50, loss is 6.684907646179199 and perplexity is 800.2367698395788
At time: 1.394047737121582 and batch: 100, loss is 5.859794387817383 and perplexity is 350.6520382763286
At time: 1.9505987167358398 and batch: 150, loss is 5.556024093627929 and perplexity is 258.79185595124954
At time: 2.507622480392456 and batch: 200, loss is 5.441316576004028 and perplexity is 230.74577791843774
At time: 3.0643179416656494 and batch: 250, loss is 5.3621045589447025 and perplexity is 213.1731100663167
At time: 3.6208951473236084 and batch: 300, loss is 5.361488485336304 and perplexity is 213.0418201854525
At time: 4.180853366851807 and batch: 350, loss is 5.279496221542359 and perplexity is 196.27097335330623
At time: 4.73806619644165 and batch: 400, loss is 5.298990421295166 and perplexity is 200.13465625985998
At time: 5.294332504272461 and batch: 450, loss is 5.257553796768189 and perplexity is 192.0112178778558
At time: 5.851537704467773 and batch: 500, loss is 5.249343309402466 and perplexity is 190.44116646307396
At time: 6.408421993255615 and batch: 550, loss is 5.2560930252075195 and perplexity is 191.73093811364433
At time: 6.9652769565582275 and batch: 600, loss is 5.294185361862183 and perplexity is 199.17530405994117
At time: 7.522351264953613 and batch: 650, loss is 5.243987035751343 and perplexity is 189.4238384369415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.116596895105698 and perplexity of 166.76687767326956
Finished 1 epochs...
Completing Train Step...
At time: 8.589753150939941 and batch: 50, loss is 5.0016383934021 and perplexity is 148.6565175472322
At time: 9.139398574829102 and batch: 100, loss is 4.941259031295776 and perplexity is 139.94633550401423
At time: 9.689229965209961 and batch: 150, loss is 4.871368207931519 and perplexity is 130.4993450441681
At time: 10.238819599151611 and batch: 200, loss is 4.820214672088623 and perplexity is 123.99170548139253
At time: 10.787801504135132 and batch: 250, loss is 4.78187313079834 and perplexity is 119.32765714751324
At time: 11.33664321899414 and batch: 300, loss is 4.834255304336548 and perplexity is 125.7449066442579
At time: 11.885745525360107 and batch: 350, loss is 4.77632625579834 and perplexity is 118.66759388586686
At time: 12.435248613357544 and batch: 400, loss is 4.812268371582031 and perplexity is 123.01033442790616
At time: 12.985949754714966 and batch: 450, loss is 4.773051109313965 and perplexity is 118.27957588767589
At time: 13.543913125991821 and batch: 500, loss is 4.743946380615235 and perplexity is 114.88669486329478
At time: 14.11087679862976 and batch: 550, loss is 4.760631294250488 and perplexity is 116.81965019919281
At time: 14.688713073730469 and batch: 600, loss is 4.818058080673218 and perplexity is 123.72459416228418
At time: 15.275651931762695 and batch: 650, loss is 4.72837498664856 and perplexity is 113.11160505419502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.879184797698376 and perplexity of 131.52340199167784
Finished 2 epochs...
Completing Train Step...
At time: 16.39749240875244 and batch: 50, loss is 4.701944608688354 and perplexity is 110.16118469652815
At time: 16.983152627944946 and batch: 100, loss is 4.665960607528686 and perplexity is 106.26761768115497
At time: 17.585215091705322 and batch: 150, loss is 4.620200834274292 and perplexity is 101.51441765682108
At time: 18.173165321350098 and batch: 200, loss is 4.569511604309082 and perplexity is 96.49696955812401
At time: 18.7613422870636 and batch: 250, loss is 4.548168315887451 and perplexity is 94.4592302882201
At time: 19.349379062652588 and batch: 300, loss is 4.622877702713013 and perplexity is 101.78652242937724
At time: 19.937533617019653 and batch: 350, loss is 4.584259529113769 and perplexity is 97.93064549607514
At time: 20.527142763137817 and batch: 400, loss is 4.6145641040802 and perplexity is 100.94381794278927
At time: 21.11577582359314 and batch: 450, loss is 4.596990966796875 and perplexity is 99.1854139610465
At time: 21.703792572021484 and batch: 500, loss is 4.571616153717041 and perplexity is 96.70026604699454
At time: 22.29607915878296 and batch: 550, loss is 4.586533813476563 and perplexity is 98.15362109060979
At time: 22.89617896080017 and batch: 600, loss is 4.656206560134888 and perplexity is 105.23611713267628
At time: 23.50238013267517 and batch: 650, loss is 4.577014408111572 and perplexity is 97.22369020040531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.807629753561581 and perplexity of 122.44105782303564
Finished 3 epochs...
Completing Train Step...
At time: 24.652244091033936 and batch: 50, loss is 4.56362473487854 and perplexity is 95.93057328406323
At time: 25.249855995178223 and batch: 100, loss is 4.530882272720337 and perplexity is 92.84043554262274
At time: 25.854973316192627 and batch: 150, loss is 4.486693515777588 and perplexity is 88.82725386264137
At time: 26.45816135406494 and batch: 200, loss is 4.442601718902588 and perplexity is 84.99578940933519
At time: 27.06193709373474 and batch: 250, loss is 4.416217803955078 and perplexity is 82.78259251437872
At time: 27.66520643234253 and batch: 300, loss is 4.507909727096558 and perplexity is 90.73196558830088
At time: 28.267576932907104 and batch: 350, loss is 4.469734945297241 and perplexity is 87.33357177149702
At time: 28.865267992019653 and batch: 400, loss is 4.5085837936401365 and perplexity is 90.79314558811609
At time: 29.469964742660522 and batch: 450, loss is 4.492795076370239 and perplexity is 89.37089557919627
At time: 30.070569276809692 and batch: 500, loss is 4.466443080902099 and perplexity is 87.04655416678443
At time: 30.68016290664673 and batch: 550, loss is 4.473544549942017 and perplexity is 87.66691269758229
At time: 31.27446699142456 and batch: 600, loss is 4.560172643661499 and perplexity is 95.59998313655758
At time: 31.869048595428467 and batch: 650, loss is 4.481578102111817 and perplexity is 88.37402592684586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.788095212450214 and perplexity of 120.07243821511463
Finished 4 epochs...
Completing Train Step...
At time: 32.980130434036255 and batch: 50, loss is 4.460000209808349 and perplexity is 86.48752724213281
At time: 33.589494466781616 and batch: 100, loss is 4.4426412010192875 and perplexity is 84.9991452892598
At time: 34.18773365020752 and batch: 150, loss is 4.405314092636108 and perplexity is 81.88485823542905
At time: 34.78756093978882 and batch: 200, loss is 4.367048320770263 and perplexity is 78.81066424537407
At time: 35.38648724555969 and batch: 250, loss is 4.3316521644592285 and perplexity is 76.06986272800678
At time: 35.98150110244751 and batch: 300, loss is 4.4151566696166995 and perplexity is 82.69479565321213
At time: 36.58460068702698 and batch: 350, loss is 4.3836031341552735 and perplexity is 80.12621942479859
At time: 37.18758749961853 and batch: 400, loss is 4.433668479919434 and perplexity is 84.23988308285558
At time: 37.79056644439697 and batch: 450, loss is 4.423550815582275 and perplexity is 83.39186941489287
At time: 38.393460273742676 and batch: 500, loss is 4.386921691894531 and perplexity is 80.39256460688651
At time: 38.99526000022888 and batch: 550, loss is 4.403274221420288 and perplexity is 81.7179939188225
At time: 39.59219932556152 and batch: 600, loss is 4.487570276260376 and perplexity is 88.90516823975508
At time: 40.195762395858765 and batch: 650, loss is 4.410881748199463 and perplexity is 82.34203644634583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.772528854070925 and perplexity of 118.21781988662542
Finished 5 epochs...
Completing Train Step...
At time: 41.33670258522034 and batch: 50, loss is 4.394062538146972 and perplexity is 80.96869011428682
At time: 41.93363666534424 and batch: 100, loss is 4.38497239112854 and perplexity is 80.23600795669067
At time: 42.53317046165466 and batch: 150, loss is 4.34028660774231 and perplexity is 76.72952746379646
At time: 43.12464904785156 and batch: 200, loss is 4.29404239654541 and perplexity is 73.26202487538961
At time: 43.71388006210327 and batch: 250, loss is 4.273183460235596 and perplexity is 71.7496847021961
At time: 44.302366733551025 and batch: 300, loss is 4.349929504394531 and perplexity is 77.47300122662128
At time: 44.892539978027344 and batch: 350, loss is 4.320364608764648 and perplexity is 75.21604772327895
At time: 45.48316264152527 and batch: 400, loss is 4.378624153137207 and perplexity is 79.72826402753515
At time: 46.08764028549194 and batch: 450, loss is 4.355454721450806 and perplexity is 77.90224110415916
At time: 46.678399324417114 and batch: 500, loss is 4.3303979301452635 and perplexity is 75.97451310379954
At time: 47.26891756057739 and batch: 550, loss is 4.337667112350464 and perplexity is 76.52879784012843
At time: 47.8591206073761 and batch: 600, loss is 4.432588090896607 and perplexity is 84.14892038428708
At time: 48.449750900268555 and batch: 650, loss is 4.3395532131195065 and perplexity is 76.67327507106683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.7628197763480395 and perplexity of 117.0755878676486
Finished 6 epochs...
Completing Train Step...
At time: 49.55218315124512 and batch: 50, loss is 4.32699592590332 and perplexity is 75.716486640361
At time: 50.15271782875061 and batch: 100, loss is 4.333774042129517 and perplexity is 76.23144503943529
At time: 50.739476919174194 and batch: 150, loss is 4.286155300140381 and perplexity is 72.686472922814
At time: 51.32567548751831 and batch: 200, loss is 4.250834159851074 and perplexity is 70.16391586426086
At time: 51.91192960739136 and batch: 250, loss is 4.2194611883163455 and perplexity is 67.99683692710492
At time: 52.498307943344116 and batch: 300, loss is 4.302195014953614 and perplexity is 73.8617435249648
At time: 53.086889028549194 and batch: 350, loss is 4.269372396469116 and perplexity is 71.47676247091607
At time: 53.67562747001648 and batch: 400, loss is 4.331530647277832 and perplexity is 76.06061949431609
At time: 54.26496338844299 and batch: 450, loss is 4.302252855300903 and perplexity is 73.86601583741651
At time: 54.85408616065979 and batch: 500, loss is 4.27698935508728 and perplexity is 72.02327675886956
At time: 55.44284462928772 and batch: 550, loss is 4.2875601673126225 and perplexity is 72.78865952498397
At time: 56.03225803375244 and batch: 600, loss is 4.376843500137329 and perplexity is 79.58642197823325
At time: 56.62050414085388 and batch: 650, loss is 4.288887214660645 and perplexity is 72.8853176433375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.766095329733456 and perplexity of 117.45970395877966
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 57.73613977432251 and batch: 50, loss is 4.247898359298706 and perplexity is 69.95823067438934
At time: 58.32171154022217 and batch: 100, loss is 4.177635865211487 and perplexity is 65.2115020518127
At time: 58.90872001647949 and batch: 150, loss is 4.111263537406922 and perplexity is 61.023774699131344
At time: 59.49698996543884 and batch: 200, loss is 4.0433411788940425 and perplexity is 57.0165273260026
At time: 60.08572006225586 and batch: 250, loss is 3.990095067024231 and perplexity is 54.06002844507822
At time: 60.67369604110718 and batch: 300, loss is 4.0636699008941655 and perplexity is 58.18746193705963
At time: 61.276548624038696 and batch: 350, loss is 4.005630683898926 and perplexity is 54.9064420904861
At time: 61.86496019363403 and batch: 400, loss is 4.045838632583618 and perplexity is 57.15910142453779
At time: 62.4554398059845 and batch: 450, loss is 3.9941378355026247 and perplexity is 54.27902299788449
At time: 63.045737743377686 and batch: 500, loss is 3.9395622634887695 and perplexity is 51.39609842690529
At time: 63.63997554779053 and batch: 550, loss is 3.9202598428726194 and perplexity is 50.413542676041935
At time: 64.23857855796814 and batch: 600, loss is 3.9930177450180055 and perplexity is 54.218259617305165
At time: 64.83534812927246 and batch: 650, loss is 3.94807957649231 and perplexity is 51.83572464370799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.560549866919424 and perplexity of 95.63605247632317
Finished 8 epochs...
Completing Train Step...
At time: 65.9472279548645 and batch: 50, loss is 4.082481951713562 and perplexity is 59.292448371472354
At time: 66.55324625968933 and batch: 100, loss is 4.057136254310608 and perplexity is 57.80852489354994
At time: 67.14504909515381 and batch: 150, loss is 4.007461562156677 and perplexity is 55.00706118406818
At time: 67.73655128479004 and batch: 200, loss is 3.952560124397278 and perplexity is 52.06849817823248
At time: 68.32798147201538 and batch: 250, loss is 3.911520395278931 and perplexity is 49.97487580690536
At time: 68.92004871368408 and batch: 300, loss is 3.9936990404129027 and perplexity is 54.25521085382613
At time: 69.51181197166443 and batch: 350, loss is 3.943652381896973 and perplexity is 51.60674504633883
At time: 70.10372352600098 and batch: 400, loss is 3.993825511932373 and perplexity is 54.26207302670757
At time: 70.69604468345642 and batch: 450, loss is 3.949466552734375 and perplexity is 51.90766944360977
At time: 71.29036331176758 and batch: 500, loss is 3.905156555175781 and perplexity is 49.65785349764699
At time: 71.8821234703064 and batch: 550, loss is 3.9028144693374633 and perplexity is 49.54168663169617
At time: 72.4739420413971 and batch: 600, loss is 3.9914245414733887 and perplexity is 54.13194766840849
At time: 73.06525683403015 and batch: 650, loss is 3.933293652534485 and perplexity is 51.07492399181463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.549076753504136 and perplexity of 94.54507959472274
Finished 9 epochs...
Completing Train Step...
At time: 74.19368720054626 and batch: 50, loss is 4.033346328735352 and perplexity is 56.44949410525186
At time: 74.78155016899109 and batch: 100, loss is 4.0122608995437625 and perplexity is 55.271693150454716
At time: 75.3702347278595 and batch: 150, loss is 3.964879369735718 and perplexity is 52.713910113186124
At time: 75.95975852012634 and batch: 200, loss is 3.9129530811309814 and perplexity is 50.04652541786826
At time: 76.56284070014954 and batch: 250, loss is 3.8744953155517576 and perplexity is 48.158387367785416
At time: 77.15159249305725 and batch: 300, loss is 3.960557179450989 and perplexity is 52.48656223734486
At time: 77.74002647399902 and batch: 350, loss is 3.9120823335647583 and perplexity is 50.00296649482805
At time: 78.32854652404785 and batch: 400, loss is 3.9667494392395017 and perplexity is 52.812581020886036
At time: 78.91883087158203 and batch: 450, loss is 3.92606276512146 and perplexity is 50.706938999189035
At time: 79.50937557220459 and batch: 500, loss is 3.8857713460922243 and perplexity is 48.70449599690227
At time: 80.09987950325012 and batch: 550, loss is 3.8897706699371337 and perplexity is 48.899671073119706
At time: 80.69085574150085 and batch: 600, loss is 3.9845957803726195 and perplexity is 53.76355280170998
At time: 81.28134155273438 and batch: 650, loss is 3.9171664762496947 and perplexity is 50.25783605877285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.544945211971507 and perplexity of 94.15526848655739
Finished 10 epochs...
Completing Train Step...
At time: 82.387864112854 and batch: 50, loss is 3.999921679496765 and perplexity is 54.59387404600887
At time: 82.99077010154724 and batch: 100, loss is 3.981022386550903 and perplexity is 53.57177730305322
At time: 83.58112478256226 and batch: 150, loss is 3.936052279472351 and perplexity is 51.21601517247817
At time: 84.17169904708862 and batch: 200, loss is 3.8854078340530394 and perplexity is 48.68679454378554
At time: 84.76173615455627 and batch: 250, loss is 3.847977147102356 and perplexity is 46.89809925890794
At time: 85.35253047943115 and batch: 300, loss is 3.936732835769653 and perplexity is 51.2508824173423
At time: 85.94251298904419 and batch: 350, loss is 3.8890432214736936 and perplexity is 48.86411201779301
At time: 86.53331518173218 and batch: 400, loss is 3.946503038406372 and perplexity is 51.75406803388562
At time: 87.12355947494507 and batch: 450, loss is 3.9080013990402223 and perplexity is 49.79932347207415
At time: 87.71411442756653 and batch: 500, loss is 3.870260968208313 and perplexity is 47.9548991521029
At time: 88.30415987968445 and batch: 550, loss is 3.8777681255340575 and perplexity is 48.31625881940841
At time: 88.8943498134613 and batch: 600, loss is 3.9753993272781374 and perplexity is 53.271385376237674
At time: 89.48512172698975 and batch: 650, loss is 3.9013280725479125 and perplexity is 49.468102728724325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.54307586071538 and perplexity of 93.97942362625338
Finished 11 epochs...
Completing Train Step...
At time: 90.60309743881226 and batch: 50, loss is 3.9735094261169435 and perplexity is 53.17080279865958
At time: 91.19168305397034 and batch: 100, loss is 3.95634539604187 and perplexity is 52.26596508512469
At time: 91.79376268386841 and batch: 150, loss is 3.9132744598388673 and perplexity is 50.06261189032765
At time: 92.38187789916992 and batch: 200, loss is 3.863068699836731 and perplexity is 47.61123200230872
At time: 92.97369146347046 and batch: 250, loss is 3.8265331602096557 and perplexity is 45.90312328652878
At time: 93.56767988204956 and batch: 300, loss is 3.916971001625061 and perplexity is 50.24801288725603
At time: 94.16431617736816 and batch: 350, loss is 3.8702084493637083 and perplexity is 47.95238068234043
At time: 94.7633147239685 and batch: 400, loss is 3.929217576980591 and perplexity is 50.867162456225266
At time: 95.3619110584259 and batch: 450, loss is 3.8924846553802492 and perplexity is 49.03256432218585
At time: 95.96188402175903 and batch: 500, loss is 3.8561532878875733 and perplexity is 47.28311655397083
At time: 96.56235098838806 and batch: 550, loss is 3.8655619287490843 and perplexity is 47.730085805791084
At time: 97.15677237510681 and batch: 600, loss is 3.9653342628479002 and perplexity is 52.73789476262514
At time: 97.75667572021484 and batch: 650, loss is 3.8864630222320558 and perplexity is 48.73819538787465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.542898739085478 and perplexity of 93.96277931164107
Finished 12 epochs...
Completing Train Step...
At time: 98.87870836257935 and batch: 50, loss is 3.9504975032806398 and perplexity is 51.96121127852769
At time: 99.4844696521759 and batch: 100, loss is 3.935421371459961 and perplexity is 51.18371276913718
At time: 100.07631301879883 and batch: 150, loss is 3.894113883972168 and perplexity is 49.112514688941445
At time: 100.6660430431366 and batch: 200, loss is 3.84401668548584 and perplexity is 46.71272845617898
At time: 101.263995885849 and batch: 250, loss is 3.8081509351730345 and perplexity is 45.06702991593204
At time: 101.86295866966248 and batch: 300, loss is 3.899395227432251 and perplexity is 49.37258089217993
At time: 102.46432089805603 and batch: 350, loss is 3.8539747524261476 and perplexity is 47.180220729627806
At time: 103.0660970211029 and batch: 400, loss is 3.9139298963546754 and perplexity is 50.09543550996166
At time: 103.66098356246948 and batch: 450, loss is 3.878323378562927 and perplexity is 48.34309401793472
At time: 104.25420236587524 and batch: 500, loss is 3.842461314201355 and perplexity is 46.64012929368047
At time: 104.84732127189636 and batch: 550, loss is 3.853553214073181 and perplexity is 47.16033664833473
At time: 105.44067621231079 and batch: 600, loss is 3.9554022979736327 and perplexity is 52.216696390675395
At time: 106.03367018699646 and batch: 650, loss is 3.8722940397262575 and perplexity is 48.052494066819236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.542505002489277 and perplexity of 93.92579000924414
Finished 13 epochs...
Completing Train Step...
At time: 107.15608978271484 and batch: 50, loss is 3.9308406591415403 and perplexity is 50.949791078567834
At time: 107.74543118476868 and batch: 100, loss is 3.9168874168395997 and perplexity is 50.24381309340086
At time: 108.33509087562561 and batch: 150, loss is 3.8772155475616454 and perplexity is 48.289567694218526
At time: 108.92399168014526 and batch: 200, loss is 3.8270729923248292 and perplexity is 45.927909956383886
At time: 109.51341891288757 and batch: 250, loss is 3.791365942955017 and perplexity is 44.31689329975893
At time: 110.10415768623352 and batch: 300, loss is 3.883547878265381 and perplexity is 48.596323420717354
At time: 110.69615697860718 and batch: 350, loss is 3.839621415138245 and perplexity is 46.50786393322626
At time: 111.2872245311737 and batch: 400, loss is 3.8996793365478517 and perplexity is 49.38661008528859
At time: 111.87863779067993 and batch: 450, loss is 3.865179109573364 and perplexity is 47.71181731067408
At time: 112.46961283683777 and batch: 500, loss is 3.8290654945373537 and perplexity is 46.019512647565314
At time: 113.06157279014587 and batch: 550, loss is 3.842196002006531 and perplexity is 46.627756739977954
At time: 113.65263295173645 and batch: 600, loss is 3.9455605506896974 and perplexity is 51.70531343938794
At time: 114.2442376613617 and batch: 650, loss is 3.8589525747299196 and perplexity is 47.41566098839957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.544024299172794 and perplexity of 94.06859960810677
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 115.34927868843079 and batch: 50, loss is 3.916996784210205 and perplexity is 50.24930842762772
At time: 115.95052075386047 and batch: 100, loss is 3.900849075317383 and perplexity is 49.4444133185516
At time: 116.53871846199036 and batch: 150, loss is 3.855573229789734 and perplexity is 47.255697552398374
At time: 117.12717080116272 and batch: 200, loss is 3.801739330291748 and perplexity is 44.77900227335978
At time: 117.71587944030762 and batch: 250, loss is 3.756338567733765 and perplexity is 42.7914607602157
At time: 118.3056480884552 and batch: 300, loss is 3.8414815616607667 and perplexity is 46.594455886483544
At time: 118.89661145210266 and batch: 350, loss is 3.791576886177063 and perplexity is 44.32624263407733
At time: 119.48777055740356 and batch: 400, loss is 3.8461393308639527 and perplexity is 46.81198832281513
At time: 120.07833766937256 and batch: 450, loss is 3.8040793418884276 and perplexity is 44.88390835081697
At time: 120.66964387893677 and batch: 500, loss is 3.7556817865371706 and perplexity is 42.76336536068859
At time: 121.26025700569153 and batch: 550, loss is 3.7623799943923952 and perplexity is 43.05076472627021
At time: 121.86675953865051 and batch: 600, loss is 3.8553163480758665 and perplexity is 47.243559986847174
At time: 122.45978474617004 and batch: 650, loss is 3.7914326190948486 and perplexity is 44.319848277645654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.52732639686734 and perplexity of 92.51089273358878
Finished 15 epochs...
Completing Train Step...
At time: 123.58488035202026 and batch: 50, loss is 3.8963768815994264 and perplexity is 49.22378204454728
At time: 124.17703342437744 and batch: 100, loss is 3.8791245508193968 and perplexity is 48.38184068295776
At time: 124.76753306388855 and batch: 150, loss is 3.8359437656402586 and perplexity is 46.337138437470024
At time: 125.35717701911926 and batch: 200, loss is 3.7838802194595336 and perplexity is 43.98638786981953
At time: 125.96815657615662 and batch: 250, loss is 3.7419856786727905 and perplexity is 42.18166629569493
At time: 126.56046056747437 and batch: 300, loss is 3.829178261756897 and perplexity is 46.024702432664625
At time: 127.1538577079773 and batch: 350, loss is 3.7813200759887695 and perplexity is 43.87392043392705
At time: 127.7478928565979 and batch: 400, loss is 3.8376864433288573 and perplexity is 46.417959536887906
At time: 128.3419737815857 and batch: 450, loss is 3.7986413478851317 and perplexity is 44.64049237352647
At time: 128.93571591377258 and batch: 500, loss is 3.753502631187439 and perplexity is 42.67027880615878
At time: 129.52715229988098 and batch: 550, loss is 3.763862442970276 and perplexity is 43.11463259993798
At time: 130.1302993297577 and batch: 600, loss is 3.859865832328796 and perplexity is 47.458983480389435
At time: 130.7445945739746 and batch: 650, loss is 3.7934160566329957 and perplexity is 44.40784116375404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.525234446806066 and perplexity of 92.3175668504164
Finished 16 epochs...
Completing Train Step...
At time: 131.89462065696716 and batch: 50, loss is 3.8879025030136107 and perplexity is 48.80840360302814
At time: 132.50786113739014 and batch: 100, loss is 3.8704064416885378 and perplexity is 47.96187582562456
At time: 133.10272908210754 and batch: 150, loss is 3.8276641273498537 and perplexity is 45.95506757870532
At time: 133.6999843120575 and batch: 200, loss is 3.77591682434082 and perplexity is 43.63749790145808
At time: 134.2931203842163 and batch: 250, loss is 3.735213346481323 and perplexity is 41.89696317933677
At time: 134.88813185691833 and batch: 300, loss is 3.8230931520462037 and perplexity is 45.74548745741534
At time: 135.4830584526062 and batch: 350, loss is 3.7765016555786133 and perplexity is 43.663025937438825
At time: 136.07355308532715 and batch: 400, loss is 3.8338036489486695 and perplexity is 46.23807759275219
At time: 136.66120266914368 and batch: 450, loss is 3.7962351274490356 and perplexity is 44.53320663683041
At time: 137.26377534866333 and batch: 500, loss is 3.752682604789734 and perplexity is 42.635302393890946
At time: 137.85307002067566 and batch: 550, loss is 3.7645905447006225 and perplexity is 43.146035869537386
At time: 138.45752215385437 and batch: 600, loss is 3.8613423204421995 and perplexity is 47.52910786154069
At time: 139.04622769355774 and batch: 650, loss is 3.792860221862793 and perplexity is 44.38316460024692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.524413464116115 and perplexity of 92.24180682914212
Finished 17 epochs...
Completing Train Step...
At time: 140.1643373966217 and batch: 50, loss is 3.881459650993347 and perplexity is 48.494949135941226
At time: 140.75211095809937 and batch: 100, loss is 3.8639907741546633 and perplexity is 47.65515334284132
At time: 141.33989262580872 and batch: 150, loss is 3.8215962743759153 and perplexity is 45.677063282809584
At time: 141.92748737335205 and batch: 200, loss is 3.7702466583251955 and perplexity is 43.39076620972812
At time: 142.51515197753906 and batch: 250, loss is 3.7302851152420042 and perplexity is 41.69099320726298
At time: 143.10758304595947 and batch: 300, loss is 3.818665137290955 and perplexity is 45.54337357583522
At time: 143.70267009735107 and batch: 350, loss is 3.7730542421340942 and perplexity is 43.51276059699217
At time: 144.3014223575592 and batch: 400, loss is 3.8310023736953736 and perplexity is 46.108733259353734
At time: 144.8973207473755 and batch: 450, loss is 3.7943479204177857 and perplexity is 44.44924250990683
At time: 145.49658203125 and batch: 500, loss is 3.751824345588684 and perplexity is 42.59872595160112
At time: 146.0958981513977 and batch: 550, loss is 3.764545340538025 and perplexity is 43.14408553319849
At time: 146.69515419006348 and batch: 600, loss is 3.861522331237793 and perplexity is 47.53766438417081
At time: 147.29477095603943 and batch: 650, loss is 3.79150954246521 and perplexity is 44.32325764087725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.524006862266391 and perplexity of 92.20430876377144
Finished 18 epochs...
Completing Train Step...
At time: 148.42696833610535 and batch: 50, loss is 3.87602828502655 and perplexity is 48.232269320496066
At time: 149.03248691558838 and batch: 100, loss is 3.8586638498306276 and perplexity is 47.4019728825994
At time: 149.62545704841614 and batch: 150, loss is 3.8165793657302856 and perplexity is 45.44847950053817
At time: 150.21730494499207 and batch: 200, loss is 3.765592923164368 and perplexity is 43.18930620969451
At time: 150.8129620552063 and batch: 250, loss is 3.7261490345001222 and perplexity is 41.51891200933125
At time: 151.40876483917236 and batch: 300, loss is 3.8149314641952516 and perplexity is 45.37364655700011
At time: 152.00446391105652 and batch: 350, loss is 3.7701406049728394 and perplexity is 43.386164717516436
At time: 152.6157808303833 and batch: 400, loss is 3.8285880374908445 and perplexity is 45.997545551564514
At time: 153.21588110923767 and batch: 450, loss is 3.7925739383697508 and perplexity is 44.370460251462326
At time: 153.81492972373962 and batch: 500, loss is 3.7507886409759523 and perplexity is 42.554629094237676
At time: 154.4147355556488 and batch: 550, loss is 3.763995871543884 and perplexity is 43.120385707673364
At time: 155.01473927497864 and batch: 600, loss is 3.860989089012146 and perplexity is 47.51232205161395
At time: 155.6139259338379 and batch: 650, loss is 3.789838056564331 and perplexity is 44.249233822768026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.523782468309589 and perplexity of 92.1836209952857
Finished 19 epochs...
Completing Train Step...
At time: 156.75495862960815 and batch: 50, loss is 3.8712205362319945 and perplexity is 48.000937224706746
At time: 157.35178303718567 and batch: 100, loss is 3.853992557525635 and perplexity is 47.18106078563034
At time: 157.9489142894745 and batch: 150, loss is 3.8121541643142702 and perplexity is 45.247805164348755
At time: 158.54596781730652 and batch: 200, loss is 3.761529326438904 and perplexity is 43.01415839246928
At time: 159.14343190193176 and batch: 250, loss is 3.7224410724639894 and perplexity is 41.36524652876518
At time: 159.73917841911316 and batch: 300, loss is 3.811575326919556 and perplexity is 45.22162162142884
At time: 160.3365933895111 and batch: 350, loss is 3.76747654914856 and perplexity is 43.270735376080935
At time: 160.93278169631958 and batch: 400, loss is 3.82632652759552 and perplexity is 45.89363918406328
At time: 161.53214859962463 and batch: 450, loss is 3.7908256483078 and perplexity is 44.2929555868192
At time: 162.13274550437927 and batch: 500, loss is 3.749595808982849 and perplexity is 42.50389883355352
At time: 162.73588180541992 and batch: 550, loss is 3.7631232643127444 and perplexity is 43.082774959387315
At time: 163.33667016029358 and batch: 600, loss is 3.8600541973114013 and perplexity is 47.467923932994836
At time: 163.93859457969666 and batch: 650, loss is 3.7880253171920777 and perplexity is 44.16909415253827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5236562093098955 and perplexity of 92.17198271824478
Finished 20 epochs...
Completing Train Step...
At time: 165.06373023986816 and batch: 50, loss is 3.866817445755005 and perplexity is 47.790049374977905
At time: 165.6763277053833 and batch: 100, loss is 3.849756498336792 and perplexity is 46.98162173559134
At time: 166.27310252189636 and batch: 150, loss is 3.808119788169861 and perplexity is 45.065626234868574
At time: 166.87037301063538 and batch: 200, loss is 3.7578526830673216 and perplexity is 42.85630104254345
At time: 167.46724390983582 and batch: 250, loss is 3.7190014028549196 and perplexity is 41.22320816996338
At time: 168.08108496665955 and batch: 300, loss is 3.8084295797348022 and perplexity is 45.079589348460665
At time: 168.67389369010925 and batch: 350, loss is 3.7649564218521117 and perplexity is 43.161824906487176
At time: 169.26348614692688 and batch: 400, loss is 3.824089722633362 and perplexity is 45.79109878839113
At time: 169.85239481925964 and batch: 450, loss is 3.789062919616699 and perplexity is 44.21494789658837
At time: 170.44101071357727 and batch: 500, loss is 3.7482475090026854 and perplexity is 42.44662944443338
At time: 171.02966785430908 and batch: 550, loss is 3.761993670463562 and perplexity is 43.03413639786927
At time: 171.6182143688202 and batch: 600, loss is 3.858782296180725 and perplexity is 47.40758780580178
At time: 172.20595622062683 and batch: 650, loss is 3.786188678741455 and perplexity is 44.08804594679699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.523579316980698 and perplexity of 92.16489567228007
Finished 21 epochs...
Completing Train Step...
At time: 173.32125401496887 and batch: 50, loss is 3.8626614379882813 and perplexity is 47.591845711872686
At time: 173.90792512893677 and batch: 100, loss is 3.8458964729309084 and perplexity is 46.80062104046258
At time: 174.49392533302307 and batch: 150, loss is 3.8043430995941163 and perplexity is 44.89574838888792
At time: 175.0795738697052 and batch: 200, loss is 3.7544271039962767 and perplexity is 42.70974455835721
At time: 175.66519260406494 and batch: 250, loss is 3.7157461547851565 and perplexity is 41.08923457808265
At time: 176.25051403045654 and batch: 300, loss is 3.8053704071044923 and perplexity is 44.94189382711106
At time: 176.83606338500977 and batch: 350, loss is 3.762550687789917 and perplexity is 43.058113834771554
At time: 177.42158436775208 and batch: 400, loss is 3.8218711709976194 and perplexity is 45.6896214792192
At time: 178.00721549987793 and batch: 450, loss is 3.7872978496551513 and perplexity is 44.13697425491477
At time: 178.59395003318787 and batch: 500, loss is 3.74678005695343 and perplexity is 42.38438673133914
At time: 179.1836278438568 and batch: 550, loss is 3.7607033395767213 and perplexity is 42.978643932007294
At time: 179.7738881111145 and batch: 600, loss is 3.857397880554199 and perplexity is 47.342001410318204
At time: 180.36463379859924 and batch: 650, loss is 3.7843510150909423 and perplexity is 44.00710134459422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.523550594554228 and perplexity of 92.16224851085764
Finished 22 epochs...
Completing Train Step...
At time: 181.47004866600037 and batch: 50, loss is 3.858749442100525 and perplexity is 47.40603029869529
At time: 182.0720818042755 and batch: 100, loss is 3.842265887260437 and perplexity is 46.63101544646324
At time: 182.6602463722229 and batch: 150, loss is 3.800774292945862 and perplexity is 44.73580970842566
At time: 183.26277017593384 and batch: 200, loss is 3.7511204719543456 and perplexity is 42.568752381587714
At time: 183.85137033462524 and batch: 250, loss is 3.7126662826538084 and perplexity is 40.96287966796981
At time: 184.44227504730225 and batch: 300, loss is 3.802461476325989 and perplexity is 44.81135093109039
At time: 185.03068327903748 and batch: 350, loss is 3.76024197101593 and perplexity is 42.9588195104457
At time: 185.6184687614441 and batch: 400, loss is 3.819758243560791 and perplexity is 45.59318454242054
At time: 186.20636463165283 and batch: 450, loss is 3.7855098056793213 and perplexity is 44.058125917141766
At time: 186.7949674129486 and batch: 500, loss is 3.745216279029846 and perplexity is 42.31815875948034
At time: 187.38582730293274 and batch: 550, loss is 3.759331750869751 and perplexity is 42.919735317779434
At time: 187.97688221931458 and batch: 600, loss is 3.8558885288238525 and perplexity is 47.270599577366816
At time: 188.5671100616455 and batch: 650, loss is 3.782399115562439 and perplexity is 43.92128768129857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.523593977385876 and perplexity of 92.16624685689824
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 189.68376350402832 and batch: 50, loss is 3.8578078317642213 and perplexity is 47.361413299773155
At time: 190.27147459983826 and batch: 100, loss is 3.8415383434295656 and perplexity is 46.59710167722063
At time: 190.85708951950073 and batch: 150, loss is 3.7987526941299437 and perplexity is 44.64546320145519
At time: 191.44286942481995 and batch: 200, loss is 3.7480463361740113 and perplexity is 42.438091194781066
At time: 192.02837920188904 and batch: 250, loss is 3.707163305282593 and perplexity is 40.73808096647433
At time: 192.61388635635376 and batch: 300, loss is 3.7956619548797605 and perplexity is 44.507688738142704
At time: 193.19988584518433 and batch: 350, loss is 3.75203387260437 and perplexity is 42.60765247066258
At time: 193.78557777404785 and batch: 400, loss is 3.8110376453399657 and perplexity is 45.19731332413127
At time: 194.37555265426636 and batch: 450, loss is 3.773489136695862 and perplexity is 43.53168817539544
At time: 194.96603846549988 and batch: 500, loss is 3.728823299407959 and perplexity is 41.63009317641502
At time: 195.55878925323486 and batch: 550, loss is 3.740160403251648 and perplexity is 42.10474336111004
At time: 196.16335320472717 and batch: 600, loss is 3.8352924680709837 and perplexity is 46.30696899754645
At time: 196.76922249794006 and batch: 650, loss is 3.7668457221984863 and perplexity is 43.24344763788145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.521701588350184 and perplexity of 91.99199738777867
Finished 24 epochs...
Completing Train Step...
At time: 197.90669870376587 and batch: 50, loss is 3.8540793228149415 and perplexity is 47.18515464161893
At time: 198.52534437179565 and batch: 100, loss is 3.837309103012085 and perplexity is 46.400447473543885
At time: 199.12988257408142 and batch: 150, loss is 3.7947104358673096 and perplexity is 44.46535896809235
At time: 199.73431062698364 and batch: 200, loss is 3.7440673160552977 and perplexity is 42.26956468365741
At time: 200.3371045589447 and batch: 250, loss is 3.7040937089920045 and perplexity is 40.613223233709114
At time: 200.94015097618103 and batch: 300, loss is 3.793010559082031 and perplexity is 44.38983754337719
At time: 201.5439910888672 and batch: 350, loss is 3.749687123298645 and perplexity is 42.50778022520478
At time: 202.14873838424683 and batch: 400, loss is 3.809032702445984 and perplexity is 45.10678607326424
At time: 202.75258612632751 and batch: 450, loss is 3.7726500988006593 and perplexity is 43.49517875790838
At time: 203.35725378990173 and batch: 500, loss is 3.7286693239212036 and perplexity is 41.62368365602169
At time: 203.9613094329834 and batch: 550, loss is 3.7414389944076536 and perplexity is 42.158612544589474
At time: 204.56493425369263 and batch: 600, loss is 3.8371224784851075 and perplexity is 46.39178881996628
At time: 205.1677370071411 and batch: 650, loss is 3.767568793296814 and perplexity is 43.274727032310665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.521370382869945 and perplexity of 91.96153417917499
Finished 25 epochs...
Completing Train Step...
At time: 206.3129014968872 and batch: 50, loss is 3.8524181413650513 and perplexity is 47.10683660626188
At time: 206.912837266922 and batch: 100, loss is 3.8354691743850706 and perplexity is 46.31515245436749
At time: 207.5127980709076 and batch: 150, loss is 3.792858557701111 and perplexity is 44.38309073954651
At time: 208.11464738845825 and batch: 200, loss is 3.7422805213928223 and perplexity is 42.19410508657439
At time: 208.71564722061157 and batch: 250, loss is 3.7027413272857665 and perplexity is 40.55833577633859
At time: 209.31574940681458 and batch: 300, loss is 3.7917309904098513 and perplexity is 44.333074022050226
At time: 209.91631603240967 and batch: 350, loss is 3.7486502504348755 and perplexity is 42.46372790366534
At time: 210.516375541687 and batch: 400, loss is 3.808288173675537 and perplexity is 45.073215272054746
At time: 211.11664509773254 and batch: 450, loss is 3.7724119234085083 and perplexity is 43.48482051023985
At time: 211.71687006950378 and batch: 500, loss is 3.7287716817855836 and perplexity is 41.62794438544421
At time: 212.3169207572937 and batch: 550, loss is 3.7421929025650025 and perplexity is 42.19040825050438
At time: 212.91686701774597 and batch: 600, loss is 3.8379898643493653 and perplexity is 46.43204585847545
At time: 213.53162837028503 and batch: 650, loss is 3.7677217435836794 and perplexity is 43.28134642043004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.521186080633425 and perplexity of 91.94458702449924
Finished 26 epochs...
Completing Train Step...
At time: 214.65990591049194 and batch: 50, loss is 3.8511425113677977 and perplexity is 47.04678402299281
At time: 215.27220153808594 and batch: 100, loss is 3.8341265296936036 and perplexity is 46.253009388154545
At time: 215.8703010082245 and batch: 150, loss is 3.791549916267395 and perplexity is 44.32504717543836
At time: 216.4689977169037 and batch: 200, loss is 3.741035513877869 and perplexity is 42.14160579644159
At time: 217.06676697731018 and batch: 250, loss is 3.701797728538513 and perplexity is 40.52008303196847
At time: 217.6627299785614 and batch: 300, loss is 3.790823469161987 and perplexity is 44.292859066115646
At time: 218.26175212860107 and batch: 350, loss is 3.747961688041687 and perplexity is 42.43449904165869
At time: 218.85956811904907 and batch: 400, loss is 3.807829942703247 and perplexity is 45.052566060211916
At time: 219.45659971237183 and batch: 450, loss is 3.772235779762268 and perplexity is 43.477161609952184
At time: 220.0543611049652 and batch: 500, loss is 3.7287959480285644 and perplexity is 41.62895455151387
At time: 220.65050411224365 and batch: 550, loss is 3.7426248455047606 and perplexity is 42.20863603587189
At time: 221.24695420265198 and batch: 600, loss is 3.838439145088196 and perplexity is 46.45291156927416
At time: 221.84519910812378 and batch: 650, loss is 3.7676639652252195 and perplexity is 43.27884576752444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5210741828469665 and perplexity of 91.93429920433721
Finished 27 epochs...
Completing Train Step...
At time: 222.98188710212708 and batch: 50, loss is 3.8500445795059206 and perplexity is 46.995158205815954
At time: 223.5664803981781 and batch: 100, loss is 3.832993497848511 and perplexity is 46.20063293328529
At time: 224.15259981155396 and batch: 150, loss is 3.7904716205596922 and perplexity is 44.277277426911326
At time: 224.73849749565125 and batch: 200, loss is 3.7400231695175172 and perplexity is 42.09896556641721
At time: 225.32648944854736 and batch: 250, loss is 3.7010217618942263 and perplexity is 40.488652995019144
At time: 225.91283631324768 and batch: 300, loss is 3.790068473815918 and perplexity is 44.2594307843431
At time: 226.50251579284668 and batch: 350, loss is 3.747406425476074 and perplexity is 42.41094329326819
At time: 227.09565567970276 and batch: 400, loss is 3.8074649667739866 and perplexity is 45.03612595835175
At time: 227.69298887252808 and batch: 450, loss is 3.77204966545105 and perplexity is 43.46907064091135
At time: 228.2885468006134 and batch: 500, loss is 3.728744387626648 and perplexity is 41.62680820121965
At time: 228.90036821365356 and batch: 550, loss is 3.7428762674331666 and perplexity is 42.219249546718196
At time: 229.49773836135864 and batch: 600, loss is 3.8386788892745973 and perplexity is 46.46404971986439
At time: 230.0936381816864 and batch: 650, loss is 3.767508029937744 and perplexity is 43.272097594420906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.521001479204963 and perplexity of 91.92761548892817
Finished 28 epochs...
Completing Train Step...
At time: 231.21557235717773 and batch: 50, loss is 3.8490505981445313 and perplexity is 46.948469102377196
At time: 231.82340693473816 and batch: 100, loss is 3.831977071762085 and perplexity is 46.153697262123636
At time: 232.4191188812256 and batch: 150, loss is 3.7895150852203368 and perplexity is 44.23494489583074
At time: 233.01500988006592 and batch: 200, loss is 3.7391359043121337 and perplexity is 42.06162918517287
At time: 233.60932564735413 and batch: 250, loss is 3.7003331661224363 and perplexity is 40.46078227669198
At time: 234.20403957366943 and batch: 300, loss is 3.789389705657959 and perplexity is 44.22939908547081
At time: 234.80175971984863 and batch: 350, loss is 3.746913948059082 and perplexity is 42.39006200366674
At time: 235.39697456359863 and batch: 400, loss is 3.8071359968185425 and perplexity is 45.02131286266716
At time: 235.9945650100708 and batch: 450, loss is 3.7718444776535036 and perplexity is 43.460152233050685
At time: 236.59180092811584 and batch: 500, loss is 3.7286382341384887 and perplexity is 41.62238960485702
At time: 237.18951034545898 and batch: 550, loss is 3.7430167388916016 and perplexity is 42.22518056283554
At time: 237.78677797317505 and batch: 600, loss is 3.8387981700897216 and perplexity is 46.46959232014532
At time: 238.38470649719238 and batch: 650, loss is 3.767297315597534 and perplexity is 43.26298050351142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520949120615043 and perplexity of 91.92280241461049
Finished 29 epochs...
Completing Train Step...
At time: 239.52472805976868 and batch: 50, loss is 3.8481251907348635 and perplexity is 46.90504273783617
At time: 240.118821144104 and batch: 100, loss is 3.831035895347595 and perplexity is 46.11027892618094
At time: 240.71105456352234 and batch: 150, loss is 3.788633279800415 and perplexity is 44.19595547473954
At time: 241.30633687973022 and batch: 200, loss is 3.7383257389068603 and perplexity is 42.0275661085439
At time: 241.90146350860596 and batch: 250, loss is 3.6996992874145507 and perplexity is 40.435143175201134
At time: 242.49548625946045 and batch: 300, loss is 3.7887559604644774 and perplexity is 44.20137779650626
At time: 243.09179854393005 and batch: 350, loss is 3.7464572858810423 and perplexity is 42.37070848497124
At time: 243.68729615211487 and batch: 400, loss is 3.806824984550476 and perplexity is 45.007312859241665
At time: 244.30277562141418 and batch: 450, loss is 3.7716225528717042 and perplexity is 43.45050841838971
At time: 244.9022045135498 and batch: 500, loss is 3.7284937286376953 and perplexity is 41.61637537515805
At time: 245.50184631347656 and batch: 550, loss is 3.7430857610702515 and perplexity is 42.22809513737586
At time: 246.10136771202087 and batch: 600, loss is 3.838843111991882 and perplexity is 46.471680798946565
At time: 246.70185947418213 and batch: 650, loss is 3.767055940628052 and perplexity is 43.25253916310249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520912918390012 and perplexity of 91.91947466486832
Finished 30 epochs...
Completing Train Step...
At time: 247.82828903198242 and batch: 50, loss is 3.847251782417297 and perplexity is 46.864093368736086
At time: 248.44054245948792 and batch: 100, loss is 3.8301497745513915 and perplexity is 46.06943774688727
At time: 249.03565573692322 and batch: 150, loss is 3.787807741165161 and perplexity is 44.15948506191185
At time: 249.630690574646 and batch: 200, loss is 3.7375734663009643 and perplexity is 41.995961810882086
At time: 250.2264792919159 and batch: 250, loss is 3.6991048526763914 and perplexity is 40.411114263972685
At time: 250.8201129436493 and batch: 300, loss is 3.7881550216674804 and perplexity is 44.174823453274286
At time: 251.41684937477112 and batch: 350, loss is 3.7460245656967164 and perplexity is 42.35237779050237
At time: 252.01535367965698 and batch: 400, loss is 3.806523599624634 and perplexity is 44.99375037745967
At time: 252.61578512191772 and batch: 450, loss is 3.7713873624801635 and perplexity is 43.440290477929956
At time: 253.21568775177002 and batch: 500, loss is 3.7283224630355836 and perplexity is 41.60924853188069
At time: 253.81519222259521 and batch: 550, loss is 3.7431080961227416 and perplexity is 42.22903831463023
At time: 254.4160439968109 and batch: 600, loss is 3.8388366889953613 and perplexity is 46.47138231246107
At time: 255.01551413536072 and batch: 650, loss is 3.7667976570129396 and perplexity is 43.24136918349811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5208892822265625 and perplexity of 91.91730206681697
Finished 31 epochs...
Completing Train Step...
At time: 256.1644506454468 and batch: 50, loss is 3.846420168876648 and perplexity is 46.82513675478936
At time: 256.7615556716919 and batch: 100, loss is 3.8293073320388795 and perplexity is 46.0306432373682
At time: 257.3577332496643 and batch: 150, loss is 3.7870265531539915 and perplexity is 44.125001672360945
At time: 257.95639419555664 and batch: 200, loss is 3.7368650531768797 and perplexity is 41.96622185570773
At time: 258.55442810058594 and batch: 250, loss is 3.6985358238220214 and perplexity is 40.38812571511335
At time: 259.1507706642151 and batch: 300, loss is 3.787580523490906 and perplexity is 44.14945238626355
At time: 259.76871132850647 and batch: 350, loss is 3.745606231689453 and perplexity is 42.334664055971245
At time: 260.3651943206787 and batch: 400, loss is 3.806224699020386 and perplexity is 44.9803037279904
At time: 260.960884809494 and batch: 450, loss is 3.771141448020935 and perplexity is 43.42960919578304
At time: 261.5597491264343 and batch: 500, loss is 3.7281318616867067 and perplexity is 41.60131850874532
At time: 262.15748715400696 and batch: 550, loss is 3.743096323013306 and perplexity is 42.22854115046737
At time: 262.75501346588135 and batch: 600, loss is 3.838790626525879 and perplexity is 46.469241775131096
At time: 263.3541524410248 and batch: 650, loss is 3.7665265369415284 and perplexity is 43.2296471695056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520875220205269 and perplexity of 91.91600953284595
Finished 32 epochs...
Completing Train Step...
At time: 264.48364424705505 and batch: 50, loss is 3.8456209993362425 and perplexity is 46.7877304807379
At time: 265.1024913787842 and batch: 100, loss is 3.8284994649887083 and perplexity is 45.993471614284964
At time: 265.69878125190735 and batch: 150, loss is 3.78627845287323 and perplexity is 44.09200409051031
At time: 266.29675245285034 and batch: 200, loss is 3.73618763923645 and perplexity is 41.937802978753986
At time: 266.8948333263397 and batch: 250, loss is 3.6979841995239258 and perplexity is 40.36585278732321
At time: 267.4935474395752 and batch: 300, loss is 3.7870258331298827 and perplexity is 44.12496990130738
At time: 268.09029507637024 and batch: 350, loss is 3.745196805000305 and perplexity is 42.31733466243081
At time: 268.6910881996155 and batch: 400, loss is 3.8059259605407716 and perplexity is 44.96686838736865
At time: 269.29155015945435 and batch: 450, loss is 3.770886821746826 and perplexity is 43.418552283957375
At time: 269.8916971683502 and batch: 500, loss is 3.7279264450073244 and perplexity is 41.5927737816841
At time: 270.49393796920776 and batch: 550, loss is 3.743057355880737 and perplexity is 42.22689565736646
At time: 271.095947265625 and batch: 600, loss is 3.8387144708633425 and perplexity is 46.4657030139862
At time: 271.6959435939789 and batch: 650, loss is 3.7662449216842653 and perplexity is 43.21747475534578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520866842830882 and perplexity of 91.91523952124726
Finished 33 epochs...
Completing Train Step...
At time: 272.84664034843445 and batch: 50, loss is 3.844847617149353 and perplexity is 46.75155967215151
At time: 273.4475853443146 and batch: 100, loss is 3.827719655036926 and perplexity is 45.957619428165714
At time: 274.0479369163513 and batch: 150, loss is 3.785555949211121 and perplexity is 44.060158961581614
At time: 274.64765334129333 and batch: 200, loss is 3.7355338191986083 and perplexity is 41.910392164669624
At time: 275.265597820282 and batch: 250, loss is 3.697446360588074 and perplexity is 40.34414829729844
At time: 275.8666582107544 and batch: 300, loss is 3.786486268043518 and perplexity is 44.10116803001857
At time: 276.4669361114502 and batch: 350, loss is 3.744793486595154 and perplexity is 42.300270743832044
At time: 277.06696033477783 and batch: 400, loss is 3.805626540184021 and perplexity is 44.953406407090185
At time: 277.66606187820435 and batch: 450, loss is 3.7706250286102296 and perplexity is 43.407187092697924
At time: 278.26558685302734 and batch: 500, loss is 3.7277092504501343 and perplexity is 41.58374103856711
At time: 278.86392998695374 and batch: 550, loss is 3.742995753288269 and perplexity is 42.22429445124344
At time: 279.46480226516724 and batch: 600, loss is 3.8386149311065676 and perplexity is 46.46107805939706
At time: 280.0637083053589 and batch: 650, loss is 3.765955171585083 and perplexity is 43.20495430173795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520865945255055 and perplexity of 91.91515702038717
Finished 34 epochs...
Completing Train Step...
At time: 281.19355869293213 and batch: 50, loss is 3.844095492362976 and perplexity is 46.71640988549018
At time: 281.80698919296265 and batch: 100, loss is 3.826963267326355 and perplexity is 45.92287079300015
At time: 282.3998861312866 and batch: 150, loss is 3.784854197502136 and perplexity is 44.02925051602785
At time: 282.9941596984863 and batch: 200, loss is 3.7348988914489745 and perplexity is 41.883790539634866
At time: 283.5871467590332 and batch: 250, loss is 3.696919684410095 and perplexity is 40.322905589974404
At time: 284.1801071166992 and batch: 300, loss is 3.785958561897278 and perplexity is 44.07790171202218
At time: 284.77545619010925 and batch: 350, loss is 3.7443951082229616 and perplexity is 42.28342258702328
At time: 285.3725128173828 and batch: 400, loss is 3.805326166152954 and perplexity is 44.939905598945096
At time: 285.9683127403259 and batch: 450, loss is 3.7703576278686524 and perplexity is 43.395581530416685
At time: 286.56580924987793 and batch: 500, loss is 3.7274825811386108 and perplexity is 41.57431634879965
At time: 287.16275119781494 and batch: 550, loss is 3.742915978431702 and perplexity is 42.22092614856465
At time: 287.7598659992218 and batch: 600, loss is 3.8384967851638794 and perplexity is 46.455589195781286
At time: 288.3564352989197 and batch: 650, loss is 3.765659308433533 and perplexity is 43.19217343858211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520865346871171 and perplexity of 91.9151020198549
Finished 35 epochs...
Completing Train Step...
At time: 289.5006763935089 and batch: 50, loss is 3.8433614110946657 and perplexity is 46.682128828150994
At time: 290.0929834842682 and batch: 100, loss is 3.82622652053833 and perplexity is 45.88904972575767
At time: 290.7077565193176 and batch: 150, loss is 3.7841696071624757 and perplexity is 43.99911883156894
At time: 291.30094170570374 and batch: 200, loss is 3.7342796087265016 and perplexity is 41.857860661593435
At time: 291.8945391178131 and batch: 250, loss is 3.6964023447036745 and perplexity is 40.302050344922776
At time: 292.48847126960754 and batch: 300, loss is 3.785440239906311 and perplexity is 44.05506108615987
At time: 293.08064341545105 and batch: 350, loss is 3.744000778198242 and perplexity is 42.266752250972345
At time: 293.6770758628845 and batch: 400, loss is 3.805024814605713 and perplexity is 44.92636492921335
At time: 294.274258852005 and batch: 450, loss is 3.7700854444503786 and perplexity is 43.38377158000701
At time: 294.8699586391449 and batch: 500, loss is 3.727247853279114 and perplexity is 41.56455884373699
At time: 295.4663665294647 and batch: 550, loss is 3.7428206110000612 and perplexity is 42.21689983926881
At time: 296.0681629180908 and batch: 600, loss is 3.8383634376525877 and perplexity is 46.449394871584474
At time: 296.67117142677307 and batch: 650, loss is 3.7653588533401487 and perplexity is 43.17919807943215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520869535558364 and perplexity of 91.91548702427195
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 297.8077745437622 and batch: 50, loss is 3.843120045661926 and perplexity is 46.67086273560253
At time: 298.42655754089355 and batch: 100, loss is 3.826073532104492 and perplexity is 45.88202976890962
At time: 299.0273509025574 and batch: 150, loss is 3.7837538862228395 and perplexity is 43.98083127806795
At time: 299.6270592212677 and batch: 200, loss is 3.7332493925094603 and perplexity is 41.814760219928225
At time: 300.225946187973 and batch: 250, loss is 3.6949860572814943 and perplexity is 40.24501145919379
At time: 300.8261733055115 and batch: 300, loss is 3.784253354072571 and perplexity is 44.002803776117204
At time: 301.42655062675476 and batch: 350, loss is 3.7420603752136232 and perplexity is 42.18481723793456
At time: 302.0276024341583 and batch: 400, loss is 3.802803430557251 and perplexity is 44.82667698240455
At time: 302.6284980773926 and batch: 450, loss is 3.7665366315841675 and perplexity is 43.230083559547786
At time: 303.2287826538086 and batch: 500, loss is 3.7232523918151856 and perplexity is 41.39882057153462
At time: 303.82841539382935 and batch: 550, loss is 3.738120837211609 and perplexity is 42.01895547119819
At time: 304.42941522598267 and batch: 600, loss is 3.833655595779419 and perplexity is 46.231232405562864
At time: 305.02874302864075 and batch: 650, loss is 3.7622600650787352 and perplexity is 43.045601987192114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520725325042126 and perplexity of 91.90223280016015
Finished 37 epochs...
Completing Train Step...
At time: 306.1741740703583 and batch: 50, loss is 3.842605986595154 and perplexity is 46.64687732094734
At time: 306.7672231197357 and batch: 100, loss is 3.8256317234039305 and perplexity is 45.86176316626864
At time: 307.35952377319336 and batch: 150, loss is 3.783345112800598 and perplexity is 43.96285675715689
At time: 307.95255851745605 and batch: 200, loss is 3.73287015914917 and perplexity is 41.79890567437659
At time: 308.5453495979309 and batch: 250, loss is 3.694690489768982 and perplexity is 40.233118098997785
At time: 309.1385555267334 and batch: 300, loss is 3.7839340925216676 and perplexity is 43.98875761505843
At time: 309.7324914932251 and batch: 350, loss is 3.741838049888611 and perplexity is 42.17543952722147
At time: 310.32545351982117 and batch: 400, loss is 3.802637052536011 and perplexity is 44.81921942899314
At time: 310.91910314559937 and batch: 450, loss is 3.7665553951263426 and perplexity is 43.23089471665396
At time: 311.5143496990204 and batch: 500, loss is 3.7233355045318604 and perplexity is 41.402261482969216
At time: 312.11339473724365 and batch: 550, loss is 3.738376860618591 and perplexity is 42.02971468458223
At time: 312.71004486083984 and batch: 600, loss is 3.833882293701172 and perplexity is 46.24171411791559
At time: 313.3059837818146 and batch: 650, loss is 3.762237620353699 and perplexity is 43.04463585133386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520678950291054 and perplexity of 91.89797095581281
Finished 38 epochs...
Completing Train Step...
At time: 314.43307065963745 and batch: 50, loss is 3.8422467422485354 and perplexity is 46.630122703663346
At time: 315.04802894592285 and batch: 100, loss is 3.825329651832581 and perplexity is 45.84791172357299
At time: 315.64624428749084 and batch: 150, loss is 3.783056364059448 and perplexity is 43.95016437015449
At time: 316.2416763305664 and batch: 200, loss is 3.7325905656814573 and perplexity is 41.78722060700287
At time: 316.83849906921387 and batch: 250, loss is 3.6944813919067383 and perplexity is 40.2247063194851
At time: 317.4351303577423 and batch: 300, loss is 3.7837089776992796 and perplexity is 43.97885620821939
At time: 318.0313308238983 and batch: 350, loss is 3.741692099571228 and perplexity is 42.16928445761482
At time: 318.62624979019165 and batch: 400, loss is 3.802551131248474 and perplexity is 44.815368669386885
At time: 319.22370529174805 and batch: 450, loss is 3.7665745878219603 and perplexity is 43.23172444201984
At time: 319.8198299407959 and batch: 500, loss is 3.7234046840667725 and perplexity is 41.40512577123684
At time: 320.41587829589844 and batch: 550, loss is 3.738568205833435 and perplexity is 42.03775763883424
At time: 321.03364419937134 and batch: 600, loss is 3.8340455436706544 and perplexity is 46.24926369255132
At time: 321.63297176361084 and batch: 650, loss is 3.7622066354751587 and perplexity is 43.04330213918276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.52064723594516 and perplexity of 91.89505651798993
Finished 39 epochs...
Completing Train Step...
At time: 322.7803888320923 and batch: 50, loss is 3.8419551849365234 and perplexity is 46.6165293321488
At time: 323.37671422958374 and batch: 100, loss is 3.8250789260864257 and perplexity is 45.83641791265382
At time: 323.9744074344635 and batch: 150, loss is 3.7828138875961304 and perplexity is 43.93950878165269
At time: 324.5735819339752 and batch: 200, loss is 3.732359776496887 and perplexity is 41.777577681217814
At time: 325.1730089187622 and batch: 250, loss is 3.6943093585968017 and perplexity is 40.217786925315934
At time: 325.7703080177307 and batch: 300, loss is 3.783532781600952 and perplexity is 43.97110798796973
At time: 326.37069869041443 and batch: 350, loss is 3.7415781021118164 and perplexity is 42.16447754031483
At time: 326.9705367088318 and batch: 400, loss is 3.8024932527542115 and perplexity is 44.81277489839098
At time: 327.5710506439209 and batch: 450, loss is 3.7665835189819337 and perplexity is 43.232110553190964
At time: 328.1706554889679 and batch: 500, loss is 3.7234558773040773 and perplexity is 41.407245487923205
At time: 328.7704153060913 and batch: 550, loss is 3.7387156009674074 and perplexity is 42.04395425641774
At time: 329.3681719303131 and batch: 600, loss is 3.834164609909058 and perplexity is 46.25477074625369
At time: 329.9691481590271 and batch: 650, loss is 3.762167673110962 and perplexity is 43.041625103039436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520623898973652 and perplexity of 91.89291199069771
Finished 40 epochs...
Completing Train Step...
At time: 331.0943624973297 and batch: 50, loss is 3.8417047691345214 and perplexity is 46.604857278063925
At time: 331.7042088508606 and batch: 100, loss is 3.8248577737808227 and perplexity is 45.82628220396084
At time: 332.2961513996124 and batch: 150, loss is 3.782599730491638 and perplexity is 43.930099831211955
At time: 332.8904552459717 and batch: 200, loss is 3.732158589363098 and perplexity is 41.769173415550895
At time: 333.490681886673 and batch: 250, loss is 3.6941590881347657 and perplexity is 40.211743833953044
At time: 334.0889894962311 and batch: 300, loss is 3.7833849000930786 and perplexity is 43.96460595499467
At time: 334.6887261867523 and batch: 350, loss is 3.7414800214767454 and perplexity is 42.160342224380756
At time: 335.29003953933716 and batch: 400, loss is 3.8024466180801393 and perplexity is 44.810685117967815
At time: 335.89147424697876 and batch: 450, loss is 3.7665828561782835 and perplexity is 43.23208189879978
At time: 336.5120189189911 and batch: 500, loss is 3.7234919977188112 and perplexity is 41.408741161815236
At time: 337.11409425735474 and batch: 550, loss is 3.7388324832916258 and perplexity is 42.04886873871302
At time: 337.71691703796387 and batch: 600, loss is 3.8342523336410523 and perplexity is 46.25882856534702
At time: 338.3178870677948 and batch: 650, loss is 3.7621226596832273 and perplexity is 43.03968769556329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520605947457108 and perplexity of 91.89126238837436
Finished 41 epochs...
Completing Train Step...
At time: 339.4700458049774 and batch: 50, loss is 3.8414814376831057 and perplexity is 46.59445010981225
At time: 340.0651123523712 and batch: 100, loss is 3.82465660572052 and perplexity is 45.81706434685928
At time: 340.6620342731476 and batch: 150, loss is 3.7824052572250366 and perplexity is 43.921557431856726
At time: 341.2594974040985 and batch: 200, loss is 3.7319777107238767 and perplexity is 41.76161894754363
At time: 341.8562562465668 and batch: 250, loss is 3.6940226554870605 and perplexity is 40.20625801350395
At time: 342.45220255851746 and batch: 300, loss is 3.783255195617676 and perplexity is 43.9589039186408
At time: 343.0489413738251 and batch: 350, loss is 3.741391072273254 and perplexity is 42.15659226230152
At time: 343.64317417144775 and batch: 400, loss is 3.802404828071594 and perplexity is 44.80881251818207
At time: 344.24023056030273 and batch: 450, loss is 3.766574292182922 and perplexity is 43.2317116610363
At time: 344.8385407924652 and batch: 500, loss is 3.7235160875320434 and perplexity is 41.40973870267124
At time: 345.43667697906494 and batch: 550, loss is 3.7389272117614745 and perplexity is 42.05285215237587
At time: 346.0335569381714 and batch: 600, loss is 3.8343174171447756 and perplexity is 46.261839349963324
At time: 346.63119626045227 and batch: 650, loss is 3.762073068618774 and perplexity is 43.037553364559045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520590987859988 and perplexity of 91.88988774239226
Finished 42 epochs...
Completing Train Step...
At time: 347.7552216053009 and batch: 50, loss is 3.8412774085998533 and perplexity is 46.584944456619525
At time: 348.36767053604126 and batch: 100, loss is 3.8244704151153566 and perplexity is 45.808534434041185
At time: 348.96148324012756 and batch: 150, loss is 3.782224907875061 and perplexity is 43.91363692177487
At time: 349.55408358573914 and batch: 200, loss is 3.7318113088607787 and perplexity is 41.75467031449355
At time: 350.1478981971741 and batch: 250, loss is 3.693895878791809 and perplexity is 40.20116112007504
At time: 350.73993849754333 and batch: 300, loss is 3.7831375980377198 and perplexity is 43.95373476186857
At time: 351.33416295051575 and batch: 350, loss is 3.7413079595565795 and perplexity is 42.15308865899187
At time: 351.9507477283478 and batch: 400, loss is 3.8023655128479006 and perplexity is 44.80705088432422
At time: 352.5505974292755 and batch: 450, loss is 3.76655951499939 and perplexity is 43.23107282281879
At time: 353.1512234210968 and batch: 500, loss is 3.7235306549072265 and perplexity is 41.41034193826492
At time: 353.74888491630554 and batch: 550, loss is 3.7390048360824584 and perplexity is 42.056116603168384
At time: 354.3461637496948 and batch: 600, loss is 3.834365725517273 and perplexity is 46.26407423811253
At time: 354.9448091983795 and batch: 650, loss is 3.7620201253890992 and perplexity is 43.03527487780237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520577524222579 and perplexity of 91.88865057859057
Finished 43 epochs...
Completing Train Step...
At time: 356.0908396244049 and batch: 50, loss is 3.841087245941162 and perplexity is 46.57608658197186
At time: 356.6844289302826 and batch: 100, loss is 3.824295129776001 and perplexity is 45.800505573228875
At time: 357.2785243988037 and batch: 150, loss is 3.7820555925369264 and perplexity is 43.90620229890634
At time: 357.8715567588806 and batch: 200, loss is 3.731655645370483 and perplexity is 41.74817114263127
At time: 358.46336579322815 and batch: 250, loss is 3.693776445388794 and perplexity is 40.19636004530737
At time: 359.0585927963257 and batch: 300, loss is 3.7830286169052125 and perplexity is 43.948944895083585
At time: 359.653635263443 and batch: 350, loss is 3.7412287759780884 and perplexity is 42.14975095873467
At time: 360.2470233440399 and batch: 400, loss is 3.802326855659485 and perplexity is 44.805318803194744
At time: 360.8412854671478 and batch: 450, loss is 3.766539659500122 and perplexity is 43.230214456805676
At time: 361.43701171875 and batch: 500, loss is 3.723537449836731 and perplexity is 41.41062331957512
At time: 362.03141713142395 and batch: 550, loss is 3.7390694427490234 and perplexity is 42.05883379644424
At time: 362.629695892334 and batch: 600, loss is 3.8344013500213623 and perplexity is 46.265722402171754
At time: 363.2267346382141 and batch: 650, loss is 3.7619647121429445 and perplexity is 43.03289021959367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520568249272365 and perplexity of 91.88779831988354
Finished 44 epochs...
Completing Train Step...
At time: 364.3558621406555 and batch: 50, loss is 3.8409077739715576 and perplexity is 46.5677282300439
At time: 364.96418619155884 and batch: 100, loss is 3.824129037857056 and perplexity is 45.792899111073005
At time: 365.55204606056213 and batch: 150, loss is 3.7818950605392456 and perplexity is 43.899154514253325
At time: 366.14331221580505 and batch: 200, loss is 3.7315082216262816 and perplexity is 41.74201692457793
At time: 366.73360538482666 and batch: 250, loss is 3.6936621713638305 and perplexity is 40.191766907899265
At time: 367.347781419754 and batch: 300, loss is 3.782925696372986 and perplexity is 43.944421879043794
At time: 367.94352769851685 and batch: 350, loss is 3.7411525535583494 and perplexity is 42.1465383251641
At time: 368.54126381874084 and batch: 400, loss is 3.802288408279419 and perplexity is 44.80359618918895
At time: 369.1359341144562 and batch: 450, loss is 3.7665156936645507 and perplexity is 43.22917842100908
At time: 369.73608899116516 and batch: 500, loss is 3.7235380363464357 and perplexity is 41.41064760731471
At time: 370.333993434906 and batch: 550, loss is 3.7391235780715943 and perplexity is 42.061110726609385
At time: 370.9317400455475 and batch: 600, loss is 3.834426875114441 and perplexity is 46.26690335411431
At time: 371.5306131839752 and batch: 650, loss is 3.7619074010849 and perplexity is 43.030424029795114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520559572706036 and perplexity of 91.88700105276534
Finished 45 epochs...
Completing Train Step...
At time: 372.68028378486633 and batch: 50, loss is 3.8407366943359373 and perplexity is 46.55976212150554
At time: 373.2775139808655 and batch: 100, loss is 3.823970022201538 and perplexity is 45.78561790213124
At time: 373.8733444213867 and batch: 150, loss is 3.7817413997650147 and perplexity is 43.89240945442141
At time: 374.468759059906 and batch: 200, loss is 3.7313675260543824 and perplexity is 41.73614442076183
At time: 375.06554794311523 and batch: 250, loss is 3.6935521364212036 and perplexity is 40.18734465243925
At time: 375.6623251438141 and batch: 300, loss is 3.782827477455139 and perplexity is 43.94010591743944
At time: 376.25700521469116 and batch: 350, loss is 3.7410784578323364 and perplexity is 42.143415562501076
At time: 376.8565571308136 and batch: 400, loss is 3.8022500038146974 and perplexity is 44.80187556409975
At time: 377.4563000202179 and batch: 450, loss is 3.7664882135391236 and perplexity is 43.22799049408622
At time: 378.05740666389465 and batch: 500, loss is 3.7235332250595095 and perplexity is 41.41044836928656
At time: 378.65920209884644 and batch: 550, loss is 3.7391691780090333 and perplexity is 42.063028754357774
At time: 379.26079845428467 and batch: 600, loss is 3.8344446086883544 and perplexity is 46.26772383893973
At time: 379.85781621932983 and batch: 650, loss is 3.7618487310409545 and perplexity is 43.02789950698395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.5205526912913605 and perplexity of 91.88636874238341
Finished 46 epochs...
Completing Train Step...
At time: 380.9882628917694 and batch: 50, loss is 3.840572028160095 and perplexity is 46.55209593472702
At time: 381.6062672138214 and batch: 100, loss is 3.823817071914673 and perplexity is 45.778615514261084
At time: 382.2053599357605 and batch: 150, loss is 3.7815935897827146 and perplexity is 43.88592219760932
At time: 382.8238949775696 and batch: 200, loss is 3.7312321710586547 and perplexity is 41.73049560741825
At time: 383.41887307167053 and batch: 250, loss is 3.693445391654968 and perplexity is 40.1830550926768
At time: 384.0177309513092 and batch: 300, loss is 3.7827327108383177 and perplexity is 43.935942059559345
At time: 384.6169617176056 and batch: 350, loss is 3.7410060071945193 and perplexity is 42.1403623557685
At time: 385.2164535522461 and batch: 400, loss is 3.802211332321167 and perplexity is 44.800143042158545
At time: 385.81528520584106 and batch: 450, loss is 3.7664577388763427 and perplexity is 43.22667315572605
At time: 386.4155526161194 and batch: 500, loss is 3.7235242700576783 and perplexity is 41.41007754030598
At time: 387.0101761817932 and batch: 550, loss is 3.73920777797699 and perplexity is 42.06465241725631
At time: 387.61073756217957 and batch: 600, loss is 3.8344559049606324 and perplexity is 46.26824649469792
At time: 388.2093367576599 and batch: 650, loss is 3.7617890548706057 and perplexity is 43.02533184333815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520546109068627 and perplexity of 91.88576392782873
Finished 47 epochs...
Completing Train Step...
At time: 389.3579161167145 and batch: 50, loss is 3.840412983894348 and perplexity is 46.54469267954834
At time: 389.95561814308167 and batch: 100, loss is 3.8236690092086794 and perplexity is 45.771837910338974
At time: 390.5482008457184 and batch: 150, loss is 3.7814503622055056 and perplexity is 43.879636973418826
At time: 391.1445052623749 and batch: 200, loss is 3.731101016998291 and perplexity is 41.72502284237387
At time: 391.74122428894043 and batch: 250, loss is 3.693341555595398 and perplexity is 40.17888285919238
At time: 392.33952140808105 and batch: 300, loss is 3.7826406002044677 and perplexity is 43.93189527846609
At time: 392.9406371116638 and batch: 350, loss is 3.740934815406799 and perplexity is 42.13736241482407
At time: 393.54268169403076 and batch: 400, loss is 3.8021722555160524 and perplexity is 44.798392429904176
At time: 394.140349149704 and batch: 450, loss is 3.766424922943115 and perplexity is 43.225254655380944
At time: 394.74179220199585 and batch: 500, loss is 3.7235116147994995 and perplexity is 41.40955348839952
At time: 395.3437955379486 and batch: 550, loss is 3.739240598678589 and perplexity is 42.06603303131741
At time: 395.94492506980896 and batch: 600, loss is 3.8344619560241697 and perplexity is 46.26852646764429
At time: 396.54718542099 and batch: 650, loss is 3.761728649139404 and perplexity is 43.022732945202925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520541022805607 and perplexity of 91.88529657385409
Finished 48 epochs...
Completing Train Step...
At time: 397.667916059494 and batch: 50, loss is 3.8402582120895388 and perplexity is 46.53748943090231
At time: 398.2801055908203 and batch: 100, loss is 3.823525242805481 and perplexity is 45.76525793083616
At time: 398.87430238723755 and batch: 150, loss is 3.7813111686706544 and perplexity is 43.873529636701264
At time: 399.46882462501526 and batch: 200, loss is 3.730973606109619 and perplexity is 41.71970695879158
At time: 400.06331038475037 and batch: 250, loss is 3.6932399225234986 and perplexity is 40.17479956340438
At time: 400.65603017807007 and batch: 300, loss is 3.7825506210327147 and perplexity is 43.92794250075189
At time: 401.24831366539 and batch: 350, loss is 3.7408647680282594 and perplexity is 42.134410906422254
At time: 401.8443269729614 and batch: 400, loss is 3.8021329975128175 and perplexity is 44.79663376899023
At time: 402.44183015823364 and batch: 450, loss is 3.7663899183273317 and perplexity is 43.223741598431744
At time: 403.0387861728668 and batch: 500, loss is 3.723495683670044 and perplexity is 41.40889379269706
At time: 403.63190817832947 and batch: 550, loss is 3.739268536567688 and perplexity is 42.06720828390004
At time: 404.22800183296204 and batch: 600, loss is 3.83446391582489 and perplexity is 46.268617144824645
At time: 404.8212559223175 and batch: 650, loss is 3.761667613983154 and perplexity is 43.02010712610977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520536834118413 and perplexity of 91.88491169589513
Finished 49 epochs...
Completing Train Step...
At time: 405.9679923057556 and batch: 50, loss is 3.8401071643829345 and perplexity is 46.530460580711875
At time: 406.56172943115234 and batch: 100, loss is 3.8233848571777345 and perplexity is 45.75883359732515
At time: 407.15619587898254 and batch: 150, loss is 3.7811754751205444 and perplexity is 43.86757668560667
At time: 407.7499017715454 and batch: 200, loss is 3.7308491706848144 and perplexity is 41.71451587231766
At time: 408.341730594635 and batch: 250, loss is 3.693140091896057 and perplexity is 40.17078908814409
At time: 408.93275213241577 and batch: 300, loss is 3.7824623441696166 and perplexity is 43.92406485094147
At time: 409.52597999572754 and batch: 350, loss is 3.7407954502105714 and perplexity is 42.131490342233384
At time: 410.11841797828674 and batch: 400, loss is 3.8020932865142822 and perplexity is 44.7948548852531
At time: 410.71411538124084 and batch: 450, loss is 3.7663529825210573 and perplexity is 43.22214512416932
At time: 411.3102037906647 and batch: 500, loss is 3.72347731590271 and perplexity is 41.40813321075544
At time: 411.9043638706207 and batch: 550, loss is 3.7392923164367677 and perplexity is 42.0682086484998
At time: 412.49836349487305 and batch: 600, loss is 3.8344621801376344 and perplexity is 46.268536837045225
At time: 413.1160337924957 and batch: 650, loss is 3.7616060066223143 and perplexity is 43.0174568524857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.520533543007047 and perplexity of 91.88460929291544
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f28e0412ac8>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.3121505403317403, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 5.053100222232462, 'seq_len': 20, 'anneal': 5.493598875085108, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8239936828613281 and batch: 50, loss is 6.6636753845214844 and perplexity is 783.4250404413782
At time: 1.4025428295135498 and batch: 100, loss is 5.88986891746521 and perplexity is 361.3579135563353
At time: 1.9634525775909424 and batch: 150, loss is 5.544065666198731 and perplexity is 255.71554291601123
At time: 2.5436995029449463 and batch: 200, loss is 5.381171388626099 and perplexity is 217.2766418018449
At time: 3.1081509590148926 and batch: 250, loss is 5.267381839752197 and perplexity is 193.90761605751516
At time: 3.671187162399292 and batch: 300, loss is 5.241122303009033 and perplexity is 188.88196629481249
At time: 4.229755640029907 and batch: 350, loss is 5.1188240814208985 and perplexity is 167.13871250006818
At time: 4.788350820541382 and batch: 400, loss is 5.118359603881836 and perplexity is 167.06109834861448
At time: 5.347292423248291 and batch: 450, loss is 5.047693357467652 and perplexity is 155.6629911814006
At time: 5.908269166946411 and batch: 500, loss is 5.023789396286011 and perplexity is 151.98614969790057
At time: 6.46738600730896 and batch: 550, loss is 5.021594142913818 and perplexity is 151.6528675434807
At time: 7.030231237411499 and batch: 600, loss is 5.044461612701416 and perplexity is 155.16074013508168
At time: 7.593311071395874 and batch: 650, loss is 4.967444353103637 and perplexity is 143.65927544643716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.884532255284927 and perplexity of 132.22860163761655
Finished 1 epochs...
Completing Train Step...
At time: 8.683210134506226 and batch: 50, loss is 4.780458002090454 and perplexity is 119.1589125800404
At time: 9.23520541191101 and batch: 100, loss is 4.702883987426758 and perplexity is 110.26471639135258
At time: 9.792295694351196 and batch: 150, loss is 4.608997764587403 and perplexity is 100.38349131331276
At time: 10.348894834518433 and batch: 200, loss is 4.539035587310791 and perplexity is 93.60048707949746
At time: 10.90664029121399 and batch: 250, loss is 4.502500085830689 and perplexity is 90.24246341244708
At time: 11.463536739349365 and batch: 300, loss is 4.566471042633057 and perplexity is 96.20401017686777
At time: 12.020397186279297 and batch: 350, loss is 4.492565383911133 and perplexity is 89.3503701157805
At time: 12.57405686378479 and batch: 400, loss is 4.528471622467041 and perplexity is 92.61689926530939
At time: 13.129860162734985 and batch: 450, loss is 4.47700909614563 and perplexity is 87.97116551178652
At time: 13.69701337814331 and batch: 500, loss is 4.440029983520508 and perplexity is 84.77748356313742
At time: 14.273939609527588 and batch: 550, loss is 4.456408472061157 and perplexity is 86.17744392820211
At time: 14.859244585037231 and batch: 600, loss is 4.515110635757447 and perplexity is 91.38767620788248
At time: 15.456613063812256 and batch: 650, loss is 4.408032093048096 and perplexity is 82.10772405133596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.578210568895527 and perplexity of 97.34005494749336
Finished 2 epochs...
Completing Train Step...
At time: 16.596020221710205 and batch: 50, loss is 4.377893533706665 and perplexity is 79.67003428315485
At time: 17.209293127059937 and batch: 100, loss is 4.342786350250244 and perplexity is 76.92157145547384
At time: 17.806997537612915 and batch: 150, loss is 4.281494159698486 and perplexity is 72.34845944043076
At time: 18.40637707710266 and batch: 200, loss is 4.215881395339966 and perplexity is 67.75385749536885
At time: 19.003766298294067 and batch: 250, loss is 4.194853553771972 and perplexity is 66.34401503508265
At time: 19.601778984069824 and batch: 300, loss is 4.289370174407959 and perplexity is 72.92052681955249
At time: 20.198662757873535 and batch: 350, loss is 4.239660692214966 and perplexity is 69.38430520928706
At time: 20.80161690711975 and batch: 400, loss is 4.287129621505738 and perplexity is 72.75732741827431
At time: 21.40228581428528 and batch: 450, loss is 4.241351518630982 and perplexity is 69.5017212625007
At time: 22.004321098327637 and batch: 500, loss is 4.204739570617676 and perplexity is 67.0031458060387
At time: 22.606295824050903 and batch: 550, loss is 4.229952840805054 and perplexity is 68.71399160091023
At time: 23.206931591033936 and batch: 600, loss is 4.302799587249756 and perplexity is 73.90641179008199
At time: 23.80346918106079 and batch: 650, loss is 4.198627676963806 and perplexity is 66.59487861812416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.4846454695159315 and perplexity of 88.64551770299025
Finished 3 epochs...
Completing Train Step...
At time: 24.952837228775024 and batch: 50, loss is 4.188005900382995 and perplexity is 65.89126611864378
At time: 25.550941467285156 and batch: 100, loss is 4.158907532691956 and perplexity is 64.00156477639494
At time: 26.150548458099365 and batch: 150, loss is 4.1093748712539675 and perplexity is 60.90862993057248
At time: 26.748896598815918 and batch: 200, loss is 4.045453243255615 and perplexity is 57.13707716108116
At time: 27.3442645072937 and batch: 250, loss is 4.023426914215088 and perplexity is 55.892316181764315
At time: 27.94434905052185 and batch: 300, loss is 4.132759318351746 and perplexity is 62.34972854276336
At time: 28.54348111152649 and batch: 350, loss is 4.089641761779785 and perplexity is 59.71849442455581
At time: 29.143106698989868 and batch: 400, loss is 4.140506997108459 and perplexity is 62.834670371052916
At time: 29.741352558135986 and batch: 450, loss is 4.095828189849853 and perplexity is 60.089083724862284
At time: 30.339881420135498 and batch: 500, loss is 4.05912712097168 and perplexity is 57.9237285980422
At time: 30.93501591682434 and batch: 550, loss is 4.089551205635071 and perplexity is 59.71308679278341
At time: 31.535045385360718 and batch: 600, loss is 4.168655924797058 and perplexity is 64.62852810226417
At time: 32.15005922317505 and batch: 650, loss is 4.063815007209778 and perplexity is 58.1959059178988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.446257348154106 and perplexity of 85.3070711219686
Finished 4 epochs...
Completing Train Step...
At time: 33.27668786048889 and batch: 50, loss is 4.061638722419739 and perplexity is 58.069392767262656
At time: 33.88659191131592 and batch: 100, loss is 4.03715350151062 and perplexity is 56.664816707477684
At time: 34.479036808013916 and batch: 150, loss is 3.993872661590576 and perplexity is 54.264631525219855
At time: 35.077396631240845 and batch: 200, loss is 3.931604256629944 and perplexity is 50.988711068782386
At time: 35.67406892776489 and batch: 250, loss is 3.912223720550537 and perplexity is 50.010036763350946
At time: 36.27096676826477 and batch: 300, loss is 4.023198614120483 and perplexity is 55.87955741716132
At time: 36.86687684059143 and batch: 350, loss is 3.9819499540328978 and perplexity is 53.621491794840715
At time: 37.4630241394043 and batch: 400, loss is 4.0352710294723515 and perplexity is 56.55824711308395
At time: 38.05602407455444 and batch: 450, loss is 3.9906857204437256 and perplexity is 54.091968617594766
At time: 38.6525604724884 and batch: 500, loss is 3.9568284225463866 and perplexity is 52.29121702973323
At time: 39.24858903884888 and batch: 550, loss is 3.988486680984497 and perplexity is 53.9731489366429
At time: 39.84420967102051 and batch: 600, loss is 4.071538953781128 and perplexity is 58.647148429002534
At time: 40.44302153587341 and batch: 650, loss is 3.9658879041671753 and perplexity is 52.76710072432487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.436419617895987 and perplexity of 84.47195771263247
Finished 5 epochs...
Completing Train Step...
At time: 41.58190941810608 and batch: 50, loss is 3.967917590141296 and perplexity is 52.87431013247085
At time: 42.175469398498535 and batch: 100, loss is 3.9461366653442385 and perplexity is 51.73511021053213
At time: 42.7676157951355 and batch: 150, loss is 3.90508180141449 and perplexity is 49.65414152506407
At time: 43.36089205741882 and batch: 200, loss is 3.8458198070526124 and perplexity is 46.7970331672812
At time: 43.95468735694885 and batch: 250, loss is 3.8283261680603027 and perplexity is 45.985501777521556
At time: 44.549429416656494 and batch: 300, loss is 3.9423639440536498 and perplexity is 51.540295780117575
At time: 45.13916063308716 and batch: 350, loss is 3.89887423992157 and perplexity is 49.346865093551635
At time: 45.73228168487549 and batch: 400, loss is 3.956731390953064 and perplexity is 52.28614337578441
At time: 46.32756495475769 and batch: 450, loss is 3.911395630836487 and perplexity is 49.96864110833153
At time: 46.9256112575531 and batch: 500, loss is 3.8794034481048585 and perplexity is 48.39533612882401
At time: 47.53622055053711 and batch: 550, loss is 3.9099426460266113 and perplexity is 49.89609015231345
At time: 48.135174036026 and batch: 600, loss is 3.9978883838653565 and perplexity is 54.48298133744461
At time: 48.73054766654968 and batch: 650, loss is 3.8889218091964723 and perplexity is 48.85817967481546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.437140969669118 and perplexity of 84.53291369181697
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 49.86160445213318 and batch: 50, loss is 3.9010181999206544 and perplexity is 49.45277629251047
At time: 50.47100520133972 and batch: 100, loss is 3.8634542417526245 and perplexity is 47.62959166689565
At time: 51.06688570976257 and batch: 150, loss is 3.8082030963897706 and perplexity is 45.069380728357224
At time: 51.66262078285217 and batch: 200, loss is 3.7375824546813963 and perplexity is 41.996339288259904
At time: 52.255871534347534 and batch: 250, loss is 3.706764302253723 and perplexity is 40.721829591167946
At time: 52.85181927680969 and batch: 300, loss is 3.8063658714294433 and perplexity is 44.98665415406975
At time: 53.448025703430176 and batch: 350, loss is 3.7485190296173094 and perplexity is 42.45815614414632
At time: 54.044052600860596 and batch: 400, loss is 3.7925654363632204 and perplexity is 44.37008301512315
At time: 54.64107322692871 and batch: 450, loss is 3.7317694997787476 and perplexity is 41.75292462655025
At time: 55.23698616027832 and batch: 500, loss is 3.6796250629425047 and perplexity is 39.63153195659185
At time: 55.83153796195984 and batch: 550, loss is 3.6836350440979 and perplexity is 39.79077271573703
At time: 56.426767110824585 and batch: 600, loss is 3.750382685661316 and perplexity is 42.53735732241547
At time: 57.023242712020874 and batch: 650, loss is 3.6275647258758545 and perplexity is 37.62108730906437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3546914493336395 and perplexity of 77.8428031821995
Finished 7 epochs...
Completing Train Step...
At time: 58.16361093521118 and batch: 50, loss is 3.8127134323120115 and perplexity is 45.273117891384516
At time: 58.75752520561218 and batch: 100, loss is 3.7884637546539306 and perplexity is 44.18846378394659
At time: 59.35012221336365 and batch: 150, loss is 3.738958172798157 and perplexity is 42.054154172429804
At time: 59.94246244430542 and batch: 200, loss is 3.6759686851501465 and perplexity is 39.486888699610745
At time: 60.536142110824585 and batch: 250, loss is 3.6498731565475464 and perplexity is 38.46978609905986
At time: 61.13180661201477 and batch: 300, loss is 3.7542371463775637 and perplexity is 42.70163228750328
At time: 61.72895622253418 and batch: 350, loss is 3.702460694313049 and perplexity is 40.54695536693495
At time: 62.325052976608276 and batch: 400, loss is 3.753878493309021 and perplexity is 42.686319962121964
At time: 62.934775829315186 and batch: 450, loss is 3.6982812309265136 and perplexity is 40.377844494061854
At time: 63.530080795288086 and batch: 500, loss is 3.65467369556427 and perplexity is 38.654905789833066
At time: 64.1285240650177 and batch: 550, loss is 3.6665694379806517 and perplexity is 39.11748047202378
At time: 64.7275550365448 and batch: 600, loss is 3.74343909740448 and perplexity is 42.24301849403996
At time: 65.32734513282776 and batch: 650, loss is 3.6275860261917114 and perplexity is 37.6218886586414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3502637077780335 and perplexity of 77.49889729279674
Finished 8 epochs...
Completing Train Step...
At time: 66.45604419708252 and batch: 50, loss is 3.776800208091736 and perplexity is 43.67606358967807
At time: 67.06502604484558 and batch: 100, loss is 3.7536080741882323 and perplexity is 42.674778325618036
At time: 67.66047406196594 and batch: 150, loss is 3.7054417085647584 and perplexity is 40.66800675706374
At time: 68.25589847564697 and batch: 200, loss is 3.6443632650375366 and perplexity is 38.25840463049581
At time: 68.85246682167053 and batch: 250, loss is 3.6195867443084717 and perplexity is 37.322141047464115
At time: 69.44850826263428 and batch: 300, loss is 3.724937481880188 and perplexity is 41.46864012237704
At time: 70.04261422157288 and batch: 350, loss is 3.676174273490906 and perplexity is 39.49500757808504
At time: 70.63588118553162 and batch: 400, loss is 3.7306524562835692 and perplexity is 41.70631083335573
At time: 71.23379063606262 and batch: 450, loss is 3.677751398086548 and perplexity is 39.557345270174054
At time: 71.82888221740723 and batch: 500, loss is 3.6381551742553713 and perplexity is 38.02162870504158
At time: 72.42499780654907 and batch: 550, loss is 3.6534860610961912 and perplexity is 38.609025141469225
At time: 73.02138996124268 and batch: 600, loss is 3.7344253301620483 and perplexity is 41.86396069357987
At time: 73.61657547950745 and batch: 650, loss is 3.620095720291138 and perplexity is 37.3411419559718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.349790984509038 and perplexity of 77.4622704185949
Finished 9 epochs...
Completing Train Step...
At time: 74.7562506198883 and batch: 50, loss is 3.7499099826812743 and perplexity is 42.51725453854332
At time: 75.34703302383423 and batch: 100, loss is 3.727504601478577 and perplexity is 41.57523183945918
At time: 75.93792915344238 and batch: 150, loss is 3.680392427444458 and perplexity is 39.661955458838285
At time: 76.53123712539673 and batch: 200, loss is 3.620618963241577 and perplexity is 37.36068555784167
At time: 77.12795901298523 and batch: 250, loss is 3.5965377378463743 and perplexity is 36.471740864998445
At time: 77.72006058692932 and batch: 300, loss is 3.7023595666885374 and perplexity is 40.54285515698325
At time: 78.33060646057129 and batch: 350, loss is 3.655760951042175 and perplexity is 38.69695640363865
At time: 78.92551970481873 and batch: 400, loss is 3.7119988012313843 and perplexity is 40.93554682987807
At time: 79.5205888748169 and batch: 450, loss is 3.6610493087768554 and perplexity is 38.90214182109231
At time: 80.11818790435791 and batch: 500, loss is 3.6238567447662353 and perplexity is 37.48184733712745
At time: 80.71717715263367 and batch: 550, loss is 3.6410220766067507 and perplexity is 38.1307894035438
At time: 81.31322956085205 and batch: 600, loss is 3.724333758354187 and perplexity is 41.44361208451173
At time: 81.9180657863617 and batch: 650, loss is 3.6102770519256593 and perplexity is 36.976295749785784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.35085999731924 and perplexity of 77.54512285524754
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 83.05620050430298 and batch: 50, loss is 3.74167377948761 and perplexity is 42.16851191987394
At time: 83.68022036552429 and batch: 100, loss is 3.7203847312927247 and perplexity is 41.28027286663139
At time: 84.2841546535492 and batch: 150, loss is 3.6685197067260744 and perplexity is 39.19384451258431
At time: 84.88420486450195 and batch: 200, loss is 3.6067322158813475 and perplexity is 36.845452889083944
At time: 85.48687076568604 and batch: 250, loss is 3.575170283317566 and perplexity is 35.700699531644894
At time: 86.08875393867493 and batch: 300, loss is 3.679083185195923 and perplexity is 39.610062328843725
At time: 86.69223070144653 and batch: 350, loss is 3.627983498573303 and perplexity is 37.636845292554064
At time: 87.29516458511353 and batch: 400, loss is 3.6801780891418456 and perplexity is 39.65345529361499
At time: 87.89819240570068 and batch: 450, loss is 3.6235095977783205 and perplexity is 37.46883788494959
At time: 88.49735498428345 and batch: 500, loss is 3.5815087795257567 and perplexity is 35.92770696309726
At time: 89.10190033912659 and batch: 550, loss is 3.5937745475769045 and perplexity is 36.37110161226516
At time: 89.7036304473877 and batch: 600, loss is 3.6699603605270386 and perplexity is 39.25034996628108
At time: 90.30704474449158 and batch: 650, loss is 3.551638083457947 and perplexity is 34.870391340199355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.337828093884038 and perplexity of 76.54111855504453
Finished 11 epochs...
Completing Train Step...
At time: 91.45740532875061 and batch: 50, loss is 3.7249572801589967 and perplexity is 41.46946113820333
At time: 92.05409693717957 and batch: 100, loss is 3.7022568130493165 and perplexity is 40.53868944509616
At time: 92.65329194068909 and batch: 150, loss is 3.6530096101760865 and perplexity is 38.590634217450464
At time: 93.25158333778381 and batch: 200, loss is 3.5928789138793946 and perplexity is 36.3385410114079
At time: 93.86494421958923 and batch: 250, loss is 3.562579855918884 and perplexity is 35.25403024516422
At time: 94.46327137947083 and batch: 300, loss is 3.668804407119751 and perplexity is 39.20500460411259
At time: 95.06367063522339 and batch: 350, loss is 3.6190648221969606 and perplexity is 37.30266687924407
At time: 95.65867590904236 and batch: 400, loss is 3.673426613807678 and perplexity is 39.38663768804223
At time: 96.25818538665771 and batch: 450, loss is 3.61839813709259 and perplexity is 37.277806034980486
At time: 96.85755705833435 and batch: 500, loss is 3.5785821056365967 and perplexity is 35.82271199918448
At time: 97.45584344863892 and batch: 550, loss is 3.5933689498901367 and perplexity is 36.35635256887716
At time: 98.05417013168335 and batch: 600, loss is 3.6723066425323485 and perplexity is 39.342550478012576
At time: 98.65508008003235 and batch: 650, loss is 3.5548406219482422 and perplexity is 34.982244121457434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.336737539253983 and perplexity of 76.45769178280945
Finished 12 epochs...
Completing Train Step...
At time: 99.77487230300903 and batch: 50, loss is 3.717688536643982 and perplexity is 41.16912312383387
At time: 100.37705397605896 and batch: 100, loss is 3.6944394063949586 and perplexity is 40.22301750005731
At time: 100.96597671508789 and batch: 150, loss is 3.645449147224426 and perplexity is 38.299971314759475
At time: 101.55568861961365 and batch: 200, loss is 3.5856070375442504 and perplexity is 36.07525010466866
At time: 102.14775848388672 and batch: 250, loss is 3.5556630182266233 and perplexity is 35.01102522194516
At time: 102.73664951324463 and batch: 300, loss is 3.6627149248123168 and perplexity is 38.96699184495047
At time: 103.32852149009705 and batch: 350, loss is 3.6138018703460695 and perplexity is 37.10686045141413
At time: 103.92321825027466 and batch: 400, loss is 3.6691552734375 and perplexity is 39.218762733206276
At time: 104.51991748809814 and batch: 450, loss is 3.615094156265259 and perplexity is 37.15484412231421
At time: 105.11563110351562 and batch: 500, loss is 3.5765026569366456 and perplexity is 35.74829790425007
At time: 105.71282172203064 and batch: 550, loss is 3.5924911737442016 and perplexity is 36.32445383186236
At time: 106.31060767173767 and batch: 600, loss is 3.672685899734497 and perplexity is 39.35747425342803
At time: 106.9088921546936 and batch: 650, loss is 3.555317549705505 and perplexity is 34.99893210385499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.336415010340073 and perplexity of 76.43303594284308
Finished 13 epochs...
Completing Train Step...
At time: 108.05049300193787 and batch: 50, loss is 3.7115318679809572 and perplexity is 40.91643712376458
At time: 108.64233231544495 and batch: 100, loss is 3.68818829536438 and perplexity is 39.97236320182535
At time: 109.24963402748108 and batch: 150, loss is 3.6394469594955443 and perplexity is 38.07077622099354
At time: 109.84207248687744 and batch: 200, loss is 3.5799415922164917 and perplexity is 35.871445614244834
At time: 110.4349513053894 and batch: 250, loss is 3.550202989578247 and perplexity is 34.82038494552209
At time: 111.03366804122925 and batch: 300, loss is 3.657783236503601 and perplexity is 38.77529187762326
At time: 111.63039827346802 and batch: 350, loss is 3.60952344417572 and perplexity is 36.94844062398234
At time: 112.22621202468872 and batch: 400, loss is 3.665514750480652 and perplexity is 39.07624550316392
At time: 112.82345509529114 and batch: 450, loss is 3.6121382331848144 and perplexity is 37.045179421082906
At time: 113.41908812522888 and batch: 500, loss is 3.5744121360778807 and perplexity is 35.673643402400955
At time: 114.01304268836975 and batch: 550, loss is 3.591173028945923 and perplexity is 36.276604485105544
At time: 114.61098432540894 and batch: 600, loss is 3.67219313621521 and perplexity is 39.33808510342988
At time: 115.20696544647217 and batch: 650, loss is 3.5547747230529785 and perplexity is 34.979938906172386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.336374320235907 and perplexity of 76.42992593792248
Finished 14 epochs...
Completing Train Step...
At time: 116.32879185676575 and batch: 50, loss is 3.7060168170928955 and perplexity is 40.691402001329486
At time: 116.93221545219421 and batch: 100, loss is 3.6827113485336302 and perplexity is 39.75403512526739
At time: 117.52177429199219 and batch: 150, loss is 3.6342112588882447 and perplexity is 37.87196993406044
At time: 118.11586737632751 and batch: 200, loss is 3.575018038749695 and perplexity is 35.69526470779371
At time: 118.70847845077515 and batch: 250, loss is 3.5454160022735595 and perplexity is 34.65409852786975
At time: 119.3025631904602 and batch: 300, loss is 3.6534105920791626 and perplexity is 38.606111466240854
At time: 119.8974187374115 and batch: 350, loss is 3.605703549385071 and perplexity is 36.80757069356991
At time: 120.49396228790283 and batch: 400, loss is 3.6621530723571776 and perplexity is 38.94510429427582
At time: 121.08921027183533 and batch: 450, loss is 3.6093067121505737 and perplexity is 36.94043358134253
At time: 121.68951106071472 and batch: 500, loss is 3.5722512722015383 and perplexity is 35.59664074113233
At time: 122.28835105895996 and batch: 550, loss is 3.589589319229126 and perplexity is 36.219198343420814
At time: 122.88612747192383 and batch: 600, loss is 3.6712384033203125 and perplexity is 39.30054566248041
At time: 123.48417091369629 and batch: 650, loss is 3.5537271547317504 and perplexity is 34.94331421707715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.336479037415748 and perplexity of 76.43792988329007
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 124.62890982627869 and batch: 50, loss is 3.7052323055267333 and perplexity is 40.65949164447473
At time: 125.22640442848206 and batch: 100, loss is 3.6835475015640258 and perplexity is 39.7872894831364
At time: 125.82280802726746 and batch: 150, loss is 3.6348926067352294 and perplexity is 37.89778271197781
At time: 126.41915726661682 and batch: 200, loss is 3.5748344755172727 and perplexity is 35.68871297096917
At time: 127.0162558555603 and batch: 250, loss is 3.5448277950286866 and perplexity is 34.633720729822876
At time: 127.61558365821838 and batch: 300, loss is 3.649244146347046 and perplexity is 38.44559581995622
At time: 128.21086192131042 and batch: 350, loss is 3.6016716718673707 and perplexity is 36.659465847741515
At time: 128.80938959121704 and batch: 400, loss is 3.6556986665725706 and perplexity is 38.694546259291805
At time: 129.4088716506958 and batch: 450, loss is 3.6022811555862426 and perplexity is 36.681816005655065
At time: 130.00571131706238 and batch: 500, loss is 3.563482975959778 and perplexity is 35.28588324777841
At time: 130.6037471294403 and batch: 550, loss is 3.57757661819458 and perplexity is 35.78671081453296
At time: 131.20312976837158 and batch: 600, loss is 3.6571973180770874 and perplexity is 38.75257937410564
At time: 131.79807329177856 and batch: 650, loss is 3.5386265611648557 and perplexity is 34.419613478700185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.333744123870251 and perplexity of 76.22916436141554
Finished 16 epochs...
Completing Train Step...
At time: 132.926100730896 and batch: 50, loss is 3.7024734163284303 and perplexity is 40.54747120920606
At time: 133.53610610961914 and batch: 100, loss is 3.6790762519836426 and perplexity is 39.60978770482518
At time: 134.13343048095703 and batch: 150, loss is 3.6305674648284914 and perplexity is 37.7342233874155
At time: 134.7330391407013 and batch: 200, loss is 3.57101655960083 and perplexity is 35.552716242912375
At time: 135.32896852493286 and batch: 250, loss is 3.5414322137832643 and perplexity is 34.5163185543697
At time: 135.93006658554077 and batch: 300, loss is 3.6467330694198608 and perplexity is 38.34917707944225
At time: 136.5289707183838 and batch: 350, loss is 3.5993607807159425 and perplexity is 36.574847621909186
At time: 137.1288845539093 and batch: 400, loss is 3.6539682483673097 and perplexity is 38.62764641105152
At time: 137.72943305969238 and batch: 450, loss is 3.6009868955612183 and perplexity is 36.6343709073237
At time: 138.33049249649048 and batch: 500, loss is 3.5628725337982177 and perplexity is 35.26434983005778
At time: 138.92853093147278 and batch: 550, loss is 3.5781973695755003 and perplexity is 35.80893236100395
At time: 139.54716873168945 and batch: 600, loss is 3.658787569999695 and perplexity is 38.81425476466794
At time: 140.15508890151978 and batch: 650, loss is 3.5403757429122926 and perplexity is 34.47987232482366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.333384495155484 and perplexity of 76.20175509388373
Finished 17 epochs...
Completing Train Step...
At time: 141.30815362930298 and batch: 50, loss is 3.701039958000183 and perplexity is 40.489389737541984
At time: 141.9068820476532 and batch: 100, loss is 3.677076845169067 and perplexity is 39.53067074521504
At time: 142.50394916534424 and batch: 150, loss is 3.628632912635803 and perplexity is 37.66129512732916
At time: 143.10222339630127 and batch: 200, loss is 3.569182906150818 and perplexity is 35.48758461478284
At time: 143.70217394828796 and batch: 250, loss is 3.539762382507324 and perplexity is 34.45873022089878
At time: 144.30040335655212 and batch: 300, loss is 3.645354790687561 and perplexity is 38.29635763259412
At time: 144.89891147613525 and batch: 350, loss is 3.5981728267669677 and perplexity is 36.531424184872066
At time: 145.4982213973999 and batch: 400, loss is 3.6531108093261717 and perplexity is 38.5945397544497
At time: 146.09780526161194 and batch: 450, loss is 3.6003714036941528 and perplexity is 36.61182968765548
At time: 146.69857263565063 and batch: 500, loss is 3.562628593444824 and perplexity is 35.25574848124873
At time: 147.30138158798218 and batch: 550, loss is 3.5784563779830934 and perplexity is 35.81820837678356
At time: 147.90228748321533 and batch: 600, loss is 3.6594792127609255 and perplexity is 38.841109648931095
At time: 148.5049970149994 and batch: 650, loss is 3.5410459423065186 and perplexity is 34.50298845971329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.333280675551471 and perplexity of 76.19384426850125
Finished 18 epochs...
Completing Train Step...
At time: 149.63899397850037 and batch: 50, loss is 3.699770064353943 and perplexity is 40.43800515215876
At time: 150.25158834457397 and batch: 100, loss is 3.6755600500106813 and perplexity is 39.47075626570411
At time: 150.85012936592102 and batch: 150, loss is 3.627192440032959 and perplexity is 37.60708411762156
At time: 151.44995069503784 and batch: 200, loss is 3.56779643535614 and perplexity is 35.43841620829696
At time: 152.04829382896423 and batch: 250, loss is 3.53850004196167 and perplexity is 34.415259012096534
At time: 152.64776515960693 and batch: 300, loss is 3.6442694330215453 and perplexity is 38.2548149356775
At time: 153.24504113197327 and batch: 350, loss is 3.5972635841369627 and perplexity is 36.498223352761656
At time: 153.84358143806458 and batch: 400, loss is 3.652423424720764 and perplexity is 38.56801957779451
At time: 154.44305610656738 and batch: 450, loss is 3.5998712968826294 and perplexity is 36.59352443991741
At time: 155.05646181106567 and batch: 500, loss is 3.562386894226074 and perplexity is 35.2472282240956
At time: 155.6558916568756 and batch: 550, loss is 3.57851037979126 and perplexity is 35.82014267702858
At time: 156.25586652755737 and batch: 600, loss is 3.659786467552185 and perplexity is 38.85304559956367
At time: 156.85214376449585 and batch: 650, loss is 3.5413072443008424 and perplexity is 34.5120053374207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.333253748276654 and perplexity of 76.19179260354026
Finished 19 epochs...
Completing Train Step...
At time: 157.99961256980896 and batch: 50, loss is 3.69858971118927 and perplexity is 40.39030218351778
At time: 158.5882866382599 and batch: 100, loss is 3.674251494407654 and perplexity is 39.419140364940205
At time: 159.1785032749176 and batch: 150, loss is 3.6259610176086428 and perplexity is 37.56080241293894
At time: 159.76583695411682 and batch: 200, loss is 3.5666071128845216 and perplexity is 35.396293557214776
At time: 160.35389637947083 and batch: 250, loss is 3.5374073076248167 and perplexity is 34.377672816465626
At time: 160.9419493675232 and batch: 300, loss is 3.643309588432312 and perplexity is 38.21811387502435
At time: 161.5366508960724 and batch: 350, loss is 3.5964669942855836 and perplexity is 36.46916081544344
At time: 162.12951683998108 and batch: 400, loss is 3.651792593002319 and perplexity is 38.5436973201596
At time: 162.7206928730011 and batch: 450, loss is 3.5993959140777587 and perplexity is 36.57613264183747
At time: 163.31299114227295 and batch: 500, loss is 3.5621150064468385 and perplexity is 35.237646236161275
At time: 163.90438151359558 and batch: 550, loss is 3.5784514284133913 and perplexity is 35.81803109250333
At time: 164.49268507957458 and batch: 600, loss is 3.6599033498764038 and perplexity is 38.85758709924168
At time: 165.0877845287323 and batch: 650, loss is 3.5413783979415894 and perplexity is 34.5144610796164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.333260031307445 and perplexity of 76.19227132042309
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 166.20913457870483 and batch: 50, loss is 3.6984777736663816 and perplexity is 40.38578124617864
At time: 166.8135426044464 and batch: 100, loss is 3.6743293476104735 and perplexity is 39.422209390735205
At time: 167.40456247329712 and batch: 150, loss is 3.6259070873260497 and perplexity is 37.55877680287174
At time: 167.99690914154053 and batch: 200, loss is 3.5664081001281738 and perplexity is 35.38924994417713
At time: 168.59348392486572 and batch: 250, loss is 3.537359161376953 and perplexity is 34.376017700353366
At time: 169.19000697135925 and batch: 300, loss is 3.642395844459534 and perplexity is 38.18320825364931
At time: 169.78520846366882 and batch: 350, loss is 3.5957519245147704 and perplexity is 36.44309214254816
At time: 170.39757657051086 and batch: 400, loss is 3.650183472633362 and perplexity is 38.481725744940555
At time: 170.99589705467224 and batch: 450, loss is 3.5968792247772217 and perplexity is 36.48419761463664
At time: 171.59004855155945 and batch: 500, loss is 3.5592132234573364 and perplexity is 35.13554244688827
At time: 172.18877744674683 and batch: 550, loss is 3.5748044776916506 and perplexity is 35.68764240323822
At time: 172.7878201007843 and batch: 600, loss is 3.6555837869644163 and perplexity is 38.69010130030229
At time: 173.3916175365448 and batch: 650, loss is 3.537585587501526 and perplexity is 34.38380221009444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332923739564185 and perplexity of 76.16665279656829
Finished 21 epochs...
Completing Train Step...
At time: 174.54047298431396 and batch: 50, loss is 3.698031101226807 and perplexity is 40.36774605895567
At time: 175.13347673416138 and batch: 100, loss is 3.6737866497039793 and perplexity is 39.40082084451405
At time: 175.73057961463928 and batch: 150, loss is 3.6253233098983766 and perplexity is 37.53685723546006
At time: 176.32814598083496 and batch: 200, loss is 3.5658075284957884 and perplexity is 35.36800254550035
At time: 176.92407631874084 and batch: 250, loss is 3.5368195247650145 and perplexity is 34.357472147009446
At time: 177.52016925811768 and batch: 300, loss is 3.6419955348968505 and perplexity is 38.167926209229776
At time: 178.1170883178711 and batch: 350, loss is 3.595325074195862 and perplexity is 36.42753971656023
At time: 178.7103033065796 and batch: 400, loss is 3.649900918006897 and perplexity is 38.470854091287315
At time: 179.3060622215271 and batch: 450, loss is 3.5968188667297363 and perplexity is 36.48199556616089
At time: 179.90299558639526 and batch: 500, loss is 3.5592778873443605 and perplexity is 35.13781452109537
At time: 180.50176239013672 and batch: 550, loss is 3.5750197887420656 and perplexity is 35.695327174289275
At time: 181.09998750686646 and batch: 600, loss is 3.655951056480408 and perplexity is 38.70431360479386
At time: 181.7001223564148 and batch: 650, loss is 3.5378504896163943 and perplexity is 34.39291175853315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332771750057445 and perplexity of 76.1550771442908
Finished 22 epochs...
Completing Train Step...
At time: 182.82932233810425 and batch: 50, loss is 3.6976828241348265 and perplexity is 40.35368934570615
At time: 183.462646484375 and batch: 100, loss is 3.6733676958084107 and perplexity is 39.38431717451246
At time: 184.0649721622467 and batch: 150, loss is 3.624895272254944 and perplexity is 37.52079348573717
At time: 184.65601205825806 and batch: 200, loss is 3.5653695487976074 and perplexity is 35.352515470180066
At time: 185.25099849700928 and batch: 250, loss is 3.5364090061187743 and perplexity is 34.3433706587154
At time: 185.85252261161804 and batch: 300, loss is 3.6417015981674195 and perplexity is 38.15670890250066
At time: 186.44019722938538 and batch: 350, loss is 3.595035982131958 and perplexity is 36.4170103259756
At time: 187.02854132652283 and batch: 400, loss is 3.649716672897339 and perplexity is 38.46376667749119
At time: 187.6160762310028 and batch: 450, loss is 3.5967732954025267 and perplexity is 36.48033307108503
At time: 188.2058494091034 and batch: 500, loss is 3.55932843208313 and perplexity is 35.139590597636555
At time: 188.79605174064636 and batch: 550, loss is 3.575180950164795 and perplexity is 35.70108034758381
At time: 189.38621544837952 and batch: 600, loss is 3.6562191915512083 and perplexity is 38.71469298013765
At time: 189.97606229782104 and batch: 650, loss is 3.5380384874343873 and perplexity is 34.3993781587137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332696054496017 and perplexity of 76.14931276114265
Finished 23 epochs...
Completing Train Step...
At time: 191.10935831069946 and batch: 50, loss is 3.6973768854141236 and perplexity is 40.341345477941914
At time: 191.69736886024475 and batch: 100, loss is 3.673010439872742 and perplexity is 39.37024940647604
At time: 192.28550481796265 and batch: 150, loss is 3.624542889595032 and perplexity is 37.507574137997906
At time: 192.87364983558655 and batch: 200, loss is 3.5650136852264405 and perplexity is 35.33993703601111
At time: 193.4614052772522 and batch: 250, loss is 3.5360710763931276 and perplexity is 34.3317669736164
At time: 194.0502052307129 and batch: 300, loss is 3.6414565086364745 and perplexity is 38.14735823853508
At time: 194.64266538619995 and batch: 350, loss is 3.5948115301132204 and perplexity is 36.40883737174389
At time: 195.24308967590332 and batch: 400, loss is 3.649574303627014 and perplexity is 38.45829100888809
At time: 195.84240245819092 and batch: 450, loss is 3.5967258739471437 and perplexity is 36.47860316161568
At time: 196.44071912765503 and batch: 500, loss is 3.5593586349487305 and perplexity is 35.140651929996196
At time: 197.03982019424438 and batch: 550, loss is 3.5752982330322265 and perplexity is 35.705267718206
At time: 197.64336585998535 and batch: 600, loss is 3.6564163160324097 and perplexity is 38.72232534614461
At time: 198.24727416038513 and batch: 650, loss is 3.5381748199462892 and perplexity is 34.40406823204342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332656261967678 and perplexity of 76.14628264774494
Finished 24 epochs...
Completing Train Step...
At time: 199.39208340644836 and batch: 50, loss is 3.6970957469940187 and perplexity is 40.330005569925945
At time: 200.01385188102722 and batch: 100, loss is 3.672690954208374 and perplexity is 39.357673185256246
At time: 200.60860538482666 and batch: 150, loss is 3.6242341566085816 and perplexity is 37.49599609997287
At time: 201.22340726852417 and batch: 200, loss is 3.5647060918807982 and perplexity is 35.32906837819252
At time: 201.8211658000946 and batch: 250, loss is 3.535779552459717 and perplexity is 34.32175990058883
At time: 202.42037844657898 and batch: 300, loss is 3.6412387752532958 and perplexity is 38.139053189342505
At time: 203.01967597007751 and batch: 350, loss is 3.594621891975403 and perplexity is 36.401933522261885
At time: 203.61817979812622 and batch: 400, loss is 3.6494516038894655 and perplexity is 38.45357247616202
At time: 204.21281385421753 and batch: 450, loss is 3.596672968864441 and perplexity is 36.47667330914849
At time: 204.81193804740906 and batch: 500, loss is 3.5593709897994996 and perplexity is 35.1410860901887
At time: 205.41000699996948 and batch: 550, loss is 3.5753826761245726 and perplexity is 35.70828290872939
At time: 206.00908422470093 and batch: 600, loss is 3.656563572883606 and perplexity is 38.72802789370538
At time: 206.60901761054993 and batch: 650, loss is 3.5382757902145388 and perplexity is 34.40754219542224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332635019339767 and perplexity of 76.1446651177762
Finished 25 epochs...
Completing Train Step...
At time: 207.7607147693634 and batch: 50, loss is 3.696831922531128 and perplexity is 40.319366931296344
At time: 208.35626912117004 and batch: 100, loss is 3.6723969078063963 and perplexity is 39.346101904396036
At time: 208.95318412780762 and batch: 150, loss is 3.6239535760879518 and perplexity is 37.485476929671755
At time: 209.5494999885559 and batch: 200, loss is 3.5644298219680786 and perplexity is 35.319309367677775
At time: 210.1462230682373 and batch: 250, loss is 3.535519299507141 and perplexity is 34.312828723470126
At time: 210.74319195747375 and batch: 300, loss is 3.641038098335266 and perplexity is 38.13140032959374
At time: 211.33459639549255 and batch: 350, loss is 3.594452729225159 and perplexity is 36.395776191883215
At time: 211.9301381111145 and batch: 400, loss is 3.6493386220932007 and perplexity is 38.44922816788936
At time: 212.5265302658081 and batch: 450, loss is 3.596614670753479 and perplexity is 36.474546849985266
At time: 213.1241295337677 and batch: 500, loss is 3.559368920326233 and perplexity is 35.14101336672573
At time: 213.7219443321228 and batch: 550, loss is 3.5754430437088014 and perplexity is 35.71043859657173
At time: 214.3208966255188 and batch: 600, loss is 3.6566748666763305 and perplexity is 38.73233832267195
At time: 214.91569590568542 and batch: 650, loss is 3.5383516120910645 and perplexity is 34.410151138744375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332623350854013 and perplexity of 76.14377663001973
Finished 26 epochs...
Completing Train Step...
At time: 216.05014848709106 and batch: 50, loss is 3.6965810012817384 and perplexity is 40.30925121454857
At time: 216.65853881835938 and batch: 100, loss is 3.672121329307556 and perplexity is 39.335260458601304
At time: 217.25173544883728 and batch: 150, loss is 3.623692684173584 and perplexity is 37.47569854744049
At time: 217.8456552028656 and batch: 200, loss is 3.564175190925598 and perplexity is 35.31031712001507
At time: 218.4353630542755 and batch: 250, loss is 3.5352809953689577 and perplexity is 34.30465280860841
At time: 219.02887797355652 and batch: 300, loss is 3.6408487653732298 and perplexity is 38.124181482027204
At time: 219.6251163482666 and batch: 350, loss is 3.5942962694168092 and perplexity is 36.39008216117059
At time: 220.22035956382751 and batch: 400, loss is 3.649230442047119 and perplexity is 38.44506895359041
At time: 220.81589698791504 and batch: 450, loss is 3.5965516567230225 and perplexity is 36.47224851419363
At time: 221.41202330589294 and batch: 500, loss is 3.5593558073043825 and perplexity is 35.140552564870866
At time: 222.00546765327454 and batch: 550, loss is 3.575485167503357 and perplexity is 35.71194288743366
At time: 222.59938192367554 and batch: 600, loss is 3.6567602729797364 and perplexity is 38.7356464497758
At time: 223.19602489471436 and batch: 650, loss is 3.538408570289612 and perplexity is 34.41211113478337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332616469439338 and perplexity of 76.14325265492063
Finished 27 epochs...
Completing Train Step...
At time: 224.33699703216553 and batch: 50, loss is 3.6963401556015016 and perplexity is 40.299544074528264
At time: 224.93024587631226 and batch: 100, loss is 3.6718598937988283 and perplexity is 39.32497816890875
At time: 225.5208809375763 and batch: 150, loss is 3.6234460210800172 and perplexity is 37.466455815670464
At time: 226.1119568347931 and batch: 200, loss is 3.563936109542847 and perplexity is 35.301876089659444
At time: 226.7049126625061 and batch: 250, loss is 3.5350584840774535 and perplexity is 34.297020485177924
At time: 227.30010652542114 and batch: 300, loss is 3.640667757987976 and perplexity is 38.117281348128635
At time: 227.8958752155304 and batch: 350, loss is 3.59414822101593 and perplexity is 36.384695066483985
At time: 228.4913148880005 and batch: 400, loss is 3.6491248035430908 and perplexity is 38.44100788852508
At time: 229.08390378952026 and batch: 450, loss is 3.596484375 and perplexity is 36.469794681021085
At time: 229.6787371635437 and batch: 500, loss is 3.559333920478821 and perplexity is 35.13978345814342
At time: 230.27510690689087 and batch: 550, loss is 3.575513391494751 and perplexity is 35.71295083522645
At time: 230.87125897407532 and batch: 600, loss is 3.6568256521224978 and perplexity is 38.738179035923245
At time: 231.48985528945923 and batch: 650, loss is 3.5384513187408446 and perplexity is 34.41358223068134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332614674287684 and perplexity of 76.14311596635737
Finished 28 epochs...
Completing Train Step...
At time: 232.61521768569946 and batch: 50, loss is 3.696107716560364 and perplexity is 40.290177975710975
At time: 233.2200472354889 and batch: 100, loss is 3.6716094970703126 and perplexity is 39.31513255573241
At time: 233.81505942344666 and batch: 150, loss is 3.623210406303406 and perplexity is 37.45762920493382
At time: 234.40776991844177 and batch: 200, loss is 3.5637088060379027 and perplexity is 35.293852761393026
At time: 235.00116276741028 and batch: 250, loss is 3.534847812652588 and perplexity is 34.28979584404311
At time: 235.598064661026 and batch: 300, loss is 3.640492844581604 and perplexity is 38.11061470766578
At time: 236.19364285469055 and batch: 350, loss is 3.594005947113037 and perplexity is 36.379518842140854
At time: 236.7850604057312 and batch: 400, loss is 3.6490202569961547 and perplexity is 38.43698922396201
At time: 237.3818964958191 and batch: 450, loss is 3.5964141511917114 and perplexity is 36.46723372307223
At time: 237.9786238670349 and batch: 500, loss is 3.5593052339553832 and perplexity is 35.13877543438007
At time: 238.5745725631714 and batch: 550, loss is 3.5755306243896485 and perplexity is 35.7135662780576
At time: 239.1715326309204 and batch: 600, loss is 3.656875967979431 and perplexity is 38.74012822963474
At time: 239.76545977592468 and batch: 650, loss is 3.5384829092025756 and perplexity is 34.41466938880565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332614674287684 and perplexity of 76.14311596635737
Finished 29 epochs...
Completing Train Step...
At time: 240.9016592502594 and batch: 50, loss is 3.69588171005249 and perplexity is 40.28107316219738
At time: 241.49158191680908 and batch: 100, loss is 3.67136812210083 and perplexity is 39.30564401200612
At time: 242.08390641212463 and batch: 150, loss is 3.6229836463928224 and perplexity is 37.44913627924858
At time: 242.67641830444336 and batch: 200, loss is 3.5634906244277955 and perplexity is 35.28615313176
At time: 243.27303457260132 and batch: 250, loss is 3.5346460342407227 and perplexity is 34.282877601493986
At time: 243.8626434803009 and batch: 300, loss is 3.640322470664978 and perplexity is 38.104122206065185
At time: 244.45603680610657 and batch: 350, loss is 3.5938679361343384 and perplexity is 36.374498415585826
At time: 245.04912185668945 and batch: 400, loss is 3.6489161968231203 and perplexity is 38.43298967231306
At time: 245.6473159790039 and batch: 450, loss is 3.5963408613204955 and perplexity is 36.46456114214683
At time: 246.24574613571167 and batch: 500, loss is 3.5592711925506593 and perplexity is 35.13757928146348
At time: 246.87006044387817 and batch: 550, loss is 3.575539174079895 and perplexity is 35.713871619292156
At time: 247.4689872264862 and batch: 600, loss is 3.656914162635803 and perplexity is 38.7416079237783
At time: 248.07351922988892 and batch: 650, loss is 3.5385054254531862 and perplexity is 34.41544428685016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332617666207108 and perplexity of 76.14334378076586
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 249.21280479431152 and batch: 50, loss is 3.6958498525619508 and perplexity is 40.27978992873061
At time: 249.82859754562378 and batch: 100, loss is 3.6713671827316285 and perplexity is 39.30560708951203
At time: 250.43190741539001 and batch: 150, loss is 3.6229234600067137 and perplexity is 37.44688241889957
At time: 251.02957153320312 and batch: 200, loss is 3.563376269340515 and perplexity is 35.28211821135006
At time: 251.63114857673645 and batch: 250, loss is 3.5345174407958986 and perplexity is 34.27846933160811
At time: 252.23399209976196 and batch: 300, loss is 3.640065712928772 and perplexity is 38.09433993379822
At time: 252.83656287193298 and batch: 350, loss is 3.593693652153015 and perplexity is 36.36815947558721
At time: 253.43908214569092 and batch: 400, loss is 3.6485454845428467 and perplexity is 38.41874473162427
At time: 254.04106307029724 and batch: 450, loss is 3.5957649326324463 and perplexity is 36.44356620166253
At time: 254.63871312141418 and batch: 500, loss is 3.5586046600341796 and perplexity is 35.11416674579694
At time: 255.24122953414917 and batch: 550, loss is 3.574732894897461 and perplexity is 35.68508787350824
At time: 255.8443295955658 and batch: 600, loss is 3.6560055446624755 and perplexity is 38.70642258993722
At time: 256.4473295211792 and batch: 650, loss is 3.5377465152740477 and perplexity is 34.38933596405034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332606296913297 and perplexity of 76.1424780896398
Finished 31 epochs...
Completing Train Step...
At time: 257.59535217285156 and batch: 50, loss is 3.6957963848114015 and perplexity is 40.27763631654543
At time: 258.1894407272339 and batch: 100, loss is 3.6713126134872436 and perplexity is 39.30346227075416
At time: 258.78810715675354 and batch: 150, loss is 3.622870726585388 and perplexity is 37.44490776873713
At time: 259.387907743454 and batch: 200, loss is 3.5633229923248293 and perplexity is 35.280238535456895
At time: 259.9874258041382 and batch: 250, loss is 3.534472498893738 and perplexity is 34.27692882661006
At time: 260.58660101890564 and batch: 300, loss is 3.640029149055481 and perplexity is 38.09294708264395
At time: 261.1850838661194 and batch: 350, loss is 3.5936567068099974 and perplexity is 36.36681586628067
At time: 261.7802481651306 and batch: 400, loss is 3.648520860671997 and perplexity is 38.41779872506302
At time: 262.3929030895233 and batch: 450, loss is 3.595757312774658 and perplexity is 36.44328850792877
At time: 262.9915380477905 and batch: 500, loss is 3.5586081027984617 and perplexity is 35.11428763580411
At time: 263.58990573883057 and batch: 550, loss is 3.5747480726242067 and perplexity is 35.68562949613117
At time: 264.1880476474762 and batch: 600, loss is 3.6560283374786375 and perplexity is 38.70730482836591
At time: 264.78740882873535 and batch: 650, loss is 3.537761998176575 and perplexity is 34.389868414908975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3325976203469665 and perplexity of 76.1418174372442
Finished 32 epochs...
Completing Train Step...
At time: 265.9144151210785 and batch: 50, loss is 3.6957442331314088 and perplexity is 40.275535824917945
At time: 266.525691986084 and batch: 100, loss is 3.6712590932846068 and perplexity is 39.301358797778754
At time: 267.12201142311096 and batch: 150, loss is 3.6228192472457885 and perplexity is 37.44298017922977
At time: 267.71801710128784 and batch: 200, loss is 3.56327130317688 and perplexity is 35.27841497711704
At time: 268.31382989883423 and batch: 250, loss is 3.534428243637085 and perplexity is 34.27541192589322
At time: 268.90674114227295 and batch: 300, loss is 3.639993357658386 and perplexity is 38.091583707247096
At time: 269.5010426044464 and batch: 350, loss is 3.5936212396621703 and perplexity is 36.36552606191932
At time: 270.0980212688446 and batch: 400, loss is 3.6484975957870485 and perplexity is 38.416904949792524
At time: 270.69330859184265 and batch: 450, loss is 3.5957493925094606 and perplexity is 36.44299986856217
At time: 271.29325318336487 and batch: 500, loss is 3.558610453605652 and perplexity is 35.11437018282099
At time: 271.89297676086426 and batch: 550, loss is 3.574761652946472 and perplexity is 35.68611412177065
At time: 272.4886465072632 and batch: 600, loss is 3.656049666404724 and perplexity is 38.708130422414094
At time: 273.0849606990814 and batch: 650, loss is 3.5377761697769166 and perplexity is 34.39035577783331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.33258804620481 and perplexity of 76.14108844814973
Finished 33 epochs...
Completing Train Step...
At time: 274.2234754562378 and batch: 50, loss is 3.695693254470825 and perplexity is 40.27348268438093
At time: 274.81244945526123 and batch: 100, loss is 3.6712065315246583 and perplexity is 39.299293103480714
At time: 275.40552401542664 and batch: 150, loss is 3.622768759727478 and perplexity is 37.44108982380245
At time: 275.9966154098511 and batch: 200, loss is 3.56322096824646 and perplexity is 35.27663928524389
At time: 276.58877539634705 and batch: 250, loss is 3.5343844938278197 and perplexity is 34.27391241596086
At time: 277.1872959136963 and batch: 300, loss is 3.6399581241607666 and perplexity is 38.090241631166386
At time: 277.80963349342346 and batch: 350, loss is 3.593587260246277 and perplexity is 36.364290403578664
At time: 278.4077961444855 and batch: 400, loss is 3.64847505569458 and perplexity is 38.4160390389615
At time: 279.00694465637207 and batch: 450, loss is 3.5957411193847655 and perplexity is 36.44269837232715
At time: 279.60283875465393 and batch: 500, loss is 3.558612141609192 and perplexity is 35.11442945605219
At time: 280.19727325439453 and batch: 550, loss is 3.5747739839553834 and perplexity is 35.68655417027501
At time: 280.79679322242737 and batch: 600, loss is 3.6560694122314454 and perplexity is 38.708894753996276
At time: 281.3948800563812 and batch: 650, loss is 3.537789216041565 and perplexity is 34.39080444644286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332579968022365 and perplexity of 76.14047336903008
Finished 34 epochs...
Completing Train Step...
At time: 282.5236165523529 and batch: 50, loss is 3.6956433200836183 and perplexity is 40.27147170291138
At time: 283.1347544193268 and batch: 100, loss is 3.671154761314392 and perplexity is 39.297258623476615
At time: 283.7258996963501 and batch: 150, loss is 3.622719488143921 and perplexity is 37.43924508746364
At time: 284.3229579925537 and batch: 200, loss is 3.563171806335449 and perplexity is 35.274905060871816
At time: 284.9189283847809 and batch: 250, loss is 3.534341230392456 and perplexity is 34.272429640841594
At time: 285.51587653160095 and batch: 300, loss is 3.639923405647278 and perplexity is 38.08891921755478
At time: 286.11108112335205 and batch: 350, loss is 3.5935545444488524 and perplexity is 36.3631007362809
At time: 286.70835399627686 and batch: 400, loss is 3.648453321456909 and perplexity is 38.41520410471201
At time: 287.30697202682495 and batch: 450, loss is 3.595732345581055 and perplexity is 36.44237863264762
At time: 287.9103226661682 and batch: 500, loss is 3.5586128425598145 and perplexity is 35.114454069542006
At time: 288.51383566856384 and batch: 550, loss is 3.5747850465774538 and perplexity is 35.68694895932049
At time: 289.1165871620178 and batch: 600, loss is 3.6560879230499266 and perplexity is 38.70961129395253
At time: 289.72101497650146 and batch: 650, loss is 3.537801117897034 and perplexity is 34.39121376326265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332572787415748 and perplexity of 76.13992663620606
Finished 35 epochs...
Completing Train Step...
At time: 290.86782670021057 and batch: 50, loss is 3.695594320297241 and perplexity is 40.26949845774553
At time: 291.46502113342285 and batch: 100, loss is 3.6711038637161253 and perplexity is 39.29525853829441
At time: 292.0606129169464 and batch: 150, loss is 3.62267098903656 and perplexity is 37.437429361527535
At time: 292.65716671943665 and batch: 200, loss is 3.563123607635498 and perplexity is 35.273204897280145
At time: 293.2672669887543 and batch: 250, loss is 3.534298520088196 and perplexity is 34.270965886202816
At time: 293.862562417984 and batch: 300, loss is 3.639889192581177 and perplexity is 38.0876161011358
At time: 294.4540686607361 and batch: 350, loss is 3.5935226917266845 and perplexity is 36.36194249098271
At time: 295.049866437912 and batch: 400, loss is 3.6484321212768553 and perplexity is 38.41438970410094
At time: 295.6457827091217 and batch: 450, loss is 3.595723338127136 and perplexity is 36.44205038107975
At time: 296.2415657043457 and batch: 500, loss is 3.5586128711700438 and perplexity is 35.1144550741746
At time: 296.8380448818207 and batch: 550, loss is 3.574795184135437 and perplexity is 35.68731073966859
At time: 297.43391704559326 and batch: 600, loss is 3.656105303764343 and perplexity is 38.71028410049851
At time: 298.024484872818 and batch: 650, loss is 3.537812156677246 and perplexity is 34.39159340240798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332567701152727 and perplexity of 76.1395393694977
Finished 36 epochs...
Completing Train Step...
At time: 299.1499629020691 and batch: 50, loss is 3.695546016693115 and perplexity is 40.267553342812086
At time: 299.7585611343384 and batch: 100, loss is 3.671053686141968 and perplexity is 39.29328684701283
At time: 300.35191321372986 and batch: 150, loss is 3.6226233196258546 and perplexity is 37.43564478386676
At time: 300.9460668563843 and batch: 200, loss is 3.5630764961242676 and perplexity is 35.27154316243523
At time: 301.53536772727966 and batch: 250, loss is 3.5342564821243285 and perplexity is 34.269525234858435
At time: 302.1291403770447 and batch: 300, loss is 3.6398552322387694 and perplexity is 38.08632265461459
At time: 302.72334337234497 and batch: 350, loss is 3.5934918880462647 and perplexity is 36.360822426577926
At time: 303.31771326065063 and batch: 400, loss is 3.648411235809326 and perplexity is 38.4135874099903
At time: 303.90854930877686 and batch: 450, loss is 3.5957138919830323 and perplexity is 36.44170614584626
At time: 304.4988098144531 and batch: 500, loss is 3.5586122369766233 and perplexity is 35.11443280482529
At time: 305.08880829811096 and batch: 550, loss is 3.5748043203353883 and perplexity is 35.68763678756465
At time: 305.6818540096283 and batch: 600, loss is 3.656121602058411 and perplexity is 38.710915017233646
At time: 306.27333664894104 and batch: 650, loss is 3.5378223991394044 and perplexity is 34.39194565880595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.3325635124655335 and perplexity of 76.13922044545215
Finished 37 epochs...
Completing Train Step...
At time: 307.4167034626007 and batch: 50, loss is 3.695498561859131 and perplexity is 40.26564249809302
At time: 308.01254200935364 and batch: 100, loss is 3.671004180908203 and perplexity is 39.29134167181065
At time: 308.6209328174591 and batch: 150, loss is 3.6225763750076294 and perplexity is 37.43388742306401
At time: 309.2152180671692 and batch: 200, loss is 3.5630302000045777 and perplexity is 35.269910264650036
At time: 309.81310296058655 and batch: 250, loss is 3.534214906692505 and perplexity is 34.26810049416572
At time: 310.41147089004517 and batch: 300, loss is 3.63982177734375 and perplexity is 38.08504850200194
At time: 311.01047372817993 and batch: 350, loss is 3.593461766242981 and perplexity is 36.3597271895329
At time: 311.60946226119995 and batch: 400, loss is 3.6483906888961792 and perplexity is 38.41279813745471
At time: 312.20806765556335 and batch: 450, loss is 3.595704140663147 and perplexity is 36.44135079284505
At time: 312.8068504333496 and batch: 500, loss is 3.5586111450195315 and perplexity is 35.1143944613923
At time: 313.4088532924652 and batch: 550, loss is 3.574812445640564 and perplexity is 35.68792676168261
At time: 314.0093026161194 and batch: 600, loss is 3.656136827468872 and perplexity is 38.71150441129099
At time: 314.6102547645569 and batch: 650, loss is 3.5378316831588745 and perplexity is 34.39226495578124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332558725394454 and perplexity of 76.13885596246438
Finished 38 epochs...
Completing Train Step...
At time: 315.74370312690735 and batch: 50, loss is 3.6954518270492556 and perplexity is 40.26376073491863
At time: 316.35514187812805 and batch: 100, loss is 3.670955443382263 and perplexity is 39.289426755691224
At time: 316.95509696006775 and batch: 150, loss is 3.622530107498169 and perplexity is 37.43215549038993
At time: 317.55396366119385 and batch: 200, loss is 3.562984676361084 and perplexity is 35.26830468637527
At time: 318.15353512763977 and batch: 250, loss is 3.5341738796234132 and perplexity is 34.26669460327909
At time: 318.75206232070923 and batch: 300, loss is 3.639788479804993 and perplexity is 38.08378038473609
At time: 319.3488190174103 and batch: 350, loss is 3.593432388305664 and perplexity is 36.35865903143688
At time: 319.9451639652252 and batch: 400, loss is 3.6483705329895018 and perplexity is 38.41202390048298
At time: 320.54665517807007 and batch: 450, loss is 3.5956941270828247 and perplexity is 36.440985886278845
At time: 321.1471164226532 and batch: 500, loss is 3.5586095380783083 and perplexity is 35.11433803466965
At time: 321.7483506202698 and batch: 550, loss is 3.5748199892044066 and perplexity is 35.688195976851965
At time: 322.35037183761597 and batch: 600, loss is 3.6561511993408202 and perplexity is 38.712060772073265
At time: 322.9499936103821 and batch: 650, loss is 3.537840347290039 and perplexity is 34.39256293616673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332555434283088 and perplexity of 76.13860538142242
Finished 39 epochs...
Completing Train Step...
At time: 324.1050100326538 and batch: 50, loss is 3.6954057216644287 and perplexity is 40.26190440152918
At time: 324.70347023010254 and batch: 100, loss is 3.670907211303711 and perplexity is 39.2875317906731
At time: 325.3015615940094 and batch: 150, loss is 3.6224845123291014 and perplexity is 37.43044880384041
At time: 325.89994525909424 and batch: 200, loss is 3.5629398918151853 and perplexity is 35.26672524673277
At time: 326.49754095077515 and batch: 250, loss is 3.5341332530975342 and perplexity is 34.265302494802455
At time: 327.09233260154724 and batch: 300, loss is 3.639755620956421 and perplexity is 38.08252901612276
At time: 327.69263458251953 and batch: 350, loss is 3.5934034538269044 and perplexity is 36.35760702780907
At time: 328.2920825481415 and batch: 400, loss is 3.6483505868911745 and perplexity is 38.41125773811831
At time: 328.89009308815 and batch: 450, loss is 3.595683732032776 and perplexity is 36.44060708237558
At time: 329.4886736869812 and batch: 500, loss is 3.5586072540283205 and perplexity is 35.114257831857884
At time: 330.0863080024719 and batch: 550, loss is 3.574826693534851 and perplexity is 35.68843524311282
At time: 330.68153071403503 and batch: 600, loss is 3.6561645889282226 and perplexity is 38.71257911406469
At time: 331.2853157520294 and batch: 650, loss is 3.5378482866287233 and perplexity is 34.39283599145604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332550946403952 and perplexity of 76.13826368133066
Finished 40 epochs...
Completing Train Step...
At time: 332.4152171611786 and batch: 50, loss is 3.695360207557678 and perplexity is 40.26007195861558
At time: 333.02494740486145 and batch: 100, loss is 3.670859580039978 and perplexity is 39.28566052045079
At time: 333.6176645755768 and batch: 150, loss is 3.6224394178390504 and perplexity is 37.42876093489631
At time: 334.20709347724915 and batch: 200, loss is 3.56289577960968 and perplexity is 35.26516958801321
At time: 334.8017179965973 and batch: 250, loss is 3.5340930795669556 and perplexity is 34.26392596427513
At time: 335.39543986320496 and batch: 300, loss is 3.6397230434417724 and perplexity is 38.08128840218406
At time: 335.9902687072754 and batch: 350, loss is 3.593375129699707 and perplexity is 36.35657724490694
At time: 336.5844120979309 and batch: 400, loss is 3.6483307790756228 and perplexity is 38.41049690254519
At time: 337.1789462566376 and batch: 450, loss is 3.5956730890274047 and perplexity is 36.44021924686255
At time: 337.7708773612976 and batch: 500, loss is 3.558604655265808 and perplexity is 35.11416657835956
At time: 338.36761713027954 and batch: 550, loss is 3.5748327112197877 and perplexity is 35.68865000551818
At time: 338.9785358905792 and batch: 600, loss is 3.6561771631240845 and perplexity is 38.71306589667723
At time: 339.57462310791016 and batch: 650, loss is 3.537855739593506 and perplexity is 34.39309232100667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.33254945044424 and perplexity of 76.13814978164086
Finished 41 epochs...
Completing Train Step...
At time: 340.71405482292175 and batch: 50, loss is 3.6953152084350585 and perplexity is 40.25826033146196
At time: 341.30340123176575 and batch: 100, loss is 3.670812335014343 and perplexity is 39.28380451225634
At time: 341.8967754840851 and batch: 150, loss is 3.622395005226135 and perplexity is 37.4270986627382
At time: 342.4887399673462 and batch: 200, loss is 3.5628523302078245 and perplexity is 35.263637370775484
At time: 343.08271527290344 and batch: 250, loss is 3.53405339717865 and perplexity is 34.26256631683735
At time: 343.6768295764923 and batch: 300, loss is 3.6396906185150146 and perplexity is 38.08005363921543
At time: 344.27303314208984 and batch: 350, loss is 3.5933472776412962 and perplexity is 36.35556465349534
At time: 344.8700542449951 and batch: 400, loss is 3.648311243057251 and perplexity is 38.409746521701784
At time: 345.47691917419434 and batch: 450, loss is 3.5956621551513672 and perplexity is 36.43982081620072
At time: 346.0840427875519 and batch: 500, loss is 3.558601665496826 and perplexity is 35.11406159527042
At time: 346.69302248954773 and batch: 550, loss is 3.574838161468506 and perplexity is 35.68884451806719
At time: 347.3000512123108 and batch: 600, loss is 3.656188998222351 and perplexity is 38.713524072327594
At time: 347.9073791503906 and batch: 650, loss is 3.537862596511841 and perplexity is 34.39332815244053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332547954484528 and perplexity of 76.1380358821214
Finished 42 epochs...
Completing Train Step...
At time: 349.0409207344055 and batch: 50, loss is 3.6952706575393677 and perplexity is 40.25646682985659
At time: 349.6560854911804 and batch: 100, loss is 3.670765781402588 and perplexity is 39.28197575184084
At time: 350.2544114589691 and batch: 150, loss is 3.622351064682007 and perplexity is 37.42545413178888
At time: 350.8540692329407 and batch: 200, loss is 3.562809467315674 and perplexity is 35.26212590168321
At time: 351.4543180465698 and batch: 250, loss is 3.5340143156051638 and perplexity is 34.26122730799946
At time: 352.04986453056335 and batch: 300, loss is 3.6396585655212403 and perplexity is 38.07883307905461
At time: 352.64862966537476 and batch: 350, loss is 3.593319797515869 and perplexity is 36.35456561174564
At time: 353.2484965324402 and batch: 400, loss is 3.6482917261123657 and perplexity is 38.40899688811117
At time: 353.8470137119293 and batch: 450, loss is 3.595650897026062 and perplexity is 36.43941057444115
At time: 354.46086382865906 and batch: 500, loss is 3.5585981369018556 and perplexity is 35.11393769218789
At time: 355.06006813049316 and batch: 550, loss is 3.5748431491851806 and perplexity is 35.68902252435602
At time: 355.65564465522766 and batch: 600, loss is 3.6562001132965087 and perplexity is 38.713954378409994
At time: 356.25402641296387 and batch: 650, loss is 3.537868857383728 and perplexity is 34.393543485335954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332546159332874 and perplexity of 76.13789920292304
Finished 43 epochs...
Completing Train Step...
At time: 357.4039170742035 and batch: 50, loss is 3.69522668838501 and perplexity is 40.254696825965716
At time: 357.997079372406 and batch: 100, loss is 3.6707195138931272 and perplexity is 39.28015831470058
At time: 358.58943724632263 and batch: 150, loss is 3.6223075437545775 and perplexity is 37.42382537675832
At time: 359.17979311943054 and batch: 200, loss is 3.562767114639282 and perplexity is 35.260632487901276
At time: 359.7706024646759 and batch: 250, loss is 3.533975429534912 and perplexity is 34.25989504941077
At time: 360.36363077163696 and batch: 300, loss is 3.6396266126632693 and perplexity is 38.07761637094829
At time: 360.95681738853455 and batch: 350, loss is 3.593292727470398 and perplexity is 36.35358150532142
At time: 361.5497546195984 and batch: 400, loss is 3.6482724714279176 and perplexity is 38.408257342116
At time: 362.1423053741455 and batch: 450, loss is 3.595639476776123 and perplexity is 36.438994429641
At time: 362.732967376709 and batch: 500, loss is 3.5585943412780763 and perplexity is 35.11380441314394
At time: 363.3285632133484 and batch: 550, loss is 3.5748473405838013 and perplexity is 35.68917211158929
At time: 363.92693424224854 and batch: 600, loss is 3.6562105131149294 and perplexity is 38.71435699859946
At time: 364.53047227859497 and batch: 650, loss is 3.5378746700286867 and perplexity is 34.39374340337413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.33254436418122 and perplexity of 76.13776252397003
Finished 44 epochs...
Completing Train Step...
At time: 365.67006397247314 and batch: 50, loss is 3.695182900428772 and perplexity is 40.25293419365405
At time: 366.28704261779785 and batch: 100, loss is 3.670673966407776 and perplexity is 39.27836924300931
At time: 366.8871741294861 and batch: 150, loss is 3.6222645473480224 and perplexity is 37.422216321339626
At time: 367.4904556274414 and batch: 200, loss is 3.562725234031677 and perplexity is 35.25915578211103
At time: 368.0945451259613 and batch: 250, loss is 3.5339370822906493 and perplexity is 34.258581302036355
At time: 368.69727993011475 and batch: 300, loss is 3.6395948791503905 and perplexity is 38.07640805359097
At time: 369.30002760887146 and batch: 350, loss is 3.593265857696533 and perplexity is 36.35260470593044
At time: 369.9164021015167 and batch: 400, loss is 3.6482532358169557 and perplexity is 38.4075185429257
At time: 370.5174174308777 and batch: 450, loss is 3.5956278610229493 and perplexity is 36.43857116573408
At time: 371.1204538345337 and batch: 500, loss is 3.5585901069641115 and perplexity is 35.11365573058634
At time: 371.7233295440674 and batch: 550, loss is 3.574851131439209 and perplexity is 35.68930740433683
At time: 372.32657766342163 and batch: 600, loss is 3.6562203121185304 and perplexity is 38.71473636258179
At time: 372.92896461486816 and batch: 650, loss is 3.5378799772262575 and perplexity is 34.39392593824994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.33254316741345 and perplexity of 76.13767140480427
Finished 45 epochs...
Completing Train Step...
At time: 374.0767397880554 and batch: 50, loss is 3.6951398468017578 and perplexity is 40.25120119614523
At time: 374.6761302947998 and batch: 100, loss is 3.6706285190582277 and perplexity is 39.27658418579601
At time: 375.2753396034241 and batch: 150, loss is 3.6222217893600464 and perplexity is 37.42061625687213
At time: 375.874844789505 and batch: 200, loss is 3.562683801651001 and perplexity is 35.25769494160961
At time: 376.4749665260315 and batch: 250, loss is 3.53389892578125 and perplexity is 34.25727413909544
At time: 377.0740475654602 and batch: 300, loss is 3.63956335067749 and perplexity is 38.075207581516125
At time: 377.67068219184875 and batch: 350, loss is 3.593239436149597 and perplexity is 36.35164422656768
At time: 378.2708604335785 and batch: 400, loss is 3.6482341623306276 and perplexity is 38.406785984632116
At time: 378.87006664276123 and batch: 450, loss is 3.5956158876419066 and perplexity is 36.4381348754488
At time: 379.4696626663208 and batch: 500, loss is 3.558585596084595 and perplexity is 35.1134973374732
At time: 380.06899523735046 and batch: 550, loss is 3.5748545360565185 and perplexity is 35.68942891297743
At time: 380.6659541130066 and batch: 600, loss is 3.6562295532226563 and perplexity is 38.71509413114481
At time: 381.26180028915405 and batch: 650, loss is 3.5378849649429323 and perplexity is 34.39409748583567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332542868221507 and perplexity of 76.13764862502988
Finished 46 epochs...
Completing Train Step...
At time: 382.3961434364319 and batch: 50, loss is 3.6950970315933227 and perplexity is 40.24947786946882
At time: 383.0031337738037 and batch: 100, loss is 3.670583658218384 and perplexity is 39.27482224476461
At time: 383.5953960418701 and batch: 150, loss is 3.622179698944092 and perplexity is 37.41904124071539
At time: 384.1861023902893 and batch: 200, loss is 3.5626427602767943 and perplexity is 35.256247947051364
At time: 384.77502703666687 and batch: 250, loss is 3.5338612461090086 and perplexity is 34.25598336055221
At time: 385.3865399360657 and batch: 300, loss is 3.6395319414138796 and perplexity is 38.074011686065376
At time: 385.98183274269104 and batch: 350, loss is 3.5932132530212404 and perplexity is 36.350692439261366
At time: 386.5781946182251 and batch: 400, loss is 3.64821494102478 and perplexity is 38.4060477631469
At time: 387.17336916923523 and batch: 450, loss is 3.5956039476394652 and perplexity is 36.437699806626796
At time: 387.7700505256653 and batch: 500, loss is 3.558580827713013 and perplexity is 35.113329903669545
At time: 388.364075422287 and batch: 550, loss is 3.5748573923110962 and perplexity is 35.68953085121771
At time: 388.96391248703003 and batch: 600, loss is 3.656238193511963 and perplexity is 38.715428642203776
At time: 389.5629036426544 and batch: 650, loss is 3.537889528274536 and perplexity is 34.394254437865825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332542868221507 and perplexity of 76.13764862502988
Finished 47 epochs...
Completing Train Step...
At time: 390.70501589775085 and batch: 50, loss is 3.695054497718811 and perplexity is 40.24776593963571
At time: 391.29999256134033 and batch: 100, loss is 3.6705390787124634 and perplexity is 39.27307143161931
At time: 391.8893756866455 and batch: 150, loss is 3.6221376848220825 and perplexity is 37.417469145576554
At time: 392.48241424560547 and batch: 200, loss is 3.562602276802063 and perplexity is 35.25482068051903
At time: 393.07587933540344 and batch: 250, loss is 3.5338238859176636 and perplexity is 34.25470357436582
At time: 393.66943883895874 and batch: 300, loss is 3.6395008182525634 and perplexity is 38.072826720897744
At time: 394.265971660614 and batch: 350, loss is 3.5931873750686645 and perplexity is 36.34975176993767
At time: 394.8624985218048 and batch: 400, loss is 3.6481959056854247 and perplexity is 38.405316697952486
At time: 395.4550950527191 and batch: 450, loss is 3.5955916452407837 and perplexity is 36.43725153827413
At time: 396.05020570755005 and batch: 500, loss is 3.5585756731033324 and perplexity is 35.11314890862579
At time: 396.6491093635559 and batch: 550, loss is 3.574859828948975 and perplexity is 35.689617813786406
At time: 397.2472162246704 and batch: 600, loss is 3.6562462759017946 and perplexity is 38.715741556655104
At time: 397.8460783958435 and batch: 650, loss is 3.537893657684326 and perplexity is 34.39439646613007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332542269837623 and perplexity of 76.13760306550157
Finished 48 epochs...
Completing Train Step...
At time: 398.9712634086609 and batch: 50, loss is 3.695012230873108 and perplexity is 40.24606482947339
At time: 399.5780975818634 and batch: 100, loss is 3.670494966506958 and perplexity is 39.2713390480314
At time: 400.17201256752014 and batch: 150, loss is 3.6220962619781494 and perplexity is 37.415919239692734
At time: 400.78001403808594 and batch: 200, loss is 3.562562065124512 and perplexity is 35.253403053540474
At time: 401.37304425239563 and batch: 250, loss is 3.5337868213653563 and perplexity is 34.25343396264236
At time: 401.96634817123413 and batch: 300, loss is 3.6394699478149413 and perplexity is 38.07165141421656
At time: 402.5565950870514 and batch: 350, loss is 3.5931615161895754 and perplexity is 36.34881181825483
At time: 403.14926075935364 and batch: 400, loss is 3.6481769704818725 and perplexity is 38.40458949234824
At time: 403.7430920600891 and batch: 450, loss is 3.595579147338867 and perplexity is 36.43679615192399
At time: 404.3391032218933 and batch: 500, loss is 3.558570294380188 and perplexity is 35.11296004522699
At time: 404.93746066093445 and batch: 550, loss is 3.5748618841171265 and perplexity is 35.68969116202765
At time: 405.5365252494812 and batch: 600, loss is 3.6562538003921508 and perplexity is 38.71603287397508
At time: 406.13183879852295 and batch: 650, loss is 3.5378974008560182 and perplexity is 34.394525210502245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332542269837623 and perplexity of 76.13760306550157
Finished 49 epochs...
Completing Train Step...
At time: 407.27458786964417 and batch: 50, loss is 3.6949703454971314 and perplexity is 40.244379143219476
At time: 407.8662428855896 and batch: 100, loss is 3.670451030731201 and perplexity is 39.269613669188516
At time: 408.4633207321167 and batch: 150, loss is 3.622055025100708 and perplexity is 37.41437635582877
At time: 409.05760741233826 and batch: 200, loss is 3.562522177696228 and perplexity is 35.25199691399824
At time: 409.6516811847687 and batch: 250, loss is 3.53375 and perplexity is 34.25217272765605
At time: 410.24575543403625 and batch: 300, loss is 3.6394389629364015 and perplexity is 38.07047178699708
At time: 410.8436670303345 and batch: 350, loss is 3.5931360006332396 and perplexity is 36.34788436993137
At time: 411.44262623786926 and batch: 400, loss is 3.6481579637527464 and perplexity is 38.40385955365545
At time: 412.0408744812012 and batch: 450, loss is 3.5955666875839234 and perplexity is 36.43634216120132
At time: 412.6388695240021 and batch: 500, loss is 3.5585645484924315 and perplexity is 35.112758290679416
At time: 413.23535919189453 and batch: 550, loss is 3.5748636865615846 and perplexity is 35.689755490771674
At time: 413.83345103263855 and batch: 600, loss is 3.656260929107666 and perplexity is 38.716308870543074
At time: 414.43959522247314 and batch: 650, loss is 3.537900938987732 and perplexity is 34.394646903077955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.332542569029565 and perplexity of 76.13762584526232
Annealing...
Finished Training.
Improved accuracyfrom -78.04083281248714 to -76.13760306550157
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f28e8faa828>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.40764913611098597, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 12.987931008225292, 'seq_len': 20, 'anneal': 5.351038312655021, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8340964317321777 and batch: 50, loss is 6.67447491645813 and perplexity is 791.9315144790112
At time: 1.4125239849090576 and batch: 100, loss is 5.8086228179931645 and perplexity is 333.15998764843465
At time: 1.9721310138702393 and batch: 150, loss is 5.5908092021942135 and perplexity is 267.95235964597606
At time: 2.536687135696411 and batch: 200, loss is 5.543821649551392 and perplexity is 255.65315167911623
At time: 3.1004741191864014 and batch: 250, loss is 5.5007343578338626 and perplexity is 244.8716896966226
At time: 3.680297374725342 and batch: 300, loss is 5.529211721420288 and perplexity is 251.9452296978528
At time: 4.244964599609375 and batch: 350, loss is 5.465247726440429 and perplexity is 236.33439413593933
At time: 4.809575080871582 and batch: 400, loss is 5.516866312026978 and perplexity is 248.85398330475263
At time: 5.371393203735352 and batch: 450, loss is 5.4834479808807375 and perplexity is 240.67512159306173
At time: 5.9338295459747314 and batch: 500, loss is 5.4924643611907955 and perplexity is 242.8549523446022
At time: 6.49898362159729 and batch: 550, loss is 5.5139123058319095 and perplexity is 248.11995179698613
At time: 7.0647852420806885 and batch: 600, loss is 5.561459741592407 and perplexity is 260.20238747969654
At time: 7.6288251876831055 and batch: 650, loss is 5.535943937301636 and perplexity is 253.64710163129828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.419453938802083 and perplexity of 225.7558123483246
Finished 1 epochs...
Completing Train Step...
At time: 8.726203918457031 and batch: 50, loss is 5.273567972183227 and perplexity is 195.11087216343734
At time: 9.280356407165527 and batch: 100, loss is 5.207010736465454 and perplexity is 182.5475590241332
At time: 9.840153694152832 and batch: 150, loss is 5.15361008644104 and perplexity is 173.0551078814264
At time: 10.398199081420898 and batch: 200, loss is 5.129643106460572 and perplexity is 168.9568076902323
At time: 10.957363605499268 and batch: 250, loss is 5.098471221923828 and perplexity is 163.77134577928967
At time: 11.515769720077515 and batch: 300, loss is 5.140606918334961 and perplexity is 170.81941029911096
At time: 12.07541036605835 and batch: 350, loss is 5.081691484451294 and perplexity is 161.0462328710209
At time: 12.629733324050903 and batch: 400, loss is 5.122558088302612 and perplexity is 167.76397624581477
At time: 13.186764478683472 and batch: 450, loss is 5.073053359985352 and perplexity is 159.6610866125684
At time: 13.750463962554932 and batch: 500, loss is 5.074977626800537 and perplexity is 159.9686129296644
At time: 14.325057029724121 and batch: 550, loss is 5.076989984512329 and perplexity is 160.29085112204953
At time: 14.910538673400879 and batch: 600, loss is 5.13872838973999 and perplexity is 170.49882236336632
At time: 15.505295991897583 and batch: 650, loss is 5.1018500328063965 and perplexity is 164.32563407499487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.234925513174019 and perplexity of 187.71512351969776
Finished 2 epochs...
Completing Train Step...
At time: 16.635984897613525 and batch: 50, loss is 5.078498458862304 and perplexity is 160.5328282217422
At time: 17.24595832824707 and batch: 100, loss is 5.0356247520446775 and perplexity is 153.79564674847774
At time: 17.84133267402649 and batch: 150, loss is 5.007480678558349 and perplexity is 149.52755325503526
At time: 18.45104455947876 and batch: 200, loss is 4.9862257099151615 and perplexity is 146.38288806813605
At time: 19.047637462615967 and batch: 250, loss is 4.956290302276611 and perplexity is 142.0657959825424
At time: 19.642849922180176 and batch: 300, loss is 5.031621608734131 and perplexity is 153.18121139093606
At time: 20.23988914489746 and batch: 350, loss is 4.9755258464813235 and perplexity is 144.82496082464522
At time: 20.842003345489502 and batch: 400, loss is 5.010684299468994 and perplexity is 150.00735098602817
At time: 21.44345474243164 and batch: 450, loss is 4.975885705947876 and perplexity is 144.8770868362479
At time: 22.04404592514038 and batch: 500, loss is 4.9763586139678955 and perplexity is 144.94561657538426
At time: 22.645185232162476 and batch: 550, loss is 4.992750968933105 and perplexity is 147.34119753700762
At time: 23.245224475860596 and batch: 600, loss is 5.043322153091431 and perplexity is 154.98404142828048
At time: 23.842588663101196 and batch: 650, loss is 5.000120763778686 and perplexity is 148.4310831187408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.198862711588542 and perplexity of 181.06620024173682
Finished 3 epochs...
Completing Train Step...
At time: 24.994492769241333 and batch: 50, loss is 4.986824455261231 and perplexity is 146.47056038518778
At time: 25.59270143508911 and batch: 100, loss is 4.962751808166504 and perplexity is 142.9867270566641
At time: 26.189759731292725 and batch: 150, loss is 4.927023019790649 and perplexity is 137.96817185038236
At time: 26.785654306411743 and batch: 200, loss is 4.899330883026123 and perplexity is 134.19995416917016
At time: 27.379855632781982 and batch: 250, loss is 4.8833810901641845 and perplexity is 132.07647226326154
At time: 27.97964334487915 and batch: 300, loss is 4.961032867431641 and perplexity is 142.74115247152824
At time: 28.581345796585083 and batch: 350, loss is 4.9124682331085205 and perplexity is 135.97461761157416
At time: 29.183104038238525 and batch: 400, loss is 4.921781721115113 and perplexity is 137.24693122387905
At time: 29.783485889434814 and batch: 450, loss is 4.876263809204102 and perplexity is 131.1397841920795
At time: 30.384003162384033 and batch: 500, loss is 4.898552942276001 and perplexity is 134.09559515398576
At time: 30.980234146118164 and batch: 550, loss is 4.912350759506226 and perplexity is 135.95864512161404
At time: 31.58102536201477 and batch: 600, loss is 4.974507551193238 and perplexity is 144.6775613102926
At time: 32.18085789680481 and batch: 650, loss is 4.931871690750122 and perplexity is 138.6387585318803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.175626866957721 and perplexity of 176.90747697661058
Finished 4 epochs...
Completing Train Step...
At time: 33.31722092628479 and batch: 50, loss is 4.922423267364502 and perplexity is 137.33500972804202
At time: 33.92445158958435 and batch: 100, loss is 4.9025328540802 and perplexity is 134.63034722320748
At time: 34.51376986503601 and batch: 150, loss is 4.864560585021973 and perplexity is 129.61397177612523
At time: 35.10789656639099 and batch: 200, loss is 4.8461868190765385 and perplexity is 127.25422014391887
At time: 35.70215201377869 and batch: 250, loss is 4.84141432762146 and perplexity is 126.64834737694113
At time: 36.29575300216675 and batch: 300, loss is 4.907148351669312 and perplexity is 135.25316947788954
At time: 36.88952374458313 and batch: 350, loss is 4.877337083816529 and perplexity is 131.28060875132593
At time: 37.48357844352722 and batch: 400, loss is 4.907306385040283 and perplexity is 135.27454568122897
At time: 38.07277178764343 and batch: 450, loss is 4.8639946460723875 and perplexity is 129.54063893399976
At time: 38.66707634925842 and batch: 500, loss is 4.876270980834961 and perplexity is 131.14072468157505
At time: 39.25981879234314 and batch: 550, loss is 4.881449728012085 and perplexity is 131.82163093821103
At time: 39.85213661193848 and batch: 600, loss is 4.9429623413085935 and perplexity is 140.1849106245503
At time: 40.44396996498108 and batch: 650, loss is 4.907456607818603 and perplexity is 135.29486852575602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 5.180909399892769 and perplexity of 177.84446921747355
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 41.578831911087036 and batch: 50, loss is 4.822497911453247 and perplexity is 124.27513166601607
At time: 42.16668963432312 and batch: 100, loss is 4.7017268943786625 and perplexity is 110.13720364085135
At time: 42.75617074966431 and batch: 150, loss is 4.626482849121094 and perplexity is 102.154140004532
At time: 43.34833121299744 and batch: 200, loss is 4.5849191856384275 and perplexity is 97.99526739712664
At time: 43.94127678871155 and batch: 250, loss is 4.547709064483643 and perplexity is 94.41585971386844
At time: 44.53330874443054 and batch: 300, loss is 4.603121900558472 and perplexity is 99.79538108754599
At time: 45.120375871658325 and batch: 350, loss is 4.536259164810181 and perplexity is 93.3409730081611
At time: 45.71162557601929 and batch: 400, loss is 4.554482965469361 and perplexity is 95.05759446902621
At time: 46.30291223526001 and batch: 450, loss is 4.506935377120971 and perplexity is 90.64360395441348
At time: 46.89527773857117 and batch: 500, loss is 4.474903554916382 and perplexity is 87.78613346048141
At time: 47.488237380981445 and batch: 550, loss is 4.4595488166809085 and perplexity is 86.44849617656648
At time: 48.084328413009644 and batch: 600, loss is 4.553186531066895 and perplexity is 94.93443838248412
At time: 48.694960594177246 and batch: 650, loss is 4.541127147674561 and perplexity is 93.79646302461857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.863445207184436 and perplexity of 129.46948381888308
Finished 6 epochs...
Completing Train Step...
At time: 49.82723355293274 and batch: 50, loss is 4.604472684860229 and perplexity is 99.93027420695029
At time: 50.436304807662964 and batch: 100, loss is 4.566604852676392 and perplexity is 96.21688410094946
At time: 51.032307147979736 and batch: 150, loss is 4.524012355804444 and perplexity is 92.20481529304159
At time: 51.628992557525635 and batch: 200, loss is 4.497608451843262 and perplexity is 89.80210821759943
At time: 52.222527742385864 and batch: 250, loss is 4.470308122634887 and perplexity is 87.38364374434131
At time: 52.81606721878052 and batch: 300, loss is 4.535448865890503 and perplexity is 93.26536955340643
At time: 53.41161012649536 and batch: 350, loss is 4.475937185287475 and perplexity is 87.8769187853387
At time: 54.0072124004364 and batch: 400, loss is 4.506806774139404 and perplexity is 90.63194766621773
At time: 54.603328704833984 and batch: 450, loss is 4.465827293395996 and perplexity is 86.99296848666867
At time: 55.199129581451416 and batch: 500, loss is 4.441564903259278 and perplexity is 84.90771011414597
At time: 55.79213905334473 and batch: 550, loss is 4.445854396820068 and perplexity is 85.27270344857212
At time: 56.386207580566406 and batch: 600, loss is 4.547458047866821 and perplexity is 94.39216273848052
At time: 56.982163190841675 and batch: 650, loss is 4.514615612030029 and perplexity is 91.34244833512471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.847483616249234 and perplexity of 127.41935010370626
Finished 7 epochs...
Completing Train Step...
At time: 58.12319040298462 and batch: 50, loss is 4.558380966186523 and perplexity is 95.42885215170827
At time: 58.71298313140869 and batch: 100, loss is 4.527675342559815 and perplexity is 92.54317964397664
At time: 59.30013108253479 and batch: 150, loss is 4.48695255279541 and perplexity is 88.85026639000284
At time: 59.88396096229553 and batch: 200, loss is 4.462398071289062 and perplexity is 86.69516119150671
At time: 60.47561955451965 and batch: 250, loss is 4.437487869262696 and perplexity is 84.56224321212582
At time: 61.06653070449829 and batch: 300, loss is 4.507177457809449 and perplexity is 90.66554967667543
At time: 61.65774321556091 and batch: 350, loss is 4.450074243545532 and perplexity is 85.63330148611452
At time: 62.24738359451294 and batch: 400, loss is 4.486264152526855 and perplexity is 88.78912289077411
At time: 62.839158058166504 and batch: 450, loss is 4.448542079925537 and perplexity is 85.50219771877627
At time: 63.427565574645996 and batch: 500, loss is 4.425464363098144 and perplexity is 83.55159649339365
At time: 64.03688144683838 and batch: 550, loss is 4.434973526000976 and perplexity is 84.34989177977849
At time: 64.63435626029968 and batch: 600, loss is 4.536667976379395 and perplexity is 93.3791396787652
At time: 65.23358988761902 and batch: 650, loss is 4.4936369609832765 and perplexity is 89.44616724161911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.838998831954657 and perplexity of 126.34279801778459
Finished 8 epochs...
Completing Train Step...
At time: 66.36820435523987 and batch: 50, loss is 4.529598436355591 and perplexity is 92.72132009408732
At time: 66.97574496269226 and batch: 100, loss is 4.501134424209595 and perplexity is 90.11930685780268
At time: 67.57428669929504 and batch: 150, loss is 4.463653326034546 and perplexity is 86.80405403383317
At time: 68.17323160171509 and batch: 200, loss is 4.440634689331055 and perplexity is 84.82876450343022
At time: 68.77187156677246 and batch: 250, loss is 4.416954870223999 and perplexity is 82.84363126301291
At time: 69.36972141265869 and batch: 300, loss is 4.4879615688323975 and perplexity is 88.93996297871911
At time: 69.96670508384705 and batch: 350, loss is 4.43253454208374 and perplexity is 84.14441443014182
At time: 70.56024742126465 and batch: 400, loss is 4.470339527130127 and perplexity is 87.38638802665649
At time: 71.15884518623352 and batch: 450, loss is 4.434816312789917 and perplexity is 84.33663190478038
At time: 71.75770688056946 and batch: 500, loss is 4.412625331878662 and perplexity is 82.4857319133373
At time: 72.35552453994751 and batch: 550, loss is 4.422921800613404 and perplexity is 83.33943117469671
At time: 72.95447874069214 and batch: 600, loss is 4.52511378288269 and perplexity is 92.30642812263854
At time: 73.55524253845215 and batch: 650, loss is 4.476665744781494 and perplexity is 87.94096567697736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.835807351505055 and perplexity of 125.94022019928762
Finished 9 epochs...
Completing Train Step...
At time: 74.70267057418823 and batch: 50, loss is 4.508427524566651 and perplexity is 90.7789585359038
At time: 75.30154228210449 and batch: 100, loss is 4.481855897903443 and perplexity is 88.39857926958712
At time: 75.9010648727417 and batch: 150, loss is 4.444142713546753 and perplexity is 85.12686843569125
At time: 76.49885058403015 and batch: 200, loss is 4.422928915023804 and perplexity is 83.34002408772173
At time: 77.09746813774109 and batch: 250, loss is 4.399354019165039 and perplexity is 81.39826995552784
At time: 77.6919732093811 and batch: 300, loss is 4.472572135925293 and perplexity is 87.58170559787553
At time: 78.28855204582214 and batch: 350, loss is 4.4179717063903805 and perplexity is 82.92791250628173
At time: 78.88688063621521 and batch: 400, loss is 4.456810789108276 and perplexity is 86.21212155820471
At time: 79.50018239021301 and batch: 450, loss is 4.423339033126831 and perplexity is 83.3742103500302
At time: 80.09882402420044 and batch: 500, loss is 4.39974157333374 and perplexity is 81.42982230810212
At time: 80.69819569587708 and batch: 550, loss is 4.411703987121582 and perplexity is 82.40976911604118
At time: 81.29531812667847 and batch: 600, loss is 4.514506378173828 and perplexity is 91.33247119218903
At time: 81.89571356773376 and batch: 650, loss is 4.459853897094726 and perplexity is 86.47487394301932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.832513248219209 and perplexity of 125.52604265244197
Finished 10 epochs...
Completing Train Step...
At time: 83.03059148788452 and batch: 50, loss is 4.4884570217132564 and perplexity is 88.98403945760799
At time: 83.64339923858643 and batch: 100, loss is 4.465912370681763 and perplexity is 87.00036992715106
At time: 84.24199533462524 and batch: 150, loss is 4.428159894943238 and perplexity is 83.77711629380165
At time: 84.83789157867432 and batch: 200, loss is 4.408296136856079 and perplexity is 82.12940694995093
At time: 85.43437647819519 and batch: 250, loss is 4.383408164978027 and perplexity is 80.11059880454069
At time: 86.03348088264465 and batch: 300, loss is 4.458751974105835 and perplexity is 86.37963777255938
At time: 86.63077211380005 and batch: 350, loss is 4.4027493381500244 and perplexity is 81.67511276571943
At time: 87.23060870170593 and batch: 400, loss is 4.4436310768127445 and perplexity is 85.0833255427701
At time: 87.83017921447754 and batch: 450, loss is 4.411931915283203 and perplexity is 82.42855476402319
At time: 88.42678046226501 and batch: 500, loss is 4.3872956275939945 and perplexity is 80.42263187802712
At time: 89.02331876754761 and batch: 550, loss is 4.400927753448486 and perplexity is 81.52647005355604
At time: 89.62318778038025 and batch: 600, loss is 4.502465009689331 and perplexity is 90.23929811055758
At time: 90.22259402275085 and batch: 650, loss is 4.445491199493408 and perplexity is 85.24173825422082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.829712811638327 and perplexity of 125.1750066871389
Finished 11 epochs...
Completing Train Step...
At time: 91.367666721344 and batch: 50, loss is 4.472683029174805 and perplexity is 87.5914183563368
At time: 91.9556725025177 and batch: 100, loss is 4.451257762908935 and perplexity is 85.7347101542996
At time: 92.5418860912323 and batch: 150, loss is 4.412770509719849 and perplexity is 82.4977078831269
At time: 93.1306209564209 and batch: 200, loss is 4.3940561008453365 and perplexity is 80.96816889608313
At time: 93.72205519676208 and batch: 250, loss is 4.369789915084839 and perplexity is 79.02702756910456
At time: 94.31089091300964 and batch: 300, loss is 4.446544389724732 and perplexity is 85.33156131233218
At time: 94.92056822776794 and batch: 350, loss is 4.390243043899536 and perplexity is 80.6600205242876
At time: 95.51154685020447 and batch: 400, loss is 4.432468242645264 and perplexity is 84.13883588764335
At time: 96.10140228271484 and batch: 450, loss is 4.401558609008789 and perplexity is 81.57791770681398
At time: 96.69632339477539 and batch: 500, loss is 4.374724311828613 and perplexity is 79.41794194675401
At time: 97.2961757183075 and batch: 550, loss is 4.390452108383179 and perplexity is 80.67688543269435
At time: 97.89571142196655 and batch: 600, loss is 4.491742258071899 and perplexity is 89.2768537781425
At time: 98.49508094787598 and batch: 650, loss is 4.431860151290894 and perplexity is 84.08768734204364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.827222637101715 and perplexity of 124.86368685410058
Finished 12 epochs...
Completing Train Step...
At time: 99.62670254707336 and batch: 50, loss is 4.457914237976074 and perplexity is 86.30730473135296
At time: 100.23473453521729 and batch: 100, loss is 4.4375940990448 and perplexity is 84.57122671794598
At time: 100.83096361160278 and batch: 150, loss is 4.400026330947876 and perplexity is 81.45301337178132
At time: 101.42758250236511 and batch: 200, loss is 4.380643148422241 and perplexity is 79.88939762594208
At time: 102.02330231666565 and batch: 250, loss is 4.356517658233643 and perplexity is 77.98509028561493
At time: 102.61916208267212 and batch: 300, loss is 4.4343946838378905 and perplexity is 84.30108063430202
At time: 103.21136713027954 and batch: 350, loss is 4.3784027004241945 and perplexity is 79.71060994200718
At time: 103.80867862701416 and batch: 400, loss is 4.420899038314819 and perplexity is 83.17102569487898
At time: 104.40616488456726 and batch: 450, loss is 4.389688081741333 and perplexity is 80.61526968387676
At time: 105.00256943702698 and batch: 500, loss is 4.363400154113769 and perplexity is 78.52367362101053
At time: 105.5989031791687 and batch: 550, loss is 4.379134473800659 and perplexity is 79.76896139159922
At time: 106.19406199455261 and batch: 600, loss is 4.480722455978394 and perplexity is 88.29844137469291
At time: 106.78566145896912 and batch: 650, loss is 4.419132289886474 and perplexity is 83.02421314455837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.82510316138174 and perplexity of 124.59932155838413
Finished 13 epochs...
Completing Train Step...
At time: 107.93428707122803 and batch: 50, loss is 4.443572130203247 and perplexity is 85.0783103170213
At time: 108.52825403213501 and batch: 100, loss is 4.423891620635986 and perplexity is 83.42029462888105
At time: 109.12295842170715 and batch: 150, loss is 4.387862691879272 and perplexity is 80.46824961316496
At time: 109.71688842773438 and batch: 200, loss is 4.368539085388184 and perplexity is 78.92824001226766
At time: 110.32233333587646 and batch: 250, loss is 4.344954795837403 and perplexity is 77.08855267706933
At time: 110.91739273071289 and batch: 300, loss is 4.423414726257324 and perplexity is 83.38052144386414
At time: 111.5115168094635 and batch: 350, loss is 4.3671372127532955 and perplexity is 78.8176701929846
At time: 112.10552906990051 and batch: 400, loss is 4.409171724319458 and perplexity is 82.20134992063639
At time: 112.7005865573883 and batch: 450, loss is 4.379674367904663 and perplexity is 79.8120398113831
At time: 113.2941026687622 and batch: 500, loss is 4.352611408233643 and perplexity is 77.6810552317518
At time: 113.88465332984924 and batch: 550, loss is 4.368518943786621 and perplexity is 78.92665028711517
At time: 114.48353624343872 and batch: 600, loss is 4.470391139984131 and perplexity is 87.39089840393937
At time: 115.08247661590576 and batch: 650, loss is 4.40692777633667 and perplexity is 82.01710116692146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.823779536228554 and perplexity of 124.43450786213906
Finished 14 epochs...
Completing Train Step...
At time: 116.21515822410583 and batch: 50, loss is 4.430401344299316 and perplexity is 83.96510906660447
At time: 116.8248565196991 and batch: 100, loss is 4.411590080261231 and perplexity is 82.40038261258262
At time: 117.4177975654602 and batch: 150, loss is 4.377003107070923 and perplexity is 79.59912553676196
At time: 118.0165445804596 and batch: 200, loss is 4.358119163513184 and perplexity is 78.11008388166931
At time: 118.61426115036011 and batch: 250, loss is 4.3329575252532955 and perplexity is 76.16922618289182
At time: 119.21106028556824 and batch: 300, loss is 4.41247784614563 and perplexity is 82.47356734177384
At time: 119.80766677856445 and batch: 350, loss is 4.356611385345459 and perplexity is 77.9923999454434
At time: 120.40475606918335 and batch: 400, loss is 4.398859853744507 and perplexity is 81.35805568229465
At time: 120.99677801132202 and batch: 450, loss is 4.369631490707397 and perplexity is 79.01450875312987
At time: 121.59428238868713 and batch: 500, loss is 4.341562566757202 and perplexity is 76.82749368322139
At time: 122.19075751304626 and batch: 550, loss is 4.357636251449585 and perplexity is 78.07237268620263
At time: 122.78897166252136 and batch: 600, loss is 4.460402336120605 and perplexity is 86.52231314621375
At time: 123.38614988327026 and batch: 650, loss is 4.39515453338623 and perplexity is 81.05715583170075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.822441849054075 and perplexity of 124.26816469927161
Finished 15 epochs...
Completing Train Step...
At time: 124.53551292419434 and batch: 50, loss is 4.418071346282959 and perplexity is 82.93617584624793
At time: 125.13557171821594 and batch: 100, loss is 4.399784536361694 and perplexity is 81.43332085498777
At time: 125.74954271316528 and batch: 150, loss is 4.365740270614624 and perplexity is 78.7076433366887
At time: 126.34907603263855 and batch: 200, loss is 4.347709007263184 and perplexity is 77.30116350283244
At time: 126.9474949836731 and batch: 250, loss is 4.3215626621246335 and perplexity is 75.30621456353666
At time: 127.54703783988953 and batch: 300, loss is 4.4017516040802 and perplexity is 81.59366336223546
At time: 128.14234733581543 and batch: 350, loss is 4.345130119323731 and perplexity is 77.10206929573638
At time: 128.7428276538849 and batch: 400, loss is 4.389147729873657 and perplexity is 80.57172083924924
At time: 129.3441903591156 and batch: 450, loss is 4.3601748752594 and perplexity is 78.27082085644079
At time: 129.94507384300232 and batch: 500, loss is 4.330688362121582 and perplexity is 75.99658173635325
At time: 130.5463891029358 and batch: 550, loss is 4.347548265457153 and perplexity is 77.28873897280015
At time: 131.14808583259583 and batch: 600, loss is 4.451086940765381 and perplexity is 85.72006601814105
At time: 131.74547410011292 and batch: 650, loss is 4.383488941192627 and perplexity is 80.11707009682117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.820167092715993 and perplexity of 123.98580617417815
Finished 16 epochs...
Completing Train Step...
At time: 132.8792278766632 and batch: 50, loss is 4.4052849292755125 and perplexity is 81.88247023260226
At time: 133.48956537246704 and batch: 100, loss is 4.38789665222168 and perplexity is 80.4709823888775
At time: 134.0860424041748 and batch: 150, loss is 4.354601430892944 and perplexity is 77.83579620982078
At time: 134.68403959274292 and batch: 200, loss is 4.335707483291626 and perplexity is 76.37897662898706
At time: 135.27581667900085 and batch: 250, loss is 4.309430837631226 and perplexity is 74.39813227038218
At time: 135.8718078136444 and batch: 300, loss is 4.391732883453369 and perplexity is 80.78028057512358
At time: 136.47018933296204 and batch: 350, loss is 4.336229400634766 and perplexity is 76.41885054608034
At time: 137.07081770896912 and batch: 400, loss is 4.378253335952759 and perplexity is 79.69870489800282
At time: 137.67054557800293 and batch: 450, loss is 4.34829568862915 and perplexity is 77.34652796096702
At time: 138.2716817855835 and batch: 500, loss is 4.31936484336853 and perplexity is 75.14088689938833
At time: 138.86685252189636 and batch: 550, loss is 4.335302896499634 and perplexity is 76.34808095426955
At time: 139.46720385551453 and batch: 600, loss is 4.440121393203736 and perplexity is 84.78523340025448
At time: 140.0671968460083 and batch: 650, loss is 4.370450391769409 and perplexity is 79.07924031901874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.814299041149663 and perplexity of 123.2603815661653
Finished 17 epochs...
Completing Train Step...
At time: 141.21390318870544 and batch: 50, loss is 4.388958282470703 and perplexity is 80.5564581817658
At time: 141.81118083000183 and batch: 100, loss is 4.37425576210022 and perplexity is 79.38073940792566
At time: 142.40403151512146 and batch: 150, loss is 4.339854917526245 and perplexity is 76.6964112259993
At time: 143.0002896785736 and batch: 200, loss is 4.320746383666992 and perplexity is 75.24476880469838
At time: 143.59718298912048 and batch: 250, loss is 4.293474264144898 and perplexity is 73.22041416664592
At time: 144.19276452064514 and batch: 300, loss is 4.379129333496094 and perplexity is 79.76855135589668
At time: 144.78980588912964 and batch: 350, loss is 4.323034553527832 and perplexity is 75.41713874751046
At time: 145.38638257980347 and batch: 400, loss is 4.363705816268921 and perplexity is 78.54767900490177
At time: 145.9814522266388 and batch: 450, loss is 4.3347030448913575 and perplexity is 76.30229716823199
At time: 146.5818293094635 and batch: 500, loss is 4.307446641921997 and perplexity is 74.25065817267681
At time: 147.18391680717468 and batch: 550, loss is 4.324186897277832 and perplexity is 75.50409530829123
At time: 147.7861843109131 and batch: 600, loss is 4.42915825843811 and perplexity is 83.8607980738816
At time: 148.38728618621826 and batch: 650, loss is 4.356000118255615 and perplexity is 77.94474032596126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.812847960229013 and perplexity of 123.08165048619449
Finished 18 epochs...
Completing Train Step...
At time: 149.5175850391388 and batch: 50, loss is 4.3776613712310795 and perplexity is 79.65154003768366
At time: 150.12948536872864 and batch: 100, loss is 4.362465343475342 and perplexity is 78.45030315463049
At time: 150.72634887695312 and batch: 150, loss is 4.328012323379516 and perplexity is 75.7934838095243
At time: 151.32314562797546 and batch: 200, loss is 4.3100909233093265 and perplexity is 74.44725762365867
At time: 151.9210193157196 and batch: 250, loss is 4.2824256706237795 and perplexity is 72.41588421941137
At time: 152.5170440673828 and batch: 300, loss is 4.367533693313598 and perplexity is 78.84892606278743
At time: 153.1102569103241 and batch: 350, loss is 4.310282764434814 and perplexity is 74.46154103938012
At time: 153.70545053482056 and batch: 400, loss is 4.352723655700683 and perplexity is 77.68977522282788
At time: 154.30329203605652 and batch: 450, loss is 4.323690643310547 and perplexity is 75.46663539703181
At time: 154.90132308006287 and batch: 500, loss is 4.2967315196990965 and perplexity is 73.4593006132638
At time: 155.49827933311462 and batch: 550, loss is 4.314077167510987 and perplexity is 74.74461484819257
At time: 156.10952019691467 and batch: 600, loss is 4.417685794830322 and perplexity is 82.90420584661018
At time: 156.70366501808167 and batch: 650, loss is 4.344258451461792 and perplexity is 77.034891182597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.80769557578891 and perplexity of 122.44911743142586
Finished 19 epochs...
Completing Train Step...
At time: 157.85200452804565 and batch: 50, loss is 4.3652917003631595 and perplexity is 78.67234534673099
At time: 158.4467475414276 and batch: 100, loss is 4.3506225490570065 and perplexity is 77.52671208646989
At time: 159.04028415679932 and batch: 150, loss is 4.317383852005005 and perplexity is 74.99218079278042
At time: 159.63473653793335 and batch: 200, loss is 4.299514093399048 and perplexity is 73.66399118239251
At time: 160.22686219215393 and batch: 250, loss is 4.271408243179321 and perplexity is 71.62242642708081
At time: 160.8184690475464 and batch: 300, loss is 4.356344537734985 and perplexity is 77.97159063646356
At time: 161.41206645965576 and batch: 350, loss is 4.299348268508911 and perplexity is 73.651776871894
At time: 162.00805807113647 and batch: 400, loss is 4.343230199813843 and perplexity is 76.95572063938525
At time: 162.60515904426575 and batch: 450, loss is 4.313606910705566 and perplexity is 74.70947394766308
At time: 163.20233345031738 and batch: 500, loss is 4.287953519821167 and perplexity is 72.81729675869636
At time: 163.79655957221985 and batch: 550, loss is 4.30362940788269 and perplexity is 73.9677663085935
At time: 164.3902714252472 and batch: 600, loss is 4.40761700630188 and perplexity is 82.07364929579927
At time: 164.9871063232422 and batch: 650, loss is 4.332714948654175 and perplexity is 76.1507515518939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.8089557722503065 and perplexity of 122.60352464719311
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 166.12085604667664 and batch: 50, loss is 4.353445768356323 and perplexity is 77.74589625314455
At time: 166.7285099029541 and batch: 100, loss is 4.322547092437744 and perplexity is 75.3803847856362
At time: 167.32187867164612 and batch: 150, loss is 4.280112056732178 and perplexity is 72.2485354885318
At time: 167.91269159317017 and batch: 200, loss is 4.250889558792114 and perplexity is 70.16780297856897
At time: 168.50831174850464 and batch: 250, loss is 4.219532713890076 and perplexity is 68.0017006138149
At time: 169.1031777858734 and batch: 300, loss is 4.295903005599976 and perplexity is 73.39846375257262
At time: 169.700825214386 and batch: 350, loss is 4.233762521743774 and perplexity is 68.9762692648265
At time: 170.3001937866211 and batch: 400, loss is 4.271195878982544 and perplexity is 71.60721800294094
At time: 170.89872670173645 and batch: 450, loss is 4.231996603012085 and perplexity is 68.85457026578696
At time: 171.50934410095215 and batch: 500, loss is 4.191124787330628 and perplexity is 66.09709433913065
At time: 172.10848355293274 and batch: 550, loss is 4.204262290000916 and perplexity is 66.97117413362021
At time: 172.70785522460938 and batch: 600, loss is 4.310459117889405 and perplexity is 74.47467374734174
At time: 173.30656051635742 and batch: 650, loss is 4.263159546852112 and perplexity is 71.03406472582371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.769952811446845 and perplexity of 117.91367765354022
Finished 21 epochs...
Completing Train Step...
At time: 174.45416736602783 and batch: 50, loss is 4.318274374008179 and perplexity is 75.05899272417457
At time: 175.04393935203552 and batch: 100, loss is 4.293122587203979 and perplexity is 73.1946687626768
At time: 175.63799810409546 and batch: 150, loss is 4.257048263549804 and perplexity is 70.60127921656198
At time: 176.2318148612976 and batch: 200, loss is 4.230422048568726 and perplexity is 68.74624030432162
At time: 176.82633876800537 and batch: 250, loss is 4.202316431999207 and perplexity is 66.84098444495098
At time: 177.42094254493713 and batch: 300, loss is 4.281522445678711 and perplexity is 72.35050591646694
At time: 178.0164065361023 and batch: 350, loss is 4.2213061237335205 and perplexity is 68.1224024943722
At time: 178.6105272769928 and batch: 400, loss is 4.262286672592163 and perplexity is 70.97208797203658
At time: 179.20959329605103 and batch: 450, loss is 4.226531248092652 and perplexity is 68.47928207719089
At time: 179.80950379371643 and batch: 500, loss is 4.191155276298523 and perplexity is 66.09910960203942
At time: 180.4089322090149 and batch: 550, loss is 4.208099322319031 and perplexity is 67.22863832648432
At time: 181.00676274299622 and batch: 600, loss is 4.314659194946289 and perplexity is 74.78813092722284
At time: 181.6061999797821 and batch: 650, loss is 4.260416259765625 and perplexity is 70.839464936945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.766510308957567 and perplexity of 117.50845741072806
Finished 22 epochs...
Completing Train Step...
At time: 182.74054312705994 and batch: 50, loss is 4.307915887832642 and perplexity is 74.28550816635604
At time: 183.35407781600952 and batch: 100, loss is 4.283114681243896 and perplexity is 72.4657967258526
At time: 183.9531648159027 and batch: 150, loss is 4.248607006072998 and perplexity is 70.00782391886291
At time: 184.55352926254272 and batch: 200, loss is 4.2220237445831295 and perplexity is 68.1713060957509
At time: 185.15291333198547 and batch: 250, loss is 4.195322861671448 and perplexity is 66.3751581126961
At time: 185.74885749816895 and batch: 300, loss is 4.274946327209473 and perplexity is 71.87628140553197
At time: 186.34580945968628 and batch: 350, loss is 4.216359434127807 and perplexity is 67.7862542101037
At time: 186.95983481407166 and batch: 400, loss is 4.259294786453247 and perplexity is 70.760064898389
At time: 187.56026887893677 and batch: 450, loss is 4.224979887008667 and perplexity is 68.37312834659059
At time: 188.1603343486786 and batch: 500, loss is 4.191239504814148 and perplexity is 66.1046772664001
At time: 188.7593777179718 and batch: 550, loss is 4.2094800090789795 and perplexity is 67.32152412565196
At time: 189.35667967796326 and batch: 600, loss is 4.31546690940857 and perplexity is 74.84856278473683
At time: 189.95352625846863 and batch: 650, loss is 4.2572329807281495 and perplexity is 70.6143216901938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.764811796300552 and perplexity of 117.30903721531556
Finished 23 epochs...
Completing Train Step...
At time: 191.1031424999237 and batch: 50, loss is 4.301034908294678 and perplexity is 73.77610570856604
At time: 191.7003035545349 and batch: 100, loss is 4.276238889694214 and perplexity is 71.9692460587879
At time: 192.29784321784973 and batch: 150, loss is 4.242758378982544 and perplexity is 69.59956929162774
At time: 192.89158654212952 and batch: 200, loss is 4.216215467453003 and perplexity is 67.77649595093654
At time: 193.48441123962402 and batch: 250, loss is 4.190281057357788 and perplexity is 66.04134975951243
At time: 194.08304119110107 and batch: 300, loss is 4.270339126586914 and perplexity is 71.54589462060096
At time: 194.68289470672607 and batch: 350, loss is 4.213046441078186 and perplexity is 67.56205041891697
At time: 195.28237962722778 and batch: 400, loss is 4.257290906906128 and perplexity is 70.61841222643329
At time: 195.88273215293884 and batch: 450, loss is 4.2235207271575925 and perplexity is 68.27343377566125
At time: 196.48025226593018 and batch: 500, loss is 4.190527906417847 and perplexity is 66.05765401688807
At time: 197.07546663284302 and batch: 550, loss is 4.209228916168213 and perplexity is 67.30462229025633
At time: 197.67797327041626 and batch: 600, loss is 4.31473524093628 and perplexity is 74.79381848093382
At time: 198.28003239631653 and batch: 650, loss is 4.253969955444336 and perplexity is 70.3842808917003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.763621610753677 and perplexity of 117.16950074831881
Finished 24 epochs...
Completing Train Step...
At time: 199.41516613960266 and batch: 50, loss is 4.295563259124756 and perplexity is 73.37353111885318
At time: 200.02529120445251 and batch: 100, loss is 4.270707111358643 and perplexity is 71.57222726500999
At time: 200.6175181865692 and batch: 150, loss is 4.23774076461792 and perplexity is 69.25122016461953
At time: 201.2149736881256 and batch: 200, loss is 4.21137692451477 and perplexity is 67.44934856167065
At time: 201.81212043762207 and batch: 250, loss is 4.185824251174926 and perplexity is 65.74767118393964
At time: 202.42298769950867 and batch: 300, loss is 4.2661683559417725 and perplexity is 71.24811452145016
At time: 203.02002930641174 and batch: 350, loss is 4.209904041290283 and perplexity is 67.35007667356749
At time: 203.6178560256958 and batch: 400, loss is 4.2550036907196045 and perplexity is 70.45707722526888
At time: 204.2126443386078 and batch: 450, loss is 4.221608190536499 and perplexity is 68.14298311891014
At time: 204.8128263950348 and batch: 500, loss is 4.189502387046814 and perplexity is 65.98994533730406
At time: 205.41233015060425 and batch: 550, loss is 4.208210744857788 and perplexity is 67.23612952938052
At time: 206.0115978717804 and batch: 600, loss is 4.313230075836182 and perplexity is 74.68132611668143
At time: 206.61120796203613 and batch: 650, loss is 4.250725789070129 and perplexity is 70.15631255790008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.762855679381127 and perplexity of 117.07979131181133
Finished 25 epochs...
Completing Train Step...
At time: 207.75963735580444 and batch: 50, loss is 4.29103307723999 and perplexity is 73.04188744761397
At time: 208.35739302635193 and batch: 100, loss is 4.266117572784424 and perplexity is 71.24449640910998
At time: 208.95509314537048 and batch: 150, loss is 4.233606653213501 and perplexity is 68.96551887295814
At time: 209.55201077461243 and batch: 200, loss is 4.206903095245361 and perplexity is 67.14826569073784
At time: 210.1490662097931 and batch: 250, loss is 4.181980032920837 and perplexity is 65.49540797429052
At time: 210.74573969841003 and batch: 300, loss is 4.26233980178833 and perplexity is 70.97585876218946
At time: 211.33811974525452 and batch: 350, loss is 4.206833119392395 and perplexity is 67.14356709796685
At time: 211.9351851940155 and batch: 400, loss is 4.251779298782349 and perplexity is 70.23026186086821
At time: 212.53146743774414 and batch: 450, loss is 4.21922872543335 and perplexity is 67.98103202345618
At time: 213.1289255619049 and batch: 500, loss is 4.187987470626831 and perplexity is 65.890051769866
At time: 213.72526574134827 and batch: 550, loss is 4.206698932647705 and perplexity is 67.13455792574037
At time: 214.32212710380554 and batch: 600, loss is 4.311420412063598 and perplexity is 74.54630023888879
At time: 214.91483545303345 and batch: 650, loss is 4.247493553161621 and perplexity is 69.92991688445737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.7622204948874085 and perplexity of 117.00544765731006
Finished 26 epochs...
Completing Train Step...
At time: 216.04246735572815 and batch: 50, loss is 4.286808738708496 and perplexity is 72.73398458888899
At time: 216.6476948261261 and batch: 100, loss is 4.261954116821289 and perplexity is 70.94848971869565
At time: 217.23748421669006 and batch: 150, loss is 4.22988468170166 and perplexity is 68.70930827645948
At time: 217.84599375724792 and batch: 200, loss is 4.203163766860962 and perplexity is 66.8976451431676
At time: 218.43380308151245 and batch: 250, loss is 4.178891201019287 and perplexity is 65.29341578928555
At time: 219.0263557434082 and batch: 300, loss is 4.259021816253662 and perplexity is 70.74075214537396
At time: 219.62363600730896 and batch: 350, loss is 4.203983116149902 and perplexity is 66.95248014258858
At time: 220.21814441680908 and batch: 400, loss is 4.2491586303710935 and perplexity is 70.04645258887032
At time: 220.8154821395874 and batch: 450, loss is 4.217117195129394 and perplexity is 67.83763945639686
At time: 221.41380548477173 and batch: 500, loss is 4.186242051124573 and perplexity is 65.7751462968004
At time: 222.00619220733643 and batch: 550, loss is 4.204912734031677 and perplexity is 67.014749304137
At time: 222.60324716567993 and batch: 600, loss is 4.309567155838013 and perplexity is 74.40827478165039
At time: 223.20199060440063 and batch: 650, loss is 4.244498062133789 and perplexity is 69.72075587222024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.761280433804381 and perplexity of 116.89550707299394
Finished 27 epochs...
Completing Train Step...
At time: 224.35713052749634 and batch: 50, loss is 4.282959394454956 and perplexity is 72.45454461864543
At time: 224.9542202949524 and batch: 100, loss is 4.257791452407837 and perplexity is 70.65376880306052
At time: 225.54551553726196 and batch: 150, loss is 4.22616641998291 and perplexity is 68.45430346688083
At time: 226.14190030097961 and batch: 200, loss is 4.19922758102417 and perplexity is 66.63484114184823
At time: 226.73736095428467 and batch: 250, loss is 4.17543210029602 and perplexity is 65.06794946767812
At time: 227.33350324630737 and batch: 300, loss is 4.255796451568603 and perplexity is 70.51295498355103
At time: 227.92981791496277 and batch: 350, loss is 4.201342329978943 and perplexity is 66.77590620853344
At time: 228.52686738967896 and batch: 400, loss is 4.246442975997925 and perplexity is 69.85648868846599
At time: 229.11959385871887 and batch: 450, loss is 4.214653062820434 and perplexity is 67.67068432149836
At time: 229.71568608283997 and batch: 500, loss is 4.184521975517273 and perplexity is 65.66210531947338
At time: 230.31447625160217 and batch: 550, loss is 4.203085255622864 and perplexity is 66.89239313239517
At time: 230.91163063049316 and batch: 600, loss is 4.30757905960083 and perplexity is 74.26049092348448
At time: 231.50834560394287 and batch: 650, loss is 4.241631555557251 and perplexity is 69.521187036334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.760415769090839 and perplexity of 116.79447533843889
Finished 28 epochs...
Completing Train Step...
At time: 232.63336849212646 and batch: 50, loss is 4.279427909851075 and perplexity is 72.19912378267254
At time: 233.2361855506897 and batch: 100, loss is 4.254134340286255 and perplexity is 70.39585195161564
At time: 233.82613801956177 and batch: 150, loss is 4.222640819549561 and perplexity is 68.21338588402821
At time: 234.41702818870544 and batch: 200, loss is 4.195696125030517 and perplexity is 66.39993815162458
At time: 235.01085829734802 and batch: 250, loss is 4.172315545082093 and perplexity is 64.86547728226054
At time: 235.60713863372803 and batch: 300, loss is 4.252664861679077 and perplexity is 70.29248272117607
At time: 236.2013294696808 and batch: 350, loss is 4.1988036870956424 and perplexity is 66.60660102309012
At time: 236.79602479934692 and batch: 400, loss is 4.243832015991211 and perplexity is 69.67433409295359
At time: 237.39302921295166 and batch: 450, loss is 4.212247543334961 and perplexity is 67.50809680388987
At time: 237.98853731155396 and batch: 500, loss is 4.182626051902771 and perplexity is 65.5377329209589
At time: 238.58529806137085 and batch: 550, loss is 4.200946865081787 and perplexity is 66.74950390258886
At time: 239.18096470832825 and batch: 600, loss is 4.3052327346801755 and perplexity is 74.08645593416898
At time: 239.77554988861084 and batch: 650, loss is 4.238453164100647 and perplexity is 69.30057227516478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.759521783566942 and perplexity of 116.69010942597353
Finished 29 epochs...
Completing Train Step...
At time: 240.9238941669464 and batch: 50, loss is 4.2759685230255124 and perplexity is 71.94979060365387
At time: 241.51668572425842 and batch: 100, loss is 4.250577621459961 and perplexity is 70.14591843478534
At time: 242.10862350463867 and batch: 150, loss is 4.219285621643066 and perplexity is 67.98489999654636
At time: 242.70214533805847 and batch: 200, loss is 4.191866660118103 and perplexity is 66.14614816835274
At time: 243.29809021949768 and batch: 250, loss is 4.169196896553039 and perplexity is 64.66349976908728
At time: 243.8951735496521 and batch: 300, loss is 4.249358310699463 and perplexity is 70.06044088407155
At time: 244.49686789512634 and batch: 350, loss is 4.1956318807601924 and perplexity is 66.39567247307258
At time: 245.10336828231812 and batch: 400, loss is 4.24076355934143 and perplexity is 69.46086909072842
At time: 245.70852708816528 and batch: 450, loss is 4.209570512771607 and perplexity is 67.32761724789951
At time: 246.31403493881226 and batch: 500, loss is 4.179951391220093 and perplexity is 65.36267593686
At time: 246.91847729682922 and batch: 550, loss is 4.198274879455567 and perplexity is 66.57138825480065
At time: 247.5201714038849 and batch: 600, loss is 4.302591800689697 and perplexity is 73.89105662636344
At time: 248.1441674232483 and batch: 650, loss is 4.235080432891846 and perplexity is 69.06723378745598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.758508420458027 and perplexity of 116.57191986848873
Finished 30 epochs...
Completing Train Step...
At time: 249.28495955467224 and batch: 50, loss is 4.272285032272339 and perplexity is 71.68525172763188
At time: 249.90194940567017 and batch: 100, loss is 4.246755142211914 and perplexity is 69.87829892809525
At time: 250.50247836112976 and batch: 150, loss is 4.215629968643189 and perplexity is 67.73682450814847
At time: 251.10035634040833 and batch: 200, loss is 4.188410234451294 and perplexity is 65.91791358921596
At time: 251.70482683181763 and batch: 250, loss is 4.165905027389527 and perplexity is 64.4509859639207
At time: 252.30652236938477 and batch: 300, loss is 4.2464220905303955 and perplexity is 69.85502971827545
At time: 252.90944004058838 and batch: 350, loss is 4.193056993484497 and perplexity is 66.22493101519935
At time: 253.5130274295807 and batch: 400, loss is 4.2377767086029055 and perplexity is 69.25370937417314
At time: 254.11406540870667 and batch: 450, loss is 4.206578950881958 and perplexity is 67.12650348614042
At time: 254.71210741996765 and batch: 500, loss is 4.177690382003784 and perplexity is 65.21505727063413
At time: 255.31669998168945 and batch: 550, loss is 4.196174993515014 and perplexity is 66.4317426038451
At time: 255.9189829826355 and batch: 600, loss is 4.299962701797486 and perplexity is 73.69704488102845
At time: 256.52152252197266 and batch: 650, loss is 4.2318990468978885 and perplexity is 68.8478534091077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.757209029852175 and perplexity of 116.42054577923889
Finished 31 epochs...
Completing Train Step...
At time: 257.6701512336731 and batch: 50, loss is 4.268621320724487 and perplexity is 71.42309816382203
At time: 258.2642869949341 and batch: 100, loss is 4.243086652755737 and perplexity is 69.62242075541462
At time: 258.864942073822 and batch: 150, loss is 4.212078218460083 and perplexity is 67.4966669715501
At time: 259.4636459350586 and batch: 200, loss is 4.184905281066895 and perplexity is 65.68727879310002
At time: 260.06310415267944 and batch: 250, loss is 4.162936959266663 and perplexity is 64.25997465450834
At time: 260.6615011692047 and batch: 300, loss is 4.2434834957122805 and perplexity is 69.65005540565437
At time: 261.26009607315063 and batch: 350, loss is 4.190493593215942 and perplexity is 66.05538740615599
At time: 261.8539762496948 and batch: 400, loss is 4.23510968208313 and perplexity is 69.06925397773281
At time: 262.45272493362427 and batch: 450, loss is 4.203998231887818 and perplexity is 66.95349218638009
At time: 263.05048727989197 and batch: 500, loss is 4.174855351448059 and perplexity is 65.03043242277963
At time: 263.66426277160645 and batch: 550, loss is 4.19291916847229 and perplexity is 66.21580419224048
At time: 264.26244950294495 and batch: 600, loss is 4.297010746002197 and perplexity is 73.47981524618167
At time: 264.8609507083893 and batch: 650, loss is 4.229102611541748 and perplexity is 68.65559378373705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.756409588982077 and perplexity of 116.32751162942952
Finished 32 epochs...
Completing Train Step...
At time: 265.9840030670166 and batch: 50, loss is 4.265565595626831 and perplexity is 71.20518192583451
At time: 266.587956905365 and batch: 100, loss is 4.239801759719849 and perplexity is 69.39409377050869
At time: 267.17612409591675 and batch: 150, loss is 4.208562612533569 and perplexity is 67.25979191277634
At time: 267.76588702201843 and batch: 200, loss is 4.181322402954102 and perplexity is 65.45235039085344
At time: 268.359295129776 and batch: 250, loss is 4.159872550964355 and perplexity is 64.06335726651196
At time: 268.94701409339905 and batch: 300, loss is 4.240280447006225 and perplexity is 69.427319792749
At time: 269.54059624671936 and batch: 350, loss is 4.187517914772034 and perplexity is 65.85911997295605
At time: 270.13580417633057 and batch: 400, loss is 4.231513376235962 and perplexity is 68.82130593153187
At time: 270.7315893173218 and batch: 450, loss is 4.2006582927703855 and perplexity is 66.73024462294387
At time: 271.3283221721649 and batch: 500, loss is 4.172225213050842 and perplexity is 64.85961811657874
At time: 271.92582607269287 and batch: 550, loss is 4.1904628419876095 and perplexity is 66.05335615308718
At time: 272.5183346271515 and batch: 600, loss is 4.2941481876373295 and perplexity is 73.26977575497719
At time: 273.11386156082153 and batch: 650, loss is 4.2253592538833615 and perplexity is 68.39907176733062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.754534253887102 and perplexity of 116.10956299159942
Finished 33 epochs...
Completing Train Step...
At time: 274.2576279640198 and batch: 50, loss is 4.261761293411255 and perplexity is 70.93481050784978
At time: 274.8512372970581 and batch: 100, loss is 4.236182098388672 and perplexity is 69.14336470358279
At time: 275.44507455825806 and batch: 150, loss is 4.205057315826416 and perplexity is 67.02443911733374
At time: 276.03466033935547 and batch: 200, loss is 4.178001079559326 and perplexity is 65.23532257754029
At time: 276.62716484069824 and batch: 250, loss is 4.156974220275879 and perplexity is 63.87794928874608
At time: 277.2208924293518 and batch: 300, loss is 4.236275615692139 and perplexity is 69.14983110695812
At time: 277.81475353240967 and batch: 350, loss is 4.183529949188232 and perplexity is 65.59699908106687
At time: 278.4098355770111 and batch: 400, loss is 4.2277717018127445 and perplexity is 68.56428016459063
At time: 279.02236104011536 and batch: 450, loss is 4.197584524154663 and perplexity is 66.52544620402419
At time: 279.6158447265625 and batch: 500, loss is 4.169321908950805 and perplexity is 64.67158401354622
At time: 280.36630415916443 and batch: 550, loss is 4.187251453399658 and perplexity is 65.84157339931046
At time: 280.9609034061432 and batch: 600, loss is 4.291087121963501 and perplexity is 73.04583508289925
At time: 281.5575728416443 and batch: 650, loss is 4.22220094203949 and perplexity is 68.18338694810437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.753024830537684 and perplexity of 115.93443670922906
Finished 34 epochs...
Completing Train Step...
At time: 282.70380687713623 and batch: 50, loss is 4.257815856933593 and perplexity is 70.65549309582127
At time: 283.29380202293396 and batch: 100, loss is 4.232154169082642 and perplexity is 68.86542026463698
At time: 283.88756561279297 and batch: 150, loss is 4.20077356338501 and perplexity is 66.73793710260549
At time: 284.48203206062317 and batch: 200, loss is 4.174554667472839 and perplexity is 65.0108817532823
At time: 285.0739703178406 and batch: 250, loss is 4.153213567733765 and perplexity is 63.63817764786466
At time: 285.66753101348877 and batch: 300, loss is 4.232499589920044 and perplexity is 68.88921192460387
At time: 286.27453541755676 and batch: 350, loss is 4.179267783164978 and perplexity is 65.31800875424808
At time: 286.864483833313 and batch: 400, loss is 4.223184304237366 and perplexity is 68.25046889086993
At time: 287.45761489868164 and batch: 450, loss is 4.193742837905884 and perplexity is 66.27036659378089
At time: 288.05234026908875 and batch: 500, loss is 4.1663820886611935 and perplexity is 64.48174036852924
At time: 288.6468448638916 and batch: 550, loss is 4.183763556480407 and perplexity is 65.61232480842816
At time: 289.2437438964844 and batch: 600, loss is 4.287478342056274 and perplexity is 72.78270381792603
At time: 289.8426477909088 and batch: 650, loss is 4.217642035484314 and perplexity is 67.87325273199065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.751127056047028 and perplexity of 115.71462793236945
Finished 35 epochs...
Completing Train Step...
At time: 290.9930512905121 and batch: 50, loss is 4.253921928405762 and perplexity is 70.38090062429971
At time: 291.58669352531433 and batch: 100, loss is 4.227845029830933 and perplexity is 68.5693080317131
At time: 292.1812403202057 and batch: 150, loss is 4.196223526000977 and perplexity is 66.43496677969847
At time: 292.7774693965912 and batch: 200, loss is 4.16998321056366 and perplexity is 64.71436558056078
At time: 293.3747293949127 and batch: 250, loss is 4.149170379638672 and perplexity is 63.381395983697864
At time: 293.96863532066345 and batch: 300, loss is 4.229176073074341 and perplexity is 68.6606375141353
At time: 294.5801773071289 and batch: 350, loss is 4.175471820831299 and perplexity is 65.07053405279068
At time: 295.1787180900574 and batch: 400, loss is 4.218550033569336 and perplexity is 67.93490950337458
At time: 295.77486777305603 and batch: 450, loss is 4.189976482391358 and perplexity is 66.02123828050837
At time: 296.3718099594116 and batch: 500, loss is 4.162929043769837 and perplexity is 64.25946600689603
At time: 296.96908736228943 and batch: 550, loss is 4.180072479248047 and perplexity is 65.37059105359424
At time: 297.5619447231293 and batch: 600, loss is 4.283096799850464 and perplexity is 72.46450094801612
At time: 298.15671491622925 and batch: 650, loss is 4.21324248790741 and perplexity is 67.57529704311438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.748114193187041 and perplexity of 115.36652029113091
Finished 36 epochs...
Completing Train Step...
At time: 299.3042302131653 and batch: 50, loss is 4.248914318084717 and perplexity is 70.02934147019913
At time: 299.9011652469635 and batch: 100, loss is 4.223572149276733 and perplexity is 68.27694463057401
At time: 300.4969696998596 and batch: 150, loss is 4.191247358322143 and perplexity is 66.10519642205013
At time: 301.0918881893158 and batch: 200, loss is 4.165665693283081 and perplexity is 64.43556249054075
At time: 301.6831684112549 and batch: 250, loss is 4.144906806945801 and perplexity is 63.111740051864246
At time: 302.2740433216095 and batch: 300, loss is 4.222837715148926 and perplexity is 68.22681812185745
At time: 302.86718916893005 and batch: 350, loss is 4.169449181556701 and perplexity is 64.67981545837867
At time: 303.4597969055176 and batch: 400, loss is 4.212669973373413 and perplexity is 67.53662027598013
At time: 304.05209708213806 and batch: 450, loss is 4.185515556335449 and perplexity is 65.72737834944532
At time: 304.6444809436798 and batch: 500, loss is 4.158176593780517 and perplexity is 63.954800635254216
At time: 305.23690271377563 and batch: 550, loss is 4.175881977081299 and perplexity is 65.09722861311943
At time: 305.8444266319275 and batch: 600, loss is 4.278606157302857 and perplexity is 72.13981833926198
At time: 306.436491727829 and batch: 650, loss is 4.208466515541077 and perplexity is 67.25332875960761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.747149598364737 and perplexity of 115.25529199673358
Finished 37 epochs...
Completing Train Step...
At time: 307.5610821247101 and batch: 50, loss is 4.244446592330933 and perplexity is 69.71716745100898
At time: 308.14886927604675 and batch: 100, loss is 4.218553857803345 and perplexity is 67.93516930286265
At time: 308.7358672618866 and batch: 150, loss is 4.187378425598144 and perplexity is 65.84993397940613
At time: 309.3293192386627 and batch: 200, loss is 4.1623351192474365 and perplexity is 64.22131206564042
At time: 309.9247667789459 and batch: 250, loss is 4.141418170928955 and perplexity is 62.89194976949572
At time: 310.52078008651733 and batch: 300, loss is 4.219570016860962 and perplexity is 68.00423732658624
At time: 311.1175448894501 and batch: 350, loss is 4.166640768051147 and perplexity is 64.49842262337566
At time: 311.71648621559143 and batch: 400, loss is 4.207238698005677 and perplexity is 67.17080461590524
At time: 312.3109700679779 and batch: 450, loss is 4.181560482978821 and perplexity is 65.4679351431879
At time: 312.91236996650696 and batch: 500, loss is 4.156732239723206 and perplexity is 63.862493937296186
At time: 313.5113925933838 and batch: 550, loss is 4.174271278381347 and perplexity is 64.99246098881028
At time: 314.1121037006378 and batch: 600, loss is 4.274073247909546 and perplexity is 71.81355509858567
At time: 314.71373867988586 and batch: 650, loss is 4.204811305999756 and perplexity is 67.00795247470569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.742417877795649 and perplexity of 114.71122436400277
Finished 38 epochs...
Completing Train Step...
At time: 315.85038805007935 and batch: 50, loss is 4.239514865875244 and perplexity is 69.37418788772817
At time: 316.46321725845337 and batch: 100, loss is 4.213007965087891 and perplexity is 67.55945095212958
At time: 317.06219601631165 and batch: 150, loss is 4.182519845962524 and perplexity is 65.53077279402234
At time: 317.6613371372223 and batch: 200, loss is 4.156294584274292 and perplexity is 63.83455028413538
At time: 318.259649515152 and batch: 250, loss is 4.137155041694641 and perplexity is 62.624403956223475
At time: 318.8586733341217 and batch: 300, loss is 4.217004251480103 and perplexity is 67.82997805849858
At time: 319.45417070388794 and batch: 350, loss is 4.163098592758178 and perplexity is 64.27036205802999
At time: 320.0554840564728 and batch: 400, loss is 4.203044400215149 and perplexity is 66.88966027222718
At time: 320.6570646762848 and batch: 450, loss is 4.1776139307022095 and perplexity is 65.2100716852032
At time: 321.2725918292999 and batch: 500, loss is 4.152542200088501 and perplexity is 63.59546737314085
At time: 321.87340116500854 and batch: 550, loss is 4.170479054450989 and perplexity is 64.74646175984581
At time: 322.47476530075073 and batch: 600, loss is 4.271105690002441 and perplexity is 71.60076011220093
At time: 323.0711009502411 and batch: 650, loss is 4.1984055757522585 and perplexity is 66.58008945730627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.740795958275888 and perplexity of 114.52532278952042
Finished 39 epochs...
Completing Train Step...
At time: 324.2157187461853 and batch: 50, loss is 4.233944034576416 and perplexity is 68.98879047919198
At time: 324.81022238731384 and batch: 100, loss is 4.208061232566833 and perplexity is 67.2260776530779
At time: 325.4050850868225 and batch: 150, loss is 4.17805356502533 and perplexity is 65.2387465736999
At time: 325.99923944473267 and batch: 200, loss is 4.151870393753052 and perplexity is 63.55275788311306
At time: 326.5891273021698 and batch: 250, loss is 4.1336013698577885 and perplexity is 62.40225233635038
At time: 327.18154883384705 and batch: 300, loss is 4.213936519622803 and perplexity is 67.62221272104043
At time: 327.776832818985 and batch: 350, loss is 4.158006572723389 and perplexity is 63.943927896765224
At time: 328.37059926986694 and batch: 400, loss is 4.198486185073852 and perplexity is 66.58545664946902
At time: 328.96515011787415 and batch: 450, loss is 4.175005645751953 and perplexity is 65.0402068608553
At time: 329.55971598625183 and batch: 500, loss is 4.150033960342407 and perplexity is 63.43615457507348
At time: 330.1527729034424 and batch: 550, loss is 4.16707094669342 and perplexity is 64.5261744359391
At time: 330.74877882003784 and batch: 600, loss is 4.267386054992675 and perplexity is 71.33492612735967
At time: 331.3483564853668 and batch: 650, loss is 4.194334273338318 and perplexity is 66.3095728295405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.739069022384345 and perplexity of 114.32771557571002
Finished 40 epochs...
Completing Train Step...
At time: 332.48133850097656 and batch: 50, loss is 4.230894107818603 and perplexity is 68.77870026386086
At time: 333.0898861885071 and batch: 100, loss is 4.205258431434632 and perplexity is 67.03792013374813
At time: 333.68395018577576 and batch: 150, loss is 4.1734717655181885 and perplexity is 64.94051944697364
At time: 334.27779507637024 and batch: 200, loss is 4.1504141712188725 and perplexity is 63.460278276760334
At time: 334.87443566322327 and batch: 250, loss is 4.132226204872131 and perplexity is 62.316497920660204
At time: 335.47119545936584 and batch: 300, loss is 4.2087252235412596 and perplexity is 67.27072998461765
At time: 336.0673222541809 and batch: 350, loss is 4.152466292381287 and perplexity is 63.59064017023662
At time: 336.67833280563354 and batch: 400, loss is 4.194710216522217 and perplexity is 66.33450614793492
At time: 337.27257561683655 and batch: 450, loss is 4.172861213684082 and perplexity is 64.90088199530818
At time: 337.8657670021057 and batch: 500, loss is 4.1510645246505735 and perplexity is 63.50156330999069
At time: 338.4632489681244 and batch: 550, loss is 4.1671191644668575 and perplexity is 64.5292858194102
At time: 339.0599031448364 and batch: 600, loss is 4.267680931091308 and perplexity is 71.3559641937314
At time: 339.656662940979 and batch: 650, loss is 4.192056331634522 and perplexity is 66.15869539846953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.740785486557904 and perplexity of 114.52412351891732
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 340.7912051677704 and batch: 50, loss is 4.227779970169068 and perplexity is 68.5648470808338
At time: 341.3763778209686 and batch: 100, loss is 4.1986953639984135 and perplexity is 66.59938638053467
At time: 341.9666118621826 and batch: 150, loss is 4.16627881526947 and perplexity is 64.47508146434687
At time: 342.5558571815491 and batch: 200, loss is 4.1392389583587645 and perplexity is 62.755044069485486
At time: 343.1447596549988 and batch: 250, loss is 4.118528184890747 and perplexity is 61.46870508488771
At time: 343.73442673683167 and batch: 300, loss is 4.195248727798462 and perplexity is 66.37023764754396
At time: 344.32674050331116 and batch: 350, loss is 4.136539378166199 and perplexity is 62.58586026090837
At time: 344.9180119037628 and batch: 400, loss is 4.179449620246888 and perplexity is 65.32988707028257
At time: 345.51483058929443 and batch: 450, loss is 4.154732799530029 and perplexity is 63.73493226854202
At time: 346.1115880012512 and batch: 500, loss is 4.125377774238586 and perplexity is 61.89118573013837
At time: 346.70862674713135 and batch: 550, loss is 4.13822678565979 and perplexity is 62.69155726238201
At time: 347.30658984184265 and batch: 600, loss is 4.23929723739624 and perplexity is 69.35909173147236
At time: 347.90597772598267 and batch: 650, loss is 4.174305448532104 and perplexity is 64.99468182894334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.731387867647059 and perplexity of 113.45291075832573
Finished 42 epochs...
Completing Train Step...
At time: 349.0465865135193 and batch: 50, loss is 4.222693381309509 and perplexity is 68.21697139387182
At time: 349.65941882133484 and batch: 100, loss is 4.193860149383545 and perplexity is 66.2781413244348
At time: 350.25980281829834 and batch: 150, loss is 4.161584601402283 and perplexity is 64.17313090756706
At time: 350.86079359054565 and batch: 200, loss is 4.13569167137146 and perplexity is 62.53282828287046
At time: 351.460253238678 and batch: 250, loss is 4.115327453613281 and perplexity is 61.27227480590318
At time: 352.07797026634216 and batch: 300, loss is 4.19270541191101 and perplexity is 66.20165164228807
At time: 352.6780278682709 and batch: 350, loss is 4.134441018104553 and perplexity is 62.45467028135698
At time: 353.27834463119507 and batch: 400, loss is 4.178204884529114 and perplexity is 65.24861921540172
At time: 353.87837433815 and batch: 450, loss is 4.153736705780029 and perplexity is 63.67147790934461
At time: 354.4782247543335 and batch: 500, loss is 4.12481520652771 and perplexity is 61.856377539358874
At time: 355.07808089256287 and batch: 550, loss is 4.13901939868927 and perplexity is 62.74126710523869
At time: 355.67311668395996 and batch: 600, loss is 4.240371675491333 and perplexity is 69.43365383087672
At time: 356.27359533309937 and batch: 650, loss is 4.174297685623169 and perplexity is 64.9941772831054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.730418784945619 and perplexity of 113.34301876089886
Finished 43 epochs...
Completing Train Step...
At time: 357.4265522956848 and batch: 50, loss is 4.220298862457275 and perplexity is 68.0538199823476
At time: 358.0250401496887 and batch: 100, loss is 4.1913887310028075 and perplexity is 66.11454255150227
At time: 358.62394738197327 and batch: 150, loss is 4.159331469535828 and perplexity is 64.0287031498409
At time: 359.2171378135681 and batch: 200, loss is 4.13356873512268 and perplexity is 62.40021588860488
At time: 359.81642746925354 and batch: 250, loss is 4.113982825279236 and perplexity is 61.18994173520265
At time: 360.4153792858124 and batch: 300, loss is 4.19138575553894 and perplexity is 66.11434583036247
At time: 361.01385378837585 and batch: 350, loss is 4.133479318618774 and perplexity is 62.39463652890428
At time: 361.61325883865356 and batch: 400, loss is 4.177585124969482 and perplexity is 65.20819328836156
At time: 362.212495803833 and batch: 450, loss is 4.1532330989837645 and perplexity is 63.63942059315991
At time: 362.8065297603607 and batch: 500, loss is 4.1241789102554325 and perplexity is 61.8170310762454
At time: 363.40504026412964 and batch: 550, loss is 4.139427318572998 and perplexity is 62.766865736361524
At time: 364.0036430358887 and batch: 600, loss is 4.240904045104981 and perplexity is 69.47062803944013
At time: 364.6025502681732 and batch: 650, loss is 4.174269890785217 and perplexity is 64.99237080558548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.730171652401195 and perplexity of 113.31501147317843
Finished 44 epochs...
Completing Train Step...
At time: 365.736825466156 and batch: 50, loss is 4.218619303703308 and perplexity is 67.93961552664881
At time: 366.3392016887665 and batch: 100, loss is 4.189229922294617 and perplexity is 65.97196785242691
At time: 366.929945230484 and batch: 150, loss is 4.157583122253418 and perplexity is 63.91685654253888
At time: 367.53727769851685 and batch: 200, loss is 4.131644020080566 and perplexity is 62.28022876200792
At time: 368.13359904289246 and batch: 250, loss is 4.113203053474426 and perplexity is 61.14224614215566
At time: 368.7298560142517 and batch: 300, loss is 4.190375471115113 and perplexity is 66.04758526583615
At time: 369.32537055015564 and batch: 350, loss is 4.132757778167725 and perplexity is 62.34963251278168
At time: 369.91738271713257 and batch: 400, loss is 4.1771402835845945 and perplexity is 65.17919243622097
At time: 370.5146188735962 and batch: 450, loss is 4.1527394199371335 and perplexity is 63.60801089846533
At time: 371.11479926109314 and batch: 500, loss is 4.123595232963562 and perplexity is 61.780960406794726
At time: 371.7140197753906 and batch: 550, loss is 4.139574265480041 and perplexity is 62.7760898108543
At time: 372.31238293647766 and batch: 600, loss is 4.2411722946167 and perplexity is 69.48926600119084
At time: 372.91249442100525 and batch: 650, loss is 4.174099740982055 and perplexity is 64.98131330722816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.729581945082721 and perplexity of 113.24820848066247
Finished 45 epochs...
Completing Train Step...
At time: 374.0597653388977 and batch: 50, loss is 4.217034273147583 and perplexity is 67.83201445811294
At time: 374.6587929725647 and batch: 100, loss is 4.187691946029663 and perplexity is 65.87058251582272
At time: 375.256543636322 and batch: 150, loss is 4.156065120697021 and perplexity is 63.819904260302486
At time: 375.8568112850189 and batch: 200, loss is 4.130429410934449 and perplexity is 62.204628548181134
At time: 376.4549000263214 and batch: 250, loss is 4.112409024238587 and perplexity is 61.09371668062928
At time: 377.0510427951813 and batch: 300, loss is 4.189546356201172 and perplexity is 65.9928469231863
At time: 377.6483254432678 and batch: 350, loss is 4.1321369695663455 and perplexity is 62.310937337016654
At time: 378.24797534942627 and batch: 400, loss is 4.17673399925232 and perplexity is 65.15271653028087
At time: 378.84771275520325 and batch: 450, loss is 4.1522321081161495 and perplexity is 63.575749986490166
At time: 379.4463801383972 and batch: 500, loss is 4.123134093284607 and perplexity is 61.75247732239214
At time: 380.0453209877014 and batch: 550, loss is 4.139651646614075 and perplexity is 62.78094768382551
At time: 380.6424560546875 and batch: 600, loss is 4.241174182891846 and perplexity is 69.48939721616863
At time: 381.23875427246094 and batch: 650, loss is 4.173670101165771 and perplexity is 64.95340074432052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.7295083438648895 and perplexity of 113.2398735813342
Finished 46 epochs...
Completing Train Step...
At time: 382.3695502281189 and batch: 50, loss is 4.215623226165771 and perplexity is 67.7363677956786
At time: 382.977441072464 and batch: 100, loss is 4.186085820198059 and perplexity is 65.7648709874343
At time: 383.57151460647583 and batch: 150, loss is 4.154689960479736 and perplexity is 63.732201983054985
At time: 384.1644163131714 and batch: 200, loss is 4.129390869140625 and perplexity is 62.14005997604842
At time: 384.7543697357178 and batch: 250, loss is 4.111633982658386 and perplexity is 61.04638485434882
At time: 385.3527293205261 and batch: 300, loss is 4.188740301132202 and perplexity is 65.93967448724285
At time: 385.9495506286621 and batch: 350, loss is 4.131610107421875 and perplexity is 62.27811670967958
At time: 386.5459728240967 and batch: 400, loss is 4.176460108757019 and perplexity is 65.13487426400533
At time: 387.1425347328186 and batch: 450, loss is 4.151821269989013 and perplexity is 63.54963600911053
At time: 387.73695063591003 and batch: 500, loss is 4.122904195785522 and perplexity is 61.7382822140659
At time: 388.3288857936859 and batch: 550, loss is 4.139612894058228 and perplexity is 62.778514808784635
At time: 388.9270145893097 and batch: 600, loss is 4.241154689788818 and perplexity is 69.4880426653916
At time: 389.52375411987305 and batch: 650, loss is 4.1733163690567014 and perplexity is 64.93042870409762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.729203766467524 and perplexity of 113.2053885273114
Finished 47 epochs...
Completing Train Step...
At time: 390.6706862449646 and batch: 50, loss is 4.214239349365235 and perplexity is 67.64269383929077
At time: 391.26348972320557 and batch: 100, loss is 4.184720239639282 and perplexity is 65.67512504976385
At time: 391.85269927978516 and batch: 150, loss is 4.1534035205841064 and perplexity is 63.65026704927119
At time: 392.44988799095154 and batch: 200, loss is 4.128317160606384 and perplexity is 62.07337546960217
At time: 393.04668498039246 and batch: 250, loss is 4.110914406776428 and perplexity is 61.00247314892372
At time: 393.64270973205566 and batch: 300, loss is 4.18803295135498 and perplexity is 65.89304856554598
At time: 394.2391836643219 and batch: 350, loss is 4.131080646514892 and perplexity is 62.245151609157375
At time: 394.83458399772644 and batch: 400, loss is 4.176142249107361 and perplexity is 65.11417380578574
At time: 395.4264953136444 and batch: 450, loss is 4.1514111471176145 and perplexity is 63.5235781937324
At time: 396.0233120918274 and batch: 500, loss is 4.122554087638855 and perplexity is 61.71667092186702
At time: 396.6200592517853 and batch: 550, loss is 4.139465289115906 and perplexity is 62.769249073578194
At time: 397.2174959182739 and batch: 600, loss is 4.241093263626099 and perplexity is 69.48377441266834
At time: 397.8305983543396 and batch: 650, loss is 4.1731178665161135 and perplexity is 64.91754112818899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.728678684608609 and perplexity of 113.14596203471628
Finished 48 epochs...
Completing Train Step...
At time: 398.95943236351013 and batch: 50, loss is 4.2129768371582035 and perplexity is 67.55734799902109
At time: 399.5659878253937 and batch: 100, loss is 4.183547887802124 and perplexity is 65.59817581086028
At time: 400.15814185142517 and batch: 150, loss is 4.1521856355667115 and perplexity is 63.572795527957
At time: 400.7531433105469 and batch: 200, loss is 4.127300772666931 and perplexity is 62.010316890831724
At time: 401.34665608406067 and batch: 250, loss is 4.11025785446167 and perplexity is 60.962434978986664
At time: 401.94355750083923 and batch: 300, loss is 4.1873530006408695 and perplexity is 65.84825976893163
At time: 402.5360496044159 and batch: 350, loss is 4.130536956787109 and perplexity is 62.21131875774349
At time: 403.13370871543884 and batch: 400, loss is 4.175761260986328 and perplexity is 65.08937080417937
At time: 403.73216009140015 and batch: 450, loss is 4.150961270332337 and perplexity is 63.495006837862
At time: 404.33028769493103 and batch: 500, loss is 4.122172040939331 and perplexity is 61.69309677493494
At time: 404.9299154281616 and batch: 550, loss is 4.139280028343201 and perplexity is 62.757621471095284
At time: 405.5260374546051 and batch: 600, loss is 4.241019039154053 and perplexity is 69.47861720759458
At time: 406.1209671497345 and batch: 650, loss is 4.1730699586868285 and perplexity is 64.9144311442079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.728215236289828 and perplexity of 113.09353687794815
Finished 49 epochs...
Completing Train Step...
At time: 407.26718497276306 and batch: 50, loss is 4.211851739883423 and perplexity is 67.48138215339237
At time: 407.86252760887146 and batch: 100, loss is 4.182412023544312 and perplexity is 65.52370748853836
At time: 408.45894408226013 and batch: 150, loss is 4.150987911224365 and perplexity is 63.49669842401609
At time: 409.05611777305603 and batch: 200, loss is 4.126337027549743 and perplexity is 61.95058353919928
At time: 409.6488907337189 and batch: 250, loss is 4.10961311340332 and perplexity is 60.923142662184794
At time: 410.24232840538025 and batch: 300, loss is 4.186627912521362 and perplexity is 65.8005312838528
At time: 410.83887672424316 and batch: 350, loss is 4.129972686767578 and perplexity is 62.17622467789273
At time: 411.43471002578735 and batch: 400, loss is 4.175339503288269 and perplexity is 65.0619246492013
At time: 412.0316812992096 and batch: 450, loss is 4.150492234230041 and perplexity is 63.46523237053546
At time: 412.62900853157043 and batch: 500, loss is 4.121690249443054 and perplexity is 61.66338072457478
At time: 413.23768281936646 and batch: 550, loss is 4.139033107757569 and perplexity is 62.74212723545034
At time: 413.8323669433594 and batch: 600, loss is 4.240949945449829 and perplexity is 69.47381683840692
At time: 414.4287180900574 and batch: 650, loss is 4.172899112701416 and perplexity is 64.90334172157024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 4.727851119695925 and perplexity of 113.05236514061899
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f28e8faa828>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 1.0, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 0.0, 'seq_len': 20, 'anneal': 2.0, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 652 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8157732486724854 and batch: 50, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 1.3890883922576904 and batch: 100, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 1.945585012435913 and batch: 150, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 2.5059237480163574 and batch: 200, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 3.0654053688049316 and batch: 250, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 3.6243083477020264 and batch: 300, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 4.183872938156128 and batch: 350, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 4.742786884307861 and batch: 400, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 5.297759294509888 and batch: 450, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 5.859042406082153 and batch: 500, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 6.4192585945129395 and batch: 550, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 6.979443073272705 and batch: 600, loss is 9.169526100158691 and perplexity is 9600.074138232805
At time: 7.539469003677368 and batch: 650, loss is 9.169526100158691 and perplexity is 9600.074138232805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 1 epochs...
Completing Train Step...
At time: 8.623454809188843 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 9.179083347320557 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 9.737815856933594 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 10.29459810256958 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 10.852667808532715 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 11.411762714385986 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 11.96918773651123 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 12.522596836090088 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 13.096757888793945 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 13.655234813690186 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 14.214182138442993 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 14.771635293960571 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 15.330254554748535 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 2 epochs...
Completing Train Step...
At time: 16.393123388290405 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 16.965294122695923 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 17.524775981903076 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 18.09312891960144 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 18.67072892189026 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 19.257601499557495 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 19.848368406295776 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 20.44874358177185 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 21.047603368759155 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 21.6457576751709 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 22.243345260620117 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 22.840797185897827 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 23.434918642044067 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 3 epochs...
Completing Train Step...
At time: 24.567585468292236 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 25.160381078720093 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 25.75438904762268 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 26.347338914871216 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 26.936398029327393 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 27.530587196350098 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 28.138139724731445 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 28.732377529144287 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 29.32640027999878 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 29.923743963241577 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 30.51536226272583 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 31.112074613571167 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 31.70908236503601 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 4 epochs...
Completing Train Step...
At time: 32.829554319381714 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 33.43599033355713 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 34.02583932876587 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 34.61886215209961 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 35.210723638534546 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 35.803797006607056 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 36.39770150184631 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 36.99285101890564 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 37.585517168045044 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 38.180888414382935 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 38.77723407745361 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 39.374247789382935 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 39.97050452232361 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 5 epochs...
Completing Train Step...
At time: 41.09829616546631 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 41.688340187072754 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 42.282116174697876 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 42.87352705001831 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 43.48144006729126 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 44.074299335479736 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 44.66837954521179 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 45.262032985687256 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 45.859436988830566 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 46.45559287071228 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 47.052603006362915 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 47.64902853965759 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 48.24392914772034 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 6 epochs...
Completing Train Step...
At time: 49.36734938621521 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 49.975632429122925 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 50.569852113723755 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 51.16486954689026 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 51.75658416748047 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 52.34697890281677 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 52.942907094955444 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 53.537391901016235 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 54.130085468292236 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 54.72499656677246 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 55.317150592803955 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 55.907440423965454 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 56.505918979644775 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 7 epochs...
Completing Train Step...
At time: 57.64598631858826 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 58.2325394153595 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 58.83128595352173 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 59.416550159454346 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 60.0061457157135 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 60.59480381011963 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 61.183305740356445 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 61.771727323532104 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 62.36112642288208 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 62.95257592201233 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 63.546648263931274 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 64.1385715007782 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 64.73042106628418 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 8 epochs...
Completing Train Step...
At time: 65.83985662460327 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 66.44250011444092 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 67.03369307518005 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 67.62403154373169 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 68.21407747268677 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 68.8051495552063 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 69.39642691612244 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 69.99136662483215 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 70.5890462398529 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 71.18811750411987 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 71.7886393070221 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 72.39062261581421 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 72.99231791496277 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 9 epochs...
Completing Train Step...
At time: 74.13110685348511 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 74.72897958755493 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 75.32656788825989 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 75.92562818527222 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 76.5245611667633 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 77.12101531028748 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 77.71791219711304 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 78.31679368019104 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 78.91518497467041 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 79.51367449760437 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 80.11236596107483 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 80.70911574363708 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 81.3047444820404 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 10 epochs...
Completing Train Step...
At time: 82.42560625076294 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 83.02843475341797 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 83.61828446388245 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 84.20608377456665 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 84.7907509803772 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 85.38157773017883 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 85.97128963470459 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 86.56037831306458 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 87.15332794189453 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 87.74610996246338 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 88.33424663543701 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 88.94235277175903 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 89.5350193977356 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 11 epochs...
Completing Train Step...
At time: 90.66214966773987 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 91.24840927124023 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 91.82980489730835 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 92.41622591018677 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 93.00234365463257 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 93.59166765213013 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 94.18133521080017 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 94.77566289901733 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 95.367506980896 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 95.9635272026062 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 96.56071376800537 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 97.15857911109924 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 97.75569081306458 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 12 epochs...
Completing Train Step...
At time: 98.87687230110168 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 99.48476386070251 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 100.08143353462219 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 100.67835569381714 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 101.27548360824585 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 101.87200117111206 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 102.4650354385376 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 103.05853652954102 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 103.65462827682495 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 104.26589155197144 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 104.86098384857178 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 105.45691776275635 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 106.05040955543518 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 13 epochs...
Completing Train Step...
At time: 107.18538999557495 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 107.78035283088684 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 108.37544441223145 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 108.96951079368591 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 109.5618543624878 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 110.15120029449463 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 110.74657487869263 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 111.34116172790527 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 111.93436527252197 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 112.52736473083496 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 113.11957931518555 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 113.70831751823425 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 114.30233359336853 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 14 epochs...
Completing Train Step...
At time: 115.42287111282349 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 116.02397632598877 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 116.61484265327454 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 117.20180702209473 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 117.79265475273132 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 118.38055324554443 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 118.9653525352478 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 119.56426882743835 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 120.15338158607483 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 120.74109315872192 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 121.33081579208374 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 121.92064929008484 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 122.51426887512207 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 15 epochs...
Completing Train Step...
At time: 123.63873863220215 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 124.22951126098633 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 124.82101798057556 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 125.41193628311157 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 126.00441431999207 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 126.60239720344543 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 127.20157265663147 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 127.79690718650818 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 128.39492511749268 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 128.99443173408508 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 129.59248232841492 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 130.18950939178467 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 130.78799891471863 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 16 epochs...
Completing Train Step...
At time: 131.9077696800232 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 132.51731729507446 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 133.11250472068787 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 133.70843863487244 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 134.30439925193787 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 134.9138822555542 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 135.5079698562622 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 136.10476660728455 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 136.70049166679382 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 137.29685282707214 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 137.89333701133728 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 138.48847031593323 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 139.08456945419312 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 17 epochs...
Completing Train Step...
At time: 140.21879243850708 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 140.80709648132324 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 141.3969750404358 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 141.98920917510986 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 142.57803392410278 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 143.17311811447144 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 143.76661658287048 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 144.3601644039154 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 144.95251369476318 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 145.54563784599304 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 146.13860487937927 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 146.74081587791443 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 147.34617638587952 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 18 epochs...
Completing Train Step...
At time: 148.4728865623474 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 149.08276081085205 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 149.67401838302612 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 150.28699493408203 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 150.88386726379395 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 151.48132348060608 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 152.0783166885376 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 152.67440938949585 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 153.26641845703125 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 153.8635973930359 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 154.45943236351013 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 155.0549671649933 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 155.65142917633057 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 19 epochs...
Completing Train Step...
At time: 156.78192138671875 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 157.36941051483154 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 157.95833563804626 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 158.54834508895874 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 159.1380000114441 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 159.7280077934265 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 160.3148798942566 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 160.91095113754272 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 161.50737977027893 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 162.10401964187622 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 162.69772815704346 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 163.29414463043213 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 163.8872790336609 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 20 epochs...
Completing Train Step...
At time: 165.00780320167542 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 165.61176204681396 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 166.20092010498047 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 166.79108905792236 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 167.3776741027832 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 167.96509838104248 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 168.55409908294678 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 169.14636373519897 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 169.7388014793396 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 170.33309674263 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 170.9262306690216 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 171.5203640460968 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 172.1171612739563 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 21 epochs...
Completing Train Step...
At time: 173.25747537612915 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 173.85346245765686 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 174.44786477088928 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 175.03921461105347 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 175.63644909858704 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 176.2338583469391 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 176.83015894889832 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 177.42587852478027 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 178.02087497711182 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 178.6121644973755 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 179.20996403694153 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 179.8055067062378 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 180.4221112728119 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 22 epochs...
Completing Train Step...
At time: 181.5406370162964 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 182.14227962493896 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 182.7345254421234 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 183.32645201683044 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 183.91785383224487 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 184.51010537147522 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 185.10163640975952 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 185.68832111358643 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 186.2790458202362 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 186.873774766922 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 187.4700710773468 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 188.0640594959259 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 188.6597502231598 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 23 epochs...
Completing Train Step...
At time: 189.7867558002472 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 190.37249302864075 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 190.96048212051392 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 191.55183911323547 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 192.14314222335815 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 192.73782801628113 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 193.33465719223022 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 193.93409371376038 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 194.53268933296204 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 195.13145351409912 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 195.74443197250366 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 196.34122776985168 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 196.93976664543152 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 24 epochs...
Completing Train Step...
At time: 198.06475710868835 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 198.6751251220703 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 199.27130389213562 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 199.8657615184784 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 200.4600009918213 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 201.05675625801086 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 201.65346717834473 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 202.25016808509827 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 202.84609246253967 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 203.44142603874207 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 204.03529024124146 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 204.63182592391968 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 205.22742533683777 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 25 epochs...
Completing Train Step...
At time: 206.35930800437927 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 206.94654488563538 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 207.53111171722412 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 208.12209510803223 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 208.71141242980957 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 209.29979014396667 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 209.8911747932434 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 210.48268604278564 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 211.08814311027527 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 211.68527746200562 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 212.28021001815796 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 212.87596011161804 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 213.4717972278595 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 26 epochs...
Completing Train Step...
At time: 214.5857334136963 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 215.1895775794983 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 215.7796447277069 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 216.36918902397156 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 216.96145963668823 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 217.55200028419495 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 218.13986778259277 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 218.73177671432495 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 219.3245816230774 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 219.91652035713196 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 220.50960040092468 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 221.1017439365387 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 221.69132828712463 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 27 epochs...
Completing Train Step...
At time: 222.82541942596436 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 223.41939902305603 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 224.01050209999084 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 224.60390305519104 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 225.19809126853943 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 225.79137015342712 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 226.4026746749878 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 227.00000643730164 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 227.59537076950073 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 228.19181203842163 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 228.78521728515625 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 229.37901186943054 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 229.9771957397461 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 28 epochs...
Completing Train Step...
At time: 231.10112690925598 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 231.71109914779663 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 232.30521059036255 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 232.89645719528198 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 233.49349427223206 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 234.08935260772705 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 234.68432688713074 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 235.27892899513245 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 235.87299132347107 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 236.46442127227783 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 237.06132674217224 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 237.65706825256348 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 238.25155067443848 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 29 epochs...
Completing Train Step...
At time: 239.3652241230011 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 239.94960570335388 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 240.5377721786499 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 241.1288619041443 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 241.74556517601013 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 242.34041714668274 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 242.93535709381104 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 243.52556109428406 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 244.123694896698 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 244.72110247612 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 245.32199501991272 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 245.92296528816223 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 246.52350401878357 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 30 epochs...
Completing Train Step...
At time: 247.64165830612183 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 248.25201988220215 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 248.84713625907898 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 249.4427409172058 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 250.03803753852844 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 250.63116073608398 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 251.22522735595703 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 251.82173204421997 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 252.41680264472961 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 253.01343393325806 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 253.608957529068 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 254.20135498046875 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 254.79581546783447 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 31 epochs...
Completing Train Step...
At time: 255.92932105064392 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 256.5191698074341 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 257.1230251789093 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 257.71055793762207 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 258.29563784599304 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 258.887757062912 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 259.48013949394226 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 260.0753650665283 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 260.6717176437378 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 261.2660245895386 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 261.85993933677673 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 262.46533012390137 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 263.0686891078949 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 32 epochs...
Completing Train Step...
At time: 264.19916677474976 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 264.8140015602112 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 265.40935754776 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 266.0107419490814 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 266.6112263202667 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 267.2118284702301 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 267.81122612953186 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 268.4110863208771 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 269.00577044487 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 269.6061797142029 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 270.20600938796997 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 270.80635261535645 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 271.40640592575073 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 33 epochs...
Completing Train Step...
At time: 272.541033744812 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 273.13276290893555 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 273.7266597747803 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 274.31975388526917 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 274.91286063194275 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 275.505163192749 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 276.09375190734863 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 276.6856892108917 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 277.2795796394348 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 277.8709442615509 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 278.46330761909485 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 279.0563018321991 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 279.645635843277 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 34 epochs...
Completing Train Step...
At time: 280.7622401714325 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 281.36657094955444 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 281.95619463920593 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 282.5441324710846 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 283.130912065506 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 283.71804785728455 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 284.3076751232147 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 284.9004125595093 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 285.4930591583252 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 286.08589124679565 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 286.6770930290222 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 287.2812807559967 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 287.8760838508606 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 35 epochs...
Completing Train Step...
At time: 289.00798201560974 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 289.5999448299408 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 290.19036316871643 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 290.7768306732178 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 291.3698573112488 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 291.96160793304443 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 292.5568974018097 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 293.1554226875305 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 293.75347661972046 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 294.3469760417938 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 294.945011138916 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 295.5433840751648 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 296.1408369541168 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 36 epochs...
Completing Train Step...
At time: 297.2659659385681 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 297.87569427490234 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 298.47048783302307 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 299.06772780418396 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 299.6635797023773 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 300.259161233902 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 300.85491132736206 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 301.4480094909668 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 302.04326725006104 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 302.65438532829285 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 303.2504539489746 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 303.84633445739746 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 304.44556427001953 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 37 epochs...
Completing Train Step...
At time: 305.5889217853546 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 306.18588972091675 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 306.7819092273712 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 307.3785152435303 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 307.9753968715668 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 308.5686979293823 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 309.16295862197876 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 309.7605879306793 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 310.3576629161835 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 310.95651292800903 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 311.5552706718445 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 312.150559425354 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 312.74793815612793 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 38 epochs...
Completing Train Step...
At time: 313.867910861969 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 314.4724335670471 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 315.0641620159149 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 315.65536165237427 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 316.24556708335876 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 316.83889079093933 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 317.43129873275757 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 318.0392994880676 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 318.6324167251587 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 319.2229766845703 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 319.81450748443604 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 320.4125609397888 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 321.0096220970154 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 39 epochs...
Completing Train Step...
At time: 322.1481029987335 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 322.74068570137024 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 323.3297641277313 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 323.92490553855896 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 324.5189120769501 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 325.1131479740143 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 325.7092373371124 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 326.30535340309143 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 326.8971679210663 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 327.49523663520813 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 328.09280037879944 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 328.68869614601135 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 329.2845914363861 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 40 epochs...
Completing Train Step...
At time: 330.40137004852295 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 331.009316444397 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 331.60340428352356 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 332.19675183296204 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 332.78997898101807 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 333.3994097709656 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 333.98773312568665 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 334.57984733581543 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 335.1738817691803 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 335.76711559295654 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 336.36020612716675 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 336.95384883880615 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 337.5426321029663 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 41 epochs...
Completing Train Step...
At time: 338.66974353790283 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 339.2554621696472 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 339.8457570075989 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 340.4385747909546 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 341.02864241600037 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 341.61877155303955 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 342.2113547325134 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 342.8042380809784 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 343.3997302055359 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 343.99414324760437 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 344.59007596969604 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 345.18403840065 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 345.7835969924927 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 42 epochs...
Completing Train Step...
At time: 346.90812158584595 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 347.52078008651733 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 348.1180319786072 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 348.72612380981445 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 349.3257460594177 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 349.9231541156769 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 350.5201790332794 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 351.1175308227539 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 351.7134675979614 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 352.30714774131775 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 352.90672063827515 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 353.50416350364685 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 354.10232615470886 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 43 epochs...
Completing Train Step...
At time: 355.23844146728516 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 355.8297736644745 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 356.4269073009491 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 357.02249932289124 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 357.6177587509155 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 358.2136251926422 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 358.8087456226349 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 359.3998341560364 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 359.99540400505066 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 360.59178948402405 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 361.1889934539795 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 361.78680896759033 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 362.3847906589508 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 44 epochs...
Completing Train Step...
At time: 363.50197410583496 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 364.1084101200104 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 364.698016166687 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 365.2878053188324 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 365.87755823135376 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 366.4661953449249 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 367.05873560905457 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 367.65356254577637 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 368.2479362487793 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 368.8425953388214 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 369.43700981140137 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 370.02977085113525 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 370.62630343437195 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 45 epochs...
Completing Train Step...
At time: 371.7728397846222 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 372.3679766654968 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 372.9650626182556 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 373.55921244621277 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 374.15135502815247 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 374.74896001815796 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 375.3445076942444 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 375.93944907188416 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 376.5358045101166 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 377.13108086586 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 377.7237412929535 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 378.3202335834503 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 378.93046140670776 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 46 epochs...
Completing Train Step...
At time: 380.0457239151001 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 380.6479902267456 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 381.2311067581177 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 381.8196213245392 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 382.4072904586792 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 382.9959318637848 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 383.5883140563965 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 384.181236743927 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 384.772225856781 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 385.3678467273712 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 385.96436047554016 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 386.56016874313354 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 387.1563584804535 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 47 epochs...
Completing Train Step...
At time: 388.28859090805054 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 388.88019323349 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 389.47432827949524 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 390.06777906417847 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 390.66254448890686 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 391.2587890625 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 391.8510663509369 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 392.4451563358307 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 393.0424063205719 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 393.63798689842224 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 394.24912214279175 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 394.84442591667175 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 395.4382243156433 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 48 epochs...
Completing Train Step...
At time: 396.55927562713623 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 397.1672463417053 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 397.7592613697052 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 398.3518466949463 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 398.94476556777954 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 399.53436064720154 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 400.1294491291046 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 400.7222466468811 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 401.3154036998749 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 401.909038066864 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 402.5021126270294 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 403.09110379219055 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 403.6846401691437 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished 49 epochs...
Completing Train Step...
At time: 404.8148036003113 and batch: 50, loss is 9.169153079986572 and perplexity is 9596.493784738954
At time: 405.4059383869171 and batch: 100, loss is 9.169716186523438 and perplexity is 9601.899154876934
At time: 405.9984521865845 and batch: 150, loss is 9.169149494171142 and perplexity is 9596.459373545167
At time: 406.5880491733551 and batch: 200, loss is 9.169184665679932 and perplexity is 9596.796901436024
At time: 407.18353176116943 and batch: 250, loss is 9.168933181762695 and perplexity is 9594.383764803564
At time: 407.77790117263794 and batch: 300, loss is 9.169246158599854 and perplexity is 9597.38705464433
At time: 408.37435483932495 and batch: 350, loss is 9.169833545684815 and perplexity is 9603.026091836275
At time: 408.96916222572327 and batch: 400, loss is 9.170208473205566 and perplexity is 9606.627205636742
At time: 409.580064535141 and batch: 450, loss is 9.170043773651123 and perplexity is 9605.045128703536
At time: 410.1740334033966 and batch: 500, loss is 9.1697513961792 and perplexity is 9602.237240392737
At time: 410.77260971069336 and batch: 550, loss is 9.17051586151123 and perplexity is 9609.58062439657
At time: 411.37016010284424 and batch: 600, loss is 9.169950504302978 and perplexity is 9604.149314182145
At time: 411.96883726119995 and batch: 650, loss is 9.169331283569337 and perplexity is 9598.204066698036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 52 batches
Done Evaluating: Achieved loss of 9.347849527994791 and perplexity of 11474.122116515384
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f28e8faa828>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.04511195580211125, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 19.292207257283696, 'seq_len': 20, 'anneal': 3.5932680391524086, 'batch_size': 80}, 'best_accuracy': -150.03565027818937}, {'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.3839706694510512, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 7.233346149393008, 'seq_len': 20, 'anneal': 4.453353708609926, 'batch_size': 80}, 'best_accuracy': -78.04083281248714}, {'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.3609633842882489, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 10.525727233259758, 'seq_len': 20, 'anneal': 6.03673053876303, 'batch_size': 80}, 'best_accuracy': -91.88460929291544}, {'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.3121505403317403, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 5.053100222232462, 'seq_len': 20, 'anneal': 5.493598875085108, 'batch_size': 80}, 'best_accuracy': -76.13760306550157}, {'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 0.40764913611098597, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 12.987931008225292, 'seq_len': 20, 'anneal': 5.351038312655021, 'batch_size': 80}, 'best_accuracy': -113.05236514061899}, {'params': {'data': 'ptb', 'num_layers': 1, 'wordvec_dim': 200, 'dropout': 1.0, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 0.0, 'seq_len': 20, 'anneal': 2.0, 'batch_size': 80}, 'best_accuracy': -11474.122116515384}]
