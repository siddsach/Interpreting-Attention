FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.058152675628662 and batch: 50, loss is 6.939925918579101 and perplexity is 1032.6937087092936
At time: 1.5354950428009033 and batch: 100, loss is 6.2593115615844725 and perplexity is 522.8588600505925
At time: 2.033029794692993 and batch: 150, loss is 6.188725757598877 and perplexity is 487.22486790883255
At time: 2.510620594024658 and batch: 200, loss is 6.0674528217315675 and perplexity is 431.5799691956539
At time: 2.987375259399414 and batch: 250, loss is 6.130610876083374 and perplexity is 459.71690498359055
At time: 3.464935779571533 and batch: 300, loss is 6.043496980667114 and perplexity is 421.3639631167382
At time: 3.941892623901367 and batch: 350, loss is 6.062770032882691 and perplexity is 429.56369590403705
At time: 4.428252935409546 and batch: 400, loss is 5.936948080062866 and perplexity is 378.77716615109046
At time: 4.907550573348999 and batch: 450, loss is 5.947270164489746 and perplexity is 382.70718413406104
At time: 5.384183168411255 and batch: 500, loss is 5.915890245437622 and perplexity is 370.88433386428017
At time: 5.877111911773682 and batch: 550, loss is 5.954395341873169 and perplexity is 385.44377848938706
At time: 6.357750654220581 and batch: 600, loss is 5.897407932281494 and perplexity is 364.09249127957474
At time: 6.834223747253418 and batch: 650, loss is 5.831310844421386 and perplexity is 340.8051288966832
At time: 7.311775207519531 and batch: 700, loss is 5.928520021438598 and perplexity is 375.59822495784096
At time: 7.7878944873809814 and batch: 750, loss is 5.885146036148071 and perplexity is 359.6552868344096
At time: 8.26505994796753 and batch: 800, loss is 5.888723459243774 and perplexity is 360.9442301371399
At time: 8.741264581680298 and batch: 850, loss is 5.927629299163819 and perplexity is 375.2638202054935
At time: 9.216152667999268 and batch: 900, loss is 5.813787508010864 and perplexity is 334.88510672893284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.703764196944563 and perplexity of 299.99451673661963
finished 1 epochs...
Completing Train Step...
At time: 10.492210626602173 and batch: 50, loss is 5.462747936248779 and perplexity is 235.7443455416384
At time: 10.964393615722656 and batch: 100, loss is 5.22611783027649 and perplexity is 186.0690479337759
At time: 11.437927722930908 and batch: 150, loss is 5.145175189971924 and perplexity is 171.60154490769372
At time: 11.910589933395386 and batch: 200, loss is 4.9920657444000245 and perplexity is 147.24027031657454
At time: 12.38333010673523 and batch: 250, loss is 5.045765027999878 and perplexity is 155.36311087540344
At time: 12.85604190826416 and batch: 300, loss is 4.9618837261199955 and perplexity is 142.86265670543062
At time: 13.329111814498901 and batch: 350, loss is 4.940170059204101 and perplexity is 139.79402079860185
At time: 13.802433490753174 and batch: 400, loss is 4.789748849868775 and perplexity is 120.27115875245944
At time: 14.275152206420898 and batch: 450, loss is 4.792579641342163 and perplexity is 120.61210366745286
At time: 14.748356342315674 and batch: 500, loss is 4.70311863899231 and perplexity is 110.29059321557884
At time: 15.221447706222534 and batch: 550, loss is 4.76751410484314 and perplexity is 117.62647113773443
At time: 15.70293378829956 and batch: 600, loss is 4.708648586273194 and perplexity is 110.90218385508322
At time: 16.180640697479248 and batch: 650, loss is 4.574944725036621 and perplexity is 97.02267606384655
At time: 16.653510570526123 and batch: 700, loss is 4.6181528186798095 and perplexity is 101.30672729553862
At time: 17.12786602973938 and batch: 750, loss is 4.649816532135009 and perplexity is 104.56579935318187
At time: 17.60070538520813 and batch: 800, loss is 4.5960170269012455 and perplexity is 99.08886035566168
At time: 18.095876932144165 and batch: 850, loss is 4.650679521560669 and perplexity is 104.65607748124133
At time: 18.56917142868042 and batch: 900, loss is 4.583456869125366 and perplexity is 97.85207202340682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6920533898758565 and perplexity of 109.07692746675889
finished 2 epochs...
Completing Train Step...
At time: 19.825460195541382 and batch: 50, loss is 4.625883436203003 and perplexity is 102.09292584148967
At time: 20.312338829040527 and batch: 100, loss is 4.485302829742432 and perplexity is 88.70380889764313
At time: 20.784417867660522 and batch: 150, loss is 4.4861954307556156 and perplexity is 88.7830213546388
At time: 21.257192611694336 and batch: 200, loss is 4.3846613216400145 and perplexity is 80.21105286431943
At time: 21.73058772087097 and batch: 250, loss is 4.510977869033813 and perplexity is 91.0107716265095
At time: 22.203997373580933 and batch: 300, loss is 4.471513462066651 and perplexity is 87.48903419871306
At time: 22.677199840545654 and batch: 350, loss is 4.464430360794068 and perplexity is 86.87153001328052
At time: 23.150355577468872 and batch: 400, loss is 4.360168642997742 and perplexity is 78.2703330537251
At time: 23.622382879257202 and batch: 450, loss is 4.398315725326538 and perplexity is 81.31379849405606
At time: 24.096070528030396 and batch: 500, loss is 4.293149452209473 and perplexity is 73.19663516426881
At time: 24.569798469543457 and batch: 550, loss is 4.375243663787842 and perplexity is 79.45919852291645
At time: 25.043717861175537 and batch: 600, loss is 4.3558121681213375 and perplexity is 77.93009197817337
At time: 25.516799926757812 and batch: 650, loss is 4.216459522247314 and perplexity is 67.79303914835621
At time: 25.990267515182495 and batch: 700, loss is 4.236581106185913 and perplexity is 69.17095895002063
At time: 26.46251130104065 and batch: 750, loss is 4.3162919569015505 and perplexity is 74.91034188565735
At time: 26.936097860336304 and batch: 800, loss is 4.276728820800781 and perplexity is 72.00451467003774
At time: 27.40912127494812 and batch: 850, loss is 4.349069576263428 and perplexity is 77.40640864998416
At time: 27.882431268692017 and batch: 900, loss is 4.29515474319458 and perplexity is 73.34356298418483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.521644069723887 and perplexity of 91.98670628662855
finished 3 epochs...
Completing Train Step...
At time: 29.158455848693848 and batch: 50, loss is 4.369248886108398 and perplexity is 78.9842832212757
At time: 29.632927179336548 and batch: 100, loss is 4.233812699317932 and perplexity is 68.97973041352797
At time: 30.12044930458069 and batch: 150, loss is 4.241414413452149 and perplexity is 69.50609269829921
At time: 30.59449005126953 and batch: 200, loss is 4.144655070304871 and perplexity is 63.095854513990204
At time: 31.069352388381958 and batch: 250, loss is 4.280353641510009 and perplexity is 72.26599174342427
At time: 31.545067310333252 and batch: 300, loss is 4.261546006202698 and perplexity is 70.919540794252
At time: 32.02037072181702 and batch: 350, loss is 4.24789870262146 and perplexity is 69.9582546926459
At time: 32.494685649871826 and batch: 400, loss is 4.159876346588135 and perplexity is 64.06360042737568
At time: 32.97010374069214 and batch: 450, loss is 4.207799859046936 and perplexity is 67.20850883264508
At time: 33.4465765953064 and batch: 500, loss is 4.092947812080383 and perplexity is 59.916253491585856
At time: 33.92448377609253 and batch: 550, loss is 4.1762577199935915 and perplexity is 65.12169303125872
At time: 34.40287232398987 and batch: 600, loss is 4.176298551559448 and perplexity is 65.12435210624314
At time: 34.88062143325806 and batch: 650, loss is 4.033510007858276 and perplexity is 56.45873446514292
At time: 35.35848593711853 and batch: 700, loss is 4.037888960838318 and perplexity is 56.706506704246834
At time: 35.836905002593994 and batch: 750, loss is 4.139478154182434 and perplexity is 62.77005660933796
At time: 36.31502890586853 and batch: 800, loss is 4.101864809989929 and perplexity is 60.4529157508828
At time: 36.79317092895508 and batch: 850, loss is 4.179403872489929 and perplexity is 65.32689844284876
At time: 37.270790815353394 and batch: 900, loss is 4.130637173652649 and perplexity is 62.21755369352811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.467997093723245 and perplexity of 87.18193078925562
finished 4 epochs...
Completing Train Step...
At time: 38.55276799201965 and batch: 50, loss is 4.217368960380554 and perplexity is 67.85472076688266
At time: 39.029913663864136 and batch: 100, loss is 4.081180176734924 and perplexity is 59.215313162995294
At time: 39.507670879364014 and batch: 150, loss is 4.093350863456726 and perplexity is 59.94040768738488
At time: 39.98568391799927 and batch: 200, loss is 3.99913649559021 and perplexity is 54.55102463924517
At time: 40.464155197143555 and batch: 250, loss is 4.137377576828003 and perplexity is 62.63834163706388
At time: 40.94186568260193 and batch: 300, loss is 4.127415990829467 and perplexity is 62.017462017218435
At time: 41.41990780830383 and batch: 350, loss is 4.109883151054382 and perplexity is 60.939596425992505
At time: 41.89795446395874 and batch: 400, loss is 4.031322283744812 and perplexity is 56.33535334147744
At time: 42.38934588432312 and batch: 450, loss is 4.084808173179627 and perplexity is 59.43053628691652
At time: 42.86760449409485 and batch: 500, loss is 3.9662148761749267 and perplexity is 52.78435691018306
At time: 43.345879554748535 and batch: 550, loss is 4.045555763244629 and perplexity is 57.142935153879534
At time: 43.82290506362915 and batch: 600, loss is 4.055463395118713 and perplexity is 57.71190021358725
At time: 44.300435304641724 and batch: 650, loss is 3.913695764541626 and perplexity is 50.083707947771494
At time: 44.77620029449463 and batch: 700, loss is 3.909788451194763 and perplexity is 49.888397026217916
At time: 45.2526068687439 and batch: 750, loss is 4.019444322586059 and perplexity is 55.670162578384485
At time: 45.7272584438324 and batch: 800, loss is 3.982470169067383 and perplexity is 53.64939375792502
At time: 46.20295214653015 and batch: 850, loss is 4.060762987136841 and perplexity is 58.01856161172671
At time: 46.685463428497314 and batch: 900, loss is 4.016902437210083 and perplexity is 55.528835101466875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434623822773973 and perplexity of 84.32039950749764
finished 5 epochs...
Completing Train Step...
At time: 47.95203351974487 and batch: 50, loss is 4.109519786834717 and perplexity is 60.917457179639044
At time: 48.44762587547302 and batch: 100, loss is 3.976907048225403 and perplexity is 53.351764339155736
At time: 48.92090964317322 and batch: 150, loss is 3.9859401750564576 and perplexity is 53.8358808441049
At time: 49.396721839904785 and batch: 200, loss is 3.8948690366744994 and perplexity is 49.149616143996006
At time: 49.86939787864685 and batch: 250, loss is 4.037437038421631 and perplexity is 56.68088555251763
At time: 50.34676432609558 and batch: 300, loss is 4.0298409128189085 and perplexity is 56.25196156926028
At time: 50.822242975234985 and batch: 350, loss is 4.009700412750244 and perplexity is 55.13035173871726
At time: 51.31378889083862 and batch: 400, loss is 3.937093210220337 and perplexity is 51.26935525430983
At time: 51.79243612289429 and batch: 450, loss is 3.9893489265441895 and perplexity is 54.01970711406246
At time: 52.27091979980469 and batch: 500, loss is 3.8705453634262086 and perplexity is 47.96853923559174
At time: 52.75017237663269 and batch: 550, loss is 3.948046131134033 and perplexity is 51.83399100831694
At time: 53.22850775718689 and batch: 600, loss is 3.965066089630127 and perplexity is 52.72375376789084
At time: 53.70699739456177 and batch: 650, loss is 3.823947424888611 and perplexity is 45.784583281885816
At time: 54.1844961643219 and batch: 700, loss is 3.8150816917419434 and perplexity is 45.38046344063586
At time: 54.67482662200928 and batch: 750, loss is 3.92681884765625 and perplexity is 50.7452921273971
At time: 55.15134263038635 and batch: 800, loss is 3.8937827014923094 and perplexity is 49.09625217761273
At time: 55.628007650375366 and batch: 850, loss is 3.97186541557312 and perplexity is 53.08346125311793
At time: 56.104355573654175 and batch: 900, loss is 3.9303821468353273 and perplexity is 50.926435327218215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.416365532025899 and perplexity of 82.79482273041927
finished 6 epochs...
Completing Train Step...
At time: 57.370338916778564 and batch: 50, loss is 4.026478476524353 and perplexity is 56.06313556764565
At time: 57.86035919189453 and batch: 100, loss is 3.8938553285598756 and perplexity is 49.099818023923795
At time: 58.337334871292114 and batch: 150, loss is 3.9030300426483153 and perplexity is 49.552367648338254
At time: 58.81372380256653 and batch: 200, loss is 3.8134688997268675 and perplexity is 45.30733317936066
At time: 59.290343046188354 and batch: 250, loss is 3.955786695480347 and perplexity is 52.236772216878386
At time: 59.76759171485901 and batch: 300, loss is 3.953231220245361 and perplexity is 52.103452858834466
At time: 60.244757890701294 and batch: 350, loss is 3.9324655675888063 and perplexity is 51.03264712299049
At time: 60.721601724624634 and batch: 400, loss is 3.8594749355316162 and perplexity is 47.440435541150734
At time: 61.1983597278595 and batch: 450, loss is 3.9152628993988037 and perplexity is 50.16225740499348
At time: 61.67545032501221 and batch: 500, loss is 3.7940930938720703 and perplexity is 44.43791710604643
At time: 62.15142607688904 and batch: 550, loss is 3.872754397392273 and perplexity is 48.07462049347835
At time: 62.62722992897034 and batch: 600, loss is 3.8913287782669066 and perplexity is 48.97592144568611
At time: 63.102747201919556 and batch: 650, loss is 3.7521267461776735 and perplexity is 42.611609779359455
At time: 63.58048987388611 and batch: 700, loss is 3.7403477478027343 and perplexity is 42.112632194295514
At time: 64.05607581138611 and batch: 750, loss is 3.8535423469543457 and perplexity is 47.15982415413672
At time: 64.53273463249207 and batch: 800, loss is 3.8251202917099 and perplexity is 45.83831400387482
At time: 65.00908970832825 and batch: 850, loss is 3.900755624771118 and perplexity is 49.43979292700965
At time: 65.48672389984131 and batch: 900, loss is 3.865032229423523 and perplexity is 47.70480990643519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.414173282989084 and perplexity of 82.6135146688601
finished 7 epochs...
Completing Train Step...
At time: 66.77285742759705 and batch: 50, loss is 3.958418912887573 and perplexity is 52.374451879914226
At time: 67.24981737136841 and batch: 100, loss is 3.824935545921326 and perplexity is 45.829846350613
At time: 67.72685313224792 and batch: 150, loss is 3.8367003297805784 and perplexity is 46.37220871956294
At time: 68.20377612113953 and batch: 200, loss is 3.7488808107376097 and perplexity is 42.473519482357325
At time: 68.68002343177795 and batch: 250, loss is 3.8898694801330564 and perplexity is 48.90450309792172
At time: 69.15610361099243 and batch: 300, loss is 3.8876049613952635 and perplexity is 48.79388323194563
At time: 69.6322193145752 and batch: 350, loss is 3.870612874031067 and perplexity is 47.97177773000484
At time: 70.10763025283813 and batch: 400, loss is 3.797602696418762 and perplexity is 44.59415053134054
At time: 70.58395910263062 and batch: 450, loss is 3.8556496620178224 and perplexity is 47.25930954868688
At time: 71.05997800827026 and batch: 500, loss is 3.731724991798401 and perplexity is 41.75106632955639
At time: 71.5357871055603 and batch: 550, loss is 3.811235866546631 and perplexity is 45.206273278113514
At time: 72.01203656196594 and batch: 600, loss is 3.8315488862991334 and perplexity is 46.13393915026454
At time: 72.48905348777771 and batch: 650, loss is 3.694887990951538 and perplexity is 40.241064972132854
At time: 72.96619606018066 and batch: 700, loss is 3.6822661542892456 and perplexity is 39.736340796637435
At time: 73.44381618499756 and batch: 750, loss is 3.791727805137634 and perplexity is 44.33293280936615
At time: 73.92071604728699 and batch: 800, loss is 3.766490559577942 and perplexity is 43.22809190874892
At time: 74.39684104919434 and batch: 850, loss is 3.842800097465515 and perplexity is 46.65593286576516
At time: 74.87279343605042 and batch: 900, loss is 3.8066323518753054 and perplexity is 44.99864381516073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.416946202108305 and perplexity of 82.84291316794483
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 76.16279768943787 and batch: 50, loss is 3.918192973136902 and perplexity is 50.30945205834411
At time: 76.6393518447876 and batch: 100, loss is 3.7723907947540285 and perplexity is 43.483901744198356
At time: 77.11584377288818 and batch: 150, loss is 3.776550965309143 and perplexity is 43.66517900256501
At time: 77.59153270721436 and batch: 200, loss is 3.67306893825531 and perplexity is 39.372552569752635
At time: 78.06774091720581 and batch: 250, loss is 3.811303243637085 and perplexity is 45.2093192478904
At time: 78.5440604686737 and batch: 300, loss is 3.792430782318115 and perplexity is 44.36410880619823
At time: 79.03375482559204 and batch: 350, loss is 3.7640105533599852 and perplexity is 43.12101879789398
At time: 79.5100347995758 and batch: 400, loss is 3.685219144821167 and perplexity is 39.85385525893505
At time: 79.9872055053711 and batch: 450, loss is 3.727757396697998 and perplexity is 41.585743187867855
At time: 80.4633092880249 and batch: 500, loss is 3.6012899112701415 and perplexity is 36.64547337922188
At time: 80.93984580039978 and batch: 550, loss is 3.6656222581863402 and perplexity is 39.080446726492774
At time: 81.41547727584839 and batch: 600, loss is 3.6870444297790526 and perplexity is 39.92666633171571
At time: 81.89189529418945 and batch: 650, loss is 3.5332575750350954 and perplexity is 34.235310254794776
At time: 82.36832785606384 and batch: 700, loss is 3.5039529466629027 and perplexity is 33.24661464275677
At time: 82.84437465667725 and batch: 750, loss is 3.606131191253662 and perplexity is 36.82331451799893
At time: 83.32030296325684 and batch: 800, loss is 3.5659923791885375 and perplexity is 35.37454094956768
At time: 83.79697012901306 and batch: 850, loss is 3.626469144821167 and perplexity is 37.57989292856368
At time: 84.27228450775146 and batch: 900, loss is 3.589326229095459 and perplexity is 36.209670683059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336008986381636 and perplexity of 76.402008598383
finished 9 epochs...
Completing Train Step...
At time: 85.53004360198975 and batch: 50, loss is 3.833523950576782 and perplexity is 46.22514668619136
At time: 86.01998043060303 and batch: 100, loss is 3.6957358741760253 and perplexity is 40.27519916491801
At time: 86.49494695663452 and batch: 150, loss is 3.703951845169067 and perplexity is 40.6074620952576
At time: 86.97090578079224 and batch: 200, loss is 3.605801944732666 and perplexity is 36.81119256546718
At time: 87.44785976409912 and batch: 250, loss is 3.7465531396865845 and perplexity is 42.37477007327928
At time: 87.92532968521118 and batch: 300, loss is 3.7343640232086184 and perplexity is 41.861394220363366
At time: 88.40136337280273 and batch: 350, loss is 3.7095148277282717 and perplexity is 40.83399020042362
At time: 88.87873435020447 and batch: 400, loss is 3.634417552947998 and perplexity is 37.87978350240765
At time: 89.35619378089905 and batch: 450, loss is 3.681579990386963 and perplexity is 39.70908450618469
At time: 89.83270215988159 and batch: 500, loss is 3.558571696281433 and perplexity is 35.113009270163914
At time: 90.30970001220703 and batch: 550, loss is 3.6244640493392946 and perplexity is 37.50461714782725
At time: 90.78585290908813 and batch: 600, loss is 3.65311897277832 and perplexity is 38.59485482041419
At time: 91.28478908538818 and batch: 650, loss is 3.5031163549423217 and perplexity is 33.21881243138075
At time: 91.76147317886353 and batch: 700, loss is 3.4793393850326537 and perplexity is 32.43828577897345
At time: 92.23812413215637 and batch: 750, loss is 3.5861086893081664 and perplexity is 36.093351857525896
At time: 92.7144525051117 and batch: 800, loss is 3.551262607574463 and perplexity is 34.85730080694703
At time: 93.19172525405884 and batch: 850, loss is 3.618076190948486 and perplexity is 37.26580652076918
At time: 93.66882944107056 and batch: 900, loss is 3.587081151008606 and perplexity is 36.12846833179355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33444715526006 and perplexity of 76.28277469944764
finished 10 epochs...
Completing Train Step...
At time: 94.93255829811096 and batch: 50, loss is 3.798161725997925 and perplexity is 44.61908694999359
At time: 95.42374086380005 and batch: 100, loss is 3.6614583253860475 and perplexity is 38.91805669773285
At time: 95.90143489837646 and batch: 150, loss is 3.66979866027832 and perplexity is 39.24400368804046
At time: 96.38010096549988 and batch: 200, loss is 3.572541522979736 and perplexity is 35.606974193382406
At time: 96.85807657241821 and batch: 250, loss is 3.71394464969635 and perplexity is 41.01527874879796
At time: 97.33508133888245 and batch: 300, loss is 3.7039755296707155 and perplexity is 40.60842387415011
At time: 97.81298065185547 and batch: 350, loss is 3.6796859979629515 and perplexity is 39.63394697838092
At time: 98.28928017616272 and batch: 400, loss is 3.6063574743270874 and perplexity is 36.83164795360391
At time: 98.76650524139404 and batch: 450, loss is 3.6549541568756103 and perplexity is 38.66574851581212
At time: 99.24582886695862 and batch: 500, loss is 3.5330876684188843 and perplexity is 34.229493943203344
At time: 99.72330164909363 and batch: 550, loss is 3.599763512611389 and perplexity is 36.58958044610761
At time: 100.20288586616516 and batch: 600, loss is 3.6314238357543944 and perplexity is 37.76655171978225
At time: 100.68568515777588 and batch: 650, loss is 3.4832578134536742 and perplexity is 32.56564223552504
At time: 101.17381358146667 and batch: 700, loss is 3.461713275909424 and perplexity is 31.871534497134085
At time: 101.65780067443848 and batch: 750, loss is 3.570340995788574 and perplexity is 35.52870622545389
At time: 102.13434290885925 and batch: 800, loss is 3.5374342584609986 and perplexity is 34.3785993359792
At time: 102.60994243621826 and batch: 850, loss is 3.6069040727615356 and perplexity is 36.85178557780905
At time: 103.10515403747559 and batch: 900, loss is 3.578642897605896 and perplexity is 35.82488979858824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335722622806078 and perplexity of 76.38013297836089
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 104.41482710838318 and batch: 50, loss is 3.7924509334564207 and perplexity is 44.36500280249808
At time: 104.89132642745972 and batch: 100, loss is 3.652991032600403 and perplexity is 38.589917303681894
At time: 105.36865234375 and batch: 150, loss is 3.6663075351715086 and perplexity is 39.1072368354787
At time: 105.84577894210815 and batch: 200, loss is 3.5580849552154543 and perplexity is 35.0959224853593
At time: 106.32277822494507 and batch: 250, loss is 3.6996770334243774 and perplexity is 40.43424334193473
At time: 106.79944896697998 and batch: 300, loss is 3.687429962158203 and perplexity is 39.94206232201381
At time: 107.27636289596558 and batch: 350, loss is 3.655814085006714 and perplexity is 38.69901258097391
At time: 107.75462126731873 and batch: 400, loss is 3.5834856414794922 and perplexity is 35.99880132879793
At time: 108.23181915283203 and batch: 450, loss is 3.6243376874923707 and perplexity is 37.499878294547706
At time: 108.70996284484863 and batch: 500, loss is 3.5004534482955934 and perplexity is 33.13047150897817
At time: 109.18731451034546 and batch: 550, loss is 3.560249910354614 and perplexity is 35.17198589032799
At time: 109.66629648208618 and batch: 600, loss is 3.5924139976501466 and perplexity is 36.321650560571086
At time: 110.14168357849121 and batch: 650, loss is 3.441372804641724 and perplexity is 31.229801170051523
At time: 110.6214280128479 and batch: 700, loss is 3.4171407890319823 and perplexity is 30.482135449150604
At time: 111.09945964813232 and batch: 750, loss is 3.519008450508118 and perplexity is 33.7509461334098
At time: 111.57762742042542 and batch: 800, loss is 3.4823803758621215 and perplexity is 32.53708044925948
At time: 112.05537438392639 and batch: 850, loss is 3.548186526298523 and perplexity is 34.750241662338624
At time: 112.53350305557251 and batch: 900, loss is 3.521321406364441 and perplexity is 33.82910093138187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31757051650792 and perplexity of 75.00618047751207
finished 12 epochs...
Completing Train Step...
At time: 113.81371307373047 and batch: 50, loss is 3.767967519760132 and perplexity is 43.29198525159703
At time: 114.28997373580933 and batch: 100, loss is 3.6275032997131347 and perplexity is 37.61877646100761
At time: 114.76644539833069 and batch: 150, loss is 3.642779998779297 and perplexity is 38.1978793158379
At time: 115.24260210990906 and batch: 200, loss is 3.537754302024841 and perplexity is 34.38960374628202
At time: 115.73332476615906 and batch: 250, loss is 3.680340232849121 and perplexity is 39.659885373146956
At time: 116.21010255813599 and batch: 300, loss is 3.6705021238327027 and perplexity is 39.27162012680328
At time: 116.68673157691956 and batch: 350, loss is 3.639821228981018 and perplexity is 38.08502761758643
At time: 117.16305160522461 and batch: 400, loss is 3.5695669174194338 and perplexity is 35.501214864088034
At time: 117.6408212184906 and batch: 450, loss is 3.611531982421875 and perplexity is 37.0227275592125
At time: 118.11750364303589 and batch: 500, loss is 3.4898317193984987 and perplexity is 32.780430931427205
At time: 118.5936872959137 and batch: 550, loss is 3.5502577686309813 and perplexity is 34.82229242546975
At time: 119.06923222541809 and batch: 600, loss is 3.5841691970825194 and perplexity is 36.023416923253784
At time: 119.54616045951843 and batch: 650, loss is 3.4346051931381227 and perplexity is 31.019163569066492
At time: 120.02228808403015 and batch: 700, loss is 3.4129040241241455 and perplexity is 30.353263001269646
At time: 120.49934649467468 and batch: 750, loss is 3.516067419052124 and perplexity is 33.651829363375036
At time: 120.97609663009644 and batch: 800, loss is 3.480872139930725 and perplexity is 32.48804384410369
At time: 121.45301222801208 and batch: 850, loss is 3.5491742658615113 and perplexity is 34.78458280811407
At time: 121.92758393287659 and batch: 900, loss is 3.5238849353790282 and perplexity is 33.9159340652326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316550476910317 and perplexity of 74.9297102113403
finished 13 epochs...
Completing Train Step...
At time: 123.1882312297821 and batch: 50, loss is 3.756900181770325 and perplexity is 42.81549979492136
At time: 123.68750429153442 and batch: 100, loss is 3.616310787200928 and perplexity is 37.20007536436831
At time: 124.16362762451172 and batch: 150, loss is 3.6316917181015014 and perplexity is 37.77667006750195
At time: 124.63996863365173 and batch: 200, loss is 3.5268717575073243 and perplexity is 34.01738636216096
At time: 125.1169376373291 and batch: 250, loss is 3.6695596885681154 and perplexity is 39.23462660183764
At time: 125.60694074630737 and batch: 300, loss is 3.6605892086029055 and perplexity is 38.884247055881374
At time: 126.08654236793518 and batch: 350, loss is 3.630211057662964 and perplexity is 37.720777036144916
At time: 126.56238555908203 and batch: 400, loss is 3.5612460803985595 and perplexity is 35.20704062639809
At time: 127.03837537765503 and batch: 450, loss is 3.603371596336365 and perplexity is 36.721837169004175
At time: 127.51491093635559 and batch: 500, loss is 3.4825207090377805 and perplexity is 32.54164680148345
At time: 128.00482487678528 and batch: 550, loss is 3.543508539199829 and perplexity is 34.588060117511944
At time: 128.48151016235352 and batch: 600, loss is 3.578343448638916 and perplexity is 35.81416367838883
At time: 128.95774793624878 and batch: 650, loss is 3.4295151948928835 and perplexity is 30.861677223556253
At time: 129.43290877342224 and batch: 700, loss is 3.4091049909591673 and perplexity is 30.238168710402864
At time: 129.90915608406067 and batch: 750, loss is 3.512912616729736 and perplexity is 33.545831782568094
At time: 130.38569808006287 and batch: 800, loss is 3.478307967185974 and perplexity is 32.40484560046831
At time: 130.8620879650116 and batch: 850, loss is 3.5480256700515747 and perplexity is 34.74465231843677
At time: 131.33848118782043 and batch: 900, loss is 3.523384900093079 and perplexity is 33.89897914082756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316576813998288 and perplexity of 74.93168366769721
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 132.60010147094727 and batch: 50, loss is 3.7562130451202393 and perplexity is 42.78608980131995
At time: 133.0903718471527 and batch: 100, loss is 3.618440146446228 and perplexity is 37.27937208441119
At time: 133.56489849090576 and batch: 150, loss is 3.6351531028747557 and perplexity is 37.90765622402094
At time: 134.0415813922882 and batch: 200, loss is 3.5259375381469726 and perplexity is 33.98562150121718
At time: 134.51864576339722 and batch: 250, loss is 3.6676701879501343 and perplexity is 39.16056274451461
At time: 135.00989532470703 and batch: 300, loss is 3.658780965805054 and perplexity is 38.81399842862108
At time: 135.50583863258362 and batch: 350, loss is 3.6276197814941407 and perplexity is 37.62315861830485
At time: 135.98833012580872 and batch: 400, loss is 3.559983344078064 and perplexity is 35.162611474517405
At time: 136.4649498462677 and batch: 450, loss is 3.59757830619812 and perplexity is 36.50971195660754
At time: 136.94160437583923 and batch: 500, loss is 3.4765094709396362 and perplexity is 32.34661798406268
At time: 137.4191267490387 and batch: 550, loss is 3.5331929397583006 and perplexity is 34.23309751755151
At time: 137.89958930015564 and batch: 600, loss is 3.5681607866287233 and perplexity is 35.45133059288415
At time: 138.37923741340637 and batch: 650, loss is 3.4168425798416138 and perplexity is 30.473046751451555
At time: 138.8726692199707 and batch: 700, loss is 3.3961470794677733 and perplexity is 29.84887287421097
At time: 139.3531527519226 and batch: 750, loss is 3.49798348903656 and perplexity is 33.04874157047841
At time: 139.84507656097412 and batch: 800, loss is 3.4621681499481203 and perplexity is 31.88603532852609
At time: 140.32256984710693 and batch: 850, loss is 3.5284249591827392 and perplexity is 34.07026327727831
At time: 140.79951167106628 and batch: 900, loss is 3.504044222831726 and perplexity is 33.249649404866325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313129163768194 and perplexity of 74.67379024985927
finished 15 epochs...
Completing Train Step...
At time: 142.0768918991089 and batch: 50, loss is 3.7483190155029296 and perplexity is 42.44966476287331
At time: 142.5530354976654 and batch: 100, loss is 3.608643250465393 and perplexity is 36.91593314747565
At time: 143.0296173095703 and batch: 150, loss is 3.626342921257019 and perplexity is 37.57514975989407
At time: 143.50496125221252 and batch: 200, loss is 3.5189670848846437 and perplexity is 33.749550033355625
At time: 143.98157024383545 and batch: 250, loss is 3.6616124391555784 and perplexity is 38.92405496834953
At time: 144.4569125175476 and batch: 300, loss is 3.65281635761261 and perplexity is 38.583177199028924
At time: 144.9321219921112 and batch: 350, loss is 3.6217826128005983 and perplexity is 37.40418560761411
At time: 145.4062819480896 and batch: 400, loss is 3.554865880012512 and perplexity is 34.98312771638666
At time: 145.88278031349182 and batch: 450, loss is 3.5926218843460083 and perplexity is 36.32920213340293
At time: 146.3605751991272 and batch: 500, loss is 3.472668032646179 and perplexity is 32.22259880559444
At time: 146.83591175079346 and batch: 550, loss is 3.5301372480392454 and perplexity is 34.128651383812475
At time: 147.31182861328125 and batch: 600, loss is 3.565991930961609 and perplexity is 35.3745250937494
At time: 147.78756189346313 and batch: 650, loss is 3.4152894830703735 and perplexity is 30.425755894076865
At time: 148.26377201080322 and batch: 700, loss is 3.39539333820343 and perplexity is 29.826383023860416
At time: 148.73913717269897 and batch: 750, loss is 3.497838606834412 and perplexity is 33.04395374286533
At time: 149.2155864238739 and batch: 800, loss is 3.462649631500244 and perplexity is 31.901391562882253
At time: 149.69055652618408 and batch: 850, loss is 3.5297327136993406 and perplexity is 34.114847964519875
At time: 150.16735529899597 and batch: 900, loss is 3.5062329721450807 and perplexity is 33.32250425358556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31272950890946 and perplexity of 74.64395246956995
finished 16 epochs...
Completing Train Step...
At time: 151.4389729499817 and batch: 50, loss is 3.745019474029541 and perplexity is 42.30983115371688
At time: 151.9158000946045 and batch: 100, loss is 3.6049848413467407 and perplexity is 36.7811263006782
At time: 152.40575242042542 and batch: 150, loss is 3.6226188373565673 and perplexity is 37.43547698760195
At time: 152.88220596313477 and batch: 200, loss is 3.5155000734329223 and perplexity is 33.63274256032662
At time: 153.3580560684204 and batch: 250, loss is 3.6582178020477296 and perplexity is 38.79214594527011
At time: 153.83283281326294 and batch: 300, loss is 3.649543595314026 and perplexity is 38.457110037783714
At time: 154.30907940864563 and batch: 350, loss is 3.618690695762634 and perplexity is 37.28871357580626
At time: 154.7851049900055 and batch: 400, loss is 3.552149543762207 and perplexity is 34.888230722843026
At time: 155.26089453697205 and batch: 450, loss is 3.5899495124816894 and perplexity is 36.232246604084025
At time: 155.73696184158325 and batch: 500, loss is 3.4705813550949096 and perplexity is 32.15543073544737
At time: 156.2134563922882 and batch: 550, loss is 3.528219962120056 and perplexity is 34.063279689213594
At time: 156.6904854774475 and batch: 600, loss is 3.564558629989624 and perplexity is 35.323859071052624
At time: 157.16577792167664 and batch: 650, loss is 3.4141052055358885 and perplexity is 30.38974468274492
At time: 157.6431474685669 and batch: 700, loss is 3.3947192907333372 and perplexity is 29.806285399978357
At time: 158.12039589881897 and batch: 750, loss is 3.497397527694702 and perplexity is 33.02938195806721
At time: 158.59714102745056 and batch: 800, loss is 3.4625020265579223 and perplexity is 31.896683107324836
At time: 159.07356929779053 and batch: 850, loss is 3.5300026750564575 and perplexity is 34.12405889841638
At time: 159.55012893676758 and batch: 900, loss is 3.5068267297744753 and perplexity is 33.34229561977254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3126157995772685 and perplexity of 74.63546523813034
finished 17 epochs...
Completing Train Step...
At time: 160.8134160041809 and batch: 50, loss is 3.742150835990906 and perplexity is 42.188633481899025
At time: 161.3030481338501 and batch: 100, loss is 3.602013821601868 and perplexity is 36.67201102028947
At time: 161.78027820587158 and batch: 150, loss is 3.619633002281189 and perplexity is 37.32386753397801
At time: 162.258234500885 and batch: 200, loss is 3.5126673364639283 and perplexity is 33.537604661048235
At time: 162.73535251617432 and batch: 250, loss is 3.655435571670532 and perplexity is 38.68436726051434
At time: 163.21472239494324 and batch: 300, loss is 3.6468928146362303 and perplexity is 38.35530366636584
At time: 163.6905586719513 and batch: 350, loss is 3.616169729232788 and perplexity is 37.19482836739683
At time: 164.1746199131012 and batch: 400, loss is 3.5500069522857665 and perplexity is 34.81355952057501
At time: 164.67819809913635 and batch: 450, loss is 3.5877913427352905 and perplexity is 36.154135584356965
At time: 165.15590453147888 and batch: 500, loss is 3.468804292678833 and perplexity is 32.09833927058723
At time: 165.63383388519287 and batch: 550, loss is 3.526531524658203 and perplexity is 34.005814498555
At time: 166.11160922050476 and batch: 600, loss is 3.5632296895980833 and perplexity is 35.276946946560784
At time: 166.58922696113586 and batch: 650, loss is 3.4129325437545774 and perplexity is 30.35412867745717
At time: 167.06677651405334 and batch: 700, loss is 3.3939344358444212 and perplexity is 29.782900969054623
At time: 167.54526114463806 and batch: 750, loss is 3.4967446374893187 and perplexity is 33.00782443621013
At time: 168.02169513702393 and batch: 800, loss is 3.4620664978027342 and perplexity is 31.88279420936324
At time: 168.4987075328827 and batch: 850, loss is 3.5298881053924562 and perplexity is 34.12014954040522
At time: 168.9733326435089 and batch: 900, loss is 3.506921482086182 and perplexity is 33.34545502903842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312602003959761 and perplexity of 74.63443560290163
finished 18 epochs...
Completing Train Step...
At time: 170.2361979484558 and batch: 50, loss is 3.7395269012451173 and perplexity is 42.078078368761176
At time: 170.72731113433838 and batch: 100, loss is 3.599353075027466 and perplexity is 36.57456578861319
At time: 171.20507550239563 and batch: 150, loss is 3.6169861555099487 and perplexity is 37.225207602165774
At time: 171.68356323242188 and batch: 200, loss is 3.5101369762420656 and perplexity is 33.4528497157703
At time: 172.16148447990417 and batch: 250, loss is 3.6529548835754393 and perplexity is 38.58852234101136
At time: 172.64022278785706 and batch: 300, loss is 3.6445292615890503 and perplexity is 38.264755920862456
At time: 173.11890745162964 and batch: 350, loss is 3.613908305168152 and perplexity is 37.11081012369192
At time: 173.5963580608368 and batch: 400, loss is 3.5481028509140016 and perplexity is 34.74733404415505
At time: 174.07440996170044 and batch: 450, loss is 3.5858442974090576 and perplexity is 36.08381032909008
At time: 174.55285930633545 and batch: 500, loss is 3.467142314910889 and perplexity is 32.045036850319505
At time: 175.03065371513367 and batch: 550, loss is 3.5249386119842527 and perplexity is 33.951689325434
At time: 175.50978016853333 and batch: 600, loss is 3.561934299468994 and perplexity is 35.23127912291181
At time: 175.9876139163971 and batch: 650, loss is 3.4117553663253783 and perplexity is 30.31841750561168
At time: 176.48804306983948 and batch: 700, loss is 3.393080768585205 and perplexity is 29.757487130637116
At time: 176.966082572937 and batch: 750, loss is 3.495981311798096 and perplexity is 32.982638329630625
At time: 177.44535779953003 and batch: 800, loss is 3.4614844799041746 and perplexity is 31.864243251495726
At time: 177.92367148399353 and batch: 850, loss is 3.5295825672149657 and perplexity is 34.10972612455577
At time: 178.40295100212097 and batch: 900, loss is 3.50677773475647 and perplexity is 33.34066205341694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3126417186162245 and perplexity of 74.63739974273147
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 179.6810941696167 and batch: 50, loss is 3.7393847608566286 and perplexity is 42.07209779940528
At time: 180.15777230262756 and batch: 100, loss is 3.6006305265426635 and perplexity is 36.62131787850376
At time: 180.63259100914001 and batch: 150, loss is 3.618644423484802 and perplexity is 37.28698818201094
At time: 181.10808682441711 and batch: 200, loss is 3.5110254955291746 and perplexity is 33.48258642681583
At time: 181.58560752868652 and batch: 250, loss is 3.653397250175476 and perplexity is 38.605596390656004
At time: 182.06216979026794 and batch: 300, loss is 3.6443835258483888 and perplexity is 38.25917978464813
At time: 182.53794050216675 and batch: 350, loss is 3.6134172010421755 and perplexity is 37.09258932624246
At time: 183.01311683654785 and batch: 400, loss is 3.547907257080078 and perplexity is 34.74053834449087
At time: 183.48857641220093 and batch: 450, loss is 3.584544448852539 and perplexity is 36.036937310832414
At time: 183.96428394317627 and batch: 500, loss is 3.464469609260559 and perplexity is 31.95950425190997
At time: 184.439026594162 and batch: 550, loss is 3.5224055242538452 and perplexity is 33.865795551937374
At time: 184.9137351512909 and batch: 600, loss is 3.558170757293701 and perplexity is 35.09893391763826
At time: 185.38926696777344 and batch: 650, loss is 3.406731352806091 and perplexity is 30.166479355425864
At time: 185.86723351478577 and batch: 700, loss is 3.387940721511841 and perplexity is 29.60492467100528
At time: 186.34256029129028 and batch: 750, loss is 3.490099186897278 and perplexity is 32.78919980393926
At time: 186.81773734092712 and batch: 800, loss is 3.4546330308914186 and perplexity is 31.646673199689896
At time: 187.29246830940247 and batch: 850, loss is 3.5225596952438356 and perplexity is 33.871017077657974
At time: 187.76815128326416 and batch: 900, loss is 3.498347144126892 and perplexity is 33.06076209911032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311507133588399 and perplexity of 74.55276528803914
Finished Training.
Improved accuracyfrom -10000000 to -74.55276528803914
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa3acf5db70>
ELAPSED
199.0548117160797


RESULTS SO FAR:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.699859619140625 and batch: 50, loss is 6.6952353286743165 and perplexity is 808.5441854551588
At time: 1.1939408779144287 and batch: 100, loss is 6.021668910980225 and perplexity is 412.2660572607873
At time: 1.675147294998169 and batch: 150, loss is 5.932918758392334 and perplexity is 377.25402178725324
At time: 2.169945001602173 and batch: 200, loss is 5.7815721893310545 and perplexity is 324.2686015026551
At time: 2.651545286178589 and batch: 250, loss is 5.845732793807984 and perplexity is 345.7558166921501
At time: 3.1325995922088623 and batch: 300, loss is 5.759788436889648 and perplexity is 317.28119681990785
At time: 3.613887071609497 and batch: 350, loss is 5.754875373840332 and perplexity is 315.7261973287447
At time: 4.095563888549805 and batch: 400, loss is 5.616653823852539 and perplexity is 274.96775159212507
At time: 4.576428413391113 and batch: 450, loss is 5.62354040145874 and perplexity is 276.86787351177077
At time: 5.058166742324829 and batch: 500, loss is 5.574967679977417 and perplexity is 263.7410313830972
At time: 5.540348768234253 and batch: 550, loss is 5.626657848358154 and perplexity is 277.7323411715013
At time: 6.022157430648804 and batch: 600, loss is 5.560807418823242 and perplexity is 260.0327068870265
At time: 6.503937721252441 and batch: 650, loss is 5.469461297988891 and perplexity is 237.3323069270388
At time: 6.985511779785156 and batch: 700, loss is 5.564951658248901 and perplexity is 261.11258076527514
At time: 7.466282367706299 and batch: 750, loss is 5.534710254669189 and perplexity is 253.33437454989337
At time: 7.947998762130737 and batch: 800, loss is 5.531941843032837 and perplexity is 252.6340106146405
At time: 8.428669452667236 and batch: 850, loss is 5.568612642288208 and perplexity is 262.07026171361844
At time: 8.909908533096313 and batch: 900, loss is 5.464222183227539 and perplexity is 236.09214724062213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.271358124197346 and perplexity of 194.68018284978962
finished 1 epochs...
Completing Train Step...
At time: 10.179133176803589 and batch: 50, loss is 5.15316367149353 and perplexity is 172.97787073571615
At time: 10.654892444610596 and batch: 100, loss is 4.9704388236999515 and perplexity is 144.09010365191492
At time: 11.130807399749756 and batch: 150, loss is 4.9258032894134525 and perplexity is 137.79999046891095
At time: 11.606378555297852 and batch: 200, loss is 4.801935434341431 and perplexity is 121.74582068384396
At time: 12.08210802078247 and batch: 250, loss is 4.880799236297608 and perplexity is 131.73590994291808
At time: 12.558152914047241 and batch: 300, loss is 4.8128014755249025 and perplexity is 123.07592920506723
At time: 13.033785343170166 and batch: 350, loss is 4.7974629211425786 and perplexity is 121.20252674423064
At time: 13.50944972038269 and batch: 400, loss is 4.662307634353637 and perplexity is 105.88013309068639
At time: 13.998133659362793 and batch: 450, loss is 4.672821006774902 and perplexity is 106.99916244076867
At time: 14.474960088729858 and batch: 500, loss is 4.577284097671509 and perplexity is 97.24991395060776
At time: 14.950989246368408 and batch: 550, loss is 4.652578678131103 and perplexity is 104.8550246145066
At time: 15.426398277282715 and batch: 600, loss is 4.60840142250061 and perplexity is 100.32364625845945
At time: 15.902364492416382 and batch: 650, loss is 4.467987556457519 and perplexity is 87.1810993159802
At time: 16.379289388656616 and batch: 700, loss is 4.512574110031128 and perplexity is 91.15616276014985
At time: 16.85511589050293 and batch: 750, loss is 4.559022579193115 and perplexity is 95.49010019112116
At time: 17.331358671188354 and batch: 800, loss is 4.507498168945313 and perplexity is 90.69463179132573
At time: 17.80733561515808 and batch: 850, loss is 4.568587970733643 and perplexity is 96.40788286517564
At time: 18.28377628326416 and batch: 900, loss is 4.50594012260437 and perplexity is 90.55343537596222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.613081840619649 and perplexity of 100.79430344719586
finished 2 epochs...
Completing Train Step...
At time: 19.55464506149292 and batch: 50, loss is 4.533144235610962 and perplexity is 93.05067484968461
At time: 20.030829906463623 and batch: 100, loss is 4.401468386650086 and perplexity is 81.57055788667554
At time: 20.506839752197266 and batch: 150, loss is 4.393411941528321 and perplexity is 80.91602929061622
At time: 20.98291254043579 and batch: 200, loss is 4.307840304374695 and perplexity is 74.27989362295948
At time: 21.457581996917725 and batch: 250, loss is 4.43126501083374 and perplexity is 84.03765824600407
At time: 21.93423342704773 and batch: 300, loss is 4.402719416618347 and perplexity is 81.67266895780699
At time: 22.40998888015747 and batch: 350, loss is 4.393931303024292 and perplexity is 80.95806487552393
At time: 22.88491153717041 and batch: 400, loss is 4.2972402048110965 and perplexity is 73.49667777161972
At time: 23.360121726989746 and batch: 450, loss is 4.335823640823365 and perplexity is 76.38784913768342
At time: 23.835870265960693 and batch: 500, loss is 4.220241141319275 and perplexity is 68.04989195177927
At time: 24.31204080581665 and batch: 550, loss is 4.312000594139099 and perplexity is 74.58956321507038
At time: 24.78744149208069 and batch: 600, loss is 4.2995884895324705 and perplexity is 73.66947170237117
At time: 25.26352596282959 and batch: 650, loss is 4.157999753952026 and perplexity is 63.943491879227416
At time: 25.74025869369507 and batch: 700, loss is 4.178731694221496 and perplexity is 65.28300187618323
At time: 26.229358434677124 and batch: 750, loss is 4.262627210617065 and perplexity is 70.99626078234472
At time: 26.705329656600952 and batch: 800, loss is 4.2272092628479 and perplexity is 68.52572778452621
At time: 27.18065309524536 and batch: 850, loss is 4.297021389007568 and perplexity is 73.48059729641169
At time: 27.656003952026367 and batch: 900, loss is 4.244454746246338 and perplexity is 69.71773592121227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.482318616893194 and perplexity of 88.43949243580165
finished 3 epochs...
Completing Train Step...
At time: 28.911270141601562 and batch: 50, loss is 4.308205413818359 and perplexity is 74.3070188651353
At time: 29.400118350982666 and batch: 100, loss is 4.17822536945343 and perplexity is 65.24995584211837
At time: 29.876376390457153 and batch: 150, loss is 4.175676245689392 and perplexity is 65.08383744720638
At time: 30.352387189865112 and batch: 200, loss is 4.095329604148865 and perplexity is 60.059131634394056
At time: 30.8290433883667 and batch: 250, loss is 4.225845217704773 and perplexity is 68.43231931952711
At time: 31.30539059638977 and batch: 300, loss is 4.205800862312317 and perplexity is 67.07429343571366
At time: 31.78237819671631 and batch: 350, loss is 4.192954235076904 and perplexity is 66.21812619637744
At time: 32.26931285858154 and batch: 400, loss is 4.1168613576889035 and perplexity is 61.366332717415425
At time: 32.7553653717041 and batch: 450, loss is 4.1596520900726315 and perplexity is 64.0492353583636
At time: 33.232266664505005 and batch: 500, loss is 4.039942855834961 and perplexity is 56.82309560433737
At time: 33.707817792892456 and batch: 550, loss is 4.129620504379273 and perplexity is 62.15433116207148
At time: 34.1833221912384 and batch: 600, loss is 4.129917616844177 and perplexity is 62.17280073224144
At time: 34.659995555877686 and batch: 650, loss is 3.989136772155762 and perplexity is 54.00824781175025
At time: 35.1358380317688 and batch: 700, loss is 3.997842493057251 and perplexity is 54.480481126771814
At time: 35.61258292198181 and batch: 750, loss is 4.095282945632935 and perplexity is 60.056329429817815
At time: 36.08875226974487 and batch: 800, loss is 4.062514152526855 and perplexity is 58.120250720036594
At time: 36.56532430648804 and batch: 850, loss is 4.138978281021118 and perplexity is 62.73868738367485
At time: 37.041367292404175 and batch: 900, loss is 4.089977645874024 and perplexity is 59.73855628600636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428271149935788 and perplexity of 83.78643743475291
finished 4 epochs...
Completing Train Step...
At time: 38.302826166152954 and batch: 50, loss is 4.163465809822083 and perplexity is 64.29396756558833
At time: 38.79348087310791 and batch: 100, loss is 4.040828471183777 and perplexity is 56.87344130014169
At time: 39.270368814468384 and batch: 150, loss is 4.039089217185974 and perplexity is 56.77460991134943
At time: 39.7491512298584 and batch: 200, loss is 3.9607289123535154 and perplexity is 52.49557668103766
At time: 40.226593255996704 and batch: 250, loss is 4.092860708236694 and perplexity is 59.91103478289539
At time: 40.70442867279053 and batch: 300, loss is 4.075797491073608 and perplexity is 58.89743204048774
At time: 41.18215537071228 and batch: 350, loss is 4.063089237213135 and perplexity is 58.153684398865344
At time: 41.66063904762268 and batch: 400, loss is 3.9977297019958495 and perplexity is 54.474336562012375
At time: 42.138355016708374 and batch: 450, loss is 4.041230974197387 and perplexity is 56.89633763927304
At time: 42.61656427383423 and batch: 500, loss is 3.9190884399414063 and perplexity is 50.35452267922588
At time: 43.09333872795105 and batch: 550, loss is 4.004861235618591 and perplexity is 54.86421067256982
At time: 43.57192850112915 and batch: 600, loss is 4.011576795578003 and perplexity is 55.233894496554925
At time: 44.05113244056702 and batch: 650, loss is 3.8726843357086183 and perplexity is 48.071252422613256
At time: 44.52885031700134 and batch: 700, loss is 3.879656810760498 and perplexity is 48.40759925314945
At time: 45.00420045852661 and batch: 750, loss is 3.9803109550476075 and perplexity is 53.53367820704779
At time: 45.4836688041687 and batch: 800, loss is 3.9488034677505492 and perplexity is 51.87326165636056
At time: 45.96083164215088 and batch: 850, loss is 4.024783272743225 and perplexity is 55.968177637510735
At time: 46.438308238983154 and batch: 900, loss is 3.980851802825928 and perplexity is 53.56263960912041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411740655768408 and perplexity of 82.4127910261642
finished 5 epochs...
Completing Train Step...
At time: 47.71485257148743 and batch: 50, loss is 4.060800156593323 and perplexity is 58.020718170206464
At time: 48.212015867233276 and batch: 100, loss is 3.939854598045349 and perplexity is 51.41112547890512
At time: 48.713645935058594 and batch: 150, loss is 3.9399579906463624 and perplexity is 51.41644128369211
At time: 49.20173764228821 and batch: 200, loss is 3.8637666654586793 and perplexity is 47.644474605212416
At time: 49.68024730682373 and batch: 250, loss is 3.995733528137207 and perplexity is 54.36570477543885
At time: 50.15835738182068 and batch: 300, loss is 3.9826634883880616 and perplexity is 53.65976622484795
At time: 50.63621401786804 and batch: 350, loss is 3.965877652168274 and perplexity is 52.766559758839215
At time: 51.12817978858948 and batch: 400, loss is 3.905420393943787 and perplexity is 49.67095689305145
At time: 51.60641956329346 and batch: 450, loss is 3.9510777044296264 and perplexity is 51.99136798064486
At time: 52.08514976501465 and batch: 500, loss is 3.8294154596328736 and perplexity is 46.03562068916655
At time: 52.56282591819763 and batch: 550, loss is 3.910535650253296 and perplexity is 49.92568751948338
At time: 53.04026794433594 and batch: 600, loss is 3.9240948295593263 and perplexity is 50.60724913447356
At time: 53.517871379852295 and batch: 650, loss is 3.786456422805786 and perplexity is 44.099851839815564
At time: 53.995962381362915 and batch: 700, loss is 3.7912566614151 and perplexity is 44.312050546031415
At time: 54.47462058067322 and batch: 750, loss is 3.8930739545822144 and perplexity is 49.061467688740194
At time: 54.952935457229614 and batch: 800, loss is 3.8625606441497804 and perplexity is 47.58704898880617
At time: 55.43025207519531 and batch: 850, loss is 3.9396078300476076 and perplexity is 51.39844042360627
At time: 55.90851020812988 and batch: 900, loss is 3.896001601219177 and perplexity is 49.20531279069536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.400437864538741 and perplexity of 81.48654092124426
finished 6 epochs...
Completing Train Step...
At time: 57.18253445625305 and batch: 50, loss is 3.97884222984314 and perplexity is 53.45510965650618
At time: 57.67097568511963 and batch: 100, loss is 3.8630942583084105 and perplexity is 47.61244888818428
At time: 58.14774751663208 and batch: 150, loss is 3.864530906677246 and perplexity is 47.680900393814284
At time: 58.62476587295532 and batch: 200, loss is 3.785420985221863 and perplexity is 44.054212828026806
At time: 59.10090708732605 and batch: 250, loss is 3.9198126888275144 and perplexity is 50.39100509576689
At time: 59.57692766189575 and batch: 300, loss is 3.9114242124557497 and perplexity is 49.970069313416886
At time: 60.053728103637695 and batch: 350, loss is 3.8918548154830934 and perplexity is 49.00169138044171
At time: 60.53037738800049 and batch: 400, loss is 3.8335444784164427 and perplexity is 46.22609559833034
At time: 61.009265184402466 and batch: 450, loss is 3.877339129447937 and perplexity is 48.29553577884836
At time: 61.485220432281494 and batch: 500, loss is 3.7565875673294067 and perplexity is 42.8021171433044
At time: 61.9600932598114 and batch: 550, loss is 3.837479419708252 and perplexity is 46.40835091748336
At time: 62.43930101394653 and batch: 600, loss is 3.8515365648269655 and perplexity is 47.06532662412791
At time: 62.941834449768066 and batch: 650, loss is 3.718727731704712 and perplexity is 41.21192811075204
At time: 63.41711187362671 and batch: 700, loss is 3.7197370195388793 and perplexity is 41.25354380599157
At time: 63.90743017196655 and batch: 750, loss is 3.821226463317871 and perplexity is 45.66017452272621
At time: 64.39249110221863 and batch: 800, loss is 3.7939334297180176 and perplexity is 44.43082252999364
At time: 64.8775007724762 and batch: 850, loss is 3.873266143798828 and perplexity is 48.099228803832546
At time: 65.36276412010193 and batch: 900, loss is 3.828557000160217 and perplexity is 45.99611793269004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.408963556159033 and perplexity of 82.18423999777126
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 66.664222240448 and batch: 50, loss is 3.928163905143738 and perplexity is 50.81359338679055
At time: 67.15961265563965 and batch: 100, loss is 3.798245139122009 and perplexity is 44.6228089226584
At time: 67.63587355613708 and batch: 150, loss is 3.80199848651886 and perplexity is 44.79060853449524
At time: 68.1111786365509 and batch: 200, loss is 3.700808720588684 and perplexity is 40.4800281582813
At time: 68.58759069442749 and batch: 250, loss is 3.8321208477020265 and perplexity is 46.1603335303857
At time: 69.06221008300781 and batch: 300, loss is 3.8123716163635253 and perplexity is 45.25764546216373
At time: 69.53757286071777 and batch: 350, loss is 3.7835161209106447 and perplexity is 43.970375405059755
At time: 70.01430606842041 and batch: 400, loss is 3.7116900634765626 and perplexity is 40.92291043182437
At time: 70.49064612388611 and batch: 450, loss is 3.7459459495544434 and perplexity is 42.34904834082036
At time: 70.96644711494446 and batch: 500, loss is 3.619310507774353 and perplexity is 37.31183273240743
At time: 71.44176363945007 and batch: 550, loss is 3.6860536336898804 and perplexity is 39.8871267379326
At time: 71.9168529510498 and batch: 600, loss is 3.6887088584899903 and perplexity is 39.9931767570664
At time: 72.39213967323303 and batch: 650, loss is 3.552348175048828 and perplexity is 34.895161305291914
At time: 72.86770987510681 and batch: 700, loss is 3.538489089012146 and perplexity is 34.41488206556605
At time: 73.34358239173889 and batch: 750, loss is 3.6223181867599488 and perplexity is 37.42422368085239
At time: 73.82025265693665 and batch: 800, loss is 3.5848094749450685 and perplexity is 36.04648930522261
At time: 74.29700136184692 and batch: 850, loss is 3.6483329200744627 and perplexity is 38.41057913946254
At time: 74.77253866195679 and batch: 900, loss is 3.5956175994873045 and perplexity is 36.43819725195569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325605836633134 and perplexity of 75.61130708612785
finished 8 epochs...
Completing Train Step...
At time: 76.05033731460571 and batch: 50, loss is 3.837191219329834 and perplexity is 46.3949779403283
At time: 76.53951740264893 and batch: 100, loss is 3.714916715621948 and perplexity is 41.055167687896855
At time: 77.01572132110596 and batch: 150, loss is 3.7227247095108034 and perplexity is 41.376980909205166
At time: 77.49117183685303 and batch: 200, loss is 3.627690863609314 and perplexity is 37.625833047049845
At time: 77.96633267402649 and batch: 250, loss is 3.7628486013412474 and perplexity is 43.0709433413247
At time: 78.44135642051697 and batch: 300, loss is 3.748665804862976 and perplexity is 42.46438840780476
At time: 78.91738080978394 and batch: 350, loss is 3.7241000366210937 and perplexity is 41.43392694352774
At time: 79.3945791721344 and batch: 400, loss is 3.657128162384033 and perplexity is 38.74989950528655
At time: 79.8710081577301 and batch: 450, loss is 3.695273928642273 and perplexity is 40.256598513117574
At time: 80.34644651412964 and batch: 500, loss is 3.5725909423828126 and perplexity is 35.608733912274175
At time: 80.82112884521484 and batch: 550, loss is 3.643639254570007 and perplexity is 38.230715170011514
At time: 81.29708313941956 and batch: 600, loss is 3.6530056619644165 and perplexity is 38.590481853758874
At time: 81.7731683254242 and batch: 650, loss is 3.520246915817261 and perplexity is 33.7927714035739
At time: 82.24914193153381 and batch: 700, loss is 3.510691442489624 and perplexity is 33.47140333502483
At time: 82.72549343109131 and batch: 750, loss is 3.600132188796997 and perplexity is 36.60307264003195
At time: 83.2017924785614 and batch: 800, loss is 3.565683488845825 and perplexity is 35.36361578291658
At time: 83.676913022995 and batch: 850, loss is 3.6357833766937255 and perplexity is 37.931555958174634
At time: 84.15356421470642 and batch: 900, loss is 3.590887179374695 and perplexity is 36.26623631520693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323525206683433 and perplexity of 75.45415148408878
finished 9 epochs...
Completing Train Step...
At time: 85.42506098747253 and batch: 50, loss is 3.797699055671692 and perplexity is 44.59844779740812
At time: 85.90128231048584 and batch: 100, loss is 3.677646141052246 and perplexity is 39.55318180044714
At time: 86.37766742706299 and batch: 150, loss is 3.685208239555359 and perplexity is 39.853420644419764
At time: 86.8532702922821 and batch: 200, loss is 3.591864404678345 and perplexity is 36.30169392121545
At time: 87.34196615219116 and batch: 250, loss is 3.7270481729507448 and perplexity is 41.556260047559306
At time: 87.81769561767578 and batch: 300, loss is 3.715296816825867 and perplexity is 41.07077577270011
At time: 88.29335904121399 and batch: 350, loss is 3.6913863754272462 and perplexity is 40.10040265072191
At time: 88.7698802947998 and batch: 400, loss is 3.626453504562378 and perplexity is 37.57930517390935
At time: 89.24510312080383 and batch: 450, loss is 3.6654490852355956 and perplexity is 39.073679636172145
At time: 89.72138214111328 and batch: 500, loss is 3.5449775981903078 and perplexity is 34.63890935931376
At time: 90.19716334342957 and batch: 550, loss is 3.617628984451294 and perplexity is 37.24914473587691
At time: 90.67351698875427 and batch: 600, loss is 3.6298296737670896 and perplexity is 37.70639368220759
At time: 91.14929604530334 and batch: 650, loss is 3.498292016983032 and perplexity is 33.0589396039569
At time: 91.62594270706177 and batch: 700, loss is 3.4909016466140748 and perplexity is 32.81552237591723
At time: 92.10105180740356 and batch: 750, loss is 3.582428312301636 and perplexity is 35.96075886103484
At time: 92.58831787109375 and batch: 800, loss is 3.549067177772522 and perplexity is 34.780857993060074
At time: 93.07357168197632 and batch: 850, loss is 3.6215883684158325 and perplexity is 37.396920760193844
At time: 93.55058598518372 and batch: 900, loss is 3.5803413915634157 and perplexity is 35.885789861994006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325955743659033 and perplexity of 75.63776864298703
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.86223578453064 and batch: 50, loss is 3.785319414138794 and perplexity is 44.049738421155126
At time: 95.33912372589111 and batch: 100, loss is 3.664110927581787 and perplexity is 39.02142786105805
At time: 95.81566452980042 and batch: 150, loss is 3.6763049173355102 and perplexity is 39.500167694779115
At time: 96.2923355102539 and batch: 200, loss is 3.5760476779937744 and perplexity is 35.73203688095226
At time: 96.77044725418091 and batch: 250, loss is 3.7091055059432985 and perplexity is 40.81727937895216
At time: 97.24736166000366 and batch: 300, loss is 3.692707643508911 and perplexity is 40.15342105085306
At time: 97.72314286231995 and batch: 350, loss is 3.667541227340698 and perplexity is 39.155512900099716
At time: 98.1975302696228 and batch: 400, loss is 3.5988291835784914 and perplexity is 36.55540970463902
At time: 98.67121815681458 and batch: 450, loss is 3.6321543979644777 and perplexity is 37.7941526161322
At time: 99.14459586143494 and batch: 500, loss is 3.508893566131592 and perplexity is 33.411279953696585
At time: 99.63621497154236 and batch: 550, loss is 3.5813060092926023 and perplexity is 35.92042263212735
At time: 100.10948395729065 and batch: 600, loss is 3.5912340688705444 and perplexity is 36.27881887389122
At time: 100.58334279060364 and batch: 650, loss is 3.4526253080368043 and perplexity is 31.583199191064985
At time: 101.05630850791931 and batch: 700, loss is 3.4422444820404055 and perplexity is 31.25703534988187
At time: 101.5304012298584 and batch: 750, loss is 3.5274971389770506 and perplexity is 34.03866685876093
At time: 102.00423979759216 and batch: 800, loss is 3.490080614089966 and perplexity is 32.78859082210465
At time: 102.47921228408813 and batch: 850, loss is 3.558695378303528 and perplexity is 35.11735238672946
At time: 102.9658534526825 and batch: 900, loss is 3.5181108999252317 and perplexity is 33.72066654279966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310700717037672 and perplexity of 74.49266893881048
finished 11 epochs...
Completing Train Step...
At time: 104.27330017089844 and batch: 50, loss is 3.761489863395691 and perplexity is 43.012460956371086
At time: 104.76998615264893 and batch: 100, loss is 3.6402029514312746 and perplexity is 38.099568302723114
At time: 105.24741387367249 and batch: 150, loss is 3.6531692504882813 and perplexity is 38.59679533011263
At time: 105.7246618270874 and batch: 200, loss is 3.5551635456085204 and perplexity is 34.99354253993902
At time: 106.20112156867981 and batch: 250, loss is 3.689564070701599 and perplexity is 40.02739403964352
At time: 106.6774652004242 and batch: 300, loss is 3.674034833908081 and perplexity is 39.41060071943134
At time: 107.16536378860474 and batch: 350, loss is 3.6496579456329346 and perplexity is 38.46150787202291
At time: 107.67214512825012 and batch: 400, loss is 3.583240270614624 and perplexity is 35.98996935538046
At time: 108.16047477722168 and batch: 450, loss is 3.6182091999053956 and perplexity is 37.270763536479336
At time: 108.63800001144409 and batch: 500, loss is 3.495534734725952 and perplexity is 32.9679123279659
At time: 109.1155652999878 and batch: 550, loss is 3.5700473737716676 and perplexity is 35.51827574645797
At time: 109.59158182144165 and batch: 600, loss is 3.582264657020569 and perplexity is 35.95487417447921
At time: 110.06929731369019 and batch: 650, loss is 3.4455196332931517 and perplexity is 31.359574692635277
At time: 110.55412292480469 and batch: 700, loss is 3.437220907211304 and perplexity is 31.100407040275858
At time: 111.04091286659241 and batch: 750, loss is 3.5243221378326415 and perplexity is 33.93076543674226
At time: 111.53056859970093 and batch: 800, loss is 3.488384795188904 and perplexity is 32.73303443015321
At time: 112.04940271377563 and batch: 850, loss is 3.5593505620956423 and perplexity is 35.14036824582081
At time: 112.55343508720398 and batch: 900, loss is 3.520581374168396 and perplexity is 33.804075568458614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309935687339469 and perplexity of 74.43570162839451
finished 12 epochs...
Completing Train Step...
At time: 113.89624404907227 and batch: 50, loss is 3.749450340270996 and perplexity is 42.49771629583529
At time: 114.40752267837524 and batch: 100, loss is 3.6284084558486938 and perplexity is 37.65284274265955
At time: 114.91689586639404 and batch: 150, loss is 3.6411737871170042 and perplexity is 38.13657468389341
At time: 115.41162133216858 and batch: 200, loss is 3.5436473989486696 and perplexity is 34.592863340332194
At time: 115.89613699913025 and batch: 250, loss is 3.678064856529236 and perplexity is 39.56974679759951
At time: 116.37671208381653 and batch: 300, loss is 3.663105592727661 and perplexity is 38.98221797240786
At time: 116.86542677879333 and batch: 350, loss is 3.639089193344116 and perplexity is 38.057158222077845
At time: 117.34397101402283 and batch: 400, loss is 3.5738930892944336 and perplexity is 35.65513191711815
At time: 117.82291150093079 and batch: 450, loss is 3.6092023611068726 and perplexity is 36.93657900966134
At time: 118.30168986320496 and batch: 500, loss is 3.487371745109558 and perplexity is 32.699891017829266
At time: 118.78003096580505 and batch: 550, loss is 3.562643041610718 and perplexity is 35.25625786583133
At time: 119.25514650344849 and batch: 600, loss is 3.5760698986053465 and perplexity is 35.73283087748598
At time: 119.73274040222168 and batch: 650, loss is 3.440162525177002 and perplexity is 31.192027246121143
At time: 120.2110059261322 and batch: 700, loss is 3.4327453565597534 and perplexity is 30.961526608308077
At time: 120.68905591964722 and batch: 750, loss is 3.5207762908935547 and perplexity is 33.81066519035834
At time: 121.19205689430237 and batch: 800, loss is 3.48552613735199 and perplexity is 32.639595503344985
At time: 121.6810142993927 and batch: 850, loss is 3.5577528619766237 and perplexity is 35.08426930187021
At time: 122.17294216156006 and batch: 900, loss is 3.5198402070999144 and perplexity is 33.77903038334654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310122555249358 and perplexity of 74.44961257209295
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 123.46695828437805 and batch: 50, loss is 3.746975736618042 and perplexity is 42.39268130543278
At time: 123.94199848175049 and batch: 100, loss is 3.628410186767578 and perplexity is 37.6529079167325
At time: 124.43089580535889 and batch: 150, loss is 3.6421950483322143 and perplexity is 38.175541983007825
At time: 124.90722751617432 and batch: 200, loss is 3.5439072370529177 and perplexity is 34.60185305224733
At time: 125.38328385353088 and batch: 250, loss is 3.6770420360565184 and perplexity is 39.52929474159681
At time: 125.85926938056946 and batch: 300, loss is 3.6604611015319826 and perplexity is 38.87926602794526
At time: 126.33456420898438 and batch: 350, loss is 3.6351100158691407 and perplexity is 37.90602293181145
At time: 126.81203627586365 and batch: 400, loss is 3.5693298768997193 and perplexity is 35.49280063496218
At time: 127.28749752044678 and batch: 450, loss is 3.600375084877014 and perplexity is 36.61196446274351
At time: 127.76440620422363 and batch: 500, loss is 3.4791278934478758 and perplexity is 32.43142607991646
At time: 128.2408266067505 and batch: 550, loss is 3.552391486167908 and perplexity is 34.89667268650808
At time: 128.7167785167694 and batch: 600, loss is 3.567127332687378 and perplexity is 35.41471220052706
At time: 129.19234490394592 and batch: 650, loss is 3.4270931816101076 and perplexity is 30.78702027794474
At time: 129.6687479019165 and batch: 700, loss is 3.4181078100204467 and perplexity is 30.51162657087182
At time: 130.145357131958 and batch: 750, loss is 3.5044357490539553 and perplexity is 33.262670063286535
At time: 130.62322282791138 and batch: 800, loss is 3.4683701324462892 and perplexity is 32.084406472897435
At time: 131.09973073005676 and batch: 850, loss is 3.5379945182800294 and perplexity is 34.39786568039707
At time: 131.5743978023529 and batch: 900, loss is 3.5012284755706786 and perplexity is 33.15615848080017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30734754588506 and perplexity of 74.24330059127152
finished 14 epochs...
Completing Train Step...
At time: 132.85295987129211 and batch: 50, loss is 3.7393033838272096 and perplexity is 42.06867423636652
At time: 133.3291003704071 and batch: 100, loss is 3.618579511642456 and perplexity is 37.28456789346605
At time: 133.80498266220093 and batch: 150, loss is 3.6325368595123293 and perplexity is 37.80861019079847
At time: 134.2807376384735 and batch: 200, loss is 3.5356787109375 and perplexity is 34.31829901657863
At time: 134.75735020637512 and batch: 250, loss is 3.670207629203796 and perplexity is 39.26005654839692
At time: 135.23342847824097 and batch: 300, loss is 3.654040665626526 and perplexity is 38.63044382062374
At time: 135.7096254825592 and batch: 350, loss is 3.6287830018997194 and perplexity is 37.66694810760818
At time: 136.1852285861969 and batch: 400, loss is 3.564567971229553 and perplexity is 35.32418904123659
At time: 136.6740472316742 and batch: 450, loss is 3.5963157320022585 and perplexity is 36.463644824098786
At time: 137.15094876289368 and batch: 500, loss is 3.4753622436523437 and perplexity is 32.30953033930646
At time: 137.62670993804932 and batch: 550, loss is 3.5489891815185546 and perplexity is 34.778145322217306
At time: 138.10196781158447 and batch: 600, loss is 3.5648497104644776 and perplexity is 35.33414265332709
At time: 138.57818269729614 and batch: 650, loss is 3.42545147895813 and perplexity is 30.736518610811
At time: 139.054053068161 and batch: 700, loss is 3.4170331382751464 and perplexity is 30.478854200817132
At time: 139.5302917957306 and batch: 750, loss is 3.504064955711365 and perplexity is 33.25033877299174
At time: 140.00674295425415 and batch: 800, loss is 3.468472728729248 and perplexity is 32.08769838260845
At time: 140.48269724845886 and batch: 850, loss is 3.539367322921753 and perplexity is 34.445119657887105
At time: 140.95775866508484 and batch: 900, loss is 3.5033599948883056 and perplexity is 33.22690684706864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307049894986087 and perplexity of 74.22120529461341
finished 15 epochs...
Completing Train Step...
At time: 142.21155858039856 and batch: 50, loss is 3.7357014799118042 and perplexity is 41.91741947999616
At time: 142.70065212249756 and batch: 100, loss is 3.614817328453064 and perplexity is 37.14456005162745
At time: 143.17665576934814 and batch: 150, loss is 3.628661022186279 and perplexity is 37.66235378428466
At time: 143.65266132354736 and batch: 200, loss is 3.5319414710998536 and perplexity is 34.19028266523758
At time: 144.1280221939087 and batch: 250, loss is 3.666515712738037 and perplexity is 39.115378932348314
At time: 144.60528802871704 and batch: 300, loss is 3.650567331314087 and perplexity is 38.49650012487261
At time: 145.0812692642212 and batch: 350, loss is 3.62543936252594 and perplexity is 37.541213739159666
At time: 145.5588059425354 and batch: 400, loss is 3.561736478805542 and perplexity is 35.22431033720908
At time: 146.03580951690674 and batch: 450, loss is 3.5936863136291506 and perplexity is 36.36789258796027
At time: 146.51214051246643 and batch: 500, loss is 3.4730370378494264 and perplexity is 32.234491306278215
At time: 146.98839211463928 and batch: 550, loss is 3.546980266571045 and perplexity is 34.70834911702605
At time: 147.46352982521057 and batch: 600, loss is 3.563326106071472 and perplexity is 35.28034838935222
At time: 147.93948698043823 and batch: 650, loss is 3.424191675186157 and perplexity is 30.697821009537915
At time: 148.41434812545776 and batch: 700, loss is 3.4160824728012087 and perplexity is 30.449892774938256
At time: 148.90239810943604 and batch: 750, loss is 3.5034961986541746 and perplexity is 33.231432785127296
At time: 149.37828969955444 and batch: 800, loss is 3.4681792640686036 and perplexity is 32.07828315867706
At time: 149.85482096672058 and batch: 850, loss is 3.539608941078186 and perplexity is 34.45344322971951
At time: 150.33020210266113 and batch: 900, loss is 3.503881049156189 and perplexity is 33.24422437998528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307021885702055 and perplexity of 74.21912644090682
finished 16 epochs...
Completing Train Step...
At time: 151.58605337142944 and batch: 50, loss is 3.7325667667388918 and perplexity is 41.78622612717351
At time: 152.07398080825806 and batch: 100, loss is 3.6117066621780394 and perplexity is 37.02919524510554
At time: 152.54980659484863 and batch: 150, loss is 3.6255163288116456 and perplexity is 37.5441032581384
At time: 153.0255720615387 and batch: 200, loss is 3.5289055776596068 and perplexity is 34.08664201096684
At time: 153.50075030326843 and batch: 250, loss is 3.6634871196746825 and perplexity is 38.997093576560474
At time: 153.97774720191956 and batch: 300, loss is 3.6477187156677244 and perplexity is 38.38699443614551
At time: 154.45374846458435 and batch: 350, loss is 3.6227061986923217 and perplexity is 37.43874754373419
At time: 154.92996835708618 and batch: 400, loss is 3.5593821573257447 and perplexity is 35.14147853138119
At time: 155.4057559967041 and batch: 450, loss is 3.5914148807525637 and perplexity is 36.285379108475546
At time: 155.882470369339 and batch: 500, loss is 3.4710252189636233 and perplexity is 32.16970653735571
At time: 156.35567021369934 and batch: 550, loss is 3.545197525024414 and perplexity is 34.6465282227516
At time: 156.8326382637024 and batch: 600, loss is 3.5619348382949827 and perplexity is 35.23129810644573
At time: 157.30874061584473 and batch: 650, loss is 3.4229553270339967 and perplexity is 30.659891267272283
At time: 157.78386926651 and batch: 700, loss is 3.4150613021850584 and perplexity is 30.41881411018189
At time: 158.259206533432 and batch: 750, loss is 3.502754774093628 and perplexity is 33.206803316253065
At time: 158.73622059822083 and batch: 800, loss is 3.4676343250274657 and perplexity is 32.06080721190653
At time: 159.21275186538696 and batch: 850, loss is 3.5394274282455442 and perplexity is 34.44719005517751
At time: 159.68893885612488 and batch: 900, loss is 3.5039075660705565 and perplexity is 33.24510592592427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307090445740582 and perplexity of 74.22421508151177
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 160.95953917503357 and batch: 50, loss is 3.7318882656097414 and perplexity is 41.757883741820834
At time: 161.43478870391846 and batch: 100, loss is 3.611978144645691 and perplexity is 37.039249387105734
At time: 161.91213703155518 and batch: 150, loss is 3.6261699056625365 and perplexity is 37.568649235382864
At time: 162.3875389099121 and batch: 200, loss is 3.529589958190918 and perplexity is 34.109978229652945
At time: 162.86410450935364 and batch: 250, loss is 3.6639912605285643 and perplexity is 39.01675856115963
At time: 163.33953642845154 and batch: 300, loss is 3.6473102283477785 and perplexity is 38.3713170378947
At time: 163.81524324417114 and batch: 350, loss is 3.6221556997299196 and perplexity is 37.41814322390641
At time: 164.29157638549805 and batch: 400, loss is 3.5587286615371703 and perplexity is 35.11852122522511
At time: 164.76740908622742 and batch: 450, loss is 3.588870449066162 and perplexity is 36.19317079873876
At time: 165.24380350112915 and batch: 500, loss is 3.4685607099533082 and perplexity is 32.09052162178363
At time: 165.72166967391968 and batch: 550, loss is 3.542252721786499 and perplexity is 34.54465109193313
At time: 166.19688749313354 and batch: 600, loss is 3.559397215843201 and perplexity is 35.14200771393343
At time: 166.67309641838074 and batch: 650, loss is 3.419339919090271 and perplexity is 30.54924339195928
At time: 167.1492314338684 and batch: 700, loss is 3.4093007707595824 and perplexity is 30.244089312586148
At time: 167.6249177455902 and batch: 750, loss is 3.4961259317398072 and perplexity is 32.98740862179472
At time: 168.1230022907257 and batch: 800, loss is 3.460717434883118 and perplexity is 31.839811313755913
At time: 168.6000373363495 and batch: 850, loss is 3.5319709014892577 and perplexity is 34.19128891337733
At time: 169.08760452270508 and batch: 900, loss is 3.4953609132766723 and perplexity is 32.962182295681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30669162697988 and perplexity of 74.19461897417263
finished 18 epochs...
Completing Train Step...
At time: 170.38279175758362 and batch: 50, loss is 3.7299213647842406 and perplexity is 41.675830847224105
At time: 170.86246705055237 and batch: 100, loss is 3.6099864053726196 and perplexity is 36.96555027852467
At time: 171.34410190582275 and batch: 150, loss is 3.623496050834656 and perplexity is 37.468330300151706
At time: 171.82347750663757 and batch: 200, loss is 3.5275514936447143 and perplexity is 34.04051706946909
At time: 172.3004858493805 and batch: 250, loss is 3.6619120979309083 and perplexity is 38.935720650766974
At time: 172.77791023254395 and batch: 300, loss is 3.6452870893478395 and perplexity is 38.293765005639095
At time: 173.26825094223022 and batch: 350, loss is 3.6199591970443725 and perplexity is 37.33604437001146
At time: 173.74556756019592 and batch: 400, loss is 3.557119073867798 and perplexity is 35.0620403541453
At time: 174.22163844108582 and batch: 450, loss is 3.587593297958374 and perplexity is 36.14697615560768
At time: 174.69835901260376 and batch: 500, loss is 3.4673355627059936 and perplexity is 32.051230081429736
At time: 175.17517471313477 and batch: 550, loss is 3.5410107612609862 and perplexity is 34.501774629857316
At time: 175.65170526504517 and batch: 600, loss is 3.558627920150757 and perplexity is 35.114983514908005
At time: 176.1286280155182 and batch: 650, loss is 3.418766555786133 and perplexity is 30.531732597332397
At time: 176.60528564453125 and batch: 700, loss is 3.4092019510269167 and perplexity is 30.2411007474326
At time: 177.08221125602722 and batch: 750, loss is 3.4961836242675783 and perplexity is 32.98931180368189
At time: 177.5591630935669 and batch: 800, loss is 3.4610344409942626 and perplexity is 31.84990632852622
At time: 178.03787970542908 and batch: 850, loss is 3.5329181814193724 and perplexity is 34.22369298058786
At time: 178.5366621017456 and batch: 900, loss is 3.4968097591400147 and perplexity is 33.0099740302151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306519390785531 and perplexity of 74.18184107579918
finished 19 epochs...
Completing Train Step...
At time: 179.79840540885925 and batch: 50, loss is 3.7288015031814576 and perplexity is 41.62918580736353
At time: 180.28901743888855 and batch: 100, loss is 3.6088000440597536 and perplexity is 36.92172178312165
At time: 180.76741099357605 and batch: 150, loss is 3.6220947265625 and perplexity is 37.415861790748906
At time: 181.26395893096924 and batch: 200, loss is 3.5263298749923706 and perplexity is 33.99895792876074
At time: 181.75462746620178 and batch: 250, loss is 3.6607329654693603 and perplexity is 38.889837335203474
At time: 182.24324297904968 and batch: 300, loss is 3.644136600494385 and perplexity is 38.24973378941177
At time: 182.72900700569153 and batch: 350, loss is 3.618789176940918 and perplexity is 37.29238599308491
At time: 183.20982432365417 and batch: 400, loss is 3.5561686086654665 and perplexity is 35.02873093709446
At time: 183.69184732437134 and batch: 450, loss is 3.586794571876526 and perplexity is 36.11811615012611
At time: 184.1711461544037 and batch: 500, loss is 3.466648454666138 and perplexity is 32.02921498779358
At time: 184.65002703666687 and batch: 550, loss is 3.5403489017486574 and perplexity is 34.47894685734883
At time: 185.12882685661316 and batch: 600, loss is 3.558218903541565 and perplexity is 35.1006238402918
At time: 185.62226915359497 and batch: 650, loss is 3.4184999227523805 and perplexity is 30.523592914046702
At time: 186.10124897956848 and batch: 700, loss is 3.4091144943237306 and perplexity is 30.238456076109312
At time: 186.58097076416016 and batch: 750, loss is 3.496191244125366 and perplexity is 32.98956317850407
At time: 187.06067395210266 and batch: 800, loss is 3.4611536169052126 and perplexity is 31.85370229631606
At time: 187.53974986076355 and batch: 850, loss is 3.533371953964233 and perplexity is 34.23922627687135
At time: 188.01897716522217 and batch: 900, loss is 3.4974528837203978 and perplexity is 33.03121038399033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306454593188142 and perplexity of 74.17703442645892
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa3acf5db70>
ELAPSED
393.8880202770233


RESULTS SO FAR:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}, {'best_accuracy': -74.17703442645892, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34768402145082, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8801144632298448, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.703758955001831 and batch: 50, loss is 7.083403654098511 and perplexity is 1192.0188414649815
At time: 1.196162462234497 and batch: 100, loss is 6.36047776222229 and perplexity is 578.5226866628138
At time: 1.6782968044281006 and batch: 150, loss is 6.282771873474121 and perplexity is 535.2703109841749
At time: 2.1607072353363037 and batch: 200, loss is 6.1481836223602295 and perplexity is 467.8667917364635
At time: 2.6434314250946045 and batch: 250, loss is 6.204463005065918 and perplexity is 494.95309725835614
At time: 3.126176595687866 and batch: 300, loss is 6.1153542423248295 and perplexity is 452.7564044481788
At time: 3.6090879440307617 and batch: 350, loss is 6.132707958221435 and perplexity is 460.68198066139917
At time: 4.0911993980407715 and batch: 400, loss is 6.018908977508545 and perplexity is 411.1297990898569
At time: 4.573469638824463 and batch: 450, loss is 6.020779571533203 and perplexity is 411.8995757807706
At time: 5.055990219116211 and batch: 500, loss is 5.989558181762695 and perplexity is 399.2381802459934
At time: 5.538558483123779 and batch: 550, loss is 6.033702793121338 and perplexity is 417.2571895211917
At time: 6.021887540817261 and batch: 600, loss is 5.972568550109863 and perplexity is 392.51256525942864
At time: 6.50473165512085 and batch: 650, loss is 5.904123525619507 and perplexity is 366.5458169369088
At time: 6.986737966537476 and batch: 700, loss is 6.006713724136352 and perplexity is 406.1464155956048
At time: 7.468865156173706 and batch: 750, loss is 5.962655115127563 and perplexity is 388.6406412074441
At time: 7.950126886367798 and batch: 800, loss is 5.974318819046021 and perplexity is 393.20016937998366
At time: 8.431435823440552 and batch: 850, loss is 6.010898332595826 and perplexity is 407.84954029182006
At time: 8.913921117782593 and batch: 900, loss is 5.885035009384155 and perplexity is 359.6153576884301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.8013368371414815 and perplexity of 330.74141190542633
finished 1 epochs...
Completing Train Step...
At time: 10.196869373321533 and batch: 50, loss is 5.535293607711792 and perplexity is 253.48220104140762
At time: 10.673802137374878 and batch: 100, loss is 5.278364629745483 and perplexity is 196.04900034499892
At time: 11.162693738937378 and batch: 150, loss is 5.193029308319092 and perplexity is 180.01304280971723
At time: 11.639072895050049 and batch: 200, loss is 5.037948150634765 and perplexity is 154.15339076733872
At time: 12.115417718887329 and batch: 250, loss is 5.080756797790527 and perplexity is 160.89577543162412
At time: 12.592425346374512 and batch: 300, loss is 4.989947080612183 and perplexity is 146.9286479158566
At time: 13.069547414779663 and batch: 350, loss is 4.959705610275268 and perplexity is 142.55182392702795
At time: 13.546799898147583 and batch: 400, loss is 4.809824333190918 and perplexity is 122.71005953920854
At time: 14.022931575775146 and batch: 450, loss is 4.804390935897827 and perplexity is 122.04513506909467
At time: 14.499908208847046 and batch: 500, loss is 4.713942918777466 and perplexity is 111.4908939301792
At time: 14.976824522018433 and batch: 550, loss is 4.780624160766601 and perplexity is 119.17871351220859
At time: 15.454533100128174 and batch: 600, loss is 4.719375019073486 and perplexity is 112.09817155129316
At time: 15.931341886520386 and batch: 650, loss is 4.578458728790284 and perplexity is 97.36421384279525
At time: 16.40859603881836 and batch: 700, loss is 4.628083124160766 and perplexity is 102.31774559703896
At time: 16.885618686676025 and batch: 750, loss is 4.659565896987915 and perplexity is 105.59023516708852
At time: 17.36324906349182 and batch: 800, loss is 4.606907062530517 and perplexity is 100.17383857861513
At time: 17.83965492248535 and batch: 850, loss is 4.651531400680542 and perplexity is 104.74526979355585
At time: 18.315953969955444 and batch: 900, loss is 4.586731672286987 and perplexity is 98.17304357070869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.682543610873288 and perplexity of 108.04454662942109
finished 2 epochs...
Completing Train Step...
At time: 19.59321093559265 and batch: 50, loss is 4.633360891342163 and perplexity is 102.85918236863583
At time: 20.06893825531006 and batch: 100, loss is 4.4979845142364505 and perplexity is 89.83588576416354
At time: 20.544990062713623 and batch: 150, loss is 4.488027200698853 and perplexity is 88.94580046605238
At time: 21.020410776138306 and batch: 200, loss is 4.395498027801514 and perplexity is 81.0850032944979
At time: 21.496772050857544 and batch: 250, loss is 4.517939300537109 and perplexity is 91.64654726597949
At time: 21.972867012023926 and batch: 300, loss is 4.479044427871704 and perplexity is 88.15039835320286
At time: 22.449118852615356 and batch: 350, loss is 4.4653004455566405 and perplexity is 86.94714850032257
At time: 22.925484895706177 and batch: 400, loss is 4.3675147771835325 and perplexity is 78.84743456035324
At time: 23.41416025161743 and batch: 450, loss is 4.398736114501953 and perplexity is 81.34798912093606
At time: 23.88989758491516 and batch: 500, loss is 4.293115096092224 and perplexity is 73.19412045528696
At time: 24.366113901138306 and batch: 550, loss is 4.386521329879761 and perplexity is 80.36038491993995
At time: 24.84235644340515 and batch: 600, loss is 4.358769006729126 and perplexity is 78.16085968614568
At time: 25.3191819190979 and batch: 650, loss is 4.214840722084046 and perplexity is 67.68338454390612
At time: 25.794936656951904 and batch: 700, loss is 4.250421228408814 and perplexity is 70.13494895837313
At time: 26.270169258117676 and batch: 750, loss is 4.329403581619263 and perplexity is 75.89900550533716
At time: 26.745638608932495 and batch: 800, loss is 4.284330644607544 and perplexity is 72.55396607427382
At time: 27.222596168518066 and batch: 850, loss is 4.34970911026001 and perplexity is 77.45592851299925
At time: 27.698970556259155 and batch: 900, loss is 4.29381076335907 and perplexity is 73.24505692437614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5128236535477315 and perplexity of 91.17891302803797
finished 3 epochs...
Completing Train Step...
At time: 28.953122854232788 and batch: 50, loss is 4.378703918457031 and perplexity is 79.73462383165685
At time: 29.441935300827026 and batch: 100, loss is 4.246565494537354 and perplexity is 69.86504792775175
At time: 29.918269395828247 and batch: 150, loss is 4.244875998497009 and perplexity is 69.74711086107617
At time: 30.394840240478516 and batch: 200, loss is 4.156376910209656 and perplexity is 63.83980573952323
At time: 30.870017290115356 and batch: 250, loss is 4.286392230987548 and perplexity is 72.70369663075451
At time: 31.345702171325684 and batch: 300, loss is 4.2616498661041256 and perplexity is 70.92690687328168
At time: 31.821502447128296 and batch: 350, loss is 4.2509775066375735 and perplexity is 70.17397435703731
At time: 32.296279430389404 and batch: 400, loss is 4.167632813453674 and perplexity is 64.5624397357002
At time: 32.772135734558105 and batch: 450, loss is 4.205023922920227 and perplexity is 67.02220101389454
At time: 33.24830865859985 and batch: 500, loss is 4.098838086128235 and perplexity is 60.27021809542252
At time: 33.724462032318115 and batch: 550, loss is 4.189683260917664 and perplexity is 66.00188227366179
At time: 34.20016622543335 and batch: 600, loss is 4.177075500488281 and perplexity is 65.17497006309044
At time: 34.675838470458984 and batch: 650, loss is 4.031969695091248 and perplexity is 56.37183729722532
At time: 35.1512668132782 and batch: 700, loss is 4.0556481838226315 and perplexity is 57.722565706229304
At time: 35.641687631607056 and batch: 750, loss is 4.152458987236023 and perplexity is 63.590175633069514
At time: 36.124921560287476 and batch: 800, loss is 4.108880352973938 and perplexity is 60.878516945984614
At time: 36.61448431015015 and batch: 850, loss is 4.181239628791809 and perplexity is 65.446932851599
At time: 37.092384815216064 and batch: 900, loss is 4.129917602539063 and perplexity is 62.17279984285242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.44753066807577 and perplexity of 85.41576350048688
finished 4 epochs...
Completing Train Step...
At time: 38.350670337677 and batch: 50, loss is 4.224376859664917 and perplexity is 68.33190990979121
At time: 38.83998703956604 and batch: 100, loss is 4.096463966369629 and perplexity is 60.12729910032132
At time: 39.31620216369629 and batch: 150, loss is 4.0965855598449705 and perplexity is 60.13461063209012
At time: 39.79322409629822 and batch: 200, loss is 4.006262159347534 and perplexity is 54.941125110221854
At time: 40.26994800567627 and batch: 250, loss is 4.142545104026794 and perplexity is 62.96286473998725
At time: 40.74652600288391 and batch: 300, loss is 4.124016060829162 and perplexity is 61.80696502784825
At time: 41.222784996032715 and batch: 350, loss is 4.117064046859741 and perplexity is 61.378772269149884
At time: 41.69916486740112 and batch: 400, loss is 4.0378144216537475 and perplexity is 56.70228000500657
At time: 42.17617440223694 and batch: 450, loss is 4.0796757364273075 and perplexity is 59.12629423766445
At time: 42.65268015861511 and batch: 500, loss is 3.969037299156189 and perplexity is 52.93354713209463
At time: 43.12888741493225 and batch: 550, loss is 4.057341680526734 and perplexity is 57.82040149992001
At time: 43.60467004776001 and batch: 600, loss is 4.05092134475708 and perplexity is 57.45036426071483
At time: 44.07981610298157 and batch: 650, loss is 3.9104065704345703 and perplexity is 49.919243536691624
At time: 44.55615735054016 and batch: 700, loss is 3.9273569059371947 and perplexity is 50.77260339891421
At time: 45.03220081329346 and batch: 750, loss is 4.030592617988586 and perplexity is 56.294262356444726
At time: 45.513800859451294 and batch: 800, loss is 3.992954411506653 and perplexity is 54.214825893280214
At time: 45.9904420375824 and batch: 850, loss is 4.064450101852417 and perplexity is 58.23287756497561
At time: 46.46691560745239 and batch: 900, loss is 4.015869684219361 and perplexity is 55.47151713369033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4217592004227315 and perplexity of 83.2425970367232
finished 5 epochs...
Completing Train Step...
At time: 47.74641489982605 and batch: 50, loss is 4.116236462593078 and perplexity is 61.32799717614488
At time: 48.22267937660217 and batch: 100, loss is 3.9882460594177247 and perplexity is 53.96016339534542
At time: 48.700700998306274 and batch: 150, loss is 3.9869675254821777 and perplexity is 53.89121757945132
At time: 49.17880654335022 and batch: 200, loss is 3.901327414512634 and perplexity is 49.468070176978294
At time: 49.656859159469604 and batch: 250, loss is 4.037679677009582 and perplexity is 56.6946401911875
At time: 50.13399410247803 and batch: 300, loss is 4.021980619430542 and perplexity is 55.81153784509871
At time: 50.61194682121277 and batch: 350, loss is 4.0139302206039424 and perplexity is 55.36403640568904
At time: 51.089767932891846 and batch: 400, loss is 3.9433891248703 and perplexity is 51.59316099620845
At time: 51.56701898574829 and batch: 450, loss is 3.987755436897278 and perplexity is 53.933695817302514
At time: 52.04535627365112 and batch: 500, loss is 3.874326024055481 and perplexity is 48.150235252391035
At time: 52.52294445037842 and batch: 550, loss is 3.9591305541992186 and perplexity is 52.411736968777994
At time: 53.00067710876465 and batch: 600, loss is 3.9563799142837524 and perplexity is 52.26776924548775
At time: 53.47848582267761 and batch: 650, loss is 3.8177934885025024 and perplexity is 45.503693045691165
At time: 53.95677089691162 and batch: 700, loss is 3.831549634933472 and perplexity is 46.133973687728485
At time: 54.43439960479736 and batch: 750, loss is 3.9385682106018067 and perplexity is 51.3450333717731
At time: 54.912532567977905 and batch: 800, loss is 3.9027193546295167 and perplexity is 49.53697471273097
At time: 55.390159606933594 and batch: 850, loss is 3.9757119941711427 and perplexity is 53.288044178981686
At time: 55.86812400817871 and batch: 900, loss is 3.9286864757537843 and perplexity is 50.84015401658316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.412757769022902 and perplexity of 82.4966568115277
finished 6 epochs...
Completing Train Step...
At time: 57.139602184295654 and batch: 50, loss is 4.030166764259338 and perplexity is 56.270294338682056
At time: 57.61819076538086 and batch: 100, loss is 3.9038863945007325 and perplexity is 49.59482008468249
At time: 58.0967743396759 and batch: 150, loss is 3.9053236722946165 and perplexity is 49.66615286851522
At time: 58.57492685317993 and batch: 200, loss is 3.8187409353256228 and perplexity is 45.54682580487885
At time: 59.05307698249817 and batch: 250, loss is 3.9589123916625977 and perplexity is 52.400303938466976
At time: 59.530306339263916 and batch: 300, loss is 3.945550060272217 and perplexity is 51.704771031909054
At time: 60.02067446708679 and batch: 350, loss is 3.933554949760437 and perplexity is 51.088271471523164
At time: 60.49683952331543 and batch: 400, loss is 3.865133147239685 and perplexity is 47.70962441460211
At time: 60.97503113746643 and batch: 450, loss is 3.9133591222763062 and perplexity is 50.0668504924975
At time: 61.45214772224426 and batch: 500, loss is 3.799485583305359 and perplexity is 44.67819537122829
At time: 61.92945575714111 and batch: 550, loss is 3.881879758834839 and perplexity is 48.51532652439663
At time: 62.40821599960327 and batch: 600, loss is 3.8803850173950196 and perplexity is 48.44286282611244
At time: 62.88595461845398 and batch: 650, loss is 3.7465157461166383 and perplexity is 42.3731855589761
At time: 63.36339831352234 and batch: 700, loss is 3.7583894872665407 and perplexity is 42.87931266072013
At time: 63.841920375823975 and batch: 750, loss is 3.86186514377594 and perplexity is 47.553963685198674
At time: 64.31931400299072 and batch: 800, loss is 3.8292657947540283 and perplexity is 46.02873128913701
At time: 64.7972686290741 and batch: 850, loss is 3.9039150285720825 and perplexity is 49.59624020663122
At time: 65.27564454078674 and batch: 900, loss is 3.8552411222457885 and perplexity is 47.240006184501866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.40470865328018 and perplexity of 81.83529692429674
finished 7 epochs...
Completing Train Step...
At time: 66.52986907958984 and batch: 50, loss is 3.963577919006348 and perplexity is 52.64535017978612
At time: 67.01868057250977 and batch: 100, loss is 3.8331708097457886 and perplexity is 46.20882558147165
At time: 67.49392914772034 and batch: 150, loss is 3.8376790618896486 and perplexity is 46.41761690680594
At time: 67.96893095970154 and batch: 200, loss is 3.75155957698822 and perplexity is 42.58744863955432
At time: 68.44497203826904 and batch: 250, loss is 3.8916172790527344 and perplexity is 48.990053075905024
At time: 68.92086291313171 and batch: 300, loss is 3.882752637863159 and perplexity is 48.557693023196045
At time: 69.39638924598694 and batch: 350, loss is 3.867432155609131 and perplexity is 47.81943542027833
At time: 69.8721764087677 and batch: 400, loss is 3.802785725593567 and perplexity is 44.82588333474227
At time: 70.34735774993896 and batch: 450, loss is 3.851266875267029 and perplexity is 47.05263530833699
At time: 70.82396030426025 and batch: 500, loss is 3.7395460176467896 and perplexity is 42.078882757897354
At time: 71.29926943778992 and batch: 550, loss is 3.8189859628677367 and perplexity is 45.55798739905006
At time: 71.77483367919922 and batch: 600, loss is 3.819074192047119 and perplexity is 45.56200712021834
At time: 72.26403999328613 and batch: 650, loss is 3.688943600654602 and perplexity is 40.00256594392397
At time: 72.74124026298523 and batch: 700, loss is 3.6987934637069704 and perplexity is 40.39853264773879
At time: 73.21857571601868 and batch: 750, loss is 3.8005857133865355 and perplexity is 44.72737424454777
At time: 73.69660568237305 and batch: 800, loss is 3.7675484561920167 and perplexity is 43.27384695860105
At time: 74.17471480369568 and batch: 850, loss is 3.8439676713943483 and perplexity is 46.710438930342576
At time: 74.65209555625916 and batch: 900, loss is 3.796562829017639 and perplexity is 44.547802629934544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411620257651969 and perplexity of 82.40286927864607
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.92474317550659 and batch: 50, loss is 3.9187443399429323 and perplexity is 50.33719866881572
At time: 76.41592359542847 and batch: 100, loss is 3.77324830532074 and perplexity is 43.521205641382984
At time: 76.89343070983887 and batch: 150, loss is 3.7869927978515623 and perplexity is 44.12351224472299
At time: 77.36964988708496 and batch: 200, loss is 3.6779437017440797 and perplexity is 39.564953023827734
At time: 77.84306216239929 and batch: 250, loss is 3.806000409126282 and perplexity is 44.970216231734504
At time: 78.32874369621277 and batch: 300, loss is 3.7914453125 and perplexity is 44.32041085100657
At time: 78.80696964263916 and batch: 350, loss is 3.758502221107483 and perplexity is 42.884146882818435
At time: 79.280202627182 and batch: 400, loss is 3.6899369955062866 and perplexity is 40.042324031457014
At time: 79.75380873680115 and batch: 450, loss is 3.7255149936676024 and perplexity is 41.4925956674948
At time: 80.2274010181427 and batch: 500, loss is 3.606631498336792 and perplexity is 36.841742092415224
At time: 80.70068955421448 and batch: 550, loss is 3.670396718978882 and perplexity is 39.26748092557429
At time: 81.17425870895386 and batch: 600, loss is 3.670103392601013 and perplexity is 39.25596442675538
At time: 81.64668440818787 and batch: 650, loss is 3.5288143444061277 and perplexity is 34.08353231757189
At time: 82.1198205947876 and batch: 700, loss is 3.5214626359939576 and perplexity is 33.83387894016355
At time: 82.593177318573 and batch: 750, loss is 3.613364214897156 and perplexity is 37.090623984993655
At time: 83.06716179847717 and batch: 800, loss is 3.567267336845398 and perplexity is 35.41967075459122
At time: 83.54053926467896 and batch: 850, loss is 3.6317434978485106 and perplexity is 37.778626184564075
At time: 84.02760076522827 and batch: 900, loss is 3.5756333589553835 and perplexity is 35.71723548425395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329323546527183 and perplexity of 75.89293116452622
finished 9 epochs...
Completing Train Step...
At time: 85.28984332084656 and batch: 50, loss is 3.8311839485168457 and perplexity is 46.11710620449862
At time: 85.76319789886475 and batch: 100, loss is 3.6975157403945924 and perplexity is 40.34694746360314
At time: 86.23708081245422 and batch: 150, loss is 3.710816044807434 and perplexity is 40.88715867024135
At time: 86.71006512641907 and batch: 200, loss is 3.6101202869415285 and perplexity is 36.97049961569666
At time: 87.18368816375732 and batch: 250, loss is 3.740520486831665 and perplexity is 42.11990731781726
At time: 87.65722966194153 and batch: 300, loss is 3.733669972419739 and perplexity is 41.83235036680175
At time: 88.13024234771729 and batch: 350, loss is 3.703579807281494 and perplexity is 40.59235739077537
At time: 88.60331320762634 and batch: 400, loss is 3.639035873413086 and perplexity is 38.05512907112382
At time: 89.07640194892883 and batch: 450, loss is 3.6796805000305177 and perplexity is 39.63372907421736
At time: 89.54973077774048 and batch: 500, loss is 3.564003949165344 and perplexity is 35.30427103684205
At time: 90.02313113212585 and batch: 550, loss is 3.631353859901428 and perplexity is 37.76390906557413
At time: 90.49844741821289 and batch: 600, loss is 3.6364603424072266 and perplexity is 37.95724301466534
At time: 90.97837734222412 and batch: 650, loss is 3.500520176887512 and perplexity is 33.132682332453314
At time: 91.45969223976135 and batch: 700, loss is 3.497334132194519 and perplexity is 33.02728811024821
At time: 91.93414068222046 and batch: 750, loss is 3.5936727285385133 and perplexity is 36.3673985301991
At time: 92.40803122520447 and batch: 800, loss is 3.5518417453765867 and perplexity is 34.87749383423265
At time: 92.88145327568054 and batch: 850, loss is 3.622890772819519 and perplexity is 37.44565840564896
At time: 93.35439968109131 and batch: 900, loss is 3.5740761756896973 and perplexity is 35.66166048432118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326148882304152 and perplexity of 75.65237862997374
finished 10 epochs...
Completing Train Step...
At time: 94.6198103427887 and batch: 50, loss is 3.7950254249572755 and perplexity is 44.47936727716516
At time: 95.09289526939392 and batch: 100, loss is 3.6636656427383425 and perplexity is 39.0040560786447
At time: 95.56710815429688 and batch: 150, loss is 3.6757653617858885 and perplexity is 39.47886090870264
At time: 96.04584264755249 and batch: 200, loss is 3.57740204334259 and perplexity is 35.78046390008225
At time: 96.55155992507935 and batch: 250, loss is 3.7078256034851074 and perplexity is 40.7650706608988
At time: 97.0291075706482 and batch: 300, loss is 3.704142336845398 and perplexity is 40.61519821559353
At time: 97.5041253566742 and batch: 350, loss is 3.6738138341903688 and perplexity is 39.4018919501507
At time: 97.97835874557495 and batch: 400, loss is 3.610577635765076 and perplexity is 36.987411897313116
At time: 98.45230650901794 and batch: 450, loss is 3.6531007623672487 and perplexity is 38.59415199864202
At time: 98.92513179779053 and batch: 500, loss is 3.538703589439392 and perplexity is 34.42226486425116
At time: 99.39872360229492 and batch: 550, loss is 3.6074754762649537 and perplexity is 36.87284883443164
At time: 99.87247776985168 and batch: 600, loss is 3.614820775985718 and perplexity is 37.14468810893189
At time: 100.34573149681091 and batch: 650, loss is 3.4815067434310913 and perplexity is 32.5086674136498
At time: 100.8192446231842 and batch: 700, loss is 3.4800405359268187 and perplexity is 32.46103788745106
At time: 101.29308605194092 and batch: 750, loss is 3.578057818412781 and perplexity is 35.80393553152202
At time: 101.76913380622864 and batch: 800, loss is 3.537604875564575 and perplexity is 34.384465413435265
At time: 102.24279761314392 and batch: 850, loss is 3.6119582414627076 and perplexity is 37.03851219548386
At time: 102.71668481826782 and batch: 900, loss is 3.565706238746643 and perplexity is 35.36442031081963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326571947907748 and perplexity of 75.68439132045893
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 103.9731752872467 and batch: 50, loss is 3.7873381662368772 and perplexity is 44.13875374271565
At time: 104.46304774284363 and batch: 100, loss is 3.6544409656524657 and perplexity is 38.645910683773515
At time: 104.9392683506012 and batch: 150, loss is 3.6682493400573732 and perplexity is 39.18324923577924
At time: 105.4151132106781 and batch: 200, loss is 3.5638727140426636 and perplexity is 35.29963818050477
At time: 105.90226626396179 and batch: 250, loss is 3.693690791130066 and perplexity is 40.19291720333327
At time: 106.38051843643188 and batch: 300, loss is 3.687305603027344 and perplexity is 39.93709547070178
At time: 106.85753035545349 and batch: 350, loss is 3.651162929534912 and perplexity is 38.51943540128189
At time: 107.33283710479736 and batch: 400, loss is 3.5867457389831543 and perplexity is 36.11635244107523
At time: 107.80965399742126 and batch: 450, loss is 3.6202645444869996 and perplexity is 37.34744657640634
At time: 108.28676891326904 and batch: 500, loss is 3.5041612720489503 and perplexity is 33.25354147807977
At time: 108.77588748931885 and batch: 550, loss is 3.570497307777405 and perplexity is 35.534260222251405
At time: 109.25256180763245 and batch: 600, loss is 3.579867739677429 and perplexity is 35.86879651472895
At time: 109.72838687896729 and batch: 650, loss is 3.440835723876953 and perplexity is 31.21303274795703
At time: 110.20581650733948 and batch: 700, loss is 3.432443828582764 and perplexity is 30.9521922491813
At time: 110.68282389640808 and batch: 750, loss is 3.525944266319275 and perplexity is 33.985850163103684
At time: 111.1586365699768 and batch: 800, loss is 3.481475620269775 and perplexity is 32.50765565689433
At time: 111.63571548461914 and batch: 850, loss is 3.5554250144958495 and perplexity is 35.0026934588588
At time: 112.11190390586853 and batch: 900, loss is 3.507019753456116 and perplexity is 33.348732093603374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310479987157534 and perplexity of 74.47622799549498
finished 12 epochs...
Completing Train Step...
At time: 113.37395715713501 and batch: 50, loss is 3.763689751625061 and perplexity is 43.10718771888761
At time: 113.86444401741028 and batch: 100, loss is 3.630221376419067 and perplexity is 37.72116626965138
At time: 114.34217286109924 and batch: 150, loss is 3.645481505393982 and perplexity is 38.301210651776486
At time: 114.81987619400024 and batch: 200, loss is 3.5430606412887573 and perplexity is 34.57257166651865
At time: 115.29746866226196 and batch: 250, loss is 3.674119415283203 and perplexity is 39.41393426321045
At time: 115.77510046958923 and batch: 300, loss is 3.669887914657593 and perplexity is 39.24750654355008
At time: 116.25291585922241 and batch: 350, loss is 3.635831427574158 and perplexity is 37.93337864662514
At time: 116.73117685317993 and batch: 400, loss is 3.572776474952698 and perplexity is 35.61534110509306
At time: 117.20881152153015 and batch: 450, loss is 3.6081913757324218 and perplexity is 36.899255538421095
At time: 117.68692946434021 and batch: 500, loss is 3.493506760597229 and perplexity is 32.90112200210382
At time: 118.16469740867615 and batch: 550, loss is 3.560781512260437 and perplexity is 35.190688355751554
At time: 118.64295840263367 and batch: 600, loss is 3.5717257928848265 and perplexity is 35.57794035644144
At time: 119.12071585655212 and batch: 650, loss is 3.434623680114746 and perplexity is 31.019737024918985
At time: 119.60134267807007 and batch: 700, loss is 3.427870125770569 and perplexity is 30.81094936814743
At time: 120.10066819190979 and batch: 750, loss is 3.522554416656494 and perplexity is 33.87083828700787
At time: 120.59799098968506 and batch: 800, loss is 3.4803504848480227 and perplexity is 32.471100710525455
At time: 121.08304929733276 and batch: 850, loss is 3.556181745529175 and perplexity is 35.02919110778125
At time: 121.5709753036499 and batch: 900, loss is 3.509595470428467 and perplexity is 33.43473970694782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309315720649614 and perplexity of 74.38956827489783
finished 13 epochs...
Completing Train Step...
At time: 122.86466240882874 and batch: 50, loss is 3.752602024078369 and perplexity is 42.63186694931193
At time: 123.34280157089233 and batch: 100, loss is 3.619378170967102 and perplexity is 37.31435745555188
At time: 123.82991647720337 and batch: 150, loss is 3.634143009185791 and perplexity is 37.86938527158333
At time: 124.31812500953674 and batch: 200, loss is 3.5324768352508547 and perplexity is 34.208591817483715
At time: 124.79625964164734 and batch: 250, loss is 3.6633849573135375 and perplexity is 38.99310974490518
At time: 125.27397656440735 and batch: 300, loss is 3.659874505996704 and perplexity is 38.85646631183797
At time: 125.75237607955933 and batch: 350, loss is 3.6266135215759276 and perplexity is 37.58531898323763
At time: 126.23029255867004 and batch: 400, loss is 3.564264831542969 and perplexity is 35.31348250051248
At time: 126.70884370803833 and batch: 450, loss is 3.60021436214447 and perplexity is 36.60608056062234
At time: 127.18649125099182 and batch: 500, loss is 3.4864287757873536 and perplexity is 32.669070557418124
At time: 127.66345739364624 and batch: 550, loss is 3.5541842651367186 and perplexity is 34.95929082084756
At time: 128.14113855361938 and batch: 600, loss is 3.5660441446304323 and perplexity is 35.37637217570847
At time: 128.6182975769043 and batch: 650, loss is 3.429912929534912 and perplexity is 30.873954423071094
At time: 129.09639859199524 and batch: 700, loss is 3.4238888120651243 and perplexity is 30.688525179411176
At time: 129.57504391670227 and batch: 750, loss is 3.519109454154968 and perplexity is 33.75435527421885
At time: 130.05304384231567 and batch: 800, loss is 3.478145341873169 and perplexity is 32.39957618079826
At time: 130.53141593933105 and batch: 850, loss is 3.554874219894409 and perplexity is 34.98341947275681
At time: 131.00932335853577 and batch: 900, loss is 3.5091270065307616 and perplexity is 33.419080406660754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309047751230736 and perplexity of 74.36963681614655
finished 14 epochs...
Completing Train Step...
At time: 132.2761619091034 and batch: 50, loss is 3.7434186220169066 and perplexity is 42.242153560718975
At time: 132.75178170204163 and batch: 100, loss is 3.6105841827392577 and perplexity is 36.987654053736556
At time: 133.24062180519104 and batch: 150, loss is 3.625068769454956 and perplexity is 37.527303803093766
At time: 133.71646976470947 and batch: 200, loss is 3.5240250873565673 and perplexity is 33.92068778357565
At time: 134.19350028038025 and batch: 250, loss is 3.6548211765289307 and perplexity is 38.66060707303286
At time: 134.67014837265015 and batch: 300, loss is 3.6519761753082274 and perplexity is 38.550773910540386
At time: 135.14662551879883 and batch: 350, loss is 3.6191353034973144 and perplexity is 37.30529611236718
At time: 135.623126745224 and batch: 400, loss is 3.557325382232666 and perplexity is 35.06927469258632
At time: 136.09915208816528 and batch: 450, loss is 3.5934810972213747 and perplexity is 36.36043006542722
At time: 136.57512664794922 and batch: 500, loss is 3.480320439338684 and perplexity is 32.47012511442203
At time: 137.0511338710785 and batch: 550, loss is 3.5484287452697756 and perplexity is 34.75865984960603
At time: 137.5433225631714 and batch: 600, loss is 3.5609338331222533 and perplexity is 35.196049039991465
At time: 138.0233597755432 and batch: 650, loss is 3.425514440536499 and perplexity is 30.738453891459834
At time: 138.49995398521423 and batch: 700, loss is 3.4199213552474976 and perplexity is 30.567010991505132
At time: 138.9767551422119 and batch: 750, loss is 3.5154855060577392 and perplexity is 33.63225262311587
At time: 139.45334243774414 and batch: 800, loss is 3.4753993225097655 and perplexity is 32.310728361985795
At time: 139.93075442314148 and batch: 850, loss is 3.5527599620819093 and perplexity is 34.90953363920199
At time: 140.40527844429016 and batch: 900, loss is 3.507561831474304 and perplexity is 33.366814608829976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309144738602312 and perplexity of 74.37685008153856
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 141.65911746025085 and batch: 50, loss is 3.7433352947235106 and perplexity is 42.23863378304435
At time: 142.14849734306335 and batch: 100, loss is 3.614046425819397 and perplexity is 37.115936246958036
At time: 142.62348914146423 and batch: 150, loss is 3.630025963783264 and perplexity is 37.71379579729023
At time: 143.0991449356079 and batch: 200, loss is 3.52390784740448 and perplexity is 33.91671115687945
At time: 143.5748794078827 and batch: 250, loss is 3.658473038673401 and perplexity is 38.80204838538266
At time: 144.04990887641907 and batch: 300, loss is 3.651744318008423 and perplexity is 38.54183666831858
At time: 144.524564743042 and batch: 350, loss is 3.616098790168762 and perplexity is 37.19218989467233
At time: 145.00099229812622 and batch: 400, loss is 3.554612202644348 and perplexity is 34.97425441414298
At time: 145.49003791809082 and batch: 450, loss is 3.5889872455596925 and perplexity is 36.19739828105056
At time: 145.96613883972168 and batch: 500, loss is 3.473644700050354 and perplexity is 32.254084940763775
At time: 146.4428267478943 and batch: 550, loss is 3.5389167785644533 and perplexity is 34.429604099074936
At time: 146.91827607154846 and batch: 600, loss is 3.5518130350112913 and perplexity is 34.87649250301845
At time: 147.39426827430725 and batch: 650, loss is 3.417649211883545 and perplexity is 30.497637203766594
At time: 147.87084913253784 and batch: 700, loss is 3.4066195344924926 and perplexity is 30.163106379160904
At time: 148.3479163646698 and batch: 750, loss is 3.4995938873291017 and perplexity is 33.102006084504104
At time: 148.82322788238525 and batch: 800, loss is 3.458020396232605 and perplexity is 31.754053809652703
At time: 149.29846000671387 and batch: 850, loss is 3.5329283809661867 and perplexity is 34.22404204852674
At time: 149.77403235435486 and batch: 900, loss is 3.490257730484009 and perplexity is 32.79439873339979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304822111782962 and perplexity of 74.05604058403598
finished 16 epochs...
Completing Train Step...
At time: 151.02794814109802 and batch: 50, loss is 3.735841164588928 and perplexity is 41.92327511016347
At time: 151.51703810691833 and batch: 100, loss is 3.6045386743545533 and perplexity is 36.76471943656014
At time: 151.9918749332428 and batch: 150, loss is 3.6206600570678713 and perplexity is 37.362220882910194
At time: 152.46760058403015 and batch: 200, loss is 3.516709008216858 and perplexity is 33.6734269401128
At time: 152.94229197502136 and batch: 250, loss is 3.6506519222259524 and perplexity is 38.499756716658894
At time: 153.41865587234497 and batch: 300, loss is 3.6456537866592407 and perplexity is 38.3078098012471
At time: 153.8954722881317 and batch: 350, loss is 3.6103818941116335 and perplexity is 36.980172628688095
At time: 154.37206625938416 and batch: 400, loss is 3.54963529586792 and perplexity is 34.80062324182552
At time: 154.847900390625 and batch: 450, loss is 3.584775071144104 and perplexity is 36.04524919031154
At time: 155.32360911369324 and batch: 500, loss is 3.4697991037368774 and perplexity is 32.130286941752
At time: 155.79923820495605 and batch: 550, loss is 3.5359659576416016 and perplexity is 34.32815825081
At time: 156.28757643699646 and batch: 600, loss is 3.549406876564026 and perplexity is 34.79267501548828
At time: 156.77231407165527 and batch: 650, loss is 3.416060881614685 and perplexity is 30.449235332721234
At time: 157.26146697998047 and batch: 700, loss is 3.405978102684021 and perplexity is 30.143765007035793
At time: 157.73847150802612 and batch: 750, loss is 3.49930344581604 and perplexity is 33.0923932838173
At time: 158.21474361419678 and batch: 800, loss is 3.4587565231323243 and perplexity is 31.777437428436595
At time: 158.69056487083435 and batch: 850, loss is 3.5344950056076048 and perplexity is 34.27770029632099
At time: 159.1667022705078 and batch: 900, loss is 3.492628288269043 and perplexity is 32.872231968264245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304399882277397 and perplexity of 74.02477853897943
finished 17 epochs...
Completing Train Step...
At time: 160.43467140197754 and batch: 50, loss is 3.7328141117095948 and perplexity is 41.79656301838705
At time: 160.91057419776917 and batch: 100, loss is 3.6011905384063723 and perplexity is 36.64183199451842
At time: 161.38649153709412 and batch: 150, loss is 3.6171033430099486 and perplexity is 37.229570186796856
At time: 161.8623445034027 and batch: 200, loss is 3.513286142349243 and perplexity is 33.558364350638456
At time: 162.33847379684448 and batch: 250, loss is 3.647181887626648 and perplexity is 38.366392751395324
At time: 162.81436014175415 and batch: 300, loss is 3.642427306175232 and perplexity is 38.184409581789645
At time: 163.2907693386078 and batch: 350, loss is 3.607508130073547 and perplexity is 36.874052893038225
At time: 163.76668047904968 and batch: 400, loss is 3.547141137123108 and perplexity is 34.71393311744828
At time: 164.24245834350586 and batch: 450, loss is 3.5825263023376466 and perplexity is 35.9642828297447
At time: 164.7187876701355 and batch: 500, loss is 3.4678636741638185 and perplexity is 32.0681611736315
At time: 165.19332242012024 and batch: 550, loss is 3.5342662143707275 and perplexity is 34.26985875594495
At time: 165.6710982322693 and batch: 600, loss is 3.547946500778198 and perplexity is 34.74190171844194
At time: 166.16174602508545 and batch: 650, loss is 3.4150551891326906 and perplexity is 30.418628158946632
At time: 166.63768911361694 and batch: 700, loss is 3.4052943897247316 and perplexity is 30.123162368206874
At time: 167.11348056793213 and batch: 750, loss is 3.498753514289856 and perplexity is 33.07419973653477
At time: 167.5932731628418 and batch: 800, loss is 3.458673429489136 and perplexity is 31.774797035090945
At time: 168.0690770149231 and batch: 850, loss is 3.534750542640686 and perplexity is 34.28646063740367
At time: 168.54552102088928 and batch: 900, loss is 3.4932201623916628 and perplexity is 32.89169395067274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304302894905822 and perplexity of 74.01759941842516
finished 18 epochs...
Completing Train Step...
At time: 169.81982326507568 and batch: 50, loss is 3.7301464653015137 and perplexity is 41.685213154247066
At time: 170.29584002494812 and batch: 100, loss is 3.598468441963196 and perplexity is 36.5422250253686
At time: 170.91312718391418 and batch: 150, loss is 3.6143005084991455 and perplexity is 37.12536796166796
At time: 171.3899838924408 and batch: 200, loss is 3.510623083114624 and perplexity is 33.46911532901669
At time: 171.86717748641968 and batch: 250, loss is 3.6445817804336547 and perplexity is 38.26676559440489
At time: 172.3434648513794 and batch: 300, loss is 3.6399580526351927 and perplexity is 38.09023890674009
At time: 172.81973958015442 and batch: 350, loss is 3.605215892791748 and perplexity is 36.78962561491058
At time: 173.29516792297363 and batch: 400, loss is 3.54514271736145 and perplexity is 34.64462937954594
At time: 173.7713804244995 and batch: 450, loss is 3.580647964477539 and perplexity is 35.89679315973797
At time: 174.24768352508545 and batch: 500, loss is 3.4662379741668703 and perplexity is 32.016070317634195
At time: 174.72361779212952 and batch: 550, loss is 3.5327874279022216 and perplexity is 34.21921840490039
At time: 175.20016241073608 and batch: 600, loss is 3.5466491985321045 and perplexity is 34.69686019386159
At time: 175.6762776374817 and batch: 650, loss is 3.41406879901886 and perplexity is 30.388638318127192
At time: 176.15274381637573 and batch: 700, loss is 3.404497594833374 and perplexity is 30.0991699461094
At time: 176.62942242622375 and batch: 750, loss is 3.4980293226242067 and perplexity is 33.05025634758536
At time: 177.10556554794312 and batch: 800, loss is 3.458273296356201 and perplexity is 31.762085429341845
At time: 177.58107614517212 and batch: 850, loss is 3.534555587768555 and perplexity is 34.279776976381584
At time: 178.0576720237732 and batch: 900, loss is 3.4932666540145876 and perplexity is 32.89322317445303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3043016407587755 and perplexity of 74.01750658952967
finished 19 epochs...
Completing Train Step...
At time: 179.33377718925476 and batch: 50, loss is 3.727691149711609 and perplexity is 41.58298834895581
At time: 179.81147122383118 and batch: 100, loss is 3.5960231399536133 and perplexity is 36.45297741223468
At time: 180.28919887542725 and batch: 150, loss is 3.611828565597534 and perplexity is 37.03370950577338
At time: 180.7672996520996 and batch: 200, loss is 3.5082866859436037 and perplexity is 33.39100946133961
At time: 181.24511241912842 and batch: 250, loss is 3.6423227691650393 and perplexity is 38.180418106408084
At time: 181.7354929447174 and batch: 300, loss is 3.6378095626831053 and perplexity is 38.0084902606947
At time: 182.21335649490356 and batch: 350, loss is 3.6031683254241944 and perplexity is 36.71437344627082
At time: 182.6898353099823 and batch: 400, loss is 3.5433463621139527 and perplexity is 34.58245118154871
At time: 183.16871857643127 and batch: 450, loss is 3.5789233350753786 and perplexity is 35.83493784888689
At time: 183.64617228507996 and batch: 500, loss is 3.4647217655181883 and perplexity is 31.967564057020258
At time: 184.1239049434662 and batch: 550, loss is 3.531379709243774 and perplexity is 34.17108126240049
At time: 184.6006510257721 and batch: 600, loss is 3.545399713516235 and perplexity is 34.653534060265784
At time: 185.07885265350342 and batch: 650, loss is 3.413064651489258 and perplexity is 30.35813895752362
At time: 185.55714654922485 and batch: 700, loss is 3.4036281156539916 and perplexity is 30.0730107186244
At time: 186.03475189208984 and batch: 750, loss is 3.4972124671936036 and perplexity is 33.02327008964124
At time: 186.51268100738525 and batch: 800, loss is 3.4577226734161375 and perplexity is 31.744601310500393
At time: 186.9907124042511 and batch: 850, loss is 3.534157600402832 and perplexity is 34.266136772739586
At time: 187.46873354911804 and batch: 900, loss is 3.4930675792694093 and perplexity is 32.88667561617985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304343445660317 and perplexity of 74.02060094878422
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa3acf5db70>
ELAPSED
588.3164134025574


RESULTS SO FAR:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}, {'best_accuracy': -74.17703442645892, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}}, {'best_accuracy': -74.01750658952967, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34768402145082, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8801144632298448, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.12854055690722077, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.06117525070097696, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7101216316223145 and batch: 50, loss is 6.641670722961425 and perplexity is 766.3743231981155
At time: 1.1912078857421875 and batch: 100, loss is 5.858016567230225 and perplexity is 350.0291956789968
At time: 1.6743888854980469 and batch: 150, loss is 5.563209571838379 and perplexity is 260.65809607744836
At time: 2.1594109535217285 and batch: 200, loss is 5.306506118774414 and perplexity is 201.64447435251694
At time: 2.6449661254882812 and batch: 250, loss is 5.296157875061035 and perplexity is 199.5685677074431
At time: 3.1309361457824707 and batch: 300, loss is 5.183254432678223 and perplexity is 178.26200971022834
At time: 3.616995096206665 and batch: 350, loss is 5.136413373947144 and perplexity is 170.10457142158737
At time: 4.102816581726074 and batch: 400, loss is 4.966803503036499 and perplexity is 143.56724088345004
At time: 4.587972402572632 and batch: 450, loss is 4.960339908599853 and perplexity is 142.64227299292438
At time: 5.074856758117676 and batch: 500, loss is 4.8626862812042235 and perplexity is 129.37126333971923
At time: 5.573446273803711 and batch: 550, loss is 4.921989707946778 and perplexity is 137.27547974701557
At time: 6.059448480606079 and batch: 600, loss is 4.842309551239014 and perplexity is 126.76177673338108
At time: 6.545365810394287 and batch: 650, loss is 4.711554794311524 and perplexity is 111.22495746972793
At time: 7.031332731246948 and batch: 700, loss is 4.770465497970581 and perplexity is 117.97414590634203
At time: 7.51713490486145 and batch: 750, loss is 4.78479232788086 and perplexity is 119.67650702916738
At time: 8.00106143951416 and batch: 800, loss is 4.726751079559326 and perplexity is 112.92807137802453
At time: 8.485712051391602 and batch: 850, loss is 4.769485282897949 and perplexity is 117.8585625278832
At time: 8.971415996551514 and batch: 900, loss is 4.700451641082764 and perplexity is 109.99684032731881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.750002508294092 and perplexity of 115.58457444692932
finished 1 epochs...
Completing Train Step...
At time: 10.254551887512207 and batch: 50, loss is 4.729899177551269 and perplexity is 113.28414018844832
At time: 10.744332790374756 and batch: 100, loss is 4.604971561431885 and perplexity is 99.98013951683447
At time: 11.22171664237976 and batch: 150, loss is 4.583617916107178 and perplexity is 97.86783207329037
At time: 11.69945478439331 and batch: 200, loss is 4.4777250003814695 and perplexity is 88.03416699063015
At time: 12.176536321640015 and batch: 250, loss is 4.591199855804444 and perplexity is 98.61268020299399
At time: 12.653291463851929 and batch: 300, loss is 4.556481351852417 and perplexity is 95.24774620645897
At time: 13.130202054977417 and batch: 350, loss is 4.541593065261841 and perplexity is 93.8401746285782
At time: 13.60810112953186 and batch: 400, loss is 4.437377233505249 and perplexity is 84.55288812180993
At time: 14.084365606307983 and batch: 450, loss is 4.46618392944336 and perplexity is 87.02399884804197
At time: 14.560788869857788 and batch: 500, loss is 4.353688688278198 and perplexity is 77.76478457427031
At time: 15.03781771659851 and batch: 550, loss is 4.442644624710083 and perplexity is 84.99943630054935
At time: 15.514208555221558 and batch: 600, loss is 4.414477338790894 and perplexity is 82.63863760649701
At time: 15.99054765701294 and batch: 650, loss is 4.271806259155273 and perplexity is 71.65093897088279
At time: 16.467216730117798 and batch: 700, loss is 4.309646682739258 and perplexity is 74.41419247647559
At time: 16.94326639175415 and batch: 750, loss is 4.379922170639038 and perplexity is 79.83181990376366
At time: 17.43222451210022 and batch: 800, loss is 4.335829830169677 and perplexity is 76.38832192999895
At time: 17.908140182495117 and batch: 850, loss is 4.402528162002564 and perplexity is 81.65705017651506
At time: 18.38417649269104 and batch: 900, loss is 4.341796674728394 and perplexity is 76.84548171738825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.551657898785317 and perplexity of 94.78942939628499
finished 2 epochs...
Completing Train Step...
At time: 19.658432006835938 and batch: 50, loss is 4.429029655456543 and perplexity is 83.8500140186584
At time: 20.13312530517578 and batch: 100, loss is 4.304088129997253 and perplexity is 74.001704742324
At time: 20.608131170272827 and batch: 150, loss is 4.296653141975403 and perplexity is 73.45354326612446
At time: 21.085257053375244 and batch: 200, loss is 4.202506766319275 and perplexity is 66.85370778908413
At time: 21.56205153465271 and batch: 250, loss is 4.335710706710816 and perplexity is 76.37922283084286
At time: 22.039928436279297 and batch: 300, loss is 4.316598334312439 and perplexity is 74.93329623842106
At time: 22.516293048858643 and batch: 350, loss is 4.30146894454956 and perplexity is 73.80813416344006
At time: 22.992456674575806 and batch: 400, loss is 4.21784842967987 and perplexity is 67.8872628231391
At time: 23.469322204589844 and batch: 450, loss is 4.253670158386231 and perplexity is 70.363183054045
At time: 23.94541883468628 and batch: 500, loss is 4.133008303642273 and perplexity is 62.365254640843695
At time: 24.42142081260681 and batch: 550, loss is 4.223584389686584 and perplexity is 68.27778037347457
At time: 24.897560834884644 and batch: 600, loss is 4.215122361183166 and perplexity is 67.70244951594924
At time: 25.372912168502808 and batch: 650, loss is 4.074400181770325 and perplexity is 58.8151915818181
At time: 25.84867835044861 and batch: 700, loss is 4.089786982536316 and perplexity is 59.72716741922917
At time: 26.32502770423889 and batch: 750, loss is 4.185597910881042 and perplexity is 65.73279152071886
At time: 26.800204753875732 and batch: 800, loss is 4.147865161895752 and perplexity is 63.2987234256704
At time: 27.276123762130737 and batch: 850, loss is 4.214351477622986 and perplexity is 67.6502789219437
At time: 27.7524471282959 and batch: 900, loss is 4.1644738674163815 and perplexity is 64.35881226596561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.479065777504281 and perplexity of 88.1522803519091
finished 3 epochs...
Completing Train Step...
At time: 29.026825428009033 and batch: 50, loss is 4.256545720100402 and perplexity is 70.56580791985273
At time: 29.504194974899292 and batch: 100, loss is 4.13438280582428 and perplexity is 62.451034758403246
At time: 29.99325704574585 and batch: 150, loss is 4.133033175468444 and perplexity is 62.36680579790621
At time: 30.4699285030365 and batch: 200, loss is 4.043472967147827 and perplexity is 57.02404192973314
At time: 30.945717334747314 and batch: 250, loss is 4.186291012763977 and perplexity is 65.77836683463589
At time: 31.421483516693115 and batch: 300, loss is 4.170911073684692 and perplexity is 64.77443951967035
At time: 31.909337759017944 and batch: 350, loss is 4.155061273574829 and perplexity is 63.755870978262415
At time: 32.40091872215271 and batch: 400, loss is 4.0819464111328125 and perplexity is 59.26070336036701
At time: 32.88276028633118 and batch: 450, loss is 4.119222269058228 and perplexity is 61.51138434967028
At time: 33.36696910858154 and batch: 500, loss is 3.9942890548706056 and perplexity is 54.28723165807541
At time: 33.84623575210571 and batch: 550, loss is 4.084526672363281 and perplexity is 59.41380889693331
At time: 34.322508573532104 and batch: 600, loss is 4.085401816368103 and perplexity is 59.465827294067324
At time: 34.798810720443726 and batch: 650, loss is 3.947060270309448 and perplexity is 51.782915088211126
At time: 35.27491283416748 and batch: 700, loss is 3.952801365852356 and perplexity is 52.081060773745214
At time: 35.750142335891724 and batch: 750, loss is 4.055363345146179 and perplexity is 57.70612642839431
At time: 36.22620439529419 and batch: 800, loss is 4.02657030582428 and perplexity is 56.06828404252333
At time: 36.702919721603394 and batch: 850, loss is 4.08796109676361 and perplexity is 59.618211934399405
At time: 37.178831338882446 and batch: 900, loss is 4.04506432056427 and perplexity is 57.11485957599765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4463785249892975 and perplexity of 85.31740898920903
finished 4 epochs...
Completing Train Step...
At time: 38.440871238708496 and batch: 50, loss is 4.140107045173645 and perplexity is 62.8095445479603
At time: 38.93005347251892 and batch: 100, loss is 4.017613744735717 and perplexity is 55.568347230735746
At time: 39.406389474868774 and batch: 150, loss is 4.02093505859375 and perplexity is 55.753213982773204
At time: 39.883177757263184 and batch: 200, loss is 3.9340857791900636 and perplexity is 51.11539782862637
At time: 40.359004497528076 and batch: 250, loss is 4.075016822814941 and perplexity is 58.85147062741914
At time: 40.83586239814758 and batch: 300, loss is 4.065237603187561 and perplexity is 58.27875409535087
At time: 41.312018632888794 and batch: 350, loss is 4.04467707157135 and perplexity is 57.09274618611638
At time: 41.78878355026245 and batch: 400, loss is 3.981061520576477 and perplexity is 53.573873823378605
At time: 42.278104066848755 and batch: 450, loss is 4.018525085449219 and perplexity is 55.61901201088093
At time: 42.754080295562744 and batch: 500, loss is 3.8963929510116575 and perplexity is 49.22457304814799
At time: 43.2298367023468 and batch: 550, loss is 3.9816258764266967 and perplexity is 53.604117085668065
At time: 43.706337690353394 and batch: 600, loss is 3.9875912952423094 and perplexity is 53.92484377772658
At time: 44.18216681480408 and batch: 650, loss is 3.8531350040435792 and perplexity is 47.14061784613681
At time: 44.6579430103302 and batch: 700, loss is 3.8534604263305665 and perplexity is 47.155960950164896
At time: 45.13356637954712 and batch: 750, loss is 3.960954813957214 and perplexity is 52.50743685556278
At time: 45.60890316963196 and batch: 800, loss is 3.9338475465774536 and perplexity is 51.103221924263394
At time: 46.08517837524414 and batch: 850, loss is 3.997076268196106 and perplexity is 54.43875281636134
At time: 46.561665296554565 and batch: 900, loss is 3.9520497274398805 and perplexity is 52.04192935608583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430720081068065 and perplexity of 83.99187609974824
finished 5 epochs...
Completing Train Step...
At time: 47.82625651359558 and batch: 50, loss is 4.052433800697327 and perplexity is 57.53732114806435
At time: 48.316208362579346 and batch: 100, loss is 3.927706546783447 and perplexity is 50.79035867873769
At time: 48.79260492324829 and batch: 150, loss is 3.935968427658081 and perplexity is 51.21172079673438
At time: 49.26916193962097 and batch: 200, loss is 3.8528236055374148 and perplexity is 47.12594061351261
At time: 49.74466896057129 and batch: 250, loss is 3.9912439441680907 and perplexity is 54.12217246724311
At time: 50.22081756591797 and batch: 300, loss is 3.9852330780029295 and perplexity is 53.797827106814225
At time: 50.697052240371704 and batch: 350, loss is 3.9583168745040895 and perplexity is 52.36910794815628
At time: 51.173779010772705 and batch: 400, loss is 3.9026986932754517 and perplexity is 49.5359512223305
At time: 51.65015363693237 and batch: 450, loss is 3.9407259035110473 and perplexity is 51.45593979417635
At time: 52.12679433822632 and batch: 500, loss is 3.8208124446868896 and perplexity is 45.64127427257776
At time: 52.60207176208496 and batch: 550, loss is 3.903287854194641 and perplexity is 49.56514446780079
At time: 53.07756495475769 and batch: 600, loss is 3.911185235977173 and perplexity is 49.95812906899374
At time: 53.55327391624451 and batch: 650, loss is 3.7782084846496584 and perplexity is 43.73761489663249
At time: 54.04266667366028 and batch: 700, loss is 3.776128878593445 and perplexity is 43.64675239965517
At time: 54.51860022544861 and batch: 750, loss is 3.885053210258484 and perplexity is 48.669532108975666
At time: 54.99539852142334 and batch: 800, loss is 3.8619036293029785 and perplexity is 47.55579385977126
At time: 55.47171998023987 and batch: 850, loss is 3.923525719642639 and perplexity is 50.57845624107259
At time: 55.9479615688324 and batch: 900, loss is 3.878288426399231 and perplexity is 48.34140435172794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43150392297196 and perplexity of 84.05773826121083
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 57.230565786361694 and batch: 50, loss is 3.991656837463379 and perplexity is 54.14452376341389
At time: 57.70778036117554 and batch: 100, loss is 3.851449518203735 and perplexity is 47.061229924678564
At time: 58.1859393119812 and batch: 150, loss is 3.861786003112793 and perplexity is 47.55020038189449
At time: 58.66481566429138 and batch: 200, loss is 3.755879817008972 and perplexity is 42.771834648667664
At time: 59.14345359802246 and batch: 250, loss is 3.888551940917969 and perplexity is 48.84011192555796
At time: 59.621710538864136 and batch: 300, loss is 3.8672068166732787 and perplexity is 47.808661053575435
At time: 60.099982023239136 and batch: 350, loss is 3.8358428144454955 and perplexity is 46.33246088408913
At time: 60.57842206954956 and batch: 400, loss is 3.7740574073791504 and perplexity is 43.55643298778939
At time: 61.05660939216614 and batch: 450, loss is 3.799097652435303 and perplexity is 44.66086668140957
At time: 61.5351288318634 and batch: 500, loss is 3.674127917289734 and perplexity is 39.41426936216146
At time: 62.012696266174316 and batch: 550, loss is 3.74193199634552 and perplexity is 42.17940194645834
At time: 62.49109435081482 and batch: 600, loss is 3.7412249755859377 and perplexity is 42.14959077345635
At time: 62.96815896034241 and batch: 650, loss is 3.6019572687149046 and perplexity is 36.669937170837166
At time: 63.44632172584534 and batch: 700, loss is 3.583241047859192 and perplexity is 35.989997328399504
At time: 63.924036741256714 and batch: 750, loss is 3.6831056833267213 and perplexity is 39.76971461576419
At time: 64.40227937698364 and batch: 800, loss is 3.6429199171066284 and perplexity is 38.2032242731394
At time: 64.88065361976624 and batch: 850, loss is 3.6938515806198122 and perplexity is 40.199380321568626
At time: 65.35901427268982 and batch: 900, loss is 3.638879656791687 and perplexity is 38.04918469175095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332858987050514 and perplexity of 76.16172097401748
finished 7 epochs...
Completing Train Step...
At time: 66.63361692428589 and batch: 50, loss is 3.8989802408218384 and perplexity is 49.352096182922175
At time: 67.11141657829285 and batch: 100, loss is 3.766752281188965 and perplexity is 43.23940711525718
At time: 67.59035968780518 and batch: 150, loss is 3.781468415260315 and perplexity is 43.880429142060635
At time: 68.0681746006012 and batch: 200, loss is 3.6818143272399904 and perplexity is 39.71839089845722
At time: 68.54648017883301 and batch: 250, loss is 3.8179111766815184 and perplexity is 45.509048607601194
At time: 69.02454328536987 and batch: 300, loss is 3.802341079711914 and perplexity is 44.80595612093112
At time: 69.50320601463318 and batch: 350, loss is 3.776301174163818 and perplexity is 43.6542731896356
At time: 69.97871017456055 and batch: 400, loss is 3.7179595470428466 and perplexity is 41.18028189631596
At time: 70.45691657066345 and batch: 450, loss is 3.7481995153427126 and perplexity is 42.444592324217645
At time: 70.93469905853271 and batch: 500, loss is 3.6256947183609007 and perplexity is 37.55080133121116
At time: 71.41121125221252 and batch: 550, loss is 3.6969669675827026 and perplexity is 40.324812229955086
At time: 71.88945531845093 and batch: 600, loss is 3.7023979091644286 and perplexity is 40.544409700231995
At time: 72.36779522895813 and batch: 650, loss is 3.5677815294265747 and perplexity is 35.437887969698004
At time: 72.84655857086182 and batch: 700, loss is 3.5533564567565916 and perplexity is 34.93036320185741
At time: 73.32480454444885 and batch: 750, loss is 3.6599312305450438 and perplexity is 38.8586704898545
At time: 73.80233478546143 and batch: 800, loss is 3.623019676208496 and perplexity is 37.4504855890334
At time: 74.27989935874939 and batch: 850, loss is 3.680780539512634 and perplexity is 39.67735172994537
At time: 74.75826048851013 and batch: 900, loss is 3.6334277820587157 and perplexity is 37.84230974368165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32998155567744 and perplexity of 75.94288584118705
finished 8 epochs...
Completing Train Step...
At time: 76.02202725410461 and batch: 50, loss is 3.8584967279434204 and perplexity is 47.394051637359084
At time: 76.51252055168152 and batch: 100, loss is 3.728125534057617 and perplexity is 41.60105527186499
At time: 76.98848056793213 and batch: 150, loss is 3.742897114753723 and perplexity is 42.22012971412168
At time: 77.46472787857056 and batch: 200, loss is 3.646089186668396 and perplexity is 38.32449265357877
At time: 77.94091987609863 and batch: 250, loss is 3.7813197135925294 and perplexity is 43.873904534186124
At time: 78.43017959594727 and batch: 300, loss is 3.7681507205963136 and perplexity is 43.299917106034094
At time: 78.90583038330078 and batch: 350, loss is 3.7436076307296755 and perplexity is 42.25013845037116
At time: 79.38244271278381 and batch: 400, loss is 3.6867839431762697 and perplexity is 39.91626732450234
At time: 79.85852932929993 and batch: 450, loss is 3.7186051177978516 and perplexity is 41.20687526501805
At time: 80.3349781036377 and batch: 500, loss is 3.5970094060897826 and perplexity is 36.48894748453529
At time: 80.81163477897644 and batch: 550, loss is 3.6695395946502685 and perplexity is 39.23383823239468
At time: 81.28784394264221 and batch: 600, loss is 3.6774155426025392 and perplexity is 39.54406194959513
At time: 81.76525902748108 and batch: 650, loss is 3.5450560665130615 and perplexity is 34.64162752307668
At time: 82.24124813079834 and batch: 700, loss is 3.532101984024048 and perplexity is 34.19577108795682
At time: 82.71473503112793 and batch: 750, loss is 3.6417549991607667 and perplexity is 38.15874656306498
At time: 83.18836259841919 and batch: 800, loss is 3.605412368774414 and perplexity is 36.796854602893184
At time: 83.66215944290161 and batch: 850, loss is 3.666353688240051 and perplexity is 39.10904179611279
At time: 84.13459968566895 and batch: 900, loss is 3.6217326021194456 and perplexity is 37.40231504558834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330327700262201 and perplexity of 75.96917760998625
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 85.37580966949463 and batch: 50, loss is 3.8451379919052124 and perplexity is 46.765137116055435
At time: 85.86141085624695 and batch: 100, loss is 3.7149472856521606 and perplexity is 41.056422764797226
At time: 86.33633160591125 and batch: 150, loss is 3.729894061088562 and perplexity is 41.67469295855585
At time: 86.81178879737854 and batch: 200, loss is 3.627539343833923 and perplexity is 37.62013242116734
At time: 87.28506755828857 and batch: 250, loss is 3.760370759963989 and perplexity is 42.964352487905835
At time: 87.75820422172546 and batch: 300, loss is 3.7420601511001585 and perplexity is 42.184807783750074
At time: 88.23296642303467 and batch: 350, loss is 3.7185294818878174 and perplexity is 41.203758663372696
At time: 88.70787811279297 and batch: 400, loss is 3.657102999687195 and perplexity is 38.74892446558015
At time: 89.18301272392273 and batch: 450, loss is 3.6848232889175416 and perplexity is 39.838081997223924
At time: 89.6586651802063 and batch: 500, loss is 3.561101698875427 and perplexity is 35.20195774719322
At time: 90.1335871219635 and batch: 550, loss is 3.6288315153121946 and perplexity is 37.66877550412467
At time: 90.62219715118408 and batch: 600, loss is 3.634820122718811 and perplexity is 37.89503582802729
At time: 91.09787440299988 and batch: 650, loss is 3.49820520401001 and perplexity is 33.056069783695534
At time: 91.57293796539307 and batch: 700, loss is 3.4785780048370363 and perplexity is 32.413597310449724
At time: 92.05021953582764 and batch: 750, loss is 3.5869862031936646 and perplexity is 36.125038175513765
At time: 92.54278922080994 and batch: 800, loss is 3.5433334827423097 and perplexity is 34.58200578417585
At time: 93.02560615539551 and batch: 850, loss is 3.605599341392517 and perplexity is 36.803735250362536
At time: 93.51060366630554 and batch: 900, loss is 3.5558129072189333 and perplexity is 35.016273382546345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314363244461687 and perplexity of 74.76600061853172
finished 10 epochs...
Completing Train Step...
At time: 94.80347514152527 and batch: 50, loss is 3.819622530937195 and perplexity is 45.586997391574926
At time: 95.2821147441864 and batch: 100, loss is 3.690039291381836 and perplexity is 40.04642040557036
At time: 95.76025438308716 and batch: 150, loss is 3.7061325550079345 and perplexity is 40.696111811903705
At time: 96.2382538318634 and batch: 200, loss is 3.606277027130127 and perplexity is 36.828685069946054
At time: 96.71747827529907 and batch: 250, loss is 3.740703139305115 and perplexity is 42.12760132571374
At time: 97.19547748565674 and batch: 300, loss is 3.7240364265441896 and perplexity is 41.43129141207249
At time: 97.67256164550781 and batch: 350, loss is 3.7010497760772707 and perplexity is 40.48978726744315
At time: 98.14931678771973 and batch: 400, loss is 3.6411865663528444 and perplexity is 38.13706204328946
At time: 98.62544560432434 and batch: 450, loss is 3.6712705421447756 and perplexity is 39.301808756115825
At time: 99.10200619697571 and batch: 500, loss is 3.54842661857605 and perplexity is 34.75858592866082
At time: 99.5792007446289 and batch: 550, loss is 3.617430987358093 and perplexity is 37.2417702435831
At time: 100.05442380905151 and batch: 600, loss is 3.6253990840911867 and perplexity is 37.53970166828364
At time: 100.52992677688599 and batch: 650, loss is 3.4905538034439085 and perplexity is 32.80410970560812
At time: 101.00721096992493 and batch: 700, loss is 3.473037362098694 and perplexity is 32.23450175829011
At time: 101.48417687416077 and batch: 750, loss is 3.583420352935791 and perplexity is 35.996451096206656
At time: 101.9608964920044 and batch: 800, loss is 3.541410222053528 and perplexity is 34.51555948916716
At time: 102.43678164482117 and batch: 850, loss is 3.605977931022644 and perplexity is 36.81767140075282
At time: 102.92666363716125 and batch: 900, loss is 3.5588305044174193 and perplexity is 35.12209797870712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313221970649614 and perplexity of 74.68072081305323
finished 11 epochs...
Completing Train Step...
At time: 104.20117378234863 and batch: 50, loss is 3.8071254825592042 and perplexity is 45.02083949939651
At time: 104.676260471344 and batch: 100, loss is 3.677834515571594 and perplexity is 39.56063331387308
At time: 105.15250992774963 and batch: 150, loss is 3.6936242151260377 and perplexity is 40.190241408588484
At time: 105.62804174423218 and batch: 200, loss is 3.5947959280014037 and perplexity is 36.40826932142351
At time: 106.10423731803894 and batch: 250, loss is 3.729241118431091 and perplexity is 41.64749065552466
At time: 106.58075094223022 and batch: 300, loss is 3.713216142654419 and perplexity is 40.98540971062603
At time: 107.05699443817139 and batch: 350, loss is 3.690636591911316 and perplexity is 40.070347298744004
At time: 107.53272533416748 and batch: 400, loss is 3.6314517307281493 and perplexity is 37.76760523144506
At time: 108.00953555107117 and batch: 450, loss is 3.662438325881958 and perplexity is 38.956215107172646
At time: 108.4855489730835 and batch: 500, loss is 3.5400503969192503 and perplexity is 34.46865626117178
At time: 108.9626293182373 and batch: 550, loss is 3.6097461700439455 and perplexity is 36.95667091401512
At time: 109.4382004737854 and batch: 600, loss is 3.6187264776229857 and perplexity is 37.29004785921955
At time: 109.91456007957458 and batch: 650, loss is 3.4847558736801147 and perplexity is 32.61446408881443
At time: 110.39031457901001 and batch: 700, loss is 3.4683109617233274 and perplexity is 32.082508071536076
At time: 110.8666524887085 and batch: 750, loss is 3.579676561355591 and perplexity is 35.86193983385016
At time: 111.33999466896057 and batch: 800, loss is 3.53842104434967 and perplexity is 34.41254039620171
At time: 111.82446479797363 and batch: 850, loss is 3.604177360534668 and perplexity is 36.75143823482885
At time: 112.30519223213196 and batch: 900, loss is 3.5582380056381226 and perplexity is 35.10129434220161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313128327670163 and perplexity of 74.67372781527638
finished 12 epochs...
Completing Train Step...
At time: 113.56674337387085 and batch: 50, loss is 3.7967768573760985 and perplexity is 44.55733814340314
At time: 114.05262923240662 and batch: 100, loss is 3.667836079597473 and perplexity is 39.1670596936589
At time: 114.52579092979431 and batch: 150, loss is 3.683577914237976 and perplexity is 39.78849953939925
At time: 115.01178288459778 and batch: 200, loss is 3.5855415201187135 and perplexity is 36.07288662458162
At time: 115.48590755462646 and batch: 250, loss is 3.7199862718582155 and perplexity is 41.26382762904618
At time: 115.95960354804993 and batch: 300, loss is 3.7044856691360475 and perplexity is 40.62914511870618
At time: 116.43467545509338 and batch: 350, loss is 3.682171244621277 and perplexity is 39.73256961268987
At time: 116.91009330749512 and batch: 400, loss is 3.6234712839126586 and perplexity is 37.467402336429245
At time: 117.3867814540863 and batch: 450, loss is 3.6550185918807983 and perplexity is 38.66824002378764
At time: 117.87855887413025 and batch: 500, loss is 3.532915940284729 and perplexity is 34.22361628076985
At time: 118.35769891738892 and batch: 550, loss is 3.6030431747436524 and perplexity is 36.70977890495933
At time: 118.8335599899292 and batch: 600, loss is 3.612763075828552 and perplexity is 37.06833406218034
At time: 119.30925416946411 and batch: 650, loss is 3.479369912147522 and perplexity is 32.439276041364415
At time: 119.78461909294128 and batch: 700, loss is 3.4636309480667116 and perplexity is 31.932712292178103
At time: 120.25995349884033 and batch: 750, loss is 3.5756568050384523 and perplexity is 35.718072923341396
At time: 120.73545908927917 and batch: 800, loss is 3.534889988899231 and perplexity is 34.29124208942836
At time: 121.21257829666138 and batch: 850, loss is 3.6015207958221436 and perplexity is 36.653935229743645
At time: 121.68901896476746 and batch: 900, loss is 3.5563344812393187 and perplexity is 35.0345417247656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313449807363014 and perplexity of 74.69773776150677
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 122.96243000030518 and batch: 50, loss is 3.7947378635406492 and perplexity is 44.466578566158354
At time: 123.45343923568726 and batch: 100, loss is 3.6692887592315673 and perplexity is 39.22399823031652
At time: 123.92968034744263 and batch: 150, loss is 3.6864634418487547 and perplexity is 39.903476157737856
At time: 124.40792417526245 and batch: 200, loss is 3.5861963129043577 and perplexity is 36.096514625378774
At time: 124.88476300239563 and batch: 250, loss is 3.7196855926513672 and perplexity is 41.251422319185984
At time: 125.3618483543396 and batch: 300, loss is 3.7003628158569337 and perplexity is 40.46198194592888
At time: 125.83885931968689 and batch: 350, loss is 3.6773562240600586 and perplexity is 39.54171632304678
At time: 126.31557965278625 and batch: 400, loss is 3.6199736547470094 and perplexity is 37.3365841673407
At time: 126.79269528388977 and batch: 450, loss is 3.6464012384414675 and perplexity is 38.336453745606306
At time: 127.28364539146423 and batch: 500, loss is 3.525263295173645 and perplexity is 33.962714657982644
At time: 127.7610125541687 and batch: 550, loss is 3.591234278678894 and perplexity is 36.27882648549114
At time: 128.2391233444214 and batch: 600, loss is 3.6038275957107544 and perplexity is 36.73858612224555
At time: 128.71811389923096 and batch: 650, loss is 3.464855728149414 and perplexity is 31.97184680287276
At time: 129.1961693763733 and batch: 700, loss is 3.4469173765182495 and perplexity is 31.403437973364337
At time: 129.67405557632446 and batch: 750, loss is 3.5589169788360597 and perplexity is 35.12513527303349
At time: 130.15161800384521 and batch: 800, loss is 3.5175635862350463 and perplexity is 33.70221580998852
At time: 130.6292552947998 and batch: 850, loss is 3.5816746282577516 and perplexity is 35.93366602187808
At time: 131.10677194595337 and batch: 900, loss is 3.537200493812561 and perplexity is 34.3705637740434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309613371548587 and perplexity of 74.41171369240725
finished 14 epochs...
Completing Train Step...
At time: 132.38176369667053 and batch: 50, loss is 3.786540446281433 and perplexity is 44.10355741831827
At time: 132.85942888259888 and batch: 100, loss is 3.659786229133606 and perplexity is 38.85303633627685
At time: 133.3372495174408 and batch: 150, loss is 3.6773670291900635 and perplexity is 39.54214357874054
At time: 133.81577920913696 and batch: 200, loss is 3.5783666706085206 and perplexity is 35.81499536346583
At time: 134.29385018348694 and batch: 250, loss is 3.712076005935669 and perplexity is 40.93870736866847
At time: 134.77215456962585 and batch: 300, loss is 3.69428572177887 and perplexity is 40.21683631604349
At time: 135.25116157531738 and batch: 350, loss is 3.672081069946289 and perplexity is 39.33367687801716
At time: 135.72980666160583 and batch: 400, loss is 3.6146396732330324 and perplexity is 37.13796171277053
At time: 136.20830631256104 and batch: 450, loss is 3.642414937019348 and perplexity is 38.18393727579621
At time: 136.6873004436493 and batch: 500, loss is 3.521532030105591 and perplexity is 33.83622689360185
At time: 137.16404032707214 and batch: 550, loss is 3.588101372718811 and perplexity is 36.16534618813823
At time: 137.6399998664856 and batch: 600, loss is 3.6018262910842895 and perplexity is 36.66513454387694
At time: 138.11817908287048 and batch: 650, loss is 3.463394770622253 and perplexity is 31.92517139632412
At time: 138.59753894805908 and batch: 700, loss is 3.4460207986831666 and perplexity is 31.375294965016
At time: 139.07583498954773 and batch: 750, loss is 3.5583849811553954 and perplexity is 35.10645375223869
At time: 139.56702613830566 and batch: 800, loss is 3.5178715705871584 and perplexity is 33.712597163654685
At time: 140.0455129146576 and batch: 850, loss is 3.5830947303771974 and perplexity is 35.98473174784607
At time: 140.5236132144928 and batch: 900, loss is 3.539491128921509 and perplexity is 34.449384434360084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309234201091609 and perplexity of 74.38350431734078
finished 15 epochs...
Completing Train Step...
At time: 141.7971818447113 and batch: 50, loss is 3.783047289848328 and perplexity is 43.949765558893674
At time: 142.27765941619873 and batch: 100, loss is 3.656110668182373 and perplexity is 38.71049175920146
At time: 142.75463271141052 and batch: 150, loss is 3.673560075759888 and perplexity is 39.39189465639388
At time: 143.2309091091156 and batch: 200, loss is 3.5747272539138795 and perplexity is 35.6848865750812
At time: 143.70722436904907 and batch: 250, loss is 3.7084730529785155 and perplexity is 40.79147253126345
At time: 144.18388676643372 and batch: 300, loss is 3.6911102151870727 and perplexity is 40.08933004287222
At time: 144.6601378917694 and batch: 350, loss is 3.6689965438842775 and perplexity is 39.21253805055331
At time: 145.13661122322083 and batch: 400, loss is 3.6117983055114746 and perplexity is 37.03258887949184
At time: 145.613511800766 and batch: 450, loss is 3.639949669837952 and perplexity is 38.0899196053288
At time: 146.09044194221497 and batch: 500, loss is 3.5193001365661623 and perplexity is 33.76079224976034
At time: 146.56651759147644 and batch: 550, loss is 3.5861113023757936 and perplexity is 36.09344617201841
At time: 147.04318833351135 and batch: 600, loss is 3.600312705039978 and perplexity is 36.60968068559832
At time: 147.51930904388428 and batch: 650, loss is 3.4621401596069337 and perplexity is 31.885142840008744
At time: 147.99558997154236 and batch: 700, loss is 3.44517436504364 and perplexity is 31.348749096150637
At time: 148.47188758850098 and batch: 750, loss is 3.5577303409576415 and perplexity is 35.08347917727253
At time: 148.94783401489258 and batch: 800, loss is 3.5175901508331298 and perplexity is 33.703111107697595
At time: 149.4245035648346 and batch: 850, loss is 3.5832344675064087 and perplexity is 35.989760502299625
At time: 149.898996591568 and batch: 900, loss is 3.54008095741272 and perplexity is 34.46970965641241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30914306640625 and perplexity of 74.37672570896676
finished 16 epochs...
Completing Train Step...
At time: 151.1627275943756 and batch: 50, loss is 3.7799807024002074 and perplexity is 43.8151961992945
At time: 151.6550271511078 and batch: 100, loss is 3.6530770683288574 and perplexity is 38.59323755815632
At time: 152.13073754310608 and batch: 150, loss is 3.6705146646499633 and perplexity is 39.272112628102995
At time: 152.60633635520935 and batch: 200, loss is 3.5718423461914064 and perplexity is 35.58208732469802
At time: 153.08256340026855 and batch: 250, loss is 3.705614585876465 and perplexity is 40.6750379404929
At time: 153.5589554309845 and batch: 300, loss is 3.6884819793701173 and perplexity is 39.98410416955208
At time: 154.0345320701599 and batch: 350, loss is 3.6664452362060547 and perplexity is 39.11262231323361
At time: 154.51092624664307 and batch: 400, loss is 3.6094887590408327 and perplexity is 36.94715908456073
At time: 154.98766016960144 and batch: 450, loss is 3.637844595909119 and perplexity is 38.00982184404913
At time: 155.46359586715698 and batch: 500, loss is 3.517353739738464 and perplexity is 33.69514426006968
At time: 155.94043612480164 and batch: 550, loss is 3.584350438117981 and perplexity is 36.02994643632781
At time: 156.41704416275024 and batch: 600, loss is 3.5988823795318603 and perplexity is 36.55735435623239
At time: 156.89403080940247 and batch: 650, loss is 3.460881719589233 and perplexity is 31.845042537493626
At time: 157.37120175361633 and batch: 700, loss is 3.44422128200531 and perplexity is 31.318885368680892
At time: 157.84687733650208 and batch: 750, loss is 3.5569130325317384 and perplexity is 35.05481686869962
At time: 158.32154488563538 and batch: 800, loss is 3.5170249700546266 and perplexity is 33.68406813899061
At time: 158.79674744606018 and batch: 850, loss is 3.582945032119751 and perplexity is 35.97934529938977
At time: 159.2728545665741 and batch: 900, loss is 3.540134491920471 and perplexity is 34.4715550247462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309174002033391 and perplexity of 74.37902663521137
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 160.53255677223206 and batch: 50, loss is 3.7791307973861694 and perplexity is 43.77797326457372
At time: 161.02037048339844 and batch: 100, loss is 3.6533910131454466 and perplexity is 38.605355607142904
At time: 161.49564743041992 and batch: 150, loss is 3.671888551712036 and perplexity is 39.3261051568685
At time: 161.9711422920227 and batch: 200, loss is 3.5729767894744873 and perplexity is 35.62247608971155
At time: 162.44655680656433 and batch: 250, loss is 3.7067217588424684 and perplexity is 40.72009718247615
At time: 162.92356872558594 and batch: 300, loss is 3.6884698915481566 and perplexity is 39.98362085174076
At time: 163.39945816993713 and batch: 350, loss is 3.664998912811279 and perplexity is 39.056093701731726
At time: 163.88865995407104 and batch: 400, loss is 3.60996985912323 and perplexity is 36.96493864237109
At time: 164.36410975456238 and batch: 450, loss is 3.6354028606414794 and perplexity is 37.91712513799936
At time: 164.83943486213684 and batch: 500, loss is 3.515639762878418 and perplexity is 33.637441027640875
At time: 165.3155345916748 and batch: 550, loss is 3.5807831478118897 and perplexity is 35.90164613594315
At time: 165.79097938537598 and batch: 600, loss is 3.5958543395996094 and perplexity is 36.44682465605132
At time: 166.26713132858276 and batch: 650, loss is 3.455583066940308 and perplexity is 31.676752966225873
At time: 166.74271535873413 and batch: 700, loss is 3.4389208221435545 and perplexity is 31.153320047666448
At time: 167.2203152179718 and batch: 750, loss is 3.5510672187805175 and perplexity is 34.850490746308594
At time: 167.69709587097168 and batch: 800, loss is 3.5103344917297363 and perplexity is 33.459457824276285
At time: 168.1731095314026 and batch: 850, loss is 3.574538702964783 and perplexity is 35.67815879013411
At time: 168.64938282966614 and batch: 900, loss is 3.532242980003357 and perplexity is 34.20059289410924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308897253585188 and perplexity of 74.35844520308233
finished 18 epochs...
Completing Train Step...
At time: 169.9232907295227 and batch: 50, loss is 3.777112522125244 and perplexity is 43.689706367570125
At time: 170.39985537528992 and batch: 100, loss is 3.650712776184082 and perplexity is 38.5020996505298
At time: 170.8772428035736 and batch: 150, loss is 3.6693504285812377 and perplexity is 39.22641722336696
At time: 171.35328030586243 and batch: 200, loss is 3.5703902530670164 and perplexity is 35.53045631593112
At time: 171.82928133010864 and batch: 250, loss is 3.7043411922454834 and perplexity is 40.62327557017042
At time: 172.305326461792 and batch: 300, loss is 3.686093554496765 and perplexity is 39.88871909600019
At time: 172.78143048286438 and batch: 350, loss is 3.66324547290802 and perplexity is 38.98767119347852
At time: 173.2588951587677 and batch: 400, loss is 3.6080773401260378 and perplexity is 36.895047949352694
At time: 173.73527884483337 and batch: 450, loss is 3.6341075229644777 and perplexity is 37.868041454040224
At time: 174.212229013443 and batch: 500, loss is 3.5143071174621583 and perplexity is 33.59264410187931
At time: 174.6866819858551 and batch: 550, loss is 3.579869246482849 and perplexity is 35.86885056206667
At time: 175.16350388526917 and batch: 600, loss is 3.5953785753250123 and perplexity is 36.429488683202734
At time: 175.6395902633667 and batch: 650, loss is 3.4553712368011475 and perplexity is 31.670043585886468
At time: 176.12869596481323 and batch: 700, loss is 3.438724007606506 and perplexity is 31.147189224741073
At time: 176.61571550369263 and batch: 750, loss is 3.5510746479034423 and perplexity is 34.85074965585007
At time: 177.10171055793762 and batch: 800, loss is 3.5106533479690554 and perplexity is 33.47012828224844
At time: 177.5779287815094 and batch: 850, loss is 3.575600061416626 and perplexity is 35.716046208021204
At time: 178.05396676063538 and batch: 900, loss is 3.5335887098312377 and perplexity is 34.246648634439275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308642243685788 and perplexity of 74.33948548101266
finished 19 epochs...
Completing Train Step...
At time: 179.32728099822998 and batch: 50, loss is 3.7759815216064454 and perplexity is 43.640321219580535
At time: 179.80280780792236 and batch: 100, loss is 3.64946093082428 and perplexity is 38.453931131798456
At time: 180.27885580062866 and batch: 150, loss is 3.667935700416565 and perplexity is 39.17096174258644
At time: 180.75512862205505 and batch: 200, loss is 3.569095044136047 and perplexity is 35.48446674107257
At time: 181.23083639144897 and batch: 250, loss is 3.7030611991882325 and perplexity is 40.57131132351006
At time: 181.70604300498962 and batch: 300, loss is 3.684904069900513 and perplexity is 39.84130028663389
At time: 182.182204246521 and batch: 350, loss is 3.6622582483291626 and perplexity is 38.94920059888666
At time: 182.65884017944336 and batch: 400, loss is 3.6070823097229003 and perplexity is 36.85835451348749
At time: 183.13437628746033 and batch: 450, loss is 3.6332983446121214 and perplexity is 37.83741184872755
At time: 183.61002135276794 and batch: 500, loss is 3.5135540771484375 and perplexity is 33.56735700892129
At time: 184.08676648139954 and batch: 550, loss is 3.579271650314331 and perplexity is 35.847421877889126
At time: 184.5628538131714 and batch: 600, loss is 3.5950379037857054 and perplexity is 36.4170803069272
At time: 185.03943347930908 and batch: 650, loss is 3.455178780555725 and perplexity is 31.663949074687743
At time: 185.51615238189697 and batch: 700, loss is 3.438612489700317 and perplexity is 31.14371594908487
At time: 185.99314188957214 and batch: 750, loss is 3.551068525314331 and perplexity is 34.85053627968292
At time: 186.46967220306396 and batch: 800, loss is 3.510789647102356 and perplexity is 33.47469054263377
At time: 186.9450535774231 and batch: 850, loss is 3.57609601020813 and perplexity is 35.733763931153895
At time: 187.42039561271667 and batch: 900, loss is 3.5342277765274046 and perplexity is 34.26854152179937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308533968990797 and perplexity of 74.3314368316368
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa3acf5db70>
ELAPSED
782.3590204715729


RESULTS SO FAR:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}, {'best_accuracy': -74.17703442645892, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}}, {'best_accuracy': -74.01750658952967, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34768402145082, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8801144632298448, 'batch_size': 32}}, {'best_accuracy': -74.3314368316368, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.12854055690722077, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.06117525070097696, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.8443580285243906, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.060788100803058964, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6836469173431396 and batch: 50, loss is 6.6237841320037845 and perplexity is 752.7883646118323
At time: 1.1776938438415527 and batch: 100, loss is 5.853633775711059 and perplexity is 348.49844761386004
At time: 1.660783052444458 and batch: 150, loss is 5.56973801612854 and perplexity is 262.36535474432475
At time: 2.1563637256622314 and batch: 200, loss is 5.315393352508545 and perplexity is 203.44452280520787
At time: 2.640134811401367 and batch: 250, loss is 5.2998814868927 and perplexity is 200.31306884378048
At time: 3.1230781078338623 and batch: 300, loss is 5.190345687866211 and perplexity is 179.5306037574382
At time: 3.6054303646087646 and batch: 350, loss is 5.14246940612793 and perplexity is 171.13785582399296
At time: 4.088009595870972 and batch: 400, loss is 4.972618236541748 and perplexity is 144.4044779254184
At time: 4.570726156234741 and batch: 450, loss is 4.9683377552032475 and perplexity is 143.78767829389565
At time: 5.053362607955933 and batch: 500, loss is 4.8759282684326175 and perplexity is 131.09578882925118
At time: 5.535832166671753 and batch: 550, loss is 4.925539722442627 and perplexity is 137.76367572873656
At time: 6.0193822383880615 and batch: 600, loss is 4.8483945178985595 and perplexity is 127.5354694785076
At time: 6.501845598220825 and batch: 650, loss is 4.7108759021759035 and perplexity is 111.14947334650502
At time: 6.983929634094238 and batch: 700, loss is 4.772506647109985 and perplexity is 118.21519465726604
At time: 7.46653413772583 and batch: 750, loss is 4.791364555358887 and perplexity is 120.46563859272318
At time: 7.952229261398315 and batch: 800, loss is 4.73470253944397 and perplexity is 113.82959386744665
At time: 8.434700965881348 and batch: 850, loss is 4.779308757781982 and perplexity is 119.02204853812842
At time: 8.917778015136719 and batch: 900, loss is 4.701732873916626 and perplexity is 110.13786221241068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.755652858786387 and perplexity of 116.23951638731762
finished 1 epochs...
Completing Train Step...
At time: 10.20213770866394 and batch: 50, loss is 4.736295642852784 and perplexity is 114.01108070672777
At time: 10.680613994598389 and batch: 100, loss is 4.607668533325195 and perplexity is 100.25014708074417
At time: 11.15913200378418 and batch: 150, loss is 4.589091634750366 and perplexity is 98.40500186723146
At time: 11.637970924377441 and batch: 200, loss is 4.4832440376281735 and perplexity is 88.52137405752447
At time: 12.117228269577026 and batch: 250, loss is 4.598420495986939 and perplexity is 99.32730379920174
At time: 12.595004796981812 and batch: 300, loss is 4.558575143814087 and perplexity is 95.44738409900677
At time: 13.072708368301392 and batch: 350, loss is 4.5433909034729005 and perplexity is 94.00903582733262
At time: 13.549938201904297 and batch: 400, loss is 4.441563763618469 and perplexity is 84.90761334990972
At time: 14.041268348693848 and batch: 450, loss is 4.469191331863403 and perplexity is 87.28610897053389
At time: 14.51930832862854 and batch: 500, loss is 4.362513027191162 and perplexity is 78.45404404578122
At time: 14.997451782226562 and batch: 550, loss is 4.444059791564942 and perplexity is 85.11980983971564
At time: 15.475355625152588 and batch: 600, loss is 4.417017269134521 and perplexity is 82.84880077663175
At time: 15.952393770217896 and batch: 650, loss is 4.27271999835968 and perplexity is 71.71643916334263
At time: 16.42872977256775 and batch: 700, loss is 4.311120915412903 and perplexity is 74.52397721464784
At time: 16.907498359680176 and batch: 750, loss is 4.383726301193238 and perplexity is 80.13608894169552
At time: 17.38771390914917 and batch: 800, loss is 4.333230962753296 and perplexity is 76.1900565534487
At time: 17.876089811325073 and batch: 850, loss is 4.404004774093628 and perplexity is 81.77771502979238
At time: 18.35421895980835 and batch: 900, loss is 4.34546238899231 and perplexity is 77.12769223137448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.555447513110017 and perplexity of 95.14932628023689
finished 2 epochs...
Completing Train Step...
At time: 19.62759566307068 and batch: 50, loss is 4.428343572616577 and perplexity is 83.79250569290284
At time: 20.10382390022278 and batch: 100, loss is 4.305108833312988 and perplexity is 74.0772770896361
At time: 20.579954624176025 and batch: 150, loss is 4.304987626075745 and perplexity is 74.06829893165741
At time: 21.056819915771484 and batch: 200, loss is 4.202552628517151 and perplexity is 66.85677391736863
At time: 21.532871961593628 and batch: 250, loss is 4.3374034023284915 and perplexity is 76.50861908995478
At time: 22.009402751922607 and batch: 300, loss is 4.311109080314636 and perplexity is 74.52309522127354
At time: 22.48476266860962 and batch: 350, loss is 4.298694791793824 and perplexity is 73.60366287308452
At time: 22.96061396598816 and batch: 400, loss is 4.213685569763183 and perplexity is 67.60524506535536
At time: 23.4365074634552 and batch: 450, loss is 4.251836428642273 and perplexity is 70.23427422050241
At time: 23.913068771362305 and batch: 500, loss is 4.138076972961426 and perplexity is 62.682165974508536
At time: 24.39017629623413 and batch: 550, loss is 4.2190168094635006 and perplexity is 67.9666272834747
At time: 24.86637830734253 and batch: 600, loss is 4.21750853061676 and perplexity is 67.86419192721844
At time: 25.343137502670288 and batch: 650, loss is 4.073543953895569 and perplexity is 58.76485392865284
At time: 25.818806886672974 and batch: 700, loss is 4.090038275718689 and perplexity is 59.742178335195504
At time: 26.308146953582764 and batch: 750, loss is 4.183184247016907 and perplexity is 65.57432597534888
At time: 26.784058332443237 and batch: 800, loss is 4.143762574195862 and perplexity is 63.0395668313471
At time: 27.261398315429688 and batch: 850, loss is 4.21779851436615 and perplexity is 67.88387429368811
At time: 27.7375009059906 and batch: 900, loss is 4.170347127914429 and perplexity is 64.73792054681005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4741624806025255 and perplexity of 87.72110151159082
finished 3 epochs...
Completing Train Step...
At time: 28.99851107597351 and batch: 50, loss is 4.2549729871749875 and perplexity is 70.45491397646458
At time: 29.487619638442993 and batch: 100, loss is 4.131639719009399 and perplexity is 62.2799608908878
At time: 29.961792945861816 and batch: 150, loss is 4.1437344026565555 and perplexity is 63.03779093472723
At time: 30.43813729286194 and batch: 200, loss is 4.041424717903137 and perplexity is 56.90736201448803
At time: 30.9139986038208 and batch: 250, loss is 4.182587051391602 and perplexity is 65.53517696571623
At time: 31.391393184661865 and batch: 300, loss is 4.163551540374756 and perplexity is 64.2994797592396
At time: 31.86832618713379 and batch: 350, loss is 4.149251146316528 and perplexity is 63.38651529522145
At time: 32.361369132995605 and batch: 400, loss is 4.074247975349426 and perplexity is 58.80624021325811
At time: 32.84404540061951 and batch: 450, loss is 4.114955720901489 and perplexity is 61.24950212996844
At time: 33.319990396499634 and batch: 500, loss is 3.997206621170044 and perplexity is 54.44584953221739
At time: 33.79630994796753 and batch: 550, loss is 4.081403908729553 and perplexity is 59.22856300527336
At time: 34.27207946777344 and batch: 600, loss is 4.086274242401123 and perplexity is 59.517729466985124
At time: 34.747480630874634 and batch: 650, loss is 3.945127239227295 and perplexity is 51.682913787770936
At time: 35.22244048118591 and batch: 700, loss is 3.955287356376648 and perplexity is 52.21069486512206
At time: 35.698179960250854 and batch: 750, loss is 4.05217679977417 and perplexity is 57.52253590340546
At time: 36.17405605316162 and batch: 800, loss is 4.02021963596344 and perplexity is 55.71334113644639
At time: 36.64942216873169 and batch: 850, loss is 4.093516855239868 and perplexity is 59.950358128362076
At time: 37.1258487701416 and batch: 900, loss is 4.049764447212219 and perplexity is 57.38393850665486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442891160102739 and perplexity of 85.02039425328566
finished 4 epochs...
Completing Train Step...
At time: 38.38508605957031 and batch: 50, loss is 4.136332273483276 and perplexity is 62.57289977829826
At time: 38.873698472976685 and batch: 100, loss is 4.010764098167419 and perplexity is 55.18902428894151
At time: 39.349865674972534 and batch: 150, loss is 4.030099081993103 and perplexity is 56.26648596652058
At time: 39.82645034790039 and batch: 200, loss is 3.9322039604187013 and perplexity is 51.01929836273551
At time: 40.30175542831421 and batch: 250, loss is 4.0734884071350095 and perplexity is 58.76158982203848
At time: 40.777995586395264 and batch: 300, loss is 4.058357062339783 and perplexity is 57.87914110054062
At time: 41.2540819644928 and batch: 350, loss is 4.044024834632873 and perplexity is 57.0555203294938
At time: 41.73055100440979 and batch: 400, loss is 3.974703335762024 and perplexity is 53.234321843412545
At time: 42.20626163482666 and batch: 450, loss is 4.015030717849731 and perplexity is 55.42499791310224
At time: 42.682005405426025 and batch: 500, loss is 3.897979211807251 and perplexity is 49.302718021321915
At time: 43.15825033187866 and batch: 550, loss is 3.980688605308533 and perplexity is 53.553899032550675
At time: 43.63525390625 and batch: 600, loss is 3.9897143840789795 and perplexity is 54.039452630908634
At time: 44.11060047149658 and batch: 650, loss is 3.8511026525497436 and perplexity is 47.04490883116012
At time: 44.58573508262634 and batch: 700, loss is 3.8581657218933105 and perplexity is 47.37836651560621
At time: 45.06263780593872 and batch: 750, loss is 3.9559356021881102 and perplexity is 52.24455120181053
At time: 45.541667222976685 and batch: 800, loss is 3.9283662843704223 and perplexity is 50.82387804319053
At time: 46.01672959327698 and batch: 850, loss is 3.9978268480300905 and perplexity is 54.47962878483235
At time: 46.492409467697144 and batch: 900, loss is 3.956337423324585 and perplexity is 52.265548385022555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42217557724208 and perplexity of 83.27726454138347
finished 5 epochs...
Completing Train Step...
At time: 47.765156507492065 and batch: 50, loss is 4.04704665184021 and perplexity is 57.22819244298639
At time: 48.24014401435852 and batch: 100, loss is 3.920969662666321 and perplexity is 50.44933990979352
At time: 48.71334266662598 and batch: 150, loss is 3.9436282539367675 and perplexity is 51.605499895869556
At time: 49.18650698661804 and batch: 200, loss is 3.8462607526779173 and perplexity is 46.81767266444714
At time: 49.6598379611969 and batch: 250, loss is 3.9903733348846435 and perplexity is 54.07507370673678
At time: 50.13394284248352 and batch: 300, loss is 3.9779596090316773 and perplexity is 53.40794987940623
At time: 50.607027769088745 and batch: 350, loss is 3.962665433883667 and perplexity is 52.597333991329194
At time: 51.09655284881592 and batch: 400, loss is 3.89706928730011 and perplexity is 49.25787667413992
At time: 51.56961250305176 and batch: 450, loss is 3.935798192024231 and perplexity is 51.20300347900411
At time: 52.043195724487305 and batch: 500, loss is 3.819124865531921 and perplexity is 45.56431596439179
At time: 52.515279054641724 and batch: 550, loss is 3.903212080001831 and perplexity is 49.56138885127843
At time: 52.98741340637207 and batch: 600, loss is 3.9131198072433473 and perplexity is 50.05487017611382
At time: 53.461071491241455 and batch: 650, loss is 3.7778088569641115 and perplexity is 43.72013962685339
At time: 53.93459939956665 and batch: 700, loss is 3.7856359672546387 and perplexity is 44.063684710358686
At time: 54.407785177230835 and batch: 750, loss is 3.880980234146118 and perplexity is 48.47170541248145
At time: 54.88013172149658 and batch: 800, loss is 3.8552447748184204 and perplexity is 47.24017873237071
At time: 55.353206634521484 and batch: 850, loss is 3.9247402143478394 and perplexity is 50.639920825025456
At time: 55.826754570007324 and batch: 900, loss is 3.880402398109436 and perplexity is 48.443704804993814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42510986328125 and perplexity of 83.52198271708478
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 57.097634077072144 and batch: 50, loss is 3.98453763961792 and perplexity is 53.76042703904274
At time: 57.57055711746216 and batch: 100, loss is 3.844856581687927 and perplexity is 46.75197878019014
At time: 58.044068336486816 and batch: 150, loss is 3.864100661277771 and perplexity is 47.66039031827623
At time: 58.51774311065674 and batch: 200, loss is 3.7483855533599852 and perplexity is 42.452489366569864
At time: 58.991841077804565 and batch: 250, loss is 3.8889788722991945 and perplexity is 48.86096775368852
At time: 59.4657506942749 and batch: 300, loss is 3.8636490058898927 and perplexity is 47.63886910665204
At time: 59.943164110183716 and batch: 350, loss is 3.8390591287612916 and perplexity is 46.48172054563956
At time: 60.41890239715576 and batch: 400, loss is 3.766839108467102 and perplexity is 43.24316163828041
At time: 60.895761489868164 and batch: 450, loss is 3.796774549484253 and perplexity is 44.557235310004444
At time: 61.37229251861572 and batch: 500, loss is 3.672108573913574 and perplexity is 39.33475872505668
At time: 61.84776544570923 and batch: 550, loss is 3.7407545471191406 and perplexity is 42.12976706927564
At time: 62.324217796325684 and batch: 600, loss is 3.747130904197693 and perplexity is 42.399259785556076
At time: 62.819403886795044 and batch: 650, loss is 3.6002247190475463 and perplexity is 36.60645968821399
At time: 63.29529356956482 and batch: 700, loss is 3.5929233837127685 and perplexity is 36.340157016203186
At time: 63.76901316642761 and batch: 750, loss is 3.6808987760543825 and perplexity is 39.68204332015289
At time: 64.24306988716125 and batch: 800, loss is 3.637789025306702 and perplexity is 38.00770967403932
At time: 64.71699047088623 and batch: 850, loss is 3.695095000267029 and perplexity is 40.24939610972915
At time: 65.19058799743652 and batch: 900, loss is 3.639542331695557 and perplexity is 38.07440728782658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335881899480951 and perplexity of 76.39229952086549
finished 7 epochs...
Completing Train Step...
At time: 66.44020175933838 and batch: 50, loss is 3.887984805107117 and perplexity is 48.812420802135044
At time: 66.93542456626892 and batch: 100, loss is 3.7595779037475587 and perplexity is 42.93030143452933
At time: 67.42482495307922 and batch: 150, loss is 3.7851693344116213 and perplexity is 44.04312794449253
At time: 67.91389155387878 and batch: 200, loss is 3.676299772262573 and perplexity is 39.49996446405812
At time: 68.38978147506714 and batch: 250, loss is 3.817961401939392 and perplexity is 45.511334368704084
At time: 68.86646771430969 and batch: 300, loss is 3.802417254447937 and perplexity is 44.809369332809496
At time: 69.34253883361816 and batch: 350, loss is 3.7807313632965087 and perplexity is 43.84809890158604
At time: 69.81699633598328 and batch: 400, loss is 3.7117830181121825 and perplexity is 40.926714582856064
At time: 70.29193830490112 and batch: 450, loss is 3.746605396270752 and perplexity is 42.37698449187666
At time: 70.76656103134155 and batch: 500, loss is 3.6240220880508422 and perplexity is 37.48804522125444
At time: 71.24173712730408 and batch: 550, loss is 3.6972987031936646 and perplexity is 40.33819162526536
At time: 71.71899652481079 and batch: 600, loss is 3.709783058166504 and perplexity is 40.844944588594345
At time: 72.19413733482361 and batch: 650, loss is 3.566045713424683 and perplexity is 35.37642767400128
At time: 72.66984748840332 and batch: 700, loss is 3.5631261444091797 and perplexity is 35.2732943775315
At time: 73.14594054222107 and batch: 750, loss is 3.657404313087463 and perplexity is 38.76060179494718
At time: 73.62416791915894 and batch: 800, loss is 3.618714280128479 and perplexity is 37.289593016839596
At time: 74.10238862037659 and batch: 850, loss is 3.6820160388946532 and perplexity is 39.72640336888307
At time: 74.57991814613342 and batch: 900, loss is 3.6347976779937743 and perplexity is 37.894185293912905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333739816325984 and perplexity of 76.22883600162282
finished 8 epochs...
Completing Train Step...
At time: 75.86087369918823 and batch: 50, loss is 3.847472834587097 and perplexity is 46.87445392333029
At time: 76.35180521011353 and batch: 100, loss is 3.7211660242080686 and perplexity is 41.312537453770986
At time: 76.83028030395508 and batch: 150, loss is 3.746789674758911 and perplexity is 42.38479437808648
At time: 77.30866765975952 and batch: 200, loss is 3.640317964553833 and perplexity is 38.10395050504229
At time: 77.79786849021912 and batch: 250, loss is 3.7807349348068238 and perplexity is 43.848255505803216
At time: 78.2789556980133 and batch: 300, loss is 3.7688151693344114 and perplexity is 43.32869724171851
At time: 78.75753855705261 and batch: 350, loss is 3.747894973754883 and perplexity is 42.431668148750795
At time: 79.23609519004822 and batch: 400, loss is 3.6799183797836306 and perplexity is 39.64315825736589
At time: 79.71413135528564 and batch: 450, loss is 3.716680746078491 and perplexity is 41.12765416948042
At time: 80.19270014762878 and batch: 500, loss is 3.595865774154663 and perplexity is 36.447241411657075
At time: 80.67086553573608 and batch: 550, loss is 3.6704533529281616 and perplexity is 39.2697048610719
At time: 81.14912295341492 and batch: 600, loss is 3.685258297920227 and perplexity is 39.8554156914256
At time: 81.62637686729431 and batch: 650, loss is 3.5429011964797974 and perplexity is 34.56705968887382
At time: 82.10585236549377 and batch: 700, loss is 3.541752133369446 and perplexity is 34.52736276725393
At time: 82.58424925804138 and batch: 750, loss is 3.638946599960327 and perplexity is 38.05173190999686
At time: 83.06283736228943 and batch: 800, loss is 3.601620035171509 and perplexity is 36.65757292292569
At time: 83.54142379760742 and batch: 850, loss is 3.6676272535324097 and perplexity is 39.15888144464847
At time: 84.02005052566528 and batch: 900, loss is 3.6232984828948975 and perplexity is 37.46092849053234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334956757009846 and perplexity of 76.32165844168307
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 85.29717469215393 and batch: 50, loss is 3.83330810546875 and perplexity is 46.21517029112783
At time: 85.77135348320007 and batch: 100, loss is 3.7094525623321535 and perplexity is 40.83144773500334
At time: 86.24691033363342 and batch: 150, loss is 3.735142912864685 and perplexity is 41.89401232861466
At time: 86.72332668304443 and batch: 200, loss is 3.620654516220093 and perplexity is 37.36201386510515
At time: 87.21293711662292 and batch: 250, loss is 3.760693573951721 and perplexity is 42.97822422073701
At time: 87.68855905532837 and batch: 300, loss is 3.7450393724441526 and perplexity is 42.3106730606556
At time: 88.16422080993652 and batch: 350, loss is 3.720249629020691 and perplexity is 41.27469618469662
At time: 88.64013004302979 and batch: 400, loss is 3.6496236419677732 and perplexity is 38.46018852396464
At time: 89.11684203147888 and batch: 450, loss is 3.6838098049163817 and perplexity is 39.79772719141207
At time: 89.59383606910706 and batch: 500, loss is 3.55828134059906 and perplexity is 35.10281548837995
At time: 90.06962823867798 and batch: 550, loss is 3.630347948074341 and perplexity is 37.72594100227151
At time: 90.54593539237976 and batch: 600, loss is 3.639534206390381 and perplexity is 38.07409792290482
At time: 91.02182149887085 and batch: 650, loss is 3.491183109283447 and perplexity is 32.82476002040706
At time: 91.49882483482361 and batch: 700, loss is 3.488829951286316 and perplexity is 32.74760898375161
At time: 91.9743766784668 and batch: 750, loss is 3.583487825393677 and perplexity is 35.99887994717663
At time: 92.45047903060913 and batch: 800, loss is 3.5405066633224487 and perplexity is 34.484386739358605
At time: 92.92606925964355 and batch: 850, loss is 3.602889657020569 and perplexity is 36.704143735847175
At time: 93.4026575088501 and batch: 900, loss is 3.5585673522949217 and perplexity is 35.112856740056564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317784975652826 and perplexity of 75.02226796383255
finished 10 epochs...
Completing Train Step...
At time: 94.67975640296936 and batch: 50, loss is 3.8074248170852663 and perplexity is 45.034317808211924
At time: 95.1558837890625 and batch: 100, loss is 3.682852210998535 and perplexity is 39.759635371068
At time: 95.63209414482117 and batch: 150, loss is 3.7113431119918823 and perplexity is 40.90871463006226
At time: 96.10861539840698 and batch: 200, loss is 3.5990506410598755 and perplexity is 36.56350607007043
At time: 96.5854275226593 and batch: 250, loss is 3.740046639442444 and perplexity is 42.09995363757348
At time: 97.06159472465515 and batch: 300, loss is 3.7260920000076294 and perplexity is 41.516544066783794
At time: 97.53852653503418 and batch: 350, loss is 3.7031809949874877 and perplexity is 40.57617188730864
At time: 98.01319766044617 and batch: 400, loss is 3.634535799026489 and perplexity is 37.88426290309146
At time: 98.48965883255005 and batch: 450, loss is 3.6704262828826906 and perplexity is 39.26864184276372
At time: 98.96603679656982 and batch: 500, loss is 3.546003484725952 and perplexity is 34.67446318401171
At time: 99.45582485198975 and batch: 550, loss is 3.6195052337646483 and perplexity is 37.31909902343088
At time: 99.93182015419006 and batch: 600, loss is 3.630252957344055 and perplexity is 37.72235755778469
At time: 100.40772533416748 and batch: 650, loss is 3.483774824142456 and perplexity is 32.58248337379696
At time: 100.88402962684631 and batch: 700, loss is 3.483133339881897 and perplexity is 32.56158892598895
At time: 101.36028456687927 and batch: 750, loss is 3.5798214530944823 and perplexity is 35.867136309126785
At time: 101.83672952651978 and batch: 800, loss is 3.538704743385315 and perplexity is 34.42230458570629
At time: 102.31384897232056 and batch: 850, loss is 3.603149724006653 and perplexity is 36.71369051323236
At time: 102.79093551635742 and batch: 900, loss is 3.5612282752990723 and perplexity is 35.20641376711775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316821790721319 and perplexity of 74.95004243465678
finished 11 epochs...
Completing Train Step...
At time: 104.05259656906128 and batch: 50, loss is 3.79503071308136 and perplexity is 44.479602490200435
At time: 104.54195404052734 and batch: 100, loss is 3.670592784881592 and perplexity is 39.27518069447551
At time: 105.01777386665344 and batch: 150, loss is 3.698902058601379 and perplexity is 40.40291996034151
At time: 105.49407029151917 and batch: 200, loss is 3.5873024606704713 and perplexity is 36.136464795718496
At time: 105.96917963027954 and batch: 250, loss is 3.728216872215271 and perplexity is 41.60485520914697
At time: 106.44513511657715 and batch: 300, loss is 3.7151594877243044 and perplexity is 41.065135947227716
At time: 106.91993284225464 and batch: 350, loss is 3.692990655899048 and perplexity is 40.164786574733014
At time: 107.3946795463562 and batch: 400, loss is 3.6252431583404543 and perplexity is 37.5338487184434
At time: 107.87066125869751 and batch: 450, loss is 3.6617927837371824 and perplexity is 38.93107534378144
At time: 108.34570121765137 and batch: 500, loss is 3.5378831005096436 and perplexity is 34.39403336039517
At time: 108.82105422019958 and batch: 550, loss is 3.6122395181655884 and perplexity is 37.04893173139148
At time: 109.29602670669556 and batch: 600, loss is 3.6237862586975096 and perplexity is 37.479205482168155
At time: 109.77208137512207 and batch: 650, loss is 3.478080701828003 and perplexity is 32.39748193841727
At time: 110.24748134613037 and batch: 700, loss is 3.478280258178711 and perplexity is 32.40394770680614
At time: 110.7230863571167 and batch: 750, loss is 3.575972580909729 and perplexity is 35.72935360992967
At time: 111.19869112968445 and batch: 800, loss is 3.5358237838745117 and perplexity is 34.32327803416183
At time: 111.68902277946472 and batch: 850, loss is 3.601161313056946 and perplexity is 36.64076113982289
At time: 112.16532707214355 and batch: 900, loss is 3.5604653215408324 and perplexity is 35.17956314561375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316858160985659 and perplexity of 74.95276843708488
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 113.42503023147583 and batch: 50, loss is 3.7917670726776125 and perplexity is 44.3346736887574
At time: 113.91478633880615 and batch: 100, loss is 3.671294322013855 and perplexity is 39.30274335909496
At time: 114.3912000656128 and batch: 150, loss is 3.701701807975769 and perplexity is 40.516196509203304
At time: 114.86618304252625 and batch: 200, loss is 3.586523942947388 and perplexity is 36.10834286555634
At time: 115.34132051467896 and batch: 250, loss is 3.7252587604522707 and perplexity is 41.48196524828587
At time: 115.81667184829712 and batch: 300, loss is 3.710763669013977 and perplexity is 40.88501722894412
At time: 116.2930383682251 and batch: 350, loss is 3.6875417041778564 and perplexity is 39.94652577809995
At time: 116.76876425743103 and batch: 400, loss is 3.621634168624878 and perplexity is 37.398633586205904
At time: 117.24507117271423 and batch: 450, loss is 3.65412269115448 and perplexity is 38.6336126331332
At time: 117.72114562988281 and batch: 500, loss is 3.527926735877991 and perplexity is 34.05329290598293
At time: 118.19700312614441 and batch: 550, loss is 3.601617360115051 and perplexity is 36.65747486197968
At time: 118.6733226776123 and batch: 600, loss is 3.6154634761810303 and perplexity is 37.16856868043629
At time: 119.14958477020264 and batch: 650, loss is 3.463272271156311 and perplexity is 31.921260819404644
At time: 119.62576389312744 and batch: 700, loss is 3.4625088596343994 and perplexity is 31.89690106054452
At time: 120.10125708580017 and batch: 750, loss is 3.5591696071624757 and perplexity is 35.13400999812893
At time: 120.57767462730408 and batch: 800, loss is 3.517688903808594 and perplexity is 33.706439554546186
At time: 121.05366587638855 and batch: 850, loss is 3.580409216880798 and perplexity is 35.88822390962501
At time: 121.53038167953491 and batch: 900, loss is 3.5425256299972534 and perplexity is 34.55407989739446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310289356806507 and perplexity of 74.46203191915352
finished 13 epochs...
Completing Train Step...
At time: 122.8094961643219 and batch: 50, loss is 3.783803548812866 and perplexity is 43.98301553429821
At time: 123.28415417671204 and batch: 100, loss is 3.661599440574646 and perplexity is 38.92354901415915
At time: 123.77256202697754 and batch: 150, loss is 3.6916798830032347 and perplexity is 40.11217415012771
At time: 124.24901819229126 and batch: 200, loss is 3.578482370376587 and perplexity is 35.8191393898495
At time: 124.72536444664001 and batch: 250, loss is 3.7181229162216187 and perplexity is 41.187010034721226
At time: 125.20195937156677 and batch: 300, loss is 3.704672245979309 and perplexity is 40.636726283559746
At time: 125.67728734016418 and batch: 350, loss is 3.6815842962265015 and perplexity is 39.70925548749891
At time: 126.15567779541016 and batch: 400, loss is 3.6163414764404296 and perplexity is 37.20121702390889
At time: 126.63194942474365 and batch: 450, loss is 3.649865384101868 and perplexity is 38.46948709589913
At time: 127.10872983932495 and batch: 500, loss is 3.524413142204285 and perplexity is 33.933853425238304
At time: 127.58484530448914 and batch: 550, loss is 3.598459916114807 and perplexity is 36.54191347322638
At time: 128.06271529197693 and batch: 600, loss is 3.6127015209197997 and perplexity is 37.06605239448418
At time: 128.53860211372375 and batch: 650, loss is 3.46132848739624 and perplexity is 31.859273055944204
At time: 129.0159089565277 and batch: 700, loss is 3.4614135456085204 and perplexity is 31.86198306400748
At time: 129.49236750602722 and batch: 750, loss is 3.5588644218444823 and perplexity is 35.12328925010593
At time: 129.96818852424622 and batch: 800, loss is 3.517886366844177 and perplexity is 33.71309598759744
At time: 130.4449977874756 and batch: 850, loss is 3.5817277145385744 and perplexity is 35.93557365719769
At time: 130.9218192100525 and batch: 900, loss is 3.544922685623169 and perplexity is 34.63700730010191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3097291711258565 and perplexity of 74.42033103632949
finished 14 epochs...
Completing Train Step...
At time: 132.2060523033142 and batch: 50, loss is 3.7801093530654906 and perplexity is 43.820833416043115
At time: 132.68386340141296 and batch: 100, loss is 3.6577238607406617 and perplexity is 38.772989633434264
At time: 133.1623911857605 and batch: 150, loss is 3.687534875869751 and perplexity is 39.94625301184545
At time: 133.63966012001038 and batch: 200, loss is 3.5747533226013184 and perplexity is 35.68581684536103
At time: 134.11782956123352 and batch: 250, loss is 3.7143289184570314 and perplexity is 41.03104266772857
At time: 134.59586477279663 and batch: 300, loss is 3.7011571073532106 and perplexity is 40.494133321202675
At time: 135.0727241039276 and batch: 350, loss is 3.678332986831665 and perplexity is 39.58035806831343
At time: 135.54925537109375 and batch: 400, loss is 3.613394317626953 and perplexity is 37.09174053083095
At time: 136.04031491279602 and batch: 450, loss is 3.6472087049484254 and perplexity is 38.367421649091256
At time: 136.5187656879425 and batch: 500, loss is 3.5220989561080933 and perplexity is 33.855414969049924
At time: 136.99738597869873 and batch: 550, loss is 3.596542739868164 and perplexity is 36.47192329789722
At time: 137.475811958313 and batch: 600, loss is 3.6110321569442747 and perplexity is 37.004227280568614
At time: 137.96555066108704 and batch: 650, loss is 3.4599475955963133 and perplexity is 31.815309208673796
At time: 138.44620990753174 and batch: 700, loss is 3.460472764968872 and perplexity is 31.83202202279361
At time: 138.92408537864685 and batch: 750, loss is 3.558305048942566 and perplexity is 35.103647727853044
At time: 139.40209102630615 and batch: 800, loss is 3.5175942087173464 and perplexity is 33.7032478712977
At time: 139.8799455165863 and batch: 850, loss is 3.5818737840652464 and perplexity is 35.940823132817364
At time: 140.35748505592346 and batch: 900, loss is 3.545550446510315 and perplexity is 34.658757884901604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309611281303511 and perplexity of 74.41155815385163
finished 15 epochs...
Completing Train Step...
At time: 141.62133765220642 and batch: 50, loss is 3.7769092988967894 and perplexity is 43.680828506516214
At time: 142.11199378967285 and batch: 100, loss is 3.654530129432678 and perplexity is 38.649356652885366
At time: 142.58963918685913 and batch: 150, loss is 3.6842004680633544 and perplexity is 39.813277734073225
At time: 143.06743025779724 and batch: 200, loss is 3.571731915473938 and perplexity is 35.57815818621852
At time: 143.54721856117249 and batch: 250, loss is 3.7112533473968505 and perplexity is 40.90504264067001
At time: 144.02534437179565 and batch: 300, loss is 3.698304224014282 and perplexity is 40.37877291605781
At time: 144.50359964370728 and batch: 350, loss is 3.67566957950592 and perplexity is 39.47507971448285
At time: 144.98262357711792 and batch: 400, loss is 3.6109910345077516 and perplexity is 37.00270560786885
At time: 145.46153330802917 and batch: 450, loss is 3.6449569845199585 and perplexity is 38.28112613512385
At time: 145.94012665748596 and batch: 500, loss is 3.520066499710083 and perplexity is 33.78667519325032
At time: 146.41869711875916 and batch: 550, loss is 3.5948274183273314 and perplexity is 36.409415847743055
At time: 146.8967764377594 and batch: 600, loss is 3.6095226573944093 and perplexity is 36.94841155365123
At time: 147.37438440322876 and batch: 650, loss is 3.458610191345215 and perplexity is 31.772787719436348
At time: 147.85157465934753 and batch: 700, loss is 3.4594516372680664 and perplexity is 31.799534053339194
At time: 148.34210300445557 and batch: 750, loss is 3.5575404453277586 and perplexity is 35.07681761041691
At time: 148.82067155838013 and batch: 800, loss is 3.517021727561951 and perplexity is 33.68395891882346
At time: 149.2984790802002 and batch: 850, loss is 3.581599316596985 and perplexity is 35.93095989971509
At time: 149.7765610218048 and batch: 900, loss is 3.5456314659118653 and perplexity is 34.66156603047952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3096334379013275 and perplexity of 74.41320687908357
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 151.03801584243774 and batch: 50, loss is 3.775967402458191 and perplexity is 43.6397050597652
At time: 151.52830243110657 and batch: 100, loss is 3.6549807596206665 and perplexity is 38.666777144544405
At time: 152.0040373802185 and batch: 150, loss is 3.6853994131088257 and perplexity is 39.861040292776586
At time: 152.48078513145447 and batch: 200, loss is 3.57257155418396 and perplexity is 35.608043529752855
At time: 152.95719146728516 and batch: 250, loss is 3.7107006883621216 and perplexity is 40.88244234499271
At time: 153.43345737457275 and batch: 300, loss is 3.6982885217666626 and perplexity is 40.37813888354479
At time: 153.90901827812195 and batch: 350, loss is 3.6745052337646484 and perplexity is 39.42914382135131
At time: 154.38595843315125 and batch: 400, loss is 3.611552286148071 and perplexity is 37.0234792661671
At time: 154.86216711997986 and batch: 450, loss is 3.6421806907653806 and perplexity is 38.17499387904712
At time: 155.33855652809143 and batch: 500, loss is 3.516479158401489 and perplexity is 33.66568799857899
At time: 155.8142867088318 and batch: 550, loss is 3.5916371726989746 and perplexity is 36.29344595258785
At time: 156.29054737091064 and batch: 600, loss is 3.6071301460266114 and perplexity is 36.860117723100664
At time: 156.76656365394592 and batch: 650, loss is 3.4549441719055176 and perplexity is 31.656521309678123
At time: 157.24421501159668 and batch: 700, loss is 3.4539945459365846 and perplexity is 31.62647372419816
At time: 157.73501324653625 and batch: 750, loss is 3.5512789249420167 and perplexity is 34.85786959097674
At time: 158.21684622764587 and batch: 800, loss is 3.5105501222610473 and perplexity is 33.466673482874505
At time: 158.69399857521057 and batch: 850, loss is 3.573618450164795 and perplexity is 35.64534096727179
At time: 159.17187929153442 and batch: 900, loss is 3.5380133247375487 and perplexity is 34.39851258847975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308700770547945 and perplexity of 74.34383646515576
finished 17 epochs...
Completing Train Step...
At time: 160.45658230781555 and batch: 50, loss is 3.7741044044494627 and perplexity is 43.55848006063591
At time: 160.93504166603088 and batch: 100, loss is 3.6523758363723755 and perplexity is 38.56618423311304
At time: 161.41328978538513 and batch: 150, loss is 3.682759609222412 and perplexity is 39.75595372868058
At time: 161.89004492759705 and batch: 200, loss is 3.5699720287323 and perplexity is 35.51559972138745
At time: 162.36980175971985 and batch: 250, loss is 3.7088241386413574 and perplexity is 40.805796346731384
At time: 162.84582233428955 and batch: 300, loss is 3.6964139986038207 and perplexity is 40.302520023729976
At time: 163.3226673603058 and batch: 350, loss is 3.6726098108291625 and perplexity is 39.35447970022294
At time: 163.8144075870514 and batch: 400, loss is 3.6095480060577394 and perplexity is 36.94934815836708
At time: 164.29260420799255 and batch: 450, loss is 3.640844488143921 and perplexity is 38.12401841650952
At time: 164.76827907562256 and batch: 500, loss is 3.5155448436737062 and perplexity is 33.63424834001608
At time: 165.24464058876038 and batch: 550, loss is 3.5904254007339476 and perplexity is 36.24949320799844
At time: 165.7209825515747 and batch: 600, loss is 3.606270408630371 and perplexity is 36.82844132010954
At time: 166.19701862335205 and batch: 650, loss is 3.454329671859741 and perplexity is 31.637074351574473
At time: 166.67283749580383 and batch: 700, loss is 3.4538684988021853 and perplexity is 31.622487549042326
At time: 167.1480131149292 and batch: 750, loss is 3.5515113735198973 and perplexity is 34.86597319499024
At time: 167.62400937080383 and batch: 800, loss is 3.5107810258865357 and perplexity is 33.474401951346096
At time: 168.09970140457153 and batch: 850, loss is 3.5744400644302368 and perplexity is 35.67463972239645
At time: 168.57602047920227 and batch: 900, loss is 3.539465670585632 and perplexity is 34.4485074215241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3083475191299225 and perplexity of 74.317579037513
finished 18 epochs...
Completing Train Step...
At time: 169.85126543045044 and batch: 50, loss is 3.7729794120788576 and perplexity is 43.50950465653582
At time: 170.32681965827942 and batch: 100, loss is 3.6510726547241212 and perplexity is 38.51595822349242
At time: 170.80367136001587 and batch: 150, loss is 3.681311597824097 and perplexity is 39.698428313310444
At time: 171.28032612800598 and batch: 200, loss is 3.5687042760848997 and perplexity is 35.47060325403847
At time: 171.7560567855835 and batch: 250, loss is 3.707657594680786 and perplexity is 40.758222345423746
At time: 172.23119711875916 and batch: 300, loss is 3.6953197526931763 and perplexity is 40.258443275803955
At time: 172.71930527687073 and batch: 350, loss is 3.6715323066711427 and perplexity is 39.31209792208082
At time: 173.1955726146698 and batch: 400, loss is 3.6085305213928223 and perplexity is 36.911771883120984
At time: 173.67127108573914 and batch: 450, loss is 3.6400575971603395 and perplexity is 38.09403077021128
At time: 174.14705419540405 and batch: 500, loss is 3.5149469327926637 and perplexity is 33.614144067838374
At time: 174.62373781204224 and batch: 550, loss is 3.589830379486084 and perplexity is 36.22793040511453
At time: 175.10030913352966 and batch: 600, loss is 3.6057944536209106 and perplexity is 36.810916809742686
At time: 175.5766954421997 and batch: 650, loss is 3.4539676332473754 and perplexity is 31.62562258219335
At time: 176.05201721191406 and batch: 700, loss is 3.453728723526001 and perplexity is 31.618067816003418
At time: 176.52716541290283 and batch: 750, loss is 3.551557960510254 and perplexity is 34.86759753358347
At time: 177.00209784507751 and batch: 800, loss is 3.5108844232559204 and perplexity is 33.47786329539344
At time: 177.4768304824829 and batch: 850, loss is 3.5748044061660766 and perplexity is 35.68763985065921
At time: 177.95135688781738 and batch: 900, loss is 3.540101308822632 and perplexity is 34.470411170741556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308211653199915 and perplexity of 74.30748249642464
finished 19 epochs...
Completing Train Step...
At time: 179.2096347808838 and batch: 50, loss is 3.7720216035842897 and perplexity is 43.46785083475794
At time: 179.69946098327637 and batch: 100, loss is 3.6500580167770384 and perplexity is 38.476898289907005
At time: 180.17463755607605 and batch: 150, loss is 3.6802191305160523 and perplexity is 39.65508275930877
At time: 180.65077710151672 and batch: 200, loss is 3.5677422380447386 and perplexity is 35.43649559346469
At time: 181.12811493873596 and batch: 250, loss is 3.7067138528823853 and perplexity is 40.719775252285835
At time: 181.60546040534973 and batch: 300, loss is 3.6944366550445555 and perplexity is 40.22290683259414
At time: 182.08176445960999 and batch: 350, loss is 3.6706829118728637 and perplexity is 39.278720607861636
At time: 182.55843114852905 and batch: 400, loss is 3.607759175300598 and perplexity is 36.88331111008603
At time: 183.03538537025452 and batch: 450, loss is 3.6394007635116576 and perplexity is 38.06901754465088
At time: 183.51129126548767 and batch: 500, loss is 3.5144234466552735 and perplexity is 33.596552134367016
At time: 183.98740577697754 and batch: 550, loss is 3.589357352256775 and perplexity is 36.21079766001833
At time: 184.4625256061554 and batch: 600, loss is 3.6053998041152955 and perplexity is 36.79639226586303
At time: 184.95539832115173 and batch: 650, loss is 3.4536464405059815 and perplexity is 31.615466292928396
At time: 185.4317409992218 and batch: 700, loss is 3.4535468769073487 and perplexity is 31.612318700027256
At time: 185.90728163719177 and batch: 750, loss is 3.551498980522156 and perplexity is 34.86554110374064
At time: 186.38438534736633 and batch: 800, loss is 3.510889482498169 and perplexity is 33.47803266844227
At time: 186.86211013793945 and batch: 850, loss is 3.5749649143218996 and perplexity is 35.69336846764988
At time: 187.337628364563 and batch: 900, loss is 3.5404273509979247 and perplexity is 34.48165181094472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308153962435788 and perplexity of 74.30319576463269
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa3acf5db70>
ELAPSED
976.3716518878937


RESULTS SO FAR:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}, {'best_accuracy': -74.17703442645892, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}}, {'best_accuracy': -74.01750658952967, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34768402145082, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8801144632298448, 'batch_size': 32}}, {'best_accuracy': -74.3314368316368, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.12854055690722077, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.06117525070097696, 'batch_size': 32}}, {'best_accuracy': -74.30319576463269, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.8443580285243906, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.060788100803058964, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34801440451239196, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.887043781024119, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6740505695343018 and batch: 50, loss is 7.138845529556274 and perplexity is 1259.9729475999595
At time: 1.1643199920654297 and batch: 100, loss is 6.395858068466186 and perplexity is 599.3573923164555
At time: 1.642796277999878 and batch: 150, loss is 6.313963117599488 and perplexity is 552.2291669269802
At time: 2.1207621097564697 and batch: 200, loss is 6.1759019947052005 and perplexity is 481.01670282232806
At time: 2.600745439529419 and batch: 250, loss is 6.231962890625 and perplexity is 508.7531307900774
At time: 3.080873966217041 and batch: 300, loss is 6.137125034332275 and perplexity is 462.7213487391146
At time: 3.570059061050415 and batch: 350, loss is 6.155018882751465 and perplexity is 471.07573758291915
At time: 4.063063621520996 and batch: 400, loss is 6.038664617538452 and perplexity is 419.3326913119213
At time: 4.554230690002441 and batch: 450, loss is 6.041229133605957 and perplexity is 420.40945683753233
At time: 5.0490875244140625 and batch: 500, loss is 6.014171943664551 and perplexity is 409.18686881372673
At time: 5.545778274536133 and batch: 550, loss is 6.053959875106812 and perplexity is 425.7957942440477
At time: 6.027593612670898 and batch: 600, loss is 5.997235307693481 and perplexity is 402.3149773999854
At time: 6.509156703948975 and batch: 650, loss is 5.932402973175049 and perplexity is 377.0594899122994
At time: 6.9907262325286865 and batch: 700, loss is 6.03259840965271 and perplexity is 416.79663194181717
At time: 7.472944021224976 and batch: 750, loss is 5.980654697418213 and perplexity is 395.6993467104578
At time: 7.954612493515015 and batch: 800, loss is 5.9975800037384035 and perplexity is 402.45367768485465
At time: 8.436200141906738 and batch: 850, loss is 6.030128297805786 and perplexity is 415.7683681296368
At time: 8.938812494277954 and batch: 900, loss is 5.905595111846924 and perplexity is 367.0856177971715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.830882033256636 and perplexity of 340.6590191813854
finished 1 epochs...
Completing Train Step...
At time: 10.230618000030518 and batch: 50, loss is 5.559741735458374 and perplexity is 259.7557419616441
At time: 10.708008527755737 and batch: 100, loss is 5.300364284515381 and perplexity is 200.4098028668093
At time: 11.186479091644287 and batch: 150, loss is 5.20624716758728 and perplexity is 182.4082245917636
At time: 11.664955377578735 and batch: 200, loss is 5.044780540466308 and perplexity is 155.21023309505208
At time: 12.143686294555664 and batch: 250, loss is 5.089078054428101 and perplexity is 162.2402164324062
At time: 12.621101379394531 and batch: 300, loss is 5.0033552360534665 and perplexity is 148.9119566085553
At time: 13.099477291107178 and batch: 350, loss is 4.969634466171264 and perplexity is 143.97425029224055
At time: 13.577734470367432 and batch: 400, loss is 4.822391424179077 and perplexity is 124.2618986505842
At time: 14.05564260482788 and batch: 450, loss is 4.82230206489563 and perplexity is 124.25079519246688
At time: 14.53462815284729 and batch: 500, loss is 4.726536703109741 and perplexity is 112.9038648537711
At time: 15.012599229812622 and batch: 550, loss is 4.793839406967163 and perplexity is 120.7641423960868
At time: 15.488621234893799 and batch: 600, loss is 4.735354204177856 and perplexity is 113.90379677452553
At time: 15.967433214187622 and batch: 650, loss is 4.591110305786133 and perplexity is 98.603849831062
At time: 16.446154356002808 and batch: 700, loss is 4.645757446289062 and perplexity is 104.14221805506637
At time: 16.92392921447754 and batch: 750, loss is 4.675575151443481 and perplexity is 107.29425979741929
At time: 17.402159452438354 and batch: 800, loss is 4.621091260910034 and perplexity is 101.60484905350175
At time: 17.880646467208862 and batch: 850, loss is 4.664745426177978 and perplexity is 106.13856168306675
At time: 18.358980655670166 and batch: 900, loss is 4.5990611267089845 and perplexity is 99.39095630825295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.699236308058647 and perplexity of 109.86323873637778
finished 2 epochs...
Completing Train Step...
At time: 19.642374753952026 and batch: 50, loss is 4.648396863937378 and perplexity is 104.41745593741005
At time: 20.120251655578613 and batch: 100, loss is 4.5141470241546635 and perplexity is 91.2996563980031
At time: 20.59885025024414 and batch: 150, loss is 4.503837985992432 and perplexity is 90.36327962083352
At time: 21.077637672424316 and batch: 200, loss is 4.40777545928955 and perplexity is 82.08665514111951
At time: 21.569881916046143 and batch: 250, loss is 4.5322797489166256 and perplexity is 92.97026853947825
At time: 22.04835557937622 and batch: 300, loss is 4.497938508987427 and perplexity is 89.8317529369343
At time: 22.52747654914856 and batch: 350, loss is 4.487956218719482 and perplexity is 88.93948714114738
At time: 23.00690722465515 and batch: 400, loss is 4.384490070343017 and perplexity is 80.1973177935908
At time: 23.485240697860718 and batch: 450, loss is 4.415793285369873 and perplexity is 82.74745722364456
At time: 23.96366572380066 and batch: 500, loss is 4.3053579521179195 and perplexity is 74.09573343119267
At time: 24.442381143569946 and batch: 550, loss is 4.3947921466827395 and perplexity is 81.02778711794167
At time: 24.920188665390015 and batch: 600, loss is 4.376090726852417 and perplexity is 79.52653398979324
At time: 25.3983793258667 and batch: 650, loss is 4.227465209960937 and perplexity is 68.54326899143629
At time: 25.8771390914917 and batch: 700, loss is 4.267087287902832 and perplexity is 71.3136167825004
At time: 26.35564422607422 and batch: 750, loss is 4.344604210853577 and perplexity is 77.06153132499279
At time: 26.834003925323486 and batch: 800, loss is 4.2989250183105465 and perplexity is 73.62061033880897
At time: 27.311434268951416 and batch: 850, loss is 4.362302579879761 and perplexity is 78.43753534031049
At time: 27.78906273841858 and batch: 900, loss is 4.304756193161011 and perplexity is 74.05115907278653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.527356291470462 and perplexity of 92.5136583513498
finished 3 epochs...
Completing Train Step...
At time: 29.089829921722412 and batch: 50, loss is 4.39133020401001 and perplexity is 80.74775856511906
At time: 29.579456090927124 and batch: 100, loss is 4.260908784866333 and perplexity is 70.87436374511121
At time: 30.055375337600708 and batch: 150, loss is 4.259644136428833 and perplexity is 70.78478924380701
At time: 30.53115725517273 and batch: 200, loss is 4.161634650230408 and perplexity is 64.17634277794058
At time: 31.008159637451172 and batch: 250, loss is 4.303992404937744 and perplexity is 73.99462126377236
At time: 31.484453916549683 and batch: 300, loss is 4.276638050079345 and perplexity is 71.99797906491976
At time: 31.960991621017456 and batch: 350, loss is 4.269239482879638 and perplexity is 71.46726286917888
At time: 32.4376003742218 and batch: 400, loss is 4.179773421287536 and perplexity is 65.35104438089613
At time: 32.914515256881714 and batch: 450, loss is 4.220823764801025 and perplexity is 68.08955096877638
At time: 33.391281604766846 and batch: 500, loss is 4.099597086906433 and perplexity is 60.315980602551484
At time: 33.90382504463196 and batch: 550, loss is 4.190784940719604 and perplexity is 66.07463528215436
At time: 34.39546751976013 and batch: 600, loss is 4.189687190055847 and perplexity is 66.00214160468704
At time: 34.87875938415527 and batch: 650, loss is 4.0411577844619755 and perplexity is 56.89217356376097
At time: 35.36853623390198 and batch: 700, loss is 4.068664374351502 and perplexity is 58.47880461727306
At time: 35.84924006462097 and batch: 750, loss is 4.16184006690979 and perplexity is 64.18952702325454
At time: 36.325276613235474 and batch: 800, loss is 4.123035578727722 and perplexity is 61.74639410409989
At time: 36.80157470703125 and batch: 850, loss is 4.18888566493988 and perplexity is 65.94926042611078
At time: 37.2775673866272 and batch: 900, loss is 4.135483021736145 and perplexity is 62.51978219213239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.456358609134203 and perplexity of 86.17314697574066
finished 4 epochs...
Completing Train Step...
At time: 38.54118275642395 and batch: 50, loss is 4.2341748714447025 and perplexity is 69.00471747373115
At time: 39.03128480911255 and batch: 100, loss is 4.107804894447327 and perplexity is 60.813079819612454
At time: 39.50782871246338 and batch: 150, loss is 4.108579201698303 and perplexity is 60.86018606327046
At time: 39.98347806930542 and batch: 200, loss is 4.009695997238159 and perplexity is 55.13010831052034
At time: 40.459585428237915 and batch: 250, loss is 4.155968160629272 and perplexity is 63.813716578045565
At time: 40.93600940704346 and batch: 300, loss is 4.133956823348999 and perplexity is 62.42443737744431
At time: 41.41181206703186 and batch: 350, loss is 4.125707960128784 and perplexity is 61.91162470053817
At time: 41.887943983078 and batch: 400, loss is 4.046022419929504 and perplexity is 57.169607509495
At time: 42.36357522010803 and batch: 450, loss is 4.09332878112793 and perplexity is 59.93908407820841
At time: 42.83950638771057 and batch: 500, loss is 3.966363797187805 and perplexity is 52.792218195419
At time: 43.315922498703 and batch: 550, loss is 4.054652533531189 and perplexity is 57.66512281812058
At time: 43.792375326156616 and batch: 600, loss is 4.061380085945129 and perplexity is 58.05437584628008
At time: 44.26828742027283 and batch: 650, loss is 3.9158163404464723 and perplexity is 50.190026940976864
At time: 44.74433970451355 and batch: 700, loss is 3.9361499738693237 and perplexity is 51.22101893461157
At time: 45.2196044921875 and batch: 750, loss is 4.038127112388611 and perplexity is 56.720013054945476
At time: 45.708468198776245 and batch: 800, loss is 4.001164383888245 and perplexity is 54.661760265548835
At time: 46.1846878528595 and batch: 850, loss is 4.070989890098572 and perplexity is 58.614956248302065
At time: 46.66135287284851 and batch: 900, loss is 4.018500328063965 and perplexity is 55.617635046618226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.422294721211473 and perplexity of 83.28718711633684
finished 5 epochs...
Completing Train Step...
At time: 47.927056312561035 and batch: 50, loss is 4.118458218574524 and perplexity is 61.46440449647987
At time: 48.40234303474426 and batch: 100, loss is 3.995131549835205 and perplexity is 54.332987649279026
At time: 48.878517627716064 and batch: 150, loss is 3.999309859275818 and perplexity is 54.56048262574233
At time: 49.354493141174316 and batch: 200, loss is 3.9033015251159666 and perplexity is 49.565822073623046
At time: 49.830718755722046 and batch: 250, loss is 4.048162975311279 and perplexity is 57.292113288949615
At time: 50.306010007858276 and batch: 300, loss is 4.032364196777344 and perplexity is 56.394080469283374
At time: 50.78133678436279 and batch: 350, loss is 4.019127707481385 and perplexity is 55.652539354069205
At time: 51.25695562362671 and batch: 400, loss is 3.946267967224121 and perplexity is 51.74190357373962
At time: 51.73294544219971 and batch: 450, loss is 3.997646632194519 and perplexity is 54.46981157764393
At time: 52.20820498466492 and batch: 500, loss is 3.8696286153793333 and perplexity is 47.92458432180514
At time: 52.6814489364624 and batch: 550, loss is 3.954212474822998 and perplexity is 52.15460470284908
At time: 53.15540814399719 and batch: 600, loss is 3.9664431858062743 and perplexity is 52.79640946305474
At time: 53.63977885246277 and batch: 650, loss is 3.8230158853530884 and perplexity is 45.74195299142456
At time: 54.11704659461975 and batch: 700, loss is 3.838345823287964 and perplexity is 46.44857670221244
At time: 54.58722996711731 and batch: 750, loss is 3.9436954832077027 and perplexity is 51.60896941262903
At time: 55.05813932418823 and batch: 800, loss is 3.909572787284851 and perplexity is 49.87763905955026
At time: 55.5282416343689 and batch: 850, loss is 3.9816488552093507 and perplexity is 53.60534885717618
At time: 55.998440980911255 and batch: 900, loss is 3.9297175931930544 and perplexity is 50.89260322200302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411328459439212 and perplexity of 82.37882777646878
finished 6 epochs...
Completing Train Step...
At time: 57.25331664085388 and batch: 50, loss is 4.031045293807983 and perplexity is 56.31975117643526
At time: 57.72413992881775 and batch: 100, loss is 3.9090836095809935 and perplexity is 49.85324599735878
At time: 58.20963740348816 and batch: 150, loss is 3.9126851749420166 and perplexity is 50.03311943982506
At time: 58.68295240402222 and batch: 200, loss is 3.818377356529236 and perplexity is 45.53026895481566
At time: 59.15682101249695 and batch: 250, loss is 3.965060429573059 and perplexity is 52.72345534928021
At time: 59.63032388687134 and batch: 300, loss is 3.9529887199401856 and perplexity is 52.090819287499855
At time: 60.10378098487854 and batch: 350, loss is 3.9372015619277954 and perplexity is 51.274910677456276
At time: 60.576871395111084 and batch: 400, loss is 3.8682809019088746 and perplexity is 47.86003921787113
At time: 61.04997992515564 and batch: 450, loss is 3.922438154220581 and perplexity is 50.52347876218517
At time: 61.5233314037323 and batch: 500, loss is 3.79305139541626 and perplexity is 44.391650298632364
At time: 61.99598240852356 and batch: 550, loss is 3.8764720916748048 and perplexity is 48.253679873002255
At time: 62.46968936920166 and batch: 600, loss is 3.8886920404434204 and perplexity is 48.84695488139797
At time: 62.94714641571045 and batch: 650, loss is 3.7503287315368654 and perplexity is 42.53506231845772
At time: 63.42023062705994 and batch: 700, loss is 3.762228765487671 and perplexity is 43.0442546985377
At time: 63.89328169822693 and batch: 750, loss is 3.86799994468689 and perplexity is 47.84659448299451
At time: 64.36644911766052 and batch: 800, loss is 3.835597677230835 and perplexity is 46.321104465676896
At time: 64.84080338478088 and batch: 850, loss is 3.9100909566879274 and perplexity is 49.903490823226655
At time: 65.3142900466919 and batch: 900, loss is 3.856268835067749 and perplexity is 47.28858030041274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.40692598525792 and perplexity of 82.01695426796596
finished 7 epochs...
Completing Train Step...
At time: 66.56313967704773 and batch: 50, loss is 3.9584582567214968 and perplexity is 52.37651253218755
At time: 67.05019187927246 and batch: 100, loss is 3.838617453575134 and perplexity is 46.461195256153836
At time: 67.523934841156 and batch: 150, loss is 3.8408890056610105 and perplexity is 46.56685424066068
At time: 67.99800276756287 and batch: 200, loss is 3.750279874801636 and perplexity is 42.532984244944416
At time: 68.4718861579895 and batch: 250, loss is 3.8971460819244386 and perplexity is 49.26165955952511
At time: 68.94512009620667 and batch: 300, loss is 3.888273935317993 and perplexity is 48.826535988119964
At time: 69.42063999176025 and batch: 350, loss is 3.8706950664520265 and perplexity is 47.975720808597565
At time: 69.89610028266907 and batch: 400, loss is 3.8053723430633544 and perplexity is 44.94198083285291
At time: 70.38444948196411 and batch: 450, loss is 3.8599436712265014 and perplexity is 47.46267777912796
At time: 70.86033964157104 and batch: 500, loss is 3.7313004684448243 and perplexity is 41.73334578852065
At time: 71.33765435218811 and batch: 550, loss is 3.81363431930542 and perplexity is 45.31482851924141
At time: 71.81410384178162 and batch: 600, loss is 3.822990574836731 and perplexity is 45.740795253626686
At time: 72.2900378704071 and batch: 650, loss is 3.6878866863250734 and perplexity is 39.96030899368169
At time: 72.7663369178772 and batch: 700, loss is 3.6984965705871584 and perplexity is 40.38654038164391
At time: 73.24253296852112 and batch: 750, loss is 3.805430698394775 and perplexity is 44.94460351356208
At time: 73.71853423118591 and batch: 800, loss is 3.7746085786819457 and perplexity is 43.58044666092021
At time: 74.19488406181335 and batch: 850, loss is 3.8512363481521605 and perplexity is 47.05119894905813
At time: 74.67098832130432 and batch: 900, loss is 3.796487040519714 and perplexity is 44.54442654682311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.410976044119221 and perplexity of 82.34980133049997
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.92445874214172 and batch: 50, loss is 3.914868960380554 and perplexity is 50.14250042633606
At time: 76.41560411453247 and batch: 100, loss is 3.783004951477051 and perplexity is 43.947904836792155
At time: 76.8936882019043 and batch: 150, loss is 3.780659976005554 and perplexity is 43.84496881631743
At time: 77.37163925170898 and batch: 200, loss is 3.6744603872299195 and perplexity is 39.42737560053318
At time: 77.85024070739746 and batch: 250, loss is 3.8150321292877196 and perplexity is 45.378214329230126
At time: 78.32931160926819 and batch: 300, loss is 3.794642629623413 and perplexity is 44.4623440413338
At time: 78.80762672424316 and batch: 350, loss is 3.7628698205947875 and perplexity is 43.07185728428823
At time: 79.28484272956848 and batch: 400, loss is 3.6911625766754153 and perplexity is 40.09142923481785
At time: 79.76147270202637 and batch: 450, loss is 3.732054214477539 and perplexity is 41.76481399036696
At time: 80.2398316860199 and batch: 500, loss is 3.599836950302124 and perplexity is 36.59226759906847
At time: 80.71710658073425 and batch: 550, loss is 3.66784556388855 and perplexity is 39.16743116721524
At time: 81.1943256855011 and batch: 600, loss is 3.6713725423812864 and perplexity is 39.30581775436016
At time: 81.67124962806702 and batch: 650, loss is 3.5239322662353514 and perplexity is 33.91753937342491
At time: 82.16192078590393 and batch: 700, loss is 3.5222686290740968 and perplexity is 33.8611598050809
At time: 82.6392924785614 and batch: 750, loss is 3.622340588569641 and perplexity is 37.42506206057974
At time: 83.11585068702698 and batch: 800, loss is 3.574748454093933 and perplexity is 35.68564310912109
At time: 83.59352040290833 and batch: 850, loss is 3.6410949850082397 and perplexity is 38.13356955979387
At time: 84.07183146476746 and batch: 900, loss is 3.5781147480010986 and perplexity is 35.80597389285297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332935490020334 and perplexity of 76.16754779474027
finished 9 epochs...
Completing Train Step...
At time: 85.3450767993927 and batch: 50, loss is 3.8298377323150636 and perplexity is 46.055064379171895
At time: 85.82339930534363 and batch: 100, loss is 3.705198998451233 and perplexity is 40.658137418269504
At time: 86.3014976978302 and batch: 150, loss is 3.709827733039856 and perplexity is 40.8467693720816
At time: 86.77888417243958 and batch: 200, loss is 3.609310660362244 and perplexity is 36.94057943028142
At time: 87.25710439682007 and batch: 250, loss is 3.752646040916443 and perplexity is 42.63374351059608
At time: 87.73558068275452 and batch: 300, loss is 3.7386592054367065 and perplexity is 42.0415832321629
At time: 88.21450614929199 and batch: 350, loss is 3.7093510246276855 and perplexity is 40.82730201400728
At time: 88.69431281089783 and batch: 400, loss is 3.6418063688278197 and perplexity is 38.16070681551941
At time: 89.1718761920929 and batch: 450, loss is 3.6873508071899415 and perplexity is 39.93890083446378
At time: 89.65092134475708 and batch: 500, loss is 3.5585026216506956 and perplexity is 35.11058393578
At time: 90.1291995048523 and batch: 550, loss is 3.629266128540039 and perplexity is 37.68515041035313
At time: 90.60794949531555 and batch: 600, loss is 3.637892737388611 and perplexity is 38.011651737154466
At time: 91.08635759353638 and batch: 650, loss is 3.493813796043396 and perplexity is 32.91122536374283
At time: 91.5652551651001 and batch: 700, loss is 3.4981092405319214 and perplexity is 33.05289776046878
At time: 92.04244709014893 and batch: 750, loss is 3.6032102966308592 and perplexity is 36.71591442516445
At time: 92.52042293548584 and batch: 800, loss is 3.5594048404693606 and perplexity is 35.14227565962624
At time: 92.99855542182922 and batch: 850, loss is 3.6310215520858766 and perplexity is 37.75136190832054
At time: 93.47714614868164 and batch: 900, loss is 3.575226655006409 and perplexity is 35.70271209709569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330217335322132 and perplexity of 75.96079373890348
finished 10 epochs...
Completing Train Step...
At time: 94.7514157295227 and batch: 50, loss is 3.7943454360961915 and perplexity is 44.44913208383099
At time: 95.22615694999695 and batch: 100, loss is 3.670452399253845 and perplexity is 39.269667410580816
At time: 95.70312690734863 and batch: 150, loss is 3.6760711240768433 and perplexity is 39.490933901297254
At time: 96.17860412597656 and batch: 200, loss is 3.5768371486663817 and perplexity is 35.76025741431828
At time: 96.6556441783905 and batch: 250, loss is 3.720900354385376 and perplexity is 41.3015634170704
At time: 97.13196206092834 and batch: 300, loss is 3.7092058849334717 and perplexity is 40.821376781880986
At time: 97.60739302635193 and batch: 350, loss is 3.6795516300201414 and perplexity is 39.62862180423435
At time: 98.08295965194702 and batch: 400, loss is 3.613449368476868 and perplexity is 37.09378251887805
At time: 98.56003522872925 and batch: 450, loss is 3.6605390739440917 and perplexity is 38.88229765628865
At time: 99.03677701950073 and batch: 500, loss is 3.5336837100982668 and perplexity is 34.24990222974816
At time: 99.51372599601746 and batch: 550, loss is 3.6049851179122925 and perplexity is 36.7811364730721
At time: 99.99067616462708 and batch: 600, loss is 3.616216549873352 and perplexity is 37.19656989385602
At time: 100.46677994728088 and batch: 650, loss is 3.473773512840271 and perplexity is 32.258239947034454
At time: 100.94324898719788 and batch: 700, loss is 3.4807023668289183 and perplexity is 32.48252871630207
At time: 101.43347692489624 and batch: 750, loss is 3.587756857872009 and perplexity is 36.15288883543138
At time: 101.9178397655487 and batch: 800, loss is 3.545041732788086 and perplexity is 34.64113098307371
At time: 102.3943932056427 and batch: 850, loss is 3.6190510606765747 and perplexity is 37.30215354136553
At time: 102.8706886768341 and batch: 900, loss is 3.566137909889221 and perplexity is 35.37968940591858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331361535477312 and perplexity of 76.04775783356466
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 104.1247706413269 and batch: 50, loss is 3.790189208984375 and perplexity is 44.264774776772086
At time: 104.61427783966064 and batch: 100, loss is 3.662682614326477 and perplexity is 38.96573282285216
At time: 105.08988833427429 and batch: 150, loss is 3.667845048904419 and perplexity is 39.16741099661493
At time: 105.56678891181946 and batch: 200, loss is 3.5613484144210816 and perplexity is 35.21064368884129
At time: 106.04204988479614 and batch: 250, loss is 3.7101489400863645 and perplexity is 40.85989174961826
At time: 106.53168320655823 and batch: 300, loss is 3.6925209760665894 and perplexity is 40.14592641396914
At time: 107.00803995132446 and batch: 350, loss is 3.6555743741989137 and perplexity is 38.68973712116545
At time: 107.48365807533264 and batch: 400, loss is 3.588899974822998 and perplexity is 36.19423944527511
At time: 107.9601411819458 and batch: 450, loss is 3.629466161727905 and perplexity is 37.69268944512837
At time: 108.43697905540466 and batch: 500, loss is 3.4991788244247437 and perplexity is 33.08826952068484
At time: 108.91412711143494 and batch: 550, loss is 3.5700972509384155 and perplexity is 35.52004734160067
At time: 109.39043879508972 and batch: 600, loss is 3.580803561210632 and perplexity is 35.90237901804152
At time: 109.86753416061401 and batch: 650, loss is 3.4304317331314085 and perplexity is 30.88997609735561
At time: 110.34398531913757 and batch: 700, loss is 3.435012102127075 and perplexity is 31.031788113898727
At time: 110.82059454917908 and batch: 750, loss is 3.5355788612365724 and perplexity is 34.31487251575587
At time: 111.29717326164246 and batch: 800, loss is 3.4905807113647462 and perplexity is 32.80499240787101
At time: 111.77321243286133 and batch: 850, loss is 3.5589842224121093 and perplexity is 35.127497292152896
At time: 112.2494945526123 and batch: 900, loss is 3.5095369958877565 and perplexity is 33.43278468305981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312168069081764 and perplexity of 74.60205614399031
finished 12 epochs...
Completing Train Step...
At time: 113.50724625587463 and batch: 50, loss is 3.7628488779067992 and perplexity is 43.07095525326556
At time: 113.99719452857971 and batch: 100, loss is 3.6376265573501585 and perplexity is 38.00153514071118
At time: 114.47356653213501 and batch: 150, loss is 3.6454060649871827 and perplexity is 38.29832130185225
At time: 114.94914937019348 and batch: 200, loss is 3.5412187671661375 and perplexity is 34.50895194915504
At time: 115.42423439025879 and batch: 250, loss is 3.6908612728118895 and perplexity is 40.07935135194289
At time: 115.89979004859924 and batch: 300, loss is 3.6750067853927613 and perplexity is 39.44892453273935
At time: 116.37638878822327 and batch: 350, loss is 3.639784197807312 and perplexity is 38.08361731042594
At time: 116.8535807132721 and batch: 400, loss is 3.5745988845825196 and perplexity is 35.680306024059334
At time: 117.330087184906 and batch: 450, loss is 3.6171157217025756 and perplexity is 37.23003104305522
At time: 117.80652642250061 and batch: 500, loss is 3.487870044708252 and perplexity is 32.71618942080737
At time: 118.28381037712097 and batch: 550, loss is 3.559921979904175 and perplexity is 35.16045381611462
At time: 118.77366375923157 and batch: 600, loss is 3.572529525756836 and perplexity is 35.60654701113872
At time: 119.2503273487091 and batch: 650, loss is 3.424118151664734 and perplexity is 30.695564080606978
At time: 119.7266001701355 and batch: 700, loss is 3.4304914569854734 and perplexity is 30.89182102087254
At time: 120.20044612884521 and batch: 750, loss is 3.532602028846741 and perplexity is 34.21287478219783
At time: 120.67752361297607 and batch: 800, loss is 3.489278120994568 and perplexity is 32.76228875938548
At time: 121.15399646759033 and batch: 850, loss is 3.559660987854004 and perplexity is 35.151278414593726
At time: 121.63134121894836 and batch: 900, loss is 3.5121281337738037 and perplexity is 33.51952596887241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311226204650043 and perplexity of 74.53182420045178
finished 13 epochs...
Completing Train Step...
At time: 122.90111207962036 and batch: 50, loss is 3.751875410079956 and perplexity is 42.60090128941145
At time: 123.37839221954346 and batch: 100, loss is 3.626485438346863 and perplexity is 37.580505242503136
At time: 123.85436868667603 and batch: 150, loss is 3.6343089151382446 and perplexity is 37.875668549217814
At time: 124.33060216903687 and batch: 200, loss is 3.530608067512512 and perplexity is 34.14472360073975
At time: 124.8068253993988 and batch: 250, loss is 3.680472455024719 and perplexity is 39.66512963617126
At time: 125.28270673751831 and batch: 300, loss is 3.6654390907287597 and perplexity is 39.073289115965444
At time: 125.75878310203552 and batch: 350, loss is 3.630459270477295 and perplexity is 37.73014097844897
At time: 126.23450636863708 and batch: 400, loss is 3.5662639427185057 and perplexity is 35.38414868927576
At time: 126.71018052101135 and batch: 450, loss is 3.6091462993621826 and perplexity is 36.93450833864243
At time: 127.18582892417908 and batch: 500, loss is 3.4805467891693116 and perplexity is 32.477475553596086
At time: 127.66123628616333 and batch: 550, loss is 3.5531650304794313 and perplexity is 34.92367725242344
At time: 128.13748383522034 and batch: 600, loss is 3.566905493736267 and perplexity is 35.406856709277804
At time: 128.6145989894867 and batch: 650, loss is 3.4192315673828126 and perplexity is 30.545933508595198
At time: 129.09055280685425 and batch: 700, loss is 3.426623978614807 and perplexity is 30.77257830418758
At time: 129.56669998168945 and batch: 750, loss is 3.529292802810669 and perplexity is 34.09984377193072
At time: 130.0428364276886 and batch: 800, loss is 3.4868632221221922 and perplexity is 32.68326659887019
At time: 130.51907587051392 and batch: 850, loss is 3.5580901765823363 and perplexity is 35.09610573452506
At time: 131.00927877426147 and batch: 900, loss is 3.5116470289230346 and perplexity is 33.50340344095748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311159734856592 and perplexity of 74.52687025013752
finished 14 epochs...
Completing Train Step...
At time: 132.32540559768677 and batch: 50, loss is 3.7428310441970827 and perplexity is 42.21734029880017
At time: 132.80732464790344 and batch: 100, loss is 3.6174494934082033 and perplexity is 37.24245944802662
At time: 133.2875211238861 and batch: 150, loss is 3.625330338478088 and perplexity is 37.53712106718044
At time: 133.76396346092224 and batch: 200, loss is 3.522069563865662 and perplexity is 33.8544198971093
At time: 134.2406027317047 and batch: 250, loss is 3.672075185775757 and perplexity is 39.333445432635685
At time: 134.71684336662292 and batch: 300, loss is 3.6577017164230345 and perplexity is 38.77213104154296
At time: 135.19311356544495 and batch: 350, loss is 3.6227803516387937 and perplexity is 37.44152384011078
At time: 135.6693685054779 and batch: 400, loss is 3.5593745708465576 and perplexity is 35.14121193229698
At time: 136.1463496685028 and batch: 450, loss is 3.6024241209030152 and perplexity is 36.687060607989295
At time: 136.6224400997162 and batch: 500, loss is 3.474298691749573 and perplexity is 32.27518574370385
At time: 137.1003885269165 and batch: 550, loss is 3.547325162887573 and perplexity is 34.720321963365905
At time: 137.5770742893219 and batch: 600, loss is 3.5618901443481445 and perplexity is 35.22972351586871
At time: 138.0539824962616 and batch: 650, loss is 3.41465386390686 and perplexity is 30.40642284544521
At time: 138.5304615497589 and batch: 700, loss is 3.422794828414917 and perplexity is 30.654970791938055
At time: 139.00737810134888 and batch: 750, loss is 3.5257362127304077 and perplexity is 33.97878002051631
At time: 139.4839060306549 and batch: 800, loss is 3.4839199018478393 and perplexity is 32.58721070862767
At time: 139.9598617553711 and batch: 850, loss is 3.555690574645996 and perplexity is 35.01199001373189
At time: 140.43514132499695 and batch: 900, loss is 3.510053267478943 and perplexity is 33.45004953629908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311435647206764 and perplexity of 74.5474359710969
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 141.69770097732544 and batch: 50, loss is 3.743959541320801 and perplexity is 42.265009338026324
At time: 142.1886818408966 and batch: 100, loss is 3.6205799341201783 and perplexity is 37.35922743156437
At time: 142.66751408576965 and batch: 150, loss is 3.6272657346725463 and perplexity is 37.60984061631495
At time: 143.15913343429565 and batch: 200, loss is 3.5217132139205933 and perplexity is 33.84235802568989
At time: 143.63785886764526 and batch: 250, loss is 3.672398533821106 and perplexity is 39.346165881788515
At time: 144.11603927612305 and batch: 300, loss is 3.6567994737625122 and perplexity is 38.73716494720091
At time: 144.59329080581665 and batch: 350, loss is 3.6220464372634886 and perplexity is 37.41405504863463
At time: 145.0706663131714 and batch: 400, loss is 3.5578577613830564 and perplexity is 35.08794981393349
At time: 145.54758024215698 and batch: 450, loss is 3.5961051034927367 and perplexity is 36.455965349724266
At time: 146.02515077590942 and batch: 500, loss is 3.4665398597717285 and perplexity is 32.02573696742501
At time: 146.5041365623474 and batch: 550, loss is 3.5386705923080446 and perplexity is 34.421129046995624
At time: 146.98215007781982 and batch: 600, loss is 3.554459476470947 and perplexity is 34.96891333797021
At time: 147.46087169647217 and batch: 650, loss is 3.404022388458252 and perplexity is 30.084870026640672
At time: 147.9393253326416 and batch: 700, loss is 3.4098781061172487 and perplexity is 30.26155533609805
At time: 148.431884765625 and batch: 750, loss is 3.5094430685043334 and perplexity is 33.42964457654728
At time: 148.91497802734375 and batch: 800, loss is 3.467665843963623 and perplexity is 32.061817750367666
At time: 149.40613555908203 and batch: 850, loss is 3.5374323558807372 and perplexity is 34.37853392799691
At time: 149.88969612121582 and batch: 900, loss is 3.4906015014648437 and perplexity is 32.80567443403654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306444141962757 and perplexity of 74.17625918960486
finished 16 epochs...
Completing Train Step...
At time: 151.1455478668213 and batch: 50, loss is 3.7356568717956544 and perplexity is 41.915549664584084
At time: 151.63684940338135 and batch: 100, loss is 3.610728392601013 and perplexity is 36.99298842283937
At time: 152.11425590515137 and batch: 150, loss is 3.619265604019165 and perplexity is 37.31015732862104
At time: 152.59289479255676 and batch: 200, loss is 3.514466209411621 and perplexity is 33.597988846258716
At time: 153.07091236114502 and batch: 250, loss is 3.665417103767395 and perplexity is 39.07243002251172
At time: 153.54988408088684 and batch: 300, loss is 3.6505481719970705 and perplexity is 38.49576256528829
At time: 154.02860593795776 and batch: 350, loss is 3.6157718324661254 and perplexity is 37.18003160943916
At time: 154.5063443183899 and batch: 400, loss is 3.5524075984954835 and perplexity is 34.89723495765944
At time: 154.98450231552124 and batch: 450, loss is 3.5920627403259275 and perplexity is 36.30889455523527
At time: 155.476731300354 and batch: 500, loss is 3.463078999519348 and perplexity is 31.915091941225946
At time: 155.95542645454407 and batch: 550, loss is 3.5352389097213743 and perplexity is 34.303209105459636
At time: 156.43379163742065 and batch: 600, loss is 3.552011709213257 and perplexity is 34.883422250692085
At time: 156.9114649295807 and batch: 650, loss is 3.402512927055359 and perplexity is 30.039492333078883
At time: 157.38916754722595 and batch: 700, loss is 3.4089749336242674 and perplexity is 30.234236270495003
At time: 157.86698055267334 and batch: 750, loss is 3.5090920162200927 and perplexity is 33.41791108311267
At time: 158.3456530570984 and batch: 800, loss is 3.468219256401062 and perplexity is 32.07956606969498
At time: 158.82373976707458 and batch: 850, loss is 3.5389086866378783 and perplexity is 34.429325498373764
At time: 159.30257201194763 and batch: 900, loss is 3.4930661964416503 and perplexity is 32.88663013960335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305939974850172 and perplexity of 74.13887138482959
finished 17 epochs...
Completing Train Step...
At time: 160.57265210151672 and batch: 50, loss is 3.732633557319641 and perplexity is 41.78901714668973
At time: 161.04943346977234 and batch: 100, loss is 3.6073910903930666 and perplexity is 36.86973741821542
At time: 161.52513456344604 and batch: 150, loss is 3.6158882427215575 and perplexity is 37.18435999834533
At time: 162.0018081665039 and batch: 200, loss is 3.5112653970718384 and perplexity is 33.49061991453651
At time: 162.47867059707642 and batch: 250, loss is 3.662155022621155 and perplexity is 38.94518024758389
At time: 162.95508313179016 and batch: 300, loss is 3.6474498987197874 and perplexity is 38.37667674830751
At time: 163.43187856674194 and batch: 350, loss is 3.612870330810547 and perplexity is 37.0723100389006
At time: 163.9078414440155 and batch: 400, loss is 3.5497431468963625 and perplexity is 34.80437672723759
At time: 164.3843810558319 and batch: 450, loss is 3.5897380399703978 and perplexity is 36.224585290012136
At time: 164.86012315750122 and batch: 500, loss is 3.461068339347839 and perplexity is 31.85098600621187
At time: 165.33755373954773 and batch: 550, loss is 3.533328094482422 and perplexity is 34.23772459508096
At time: 165.81417417526245 and batch: 600, loss is 3.550570511817932 and perplexity is 34.83318456331199
At time: 166.2911033630371 and batch: 650, loss is 3.401451416015625 and perplexity is 30.00762199868785
At time: 166.76795649528503 and batch: 700, loss is 3.4082477951049803 and perplexity is 30.212259783644857
At time: 167.24325227737427 and batch: 750, loss is 3.5085890674591065 and perplexity is 33.40110781208505
At time: 167.73246359825134 and batch: 800, loss is 3.4681049966812134 and perplexity is 32.075900876858924
At time: 168.20892095565796 and batch: 850, loss is 3.539153985977173 and perplexity is 34.43777202509224
At time: 168.8455147743225 and batch: 900, loss is 3.4936626815795897 and perplexity is 32.90625237732333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305860545537243 and perplexity of 74.13298281907964
finished 18 epochs...
Completing Train Step...
At time: 170.10895705223083 and batch: 50, loss is 3.7300447702407835 and perplexity is 41.68097418950833
At time: 170.58588194847107 and batch: 100, loss is 3.604656624794006 and perplexity is 36.76905610712512
At time: 171.06196689605713 and batch: 150, loss is 3.6131712293624876 and perplexity is 37.083466721739086
At time: 171.53820776939392 and batch: 200, loss is 3.5086818313598633 and perplexity is 33.404206372850375
At time: 172.02744960784912 and batch: 250, loss is 3.6595825290679933 and perplexity is 38.84512277624962
At time: 172.51000094413757 and batch: 300, loss is 3.6450251483917238 and perplexity is 38.28373561383183
At time: 172.9869360923767 and batch: 350, loss is 3.610589108467102 and perplexity is 36.98783624530274
At time: 173.47551488876343 and batch: 400, loss is 3.547693362236023 and perplexity is 34.733108317109895
At time: 173.95208883285522 and batch: 450, loss is 3.587800769805908 and perplexity is 36.154476413552736
At time: 174.42846727371216 and batch: 500, loss is 3.4593597221374512 and perplexity is 31.796611329336518
At time: 174.90549659729004 and batch: 550, loss is 3.531705265045166 and perplexity is 34.18220766717983
At time: 175.38258171081543 and batch: 600, loss is 3.5492902517318727 and perplexity is 34.788617562208934
At time: 175.85932445526123 and batch: 650, loss is 3.4004148483276366 and perplexity is 29.976533182945246
At time: 176.33695363998413 and batch: 700, loss is 3.407446069717407 and perplexity is 30.18804755503643
At time: 176.81413960456848 and batch: 750, loss is 3.5079273080825804 and perplexity is 33.37901162778899
At time: 177.29089832305908 and batch: 800, loss is 3.4676719760894774 and perplexity is 32.062014358072034
At time: 177.76721048355103 and batch: 850, loss is 3.538955359458923 and perplexity is 34.43093244962163
At time: 178.24347257614136 and batch: 900, loss is 3.493705916404724 and perplexity is 32.907675104146136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305893989458476 and perplexity of 74.13546215817702
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 179.5127341747284 and batch: 50, loss is 3.7303797578811646 and perplexity is 41.69493913961311
At time: 180.00253868103027 and batch: 100, loss is 3.605702214241028 and perplexity is 36.80752155019397
At time: 180.4794204235077 and batch: 150, loss is 3.6136912631988527 and perplexity is 37.10275639441063
At time: 180.95635843276978 and batch: 200, loss is 3.5093138456344604 and perplexity is 33.42532498103754
At time: 181.43389558792114 and batch: 250, loss is 3.65949312210083 and perplexity is 38.84164990688479
At time: 181.90811824798584 and batch: 300, loss is 3.6442680025100707 and perplexity is 38.25476021176492
At time: 182.3829426765442 and batch: 350, loss is 3.6101505374908447 and perplexity is 36.97161801053447
At time: 182.8594732284546 and batch: 400, loss is 3.547928171157837 and perplexity is 34.74126491840898
At time: 183.35016441345215 and batch: 450, loss is 3.585691375732422 and perplexity is 36.0782927542043
At time: 183.83059072494507 and batch: 500, loss is 3.4572319078445433 and perplexity is 31.729025975324962
At time: 184.3065435886383 and batch: 550, loss is 3.5281698894500733 and perplexity is 34.061574092553435
At time: 184.78290033340454 and batch: 600, loss is 3.546477727890015 and perplexity is 34.69091121101818
At time: 185.2583713531494 and batch: 650, loss is 3.3972305154800413 and perplexity is 29.88122974314189
At time: 185.73368501663208 and batch: 700, loss is 3.402568383216858 and perplexity is 30.04115825420942
At time: 186.20954394340515 and batch: 750, loss is 3.502457799911499 and perplexity is 33.19694321716701
At time: 186.68650221824646 and batch: 800, loss is 3.4605811929702757 and perplexity is 31.835473692447618
At time: 187.162757396698 and batch: 850, loss is 3.5311904668807985 and perplexity is 34.16461525807723
At time: 187.63842868804932 and batch: 900, loss is 3.4849741744995115 and perplexity is 32.62158463023094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30527235057256 and perplexity of 74.08939099337151
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa3acf5db70>
ELAPSED
1171.0376224517822


RESULTS SO FAR:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}, {'best_accuracy': -74.17703442645892, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}}, {'best_accuracy': -74.01750658952967, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34768402145082, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8801144632298448, 'batch_size': 32}}, {'best_accuracy': -74.3314368316368, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.12854055690722077, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.06117525070097696, 'batch_size': 32}}, {'best_accuracy': -74.30319576463269, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.8443580285243906, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.060788100803058964, 'batch_size': 32}}, {'best_accuracy': -74.08939099337151, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34801440451239196, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.887043781024119, 'batch_size': 32}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -74.55276528803914, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.21580183708893874, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8549683470178198, 'batch_size': 32}}, {'best_accuracy': -74.17703442645892, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.5801499722248066, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.711928117945114, 'batch_size': 32}}, {'best_accuracy': -74.01750658952967, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34768402145082, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.8801144632298448, 'batch_size': 32}}, {'best_accuracy': -74.3314368316368, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.12854055690722077, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.06117525070097696, 'batch_size': 32}}, {'best_accuracy': -74.30319576463269, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.8443580285243906, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.060788100803058964, 'batch_size': 32}}, {'best_accuracy': -74.08939099337151, 'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'rnn_dropout': 0.34801440451239196, 'data': 'ptb', 'num_layers': 1, 'dropout': 0.887043781024119, 'batch_size': 32}}]
