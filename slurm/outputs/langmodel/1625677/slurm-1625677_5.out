TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'rnn_dropout', 'domain': [0, 1], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2210655212402344 and batch: 50, loss is 7.463604974746704 and perplexity is 1743.4217336549
At time: 1.7568023204803467 and batch: 100, loss is 6.68732180595398 and perplexity is 802.1710030942658
At time: 2.289532423019409 and batch: 150, loss is 6.491607122421264 and perplexity is 659.5825417639928
At time: 2.8273696899414062 and batch: 200, loss is 6.316638679504394 and perplexity is 553.7086686153656
At time: 3.361527681350708 and batch: 250, loss is 6.3480990028381346 and perplexity is 571.4054357777467
At time: 3.8982934951782227 and batch: 300, loss is 6.229744701385498 and perplexity is 507.62587077003025
At time: 4.4327239990234375 and batch: 350, loss is 6.2356227684020995 and perplexity is 510.6185165271979
At time: 4.968258380889893 and batch: 400, loss is 6.127437362670898 and perplexity is 458.2602997203317
At time: 5.504043817520142 and batch: 450, loss is 6.120709199905395 and perplexity is 455.1873989152764
At time: 6.04312539100647 and batch: 500, loss is 6.099502735137939 and perplexity is 445.63611579498985
At time: 6.584486246109009 and batch: 550, loss is 6.123041830062866 and perplexity is 456.250422107258
At time: 7.12202262878418 and batch: 600, loss is 6.065996809005737 and perplexity is 430.952040515299
At time: 7.657247543334961 and batch: 650, loss is 5.997501029968261 and perplexity is 402.4218956556102
At time: 8.190034866333008 and batch: 700, loss is 6.102356767654419 and perplexity is 446.90979245272354
At time: 8.722140789031982 and batch: 750, loss is 6.048088703155518 and perplexity is 423.3031983096858
At time: 9.254676342010498 and batch: 800, loss is 6.052540855407715 and perplexity is 425.1920101163243
At time: 9.786255598068237 and batch: 850, loss is 6.09967942237854 and perplexity is 445.7148609670306
At time: 10.319316148757935 and batch: 900, loss is 5.969853363037109 and perplexity is 391.4482657563248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.944674557202483 and perplexity of 381.7151146464097
finished 1 epochs...
Completing Train Step...
At time: 11.770874738693237 and batch: 50, loss is 5.747046318054199 and perplexity is 313.2640101933858
At time: 12.295706272125244 and batch: 100, loss is 5.474528779983521 and perplexity is 238.53803654550407
At time: 12.806414604187012 and batch: 150, loss is 5.344477977752685 and perplexity is 209.44851926906105
At time: 13.316653966903687 and batch: 200, loss is 5.149168910980225 and perplexity is 172.2882439320824
At time: 13.824813842773438 and batch: 250, loss is 5.179478483200073 and perplexity is 177.59017058123854
At time: 14.338536500930786 and batch: 300, loss is 5.085009317398072 and perplexity is 161.58144474922796
At time: 14.847434520721436 and batch: 350, loss is 5.044126014709473 and perplexity is 155.10867723885434
At time: 15.355772972106934 and batch: 400, loss is 4.877063722610473 and perplexity is 131.24472663039936
At time: 15.866823673248291 and batch: 450, loss is 4.8748822593688965 and perplexity is 130.95873313916363
At time: 16.374316215515137 and batch: 500, loss is 4.784983806610107 and perplexity is 119.69942472871007
At time: 16.88295316696167 and batch: 550, loss is 4.841298208236695 and perplexity is 126.633641902574
At time: 17.404523372650146 and batch: 600, loss is 4.765722694396973 and perplexity is 117.41594247651555
At time: 17.91535973548889 and batch: 650, loss is 4.634747657775879 and perplexity is 103.0019229812136
At time: 18.422608375549316 and batch: 700, loss is 4.692917413711548 and perplexity is 109.17121325873434
At time: 18.930675983428955 and batch: 750, loss is 4.709406499862671 and perplexity is 110.98626998831605
At time: 19.445087909698486 and batch: 800, loss is 4.645769948959351 and perplexity is 104.14352011902143
At time: 19.958170413970947 and batch: 850, loss is 4.697551441192627 and perplexity is 109.67828965667779
At time: 20.470371961593628 and batch: 900, loss is 4.622698612213135 and perplexity is 101.76829506241738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.69590194911173 and perplexity of 109.49752531210584
finished 2 epochs...
Completing Train Step...
At time: 21.915593147277832 and batch: 50, loss is 4.677488317489624 and perplexity is 107.49972801694905
At time: 22.43955421447754 and batch: 100, loss is 4.544627981185913 and perplexity is 94.12540427394184
At time: 22.947104930877686 and batch: 150, loss is 4.5359358215332035 and perplexity is 93.31079671098671
At time: 23.456685781478882 and batch: 200, loss is 4.434131107330322 and perplexity is 84.27886377793354
At time: 23.964958429336548 and batch: 250, loss is 4.561381883621216 and perplexity is 95.71565638057369
At time: 24.47556734085083 and batch: 300, loss is 4.520104846954346 and perplexity is 91.84522716569197
At time: 24.98344135284424 and batch: 350, loss is 4.509519186019897 and perplexity is 90.87811253715684
At time: 25.493415355682373 and batch: 400, loss is 4.405389194488525 and perplexity is 81.8910081709003
At time: 26.000404357910156 and batch: 450, loss is 4.4313694286346434 and perplexity is 84.04643373162168
At time: 26.513474464416504 and batch: 500, loss is 4.320050668716431 and perplexity is 75.19243809982706
At time: 27.02293825149536 and batch: 550, loss is 4.39976993560791 and perplexity is 81.43213187580018
At time: 27.537516593933105 and batch: 600, loss is 4.3768980503082275 and perplexity is 79.59076354956902
At time: 28.04947543144226 and batch: 650, loss is 4.234029822349548 and perplexity is 68.9947091277685
At time: 28.560281991958618 and batch: 700, loss is 4.268231625556946 and perplexity is 71.39527035015116
At time: 29.07084059715271 and batch: 750, loss is 4.34428113937378 and perplexity is 77.03663896325477
At time: 29.577346086502075 and batch: 800, loss is 4.289366812705993 and perplexity is 72.92028168288618
At time: 30.113216876983643 and batch: 850, loss is 4.35837221622467 and perplexity is 78.12985235131694
At time: 30.622549533843994 and batch: 900, loss is 4.30280704498291 and perplexity is 73.90696296643479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.486287156196489 and perplexity of 88.79116538991657
finished 3 epochs...
Completing Train Step...
At time: 32.04747676849365 and batch: 50, loss is 4.387031326293945 and perplexity is 80.4013788805895
At time: 32.57105207443237 and batch: 100, loss is 4.253732352256775 and perplexity is 70.36755934883088
At time: 33.078115463256836 and batch: 150, loss is 4.252945313453674 and perplexity is 70.3121991373188
At time: 33.586443185806274 and batch: 200, loss is 4.1559621667861935 and perplexity is 63.81333408978841
At time: 34.09803652763367 and batch: 250, loss is 4.2950815582275395 and perplexity is 73.33819553435593
At time: 34.60944604873657 and batch: 300, loss is 4.261396903991699 and perplexity is 70.90896732220014
At time: 35.11998772621155 and batch: 350, loss is 4.258909368515015 and perplexity is 70.73279795498878
At time: 35.6321234703064 and batch: 400, loss is 4.169813251495361 and perplexity is 64.70336772190052
At time: 36.142329692840576 and batch: 450, loss is 4.207597966194153 and perplexity is 67.19494128470711
At time: 36.65219211578369 and batch: 500, loss is 4.0822474479675295 and perplexity is 59.27854570039575
At time: 37.16019797325134 and batch: 550, loss is 4.168357830047608 and perplexity is 64.60926554854791
At time: 37.66844630241394 and batch: 600, loss is 4.166237888336181 and perplexity is 64.4724427509866
At time: 38.17720818519592 and batch: 650, loss is 4.015010786056519 and perplexity is 55.42389320451449
At time: 38.6867401599884 and batch: 700, loss is 4.032869272232055 and perplexity is 56.422570929429824
At time: 39.19623684883118 and batch: 750, loss is 4.14115713596344 and perplexity is 62.87553491407529
At time: 39.707247495651245 and batch: 800, loss is 4.0850927352905275 and perplexity is 59.44745037221406
At time: 40.21556639671326 and batch: 850, loss is 4.161973147392273 and perplexity is 64.19806996491792
At time: 40.72652816772461 and batch: 900, loss is 4.1154199314117434 and perplexity is 61.27794139299673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.392298032159674 and perplexity of 80.82594634894662
finished 4 epochs...
Completing Train Step...
At time: 42.184324502944946 and batch: 50, loss is 4.2026881980896 and perplexity is 66.86583827603566
At time: 42.69851088523865 and batch: 100, loss is 4.07171097278595 and perplexity is 58.65723772086102
At time: 43.2244074344635 and batch: 150, loss is 4.076354570388794 and perplexity is 58.93025172232925
At time: 43.7337429523468 and batch: 200, loss is 3.981016526222229 and perplexity is 53.57146335575047
At time: 44.241501569747925 and batch: 250, loss is 4.127284140586853 and perplexity is 62.009285538852296
At time: 44.751238107681274 and batch: 300, loss is 4.094829807281494 and perplexity is 60.029121768589334
At time: 45.26184415817261 and batch: 350, loss is 4.089903707504273 and perplexity is 59.734139477831086
At time: 45.76718330383301 and batch: 400, loss is 4.014763011932373 and perplexity is 55.41016229907076
At time: 46.27551984786987 and batch: 450, loss is 4.0549730730056766 and perplexity is 57.68360972901903
At time: 46.7853524684906 and batch: 500, loss is 3.9270256614685057 and perplexity is 50.75578804002844
At time: 47.29195857048035 and batch: 550, loss is 4.013344893455505 and perplexity is 55.33163981436088
At time: 47.803011894226074 and batch: 600, loss is 4.02528929233551 and perplexity is 55.99650579863764
At time: 48.31598377227783 and batch: 650, loss is 3.8724470043182375 and perplexity is 48.059844959166576
At time: 48.825321435928345 and batch: 700, loss is 3.880875210762024 and perplexity is 48.46661501725615
At time: 49.337366819381714 and batch: 750, loss is 3.9978012609481812 and perplexity is 54.47823482794197
At time: 49.846696853637695 and batch: 800, loss is 3.9464396381378175 and perplexity is 51.75078691608645
At time: 50.36013054847717 and batch: 850, loss is 4.0243607521057125 and perplexity is 55.944534922534174
At time: 50.86819791793823 and batch: 900, loss is 3.9783817768096923 and perplexity is 53.43050175493781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.343732023892337 and perplexity of 76.99434856438077
finished 5 epochs...
Completing Train Step...
At time: 52.29457116127014 and batch: 50, loss is 4.069543371200561 and perplexity is 58.53022990028819
At time: 52.81720948219299 and batch: 100, loss is 3.9394025373458863 and perplexity is 51.38788978192937
At time: 53.32360911369324 and batch: 150, loss is 3.94948468208313 and perplexity is 51.908610504382565
At time: 53.83170175552368 and batch: 200, loss is 3.8553884124755857 and perplexity is 47.24696468831563
At time: 54.34267258644104 and batch: 250, loss is 4.00499270439148 and perplexity is 54.871424077180784
At time: 54.853333473205566 and batch: 300, loss is 3.9727344036102297 and perplexity is 53.12961019445016
At time: 55.36470437049866 and batch: 350, loss is 3.9673216438293455 and perplexity is 52.842809269694264
At time: 55.902769327163696 and batch: 400, loss is 3.8965843105316162 and perplexity is 49.23399354013832
At time: 56.41589045524597 and batch: 450, loss is 3.9407824325561522 and perplexity is 51.45884863153401
At time: 56.92759847640991 and batch: 500, loss is 3.8104488897323607 and perplexity is 45.170710984369805
At time: 57.4315288066864 and batch: 550, loss is 3.8992001485824583 and perplexity is 49.36295028528259
At time: 57.90621018409729 and batch: 600, loss is 3.9179197788238525 and perplexity is 50.29570967940448
At time: 58.411856174468994 and batch: 650, loss is 3.764204907417297 and perplexity is 43.12940035732143
At time: 58.92613410949707 and batch: 700, loss is 3.766686019897461 and perplexity is 43.23654211121834
At time: 59.436190128326416 and batch: 750, loss is 3.888949489593506 and perplexity is 48.859532107345046
At time: 59.94472408294678 and batch: 800, loss is 3.8402257680892946 and perplexity is 46.535979593076576
At time: 60.45524740219116 and batch: 850, loss is 3.9169638633728026 and perplexity is 50.24765420554473
At time: 60.96462297439575 and batch: 900, loss is 3.8739375114440917 and perplexity is 48.13153191223114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326475378585188 and perplexity of 75.67708288295186
finished 6 epochs...
Completing Train Step...
At time: 62.392587661743164 and batch: 50, loss is 3.9694652366638183 and perplexity is 52.95620422988954
At time: 62.923354387283325 and batch: 100, loss is 3.8389692974090575 and perplexity is 46.477545217369276
At time: 63.433130979537964 and batch: 150, loss is 3.848363676071167 and perplexity is 46.91623023670203
At time: 63.94385886192322 and batch: 200, loss is 3.7564054250717165 and perplexity is 42.79432177900818
At time: 64.45669293403625 and batch: 250, loss is 3.904245638847351 and perplexity is 49.61263994406969
At time: 64.97000288963318 and batch: 300, loss is 3.8763086223602294 and perplexity is 48.245792521715195
At time: 65.48000144958496 and batch: 350, loss is 3.8702910423278807 and perplexity is 47.956341375160534
At time: 65.98907423019409 and batch: 400, loss is 3.8031691312789917 and perplexity is 44.843073128388724
At time: 66.49773073196411 and batch: 450, loss is 3.847631645202637 and perplexity is 46.88189867534813
At time: 67.0082061290741 and batch: 500, loss is 3.719188370704651 and perplexity is 41.230916305118214
At time: 67.51499032974243 and batch: 550, loss is 3.808553767204285 and perplexity is 45.08518801622216
At time: 68.02537775039673 and batch: 600, loss is 3.83062105178833 and perplexity is 46.09115434108224
At time: 68.53451919555664 and batch: 650, loss is 3.6745204067230226 and perplexity is 39.42974208264793
At time: 69.05862236022949 and batch: 700, loss is 3.6774978923797605 and perplexity is 39.54731852837454
At time: 69.56761527061462 and batch: 750, loss is 3.803700180053711 and perplexity is 44.86689331170799
At time: 70.07537007331848 and batch: 800, loss is 3.7540609455108642 and perplexity is 42.69410888571916
At time: 70.58575940132141 and batch: 850, loss is 3.82928653717041 and perplexity is 46.02968604614889
At time: 71.09759426116943 and batch: 900, loss is 3.787560043334961 and perplexity is 44.148548207852684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3310789343428935 and perplexity of 76.0262696873612
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 72.53082728385925 and batch: 50, loss is 3.90179753780365 and perplexity is 49.49133173640209
At time: 73.0381600856781 and batch: 100, loss is 3.7624520206451417 and perplexity is 43.05386562320297
At time: 73.54687404632568 and batch: 150, loss is 3.7757090377807616 and perplexity is 43.62843155784438
At time: 74.05736589431763 and batch: 200, loss is 3.6636704206466675 and perplexity is 39.00424243689415
At time: 74.56608605384827 and batch: 250, loss is 3.803450026512146 and perplexity is 44.85567110314317
At time: 75.07530093193054 and batch: 300, loss is 3.7667566967010497 and perplexity is 43.239598039803354
At time: 75.58537721633911 and batch: 350, loss is 3.7504456186294557 and perplexity is 42.54003440880621
At time: 76.09783673286438 and batch: 400, loss is 3.66957941532135 and perplexity is 39.235400581268884
At time: 76.60746240615845 and batch: 450, loss is 3.707319278717041 and perplexity is 40.74443552043341
At time: 77.11529016494751 and batch: 500, loss is 3.572987356185913 and perplexity is 35.62285250412539
At time: 77.62695741653442 and batch: 550, loss is 3.6469345474243164 and perplexity is 38.35690437352649
At time: 78.13869214057922 and batch: 600, loss is 3.6622421979904174 and perplexity is 38.94857545604009
At time: 78.65128540992737 and batch: 650, loss is 3.492425031661987 and perplexity is 32.86555114891136
At time: 79.1622416973114 and batch: 700, loss is 3.48379611492157 and perplexity is 32.58317708763828
At time: 79.67614078521729 and batch: 750, loss is 3.597539529800415 and perplexity is 36.50829626894441
At time: 80.1881275177002 and batch: 800, loss is 3.5342180156707763 and perplexity is 34.26820703311117
At time: 80.69823789596558 and batch: 850, loss is 3.5942435598373415 and perplexity is 36.388164105793464
At time: 81.21017241477966 and batch: 900, loss is 3.546682286262512 and perplexity is 34.69800825321091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.281522986007063 and perplexity of 72.35054500950719
finished 8 epochs...
Completing Train Step...
At time: 82.64292001724243 and batch: 50, loss is 3.8075528240203855 and perplexity is 45.040082882186596
At time: 83.1714220046997 and batch: 100, loss is 3.671948447227478 and perplexity is 39.328460684749274
At time: 83.68241763114929 and batch: 150, loss is 3.685420594215393 and perplexity is 39.86188460266059
At time: 84.19273042678833 and batch: 200, loss is 3.5807911586761474 and perplexity is 35.901933740308955
At time: 84.70801401138306 and batch: 250, loss is 3.7212810707092285 and perplexity is 41.31729059006969
At time: 85.21917057037354 and batch: 300, loss is 3.6911080837249757 and perplexity is 40.08924459407581
At time: 85.7330482006073 and batch: 350, loss is 3.678426513671875 and perplexity is 39.58406006725336
At time: 86.24618291854858 and batch: 400, loss is 3.602851219177246 and perplexity is 36.7027329348352
At time: 86.75967693328857 and batch: 450, loss is 3.6443587017059325 and perplexity is 38.25823004510718
At time: 87.26905369758606 and batch: 500, loss is 3.5144497299194337 and perplexity is 33.597435173026156
At time: 87.77801728248596 and batch: 550, loss is 3.592007431983948 and perplexity is 36.306886426011964
At time: 88.28833889961243 and batch: 600, loss is 3.6123546028137206 and perplexity is 37.05319574001971
At time: 88.79679989814758 and batch: 650, loss is 3.446350555419922 and perplexity is 31.385642885952947
At time: 89.30903649330139 and batch: 700, loss is 3.443984613418579 and perplexity is 31.311474049390522
At time: 89.82173156738281 and batch: 750, loss is 3.5629336214065552 and perplexity is 35.26650411064774
At time: 90.3318018913269 and batch: 800, loss is 3.50466917514801 and perplexity is 33.270435344711636
At time: 90.84164667129517 and batch: 850, loss is 3.5717755699157716 and perplexity is 35.57971136475693
At time: 91.3532395362854 and batch: 900, loss is 3.532956671714783 and perplexity is 34.2250102859923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.284368227605951 and perplexity of 72.55669292110642
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 92.79642963409424 and batch: 50, loss is 3.7807025623321535 and perplexity is 43.84683605223826
At time: 93.32402276992798 and batch: 100, loss is 3.6520743370056152 and perplexity is 38.55455830568135
At time: 93.83655190467834 and batch: 150, loss is 3.6629192638397217 and perplexity is 38.9749551357428
At time: 94.34873628616333 and batch: 200, loss is 3.5532109117507935 and perplexity is 34.92527963189575
At time: 94.86318635940552 and batch: 250, loss is 3.6935967206954956 and perplexity is 40.18913641597825
At time: 95.38819408416748 and batch: 300, loss is 3.6592875385284422 and perplexity is 38.83366552249674
At time: 95.89780521392822 and batch: 350, loss is 3.6424138069152834 and perplexity is 38.18389412399788
At time: 96.40836668014526 and batch: 400, loss is 3.5676407337188722 and perplexity is 35.432898818415616
At time: 96.91913151741028 and batch: 450, loss is 3.601886959075928 and perplexity is 36.66735901142918
At time: 97.43094539642334 and batch: 500, loss is 3.4717334079742432 and perplexity is 32.1924968389684
At time: 97.94154620170593 and batch: 550, loss is 3.5444214868545534 and perplexity is 34.61965162437953
At time: 98.45187067985535 and batch: 600, loss is 3.56701322555542 and perplexity is 35.4106713598375
At time: 98.9632043838501 and batch: 650, loss is 3.393017764091492 and perplexity is 29.755612334287186
At time: 99.47450613975525 and batch: 700, loss is 3.3860613012313845 and perplexity is 29.54933682799933
At time: 99.98854064941406 and batch: 750, loss is 3.501562852859497 and perplexity is 33.16724700095196
At time: 100.50160813331604 and batch: 800, loss is 3.4402447032928465 and perplexity is 31.194590653476155
At time: 101.00999450683594 and batch: 850, loss is 3.4992616748809815 and perplexity is 33.0910110124761
At time: 101.50961804389954 and batch: 900, loss is 3.466164264678955 and perplexity is 32.01371051645697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271009471318493 and perplexity of 71.59387111272378
finished 10 epochs...
Completing Train Step...
At time: 102.93651485443115 and batch: 50, loss is 3.753358302116394 and perplexity is 42.66412068886582
At time: 103.45235252380371 and batch: 100, loss is 3.622487359046936 and perplexity is 37.43055535791825
At time: 103.9791533946991 and batch: 150, loss is 3.631773142814636 and perplexity is 37.77974614725813
At time: 104.49137997627258 and batch: 200, loss is 3.5258848667144775 and perplexity is 33.98383147699046
At time: 104.9980731010437 and batch: 250, loss is 3.667388467788696 and perplexity is 39.14953197832378
At time: 105.50843048095703 and batch: 300, loss is 3.6341043281555176 and perplexity is 37.867920473075344
At time: 106.01610350608826 and batch: 350, loss is 3.6185643243789674 and perplexity is 37.28400164720927
At time: 106.5240547657013 and batch: 400, loss is 3.5461917448043825 and perplexity is 34.68099161567251
At time: 107.03518867492676 and batch: 450, loss is 3.5819343090057374 and perplexity is 35.94299851483044
At time: 107.54475474357605 and batch: 500, loss is 3.453409037590027 and perplexity is 31.607961579896813
At time: 108.07265591621399 and batch: 550, loss is 3.527438235282898 and perplexity is 34.03666191458881
At time: 108.58409190177917 and batch: 600, loss is 3.5528977584838866 and perplexity is 34.914344378775866
At time: 109.09666848182678 and batch: 650, loss is 3.380989122390747 and perplexity is 29.399836773008776
At time: 109.60741019248962 and batch: 700, loss is 3.3766480541229247 and perplexity is 29.272486692202946
At time: 110.11829829216003 and batch: 750, loss is 3.494534492492676 and perplexity is 32.934952916158295
At time: 110.62771153450012 and batch: 800, loss is 3.4358904361724854 and perplexity is 31.059056363452033
At time: 111.13838171958923 and batch: 850, loss is 3.4974187135696413 and perplexity is 33.030081721835224
At time: 111.64785146713257 and batch: 900, loss is 3.4675627946853638 and perplexity is 32.05851397341765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27122476656143 and perplexity of 71.60928659197789
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 113.06900382041931 and batch: 50, loss is 3.7459731674194336 and perplexity is 42.35020100718705
At time: 113.5939404964447 and batch: 100, loss is 3.6210957527160645 and perplexity is 37.37850298671903
At time: 114.10401439666748 and batch: 150, loss is 3.6285912227630615 and perplexity is 37.659725065456094
At time: 114.61719059944153 and batch: 200, loss is 3.521259331703186 and perplexity is 33.82700106657587
At time: 115.12845349311829 and batch: 250, loss is 3.664139471054077 and perplexity is 39.02254168399904
At time: 115.6406786441803 and batch: 300, loss is 3.630170130729675 and perplexity is 37.71923327201054
At time: 116.15086197853088 and batch: 350, loss is 3.6100764417648317 and perplexity is 36.968878673143955
At time: 116.66264939308167 and batch: 400, loss is 3.538661046028137 and perplexity is 34.42080045483143
At time: 117.17449498176575 and batch: 450, loss is 3.571029667854309 and perplexity is 35.55318227998322
At time: 117.6849958896637 and batch: 500, loss is 3.443148636817932 and perplexity is 31.285309327824745
At time: 118.19614481925964 and batch: 550, loss is 3.5145203685760498 and perplexity is 33.59980853453707
At time: 118.7079405784607 and batch: 600, loss is 3.5394812536239626 and perplexity is 34.449044238118276
At time: 119.21629476547241 and batch: 650, loss is 3.3668444633483885 and perplexity is 28.986913321162817
At time: 119.72796535491943 and batch: 700, loss is 3.361837778091431 and perplexity is 28.842147670020697
At time: 120.24012660980225 and batch: 750, loss is 3.476582269668579 and perplexity is 32.34897286245256
At time: 120.75044012069702 and batch: 800, loss is 3.418153133392334 and perplexity is 30.51300949200886
At time: 121.27643394470215 and batch: 850, loss is 3.474237380027771 and perplexity is 32.273206957156454
At time: 121.78723073005676 and batch: 900, loss is 3.4450430393218996 and perplexity is 31.344632469365337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266540945392766 and perplexity of 71.27466576337319
finished 12 epochs...
Completing Train Step...
At time: 123.21341013908386 and batch: 50, loss is 3.73695746421814 and perplexity is 41.97010017716427
At time: 123.73812961578369 and batch: 100, loss is 3.6094452714920044 and perplexity is 36.9455523781121
At time: 124.25049090385437 and batch: 150, loss is 3.617150163650513 and perplexity is 37.231313339928384
At time: 124.76038432121277 and batch: 200, loss is 3.511482310295105 and perplexity is 33.49788526079772
At time: 125.27059626579285 and batch: 250, loss is 3.655191559791565 and perplexity is 38.674928966947164
At time: 125.78007864952087 and batch: 300, loss is 3.6212602853775024 and perplexity is 37.384653477260414
At time: 126.28886532783508 and batch: 350, loss is 3.601830563545227 and perplexity is 36.665291194566684
At time: 126.80192852020264 and batch: 400, loss is 3.531509699821472 and perplexity is 34.17552346971065
At time: 127.31072211265564 and batch: 450, loss is 3.565074586868286 and perplexity is 35.34208936173874
At time: 127.82203698158264 and batch: 500, loss is 3.437151160240173 and perplexity is 31.098237956728255
At time: 128.33228635787964 and batch: 550, loss is 3.509510884284973 and perplexity is 33.431911710863616
At time: 128.8424243927002 and batch: 600, loss is 3.5358851099014283 and perplexity is 34.32538300897863
At time: 129.353422164917 and batch: 650, loss is 3.3639045763015747 and perplexity is 28.901820213513137
At time: 129.86211252212524 and batch: 700, loss is 3.359731993675232 and perplexity is 28.781476227835945
At time: 130.37209510803223 and batch: 750, loss is 3.4751229667663575 and perplexity is 32.30180034034112
At time: 130.88114881515503 and batch: 800, loss is 3.4180947446823122 and perplexity is 30.51122792875784
At time: 131.39060854911804 and batch: 850, loss is 3.475539622306824 and perplexity is 32.31526186863539
At time: 131.90487456321716 and batch: 900, loss is 3.4476658725738525 and perplexity is 31.42695212184566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266083181720891 and perplexity of 71.24204627723917
finished 13 epochs...
Completing Train Step...
At time: 133.36175775527954 and batch: 50, loss is 3.7320743465423583 and perplexity is 41.76565481077308
At time: 133.87257742881775 and batch: 100, loss is 3.6040145778656005 and perplexity is 36.74545622451794
At time: 134.4000105857849 and batch: 150, loss is 3.611441125869751 and perplexity is 37.01936395464133
At time: 134.90907192230225 and batch: 200, loss is 3.5060482120513914 and perplexity is 33.31634815329605
At time: 135.41977977752686 and batch: 250, loss is 3.649855918884277 and perplexity is 38.46912297555641
At time: 135.92899799346924 and batch: 300, loss is 3.616047387123108 and perplexity is 37.190278151972315
At time: 136.4418318271637 and batch: 350, loss is 3.5968829584121704 and perplexity is 36.484333833566225
At time: 136.94979286193848 and batch: 400, loss is 3.527140908241272 and perplexity is 34.02654339892363
At time: 137.46297407150269 and batch: 450, loss is 3.561025128364563 and perplexity is 35.19926241849776
At time: 137.97227096557617 and batch: 500, loss is 3.4334300804138183 and perplexity is 30.982733963897594
At time: 138.4845631122589 and batch: 550, loss is 3.506083993911743 and perplexity is 33.3175402955415
At time: 138.99237251281738 and batch: 600, loss is 3.533149199485779 and perplexity is 34.23160018528393
At time: 139.5011546611786 and batch: 650, loss is 3.3615607738494875 and perplexity is 28.834159379215645
At time: 140.01241445541382 and batch: 700, loss is 3.3579264068603516 and perplexity is 28.729555661496885
At time: 140.5217821598053 and batch: 750, loss is 3.473704071044922 and perplexity is 32.255999954713175
At time: 141.03178930282593 and batch: 800, loss is 3.417440733909607 and perplexity is 30.491279780866808
At time: 141.54197335243225 and batch: 850, loss is 3.4754062509536743 and perplexity is 32.31095222583042
At time: 142.0522119998932 and batch: 900, loss is 3.4481660604476927 and perplexity is 31.442675434186032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266187275925728 and perplexity of 71.24946254738605
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 143.4970076084137 and batch: 50, loss is 3.7301626777648926 and perplexity is 41.68588897971715
At time: 144.02233123779297 and batch: 100, loss is 3.604005002975464 and perplexity is 36.74510439249594
At time: 144.53296637535095 and batch: 150, loss is 3.6113154554367064 and perplexity is 37.014712007454335
At time: 145.04403638839722 and batch: 200, loss is 3.5060749292373656 and perplexity is 33.31723828425649
At time: 145.55318665504456 and batch: 250, loss is 3.64988365650177 and perplexity is 38.47019003217352
At time: 146.0637390613556 and batch: 300, loss is 3.616226010322571 and perplexity is 37.19692179178118
At time: 146.57546639442444 and batch: 350, loss is 3.5958750104904174 and perplexity is 36.44757805217074
At time: 147.10130953788757 and batch: 400, loss is 3.525750813484192 and perplexity is 33.97927613993913
At time: 147.61134839057922 and batch: 450, loss is 3.5583420181274414 and perplexity is 35.10494550508444
At time: 148.12160849571228 and batch: 500, loss is 3.430349555015564 and perplexity is 30.887437721622337
At time: 148.63335633277893 and batch: 550, loss is 3.5022442388534545 and perplexity is 33.18985439982431
At time: 149.14478015899658 and batch: 600, loss is 3.52780677318573 and perplexity is 34.04920802630706
At time: 149.65782928466797 and batch: 650, loss is 3.355915250778198 and perplexity is 28.671834103877305
At time: 150.16583132743835 and batch: 700, loss is 3.3532632303237917 and perplexity is 28.595896551846167
At time: 150.67757177352905 and batch: 750, loss is 3.4676066255569458 and perplexity is 32.05991915682171
At time: 151.19083309173584 and batch: 800, loss is 3.4096081590652467 and perplexity is 30.25338742094798
At time: 151.70163893699646 and batch: 850, loss is 3.4669706439971923 and perplexity is 32.03953612173503
At time: 152.21672081947327 and batch: 900, loss is 3.4393698930740357 and perplexity is 31.16731323982058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264852027370505 and perplexity of 71.15439029213026
finished 15 epochs...
Completing Train Step...
At time: 153.64535355567932 and batch: 50, loss is 3.7275218868255617 and perplexity is 41.57595048797853
At time: 154.17140221595764 and batch: 100, loss is 3.6008456707000733 and perplexity is 36.629197588688385
At time: 154.67967247962952 and batch: 150, loss is 3.608004550933838 and perplexity is 36.89236248635381
At time: 155.18720364570618 and batch: 200, loss is 3.5032446575164795 and perplexity is 33.223074763954436
At time: 155.69673418998718 and batch: 250, loss is 3.6473547410964966 and perplexity is 38.373025088702725
At time: 156.20959639549255 and batch: 300, loss is 3.613611397743225 and perplexity is 37.0997932841928
At time: 156.72062277793884 and batch: 350, loss is 3.5932575273513794 and perplexity is 36.35230187744734
At time: 157.2321422100067 and batch: 400, loss is 3.523606505393982 and perplexity is 33.906492166732804
At time: 157.74175453186035 and batch: 450, loss is 3.5566021966934205 and perplexity is 35.04392226861495
At time: 158.25261759757996 and batch: 500, loss is 3.4285916328430175 and perplexity is 30.833187707620283
At time: 158.76373147964478 and batch: 550, loss is 3.500709686279297 and perplexity is 33.13896188192835
At time: 159.27220153808594 and batch: 600, loss is 3.5271147298812866 and perplexity is 34.02565265148067
At time: 159.78220534324646 and batch: 650, loss is 3.3554200553894042 and perplexity is 28.657639458692287
At time: 160.30663084983826 and batch: 700, loss is 3.35304181098938 and perplexity is 28.589565568391684
At time: 160.81861114501953 and batch: 750, loss is 3.467635416984558 and perplexity is 32.060842220951464
At time: 161.33091640472412 and batch: 800, loss is 3.41013418674469 and perplexity is 30.269305726495933
At time: 161.8414764404297 and batch: 850, loss is 3.4679159784317015 and perplexity is 32.06983851918983
At time: 162.35215282440186 and batch: 900, loss is 3.440760531425476 and perplexity is 31.210685851742234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264430215913955 and perplexity of 71.12438288430032
finished 16 epochs...
Completing Train Step...
At time: 163.81295800209045 and batch: 50, loss is 3.7259462594985964 and perplexity is 41.51049386540863
At time: 164.32537484169006 and batch: 100, loss is 3.598908166885376 and perplexity is 36.558297085807965
At time: 164.83415293693542 and batch: 150, loss is 3.6061032485961912 and perplexity is 36.822285591109946
At time: 165.34338402748108 and batch: 200, loss is 3.5014999532699584 and perplexity is 33.16516086033885
At time: 165.853453874588 and batch: 250, loss is 3.645654168128967 and perplexity is 38.307824414519615
At time: 166.36456394195557 and batch: 300, loss is 3.6119116735458374 and perplexity is 37.03678742928664
At time: 166.8755943775177 and batch: 350, loss is 3.59164430141449 and perplexity is 36.29370467916131
At time: 167.386132478714 and batch: 400, loss is 3.5222215127944945 and perplexity is 33.85956443079221
At time: 167.895516872406 and batch: 450, loss is 3.555387043952942 and perplexity is 35.001364412817395
At time: 168.4034242630005 and batch: 500, loss is 3.4274348497390745 and perplexity is 30.797541018758142
At time: 168.91528606414795 and batch: 550, loss is 3.4996993207931517 and perplexity is 33.10549632766358
At time: 169.4259548187256 and batch: 600, loss is 3.5265332412719728 and perplexity is 34.00587287345452
At time: 169.93574619293213 and batch: 650, loss is 3.3550189781188964 and perplexity is 28.646147835547467
At time: 170.45187902450562 and batch: 700, loss is 3.352816228866577 and perplexity is 28.583117000868878
At time: 170.95924305915833 and batch: 750, loss is 3.467541980743408 and perplexity is 32.05784671631277
At time: 171.46771574020386 and batch: 800, loss is 3.4102918195724485 and perplexity is 30.274077538839137
At time: 171.97900581359863 and batch: 850, loss is 3.4683359336853026 and perplexity is 32.0833092447111
At time: 172.49019241333008 and batch: 900, loss is 3.4414048385620117 and perplexity is 31.23080159903656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264290587542808 and perplexity of 71.11445259586048
finished 17 epochs...
Completing Train Step...
At time: 173.9262557029724 and batch: 50, loss is 3.724609441757202 and perplexity is 41.455038975554146
At time: 174.4535367488861 and batch: 100, loss is 3.5973736333847044 and perplexity is 36.50224017580566
At time: 174.96237087249756 and batch: 150, loss is 3.604590559005737 and perplexity is 36.76662701069046
At time: 175.4711947441101 and batch: 200, loss is 3.500085291862488 and perplexity is 33.11827655772294
At time: 175.9824891090393 and batch: 250, loss is 3.6442552280426024 and perplexity is 38.254271530696414
At time: 176.4915714263916 and batch: 300, loss is 3.6105381631851197 and perplexity is 36.985951937554006
At time: 177.00106239318848 and batch: 350, loss is 3.5903432893753053 and perplexity is 36.24651683505972
At time: 177.51222157478333 and batch: 400, loss is 3.521086254119873 and perplexity is 33.82114687760959
At time: 178.02330470085144 and batch: 450, loss is 3.5543461990356446 and perplexity is 34.964952373500154
At time: 178.53191137313843 and batch: 500, loss is 3.426471858024597 and perplexity is 30.767897517444705
At time: 179.04314303398132 and batch: 550, loss is 3.4988439178466795 and perplexity is 33.077189896988585
At time: 179.55499720573425 and batch: 600, loss is 3.525959300994873 and perplexity is 33.98636113317693
At time: 180.06466221809387 and batch: 650, loss is 3.3545761346817016 and perplexity is 28.633464885465457
At time: 180.57483315467834 and batch: 700, loss is 3.3525199556350707 and perplexity is 28.574649842786584
At time: 181.0852086544037 and batch: 750, loss is 3.4673338651657106 and perplexity is 32.051175673221735
At time: 181.59523701667786 and batch: 800, loss is 3.410269093513489 and perplexity is 30.273389536185867
At time: 182.10461235046387 and batch: 850, loss is 3.4684949779510497 and perplexity is 32.08841231686909
At time: 182.6160819530487 and batch: 900, loss is 3.4417224645614626 and perplexity is 31.24072288915454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264263832405822 and perplexity of 71.11254994439257
finished 18 epochs...
Completing Train Step...
At time: 184.0445020198822 and batch: 50, loss is 3.7233853483200074 and perplexity is 41.40432517995019
At time: 184.5748951435089 and batch: 100, loss is 3.596030044555664 and perplexity is 36.45322910640619
At time: 185.08578777313232 and batch: 150, loss is 3.603247685432434 and perplexity is 36.71728721486685
At time: 185.59689259529114 and batch: 200, loss is 3.4988230752944944 and perplexity is 33.07650049111654
At time: 186.10784149169922 and batch: 250, loss is 3.643010859489441 and perplexity is 38.20669872337091
At time: 186.63189816474915 and batch: 300, loss is 3.6093296432495117 and perplexity is 36.941280675792164
At time: 187.14439916610718 and batch: 350, loss is 3.5891865062713624 and perplexity is 36.20461171904616
At time: 187.65279459953308 and batch: 400, loss is 3.5200709819793703 and perplexity is 33.78682663456626
At time: 188.16066122055054 and batch: 450, loss is 3.553390703201294 and perplexity is 34.93155946309296
At time: 188.67531776428223 and batch: 500, loss is 3.425595188140869 and perplexity is 30.740936048186363
At time: 189.18722653388977 and batch: 550, loss is 3.4980527639389036 and perplexity is 33.05103109812577
At time: 189.69935822486877 and batch: 600, loss is 3.5253812503814697 and perplexity is 33.96672097332666
At time: 190.2077615261078 and batch: 650, loss is 3.354099040031433 and perplexity is 28.619807270796457
At time: 190.7184329032898 and batch: 700, loss is 3.352170424461365 and perplexity is 28.5646638571971
At time: 191.22935128211975 and batch: 750, loss is 3.467054877281189 and perplexity is 32.04223503074769
At time: 191.7374291419983 and batch: 800, loss is 3.4101479768753054 and perplexity is 30.269723147053668
At time: 192.24564266204834 and batch: 850, loss is 3.4685101747512816 and perplexity is 32.08889996176614
At time: 192.75408720970154 and batch: 900, loss is 3.4418708944320677 and perplexity is 31.24536028976646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264291841689855 and perplexity of 71.11454178389711
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 194.1968777179718 and batch: 50, loss is 3.722821078300476 and perplexity is 41.38096855091471
At time: 194.70595836639404 and batch: 100, loss is 3.5959430599212645 and perplexity is 36.45005837350421
At time: 195.21594214439392 and batch: 150, loss is 3.6031967639923095 and perplexity is 36.71541756532743
At time: 195.72648525238037 and batch: 200, loss is 3.498928828239441 and perplexity is 33.079998613417104
At time: 196.23464584350586 and batch: 250, loss is 3.643158402442932 and perplexity is 38.212336268423506
At time: 196.74494051933289 and batch: 300, loss is 3.60954035282135 and perplexity is 36.94906537735328
At time: 197.2539668083191 and batch: 350, loss is 3.5889990615844725 and perplexity is 36.197825992932536
At time: 197.76378083229065 and batch: 400, loss is 3.5198341369628907 and perplexity is 33.7788253406259
At time: 198.2733302116394 and batch: 450, loss is 3.552780785560608 and perplexity is 34.91026058470084
At time: 198.78408336639404 and batch: 500, loss is 3.424322052001953 and perplexity is 30.701823554606882
At time: 199.3087396621704 and batch: 550, loss is 3.4966156768798826 and perplexity is 33.00356800151678
At time: 199.8197901248932 and batch: 600, loss is 3.5234138679504396 and perplexity is 33.899961135843064
At time: 200.32892847061157 and batch: 650, loss is 3.351696238517761 and perplexity is 28.551122106020024
At time: 200.84159469604492 and batch: 700, loss is 3.3498844385147093 and perplexity is 28.499440015844492
At time: 201.35141468048096 and batch: 750, loss is 3.4649609994888304 and perplexity is 31.97521269917241
At time: 201.86347770690918 and batch: 800, loss is 3.4075709104537966 and perplexity is 30.191816488376674
At time: 202.37844228744507 and batch: 850, loss is 3.4656709671020507 and perplexity is 31.99792212513923
At time: 202.8904004096985 and batch: 900, loss is 3.4389166688919066 and perplexity is 31.15319066035731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263827807282748 and perplexity of 71.08154984495319
Finished Training.
Improved accuracyfrom -10000000 to -71.08154984495319
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8ad1a05b70>
ELAPSED
218.81396579742432


RESULTS SO FAR:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8072543144226074 and batch: 50, loss is 6.8109550952911375 and perplexity is 907.7373687436296
At time: 1.361083745956421 and batch: 100, loss is 6.033236169815064 and perplexity is 417.06253301101475
At time: 1.9007654190063477 and batch: 150, loss is 5.93540919303894 and perplexity is 378.19471916014413
At time: 2.4385592937469482 and batch: 200, loss is 5.778639822006226 and perplexity is 323.3191196462916
At time: 2.9760725498199463 and batch: 250, loss is 5.824383888244629 and perplexity is 338.45254423636754
At time: 3.515561103820801 and batch: 300, loss is 5.732920169830322 and perplexity is 308.8699053099854
At time: 4.056958436965942 and batch: 350, loss is 5.70725378036499 and perplexity is 301.0432013004069
At time: 4.591444492340088 and batch: 400, loss is 5.566881904602051 and perplexity is 261.61707911847145
At time: 5.1242125034332275 and batch: 450, loss is 5.56817156791687 and perplexity is 261.9546947263785
At time: 5.66025710105896 and batch: 500, loss is 5.516142883300781 and perplexity is 248.67402028765827
At time: 6.198218822479248 and batch: 550, loss is 5.559501523971558 and perplexity is 259.69335314221394
At time: 6.73784327507019 and batch: 600, loss is 5.482835140228271 and perplexity is 240.52767128089988
At time: 7.2777276039123535 and batch: 650, loss is 5.378942337036133 and perplexity is 216.7928603451992
At time: 7.813063383102417 and batch: 700, loss is 5.477611198425293 and perplexity is 239.27444496479222
At time: 8.348576784133911 and batch: 750, loss is 5.447833690643311 and perplexity is 232.25448547580325
At time: 8.885492324829102 and batch: 800, loss is 5.432192487716675 and perplexity is 228.65000860012356
At time: 9.422375440597534 and batch: 850, loss is 5.463010559082031 and perplexity is 235.80626552003184
At time: 9.958156824111938 and batch: 900, loss is 5.356638927459716 and perplexity is 212.01116268486388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.201709904082834 and perplexity of 181.58246517209375
finished 1 epochs...
Completing Train Step...
At time: 11.441738367080688 and batch: 50, loss is 5.105833702087402 and perplexity is 164.98155868145454
At time: 11.957188129425049 and batch: 100, loss is 4.938507108688355 and perplexity is 139.56174344601655
At time: 12.472795248031616 and batch: 150, loss is 4.892321281433105 and perplexity is 133.26255519154356
At time: 12.986835956573486 and batch: 200, loss is 4.764710893630982 and perplexity is 117.29720101745787
At time: 13.49750828742981 and batch: 250, loss is 4.850326976776123 and perplexity is 127.78216481623176
At time: 14.012043714523315 and batch: 300, loss is 4.784032430648804 and perplexity is 119.58559972721183
At time: 14.5211763381958 and batch: 350, loss is 4.759652624130249 and perplexity is 116.70537822449043
At time: 15.037050008773804 and batch: 400, loss is 4.620794124603272 and perplexity is 101.57466304880595
At time: 15.548367738723755 and batch: 450, loss is 4.636293840408325 and perplexity is 103.16130595147317
At time: 16.059972763061523 and batch: 500, loss is 4.532301454544068 and perplexity is 92.97228653939126
At time: 16.57243251800537 and batch: 550, loss is 4.60678542137146 and perplexity is 100.16165405786782
At time: 17.084500312805176 and batch: 600, loss is 4.555787696838379 and perplexity is 95.1817000390028
At time: 17.595032215118408 and batch: 650, loss is 4.411167802810669 and perplexity is 82.3655941347995
At time: 18.10647940635681 and batch: 700, loss is 4.4523377609252925 and perplexity is 85.82735348950345
At time: 18.618102073669434 and batch: 750, loss is 4.5044385528564455 and perplexity is 90.41756511169928
At time: 19.129498958587646 and batch: 800, loss is 4.4471758842468265 and perplexity is 85.38546474393227
At time: 19.644091844558716 and batch: 850, loss is 4.51283332824707 and perplexity is 91.1797951608747
At time: 20.15549921989441 and batch: 900, loss is 4.441896743774414 and perplexity is 84.9358906078655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.550578078178511 and perplexity of 94.68712906006132
finished 2 epochs...
Completing Train Step...
At time: 21.59030532836914 and batch: 50, loss is 4.479514589309693 and perplexity is 88.19185301568007
At time: 22.11963415145874 and batch: 100, loss is 4.34417555809021 and perplexity is 77.02850576539547
At time: 22.632253885269165 and batch: 150, loss is 4.33902060508728 and perplexity is 76.63244914198086
At time: 23.144238471984863 and batch: 200, loss is 4.243435425758362 and perplexity is 69.64670741117021
At time: 23.667935609817505 and batch: 250, loss is 4.378990545272827 and perplexity is 79.75748118860362
At time: 24.19947624206543 and batch: 300, loss is 4.345092663764953 and perplexity is 77.09918144873133
At time: 24.714949369430542 and batch: 350, loss is 4.3338042163848876 and perplexity is 76.23374530122936
At time: 25.229121923446655 and batch: 400, loss is 4.238635973930359 and perplexity is 69.31324225904488
At time: 25.74758529663086 and batch: 450, loss is 4.271639895439148 and perplexity is 71.63901984589421
At time: 26.262852907180786 and batch: 500, loss is 4.147945227622986 and perplexity is 63.30379168688876
At time: 26.76883292198181 and batch: 550, loss is 4.234934105873108 and perplexity is 69.05712812442715
At time: 27.275309324264526 and batch: 600, loss is 4.225185837745666 and perplexity is 68.38721129291135
At time: 27.78177833557129 and batch: 650, loss is 4.0767985486984255 and perplexity is 58.956421284804264
At time: 28.29867458343506 and batch: 700, loss is 4.092747440338135 and perplexity is 59.90424917018833
At time: 28.80974793434143 and batch: 750, loss is 4.189934120178223 and perplexity is 66.01844153397953
At time: 29.322330474853516 and batch: 800, loss is 4.14383906364441 and perplexity is 63.044388877466176
At time: 29.83196449279785 and batch: 850, loss is 4.217554187774658 and perplexity is 67.86729048407999
At time: 30.344923496246338 and batch: 900, loss is 4.161920986175537 and perplexity is 64.19472140280975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3978597562607025 and perplexity of 81.27673036891863
finished 3 epochs...
Completing Train Step...
At time: 31.776861667633057 and batch: 50, loss is 4.232898802757263 and perplexity is 68.9167188725481
At time: 32.304943561553955 and batch: 100, loss is 4.097826857566833 and perplexity is 60.20930193475426
At time: 32.81522870063782 and batch: 150, loss is 4.10145785331726 and perplexity is 60.42831903867323
At time: 33.32746410369873 and batch: 200, loss is 4.007061004638672 and perplexity is 54.98503210441881
At time: 33.840858936309814 and batch: 250, loss is 4.148710603713989 and perplexity is 63.352261441945444
At time: 34.35589814186096 and batch: 300, loss is 4.123894991874695 and perplexity is 61.79948257616899
At time: 34.86543321609497 and batch: 350, loss is 4.115550751686096 and perplexity is 61.285958314478044
At time: 35.37811899185181 and batch: 400, loss is 4.035959033966065 and perplexity is 56.597172830253
At time: 35.88835549354553 and batch: 450, loss is 4.075218462944031 and perplexity is 58.86333864204737
At time: 36.400678634643555 and batch: 500, loss is 3.9477638387680054 and perplexity is 51.81936073345915
At time: 36.91130304336548 and batch: 550, loss is 4.0323557901382445 and perplexity is 56.39360638659424
At time: 37.44326734542847 and batch: 600, loss is 4.039860091209412 and perplexity is 56.81839285672037
At time: 37.955427169799805 and batch: 650, loss is 3.89041401386261 and perplexity is 48.93114050120967
At time: 38.468292236328125 and batch: 700, loss is 3.894637498855591 and perplexity is 49.13823746642177
At time: 38.979838609695435 and batch: 750, loss is 4.008866591453552 and perplexity is 55.084402036925596
At time: 39.49433898925781 and batch: 800, loss is 3.966258029937744 and perplexity is 52.7866348029511
At time: 40.00556993484497 and batch: 850, loss is 4.040120797157288 and perplexity is 56.83320768075934
At time: 40.514565229415894 and batch: 900, loss is 3.9936964464187623 and perplexity is 54.25507011630962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344049323095034 and perplexity of 77.0187826860511
finished 4 epochs...
Completing Train Step...
At time: 41.961403131484985 and batch: 50, loss is 4.075605053901672 and perplexity is 58.88609907570839
At time: 42.474135875701904 and batch: 100, loss is 3.9400227737426756 and perplexity is 51.41977230785574
At time: 42.98587656021118 and batch: 150, loss is 3.947509913444519 and perplexity is 51.806204155986826
At time: 43.4979887008667 and batch: 200, loss is 3.8556406688690186 and perplexity is 47.25888454059482
At time: 44.009084939956665 and batch: 250, loss is 3.998751330375671 and perplexity is 54.530017528000656
At time: 44.52095007896423 and batch: 300, loss is 3.9783900928497316 and perplexity is 53.43094608697726
At time: 45.02994918823242 and batch: 350, loss is 3.9667847394943236 and perplexity is 52.81444535135935
At time: 45.543784379959106 and batch: 400, loss is 3.9003407049179075 and perplexity is 49.41928363053699
At time: 46.05410814285278 and batch: 450, loss is 3.938143949508667 and perplexity is 51.32325429212224
At time: 46.564735412597656 and batch: 500, loss is 3.81552538394928 and perplexity is 45.400602866152475
At time: 47.0730459690094 and batch: 550, loss is 3.895088458061218 and perplexity is 49.16040180418601
At time: 47.585525035858154 and batch: 600, loss is 3.9095670318603517 and perplexity is 49.87735199339055
At time: 48.09955024719238 and batch: 650, loss is 3.7629258251190185 and perplexity is 43.07426957071203
At time: 48.60988807678223 and batch: 700, loss is 3.760712924003601 and perplexity is 42.979055859651496
At time: 49.119659662246704 and batch: 750, loss is 3.8793042945861815 and perplexity is 48.39053779884887
At time: 49.627466440200806 and batch: 800, loss is 3.8417122983932495 and perplexity is 46.60520817941337
At time: 50.154454708099365 and batch: 850, loss is 3.9159398555755613 and perplexity is 50.19622655149843
At time: 50.667590856552124 and batch: 900, loss is 3.870206298828125 and perplexity is 47.95227755915035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32404567770762 and perplexity of 75.49343340525495
finished 5 epochs...
Completing Train Step...
At time: 52.10273218154907 and batch: 50, loss is 3.9555499219894408 and perplexity is 52.22440539809213
At time: 52.62843084335327 and batch: 100, loss is 3.825962061882019 and perplexity is 45.87691557388929
At time: 53.13780903816223 and batch: 150, loss is 3.8367271280288695 and perplexity is 46.37345143017717
At time: 53.64739656448364 and batch: 200, loss is 3.7438471364974975 and perplexity is 42.260258814115666
At time: 54.16044282913208 and batch: 250, loss is 3.8852803897857666 and perplexity is 48.680590086298714
At time: 54.673338890075684 and batch: 300, loss is 3.8670714902877807 and perplexity is 47.80219171802533
At time: 55.18656921386719 and batch: 350, loss is 3.857430648803711 and perplexity is 47.34355275025002
At time: 55.700459718704224 and batch: 400, loss is 3.798166151046753 and perplexity is 44.619284392068856
At time: 56.21269464492798 and batch: 450, loss is 3.8308537769317628 and perplexity is 46.101882159855414
At time: 56.72593450546265 and batch: 500, loss is 3.7178679752349852 and perplexity is 41.17651111610569
At time: 57.23559808731079 and batch: 550, loss is 3.7940293836593626 and perplexity is 44.435086047079956
At time: 57.748685359954834 and batch: 600, loss is 3.8095831727981566 and perplexity is 45.13162285700876
At time: 58.25892639160156 and batch: 650, loss is 3.6638665103912356 and perplexity is 39.01189151875942
At time: 58.77100586891174 and batch: 700, loss is 3.6599939584732057 and perplexity is 38.86110809019747
At time: 59.28033638000488 and batch: 750, loss is 3.7819755411148073 and perplexity is 43.90268768564923
At time: 59.79187774658203 and batch: 800, loss is 3.7429709148406984 and perplexity is 42.2232456783445
At time: 60.30441069602966 and batch: 850, loss is 3.8213124561309812 and perplexity is 45.66410113840846
At time: 60.820213317871094 and batch: 900, loss is 3.77554753780365 and perplexity is 43.621386136079416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320914490582192 and perplexity of 75.25741903393278
finished 6 epochs...
Completing Train Step...
At time: 62.25269818305969 and batch: 50, loss is 3.8634495401382445 and perplexity is 47.62936773144899
At time: 62.776862382888794 and batch: 100, loss is 3.7378987407684328 and perplexity is 42.00962424689629
At time: 63.29041814804077 and batch: 150, loss is 3.7493524980545043 and perplexity is 42.49355842848786
At time: 63.81547713279724 and batch: 200, loss is 3.657011880874634 and perplexity is 38.74539387044909
At time: 64.32764410972595 and batch: 250, loss is 3.7963699531555175 and perplexity is 44.53921126265699
At time: 64.8398015499115 and batch: 300, loss is 3.782939763069153 and perplexity is 43.9450400362223
At time: 65.3521580696106 and batch: 350, loss is 3.770659613609314 and perplexity is 43.40868835618422
At time: 65.86305117607117 and batch: 400, loss is 3.712703242301941 and perplexity is 40.964393669567144
At time: 66.37574076652527 and batch: 450, loss is 3.7470428943634033 and perplexity is 42.39552839793016
At time: 66.88724851608276 and batch: 500, loss is 3.6360448932647706 and perplexity is 37.94147698582324
At time: 67.39680457115173 and batch: 550, loss is 3.709732484817505 and perplexity is 40.84287897518976
At time: 67.90725302696228 and batch: 600, loss is 3.727042212486267 and perplexity is 41.55601235368566
At time: 68.41938042640686 and batch: 650, loss is 3.581943082809448 and perplexity is 35.94331387302763
At time: 68.92910194396973 and batch: 700, loss is 3.5818738603591918 and perplexity is 35.940825874884666
At time: 69.44070076942444 and batch: 750, loss is 3.705446901321411 and perplexity is 40.66821793667469
At time: 69.9502272605896 and batch: 800, loss is 3.6649518156051637 and perplexity is 39.05425431215198
At time: 70.46175909042358 and batch: 850, loss is 3.7456427907943723 and perplexity is 42.33621180168812
At time: 70.9700026512146 and batch: 900, loss is 3.695020804405212 and perplexity is 40.24640988188143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332328482849957 and perplexity of 76.12132757649997
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 72.41640949249268 and batch: 50, loss is 3.8112117862701416 and perplexity is 45.20518471166047
At time: 72.92828106880188 and batch: 100, loss is 3.683857421875 and perplexity is 39.799622283259744
At time: 73.43782043457031 and batch: 150, loss is 3.6992567014694213 and perplexity is 40.41725110882311
At time: 73.94889736175537 and batch: 200, loss is 3.5829791450500488 and perplexity is 35.9805726812228
At time: 74.45943808555603 and batch: 250, loss is 3.719401726722717 and perplexity is 41.23971410774099
At time: 74.96891403198242 and batch: 300, loss is 3.6939250564575197 and perplexity is 40.202334113227906
At time: 75.47986888885498 and batch: 350, loss is 3.6713033723831177 and perplexity is 39.303099065045025
At time: 75.99185228347778 and batch: 400, loss is 3.6052509689331056 and perplexity is 36.7909160756512
At time: 76.51996278762817 and batch: 450, loss is 3.6242007350921632 and perplexity is 37.494742947864836
At time: 77.03130149841309 and batch: 500, loss is 3.5071400117874147 and perplexity is 33.352742797631336
At time: 77.54421496391296 and batch: 550, loss is 3.5626466131210326 and perplexity is 35.256383784144816
At time: 78.05379867553711 and batch: 600, loss is 3.5750036668777465 and perplexity is 35.69475170370659
At time: 78.56640100479126 and batch: 650, loss is 3.4106993675231934 and perplexity is 30.28641819163385
At time: 79.08092331886292 and batch: 700, loss is 3.3998928451538086 and perplexity is 29.960889420885337
At time: 79.59005308151245 and batch: 750, loss is 3.5056984090805052 and perplexity is 33.30469603382477
At time: 80.1036946773529 and batch: 800, loss is 3.449644432067871 and perplexity is 31.489193770432
At time: 80.61242842674255 and batch: 850, loss is 3.5116534566879274 and perplexity is 33.50361879365002
At time: 81.12498545646667 and batch: 900, loss is 3.461378297805786 and perplexity is 31.860860018906266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2785230662724745 and perplexity of 72.13382441644863
finished 8 epochs...
Completing Train Step...
At time: 82.55564475059509 and batch: 50, loss is 3.719138293266296 and perplexity is 41.22885161814619
At time: 83.08067917823792 and batch: 100, loss is 3.590149383544922 and perplexity is 36.2394891054952
At time: 83.59377527236938 and batch: 150, loss is 3.606434464454651 and perplexity is 36.83448373604046
At time: 84.10574054718018 and batch: 200, loss is 3.496495590209961 and perplexity is 32.99960495089948
At time: 84.6166512966156 and batch: 250, loss is 3.6334101819992064 and perplexity is 37.84164372263921
At time: 85.12904977798462 and batch: 300, loss is 3.6177751350402834 and perplexity is 37.25458911815955
At time: 85.63923525810242 and batch: 350, loss is 3.5957900619506837 and perplexity is 36.44448201514215
At time: 86.15175533294678 and batch: 400, loss is 3.5363171768188475 and perplexity is 34.340217075828555
At time: 86.66232490539551 and batch: 450, loss is 3.5596545124053955 and perplexity is 35.1510507950338
At time: 87.17347025871277 and batch: 500, loss is 3.4468759298324585 and perplexity is 31.402136431910375
At time: 87.68512630462646 and batch: 550, loss is 3.505367374420166 and perplexity is 33.29367284971418
At time: 88.19572901725769 and batch: 600, loss is 3.524587802886963 and perplexity is 33.93978085287267
At time: 88.70887470245361 and batch: 650, loss is 3.365078101158142 and perplexity is 28.93575712694475
At time: 89.22345304489136 and batch: 700, loss is 3.3590090227127076 and perplexity is 28.760675576304187
At time: 89.7482008934021 and batch: 750, loss is 3.470803680419922 and perplexity is 32.16258049679359
At time: 90.26103258132935 and batch: 800, loss is 3.420494599342346 and perplexity is 30.584538373307268
At time: 90.77296614646912 and batch: 850, loss is 3.4888448238372805 and perplexity is 32.748096027856974
At time: 91.28428983688354 and batch: 900, loss is 3.4472561120986938 and perplexity is 31.41407723700103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.282960656571062 and perplexity of 72.45463606475084
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 92.72204160690308 and batch: 50, loss is 3.6938566303253175 and perplexity is 40.19958331711328
At time: 93.25042223930359 and batch: 100, loss is 3.57240668296814 and perplexity is 35.60217327225467
At time: 93.7626223564148 and batch: 150, loss is 3.591249041557312 and perplexity is 36.279362069349055
At time: 94.27419996261597 and batch: 200, loss is 3.476364097595215 and perplexity is 32.34191598980622
At time: 94.78446388244629 and batch: 250, loss is 3.615383520126343 and perplexity is 37.16559694713184
At time: 95.29510879516602 and batch: 300, loss is 3.596434302330017 and perplexity is 36.46796858674676
At time: 95.80854630470276 and batch: 350, loss is 3.569369478225708 and perplexity is 35.4942062247618
At time: 96.31676054000854 and batch: 400, loss is 3.510717787742615 and perplexity is 33.472285159229536
At time: 96.82768082618713 and batch: 450, loss is 3.524568781852722 and perplexity is 33.939135289278596
At time: 97.33874177932739 and batch: 500, loss is 3.409305090904236 and perplexity is 30.24421997170913
At time: 97.84961819648743 and batch: 550, loss is 3.46357008934021 and perplexity is 31.93076896710901
At time: 98.36121845245361 and batch: 600, loss is 3.4844888401031495 and perplexity is 32.605756094523166
At time: 98.8717942237854 and batch: 650, loss is 3.315104727745056 and perplexity is 27.525276493765706
At time: 99.382559299469 and batch: 700, loss is 3.3018595457077025 and perplexity is 27.163103017593016
At time: 99.89348840713501 and batch: 750, loss is 3.41211772441864 and perplexity is 30.329405620265632
At time: 100.40524888038635 and batch: 800, loss is 3.3568372392654418 and perplexity is 28.698281395004717
At time: 100.91546583175659 and batch: 850, loss is 3.4220852756500246 and perplexity is 30.63322718768447
At time: 101.42761325836182 and batch: 900, loss is 3.3811590576171877 and perplexity is 29.404833265456134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269788768193493 and perplexity of 71.5065295704182
finished 10 epochs...
Completing Train Step...
At time: 102.87207674980164 and batch: 50, loss is 3.6693027639389038 and perplexity is 39.22454755477886
At time: 103.38332939147949 and batch: 100, loss is 3.5402637100219727 and perplexity is 34.47600966144596
At time: 103.89928483963013 and batch: 150, loss is 3.5574249696731566 and perplexity is 35.072767325801124
At time: 104.41221761703491 and batch: 200, loss is 3.44439857006073 and perplexity is 31.32443832518771
At time: 104.921058177948 and batch: 250, loss is 3.5842406129837037 and perplexity is 36.02598965990315
At time: 105.43248844146729 and batch: 300, loss is 3.5679145002365114 and perplexity is 35.442600487670504
At time: 105.94488835334778 and batch: 350, loss is 3.542498254776001 and perplexity is 34.553133984759434
At time: 106.45661425590515 and batch: 400, loss is 3.487184705734253 and perplexity is 32.69377542258713
At time: 106.96728110313416 and batch: 450, loss is 3.502550892829895 and perplexity is 33.200033761344585
At time: 107.47874641418457 and batch: 500, loss is 3.3890579032897947 and perplexity is 29.638017235148357
At time: 107.98971557617188 and batch: 550, loss is 3.445254740715027 and perplexity is 31.35126887416945
At time: 108.49954748153687 and batch: 600, loss is 3.469164628982544 and perplexity is 32.109907551625895
At time: 109.00987672805786 and batch: 650, loss is 3.3023256731033324 and perplexity is 27.175767435446584
At time: 109.51992702484131 and batch: 700, loss is 3.292495379447937 and perplexity is 26.90993042991241
At time: 110.03133416175842 and batch: 750, loss is 3.4052882766723633 and perplexity is 30.12297822430066
At time: 110.54200005531311 and batch: 800, loss is 3.3530112504959106 and perplexity is 28.58869187051023
At time: 111.05490183830261 and batch: 850, loss is 3.4211986541748045 and perplexity is 30.60607914740075
At time: 111.56619358062744 and batch: 900, loss is 3.3828593158721922 and perplexity is 29.45487160294639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270437162216395 and perplexity of 71.55290901124137
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 113.01571106910706 and batch: 50, loss is 3.662569522857666 and perplexity is 38.96132638006417
At time: 113.54368019104004 and batch: 100, loss is 3.5377104330062865 and perplexity is 34.38809514120794
At time: 114.05434274673462 and batch: 150, loss is 3.556554594039917 and perplexity is 35.04225412463015
At time: 114.56943368911743 and batch: 200, loss is 3.4428735256195067 and perplexity is 31.276703572706538
At time: 115.0790638923645 and batch: 250, loss is 3.581354441642761 and perplexity is 35.922162384743935
At time: 115.60652709007263 and batch: 300, loss is 3.5647328376770018 and perplexity is 35.33001329489164
At time: 116.11612582206726 and batch: 350, loss is 3.537850217819214 and perplexity is 34.39290241063799
At time: 116.62589406967163 and batch: 400, loss is 3.4813058137893678 and perplexity is 32.502136114941194
At time: 117.13748979568481 and batch: 450, loss is 3.494443106651306 and perplexity is 32.93194326529741
At time: 117.64797639846802 and batch: 500, loss is 3.3794241762161255 and perplexity is 29.353863593074976
At time: 118.16009092330933 and batch: 550, loss is 3.4326000308990476 and perplexity is 30.957027430928147
At time: 118.67037606239319 and batch: 600, loss is 3.46042133808136 and perplexity is 31.83038504307051
At time: 119.1819920539856 and batch: 650, loss is 3.288380336761475 and perplexity is 26.799422445980376
At time: 119.69454622268677 and batch: 700, loss is 3.2750612831115724 and perplexity is 26.444846053356617
At time: 120.20358514785767 and batch: 750, loss is 3.3868393230438234 and perplexity is 29.572335802286307
At time: 120.71664023399353 and batch: 800, loss is 3.3327255201339723 and perplexity is 28.01459207982488
At time: 121.2276554107666 and batch: 850, loss is 3.3970087957382202 and perplexity is 29.87460521901897
At time: 121.7367570400238 and batch: 900, loss is 3.3624812269210818 and perplexity is 28.860712088169645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267217348699701 and perplexity of 71.32289249151668
finished 12 epochs...
Completing Train Step...
At time: 123.18696737289429 and batch: 50, loss is 3.6518182373046875 and perplexity is 38.544685759061935
At time: 123.71574378013611 and batch: 100, loss is 3.5267425107955934 and perplexity is 34.01299001094506
At time: 124.228435754776 and batch: 150, loss is 3.544731559753418 and perplexity is 34.63038790454512
At time: 124.74183583259583 and batch: 200, loss is 3.4305829572677613 and perplexity is 30.894647760538103
At time: 125.25160145759583 and batch: 250, loss is 3.5701811122894287 and perplexity is 35.52302622566367
At time: 125.7625036239624 and batch: 300, loss is 3.5546245765686035 and perplexity is 34.974687185595535
At time: 126.27348971366882 and batch: 350, loss is 3.528462586402893 and perplexity is 34.071545270694116
At time: 126.78481674194336 and batch: 400, loss is 3.473572497367859 and perplexity is 32.251756193381546
At time: 127.29911422729492 and batch: 450, loss is 3.486991605758667 and perplexity is 32.6874628648482
At time: 127.80893063545227 and batch: 500, loss is 3.3726546812057494 and perplexity is 29.15582383081892
At time: 128.31887650489807 and batch: 550, loss is 3.4266725301742555 and perplexity is 30.774072397122474
At time: 128.84671473503113 and batch: 600, loss is 3.455593023300171 and perplexity is 31.677068352947753
At time: 129.35880303382874 and batch: 650, loss is 3.284896926879883 and perplexity is 26.706231478412445
At time: 129.8767807483673 and batch: 700, loss is 3.273110017776489 and perplexity is 26.393295452767358
At time: 130.387921333313 and batch: 750, loss is 3.3859942293167116 and perplexity is 29.547354963865416
At time: 130.8996558189392 and batch: 800, loss is 3.3333884859085083 and perplexity is 28.03317095345958
At time: 131.41135501861572 and batch: 850, loss is 3.3990147495269776 and perplexity is 29.93459244222956
At time: 131.92477321624756 and batch: 900, loss is 3.365706596374512 and perplexity is 28.9539488279808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266838178242723 and perplexity of 71.29585408418426
finished 13 epochs...
Completing Train Step...
At time: 133.3669400215149 and batch: 50, loss is 3.646395559310913 and perplexity is 38.33623602849872
At time: 133.88022089004517 and batch: 100, loss is 3.520962038040161 and perplexity is 33.81694600824626
At time: 134.39262676239014 and batch: 150, loss is 3.5384840679168703 and perplexity is 34.41470926559813
At time: 134.90252494812012 and batch: 200, loss is 3.424435086250305 and perplexity is 30.70529410829743
At time: 135.41201639175415 and batch: 250, loss is 3.564169654846191 and perplexity is 35.31012163983671
At time: 135.92239117622375 and batch: 300, loss is 3.548929281234741 and perplexity is 34.77606216383351
At time: 136.43461060523987 and batch: 350, loss is 3.5229274559020998 and perplexity is 33.883475795967335
At time: 136.94306778907776 and batch: 400, loss is 3.4686955499649046 and perplexity is 32.09484899983654
At time: 137.45299744606018 and batch: 450, loss is 3.4823119831085205 and perplexity is 32.534855224828895
At time: 137.9676160812378 and batch: 500, loss is 3.3683778381347658 and perplexity is 29.031395218178428
At time: 138.4790871143341 and batch: 550, loss is 3.422928800582886 and perplexity is 30.65907797995238
At time: 138.989666223526 and batch: 600, loss is 3.4525316762924194 and perplexity is 31.580242139470577
At time: 139.5024290084839 and batch: 650, loss is 3.282451286315918 and perplexity is 26.64099743742119
At time: 140.01523995399475 and batch: 700, loss is 3.271425290107727 and perplexity is 26.348867372764467
At time: 140.52486062049866 and batch: 750, loss is 3.3848607254028322 and perplexity is 29.513881895879813
At time: 141.0354254245758 and batch: 800, loss is 3.332952923774719 and perplexity is 28.020963424463897
At time: 141.5602457523346 and batch: 850, loss is 3.3991731357574464 and perplexity is 29.93933404497949
At time: 142.070228099823 and batch: 900, loss is 3.366393632888794 and perplexity is 28.97384808303295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267034243230951 and perplexity of 71.3098340754246
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 143.5064570903778 and batch: 50, loss is 3.6440654945373536 and perplexity is 38.2470141021786
At time: 144.03166508674622 and batch: 100, loss is 3.5203542041778566 and perplexity is 33.796397169115515
At time: 144.54385614395142 and batch: 150, loss is 3.538874397277832 and perplexity is 34.42814495907568
At time: 145.05656099319458 and batch: 200, loss is 3.4249423503875733 and perplexity is 30.720873753986535
At time: 145.56499671936035 and batch: 250, loss is 3.5640375089645384 and perplexity is 35.30545586096987
At time: 146.07456707954407 and batch: 300, loss is 3.54840087890625 and perplexity is 34.75769126565052
At time: 146.58551716804504 and batch: 350, loss is 3.522786202430725 and perplexity is 33.87868997540376
At time: 147.0974142551422 and batch: 400, loss is 3.468026051521301 and perplexity is 32.07336873968617
At time: 147.61123180389404 and batch: 450, loss is 3.4814064931869506 and perplexity is 32.5054085751573
At time: 148.12161254882812 and batch: 500, loss is 3.365686321258545 and perplexity is 28.953361789261766
At time: 148.63475823402405 and batch: 550, loss is 3.4179917669296263 and perplexity is 30.508086112845394
At time: 149.14928650856018 and batch: 600, loss is 3.448433699607849 and perplexity is 31.45109185166352
At time: 149.6611053943634 and batch: 650, loss is 3.2777225971221924 and perplexity is 26.51531782482993
At time: 150.17142868041992 and batch: 700, loss is 3.2653646421432496 and perplexity is 26.18965910332147
At time: 150.67891597747803 and batch: 750, loss is 3.3775903367996216 and perplexity is 29.300082648870948
At time: 151.18822765350342 and batch: 800, loss is 3.3245941638946532 and perplexity is 27.787719094306784
At time: 151.699059009552 and batch: 850, loss is 3.3894756031036377 and perplexity is 29.650399615309066
At time: 152.20974445343018 and batch: 900, loss is 3.3569572877883913 and perplexity is 28.70172678810042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2662244822880995 and perplexity of 71.25211353002476
finished 15 epochs...
Completing Train Step...
At time: 153.66181063652039 and batch: 50, loss is 3.641529984474182 and perplexity is 38.15016125061354
At time: 154.18897318840027 and batch: 100, loss is 3.5175814199447633 and perplexity is 33.702816850881476
At time: 154.70045495033264 and batch: 150, loss is 3.5355788803100587 and perplexity is 34.31487317026013
At time: 155.22641253471375 and batch: 200, loss is 3.421412262916565 and perplexity is 30.61261757176509
At time: 155.7367012500763 and batch: 250, loss is 3.560966062545776 and perplexity is 35.197183406642196
At time: 156.24768662452698 and batch: 300, loss is 3.5452385568618774 and perplexity is 34.647949862632345
At time: 156.75900077819824 and batch: 350, loss is 3.519620032310486 and perplexity is 33.771593911136684
At time: 157.27307963371277 and batch: 400, loss is 3.46540406703949 and perplexity is 31.98938301731711
At time: 157.78422379493713 and batch: 450, loss is 3.4790288829803466 and perplexity is 32.42821518821622
At time: 158.2947278022766 and batch: 500, loss is 3.36371075630188 and perplexity is 28.896219005558777
At time: 158.8053069114685 and batch: 550, loss is 3.41649444103241 and perplexity is 30.462439747704646
At time: 159.31378054618835 and batch: 600, loss is 3.447344198226929 and perplexity is 31.416844503314014
At time: 159.82498264312744 and batch: 650, loss is 3.276854548454285 and perplexity is 26.492311225381577
At time: 160.33693480491638 and batch: 700, loss is 3.2651490211486816 and perplexity is 26.184012671744686
At time: 160.84592485427856 and batch: 750, loss is 3.3776874494552613 and perplexity is 29.302928195874518
At time: 161.3467457294464 and batch: 800, loss is 3.3252098751068115 and perplexity is 27.804833572760916
At time: 161.86319732666016 and batch: 850, loss is 3.390640082359314 and perplexity is 29.68494700153701
At time: 162.38917231559753 and batch: 900, loss is 3.358726530075073 and perplexity is 28.752552044673447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265969890437714 and perplexity of 71.2339756315758
finished 16 epochs...
Completing Train Step...
At time: 163.82919001579285 and batch: 50, loss is 3.639830484390259 and perplexity is 38.08538011173421
At time: 164.34163331985474 and batch: 100, loss is 3.5157830333709716 and perplexity is 33.642260625629994
At time: 164.85326027870178 and batch: 150, loss is 3.5335147094726564 and perplexity is 34.24411446392604
At time: 165.36476588249207 and batch: 200, loss is 3.4193349647521973 and perplexity is 30.549092041054546
At time: 165.8754858970642 and batch: 250, loss is 3.558991265296936 and perplexity is 35.127744691941786
At time: 166.38557982444763 and batch: 300, loss is 3.5433993911743165 and perplexity is 34.58428510506514
At time: 166.89527368545532 and batch: 350, loss is 3.5177894020080567 and perplexity is 33.70982716125306
At time: 167.4077126979828 and batch: 400, loss is 3.463812518119812 and perplexity is 31.938510842850278
At time: 167.93680500984192 and batch: 450, loss is 3.4775465965270995 and perplexity is 32.38018289174554
At time: 168.45074677467346 and batch: 500, loss is 3.36244460105896 and perplexity is 28.859655059065386
At time: 168.96410131454468 and batch: 550, loss is 3.4154943656921386 and perplexity is 30.431990241346526
At time: 169.47884464263916 and batch: 600, loss is 3.4466177082061766 and perplexity is 31.39402876800433
At time: 169.99143362045288 and batch: 650, loss is 3.2762950706481933 and perplexity is 26.47749351069667
At time: 170.5049512386322 and batch: 700, loss is 3.2649530220031737 and perplexity is 26.178881130540333
At time: 171.01759839057922 and batch: 750, loss is 3.3776475954055787 and perplexity is 29.301760378789627
At time: 171.52959871292114 and batch: 800, loss is 3.325471396446228 and perplexity is 27.81210608099572
At time: 172.03974032402039 and batch: 850, loss is 3.3911892127990724 and perplexity is 29.701252386019917
At time: 172.5524387359619 and batch: 900, loss is 3.3595395517349242 and perplexity is 28.77593799761672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26590927333048 and perplexity of 71.22965776490584
finished 17 epochs...
Completing Train Step...
At time: 174.00572896003723 and batch: 50, loss is 3.638379373550415 and perplexity is 38.03015408304983
At time: 174.5247344970703 and batch: 100, loss is 3.514251208305359 and perplexity is 33.590766017974445
At time: 175.04340410232544 and batch: 150, loss is 3.531836261749268 and perplexity is 34.18668571702164
At time: 175.5687394142151 and batch: 200, loss is 3.417698664665222 and perplexity is 30.499145434053688
At time: 176.08450031280518 and batch: 250, loss is 3.5573910760879515 and perplexity is 35.07157860411852
At time: 176.5970275402069 and batch: 300, loss is 3.5419280529022217 and perplexity is 34.533437339082184
At time: 177.1079502105713 and batch: 350, loss is 3.5163405990600585 and perplexity is 33.661023626174746
At time: 177.62118291854858 and batch: 400, loss is 3.4625414752960206 and perplexity is 31.897941416042126
At time: 178.13641619682312 and batch: 450, loss is 3.4763482189178467 and perplexity is 32.34140244703395
At time: 178.6479413509369 and batch: 500, loss is 3.361387734413147 and perplexity is 28.829170364189935
At time: 179.15981769561768 and batch: 550, loss is 3.414624085426331 and perplexity is 30.40551740185597
At time: 179.67071104049683 and batch: 600, loss is 3.445964632034302 and perplexity is 31.373532769329447
At time: 180.17894625663757 and batch: 650, loss is 3.275782766342163 and perplexity is 26.463932450749557
At time: 180.69036197662354 and batch: 700, loss is 3.2646816778182983 and perplexity is 26.171778607036817
At time: 181.22707629203796 and batch: 750, loss is 3.377497143745422 and perplexity is 29.29735221191146
At time: 181.73816657066345 and batch: 800, loss is 3.3255285358428956 and perplexity is 27.813695293360126
At time: 182.2498905658722 and batch: 850, loss is 3.391428747177124 and perplexity is 29.70836770918579
At time: 182.7591404914856 and batch: 900, loss is 3.3599395036697386 and perplexity is 28.787449291523654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265931429928297 and perplexity of 71.23123598926955
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 184.19314527511597 and batch: 50, loss is 3.6376663494110106 and perplexity is 38.00304733019634
At time: 184.71855068206787 and batch: 100, loss is 3.513964409828186 and perplexity is 33.58113361877816
At time: 185.2322804927826 and batch: 150, loss is 3.5317268991470336 and perplexity is 34.18294717654185
At time: 185.74354553222656 and batch: 200, loss is 3.4176141691207884 and perplexity is 30.496568501026687
At time: 186.2555446624756 and batch: 250, loss is 3.5571960973739625 and perplexity is 35.064741059434226
At time: 186.76551055908203 and batch: 300, loss is 3.5413011121749878 and perplexity is 34.51179370610936
At time: 187.27626419067383 and batch: 350, loss is 3.515922350883484 and perplexity is 33.64694790819834
At time: 187.78569293022156 and batch: 400, loss is 3.4620616912841795 and perplexity is 31.882640964489582
At time: 188.29643154144287 and batch: 450, loss is 3.476012420654297 and perplexity is 32.33054408346332
At time: 188.80952215194702 and batch: 500, loss is 3.3605329751968385 and perplexity is 28.804538893610534
At time: 189.31998801231384 and batch: 550, loss is 3.4129761028289796 and perplexity is 30.355450904003913
At time: 189.83253455162048 and batch: 600, loss is 3.4441771650314332 and perplexity is 31.317503704710873
At time: 190.3432765007019 and batch: 650, loss is 3.2739953422546386 and perplexity is 26.416672429868843
At time: 190.8567295074463 and batch: 700, loss is 3.2626522159576417 and perplexity is 26.118717841140477
At time: 191.36966395378113 and batch: 750, loss is 3.3750265550613405 and perplexity is 29.225059844187538
At time: 191.88499450683594 and batch: 800, loss is 3.3226838397979734 and perplexity is 27.734686216049695
At time: 192.3960943222046 and batch: 850, loss is 3.3882910060882567 and perplexity is 29.61529663596711
At time: 192.90926027297974 and batch: 900, loss is 3.356691770553589 and perplexity is 28.69410699660734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265735782989084 and perplexity of 71.2173011791678
finished 19 epochs...
Completing Train Step...
At time: 194.35608100891113 and batch: 50, loss is 3.637189440727234 and perplexity is 37.984927667970155
At time: 194.86913418769836 and batch: 100, loss is 3.5134280729293823 and perplexity is 33.56312764677997
At time: 195.38313174247742 and batch: 150, loss is 3.5311375427246094 and perplexity is 34.162807172489295
At time: 195.89273500442505 and batch: 200, loss is 3.4169950389862063 and perplexity is 30.477693000264967
At time: 196.40195274353027 and batch: 250, loss is 3.556671118736267 and perplexity is 35.046337650562634
At time: 196.91682386398315 and batch: 300, loss is 3.540737524032593 and perplexity is 34.49234874839231
At time: 197.42992639541626 and batch: 350, loss is 3.51536078453064 and perplexity is 33.628058218780616
At time: 197.94096422195435 and batch: 400, loss is 3.4615980339050294 and perplexity is 31.867861769245838
At time: 198.45432472229004 and batch: 450, loss is 3.4755490970611573 and perplexity is 32.31556804925331
At time: 198.9675703048706 and batch: 500, loss is 3.3601697444915772 and perplexity is 28.79407810058911
At time: 199.48272895812988 and batch: 550, loss is 3.412717733383179 and perplexity is 30.34760899607514
At time: 199.99629831314087 and batch: 600, loss is 3.4440445613861086 and perplexity is 31.313351164884207
At time: 200.50858783721924 and batch: 650, loss is 3.2738869762420655 and perplexity is 26.4138099155146
At time: 201.02239561080933 and batch: 700, loss is 3.2626223516464234 and perplexity is 26.117937835269476
At time: 201.53399753570557 and batch: 750, loss is 3.375090765953064 and perplexity is 29.226936471590115
At time: 202.0476951599121 and batch: 800, loss is 3.322806925773621 and perplexity is 27.738100177062922
At time: 202.55763983726501 and batch: 850, loss is 3.388552951812744 and perplexity is 29.623055252424162
At time: 203.06986117362976 and batch: 900, loss is 3.3570085906982423 and perplexity is 28.70319930797435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2656316887842465 and perplexity of 71.20988825665874
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8ad1a05b70>
ELAPSED
430.0063102245331


RESULTS SO FAR:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}, {'best_accuracy': -71.20988825665874, 'params': {'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.982441793390317, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9171483614956}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.798682689666748 and batch: 50, loss is 8.692491092681884 and perplexity is 5958.005716159517
At time: 1.357344627380371 and batch: 100, loss is 7.706203889846802 and perplexity is 2222.090925316441
At time: 1.9022600650787354 and batch: 150, loss is 7.33072226524353 and perplexity is 1526.483900194116
At time: 2.4445769786834717 and batch: 200, loss is 7.026053714752197 and perplexity is 1125.5799745745285
At time: 2.9843084812164307 and batch: 250, loss is 6.9660249614715575 and perplexity is 1060.000820827689
At time: 3.52803897857666 and batch: 300, loss is 6.815895385742188 and perplexity is 912.2329505951052
At time: 4.067704439163208 and batch: 350, loss is 6.761676845550537 and perplexity is 864.0899269885327
At time: 4.606945276260376 and batch: 400, loss is 6.651018686294556 and perplexity is 773.5719514399659
At time: 5.147343397140503 and batch: 450, loss is 6.617275714874268 and perplexity is 747.9048132580516
At time: 5.704437017440796 and batch: 500, loss is 6.570119829177856 and perplexity is 713.4553307765327
At time: 6.2446653842926025 and batch: 550, loss is 6.566801147460938 and perplexity is 711.0915241448828
At time: 6.785933971405029 and batch: 600, loss is 6.509289922714234 and perplexity is 671.3495382426189
At time: 7.328061819076538 and batch: 650, loss is 6.432036552429199 and perplexity is 621.4382517465049
At time: 7.865772724151611 and batch: 700, loss is 6.501501045227051 and perplexity is 666.1407904205579
At time: 8.406250715255737 and batch: 750, loss is 6.447608852386475 and perplexity is 631.1912155590837
At time: 8.947101593017578 and batch: 800, loss is 6.448009176254272 and perplexity is 631.4439470516652
At time: 9.486346244812012 and batch: 850, loss is 6.468324184417725 and perplexity is 644.4029211458659
At time: 10.027441263198853 and batch: 900, loss is 6.367564926147461 and perplexity is 582.6373351492866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.438793861702697 and perplexity of 625.6517220361952
finished 1 epochs...
Completing Train Step...
At time: 11.49790358543396 and batch: 50, loss is 6.255738286972046 and perplexity is 520.9938757949263
At time: 12.008081912994385 and batch: 100, loss is 5.878288669586182 and perplexity is 357.19743552175254
At time: 12.519061803817749 and batch: 150, loss is 5.70677882194519 and perplexity is 300.90025224734114
At time: 13.030255317687988 and batch: 200, loss is 5.468860273361206 and perplexity is 237.18970722288992
At time: 13.539223194122314 and batch: 250, loss is 5.457202196121216 and perplexity is 234.44058715876002
At time: 14.05176329612732 and batch: 300, loss is 5.332815399169922 and perplexity is 207.0199983882081
At time: 14.562069416046143 and batch: 350, loss is 5.2822558879852295 and perplexity is 196.81336383636025
At time: 15.07435941696167 and batch: 400, loss is 5.109529418945312 and perplexity is 165.59241188408822
At time: 15.584195613861084 and batch: 450, loss is 5.092217168807983 and perplexity is 162.75030722660682
At time: 16.0990149974823 and batch: 500, loss is 4.998390226364136 and perplexity is 148.17443970548476
At time: 16.614521503448486 and batch: 550, loss is 5.038108129501342 and perplexity is 154.17805402482009
At time: 17.126983165740967 and batch: 600, loss is 4.941036701202393 and perplexity is 139.91522468073717
At time: 17.642253637313843 and batch: 650, loss is 4.815980157852173 and perplexity is 123.46777092638972
At time: 18.153964042663574 and batch: 700, loss is 4.866219053268432 and perplexity is 129.82911078390794
At time: 18.665600061416626 and batch: 750, loss is 4.864618425369263 and perplexity is 129.62146891008265
At time: 19.19154381752014 and batch: 800, loss is 4.80682713508606 and perplexity is 122.34282379536083
At time: 19.704280853271484 and batch: 850, loss is 4.841296892166138 and perplexity is 126.63347524387609
At time: 20.215713500976562 and batch: 900, loss is 4.755451774597168 and perplexity is 116.21614480831585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.828045152638056 and perplexity of 124.9664314251734
finished 2 epochs...
Completing Train Step...
At time: 21.64472794532776 and batch: 50, loss is 4.808634080886841 and perplexity is 122.56409049533679
At time: 22.173418283462524 and batch: 100, loss is 4.68890308380127 and perplexity is 108.73384245470538
At time: 22.684329748153687 and batch: 150, loss is 4.6964727401733395 and perplexity is 109.56004336149688
At time: 23.19708514213562 and batch: 200, loss is 4.586517066955566 and perplexity is 98.15197737269662
At time: 23.709325313568115 and batch: 250, loss is 4.7065971565246585 and perplexity is 110.67490901483417
At time: 24.221742153167725 and batch: 300, loss is 4.660015239715576 and perplexity is 105.6376920327731
At time: 24.73434567451477 and batch: 350, loss is 4.65120304107666 and perplexity is 104.71088132445892
At time: 25.247126817703247 and batch: 400, loss is 4.5240401268005375 and perplexity is 92.20737594816273
At time: 25.758651733398438 and batch: 450, loss is 4.563395099639893 and perplexity is 95.9085467731019
At time: 26.27184534072876 and batch: 500, loss is 4.462274160385132 and perplexity is 86.68441938124454
At time: 26.783393383026123 and batch: 550, loss is 4.526500186920166 and perplexity is 92.4344908801221
At time: 27.293389081954956 and batch: 600, loss is 4.489686641693115 and perplexity is 89.09352330838416
At time: 27.80549669265747 and batch: 650, loss is 4.355107851028443 and perplexity is 77.87522380690588
At time: 28.31720471382141 and batch: 700, loss is 4.3960196018219 and perplexity is 81.12730615673718
At time: 28.827580451965332 and batch: 750, loss is 4.4484389209747315 and perplexity is 85.49337785669971
At time: 29.338173627853394 and batch: 800, loss is 4.410307474136353 and perplexity is 82.29476312573117
At time: 29.849928855895996 and batch: 850, loss is 4.4666105651855466 and perplexity is 87.06113431747458
At time: 30.361710786819458 and batch: 900, loss is 4.399769477844238 and perplexity is 81.43209459913699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.569104338345462 and perplexity of 96.45767762850639
finished 3 epochs...
Completing Train Step...
At time: 31.797552347183228 and batch: 50, loss is 4.4975724792480465 and perplexity is 89.79887786081353
At time: 32.32679486274719 and batch: 100, loss is 4.370588169097901 and perplexity is 79.09013639608801
At time: 32.84100556373596 and batch: 150, loss is 4.377373933792114 and perplexity is 79.62864849310668
At time: 33.35521864891052 and batch: 200, loss is 4.276643867492676 and perplexity is 71.99839790814124
At time: 33.86953258514404 and batch: 250, loss is 4.410192575454712 and perplexity is 82.28530810913692
At time: 34.379393577575684 and batch: 300, loss is 4.382609896659851 and perplexity is 80.04667456930532
At time: 34.89121985435486 and batch: 350, loss is 4.374664459228516 and perplexity is 79.41318871868206
At time: 35.40480995178223 and batch: 400, loss is 4.269573526382446 and perplexity is 71.4911400317873
At time: 35.918254137039185 and batch: 450, loss is 4.315599627494812 and perplexity is 74.85849720197149
At time: 36.428303480148315 and batch: 500, loss is 4.207992525100708 and perplexity is 67.22145887829876
At time: 36.93206477165222 and batch: 550, loss is 4.2733789873123165 and perplexity is 71.76371507991622
At time: 37.429851770401 and batch: 600, loss is 4.2623281669616695 and perplexity is 70.97503297517966
At time: 37.939475536346436 and batch: 650, loss is 4.122166547775269 and perplexity is 61.69275788556364
At time: 38.43792366981506 and batch: 700, loss is 4.151117582321167 and perplexity is 63.504932644402665
At time: 38.951740980148315 and batch: 750, loss is 4.230907068252564 and perplexity is 68.77959167144003
At time: 39.45234727859497 and batch: 800, loss is 4.196240191459656 and perplexity is 66.43607395811797
At time: 39.947080850601196 and batch: 850, loss is 4.261033005714417 and perplexity is 70.8831683655304
At time: 40.44891333580017 and batch: 900, loss is 4.203699498176575 and perplexity is 66.93349390840721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.451528470810145 and perplexity of 85.75792235946825
finished 4 epochs...
Completing Train Step...
At time: 41.85227847099304 and batch: 50, loss is 4.310404348373413 and perplexity is 74.47059491720574
At time: 42.354804039001465 and batch: 100, loss is 4.183051981925964 and perplexity is 65.56565335471537
At time: 42.85913968086243 and batch: 150, loss is 4.184345626831055 and perplexity is 65.65052691441423
At time: 43.37384557723999 and batch: 200, loss is 4.08846188545227 and perplexity is 59.64807553762751
At time: 43.87135195732117 and batch: 250, loss is 4.229818558692932 and perplexity is 68.70476516047253
At time: 44.36957120895386 and batch: 300, loss is 4.20557062625885 and perplexity is 67.05885229272607
At time: 44.89197659492493 and batch: 350, loss is 4.196455407142639 and perplexity is 66.45037358184605
At time: 45.40363144874573 and batch: 400, loss is 4.106887145042419 and perplexity is 60.757294254297626
At time: 45.91670298576355 and batch: 450, loss is 4.155566954612732 and perplexity is 63.78811926625779
At time: 46.43224334716797 and batch: 500, loss is 4.037994813919068 and perplexity is 56.712509580385884
At time: 46.946656227111816 and batch: 550, loss is 4.105978755950928 and perplexity is 60.70212805095062
At time: 47.45861458778381 and batch: 600, loss is 4.111516671180725 and perplexity is 61.03922383277892
At time: 47.96838045120239 and batch: 650, loss is 3.9647607469558714 and perplexity is 52.70765741349556
At time: 48.481114864349365 and batch: 700, loss is 3.9842332220077514 and perplexity is 53.7440639090607
At time: 48.99295902252197 and batch: 750, loss is 4.080387606620788 and perplexity is 59.16839946913475
At time: 49.50412583351135 and batch: 800, loss is 4.04805860042572 and perplexity is 57.28613374324426
At time: 50.017367362976074 and batch: 850, loss is 4.1134164524078365 and perplexity is 61.15529522454933
At time: 50.527562379837036 and batch: 900, loss is 4.062096643447876 and perplexity is 58.0959900525653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393217321944563 and perplexity of 80.90028297899642
finished 5 epochs...
Completing Train Step...
At time: 52.000956773757935 and batch: 50, loss is 4.167716727256775 and perplexity is 64.5678576428713
At time: 52.531578063964844 and batch: 100, loss is 4.046946954727173 and perplexity is 57.22248724181626
At time: 53.04287910461426 and batch: 150, loss is 4.0451532649993895 and perplexity is 57.11993985084729
At time: 53.55517935752869 and batch: 200, loss is 3.951826362609863 and perplexity is 52.03030631751877
At time: 54.06779456138611 and batch: 250, loss is 4.094203109741211 and perplexity is 59.99151345138245
At time: 54.580225467681885 and batch: 300, loss is 4.071267809867859 and perplexity is 58.631248767321054
At time: 55.09152841567993 and batch: 350, loss is 4.061536898612976 and perplexity is 58.06348022165878
At time: 55.60406732559204 and batch: 400, loss is 3.9843140029907227 and perplexity is 53.74840558273212
At time: 56.12063789367676 and batch: 450, loss is 4.034367213249206 and perplexity is 56.50715194558938
At time: 56.63264465332031 and batch: 500, loss is 3.9120572090148924 and perplexity is 50.00171020858478
At time: 57.14870834350586 and batch: 550, loss is 3.9794619560241697 and perplexity is 53.48824745459009
At time: 57.66337537765503 and batch: 600, loss is 3.9937757873535156 and perplexity is 54.25937493505967
At time: 58.18883180618286 and batch: 650, loss is 3.847457208633423 and perplexity is 46.873721471007435
At time: 58.70067310333252 and batch: 700, loss is 3.860135555267334 and perplexity is 47.471785983360526
At time: 59.21342158317566 and batch: 750, loss is 3.9655841732025148 and perplexity is 52.75107615562158
At time: 59.72680449485779 and batch: 800, loss is 3.934044532775879 and perplexity is 51.113289545236185
At time: 60.2387375831604 and batch: 850, loss is 3.995413055419922 and perplexity is 54.34828484175819
At time: 60.75123405456543 and batch: 900, loss is 3.9510388803482055 and perplexity is 51.989349502724224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365062556854666 and perplexity of 78.65432015475359
finished 6 epochs...
Completing Train Step...
At time: 62.185210943222046 and batch: 50, loss is 4.061589632034302 and perplexity is 58.06654218835822
At time: 62.71259021759033 and batch: 100, loss is 3.937491765022278 and perplexity is 51.289792974544085
At time: 63.22301363945007 and batch: 150, loss is 3.938282446861267 and perplexity is 51.330362919220185
At time: 63.73790431022644 and batch: 200, loss is 3.8458119201660157 and perplexity is 46.796664085843
At time: 64.24665713310242 and batch: 250, loss is 3.9916108894348143 and perplexity is 54.14203598644406
At time: 64.75662064552307 and batch: 300, loss is 3.9685962200164795 and perplexity is 52.91020439703904
At time: 65.26856780052185 and batch: 350, loss is 3.9578987169265747 and perplexity is 52.347213986722025
At time: 65.78333330154419 and batch: 400, loss is 3.884530658721924 and perplexity is 48.64410641388504
At time: 66.29453134536743 and batch: 450, loss is 3.9334034299850464 and perplexity is 51.08053117452355
At time: 66.80499935150146 and batch: 500, loss is 3.8153719186782835 and perplexity is 45.39363598493105
At time: 67.31627821922302 and batch: 550, loss is 3.881153926849365 and perplexity is 48.480125325243066
At time: 67.82935571670532 and batch: 600, loss is 3.896589150428772 and perplexity is 49.23423182818028
At time: 68.34113621711731 and batch: 650, loss is 3.7553085136413573 and perplexity is 42.74740593426165
At time: 68.85179734230042 and batch: 700, loss is 3.7616389989852905 and perplexity is 43.01887612344891
At time: 69.36055111885071 and batch: 750, loss is 3.875086793899536 and perplexity is 48.186880436863404
At time: 69.87239384651184 and batch: 800, loss is 3.8410719442367554 and perplexity is 46.57537389391524
At time: 70.38370370864868 and batch: 850, loss is 3.9037743759155275 and perplexity is 49.589264854253436
At time: 70.91099643707275 and batch: 900, loss is 3.860671863555908 and perplexity is 47.49725232395119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360103659433861 and perplexity of 78.26524693379599
finished 7 epochs...
Completing Train Step...
At time: 72.35989475250244 and batch: 50, loss is 3.9707375955581665 and perplexity is 53.023626410862384
At time: 72.87210941314697 and batch: 100, loss is 3.845802946090698 and perplexity is 46.796244130939236
At time: 73.38494324684143 and batch: 150, loss is 3.8479167795181275 and perplexity is 46.89526821940314
At time: 73.89612603187561 and batch: 200, loss is 3.759521541595459 and perplexity is 42.92788185853709
At time: 74.40865993499756 and batch: 250, loss is 3.904004969596863 and perplexity is 49.60070114390829
At time: 74.92411041259766 and batch: 300, loss is 3.8817125749588013 and perplexity is 48.50721622203582
At time: 75.43550395965576 and batch: 350, loss is 3.871944751739502 and perplexity is 48.03571283881933
At time: 75.94765400886536 and batch: 400, loss is 3.7990705585479736 and perplexity is 44.65965666131183
At time: 76.45835208892822 and batch: 450, loss is 3.8496523332595824 and perplexity is 46.9767281462108
At time: 76.96819472312927 and batch: 500, loss is 3.735607695579529 and perplexity is 41.91348846713612
At time: 77.47922444343567 and batch: 550, loss is 3.798185691833496 and perplexity is 44.620156296508604
At time: 77.9902012348175 and batch: 600, loss is 3.8177289962768555 and perplexity is 45.50075850588001
At time: 78.49974870681763 and batch: 650, loss is 3.6749236488342287 and perplexity is 39.44564502124142
At time: 79.01138949394226 and batch: 700, loss is 3.6804247379302977 and perplexity is 39.663236976591634
At time: 79.52005577087402 and batch: 750, loss is 3.796393938064575 and perplexity is 44.54027954439989
At time: 80.03199744224548 and batch: 800, loss is 3.7624701929092406 and perplexity is 43.05464801652845
At time: 80.54477953910828 and batch: 850, loss is 3.828088655471802 and perplexity is 45.97458093893053
At time: 81.05572652816772 and batch: 900, loss is 3.782590379714966 and perplexity is 43.9296890525798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.35569198817423 and perplexity of 77.9207269065995
finished 8 epochs...
Completing Train Step...
At time: 82.49055528640747 and batch: 50, loss is 3.894579176902771 and perplexity is 49.1353717120236
At time: 83.01525044441223 and batch: 100, loss is 3.7682974195480345 and perplexity is 43.30626962442565
At time: 83.52711701393127 and batch: 150, loss is 3.7733684015274047 and perplexity is 43.52643268695788
At time: 84.05326461791992 and batch: 200, loss is 3.6863780212402344 and perplexity is 39.9000677240997
At time: 84.56416296958923 and batch: 250, loss is 3.8290454053878786 and perplexity is 46.01858816398304
At time: 85.07484602928162 and batch: 300, loss is 3.807258405685425 and perplexity is 45.02682420787211
At time: 85.58421778678894 and batch: 350, loss is 3.7966178464889526 and perplexity is 44.550253604810415
At time: 86.09494543075562 and batch: 400, loss is 3.72553530216217 and perplexity is 41.49343832820507
At time: 86.60426950454712 and batch: 450, loss is 3.779522557258606 and perplexity is 43.7951270776638
At time: 87.11399722099304 and batch: 500, loss is 3.669162278175354 and perplexity is 39.21903745132033
At time: 87.6230161190033 and batch: 550, loss is 3.7298996877670287 and perplexity is 41.67492744931303
At time: 88.13439559936523 and batch: 600, loss is 3.7471264028549194 and perplexity is 42.39906893238398
At time: 88.64532351493835 and batch: 650, loss is 3.6065048027038573 and perplexity is 36.83707470025775
At time: 89.15646266937256 and batch: 700, loss is 3.609952440261841 and perplexity is 36.96429476083649
At time: 89.66651344299316 and batch: 750, loss is 3.723518886566162 and perplexity is 41.40985461010493
At time: 90.18128490447998 and batch: 800, loss is 3.6937082290649412 and perplexity is 40.19361809091691
At time: 90.69201731681824 and batch: 850, loss is 3.757894549369812 and perplexity is 42.858095314966036
At time: 91.20282363891602 and batch: 900, loss is 3.7169551944732664 and perplexity is 41.13894313719678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.359696479692851 and perplexity of 78.2333853979456
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 92.63541007041931 and batch: 50, loss is 3.8584494733810426 and perplexity is 47.39181210510412
At time: 93.15980672836304 and batch: 100, loss is 3.7226053190231325 and perplexity is 41.372041186159905
At time: 93.67054200172424 and batch: 150, loss is 3.722388229370117 and perplexity is 41.36306071891279
At time: 94.17979574203491 and batch: 200, loss is 3.6164842414855958 and perplexity is 37.20652843647055
At time: 94.69064903259277 and batch: 250, loss is 3.7541281843185423 and perplexity is 42.69697968320894
At time: 95.20417952537537 and batch: 300, loss is 3.7250956535339355 and perplexity is 41.47519980452807
At time: 95.71748542785645 and batch: 350, loss is 3.7009805488586425 and perplexity is 40.486984369107326
At time: 96.23035645484924 and batch: 400, loss is 3.6315770530700684 and perplexity is 37.77233865277687
At time: 96.74398517608643 and batch: 450, loss is 3.6644038581848144 and perplexity is 39.0328601057978
At time: 97.26814842224121 and batch: 500, loss is 3.5474899864196776 and perplexity is 34.72604516111359
At time: 97.77927112579346 and batch: 550, loss is 3.5894758605957033 and perplexity is 36.21508919578665
At time: 98.2905867099762 and batch: 600, loss is 3.605296697616577 and perplexity is 36.7925985142746
At time: 98.79879713058472 and batch: 650, loss is 3.453090476989746 and perplexity is 31.597894132313478
At time: 99.31094431877136 and batch: 700, loss is 3.4400312948226928 and perplexity is 31.187934173907546
At time: 99.82362627983093 and batch: 750, loss is 3.5413933992385864 and perplexity is 34.51497884518132
At time: 100.3360242843628 and batch: 800, loss is 3.500628113746643 and perplexity is 33.13625876312974
At time: 100.84799933433533 and batch: 850, loss is 3.5453805446624758 and perplexity is 34.652869798105726
At time: 101.35792183876038 and batch: 900, loss is 3.5045180988311766 and perplexity is 33.26540934954429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.301659152932363 and perplexity of 73.8221744245223
finished 10 epochs...
Completing Train Step...
At time: 102.80371737480164 and batch: 50, loss is 3.767322268486023 and perplexity is 43.264060053308505
At time: 103.31568193435669 and batch: 100, loss is 3.633628077507019 and perplexity is 37.849890145211205
At time: 103.83037638664246 and batch: 150, loss is 3.63717574596405 and perplexity is 37.98440747694314
At time: 104.3406150341034 and batch: 200, loss is 3.5362185621261597 and perplexity is 34.336830792846165
At time: 104.85247850418091 and batch: 250, loss is 3.676998381614685 and perplexity is 39.52756914996394
At time: 105.36364483833313 and batch: 300, loss is 3.6524121522903443 and perplexity is 38.567584824927756
At time: 105.8751232624054 and batch: 350, loss is 3.630831952095032 and perplexity is 37.7442049289514
At time: 106.38949513435364 and batch: 400, loss is 3.566309275627136 and perplexity is 35.38575279201431
At time: 106.90015435218811 and batch: 450, loss is 3.6033288478851317 and perplexity is 36.72026740089158
At time: 107.4114077091217 and batch: 500, loss is 3.491464433670044 and perplexity is 32.83399572493854
At time: 107.92154908180237 and batch: 550, loss is 3.5362447404861452 and perplexity is 34.33772968652915
At time: 108.43362641334534 and batch: 600, loss is 3.56043824672699 and perplexity is 35.178610678384516
At time: 108.94454741477966 and batch: 650, loss is 3.4118285274505613 and perplexity is 30.320635716292564
At time: 109.45821809768677 and batch: 700, loss is 3.403781113624573 and perplexity is 30.07761218023178
At time: 109.98475909233093 and batch: 750, loss is 3.5107492637634277 and perplexity is 33.4733387501552
At time: 110.49629020690918 and batch: 800, loss is 3.4746288728713988 and perplexity is 32.28584416024698
At time: 111.00802397727966 and batch: 850, loss is 3.5257028579711913 and perplexity is 33.97764668539143
At time: 111.52084112167358 and batch: 900, loss is 3.4917234230041503 and perplexity is 32.84250048090039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303648648196703 and perplexity of 73.96918948533526
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 112.95249581336975 and batch: 50, loss is 3.747842903137207 and perplexity is 42.42945876310381
At time: 113.47798466682434 and batch: 100, loss is 3.627945885658264 and perplexity is 37.635429687712424
At time: 113.99154019355774 and batch: 150, loss is 3.6270837354660035 and perplexity is 37.60299627801554
At time: 114.50613570213318 and batch: 200, loss is 3.5201822328567505 and perplexity is 33.790585657766684
At time: 115.01575040817261 and batch: 250, loss is 3.6610069561004637 and perplexity is 38.900494246158665
At time: 115.52483129501343 and batch: 300, loss is 3.6340586519241334 and perplexity is 37.86619084867944
At time: 116.03807044029236 and batch: 350, loss is 3.6071339559555056 and perplexity is 36.86025815779574
At time: 116.54996705055237 and batch: 400, loss is 3.5449812746047975 and perplexity is 34.639036706536125
At time: 117.0582001209259 and batch: 450, loss is 3.5734863233566285 and perplexity is 35.64063157326923
At time: 117.56346249580383 and batch: 500, loss is 3.4579481983184817 and perplexity is 31.751761315960316
At time: 118.0690016746521 and batch: 550, loss is 3.4953558349609377 and perplexity is 32.96201490373704
At time: 118.57178282737732 and batch: 600, loss is 3.5273643636703493 and perplexity is 34.03414766435441
At time: 119.07548689842224 and batch: 650, loss is 3.374746685028076 and perplexity is 29.216881770169092
At time: 119.5839319229126 and batch: 700, loss is 3.3559337854385376 and perplexity is 28.67236553150863
At time: 120.08926105499268 and batch: 750, loss is 3.4583424615859983 and perplexity is 31.76428233725335
At time: 120.5954806804657 and batch: 800, loss is 3.421755247116089 and perplexity is 30.62311901671017
At time: 121.10046482086182 and batch: 850, loss is 3.4708862781524656 and perplexity is 32.16523716273117
At time: 121.60543179512024 and batch: 900, loss is 3.440061159133911 and perplexity is 31.18886559398801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.289437908015839 and perplexity of 72.92546615720018
finished 12 epochs...
Completing Train Step...
At time: 123.0129063129425 and batch: 50, loss is 3.7247148180007934 and perplexity is 41.45940758200895
At time: 123.53163957595825 and batch: 100, loss is 3.5938309621810913 and perplexity is 36.37315353144501
At time: 124.03512072563171 and batch: 150, loss is 3.5955487489700317 and perplexity is 36.435688549590125
At time: 124.5384840965271 and batch: 200, loss is 3.4904176664352415 and perplexity is 32.799644156210974
At time: 125.04117846488953 and batch: 250, loss is 3.6329543018341064 and perplexity is 37.82439639950625
At time: 125.54697108268738 and batch: 300, loss is 3.608605828285217 and perplexity is 36.91455169862261
At time: 126.05082607269287 and batch: 350, loss is 3.583123540878296 and perplexity is 35.985768500934235
At time: 126.55928325653076 and batch: 400, loss is 3.5229128789901734 and perplexity is 33.88298188312477
At time: 127.06940388679504 and batch: 450, loss is 3.5537828397750855 and perplexity is 34.94526009122114
At time: 127.58232283592224 and batch: 500, loss is 3.4399612092971803 and perplexity is 31.1857484277468
At time: 128.09069728851318 and batch: 550, loss is 3.4792754077911376 and perplexity is 32.43621053331476
At time: 128.6023769378662 and batch: 600, loss is 3.514193139076233 and perplexity is 33.58881548471956
At time: 129.11204719543457 and batch: 650, loss is 3.3638399124145506 and perplexity is 28.89995136990006
At time: 129.6199607849121 and batch: 700, loss is 3.347317819595337 and perplexity is 28.426386604146074
At time: 130.1322226524353 and batch: 750, loss is 3.4521059083938597 and perplexity is 31.56679914813901
At time: 130.64396834373474 and batch: 800, loss is 3.418060369491577 and perplexity is 30.51017911750487
At time: 131.15574550628662 and batch: 850, loss is 3.469638662338257 and perplexity is 32.12513232709483
At time: 131.6691906452179 and batch: 900, loss is 3.441040940284729 and perplexity is 31.219438831707624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.289455048025471 and perplexity of 72.92671611110461
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 133.13708353042603 and batch: 50, loss is 3.72034806728363 and perplexity is 41.278759394076744
At time: 133.65030360221863 and batch: 100, loss is 3.597969317436218 and perplexity is 36.523990455627164
At time: 134.1618709564209 and batch: 150, loss is 3.599362769126892 and perplexity is 36.574920347808984
At time: 134.67233991622925 and batch: 200, loss is 3.491671919822693 and perplexity is 32.8408090311965
At time: 135.18321919441223 and batch: 250, loss is 3.63409836769104 and perplexity is 37.86769476335319
At time: 135.69559502601624 and batch: 300, loss is 3.607701382637024 and perplexity is 36.881179586889324
At time: 136.220055103302 and batch: 350, loss is 3.579512190818787 and perplexity is 35.85604567197538
At time: 136.73137021064758 and batch: 400, loss is 3.5187733697891237 and perplexity is 33.743012869241284
At time: 137.2427954673767 and batch: 450, loss is 3.55004376411438 and perplexity is 34.81484109494991
At time: 137.7551076412201 and batch: 500, loss is 3.433640789985657 and perplexity is 30.989263010347653
At time: 138.268874168396 and batch: 550, loss is 3.470205068588257 and perplexity is 32.1433333569289
At time: 138.78351354599 and batch: 600, loss is 3.505221576690674 and perplexity is 33.28881906165048
At time: 139.2958266735077 and batch: 650, loss is 3.352270827293396 and perplexity is 28.56753197432551
At time: 139.8067455291748 and batch: 700, loss is 3.335784149169922 and perplexity is 28.100409499513013
At time: 140.31832194328308 and batch: 750, loss is 3.436675682067871 and perplexity is 31.08345493816261
At time: 140.8329541683197 and batch: 800, loss is 3.400946545600891 and perplexity is 29.992475861864104
At time: 141.34090733528137 and batch: 850, loss is 3.4516503763198854 and perplexity is 31.55242273336195
At time: 141.8542721271515 and batch: 900, loss is 3.424161853790283 and perplexity is 30.69690557131501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.283440158791738 and perplexity of 72.48938655444483
finished 14 epochs...
Completing Train Step...
At time: 143.28105473518372 and batch: 50, loss is 3.711520676612854 and perplexity is 40.91597921541757
At time: 143.8092770576477 and batch: 100, loss is 3.582487173080444 and perplexity is 35.96287560160383
At time: 144.3194260597229 and batch: 150, loss is 3.586031551361084 and perplexity is 36.09056779783999
At time: 144.82931327819824 and batch: 200, loss is 3.4798363637924195 and perplexity is 32.45441092460436
At time: 145.34411692619324 and batch: 250, loss is 3.6227302837371824 and perplexity is 37.4396492685073
At time: 145.85576558113098 and batch: 300, loss is 3.598026394844055 and perplexity is 36.52607520982185
At time: 146.36675453186035 and batch: 350, loss is 3.5706909704208374 and perplexity is 35.5411425474216
At time: 146.87720799446106 and batch: 400, loss is 3.5101003789901735 and perplexity is 33.45162545580515
At time: 147.3880271911621 and batch: 450, loss is 3.5423222589492798 and perplexity is 34.547053312480145
At time: 147.8980803489685 and batch: 500, loss is 3.4271913051605223 and perplexity is 30.790041357898225
At time: 148.41035437583923 and batch: 550, loss is 3.464298224449158 and perplexity is 31.954027347643784
At time: 148.9216091632843 and batch: 600, loss is 3.501003232002258 and perplexity is 33.148691110369
At time: 149.4481270313263 and batch: 650, loss is 3.349419980049133 and perplexity is 28.48620628316244
At time: 149.9587414264679 and batch: 700, loss is 3.3340082693099977 and perplexity is 28.05055083283168
At time: 150.4703176021576 and batch: 750, loss is 3.4352422046661375 and perplexity is 31.038929428721204
At time: 150.98120999336243 and batch: 800, loss is 3.4012867641448974 and perplexity is 30.002681594324255
At time: 151.49330735206604 and batch: 850, loss is 3.4534487915039063 and perplexity is 31.60921814505589
At time: 152.00510048866272 and batch: 900, loss is 3.427844910621643 and perplexity is 30.810172475265333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.282910490689212 and perplexity of 72.45100140520728
finished 15 epochs...
Completing Train Step...
At time: 153.43133997917175 and batch: 50, loss is 3.706866111755371 and perplexity is 40.72597567139641
At time: 153.9536097049713 and batch: 100, loss is 3.576931719779968 and perplexity is 35.76363946160356
At time: 154.45730805397034 and batch: 150, loss is 3.580351777076721 and perplexity is 35.8861625562774
At time: 154.96260833740234 and batch: 200, loss is 3.4740848302841187 and perplexity is 32.268284063210956
At time: 155.4694447517395 and batch: 250, loss is 3.617122230529785 and perplexity is 37.23027336768291
At time: 155.97821068763733 and batch: 300, loss is 3.5929453372955322 and perplexity is 36.3409548216052
At time: 156.48812985420227 and batch: 350, loss is 3.5658626556396484 and perplexity is 35.369952336207426
At time: 156.99820613861084 and batch: 400, loss is 3.5054992198944093 and perplexity is 33.29806275918884
At time: 157.508141040802 and batch: 450, loss is 3.538162069320679 and perplexity is 34.403629561446586
At time: 158.01989364624023 and batch: 500, loss is 3.4234196186065673 and perplexity is 30.67412970154035
At time: 158.53108191490173 and batch: 550, loss is 3.4607425355911254 and perplexity is 31.840610525593068
At time: 159.04303193092346 and batch: 600, loss is 3.498218464851379 and perplexity is 33.0565081378997
At time: 159.55399894714355 and batch: 650, loss is 3.3472252225875856 and perplexity is 28.423754527668432
At time: 160.0670623779297 and batch: 700, loss is 3.3324268579483034 and perplexity is 28.006226429837536
At time: 160.58097910881042 and batch: 750, loss is 3.4339529418945314 and perplexity is 30.998937877886572
At time: 161.09152626991272 and batch: 800, loss is 3.4008138179779053 and perplexity is 29.98849529600687
At time: 161.60211968421936 and batch: 850, loss is 3.453480153083801 and perplexity is 31.610209475620927
At time: 162.1291584968567 and batch: 900, loss is 3.4285525608062746 and perplexity is 30.831983015712307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.282966091208262 and perplexity of 72.45502983048135
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 163.59304571151733 and batch: 50, loss is 3.7055022859573366 and perplexity is 40.67047039349402
At time: 164.1026337146759 and batch: 100, loss is 3.5781097841262817 and perplexity is 35.805796156922
At time: 164.61369729042053 and batch: 150, loss is 3.5821849870681763 and perplexity is 35.95200976547062
At time: 165.12507438659668 and batch: 200, loss is 3.475030851364136 and perplexity is 32.29882498405046
At time: 165.63886094093323 and batch: 250, loss is 3.6193690490722656 and perplexity is 37.314017079459724
At time: 166.15122842788696 and batch: 300, loss is 3.5947984504699706 and perplexity is 36.40836116025428
At time: 166.6618194580078 and batch: 350, loss is 3.567703652381897 and perplexity is 35.435128279172964
At time: 167.17393016815186 and batch: 400, loss is 3.5033663177490233 and perplexity is 33.2271169368369
At time: 167.6855025291443 and batch: 450, loss is 3.538660855293274 and perplexity is 34.4207938895854
At time: 168.19926357269287 and batch: 500, loss is 3.422547974586487 and perplexity is 30.647404428968912
At time: 168.71078848838806 and batch: 550, loss is 3.4581338787078857 and perplexity is 31.75765754275563
At time: 169.2235403060913 and batch: 600, loss is 3.493839364051819 and perplexity is 32.91206684898764
At time: 169.73402190208435 and batch: 650, loss is 3.3400996017456053 and perplexity is 28.221937519666607
At time: 170.2443573474884 and batch: 700, loss is 3.3260284852981568 and perplexity is 27.827604211759216
At time: 170.75722670555115 and batch: 750, loss is 3.427634677886963 and perplexity is 30.803695849273147
At time: 171.2661018371582 and batch: 800, loss is 3.393075318336487 and perplexity is 29.757324945373004
At time: 171.77996182441711 and batch: 850, loss is 3.4450829553604128 and perplexity is 31.345883647893043
At time: 172.29114651679993 and batch: 900, loss is 3.4204438018798826 and perplexity is 30.582984795826516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280203623314426 and perplexity of 72.25515134288987
finished 17 epochs...
Completing Train Step...
At time: 173.74203038215637 and batch: 50, loss is 3.7020806884765625 and perplexity is 40.53155021445306
At time: 174.2768542766571 and batch: 100, loss is 3.573658514022827 and perplexity is 35.64676908575959
At time: 174.7892780303955 and batch: 150, loss is 3.5781432390213013 and perplexity is 35.8069940561112
At time: 175.3147099018097 and batch: 200, loss is 3.471894154548645 and perplexity is 32.19767208849738
At time: 175.8263714313507 and batch: 250, loss is 3.615613751411438 and perplexity is 37.174154615361985
At time: 176.33817958831787 and batch: 300, loss is 3.5911217451095583 and perplexity is 36.27474412936076
At time: 176.84835362434387 and batch: 350, loss is 3.564104790687561 and perplexity is 35.307831352784994
At time: 177.35494136810303 and batch: 400, loss is 3.5008246803283694 and perplexity is 33.14277288445432
At time: 177.8669831752777 and batch: 450, loss is 3.53583016872406 and perplexity is 34.3234971838277
At time: 178.37871146202087 and batch: 500, loss is 3.420416202545166 and perplexity is 30.582140737440277
At time: 178.89292097091675 and batch: 550, loss is 3.456255178451538 and perplexity is 31.698050432867383
At time: 179.403737783432 and batch: 600, loss is 3.4928473234176636 and perplexity is 32.87943293108209
At time: 179.91522693634033 and batch: 650, loss is 3.339705777168274 and perplexity is 28.21082521534436
At time: 180.42675757408142 and batch: 700, loss is 3.326170468330383 and perplexity is 27.831555539888935
At time: 180.93878149986267 and batch: 750, loss is 3.427746982574463 and perplexity is 30.807155442970007
At time: 181.45246744155884 and batch: 800, loss is 3.393641333580017 and perplexity is 29.774172812523776
At time: 181.9624638557434 and batch: 850, loss is 3.4467967128753663 and perplexity is 31.399648948742765
At time: 182.47234201431274 and batch: 900, loss is 3.4228066873550413 and perplexity is 30.655334329556776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279631732261344 and perplexity of 72.21384108190804
finished 18 epochs...
Completing Train Step...
At time: 183.90393733978271 and batch: 50, loss is 3.700343451499939 and perplexity is 40.4611984332519
At time: 184.42978286743164 and batch: 100, loss is 3.5714946126937868 and perplexity is 35.569716392038494
At time: 184.94221568107605 and batch: 150, loss is 3.5759307193756102 and perplexity is 35.727857955679895
At time: 185.45108222961426 and batch: 200, loss is 3.4699405670166015 and perplexity is 32.13483251902981
At time: 185.96047115325928 and batch: 250, loss is 3.613523416519165 and perplexity is 37.09652934255218
At time: 186.47252702713013 and batch: 300, loss is 3.5891510486602782 and perplexity is 36.203328012763095
At time: 186.98477745056152 and batch: 350, loss is 3.562276635169983 and perplexity is 35.243342112225235
At time: 187.49788331985474 and batch: 400, loss is 3.499288878440857 and perplexity is 33.09191121801986
At time: 188.01016902923584 and batch: 450, loss is 3.5342170000076294 and perplexity is 34.268172228173846
At time: 188.53766083717346 and batch: 500, loss is 3.419199895858765 and perplexity is 30.544966087647445
At time: 189.0498023033142 and batch: 550, loss is 3.4551521921157837 and perplexity is 31.663107190871703
At time: 189.56096935272217 and batch: 600, loss is 3.492175941467285 and perplexity is 32.8573656818788
At time: 190.0693063735962 and batch: 650, loss is 3.3393869638442992 and perplexity is 28.20183266193447
At time: 190.58094716072083 and batch: 700, loss is 3.3261313915252684 and perplexity is 27.830467992866133
At time: 191.09228587150574 and batch: 750, loss is 3.427728543281555 and perplexity is 30.80658738604443
At time: 191.6009669303894 and batch: 800, loss is 3.3939623498916625 and perplexity is 29.78373234196269
At time: 192.11237478256226 and batch: 850, loss is 3.447575511932373 and perplexity is 31.424112490589565
At time: 192.62736916542053 and batch: 900, loss is 3.423877077102661 and perplexity is 30.688165052833188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279428560439855 and perplexity of 72.1991707546277
finished 19 epochs...
Completing Train Step...
At time: 194.0726022720337 and batch: 50, loss is 3.698965001106262 and perplexity is 40.40546310136338
At time: 194.5853786468506 and batch: 100, loss is 3.5698892402648927 and perplexity is 35.51265956102483
At time: 195.0967710018158 and batch: 150, loss is 3.5742692375183105 and perplexity is 35.66854605435466
At time: 195.6097490787506 and batch: 200, loss is 3.468374767303467 and perplexity is 32.08455517988369
At time: 196.11964201927185 and batch: 250, loss is 3.61193696975708 and perplexity is 37.03772433153519
At time: 196.63697218894958 and batch: 300, loss is 3.587701745033264 and perplexity is 36.15089640200366
At time: 197.1477916240692 and batch: 350, loss is 3.560932936668396 and perplexity is 35.19601748837168
At time: 197.65996551513672 and batch: 400, loss is 3.4980754804611207 and perplexity is 33.051781911135905
At time: 198.1709485054016 and batch: 450, loss is 3.533025507926941 and perplexity is 34.22736628714962
At time: 198.68095874786377 and batch: 500, loss is 3.4182460260391236 and perplexity is 30.515844057875217
At time: 199.19108748435974 and batch: 550, loss is 3.454264702796936 and perplexity is 31.635018987272225
At time: 199.70076298713684 and batch: 600, loss is 3.4915721940994264 and perplexity is 32.83753412106231
At time: 200.2114086151123 and batch: 650, loss is 3.339004716873169 and perplexity is 28.19105465688025
At time: 200.72489285469055 and batch: 700, loss is 3.3259379863739014 and perplexity is 27.825085957464818
At time: 201.25218152999878 and batch: 750, loss is 3.4275804996490478 and perplexity is 30.802027004518703
At time: 201.76688861846924 and batch: 800, loss is 3.3940692710876466 and perplexity is 29.786917024497562
At time: 202.28030633926392 and batch: 850, loss is 3.447923812866211 and perplexity is 31.435059444621558
At time: 202.79072761535645 and batch: 900, loss is 3.424401707649231 and perplexity is 30.70426922563919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279362926744435 and perplexity of 72.19443221175057
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8ad1a05b70>
ELAPSED
640.9512972831726


RESULTS SO FAR:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}, {'best_accuracy': -71.20988825665874, 'params': {'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}}, {'best_accuracy': -72.19443221175057, 'params': {'batch_size': 32, 'dropout': 0.982441793390317, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9171483614956}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.703003270020777, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.11712669393286701}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8302602767944336 and batch: 50, loss is 6.8909087753295895 and perplexity is 983.294605182102
At time: 1.3697454929351807 and batch: 100, loss is 6.0500713729858395 and perplexity is 424.14330133831544
At time: 1.9106764793395996 and batch: 150, loss is 5.931693849563598 and perplexity is 376.7922029060453
At time: 2.4513754844665527 and batch: 200, loss is 5.76432957649231 and perplexity is 318.7252914639861
At time: 2.9950311183929443 and batch: 250, loss is 5.808453207015991 and perplexity is 333.1034848492575
At time: 3.5330874919891357 and batch: 300, loss is 5.707720775604248 and perplexity is 301.18381987377165
At time: 4.074683427810669 and batch: 350, loss is 5.679330854415894 and perplexity is 292.75346967543027
At time: 4.641572713851929 and batch: 400, loss is 5.535563383102417 and perplexity is 253.55059352609987
At time: 5.184797048568726 and batch: 450, loss is 5.527355947494507 and perplexity is 251.47810987830462
At time: 5.723843812942505 and batch: 500, loss is 5.472960758209228 and perplexity is 238.16429680280692
At time: 6.264955520629883 and batch: 550, loss is 5.52082311630249 and perplexity is 249.8406004481059
At time: 6.807049751281738 and batch: 600, loss is 5.440748300552368 and perplexity is 230.6146880083892
At time: 7.346725940704346 and batch: 650, loss is 5.331939172744751 and perplexity is 206.83868144403127
At time: 7.885154962539673 and batch: 700, loss is 5.42995177268982 and perplexity is 228.13824266489638
At time: 8.423815250396729 and batch: 750, loss is 5.399854583740234 and perplexity is 221.3742224520588
At time: 8.965535879135132 and batch: 800, loss is 5.381779146194458 and perplexity is 217.40873346111468
At time: 9.50618839263916 and batch: 850, loss is 5.410399007797241 and perplexity is 223.72083623198984
At time: 10.046682357788086 and batch: 900, loss is 5.3115840339660645 and perplexity is 202.67101202196628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.15355985458583 and perplexity of 173.04641522063008
finished 1 epochs...
Completing Train Step...
At time: 11.546416282653809 and batch: 50, loss is 5.063506460189819 and perplexity is 158.1440711369311
At time: 12.06430172920227 and batch: 100, loss is 4.901969623565674 and perplexity is 134.55454065376065
At time: 12.57574200630188 and batch: 150, loss is 4.859304819107056 and perplexity is 128.93453811730316
At time: 13.089035034179688 and batch: 200, loss is 4.733162860870362 and perplexity is 113.65446773430017
At time: 13.603263854980469 and batch: 250, loss is 4.822100877761841 and perplexity is 124.22580004554639
At time: 14.118744611740112 and batch: 300, loss is 4.762306489944458 and perplexity is 117.01550998010526
At time: 14.632292747497559 and batch: 350, loss is 4.734987335205078 and perplexity is 113.86201666998798
At time: 15.14323091506958 and batch: 400, loss is 4.599303569793701 and perplexity is 99.41505587956242
At time: 15.654290437698364 and batch: 450, loss is 4.613315658569336 and perplexity is 100.8178737200371
At time: 16.168341159820557 and batch: 500, loss is 4.508818826675415 and perplexity is 90.81448748463326
At time: 16.67837905883789 and batch: 550, loss is 4.57430172920227 and perplexity is 96.96031093970612
At time: 17.196962118148804 and batch: 600, loss is 4.537138395309448 and perplexity is 93.42307732748277
At time: 17.70771312713623 and batch: 650, loss is 4.389364013671875 and perplexity is 80.5891490817166
At time: 18.22127413749695 and batch: 700, loss is 4.431874418258667 and perplexity is 84.088887026927
At time: 18.736035346984863 and batch: 750, loss is 4.481610069274902 and perplexity is 88.37685103890036
At time: 19.247615337371826 and batch: 800, loss is 4.428122730255127 and perplexity is 83.7740028012601
At time: 19.759655237197876 and batch: 850, loss is 4.491828060150146 and perplexity is 89.28451424637335
At time: 20.271749019622803 and batch: 900, loss is 4.423052883148193 and perplexity is 83.35035623460962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.534111127461473 and perplexity of 93.1406882985037
finished 2 epochs...
Completing Train Step...
At time: 21.71113634109497 and batch: 50, loss is 4.460944299697876 and perplexity is 86.56921779771847
At time: 22.255284786224365 and batch: 100, loss is 4.32565908908844 and perplexity is 75.61533368109116
At time: 22.7671115398407 and batch: 150, loss is 4.316992225646973 and perplexity is 74.96281762820557
At time: 23.270888090133667 and batch: 200, loss is 4.222160830497741 and perplexity is 68.18065206218286
At time: 23.792291164398193 and batch: 250, loss is 4.360777015686035 and perplexity is 78.31796507419966
At time: 24.30786681175232 and batch: 300, loss is 4.3293787860870365 and perplexity is 75.89712357243205
At time: 24.81767725944519 and batch: 350, loss is 4.312144784927368 and perplexity is 74.60031911841948
At time: 25.328186511993408 and batch: 400, loss is 4.217946085929871 and perplexity is 67.89389276237142
At time: 25.838382244110107 and batch: 450, loss is 4.253896584510803 and perplexity is 70.37911692075028
At time: 26.35083508491516 and batch: 500, loss is 4.129553227424622 and perplexity is 62.150149748610495
At time: 26.86554455757141 and batch: 550, loss is 4.213182897567749 and perplexity is 67.57127032818873
At time: 27.378057956695557 and batch: 600, loss is 4.2117576408386235 and perplexity is 67.47503251854194
At time: 27.889920473098755 and batch: 650, loss is 4.054120659828186 and perplexity is 57.63446041070012
At time: 28.40166211128235 and batch: 700, loss is 4.082574877738953 and perplexity is 59.297958439045594
At time: 28.917186975479126 and batch: 750, loss is 4.162438406944275 and perplexity is 64.22794567963105
At time: 29.429422616958618 and batch: 800, loss is 4.1251721906661984 and perplexity is 61.87846322689028
At time: 29.939144611358643 and batch: 850, loss is 4.198516979217529 and perplexity is 66.587507123159
At time: 30.45049476623535 and batch: 900, loss is 4.147263793945313 and perplexity is 63.26066904558759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388013865849743 and perplexity of 80.4804152374955
finished 3 epochs...
Completing Train Step...
At time: 31.88272762298584 and batch: 50, loss is 4.213208074569702 and perplexity is 67.57297159161006
At time: 32.40869975090027 and batch: 100, loss is 4.081455898284912 and perplexity is 59.23164235197482
At time: 32.92419981956482 and batch: 150, loss is 4.0821734809875485 and perplexity is 59.274161207548474
At time: 33.43745565414429 and batch: 200, loss is 3.9865231275558473 and perplexity is 53.86727375479866
At time: 33.95094633102417 and batch: 250, loss is 4.138053970336914 and perplexity is 62.68072413676413
At time: 34.46485757827759 and batch: 300, loss is 4.112031917572022 and perplexity is 61.07068217628934
At time: 34.978265047073364 and batch: 350, loss is 4.096421694755554 and perplexity is 60.12475747605805
At time: 35.50941252708435 and batch: 400, loss is 4.016328868865966 and perplexity is 55.49699465167693
At time: 36.021567821502686 and batch: 450, loss is 4.053943848609924 and perplexity is 57.62427089237814
At time: 36.534892082214355 and batch: 500, loss is 3.929488806724548 and perplexity is 50.88096101487928
At time: 37.047722578048706 and batch: 550, loss is 4.014780964851379 and perplexity is 55.41115708215626
At time: 37.56105017662048 and batch: 600, loss is 4.028765435218811 and perplexity is 56.191496364945515
At time: 38.07172727584839 and batch: 650, loss is 3.8664423418045044 and perplexity is 47.77212650034342
At time: 38.583268880844116 and batch: 700, loss is 3.893502674102783 and perplexity is 49.08250580704988
At time: 39.096195459365845 and batch: 750, loss is 3.9813099479675294 and perplexity is 53.58718469440514
At time: 39.60838747024536 and batch: 800, loss is 3.9471784734725954 and perplexity is 51.789036354340915
At time: 40.12164759635925 and batch: 850, loss is 4.02301510810852 and perplexity is 55.869304123228765
At time: 40.63696908950806 and batch: 900, loss is 3.978407063484192 and perplexity is 53.43185285172633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3321424510380995 and perplexity of 76.10716790512542
finished 4 epochs...
Completing Train Step...
At time: 42.08987236022949 and batch: 50, loss is 4.052107934951782 and perplexity is 57.51857476058045
At time: 42.60258436203003 and batch: 100, loss is 3.925721468925476 and perplexity is 50.68963586671323
At time: 43.11484384536743 and batch: 150, loss is 3.9344469261169435 and perplexity is 51.13386133128636
At time: 43.630715131759644 and batch: 200, loss is 3.8330049991607664 and perplexity is 46.20116430424678
At time: 44.143203258514404 and batch: 250, loss is 3.9869565200805663 and perplexity is 53.890624488222144
At time: 44.653854846954346 and batch: 300, loss is 3.9673643112182617 and perplexity is 52.84506398248981
At time: 45.1646933555603 and batch: 350, loss is 3.948285813331604 and perplexity is 51.84641618217769
At time: 45.67516899108887 and batch: 400, loss is 3.8768038892745973 and perplexity is 48.26969298457436
At time: 46.18797731399536 and batch: 450, loss is 3.9175261497497558 and perplexity is 50.27591572176653
At time: 46.69954538345337 and batch: 500, loss is 3.7950678205490114 and perplexity is 44.48125304623478
At time: 47.21347379684448 and batch: 550, loss is 3.8755895376205443 and perplexity is 48.21111217910577
At time: 47.725423097610474 and batch: 600, loss is 3.898693356513977 and perplexity is 49.337939871676106
At time: 48.25257396697998 and batch: 650, loss is 3.740075082778931 and perplexity is 42.101151117750966
At time: 48.76641845703125 and batch: 700, loss is 3.7599072313308715 and perplexity is 42.94444189524573
At time: 49.27752208709717 and batch: 750, loss is 3.8538719749450685 and perplexity is 47.17537191456322
At time: 49.78983283042908 and batch: 800, loss is 3.8219478702545167 and perplexity is 45.69312597362891
At time: 50.301528215408325 and batch: 850, loss is 3.897110900878906 and perplexity is 49.25992651332251
At time: 50.81321382522583 and batch: 900, loss is 3.8575232553482057 and perplexity is 47.347937276089084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314993662376926 and perplexity of 74.81314930489923
finished 5 epochs...
Completing Train Step...
At time: 52.24712514877319 and batch: 50, loss is 3.9365419006347655 and perplexity is 51.241097757342246
At time: 52.77372336387634 and batch: 100, loss is 3.809534373283386 and perplexity is 45.129420509449716
At time: 53.285263538360596 and batch: 150, loss is 3.8201528453826903 and perplexity is 45.611179246245136
At time: 53.794445276260376 and batch: 200, loss is 3.7209427785873412 and perplexity is 41.303315640106355
At time: 54.306275606155396 and batch: 250, loss is 3.878384304046631 and perplexity is 48.34603943404604
At time: 54.82104730606079 and batch: 300, loss is 3.8597848510742185 and perplexity is 47.45514034797915
At time: 55.332335233688354 and batch: 350, loss is 3.8403620815277097 and perplexity is 46.542323504835316
At time: 55.84585523605347 and batch: 400, loss is 3.7732539224624633 and perplexity is 43.52145010684964
At time: 56.35968327522278 and batch: 450, loss is 3.8154999256134032 and perplexity is 45.39944705706826
At time: 56.87464880943298 and batch: 500, loss is 3.695379934310913 and perplexity is 40.26086616695388
At time: 57.3894784450531 and batch: 550, loss is 3.7709639263153076 and perplexity is 43.42190018176277
At time: 57.900174617767334 and batch: 600, loss is 3.8004419898986814 and perplexity is 44.72094633225084
At time: 58.410162687301636 and batch: 650, loss is 3.642632818222046 and perplexity is 38.19225774437754
At time: 58.92151498794556 and batch: 700, loss is 3.6553738737106323 and perplexity is 38.681980587601544
At time: 59.43292284011841 and batch: 750, loss is 3.7602604532241823 and perplexity is 42.95961349163141
At time: 59.94464874267578 and batch: 800, loss is 3.725432677268982 and perplexity is 41.48918028702287
At time: 60.45540404319763 and batch: 850, loss is 3.804104137420654 and perplexity is 44.88502128501077
At time: 60.965879678726196 and batch: 900, loss is 3.7664594268798828 and perplexity is 43.22674612256494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31389921005458 and perplexity of 74.73131467012213
finished 6 epochs...
Completing Train Step...
At time: 62.41421294212341 and batch: 50, loss is 3.8460662603378295 and perplexity is 46.80856787116814
At time: 62.941301345825195 and batch: 100, loss is 3.7223330211639403 and perplexity is 41.36077720156354
At time: 63.452932834625244 and batch: 150, loss is 3.732887692451477 and perplexity is 41.79963855365076
At time: 63.9619345664978 and batch: 200, loss is 3.633497676849365 and perplexity is 37.84495481643621
At time: 64.47100377082825 and batch: 250, loss is 3.7916435527801515 and perplexity is 44.32919781260619
At time: 64.98024559020996 and batch: 300, loss is 3.7731820154190063 and perplexity is 43.5183207205593
At time: 65.48781132698059 and batch: 350, loss is 3.7549158906936646 and perplexity is 42.73062561612207
At time: 65.99537897109985 and batch: 400, loss is 3.685958924293518 and perplexity is 39.88334923112214
At time: 66.50329399108887 and batch: 450, loss is 3.735703625679016 and perplexity is 41.917509425117
At time: 67.01161313056946 and batch: 500, loss is 3.6181427145004275 and perplexity is 37.26828565704446
At time: 67.51968622207642 and batch: 550, loss is 3.688739957809448 and perplexity is 39.99442053698676
At time: 68.02719616889954 and batch: 600, loss is 3.7189569807052614 and perplexity is 41.2213769871135
At time: 68.53447222709656 and batch: 650, loss is 3.5668464279174805 and perplexity is 35.404765436057595
At time: 69.0418791770935 and batch: 700, loss is 3.577496037483215 and perplexity is 35.783827212101045
At time: 69.55011677742004 and batch: 750, loss is 3.6788555812835693 and perplexity is 39.60104794958202
At time: 70.06096959114075 and batch: 800, loss is 3.6476283407211305 and perplexity is 38.38352537033411
At time: 70.57197785377502 and batch: 850, loss is 3.723078546524048 and perplexity is 41.391624207064766
At time: 71.08305644989014 and batch: 900, loss is 3.6896468019485473 and perplexity is 40.030705692851214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323085837168236 and perplexity of 75.42100651210994
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 72.52619671821594 and batch: 50, loss is 3.7986504507064818 and perplexity is 44.64089872980302
At time: 73.03562808036804 and batch: 100, loss is 3.674320273399353 and perplexity is 39.4218516669074
At time: 73.54649901390076 and batch: 150, loss is 3.680840616226196 and perplexity is 39.67973548644357
At time: 74.05557799339294 and batch: 200, loss is 3.560591387748718 and perplexity is 35.18399837929526
At time: 74.57769775390625 and batch: 250, loss is 3.7100968551635742 and perplexity is 40.85776362073346
At time: 75.08693528175354 and batch: 300, loss is 3.6808694648742675 and perplexity is 39.680880209679955
At time: 75.59738159179688 and batch: 350, loss is 3.6541517782211304 and perplexity is 38.63473638794209
At time: 76.10469913482666 and batch: 400, loss is 3.5758660554885866 and perplexity is 35.725547728204404
At time: 76.61795449256897 and batch: 450, loss is 3.611082091331482 and perplexity is 37.00607511011669
At time: 77.13302850723267 and batch: 500, loss is 3.4869226932525637 and perplexity is 32.68521036747754
At time: 77.64647316932678 and batch: 550, loss is 3.544047961235046 and perplexity is 34.606722712349665
At time: 78.16614818572998 and batch: 600, loss is 3.5621996974945067 and perplexity is 35.24063067571425
At time: 78.6808590888977 and batch: 650, loss is 3.399304585456848 and perplexity is 29.943269820112555
At time: 79.19388365745544 and batch: 700, loss is 3.392280898094177 and perplexity is 29.733694511569652
At time: 79.7083625793457 and batch: 750, loss is 3.484068455696106 and perplexity is 32.59205202377181
At time: 80.22234106063843 and batch: 800, loss is 3.4329522800445558 and perplexity is 30.96793393817932
At time: 80.7326250076294 and batch: 850, loss is 3.4910905313491822 and perplexity is 32.821721312592445
At time: 81.24405646324158 and batch: 900, loss is 3.4505283498764037 and perplexity is 31.517039934629196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27738931734268 and perplexity of 72.0520891126262
finished 8 epochs...
Completing Train Step...
At time: 82.66951322555542 and batch: 50, loss is 3.7060965967178343 and perplexity is 40.69464847561891
At time: 83.1964943408966 and batch: 100, loss is 3.582365493774414 and perplexity is 35.95849993007751
At time: 83.70783519744873 and batch: 150, loss is 3.5865177011489866 and perplexity is 36.10811748526296
At time: 84.22000002861023 and batch: 200, loss is 3.4724284744262697 and perplexity is 32.2148805417074
At time: 84.73437452316284 and batch: 250, loss is 3.6251697301864625 and perplexity is 37.53109277840284
At time: 85.24817395210266 and batch: 300, loss is 3.599638776779175 and perplexity is 36.58501669897761
At time: 85.76177883148193 and batch: 350, loss is 3.5771083545684816 and perplexity is 35.76995712243925
At time: 86.27470374107361 and batch: 400, loss is 3.5060850954055787 and perplexity is 33.31757699462697
At time: 86.79154706001282 and batch: 450, loss is 3.5447748565673827 and perplexity is 34.63188732246752
At time: 87.3054027557373 and batch: 500, loss is 3.4250802564620972 and perplexity is 30.72511064123142
At time: 87.83167958259583 and batch: 550, loss is 3.485479860305786 and perplexity is 32.63808507422517
At time: 88.34235000610352 and batch: 600, loss is 3.511705174446106 and perplexity is 33.50535157051218
At time: 88.85297584533691 and batch: 650, loss is 3.352493362426758 and perplexity is 28.57388996127474
At time: 89.36455154418945 and batch: 700, loss is 3.34989830493927 and perplexity is 28.499835203919407
At time: 89.87535667419434 and batch: 750, loss is 3.4487558364868165 and perplexity is 31.46122504028198
At time: 90.38382291793823 and batch: 800, loss is 3.402272024154663 and perplexity is 30.032256603829488
At time: 90.89225578308105 and batch: 850, loss is 3.4681225967407228 and perplexity is 32.07646541959115
At time: 91.4064416885376 and batch: 900, loss is 3.4352696466445924 and perplexity is 31.03978121004108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2818975579248715 and perplexity of 72.37765056807946
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 92.8580687046051 and batch: 50, loss is 3.6842966651916504 and perplexity is 39.817107841279
At time: 93.38688278198242 and batch: 100, loss is 3.5689215898513793 and perplexity is 35.478312342046046
At time: 93.89726972579956 and batch: 150, loss is 3.574811978340149 and perplexity is 35.68791008470351
At time: 94.40538048744202 and batch: 200, loss is 3.4543083190917967 and perplexity is 31.636398819679666
At time: 94.91816878318787 and batch: 250, loss is 3.60806254863739 and perplexity is 36.894502220705874
At time: 95.42787742614746 and batch: 300, loss is 3.5747016859054566 and perplexity is 35.6839741952646
At time: 95.93928241729736 and batch: 350, loss is 3.5511336517333985 and perplexity is 34.85280604422341
At time: 96.4499397277832 and batch: 400, loss is 3.478706884384155 and perplexity is 32.41777502939706
At time: 96.95996570587158 and batch: 450, loss is 3.5137529468536375 and perplexity is 33.574033203138384
At time: 97.47066521644592 and batch: 500, loss is 3.38850492477417 and perplexity is 29.62163257897054
At time: 97.97926545143127 and batch: 550, loss is 3.442518696784973 and perplexity is 31.26560766512018
At time: 98.49105143547058 and batch: 600, loss is 3.4684150791168213 and perplexity is 32.08584859255339
At time: 99.00361514091492 and batch: 650, loss is 3.300224018096924 and perplexity is 27.118713322794083
At time: 99.51654744148254 and batch: 700, loss is 3.294701018333435 and perplexity is 26.969349523317447
At time: 100.03010177612305 and batch: 750, loss is 3.390311150550842 and perplexity is 29.675184283957606
At time: 100.55822491645813 and batch: 800, loss is 3.3374431419372557 and perplexity is 28.147066566799385
At time: 101.069650888443 and batch: 850, loss is 3.4000866746902467 and perplexity is 29.966697289043076
At time: 101.57996559143066 and batch: 900, loss is 3.372144842147827 and perplexity is 29.14096284174231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264684807764341 and perplexity of 71.14249287777692
finished 10 epochs...
Completing Train Step...
At time: 103.03251433372498 and batch: 50, loss is 3.6549278783798216 and perplexity is 38.66473245145296
At time: 103.54648804664612 and batch: 100, loss is 3.5333553647994993 and perplexity is 34.238658281417585
At time: 104.05599617958069 and batch: 150, loss is 3.5405319786071776 and perplexity is 34.4852597324776
At time: 104.56723523139954 and batch: 200, loss is 3.422312030792236 and perplexity is 30.64017421707936
At time: 105.07630181312561 and batch: 250, loss is 3.5761984062194823 and perplexity is 35.73742311339068
At time: 105.58896923065186 and batch: 300, loss is 3.545445671081543 and perplexity is 34.6551266889169
At time: 106.1006383895874 and batch: 350, loss is 3.523089723587036 and perplexity is 33.88897443525602
At time: 106.61255645751953 and batch: 400, loss is 3.453711705207825 and perplexity is 31.61752973424385
At time: 107.12711095809937 and batch: 450, loss is 3.4906255292892454 and perplexity is 32.80646269249125
At time: 107.6423192024231 and batch: 500, loss is 3.3675742673873903 and perplexity is 29.008075808878015
At time: 108.15243697166443 and batch: 550, loss is 3.4235731172561645 and perplexity is 30.678838500414994
At time: 108.66473364830017 and batch: 600, loss is 3.452656512260437 and perplexity is 31.58418473565207
At time: 109.18286395072937 and batch: 650, loss is 3.2872523164749143 and perplexity is 26.769209197575453
At time: 109.70273351669312 and batch: 700, loss is 3.284671859741211 and perplexity is 26.70022145966312
At time: 110.23585724830627 and batch: 750, loss is 3.3834938526153566 and perplexity is 29.473567732306854
At time: 110.78701567649841 and batch: 800, loss is 3.333591866493225 and perplexity is 28.03887293597617
At time: 111.32265448570251 and batch: 850, loss is 3.3991153764724733 and perplexity is 29.937604820392348
At time: 111.84472250938416 and batch: 900, loss is 3.373697247505188 and perplexity is 29.186236561066366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265446911119435 and perplexity of 71.19673147537229
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 113.28041243553162 and batch: 50, loss is 3.647123737335205 and perplexity is 38.364161799341105
At time: 113.80656456947327 and batch: 100, loss is 3.5298828649520875 and perplexity is 34.11997073626469
At time: 114.31800174713135 and batch: 150, loss is 3.5403813552856445 and perplexity is 34.480065839283306
At time: 114.82936811447144 and batch: 200, loss is 3.421082410812378 and perplexity is 30.602521600624573
At time: 115.33930540084839 and batch: 250, loss is 3.573622169494629 and perplexity is 35.64547354429844
At time: 115.85062718391418 and batch: 300, loss is 3.5441335678100585 and perplexity is 34.60968540216452
At time: 116.36266326904297 and batch: 350, loss is 3.5185416555404663 and perplexity is 33.73519503815326
At time: 116.87188029289246 and batch: 400, loss is 3.447956566810608 and perplexity is 31.436089083672993
At time: 117.38484168052673 and batch: 450, loss is 3.482631025314331 and perplexity is 32.545236872809426
At time: 117.89775943756104 and batch: 500, loss is 3.358089790344238 and perplexity is 28.734249979867975
At time: 118.40974450111389 and batch: 550, loss is 3.4115514755249023 and perplexity is 30.312236489344887
At time: 118.9212052822113 and batch: 600, loss is 3.4432113695144655 and perplexity is 31.287272001202016
At time: 119.43181824684143 and batch: 650, loss is 3.271358995437622 and perplexity is 26.347120641194483
At time: 119.94315123558044 and batch: 700, loss is 3.2675037717819215 and perplexity is 26.245742142395905
At time: 120.45338129997253 and batch: 750, loss is 3.365039420127869 and perplexity is 28.93463788369423
At time: 120.96462321281433 and batch: 800, loss is 3.312528648376465 and perplexity is 27.454460449912474
At time: 121.47703838348389 and batch: 850, loss is 3.373965654373169 and perplexity is 29.194071398824978
At time: 121.99561667442322 and batch: 900, loss is 3.3529039669036864 and perplexity is 28.58562493746811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262571988040453 and perplexity of 70.99234029414522
finished 12 epochs...
Completing Train Step...
At time: 123.42627930641174 and batch: 50, loss is 3.637212986946106 and perplexity is 37.98582207992083
At time: 123.95273184776306 and batch: 100, loss is 3.5162694454193115 and perplexity is 33.65862860700065
At time: 124.46382236480713 and batch: 150, loss is 3.527137141227722 and perplexity is 34.02641522071501
At time: 124.97462844848633 and batch: 200, loss is 3.409415798187256 and perplexity is 30.247568412474223
At time: 125.48670530319214 and batch: 250, loss is 3.5623828172683716 and perplexity is 35.24708452292993
At time: 125.99488639831543 and batch: 300, loss is 3.5336548280715943 and perplexity is 34.24891303744344
At time: 126.50622630119324 and batch: 350, loss is 3.508483119010925 and perplexity is 33.397569204003155
At time: 127.03057527542114 and batch: 400, loss is 3.4390714263916013 and perplexity is 31.15801222332831
At time: 127.54071640968323 and batch: 450, loss is 3.4747179794311522 and perplexity is 32.288721168927104
At time: 128.0526614189148 and batch: 500, loss is 3.3509541654586794 and perplexity is 28.52994294671226
At time: 128.56885647773743 and batch: 550, loss is 3.4055649995803834 and perplexity is 30.131315095881586
At time: 129.07893419265747 and batch: 600, loss is 3.4379310321807863 and perplexity is 31.12250005935006
At time: 129.58971309661865 and batch: 650, loss is 3.26760657787323 and perplexity is 26.248440503260642
At time: 130.0982756614685 and batch: 700, loss is 3.2654029178619384 and perplexity is 26.190661550530436
At time: 130.60978364944458 and batch: 750, loss is 3.364389681816101 and perplexity is 28.915844047123056
At time: 131.1202073097229 and batch: 800, loss is 3.3131558465957642 and perplexity is 27.471685239732583
At time: 131.63067412376404 and batch: 850, loss is 3.376207027435303 and perplexity is 29.259579590755937
At time: 132.1429615020752 and batch: 900, loss is 3.356436023712158 and perplexity is 28.686769507685074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262199924416738 and perplexity of 70.96593153993238
finished 13 epochs...
Completing Train Step...
At time: 133.58928513526917 and batch: 50, loss is 3.6317593479156494 and perplexity is 37.779224983070996
At time: 134.0989532470703 and batch: 100, loss is 3.5101305532455442 and perplexity is 33.45263484892299
At time: 134.60740876197815 and batch: 150, loss is 3.5205795049667357 and perplexity is 33.804012381883496
At time: 135.11697793006897 and batch: 200, loss is 3.4032423305511474 and perplexity is 30.061411236689008
At time: 135.6276752948761 and batch: 250, loss is 3.556094274520874 and perplexity is 35.026127203118456
At time: 136.1409821510315 and batch: 300, loss is 3.527736253738403 and perplexity is 34.046806979636365
At time: 136.65038752555847 and batch: 350, loss is 3.502611837387085 and perplexity is 33.20205718435854
At time: 137.16054248809814 and batch: 400, loss is 3.4339533281326293 and perplexity is 30.998949850859688
At time: 137.67207503318787 and batch: 450, loss is 3.469829182624817 and perplexity is 32.131253399587294
At time: 138.18225860595703 and batch: 500, loss is 3.3465659618377686 and perplexity is 28.405022037422157
At time: 138.69217920303345 and batch: 550, loss is 3.401727705001831 and perplexity is 30.015913919578782
At time: 139.20201683044434 and batch: 600, loss is 3.43468581199646 and perplexity is 31.021664399425692
At time: 139.7259464263916 and batch: 650, loss is 3.265062427520752 and perplexity is 26.18174540126069
At time: 140.23575377464294 and batch: 700, loss is 3.263606114387512 and perplexity is 26.143644331859456
At time: 140.74775457382202 and batch: 750, loss is 3.3633364772796632 and perplexity is 28.885405780671462
At time: 141.25728702545166 and batch: 800, loss is 3.3128104877471922 and perplexity is 27.462199288272778
At time: 141.7681188583374 and batch: 850, loss is 3.376487469673157 and perplexity is 29.2677863634441
At time: 142.27941942214966 and batch: 900, loss is 3.357173066139221 and perplexity is 28.707920667601243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262431941620291 and perplexity of 70.98239876718199
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 143.71670746803284 and batch: 50, loss is 3.6295607709884643 and perplexity is 37.696255691302554
At time: 144.2422149181366 and batch: 100, loss is 3.508941206932068 and perplexity is 33.412871731735
At time: 144.7507357597351 and batch: 150, loss is 3.520081148147583 and perplexity is 33.78717011887515
At time: 145.2598786354065 and batch: 200, loss is 3.4035815620422363 and perplexity is 30.07161074394628
At time: 145.77230310440063 and batch: 250, loss is 3.5555349588394165 and perplexity is 35.006542018574024
At time: 146.2847969532013 and batch: 300, loss is 3.528345980644226 and perplexity is 34.06757256393361
At time: 146.79684162139893 and batch: 350, loss is 3.502309637069702 and perplexity is 33.19202502807642
At time: 147.30524015426636 and batch: 400, loss is 3.432349042892456 and perplexity is 30.94925856331034
At time: 147.8177363872528 and batch: 450, loss is 3.4673420667648314 and perplexity is 32.05143854519394
At time: 148.32638263702393 and batch: 500, loss is 3.3435890245437623 and perplexity is 28.320587808040816
At time: 148.83466744422913 and batch: 550, loss is 3.3976571464538576 and perplexity is 29.893980721073696
At time: 149.34715747833252 and batch: 600, loss is 3.4303764390945433 and perplexity is 30.888268113099624
At time: 149.85764265060425 and batch: 650, loss is 3.2597377157211302 and perplexity is 26.042705654389156
At time: 150.36725616455078 and batch: 700, loss is 3.2570012617111206 and perplexity is 25.971538405431026
At time: 150.88340401649475 and batch: 750, loss is 3.3559340190887452 and perplexity is 28.672372230813572
At time: 151.39589381217957 and batch: 800, loss is 3.304923996925354 and perplexity is 27.246470694558884
At time: 151.90757131576538 and batch: 850, loss is 3.366896462440491 and perplexity is 28.988420653527985
At time: 152.41765689849854 and batch: 900, loss is 3.3471743726730345 and perplexity is 28.422309218926706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262633859294734 and perplexity of 70.99673281516755
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 153.86865282058716 and batch: 50, loss is 3.627905321121216 and perplexity is 37.63390305489432
At time: 154.39616227149963 and batch: 100, loss is 3.5069826459884643 and perplexity is 33.34749462956571
At time: 154.9061529636383 and batch: 150, loss is 3.5182673835754397 and perplexity is 33.725943688670476
At time: 155.4164936542511 and batch: 200, loss is 3.4020783615112307 and perplexity is 30.026441040774184
At time: 155.928382396698 and batch: 250, loss is 3.5544094800949098 and perplexity is 34.96716506273342
At time: 156.44056391716003 and batch: 300, loss is 3.5269444942474366 and perplexity is 34.019860765941026
At time: 156.9519567489624 and batch: 350, loss is 3.5009954023361205 and perplexity is 33.14843156820077
At time: 157.46297955513 and batch: 400, loss is 3.4305375957489015 and perplexity is 30.893246364176022
At time: 157.97075080871582 and batch: 450, loss is 3.4656864690780638 and perplexity is 31.998418160005233
At time: 158.47913765907288 and batch: 500, loss is 3.3420511293411255 and perplexity is 28.277067185570864
At time: 158.99091053009033 and batch: 550, loss is 3.395715928077698 and perplexity is 29.836006265106583
At time: 159.5008053779602 and batch: 600, loss is 3.428291082382202 and perplexity is 30.823922171296704
At time: 160.01247239112854 and batch: 650, loss is 3.2580730390548704 and perplexity is 25.99938903404666
At time: 160.52324056625366 and batch: 700, loss is 3.2552176475524903 and perplexity is 25.925256488620388
At time: 161.03382992744446 and batch: 750, loss is 3.353865818977356 and perplexity is 28.613133307461915
At time: 161.54401540756226 and batch: 800, loss is 3.302640161514282 and perplexity is 27.184315243386518
At time: 162.05333352088928 and batch: 850, loss is 3.364248971939087 and perplexity is 28.911775588506295
At time: 162.56469678878784 and batch: 900, loss is 3.3444187784194948 and perplexity is 28.34409667747652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26231906838613 and perplexity of 70.97438720641853
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 164.01282858848572 and batch: 50, loss is 3.627394094467163 and perplexity is 37.61466851757962
At time: 164.52255725860596 and batch: 100, loss is 3.506448354721069 and perplexity is 33.3296821133553
At time: 165.03167748451233 and batch: 150, loss is 3.5177827405929567 and perplexity is 33.709602606849316
At time: 165.5421326160431 and batch: 200, loss is 3.4016521835327147 and perplexity is 30.013647159258323
At time: 166.06855034828186 and batch: 250, loss is 3.5540458250045774 and perplexity is 34.954451387000425
At time: 166.5814938545227 and batch: 300, loss is 3.526538577079773 and perplexity is 34.00605432274033
At time: 167.09242248535156 and batch: 350, loss is 3.5006466770172118 and perplexity is 33.136873886176126
At time: 167.60054850578308 and batch: 400, loss is 3.430041832923889 and perplexity is 30.87793443694048
At time: 168.1110384464264 and batch: 450, loss is 3.4652359962463377 and perplexity is 31.984006988130343
At time: 168.62080788612366 and batch: 500, loss is 3.341628084182739 and perplexity is 28.265107239181717
At time: 169.13074326515198 and batch: 550, loss is 3.395198578834534 and perplexity is 29.820574621964933
At time: 169.64201951026917 and batch: 600, loss is 3.42772901058197 and perplexity is 30.80660178197887
At time: 170.1522068977356 and batch: 650, loss is 3.2576212978363035 and perplexity is 25.987646690812333
At time: 170.66289973258972 and batch: 700, loss is 3.2547397327423098 and perplexity is 25.9128693848127
At time: 171.17378687858582 and batch: 750, loss is 3.353329825401306 and perplexity is 28.59780096120507
At time: 171.68439745903015 and batch: 800, loss is 3.3020577907562254 and perplexity is 27.16848850207395
At time: 172.1947374343872 and batch: 850, loss is 3.3635808420181275 and perplexity is 28.892465217804116
At time: 172.70514917373657 and batch: 900, loss is 3.343722381591797 and perplexity is 28.32436480986881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2622308600438785 and perplexity of 70.96812694948821
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 174.14028358459473 and batch: 50, loss is 3.6272611951828004 and perplexity is 37.60966988721664
At time: 174.66601371765137 and batch: 100, loss is 3.5063112688064577 and perplexity is 33.325113396559594
At time: 175.1792345046997 and batch: 150, loss is 3.5176567316055296 and perplexity is 33.705355161572015
At time: 175.690575838089 and batch: 200, loss is 3.4015420293807983 and perplexity is 30.010341213494744
At time: 176.19923639297485 and batch: 250, loss is 3.5539490604400634 and perplexity is 34.95106919837478
At time: 176.70919013023376 and batch: 300, loss is 3.5264320755004883 and perplexity is 34.00243281710119
At time: 177.22198367118835 and batch: 350, loss is 3.5005587577819823 and perplexity is 33.133960645632975
At time: 177.73479676246643 and batch: 400, loss is 3.429915246963501 and perplexity is 30.874025971338636
At time: 178.24450731277466 and batch: 450, loss is 3.4651209688186646 and perplexity is 31.980328161666847
At time: 178.75441026687622 and batch: 500, loss is 3.3415200185775755 and perplexity is 28.262052918299393
At time: 179.2926321029663 and batch: 550, loss is 3.3950674676895143 and perplexity is 29.81666506857972
At time: 179.8056399822235 and batch: 600, loss is 3.427586121559143 and perplexity is 30.802200171232034
At time: 180.31731128692627 and batch: 650, loss is 3.2575059604644774 and perplexity is 25.984649516789478
At time: 180.82821106910706 and batch: 700, loss is 3.2546176958084105 and perplexity is 25.909707250636952
At time: 181.3382432460785 and batch: 750, loss is 3.353194041252136 and perplexity is 28.593918096755115
At time: 181.84871768951416 and batch: 800, loss is 3.301910648345947 and perplexity is 27.164491159288673
At time: 182.35912132263184 and batch: 850, loss is 3.3634127283096316 and perplexity is 28.887608406588473
At time: 182.87253379821777 and batch: 900, loss is 3.3435468435287476 and perplexity is 28.319393242095444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262207449299016 and perplexity of 70.96646555222216
Annealing...
Model not improving. Stopping early with 70.96593153993238 lossat 17 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8ad1a05b70>
ELAPSED
832.1340715885162


RESULTS SO FAR:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}, {'best_accuracy': -71.20988825665874, 'params': {'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}}, {'best_accuracy': -72.19443221175057, 'params': {'batch_size': 32, 'dropout': 0.982441793390317, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9171483614956}}, {'best_accuracy': -70.96593153993238, 'params': {'batch_size': 32, 'dropout': 0.703003270020777, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.11712669393286701}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.10703692335741788, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.25510031122950216}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.805229902267456 and batch: 50, loss is 6.887162971496582 and perplexity is 979.6182662019295
At time: 1.3576107025146484 and batch: 100, loss is 5.989328393936157 and perplexity is 399.14645071185237
At time: 1.8951997756958008 and batch: 150, loss is 5.711773910522461 and perplexity is 302.40703578589364
At time: 2.4390981197357178 and batch: 200, loss is 5.431882085800171 and perplexity is 228.57904621324576
At time: 2.9750301837921143 and batch: 250, loss is 5.409080486297608 and perplexity is 223.42604988327847
At time: 3.5125491619110107 and batch: 300, loss is 5.286034126281738 and perplexity is 197.5583781594332
At time: 4.050727605819702 and batch: 350, loss is 5.230665369033813 and perplexity is 186.91713102461856
At time: 4.590653657913208 and batch: 400, loss is 5.049182319641114 and perplexity is 155.89494012582713
At time: 5.126132011413574 and batch: 450, loss is 5.033898591995239 and perplexity is 153.53039984245063
At time: 5.6650872230529785 and batch: 500, loss is 4.946944150924683 and perplexity is 140.74421302858192
At time: 6.2040534019470215 and batch: 550, loss is 4.997555685043335 and perplexity is 148.0508335972403
At time: 6.7428083419799805 and batch: 600, loss is 4.909908418655395 and perplexity is 135.6269929373085
At time: 7.278974533081055 and batch: 650, loss is 4.775300931930542 and perplexity is 118.54598352517822
At time: 7.819713830947876 and batch: 700, loss is 4.839686851501465 and perplexity is 126.42975424278028
At time: 8.357874393463135 and batch: 750, loss is 4.846409549713135 and perplexity is 127.28256671408874
At time: 8.895776987075806 and batch: 800, loss is 4.792290916442871 and perplexity is 120.57728497672144
At time: 9.405735969543457 and batch: 850, loss is 4.831792917251587 and perplexity is 125.4356549151278
At time: 9.942132949829102 and batch: 900, loss is 4.749313573837281 and perplexity is 115.50497167461175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.773015270494435 and perplexity of 118.27533696326105
finished 1 epochs...
Completing Train Step...
At time: 11.406387567520142 and batch: 50, loss is 4.7399547576904295 and perplexity is 114.42902452969481
At time: 11.921464681625366 and batch: 100, loss is 4.609836187362671 and perplexity is 100.467690410979
At time: 12.432953834533691 and batch: 150, loss is 4.589727439880371 and perplexity is 98.46758816647353
At time: 12.945276498794556 and batch: 200, loss is 4.4773561477661135 and perplexity is 88.00170134578207
At time: 13.455239295959473 and batch: 250, loss is 4.596400260925293 and perplexity is 99.12684185579077
At time: 13.967210054397583 and batch: 300, loss is 4.551401014328003 and perplexity is 94.76508259244706
At time: 14.479624509811401 and batch: 350, loss is 4.530968608856202 and perplexity is 92.8484513731024
At time: 14.992148876190186 and batch: 400, loss is 4.419669184684754 and perplexity is 83.06880038098352
At time: 15.507472515106201 and batch: 450, loss is 4.446014652252197 and perplexity is 85.28636995754897
At time: 16.019827365875244 and batch: 500, loss is 4.337450561523437 and perplexity is 76.51222725991597
At time: 16.52959179878235 and batch: 550, loss is 4.4157420253753665 and perplexity is 82.74321569815304
At time: 17.041094541549683 and batch: 600, loss is 4.390208415985107 and perplexity is 80.65722748435788
At time: 17.557531356811523 and batch: 650, loss is 4.239884634017944 and perplexity is 69.39984499563295
At time: 18.06851077079773 and batch: 700, loss is 4.274851002693176 and perplexity is 71.86943016032481
At time: 18.578418970108032 and batch: 750, loss is 4.343898344039917 and perplexity is 77.00715534077975
At time: 19.09033489227295 and batch: 800, loss is 4.295218725204467 and perplexity is 73.34825580288326
At time: 19.601982593536377 and batch: 850, loss is 4.364789099693298 and perplexity is 78.63281450820526
At time: 20.113487720489502 and batch: 900, loss is 4.3022710847854615 and perplexity is 73.86736238908503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.495459987692637 and perplexity of 89.60937871778893
finished 2 epochs...
Completing Train Step...
At time: 21.534305334091187 and batch: 50, loss is 4.375131015777588 and perplexity is 79.45024810643925
At time: 22.06042218208313 and batch: 100, loss is 4.2513175249099735 and perplexity is 70.19783884751713
At time: 22.569657564163208 and batch: 150, loss is 4.2388706874847415 and perplexity is 69.3295129258996
At time: 23.080924034118652 and batch: 200, loss is 4.139076995849609 and perplexity is 62.74488092813199
At time: 23.58881950378418 and batch: 250, loss is 4.283568568229676 and perplexity is 72.49869547349249
At time: 24.114603757858276 and batch: 300, loss is 4.252056527137756 and perplexity is 70.24973437990622
At time: 24.621127367019653 and batch: 350, loss is 4.2407852029800415 and perplexity is 69.46237249294613
At time: 25.13048267364502 and batch: 400, loss is 4.1537679052352905 and perplexity is 63.673464455760474
At time: 25.640085220336914 and batch: 450, loss is 4.184796876907349 and perplexity is 65.68015840479643
At time: 26.151593446731567 and batch: 500, loss is 4.065358629226685 and perplexity is 58.28580776895458
At time: 26.6626615524292 and batch: 550, loss is 4.150389552116394 and perplexity is 63.458715960897614
At time: 27.17237138748169 and batch: 600, loss is 4.1496604347229 and perplexity is 63.41246397093594
At time: 27.681424617767334 and batch: 650, loss is 3.995711045265198 and perplexity is 54.364482491996974
At time: 28.19332480430603 and batch: 700, loss is 4.012023816108703 and perplexity is 55.25859070082933
At time: 28.706955194473267 and batch: 750, loss is 4.108816542625427 and perplexity is 60.8746323905402
At time: 29.21680736541748 and batch: 800, loss is 4.069052319526673 and perplexity is 58.50149558851592
At time: 29.727817058563232 and batch: 850, loss is 4.141417798995971 and perplexity is 62.89192637790953
At time: 30.240713119506836 and batch: 900, loss is 4.089825406074524 and perplexity is 59.729462392418775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.392334820473031 and perplexity of 80.82891985388302
finished 3 epochs...
Completing Train Step...
At time: 31.66539239883423 and batch: 50, loss is 4.1735672187805175 and perplexity is 64.9467185272687
At time: 32.192020893096924 and batch: 100, loss is 4.052663698196411 and perplexity is 57.550550354922805
At time: 32.70577573776245 and batch: 150, loss is 4.046670327186584 and perplexity is 57.20666011511468
At time: 33.21628737449646 and batch: 200, loss is 3.951096796989441 and perplexity is 51.992360638424024
At time: 33.72848677635193 and batch: 250, loss is 4.102772603034973 and perplexity is 60.507819404160905
At time: 34.237194299697876 and batch: 300, loss is 4.071815733909607 and perplexity is 58.66338304088496
At time: 34.74815034866333 and batch: 350, loss is 4.065716261863709 and perplexity is 58.30665640394199
At time: 35.259665727615356 and batch: 400, loss is 3.9880689191818237 and perplexity is 53.95060572582122
At time: 35.771798849105835 and batch: 450, loss is 4.021694784164429 and perplexity is 55.7955872190609
At time: 36.282851696014404 and batch: 500, loss is 3.897319612503052 and perplexity is 49.2702087055596
At time: 36.79243326187134 and batch: 550, loss is 3.9841207027435304 and perplexity is 53.738017006736285
At time: 37.320390462875366 and batch: 600, loss is 3.995429630279541 and perplexity is 54.349185664415465
At time: 37.83377647399902 and batch: 650, loss is 3.8387912797927854 and perplexity is 46.469272131958824
At time: 38.34426426887512 and batch: 700, loss is 3.8485760068893433 and perplexity is 46.92619305592328
At time: 38.85697937011719 and batch: 750, loss is 3.956070971488953 and perplexity is 52.25162398888787
At time: 39.36416578292847 and batch: 800, loss is 3.922817931175232 and perplexity is 50.542670059063326
At time: 39.87508225440979 and batch: 850, loss is 3.993444972038269 and perplexity is 54.24142807154778
At time: 40.38841223716736 and batch: 900, loss is 3.949176216125488 and perplexity is 51.89260093446408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348538751471533 and perplexity of 77.36533031288559
finished 4 epochs...
Completing Train Step...
At time: 41.82670736312866 and batch: 50, loss is 4.0320064115524294 and perplexity is 56.37390710959904
At time: 42.338058948516846 and batch: 100, loss is 3.912797031402588 and perplexity is 50.0387162804925
At time: 42.84889316558838 and batch: 150, loss is 3.9145557165145872 and perplexity is 50.12679605543062
At time: 43.35786747932434 and batch: 200, loss is 3.8160151720046995 and perplexity is 45.422844985662756
At time: 43.86941409111023 and batch: 250, loss is 3.9668821477890015 and perplexity is 52.81959016698518
At time: 44.38008189201355 and batch: 300, loss is 3.940933585166931 and perplexity is 51.466627358725
At time: 44.891756534576416 and batch: 350, loss is 3.940684642791748 and perplexity is 51.453816728888114
At time: 45.403767824172974 and batch: 400, loss is 3.8652474069595337 and perplexity is 47.715076014365
At time: 45.91487765312195 and batch: 450, loss is 3.900999174118042 and perplexity is 49.45183542270052
At time: 46.42620038986206 and batch: 500, loss is 3.7803669071197508 and perplexity is 43.83212110288284
At time: 46.93570947647095 and batch: 550, loss is 3.8604252481460573 and perplexity is 47.4855402138553
At time: 47.44988560676575 and batch: 600, loss is 3.881066527366638 and perplexity is 48.475888372523535
At time: 47.96307873725891 and batch: 650, loss is 3.7227194786071776 and perplexity is 41.37676447077179
At time: 48.47385621070862 and batch: 700, loss is 3.7328460359573366 and perplexity is 41.79789736351847
At time: 48.98277497291565 and batch: 750, loss is 3.8421877908706663 and perplexity is 46.62737387470418
At time: 49.493666887283325 and batch: 800, loss is 3.8097405195236207 and perplexity is 45.13872472879367
At time: 50.01974153518677 and batch: 850, loss is 3.882401189804077 and perplexity is 48.54063051469791
At time: 50.53100037574768 and batch: 900, loss is 3.840751895904541 and perplexity is 46.56046990830331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332770778708262 and perplexity of 76.1550031711548
finished 5 epochs...
Completing Train Step...
At time: 51.9621798992157 and batch: 50, loss is 3.9240853881835935 and perplexity is 50.60677133467523
At time: 52.485431432724 and batch: 100, loss is 3.809409203529358 and perplexity is 45.123772024502216
At time: 52.991801500320435 and batch: 150, loss is 3.8115522193908693 and perplexity is 45.220576673583096
At time: 53.500070571899414 and batch: 200, loss is 3.7172600078582763 and perplexity is 41.15148474903863
At time: 54.00915336608887 and batch: 250, loss is 3.8664926862716675 and perplexity is 47.77453162313913
At time: 54.51450228691101 and batch: 300, loss is 3.84137930393219 and perplexity is 46.58969148686282
At time: 55.02910256385803 and batch: 350, loss is 3.8438546228408814 and perplexity is 46.70515868125758
At time: 55.53863596916199 and batch: 400, loss is 3.7696180009841918 and perplexity is 43.363496858455626
At time: 56.05076813697815 and batch: 450, loss is 3.807566456794739 and perplexity is 45.04069690765881
At time: 56.56208872795105 and batch: 500, loss is 3.687344121932983 and perplexity is 39.93863383354156
At time: 57.073275327682495 and batch: 550, loss is 3.763640055656433 and perplexity is 43.105045518668874
At time: 57.58134055137634 and batch: 600, loss is 3.7910931873321534 and perplexity is 44.3048072662653
At time: 58.09158802032471 and batch: 650, loss is 3.634785461425781 and perplexity is 37.893722359849455
At time: 58.60590434074402 and batch: 700, loss is 3.6431116771698 and perplexity is 38.21055082828726
At time: 59.11788606643677 and batch: 750, loss is 3.752662305831909 and perplexity is 42.63443695046963
At time: 59.629974126815796 and batch: 800, loss is 3.7231037521362307 and perplexity is 41.39266752144078
At time: 60.139896392822266 and batch: 850, loss is 3.7936443996429445 and perplexity is 44.41798254168302
At time: 60.64978623390198 and batch: 900, loss is 3.7539614248275757 and perplexity is 42.68986015025249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332435085348887 and perplexity of 76.12944273278181
finished 6 epochs...
Completing Train Step...
At time: 62.070412397384644 and batch: 50, loss is 3.8412927341461183 and perplexity is 46.58565840181183
At time: 62.59618401527405 and batch: 100, loss is 3.7258553647994996 and perplexity is 41.50672095303084
At time: 63.10647177696228 and batch: 150, loss is 3.7336443996429445 and perplexity is 41.83128061112141
At time: 63.631160736083984 and batch: 200, loss is 3.638181381225586 and perplexity is 38.02262514978941
At time: 64.14388155937195 and batch: 250, loss is 3.7841056299209597 and perplexity is 43.99630397936116
At time: 64.65687370300293 and batch: 300, loss is 3.7647370862960816 and perplexity is 43.152359021762535
At time: 65.16732263565063 and batch: 350, loss is 3.768025507926941 and perplexity is 43.29449574724813
At time: 65.67883086204529 and batch: 400, loss is 3.6914968061447144 and perplexity is 40.10483121147746
At time: 66.192214012146 and batch: 450, loss is 3.7328370094299315 and perplexity is 41.79752007535525
At time: 66.70098328590393 and batch: 500, loss is 3.6164365673065184 and perplexity is 37.204754688052354
At time: 67.21302199363708 and batch: 550, loss is 3.685768179893494 and perplexity is 39.87574243110241
At time: 67.72907638549805 and batch: 600, loss is 3.718635444641113 and perplexity is 41.20812495841504
At time: 68.24209094047546 and batch: 650, loss is 3.5613309144973755 and perplexity is 35.21002751065463
At time: 68.75276947021484 and batch: 700, loss is 3.5689808177947997 and perplexity is 35.48041371175138
At time: 69.2673888206482 and batch: 750, loss is 3.682597322463989 and perplexity is 39.749502387319744
At time: 69.77974271774292 and batch: 800, loss is 3.6491267347335814 and perplexity is 38.44108212550565
At time: 70.29045867919922 and batch: 850, loss is 3.7214612627029418 and perplexity is 41.32473630584498
At time: 70.80157017707825 and batch: 900, loss is 3.6820063400268555 and perplexity is 39.7260180696172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334616883160317 and perplexity of 76.29572311344563
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 72.27324271202087 and batch: 50, loss is 3.789186120033264 and perplexity is 44.220395532156225
At time: 72.7869131565094 and batch: 100, loss is 3.6710570001602174 and perplexity is 39.293417065898296
At time: 73.29867839813232 and batch: 150, loss is 3.680764093399048 and perplexity is 39.67669919707786
At time: 73.81062126159668 and batch: 200, loss is 3.571066460609436 and perplexity is 35.554490403577425
At time: 74.32473945617676 and batch: 250, loss is 3.7036212682724 and perplexity is 40.59404042502589
At time: 74.83805966377258 and batch: 300, loss is 3.6725311517715453 and perplexity is 39.35138423568146
At time: 75.34761238098145 and batch: 350, loss is 3.664299111366272 and perplexity is 39.02877175200788
At time: 75.85932683944702 and batch: 400, loss is 3.586019377708435 and perplexity is 36.09012844647797
At time: 76.38310766220093 and batch: 450, loss is 3.6083382511138917 and perplexity is 36.90467552867608
At time: 76.89419341087341 and batch: 500, loss is 3.4822347021102904 and perplexity is 32.53234099589216
At time: 77.40614795684814 and batch: 550, loss is 3.5384787368774413 and perplexity is 34.414525799915126
At time: 77.91699361801147 and batch: 600, loss is 3.5606388568878176 and perplexity is 35.18566857304942
At time: 78.4282455444336 and batch: 650, loss is 3.3951483535766602 and perplexity is 29.81907691352632
At time: 78.93903112411499 and batch: 700, loss is 3.386672282218933 and perplexity is 29.567396427470065
At time: 79.45050859451294 and batch: 750, loss is 3.487549595832825 and perplexity is 32.70570723429135
At time: 79.96169996261597 and batch: 800, loss is 3.435070443153381 and perplexity is 31.033598593077492
At time: 80.47327446937561 and batch: 850, loss is 3.495612802505493 and perplexity is 32.97048616014282
At time: 80.98335695266724 and batch: 900, loss is 3.4447878646850585 and perplexity is 31.336635134559945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.281501247458262 and perplexity of 72.34897223074874
finished 8 epochs...
Completing Train Step...
At time: 82.41650080680847 and batch: 50, loss is 3.697284574508667 and perplexity is 40.33762170368865
At time: 82.94641208648682 and batch: 100, loss is 3.577029242515564 and perplexity is 35.767127399632585
At time: 83.46141314506531 and batch: 150, loss is 3.5881904649734495 and perplexity is 36.16856838390413
At time: 83.97389888763428 and batch: 200, loss is 3.483884544372559 and perplexity is 32.58605852749981
At time: 84.48816394805908 and batch: 250, loss is 3.617540102005005 and perplexity is 37.245834087902026
At time: 85.00103521347046 and batch: 300, loss is 3.5936605167388915 and perplexity is 36.36695442152717
At time: 85.51516270637512 and batch: 350, loss is 3.590202851295471 and perplexity is 36.24142680126038
At time: 86.03220796585083 and batch: 400, loss is 3.5157201433181764 and perplexity is 33.640144928611946
At time: 86.54914617538452 and batch: 450, loss is 3.543891372680664 and perplexity is 34.60130411992409
At time: 87.06475520133972 and batch: 500, loss is 3.420958580970764 and perplexity is 30.59873232983902
At time: 87.58024477958679 and batch: 550, loss is 3.4813898849487304 and perplexity is 32.50486872207125
At time: 88.09332418441772 and batch: 600, loss is 3.509147777557373 and perplexity is 33.41977456247835
At time: 88.6097207069397 and batch: 650, loss is 3.348791828155518 and perplexity is 28.468318237536803
At time: 89.12355923652649 and batch: 700, loss is 3.3453230333328245 and perplexity is 28.36973855781193
At time: 89.65463447570801 and batch: 750, loss is 3.453343563079834 and perplexity is 31.605892131842968
At time: 90.18562936782837 and batch: 800, loss is 3.4061705493927 and perplexity is 30.14956663363266
At time: 90.7169418334961 and batch: 850, loss is 3.472656307220459 and perplexity is 32.222220984120696
At time: 91.2303729057312 and batch: 900, loss is 3.429899282455444 and perplexity is 30.87353308663661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.286455546339897 and perplexity of 72.70830003665533
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 92.64758372306824 and batch: 50, loss is 3.6709287214279174 and perplexity is 39.288376879450624
At time: 93.16882944107056 and batch: 100, loss is 3.559056029319763 and perplexity is 35.13001977967199
At time: 93.67747592926025 and batch: 150, loss is 3.575391664505005 and perplexity is 35.70860386980322
At time: 94.19147825241089 and batch: 200, loss is 3.4637880659103395 and perplexity is 31.93772988524102
At time: 94.70095038414001 and batch: 250, loss is 3.598919725418091 and perplexity is 36.55871964852293
At time: 95.21056962013245 and batch: 300, loss is 3.5708966588974 and perplexity is 35.5484537027719
At time: 95.72150874137878 and batch: 350, loss is 3.562660131454468 and perplexity is 35.256860394918014
At time: 96.23107743263245 and batch: 400, loss is 3.487889370918274 and perplexity is 32.71682170686506
At time: 96.74230694770813 and batch: 450, loss is 3.5125214099884032 and perplexity is 33.532710993669
At time: 97.25179648399353 and batch: 500, loss is 3.3843829679489135 and perplexity is 29.499784786577646
At time: 97.76200985908508 and batch: 550, loss is 3.437629852294922 and perplexity is 31.11312799974314
At time: 98.27344679832458 and batch: 600, loss is 3.467402377128601 and perplexity is 32.053371637404105
At time: 98.78584170341492 and batch: 650, loss is 3.29998929977417 and perplexity is 27.112348810850712
At time: 99.29561018943787 and batch: 700, loss is 3.2908301162719726 and perplexity is 26.8651556049804
At time: 99.81095623970032 and batch: 750, loss is 3.3951293992996217 and perplexity is 29.8185117198379
At time: 100.3253242969513 and batch: 800, loss is 3.3430069065093995 and perplexity is 28.304106680580162
At time: 100.83703017234802 and batch: 850, loss is 3.405170245170593 and perplexity is 30.119422973762962
At time: 101.34998059272766 and batch: 900, loss is 3.3650466775894166 and perplexity is 28.934847876478074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270936730789812 and perplexity of 71.58866352609229
finished 10 epochs...
Completing Train Step...
At time: 102.78845643997192 and batch: 50, loss is 3.6468552780151366 and perplexity is 38.35386396488612
At time: 103.30026531219482 and batch: 100, loss is 3.5272979068756105 and perplexity is 34.0318859391433
At time: 103.81081104278564 and batch: 150, loss is 3.5422729730606077 and perplexity is 34.54535067221506
At time: 104.32193446159363 and batch: 200, loss is 3.4325934219360352 and perplexity is 30.956822837754956
At time: 104.83301401138306 and batch: 250, loss is 3.5680140256881714 and perplexity is 35.44612810403294
At time: 105.34317111968994 and batch: 300, loss is 3.5425484561920166 and perplexity is 34.55486864455408
At time: 105.85151743888855 and batch: 350, loss is 3.535798931121826 and perplexity is 34.32242501682145
At time: 106.3610486984253 and batch: 400, loss is 3.463292708396912 and perplexity is 31.92191320855878
At time: 106.87206149101257 and batch: 450, loss is 3.4903408002853396 and perplexity is 32.79712307074084
At time: 107.38507676124573 and batch: 500, loss is 3.3638762521743772 and perplexity is 28.901001606274395
At time: 107.89813661575317 and batch: 550, loss is 3.419828691482544 and perplexity is 30.564178668412165
At time: 108.41307497024536 and batch: 600, loss is 3.4515549898147584 and perplexity is 31.549413201565766
At time: 108.92663025856018 and batch: 650, loss is 3.2871997213363646 and perplexity is 26.767801304333336
At time: 109.43949770927429 and batch: 700, loss is 3.2806639862060547 and perplexity is 26.593424505991283
At time: 109.94999837875366 and batch: 750, loss is 3.388169279098511 and perplexity is 29.61169187446002
At time: 110.46401953697205 and batch: 800, loss is 3.338786768913269 and perplexity is 28.18491114353777
At time: 110.97401332855225 and batch: 850, loss is 3.404114398956299 and perplexity is 30.087638277867633
At time: 111.4835114479065 and batch: 900, loss is 3.366878743171692 and perplexity is 28.987907004461125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271810035183005 and perplexity of 71.6512095273262
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 112.90771532058716 and batch: 50, loss is 3.6390692663192747 and perplexity is 38.056399863696505
At time: 113.43209743499756 and batch: 100, loss is 3.526639003753662 and perplexity is 34.009469609158124
At time: 113.94157433509827 and batch: 150, loss is 3.5420732355117797 and perplexity is 34.53845135759742
At time: 114.4572491645813 and batch: 200, loss is 3.4305643892288207 and perplexity is 30.894074112841203
At time: 114.96764826774597 and batch: 250, loss is 3.566002788543701 and perplexity is 35.37490917764468
At time: 115.49434018135071 and batch: 300, loss is 3.540581784248352 and perplexity is 34.48697733572246
At time: 116.0034441947937 and batch: 350, loss is 3.529686369895935 and perplexity is 34.11326698934708
At time: 116.51315188407898 and batch: 400, loss is 3.45881965637207 and perplexity is 31.779443704342956
At time: 117.0231704711914 and batch: 450, loss is 3.483105435371399 and perplexity is 32.56068032346605
At time: 117.53316879272461 and batch: 500, loss is 3.357130699157715 and perplexity is 28.706704425421673
At time: 118.04502749443054 and batch: 550, loss is 3.407376003265381 and perplexity is 30.18593245975013
At time: 118.55313014984131 and batch: 600, loss is 3.440400023460388 and perplexity is 31.19943617881701
At time: 119.06236481666565 and batch: 650, loss is 3.269836926460266 and perplexity is 26.30704900985348
At time: 119.57476353645325 and batch: 700, loss is 3.2643485355377195 and perplexity is 26.163061133185053
At time: 120.08654165267944 and batch: 750, loss is 3.3725592136383056 and perplexity is 29.153040528100657
At time: 120.60015344619751 and batch: 800, loss is 3.3199731349945067 and perplexity is 27.659607473028593
At time: 121.11001062393188 and batch: 850, loss is 3.3810220193862914 and perplexity is 29.400803955216716
At time: 121.62229919433594 and batch: 900, loss is 3.345847096443176 and perplexity is 28.384609987683938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2668870899775255 and perplexity of 71.29934137337574
finished 12 epochs...
Completing Train Step...
At time: 123.04585289955139 and batch: 50, loss is 3.6307450103759766 and perplexity is 37.740923525537994
At time: 123.57494854927063 and batch: 100, loss is 3.513696942329407 and perplexity is 33.572152958033946
At time: 124.0848639011383 and batch: 150, loss is 3.529482936859131 and perplexity is 34.10632792968905
At time: 124.59753346443176 and batch: 200, loss is 3.418455386161804 and perplexity is 30.522233527557614
At time: 125.10675382614136 and batch: 250, loss is 3.5549103260040282 and perplexity is 34.9846826107385
At time: 125.61759424209595 and batch: 300, loss is 3.5299635791778563 and perplexity is 34.12272481443107
At time: 126.12936902046204 and batch: 350, loss is 3.519864492416382 and perplexity is 33.779850727750485
At time: 126.64010071754456 and batch: 400, loss is 3.4507057332992552 and perplexity is 31.522631030920188
At time: 127.14564156532288 and batch: 450, loss is 3.4748779058456423 and perplexity is 32.2938854012697
At time: 127.65553283691406 and batch: 500, loss is 3.3498770093917845 and perplexity is 28.49922829078779
At time: 128.1638879776001 and batch: 550, loss is 3.401963262557983 and perplexity is 30.022985227724693
At time: 128.68992710113525 and batch: 600, loss is 3.436039004325867 and perplexity is 31.06367109289415
At time: 129.19949650764465 and batch: 650, loss is 3.2667318391799927 and perplexity is 26.22549001601833
At time: 129.7100327014923 and batch: 700, loss is 3.2624496030807495 and perplexity is 26.113426388654258
At time: 130.21958446502686 and batch: 750, loss is 3.3716243839263917 and perplexity is 29.125800134170962
At time: 130.73147583007812 and batch: 800, loss is 3.3202046728134156 and perplexity is 27.66601245968448
At time: 131.24371123313904 and batch: 850, loss is 3.382931489944458 and perplexity is 29.456997557696525
At time: 131.7554488182068 and batch: 900, loss is 3.3492712020874023 and perplexity is 28.48196847870796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266587766882491 and perplexity of 71.27800302753113
finished 13 epochs...
Completing Train Step...
At time: 133.19061493873596 and batch: 50, loss is 3.62540246963501 and perplexity is 37.539828760803886
At time: 133.70355796813965 and batch: 100, loss is 3.5077018213272093 and perplexity is 33.371485951261626
At time: 134.21557569503784 and batch: 150, loss is 3.5232614421844484 and perplexity is 33.89479430208916
At time: 134.72616147994995 and batch: 200, loss is 3.412210741043091 and perplexity is 30.332226890408506
At time: 135.23842334747314 and batch: 250, loss is 3.5486379957199095 and perplexity is 34.765933875845704
At time: 135.75150156021118 and batch: 300, loss is 3.524018654823303 and perplexity is 33.92046958832491
At time: 136.26253175735474 and batch: 350, loss is 3.5142565393447875 and perplexity is 33.59094509214985
At time: 136.77090120315552 and batch: 400, loss is 3.4457253122329714 and perplexity is 31.366025360071735
At time: 137.2815124988556 and batch: 450, loss is 3.4701052951812743 and perplexity is 32.14012646703193
At time: 137.7922670841217 and batch: 500, loss is 3.345600390434265 and perplexity is 28.37760819756674
At time: 138.30881023406982 and batch: 550, loss is 3.3984187173843385 and perplexity is 29.916755779097514
At time: 138.8241765499115 and batch: 600, loss is 3.4330425596237184 and perplexity is 30.970729836427154
At time: 139.33633518218994 and batch: 650, loss is 3.264304685592651 and perplexity is 26.161913909544573
At time: 139.8494336605072 and batch: 700, loss is 3.2606818008422853 and perplexity is 26.067303794854276
At time: 140.36143922805786 and batch: 750, loss is 3.370503349304199 and perplexity is 29.093167398452287
At time: 140.87200474739075 and batch: 800, loss is 3.3196302604675294 and perplexity is 27.650125323886495
At time: 141.39639949798584 and batch: 850, loss is 3.38306423664093 and perplexity is 29.46090813636275
At time: 141.9053032398224 and batch: 900, loss is 3.350014691352844 and perplexity is 28.50315239056023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266857408497431 and perplexity of 71.29722513480074
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 143.33303236961365 and batch: 50, loss is 3.6232720518112185 and perplexity is 37.45993837068173
At time: 143.86053562164307 and batch: 100, loss is 3.507525796890259 and perplexity is 33.36561227120763
At time: 144.37163710594177 and batch: 150, loss is 3.5236798524856567 and perplexity is 33.908979200529366
At time: 144.88129806518555 and batch: 200, loss is 3.4122953462600707 and perplexity is 30.334793263608802
At time: 145.39261317253113 and batch: 250, loss is 3.5486341190338133 and perplexity is 34.765799099494465
At time: 145.90227055549622 and batch: 300, loss is 3.5240373706817625 and perplexity is 33.92110444497354
At time: 146.41465306282043 and batch: 350, loss is 3.5130442667007444 and perplexity is 33.55024838106586
At time: 146.9266493320465 and batch: 400, loss is 3.4451488637924195 and perplexity is 31.347949674017663
At time: 147.43833589553833 and batch: 450, loss is 3.4674085712432863 and perplexity is 32.05357018027897
At time: 147.94972038269043 and batch: 500, loss is 3.343561415672302 and perplexity is 28.31980591936593
At time: 148.4632248878479 and batch: 550, loss is 3.3955851125717165 and perplexity is 29.832103508126682
At time: 148.97422313690186 and batch: 600, loss is 3.4298425054550172 and perplexity is 30.871780229796816
At time: 149.4847810268402 and batch: 650, loss is 3.2577840423583986 and perplexity is 25.991876382123614
At time: 149.99380803108215 and batch: 700, loss is 3.2540518283843993 and perplexity is 25.895049938776914
At time: 150.5059745311737 and batch: 750, loss is 3.363192934989929 and perplexity is 28.881259800954638
At time: 151.01920127868652 and batch: 800, loss is 3.3121712255477904 and perplexity is 27.444649352458285
At time: 151.52799248695374 and batch: 850, loss is 3.3749999618530273 and perplexity is 29.224282666417007
At time: 152.04022407531738 and batch: 900, loss is 3.340451488494873 and perplexity is 28.23187019300401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266258344258348 and perplexity of 71.25452630782371
finished 15 epochs...
Completing Train Step...
At time: 153.48334670066833 and batch: 50, loss is 3.6205675983428955 and perplexity is 37.358766579297814
At time: 154.0106565952301 and batch: 100, loss is 3.5040310430526733 and perplexity is 33.249211184721425
At time: 154.52609133720398 and batch: 150, loss is 3.520406889915466 and perplexity is 33.79817780413554
At time: 155.05044627189636 and batch: 200, loss is 3.4093409156799317 and perplexity is 30.245303483513876
At time: 155.56375241279602 and batch: 250, loss is 3.5457160663604737 and perplexity is 34.664498538559165
At time: 156.0749068260193 and batch: 300, loss is 3.521092162132263 and perplexity is 33.821346693954645
At time: 156.5873532295227 and batch: 350, loss is 3.510069003105164 and perplexity is 33.450575897916956
At time: 157.0983726978302 and batch: 400, loss is 3.442663426399231 and perplexity is 31.27013305192794
At time: 157.60936856269836 and batch: 450, loss is 3.465103087425232 and perplexity is 31.979756313949615
At time: 158.11985754966736 and batch: 500, loss is 3.341392045021057 and perplexity is 28.25843635429014
At time: 158.62987542152405 and batch: 550, loss is 3.3938621377944944 and perplexity is 29.780747801229253
At time: 159.1407265663147 and batch: 600, loss is 3.4286117553710938 and perplexity is 30.83380815554808
At time: 159.65000987052917 and batch: 650, loss is 3.257059154510498 and perplexity is 25.973042014017093
At time: 160.15929126739502 and batch: 700, loss is 3.2536354732513426 and perplexity is 25.884270645976805
At time: 160.67188048362732 and batch: 750, loss is 3.363470392227173 and perplexity is 28.88927422728601
At time: 161.18078565597534 and batch: 800, loss is 3.312808246612549 and perplexity is 27.46213774185554
At time: 161.688791513443 and batch: 850, loss is 3.3762029504776 and perplexity is 29.25946030093071
At time: 162.20064043998718 and batch: 900, loss is 3.3423775005340577 and perplexity is 28.286297511895217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265934356271404 and perplexity of 71.23144443661101
finished 16 epochs...
Completing Train Step...
At time: 163.66305494308472 and batch: 50, loss is 3.6188526248931883 and perplexity is 37.29475219367595
At time: 164.17490243911743 and batch: 100, loss is 3.501981930732727 and perplexity is 33.18114957321275
At time: 164.68563961982727 and batch: 150, loss is 3.5184066915512084 and perplexity is 33.73064230888668
At time: 165.19546818733215 and batch: 200, loss is 3.407371735572815 and perplexity is 30.185803635745465
At time: 165.70532250404358 and batch: 250, loss is 3.5437501764297483 and perplexity is 34.59641889040172
At time: 166.21658730506897 and batch: 300, loss is 3.51919517993927 and perplexity is 33.75724901683074
At time: 166.7288794517517 and batch: 350, loss is 3.508277473449707 and perplexity is 33.39070184828569
At time: 167.24264788627625 and batch: 400, loss is 3.4410433673858645 and perplexity is 31.21951460453502
At time: 167.7702295780182 and batch: 450, loss is 3.46358172416687 and perplexity is 31.931140478232287
At time: 168.2796220779419 and batch: 500, loss is 3.3400449848175047 and perplexity is 28.220396166226614
At time: 168.7906358242035 and batch: 550, loss is 3.392788276672363 and perplexity is 29.748784579079924
At time: 169.30100750923157 and batch: 600, loss is 3.4278330755233766 and perplexity is 30.80980783600426
At time: 169.81028079986572 and batch: 650, loss is 3.256536531448364 and perplexity is 25.95947144972061
At time: 170.32159280776978 and batch: 700, loss is 3.253379964828491 and perplexity is 25.87765784165697
At time: 170.83281660079956 and batch: 750, loss is 3.363524899482727 and perplexity is 28.890848945255474
At time: 171.34463715553284 and batch: 800, loss is 3.3130497884750367 and perplexity is 27.4687717989226
At time: 171.85466742515564 and batch: 850, loss is 3.376759901046753 and perplexity is 29.275760912894622
At time: 172.36466431617737 and batch: 900, loss is 3.343285846710205 and perplexity is 28.312002935021464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265820646939212 and perplexity of 71.22334521712021
finished 17 epochs...
Completing Train Step...
At time: 173.80812859535217 and batch: 50, loss is 3.617386636734009 and perplexity is 37.240118584457115
At time: 174.33480262756348 and batch: 100, loss is 3.5003382062911985 and perplexity is 33.12665370702473
At time: 174.84473729133606 and batch: 150, loss is 3.5167747020721434 and perplexity is 33.67563915001267
At time: 175.35517001152039 and batch: 200, loss is 3.4057489395141602 and perplexity is 30.136857957746127
At time: 175.8671956062317 and batch: 250, loss is 3.542115626335144 and perplexity is 34.53991550202112
At time: 176.37941360473633 and batch: 300, loss is 3.517651047706604 and perplexity is 33.70516358428447
At time: 176.8921411037445 and batch: 350, loss is 3.5068331146240235 and perplexity is 33.34250850599329
At time: 177.40343141555786 and batch: 400, loss is 3.4397251176834107 and perplexity is 31.178386603140602
At time: 177.9136447906494 and batch: 450, loss is 3.462335696220398 and perplexity is 31.89137816245656
At time: 178.42339992523193 and batch: 500, loss is 3.3389570093154908 and perplexity is 28.189709762595264
At time: 178.93697905540466 and batch: 550, loss is 3.3918949794769286 and perplexity is 29.72222193918008
At time: 179.45010781288147 and batch: 600, loss is 3.427157831192017 and perplexity is 30.789010710273555
At time: 179.96036839485168 and batch: 650, loss is 3.256027646064758 and perplexity is 25.946264414852227
At time: 180.47306418418884 and batch: 700, loss is 3.2530818462371824 and perplexity is 25.86994438057505
At time: 180.99873566627502 and batch: 750, loss is 3.3634313631057737 and perplexity is 28.888146726297887
At time: 181.51123714447021 and batch: 800, loss is 3.3130899238586426 and perplexity is 27.46987429074025
At time: 182.0234534740448 and batch: 850, loss is 3.3770181369781493 and perplexity is 29.283321942504042
At time: 182.53488278388977 and batch: 900, loss is 3.343746156692505 and perplexity is 28.325038232499978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265815630351027 and perplexity of 71.22298791982432
finished 18 epochs...
Completing Train Step...
At time: 183.96146082878113 and batch: 50, loss is 3.616050148010254 and perplexity is 37.19038083027496
At time: 184.48781991004944 and batch: 100, loss is 3.4988878345489502 and perplexity is 33.078642569987316
At time: 185.00091290473938 and batch: 150, loss is 3.5153201055526733 and perplexity is 33.6266902915644
At time: 185.51456332206726 and batch: 200, loss is 3.404305143356323 and perplexity is 30.093377873759575
At time: 186.0274419784546 and batch: 250, loss is 3.5406584978103637 and perplexity is 34.48962305607691
At time: 186.54186463356018 and batch: 300, loss is 3.5162831258773806 and perplexity is 33.659089075607675
At time: 187.0540418624878 and batch: 350, loss is 3.5055493021011355 and perplexity is 33.299730441411775
At time: 187.56184482574463 and batch: 400, loss is 3.4385529232025145 and perplexity is 31.141860882247688
At time: 188.06462979316711 and batch: 450, loss is 3.461225371360779 and perplexity is 31.85598802338667
At time: 188.58149909973145 and batch: 500, loss is 3.337981786727905 and perplexity is 28.16223192158066
At time: 189.098650932312 and batch: 550, loss is 3.391077036857605 and perplexity is 29.697920806935763
At time: 189.61859011650085 and batch: 600, loss is 3.4265139961242674 and perplexity is 30.769194045493368
At time: 190.13179230690002 and batch: 650, loss is 3.2555079364776613 and perplexity is 25.93278339589731
At time: 190.64208579063416 and batch: 700, loss is 3.252735695838928 and perplexity is 25.860991038715724
At time: 191.15176343917847 and batch: 750, loss is 3.3632453060150147 and perplexity is 28.882772381743532
At time: 191.6652171611786 and batch: 800, loss is 3.3130124521255495 and perplexity is 27.467746234404267
At time: 192.1827507019043 and batch: 850, loss is 3.377112102508545 and perplexity is 29.28607369466502
At time: 192.68842554092407 and batch: 900, loss is 3.3439785718917845 and perplexity is 28.331622166978963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265870812821062 and perplexity of 71.22691828866368
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 194.1528148651123 and batch: 50, loss is 3.6154090547561646 and perplexity is 37.166545969008396
At time: 194.6647593975067 and batch: 100, loss is 3.4987163877487184 and perplexity is 33.0729718286918
At time: 195.18043518066406 and batch: 150, loss is 3.5153654527664187 and perplexity is 33.62821520285158
At time: 195.6943325996399 and batch: 200, loss is 3.404343595504761 and perplexity is 30.094535051040406
At time: 196.20635890960693 and batch: 250, loss is 3.540634665489197 and perplexity is 34.48880109809794
At time: 196.72186374664307 and batch: 300, loss is 3.516385416984558 and perplexity is 33.66253227719725
At time: 197.236474275589 and batch: 350, loss is 3.5050712251663207 and perplexity is 33.28381441319833
At time: 197.75413346290588 and batch: 400, loss is 3.4380699157714845 and perplexity is 31.126822764079233
At time: 198.2660140991211 and batch: 450, loss is 3.4602665805816653 and perplexity is 31.82545943341427
At time: 198.7795286178589 and batch: 500, loss is 3.3369727897644044 and perplexity is 28.133830645896396
At time: 199.29260897636414 and batch: 550, loss is 3.389737730026245 and perplexity is 29.658172802050554
At time: 199.80724430084229 and batch: 600, loss is 3.4251651191711425 and perplexity is 30.72771816799548
At time: 200.3197512626648 and batch: 650, loss is 3.253582572937012 and perplexity is 25.882901396140603
At time: 200.83297634124756 and batch: 700, loss is 3.2508186292648316 and perplexity is 25.811461288305935
At time: 201.34380078315735 and batch: 750, loss is 3.360525493621826 and perplexity is 28.804323391098254
At time: 201.86008834838867 and batch: 800, loss is 3.31010217666626 and perplexity is 27.387923735685245
At time: 202.37413096427917 and batch: 850, loss is 3.3740861225128174 and perplexity is 29.1975885661442
At time: 202.8801188468933 and batch: 900, loss is 3.340565366744995 and perplexity is 28.235085372045273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26566513270548 and perplexity of 71.21226983437707
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8ad1a05b70>
ELAPSED
1043.1865067481995


RESULTS SO FAR:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}, {'best_accuracy': -71.20988825665874, 'params': {'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}}, {'best_accuracy': -72.19443221175057, 'params': {'batch_size': 32, 'dropout': 0.982441793390317, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9171483614956}}, {'best_accuracy': -70.96593153993238, 'params': {'batch_size': 32, 'dropout': 0.703003270020777, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.11712669393286701}}, {'best_accuracy': -71.21226983437707, 'params': {'batch_size': 32, 'dropout': 0.10703692335741788, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.25510031122950216}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 1.0, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.0}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8100895881652832 and batch: 50, loss is 7.286971349716186 and perplexity is 1461.1387109519396
At time: 1.3254072666168213 and batch: 100, loss is 6.50290729522705 and perplexity is 667.078209875659
At time: 1.84027099609375 and batch: 150, loss is 6.469013032913208 and perplexity is 644.8469700522337
At time: 2.3549880981445312 and batch: 200, loss is 6.397906541824341 and perplexity is 600.586418350398
At time: 2.8703434467315674 and batch: 250, loss is 6.497211418151855 and perplexity is 663.2894148912358
At time: 3.397848129272461 and batch: 300, loss is 6.459877281188965 and perplexity is 638.9826365684149
At time: 3.9105100631713867 and batch: 350, loss is 6.4893911170959475 and perplexity is 658.1225216426141
At time: 4.423483610153198 and batch: 400, loss is 6.4264294624328615 and perplexity is 617.9635421500819
At time: 4.937150716781616 and batch: 450, loss is 6.4362398147583 and perplexity is 624.0558170429358
At time: 5.455278635025024 and batch: 500, loss is 6.432120780944825 and perplexity is 621.4905967724432
At time: 5.96845555305481 and batch: 550, loss is 6.4715178680419925 and perplexity is 646.4642300347351
At time: 6.484151840209961 and batch: 600, loss is 6.4445754909515385 and perplexity is 629.279485417842
At time: 6.999385595321655 and batch: 650, loss is 6.391737403869629 and perplexity is 596.8927230544183
At time: 7.512115001678467 and batch: 700, loss is 6.469462413787841 and perplexity is 645.1368170686159
At time: 8.027436256408691 and batch: 750, loss is 6.455672693252564 and perplexity is 636.3016181217494
At time: 8.543062925338745 and batch: 800, loss is 6.456993713378906 and perplexity is 637.1427408133947
At time: 9.057765007019043 and batch: 850, loss is 6.490346202850342 and perplexity is 658.7513853492611
At time: 9.571056604385376 and batch: 900, loss is 6.417553577423096 and perplexity is 612.5028389466602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.495233405126284 and perplexity of 661.9787165054435
finished 1 epochs...
Completing Train Step...
At time: 11.028418779373169 and batch: 50, loss is 5.979766006469727 and perplexity is 395.3478484924901
At time: 11.54379940032959 and batch: 100, loss is 5.533285751342773 and perplexity is 252.9737558029347
At time: 12.056137800216675 and batch: 150, loss is 5.342830963134766 and perplexity is 209.1038384212375
At time: 12.569492101669312 and batch: 200, loss is 5.132950372695923 and perplexity is 169.51651788067
At time: 13.083215475082397 and batch: 250, loss is 5.155130529403687 and perplexity is 173.31842843356753
At time: 13.595568895339966 and batch: 300, loss is 5.041711673736573 and perplexity is 154.73464370807156
At time: 14.108342409133911 and batch: 350, loss is 4.9892847442626955 and perplexity is 146.83136395246999
At time: 14.621220827102661 and batch: 400, loss is 4.825404825210572 and perplexity is 124.63691433670087
At time: 15.135130882263184 and batch: 450, loss is 4.817469110488892 and perplexity is 123.65174552019965
At time: 15.649474143981934 and batch: 500, loss is 4.716098213195801 and perplexity is 111.73144877169507
At time: 16.179605722427368 and batch: 550, loss is 4.769035034179687 and perplexity is 117.80550880575029
At time: 16.693440914154053 and batch: 600, loss is 4.702329769134521 and perplexity is 110.20362259975239
At time: 17.20750665664673 and batch: 650, loss is 4.559892091751099 and perplexity is 95.57316614061003
At time: 17.72016716003418 and batch: 700, loss is 4.595783996582031 and perplexity is 99.06577233711106
At time: 18.231664657592773 and batch: 750, loss is 4.617596883773803 and perplexity is 101.25042300183303
At time: 18.744176864624023 and batch: 800, loss is 4.553680400848389 and perplexity is 94.98133521233473
At time: 19.255842924118042 and batch: 850, loss is 4.603279218673706 and perplexity is 99.81108194378996
At time: 19.768314123153687 and batch: 900, loss is 4.5363491249084475 and perplexity is 93.34937034897236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.63490316312607 and perplexity of 103.01794157677355
finished 2 epochs...
Completing Train Step...
At time: 21.209041833877563 and batch: 50, loss is 4.618496885299683 and perplexity is 101.34158955589574
At time: 21.741464853286743 and batch: 100, loss is 4.472508382797241 and perplexity is 87.57612216816604
At time: 22.255820274353027 and batch: 150, loss is 4.452889757156372 and perplexity is 85.87474294334986
At time: 22.772053480148315 and batch: 200, loss is 4.3502356767654415 and perplexity is 77.49672495067749
At time: 23.28812885284424 and batch: 250, loss is 4.480888996124268 and perplexity is 88.3131478345739
At time: 23.803441762924194 and batch: 300, loss is 4.435805983543396 and perplexity is 84.42013871813303
At time: 24.31693148612976 and batch: 350, loss is 4.416888608932495 and perplexity is 82.8381421188734
At time: 24.83034324645996 and batch: 400, loss is 4.318107342720031 and perplexity is 75.04645657092685
At time: 25.34342622756958 and batch: 450, loss is 4.352013626098633 and perplexity is 77.63463276134965
At time: 25.85369300842285 and batch: 500, loss is 4.234720721244812 and perplexity is 69.04239396688841
At time: 26.367936849594116 and batch: 550, loss is 4.314737796783447 and perplexity is 74.79400964274723
At time: 26.87965989112854 and batch: 600, loss is 4.299525384902954 and perplexity is 73.6648229643327
At time: 27.393667697906494 and batch: 650, loss is 4.147624850273132 and perplexity is 63.28351383432819
At time: 27.906635761260986 and batch: 700, loss is 4.173135995864868 and perplexity is 64.91871805160079
At time: 28.418511867523193 and batch: 750, loss is 4.250387215614319 and perplexity is 70.13256351333537
At time: 28.935399770736694 and batch: 800, loss is 4.2056565093994145 and perplexity is 67.06461176488087
At time: 29.473265409469604 and batch: 850, loss is 4.268012952804566 and perplexity is 71.37965985673375
At time: 29.987422227859497 and batch: 900, loss is 4.213979578018188 and perplexity is 67.62512448770015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442148287002355 and perplexity of 84.9572583432989
finished 3 epochs...
Completing Train Step...
At time: 31.4263973236084 and batch: 50, loss is 4.324970350265503 and perplexity is 75.56327239554795
At time: 31.953338384628296 and batch: 100, loss is 4.182185587882995 and perplexity is 65.5088722641766
At time: 32.46522808074951 and batch: 150, loss is 4.173252229690552 and perplexity is 64.92626424111114
At time: 32.98020648956299 and batch: 200, loss is 4.073574910163879 and perplexity is 58.766673097395476
At time: 33.49570918083191 and batch: 250, loss is 4.219280915260315 and perplexity is 67.98458003433862
At time: 34.00888466835022 and batch: 300, loss is 4.189273195266724 and perplexity is 65.97482271732044
At time: 34.52244305610657 and batch: 350, loss is 4.1713375473022465 and perplexity is 64.80207000063415
At time: 35.03392267227173 and batch: 400, loss is 4.090180969238281 and perplexity is 59.7507037651376
At time: 35.545361280441284 and batch: 450, loss is 4.131365942955017 and perplexity is 62.26291246276018
At time: 36.05994772911072 and batch: 500, loss is 4.008935761451721 and perplexity is 55.088212356692026
At time: 36.576799392700195 and batch: 550, loss is 4.089955625534057 and perplexity is 59.73724083717119
At time: 37.09272265434265 and batch: 600, loss is 4.094260311126709 and perplexity is 59.99494514721791
At time: 37.606422901153564 and batch: 650, loss is 3.937030563354492 and perplexity is 51.26614349049377
At time: 38.11898756027222 and batch: 700, loss is 3.954106011390686 and perplexity is 52.14905244018326
At time: 38.632436752319336 and batch: 750, loss is 4.051178026199341 and perplexity is 57.46511259580263
At time: 39.14326620101929 and batch: 800, loss is 4.010077619552613 and perplexity is 55.151151205013534
At time: 39.65649747848511 and batch: 850, loss is 4.076695070266724 and perplexity is 58.95032088242645
At time: 40.16875720024109 and batch: 900, loss is 4.032374887466431 and perplexity is 56.39468336408669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37662704676798 and perplexity of 79.56919709330106
finished 4 epochs...
Completing Train Step...
At time: 41.64018225669861 and batch: 50, loss is 4.142130155563354 and perplexity is 62.93674381580307
At time: 42.153136014938354 and batch: 100, loss is 4.004685306549073 and perplexity is 54.85455931203865
At time: 42.68153667449951 and batch: 150, loss is 4.0058695936203 and perplexity is 54.91956134036335
At time: 43.19638419151306 and batch: 200, loss is 3.9065197610855105 and perplexity is 49.72559353832106
At time: 43.711639165878296 and batch: 250, loss is 4.054870376586914 and perplexity is 57.677686133049804
At time: 44.22594213485718 and batch: 300, loss is 4.0310582208633425 and perplexity is 56.320479229682334
At time: 44.73827934265137 and batch: 350, loss is 4.015567536354065 and perplexity is 55.4547590650385
At time: 45.25270915031433 and batch: 400, loss is 3.9401007413864138 and perplexity is 51.42378154263739
At time: 45.76728343963623 and batch: 450, loss is 3.980926947593689 and perplexity is 53.56666471246535
At time: 46.28637456893921 and batch: 500, loss is 3.857661304473877 and perplexity is 47.35447406862125
At time: 46.801047563552856 and batch: 550, loss is 3.9386900997161867 and perplexity is 51.35129215384954
At time: 47.31614017486572 and batch: 600, loss is 3.95269371509552 and perplexity is 52.075454509900794
At time: 47.8295783996582 and batch: 650, loss is 3.800716552734375 and perplexity is 44.733226727883704
At time: 48.34284448623657 and batch: 700, loss is 3.808971619606018 and perplexity is 45.10403090681672
At time: 48.85665559768677 and batch: 750, loss is 3.915344696044922 and perplexity is 50.1663606772256
At time: 49.367432594299316 and batch: 800, loss is 3.877449145317078 and perplexity is 48.300849346475715
At time: 49.87989807128906 and batch: 850, loss is 3.9427191066741942 and perplexity is 51.55860421767436
At time: 50.397536516189575 and batch: 900, loss is 3.9041091775894166 and perplexity is 49.605870202727694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3482728722977315 and perplexity of 77.34476321708354
finished 5 epochs...
Completing Train Step...
At time: 51.83816695213318 and batch: 50, loss is 4.016057801246643 and perplexity is 55.481953252167386
At time: 52.366432189941406 and batch: 100, loss is 3.881475501060486 and perplexity is 48.49571779023252
At time: 52.88061499595642 and batch: 150, loss is 3.8873395824432375 and perplexity is 48.780936080375
At time: 53.400761127471924 and batch: 200, loss is 3.7822675275802613 and perplexity is 43.915508547918535
At time: 53.916780948638916 and batch: 250, loss is 3.935524091720581 and perplexity is 51.188970643492816
At time: 54.42513465881348 and batch: 300, loss is 3.913244366645813 and perplexity is 50.061105369151356
At time: 54.93441867828369 and batch: 350, loss is 3.9013825464248657 and perplexity is 49.47079752146272
At time: 55.46942639350891 and batch: 400, loss is 3.831180171966553 and perplexity is 46.11693204125654
At time: 55.97997999191284 and batch: 450, loss is 3.8697726726531982 and perplexity is 47.93148870407495
At time: 56.48920774459839 and batch: 500, loss is 3.7483100414276125 and perplexity is 42.44928381809386
At time: 56.99689817428589 and batch: 550, loss is 3.830335612297058 and perplexity is 46.07799998291096
At time: 57.50887417793274 and batch: 600, loss is 3.847963390350342 and perplexity is 46.89745409782418
At time: 58.02030396461487 and batch: 650, loss is 3.694436149597168 and perplexity is 40.222886502036104
At time: 58.53092312812805 and batch: 700, loss is 3.7000936174392702 and perplexity is 40.451091110377135
At time: 59.04241919517517 and batch: 750, loss is 3.813583002090454 and perplexity is 45.31250314811144
At time: 59.55550765991211 and batch: 800, loss is 3.776572222709656 and perplexity is 43.66610722062925
At time: 60.06547427177429 and batch: 850, loss is 3.8392256259918214 and perplexity is 46.48946026768393
At time: 60.57551431655884 and batch: 900, loss is 3.805962882041931 and perplexity is 44.968528662301686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3399595495772685 and perplexity of 76.70443654865767
finished 6 epochs...
Completing Train Step...
At time: 62.01938986778259 and batch: 50, loss is 3.9164233922958376 and perplexity is 50.220504139334956
At time: 62.5450222492218 and batch: 100, loss is 3.7903040933609007 and perplexity is 44.2698603999481
At time: 63.05345320701599 and batch: 150, loss is 3.7942892265319825 and perplexity is 44.446633687704626
At time: 63.5613648891449 and batch: 200, loss is 3.6882143688201903 and perplexity is 39.97340543305816
At time: 64.0707585811615 and batch: 250, loss is 3.8414897871017457 and perplexity is 46.594839148006635
At time: 64.58117032051086 and batch: 300, loss is 3.8249279737472532 and perplexity is 45.829499320352596
At time: 65.09210467338562 and batch: 350, loss is 3.811395926475525 and perplexity is 45.21350957010471
At time: 65.60297846794128 and batch: 400, loss is 3.7456537961959837 and perplexity is 42.33667773126557
At time: 66.11504697799683 and batch: 450, loss is 3.783058681488037 and perplexity is 43.95026622163991
At time: 66.62477803230286 and batch: 500, loss is 3.6631826686859132 and perplexity is 38.98522268000675
At time: 67.13861036300659 and batch: 550, loss is 3.7414370489120485 and perplexity is 42.15853052527383
At time: 67.65103912353516 and batch: 600, loss is 3.7620974731445314 and perplexity is 43.03860368845493
At time: 68.16384840011597 and batch: 650, loss is 3.611819000244141 and perplexity is 37.0333552669487
At time: 68.69244074821472 and batch: 700, loss is 3.6154064893722535 and perplexity is 37.16645062267164
At time: 69.20545887947083 and batch: 750, loss is 3.7295077896118163 and perplexity is 41.658598322013454
At time: 69.71739888191223 and batch: 800, loss is 3.694064517021179 and perplexity is 40.20794114437454
At time: 70.23051929473877 and batch: 850, loss is 3.756889133453369 and perplexity is 42.815026758322134
At time: 70.74253606796265 and batch: 900, loss is 3.728054852485657 and perplexity is 41.59811494779776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3463506829248715 and perplexity of 77.19623473085225
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 72.2285635471344 and batch: 50, loss is 3.8611598348617555 and perplexity is 47.52043527603955
At time: 72.74252676963806 and batch: 100, loss is 3.733049540519714 and perplexity is 41.806404291899746
At time: 73.25691986083984 and batch: 150, loss is 3.7327588033676147 and perplexity is 41.79425138371296
At time: 73.76937198638916 and batch: 200, loss is 3.6094937229156496 and perplexity is 36.94734248608846
At time: 74.2813949584961 and batch: 250, loss is 3.7519165802001955 and perplexity is 42.602655209744164
At time: 74.79342603683472 and batch: 300, loss is 3.7284450054168703 and perplexity is 41.61434774070755
At time: 75.30640983581543 and batch: 350, loss is 3.700912022590637 and perplexity is 40.484210042223935
At time: 75.8188533782959 and batch: 400, loss is 3.628579158782959 and perplexity is 37.65927074202272
At time: 76.33207321166992 and batch: 450, loss is 3.6534122705459593 and perplexity is 38.60617626537148
At time: 76.84712886810303 and batch: 500, loss is 3.5269694709777832 and perplexity is 34.02071048144132
At time: 77.35991096496582 and batch: 550, loss is 3.581067099571228 and perplexity is 35.91184191901337
At time: 77.87323451042175 and batch: 600, loss is 3.5982300662994384 and perplexity is 36.53351528635916
At time: 78.38690400123596 and batch: 650, loss is 3.4380967521667483 and perplexity is 31.127658107006983
At time: 78.90599250793457 and batch: 700, loss is 3.4211345529556274 and perplexity is 30.6041173232915
At time: 79.4218430519104 and batch: 750, loss is 3.522249221801758 and perplexity is 33.86050265870758
At time: 79.93639206886292 and batch: 800, loss is 3.468461751937866 and perplexity is 32.087346164570505
At time: 80.45133233070374 and batch: 850, loss is 3.5226430606842043 and perplexity is 33.873840867614
At time: 80.96672058105469 and batch: 900, loss is 3.4819057273864744 and perplexity is 32.52164043820096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.283050537109375 and perplexity of 72.4611486191152
finished 8 epochs...
Completing Train Step...
At time: 82.42383599281311 and batch: 50, loss is 3.7631975507736204 and perplexity is 43.08597554514239
At time: 82.95184421539307 and batch: 100, loss is 3.6368098878860473 and perplexity is 37.97051311646654
At time: 83.46506786346436 and batch: 150, loss is 3.6366774463653564 and perplexity is 37.96548457696904
At time: 83.97935581207275 and batch: 200, loss is 3.517457046508789 and perplexity is 33.698625376407165
At time: 84.49145269393921 and batch: 250, loss is 3.66256299495697 and perplexity is 38.96107204522471
At time: 85.002436876297 and batch: 300, loss is 3.64588915348053 and perplexity is 38.316827249833054
At time: 85.51642394065857 and batch: 350, loss is 3.6221931743621827 and perplexity is 37.419545481338076
At time: 86.02970480918884 and batch: 400, loss is 3.5547351741790774 and perplexity is 34.978555516335454
At time: 86.54448103904724 and batch: 450, loss is 3.5864576959609984 and perplexity is 36.1059508758899
At time: 87.05721974372864 and batch: 500, loss is 3.4617893409729006 and perplexity is 31.87395889963371
At time: 87.57320737838745 and batch: 550, loss is 3.5208546543121337 and perplexity is 33.8133148134825
At time: 88.0857744216919 and batch: 600, loss is 3.54631453037262 and perplexity is 34.68525020237622
At time: 88.59847497940063 and batch: 650, loss is 3.389719386100769 and perplexity is 29.657628759728873
At time: 89.11330580711365 and batch: 700, loss is 3.3782276725769043 and perplexity is 29.3187625918861
At time: 89.62447452545166 and batch: 750, loss is 3.485938811302185 and perplexity is 32.653067793794584
At time: 90.13856887817383 and batch: 800, loss is 3.438464322090149 and perplexity is 31.13910180096303
At time: 90.65110182762146 and batch: 850, loss is 3.4991006231307984 and perplexity is 33.08568207636598
At time: 91.16582322120667 and batch: 900, loss is 3.4641439533233642 and perplexity is 31.949098144097892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.285224810038527 and perplexity of 72.61887033586727
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 92.60489749908447 and batch: 50, loss is 3.739896287918091 and perplexity is 42.093624321191996
At time: 93.13510346412659 and batch: 100, loss is 3.625079436302185 and perplexity is 37.52770410324531
At time: 93.65076923370361 and batch: 150, loss is 3.620202217102051 and perplexity is 37.34511888026708
At time: 94.1670241355896 and batch: 200, loss is 3.4965720653533934 and perplexity is 33.00212869692201
At time: 94.67822623252869 and batch: 250, loss is 3.644707593917847 and perplexity is 38.27158037238873
At time: 95.20607304573059 and batch: 300, loss is 3.622234082221985 and perplexity is 37.421076266168846
At time: 95.717529296875 and batch: 350, loss is 3.5949285411834717 and perplexity is 36.413097858028536
At time: 96.23102116584778 and batch: 400, loss is 3.524783754348755 and perplexity is 33.94643205417871
At time: 96.74264621734619 and batch: 450, loss is 3.5518468523025515 and perplexity is 34.87767195146632
At time: 97.25440192222595 and batch: 500, loss is 3.4219545698165894 and perplexity is 30.629223507851947
At time: 97.76515579223633 and batch: 550, loss is 3.4748831510543825 and perplexity is 32.294054789883894
At time: 98.27673625946045 and batch: 600, loss is 3.4998876810073853 and perplexity is 33.11173267336544
At time: 98.78857421875 and batch: 650, loss is 3.3391860198974608 and perplexity is 28.196166243706593
At time: 99.30862212181091 and batch: 700, loss is 3.3205213260650637 and perplexity is 27.674774379661987
At time: 99.823894739151 and batch: 750, loss is 3.4255011653900147 and perplexity is 30.738045836690223
At time: 100.33917689323425 and batch: 800, loss is 3.372402701377869 and perplexity is 29.14847807687809
At time: 100.85439157485962 and batch: 850, loss is 3.434692316055298 and perplexity is 31.021866166812355
At time: 101.36793756484985 and batch: 900, loss is 3.3993777227401734 and perplexity is 29.945459869607227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269499478274828 and perplexity of 71.48584644414899
finished 10 epochs...
Completing Train Step...
At time: 102.83606505393982 and batch: 50, loss is 3.713219108581543 and perplexity is 40.98553127054464
At time: 103.34829521179199 and batch: 100, loss is 3.5884175539016723 and perplexity is 36.17678279799973
At time: 103.8615071773529 and batch: 150, loss is 3.5838195085525513 and perplexity is 36.01082214979772
At time: 104.37520122528076 and batch: 200, loss is 3.462557635307312 and perplexity is 31.89845689130061
At time: 104.88949918746948 and batch: 250, loss is 3.6109203052520753 and perplexity is 37.000088526596386
At time: 105.40184545516968 and batch: 300, loss is 3.5911845445632933 and perplexity is 36.277022235007614
At time: 105.91495323181152 and batch: 350, loss is 3.5663169050216674 and perplexity is 35.38602276491301
At time: 106.42767071723938 and batch: 400, loss is 3.499318552017212 and perplexity is 33.09289318794333
At time: 106.94104838371277 and batch: 450, loss is 3.5282228565216065 and perplexity is 34.06337828216583
At time: 107.45487785339355 and batch: 500, loss is 3.399974708557129 and perplexity is 29.963342221655772
At time: 107.98543882369995 and batch: 550, loss is 3.455345392227173 and perplexity is 31.669225097679014
At time: 108.49789547920227 and batch: 600, loss is 3.4836372470855714 and perplexity is 32.57800107996587
At time: 109.01263475418091 and batch: 650, loss is 3.3257001352310183 and perplexity is 27.818468515983223
At time: 109.52454829216003 and batch: 700, loss is 3.3101917791366575 and perplexity is 27.390377871257684
At time: 110.03743481636047 and batch: 750, loss is 3.4185760831832885 and perplexity is 30.52591769256284
At time: 110.5515501499176 and batch: 800, loss is 3.3687102508544924 and perplexity is 29.041047227356795
At time: 111.06592774391174 and batch: 850, loss is 3.434191918373108 and perplexity is 31.00634678014167
At time: 111.57866549491882 and batch: 900, loss is 3.401281099319458 and perplexity is 30.002511634851704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26969972375321 and perplexity of 71.50016259499213
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 113.02028751373291 and batch: 50, loss is 3.7083148527145386 and perplexity is 40.78501981996478
At time: 113.5516927242279 and batch: 100, loss is 3.590981011390686 and perplexity is 36.26963940892987
At time: 114.06612014770508 and batch: 150, loss is 3.586962561607361 and perplexity is 36.12418413240152
At time: 114.5831949710846 and batch: 200, loss is 3.4647013664245607 and perplexity is 31.966911954339185
At time: 115.09690427780151 and batch: 250, loss is 3.6115616178512573 and perplexity is 37.02382475989855
At time: 115.61368799209595 and batch: 300, loss is 3.5889142847061155 and perplexity is 36.194757384316915
At time: 116.13151001930237 and batch: 350, loss is 3.5593511772155764 and perplexity is 35.140389861368455
At time: 116.64339423179626 and batch: 400, loss is 3.494199204444885 and perplexity is 32.92391207112608
At time: 117.15562272071838 and batch: 450, loss is 3.519391236305237 and perplexity is 33.76386798922265
At time: 117.6689658164978 and batch: 500, loss is 3.388631806373596 and perplexity is 29.625391257538308
At time: 118.18822121620178 and batch: 550, loss is 3.443176922798157 and perplexity is 31.286194275981472
At time: 118.70263290405273 and batch: 600, loss is 3.471733708381653 and perplexity is 32.192506509834445
At time: 119.21950578689575 and batch: 650, loss is 3.31126793384552 and perplexity is 27.41987002159333
At time: 119.73254990577698 and batch: 700, loss is 3.2939804935455324 and perplexity is 26.94992443744061
At time: 120.24706387519836 and batch: 750, loss is 3.4000345706939696 and perplexity is 29.965135945035573
At time: 120.7635407447815 and batch: 800, loss is 3.347785401344299 and perplexity is 28.43968137166422
At time: 121.29417753219604 and batch: 850, loss is 3.412970633506775 and perplexity is 30.355284880716276
At time: 121.80990028381348 and batch: 900, loss is 3.382314610481262 and perplexity is 29.438831744490383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262914788233091 and perplexity of 71.01668065377547
finished 12 epochs...
Completing Train Step...
At time: 123.24343228340149 and batch: 50, loss is 3.694150376319885 and perplexity is 40.21139351821068
At time: 123.77066779136658 and batch: 100, loss is 3.573771119117737 and perplexity is 35.65078331958306
At time: 124.28452253341675 and batch: 150, loss is 3.5711169719696043 and perplexity is 35.55628635460538
At time: 124.80032205581665 and batch: 200, loss is 3.449663977622986 and perplexity is 31.489809250219288
At time: 125.31281685829163 and batch: 250, loss is 3.5980519390106203 and perplexity is 36.52700824988779
At time: 125.83095145225525 and batch: 300, loss is 3.5776519346237183 and perplexity is 35.789406243305876
At time: 126.34578037261963 and batch: 350, loss is 3.548733057975769 and perplexity is 34.769238961038944
At time: 126.85972380638123 and batch: 400, loss is 3.484656047821045 and perplexity is 32.61120848441812
At time: 127.37407279014587 and batch: 450, loss is 3.511514344215393 and perplexity is 33.49895834657146
At time: 127.88716340065002 and batch: 500, loss is 3.3811466550827025 and perplexity is 29.404468573259088
At time: 128.40077352523804 and batch: 550, loss is 3.4364649200439454 and perplexity is 31.076904416614077
At time: 128.91246604919434 and batch: 600, loss is 3.4668192529678343 and perplexity is 32.034685990523855
At time: 129.42629051208496 and batch: 650, loss is 3.3074172258377077 and perplexity is 27.314487138102344
At time: 129.94072794914246 and batch: 700, loss is 3.2915655040740965 and perplexity is 26.884919178769337
At time: 130.45236468315125 and batch: 750, loss is 3.399352202415466 and perplexity is 29.94469566149931
At time: 130.97002148628235 and batch: 800, loss is 3.3483410835266114 and perplexity is 28.455489187527135
At time: 131.48194360733032 and batch: 850, loss is 3.4154071283340453 and perplexity is 30.42933555071213
At time: 131.99573588371277 and batch: 900, loss is 3.386539578437805 and perplexity is 29.563472982499782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262542306560359 and perplexity of 70.99023316768142
finished 13 epochs...
Completing Train Step...
At time: 133.44987750053406 and batch: 50, loss is 3.6884901094436646 and perplexity is 39.98442924458114
At time: 133.96090126037598 and batch: 100, loss is 3.567385153770447 and perplexity is 35.4238440371226
At time: 134.48840069770813 and batch: 150, loss is 3.5645106410980225 and perplexity is 35.32216395888232
At time: 135.00346660614014 and batch: 200, loss is 3.443127989768982 and perplexity is 31.284663385180064
At time: 135.51617455482483 and batch: 250, loss is 3.591321973800659 and perplexity is 36.28200810110131
At time: 136.02919626235962 and batch: 300, loss is 3.5714456748962404 and perplexity is 35.56797573105133
At time: 136.54035782814026 and batch: 350, loss is 3.5428446245193483 and perplexity is 34.56510421785319
At time: 137.05311226844788 and batch: 400, loss is 3.4792858457565305 and perplexity is 32.43654910312477
At time: 137.56570267677307 and batch: 450, loss is 3.506658844947815 and perplexity is 33.33669842410722
At time: 138.07740688323975 and batch: 500, loss is 3.3766283273696898 and perplexity is 29.27190924677698
At time: 138.58954501152039 and batch: 550, loss is 3.4324129104614256 and perplexity is 30.95123528033957
At time: 139.10380268096924 and batch: 600, loss is 3.4636392259597777 and perplexity is 31.932976628849843
At time: 139.61819124221802 and batch: 650, loss is 3.3047062873840334 and perplexity is 27.240539523581052
At time: 140.13162779808044 and batch: 700, loss is 3.289681649208069 and perplexity is 26.834319569076573
At time: 140.64369201660156 and batch: 750, loss is 3.3982797193527223 and perplexity is 29.912597697921118
At time: 141.15429019927979 and batch: 800, loss is 3.347987103462219 and perplexity is 28.445418294185114
At time: 141.67057180404663 and batch: 850, loss is 3.415795111656189 and perplexity is 30.441143915985986
At time: 142.18496942520142 and batch: 900, loss is 3.38761314868927 and perplexity is 29.595228490458126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262691132009846 and perplexity of 71.00079910726262
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 143.63975429534912 and batch: 50, loss is 3.686595764160156 and perplexity is 39.908756627290266
At time: 144.16727423667908 and batch: 100, loss is 3.567674169540405 and perplexity is 35.4340835663033
At time: 144.68192744255066 and batch: 150, loss is 3.566615719795227 and perplexity is 35.39659821126379
At time: 145.1956868171692 and batch: 200, loss is 3.4448939180374145 and perplexity is 31.339958666000232
At time: 145.70846390724182 and batch: 250, loss is 3.5933321619033816 and perplexity is 36.35501511646164
At time: 146.22224926948547 and batch: 300, loss is 3.5719731569290163 and perplexity is 35.58674214823048
At time: 146.73466086387634 and batch: 350, loss is 3.543093891143799 and perplexity is 34.57372121862609
At time: 147.2615044116974 and batch: 400, loss is 3.4783012866973877 and perplexity is 32.404629120990236
At time: 147.77584552764893 and batch: 450, loss is 3.5033728075027466 and perplexity is 33.227332573342466
At time: 148.28863215446472 and batch: 500, loss is 3.3734320497512815 and perplexity is 29.178497462926472
At time: 148.8019917011261 and batch: 550, loss is 3.4301265335083007 and perplexity is 30.880549926797904
At time: 149.31264805793762 and batch: 600, loss is 3.461021971702576 and perplexity is 31.849509185230065
At time: 149.82836437225342 and batch: 650, loss is 3.2994637298583984 and perplexity is 27.09810311985105
At time: 150.34606432914734 and batch: 700, loss is 3.284460415840149 and perplexity is 26.694576457500144
At time: 150.85886597633362 and batch: 750, loss is 3.391098337173462 and perplexity is 29.698553388766314
At time: 151.37072491645813 and batch: 800, loss is 3.3399508714675905 and perplexity is 28.217740375182164
At time: 151.88186931610107 and batch: 850, loss is 3.4052768325805665 and perplexity is 30.12263349614522
At time: 152.39553427696228 and batch: 900, loss is 3.378705358505249 and perplexity is 29.33277109777942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261452034728168 and perplexity of 70.91287669355317
finished 15 epochs...
Completing Train Step...
At time: 153.83298015594482 and batch: 50, loss is 3.6827027225494384 and perplexity is 39.753692209067836
At time: 154.36236786842346 and batch: 100, loss is 3.5627031564712524 and perplexity is 35.258377354561645
At time: 154.87567472457886 and batch: 150, loss is 3.562070755958557 and perplexity is 35.23608698760844
At time: 155.38836312294006 and batch: 200, loss is 3.440184178352356 and perplexity is 31.192702659869152
At time: 155.9005582332611 and batch: 250, loss is 3.5890116691589355 and perplexity is 36.198282362596
At time: 156.4115686416626 and batch: 300, loss is 3.5683914184570313 and perplexity is 35.4595077409938
At time: 156.93548607826233 and batch: 350, loss is 3.5392983198165893 and perplexity is 34.442742919675084
At time: 157.45824575424194 and batch: 400, loss is 3.475173602104187 and perplexity is 32.303435994324445
At time: 157.97693133354187 and batch: 450, loss is 3.5012562704086303 and perplexity is 33.15708006365981
At time: 158.4920778274536 and batch: 500, loss is 3.3710693407058714 and perplexity is 29.10963854187029
At time: 159.00577569007874 and batch: 550, loss is 3.4276554870605467 and perplexity is 30.80433685539648
At time: 159.51998019218445 and batch: 600, loss is 3.4592861843109133 and perplexity is 31.794273161621025
At time: 160.0325448513031 and batch: 650, loss is 3.2985848140716554 and perplexity is 27.074296632709437
At time: 160.5574414730072 and batch: 700, loss is 3.2839549207687377 and perplexity is 26.681085890662253
At time: 161.06911492347717 and batch: 750, loss is 3.391231050491333 and perplexity is 29.702495043871792
At time: 161.58167839050293 and batch: 800, loss is 3.340443434715271 and perplexity is 28.23164282065933
At time: 162.09657287597656 and batch: 850, loss is 3.407036552429199 and perplexity is 30.17568755865423
At time: 162.60860538482666 and batch: 900, loss is 3.3814441108703615 and perplexity is 29.41321640360112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261240919975386 and perplexity of 70.89790751928327
finished 16 epochs...
Completing Train Step...
At time: 164.0627739429474 and batch: 50, loss is 3.6808232736587523 and perplexity is 39.67904734392184
At time: 164.57544803619385 and batch: 100, loss is 3.5603223133087156 and perplexity is 35.174532538199365
At time: 165.09192419052124 and batch: 150, loss is 3.559785885810852 and perplexity is 35.15566901163089
At time: 165.60238790512085 and batch: 200, loss is 3.4378020429611205 and perplexity is 31.118485851253702
At time: 166.1123332977295 and batch: 250, loss is 3.5867174863815308 and perplexity is 36.11533207457168
At time: 166.62563252449036 and batch: 300, loss is 3.5663257694244384 and perplexity is 35.38633644226154
At time: 167.13869166374207 and batch: 350, loss is 3.5373935174942015 and perplexity is 34.37719874713599
At time: 167.65078973770142 and batch: 400, loss is 3.473389439582825 and perplexity is 32.245852798677
At time: 168.16360449790955 and batch: 450, loss is 3.499853463172913 and perplexity is 33.1105996809621
At time: 168.6834979057312 and batch: 500, loss is 3.369681749343872 and perplexity is 29.069274269912704
At time: 169.19765830039978 and batch: 550, loss is 3.4263125467300415 and perplexity is 30.762996234285698
At time: 169.70796966552734 and batch: 600, loss is 3.4583298683166506 and perplexity is 31.763882323608982
At time: 170.22104144096375 and batch: 650, loss is 3.298011589050293 and perplexity is 27.05878141572391
At time: 170.73399543762207 and batch: 700, loss is 3.283674383163452 and perplexity is 26.673601892540603
At time: 171.24578499794006 and batch: 750, loss is 3.391266493797302 and perplexity is 29.703547817148443
At time: 171.75968217849731 and batch: 800, loss is 3.3407093238830567 and perplexity is 28.239150306709522
At time: 172.2726583480835 and batch: 850, loss is 3.407803120613098 and perplexity is 30.198828148950945
At time: 172.78757500648499 and batch: 900, loss is 3.382654142379761 and perplexity is 29.448828863996624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261201205318922 and perplexity of 70.89509188915339
finished 17 epochs...
Completing Train Step...
At time: 174.22834730148315 and batch: 50, loss is 3.6793087244033815 and perplexity is 39.61899695842553
At time: 174.75414562225342 and batch: 100, loss is 3.5585567712783814 and perplexity is 35.11248521230419
At time: 175.26724648475647 and batch: 150, loss is 3.55803102016449 and perplexity is 35.09402963603733
At time: 175.78311276435852 and batch: 200, loss is 3.4360009384155275 and perplexity is 31.06248864848107
At time: 176.29811573028564 and batch: 250, loss is 3.5849474048614502 and perplexity is 36.051461537380234
At time: 176.81569623947144 and batch: 300, loss is 3.564689211845398 and perplexity is 35.32847202730087
At time: 177.32924485206604 and batch: 350, loss is 3.5359055614471435 and perplexity is 34.32608502329705
At time: 177.84359216690063 and batch: 400, loss is 3.471993741989136 and perplexity is 32.20087873191849
At time: 178.35869574546814 and batch: 450, loss is 3.498664588928223 and perplexity is 33.071258732129365
At time: 178.8717143535614 and batch: 500, loss is 3.3685494661331177 and perplexity is 29.036378246030406
At time: 179.38498902320862 and batch: 550, loss is 3.4252651596069335 and perplexity is 30.73079233607986
At time: 179.89596462249756 and batch: 600, loss is 3.4575479888916014 and perplexity is 31.739056504223907
At time: 180.40733242034912 and batch: 650, loss is 3.2974515628814696 and perplexity is 27.043632032456117
At time: 180.91754603385925 and batch: 700, loss is 3.2833481550216677 and perplexity is 26.664901632172548
At time: 181.4285764694214 and batch: 750, loss is 3.391156415939331 and perplexity is 29.700278294184933
At time: 181.93956422805786 and batch: 800, loss is 3.3407823038101196 and perplexity is 28.241211273042893
At time: 182.44918847084045 and batch: 850, loss is 3.4081564140319824 and perplexity is 30.209499081068042
At time: 182.9634222984314 and batch: 900, loss is 3.383270034790039 and perplexity is 29.466971760648203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2612283785049225 and perplexity of 70.89701836084592
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 184.39884853363037 and batch: 50, loss is 3.6786331844329836 and perplexity is 39.59224178050657
At time: 184.92918920516968 and batch: 100, loss is 3.5582503509521484 and perplexity is 35.101727681377824
At time: 185.4419538974762 and batch: 150, loss is 3.558379487991333 and perplexity is 35.10626090725825
At time: 185.95592331886292 and batch: 200, loss is 3.4360702085494994 and perplexity is 31.064640425757343
At time: 186.46961092948914 and batch: 250, loss is 3.585131998062134 and perplexity is 36.058117006313
At time: 186.99554347991943 and batch: 300, loss is 3.5644055128097536 and perplexity is 35.31845079543005
At time: 187.50781154632568 and batch: 350, loss is 3.5358251667022706 and perplexity is 34.323325497376295
At time: 188.02013659477234 and batch: 400, loss is 3.471381497383118 and perplexity is 32.181169951518726
At time: 188.53472065925598 and batch: 450, loss is 3.497453751564026 and perplexity is 33.031239049928224
At time: 189.04601335525513 and batch: 500, loss is 3.3673497915267943 and perplexity is 29.001564926891838
At time: 189.55842447280884 and batch: 550, loss is 3.424178786277771 and perplexity is 30.697425350685073
At time: 190.07272100448608 and batch: 600, loss is 3.4565294551849366 and perplexity is 31.70674566299036
At time: 190.5866470336914 and batch: 650, loss is 3.2955268001556397 and perplexity is 26.991629519852463
At time: 191.10017824172974 and batch: 700, loss is 3.2815549659729 and perplexity is 26.61712926782068
At time: 191.61369514465332 and batch: 750, loss is 3.3887800407409667 and perplexity is 29.629783084171258
At time: 192.12847328186035 and batch: 800, loss is 3.3382294178009033 and perplexity is 28.169206628830903
At time: 192.6429579257965 and batch: 850, loss is 3.404161581993103 and perplexity is 30.0890579375035
At time: 193.15887928009033 and batch: 900, loss is 3.3794358587265014 and perplexity is 29.354206521894106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2608404290186215 and perplexity of 70.86951923347361
finished 19 epochs...
Completing Train Step...
At time: 194.62561678886414 and batch: 50, loss is 3.6779656314849856 and perplexity is 39.565820682510235
At time: 195.13781261444092 and batch: 100, loss is 3.557477135658264 and perplexity is 35.07459697898154
At time: 195.65080547332764 and batch: 150, loss is 3.5575922870635988 and perplexity is 35.078636100666024
At time: 196.16927003860474 and batch: 200, loss is 3.4353202295303347 and perplexity is 31.041351331457793
At time: 196.68123650550842 and batch: 250, loss is 3.5844257068634033 and perplexity is 36.032658467258
At time: 197.19699573516846 and batch: 300, loss is 3.5637380695343017 and perplexity is 35.29488559803835
At time: 197.7074580192566 and batch: 350, loss is 3.535176978111267 and perplexity is 34.30108471827193
At time: 198.2240273952484 and batch: 400, loss is 3.470835781097412 and perplexity is 32.163612953988576
At time: 198.737699508667 and batch: 450, loss is 3.4970908641815184 and perplexity is 33.01925460468192
At time: 199.24714732170105 and batch: 500, loss is 3.366973671913147 and perplexity is 28.99065892060638
At time: 199.77668356895447 and batch: 550, loss is 3.4237983322143553 and perplexity is 30.685748611846225
At time: 200.29063367843628 and batch: 600, loss is 3.4562231636047365 and perplexity is 31.697035640883158
At time: 200.80274105072021 and batch: 650, loss is 3.295443305969238 and perplexity is 26.989375969786526
At time: 201.31551337242126 and batch: 700, loss is 3.2814841508865356 and perplexity is 26.615244440250702
At time: 201.83057308197021 and batch: 750, loss is 3.388824210166931 and perplexity is 29.631091843584894
At time: 202.34163880348206 and batch: 800, loss is 3.338324522972107 and perplexity is 28.171885793449196
At time: 202.8550522327423 and batch: 850, loss is 3.404552907943726 and perplexity is 30.100834870863775
At time: 203.36664867401123 and batch: 900, loss is 3.380012187957764 and perplexity is 29.371129085188723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260667356726241 and perplexity of 70.85725474467219
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8ad1a05b70>
ELAPSED
1254.3808200359344


RESULTS SO FAR:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}, {'best_accuracy': -71.20988825665874, 'params': {'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}}, {'best_accuracy': -72.19443221175057, 'params': {'batch_size': 32, 'dropout': 0.982441793390317, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9171483614956}}, {'best_accuracy': -70.96593153993238, 'params': {'batch_size': 32, 'dropout': 0.703003270020777, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.11712669393286701}}, {'best_accuracy': -71.21226983437707, 'params': {'batch_size': 32, 'dropout': 0.10703692335741788, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.25510031122950216}}, {'best_accuracy': -70.85725474467219, 'params': {'batch_size': 32, 'dropout': 1.0, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.0}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -71.08154984495319, 'params': {'batch_size': 32, 'dropout': 0.950824364249188, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.3721478967281897}}, {'best_accuracy': -71.20988825665874, 'params': {'batch_size': 32, 'dropout': 0.6854472190258725, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.4837693735986168}}, {'best_accuracy': -72.19443221175057, 'params': {'batch_size': 32, 'dropout': 0.982441793390317, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9171483614956}}, {'best_accuracy': -70.96593153993238, 'params': {'batch_size': 32, 'dropout': 0.703003270020777, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.11712669393286701}}, {'best_accuracy': -71.21226983437707, 'params': {'batch_size': 32, 'dropout': 0.10703692335741788, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.25510031122950216}}, {'best_accuracy': -70.85725474467219, 'params': {'batch_size': 32, 'dropout': 1.0, 'tune_wordvecs': 'TRUE', 'wordvec_source': 'glove', 'wordvec_dim': 300, 'num_layers': 2, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'rnn_dropout': 0.0}}]
