FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3719608783721924 and batch: 50, loss is 7.060044450759888 and perplexity is 1164.496927395684
At time: 2.2381601333618164 and batch: 100, loss is 6.132793006896972 and perplexity is 460.72116271986556
At time: 3.127056837081909 and batch: 150, loss is 5.91862961769104 and perplexity is 371.90171697701317
At time: 3.9942116737365723 and batch: 200, loss is 5.71669560432434 and perplexity is 303.8990592491002
At time: 4.867262363433838 and batch: 250, loss is 5.731205768585205 and perplexity is 308.3408320112903
At time: 5.7307798862457275 and batch: 300, loss is 5.613386163711548 and perplexity is 274.0707168307384
At time: 6.601825475692749 and batch: 350, loss is 5.57976637840271 and perplexity is 265.00968656740196
At time: 7.47150182723999 and batch: 400, loss is 5.432648124694825 and perplexity is 228.75421373715014
At time: 8.341390371322632 and batch: 450, loss is 5.422543706893921 and perplexity is 226.45442417234224
At time: 9.211735963821411 and batch: 500, loss is 5.3662584781646725 and perplexity is 214.06045565040824
At time: 10.085367679595947 and batch: 550, loss is 5.403464231491089 and perplexity is 222.17474935709214
At time: 10.977519512176514 and batch: 600, loss is 5.317565507888794 and perplexity is 203.88691621967521
At time: 11.84551215171814 and batch: 650, loss is 5.212452907562255 and perplexity is 183.54372226040053
At time: 12.706153154373169 and batch: 700, loss is 5.294282665252686 and perplexity is 199.19468543525215
At time: 13.570950031280518 and batch: 750, loss is 5.269086694717407 and perplexity is 194.23848237894887
At time: 14.431895017623901 and batch: 800, loss is 5.242163734436035 and perplexity is 189.07877637481914
At time: 15.294788837432861 and batch: 850, loss is 5.2713837146759035 and perplexity is 194.68516487258032
At time: 16.154544830322266 and batch: 900, loss is 5.178209867477417 and perplexity is 177.36501974379232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.064264584894049 and perplexity of 158.26400952246954
finished 1 epochs...
Completing Train Step...
At time: 18.185606479644775 and batch: 50, loss is 4.994083633422852 and perplexity is 147.53768481557128
At time: 18.976089477539062 and batch: 100, loss is 4.862001914978027 and perplexity is 129.28275630551605
At time: 19.752509355545044 and batch: 150, loss is 4.835871086120606 and perplexity is 125.94824720660077
At time: 20.531635999679565 and batch: 200, loss is 4.715093069076538 and perplexity is 111.61919898608201
At time: 21.307584285736084 and batch: 250, loss is 4.808218975067138 and perplexity is 122.51322398630799
At time: 22.083479404449463 and batch: 300, loss is 4.737420120239258 and perplexity is 114.13935569643613
At time: 22.859650373458862 and batch: 350, loss is 4.726440553665161 and perplexity is 112.89300973173977
At time: 23.635781288146973 and batch: 400, loss is 4.593551054000854 and perplexity is 98.84481094458349
At time: 24.412240982055664 and batch: 450, loss is 4.615003480911255 and perplexity is 100.98818006276167
At time: 25.1884822845459 and batch: 500, loss is 4.516529207229614 and perplexity is 91.51740815352177
At time: 25.96394991874695 and batch: 550, loss is 4.578138933181763 and perplexity is 97.33308217293224
At time: 26.739980459213257 and batch: 600, loss is 4.533901672363282 and perplexity is 93.1211815494559
At time: 27.516119241714478 and batch: 650, loss is 4.3910540676116945 and perplexity is 80.72546424817472
At time: 28.292479276657104 and batch: 700, loss is 4.440717000961303 and perplexity is 84.83574718470248
At time: 29.069019317626953 and batch: 750, loss is 4.480824794769287 and perplexity is 88.30747819282159
At time: 29.86685848236084 and batch: 800, loss is 4.436733102798462 and perplexity is 84.49844254713886
At time: 30.64328360557556 and batch: 850, loss is 4.4833398532867434 and perplexity is 88.52985619763174
At time: 31.42074227333069 and batch: 900, loss is 4.423279733657837 and perplexity is 83.36926645021535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.548157574379281 and perplexity of 94.45821565907555
finished 2 epochs...
Completing Train Step...
At time: 33.3731951713562 and batch: 50, loss is 4.456305742263794 and perplexity is 86.16859139156753
At time: 34.163816690444946 and batch: 100, loss is 4.334993572235107 and perplexity is 76.32446829245323
At time: 34.94011449813843 and batch: 150, loss is 4.330210261344909 and perplexity is 75.96025639587933
At time: 35.71518850326538 and batch: 200, loss is 4.222912101745606 and perplexity is 68.23189347142646
At time: 36.491862297058105 and batch: 250, loss is 4.35513201713562 and perplexity is 77.87710577065066
At time: 37.26792931556702 and batch: 300, loss is 4.314603638648987 and perplexity is 74.7839760910011
At time: 38.04414105415344 and batch: 350, loss is 4.31484281539917 and perplexity is 74.8018648185669
At time: 38.821601152420044 and batch: 400, loss is 4.220660505294799 and perplexity is 68.07843560967477
At time: 39.59755873680115 and batch: 450, loss is 4.258229985237121 and perplexity is 70.68475959493651
At time: 40.37556481361389 and batch: 500, loss is 4.136710605621338 and perplexity is 62.59657759601336
At time: 41.15298581123352 and batch: 550, loss is 4.206832151412964 and perplexity is 67.14350210440645
At time: 41.930145025253296 and batch: 600, loss is 4.201276688575745 and perplexity is 66.77152308820874
At time: 42.70807385444641 and batch: 650, loss is 4.04844699382782 and perplexity is 57.30838762097186
At time: 43.48536944389343 and batch: 700, loss is 4.073609771728516 and perplexity is 58.76872183127898
At time: 44.262770891189575 and batch: 750, loss is 4.155459079742432 and perplexity is 63.78123850230266
At time: 45.03915548324585 and batch: 800, loss is 4.121683731079101 and perplexity is 61.662978781526675
At time: 45.815789461135864 and batch: 850, loss is 4.180979661941528 and perplexity is 65.42992102995565
At time: 46.592312812805176 and batch: 900, loss is 4.130928530693054 and perplexity is 62.23568385688061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.398014434396404 and perplexity of 81.28930307438426
finished 3 epochs...
Completing Train Step...
At time: 48.56190872192383 and batch: 50, loss is 4.190537576675415 and perplexity is 66.0582928145054
At time: 49.35421419143677 and batch: 100, loss is 4.07342613697052 and perplexity is 58.75793084209841
At time: 50.134042501449585 and batch: 150, loss is 4.070870599746704 and perplexity is 58.60796446658084
At time: 50.91340947151184 and batch: 200, loss is 3.961834444999695 and perplexity is 52.55364434677445
At time: 51.692805767059326 and batch: 250, loss is 4.107864918708802 and perplexity is 60.81673018937093
At time: 52.472273111343384 and batch: 300, loss is 4.075857067108155 and perplexity is 58.900941020458205
At time: 53.25202655792236 and batch: 350, loss is 4.074454507827759 and perplexity is 58.818386866086776
At time: 54.03142809867859 and batch: 400, loss is 3.9954331493377686 and perplexity is 54.349376922700955
At time: 54.810195207595825 and batch: 450, loss is 4.033819737434388 and perplexity is 56.476224113427435
At time: 55.58867812156677 and batch: 500, loss is 3.915353288650513 and perplexity is 50.1667917388288
At time: 56.36898684501648 and batch: 550, loss is 3.9822261810302733 and perplexity is 53.63630554439859
At time: 57.14816474914551 and batch: 600, loss is 3.9904073524475097 and perplexity is 54.07691324024414
At time: 57.927438497543335 and batch: 650, loss is 3.8394528436660766 and perplexity is 46.50002469489018
At time: 58.70749521255493 and batch: 700, loss is 3.853177614212036 and perplexity is 47.14262655859987
At time: 59.48663878440857 and batch: 750, loss is 3.946748285293579 and perplexity is 51.76676211449947
At time: 60.2655553817749 and batch: 800, loss is 3.9204660129547118 and perplexity is 50.42393751178927
At time: 61.04427933692932 and batch: 850, loss is 3.981170530319214 and perplexity is 53.579714215905575
At time: 61.82266879081726 and batch: 900, loss is 3.9370904064178465 and perplexity is 51.2692115053654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3507013190282535 and perplexity of 77.53281910387345
finished 4 epochs...
Completing Train Step...
At time: 63.79209303855896 and batch: 50, loss is 4.005557823181152 and perplexity is 54.902441713441924
At time: 64.57106423377991 and batch: 100, loss is 3.8901760625839232 and perplexity is 48.91949865891037
At time: 65.35100197792053 and batch: 150, loss is 3.891401662826538 and perplexity is 48.979491164240336
At time: 66.13012051582336 and batch: 200, loss is 3.781438446044922 and perplexity is 43.879114099733634
At time: 66.90965270996094 and batch: 250, loss is 3.9316866111755373 and perplexity is 50.99291039382722
At time: 67.68933725357056 and batch: 300, loss is 3.9044135570526124 and perplexity is 49.620971509018524
At time: 68.48843574523926 and batch: 350, loss is 3.904410710334778 and perplexity is 49.62083025231503
At time: 69.26800012588501 and batch: 400, loss is 3.8298929834365847 and perplexity is 46.057609043427696
At time: 70.04870843887329 and batch: 450, loss is 3.8685885286331176 and perplexity is 47.8747645097884
At time: 70.83655595779419 and batch: 500, loss is 3.7552248191833497 and perplexity is 42.74382836300428
At time: 71.62111330032349 and batch: 550, loss is 3.8164892959594727 and perplexity is 45.444386150752074
At time: 72.40616488456726 and batch: 600, loss is 3.831340665817261 and perplexity is 46.12433411924079
At time: 73.19150257110596 and batch: 650, loss is 3.685992383956909 and perplexity is 39.884683736888256
At time: 73.97576451301575 and batch: 700, loss is 3.6916565895080566 and perplexity is 40.11123980827465
At time: 74.76067519187927 and batch: 750, loss is 3.7903398084640503 and perplexity is 44.27144153081366
At time: 75.54579091072083 and batch: 800, loss is 3.7685446071624757 and perplexity is 43.31697572105719
At time: 76.33237719535828 and batch: 850, loss is 3.8272474336624147 and perplexity is 45.935922381257924
At time: 77.11684584617615 and batch: 900, loss is 3.78532910823822 and perplexity is 44.05016544576889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341245886397688 and perplexity of 76.80316777687295
finished 5 epochs...
Completing Train Step...
At time: 79.08276724815369 and batch: 50, loss is 3.8619860506057737 and perplexity is 47.559713631790785
At time: 79.88111925125122 and batch: 100, loss is 3.7471024322509767 and perplexity is 42.39805261327601
At time: 80.6668975353241 and batch: 150, loss is 3.751929874420166 and perplexity is 42.60322158257859
At time: 81.45127892494202 and batch: 200, loss is 3.6418042182922363 and perplexity is 38.16062474964976
At time: 82.23551344871521 and batch: 250, loss is 3.788925132751465 and perplexity is 44.20885607721718
At time: 83.01899027824402 and batch: 300, loss is 3.7668389320373534 and perplexity is 43.243154008900945
At time: 83.80324053764343 and batch: 350, loss is 3.769852499961853 and perplexity is 43.37366674650432
At time: 84.5877640247345 and batch: 400, loss is 3.699075756072998 and perplexity is 40.40993845491437
At time: 85.37188720703125 and batch: 450, loss is 3.733097543716431 and perplexity is 41.80841118111715
At time: 86.15543389320374 and batch: 500, loss is 3.6277237701416016 and perplexity is 37.62707120311145
At time: 86.93947911262512 and batch: 550, loss is 3.685631160736084 and perplexity is 39.87027906477488
At time: 87.72334575653076 and batch: 600, loss is 3.704483609199524 and perplexity is 40.629061425332424
At time: 88.51958227157593 and batch: 650, loss is 3.5598430156707765 and perplexity is 35.15767750744908
At time: 89.30328011512756 and batch: 700, loss is 3.563799276351929 and perplexity is 35.29704595177783
At time: 90.08751630783081 and batch: 750, loss is 3.667429552078247 and perplexity is 39.151140442072425
At time: 90.8717999458313 and batch: 800, loss is 3.644808440208435 and perplexity is 38.27544011392127
At time: 91.65566444396973 and batch: 850, loss is 3.7030872964859007 and perplexity is 40.57237013891451
At time: 92.43897438049316 and batch: 900, loss is 3.664922413825989 and perplexity is 39.0531060644712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342900942449701 and perplexity of 76.93038657259476
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 94.4000232219696 and batch: 50, loss is 3.7762459993362425 and perplexity is 43.651864639085716
At time: 95.19697618484497 and batch: 100, loss is 3.6648994636535646 and perplexity is 39.05220979923808
At time: 95.9811909198761 and batch: 150, loss is 3.676543126106262 and perplexity is 39.50957810194644
At time: 96.7662410736084 and batch: 200, loss is 3.5505903148651123 and perplexity is 34.833874373339484
At time: 97.55148720741272 and batch: 250, loss is 3.694346995353699 and perplexity is 40.219300620870506
At time: 98.33525443077087 and batch: 300, loss is 3.654313015937805 and perplexity is 38.64096626685384
At time: 99.1188051700592 and batch: 350, loss is 3.6458901071548464 and perplexity is 38.31686379162451
At time: 99.90360951423645 and batch: 400, loss is 3.567234044075012 and perplexity is 35.41849155525516
At time: 100.68706917762756 and batch: 450, loss is 3.5921170473098756 and perplexity is 36.310866435331995
At time: 101.4715929031372 and batch: 500, loss is 3.4802639722824096 and perplexity is 32.46829167380495
At time: 102.25569272041321 and batch: 550, loss is 3.5191665840148927 and perplexity is 33.756283710892646
At time: 103.0408296585083 and batch: 600, loss is 3.527247500419617 and perplexity is 34.030170555615896
At time: 103.82514882087708 and batch: 650, loss is 3.3669646835327147 and perplexity is 28.990398342706104
At time: 104.60935759544373 and batch: 700, loss is 3.3497562408447266 and perplexity is 28.495786688217695
At time: 105.39394474029541 and batch: 750, loss is 3.4478819704055788 and perplexity is 31.433744151902005
At time: 106.17801690101624 and batch: 800, loss is 3.408957085609436 and perplexity is 30.233696654213187
At time: 106.96210074424744 and batch: 850, loss is 3.4449357891082766 and perplexity is 31.341270931103132
At time: 107.76547431945801 and batch: 900, loss is 3.398737235069275 and perplexity is 29.92628631263024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30339572854238 and perplexity of 73.9504835891442
finished 7 epochs...
Completing Train Step...
At time: 109.74747157096863 and batch: 50, loss is 3.6832073307037354 and perplexity is 39.77375730840055
At time: 110.53290748596191 and batch: 100, loss is 3.562436828613281 and perplexity is 35.24898831678192
At time: 111.31774353981018 and batch: 150, loss is 3.570969648361206 and perplexity is 35.55104846004061
At time: 112.10310077667236 and batch: 200, loss is 3.4511942052841187 and perplexity is 31.538032714407187
At time: 112.88765454292297 and batch: 250, loss is 3.5938415145874023 and perplexity is 36.37353735776503
At time: 113.67276930809021 and batch: 300, loss is 3.558643126487732 and perplexity is 35.115517489240084
At time: 114.45846819877625 and batch: 350, loss is 3.5541952562332155 and perplexity is 34.95967506389806
At time: 115.2435553073883 and batch: 400, loss is 3.4813606214523314 and perplexity is 32.50391752988013
At time: 116.02893900871277 and batch: 450, loss is 3.510933241844177 and perplexity is 33.47949767731116
At time: 116.81361603736877 and batch: 500, loss is 3.4033978891372683 and perplexity is 30.06608791105678
At time: 117.59818005561829 and batch: 550, loss is 3.4456357765197754 and perplexity is 31.363217106342372
At time: 118.38400411605835 and batch: 600, loss is 3.4593179559707643 and perplexity is 31.79528333450048
At time: 119.16921138763428 and batch: 650, loss is 3.306555986404419 and perplexity is 27.290972951804633
At time: 119.95444965362549 and batch: 700, loss is 3.293938627243042 and perplexity is 26.94879616737044
At time: 120.73973560333252 and batch: 750, loss is 3.4003844594955446 and perplexity is 29.975622244952888
At time: 121.52511096000671 and batch: 800, loss is 3.367835087776184 and perplexity is 29.015642693244608
At time: 122.31018924713135 and batch: 850, loss is 3.410742745399475 and perplexity is 30.287731980629662
At time: 123.09596109390259 and batch: 900, loss is 3.3718735361099244 and perplexity is 29.133057794964998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3135091703232025 and perplexity of 74.70217217195824
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 125.06368947029114 and batch: 50, loss is 3.651192307472229 and perplexity is 38.52056703946311
At time: 125.8602602481842 and batch: 100, loss is 3.5427736759185793 and perplexity is 34.56265195906669
At time: 126.64242553710938 and batch: 150, loss is 3.557328233718872 and perplexity is 35.069374692281926
At time: 127.43628668785095 and batch: 200, loss is 3.4348576641082764 and perplexity is 31.02699599607475
At time: 128.21879959106445 and batch: 250, loss is 3.5780511331558227 and perplexity is 35.80369617381296
At time: 129.00170230865479 and batch: 300, loss is 3.5372012853622437 and perplexity is 34.370590980063014
At time: 129.7835669517517 and batch: 350, loss is 3.5284930276870727 and perplexity is 34.072582468072866
At time: 130.56568264961243 and batch: 400, loss is 3.4514596176147463 and perplexity is 31.54640440809955
At time: 131.3494029045105 and batch: 450, loss is 3.4773995876312256 and perplexity is 32.37542306668731
At time: 132.1314640045166 and batch: 500, loss is 3.3629160737991333 and perplexity is 28.873264807777193
At time: 132.9141561985016 and batch: 550, loss is 3.4006736707687377 and perplexity is 29.984292786575864
At time: 133.69629549980164 and batch: 600, loss is 3.413592596054077 and perplexity is 30.37417060352209
At time: 134.47836709022522 and batch: 650, loss is 3.249529604911804 and perplexity is 25.77821112085987
At time: 135.25981998443604 and batch: 700, loss is 3.2336417388916017 and perplexity is 25.371886717316293
At time: 136.04198789596558 and batch: 750, loss is 3.3308803415298462 and perplexity is 27.962947815014797
At time: 136.82329654693604 and batch: 800, loss is 3.292884192466736 and perplexity is 26.920395395524135
At time: 137.60552883148193 and batch: 850, loss is 3.336201171875 and perplexity is 28.112130452078105
At time: 138.38706398010254 and batch: 900, loss is 3.3001301145553588 and perplexity is 27.11616689913135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308988806319563 and perplexity of 74.3652532337067
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 140.34747076034546 and batch: 50, loss is 3.6369436311721803 and perplexity is 37.975591757276376
At time: 141.143981218338 and batch: 100, loss is 3.5296067428588866 and perplexity is 34.1105507591168
At time: 141.92652916908264 and batch: 150, loss is 3.5430723667144775 and perplexity is 34.5729770470163
At time: 142.70959043502808 and batch: 200, loss is 3.4171198272705077 and perplexity is 30.48149649659489
At time: 143.49369406700134 and batch: 250, loss is 3.5650991725921632 and perplexity is 35.34295828327052
At time: 144.27688455581665 and batch: 300, loss is 3.5195609092712403 and perplexity is 33.76959729088819
At time: 145.0602684020996 and batch: 350, loss is 3.5124548435211183 and perplexity is 33.53047891385132
At time: 145.8432981967926 and batch: 400, loss is 3.4373790216445923 and perplexity is 31.10532485228836
At time: 146.62545895576477 and batch: 450, loss is 3.4602471208572387 and perplexity is 31.824840124769757
At time: 147.41950058937073 and batch: 500, loss is 3.343093900680542 and perplexity is 28.30656907998641
At time: 148.20187854766846 and batch: 550, loss is 3.3823299407958984 and perplexity is 29.43928305450291
At time: 148.98437571525574 and batch: 600, loss is 3.3974438285827637 and perplexity is 29.88760448085493
At time: 149.7675428390503 and batch: 650, loss is 3.229452142715454 and perplexity is 25.265811119862033
At time: 150.55002093315125 and batch: 700, loss is 3.2155112552642824 and perplexity is 24.916027105443046
At time: 151.33373045921326 and batch: 750, loss is 3.3088285779953 and perplexity is 27.35306471532806
At time: 152.11650562286377 and batch: 800, loss is 3.269477047920227 and perplexity is 26.297583370805423
At time: 152.89947319030762 and batch: 850, loss is 3.3115029239654543 and perplexity is 27.42631417726509
At time: 153.69362616539001 and batch: 900, loss is 3.277488946914673 and perplexity is 26.50912323902902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306064135407748 and perplexity of 74.14807707991912
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 155.67211771011353 and batch: 50, loss is 3.6277549982070925 and perplexity is 37.62824624210221
At time: 156.45646357536316 and batch: 100, loss is 3.5182808589935304 and perplexity is 33.7263981629243
At time: 157.24170303344727 and batch: 150, loss is 3.5336800909042356 and perplexity is 34.249778272930754
At time: 158.0250265598297 and batch: 200, loss is 3.4082913112640383 and perplexity is 30.213574533753334
At time: 158.80931854248047 and batch: 250, loss is 3.5583478832244873 and perplexity is 35.10515139960041
At time: 159.5940272808075 and batch: 300, loss is 3.5131964826583864 and perplexity is 33.555355652946595
At time: 160.3782982826233 and batch: 350, loss is 3.504931640625 and perplexity is 33.2791688314633
At time: 161.16270518302917 and batch: 400, loss is 3.431694116592407 and perplexity is 30.928995715970306
At time: 161.94779682159424 and batch: 450, loss is 3.4527487325668336 and perplexity is 31.587097573155
At time: 162.7325940132141 and batch: 500, loss is 3.3353728294372558 and perplexity is 28.088853623332735
At time: 163.5176236629486 and batch: 550, loss is 3.374995756149292 and perplexity is 29.224159758000688
At time: 164.30310463905334 and batch: 600, loss is 3.391740164756775 and perplexity is 29.717620857874966
At time: 165.08757996559143 and batch: 650, loss is 3.223848333358765 and perplexity is 25.124622298199572
At time: 165.87182354927063 and batch: 700, loss is 3.2093221092224122 and perplexity is 24.762294402712627
At time: 166.67576694488525 and batch: 750, loss is 3.3022941970825195 and perplexity is 27.174912063887096
At time: 167.46016931533813 and batch: 800, loss is 3.2625816297531127 and perplexity is 26.11687428504649
At time: 168.24486112594604 and batch: 850, loss is 3.3022323751449587 and perplexity is 27.173232110099796
At time: 169.02946853637695 and batch: 900, loss is 3.2679985094070436 and perplexity is 26.258730111086475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304855555704195 and perplexity of 74.05851734984027
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 170.99562621116638 and batch: 50, loss is 3.6247834396362304 and perplexity is 37.51659767176426
At time: 171.79450488090515 and batch: 100, loss is 3.5143700218200684 and perplexity is 33.59475729205078
At time: 172.57929682731628 and batch: 150, loss is 3.5294186305999755 and perplexity is 34.10413474984472
At time: 173.364816904068 and batch: 200, loss is 3.4049918603897096 and perplexity is 30.11405060628399
At time: 174.15017533302307 and batch: 250, loss is 3.555584554672241 and perplexity is 35.00827824023407
At time: 174.93469071388245 and batch: 300, loss is 3.511688299179077 and perplexity is 33.50478616352824
At time: 175.719571352005 and batch: 350, loss is 3.50212167263031 and perplexity is 33.185786694010574
At time: 176.50444912910461 and batch: 400, loss is 3.429867992401123 and perplexity is 30.872567067222715
At time: 177.28957200050354 and batch: 450, loss is 3.451355776786804 and perplexity is 31.543128773422467
At time: 178.07459425926208 and batch: 500, loss is 3.3332434415817263 and perplexity is 28.02910519591573
At time: 178.8595540523529 and batch: 550, loss is 3.3722618293762205 and perplexity is 29.14437216163744
At time: 179.64374113082886 and batch: 600, loss is 3.3896259593963625 and perplexity is 29.65485807464334
At time: 180.42855548858643 and batch: 650, loss is 3.2216947364807127 and perplexity is 25.07057221199465
At time: 181.212984085083 and batch: 700, loss is 3.2068889284133912 and perplexity is 24.70211650472748
At time: 181.99711346626282 and batch: 750, loss is 3.3005428171157836 and perplexity is 27.127360120216157
At time: 182.7816653251648 and batch: 800, loss is 3.2602210426330567 and perplexity is 26.055295837233306
At time: 183.56607460975647 and batch: 850, loss is 3.2998373556137084 and perplexity is 27.108229560728432
At time: 184.35156202316284 and batch: 900, loss is 3.2649790048599243 and perplexity is 26.17956134149551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303903240047089 and perplexity of 73.98802383559217
Annealing...
Model not improving. Stopping early with 73.9504835891442 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -73.9504835891442
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe9c35db70>
ELAPSED
199.008891582489


RESULTS SO FAR:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1036694049835205 and batch: 50, loss is 7.09037636756897 and perplexity is 1200.359491988406
At time: 1.9782323837280273 and batch: 100, loss is 6.222184419631958 and perplexity is 503.8025470748738
At time: 2.839568614959717 and batch: 150, loss is 6.070656023025513 and perplexity is 432.9646231902193
At time: 3.714616298675537 and batch: 200, loss is 5.933260097503662 and perplexity is 377.3828153196795
At time: 4.576800107955933 and batch: 250, loss is 5.9665902233123775 and perplexity is 390.17299719151146
At time: 5.439199924468994 and batch: 300, loss is 5.876359605789185 and perplexity is 356.5090430704609
At time: 6.3013222217559814 and batch: 350, loss is 5.862630214691162 and perplexity is 351.64783804033937
At time: 7.16515040397644 and batch: 400, loss is 5.728773498535157 and perplexity is 307.591775163931
At time: 8.032974243164062 and batch: 450, loss is 5.728723382949829 and perplexity is 307.5763604083396
At time: 8.89994215965271 and batch: 500, loss is 5.6773831748962404 and perplexity is 292.1838346515446
At time: 9.767110586166382 and batch: 550, loss is 5.721419267654419 and perplexity is 305.33797188521106
At time: 10.633888483047485 and batch: 600, loss is 5.639865970611572 and perplexity is 281.42499672722545
At time: 11.510279655456543 and batch: 650, loss is 5.550156764984131 and perplexity is 257.27788490814925
At time: 12.377859830856323 and batch: 700, loss is 5.653763408660889 and perplexity is 285.3633865613569
At time: 13.245623111724854 and batch: 750, loss is 5.606364841461182 and perplexity is 272.1531179263721
At time: 14.11298131942749 and batch: 800, loss is 5.602994594573975 and perplexity is 271.2374386272242
At time: 14.980738162994385 and batch: 850, loss is 5.629903354644775 and perplexity is 278.6351875350491
At time: 15.8476243019104 and batch: 900, loss is 5.5199971199035645 and perplexity is 249.63431821774836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.407255407882063 and perplexity of 223.01865170206358
finished 1 epochs...
Completing Train Step...
At time: 17.87813687324524 and batch: 50, loss is 5.252471990585327 and perplexity is 191.03792921042677
At time: 18.66407084465027 and batch: 100, loss is 5.07226303100586 and perplexity is 159.5349516795333
At time: 19.44835638999939 and batch: 150, loss is 5.026780738830566 and perplexity is 152.44147300814546
At time: 20.233086824417114 and batch: 200, loss is 4.881005306243896 and perplexity is 131.76305955206743
At time: 21.01783585548401 and batch: 250, loss is 4.942797288894654 and perplexity is 140.16177467602836
At time: 21.80211901664734 and batch: 300, loss is 4.860130109786987 and perplexity is 129.04099051097475
At time: 22.587027072906494 and batch: 350, loss is 4.842361402511597 and perplexity is 126.76834966322552
At time: 23.371862173080444 and batch: 400, loss is 4.696904563903809 and perplexity is 109.6073642045278
At time: 24.168357610702515 and batch: 450, loss is 4.708199834823608 and perplexity is 110.85242750426829
At time: 24.95431923866272 and batch: 500, loss is 4.613465070724487 and perplexity is 100.83293826121213
At time: 25.738749742507935 and batch: 550, loss is 4.672597789764405 and perplexity is 106.9752810730658
At time: 26.523701667785645 and batch: 600, loss is 4.607263050079346 and perplexity is 100.2095055659925
At time: 27.30885076522827 and batch: 650, loss is 4.4698811054229735 and perplexity is 87.34633739021727
At time: 28.09266448020935 and batch: 700, loss is 4.520203304290772 and perplexity is 91.85427044730355
At time: 28.878357887268066 and batch: 750, loss is 4.5522112369537355 and perplexity is 94.84189451967156
At time: 29.663188219070435 and batch: 800, loss is 4.498304462432861 and perplexity is 89.8646331923854
At time: 30.447921752929688 and batch: 850, loss is 4.551349906921387 and perplexity is 94.76023951859753
At time: 31.232951164245605 and batch: 900, loss is 4.480121765136719 and perplexity is 88.24541723677638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.58073947854238 and perplexity of 97.58653067754217
finished 2 epochs...
Completing Train Step...
At time: 33.20514678955078 and batch: 50, loss is 4.518550214767456 and perplexity is 91.70255255133746
At time: 34.00263595581055 and batch: 100, loss is 4.384520559310913 and perplexity is 80.19976296431354
At time: 34.788100719451904 and batch: 150, loss is 4.382886180877685 and perplexity is 80.0687932575605
At time: 35.57429051399231 and batch: 200, loss is 4.265166373252868 and perplexity is 71.17676089760029
At time: 36.36001014709473 and batch: 250, loss is 4.396872711181641 and perplexity is 81.19654615139444
At time: 37.14589071273804 and batch: 300, loss is 4.355994000434875 and perplexity is 77.94426347547099
At time: 37.930986642837524 and batch: 350, loss is 4.3521403789520265 and perplexity is 77.64447379625078
At time: 38.716928243637085 and batch: 400, loss is 4.253399410247803 and perplexity is 70.3441349319574
At time: 39.50201225280762 and batch: 450, loss is 4.290856475830078 and perplexity is 73.02898928625837
At time: 40.28748369216919 and batch: 500, loss is 4.171622676849365 and perplexity is 64.82054961992333
At time: 41.07392430305481 and batch: 550, loss is 4.241745958328247 and perplexity is 69.52914090773767
At time: 41.85920262336731 and batch: 600, loss is 4.2310879611968994 and perplexity is 68.79203453966728
At time: 42.64500617980957 and batch: 650, loss is 4.075229635238648 and perplexity is 58.86399628428247
At time: 43.431299924850464 and batch: 700, loss is 4.100201444625855 and perplexity is 60.35244404840574
At time: 44.22872471809387 and batch: 750, loss is 4.183176288604736 and perplexity is 65.57380410991155
At time: 45.014071226119995 and batch: 800, loss is 4.146340179443359 and perplexity is 63.20226754864024
At time: 45.80016827583313 and batch: 850, loss is 4.212567615509033 and perplexity is 67.52970772554423
At time: 46.5860059261322 and batch: 900, loss is 4.1532737827301025 and perplexity is 63.64200973587208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404694021564641 and perplexity of 81.834099542271
finished 3 epochs...
Completing Train Step...
At time: 48.54568934440613 and batch: 50, loss is 4.2250754833221436 and perplexity is 68.37966487803058
At time: 49.34245324134827 and batch: 100, loss is 4.098337469100952 and perplexity is 60.24005334912867
At time: 50.1254825592041 and batch: 150, loss is 4.100103797912598 and perplexity is 60.34655111832472
At time: 50.9082396030426 and batch: 200, loss is 3.9813668394088744 and perplexity is 53.59023343330278
At time: 51.690136432647705 and batch: 250, loss is 4.124904389381409 and perplexity is 61.86189431362446
At time: 52.473267793655396 and batch: 300, loss is 4.094296836853028 and perplexity is 59.99713654618586
At time: 53.25602388381958 and batch: 350, loss is 4.089165902137756 and perplexity is 59.69008356350361
At time: 54.03905940055847 and batch: 400, loss is 4.009253582954407 and perplexity is 55.105723357662406
At time: 54.82230496406555 and batch: 450, loss is 4.050812945365906 and perplexity is 57.444137013727214
At time: 55.60502767562866 and batch: 500, loss is 3.9300582599639893 and perplexity is 50.909943594283746
At time: 56.387917280197144 and batch: 550, loss is 3.998747181892395 and perplexity is 54.52979131160413
At time: 57.1708345413208 and batch: 600, loss is 4.0077348327636715 and perplexity is 55.02209505113037
At time: 57.954951763153076 and batch: 650, loss is 3.846116094589233 and perplexity is 46.81090059923167
At time: 58.73777508735657 and batch: 700, loss is 3.8663475704193115 and perplexity is 47.76759928427003
At time: 59.520344495773315 and batch: 750, loss is 3.9609788513183593 and perplexity is 52.508699010954665
At time: 60.30369162559509 and batch: 800, loss is 3.9305070638656616 and perplexity is 50.932797303636136
At time: 61.08637762069702 and batch: 850, loss is 3.994477128982544 and perplexity is 54.29744264113979
At time: 61.87010741233826 and batch: 900, loss is 3.9428367233276367 and perplexity is 51.564668724795126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345650868873074 and perplexity of 77.14223061966366
finished 4 epochs...
Completing Train Step...
At time: 63.842339754104614 and batch: 50, loss is 4.027922077178955 and perplexity is 56.14412679227721
At time: 64.62606477737427 and batch: 100, loss is 3.906632261276245 and perplexity is 49.73118799176115
At time: 65.4095687866211 and batch: 150, loss is 3.9097509288787844 and perplexity is 49.886525133140125
At time: 66.19326782226562 and batch: 200, loss is 3.7935464906692506 and perplexity is 44.41363383549096
At time: 66.97738552093506 and batch: 250, loss is 3.939218907356262 and perplexity is 51.378454290608
At time: 67.76154088973999 and batch: 300, loss is 3.9113074207305907 and perplexity is 49.96423356360573
At time: 68.54477334022522 and batch: 350, loss is 3.9050695371627806 and perplexity is 49.65353255790827
At time: 69.32754826545715 and batch: 400, loss is 3.8363062572479247 and perplexity is 46.35393830599859
At time: 70.1110827922821 and batch: 450, loss is 3.8779268741607664 and perplexity is 48.323929567987996
At time: 70.894770860672 and batch: 500, loss is 3.758263711929321 and perplexity is 42.87391983985953
At time: 71.67853713035583 and batch: 550, loss is 3.825453796386719 and perplexity is 45.85360384544773
At time: 72.46250462532043 and batch: 600, loss is 3.8431382989883422 and perplexity is 46.67171464186921
At time: 73.24628853797913 and batch: 650, loss is 3.6804065561294554 and perplexity is 39.662515834072025
At time: 74.0296413898468 and batch: 700, loss is 3.7031962156295775 and perplexity is 40.576789487398436
At time: 74.81179666519165 and batch: 750, loss is 3.7991224765777587 and perplexity is 44.66197536288725
At time: 75.59586834907532 and batch: 800, loss is 3.769679570198059 and perplexity is 43.366166797059904
At time: 76.37955689430237 and batch: 850, loss is 3.834190640449524 and perplexity is 46.25597479860636
At time: 77.16349411010742 and batch: 900, loss is 3.7840574645996092 and perplexity is 43.994184934274415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332943014902611 and perplexity of 76.16812094872722
finished 5 epochs...
Completing Train Step...
At time: 79.12572979927063 and batch: 50, loss is 3.874811511039734 and perplexity is 48.17361724026091
At time: 79.9256980419159 and batch: 100, loss is 3.7618332529067993 and perplexity is 43.027233520537095
At time: 80.71135711669922 and batch: 150, loss is 3.7618860387802124 and perplexity is 43.02950481058451
At time: 81.49755120277405 and batch: 200, loss is 3.64831120967865 and perplexity is 38.409745239638205
At time: 82.28367972373962 and batch: 250, loss is 3.7937685537338255 and perplexity is 44.4234975582731
At time: 83.08098793029785 and batch: 300, loss is 3.770087466239929 and perplexity is 43.38385929295172
At time: 83.86760640144348 and batch: 350, loss is 3.7638256692886354 and perplexity is 43.11304714531637
At time: 84.65387463569641 and batch: 400, loss is 3.6964465284347536 and perplexity is 40.30383107921661
At time: 85.44038081169128 and batch: 450, loss is 3.74316125869751 and perplexity is 42.23128337871318
At time: 86.2263593673706 and batch: 500, loss is 3.6212993621826173 and perplexity is 37.386114378622125
At time: 87.01246428489685 and batch: 550, loss is 3.6903103828430175 and perplexity is 40.05727811984351
At time: 87.79863119125366 and batch: 600, loss is 3.711146993637085 and perplexity is 40.90069246692457
At time: 88.58473944664001 and batch: 650, loss is 3.5511935186386108 and perplexity is 34.85489263631752
At time: 89.3713629245758 and batch: 700, loss is 3.5705160427093507 and perplexity is 35.53492596043468
At time: 90.15706777572632 and batch: 750, loss is 3.669239950180054 and perplexity is 39.2220837908877
At time: 90.94266939163208 and batch: 800, loss is 3.6400920963287353 and perplexity is 38.095345005263574
At time: 91.72992277145386 and batch: 850, loss is 3.7067255783081055 and perplexity is 40.7202527117851
At time: 92.51652550697327 and batch: 900, loss is 3.656140851974487 and perplexity is 38.711660206271354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341126742428297 and perplexity of 76.79401768770195
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 94.48002338409424 and batch: 50, loss is 3.783905100822449 and perplexity is 43.98748232471516
At time: 95.2795352935791 and batch: 100, loss is 3.676947627067566 and perplexity is 39.525562997004144
At time: 96.06531047821045 and batch: 150, loss is 3.6802417850494384 and perplexity is 39.655981136881195
At time: 96.85052680969238 and batch: 200, loss is 3.547137584686279 and perplexity is 34.71380979861284
At time: 97.63615226745605 and batch: 250, loss is 3.687454023361206 and perplexity is 39.94302338764585
At time: 98.42156147956848 and batch: 300, loss is 3.6537334060668947 and perplexity is 38.61857607080021
At time: 99.20704746246338 and batch: 350, loss is 3.642555093765259 and perplexity is 38.18928938724935
At time: 99.9918270111084 and batch: 400, loss is 3.5613293838500977 and perplexity is 35.20997361656312
At time: 100.77766680717468 and batch: 450, loss is 3.59144428730011 and perplexity is 36.286446151890495
At time: 101.56202292442322 and batch: 500, loss is 3.4686071920394896 and perplexity is 32.092013290842935
At time: 102.34759449958801 and batch: 550, loss is 3.5167719507217408 and perplexity is 33.67554649665679
At time: 103.14608812332153 and batch: 600, loss is 3.5244731426239015 and perplexity is 33.935889531766044
At time: 103.93117165565491 and batch: 650, loss is 3.359722623825073 and perplexity is 28.781206550979757
At time: 104.71661591529846 and batch: 700, loss is 3.359583897590637 and perplexity is 28.777214119506315
At time: 105.50278830528259 and batch: 750, loss is 3.4435536909103392 and perplexity is 31.29798413721862
At time: 106.28920984268188 and batch: 800, loss is 3.3941080808639525 and perplexity is 29.788073070516926
At time: 107.07508540153503 and batch: 850, loss is 3.443280634880066 and perplexity is 31.289439200590884
At time: 107.8616099357605 and batch: 900, loss is 3.385324788093567 and perplexity is 29.527581365787025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313948121789384 and perplexity of 74.73496999776029
finished 7 epochs...
Completing Train Step...
At time: 109.84781813621521 and batch: 50, loss is 3.688509855270386 and perplexity is 39.98521877798753
At time: 110.63587760925293 and batch: 100, loss is 3.5751510667800903 and perplexity is 35.70001349440609
At time: 111.42216086387634 and batch: 150, loss is 3.5739107656478883 and perplexity is 35.65576217540271
At time: 112.20828676223755 and batch: 200, loss is 3.445726294517517 and perplexity is 31.36605617044884
At time: 112.99518132209778 and batch: 250, loss is 3.5879323959350584 and perplexity is 36.15923560054432
At time: 113.7814347743988 and batch: 300, loss is 3.5606677770614623 and perplexity is 35.18668616340874
At time: 114.56715035438538 and batch: 350, loss is 3.5513372135162355 and perplexity is 34.85990146571247
At time: 115.352787733078 and batch: 400, loss is 3.473924741744995 and perplexity is 32.26311869422447
At time: 116.13976240158081 and batch: 450, loss is 3.510825023651123 and perplexity is 33.47587478260325
At time: 116.92479991912842 and batch: 500, loss is 3.3924450778961184 and perplexity is 29.73857658440345
At time: 117.71009922027588 and batch: 550, loss is 3.444677166938782 and perplexity is 31.333166431666996
At time: 118.49723625183105 and batch: 600, loss is 3.4589292573928834 and perplexity is 31.78292695469424
At time: 119.28316116333008 and batch: 650, loss is 3.30117075920105 and perplexity is 27.144399880729278
At time: 120.0695948600769 and batch: 700, loss is 3.30467405796051 and perplexity is 27.239661590843454
At time: 120.85556745529175 and batch: 750, loss is 3.3955906009674073 and perplexity is 29.83226723896433
At time: 121.64107060432434 and batch: 800, loss is 3.3528690767288207 and perplexity is 28.584627597414183
At time: 122.43925380706787 and batch: 850, loss is 3.409659576416016 and perplexity is 30.254943009972745
At time: 123.22542214393616 and batch: 900, loss is 3.3593599462509154 and perplexity is 28.770770145446036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323458736889983 and perplexity of 75.44913622890803
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 125.18849873542786 and batch: 50, loss is 3.6587916517257693 and perplexity is 38.81441319414701
At time: 125.98828792572021 and batch: 100, loss is 3.5560743474960326 and perplexity is 35.02542924356573
At time: 126.77387189865112 and batch: 150, loss is 3.5605748653411866 and perplexity is 35.18341705973787
At time: 127.55917072296143 and batch: 200, loss is 3.4259710693359375 and perplexity is 30.752493159880085
At time: 128.34459567070007 and batch: 250, loss is 3.5683935689926147 and perplexity is 35.459583998008966
At time: 129.12967681884766 and batch: 300, loss is 3.537699275016785 and perplexity is 34.38771144134404
At time: 129.91448497772217 and batch: 350, loss is 3.5215148305892945 and perplexity is 33.835644931870725
At time: 130.6992061138153 and batch: 400, loss is 3.4442695426940917 and perplexity is 31.320396876133564
At time: 131.48436284065247 and batch: 450, loss is 3.4746625232696533 and perplexity is 32.286930610040585
At time: 132.270653963089 and batch: 500, loss is 3.354958610534668 and perplexity is 28.644418589000928
At time: 133.05571746826172 and batch: 550, loss is 3.397780442237854 and perplexity is 29.89766675009472
At time: 133.8410656452179 and batch: 600, loss is 3.409301743507385 and perplexity is 30.244118732471883
At time: 134.6266520023346 and batch: 650, loss is 3.2449717044830324 and perplexity is 25.66098395914859
At time: 135.41166925430298 and batch: 700, loss is 3.241677007675171 and perplexity is 25.576577919718606
At time: 136.19674563407898 and batch: 750, loss is 3.3308204364776612 and perplexity is 27.96127274333982
At time: 136.98237204551697 and batch: 800, loss is 3.2825705718994143 and perplexity is 26.64417551389102
At time: 137.76780438423157 and batch: 850, loss is 3.3329424715042113 and perplexity is 28.020670543304934
At time: 138.55382823944092 and batch: 900, loss is 3.284183373451233 and perplexity is 26.687181952610203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319665778173159 and perplexity of 75.16350281029376
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 140.51466488838196 and batch: 50, loss is 3.647769184112549 and perplexity is 38.38893181694407
At time: 141.31519770622253 and batch: 100, loss is 3.539422755241394 and perplexity is 34.44702908369152
At time: 142.10046243667603 and batch: 150, loss is 3.5426682901382445 and perplexity is 34.559009738942024
At time: 142.89783477783203 and batch: 200, loss is 3.4086274814605715 and perplexity is 30.223733144457725
At time: 143.68398785591125 and batch: 250, loss is 3.554214458465576 and perplexity is 34.9603463741472
At time: 144.46964359283447 and batch: 300, loss is 3.522602982521057 and perplexity is 33.87248329349801
At time: 145.25563764572144 and batch: 350, loss is 3.5069650077819823 and perplexity is 33.34690644475705
At time: 146.04089307785034 and batch: 400, loss is 3.4284155559539795 and perplexity is 30.8277591737833
At time: 146.82525253295898 and batch: 450, loss is 3.457104721069336 and perplexity is 31.72499071945172
At time: 147.61072850227356 and batch: 500, loss is 3.340269603729248 and perplexity is 28.22673571286682
At time: 148.39626336097717 and batch: 550, loss is 3.3799214363098145 and perplexity is 29.368463727766727
At time: 149.18226790428162 and batch: 600, loss is 3.39269681930542 and perplexity is 29.746063957984877
At time: 149.9681680202484 and batch: 650, loss is 3.2228403759002684 and perplexity is 25.099310506509827
At time: 150.75326204299927 and batch: 700, loss is 3.219357523918152 and perplexity is 25.01204537713923
At time: 151.5383596420288 and batch: 750, loss is 3.3088993930816653 and perplexity is 27.355001793554596
At time: 152.3236780166626 and batch: 800, loss is 3.261701898574829 and perplexity is 26.09390855977205
At time: 153.1096694469452 and batch: 850, loss is 3.3107087564468385 and perplexity is 27.4045417360173
At time: 153.89546537399292 and batch: 900, loss is 3.2624887657165527 and perplexity is 26.114449079286988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314771678349743 and perplexity of 74.79654382387423
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 155.8702688217163 and batch: 50, loss is 3.6395487117767336 and perplexity is 38.07465020641075
At time: 156.6530454158783 and batch: 100, loss is 3.5290363121032713 and perplexity is 34.09109860045506
At time: 157.43551969528198 and batch: 150, loss is 3.5344840717315673 and perplexity is 34.27732551024403
At time: 158.21849036216736 and batch: 200, loss is 3.4020883369445802 and perplexity is 30.02674056902947
At time: 159.0020079612732 and batch: 250, loss is 3.547502598762512 and perplexity is 34.72648314066242
At time: 159.7841854095459 and batch: 300, loss is 3.5160172843933104 and perplexity is 33.65014228268241
At time: 160.5663981437683 and batch: 350, loss is 3.5017168617248533 and perplexity is 33.17235544439226
At time: 161.349627494812 and batch: 400, loss is 3.423217940330505 and perplexity is 30.667944019722224
At time: 162.14409136772156 and batch: 450, loss is 3.450208692550659 and perplexity is 31.506966891975015
At time: 162.92679643630981 and batch: 500, loss is 3.334080662727356 and perplexity is 28.05258158157078
At time: 163.70880651474 and batch: 550, loss is 3.372895531654358 and perplexity is 29.162846869785223
At time: 164.49158573150635 and batch: 600, loss is 3.387846031188965 and perplexity is 29.602121503847957
At time: 165.27355289459229 and batch: 650, loss is 3.2169433641433716 and perplexity is 24.95173513188033
At time: 166.05524230003357 and batch: 700, loss is 3.2125431442260743 and perplexity is 24.842183213010973
At time: 166.83719944953918 and batch: 750, loss is 3.300166754722595 and perplexity is 27.117160458223314
At time: 167.61992621421814 and batch: 800, loss is 3.2543309783935546 and perplexity is 25.902279551230645
At time: 168.40216898918152 and batch: 850, loss is 3.302934422492981 and perplexity is 27.1923157036492
At time: 169.18471574783325 and batch: 900, loss is 3.2552758693695067 and perplexity is 25.926765948101085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310934824486301 and perplexity of 74.51011026897034
finished 11 epochs...
Completing Train Step...
At time: 171.14540028572083 and batch: 50, loss is 3.6338354206085204 and perplexity is 37.85773887248702
At time: 171.9436981678009 and batch: 100, loss is 3.523047103881836 and perplexity is 33.88753012793425
At time: 172.72771883010864 and batch: 150, loss is 3.528392515182495 and perplexity is 34.06915791957943
At time: 173.5111711025238 and batch: 200, loss is 3.396382703781128 and perplexity is 29.855906823038506
At time: 174.29457116127014 and batch: 250, loss is 3.541711006164551 and perplexity is 34.525942782531125
At time: 175.07801580429077 and batch: 300, loss is 3.510462875366211 and perplexity is 33.46375374690342
At time: 175.86133575439453 and batch: 350, loss is 3.495693120956421 and perplexity is 32.97313440486757
At time: 176.64507031440735 and batch: 400, loss is 3.418022403717041 and perplexity is 30.509020796911845
At time: 177.42888641357422 and batch: 450, loss is 3.4464050769805907 and perplexity is 31.387354126835024
At time: 178.2144238948822 and batch: 500, loss is 3.3304238605499266 and perplexity is 27.95018617413952
At time: 178.99874877929688 and batch: 550, loss is 3.369636721611023 and perplexity is 29.06796537586524
At time: 179.78304600715637 and batch: 600, loss is 3.3852907609939575 and perplexity is 29.526576644928628
At time: 180.56682348251343 and batch: 650, loss is 3.2149410343170164 and perplexity is 24.901823514842178
At time: 181.3506236076355 and batch: 700, loss is 3.2114014530181887 and perplexity is 24.813837295073615
At time: 182.15369081497192 and batch: 750, loss is 3.3005109167099 and perplexity is 27.126494760220513
At time: 182.93718028068542 and batch: 800, loss is 3.254968843460083 and perplexity is 25.918806981074518
At time: 183.7216601371765 and batch: 850, loss is 3.3054478788375854 and perplexity is 27.260748367308615
At time: 184.5058331489563 and batch: 900, loss is 3.259229598045349 and perplexity is 26.029476256651048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309988361515411 and perplexity of 74.43962257090375
finished 12 epochs...
Completing Train Step...
At time: 186.47024512290955 and batch: 50, loss is 3.6309629583358767 and perplexity is 37.74914997926197
At time: 187.26886510849 and batch: 100, loss is 3.5195993089675905 and perplexity is 33.770894058067604
At time: 188.05451798439026 and batch: 150, loss is 3.5249315214157106 and perplexity is 33.9514485895072
At time: 188.84088897705078 and batch: 200, loss is 3.3929520893096923 and perplexity is 29.753658205109026
At time: 189.62666177749634 and batch: 250, loss is 3.538027958869934 and perplexity is 34.3990159845502
At time: 190.41165137290955 and batch: 300, loss is 3.506923542022705 and perplexity is 33.34552371862986
At time: 191.1975245475769 and batch: 350, loss is 3.4919916820526122 and perplexity is 32.85131196065525
At time: 191.98311376571655 and batch: 400, loss is 3.4147865533828736 and perplexity is 30.41045772544715
At time: 192.7690691947937 and batch: 450, loss is 3.4436004304885866 and perplexity is 31.29944702598432
At time: 193.5550570487976 and batch: 500, loss is 3.327804675102234 and perplexity is 27.87707524059441
At time: 194.34101605415344 and batch: 550, loss is 3.3672505950927736 and perplexity is 28.998688217752076
At time: 195.12713861465454 and batch: 600, loss is 3.3834867429733277 and perplexity is 29.47335818653586
At time: 195.9129135608673 and batch: 650, loss is 3.213660898208618 and perplexity is 24.869966186604476
At time: 196.69860577583313 and batch: 700, loss is 3.2106643867492677 and perplexity is 24.79555459121078
At time: 197.48545956611633 and batch: 750, loss is 3.300564513206482 and perplexity is 27.127948684266475
At time: 198.2730975151062 and batch: 800, loss is 3.255444211959839 and perplexity is 25.9311308944326
At time: 199.05949711799622 and batch: 850, loss is 3.3069781970977785 and perplexity is 27.302497925229375
At time: 199.84573698043823 and batch: 900, loss is 3.2615205001831056 and perplexity is 26.08917559601451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309705342331978 and perplexity of 74.41855771072903
finished 13 epochs...
Completing Train Step...
At time: 201.82219696044922 and batch: 50, loss is 3.628685040473938 and perplexity is 37.663258380395135
At time: 202.60732531547546 and batch: 100, loss is 3.5168593978881835 and perplexity is 33.67849145653866
At time: 203.393150806427 and batch: 150, loss is 3.5221133470535277 and perplexity is 33.85590218398514
At time: 204.17914175987244 and batch: 200, loss is 3.3901424741744997 and perplexity is 29.670179203536577
At time: 204.9642026424408 and batch: 250, loss is 3.5350449180603025 and perplexity is 34.29655521436574
At time: 205.74983286857605 and batch: 300, loss is 3.504006929397583 and perplexity is 33.248409434377486
At time: 206.5487506389618 and batch: 350, loss is 3.489016532897949 and perplexity is 32.7537196554647
At time: 207.3434121608734 and batch: 400, loss is 3.4121886777877806 and perplexity is 30.33155767012511
At time: 208.12944340705872 and batch: 450, loss is 3.441230449676514 and perplexity is 31.225355769212396
At time: 208.9158010482788 and batch: 500, loss is 3.325622706413269 and perplexity is 27.816314648241224
At time: 209.70127177238464 and batch: 550, loss is 3.3652437782287596 and perplexity is 28.940551515570814
At time: 210.48640418052673 and batch: 600, loss is 3.381943497657776 and perplexity is 29.42790864349835
At time: 211.27166104316711 and batch: 650, loss is 3.2125645971298216 and perplexity is 24.842716155692877
At time: 212.0571324825287 and batch: 700, loss is 3.2100043201446535 and perplexity is 24.779193274055803
At time: 212.84236311912537 and batch: 750, loss is 3.3004660320281984 and perplexity is 27.1252772234621
At time: 213.62815237045288 and batch: 800, loss is 3.2557530927658083 and perplexity is 25.939141760180615
At time: 214.41391968727112 and batch: 850, loss is 3.307982301712036 and perplexity is 27.329926257529877
At time: 215.19961810112 and batch: 900, loss is 3.2630316972732545 and perplexity is 26.128631287413803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309643053028681 and perplexity of 74.41392237498431
finished 14 epochs...
Completing Train Step...
At time: 217.1620638370514 and batch: 50, loss is 3.6266573905944823 and perplexity is 37.58696785046031
At time: 217.96065521240234 and batch: 100, loss is 3.514475841522217 and perplexity is 33.59831246736165
At time: 218.74673199653625 and batch: 150, loss is 3.5196163415908814 and perplexity is 33.771469269882964
At time: 219.5315306186676 and batch: 200, loss is 3.3876549291610716 and perplexity is 29.596465018898716
At time: 220.31765604019165 and batch: 250, loss is 3.532435727119446 and perplexity is 34.20718559509976
At time: 221.1142704486847 and batch: 300, loss is 3.501440043449402 and perplexity is 33.16317400101977
At time: 221.90044045448303 and batch: 350, loss is 3.4864294576644896 and perplexity is 32.66909283371799
At time: 222.68563151359558 and batch: 400, loss is 3.409915118217468 and perplexity is 30.26267540054478
At time: 223.47049498558044 and batch: 450, loss is 3.4391253566741944 and perplexity is 31.159692629044525
At time: 224.25531554222107 and batch: 500, loss is 3.3236878156661986 and perplexity is 27.762545154234157
At time: 225.03986763954163 and batch: 550, loss is 3.3634533309936523 and perplexity is 28.88878134483678
At time: 225.8260395526886 and batch: 600, loss is 3.380541272163391 and perplexity is 29.38667299734574
At time: 226.6121904850006 and batch: 650, loss is 3.2115448904037476 and perplexity is 24.817396782296573
At time: 227.39879250526428 and batch: 700, loss is 3.2093551111221315 and perplexity is 24.763111618954092
At time: 228.1853482723236 and batch: 750, loss is 3.3002692222595216 and perplexity is 27.11993922922839
At time: 228.97187161445618 and batch: 800, loss is 3.2559264659881593 and perplexity is 25.943639302638044
At time: 229.75830101966858 and batch: 850, loss is 3.308665943145752 and perplexity is 27.348616515489365
At time: 230.54409384727478 and batch: 900, loss is 3.2640984582901003 and perplexity is 26.156519164900374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309679005244007 and perplexity of 74.41659776843764
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 232.50953459739685 and batch: 50, loss is 3.6258313941955564 and perplexity is 37.555933969070885
At time: 233.3087830543518 and batch: 100, loss is 3.514074430465698 and perplexity is 33.584828439756826
At time: 234.0951907634735 and batch: 150, loss is 3.519611144065857 and perplexity is 33.771293742282474
At time: 234.88099122047424 and batch: 200, loss is 3.388190884590149 and perplexity is 29.61233165653258
At time: 235.66788411140442 and batch: 250, loss is 3.532788619995117 and perplexity is 34.21925919741185
At time: 236.45420932769775 and batch: 300, loss is 3.501326198577881 and perplexity is 33.15939875863646
At time: 237.24040818214417 and batch: 350, loss is 3.4867951250076294 and perplexity is 32.6810410384983
At time: 238.02541828155518 and batch: 400, loss is 3.4104995250701906 and perplexity is 30.280366284263916
At time: 238.81287145614624 and batch: 450, loss is 3.438962984085083 and perplexity is 31.154633559814624
At time: 239.59837412834167 and batch: 500, loss is 3.3219050788879394 and perplexity is 27.71309593449738
At time: 240.38322234153748 and batch: 550, loss is 3.361316990852356 and perplexity is 28.827130958162
At time: 241.18024706840515 and batch: 600, loss is 3.3788465213775636 and perplexity is 29.336912088270708
At time: 241.96601223945618 and batch: 650, loss is 3.2100542831420897 and perplexity is 24.780431347754504
At time: 242.75288271903992 and batch: 700, loss is 3.2072511529922485 and perplexity is 24.711065839211894
At time: 243.5389952659607 and batch: 750, loss is 3.2967847633361815 and perplexity is 27.0256053616698
At time: 244.32467651367188 and batch: 800, loss is 3.252915034294128 and perplexity is 25.865629324797645
At time: 245.11005330085754 and batch: 850, loss is 3.3053501272201538 and perplexity is 27.258083715302458
At time: 245.89570021629333 and batch: 900, loss is 3.260467653274536 and perplexity is 26.061722142818763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308027293584118 and perplexity of 74.29378446022193
finished 16 epochs...
Completing Train Step...
At time: 247.88367652893066 and batch: 50, loss is 3.624488353729248 and perplexity is 37.5055286857446
At time: 248.6695692539215 and batch: 100, loss is 3.512837862968445 and perplexity is 33.54332419919351
At time: 249.4556484222412 and batch: 150, loss is 3.5182802534103392 and perplexity is 33.72637773879065
At time: 250.2410271167755 and batch: 200, loss is 3.3868473863601682 and perplexity is 29.572574254346293
At time: 251.02753448486328 and batch: 250, loss is 3.5315870666503906 and perplexity is 34.178167623871715
At time: 251.81486415863037 and batch: 300, loss is 3.500076379776001 and perplexity is 33.117981406093165
At time: 252.60149931907654 and batch: 350, loss is 3.4855947256088258 and perplexity is 32.64183427308017
At time: 253.38709998130798 and batch: 400, loss is 3.4092681407928467 and perplexity is 30.243102465058417
At time: 254.17372012138367 and batch: 450, loss is 3.4380405521392823 and perplexity is 31.125908780922966
At time: 254.96017479896545 and batch: 500, loss is 3.321433062553406 and perplexity is 27.700017987280972
At time: 255.74606680870056 and batch: 550, loss is 3.360852861404419 and perplexity is 28.813754542219172
At time: 256.5322206020355 and batch: 600, loss is 3.378444333076477 and perplexity is 29.325115497823152
At time: 257.31953740119934 and batch: 650, loss is 3.2097172212600706 and perplexity is 24.772080216430272
At time: 258.1067326068878 and batch: 700, loss is 3.2070867156982423 and perplexity is 24.707002752483934
At time: 258.89367938041687 and batch: 750, loss is 3.296973557472229 and perplexity is 27.03070811915541
At time: 259.67887568473816 and batch: 800, loss is 3.253194251060486 and perplexity is 25.8728524505395
At time: 260.4763889312744 and batch: 850, loss is 3.3059974813461306 and perplexity is 27.275735060981084
At time: 261.26305079460144 and batch: 900, loss is 3.2613183641433716 and perplexity is 26.08390256633223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30732204489512 and perplexity of 74.24140733775003
finished 17 epochs...
Completing Train Step...
At time: 263.22487568855286 and batch: 50, loss is 3.6236267948150633 and perplexity is 37.47322937904993
At time: 264.02198457717896 and batch: 100, loss is 3.5119827651977538 and perplexity is 33.51465363726298
At time: 264.8051507472992 and batch: 150, loss is 3.5173503446578978 and perplexity is 33.69502986253441
At time: 265.5883905887604 and batch: 200, loss is 3.385930395126343 and perplexity is 29.545468892582317
At time: 266.37162828445435 and batch: 250, loss is 3.5306902027130125 and perplexity is 34.1475281996353
At time: 267.15501165390015 and batch: 300, loss is 3.4992096996307374 and perplexity is 33.08929114359359
At time: 267.93932914733887 and batch: 350, loss is 3.4847174787521364 and perplexity is 32.613211882854024
At time: 268.7229537963867 and batch: 400, loss is 3.40841655254364 and perplexity is 30.217358757454846
At time: 269.5067298412323 and batch: 450, loss is 3.4373730850219726 and perplexity is 31.105140192261373
At time: 270.29219913482666 and batch: 500, loss is 3.3209643173217773 and perplexity is 27.687036778610928
At time: 271.0791952610016 and batch: 550, loss is 3.3604276466369627 and perplexity is 28.801505112785673
At time: 271.87411236763 and batch: 600, loss is 3.378113212585449 and perplexity is 29.3154069586169
At time: 272.6647937297821 and batch: 650, loss is 3.209436469078064 and perplexity is 24.765126377055132
At time: 273.455766916275 and batch: 700, loss is 3.206951642036438 and perplexity is 24.70366571252812
At time: 274.24505281448364 and batch: 750, loss is 3.2970872640609743 and perplexity is 27.033781863516186
At time: 275.0386862754822 and batch: 800, loss is 3.2533668184280398 and perplexity is 25.87731764584042
At time: 275.8430757522583 and batch: 850, loss is 3.306452956199646 and perplexity is 27.28816130211786
At time: 276.63566994667053 and batch: 900, loss is 3.261938409805298 and perplexity is 26.100080792065615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30698593348673 and perplexity of 74.21645814685762
finished 18 epochs...
Completing Train Step...
At time: 278.6464931964874 and batch: 50, loss is 3.622944278717041 and perplexity is 37.44766202281245
At time: 279.4543194770813 and batch: 100, loss is 3.511268095970154 and perplexity is 33.490710302436156
At time: 280.2466251850128 and batch: 150, loss is 3.516585736274719 and perplexity is 33.66927620721513
At time: 281.0548903942108 and batch: 200, loss is 3.3851824760437013 and perplexity is 29.52337953414801
At time: 281.8494656085968 and batch: 250, loss is 3.52992582321167 and perplexity is 34.12143650230768
At time: 282.64460730552673 and batch: 300, loss is 3.498484649658203 and perplexity is 33.0653084493353
At time: 283.43994879722595 and batch: 350, loss is 3.483974642753601 and perplexity is 32.58899461088387
At time: 284.23521637916565 and batch: 400, loss is 3.4077235507965087 and perplexity is 30.19642532933114
At time: 285.02828574180603 and batch: 450, loss is 3.4368037366867066 and perplexity is 31.087435573000704
At time: 285.8213002681732 and batch: 500, loss is 3.3204997205734252 and perplexity is 27.674176459014742
At time: 286.6137845516205 and batch: 550, loss is 3.3600101375579836 and perplexity is 28.789482732813855
At time: 287.4061453342438 and batch: 600, loss is 3.377796769142151 and perplexity is 29.306131757912848
At time: 288.1991858482361 and batch: 650, loss is 3.2091759252548218 and perplexity is 24.758674816839353
At time: 288.99379229545593 and batch: 700, loss is 3.20681519985199 and perplexity is 24.700295320351493
At time: 289.78938126564026 and batch: 750, loss is 3.297146716117859 and perplexity is 27.035389125230374
At time: 290.5849609375 and batch: 800, loss is 3.2534819936752317 and perplexity is 25.88029824393921
At time: 291.38000297546387 and batch: 850, loss is 3.306794829368591 and perplexity is 27.297491987161457
At time: 292.17414832115173 and batch: 900, loss is 3.262414183616638 and perplexity is 26.11250148146541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306815787537457 and perplexity of 74.20383159134364
finished 19 epochs...
Completing Train Step...
At time: 294.19934844970703 and batch: 50, loss is 3.6223469257354735 and perplexity is 37.4252992301558
At time: 294.9958817958832 and batch: 100, loss is 3.510622944831848 and perplexity is 33.46911070081484
At time: 295.7925498485565 and batch: 150, loss is 3.515907073020935 and perplexity is 33.646433858679806
At time: 296.5912675857544 and batch: 200, loss is 3.3845176601409914 and perplexity is 29.503758444860743
At time: 297.3908293247223 and batch: 250, loss is 3.529234914779663 and perplexity is 34.09786985625075
At time: 298.1887061595917 and batch: 300, loss is 3.4978259420394897 and perplexity is 33.04353525061782
At time: 298.9872236251831 and batch: 350, loss is 3.483302731513977 and perplexity is 32.56710505386001
At time: 299.7829854488373 and batch: 400, loss is 3.407111186981201 and perplexity is 30.17793979162268
At time: 300.5914103984833 and batch: 450, loss is 3.4362817668914794 and perplexity is 31.071213104808045
At time: 301.3874216079712 and batch: 500, loss is 3.3200452661514284 and perplexity is 27.66160266447249
At time: 302.1883466243744 and batch: 550, loss is 3.35959903717041 and perplexity is 28.777649797733105
At time: 302.98810482025146 and batch: 600, loss is 3.377484927177429 and perplexity is 29.296994301002368
At time: 303.7903425693512 and batch: 650, loss is 3.2089270782470702 and perplexity is 24.752514461220112
At time: 304.5938322544098 and batch: 700, loss is 3.2066744995117187 and perplexity is 24.696820224874415
At time: 305.3998968601227 and batch: 750, loss is 3.2971695470809936 and perplexity is 27.036006376249016
At time: 306.19814920425415 and batch: 800, loss is 3.2535643577575684 and perplexity is 25.88242993874101
At time: 306.99607157707214 and batch: 850, loss is 3.307064719200134 and perplexity is 27.304860296946636
At time: 307.7932720184326 and batch: 900, loss is 3.2627958822250367 and perplexity is 26.122470489399888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306729251391267 and perplexity of 74.19741055555504
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe9c35db70>
ELAPSED
515.5151348114014


RESULTS SO FAR:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.19741055555504, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.24622883758484704, 'dropout': 0.5513232670420983, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1371357440948486 and batch: 50, loss is 7.064636325836181 and perplexity is 1169.8564475191893
At time: 2.0273027420043945 and batch: 100, loss is 6.082175807952881 and perplexity is 437.98112154533493
At time: 2.906203269958496 and batch: 150, loss is 5.898015155792236 and perplexity is 364.3136439381159
At time: 3.7866997718811035 and batch: 200, loss is 5.688857517242432 and perplexity is 295.55576031737434
At time: 4.666423320770264 and batch: 250, loss is 5.70272608757019 and perplexity is 299.68325120831975
At time: 5.546568870544434 and batch: 300, loss is 5.580650882720947 and perplexity is 265.2441924750007
At time: 6.4241437911987305 and batch: 350, loss is 5.541951370239258 and perplexity is 255.1754557301206
At time: 7.301614999771118 and batch: 400, loss is 5.382328729629517 and perplexity is 217.52825053895685
At time: 8.17811894416809 and batch: 450, loss is 5.371917190551758 and perplexity is 215.27519589377954
At time: 9.054720401763916 and batch: 500, loss is 5.3075254535675045 and perplexity is 201.85012237534485
At time: 9.931169509887695 and batch: 550, loss is 5.353668088912964 and perplexity is 211.3822464173247
At time: 10.811144828796387 and batch: 600, loss is 5.253155355453491 and perplexity is 191.16852243606502
At time: 11.69143033027649 and batch: 650, loss is 5.140038204193115 and perplexity is 170.72229050409024
At time: 12.572789907455444 and batch: 700, loss is 5.22309139251709 and perplexity is 185.50677281574943
At time: 13.45531439781189 and batch: 750, loss is 5.200267028808594 and perplexity is 181.32065324925836
At time: 14.331560134887695 and batch: 800, loss is 5.161914615631104 and perplexity is 174.49823301996076
At time: 15.207611083984375 and batch: 850, loss is 5.192961769104004 and perplexity is 180.00088528065984
At time: 16.098546028137207 and batch: 900, loss is 5.103143730163574 and perplexity is 164.53835928486615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.997087452509632 and perplexity of 147.9815276072341
finished 1 epochs...
Completing Train Step...
At time: 18.190122604370117 and batch: 50, loss is 4.932732849121094 and perplexity is 138.75819988090856
At time: 18.987207412719727 and batch: 100, loss is 4.802118101119995 and perplexity is 121.7680616319913
At time: 19.783694982528687 and batch: 150, loss is 4.779847660064697 and perplexity is 119.08620707781805
At time: 20.58092999458313 and batch: 200, loss is 4.658718996047973 and perplexity is 105.50084855382292
At time: 21.37788438796997 and batch: 250, loss is 4.763768939971924 and perplexity is 117.1867645109636
At time: 22.17177724838257 and batch: 300, loss is 4.69258560180664 and perplexity is 109.13499495966556
At time: 22.965851068496704 and batch: 350, loss is 4.685465221405029 and perplexity is 108.36067228791178
At time: 23.759506225585938 and batch: 400, loss is 4.557661676406861 and perplexity is 95.36023583413696
At time: 24.552252531051636 and batch: 450, loss is 4.581629772186279 and perplexity is 97.67345003166757
At time: 25.3453950881958 and batch: 500, loss is 4.474636116027832 and perplexity is 87.76265917362718
At time: 26.139832496643066 and batch: 550, loss is 4.540059938430786 and perplexity is 93.69641596731307
At time: 26.938692331314087 and batch: 600, loss is 4.499176931381226 and perplexity is 89.94307150692322
At time: 27.739409923553467 and batch: 650, loss is 4.352928962707519 and perplexity is 77.70572711550228
At time: 28.537003755569458 and batch: 700, loss is 4.394256715774536 and perplexity is 80.98441394899922
At time: 29.33179783821106 and batch: 750, loss is 4.443388004302978 and perplexity is 85.06264663862953
At time: 30.125791788101196 and batch: 800, loss is 4.395986213684082 and perplexity is 81.12459751227696
At time: 30.919627904891968 and batch: 850, loss is 4.454674406051636 and perplexity is 86.02813604417598
At time: 31.724876403808594 and batch: 900, loss is 4.3873719882965085 and perplexity is 80.42877324117177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.522462191647047 and perplexity of 92.06199342050894
finished 2 epochs...
Completing Train Step...
At time: 33.859018325805664 and batch: 50, loss is 4.429650621414185 and perplexity is 83.90209819248756
At time: 34.68499684333801 and batch: 100, loss is 4.30106472492218 and perplexity is 73.77830549602346
At time: 35.48399233818054 and batch: 150, loss is 4.302528595924377 and perplexity is 73.88638650705856
At time: 36.30152702331543 and batch: 200, loss is 4.183835463523865 and perplexity is 65.61704296635186
At time: 37.09679651260376 and batch: 250, loss is 4.331551475524902 and perplexity is 76.06220372018949
At time: 37.88963317871094 and batch: 300, loss is 4.287238917350769 and perplexity is 72.76527992643685
At time: 38.68209099769592 and batch: 350, loss is 4.287875785827636 and perplexity is 72.81163659941743
At time: 39.476847648620605 and batch: 400, loss is 4.1942702627182005 and perplexity is 66.30532844850815
At time: 40.273003339767456 and batch: 450, loss is 4.230424103736877 and perplexity is 68.7463815895504
At time: 41.0682270526886 and batch: 500, loss is 4.108044872283935 and perplexity is 60.8276753621784
At time: 41.86227107048035 and batch: 550, loss is 4.179486799240112 and perplexity is 65.33231601486388
At time: 42.66509675979614 and batch: 600, loss is 4.17570743560791 and perplexity is 65.08586743845066
At time: 43.470030784606934 and batch: 650, loss is 4.0202250480651855 and perplexity is 55.71364266353314
At time: 44.27319407463074 and batch: 700, loss is 4.041099457740784 and perplexity is 56.888855326587425
At time: 45.07657265663147 and batch: 750, loss is 4.128177366256714 and perplexity is 62.06469856895152
At time: 45.87630343437195 and batch: 800, loss is 4.09008177280426 and perplexity is 59.7447770023556
At time: 46.674262046813965 and batch: 850, loss is 4.153891544342041 and perplexity is 63.681337472725886
At time: 47.476969957351685 and batch: 900, loss is 4.101455049514771 and perplexity is 60.42814960983938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381592632973031 and perplexity of 79.96528739714114
finished 3 epochs...
Completing Train Step...
At time: 49.530945777893066 and batch: 50, loss is 4.170801100730896 and perplexity is 64.76731647490416
At time: 50.356343030929565 and batch: 100, loss is 4.045079798698425 and perplexity is 57.11574361429801
At time: 51.15974259376526 and batch: 150, loss is 4.052859115600586 and perplexity is 57.561797833022695
At time: 51.96142554283142 and batch: 200, loss is 3.9290925216674806 and perplexity is 50.86080164503218
At time: 52.76098036766052 and batch: 250, loss is 4.089584932327271 and perplexity is 59.71510075164387
At time: 53.56054401397705 and batch: 300, loss is 4.048222041130066 and perplexity is 57.29549739447252
At time: 54.35736584663391 and batch: 350, loss is 4.047316780090332 and perplexity is 57.24365348260643
At time: 55.15668988227844 and batch: 400, loss is 3.973033561706543 and perplexity is 53.14550672516318
At time: 55.95155191421509 and batch: 450, loss is 4.011075553894043 and perplexity is 55.20621590367752
At time: 56.774882555007935 and batch: 500, loss is 3.890381054878235 and perplexity is 48.92952780709096
At time: 57.57581663131714 and batch: 550, loss is 3.9571711349487306 and perplexity is 52.30914094954236
At time: 58.37490129470825 and batch: 600, loss is 3.9680909633636476 and perplexity is 52.88347791669931
At time: 59.17540001869202 and batch: 650, loss is 3.811881351470947 and perplexity is 45.23546266564038
At time: 59.97361373901367 and batch: 700, loss is 3.828634624481201 and perplexity is 45.99968848869166
At time: 60.77216982841492 and batch: 750, loss is 3.925789680480957 and perplexity is 50.69309360354992
At time: 61.56771111488342 and batch: 800, loss is 3.8922637414932253 and perplexity is 49.02173354418937
At time: 62.36417770385742 and batch: 850, loss is 3.9539137935638426 and perplexity is 52.139029425983125
At time: 63.15872836112976 and batch: 900, loss is 3.9105645704269407 and perplexity is 49.927131399914316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33477323349208 and perplexity of 76.30765290765586
finished 4 epochs...
Completing Train Step...
At time: 65.26692628860474 and batch: 50, loss is 3.987843337059021 and perplexity is 53.93843680625205
At time: 66.06084394454956 and batch: 100, loss is 3.86673122882843 and perplexity is 47.785929241415104
At time: 66.85520529747009 and batch: 150, loss is 3.87441508769989 and perplexity is 48.15452387879927
At time: 67.6508059501648 and batch: 200, loss is 3.753618321418762 and perplexity is 42.67521562614991
At time: 68.4454174041748 and batch: 250, loss is 3.9144067573547363 and perplexity is 50.11932976610408
At time: 69.23036360740662 and batch: 300, loss is 3.8753158807754517 and perplexity is 48.197920683302314
At time: 70.01868486404419 and batch: 350, loss is 3.875968060493469 and perplexity is 48.229364642064134
At time: 70.80987691879272 and batch: 400, loss is 3.8061451959609984 and perplexity is 44.97672779838241
At time: 71.60287404060364 and batch: 450, loss is 3.8484138822555543 and perplexity is 46.91858578073899
At time: 72.3948004245758 and batch: 500, loss is 3.7328274059295654 and perplexity is 41.797118674783334
At time: 73.18809342384338 and batch: 550, loss is 3.7918096494674685 and perplexity is 44.33656135702752
At time: 73.98056483268738 and batch: 600, loss is 3.808867974281311 and perplexity is 45.09935632714112
At time: 74.77236557006836 and batch: 650, loss is 3.6579220485687256 and perplexity is 38.780674729558356
At time: 75.56663608551025 and batch: 700, loss is 3.6715506315231323 and perplexity is 39.312818317057186
At time: 76.384681224823 and batch: 750, loss is 3.7706385469436645 and perplexity is 43.407773889492766
At time: 77.1807427406311 and batch: 800, loss is 3.739317660331726 and perplexity is 42.06927483427146
At time: 77.97530555725098 and batch: 850, loss is 3.8003297662734985 and perplexity is 44.715927867132265
At time: 78.7698712348938 and batch: 900, loss is 3.7608679151535034 and perplexity is 42.98571774919452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324327442744007 and perplexity of 75.51470781231667
finished 5 epochs...
Completing Train Step...
At time: 80.81944274902344 and batch: 50, loss is 3.8426307010650635 and perplexity is 46.64803018803946
At time: 81.63884830474854 and batch: 100, loss is 3.7266112089157106 and perplexity is 41.53810542323581
At time: 82.43678832054138 and batch: 150, loss is 3.735399022102356 and perplexity is 41.904743146247235
At time: 83.23502135276794 and batch: 200, loss is 3.6169727182388307 and perplexity is 37.22470740031948
At time: 84.0308313369751 and batch: 250, loss is 3.7758712816238402 and perplexity is 43.63551057649588
At time: 84.82652974128723 and batch: 300, loss is 3.7413822412490845 and perplexity is 42.15621997806017
At time: 85.62078309059143 and batch: 350, loss is 3.7395358324050902 and perplexity is 42.078454176488634
At time: 86.41216015815735 and batch: 400, loss is 3.673680610656738 and perplexity is 39.396643040520274
At time: 87.20396304130554 and batch: 450, loss is 3.7166551208496093 and perplexity is 41.12660027743212
At time: 87.99754858016968 and batch: 500, loss is 3.607129487991333 and perplexity is 36.860093467850824
At time: 88.78889107704163 and batch: 550, loss is 3.660604672431946 and perplexity is 38.88484835987943
At time: 89.58342123031616 and batch: 600, loss is 3.682691903114319 and perplexity is 39.753262098901004
At time: 90.37489056587219 and batch: 650, loss is 3.5341897344589235 and perplexity is 34.26723790039244
At time: 91.17075395584106 and batch: 700, loss is 3.5459932565689085 and perplexity is 34.67410852997059
At time: 91.96784925460815 and batch: 750, loss is 3.6453402853012085 and perplexity is 38.29580213315965
At time: 92.76274132728577 and batch: 800, loss is 3.615711817741394 and perplexity is 37.17780032703218
At time: 93.55717587471008 and batch: 850, loss is 3.6746833658218385 and perplexity is 39.436168041454266
At time: 94.35083723068237 and batch: 900, loss is 3.6364362478256225 and perplexity is 37.95632846179398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341516782159674 and perplexity of 76.82397624786685
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 96.38611388206482 and batch: 50, loss is 3.7587659645080564 and perplexity is 42.89545878520223
At time: 97.1956856250763 and batch: 100, loss is 3.6507555866241455 and perplexity is 38.503747977641765
At time: 97.99028491973877 and batch: 150, loss is 3.6630504083633424 and perplexity is 38.980066822844776
At time: 98.78719830513 and batch: 200, loss is 3.5278603172302248 and perplexity is 34.05103120742644
At time: 99.5858588218689 and batch: 250, loss is 3.680193829536438 and perplexity is 39.65407945956055
At time: 100.38407564163208 and batch: 300, loss is 3.6339266395568846 and perplexity is 37.86119237312436
At time: 101.18166327476501 and batch: 350, loss is 3.622236180305481 and perplexity is 37.42115477879371
At time: 101.97794985771179 and batch: 400, loss is 3.549147615432739 and perplexity is 34.78365579642025
At time: 102.77308917045593 and batch: 450, loss is 3.5732404565811158 and perplexity is 35.63186980326528
At time: 103.56737327575684 and batch: 500, loss is 3.457972927093506 and perplexity is 31.752546507830896
At time: 104.36032891273499 and batch: 550, loss is 3.4875248575210573 and perplexity is 32.704898160316816
At time: 105.15463161468506 and batch: 600, loss is 3.5118183755874632 and perplexity is 33.50914462923678
At time: 105.94735741615295 and batch: 650, loss is 3.3481176567077635 and perplexity is 28.449132178288828
At time: 106.74452233314514 and batch: 700, loss is 3.3354086112976074 and perplexity is 28.0898587127524
At time: 107.54295206069946 and batch: 750, loss is 3.4217340898513795 and perplexity is 30.622471122128765
At time: 108.34091281890869 and batch: 800, loss is 3.376663212776184 and perplexity is 29.272930427041956
At time: 109.13732647895813 and batch: 850, loss is 3.412332320213318 and perplexity is 30.335914881571888
At time: 109.93101406097412 and batch: 900, loss is 3.3657172775268553 and perplexity is 28.954258091170818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311604957058005 and perplexity of 74.5600586549338
finished 7 epochs...
Completing Train Step...
At time: 111.98254346847534 and batch: 50, loss is 3.6659264945983887 and perplexity is 39.09233823020858
At time: 112.7753918170929 and batch: 100, loss is 3.546800374984741 and perplexity is 34.70210593860988
At time: 113.57047820091248 and batch: 150, loss is 3.5569926595687864 and perplexity is 35.057608291036
At time: 114.36426782608032 and batch: 200, loss is 3.4270574378967287 and perplexity is 30.78591985518285
At time: 115.16175937652588 and batch: 250, loss is 3.5782271337509157 and perplexity is 35.80999820021002
At time: 115.96080422401428 and batch: 300, loss is 3.5375932359695437 and perplexity is 34.38406519451168
At time: 116.78790879249573 and batch: 350, loss is 3.52836332321167 and perplexity is 34.06816338823164
At time: 117.58598709106445 and batch: 400, loss is 3.4621328783035277 and perplexity is 31.88491067545481
At time: 118.38748693466187 and batch: 450, loss is 3.4920450115203856 and perplexity is 32.85306395035369
At time: 119.1867184638977 and batch: 500, loss is 3.3806933832168578 and perplexity is 29.39114337512157
At time: 119.9858705997467 and batch: 550, loss is 3.4148662948608397 and perplexity is 30.41288279697991
At time: 120.78910779953003 and batch: 600, loss is 3.4455573034286497 and perplexity is 31.36075603431362
At time: 121.58901572227478 and batch: 650, loss is 3.2883910846710207 and perplexity is 26.799710485296615
At time: 122.38829493522644 and batch: 700, loss is 3.278394384384155 and perplexity is 26.533136462088184
At time: 123.18613171577454 and batch: 750, loss is 3.374214472770691 and perplexity is 29.20133632467369
At time: 123.9838318824768 and batch: 800, loss is 3.3355980682373048 and perplexity is 28.095181035580225
At time: 124.77969980239868 and batch: 850, loss is 3.378654079437256 and perplexity is 29.331266979181102
At time: 125.57412147521973 and batch: 900, loss is 3.340043792724609 and perplexity is 28.22036252491289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321752678858091 and perplexity of 75.32052536403977
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 127.59823393821716 and batch: 50, loss is 3.6336106491088866 and perplexity is 37.8492304880049
At time: 128.41820001602173 and batch: 100, loss is 3.5281061697006226 and perplexity is 34.0594037667334
At time: 129.21344637870789 and batch: 150, loss is 3.5450474643707275 and perplexity is 34.64132953214773
At time: 130.0086088180542 and batch: 200, loss is 3.4125718784332277 and perplexity is 30.343182969870647
At time: 130.80715084075928 and batch: 250, loss is 3.557390627861023 and perplexity is 35.07156288409609
At time: 131.60508131980896 and batch: 300, loss is 3.5140287017822267 and perplexity is 33.583292684881954
At time: 132.40190720558167 and batch: 350, loss is 3.503838758468628 and perplexity is 33.242818488607085
At time: 133.19688653945923 and batch: 400, loss is 3.4365757369995116 and perplexity is 31.08034845537537
At time: 133.9927318096161 and batch: 450, loss is 3.4608495092391967 and perplexity is 31.84401681404611
At time: 134.7857849597931 and batch: 500, loss is 3.345296301841736 and perplexity is 28.368980202534516
At time: 135.57868003845215 and batch: 550, loss is 3.3691414833068847 and perplexity is 29.053573370018185
At time: 136.37313675880432 and batch: 600, loss is 3.4029263257980347 and perplexity is 30.05191318864614
At time: 137.18974375724792 and batch: 650, loss is 3.2390128660202024 and perplexity is 25.508528979348412
At time: 137.98658871650696 and batch: 700, loss is 3.2160737037658693 and perplexity is 24.93004502936461
At time: 138.78415656089783 and batch: 750, loss is 3.312529182434082 and perplexity is 27.45447511218012
At time: 139.58213186264038 and batch: 800, loss is 3.2622939348220825 and perplexity is 26.109361673422285
At time: 140.37884283065796 and batch: 850, loss is 3.305366940498352 and perplexity is 27.258542016899877
At time: 141.17314529418945 and batch: 900, loss is 3.2668994569778445 and perplexity is 26.229886243335077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3098817590164815 and perplexity of 74.43168754407267
finished 9 epochs...
Completing Train Step...
At time: 143.19624996185303 and batch: 50, loss is 3.6075483465194704 and perplexity is 36.875535866212125
At time: 144.0065414905548 and batch: 100, loss is 3.4898225831985474 and perplexity is 32.78013144422381
At time: 144.80048418045044 and batch: 150, loss is 3.505004196166992 and perplexity is 33.2815835071929
At time: 145.59693098068237 and batch: 200, loss is 3.3717831134796143 and perplexity is 29.130423626346225
At time: 146.3929524421692 and batch: 250, loss is 3.5179151487350464 and perplexity is 33.714066328211075
At time: 147.18991351127625 and batch: 300, loss is 3.4762737131118775 and perplexity is 32.33899291454165
At time: 147.98654460906982 and batch: 350, loss is 3.46798321723938 and perplexity is 32.07199492939072
At time: 148.78069591522217 and batch: 400, loss is 3.4022249364852906 and perplexity is 30.030842488153983
At time: 149.57472038269043 and batch: 450, loss is 3.430316963195801 and perplexity is 30.886431060223728
At time: 150.368408203125 and batch: 500, loss is 3.31751344203949 and perplexity is 27.591656934378157
At time: 151.16229438781738 and batch: 550, loss is 3.3435649156570433 and perplexity is 28.31990503842798
At time: 151.95638751983643 and batch: 600, loss is 3.3813383531570436 and perplexity is 29.410105893576063
At time: 152.7519211769104 and batch: 650, loss is 3.221587505340576 and perplexity is 25.067884010084533
At time: 153.54990792274475 and batch: 700, loss is 3.201789617538452 and perplexity is 24.576473352405234
At time: 154.34682416915894 and batch: 750, loss is 3.3026458978652955 and perplexity is 27.18447118260807
At time: 155.14615440368652 and batch: 800, loss is 3.2566526460647585 and perplexity is 25.96248589879721
At time: 155.93797087669373 and batch: 850, loss is 3.3034146642684936 and perplexity is 27.205377725836335
At time: 156.7380495071411 and batch: 900, loss is 3.2676259517669677 and perplexity is 26.248949042683915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313026323710402 and perplexity of 74.66611118781883
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 158.77397513389587 and batch: 50, loss is 3.6001414966583254 and perplexity is 36.60341333794185
At time: 159.56885766983032 and batch: 100, loss is 3.487370743751526 and perplexity is 32.69985827354714
At time: 160.3640079498291 and batch: 150, loss is 3.5044923877716063 and perplexity is 33.26455407161797
At time: 161.15915775299072 and batch: 200, loss is 3.371869082450867 and perplexity is 29.132928046547203
At time: 161.95453214645386 and batch: 250, loss is 3.5169244813919067 and perplexity is 33.68068344209304
At time: 162.7516098022461 and batch: 300, loss is 3.47175989151001 and perplexity is 32.1933494213995
At time: 163.55042672157288 and batch: 350, loss is 3.4647657537460326 and perplexity is 31.968970284440122
At time: 164.34807920455933 and batch: 400, loss is 3.398951921463013 and perplexity is 29.932711768820997
At time: 165.145605802536 and batch: 450, loss is 3.4244590997695923 and perplexity is 30.706031459322883
At time: 165.94121289253235 and batch: 500, loss is 3.3098230504989625 and perplexity is 27.380280116319383
At time: 166.73723196983337 and batch: 550, loss is 3.3316983127593995 and perplexity is 27.985830059053793
At time: 167.53362917900085 and batch: 600, loss is 3.3724786615371705 and perplexity is 29.15069228401091
At time: 168.32995057106018 and batch: 650, loss is 3.2066788482666015 and perplexity is 24.696927625525486
At time: 169.12541007995605 and batch: 700, loss is 3.1813577938079836 and perplexity is 24.079426261521263
At time: 169.92398262023926 and batch: 750, loss is 3.2796660089492797 and perplexity is 26.566898111725234
At time: 170.72070050239563 and batch: 800, loss is 3.231891980171204 and perplexity is 25.32753085462536
At time: 171.51845741271973 and batch: 850, loss is 3.2787779140472413 and perplexity is 26.543314658671495
At time: 172.31651949882507 and batch: 900, loss is 3.2423164653778076 and perplexity is 25.592938289801307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310209509444563 and perplexity of 74.45608655970359
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 174.34296989440918 and batch: 50, loss is 3.593999562263489 and perplexity is 36.379286565127934
At time: 175.15187191963196 and batch: 100, loss is 3.480219864845276 and perplexity is 32.466859612253614
At time: 175.94367694854736 and batch: 150, loss is 3.4981099462509153 and perplexity is 33.05292108653477
At time: 176.74790978431702 and batch: 200, loss is 3.365711932182312 and perplexity is 28.95410332109897
At time: 177.5397436618805 and batch: 250, loss is 3.5119429349899294 and perplexity is 33.51331876822768
At time: 178.33371877670288 and batch: 300, loss is 3.466052632331848 and perplexity is 32.010136950278905
At time: 179.12881469726562 and batch: 350, loss is 3.457452630996704 and perplexity is 31.736030078908705
At time: 179.9233627319336 and batch: 400, loss is 3.3937435865402223 and perplexity is 29.777217465491255
At time: 180.7169725894928 and batch: 450, loss is 3.4200457620620726 and perplexity is 30.570813972527134
At time: 181.5151653289795 and batch: 500, loss is 3.3062191009521484 and perplexity is 27.2817805685159
At time: 182.313138961792 and batch: 550, loss is 3.3255719327926636 and perplexity is 27.81490234908871
At time: 183.11134696006775 and batch: 600, loss is 3.3687342405319214 and perplexity is 29.041743921068676
At time: 183.90988063812256 and batch: 650, loss is 3.200209140777588 and perplexity is 24.53766148611315
At time: 184.70688462257385 and batch: 700, loss is 3.17481077671051 and perplexity is 23.922292785140904
At time: 185.50530529022217 and batch: 750, loss is 3.2721320295333864 and perplexity is 26.367495738083
At time: 186.30678153038025 and batch: 800, loss is 3.222641067504883 and perplexity is 25.094308501694826
At time: 187.11241841316223 and batch: 850, loss is 3.2705452728271482 and perplexity is 26.325690113842708
At time: 187.91438841819763 and batch: 900, loss is 3.2343378925323485 and perplexity is 25.38955559804142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308102960455908 and perplexity of 74.29940625117443
finished 12 epochs...
Completing Train Step...
At time: 189.93581295013428 and batch: 50, loss is 3.58901123046875 and perplexity is 36.198266482768275
At time: 190.7429323196411 and batch: 100, loss is 3.475523819923401 and perplexity is 32.31475121451172
At time: 191.53431701660156 and batch: 150, loss is 3.4930155181884768 and perplexity is 32.88496354486552
At time: 192.3280746936798 and batch: 200, loss is 3.360203356742859 and perplexity is 28.795045950643377
At time: 193.12057280540466 and batch: 250, loss is 3.506307101249695 and perplexity is 33.32497451254729
At time: 193.91759371757507 and batch: 300, loss is 3.461388101577759 and perplexity is 31.861172377043893
At time: 194.71398901939392 and batch: 350, loss is 3.4527059984207153 and perplexity is 31.58574775435374
At time: 195.5100667476654 and batch: 400, loss is 3.3894242334365843 and perplexity is 29.648876523273525
At time: 196.30473017692566 and batch: 450, loss is 3.4162529850006105 and perplexity is 30.455085295808356
At time: 197.10887384414673 and batch: 500, loss is 3.3024867391586303 and perplexity is 27.18014488162664
At time: 197.8997986316681 and batch: 550, loss is 3.322970066070557 and perplexity is 27.742625748103713
At time: 198.69158911705017 and batch: 600, loss is 3.367079191207886 and perplexity is 28.99371815589103
At time: 199.48383903503418 and batch: 650, loss is 3.1986179161071777 and perplexity is 24.498647601961668
At time: 200.2760078907013 and batch: 700, loss is 3.1738510942459106 and perplexity is 23.899345992820244
At time: 201.07017970085144 and batch: 750, loss is 3.2719138526916502 and perplexity is 26.36174358865409
At time: 201.86454248428345 and batch: 800, loss is 3.223358864784241 and perplexity is 25.112327594318874
At time: 202.6594226360321 and batch: 850, loss is 3.2726742887496947 and perplexity is 26.381797632974653
At time: 203.45507502555847 and batch: 900, loss is 3.237414379119873 and perplexity is 25.4677865017619
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307611334813784 and perplexity of 74.26288773532175
finished 13 epochs...
Completing Train Step...
At time: 205.4901053905487 and batch: 50, loss is 3.5866032600402833 and perplexity is 36.11120698792713
At time: 206.2848629951477 and batch: 100, loss is 3.4727847003936767 and perplexity is 32.22635836292088
At time: 207.07946157455444 and batch: 150, loss is 3.489878468513489 and perplexity is 32.78196342338329
At time: 207.87864017486572 and batch: 200, loss is 3.356994071006775 and perplexity is 28.70278254940188
At time: 208.68009495735168 and batch: 250, loss is 3.5030275535583497 and perplexity is 33.21586268583562
At time: 209.48901104927063 and batch: 300, loss is 3.458380689620972 and perplexity is 31.76549664655963
At time: 210.28793787956238 and batch: 350, loss is 3.4497139739990232 and perplexity is 31.49138366592112
At time: 211.0862157344818 and batch: 400, loss is 3.3865954875946045 and perplexity is 29.565125897552406
At time: 211.89015698432922 and batch: 450, loss is 3.4136425971984865 and perplexity is 30.375689384782834
At time: 212.69143152236938 and batch: 500, loss is 3.3001509618759157 and perplexity is 27.1167322044475
At time: 213.49083423614502 and batch: 550, loss is 3.3210231494903564 and perplexity is 27.6886657149426
At time: 214.29185152053833 and batch: 600, loss is 3.3656500482559206 and perplexity is 28.95231158294079
At time: 215.09173798561096 and batch: 650, loss is 3.19750524520874 and perplexity is 24.47140382921117
At time: 215.89212775230408 and batch: 700, loss is 3.1731297254562376 and perplexity is 23.882111967318828
At time: 216.71458220481873 and batch: 750, loss is 3.271785635948181 and perplexity is 26.358363788417105
At time: 217.52254724502563 and batch: 800, loss is 3.2238512659072875 and perplexity is 25.12469597748161
At time: 218.3223602771759 and batch: 850, loss is 3.2739109945297242 and perplexity is 26.414444337618153
At time: 219.12002730369568 and batch: 900, loss is 3.239063868522644 and perplexity is 25.509830011337616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307502224020762 and perplexity of 74.25478529478875
finished 14 epochs...
Completing Train Step...
At time: 221.14430713653564 and batch: 50, loss is 3.584611577987671 and perplexity is 36.03935652046572
At time: 221.95742964744568 and batch: 100, loss is 3.470498003959656 and perplexity is 32.1527506554864
At time: 222.75546073913574 and batch: 150, loss is 3.4873213148117066 and perplexity is 32.69824199416625
At time: 223.54971885681152 and batch: 200, loss is 3.354407062530518 and perplexity is 28.628624173187777
At time: 224.34374237060547 and batch: 250, loss is 3.500391502380371 and perplexity is 33.12841927516321
At time: 225.1416585445404 and batch: 300, loss is 3.455899133682251 and perplexity is 31.68676651672303
At time: 225.93825316429138 and batch: 350, loss is 3.4472622632980348 and perplexity is 31.414270471846542
At time: 226.73585510253906 and batch: 400, loss is 3.384255471229553 and perplexity is 29.49602390055093
At time: 227.5311496257782 and batch: 450, loss is 3.4114855670928956 and perplexity is 30.310238723202797
At time: 228.32508063316345 and batch: 500, loss is 3.2982458591461183 and perplexity is 27.065121221623567
At time: 229.1259424686432 and batch: 550, loss is 3.3193523836135865 and perplexity is 27.642443061461258
At time: 229.9255232810974 and batch: 600, loss is 3.364389281272888 and perplexity is 28.9158324650803
At time: 230.72410988807678 and batch: 650, loss is 3.1965466117858887 and perplexity is 24.447955964345546
At time: 231.52388644218445 and batch: 700, loss is 3.172476897239685 and perplexity is 23.866526138743563
At time: 232.3230094909668 and batch: 750, loss is 3.2716016006469726 and perplexity is 26.353513365336163
At time: 233.12884140014648 and batch: 800, loss is 3.2241344165802004 and perplexity is 25.131811059326935
At time: 233.92603468894958 and batch: 850, loss is 3.2746906661987305 and perplexity is 26.435046962118378
At time: 234.7231147289276 and batch: 900, loss is 3.240074939727783 and perplexity is 25.535635309208566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307535667941995 and perplexity of 74.25726870750657
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 236.75433993339539 and batch: 50, loss is 3.583726887702942 and perplexity is 36.00748695131191
At time: 237.56362676620483 and batch: 100, loss is 3.4702555036544798 and perplexity is 32.14495454895751
At time: 238.3575987815857 and batch: 150, loss is 3.48754967212677 and perplexity is 32.705709729538874
At time: 239.15160870552063 and batch: 200, loss is 3.354244408607483 and perplexity is 28.6239679938381
At time: 239.9470193386078 and batch: 250, loss is 3.500456328392029 and perplexity is 33.13056692806848
At time: 240.74817609786987 and batch: 300, loss is 3.4555862808227538 and perplexity is 31.67685477174977
At time: 241.54734015464783 and batch: 350, loss is 3.446276650428772 and perplexity is 31.383323416004483
At time: 242.34402799606323 and batch: 400, loss is 3.3835151624679565 and perplexity is 29.474195816382988
At time: 243.14049196243286 and batch: 450, loss is 3.4112688064575196 and perplexity is 30.303669368613214
At time: 243.93370509147644 and batch: 500, loss is 3.297964406013489 and perplexity is 27.05750473036369
At time: 244.72726559638977 and batch: 550, loss is 3.3175238037109374 and perplexity is 27.591942831543186
At time: 245.52126717567444 and batch: 600, loss is 3.362622127532959 and perplexity is 28.864778866661137
At time: 246.3157675266266 and batch: 650, loss is 3.1944866132736207 and perplexity is 24.39764504942753
At time: 247.1099362373352 and batch: 700, loss is 3.1701959419250487 and perplexity is 23.812149697797093
At time: 247.90298223495483 and batch: 750, loss is 3.2693372201919555 and perplexity is 26.293906496534127
At time: 248.6968936920166 and batch: 800, loss is 3.2207025051116944 and perplexity is 25.04570874100449
At time: 249.49339699745178 and batch: 850, loss is 3.27138503074646 and perplexity is 26.34780660554698
At time: 250.29017448425293 and batch: 900, loss is 3.236330757141113 and perplexity is 25.440203995762122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306658601107663 and perplexity of 74.19216867262905
finished 16 epochs...
Completing Train Step...
At time: 252.32949924468994 and batch: 50, loss is 3.5827069759368895 and perplexity is 35.97078121319322
At time: 253.12513709068298 and batch: 100, loss is 3.4692412567138673 and perplexity is 32.11236815526861
At time: 253.92036128044128 and batch: 150, loss is 3.4864585256576537 and perplexity is 32.67004247248714
At time: 254.7155840396881 and batch: 200, loss is 3.353170337677002 and perplexity is 28.59324032670194
At time: 255.51184034347534 and batch: 250, loss is 3.4993144178390505 and perplexity is 33.092756376309815
At time: 256.3066945075989 and batch: 300, loss is 3.4546851778030394 and perplexity is 31.648323518989486
At time: 257.1173493862152 and batch: 350, loss is 3.445361099243164 and perplexity is 31.35460352631334
At time: 257.91405153274536 and batch: 400, loss is 3.382676978111267 and perplexity is 29.44950135722414
At time: 258.7102506160736 and batch: 450, loss is 3.4103583478927613 and perplexity is 30.2760916893651
At time: 259.5051848888397 and batch: 500, loss is 3.297121133804321 and perplexity is 27.0346975062758
At time: 260.29861545562744 and batch: 550, loss is 3.3170411729812623 and perplexity is 27.578629325053353
At time: 261.09704661369324 and batch: 600, loss is 3.3624156427383425 and perplexity is 28.858819344021786
At time: 261.8970687389374 and batch: 650, loss is 3.1942600917816164 and perplexity is 24.392119084368098
At time: 262.69816851615906 and batch: 700, loss is 3.170115008354187 and perplexity is 23.810222573477738
At time: 263.4977135658264 and batch: 750, loss is 3.2692598915100097 and perplexity is 26.291873302014686
At time: 264.3023273944855 and batch: 800, loss is 3.220855040550232 and perplexity is 25.0495293905556
At time: 265.1079967021942 and batch: 850, loss is 3.271930904388428 and perplexity is 26.36219310494479
At time: 265.91442799568176 and batch: 900, loss is 3.2370973014831543 and perplexity is 25.459712516313225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306286955532962 and perplexity of 74.16460060455698
finished 17 epochs...
Completing Train Step...
At time: 267.9749822616577 and batch: 50, loss is 3.581975049972534 and perplexity is 35.94446289716948
At time: 268.7953977584839 and batch: 100, loss is 3.4684932327270506 and perplexity is 32.08835631545069
At time: 269.5895297527313 and batch: 150, loss is 3.4856573390960692 and perplexity is 32.64387815614069
At time: 270.38325238227844 and batch: 200, loss is 3.3523447513580322 and perplexity is 28.569643880465062
At time: 271.17735385894775 and batch: 250, loss is 3.4984458589553835 and perplexity is 33.06402584765827
At time: 271.9732413291931 and batch: 300, loss is 3.4539684772491457 and perplexity is 31.625649274286058
At time: 272.7738995552063 and batch: 350, loss is 3.4446225643157957 and perplexity is 31.331455605301592
At time: 273.57397079467773 and batch: 400, loss is 3.3820002985000612 and perplexity is 29.429580220969044
At time: 274.37091612815857 and batch: 450, loss is 3.4096845149993897 and perplexity is 30.255697534799825
At time: 275.16715002059937 and batch: 500, loss is 3.2964937543869017 and perplexity is 27.01774181288688
At time: 275.9631006717682 and batch: 550, loss is 3.3166279697418215 and perplexity is 27.567236100097734
At time: 276.7574601173401 and batch: 600, loss is 3.362194857597351 and perplexity is 28.852448448849444
At time: 277.56566047668457 and batch: 650, loss is 3.194052538871765 and perplexity is 24.387056954422768
At time: 278.3593454360962 and batch: 700, loss is 3.170018877983093 and perplexity is 23.807933797958103
At time: 279.15368580818176 and batch: 750, loss is 3.269218153953552 and perplexity is 26.290775966368578
At time: 279.95040464401245 and batch: 800, loss is 3.220979690551758 and perplexity is 25.052652009045513
At time: 280.7495713233948 and batch: 850, loss is 3.2723256587982177 and perplexity is 26.37260175121904
At time: 281.54791617393494 and batch: 900, loss is 3.237643485069275 and perplexity is 25.473621991614745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306119317877783 and perplexity of 74.1521688668571
finished 18 epochs...
Completing Train Step...
At time: 283.56945180892944 and batch: 50, loss is 3.581365385055542 and perplexity is 35.922555497945886
At time: 284.3765985965729 and batch: 100, loss is 3.46785560131073 and perplexity is 32.06790229312248
At time: 285.16794323921204 and batch: 150, loss is 3.4849703121185303 and perplexity is 32.62145863348621
At time: 285.96113777160645 and batch: 200, loss is 3.3516355466842653 and perplexity is 28.549389338653995
At time: 286.7525734901428 and batch: 250, loss is 3.4977074670791626 and perplexity is 33.03962065098553
At time: 287.5453097820282 and batch: 300, loss is 3.4533321237564087 and perplexity is 31.60553058388866
At time: 288.3407492637634 and batch: 350, loss is 3.443975763320923 and perplexity is 31.311196941013648
At time: 289.1357274055481 and batch: 400, loss is 3.3814018106460573 and perplexity is 29.411972244264035
At time: 289.93144273757935 and batch: 450, loss is 3.4091127061843873 and perplexity is 30.238402005584668
At time: 290.7242319583893 and batch: 500, loss is 3.2959674119949343 and perplexity is 27.003524971826803
At time: 291.5156891345978 and batch: 550, loss is 3.3162419414520263 and perplexity is 27.556596420832356
At time: 292.30683946609497 and batch: 600, loss is 3.361954064369202 and perplexity is 28.845501811032676
At time: 293.09863901138306 and batch: 650, loss is 3.1938496255874633 and perplexity is 24.38210899862165
At time: 293.89023184776306 and batch: 700, loss is 3.1699055290222167 and perplexity is 23.80523534633772
At time: 294.6828467845917 and batch: 750, loss is 3.2691814661026 and perplexity is 26.28981143199197
At time: 295.4744679927826 and batch: 800, loss is 3.221077389717102 and perplexity is 25.055099751805717
At time: 296.270295381546 and batch: 850, loss is 3.27262396812439 and perplexity is 26.38047011782207
At time: 297.0873556137085 and batch: 900, loss is 3.238049521446228 and perplexity is 25.483967308941548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30604490515304 and perplexity of 74.14665120722071
finished 19 epochs...
Completing Train Step...
At time: 299.12633657455444 and batch: 50, loss is 3.5808190298080445 and perplexity is 35.90293438178403
At time: 299.9186601638794 and batch: 100, loss is 3.4672745990753175 and perplexity is 32.049276181635356
At time: 300.7128052711487 and batch: 150, loss is 3.4843408203125 and perplexity is 32.60093015450822
At time: 301.5052909851074 and batch: 200, loss is 3.3509924936294557 and perplexity is 28.531036468193967
At time: 302.2977843284607 and batch: 250, loss is 3.497042989730835 and perplexity is 33.0176738638446
At time: 303.09126591682434 and batch: 300, loss is 3.4527409410476686 and perplexity is 31.586851462637682
At time: 303.8864469528198 and batch: 350, loss is 3.4433838367462157 and perplexity is 31.29266849573894
At time: 304.68218994140625 and batch: 400, loss is 3.380848970413208 and perplexity is 29.395716616476502
At time: 305.47811698913574 and batch: 450, loss is 3.408593273162842 and perplexity is 30.22269925967964
At time: 306.2718126773834 and batch: 500, loss is 3.2954958963394163 and perplexity is 26.9907953883832
At time: 307.06453108787537 and batch: 550, loss is 3.3158705615997315 and perplexity is 27.54636435623276
At time: 307.8556499481201 and batch: 600, loss is 3.361701102256775 and perplexity is 28.838205914792116
At time: 308.64759969711304 and batch: 650, loss is 3.1936483907699587 and perplexity is 24.377202963016078
At time: 309.44010615348816 and batch: 700, loss is 3.1697803354263305 and perplexity is 23.802255269870933
At time: 310.23176646232605 and batch: 750, loss is 3.269141173362732 and perplexity is 26.28875216479929
At time: 311.0250823497772 and batch: 800, loss is 3.2211533546447755 and perplexity is 25.05700313294041
At time: 311.82158851623535 and batch: 850, loss is 3.272858166694641 and perplexity is 26.38664910973351
At time: 312.6165614128113 and batch: 900, loss is 3.2383636617660523 and perplexity is 25.491974108145754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30602024026113 and perplexity of 74.14482241063688
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe9c35db70>
ELAPSED
837.8179740905762


RESULTS SO FAR:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.19741055555504, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.14482241063688, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.24622883758484704, 'dropout': 0.5513232670420983, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.04148723988553382, 'dropout': 0.0028119281465838197, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1426336765289307 and batch: 50, loss is 7.040993738174438 and perplexity is 1142.5224108212426
At time: 2.0294950008392334 and batch: 100, loss is 6.108737421035767 and perplexity is 449.7704857717962
At time: 2.9043171405792236 and batch: 150, loss is 5.824623727798462 and perplexity is 338.53372827875404
At time: 3.779787063598633 and batch: 200, loss is 5.495904102325439 and perplexity is 243.6917488705811
At time: 4.6575767993927 and batch: 250, loss is 5.438080415725708 and perplexity is 230.00025456471886
At time: 5.537407398223877 and batch: 300, loss is 5.28414176940918 and perplexity is 197.18488071148136
At time: 6.416158676147461 and batch: 350, loss is 5.2030942916870115 and perplexity is 181.8340197703719
At time: 7.295623540878296 and batch: 400, loss is 5.023050813674927 and perplexity is 151.8739368149512
At time: 8.171717405319214 and batch: 450, loss is 4.993937339782715 and perplexity is 147.5161025693134
At time: 9.047563791275024 and batch: 500, loss is 4.902770948410034 and perplexity is 134.6624057618316
At time: 9.92373275756836 and batch: 550, loss is 4.9492036151885985 and perplexity is 141.06257908118937
At time: 10.80060338973999 and batch: 600, loss is 4.848490543365479 and perplexity is 127.54771671952714
At time: 11.676304578781128 and batch: 650, loss is 4.71515438079834 and perplexity is 111.62604276115766
At time: 12.55605697631836 and batch: 700, loss is 4.770539293289184 and perplexity is 117.98285216726285
At time: 13.436463832855225 and batch: 750, loss is 4.767034740447998 and perplexity is 117.57009870809212
At time: 14.316215515136719 and batch: 800, loss is 4.712295646667481 and perplexity is 111.30738927263796
At time: 15.195542812347412 and batch: 850, loss is 4.744377603530884 and perplexity is 114.93624732213306
At time: 16.071709394454956 and batch: 900, loss is 4.666381683349609 and perplexity is 106.3123738277119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.744952894236944 and perplexity of 115.00238810027017
finished 1 epochs...
Completing Train Step...
At time: 18.167747735977173 and batch: 50, loss is 4.71838755607605 and perplexity is 111.98753338934164
At time: 18.9635648727417 and batch: 100, loss is 4.600768833160401 and perplexity is 99.56083189308559
At time: 19.75820302963257 and batch: 150, loss is 4.599703798294067 and perplexity is 99.45485258164688
At time: 20.55627942085266 and batch: 200, loss is 4.480659971237182 and perplexity is 88.29292424180436
At time: 21.354708194732666 and batch: 250, loss is 4.599492740631104 and perplexity is 99.43386408785976
At time: 22.152859449386597 and batch: 300, loss is 4.541571025848389 and perplexity is 93.83810646896171
At time: 22.94922161102295 and batch: 350, loss is 4.528225870132446 and perplexity is 92.59414124262511
At time: 23.743995189666748 and batch: 400, loss is 4.417125134468079 and perplexity is 82.8577377721497
At time: 24.55403423309326 and batch: 450, loss is 4.451311273574829 and perplexity is 85.73929799847838
At time: 25.34964609146118 and batch: 500, loss is 4.336916599273682 and perplexity is 76.47138352439137
At time: 26.14505386352539 and batch: 550, loss is 4.408617906570434 and perplexity is 82.15583795788625
At time: 26.93901491165161 and batch: 600, loss is 4.37950719833374 and perplexity is 79.79869878207148
At time: 27.73721432685852 and batch: 650, loss is 4.2331554269790646 and perplexity is 68.93440684138145
At time: 28.544081211090088 and batch: 700, loss is 4.261331357955933 and perplexity is 70.90431967281107
At time: 29.351267099380493 and batch: 750, loss is 4.3307935333251955 and perplexity is 76.00457480863813
At time: 30.149629592895508 and batch: 800, loss is 4.286715607643128 and perplexity is 72.72721111083128
At time: 30.94815945625305 and batch: 850, loss is 4.347542490959167 and perplexity is 77.28829267042123
At time: 31.742645025253296 and batch: 900, loss is 4.287219996452332 and perplexity is 72.76390315499052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.488834746896404 and perplexity of 89.01765731896604
finished 2 epochs...
Completing Train Step...
At time: 33.76926565170288 and batch: 50, loss is 4.368655567169189 and perplexity is 78.9374342497061
At time: 34.56939172744751 and batch: 100, loss is 4.254756393432618 and perplexity is 70.43965553547913
At time: 35.35582947731018 and batch: 150, loss is 4.256317548751831 and perplexity is 70.54970866106125
At time: 36.151073932647705 and batch: 200, loss is 4.141192197799683 and perplexity is 62.87773948443201
At time: 36.94717788696289 and batch: 250, loss is 4.2829266548156735 and perplexity is 72.45217252182127
At time: 37.74781799316406 and batch: 300, loss is 4.236123504638672 and perplexity is 69.13931345324735
At time: 38.548988342285156 and batch: 350, loss is 4.234134268760681 and perplexity is 69.00191575387065
At time: 39.35012984275818 and batch: 400, loss is 4.143252882957459 and perplexity is 63.00744430344134
At time: 40.149922609329224 and batch: 450, loss is 4.184642071723938 and perplexity is 65.66999156278834
At time: 40.95228862762451 and batch: 500, loss is 4.0593668603897095 and perplexity is 57.93761686374225
At time: 41.75403714179993 and batch: 550, loss is 4.1348721790313725 and perplexity is 62.48160410086773
At time: 42.57394194602966 and batch: 600, loss is 4.1288785552978515 and perplexity is 62.10823291654961
At time: 43.37412190437317 and batch: 650, loss is 3.9766052770614624 and perplexity is 53.335666744149975
At time: 44.174238443374634 and batch: 700, loss is 3.99029296875 and perplexity is 54.07072807670549
At time: 44.973660707473755 and batch: 750, loss is 4.082659358978272 and perplexity is 59.302968215676685
At time: 45.77349233627319 and batch: 800, loss is 4.049451112747192 and perplexity is 57.365960957621695
At time: 46.573365926742554 and batch: 850, loss is 4.111398177146912 and perplexity is 61.031991477430076
At time: 47.37012696266174 and batch: 900, loss is 4.0600648736953735 and perplexity is 57.97807220875359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393111555543665 and perplexity of 80.89172689971485
finished 3 epochs...
Completing Train Step...
At time: 49.386558532714844 and batch: 50, loss is 4.1519272518157955 and perplexity is 63.55637147253817
At time: 50.19715738296509 and batch: 100, loss is 4.037280216217041 and perplexity is 56.671997428033016
At time: 50.99187135696411 and batch: 150, loss is 4.040308446884155 and perplexity is 56.84387341733761
At time: 51.78556799888611 and batch: 200, loss is 3.926747159957886 and perplexity is 50.74165444459178
At time: 52.579021692276 and batch: 250, loss is 4.074039144515991 and perplexity is 58.79396093929335
At time: 53.37900519371033 and batch: 300, loss is 4.036297550201416 and perplexity is 56.61633513532374
At time: 54.17323613166809 and batch: 350, loss is 4.0332411193847655 and perplexity is 56.4435554030451
At time: 54.96761989593506 and batch: 400, loss is 3.9549790382385255 and perplexity is 52.194599842212405
At time: 55.76270866394043 and batch: 450, loss is 3.9978466796875 and perplexity is 54.48070921687955
At time: 56.56371188163757 and batch: 500, loss is 3.870479588508606 and perplexity is 47.965384212637815
At time: 57.36577224731445 and batch: 550, loss is 3.945068984031677 and perplexity is 51.67990307721374
At time: 58.16450238227844 and batch: 600, loss is 3.9504114389419556 and perplexity is 51.95673946367643
At time: 58.96700143814087 and batch: 650, loss is 3.798662419319153 and perplexity is 44.64143302262657
At time: 59.76855945587158 and batch: 700, loss is 3.806907687187195 and perplexity is 45.01103523660985
At time: 60.569117069244385 and batch: 750, loss is 3.9045312929153444 and perplexity is 49.62681402083857
At time: 61.36579418182373 and batch: 800, loss is 3.8769595050811767 and perplexity is 48.277205096268
At time: 62.16140580177307 and batch: 850, loss is 3.9403017234802244 and perplexity is 51.434117840594105
At time: 62.97788405418396 and batch: 900, loss is 3.89168399810791 and perplexity is 48.993321754999634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36222902062821 and perplexity of 78.43176574612352
finished 4 epochs...
Completing Train Step...
At time: 65.01103734970093 and batch: 50, loss is 3.9903956699371337 and perplexity is 54.07628148983433
At time: 65.80698299407959 and batch: 100, loss is 3.8741779708862305 and perplexity is 48.14310698515605
At time: 66.60246109962463 and batch: 150, loss is 3.878738145828247 and perplexity is 48.36314930968938
At time: 67.39597201347351 and batch: 200, loss is 3.7680361223220826 and perplexity is 43.29495529457234
At time: 68.19251561164856 and batch: 250, loss is 3.9121690082550047 and perplexity is 50.0073006742895
At time: 68.98762512207031 and batch: 300, loss is 3.8805700969696044 and perplexity is 48.45182944029894
At time: 69.78236126899719 and batch: 350, loss is 3.8773025465011597 and perplexity is 48.293769018150236
At time: 70.57694840431213 and batch: 400, loss is 3.8068624353408813 and perplexity is 45.00899845024543
At time: 71.37491750717163 and batch: 450, loss is 3.8481535053253175 and perplexity is 46.906370853713966
At time: 72.17242622375488 and batch: 500, loss is 3.729574971199036 and perplexity is 41.66139710678242
At time: 72.96895813941956 and batch: 550, loss is 3.797027111053467 and perplexity is 44.56849017648772
At time: 73.76652765274048 and batch: 600, loss is 3.8043350887298586 and perplexity is 44.89538873658239
At time: 74.56266689300537 and batch: 650, loss is 3.6554160213470457 and perplexity is 38.68361097601337
At time: 75.35941457748413 and batch: 700, loss is 3.6633678579330446 and perplexity is 38.9924429925856
At time: 76.15589952468872 and batch: 750, loss is 3.7658877038955687 and perplexity is 43.20203946162551
At time: 76.95053887367249 and batch: 800, loss is 3.7382164669036864 and perplexity is 42.02297392310992
At time: 77.74564409255981 and batch: 850, loss is 3.8016651105880737 and perplexity is 44.775678912411166
At time: 78.54271531105042 and batch: 900, loss is 3.7559907388687135 and perplexity is 42.7765792432462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3625509183700775 and perplexity of 78.45701681832136
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 80.55992412567139 and batch: 50, loss is 3.8808728742599485 and perplexity is 48.46650177504188
At time: 81.37412810325623 and batch: 100, loss is 3.766778621673584 and perplexity is 43.24054607719558
At time: 82.18823957443237 and batch: 150, loss is 3.768612852096558 and perplexity is 43.31993198608382
At time: 82.99302530288696 and batch: 200, loss is 3.6452304220199583 and perplexity is 38.291595061784754
At time: 83.78514218330383 and batch: 250, loss is 3.783651490211487 and perplexity is 43.976328046929495
At time: 84.57700324058533 and batch: 300, loss is 3.7355554723739623 and perplexity is 41.91129966756545
At time: 85.36891031265259 and batch: 350, loss is 3.7304743051528932 and perplexity is 41.698881468718945
At time: 86.18759489059448 and batch: 400, loss is 3.6524862575531007 and perplexity is 38.57044299183638
At time: 86.97987365722656 and batch: 450, loss is 3.6811643695831298 and perplexity is 39.69258401377424
At time: 87.77066731452942 and batch: 500, loss is 3.557292070388794 and perplexity is 35.068106489840645
At time: 88.56325697898865 and batch: 550, loss is 3.6052347898483275 and perplexity is 36.79032083711616
At time: 89.35575866699219 and batch: 600, loss is 3.6075403881073 and perplexity is 36.875242396666465
At time: 90.14795899391174 and batch: 650, loss is 3.4473475933074953 and perplexity is 31.41695116621331
At time: 90.94008660316467 and batch: 700, loss is 3.439022068977356 and perplexity is 31.156474382364177
At time: 91.73178696632385 and batch: 750, loss is 3.529759306907654 and perplexity is 34.115755199841466
At time: 92.5226719379425 and batch: 800, loss is 3.4877143621444704 and perplexity is 32.71109647701274
At time: 93.31667494773865 and batch: 850, loss is 3.52704430103302 and perplexity is 34.02325634833833
At time: 94.1094319820404 and batch: 900, loss is 3.4788766956329344 and perplexity is 32.423280399681126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.298879545028895 and perplexity of 73.61726264417561
finished 6 epochs...
Completing Train Step...
At time: 96.12505912780762 and batch: 50, loss is 3.7755611085891725 and perplexity is 43.62197811657167
At time: 96.933753490448 and batch: 100, loss is 3.6585819053649904 and perplexity is 38.80627286596572
At time: 97.72921633720398 and batch: 150, loss is 3.6616535234451293 and perplexity is 38.92565416834511
At time: 98.5251350402832 and batch: 200, loss is 3.54329035282135 and perplexity is 34.580514297163845
At time: 99.32097625732422 and batch: 250, loss is 3.6836431884765624 and perplexity is 39.791096788176965
At time: 100.11514639854431 and batch: 300, loss is 3.640795736312866 and perplexity is 38.12215984610927
At time: 100.90951895713806 and batch: 350, loss is 3.64029598236084 and perplexity is 38.10311290585466
At time: 101.70402359962463 and batch: 400, loss is 3.568459725379944 and perplexity is 35.46192995358159
At time: 102.49956250190735 and batch: 450, loss is 3.6017105054855345 and perplexity is 36.66088949508284
At time: 103.31620764732361 and batch: 500, loss is 3.480751280784607 and perplexity is 32.484117604131434
At time: 104.11113429069519 and batch: 550, loss is 3.530659990310669 and perplexity is 34.146496536358896
At time: 104.90546035766602 and batch: 600, loss is 3.539885244369507 and perplexity is 34.46296414475073
At time: 105.70030188560486 and batch: 650, loss is 3.385974087715149 and perplexity is 29.546759838807912
At time: 106.49528741836548 and batch: 700, loss is 3.382221179008484 and perplexity is 29.436081359571887
At time: 107.28992104530334 and batch: 750, loss is 3.4798585605621337 and perplexity is 32.455131315685016
At time: 108.08443522453308 and batch: 800, loss is 3.4439738321304323 and perplexity is 31.311136473186256
At time: 108.88082551956177 and batch: 850, loss is 3.4895151042938233 and perplexity is 32.770053794722344
At time: 109.6757607460022 and batch: 900, loss is 3.4508016538619994 and perplexity is 31.52565484444848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305232635916096 and perplexity of 74.0864486170887
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 111.71969890594482 and batch: 50, loss is 3.736319499015808 and perplexity is 41.94333325280387
At time: 112.5164566040039 and batch: 100, loss is 3.628164710998535 and perplexity is 37.643666174563236
At time: 113.3124475479126 and batch: 150, loss is 3.636100001335144 and perplexity is 37.943567925020325
At time: 114.10721015930176 and batch: 200, loss is 3.51537109375 and perplexity is 33.62840489959644
At time: 114.9022171497345 and batch: 250, loss is 3.6510035943984986 and perplexity is 38.51329839072135
At time: 115.69914388656616 and batch: 300, loss is 3.604493064880371 and perplexity is 36.763042655277026
At time: 116.49481344223022 and batch: 350, loss is 3.601802639961243 and perplexity is 36.66426738252303
At time: 117.2893271446228 and batch: 400, loss is 3.532758369445801 and perplexity is 34.21822406168073
At time: 118.08458304405212 and batch: 450, loss is 3.5580227518081666 and perplexity is 35.093739467295094
At time: 118.88378953933716 and batch: 500, loss is 3.4320005321502687 and perplexity is 30.938474293564372
At time: 119.6799054145813 and batch: 550, loss is 3.475040068626404 and perplexity is 32.299122692163316
At time: 120.47560691833496 and batch: 600, loss is 3.4893375730514524 and perplexity is 32.76423660274155
At time: 121.2775330543518 and batch: 650, loss is 3.32657103061676 and perplexity is 27.84270604449614
At time: 122.0779447555542 and batch: 700, loss is 3.314847259521484 and perplexity is 27.51819052196896
At time: 122.90651345252991 and batch: 750, loss is 3.409760928153992 and perplexity is 30.258009556426455
At time: 123.70529747009277 and batch: 800, loss is 3.3669660902023315 and perplexity is 28.990439122647313
At time: 124.50457906723022 and batch: 850, loss is 3.4096967363357544 and perplexity is 30.256067302115866
At time: 125.30569267272949 and batch: 900, loss is 3.3692883920669554 and perplexity is 29.05784190799273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2965635534835185 and perplexity of 73.44696296872353
finished 8 epochs...
Completing Train Step...
At time: 127.39182496070862 and batch: 50, loss is 3.7067296743392943 and perplexity is 40.720419503551824
At time: 128.2113013267517 and batch: 100, loss is 3.5899895334243777 and perplexity is 36.23369668176538
At time: 129.0114312171936 and batch: 150, loss is 3.59909556388855 and perplexity is 36.56514864308358
At time: 129.8119022846222 and batch: 200, loss is 3.4779860305786134 and perplexity is 32.39441497350407
At time: 130.60995745658875 and batch: 250, loss is 3.6148008728027343 and perplexity is 37.14394881876473
At time: 131.40960669517517 and batch: 300, loss is 3.5705222272872925 and perplexity is 35.53514572963352
At time: 132.20623326301575 and batch: 350, loss is 3.5692057514190676 and perplexity is 35.48839534743356
At time: 133.00091218948364 and batch: 400, loss is 3.502364354133606 and perplexity is 33.193841247919636
At time: 133.79518461227417 and batch: 450, loss is 3.529984545707703 and perplexity is 34.123440257059485
At time: 134.58921432495117 and batch: 500, loss is 3.4069937753677366 and perplexity is 30.17439675902137
At time: 135.38423466682434 and batch: 550, loss is 3.4518783950805663 and perplexity is 31.55961809799798
At time: 136.17793035507202 and batch: 600, loss is 3.4688692474365235 and perplexity is 32.100424278151664
At time: 136.971515417099 and batch: 650, loss is 3.309298539161682 and perplexity is 27.36592261464582
At time: 137.76584100723267 and batch: 700, loss is 3.3002962827682496 and perplexity is 27.12067311851027
At time: 138.55963277816772 and batch: 750, loss is 3.399098825454712 and perplexity is 29.937109326663712
At time: 139.35358357429504 and batch: 800, loss is 3.3598907661437987 and perplexity is 28.78604629665659
At time: 140.1477861404419 and batch: 850, loss is 3.40611300945282 and perplexity is 30.147831879290454
At time: 140.9414176940918 and batch: 900, loss is 3.3683946561813354 and perplexity is 29.03188347364093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2987737786279965 and perplexity of 73.60947682300811
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 142.97847437858582 and batch: 50, loss is 3.6974979639053345 and perplexity is 40.34623024289982
At time: 143.79163670539856 and batch: 100, loss is 3.5851395082473756 and perplexity is 36.058387810468076
At time: 144.58859848976135 and batch: 150, loss is 3.599380302429199 and perplexity is 36.5755616325662
At time: 145.3861756324768 and batch: 200, loss is 3.4759275102615357 and perplexity is 32.32779900081168
At time: 146.18135833740234 and batch: 250, loss is 3.6100870323181153 and perplexity is 36.9692701960966
At time: 146.97488570213318 and batch: 300, loss is 3.563376202583313 and perplexity is 35.28211585601465
At time: 147.77212190628052 and batch: 350, loss is 3.5608446311950686 and perplexity is 35.19290962461084
At time: 148.56688046455383 and batch: 400, loss is 3.4947623777389527 and perplexity is 32.942459161263486
At time: 149.36601209640503 and batch: 450, loss is 3.5198089599609377 and perplexity is 33.77797490178013
At time: 150.167298078537 and batch: 500, loss is 3.396632127761841 and perplexity is 29.86335453094597
At time: 150.96854043006897 and batch: 550, loss is 3.4365136432647705 and perplexity is 31.07841862037867
At time: 151.7692472934723 and batch: 600, loss is 3.458318066596985 and perplexity is 31.763507457386346
At time: 152.5674228668213 and batch: 650, loss is 3.2906032276153563 and perplexity is 26.859060897351554
At time: 153.3636748790741 and batch: 700, loss is 3.278025913238525 and perplexity is 26.523361567892668
At time: 154.15909218788147 and batch: 750, loss is 3.375194048881531 and perplexity is 29.22995527107156
At time: 154.95501017570496 and batch: 800, loss is 3.338158941268921 and perplexity is 28.16722143079477
At time: 155.75030636787415 and batch: 850, loss is 3.380240716934204 and perplexity is 29.377842006274737
At time: 156.545490026474 and batch: 900, loss is 3.342312150001526 and perplexity is 28.284449047689183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.295097455586473 and perplexity of 73.33936142724114
finished 10 epochs...
Completing Train Step...
At time: 158.63170194625854 and batch: 50, loss is 3.6858190584182737 and perplexity is 39.87777130166434
At time: 159.4289848804474 and batch: 100, loss is 3.571537618637085 and perplexity is 35.57124613413855
At time: 160.2240948677063 and batch: 150, loss is 3.5864711141586305 and perplexity is 36.10643535592487
At time: 161.0184211730957 and batch: 200, loss is 3.463795018196106 and perplexity is 31.937951926237766
At time: 161.81331133842468 and batch: 250, loss is 3.598257312774658 and perplexity is 36.53451070943892
At time: 162.6094615459442 and batch: 300, loss is 3.5520020389556883 and perplexity is 34.88308492064509
At time: 163.418062210083 and batch: 350, loss is 3.549689221382141 and perplexity is 34.80249993392939
At time: 164.213308095932 and batch: 400, loss is 3.4846656465530397 and perplexity is 32.61152151217072
At time: 165.00870013237 and batch: 450, loss is 3.510603518486023 and perplexity is 33.4684605246112
At time: 165.80416536331177 and batch: 500, loss is 3.38866455078125 and perplexity is 29.62636133930884
At time: 166.5995602607727 and batch: 550, loss is 3.429564776420593 and perplexity is 30.86320743059537
At time: 167.39556431770325 and batch: 600, loss is 3.452728385925293 and perplexity is 31.586454888341635
At time: 168.18981409072876 and batch: 650, loss is 3.2863854503631593 and perplexity is 26.74601393233081
At time: 168.98420310020447 and batch: 700, loss is 3.275346031188965 and perplexity is 26.452377244622486
At time: 169.78011870384216 and batch: 750, loss is 3.3735819816589356 and perplexity is 29.1828725786901
At time: 170.57483983039856 and batch: 800, loss is 3.338033890724182 and perplexity is 28.16369932463631
At time: 171.37279891967773 and batch: 850, loss is 3.3819681215286255 and perplexity is 29.42863328144181
At time: 172.16701340675354 and batch: 900, loss is 3.345272741317749 and perplexity is 28.368311822369694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2950502160477315 and perplexity of 73.33589699146556
finished 11 epochs...
Completing Train Step...
At time: 174.18941020965576 and batch: 50, loss is 3.679203109741211 and perplexity is 39.61481283240264
At time: 174.99783182144165 and batch: 100, loss is 3.5644085216522217 and perplexity is 35.31855706324458
At time: 175.79099583625793 and batch: 150, loss is 3.5791947078704833 and perplexity is 35.844663795752346
At time: 176.5847749710083 and batch: 200, loss is 3.456540055274963 and perplexity is 31.70708175913016
At time: 177.38365507125854 and batch: 250, loss is 3.5908574438095093 and perplexity is 36.26515793420616
At time: 178.17782497406006 and batch: 300, loss is 3.5448303699493406 and perplexity is 34.63380990902053
At time: 178.97401332855225 and batch: 350, loss is 3.542735209465027 and perplexity is 34.561322481990764
At time: 179.76955103874207 and batch: 400, loss is 3.478159284591675 and perplexity is 32.40002792211791
At time: 180.56414651870728 and batch: 450, loss is 3.5044735622406007 and perplexity is 33.26392785461836
At time: 181.35929536819458 and batch: 500, loss is 3.383332166671753 and perplexity is 29.46880265593001
At time: 182.15308952331543 and batch: 550, loss is 3.424775266647339 and perplexity is 30.715741224289516
At time: 182.94655680656433 and batch: 600, loss is 3.448706531524658 and perplexity is 31.45967388401208
At time: 183.75350308418274 and batch: 650, loss is 3.2830065155029295 and perplexity is 26.655793403965674
At time: 184.54886412620544 and batch: 700, loss is 3.2727566385269165 and perplexity is 26.383970257588786
At time: 185.3446228504181 and batch: 750, loss is 3.37167875289917 and perplexity is 29.127383717054094
At time: 186.13944816589355 and batch: 800, loss is 3.336936550140381 and perplexity is 28.13281110492545
At time: 186.93456602096558 and batch: 850, loss is 3.3816627788543703 and perplexity is 29.419648835593147
At time: 187.72912907600403 and batch: 900, loss is 3.3454173803329468 and perplexity is 28.372415283807662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2956258695419525 and perplexity of 73.3781252100652
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 189.74710297584534 and batch: 50, loss is 3.676514592170715 and perplexity is 39.5084507542753
At time: 190.554518699646 and batch: 100, loss is 3.5652499914169313 and perplexity is 35.34828906868392
At time: 191.34518671035767 and batch: 150, loss is 3.583163089752197 and perplexity is 35.987191725698196
At time: 192.14008498191833 and batch: 200, loss is 3.458911633491516 and perplexity is 31.782366820460318
At time: 192.93135523796082 and batch: 250, loss is 3.5925971555709837 and perplexity is 36.32830376784434
At time: 193.72258496284485 and batch: 300, loss is 3.544263548851013 and perplexity is 34.61418429747942
At time: 194.51395964622498 and batch: 350, loss is 3.5412537622451783 and perplexity is 34.51015961378715
At time: 195.30602860450745 and batch: 400, loss is 3.474973030090332 and perplexity is 32.2969574788388
At time: 196.0972499847412 and batch: 450, loss is 3.500200142860413 and perplexity is 33.122080443270995
At time: 196.8847198486328 and batch: 500, loss is 3.3814860010147094 and perplexity is 29.41444855328929
At time: 197.67372798919678 and batch: 550, loss is 3.419727239608765 and perplexity is 30.561078032501054
At time: 198.46510910987854 and batch: 600, loss is 3.445638699531555 and perplexity is 31.36330878152941
At time: 199.2571620941162 and batch: 650, loss is 3.276899175643921 and perplexity is 26.493493529159785
At time: 200.04784035682678 and batch: 700, loss is 3.2656176137924193 and perplexity is 26.196285182645802
At time: 200.83945846557617 and batch: 750, loss is 3.36242467880249 and perplexity is 28.859080115342774
At time: 201.63011050224304 and batch: 800, loss is 3.3288632011413575 and perplexity is 27.90659947395931
At time: 202.42046451568604 and batch: 850, loss is 3.3721086025238036 and perplexity is 29.139906803340576
At time: 203.22441720962524 and batch: 900, loss is 3.336669421195984 and perplexity is 28.125297020451484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.29305528614619 and perplexity of 73.189742849806
finished 13 epochs...
Completing Train Step...
At time: 205.24967217445374 and batch: 50, loss is 3.6719868993759155 and perplexity is 39.32997297763261
At time: 206.0443935394287 and batch: 100, loss is 3.5603291463851927 and perplexity is 35.17477288929141
At time: 206.83583784103394 and batch: 150, loss is 3.5774852228164673 and perplexity is 35.78344022402736
At time: 207.62681794166565 and batch: 200, loss is 3.4545852613449095 and perplexity is 31.64516148856979
At time: 208.41827917099 and batch: 250, loss is 3.588150157928467 and perplexity is 36.167110565171704
At time: 209.20919680595398 and batch: 300, loss is 3.540995569229126 and perplexity is 34.50125048177936
At time: 210.001544713974 and batch: 350, loss is 3.537910203933716 and perplexity is 34.39496556909985
At time: 210.793771982193 and batch: 400, loss is 3.47267578125 and perplexity is 32.22284848671399
At time: 211.58482003211975 and batch: 450, loss is 3.4980127382278443 and perplexity is 33.049708233579295
At time: 212.3760280609131 and batch: 500, loss is 3.379061322212219 and perplexity is 29.343214358315688
At time: 213.1671211719513 and batch: 550, loss is 3.4180392122268675 and perplexity is 30.50953361239753
At time: 213.9589545726776 and batch: 600, loss is 3.4447120332717898 and perplexity is 31.33425892332747
At time: 214.75514197349548 and batch: 650, loss is 3.2757136821746826 and perplexity is 26.46210427515766
At time: 215.54961943626404 and batch: 700, loss is 3.26490873336792 and perplexity is 26.177721729296927
At time: 216.34182119369507 and batch: 750, loss is 3.3623550033569334 and perplexity is 28.85706941612638
At time: 217.13446378707886 and batch: 800, loss is 3.3291943740844725 and perplexity is 27.915842915143763
At time: 217.92644119262695 and batch: 850, loss is 3.373193063735962 and perplexity is 29.171525043273377
At time: 218.7191607952118 and batch: 900, loss is 3.3383911943435667 and perplexity is 28.17376411432607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2926221873662245 and perplexity of 73.15805132474755
finished 14 epochs...
Completing Train Step...
At time: 220.73605966567993 and batch: 50, loss is 3.6697750520706176 and perplexity is 39.2430772183865
At time: 221.55484008789062 and batch: 100, loss is 3.557956504821777 and perplexity is 35.09141468981985
At time: 222.35080814361572 and batch: 150, loss is 3.5749648427963256 and perplexity is 35.693365914661314
At time: 223.16001844406128 and batch: 200, loss is 3.4521859169006346 and perplexity is 31.56932486164045
At time: 223.95561861991882 and batch: 250, loss is 3.585728611946106 and perplexity is 36.07963619823487
At time: 224.751633644104 and batch: 300, loss is 3.538850622177124 and perplexity is 34.42732643619237
At time: 225.5504014492035 and batch: 350, loss is 3.535844531059265 and perplexity is 34.32399015293975
At time: 226.34624409675598 and batch: 400, loss is 3.470931544303894 and perplexity is 32.166693192181484
At time: 227.1422131061554 and batch: 450, loss is 3.496369776725769 and perplexity is 32.99545341678862
At time: 227.93858337402344 and batch: 500, loss is 3.3775327968597413 and perplexity is 29.298396772379924
At time: 228.7322061061859 and batch: 550, loss is 3.4167813539505003 and perplexity is 30.47118106912387
At time: 229.52856373786926 and batch: 600, loss is 3.4438313674926757 and perplexity is 31.306676061203888
At time: 230.32336044311523 and batch: 650, loss is 3.2748950242996218 and perplexity is 26.44044973014343
At time: 231.11785531044006 and batch: 700, loss is 3.2643961238861086 and perplexity is 26.16430621967875
At time: 231.91147017478943 and batch: 750, loss is 3.3621181058883667 and perplexity is 28.8502340591028
At time: 232.70741891860962 and batch: 800, loss is 3.329219408035278 and perplexity is 27.916541767729484
At time: 233.50392484664917 and batch: 850, loss is 3.3735911989212037 and perplexity is 29.183141566120057
At time: 234.29869604110718 and batch: 900, loss is 3.3390611219406128 and perplexity is 28.192644820065695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.292597522474315 and perplexity of 73.15624691157224
finished 15 epochs...
Completing Train Step...
At time: 236.31303763389587 and batch: 50, loss is 3.6679894876480104 and perplexity is 39.17306869683473
At time: 237.12315320968628 and batch: 100, loss is 3.556067967414856 and perplexity is 35.025205779196774
At time: 237.9171073436737 and batch: 150, loss is 3.573010654449463 and perplexity is 35.623682464399714
At time: 238.71024990081787 and batch: 200, loss is 3.450263113975525 and perplexity is 31.50868159266429
At time: 239.50315952301025 and batch: 250, loss is 3.5837763690948488 and perplexity is 36.00926869596657
At time: 240.29723143577576 and batch: 300, loss is 3.5370339727401734 and perplexity is 34.36484082741401
At time: 241.09087443351746 and batch: 350, loss is 3.5340895318984984 and perplexity is 34.263804407441384
At time: 241.88631319999695 and batch: 400, loss is 3.469363136291504 and perplexity is 32.11628223565467
At time: 242.68081188201904 and batch: 450, loss is 3.4948996543884276 and perplexity is 32.94698170209514
At time: 243.4889976978302 and batch: 500, loss is 3.376223201751709 and perplexity is 29.26005284828145
At time: 244.28453731536865 and batch: 550, loss is 3.4156431674957277 and perplexity is 30.436518913310167
At time: 245.0806782245636 and batch: 600, loss is 3.442962760925293 and perplexity is 31.2794946834448
At time: 245.87700176239014 and batch: 650, loss is 3.274120025634766 and perplexity is 26.419966355223718
At time: 246.67348909378052 and batch: 700, loss is 3.263860101699829 and perplexity is 26.150285329146303
At time: 247.46871638298035 and batch: 750, loss is 3.3617576217651366 and perplexity is 28.839835882074485
At time: 248.26374053955078 and batch: 800, loss is 3.329066858291626 and perplexity is 27.912283431251485
At time: 249.05897784233093 and batch: 850, loss is 3.3736920022964476 and perplexity is 29.18608347356475
At time: 249.8539445400238 and batch: 900, loss is 3.339307870864868 and perplexity is 28.199602183172587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2927049610712755 and perplexity of 73.16410713833761
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 251.91558027267456 and batch: 50, loss is 3.6670498132705687 and perplexity is 39.13627605715224
At time: 252.71464014053345 and batch: 100, loss is 3.556130232810974 and perplexity is 35.027386705406144
At time: 253.5138578414917 and batch: 150, loss is 3.574094820022583 and perplexity is 35.662325378388736
At time: 254.31202626228333 and batch: 200, loss is 3.451045045852661 and perplexity is 31.533328870198293
At time: 255.11129522323608 and batch: 250, loss is 3.5846547746658324 and perplexity is 36.04091333457483
At time: 255.9125406742096 and batch: 300, loss is 3.5373595094680788 and perplexity is 34.37602966634215
At time: 256.7122905254364 and batch: 350, loss is 3.533785734176636 and perplexity is 34.253396722715564
At time: 257.50870513916016 and batch: 400, loss is 3.468729181289673 and perplexity is 32.095928410277544
At time: 258.3028185367584 and batch: 450, loss is 3.493772487640381 and perplexity is 32.90986588166099
At time: 259.095285654068 and batch: 500, loss is 3.375341510772705 and perplexity is 29.234265893373223
At time: 259.8886034488678 and batch: 550, loss is 3.4139886140823363 and perplexity is 30.386201704783765
At time: 260.6824827194214 and batch: 600, loss is 3.4412087392807007 and perplexity is 31.224677861738087
At time: 261.4765615463257 and batch: 650, loss is 3.272428297996521 and perplexity is 26.375308752839814
At time: 262.2695951461792 and batch: 700, loss is 3.2619752883911133 and perplexity is 26.10104334388356
At time: 263.0854847431183 and batch: 750, loss is 3.358902792930603 and perplexity is 28.757620498277543
At time: 263.87910652160645 and batch: 800, loss is 3.325927939414978 and perplexity is 27.824806401373507
At time: 264.67188811302185 and batch: 850, loss is 3.3700558280944826 and perplexity is 29.080150501869543
At time: 265.4629456996918 and batch: 900, loss is 3.3359859800338745 and perplexity is 28.106081601823117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.291655239993578 and perplexity of 73.0873455291319
finished 17 epochs...
Completing Train Step...
At time: 267.52012062072754 and batch: 50, loss is 3.6661155796051026 and perplexity is 39.099730704126266
At time: 268.3334958553314 and batch: 100, loss is 3.5549425506591796 and perplexity is 34.98580999823596
At time: 269.12808299064636 and batch: 150, loss is 3.572830023765564 and perplexity is 35.61724831539302
At time: 269.9221987724304 and batch: 200, loss is 3.449918746948242 and perplexity is 31.497832909722177
At time: 270.7144944667816 and batch: 250, loss is 3.583651328086853 and perplexity is 36.00476634220693
At time: 271.5077986717224 and batch: 300, loss is 3.5364575576782227 and perplexity is 34.34503812339627
At time: 272.302316904068 and batch: 350, loss is 3.5329674530029296 and perplexity is 34.22537927767911
At time: 273.09554862976074 and batch: 400, loss is 3.4680731964111327 and perplexity is 32.07488087076628
At time: 273.88818740844727 and batch: 450, loss is 3.4931733226776123 and perplexity is 32.8901533492144
At time: 274.6829876899719 and batch: 500, loss is 3.3747730588912965 and perplexity is 29.217652342374066
At time: 275.47702264785767 and batch: 550, loss is 3.4135980939865114 and perplexity is 30.374337599118878
At time: 276.2737846374512 and batch: 600, loss is 3.4410982227325437 and perplexity is 31.221227208804134
At time: 277.06879925727844 and batch: 650, loss is 3.2721090269088746 and perplexity is 26.36688922345496
At time: 277.8616805076599 and batch: 700, loss is 3.2617762231826783 and perplexity is 26.09584805136809
At time: 278.6558692455292 and batch: 750, loss is 3.3588524580001833 and perplexity is 28.756173021880343
At time: 279.4480674266815 and batch: 800, loss is 3.3260369205474856 and perplexity is 27.827838945528985
At time: 280.24080634117126 and batch: 850, loss is 3.370515308380127 and perplexity is 29.093515327930636
At time: 281.0348651409149 and batch: 900, loss is 3.336579327583313 and perplexity is 28.12276322497657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2912050012039815 and perplexity of 73.0544461779838
finished 18 epochs...
Completing Train Step...
At time: 283.0552191734314 and batch: 50, loss is 3.6654521226882935 and perplexity is 39.07379832080601
At time: 283.86420798301697 and batch: 100, loss is 3.554149379730225 and perplexity is 34.95807127304888
At time: 284.65861082077026 and batch: 150, loss is 3.5719638776779177 and perplexity is 35.586411931446385
At time: 285.4523675441742 and batch: 200, loss is 3.4491514110565187 and perplexity is 31.473672762678493
At time: 286.24718475341797 and batch: 250, loss is 3.5829135274887083 and perplexity is 35.97821180124627
At time: 287.04109811782837 and batch: 300, loss is 3.5358262395858766 and perplexity is 34.323362322329274
At time: 287.83480072021484 and batch: 350, loss is 3.532377700805664 and perplexity is 34.205200735802414
At time: 288.62887501716614 and batch: 400, loss is 3.467605710029602 and perplexity is 32.059889805102515
At time: 289.421626329422 and batch: 450, loss is 3.492721829414368 and perplexity is 32.875307018311545
At time: 290.2183542251587 and batch: 500, loss is 3.37434645652771 and perplexity is 29.205190681102426
At time: 291.0132989883423 and batch: 550, loss is 3.4132859897613526 and perplexity is 30.364859119231543
At time: 291.8084270954132 and batch: 600, loss is 3.440969762802124 and perplexity is 31.217216789723867
At time: 292.6036596298218 and batch: 650, loss is 3.2718801736831664 and perplexity is 26.36085576621869
At time: 293.3979196548462 and batch: 700, loss is 3.2616342306137085 and perplexity is 26.092142897922184
At time: 294.1916558742523 and batch: 750, loss is 3.3588263177871704 and perplexity is 28.755421339216735
At time: 294.98526334762573 and batch: 800, loss is 3.3260953569412233 and perplexity is 27.82946515159682
At time: 295.7798194885254 and batch: 850, loss is 3.3707871341705324 and perplexity is 29.10142477067689
At time: 296.5740773677826 and batch: 900, loss is 3.3369491052627565 and perplexity is 28.133164318028953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2909917962061215 and perplexity of 73.03887226521978
finished 19 epochs...
Completing Train Step...
At time: 298.60317492485046 and batch: 50, loss is 3.6648947191238403 and perplexity is 39.05202451530743
At time: 299.39576387405396 and batch: 100, loss is 3.55352023601532 and perplexity is 34.936084539355136
At time: 300.18960785865784 and batch: 150, loss is 3.5712790966033934 and perplexity is 35.56205137182255
At time: 300.9825794696808 and batch: 200, loss is 3.4485314655303956 and perplexity is 31.45416684698592
At time: 301.7749352455139 and batch: 250, loss is 3.582296895980835 and perplexity is 35.956033340924144
At time: 302.56729102134705 and batch: 300, loss is 3.53529324054718 and perplexity is 34.30507287776781
At time: 303.37257862091064 and batch: 350, loss is 3.531874208450317 and perplexity is 34.187983013578325
At time: 304.16527676582336 and batch: 400, loss is 3.4671971893310545 and perplexity is 32.04679535138378
At time: 304.9598035812378 and batch: 450, loss is 3.4923218870162964 and perplexity is 32.86216141810197
At time: 305.7522611618042 and batch: 500, loss is 3.3739751863479612 and perplexity is 29.1943496773039
At time: 306.5446524620056 and batch: 550, loss is 3.412996892929077 and perplexity is 30.356082003426998
At time: 307.33931493759155 and batch: 600, loss is 3.4408141660690306 and perplexity is 31.212359870645283
At time: 308.13303542137146 and batch: 650, loss is 3.2716792917251585 and perplexity is 26.355560877739197
At time: 308.9228575229645 and batch: 700, loss is 3.2615026330947874 and perplexity is 26.088709462574222
At time: 309.715106010437 and batch: 750, loss is 3.3587871170043946 and perplexity is 28.754294126285146
At time: 310.50601506233215 and batch: 800, loss is 3.326119089126587 and perplexity is 27.830125613459437
At time: 311.29644989967346 and batch: 850, loss is 3.3709521293640137 and perplexity is 29.106226762029358
At time: 312.08741068840027 and batch: 900, loss is 3.337190661430359 and perplexity is 28.13996087822662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2908914644424225 and perplexity of 73.03154451395608
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe9c35db70>
ELAPSED
1159.740233182907


RESULTS SO FAR:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.19741055555504, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.14482241063688, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.24622883758484704, 'dropout': 0.5513232670420983, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -73.03154451395608, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.04148723988553382, 'dropout': 0.0028119281465838197, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.4783637579685053, 'dropout': 0.28177637219722296, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1336278915405273 and batch: 50, loss is 7.070517292022705 and perplexity is 1176.7566036352846
At time: 2.0197763442993164 and batch: 100, loss is 6.136403245925903 and perplexity is 462.3874823391572
At time: 2.8890089988708496 and batch: 150, loss is 5.890004329681396 and perplexity is 361.40684914540975
At time: 3.760662317276001 and batch: 200, loss is 5.619661979675293 and perplexity is 275.796142775351
At time: 4.630479097366333 and batch: 250, loss is 5.601325435638428 and perplexity is 270.7850778689121
At time: 5.50198769569397 and batch: 300, loss is 5.460661458969116 and perplexity is 235.25298310739186
At time: 6.3728649616241455 and batch: 350, loss is 5.406693181991577 and perplexity is 222.89330008328005
At time: 7.24392032623291 and batch: 400, loss is 5.2397881031036375 and perplexity is 188.63012803190225
At time: 8.116175889968872 and batch: 450, loss is 5.215333957672119 and perplexity is 184.07328340137568
At time: 8.987694501876831 and batch: 500, loss is 5.142853116989135 and perplexity is 171.20353587830533
At time: 9.858792781829834 and batch: 550, loss is 5.182147493362427 and perplexity is 178.06479365640678
At time: 10.728895425796509 and batch: 600, loss is 5.083308916091919 and perplexity is 161.3069249126155
At time: 11.600709676742554 and batch: 650, loss is 4.9641478061676025 and perplexity is 143.18647563368657
At time: 12.471268892288208 and batch: 700, loss is 5.040154523849488 and perplexity is 154.4938861715262
At time: 13.3427574634552 and batch: 750, loss is 5.0256374645233155 and perplexity is 152.2672901770461
At time: 14.228575706481934 and batch: 800, loss is 4.984881896972656 and perplexity is 146.1863089609454
At time: 15.099996566772461 and batch: 850, loss is 5.008698558807373 and perplexity is 149.70977084590928
At time: 15.977182865142822 and batch: 900, loss is 4.927055950164795 and perplexity is 137.97271526870944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.891962338800299 and perplexity of 133.21473016286302
finished 1 epochs...
Completing Train Step...
At time: 18.06256651878357 and batch: 50, loss is 4.842580833435059 and perplexity is 126.79616961142469
At time: 18.85575532913208 and batch: 100, loss is 4.724375915527344 and perplexity is 112.66016696909075
At time: 19.64932155609131 and batch: 150, loss is 4.708328905105591 and perplexity is 110.86673618173745
At time: 20.441457509994507 and batch: 200, loss is 4.591712875366211 and perplexity is 98.66328341608566
At time: 21.23885154724121 and batch: 250, loss is 4.702599534988403 and perplexity is 110.23335578442256
At time: 22.03608250617981 and batch: 300, loss is 4.634561843872071 and perplexity is 102.9827855698583
At time: 22.834731817245483 and batch: 350, loss is 4.632728700637817 and perplexity is 102.79417629996696
At time: 23.63315987586975 and batch: 400, loss is 4.506469297409057 and perplexity is 90.60136665333474
At time: 24.42854928970337 and batch: 450, loss is 4.534066743850708 and perplexity is 93.13655447018573
At time: 25.223693370819092 and batch: 500, loss is 4.425095405578613 and perplexity is 83.52077518982408
At time: 26.01707887649536 and batch: 550, loss is 4.495158290863037 and perplexity is 89.5823479300838
At time: 26.810887813568115 and batch: 600, loss is 4.457728500366211 and perplexity is 86.2912757075008
At time: 27.604145765304565 and batch: 650, loss is 4.3125208568573 and perplexity is 74.62837948043038
At time: 28.398165464401245 and batch: 700, loss is 4.353092136383057 and perplexity is 77.718407679145
At time: 29.19351291656494 and batch: 750, loss is 4.406130819320679 and perplexity is 81.95176310198809
At time: 29.98969602584839 and batch: 800, loss is 4.360588502883911 and perplexity is 78.3032025266552
At time: 30.785703659057617 and batch: 850, loss is 4.418274888992309 and perplexity is 82.95305861830713
At time: 31.582200527191162 and batch: 900, loss is 4.363387947082519 and perplexity is 78.52271508592324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.515604933647261 and perplexity of 91.43286010962069
finished 2 epochs...
Completing Train Step...
At time: 33.596367835998535 and batch: 50, loss is 4.403800945281983 and perplexity is 81.76104807397941
At time: 34.40485167503357 and batch: 100, loss is 4.284105005264283 and perplexity is 72.53759689185306
At time: 35.19826555252075 and batch: 150, loss is 4.278464145660401 and perplexity is 72.12957437257161
At time: 35.99216651916504 and batch: 200, loss is 4.1726969051361085 and perplexity is 64.89021910165161
At time: 36.78660249710083 and batch: 250, loss is 4.317047624588013 and perplexity is 74.96697060395371
At time: 37.582592248916626 and batch: 300, loss is 4.270656795501709 and perplexity is 71.56862613766012
At time: 38.38192558288574 and batch: 350, loss is 4.269583196640014 and perplexity is 71.49183137286796
At time: 39.178122997283936 and batch: 400, loss is 4.176685438156128 and perplexity is 65.14955271976599
At time: 39.97276544570923 and batch: 450, loss is 4.215877313613891 and perplexity is 67.75358094324648
At time: 40.767284870147705 and batch: 500, loss is 4.094556245803833 and perplexity is 60.01270235929697
At time: 41.561923027038574 and batch: 550, loss is 4.164095830917359 and perplexity is 64.3344868841261
At time: 42.35560965538025 and batch: 600, loss is 4.164984459877014 and perplexity is 64.39168178103307
At time: 43.14852857589722 and batch: 650, loss is 4.00795229434967 and perplexity is 55.0340615442643
At time: 43.94374871253967 and batch: 700, loss is 4.033227734565735 and perplexity is 56.442799921326575
At time: 44.74361228942871 and batch: 750, loss is 4.116405615806579 and perplexity is 61.33837188137744
At time: 45.540255546569824 and batch: 800, loss is 4.07817795753479 and perplexity is 59.03780240930197
At time: 46.33518958091736 and batch: 850, loss is 4.1426225805282595 and perplexity is 62.96774307144511
At time: 47.13289833068848 and batch: 900, loss is 4.098424220085144 and perplexity is 60.24527945972637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3920267183486725 and perplexity of 80.80402012799266
finished 3 epochs...
Completing Train Step...
At time: 49.15802335739136 and batch: 50, loss is 4.1597350358963014 and perplexity is 64.0545481952816
At time: 49.96437692642212 and batch: 100, loss is 4.037940840721131 and perplexity is 56.70944870748411
At time: 50.75754761695862 and batch: 150, loss is 4.04172610282898 and perplexity is 56.924515620357944
At time: 51.55106282234192 and batch: 200, loss is 3.93502516746521 and perplexity is 51.1634375944897
At time: 52.345738649368286 and batch: 250, loss is 4.089903502464295 and perplexity is 59.734127229945706
At time: 53.1414589881897 and batch: 300, loss is 4.049640312194824 and perplexity is 57.37681559256082
At time: 53.936689376831055 and batch: 350, loss is 4.047404637336731 and perplexity is 57.2486829733106
At time: 54.74718141555786 and batch: 400, loss is 3.9715474605560304 and perplexity is 53.06658578325034
At time: 55.542033195495605 and batch: 450, loss is 4.0088360357284545 and perplexity is 55.08271891879436
At time: 56.33684730529785 and batch: 500, loss is 3.891605176925659 and perplexity is 48.989460195644845
At time: 57.1306266784668 and batch: 550, loss is 3.953521041870117 and perplexity is 52.118555754664584
At time: 57.924686431884766 and batch: 600, loss is 3.971987509727478 and perplexity is 53.08994282910324
At time: 58.71859550476074 and batch: 650, loss is 3.814694619178772 and perplexity is 45.36290130746321
At time: 59.51410460472107 and batch: 700, loss is 3.8293750143051146 and perplexity is 46.03375880105178
At time: 60.30862331390381 and batch: 750, loss is 3.9217136240005495 and perplexity is 50.48688623279829
At time: 61.104840993881226 and batch: 800, loss is 3.890902948379517 and perplexity is 48.955070474370544
At time: 61.90157699584961 and batch: 850, loss is 3.955623173713684 and perplexity is 52.22823106595179
At time: 62.69834780693054 and batch: 900, loss is 3.9168855428695677 and perplexity is 50.24371893808905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3462461706710185 and perplexity of 77.18816719995682
finished 4 epochs...
Completing Train Step...
At time: 64.7283182144165 and batch: 50, loss is 3.984881725311279 and perplexity is 53.77892841570272
At time: 65.52088904380798 and batch: 100, loss is 3.864086937904358 and perplexity is 47.65973626143083
At time: 66.3143880367279 and batch: 150, loss is 3.8748832178115844 and perplexity is 48.17707173869556
At time: 67.10877513885498 and batch: 200, loss is 3.7670594358444216 and perplexity is 43.2526903403496
At time: 67.9020574092865 and batch: 250, loss is 3.921316885948181 and perplexity is 50.466860136704
At time: 68.69775414466858 and batch: 300, loss is 3.8869539880752564 and perplexity is 48.76213005213986
At time: 69.49440097808838 and batch: 350, loss is 3.8833321857452394 and perplexity is 48.58584268759763
At time: 70.28970956802368 and batch: 400, loss is 3.8157954740524294 and perplexity is 45.412866775769395
At time: 71.08354830741882 and batch: 450, loss is 3.8513543224334716 and perplexity is 47.05675010787952
At time: 71.87673377990723 and batch: 500, loss is 3.7398985481262206 and perplexity is 42.0937194616514
At time: 72.66941690444946 and batch: 550, loss is 3.795739598274231 and perplexity is 44.51114460033706
At time: 73.46264553070068 and batch: 600, loss is 3.821831455230713 and perplexity is 45.68780691689609
At time: 74.27907085418701 and batch: 650, loss is 3.6687161350250244 and perplexity is 39.20154404896989
At time: 75.07336258888245 and batch: 700, loss is 3.680430679321289 and perplexity is 39.663472632090546
At time: 75.86621975898743 and batch: 750, loss is 3.7731647872924805 and perplexity is 43.517570987882
At time: 76.66234016418457 and batch: 800, loss is 3.747703723907471 and perplexity is 42.42355387464348
At time: 77.45891165733337 and batch: 850, loss is 3.810701775550842 and perplexity is 45.18213546107166
At time: 78.25654172897339 and batch: 900, loss is 3.7764882040023804 and perplexity is 43.662438604867155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340959104773116 and perplexity of 76.78114519763207
finished 5 epochs...
Completing Train Step...
At time: 80.2770094871521 and batch: 50, loss is 3.846682267189026 and perplexity is 46.83741115258887
At time: 81.08572030067444 and batch: 100, loss is 3.727870435714722 and perplexity is 41.59044426508511
At time: 81.88012099266052 and batch: 150, loss is 3.7417114782333374 and perplexity is 42.17010164984773
At time: 82.67464828491211 and batch: 200, loss is 3.6366823434829714 and perplexity is 37.96567049886756
At time: 83.46933126449585 and batch: 250, loss is 3.78344810962677 and perplexity is 43.96738502506694
At time: 84.26386284828186 and batch: 300, loss is 3.7555690240859985 and perplexity is 42.75854353065595
At time: 85.06170415878296 and batch: 350, loss is 3.7526128339767455 and perplexity is 42.63232779795211
At time: 85.85860824584961 and batch: 400, loss is 3.6878655529022217 and perplexity is 39.959464504497944
At time: 86.65550947189331 and batch: 450, loss is 3.7251204299926757 and perplexity is 41.47622742583513
At time: 87.4483585357666 and batch: 500, loss is 3.615845561027527 and perplexity is 37.18277294073855
At time: 88.24216723442078 and batch: 550, loss is 3.669285845756531 and perplexity is 39.22388395234332
At time: 89.03553247451782 and batch: 600, loss is 3.701566023826599 and perplexity is 40.51069542542115
At time: 89.8305082321167 and batch: 650, loss is 3.549191427230835 and perplexity is 34.78517976430869
At time: 90.62643551826477 and batch: 700, loss is 3.559672975540161 and perplexity is 35.151699799613084
At time: 91.42150974273682 and batch: 750, loss is 3.6537962627410887 and perplexity is 38.62100358234598
At time: 92.21815776824951 and batch: 800, loss is 3.6301632833480837 and perplexity is 37.71897499491126
At time: 93.01631450653076 and batch: 850, loss is 3.6909272813797 and perplexity is 40.081997019841815
At time: 93.81226873397827 and batch: 900, loss is 3.6602474212646485 and perplexity is 38.870959183522636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354271039570848 and perplexity of 77.81008418595326
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 95.86831378936768 and batch: 50, loss is 3.762307605743408 and perplexity is 43.04764845236661
At time: 96.67997550964355 and batch: 100, loss is 3.6511343193054198 and perplexity is 38.51833336715995
At time: 97.47198987007141 and batch: 150, loss is 3.6734939765930177 and perplexity is 39.38929097102732
At time: 98.26417708396912 and batch: 200, loss is 3.5450448322296144 and perplexity is 34.641238351400055
At time: 99.05535507202148 and batch: 250, loss is 3.6869611597061156 and perplexity is 39.92334177371815
At time: 99.84738397598267 and batch: 300, loss is 3.654525709152222 and perplexity is 38.64918581226709
At time: 100.64094734191895 and batch: 350, loss is 3.640420699119568 and perplexity is 38.107865298939096
At time: 101.43537592887878 and batch: 400, loss is 3.5616707944869996 and perplexity is 35.2219967283732
At time: 102.2285840511322 and batch: 450, loss is 3.5898168516159057 and perplexity is 36.227440321690075
At time: 103.02061057090759 and batch: 500, loss is 3.474222168922424 and perplexity is 32.27271604973918
At time: 103.81135678291321 and batch: 550, loss is 3.502113947868347 and perplexity is 33.18553034269794
At time: 104.60246467590332 and batch: 600, loss is 3.5287833070755004 and perplexity is 34.08247447212683
At time: 105.39296746253967 and batch: 650, loss is 3.3592162895202637 and perplexity is 28.766637327529274
At time: 106.1823194026947 and batch: 700, loss is 3.352540578842163 and perplexity is 28.57523914978456
At time: 106.97420406341553 and batch: 750, loss is 3.4316390800476073 and perplexity is 30.927293537753407
At time: 107.76567101478577 and batch: 800, loss is 3.389285473823547 and perplexity is 29.644762742060117
At time: 108.55966663360596 and batch: 850, loss is 3.4369361495971678 and perplexity is 31.091552223366513
At time: 109.35558247566223 and batch: 900, loss is 3.3913341522216798 and perplexity is 29.70555758038012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3129820105147685 and perplexity of 74.66280256713486
finished 7 epochs...
Completing Train Step...
At time: 111.38870048522949 and batch: 50, loss is 3.6682595920562746 and perplexity is 39.183650944466514
At time: 112.18072628974915 and batch: 100, loss is 3.5484249639511107 and perplexity is 34.75852841628527
At time: 112.97247672080994 and batch: 150, loss is 3.5687954235076904 and perplexity is 35.47383645545666
At time: 113.76444005966187 and batch: 200, loss is 3.4427779626846315 and perplexity is 31.273714821929087
At time: 114.5692846775055 and batch: 250, loss is 3.585453314781189 and perplexity is 36.06970494376409
At time: 115.36017870903015 and batch: 300, loss is 3.557602195739746 and perplexity is 35.07898368523288
At time: 116.15632581710815 and batch: 350, loss is 3.548522448539734 and perplexity is 34.761917002293856
At time: 116.95058631896973 and batch: 400, loss is 3.4759459924697875 and perplexity is 32.328396495446626
At time: 117.74386787414551 and batch: 450, loss is 3.509619436264038 and perplexity is 33.4355410080239
At time: 118.54377746582031 and batch: 500, loss is 3.3971460390090944 and perplexity is 29.87870558892296
At time: 119.34078884124756 and batch: 550, loss is 3.4283384132385253 and perplexity is 30.825381128454882
At time: 120.13295674324036 and batch: 600, loss is 3.461362056732178 and perplexity is 31.86034256853546
At time: 120.92554235458374 and batch: 650, loss is 3.2997974157333374 and perplexity is 27.107146882903862
At time: 121.71747541427612 and batch: 700, loss is 3.2991164779663085 and perplexity is 27.088694885878695
At time: 122.50929164886475 and batch: 750, loss is 3.3850657892227174 and perplexity is 29.519934745830078
At time: 123.30607080459595 and batch: 800, loss is 3.347706823348999 and perplexity is 28.43744672631318
At time: 124.10373520851135 and batch: 850, loss is 3.4033745908737183 and perplexity is 30.06538743157672
At time: 124.90303444862366 and batch: 900, loss is 3.365207328796387 and perplexity is 28.93949666811993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324152280206549 and perplexity of 75.50148162288116
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 126.95933365821838 and batch: 50, loss is 3.635586543083191 and perplexity is 37.924090487814034
At time: 127.77090287208557 and batch: 100, loss is 3.5280568599700928 and perplexity is 34.05772434811784
At time: 128.56581258773804 and batch: 150, loss is 3.553781328201294 and perplexity is 34.94520726892177
At time: 129.3619258403778 and batch: 200, loss is 3.423204822540283 and perplexity is 30.667541726704634
At time: 130.1576704978943 and batch: 250, loss is 3.5611396646499633 and perplexity is 35.203294242154655
At time: 130.9550497531891 and batch: 300, loss is 3.5305057907104493 and perplexity is 34.14123156618267
At time: 131.75370025634766 and batch: 350, loss is 3.5197504425048827 and perplexity is 33.77599835844989
At time: 132.55371928215027 and batch: 400, loss is 3.4481774854660032 and perplexity is 31.44303466938073
At time: 133.3533480167389 and batch: 450, loss is 3.4749958324432373 and perplexity is 32.297693933857424
At time: 134.153000831604 and batch: 500, loss is 3.361585955619812 and perplexity is 28.83488548353689
At time: 134.97120571136475 and batch: 550, loss is 3.3825581550598143 and perplexity is 29.44600228549842
At time: 135.76734614372253 and batch: 600, loss is 3.4160385799407957 and perplexity is 30.44855627137679
At time: 136.56326460838318 and batch: 650, loss is 3.2438371801376342 and perplexity is 25.631887456586277
At time: 137.35929346084595 and batch: 700, loss is 3.2370545482635498 and perplexity is 25.458624054900724
At time: 138.15521264076233 and batch: 750, loss is 3.3197259140014648 and perplexity is 27.65277028258529
At time: 138.95070695877075 and batch: 800, loss is 3.2751567220687865 and perplexity is 26.44737004232916
At time: 139.75017261505127 and batch: 850, loss is 3.3302922821044922 and perplexity is 27.946508774031727
At time: 140.54917097091675 and batch: 900, loss is 3.294314618110657 and perplexity is 26.958930573725308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315223171286387 and perplexity of 74.83032155972691
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 142.57510995864868 and batch: 50, loss is 3.6234984731674196 and perplexity is 37.46842106102572
At time: 143.3846230506897 and batch: 100, loss is 3.5155220890045165 and perplexity is 33.63348301252908
At time: 144.18231177330017 and batch: 150, loss is 3.5334407758712767 and perplexity is 34.241582766807554
At time: 144.97724986076355 and batch: 200, loss is 3.4080779361724853 and perplexity is 30.20712839726799
At time: 145.77257466316223 and batch: 250, loss is 3.543458595275879 and perplexity is 34.5863326972053
At time: 146.56843662261963 and batch: 300, loss is 3.5124585580825807 and perplexity is 33.53060346510744
At time: 147.36541604995728 and batch: 350, loss is 3.5013662338256837 and perplexity is 33.160726329957406
At time: 148.1641399860382 and batch: 400, loss is 3.43285945892334 and perplexity is 30.965059593231462
At time: 148.9613974094391 and batch: 450, loss is 3.458764886856079 and perplexity is 31.777703207256334
At time: 149.75643682479858 and batch: 500, loss is 3.3441936349868775 and perplexity is 28.337715908577177
At time: 150.54961276054382 and batch: 550, loss is 3.3661810255050657 and perplexity is 28.967688683785564
At time: 151.34255456924438 and batch: 600, loss is 3.400916352272034 and perplexity is 29.991570302848157
At time: 152.13604593276978 and batch: 650, loss is 3.2230377340316774 and perplexity is 25.104264548375188
At time: 152.929603099823 and batch: 700, loss is 3.2156665658950807 and perplexity is 24.919897129849968
At time: 153.7233440876007 and batch: 750, loss is 3.2956525659561158 and perplexity is 26.995024357217925
At time: 154.53065729141235 and batch: 800, loss is 3.251844449043274 and perplexity is 25.837952781231582
At time: 155.3265507221222 and batch: 850, loss is 3.307309799194336 and perplexity is 27.311552992039235
At time: 156.12237000465393 and batch: 900, loss is 3.2708890008926392 and perplexity is 26.33474054773321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311217007571703 and perplexity of 74.53113872857797
finished 10 epochs...
Completing Train Step...
At time: 158.16139316558838 and batch: 50, loss is 3.6110289192199705 and perplexity is 37.00410747127655
At time: 158.95475697517395 and batch: 100, loss is 3.499917678833008 and perplexity is 33.1127259682465
At time: 159.74852681159973 and batch: 150, loss is 3.5167342376708985 and perplexity is 33.67427651300726
At time: 160.54198217391968 and batch: 200, loss is 3.391773042678833 and perplexity is 29.71859792755921
At time: 161.33529210090637 and batch: 250, loss is 3.52859911441803 and perplexity is 34.0761973087021
At time: 162.12967491149902 and batch: 300, loss is 3.4976969814300536 and perplexity is 33.03927421093302
At time: 162.92405319213867 and batch: 350, loss is 3.487330675125122 and perplexity is 32.69854806139189
At time: 163.72006607055664 and batch: 400, loss is 3.4199783849716185 and perplexity is 30.568754269418033
At time: 164.5170922279358 and batch: 450, loss is 3.4467415189743043 and perplexity is 31.39791592745185
At time: 165.3133053779602 and batch: 500, loss is 3.333234419822693 and perplexity is 28.028852325223408
At time: 166.10766124725342 and batch: 550, loss is 3.356712832450867 and perplexity is 28.694711355305827
At time: 166.90670824050903 and batch: 600, loss is 3.3930822801589966 and perplexity is 29.75753211130876
At time: 167.70577788352966 and batch: 650, loss is 3.217351016998291 and perplexity is 24.961908851474064
At time: 168.50540375709534 and batch: 700, loss is 3.2117742824554445 and perplexity is 24.82309034886673
At time: 169.304616689682 and batch: 750, loss is 3.2936423778533936 and perplexity is 26.94081378540037
At time: 170.10488414764404 and batch: 800, loss is 3.2526079702377317 and perplexity is 25.85768813902468
At time: 170.90679359436035 and batch: 850, loss is 3.311124005317688 and perplexity is 27.41592380406123
At time: 171.70827507972717 and batch: 900, loss is 3.276162443161011 and perplexity is 26.473982100125387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310982064025043 and perplexity of 74.51363017534979
finished 11 epochs...
Completing Train Step...
At time: 173.73647236824036 and batch: 50, loss is 3.6041456699371337 and perplexity is 36.75027357824538
At time: 174.54449677467346 and batch: 100, loss is 3.4917462682724 and perplexity is 32.84325078520428
At time: 175.33770966529846 and batch: 150, loss is 3.5075684070587156 and perplexity is 33.36703401585735
At time: 176.13230061531067 and batch: 200, loss is 3.3826740646362303 and perplexity is 29.44941555696208
At time: 176.92639183998108 and batch: 250, loss is 3.5194716596603395 and perplexity is 33.76658350196145
At time: 177.72045421600342 and batch: 300, loss is 3.488707408905029 and perplexity is 32.7435962596347
At time: 178.51521968841553 and batch: 350, loss is 3.4786910963058473 and perplexity is 32.4172632190666
At time: 179.31367683410645 and batch: 400, loss is 3.4119363498687743 and perplexity is 30.323905136812492
At time: 180.1103401184082 and batch: 450, loss is 3.439258127212524 and perplexity is 31.163829992863008
At time: 180.90724158287048 and batch: 500, loss is 3.3262470054626463 and perplexity is 27.83368576885615
At time: 181.7059943675995 and batch: 550, loss is 3.3504351139068604 and perplexity is 28.515138278077963
At time: 182.50469636917114 and batch: 600, loss is 3.387732996940613 and perplexity is 29.598775639396347
At time: 183.3045802116394 and batch: 650, loss is 3.2132227993011475 and perplexity is 24.859073067895224
At time: 184.10240054130554 and batch: 700, loss is 3.208691191673279 and perplexity is 24.746676363982584
At time: 184.8955636024475 and batch: 750, loss is 3.2916703605651856 and perplexity is 26.887738384861084
At time: 185.68769097328186 and batch: 800, loss is 3.2522130584716797 and perplexity is 25.847478649792073
At time: 186.48202776908875 and batch: 850, loss is 3.3120893335342405 and perplexity is 27.44240194688518
At time: 187.27875757217407 and batch: 900, loss is 3.2775848436355592 and perplexity is 26.51166549891646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311592833636558 and perplexity of 74.55915473739888
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 189.30611538887024 and batch: 50, loss is 3.6012606811523438 and perplexity is 36.644402243373044
At time: 190.1173927783966 and batch: 100, loss is 3.491676440238953 and perplexity is 32.840957485659175
At time: 190.9144926071167 and batch: 150, loss is 3.506936717033386 and perplexity is 33.34596304915511
At time: 191.71004176139832 and batch: 200, loss is 3.383167905807495 and perplexity is 29.463962482473516
At time: 192.50618481636047 and batch: 250, loss is 3.5194259977340696 and perplexity is 33.765041689916515
At time: 193.30150508880615 and batch: 300, loss is 3.4875491428375245 and perplexity is 32.705692418763036
At time: 194.0961241722107 and batch: 350, loss is 3.476455039978027 and perplexity is 32.344857374457284
At time: 194.9110813140869 and batch: 400, loss is 3.4106555652618407 and perplexity is 30.2850916070827
At time: 195.70923686027527 and batch: 450, loss is 3.436447319984436 and perplexity is 31.076357466060173
At time: 196.50910067558289 and batch: 500, loss is 3.322930359840393 and perplexity is 27.74152421488942
At time: 197.3043818473816 and batch: 550, loss is 3.345939164161682 and perplexity is 28.387223414270448
At time: 198.0987513065338 and batch: 600, loss is 3.383006443977356 and perplexity is 29.459205561208893
At time: 198.89376306533813 and batch: 650, loss is 3.206587915420532 and perplexity is 24.694681965710995
At time: 199.68926191329956 and batch: 700, loss is 3.2009874963760376 and perplexity is 24.556767947149574
At time: 200.4838421344757 and batch: 750, loss is 3.283972225189209 and perplexity is 26.681547595385886
At time: 201.27818417549133 and batch: 800, loss is 3.243145298957825 and perplexity is 25.61415936962414
At time: 202.0726456642151 and batch: 850, loss is 3.301806240081787 and perplexity is 27.161655109976007
At time: 202.87072372436523 and batch: 900, loss is 3.26664843082428 and perplexity is 26.223302682240554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310918520574701 and perplexity of 74.50889547262217
finished 13 epochs...
Completing Train Step...
At time: 204.946679353714 and batch: 50, loss is 3.598184962272644 and perplexity is 36.531867514867635
At time: 205.75033450126648 and batch: 100, loss is 3.4870100688934325 and perplexity is 32.68806638345163
At time: 206.54104852676392 and batch: 150, loss is 3.5028322744369507 and perplexity is 33.20937695463837
At time: 207.33242058753967 and batch: 200, loss is 3.3790855741500856 and perplexity is 29.343925996756376
At time: 208.12409162521362 and batch: 250, loss is 3.515852108001709 and perplexity is 33.64458453262047
At time: 208.91608905792236 and batch: 300, loss is 3.4841340303421022 and perplexity is 32.59418930612056
At time: 209.70715260505676 and batch: 350, loss is 3.472867736816406 and perplexity is 32.22903443554114
At time: 210.49545884132385 and batch: 400, loss is 3.4072725915908815 and perplexity is 30.182811043326367
At time: 211.28765678405762 and batch: 450, loss is 3.4333212089538576 and perplexity is 30.979361012029802
At time: 212.08233833312988 and batch: 500, loss is 3.3201544427871705 and perplexity is 27.664622830053393
At time: 212.8760793209076 and batch: 550, loss is 3.343778510093689 and perplexity is 28.325954658650133
At time: 213.66950178146362 and batch: 600, loss is 3.3814486122131346 and perplexity is 29.413348802868203
At time: 214.47558283805847 and batch: 650, loss is 3.2054570579528807 and perplexity is 24.666771584504705
At time: 215.2707040309906 and batch: 700, loss is 3.2004087495803835 and perplexity is 24.54255990821362
At time: 216.0626745223999 and batch: 750, loss is 3.283630819320679 and perplexity is 26.67243991325189
At time: 216.85471200942993 and batch: 800, loss is 3.2436665534973144 and perplexity is 25.627514346839625
At time: 217.64792442321777 and batch: 850, loss is 3.3031617546081544 and perplexity is 27.19849809299577
At time: 218.43943572044373 and batch: 900, loss is 3.2686267423629762 and perplexity is 26.275231893662284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310741267792166 and perplexity of 74.49568973398763
finished 14 epochs...
Completing Train Step...
At time: 220.46522092819214 and batch: 50, loss is 3.5961305952072142 and perplexity is 36.456894686629106
At time: 221.27657771110535 and batch: 100, loss is 3.484395022392273 and perplexity is 32.60269724061456
At time: 222.06880903244019 and batch: 150, loss is 3.5001591491699218 and perplexity is 33.12072267478705
At time: 222.86047291755676 and batch: 200, loss is 3.3764306259155275 and perplexity is 29.26612271977403
At time: 223.65159487724304 and batch: 250, loss is 3.513350167274475 and perplexity is 33.56051299118946
At time: 224.4437961578369 and batch: 300, loss is 3.481768431663513 and perplexity is 32.51717566256942
At time: 225.23567605018616 and batch: 350, loss is 3.470516200065613 and perplexity is 32.153335715667026
At time: 226.0268018245697 and batch: 400, loss is 3.405039644241333 and perplexity is 30.115489605990145
At time: 226.82024812698364 and batch: 450, loss is 3.4312492561340333 and perplexity is 30.915239688742208
At time: 227.61441373825073 and batch: 500, loss is 3.3182831239700317 and perplexity is 27.612901909042
At time: 228.40984416007996 and batch: 550, loss is 3.3422017383575437 and perplexity is 28.281326287568405
At time: 229.2053461074829 and batch: 600, loss is 3.380277090072632 and perplexity is 29.378910590022496
At time: 229.99816799163818 and batch: 650, loss is 3.204618773460388 and perplexity is 24.646102476912127
At time: 230.7900185585022 and batch: 700, loss is 3.1998369312286377 and perplexity is 24.528530033711426
At time: 231.58089232444763 and batch: 750, loss is 3.283341841697693 and perplexity is 26.664733288541196
At time: 232.37055563926697 and batch: 800, loss is 3.2438430070877073 and perplexity is 25.632036812749913
At time: 233.161785364151 and batch: 850, loss is 3.303865284919739 and perplexity is 27.217639793427477
At time: 233.95362496376038 and batch: 900, loss is 3.2696036672592164 and perplexity is 26.300913364245183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310789343428938 and perplexity of 74.49927124779944
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 235.9861376285553 and batch: 50, loss is 3.595143156051636 and perplexity is 36.42091348887008
At time: 236.7986934185028 and batch: 100, loss is 3.4839170503616335 and perplexity is 32.587117786778336
At time: 237.59675240516663 and batch: 150, loss is 3.499778628349304 and perplexity is 33.10812194778692
At time: 238.3913230895996 and batch: 200, loss is 3.3765771865844725 and perplexity is 29.270412296631218
At time: 239.18881130218506 and batch: 250, loss is 3.5137964582443235 and perplexity is 33.57549408779632
At time: 239.98410487174988 and batch: 300, loss is 3.4819518089294434 and perplexity is 32.523139120102755
At time: 240.77827715873718 and batch: 350, loss is 3.4698936128616333 and perplexity is 32.133323690546995
At time: 241.57269406318665 and batch: 400, loss is 3.404550633430481 and perplexity is 30.100766406194044
At time: 242.3664743900299 and batch: 450, loss is 3.4305075645446776 and perplexity is 30.89231861671608
At time: 243.16412162780762 and batch: 500, loss is 3.3175074434280396 and perplexity is 27.59149142324536
At time: 243.96123361587524 and batch: 550, loss is 3.3401143074035646 and perplexity is 28.222352544878323
At time: 244.75765657424927 and batch: 600, loss is 3.3780578708648683 and perplexity is 29.31378463844769
At time: 245.5538170337677 and batch: 650, loss is 3.2027957010269166 and perplexity is 24.601211778824254
At time: 246.34744453430176 and batch: 700, loss is 3.1975062608718874 and perplexity is 24.471428683926824
At time: 247.14104056358337 and batch: 750, loss is 3.2814086151123045 and perplexity is 26.613234113082324
At time: 247.93465614318848 and batch: 800, loss is 3.240732636451721 and perplexity is 25.552435537016905
At time: 248.72829627990723 and batch: 850, loss is 3.30029317855835 and perplexity is 27.120588930378954
At time: 249.52337551116943 and batch: 900, loss is 3.2657091856002807 and perplexity is 26.19868413367569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31001846104452 and perplexity of 74.441863202211
finished 16 epochs...
Completing Train Step...
At time: 251.56075072288513 and batch: 50, loss is 3.5943828678131102 and perplexity is 36.39323362038069
At time: 252.35668206214905 and batch: 100, loss is 3.4829491901397707 and perplexity is 32.55559326985122
At time: 253.15363121032715 and batch: 150, loss is 3.4987486505508425 and perplexity is 33.074038872650384
At time: 253.9467453956604 and batch: 200, loss is 3.3754464054107665 and perplexity is 29.237332571949352
At time: 254.7525327205658 and batch: 250, loss is 3.51280547618866 and perplexity is 33.542237856531074
At time: 255.54752349853516 and batch: 300, loss is 3.480955581665039 and perplexity is 32.49075481592904
At time: 256.3414092063904 and batch: 350, loss is 3.4690338802337646 and perplexity is 32.105709495841346
At time: 257.14077949523926 and batch: 400, loss is 3.4037958097457888 and perplexity is 30.078054207714306
At time: 257.9411623477936 and batch: 450, loss is 3.429772529602051 and perplexity is 30.86962002622457
At time: 258.74611592292786 and batch: 500, loss is 3.316787314414978 and perplexity is 27.57162914231858
At time: 259.54427313804626 and batch: 550, loss is 3.3397504615783693 and perplexity is 28.212085827592052
At time: 260.3416817188263 and batch: 600, loss is 3.377854061126709 and perplexity is 29.30781081245871
At time: 261.13704562187195 and batch: 650, loss is 3.2025602293014526 and perplexity is 24.595419571015537
At time: 261.9311316013336 and batch: 700, loss is 3.197383375167847 and perplexity is 24.468421679946808
At time: 262.7250623703003 and batch: 750, loss is 3.2812811088562013 and perplexity is 26.609840975565493
At time: 263.5183675289154 and batch: 800, loss is 3.240999221801758 and perplexity is 25.559248350046555
At time: 264.311048746109 and batch: 850, loss is 3.300734968185425 and perplexity is 27.132573172310533
At time: 265.10393714904785 and batch: 900, loss is 3.266215295791626 and perplexity is 26.21194691064578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3096597749892975 and perplexity of 74.41516673206758
finished 17 epochs...
Completing Train Step...
At time: 267.13221979141235 and batch: 50, loss is 3.593783345222473 and perplexity is 36.37142159373362
At time: 267.94509959220886 and batch: 100, loss is 3.4821882581710817 and perplexity is 32.53083010091086
At time: 268.74066185951233 and batch: 150, loss is 3.497955627441406 and perplexity is 33.04782079264765
At time: 269.5359628200531 and batch: 200, loss is 3.3746154499053955 and perplexity is 29.213047740690808
At time: 270.32982325553894 and batch: 250, loss is 3.512037935256958 and perplexity is 33.516502693694164
At time: 271.122878074646 and batch: 300, loss is 3.4802166414260864 and perplexity is 32.466754958124
At time: 271.91755747795105 and batch: 350, loss is 3.4683584547042847 and perplexity is 32.08403180166394
At time: 272.71059489250183 and batch: 400, loss is 3.4031786727905273 and perplexity is 30.059497655476395
At time: 273.5034213066101 and batch: 450, loss is 3.429183406829834 and perplexity is 30.851439385937862
At time: 274.29788756370544 and batch: 500, loss is 3.316236290931702 and perplexity is 27.55644071216412
At time: 275.118852853775 and batch: 550, loss is 3.3393920707702636 and perplexity is 28.201976686973698
At time: 275.91482186317444 and batch: 600, loss is 3.3776290464401244 and perplexity is 29.301216866489423
At time: 276.7107207775116 and batch: 650, loss is 3.202364044189453 and perplexity is 24.590594789162985
At time: 277.50414752960205 and batch: 700, loss is 3.1972793531417847 and perplexity is 24.465876557525807
At time: 278.2986390590668 and batch: 750, loss is 3.2812075090408324 and perplexity is 26.607882568252787
At time: 279.091933965683 and batch: 800, loss is 3.2411608266830445 and perplexity is 25.563379183114346
At time: 279.8873414993286 and batch: 850, loss is 3.3010441064834595 and perplexity is 27.140962186418278
At time: 280.68110513687134 and batch: 900, loss is 3.2665919923782347 and perplexity is 26.221822721550733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309487956843964 and perplexity of 74.40238195449481
finished 18 epochs...
Completing Train Step...
At time: 282.7042520046234 and batch: 50, loss is 3.5932475090026856 and perplexity is 36.35193768923559
At time: 283.51595306396484 and batch: 100, loss is 3.481525673866272 and perplexity is 32.50928282269605
At time: 284.3124108314514 and batch: 150, loss is 3.4972710609436035 and perplexity is 33.02520510356236
At time: 285.11100721359253 and batch: 200, loss is 3.373918209075928 and perplexity is 29.192686310288007
At time: 285.91140699386597 and batch: 250, loss is 3.5113759803771973 and perplexity is 33.494323622765556
At time: 286.7112879753113 and batch: 300, loss is 3.479587993621826 and perplexity is 32.4463512179633
At time: 287.51005029678345 and batch: 350, loss is 3.4677656364440916 and perplexity is 32.06501743833905
At time: 288.30886125564575 and batch: 400, loss is 3.402626872062683 and perplexity is 30.04291537826854
At time: 289.1092748641968 and batch: 450, loss is 3.4286638784408567 and perplexity is 30.835415350167224
At time: 289.91136360168457 and batch: 500, loss is 3.315762228965759 and perplexity is 27.54338034766136
At time: 290.70888924598694 and batch: 550, loss is 3.3390370416641235 and perplexity is 28.19196594155729
At time: 291.5053198337555 and batch: 600, loss is 3.377388582229614 and perplexity is 29.2941718195834
At time: 292.3023009300232 and batch: 650, loss is 3.202177700996399 and perplexity is 24.586012926123622
At time: 293.0978581905365 and batch: 700, loss is 3.197169556617737 and perplexity is 24.463190436788064
At time: 293.89175057411194 and batch: 750, loss is 3.281144609451294 and perplexity is 26.60620899599482
At time: 294.69899892807007 and batch: 800, loss is 3.2412610721588133 and perplexity is 25.565941924672295
At time: 295.49355912208557 and batch: 850, loss is 3.3012711572647095 and perplexity is 27.14712526272575
At time: 296.28793120384216 and batch: 900, loss is 3.2668817329406736 and perplexity is 26.229421347976235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309408527531036 and perplexity of 74.39647245911266
finished 19 epochs...
Completing Train Step...
At time: 298.3223967552185 and batch: 50, loss is 3.592745680809021 and perplexity is 36.33369983852537
At time: 299.11965227127075 and batch: 100, loss is 3.480921196937561 and perplexity is 32.489637649385976
At time: 299.9169192314148 and batch: 150, loss is 3.4966469907760622 and perplexity is 33.0046014879999
At time: 300.7122769355774 and batch: 200, loss is 3.373291573524475 and perplexity is 29.174398865583484
At time: 301.5065996646881 and batch: 250, loss is 3.5107730531692507 and perplexity is 33.474135070466936
At time: 302.3009147644043 and batch: 300, loss is 3.479015965461731 and perplexity is 32.427796298848364
At time: 303.0936996936798 and batch: 350, loss is 3.4672184991836548 and perplexity is 32.04747827114546
At time: 303.88634848594666 and batch: 400, loss is 3.40211407661438 and perplexity is 30.027513457363778
At time: 304.68000769615173 and batch: 450, loss is 3.428184814453125 and perplexity is 30.82064675096067
At time: 305.47361397743225 and batch: 500, loss is 3.3153292608261107 and perplexity is 27.53145752280051
At time: 306.2703092098236 and batch: 550, loss is 3.338688139915466 and perplexity is 28.182131431080908
At time: 307.0681254863739 and batch: 600, loss is 3.377140588760376 and perplexity is 29.286907957017867
At time: 307.8657896518707 and batch: 650, loss is 3.2019930839538575 and perplexity is 24.581474348091632
At time: 308.6602075099945 and batch: 700, loss is 3.1970507097244263 and perplexity is 24.460283235363537
At time: 309.45409178733826 and batch: 750, loss is 3.281079750061035 and perplexity is 26.604483389463734
At time: 310.24824571609497 and batch: 800, loss is 3.241323986053467 and perplexity is 25.56755042824734
At time: 311.0421543121338 and batch: 850, loss is 3.3014456748962404 and perplexity is 27.15186332815543
At time: 311.83609223365784 and batch: 900, loss is 3.2671114587783814 and perplexity is 26.235447615936216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309379682148973 and perplexity of 74.39432649539114
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe9c35db70>
ELAPSED
1481.2194724082947


RESULTS SO FAR:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.19741055555504, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.14482241063688, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.24622883758484704, 'dropout': 0.5513232670420983, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -73.03154451395608, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.04148723988553382, 'dropout': 0.0028119281465838197, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.39432649539114, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.4783637579685053, 'dropout': 0.28177637219722296, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.0, 'dropout': 0.056120837577113517, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0971965789794922 and batch: 50, loss is 7.0679082107543945 and perplexity is 1173.6903518128845
At time: 1.898075819015503 and batch: 100, loss is 6.141449947357177 and perplexity is 464.72691214467454
At time: 2.699504852294922 and batch: 150, loss is 5.8190264320373535 and perplexity is 336.64414807941927
At time: 3.4994006156921387 and batch: 200, loss is 5.504658880233765 and perplexity is 245.83458233894828
At time: 4.298894166946411 and batch: 250, loss is 5.4519643974304195 and perplexity is 233.21584483515832
At time: 5.098193407058716 and batch: 300, loss is 5.2985901927948 and perplexity is 200.05457269344564
At time: 5.898447036743164 and batch: 350, loss is 5.224482774734497 and perplexity is 185.76506328937012
At time: 6.6984264850616455 and batch: 400, loss is 5.04936520576477 and perplexity is 155.9234537544185
At time: 7.499780654907227 and batch: 450, loss is 5.022730350494385 and perplexity is 151.8252746077573
At time: 8.302467107772827 and batch: 500, loss is 4.9281442260742185 and perplexity is 138.122949384151
At time: 9.104766130447388 and batch: 550, loss is 4.968501319885254 and perplexity is 143.81119880328217
At time: 9.907457113265991 and batch: 600, loss is 4.869018316268921 and perplexity is 130.1930457473048
At time: 10.708468914031982 and batch: 650, loss is 4.7386188793182376 and perplexity is 114.27626332858468
At time: 11.508630990982056 and batch: 700, loss is 4.79537446975708 and perplexity is 120.94966529565437
At time: 12.308399200439453 and batch: 750, loss is 4.792739057540894 and perplexity is 120.63133272321119
At time: 13.107908248901367 and batch: 800, loss is 4.735815238952637 and perplexity is 113.95632249297917
At time: 13.907602787017822 and batch: 850, loss is 4.770652208328247 and perplexity is 117.9961749577816
At time: 14.705853939056396 and batch: 900, loss is 4.688376760482788 and perplexity is 108.6766283557913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.754802965138056 and perplexity of 116.14076712978239
finished 1 epochs...
Completing Train Step...
At time: 16.75629997253418 and batch: 50, loss is 4.7240074634552 and perplexity is 112.61866474338294
At time: 17.55317711830139 and batch: 100, loss is 4.611485338211059 and perplexity is 100.6335134839042
At time: 18.346342086791992 and batch: 150, loss is 4.596173648834228 and perplexity is 99.10438105991729
At time: 19.137540817260742 and batch: 200, loss is 4.485946950912475 and perplexity is 88.76096330402305
At time: 19.92867350578308 and batch: 250, loss is 4.601980571746826 and perplexity is 99.6815467174218
At time: 20.720481395721436 and batch: 300, loss is 4.544806442260742 and perplexity is 94.14220349371611
At time: 21.5262131690979 and batch: 350, loss is 4.5301259422302245 and perplexity is 92.77024403784014
At time: 22.31849479675293 and batch: 400, loss is 4.421745233535766 and perplexity is 83.2414344048989
At time: 23.113462924957275 and batch: 450, loss is 4.453079319000244 and perplexity is 85.89102306096001
At time: 23.908528566360474 and batch: 500, loss is 4.337561435699463 and perplexity is 76.5207109603723
At time: 24.706713914871216 and batch: 550, loss is 4.409964952468872 and perplexity is 82.26658021314506
At time: 25.50293278694153 and batch: 600, loss is 4.380309734344483 and perplexity is 79.86276581606684
At time: 26.29582381248474 and batch: 650, loss is 4.232933564186096 and perplexity is 68.91911455780509
At time: 27.086108684539795 and batch: 700, loss is 4.263954391479492 and perplexity is 71.09054821535736
At time: 27.87765884399414 and batch: 750, loss is 4.332095637321472 and perplexity is 76.10360512912783
At time: 28.66947340965271 and batch: 800, loss is 4.285500807762146 and perplexity is 72.63891574488811
At time: 29.46077537536621 and batch: 850, loss is 4.351278400421142 and perplexity is 77.57757476370371
At time: 30.252636194229126 and batch: 900, loss is 4.285740175247192 and perplexity is 72.65630522062098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493876418022261 and perplexity of 89.46758832058698
finished 2 epochs...
Completing Train Step...
At time: 32.28113150596619 and batch: 50, loss is 4.36945348739624 and perplexity is 79.00044516066248
At time: 33.107001543045044 and batch: 100, loss is 4.258668923377991 and perplexity is 70.71579264219673
At time: 33.912649631500244 and batch: 150, loss is 4.248007068634033 and perplexity is 69.96583620053498
At time: 34.71349024772644 and batch: 200, loss is 4.1360952234268185 and perplexity is 62.55806862680517
At time: 35.50921821594238 and batch: 250, loss is 4.276514158248902 and perplexity is 71.98905965603753
At time: 36.30530905723572 and batch: 300, loss is 4.234699091911316 and perplexity is 69.04090064207382
At time: 37.099395990371704 and batch: 350, loss is 4.231030330657959 and perplexity is 68.78807013187856
At time: 37.8936333656311 and batch: 400, loss is 4.141361126899719 and perplexity is 62.888362261598274
At time: 38.68803644180298 and batch: 450, loss is 4.184101014137268 and perplexity is 65.63446992612882
At time: 39.48522925376892 and batch: 500, loss is 4.057238426208496 and perplexity is 57.81443160199708
At time: 40.28285479545593 and batch: 550, loss is 4.130342507362366 and perplexity is 62.19922297864528
At time: 41.08047413825989 and batch: 600, loss is 4.125385527610779 and perplexity is 61.89166559739704
At time: 41.89145827293396 and batch: 650, loss is 3.9741343545913694 and perplexity is 53.204041132045305
At time: 42.68550443649292 and batch: 700, loss is 3.9909324264526367 and perplexity is 54.105315077544795
At time: 43.483031034469604 and batch: 750, loss is 4.081293935775757 and perplexity is 59.222049823394066
At time: 44.282206535339355 and batch: 800, loss is 4.046014938354492 and perplexity is 57.169179792388015
At time: 45.08844017982483 and batch: 850, loss is 4.112407550811768 and perplexity is 61.09362666357497
At time: 45.882895946502686 and batch: 900, loss is 4.0580239629745485 and perplexity is 57.85986480601868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388522631501498 and perplexity of 80.52137132604946
finished 3 epochs...
Completing Train Step...
At time: 47.939106702804565 and batch: 50, loss is 4.150570487976074 and perplexity is 63.47019895703574
At time: 48.7504141330719 and batch: 100, loss is 4.0383667516708375 and perplexity is 56.733607026921646
At time: 49.55048060417175 and batch: 150, loss is 4.0365491771698 and perplexity is 56.63058312460882
At time: 50.35068869590759 and batch: 200, loss is 3.9220029878616334 and perplexity is 50.50149742700655
At time: 51.147716760635376 and batch: 250, loss is 4.067953057289124 and perplexity is 58.43722243657192
At time: 51.94581580162048 and batch: 300, loss is 4.030967984199524 and perplexity is 56.31539728682422
At time: 52.747584104537964 and batch: 350, loss is 4.0272795104980466 and perplexity is 56.10806203531829
At time: 53.54595613479614 and batch: 400, loss is 3.951320219039917 and perplexity is 52.00397817600565
At time: 54.344969749450684 and batch: 450, loss is 3.996222162246704 and perplexity is 54.39227620450358
At time: 55.1450629234314 and batch: 500, loss is 3.8658078861236573 and perplexity is 47.741826816218875
At time: 55.94977521896362 and batch: 550, loss is 3.9396950244903564 and perplexity is 51.40292227737069
At time: 56.75211238861084 and batch: 600, loss is 3.9458552026748657 and perplexity is 51.720550757379364
At time: 57.54958987236023 and batch: 650, loss is 3.796107749938965 and perplexity is 44.527534469114514
At time: 58.3439838886261 and batch: 700, loss is 3.8047807884216307 and perplexity is 44.91540305736126
At time: 59.13851308822632 and batch: 750, loss is 3.9030744075775146 and perplexity is 49.554566084387
At time: 59.931538105010986 and batch: 800, loss is 3.873217878341675 and perplexity is 48.096907328589594
At time: 60.72483682632446 and batch: 850, loss is 3.942518458366394 and perplexity is 51.54826010878421
At time: 61.531609535217285 and batch: 900, loss is 3.888122138977051 and perplexity is 48.81912486112129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3531908009150255 and perplexity of 77.72607610775874
finished 4 epochs...
Completing Train Step...
At time: 63.56128907203674 and batch: 50, loss is 3.986321110725403 and perplexity is 53.85639275799928
At time: 64.35765147209167 and batch: 100, loss is 3.875284695625305 and perplexity is 48.19641764734538
At time: 65.15565896034241 and batch: 150, loss is 3.8763838815689087 and perplexity is 48.249423598516756
At time: 65.95093250274658 and batch: 200, loss is 3.7630414295196535 and perplexity is 43.07924943367003
At time: 66.74360036849976 and batch: 250, loss is 3.91131196975708 and perplexity is 49.9644608527447
At time: 67.53660559654236 and batch: 300, loss is 3.8749740505218506 and perplexity is 48.18144799144471
At time: 68.32984662055969 and batch: 350, loss is 3.8721319389343263 and perplexity is 48.044705350772304
At time: 69.12406659126282 and batch: 400, loss is 3.8045971393585205 and perplexity is 44.9071551430546
At time: 69.9191620349884 and batch: 450, loss is 3.846980037689209 and perplexity is 46.85136002861388
At time: 70.71352410316467 and batch: 500, loss is 3.721192092895508 and perplexity is 41.31361443143483
At time: 71.51016426086426 and batch: 550, loss is 3.789588360786438 and perplexity is 44.23818635521787
At time: 72.30652904510498 and batch: 600, loss is 3.802013649940491 and perplexity is 44.79128771852691
At time: 73.11016869544983 and batch: 650, loss is 3.653805284500122 and perplexity is 38.621352013305646
At time: 73.91031980514526 and batch: 700, loss is 3.66499942779541 and perplexity is 39.056113815005375
At time: 74.70678281784058 and batch: 750, loss is 3.7663613033294676 and perplexity is 43.22250476885424
At time: 75.50523257255554 and batch: 800, loss is 3.735647029876709 and perplexity is 41.915137137171754
At time: 76.29836654663086 and batch: 850, loss is 3.803831434249878 and perplexity is 44.87278266621692
At time: 77.09208512306213 and batch: 900, loss is 3.7532526302337645 and perplexity is 42.659612529108955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344808082058005 and perplexity of 77.0772435537802
finished 5 epochs...
Completing Train Step...
At time: 79.10961079597473 and batch: 50, loss is 3.8522633838653566 and perplexity is 47.0995470340827
At time: 79.92423462867737 and batch: 100, loss is 3.744731845855713 and perplexity is 42.29766340422483
At time: 80.71983170509338 and batch: 150, loss is 3.7468014621734618 and perplexity is 42.38529398817301
At time: 81.52961277961731 and batch: 200, loss is 3.637300577163696 and perplexity is 37.9891494120609
At time: 82.32355833053589 and batch: 250, loss is 3.782671685218811 and perplexity is 43.93326092328639
At time: 83.11752319335938 and batch: 300, loss is 3.74728054523468 and perplexity is 42.40560492949299
At time: 83.9115777015686 and batch: 350, loss is 3.745057535171509 and perplexity is 42.31144154485353
At time: 84.7061812877655 and batch: 400, loss is 3.6819973182678223 and perplexity is 39.72565967267151
At time: 85.50081491470337 and batch: 450, loss is 3.7224194860458373 and perplexity is 41.36435361089413
At time: 86.29570698738098 and batch: 500, loss is 3.605500841140747 and perplexity is 36.800110251708865
At time: 87.09538698196411 and batch: 550, loss is 3.6682176923751832 and perplexity is 39.18200919638255
At time: 87.89233493804932 and batch: 600, loss is 3.683964242935181 and perplexity is 39.80387394818648
At time: 88.69017124176025 and batch: 650, loss is 3.5382596826553345 and perplexity is 34.40698797836281
At time: 89.48686599731445 and batch: 700, loss is 3.549149742126465 and perplexity is 34.78372977068146
At time: 90.27723598480225 and batch: 750, loss is 3.6519419956207275 and perplexity is 38.54945627965348
At time: 91.07720255851746 and batch: 800, loss is 3.6238620853424073 and perplexity is 37.48204751232274
At time: 91.87182235717773 and batch: 850, loss is 3.6887636375427246 and perplexity is 39.99536760541074
At time: 92.66726613044739 and batch: 900, loss is 3.641445851325989 and perplexity is 38.14695169246047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36131809182363 and perplexity of 78.36035252263062
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 94.68030548095703 and batch: 50, loss is 3.7648610162734983 and perplexity is 43.15770722403603
At time: 95.49754762649536 and batch: 100, loss is 3.667488861083984 and perplexity is 39.15346252614509
At time: 96.2916944026947 and batch: 150, loss is 3.6754649591445925 and perplexity is 39.46700313575264
At time: 97.08745241165161 and batch: 200, loss is 3.5519038248062134 and perplexity is 34.87965907636449
At time: 97.88146877288818 and batch: 250, loss is 3.6887978410720823 and perplexity is 39.99673561153599
At time: 98.6752655506134 and batch: 300, loss is 3.6472787475585937 and perplexity is 38.370109097565845
At time: 99.46996665000916 and batch: 350, loss is 3.631412024497986 and perplexity is 37.76610565199054
At time: 100.26423835754395 and batch: 400, loss is 3.560494303703308 and perplexity is 35.18058274020362
At time: 101.05903458595276 and batch: 450, loss is 3.5866482925415037 and perplexity is 36.112833202515866
At time: 101.86670184135437 and batch: 500, loss is 3.461331033706665 and perplexity is 31.859354179646594
At time: 102.66026997566223 and batch: 550, loss is 3.507079629898071 and perplexity is 33.35072895680679
At time: 103.45479798316956 and batch: 600, loss is 3.5187152671813964 and perplexity is 33.741052369156634
At time: 104.2483537197113 and batch: 650, loss is 3.3506040573120117 and perplexity is 28.51995612959768
At time: 105.04510354995728 and batch: 700, loss is 3.345507502555847 and perplexity is 28.374972384166146
At time: 105.83937668800354 and batch: 750, loss is 3.4350805854797364 and perplexity is 31.03391334755857
At time: 106.63415026664734 and batch: 800, loss is 3.3919644212722777 and perplexity is 29.724285975297658
At time: 107.42795467376709 and batch: 850, loss is 3.436165623664856 and perplexity is 31.06760460341851
At time: 108.22244787216187 and batch: 900, loss is 3.3784731912612913 and perplexity is 29.325961779636906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308393086472603 and perplexity of 74.32096556926123
finished 7 epochs...
Completing Train Step...
At time: 110.25174021720886 and batch: 50, loss is 3.672963094711304 and perplexity is 39.368385459786815
At time: 111.0433657169342 and batch: 100, loss is 3.5673265504837035 and perplexity is 35.421768144260604
At time: 111.83398962020874 and batch: 150, loss is 3.570421371459961 and perplexity is 35.53156198383548
At time: 112.62484312057495 and batch: 200, loss is 3.451056890487671 and perplexity is 31.533702373181416
At time: 113.41654372215271 and batch: 250, loss is 3.590653352737427 and perplexity is 36.257757294472064
At time: 114.20752620697021 and batch: 300, loss is 3.551673150062561 and perplexity is 34.87161414786495
At time: 114.9985899925232 and batch: 350, loss is 3.539407739639282 and perplexity is 34.446511844692196
At time: 115.79006242752075 and batch: 400, loss is 3.472766456604004 and perplexity is 32.22577043738
At time: 116.58360481262207 and batch: 450, loss is 3.506247572898865 and perplexity is 33.322990790817535
At time: 117.3752167224884 and batch: 500, loss is 3.384632458686829 and perplexity is 29.50714562784507
At time: 118.16568422317505 and batch: 550, loss is 3.433437719345093 and perplexity is 30.98297063977702
At time: 118.9567141532898 and batch: 600, loss is 3.451672921180725 and perplexity is 31.55313408636027
At time: 119.74986100196838 and batch: 650, loss is 3.2894965314865114 and perplexity is 26.829352520736922
At time: 120.53972482681274 and batch: 700, loss is 3.2906524562835693 and perplexity is 26.86038316569547
At time: 121.34394884109497 and batch: 750, loss is 3.3875049352645874 and perplexity is 29.592026062704864
At time: 122.13530659675598 and batch: 800, loss is 3.350563020706177 and perplexity is 28.518785791413084
At time: 122.92441511154175 and batch: 850, loss is 3.4018240785598755 and perplexity is 30.018806799397492
At time: 123.71546173095703 and batch: 900, loss is 3.351541085243225 and perplexity is 28.546692649564818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320070031571062 and perplexity of 75.193894054171
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 125.72916626930237 and batch: 50, loss is 3.6410789823532106 and perplexity is 38.13295932631787
At time: 126.53545832633972 and batch: 100, loss is 3.5482366609573366 and perplexity is 34.75198389752088
At time: 127.32766819000244 and batch: 150, loss is 3.560698947906494 and perplexity is 35.18778297924457
At time: 128.1202414035797 and batch: 200, loss is 3.4365561771392823 and perplexity is 31.07974053404915
At time: 128.911630153656 and batch: 250, loss is 3.57764600276947 and perplexity is 35.78919394639407
At time: 129.70323848724365 and batch: 300, loss is 3.5321890449523927 and perplexity is 34.19874833313219
At time: 130.49490880966187 and batch: 350, loss is 3.5165301370620727 and perplexity is 33.667404274007126
At time: 131.2873556613922 and batch: 400, loss is 3.445151243209839 and perplexity is 31.348024263963925
At time: 132.0786988735199 and batch: 450, loss is 3.471128387451172 and perplexity is 32.17302560853288
At time: 132.87076354026794 and batch: 500, loss is 3.3494996500015257 and perplexity is 28.488475868268534
At time: 133.66239643096924 and batch: 550, loss is 3.3906990671157837 and perplexity is 29.68669801254769
At time: 134.4539783000946 and batch: 600, loss is 3.4058963298797607 and perplexity is 30.141300167619992
At time: 135.24538373947144 and batch: 650, loss is 3.2421440839767457 and perplexity is 25.588526923471296
At time: 136.03676342964172 and batch: 700, loss is 3.231719455718994 and perplexity is 25.323161613150173
At time: 136.82872414588928 and batch: 750, loss is 3.323907980918884 and perplexity is 27.768658174915906
At time: 137.6219985485077 and batch: 800, loss is 3.279180254936218 and perplexity is 26.553996268167694
At time: 138.4152045249939 and batch: 850, loss is 3.327285284996033 and perplexity is 27.862599923018397
At time: 139.20907068252563 and batch: 900, loss is 3.278823275566101 and perplexity is 26.544518731049056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317555466743364 and perplexity of 75.00505166064991
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 141.22520756721497 and batch: 50, loss is 3.6368088340759277 and perplexity is 37.97047310277665
At time: 142.04181051254272 and batch: 100, loss is 3.539107294082642 and perplexity is 34.43616409781183
At time: 142.83615136146545 and batch: 150, loss is 3.546398386955261 and perplexity is 34.68815891088172
At time: 143.63094115257263 and batch: 200, loss is 3.424212770462036 and perplexity is 30.698468595371473
At time: 144.42678499221802 and batch: 250, loss is 3.5631672763824462 and perplexity is 35.27474526757164
At time: 145.22060227394104 and batch: 300, loss is 3.520510492324829 and perplexity is 33.801679558180076
At time: 146.0153203010559 and batch: 350, loss is 3.5046706295013426 and perplexity is 33.27048373171534
At time: 146.80902242660522 and batch: 400, loss is 3.4318003129959105 and perplexity is 30.932280438489087
At time: 147.60360169410706 and batch: 450, loss is 3.456260380744934 and perplexity is 31.698215335854755
At time: 148.40066838264465 and batch: 500, loss is 3.335495238304138 and perplexity is 28.092292158526092
At time: 149.1958725452423 and batch: 550, loss is 3.373854012489319 and perplexity is 29.19081229962613
At time: 149.99197936058044 and batch: 600, loss is 3.3913673400878905 and perplexity is 29.706543460810355
At time: 150.7892050743103 and batch: 650, loss is 3.223559455871582 and perplexity is 25.117365408670125
At time: 151.58296298980713 and batch: 700, loss is 3.212347402572632 and perplexity is 24.837321038874265
At time: 152.3797082901001 and batch: 750, loss is 3.3035903692245485 and perplexity is 27.21015826550407
At time: 153.1766300201416 and batch: 800, loss is 3.2586958360672 and perplexity is 26.01558641917744
At time: 153.97350597381592 and batch: 850, loss is 3.3027149772644044 and perplexity is 27.186349134405603
At time: 154.76790237426758 and batch: 900, loss is 3.263019700050354 and perplexity is 26.12831781828055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31059327844071 and perplexity of 74.48466598089678
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 156.8019504547119 and batch: 50, loss is 3.626193585395813 and perplexity is 37.56953886150933
At time: 157.59639930725098 and batch: 100, loss is 3.528926372528076 and perplexity is 34.08735084557406
At time: 158.3896825313568 and batch: 150, loss is 3.5386152124404906 and perplexity is 34.41922286221056
At time: 159.1836657524109 and batch: 200, loss is 3.415047254562378 and perplexity is 30.41838680116022
At time: 159.97766780853271 and batch: 250, loss is 3.55497567653656 and perplexity is 34.9869689530836
At time: 160.77211737632751 and batch: 300, loss is 3.5157594776153562 and perplexity is 33.64146816609388
At time: 161.57933115959167 and batch: 350, loss is 3.4998202228546145 and perplexity is 33.10949909238172
At time: 162.3736379146576 and batch: 400, loss is 3.4270008420944214 and perplexity is 30.784177550652906
At time: 163.16771745681763 and batch: 450, loss is 3.4520926332473754 and perplexity is 31.566380097037765
At time: 163.96038246154785 and batch: 500, loss is 3.330853762626648 and perplexity is 27.96220460040864
At time: 164.75362300872803 and batch: 550, loss is 3.369253692626953 and perplexity is 29.05683363464421
At time: 165.54751300811768 and batch: 600, loss is 3.385878782272339 and perplexity is 29.5439440059621
At time: 166.34119701385498 and batch: 650, loss is 3.2164290142059326 and perplexity is 24.938904508474295
At time: 167.13622498512268 and batch: 700, loss is 3.2043712663650514 and perplexity is 24.640003146521607
At time: 167.92940163612366 and batch: 750, loss is 3.2962713861465454 and perplexity is 27.011734593113744
At time: 168.7227098941803 and batch: 800, loss is 3.2522672319412234 and perplexity is 25.848878935318307
At time: 169.51554703712463 and batch: 850, loss is 3.2931966638565062 and perplexity is 26.928808563255195
At time: 170.30866503715515 and batch: 900, loss is 3.255610952377319 and perplexity is 25.935455022517115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307174055543665 and perplexity of 74.2304212129618
finished 11 epochs...
Completing Train Step...
At time: 172.3397994041443 and batch: 50, loss is 3.620721335411072 and perplexity is 37.364510448053856
At time: 173.14815545082092 and batch: 100, loss is 3.5218290758132933 and perplexity is 33.84627929250249
At time: 173.9427673816681 and batch: 150, loss is 3.5325576877593994 and perplexity is 34.211357779761926
At time: 174.7370264530182 and batch: 200, loss is 3.4091275787353514 and perplexity is 30.238851731103843
At time: 175.53708958625793 and batch: 250, loss is 3.550072054862976 and perplexity is 34.815826046799195
At time: 176.3352086544037 and batch: 300, loss is 3.5092097616195677 and perplexity is 33.42184612006447
At time: 177.13450264930725 and batch: 350, loss is 3.4937517166137697 and perplexity is 32.909182317060186
At time: 177.934574842453 and batch: 400, loss is 3.4218164777755735 and perplexity is 30.62499414789021
At time: 178.7272481918335 and batch: 450, loss is 3.447384729385376 and perplexity is 31.418117890222238
At time: 179.52099537849426 and batch: 500, loss is 3.3267481613159178 and perplexity is 27.847638279296444
At time: 180.3143355846405 and batch: 550, loss is 3.3652048683166504 and perplexity is 28.939425463162394
At time: 181.10783982276917 and batch: 600, loss is 3.3836468839645386 and perplexity is 29.478078457274496
At time: 181.9144070148468 and batch: 650, loss is 3.214879159927368 and perplexity is 24.900282777377654
At time: 182.70798254013062 and batch: 700, loss is 3.2036086559295653 and perplexity is 24.621219586171566
At time: 183.50124502182007 and batch: 750, loss is 3.2960943508148195 and perplexity is 27.00695298498884
At time: 184.29438424110413 and batch: 800, loss is 3.2530951499938965 and perplexity is 25.870288550310665
At time: 185.088045835495 and batch: 850, loss is 3.2955163049697878 and perplexity is 26.991346239170753
At time: 185.8890564441681 and batch: 900, loss is 3.2596692085266112 and perplexity is 26.040921602797972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306271069670377 and perplexity of 74.16342244526115
finished 12 epochs...
Completing Train Step...
At time: 187.90328931808472 and batch: 50, loss is 3.617923526763916 and perplexity is 37.26011780104726
At time: 188.7100648880005 and batch: 100, loss is 3.5181400775909424 and perplexity is 33.72165044748954
At time: 189.50318002700806 and batch: 150, loss is 3.5289315795898437 and perplexity is 34.08752834097752
At time: 190.29622626304626 and batch: 200, loss is 3.4055161571502683 and perplexity is 30.12984344516953
At time: 191.08978962898254 and batch: 250, loss is 3.546326508522034 and perplexity is 34.685665669973865
At time: 191.88381934165955 and batch: 300, loss is 3.505268497467041 and perplexity is 33.290381035529414
At time: 192.67856192588806 and batch: 350, loss is 3.489934301376343 and perplexity is 32.78379378534789
At time: 193.4718656539917 and batch: 400, loss is 3.41855007648468 and perplexity is 30.525123824544654
At time: 194.2659420967102 and batch: 450, loss is 3.444242763519287 and perplexity is 31.31955815298087
At time: 195.05839776992798 and batch: 500, loss is 3.324006938934326 and perplexity is 27.771406242189784
At time: 195.8522458076477 and batch: 550, loss is 3.3626264429092405 and perplexity is 28.864903429312
At time: 196.6462378501892 and batch: 600, loss is 3.3819241094589234 and perplexity is 29.427338094884725
At time: 197.43893194198608 and batch: 650, loss is 3.2136826801300047 and perplexity is 24.87050790815269
At time: 198.23334217071533 and batch: 700, loss is 3.2029460144042967 and perplexity is 24.60490994798956
At time: 199.02646446228027 and batch: 750, loss is 3.2959991312026977 and perplexity is 27.004381515830108
At time: 199.8201982975006 and batch: 800, loss is 3.2536816692352293 and perplexity is 25.88546642294632
At time: 200.6136817932129 and batch: 850, loss is 3.2970209646224977 and perplexity is 27.031989598372476
At time: 201.41980719566345 and batch: 900, loss is 3.2619103956222535 and perplexity is 26.099349629866335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305886464576199 and perplexity of 74.13490429965061
finished 13 epochs...
Completing Train Step...
At time: 203.44540357589722 and batch: 50, loss is 3.615677261352539 and perplexity is 37.176515618704876
At time: 204.23988318443298 and batch: 100, loss is 3.515300917625427 and perplexity is 33.6260450712678
At time: 205.0340540409088 and batch: 150, loss is 3.5260197162628173 and perplexity is 33.98841449031768
At time: 205.82852983474731 and batch: 200, loss is 3.4025472354888917 and perplexity is 30.040522958684424
At time: 206.62256956100464 and batch: 250, loss is 3.5432043170928953 and perplexity is 34.57753926540729
At time: 207.41793584823608 and batch: 300, loss is 3.5021615743637087 and perplexity is 33.18711089084256
At time: 208.2191460132599 and batch: 350, loss is 3.4868976974487307 and perplexity is 32.68439338458157
At time: 209.01315546035767 and batch: 400, loss is 3.4158824920654296 and perplexity is 30.443803991816683
At time: 209.8066599369049 and batch: 450, loss is 3.441681032180786 and perplexity is 31.239428538445388
At time: 210.59955382347107 and batch: 500, loss is 3.3217583751678466 and perplexity is 27.709030618434287
At time: 211.39374208450317 and batch: 550, loss is 3.360566244125366 and perplexity is 28.80549720569718
At time: 212.1876199245453 and batch: 600, loss is 3.38041597366333 and perplexity is 29.382991121968942
At time: 212.98216795921326 and batch: 650, loss is 3.2125910091400147 and perplexity is 24.84337231043035
At time: 213.77580761909485 and batch: 700, loss is 3.2022536611557006 and perplexity is 24.587880554513863
At time: 214.57032823562622 and batch: 750, loss is 3.295814023017883 and perplexity is 26.999383246410236
At time: 215.36527562141418 and batch: 800, loss is 3.2540125894546508 and perplexity is 25.894033864666493
At time: 216.16006302833557 and batch: 850, loss is 3.2980162858963014 and perplexity is 27.058908506951866
At time: 216.95520615577698 and batch: 900, loss is 3.2633171224594117 and perplexity is 26.136090121281935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305720081068065 and perplexity of 74.12257050029693
finished 14 epochs...
Completing Train Step...
At time: 218.96981859207153 and batch: 50, loss is 3.6136718702316286 and perplexity is 37.10203686884884
At time: 219.77733945846558 and batch: 100, loss is 3.5128572034835814 and perplexity is 33.54397295063649
At time: 220.56796884536743 and batch: 150, loss is 3.5234599351882934 and perplexity is 33.90152284938758
At time: 221.37221240997314 and batch: 200, loss is 3.3999328136444094 and perplexity is 29.96208693634383
At time: 222.1638171672821 and batch: 250, loss is 3.540469136238098 and perplexity is 34.483092665150224
At time: 222.9553928375244 and batch: 300, loss is 3.4994993829727172 and perplexity is 33.09887794853762
At time: 223.74583435058594 and batch: 350, loss is 3.4842727518081666 and perplexity is 32.5987111334763
At time: 224.5370774269104 and batch: 400, loss is 3.413533983230591 and perplexity is 30.372390339795615
At time: 225.33201479911804 and batch: 450, loss is 3.43944212436676 and perplexity is 31.169564576454135
At time: 226.12410879135132 and batch: 500, loss is 3.319783511161804 and perplexity is 27.65436304949807
At time: 226.91775584220886 and batch: 550, loss is 3.3587712240219116 and perplexity is 28.753837138423748
At time: 227.71035766601562 and batch: 600, loss is 3.3790314197540283 and perplexity is 29.342336937193743
At time: 228.50401639938354 and batch: 650, loss is 3.211551160812378 and perplexity is 24.81755239800343
At time: 229.29683828353882 and batch: 700, loss is 3.201538977622986 and perplexity is 24.57031427908878
At time: 230.0883595943451 and batch: 750, loss is 3.2955459451675413 and perplexity is 26.992146279867537
At time: 230.88055062294006 and batch: 800, loss is 3.254154152870178 and perplexity is 25.897699772015216
At time: 231.674569606781 and batch: 850, loss is 3.298677935600281 and perplexity is 27.076817949990975
At time: 232.46722412109375 and batch: 900, loss is 3.2642606115341186 and perplexity is 26.160760873229282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305675767872431 and perplexity of 74.11928596510413
finished 15 epochs...
Completing Train Step...
At time: 234.47856879234314 and batch: 50, loss is 3.611805672645569 and perplexity is 37.03286170454494
At time: 235.28782677650452 and batch: 100, loss is 3.5106481409072874 and perplexity is 33.46995400167683
At time: 236.08136701583862 and batch: 150, loss is 3.521121034622192 and perplexity is 33.82232321454368
At time: 236.87418222427368 and batch: 200, loss is 3.3975548934936524 and perplexity is 29.890924129328088
At time: 237.66641402244568 and batch: 250, loss is 3.5379981327056886 and perplexity is 34.39799000915009
At time: 238.45798873901367 and batch: 300, loss is 3.497118535041809 and perplexity is 33.02016828850414
At time: 239.2482099533081 and batch: 350, loss is 3.4819097185134886 and perplexity is 32.52177023645768
At time: 240.0396547317505 and batch: 400, loss is 3.41139594078064 and perplexity is 30.307522250017996
At time: 240.82969570159912 and batch: 450, loss is 3.4374140167236327 and perplexity is 31.106413404637017
At time: 241.64250588417053 and batch: 500, loss is 3.3179868030548096 and perplexity is 27.604720840847047
At time: 242.43419241905212 and batch: 550, loss is 3.357138600349426 and perplexity is 28.7069312434928
At time: 243.22553038597107 and batch: 600, loss is 3.37772912979126 and perplexity is 29.304149577221082
At time: 244.01826429367065 and batch: 650, loss is 3.210543646812439 and perplexity is 24.792560958244973
At time: 244.8092679977417 and batch: 700, loss is 3.200808873176575 and perplexity is 24.55238193042422
At time: 245.60102891921997 and batch: 750, loss is 3.2952119445800783 and perplexity is 26.983132392558797
At time: 246.39252924919128 and batch: 800, loss is 3.2541572713851927 and perplexity is 25.89778053450673
At time: 247.18379139900208 and batch: 850, loss is 3.2991082239151 and perplexity is 27.08847129532671
At time: 247.97544360160828 and batch: 900, loss is 3.264910926818848 and perplexity is 26.17777914890792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305707121548587 and perplexity of 74.12160991362515
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 250.00539255142212 and batch: 50, loss is 3.611004700660706 and perplexity is 37.0032112959588
At time: 250.79924535751343 and batch: 100, loss is 3.5106840085983277 and perplexity is 33.47115451317576
At time: 251.59293484687805 and batch: 150, loss is 3.5214363145828247 and perplexity is 33.832988396445984
At time: 252.39317083358765 and batch: 200, loss is 3.3973014640808104 and perplexity is 29.883349849789834
At time: 253.19371604919434 and batch: 250, loss is 3.5379446649551394 and perplexity is 34.39615087516851
At time: 253.99496412277222 and batch: 300, loss is 3.497449655532837 and perplexity is 33.03110375321996
At time: 254.796040058136 and batch: 350, loss is 3.481904344558716 and perplexity is 32.521595466404904
At time: 255.5958616733551 and batch: 400, loss is 3.411153874397278 and perplexity is 30.300186705598446
At time: 256.3969211578369 and batch: 450, loss is 3.437757344245911 and perplexity is 31.1170949260034
At time: 257.1964445114136 and batch: 500, loss is 3.3178148221969606 and perplexity is 27.5999737654909
At time: 257.998583316803 and batch: 550, loss is 3.356236071586609 and perplexity is 28.681034100569413
At time: 258.79934453964233 and batch: 600, loss is 3.375169610977173 and perplexity is 29.229240960948417
At time: 259.5970194339752 and batch: 650, loss is 3.207904930114746 and perplexity is 24.727226650938974
At time: 260.3964331150055 and batch: 700, loss is 3.1982736110687258 and perplexity is 24.490214046098256
At time: 261.21474289894104 and batch: 750, loss is 3.292362108230591 and perplexity is 26.90634434969127
At time: 262.0139157772064 and batch: 800, loss is 3.2511012411117552 and perplexity is 25.81875694392322
At time: 262.81407260894775 and batch: 850, loss is 3.2953334999084474 and perplexity is 26.986412535432585
At time: 263.6122543811798 and batch: 900, loss is 3.2607935857772827 and perplexity is 26.070217889587454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304816677145762 and perplexity of 74.05563811741658
finished 17 epochs...
Completing Train Step...
At time: 265.65707206726074 and batch: 50, loss is 3.610064353942871 and perplexity is 36.96843180262132
At time: 266.47638463974 and batch: 100, loss is 3.5093848419189455 and perplexity is 33.42769813916061
At time: 267.27462363243103 and batch: 150, loss is 3.520361313819885 and perplexity is 33.796637450255396
At time: 268.0699727535248 and batch: 200, loss is 3.396176815032959 and perplexity is 29.849760460512414
At time: 268.86522364616394 and batch: 250, loss is 3.537211203575134 and perplexity is 34.37093187659207
At time: 269.66340589523315 and batch: 300, loss is 3.4962079095840455 and perplexity is 32.99011296928732
At time: 270.45774698257446 and batch: 350, loss is 3.4808918762207033 and perplexity is 32.48868504388526
At time: 271.25169253349304 and batch: 400, loss is 3.4101661157608034 and perplexity is 30.27027221107558
At time: 272.0461149215698 and batch: 450, loss is 3.4367920637130736 and perplexity is 31.087072692302893
At time: 272.84149622917175 and batch: 500, loss is 3.317030529975891 and perplexity is 27.578335807115273
At time: 273.63818073272705 and batch: 550, loss is 3.3556020927429198 and perplexity is 28.662856694388832
At time: 274.4353940486908 and batch: 600, loss is 3.3749718284606933 and perplexity is 29.223460499772294
At time: 275.229740858078 and batch: 650, loss is 3.20771436214447 and perplexity is 24.722514882515878
At time: 276.026171207428 and batch: 700, loss is 3.1980879354476928 and perplexity is 24.485667232525294
At time: 276.81886172294617 and batch: 750, loss is 3.292342309951782 and perplexity is 26.90581165565734
At time: 277.61430740356445 and batch: 800, loss is 3.2513473320007322 and perplexity is 25.825111486637294
At time: 278.40924096107483 and batch: 850, loss is 3.295749487876892 and perplexity is 26.99764089362785
At time: 279.20351696014404 and batch: 900, loss is 3.261514286994934 and perplexity is 26.08901349956086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304387758855951 and perplexity of 74.0238811108317
finished 18 epochs...
Completing Train Step...
At time: 281.2324414253235 and batch: 50, loss is 3.6093573999404907 and perplexity is 36.94230605773479
At time: 282.04190278053284 and batch: 100, loss is 3.508469233512878 and perplexity is 33.39710546534083
At time: 282.8373980522156 and batch: 150, loss is 3.519523000717163 and perplexity is 33.76831715854726
At time: 283.632798910141 and batch: 200, loss is 3.39536057472229 and perplexity is 29.82540582373106
At time: 284.4304554462433 and batch: 250, loss is 3.5365192794799807 and perplexity is 34.34715802645214
At time: 285.2294762134552 and batch: 300, loss is 3.4952857065200806 and perplexity is 32.959703410076
At time: 286.0278561115265 and batch: 350, loss is 3.4800740480422974 and perplexity is 32.46212574372938
At time: 286.8231587409973 and batch: 400, loss is 3.4094356298446655 and perplexity is 30.248168277836598
At time: 287.6181790828705 and batch: 450, loss is 3.4360873651504518 and perplexity is 31.065173393968806
At time: 288.4135465621948 and batch: 500, loss is 3.316436462402344 and perplexity is 27.56195727753822
At time: 289.2096083164215 and batch: 550, loss is 3.35509699344635 and perplexity is 28.648382761329245
At time: 290.0059711933136 and batch: 600, loss is 3.374748582839966 and perplexity is 29.216937218367317
At time: 290.80318212509155 and batch: 650, loss is 3.207522087097168 and perplexity is 24.717761816760035
At time: 291.59881615638733 and batch: 700, loss is 3.1979556322097777 and perplexity is 24.482427913758826
At time: 292.39350509643555 and batch: 750, loss is 3.2923360300064086 and perplexity is 26.905642689160462
At time: 293.18963956832886 and batch: 800, loss is 3.251509819030762 and perplexity is 25.829308073239154
At time: 293.98566603660583 and batch: 850, loss is 3.2960584926605225 and perplexity is 27.005984582864272
At time: 294.7811772823334 and batch: 900, loss is 3.2620308113098146 and perplexity is 26.10249259022398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304167447024828 and perplexity of 74.00757457036524
finished 19 epochs...
Completing Train Step...
At time: 296.82370686531067 and batch: 50, loss is 3.608749952316284 and perplexity is 36.91987235602653
At time: 297.61847972869873 and batch: 100, loss is 3.507721004486084 and perplexity is 33.3721261279188
At time: 298.4147126674652 and batch: 150, loss is 3.518797082901001 and perplexity is 33.743813030567644
At time: 299.20964884757996 and batch: 200, loss is 3.3946678113937376 and perplexity is 29.804751031584527
At time: 300.00546288490295 and batch: 250, loss is 3.5358562088012695 and perplexity is 34.32439098198171
At time: 300.80151295661926 and batch: 300, loss is 3.494511570930481 and perplexity is 32.93419800423855
At time: 301.61898279190063 and batch: 350, loss is 3.479353733062744 and perplexity is 32.43875120781287
At time: 302.4152054786682 and batch: 400, loss is 3.408809504508972 and perplexity is 30.229235061221367
At time: 303.21052646636963 and batch: 450, loss is 3.4354895877838136 and perplexity is 31.046608885697452
At time: 304.00681018829346 and batch: 500, loss is 3.315924210548401 and perplexity is 27.54784222936295
At time: 304.8021831512451 and batch: 550, loss is 3.3546448802947997 and perplexity is 28.635433378225976
At time: 305.5976243019104 and batch: 600, loss is 3.374497137069702 and perplexity is 29.209591666626356
At time: 306.3924038410187 and batch: 650, loss is 3.2073180198669435 and perplexity is 24.712718246199977
At time: 307.18797850608826 and batch: 700, loss is 3.1978287887573242 and perplexity is 24.479322675021567
At time: 307.9856307506561 and batch: 750, loss is 3.2923179340362547 and perplexity is 26.90515580985868
At time: 308.78186869621277 and batch: 800, loss is 3.251618323326111 and perplexity is 25.832110816163066
At time: 309.577659368515 and batch: 850, loss is 3.2962954521179197 and perplexity is 27.012384664567502
At time: 310.37383484840393 and batch: 900, loss is 3.2624229764938355 and perplexity is 26.1127310864937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304049975251498 and perplexity of 73.99888127995871
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe9c35db70>
ELAPSED
1801.67449092865


RESULTS SO FAR:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.19741055555504, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.14482241063688, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.24622883758484704, 'dropout': 0.5513232670420983, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -73.03154451395608, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.04148723988553382, 'dropout': 0.0028119281465838197, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.39432649539114, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.4783637579685053, 'dropout': 0.28177637219722296, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -73.99888127995871, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.0, 'dropout': 0.056120837577113517, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -73.9504835891442, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6608802018934284, 'dropout': 0.5222314943672368, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.19741055555504, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.6705236265624208, 'dropout': 0.7868697094937604, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.14482241063688, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.24622883758484704, 'dropout': 0.5513232670420983, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -73.03154451395608, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.04148723988553382, 'dropout': 0.0028119281465838197, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -74.39432649539114, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.4783637579685053, 'dropout': 0.28177637219722296, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}, {'best_accuracy': -73.99888127995871, 'params': {'batch_size': 32, 'wordvec_source': 'gigavec', 'data': 'ptb', 'seq_len': 35, 'wordvec_dim': 300, 'rnn_dropout': 0.0, 'dropout': 0.056120837577113517, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'tie_weights': 'TRUE'}}]
