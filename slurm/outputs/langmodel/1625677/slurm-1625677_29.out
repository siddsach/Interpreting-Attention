FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'rnn_dropout', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0900275707244873 and batch: 50, loss is 6.8176931953430175 and perplexity is 913.8744468586235
At time: 1.704904556274414 and batch: 100, loss is 5.998415079116821 and perplexity is 402.7898972068266
At time: 2.335812568664551 and batch: 150, loss is 5.738830299377441 and perplexity is 310.7007714623589
At time: 2.95025372505188 and batch: 200, loss is 5.479397878646851 and perplexity is 239.70233401976438
At time: 3.5644164085388184 and batch: 250, loss is 5.473162488937378 and perplexity is 238.21234670622823
At time: 4.178759574890137 and batch: 300, loss is 5.353596506118774 and perplexity is 211.36711562704258
At time: 4.796963453292847 and batch: 350, loss is 5.29219422340393 and perplexity is 198.7791130185989
At time: 5.412653207778931 and batch: 400, loss is 5.1205677318572995 and perplexity is 167.4303982141183
At time: 6.030099153518677 and batch: 450, loss is 5.105235481262207 and perplexity is 164.8828927922155
At time: 6.6476054191589355 and batch: 500, loss is 5.016365375518799 and perplexity is 150.86197946112364
At time: 7.267125606536865 and batch: 550, loss is 5.066482982635498 and perplexity is 158.61549176330595
At time: 7.883446455001831 and batch: 600, loss is 4.977341432571411 and perplexity is 145.08814185062604
At time: 8.500642776489258 and batch: 650, loss is 4.851177082061768 and perplexity is 127.8908392957832
At time: 9.118579149246216 and batch: 700, loss is 4.922655744552612 and perplexity is 137.3669406963997
At time: 9.737338304519653 and batch: 750, loss is 4.918977088928223 and perplexity is 136.86254334828578
At time: 10.354946851730347 and batch: 800, loss is 4.870905523300171 and perplexity is 130.43897896911972
At time: 10.971937656402588 and batch: 850, loss is 4.916059722900391 and perplexity is 136.46384706810653
At time: 11.589774131774902 and batch: 900, loss is 4.82729549407959 and perplexity is 124.87278437637603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.810005501525043 and perplexity of 122.7322927301899
finished 1 epochs...
Completing Train Step...
At time: 13.055133581161499 and batch: 50, loss is 4.769234733581543 and perplexity is 117.82903684458358
At time: 13.625581979751587 and batch: 100, loss is 4.634701032638549 and perplexity is 102.99712061436576
At time: 14.17496633529663 and batch: 150, loss is 4.617848539352417 and perplexity is 101.27590644200892
At time: 14.722628355026245 and batch: 200, loss is 4.508910102844238 and perplexity is 90.82277706143915
At time: 15.271840810775757 and batch: 250, loss is 4.628253965377808 and perplexity is 102.33522717846651
At time: 15.819952487945557 and batch: 300, loss is 4.5757951736450195 and perplexity is 97.10522396008453
At time: 16.367665767669678 and batch: 350, loss is 4.559791879653931 and perplexity is 95.56358903307711
At time: 16.9177987575531 and batch: 400, loss is 4.446100645065307 and perplexity is 85.29370428776691
At time: 17.466264486312866 and batch: 450, loss is 4.468453226089477 and perplexity is 87.2217063604123
At time: 18.014842987060547 and batch: 500, loss is 4.359147396087646 and perplexity is 78.19044051988712
At time: 18.56554126739502 and batch: 550, loss is 4.430411911010742 and perplexity is 83.9659963063694
At time: 19.11802649497986 and batch: 600, loss is 4.410142164230347 and perplexity is 82.28116011056164
At time: 19.708261966705322 and batch: 650, loss is 4.25832733631134 and perplexity is 70.69164116717276
At time: 20.282207489013672 and batch: 700, loss is 4.301732053756714 and perplexity is 73.82755631806312
At time: 20.835556507110596 and batch: 750, loss is 4.360631513595581 and perplexity is 78.30657047555029
At time: 21.41074514389038 and batch: 800, loss is 4.314392533302307 and perplexity is 74.76819046007647
At time: 21.961334466934204 and batch: 850, loss is 4.386498737335205 and perplexity is 80.35856939487184
At time: 22.51057529449463 and batch: 900, loss is 4.323284101486206 and perplexity is 75.43596128897703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.494246391400899 and perplexity of 89.50069507044697
finished 2 epochs...
Completing Train Step...
At time: 23.92674732208252 and batch: 50, loss is 4.3837218093872075 and perplexity is 80.1357289867364
At time: 24.49822211265564 and batch: 100, loss is 4.254714293479919 and perplexity is 70.43669009173597
At time: 25.04756259918213 and batch: 150, loss is 4.254526219367981 and perplexity is 70.42344401945977
At time: 25.59564733505249 and batch: 200, loss is 4.153090105056763 and perplexity is 63.63032119309218
At time: 26.144365072250366 and batch: 250, loss is 4.300660476684571 and perplexity is 73.74848677354196
At time: 26.693636417388916 and batch: 300, loss is 4.260816245079041 and perplexity is 70.86780535002667
At time: 27.242441654205322 and batch: 350, loss is 4.251013498306275 and perplexity is 70.17650008092606
At time: 27.79084324836731 and batch: 400, loss is 4.1640240097045895 and perplexity is 64.3298664691791
At time: 28.33927822113037 and batch: 450, loss is 4.1959391117095945 and perplexity is 66.4160744124558
At time: 28.888628482818604 and batch: 500, loss is 4.073875098228455 and perplexity is 58.78431679933601
At time: 29.43664813041687 and batch: 550, loss is 4.156629881858826 and perplexity is 63.85595744333927
At time: 29.9822838306427 and batch: 600, loss is 4.1556454229354856 and perplexity is 63.793124809373914
At time: 30.530661582946777 and batch: 650, loss is 4.000727553367614 and perplexity is 54.63788755489738
At time: 31.079173803329468 and batch: 700, loss is 4.027177810668945 and perplexity is 56.10235614514704
At time: 31.62784242630005 and batch: 750, loss is 4.112337608337402 and perplexity is 61.08935377358812
At time: 32.17604899406433 and batch: 800, loss is 4.072184977531433 and perplexity is 58.685048120507616
At time: 32.72560095787048 and batch: 850, loss is 4.153814868927002 and perplexity is 63.6764548669352
At time: 33.27458620071411 and batch: 900, loss is 4.099123692512512 and perplexity is 60.287434112877555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385715850412029 and perplexity of 80.29568234166528
finished 3 epochs...
Completing Train Step...
At time: 34.69123935699463 and batch: 50, loss is 4.176442461013794 and perplexity is 65.13372479061216
At time: 35.25293517112732 and batch: 100, loss is 4.0518595457077025 and perplexity is 57.504289539496206
At time: 35.80025029182434 and batch: 150, loss is 4.053374662399292 and perplexity is 57.591481284570015
At time: 36.347649812698364 and batch: 200, loss is 3.953669786453247 and perplexity is 52.12630868410249
At time: 36.895934104919434 and batch: 250, loss is 4.1093085622787475 and perplexity is 60.90459127564074
At time: 37.446059465408325 and batch: 300, loss is 4.074518504142762 and perplexity is 58.82215114654937
At time: 37.99254775047302 and batch: 350, loss is 4.069162607192993 and perplexity is 58.507947937741356
At time: 38.54019284248352 and batch: 400, loss is 3.9906823205947877 and perplexity is 54.09178471338533
At time: 39.087241411209106 and batch: 450, loss is 4.025710301399231 and perplexity is 56.02008579846409
At time: 39.63413691520691 and batch: 500, loss is 3.906853175163269 and perplexity is 49.742175515410324
At time: 40.18089318275452 and batch: 550, loss is 3.9852775621414183 and perplexity is 53.80022031003504
At time: 40.72548484802246 and batch: 600, loss is 3.996157646179199 and perplexity is 54.38876714193691
At time: 41.271174907684326 and batch: 650, loss is 3.8400919961929323 and perplexity is 46.52975480319759
At time: 41.81980538368225 and batch: 700, loss is 3.8585090827941895 and perplexity is 47.39463718741159
At time: 42.36794662475586 and batch: 750, loss is 3.958393235206604 and perplexity is 52.373107042714146
At time: 42.915595054626465 and batch: 800, loss is 3.918196120262146 and perplexity is 50.309610388739834
At time: 43.46345257759094 and batch: 850, loss is 4.001496777534485 and perplexity is 54.679932507354955
At time: 44.01096510887146 and batch: 900, loss is 3.950119662284851 and perplexity is 51.94158191133917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344757916176156 and perplexity of 77.07337700287208
finished 4 epochs...
Completing Train Step...
At time: 45.439337491989136 and batch: 50, loss is 4.036755156517029 and perplexity is 56.64224905658332
At time: 45.987855672836304 and batch: 100, loss is 3.9140712785720826 and perplexity is 50.10251861441675
At time: 46.536460399627686 and batch: 150, loss is 3.9186353158950804 and perplexity is 50.33171100280853
At time: 47.08397054672241 and batch: 200, loss is 3.817288331985474 and perplexity is 45.48071236350964
At time: 47.631765842437744 and batch: 250, loss is 3.974488949775696 and perplexity is 53.2229103740909
At time: 48.179651498794556 and batch: 300, loss is 3.9452166604995726 and perplexity is 51.68753554631553
At time: 48.738059997558594 and batch: 350, loss is 3.940092029571533 and perplexity is 51.42333355012355
At time: 49.28622007369995 and batch: 400, loss is 3.8654921197891237 and perplexity is 47.72675393444074
At time: 49.834569454193115 and batch: 450, loss is 3.9003768157958985 and perplexity is 49.42106823648022
At time: 50.3827645778656 and batch: 500, loss is 3.7875144147872923 and perplexity is 44.14653381967344
At time: 50.93206763267517 and batch: 550, loss is 3.8641451501846316 and perplexity is 47.66251072410896
At time: 51.48097538948059 and batch: 600, loss is 3.880793237686157 and perplexity is 48.46264222257967
At time: 52.03004837036133 and batch: 650, loss is 3.7242159175872804 and perplexity is 41.43872862522125
At time: 52.57716751098633 and batch: 700, loss is 3.73958514213562 and perplexity is 42.08052910488185
At time: 53.12648391723633 and batch: 750, loss is 3.846925501823425 and perplexity is 46.84880501880204
At time: 53.67660713195801 and batch: 800, loss is 3.8071089696884157 and perplexity is 45.02009608222905
At time: 54.22570085525513 and batch: 850, loss is 3.8866063356399536 and perplexity is 48.74518072528477
At time: 54.774885416030884 and batch: 900, loss is 3.8381756019592284 and perplexity is 46.44067083666088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3296487886611725 and perplexity of 75.91761875791804
finished 5 epochs...
Completing Train Step...
At time: 56.19272565841675 and batch: 50, loss is 3.9297336864471437 and perplexity is 50.89342225618839
At time: 56.75531005859375 and batch: 100, loss is 3.809746036529541 and perplexity is 45.13897376009218
At time: 57.303613901138306 and batch: 150, loss is 3.8120506477355955 and perplexity is 45.24312150878779
At time: 57.84885597229004 and batch: 200, loss is 3.7119511795043945 and perplexity is 40.93359745485943
At time: 58.39591097831726 and batch: 250, loss is 3.8723347854614256 and perplexity is 48.05445204090592
At time: 58.944340467453 and batch: 300, loss is 3.8453430557250976 and perplexity is 46.77472793704149
At time: 59.49404311180115 and batch: 350, loss is 3.8390435791015625 and perplexity is 46.480997776320876
At time: 60.04282832145691 and batch: 400, loss is 3.768656668663025 and perplexity is 43.32183015834843
At time: 60.62457537651062 and batch: 450, loss is 3.8083073329925536 and perplexity is 45.074078852347874
At time: 61.19148898124695 and batch: 500, loss is 3.6939580965042116 and perplexity is 40.2036624221677
At time: 61.740023374557495 and batch: 550, loss is 3.7696965599060057 and perplexity is 43.366903581827415
At time: 62.289214849472046 and batch: 600, loss is 3.790006275177002 and perplexity is 44.256677993599936
At time: 62.85108518600464 and batch: 650, loss is 3.635023331642151 and perplexity is 37.902737219927104
At time: 63.399826765060425 and batch: 700, loss is 3.647017216682434 and perplexity is 38.36007544142785
At time: 63.949193716049194 and batch: 750, loss is 3.756765503883362 and perplexity is 42.8097338821589
At time: 64.49820160865784 and batch: 800, loss is 3.718013873100281 and perplexity is 41.18251911944463
At time: 65.04716849327087 and batch: 850, loss is 3.7961987924575804 and perplexity is 44.531588552544505
At time: 65.59474062919617 and batch: 900, loss is 3.7501198482513427 and perplexity is 42.52617838277587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3316282507491435 and perplexity of 76.06804363711399
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 66.99994921684265 and batch: 50, loss is 3.861650891304016 and perplexity is 47.543776222312864
At time: 67.58429479598999 and batch: 100, loss is 3.733665556907654 and perplexity is 41.832165655960964
At time: 68.15189361572266 and batch: 150, loss is 3.740784921646118 and perplexity is 42.131046760456954
At time: 68.70072722434998 and batch: 200, loss is 3.625106678009033 and perplexity is 37.52872643588416
At time: 69.24880051612854 and batch: 250, loss is 3.775543451309204 and perplexity is 43.62120787789147
At time: 69.7976930141449 and batch: 300, loss is 3.7390203619003297 and perplexity is 42.056769563844
At time: 70.3465690612793 and batch: 350, loss is 3.7202411842346192 and perplexity is 41.2743476301889
At time: 70.89632558822632 and batch: 400, loss is 3.64015983581543 and perplexity is 38.097925651784536
At time: 71.45280838012695 and batch: 450, loss is 3.67403094291687 and perplexity is 39.41044737342865
At time: 72.05800747871399 and batch: 500, loss is 3.545400767326355 and perplexity is 34.6535705785299
At time: 72.6299684047699 and batch: 550, loss is 3.6039645624160768 and perplexity is 36.743618429966354
At time: 73.19281768798828 and batch: 600, loss is 3.6204291582107544 and perplexity is 37.353594984702326
At time: 73.77007842063904 and batch: 650, loss is 3.4553389072418215 and perplexity is 31.66901972388409
At time: 74.3566997051239 and batch: 700, loss is 3.45178982257843 and perplexity is 31.556822907446914
At time: 74.9086184501648 and batch: 750, loss is 3.551029667854309 and perplexity is 34.84918210267279
At time: 75.45980525016785 and batch: 800, loss is 3.4995355129241945 and perplexity is 33.10007383099531
At time: 76.00615406036377 and batch: 850, loss is 3.55809823513031 and perplexity is 35.096388559316395
At time: 76.5722119808197 and batch: 900, loss is 3.5032584953308104 and perplexity is 33.22353450187539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27198268942637 and perplexity of 71.66358148069767
finished 7 epochs...
Completing Train Step...
At time: 77.98317098617554 and batch: 50, loss is 3.7681481885910033 and perplexity is 43.29980747055284
At time: 78.52749490737915 and batch: 100, loss is 3.6383923387527464 and perplexity is 38.030647154888584
At time: 79.07356357574463 and batch: 150, loss is 3.6442315006256103 and perplexity is 38.253363866412386
At time: 79.64913129806519 and batch: 200, loss is 3.5372475719451906 and perplexity is 34.372181914092515
At time: 80.22501707077026 and batch: 250, loss is 3.6903118658065797 and perplexity is 40.05733752337141
At time: 80.77803468704224 and batch: 300, loss is 3.6603755140304566 and perplexity is 38.8759385911003
At time: 81.33034682273865 and batch: 350, loss is 3.6451632595062256 and perplexity is 38.28902338836656
At time: 81.90690875053406 and batch: 400, loss is 3.570033655166626 and perplexity is 35.51778848860349
At time: 82.46619296073914 and batch: 450, loss is 3.608610849380493 and perplexity is 36.914737050569094
At time: 83.02179479598999 and batch: 500, loss is 3.4847224807739257 and perplexity is 32.61337501525847
At time: 83.57162475585938 and batch: 550, loss is 3.5455024242401123 and perplexity is 34.65709353262888
At time: 84.12049174308777 and batch: 600, loss is 3.568617763519287 and perplexity is 35.46753473388183
At time: 84.67051434516907 and batch: 650, loss is 3.4090456771850586 and perplexity is 30.23637522368427
At time: 85.22064518928528 and batch: 700, loss is 3.4087339067459106 and perplexity is 30.22694988504987
At time: 85.77347350120544 and batch: 750, loss is 3.5152286291122437 and perplexity is 33.62361438232186
At time: 86.35542154312134 and batch: 800, loss is 3.4690112113952636 and perplexity is 32.104981704946944
At time: 86.91043043136597 and batch: 850, loss is 3.533617477416992 and perplexity is 34.24763384201162
At time: 87.4612078666687 and batch: 900, loss is 3.486536684036255 and perplexity is 32.67259600982449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.275376411333476 and perplexity of 71.90720090157248
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 88.95698571205139 and batch: 50, loss is 3.7398851203918455 and perplexity is 42.093154242162434
At time: 89.52618980407715 and batch: 100, loss is 3.6160838270187377 and perplexity is 37.191633386518774
At time: 90.0776264667511 and batch: 150, loss is 3.623952865600586 and perplexity is 37.48545029672346
At time: 90.6401937007904 and batch: 200, loss is 3.512195053100586 and perplexity is 33.52176914803949
At time: 91.1904993057251 and batch: 250, loss is 3.6639231395721437 and perplexity is 39.0141007927759
At time: 91.74116897583008 and batch: 300, loss is 3.63284378528595 and perplexity is 37.82021640876339
At time: 92.29115319252014 and batch: 350, loss is 3.6111161422729494 and perplexity is 37.00733522326809
At time: 92.84059500694275 and batch: 400, loss is 3.537278747558594 and perplexity is 34.37325350465135
At time: 93.39121055603027 and batch: 450, loss is 3.5694623947143556 and perplexity is 35.49750437499544
At time: 93.94124746322632 and batch: 500, loss is 3.4431799697875975 and perplexity is 31.286289604830305
At time: 94.49120831489563 and batch: 550, loss is 3.4989067935943603 and perplexity is 33.079269715418924
At time: 95.04207825660706 and batch: 600, loss is 3.5208567142486573 and perplexity is 33.8133844668364
At time: 95.59456133842468 and batch: 650, loss is 3.3536317110061646 and perplexity is 28.60643552890614
At time: 96.14712142944336 and batch: 700, loss is 3.349395418167114 and perplexity is 28.485506616917142
At time: 96.69714093208313 and batch: 750, loss is 3.4513176250457764 and perplexity is 31.541925371098394
At time: 97.24832344055176 and batch: 800, loss is 3.4020542192459104 and perplexity is 30.025716143218325
At time: 97.79922103881836 and batch: 850, loss is 3.4640214014053345 and perplexity is 31.94518296075253
At time: 98.35108828544617 and batch: 900, loss is 3.4170219898223877 and perplexity is 30.478514410645005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258877270842252 and perplexity of 70.73052763322262
finished 9 epochs...
Completing Train Step...
At time: 99.78214287757874 and batch: 50, loss is 3.713219861984253 and perplexity is 40.985562149166604
At time: 100.34524393081665 and batch: 100, loss is 3.583684425354004 and perplexity is 36.005958021297815
At time: 100.89626383781433 and batch: 150, loss is 3.5908508110046387 and perplexity is 36.264917395287696
At time: 101.44881558418274 and batch: 200, loss is 3.48160728931427 and perplexity is 32.511936190654154
At time: 101.99968218803406 and batch: 250, loss is 3.6335337066650393 and perplexity is 37.8463183877469
At time: 102.55072379112244 and batch: 300, loss is 3.604472641944885 and perplexity is 36.76229185369543
At time: 103.10111260414124 and batch: 350, loss is 3.5847880601882935 and perplexity is 36.045717386686796
At time: 103.65144157409668 and batch: 400, loss is 3.512775993347168 and perplexity is 33.54124895062702
At time: 104.20253682136536 and batch: 450, loss is 3.5473862791061403 and perplexity is 34.7224440029966
At time: 104.77614283561707 and batch: 500, loss is 3.4234225511550904 and perplexity is 30.674219655046
At time: 105.32700443267822 and batch: 550, loss is 3.481040186882019 and perplexity is 32.493503819578464
At time: 105.87881922721863 and batch: 600, loss is 3.5054048013687136 and perplexity is 33.29491895361367
At time: 106.42924499511719 and batch: 650, loss is 3.3411911058425905 and perplexity is 28.252758697755283
At time: 106.97940969467163 and batch: 700, loss is 3.33961172580719 and perplexity is 28.2081720736033
At time: 107.53032898902893 and batch: 750, loss is 3.4442829656600953 and perplexity is 31.320817291577654
At time: 108.08130311965942 and batch: 800, loss is 3.3973047733306885 and perplexity is 29.883448741425312
At time: 108.63246178627014 and batch: 850, loss is 3.462487721443176 and perplexity is 31.89622682487656
At time: 109.18423819541931 and batch: 900, loss is 3.418699541091919 and perplexity is 30.52968659116557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.25945585067958 and perplexity of 70.77146273136407
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 110.62396192550659 and batch: 50, loss is 3.7059779262542722 and perplexity is 40.68981950935332
At time: 111.1742217540741 and batch: 100, loss is 3.5806091737747194 and perplexity is 35.895400724909265
At time: 111.723965883255 and batch: 150, loss is 3.5896605396270753 and perplexity is 36.22177798100056
At time: 112.27426838874817 and batch: 200, loss is 3.4775116729736326 and perplexity is 32.37905208044314
At time: 112.82637476921082 and batch: 250, loss is 3.6282065153121947 and perplexity is 37.6452398750848
At time: 113.37744426727295 and batch: 300, loss is 3.6009688997268676 and perplexity is 36.6337116471853
At time: 113.92919874191284 and batch: 350, loss is 3.577415542602539 and perplexity is 35.78094691312569
At time: 114.4799907207489 and batch: 400, loss is 3.507016644477844 and perplexity is 33.34862841328107
At time: 115.03205633163452 and batch: 450, loss is 3.5376605129241945 and perplexity is 34.38637852752261
At time: 115.58264970779419 and batch: 500, loss is 3.414048228263855 and perplexity is 30.388013207322945
At time: 116.13373613357544 and batch: 550, loss is 3.467665891647339 and perplexity is 32.0618192791943
At time: 116.68465328216553 and batch: 600, loss is 3.493846559524536 and perplexity is 32.91230366771873
At time: 117.23701333999634 and batch: 650, loss is 3.3256752634048463 and perplexity is 27.81777662847421
At time: 117.78765821456909 and batch: 700, loss is 3.3210895586013796 and perplexity is 27.69050455567536
At time: 118.36010646820068 and batch: 750, loss is 3.4236830186843874 and perplexity is 30.68221033386367
At time: 118.91097617149353 and batch: 800, loss is 3.3759515953063963 and perplexity is 29.252106708497905
At time: 119.46184921264648 and batch: 850, loss is 3.438983039855957 and perplexity is 31.15525839627274
At time: 120.01423478126526 and batch: 900, loss is 3.3972692584991453 and perplexity is 29.882387454623153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.254550463532748 and perplexity of 70.42515139773695
finished 11 epochs...
Completing Train Step...
At time: 121.4630970954895 and batch: 50, loss is 3.69538281917572 and perplexity is 40.26098231427732
At time: 122.02744889259338 and batch: 100, loss is 3.5676463508605956 and perplexity is 35.43309785058894
At time: 122.57989740371704 and batch: 150, loss is 3.576422400474548 and perplexity is 35.745428987460365
At time: 123.13150119781494 and batch: 200, loss is 3.4666014432907106 and perplexity is 32.02770928573599
At time: 123.68207263946533 and batch: 250, loss is 3.6177571296691893 and perplexity is 37.25391834149633
At time: 124.23261952400208 and batch: 300, loss is 3.5905154752731323 and perplexity is 36.252758511452974
At time: 124.78487873077393 and batch: 350, loss is 3.568058433532715 and perplexity is 35.447702225130854
At time: 125.33807492256165 and batch: 400, loss is 3.49838698387146 and perplexity is 33.06207925766502
At time: 125.89038705825806 and batch: 450, loss is 3.5301861476898195 and perplexity is 34.13032030374417
At time: 126.44231629371643 and batch: 500, loss is 3.408068799972534 and perplexity is 30.206852420163692
At time: 126.9929871559143 and batch: 550, loss is 3.462079257965088 and perplexity is 31.883201041589263
At time: 127.54210329055786 and batch: 600, loss is 3.489074182510376 and perplexity is 32.755607949137556
At time: 128.0931613445282 and batch: 650, loss is 3.3223312520980834 and perplexity is 27.724909030588563
At time: 128.64419507980347 and batch: 700, loss is 3.3190930557250975 and perplexity is 27.63527553447944
At time: 129.195143699646 and batch: 750, loss is 3.4228412199020384 and perplexity is 30.656392954608627
At time: 129.74745965003967 and batch: 800, loss is 3.3762233781814577 and perplexity is 29.260058010625674
At time: 130.2988405227661 and batch: 850, loss is 3.4410270738601683 and perplexity is 31.21900593271563
At time: 130.85063982009888 and batch: 900, loss is 3.4004726076126097 and perplexity is 29.97826465607171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.254301306319563 and perplexity of 70.40760664906767
finished 12 epochs...
Completing Train Step...
At time: 132.2722282409668 and batch: 50, loss is 3.689974932670593 and perplexity is 40.0438431524984
At time: 132.8386528491974 and batch: 100, loss is 3.5617353677749635 and perplexity is 35.22427120194493
At time: 133.3912854194641 and batch: 150, loss is 3.5701095867156982 and perplexity is 35.520485511696336
At time: 133.94253587722778 and batch: 200, loss is 3.4607157850265504 and perplexity is 31.83975878267745
At time: 134.49444603919983 and batch: 250, loss is 3.6116551542282105 and perplexity is 37.02728799629431
At time: 135.0458300113678 and batch: 300, loss is 3.5846722650527956 and perplexity is 36.0415437096083
At time: 135.59773659706116 and batch: 350, loss is 3.562646579742432 and perplexity is 35.25638260733607
At time: 136.14835166931152 and batch: 400, loss is 3.493373441696167 and perplexity is 32.8967359530529
At time: 136.70039868354797 and batch: 450, loss is 3.5255184984207153 and perplexity is 33.97138315911046
At time: 137.2523787021637 and batch: 500, loss is 3.4040583610534667 and perplexity is 30.085952276957784
At time: 137.80465841293335 and batch: 550, loss is 3.4583746719360353 and perplexity is 31.76530549238411
At time: 138.35632395744324 and batch: 600, loss is 3.485973711013794 and perplexity is 32.654207396329525
At time: 138.9078917503357 and batch: 650, loss is 3.319909076690674 and perplexity is 27.65783570223811
At time: 139.46056509017944 and batch: 700, loss is 3.317312593460083 and perplexity is 27.586115745767966
At time: 140.0410966873169 and batch: 750, loss is 3.4216571855545044 and perplexity is 30.620116213071014
At time: 140.59451127052307 and batch: 800, loss is 3.37559513092041 and perplexity is 29.2416812325071
At time: 141.14636206626892 and batch: 850, loss is 3.441151623725891 and perplexity is 31.222894497867653
At time: 141.6990785598755 and batch: 900, loss is 3.401146402359009 and perplexity is 29.998470659888287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.254519109856592 and perplexity of 70.42294334496228
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 143.14351654052734 and batch: 50, loss is 3.687810297012329 and perplexity is 39.957256569728386
At time: 143.69375014305115 and batch: 100, loss is 3.5610855054855346 and perplexity is 35.201387712781845
At time: 144.24411582946777 and batch: 150, loss is 3.5700246047973634 and perplexity is 35.5174670409569
At time: 144.79454040527344 and batch: 200, loss is 3.460232849121094 and perplexity is 31.824385932289708
At time: 145.3459780216217 and batch: 250, loss is 3.6106275701522828 and perplexity is 36.989258887174344
At time: 145.8968026638031 and batch: 300, loss is 3.584083333015442 and perplexity is 36.020323938956736
At time: 146.4606146812439 and batch: 350, loss is 3.5611200618743895 and perplexity is 35.20260416664188
At time: 147.01167011260986 and batch: 400, loss is 3.492424907684326 and perplexity is 32.86554707431745
At time: 147.56234788894653 and batch: 450, loss is 3.5232828283309936 and perplexity is 33.89551918887847
At time: 148.11229467391968 and batch: 500, loss is 3.4006496047973633 and perplexity is 29.983571194126927
At time: 148.66237449645996 and batch: 550, loss is 3.4542999267578125 and perplexity is 31.636133317568806
At time: 149.2121880054474 and batch: 600, loss is 3.483082876205444 and perplexity is 32.559945789960295
At time: 149.76406812667847 and batch: 650, loss is 3.315211868286133 and perplexity is 27.52822572477082
At time: 150.31526136398315 and batch: 700, loss is 3.3107185125350953 and perplexity is 27.40480909844932
At time: 150.86723160743713 and batch: 750, loss is 3.41371732711792 and perplexity is 30.37795944242285
At time: 151.4180781841278 and batch: 800, loss is 3.366906762123108 and perplexity is 28.988719226597887
At time: 151.9691903591156 and batch: 850, loss is 3.4321954345703123 and perplexity is 30.94450486474401
At time: 152.52036595344543 and batch: 900, loss is 3.3919514751434328 and perplexity is 29.723901163352515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2531759183700775 and perplexity of 70.32841534581983
finished 14 epochs...
Completing Train Step...
At time: 153.93910789489746 and batch: 50, loss is 3.68437029838562 and perplexity is 39.820039810047774
At time: 154.50308656692505 and batch: 100, loss is 3.557638654708862 and perplexity is 35.08026265213045
At time: 155.05303072929382 and batch: 150, loss is 3.566566576957703 and perplexity is 35.39485876473
At time: 155.60310554504395 and batch: 200, loss is 3.45740975856781 and perplexity is 31.734669507381476
At time: 156.15467810630798 and batch: 250, loss is 3.6079399824142455 and perplexity is 36.88998047802601
At time: 156.7059109210968 and batch: 300, loss is 3.5811757135391233 and perplexity is 35.91574265849218
At time: 157.25677227973938 and batch: 350, loss is 3.558654956817627 and perplexity is 35.11593291985376
At time: 157.80736088752747 and batch: 400, loss is 3.489890275001526 and perplexity is 32.782350465527074
At time: 158.3592426776886 and batch: 450, loss is 3.5210297536849975 and perplexity is 33.81923602208561
At time: 158.90947008132935 and batch: 500, loss is 3.399168276786804 and perplexity is 29.939188570987742
At time: 159.4593210220337 and batch: 550, loss is 3.4526456356048585 and perplexity is 31.583841207221198
At time: 160.0089237689972 and batch: 600, loss is 3.481683192253113 and perplexity is 32.51440403581566
At time: 160.57013511657715 and batch: 650, loss is 3.3142505645751954 and perplexity is 27.50177545463439
At time: 161.12096190452576 and batch: 700, loss is 3.310403656959534 and perplexity is 27.396181899739553
At time: 161.67097425460815 and batch: 750, loss is 3.4139770317077636 and perplexity is 30.385849762451947
At time: 162.22120833396912 and batch: 800, loss is 3.3674524545669557 and perplexity is 29.0045424685558
At time: 162.7713658809662 and batch: 850, loss is 3.433229923248291 and perplexity is 30.976533168274635
At time: 163.32218718528748 and batch: 900, loss is 3.3936761093139647 and perplexity is 29.775208249239945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252945573362585 and perplexity of 70.3122174120883
finished 15 epochs...
Completing Train Step...
At time: 164.75508618354797 and batch: 50, loss is 3.682628450393677 and perplexity is 39.75073972629297
At time: 165.3199164867401 and batch: 100, loss is 3.5556920623779296 and perplexity is 35.01204210222624
At time: 165.8708200454712 and batch: 150, loss is 3.5645694828033445 and perplexity is 35.32424243639531
At time: 166.4215383529663 and batch: 200, loss is 3.45564582824707 and perplexity is 31.6787411030244
At time: 166.97328305244446 and batch: 250, loss is 3.6061256885528565 and perplexity is 36.82311189087396
At time: 167.52377462387085 and batch: 300, loss is 3.579341173171997 and perplexity is 35.849914179733005
At time: 168.09020614624023 and batch: 350, loss is 3.5569922351837158 and perplexity is 35.05759341311359
At time: 168.6414873600006 and batch: 400, loss is 3.488295216560364 and perplexity is 32.730102381147425
At time: 169.19248914718628 and batch: 450, loss is 3.5195949506759643 and perplexity is 33.77074687498356
At time: 169.74395775794983 and batch: 500, loss is 3.3980522775650024 and perplexity is 29.905795096852557
At time: 170.295170545578 and batch: 550, loss is 3.45154625415802 and perplexity is 31.54913759792553
At time: 170.84664392471313 and batch: 600, loss is 3.480801100730896 and perplexity is 32.48573600143962
At time: 171.39845514297485 and batch: 650, loss is 3.313620758056641 and perplexity is 27.484460110411923
At time: 171.94960761070251 and batch: 700, loss is 3.3101134204864504 and perplexity is 27.388231682306362
At time: 172.4981243610382 and batch: 750, loss is 3.4139717626571655 and perplexity is 30.38568965829388
At time: 173.04909539222717 and batch: 800, loss is 3.367645139694214 and perplexity is 29.010131750980925
At time: 173.60001397132874 and batch: 850, loss is 3.4337397956848146 and perplexity is 30.992331275883753
At time: 174.1631052494049 and batch: 900, loss is 3.3944811248779296 and perplexity is 29.79918740580303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2528970796767975 and perplexity of 70.30880779618316
finished 16 epochs...
Completing Train Step...
At time: 175.595867395401 and batch: 50, loss is 3.6812087392807005 and perplexity is 39.69434520079411
At time: 176.14624857902527 and batch: 100, loss is 3.554134588241577 and perplexity is 34.95755419495869
At time: 176.69666481018066 and batch: 150, loss is 3.562950305938721 and perplexity is 35.2670925206786
At time: 177.2484405040741 and batch: 200, loss is 3.454179058074951 and perplexity is 31.63230973088448
At time: 177.7992558479309 and batch: 250, loss is 3.6046047735214235 and perplexity is 36.767149634201246
At time: 178.35125064849854 and batch: 300, loss is 3.5778503370285035 and perplexity is 35.79650765201537
At time: 178.9010841846466 and batch: 350, loss is 3.555610327720642 and perplexity is 35.00918052191081
At time: 179.4502546787262 and batch: 400, loss is 3.487000575065613 and perplexity is 32.68775605005076
At time: 180.00124502182007 and batch: 450, loss is 3.5184153842926027 and perplexity is 33.73093552191174
At time: 180.55422711372375 and batch: 500, loss is 3.3970721435546873 and perplexity is 29.876497769971266
At time: 181.10620307922363 and batch: 550, loss is 3.4506105899810793 and perplexity is 31.519632005877128
At time: 181.65746760368347 and batch: 600, loss is 3.480050230026245 and perplexity is 32.4613525695051
At time: 182.20850086212158 and batch: 650, loss is 3.3130568218231202 and perplexity is 27.4689649970355
At time: 182.76016092300415 and batch: 700, loss is 3.3097745323181154 and perplexity is 27.378951707163314
At time: 183.3115749359131 and batch: 750, loss is 3.413816685676575 and perplexity is 30.38097790264037
At time: 183.86313557624817 and batch: 800, loss is 3.367643780708313 and perplexity is 29.010092326647673
At time: 184.41219568252563 and batch: 850, loss is 3.4339700508117676 and perplexity is 30.999468240686088
At time: 184.96460962295532 and batch: 900, loss is 3.3948858118057252 and perplexity is 29.811249187863236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2529188182255995 and perplexity of 70.31033622424549
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 186.38551306724548 and batch: 50, loss is 3.680499291419983 and perplexity is 39.66619411955175
At time: 186.95005440711975 and batch: 100, loss is 3.553745369911194 and perplexity is 34.943950721612964
At time: 187.5010702610016 and batch: 150, loss is 3.562641701698303 and perplexity is 35.256210625565366
At time: 188.06459140777588 and batch: 200, loss is 3.4540384292602537 and perplexity is 31.627861629433838
At time: 188.61650919914246 and batch: 250, loss is 3.604338927268982 and perplexity is 36.7573765243879
At time: 189.16755652427673 and batch: 300, loss is 3.5776229906082153 and perplexity is 35.78837036916798
At time: 189.71680760383606 and batch: 350, loss is 3.5551798820495604 and perplexity is 34.99411421455306
At time: 190.26734042167664 and batch: 400, loss is 3.4864682006835936 and perplexity is 32.67035855752458
At time: 190.81804776191711 and batch: 450, loss is 3.517676911354065 and perplexity is 33.706035334026296
At time: 191.39493584632874 and batch: 500, loss is 3.395930552482605 and perplexity is 29.842410487422654
At time: 191.95509386062622 and batch: 550, loss is 3.449149212837219 and perplexity is 31.473603576719643
At time: 192.50609707832336 and batch: 600, loss is 3.4786062908172606 and perplexity is 32.41451417378937
At time: 193.05777883529663 and batch: 650, loss is 3.3114797353744505 and perplexity is 27.42567820705655
At time: 193.60807728767395 and batch: 700, loss is 3.307569522857666 and perplexity is 27.31864736988258
At time: 194.15913939476013 and batch: 750, loss is 3.411261863708496 and perplexity is 30.303458978572635
At time: 194.70914554595947 and batch: 800, loss is 3.3647383880615234 and perplexity is 28.925928940764454
At time: 195.25955057144165 and batch: 850, loss is 3.4310296297073366 and perplexity is 30.90845063067447
At time: 195.81116604804993 and batch: 900, loss is 3.3916304874420167 and perplexity is 29.714361687750053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252503695553297 and perplexity of 70.28115486692214
finished 18 epochs...
Completing Train Step...
At time: 197.22229671478271 and batch: 50, loss is 3.67987624168396 and perplexity is 39.64148780520708
At time: 197.7869758605957 and batch: 100, loss is 3.553128552436829 and perplexity is 34.92240332827206
At time: 198.33837819099426 and batch: 150, loss is 3.562013039588928 and perplexity is 35.23405334727532
At time: 198.88997340202332 and batch: 200, loss is 3.453422255516052 and perplexity is 31.60837937435597
At time: 199.44188737869263 and batch: 250, loss is 3.6037519645690916 and perplexity is 36.73580764610504
At time: 199.99921321868896 and batch: 300, loss is 3.577046027183533 and perplexity is 35.76772774402847
At time: 200.55019402503967 and batch: 350, loss is 3.5546869087219237 and perplexity is 34.97686730110445
At time: 201.10159707069397 and batch: 400, loss is 3.4860055303573607 and perplexity is 32.65524644830445
At time: 201.65290307998657 and batch: 450, loss is 3.517244372367859 and perplexity is 33.69145931224999
At time: 202.21624326705933 and batch: 500, loss is 3.3956379318237304 and perplexity is 29.833679259134612
At time: 202.76854252815247 and batch: 550, loss is 3.4488414764404296 and perplexity is 31.46391949350982
At time: 203.32044100761414 and batch: 600, loss is 3.478403534889221 and perplexity is 32.40794260512088
At time: 203.87207293510437 and batch: 650, loss is 3.3113291215896608 and perplexity is 27.421547832914808
At time: 204.4237756729126 and batch: 700, loss is 3.307534627914429 and perplexity is 27.31769410386547
At time: 204.97555136680603 and batch: 750, loss is 3.4113532638549806 and perplexity is 30.306228845743473
At time: 205.52751398086548 and batch: 800, loss is 3.364884910583496 and perplexity is 28.930167551341633
At time: 206.07848000526428 and batch: 850, loss is 3.4312853956222535 and perplexity is 30.91635696987163
At time: 206.62984609603882 and batch: 900, loss is 3.391953716278076 and perplexity is 29.72396777869179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252316409594392 and perplexity of 70.26799342595496
finished 19 epochs...
Completing Train Step...
At time: 208.06256413459778 and batch: 50, loss is 3.679368929862976 and perplexity is 39.62138231015096
At time: 208.61301851272583 and batch: 100, loss is 3.552610034942627 and perplexity is 34.90430014502099
At time: 209.16488027572632 and batch: 150, loss is 3.5614924669265746 and perplexity is 35.21571623563245
At time: 209.71754360198975 and batch: 200, loss is 3.45292414188385 and perplexity is 31.592638730337924
At time: 210.26945304870605 and batch: 250, loss is 3.6032547330856324 and perplexity is 36.71754598648539
At time: 210.82210636138916 and batch: 300, loss is 3.5765641784667968 and perplexity is 35.75049726189067
At time: 211.3750171661377 and batch: 350, loss is 3.554265384674072 and perplexity is 34.96212681737097
At time: 211.92729878425598 and batch: 400, loss is 3.4856112146377565 and perplexity is 32.64237250966733
At time: 212.48034977912903 and batch: 450, loss is 3.516879334449768 and perplexity is 33.67916289655074
At time: 213.0335922241211 and batch: 500, loss is 3.395374083518982 and perplexity is 29.825808731795984
At time: 213.5847909450531 and batch: 550, loss is 3.4485755443572996 and perplexity is 31.455553340319053
At time: 214.13607597351074 and batch: 600, loss is 3.4782231712341307 and perplexity is 32.40209791723914
At time: 214.68762946128845 and batch: 650, loss is 3.3111973333358766 and perplexity is 27.41793423313008
At time: 215.24050998687744 and batch: 700, loss is 3.3074928379058837 and perplexity is 27.316552521048976
At time: 215.81479597091675 and batch: 750, loss is 3.411410131454468 and perplexity is 30.30795233723239
At time: 216.3675148487091 and batch: 800, loss is 3.3649868822097777 and perplexity is 28.93311775799157
At time: 216.9215624332428 and batch: 850, loss is 3.431475238800049 and perplexity is 30.922226786479335
At time: 217.4741153717041 and batch: 900, loss is 3.392207827568054 and perplexity is 29.731521934244757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2522256929580475 and perplexity of 70.26161923907547
Finished Training.
Improved accuracyfrom -10000000 to -70.26161923907547
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8858900b70>
ELAPSED
228.6494381427765


RESULTS SO FAR:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8462543487548828 and batch: 50, loss is 6.877729282379151 and perplexity is 970.4203055891747
At time: 1.4772956371307373 and batch: 100, loss is 6.006142034530639 and perplexity is 405.91429226897986
At time: 2.0983903408050537 and batch: 150, loss is 5.836477279663086 and perplexity is 342.57043276258923
At time: 2.7179086208343506 and batch: 200, loss is 5.636924362182617 and perplexity is 280.5983709854289
At time: 3.3393969535827637 and batch: 250, loss is 5.657067823410034 and perplexity is 286.307905226034
At time: 3.958824634552002 and batch: 300, loss is 5.544142990112305 and perplexity is 255.73531660703102
At time: 4.580357313156128 and batch: 350, loss is 5.502313022613525 and perplexity is 245.2585653017821
At time: 5.2006940841674805 and batch: 400, loss is 5.3408798122406 and perplexity is 208.69624304927493
At time: 5.822512149810791 and batch: 450, loss is 5.336437320709228 and perplexity is 207.77116809713843
At time: 6.4419050216674805 and batch: 500, loss is 5.272675323486328 and perplexity is 194.9367844088132
At time: 7.061084270477295 and batch: 550, loss is 5.30995771408081 and perplexity is 202.3416720036505
At time: 7.681668758392334 and batch: 600, loss is 5.229482192993164 and perplexity is 186.69610593518286
At time: 8.301568984985352 and batch: 650, loss is 5.115314359664917 and perplexity is 166.55313034086913
At time: 8.922180652618408 and batch: 700, loss is 5.20334228515625 and perplexity is 181.87911901168783
At time: 9.544047355651855 and batch: 750, loss is 5.179136457443238 and perplexity is 177.52944055494754
At time: 10.165453433990479 and batch: 800, loss is 5.1501538372039795 and perplexity is 172.45801873567922
At time: 10.78743052482605 and batch: 850, loss is 5.191021089553833 and perplexity is 179.65189998742403
At time: 11.410603046417236 and batch: 900, loss is 5.09221284866333 and perplexity is 162.74960412325595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.9706178430008565 and perplexity of 144.11590087056913
finished 1 epochs...
Completing Train Step...
At time: 12.892271280288696 and batch: 50, loss is 4.91383713722229 and perplexity is 136.16088128445836
At time: 13.442224740982056 and batch: 100, loss is 4.7738096714019775 and perplexity is 118.36933232834309
At time: 13.992330551147461 and batch: 150, loss is 4.74541558265686 and perplexity is 115.05561068529428
At time: 14.542081594467163 and batch: 200, loss is 4.62930552482605 and perplexity is 102.44289535331586
At time: 15.104289293289185 and batch: 250, loss is 4.734923181533813 and perplexity is 113.85471223790653
At time: 15.654242515563965 and batch: 300, loss is 4.670093421936035 and perplexity is 106.70771080774465
At time: 16.207562685012817 and batch: 350, loss is 4.651087617874145 and perplexity is 104.69879595667764
At time: 16.75590229034424 and batch: 400, loss is 4.526824579238892 and perplexity is 92.46448078293302
At time: 17.304951667785645 and batch: 450, loss is 4.543655023574829 and perplexity is 94.03386878275457
At time: 17.852535724639893 and batch: 500, loss is 4.441696500778198 and perplexity is 84.91888449337836
At time: 18.40141463279724 and batch: 550, loss is 4.516671838760376 and perplexity is 91.53046235248615
At time: 18.951205730438232 and batch: 600, loss is 4.479086723327637 and perplexity is 88.1541267933393
At time: 19.501155614852905 and batch: 650, loss is 4.336116733551026 and perplexity is 76.41024114205388
At time: 20.050318002700806 and batch: 700, loss is 4.380884857177734 and perplexity is 79.90870992670143
At time: 20.59676504135132 and batch: 750, loss is 4.4306673908233645 and perplexity is 83.98745066383336
At time: 21.146542072296143 and batch: 800, loss is 4.382229995727539 and perplexity is 80.01627053863348
At time: 21.69540786743164 and batch: 850, loss is 4.4508894920349125 and perplexity is 85.7031423707614
At time: 22.245242834091187 and batch: 900, loss is 4.387756090164185 and perplexity is 80.45967201694748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.518346499090326 and perplexity of 91.68387320645222
finished 2 epochs...
Completing Train Step...
At time: 23.66423010826111 and batch: 50, loss is 4.422844390869141 and perplexity is 83.33298014033217
At time: 24.224559545516968 and batch: 100, loss is 4.293774814605713 and perplexity is 73.24242390321733
At time: 24.77337121963501 and batch: 150, loss is 4.291010689735413 and perplexity is 73.04025224032858
At time: 25.323177814483643 and batch: 200, loss is 4.186612763404846 and perplexity is 65.799534471488
At time: 25.873263120651245 and batch: 250, loss is 4.3319957447052 and perplexity is 76.09600332059233
At time: 26.423145055770874 and batch: 300, loss is 4.298182873725891 and perplexity is 73.56599347084023
At time: 26.972491025924683 and batch: 350, loss is 4.283339900970459 and perplexity is 72.48211929078914
At time: 27.521645545959473 and batch: 400, loss is 4.192143030166626 and perplexity is 66.16443150889911
At time: 28.070745944976807 and batch: 450, loss is 4.233850345611573 and perplexity is 68.98232729359552
At time: 28.63253164291382 and batch: 500, loss is 4.108265509605408 and perplexity is 60.8410976982214
At time: 29.182233333587646 and batch: 550, loss is 4.188028512001037 and perplexity is 65.89275604363031
At time: 29.731966018676758 and batch: 600, loss is 4.185065226554871 and perplexity is 65.69778601722953
At time: 30.28109574317932 and batch: 650, loss is 4.039136762619019 and perplexity is 56.77730934893607
At time: 30.830076694488525 and batch: 700, loss is 4.063116993904114 and perplexity is 58.15529857511446
At time: 31.381698846817017 and batch: 750, loss is 4.150100145339966 and perplexity is 63.44035323575182
At time: 31.93119478225708 and batch: 800, loss is 4.106439995765686 and perplexity is 60.73013274719222
At time: 32.479053258895874 and batch: 850, loss is 4.1825329685211186 and perplexity is 65.53163273107035
At time: 33.02819323539734 and batch: 900, loss is 4.135795950889587 and perplexity is 62.53934951608055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3980733793075775 and perplexity of 81.29409480635606
finished 3 epochs...
Completing Train Step...
At time: 34.447134017944336 and batch: 50, loss is 4.199429388046265 and perplexity is 66.64828987768585
At time: 35.010109424591064 and batch: 100, loss is 4.065276074409485 and perplexity is 58.280996193360956
At time: 35.56035900115967 and batch: 150, loss is 4.074312777519226 and perplexity is 58.81005110869752
At time: 36.132187843322754 and batch: 200, loss is 3.9697546577453613 and perplexity is 52.97153308993279
At time: 36.68730449676514 and batch: 250, loss is 4.122419643402099 and perplexity is 61.70837402889703
At time: 37.23716616630554 and batch: 300, loss is 4.098335480690002 and perplexity is 60.23993356726608
At time: 37.78776574134827 and batch: 350, loss is 4.085642414093018 and perplexity is 59.480136358119545
At time: 38.33759427070618 and batch: 400, loss is 4.007693920135498 and perplexity is 55.01984399866278
At time: 38.88738012313843 and batch: 450, loss is 4.053628768920898 and perplexity is 57.606117515055466
At time: 39.43717002868652 and batch: 500, loss is 3.9225151443481447 and perplexity is 50.52736872100233
At time: 39.98738074302673 and batch: 550, loss is 4.001595373153687 and perplexity is 54.68532397494159
At time: 40.53726649284363 and batch: 600, loss is 4.0125322961807255 and perplexity is 55.28669573782879
At time: 41.087322473526 and batch: 650, loss is 3.868499994277954 and perplexity is 47.870526136007534
At time: 41.6363525390625 and batch: 700, loss is 3.878347535133362 and perplexity is 48.344261835395564
At time: 42.18621230125427 and batch: 750, loss is 3.9823634433746338 and perplexity is 53.64366829474307
At time: 42.74830603599548 and batch: 800, loss is 3.9358614826202394 and perplexity is 51.20624425016581
At time: 43.29842948913574 and batch: 850, loss is 4.017151126861572 and perplexity is 55.54264626539143
At time: 43.84850478172302 and batch: 900, loss is 3.9757529640197755 and perplexity is 53.290227426809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349170005484803 and perplexity of 77.41418290570898
finished 4 epochs...
Completing Train Step...
At time: 45.27135729789734 and batch: 50, loss is 4.048787899017334 and perplexity is 57.32792767817874
At time: 45.82233166694641 and batch: 100, loss is 3.915526852607727 and perplexity is 50.17549964139088
At time: 46.37270665168762 and batch: 150, loss is 3.927216815948486 and perplexity is 50.76549116366546
At time: 46.92278456687927 and batch: 200, loss is 3.8280965375900267 and perplexity is 45.974943317440975
At time: 47.472681760787964 and batch: 250, loss is 3.9813162899017334 and perplexity is 53.58752454188229
At time: 48.02232599258423 and batch: 300, loss is 3.9606805086135863 and perplexity is 52.493035760292095
At time: 48.572410583496094 and batch: 350, loss is 3.9477570724487303 and perplexity is 51.81901010830602
At time: 49.12260675430298 and batch: 400, loss is 3.876214303970337 and perplexity is 48.241242270835
At time: 49.67236042022705 and batch: 450, loss is 3.924119162559509 and perplexity is 50.60848057565827
At time: 50.22250509262085 and batch: 500, loss is 3.793930644989014 and perplexity is 44.43069880236575
At time: 50.772223711013794 and batch: 550, loss is 3.8734495878219604 and perplexity is 48.108053129234015
At time: 51.32241153717041 and batch: 600, loss is 3.8862357187271117 and perplexity is 48.72711828421827
At time: 51.874287843704224 and batch: 650, loss is 3.7450651693344117 and perplexity is 42.3117645585239
At time: 52.42370414733887 and batch: 700, loss is 3.7478951835632324 and perplexity is 42.431677051269986
At time: 52.97374749183655 and batch: 750, loss is 3.8598459005355834 and perplexity is 47.45803754717171
At time: 53.524370431900024 and batch: 800, loss is 3.8166857957839966 and perplexity is 45.45331684206719
At time: 54.07437062263489 and batch: 850, loss is 3.894567623138428 and perplexity is 49.134804016797446
At time: 54.624165773391724 and batch: 900, loss is 3.8587993574142456 and perplexity is 47.40839664462786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3320074212061215 and perplexity of 76.09689186083276
finished 5 epochs...
Completing Train Step...
At time: 56.05347466468811 and batch: 50, loss is 3.9355876064300537 and perplexity is 51.19222199934482
At time: 56.614996671676636 and batch: 100, loss is 3.8064660930633547 and perplexity is 44.99116301599226
At time: 57.165613889694214 and batch: 150, loss is 3.8151282787323 and perplexity is 45.382577629094996
At time: 57.71522569656372 and batch: 200, loss is 3.7244652795791624 and perplexity is 41.44906315759853
At time: 58.265411376953125 and batch: 250, loss is 3.872733292579651 and perplexity is 48.07360589832742
At time: 58.81393575668335 and batch: 300, loss is 3.8547417020797727 and perplexity is 47.21641946310302
At time: 59.36336421966553 and batch: 350, loss is 3.8433958673477173 and perplexity is 46.6837373471065
At time: 59.91339588165283 and batch: 400, loss is 3.777722725868225 and perplexity is 43.71637412548072
At time: 60.46317791938782 and batch: 450, loss is 3.8275432777404785 and perplexity is 45.94951426230444
At time: 61.014073848724365 and batch: 500, loss is 3.6944302463531495 and perplexity is 40.2226490572228
At time: 61.56442379951477 and batch: 550, loss is 3.7724812698364256 and perplexity is 43.48783613177095
At time: 62.11594867706299 and batch: 600, loss is 3.7896086978912353 and perplexity is 44.2390860409983
At time: 62.66646480560303 and batch: 650, loss is 3.651460886001587 and perplexity is 38.5309142261625
At time: 63.21694612503052 and batch: 700, loss is 3.6512372589111326 and perplexity is 38.522298633297446
At time: 63.766907691955566 and batch: 750, loss is 3.763618860244751 and perplexity is 43.10413189916585
At time: 64.31708812713623 and batch: 800, loss is 3.720919942855835 and perplexity is 41.30237245944923
At time: 64.86752843856812 and batch: 850, loss is 3.8041315031051637 and perplexity is 44.88624961114938
At time: 65.41741299629211 and batch: 900, loss is 3.766923122406006 and perplexity is 43.24679481923709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328178092224957 and perplexity of 75.80604904926119
finished 6 epochs...
Completing Train Step...
At time: 66.84540915489197 and batch: 50, loss is 3.848921060562134 and perplexity is 46.94238790507271
At time: 67.40948128700256 and batch: 100, loss is 3.7208594846725465 and perplexity is 41.299875468527354
At time: 67.96074533462524 and batch: 150, loss is 3.727753119468689 and perplexity is 41.585565316488655
At time: 68.51082682609558 and batch: 200, loss is 3.639325065612793 and perplexity is 38.06613590907916
At time: 69.06091618537903 and batch: 250, loss is 3.7882791423797606 and perplexity is 44.18030680411782
At time: 69.60994911193848 and batch: 300, loss is 3.773127636909485 and perplexity is 43.515954323482795
At time: 70.16091227531433 and batch: 350, loss is 3.761552333831787 and perplexity is 43.0151480474956
At time: 70.72404980659485 and batch: 400, loss is 3.6977220582962036 and perplexity is 40.35527261992512
At time: 71.27470803260803 and batch: 450, loss is 3.744042453765869 and perplexity is 42.26851377857023
At time: 71.8244457244873 and batch: 500, loss is 3.6190140533447264 and perplexity is 37.3007731137339
At time: 72.37458539009094 and batch: 550, loss is 3.694504141807556 and perplexity is 40.225621437973686
At time: 72.92502903938293 and batch: 600, loss is 3.7096541357040405 and perplexity is 40.839679097186156
At time: 73.47478723526001 and batch: 650, loss is 3.57182719707489 and perplexity is 35.581548291594196
At time: 74.02513790130615 and batch: 700, loss is 3.5741771507263183 and perplexity is 35.66526160360307
At time: 74.57634592056274 and batch: 750, loss is 3.690631980895996 and perplexity is 40.07016253418472
At time: 75.12632870674133 and batch: 800, loss is 3.6489134073257445 and perplexity is 38.432882463738764
At time: 75.67641115188599 and batch: 850, loss is 3.7288247299194337 and perplexity is 41.6301527287836
At time: 76.23266887664795 and batch: 900, loss is 3.6949321365356447 and perplexity is 40.242841476663244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3332281243311215 and perplexity of 76.1898402942096
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 77.67751455307007 and batch: 50, loss is 3.801864495277405 and perplexity is 44.78460738731174
At time: 78.22720503807068 and batch: 100, loss is 3.671292238235474 and perplexity is 39.30266146097335
At time: 78.7767584323883 and batch: 150, loss is 3.678047842979431 and perplexity is 39.56907358146851
At time: 79.32648921012878 and batch: 200, loss is 3.570125765800476 and perplexity is 35.52106020529177
At time: 79.87641334533691 and batch: 250, loss is 3.716808853149414 and perplexity is 41.132923250286055
At time: 80.42651414871216 and batch: 300, loss is 3.677145199775696 and perplexity is 39.53337294101631
At time: 80.97449374198914 and batch: 350, loss is 3.663463554382324 and perplexity is 38.99617460947712
At time: 81.52434515953064 and batch: 400, loss is 3.588875651359558 and perplexity is 36.19335908672196
At time: 82.0748119354248 and batch: 450, loss is 3.6243590450286867 and perplexity is 37.50067920811296
At time: 82.62450885772705 and batch: 500, loss is 3.492120451927185 and perplexity is 32.85554249235265
At time: 83.17484760284424 and batch: 550, loss is 3.5478910779953003 and perplexity is 34.73997627892264
At time: 83.72413921356201 and batch: 600, loss is 3.553011207580566 and perplexity is 34.91830560430128
At time: 84.28753924369812 and batch: 650, loss is 3.4055278730392455 and perplexity is 30.130196445138086
At time: 84.8382523059845 and batch: 700, loss is 3.387829442024231 and perplexity is 29.601630433451092
At time: 85.38841009140015 and batch: 750, loss is 3.4952848386764526 and perplexity is 32.95967480621983
At time: 85.93838453292847 and batch: 800, loss is 3.4379897928237915 and perplexity is 31.124328891196615
At time: 86.48762845993042 and batch: 850, loss is 3.501444149017334 and perplexity is 33.163310154962964
At time: 87.03832149505615 and batch: 900, loss is 3.4532064390182495 and perplexity is 31.60155850067319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.28574110057256 and perplexity of 72.65637245137441
finished 8 epochs...
Completing Train Step...
At time: 88.45923471450806 and batch: 50, loss is 3.7119458389282225 and perplexity is 40.93337884644798
At time: 89.02240824699402 and batch: 100, loss is 3.578703022003174 and perplexity is 35.827043813248686
At time: 89.57140564918518 and batch: 150, loss is 3.5856498861312867 and perplexity is 36.07679591128021
At time: 90.12056422233582 and batch: 200, loss is 3.481423659324646 and perplexity is 32.505966572266054
At time: 90.6699435710907 and batch: 250, loss is 3.6310692024230957 and perplexity is 37.7531608163049
At time: 91.21948385238647 and batch: 300, loss is 3.5972328042984008 and perplexity is 36.49709996062806
At time: 91.76930832862854 and batch: 350, loss is 3.588600072860718 and perplexity is 36.18338634935596
At time: 92.31910681724548 and batch: 400, loss is 3.517116322517395 and perplexity is 33.687145402126816
At time: 92.86921954154968 and batch: 450, loss is 3.5578192329406737 and perplexity is 35.086597955923416
At time: 93.41650605201721 and batch: 500, loss is 3.4299947452545165 and perplexity is 30.876480501204295
At time: 93.96549487113953 and batch: 550, loss is 3.4892047595977784 and perplexity is 32.759885360279505
At time: 94.51502823829651 and batch: 600, loss is 3.5007519149780273 and perplexity is 33.14036132671406
At time: 95.06527090072632 and batch: 650, loss is 3.358828740119934 and perplexity is 28.75549099450034
At time: 95.61534786224365 and batch: 700, loss is 3.3458351564407347 and perplexity is 28.384271077394683
At time: 96.1823935508728 and batch: 750, loss is 3.4597778701782227 and perplexity is 31.80990980023806
At time: 96.7452163696289 and batch: 800, loss is 3.409308614730835 and perplexity is 30.244326547283706
At time: 97.29395484924316 and batch: 850, loss is 3.479766058921814 and perplexity is 32.452129301649265
At time: 97.84341287612915 and batch: 900, loss is 3.43732195854187 and perplexity is 31.103549936582688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2899190824325775 and perplexity of 72.9605644693715
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 99.27624297142029 and batch: 50, loss is 3.6891723680496216 and perplexity is 40.011718273566444
At time: 99.8394079208374 and batch: 100, loss is 3.565524487495422 and perplexity is 35.357993367249655
At time: 100.3893620967865 and batch: 150, loss is 3.5743512392044066 and perplexity is 35.67147105519757
At time: 100.93916749954224 and batch: 200, loss is 3.4644435977935792 and perplexity is 31.958672949132175
At time: 101.48809957504272 and batch: 250, loss is 3.6156356954574584 and perplexity is 37.17497037567214
At time: 102.03828263282776 and batch: 300, loss is 3.57311448097229 and perplexity is 35.627381339497624
At time: 102.58894395828247 and batch: 350, loss is 3.5647698736190794 and perplexity is 35.33132179944833
At time: 103.13824081420898 and batch: 400, loss is 3.489196095466614 and perplexity is 32.7596015255654
At time: 103.6882836818695 and batch: 450, loss is 3.5245499277114867 and perplexity is 33.93849540206073
At time: 104.23784685134888 and batch: 500, loss is 3.39522620677948 and perplexity is 29.821398514540025
At time: 104.78783512115479 and batch: 550, loss is 3.450632305145264 and perplexity is 31.52031646729274
At time: 105.33412432670593 and batch: 600, loss is 3.458545198440552 and perplexity is 31.770722780777128
At time: 105.87975597381592 and batch: 650, loss is 3.3104410934448243 and perplexity is 27.39720753569824
At time: 106.42614698410034 and batch: 700, loss is 3.2902829360961916 and perplexity is 26.85045954547697
At time: 106.97203063964844 and batch: 750, loss is 3.4032896900177003 and perplexity is 30.062834962802192
At time: 107.51823806762695 and batch: 800, loss is 3.346755819320679 and perplexity is 28.41041545538237
At time: 108.063649892807 and batch: 850, loss is 3.4117184591293337 and perplexity is 30.317298558481763
At time: 108.61011934280396 and batch: 900, loss is 3.3709725284576417 and perplexity is 29.106820508730163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.276067028306935 and perplexity of 71.95687838711189
finished 10 epochs...
Completing Train Step...
At time: 110.02884554862976 and batch: 50, loss is 3.6598399305343627 and perplexity is 38.85512285477575
At time: 110.58053231239319 and batch: 100, loss is 3.531451606750488 and perplexity is 34.17353816626655
At time: 111.12914752960205 and batch: 150, loss is 3.5396737003326417 and perplexity is 34.45567448126282
At time: 111.67807078361511 and batch: 200, loss is 3.4326531171798704 and perplexity is 30.95867086800138
At time: 112.25363850593567 and batch: 250, loss is 3.5847748804092405 and perplexity is 36.04524231522651
At time: 112.83996248245239 and batch: 300, loss is 3.5452006816864015 and perplexity is 34.64663759030285
At time: 113.39229130744934 and batch: 350, loss is 3.5381907844543456 and perplexity is 34.40461748045211
At time: 113.93990516662598 and batch: 400, loss is 3.4634005546569826 and perplexity is 31.92535605315825
At time: 114.49887156486511 and batch: 450, loss is 3.501357145309448 and perplexity is 33.16042494952732
At time: 115.0658791065216 and batch: 500, loss is 3.3742546939849856 and perplexity is 29.202510861500187
At time: 115.61328434944153 and batch: 550, loss is 3.4318939733505247 and perplexity is 30.93517770252125
At time: 116.1601722240448 and batch: 600, loss is 3.442862548828125 and perplexity is 31.276360256740574
At time: 116.70829582214355 and batch: 650, loss is 3.2974480485916136 and perplexity is 27.04353699346139
At time: 117.25507640838623 and batch: 700, loss is 3.280153980255127 and perplexity is 26.57986515920537
At time: 117.81999063491821 and batch: 750, loss is 3.3962854099273683 and perplexity is 29.85300216811103
At time: 118.37771987915039 and batch: 800, loss is 3.3425299882888795 and perplexity is 28.290611154775526
At time: 118.92509531974792 and batch: 850, loss is 3.4108594179153444 and perplexity is 30.29126593267335
At time: 119.47204971313477 and batch: 900, loss is 3.372774877548218 and perplexity is 29.159328464822913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.276970432229238 and perplexity of 72.02191388552248
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 120.88739705085754 and batch: 50, loss is 3.6522410345077514 and perplexity is 38.56098578995476
At time: 121.44716620445251 and batch: 100, loss is 3.5294318628311157 and perplexity is 34.104586026624254
At time: 121.99359393119812 and batch: 150, loss is 3.5398415613174437 and perplexity is 34.461458730174535
At time: 122.54087328910828 and batch: 200, loss is 3.4313235759735106 and perplexity is 30.917537389774605
At time: 123.12270855903625 and batch: 250, loss is 3.583829107284546 and perplexity is 36.01116780968738
At time: 123.6729576587677 and batch: 300, loss is 3.5456645345687865 and perplexity is 34.66271226086698
At time: 124.21992564201355 and batch: 350, loss is 3.5383645057678224 and perplexity is 34.41059481497065
At time: 124.76683068275452 and batch: 400, loss is 3.462479772567749 and perplexity is 31.895973286750603
At time: 125.3142158985138 and batch: 450, loss is 3.492974529266357 and perplexity is 32.88361565328038
At time: 125.86126732826233 and batch: 500, loss is 3.3638977241516113 and perplexity is 28.901622174585317
At time: 126.45484161376953 and batch: 550, loss is 3.4191751623153688 and perplexity is 30.544210611746024
At time: 127.0055160522461 and batch: 600, loss is 3.4299170446395872 and perplexity is 30.874081472886694
At time: 127.55230665206909 and batch: 650, loss is 3.2824983406066894 and perplexity is 26.642251040154513
At time: 128.09821319580078 and batch: 700, loss is 3.263493332862854 and perplexity is 26.140695978054634
At time: 128.64511394500732 and batch: 750, loss is 3.378778533935547 and perplexity is 29.334917614461514
At time: 129.19093942642212 and batch: 800, loss is 3.3231944179534914 and perplexity is 27.7488505566755
At time: 129.7915804386139 and batch: 850, loss is 3.387058310508728 and perplexity is 29.578812482264944
At time: 130.3835952281952 and batch: 900, loss is 3.350646901130676 and perplexity is 28.5211780596023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273913657828553 and perplexity of 71.80209528197194
finished 12 epochs...
Completing Train Step...
At time: 131.85308241844177 and batch: 50, loss is 3.6416414260864256 and perplexity is 38.15441300299734
At time: 132.45629215240479 and batch: 100, loss is 3.515145673751831 and perplexity is 33.6208252389612
At time: 133.00540280342102 and batch: 150, loss is 3.526524987220764 and perplexity is 34.00559218839682
At time: 133.5527286529541 and batch: 200, loss is 3.4188724660873415 and perplexity is 30.53496639357135
At time: 134.1024181842804 and batch: 250, loss is 3.571980218887329 and perplexity is 35.5869934612074
At time: 134.64987063407898 and batch: 300, loss is 3.5343317651748656 and perplexity is 34.272105246372924
At time: 135.1967010498047 and batch: 350, loss is 3.527502598762512 and perplexity is 34.03885270308671
At time: 135.74374842643738 and batch: 400, loss is 3.4526400852203367 and perplexity is 31.58366590524432
At time: 136.29385328292847 and batch: 450, loss is 3.484800753593445 and perplexity is 32.61592785598267
At time: 136.84142398834229 and batch: 500, loss is 3.357168445587158 and perplexity is 28.70778802146562
At time: 137.38781332969666 and batch: 550, loss is 3.41314067363739 and perplexity is 30.360446936188104
At time: 137.93474912643433 and batch: 600, loss is 3.4259284925460816 and perplexity is 30.75118384531467
At time: 138.48059511184692 and batch: 650, loss is 3.2793801259994506 and perplexity is 26.559304174065613
At time: 139.02698612213135 and batch: 700, loss is 3.2613234424591067 and perplexity is 26.084035028961406
At time: 139.57412123680115 and batch: 750, loss is 3.3779587316513062 and perplexity is 29.31087863694384
At time: 140.1441249847412 and batch: 800, loss is 3.3237150526046753 and perplexity is 27.763301331266405
At time: 140.69060397148132 and batch: 850, loss is 3.389188461303711 and perplexity is 29.641886968421847
At time: 141.26044464111328 and batch: 900, loss is 3.354057731628418 and perplexity is 28.618625056681356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2736444342626285 and perplexity of 71.78276706776143
finished 13 epochs...
Completing Train Step...
At time: 142.75022768974304 and batch: 50, loss is 3.6361023569107056 and perplexity is 37.943657304066924
At time: 143.3010585308075 and batch: 100, loss is 3.5088601875305176 and perplexity is 33.4101647505237
At time: 143.84851789474487 and batch: 150, loss is 3.5200142240524293 and perplexity is 33.784909018749026
At time: 144.39655780792236 and batch: 200, loss is 3.412559690475464 and perplexity is 30.34281315069186
At time: 144.9428882598877 and batch: 250, loss is 3.5655365324020387 and perplexity is 35.358419253542785
At time: 145.48963928222656 and batch: 300, loss is 3.528228621482849 and perplexity is 34.06357465678746
At time: 146.03605961799622 and batch: 350, loss is 3.5217194604873656 and perplexity is 33.84256942489929
At time: 146.58312678337097 and batch: 400, loss is 3.4472349309921264 and perplexity is 31.413411859130104
At time: 147.13090085983276 and batch: 450, loss is 3.4799901342391966 and perplexity is 32.45940183758975
At time: 147.68060398101807 and batch: 500, loss is 3.3529620504379274 and perplexity is 28.587285339813516
At time: 148.22977089881897 and batch: 550, loss is 3.409416618347168 and perplexity is 30.247593220327445
At time: 148.7783751487732 and batch: 600, loss is 3.423064737319946 and perplexity is 30.663245958258514
At time: 149.32289147377014 and batch: 650, loss is 3.2770153760910032 and perplexity is 26.49657226382435
At time: 149.87039828300476 and batch: 700, loss is 3.2595714998245238 and perplexity is 26.03837730244917
At time: 150.41578221321106 and batch: 750, loss is 3.3768511629104614 and perplexity is 29.27843279531564
At time: 150.96275281906128 and batch: 800, loss is 3.323177390098572 and perplexity is 27.748378057296875
At time: 151.51097321510315 and batch: 850, loss is 3.389437508583069 and perplexity is 29.649270119064994
At time: 152.05913066864014 and batch: 900, loss is 3.354816393852234 and perplexity is 28.640345164480276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273887320740582 and perplexity of 71.80020424877428
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 153.63193225860596 and batch: 50, loss is 3.633780822753906 and perplexity is 37.85567197758856
At time: 154.21622014045715 and batch: 100, loss is 3.507774577140808 and perplexity is 33.3739140091996
At time: 154.7643847465515 and batch: 150, loss is 3.5202628135681153 and perplexity is 33.79330863690475
At time: 155.33458924293518 and batch: 200, loss is 3.4121899509429934 and perplexity is 30.331596286930452
At time: 155.89485144615173 and batch: 250, loss is 3.564936466217041 and perplexity is 35.33720822644028
At time: 156.44217944145203 and batch: 300, loss is 3.5287801933288576 and perplexity is 34.08236834810159
At time: 156.99082899093628 and batch: 350, loss is 3.522779355049133 and perplexity is 33.87845799587989
At time: 157.53808760643005 and batch: 400, loss is 3.4481028175354003 and perplexity is 31.44068697070008
At time: 158.0855860710144 and batch: 450, loss is 3.4776608180999755 and perplexity is 32.38388161839861
At time: 158.6329734325409 and batch: 500, loss is 3.3506850528717043 and perplexity is 28.52226621295879
At time: 159.18107342720032 and batch: 550, loss is 3.40606360912323 and perplexity is 30.146342603244868
At time: 159.72891116142273 and batch: 600, loss is 3.4185227060317995 and perplexity is 30.524288349515054
At time: 160.29885911941528 and batch: 650, loss is 3.2711283826828 and perplexity is 26.341045359667426
At time: 160.84716725349426 and batch: 700, loss is 3.2544064331054687 and perplexity is 25.90423407401015
At time: 161.39471578598022 and batch: 750, loss is 3.369253993034363 and perplexity is 29.056842363533647
At time: 161.9429566860199 and batch: 800, loss is 3.3150884771347044 and perplexity is 27.524829194857034
At time: 162.49085974693298 and batch: 850, loss is 3.3799901819229126 and perplexity is 29.370482750210115
At time: 163.03933715820312 and batch: 900, loss is 3.344437184333801 and perplexity is 28.344618381292264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274014407641267 and perplexity of 71.80932969405079
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 164.48691487312317 and batch: 50, loss is 3.6319022607803344 and perplexity is 37.78462450616038
At time: 165.05892300605774 and batch: 100, loss is 3.505804533958435 and perplexity is 33.308230678179385
At time: 165.6085822582245 and batch: 150, loss is 3.5189805746078493 and perplexity is 33.750005308514645
At time: 166.15716552734375 and batch: 200, loss is 3.4106864166259765 and perplexity is 30.28602595788468
At time: 166.703209400177 and batch: 250, loss is 3.563139786720276 and perplexity is 35.27377559006921
At time: 167.25105953216553 and batch: 300, loss is 3.5270022106170655 and perplexity is 34.02182432546394
At time: 167.7998399734497 and batch: 350, loss is 3.5211634254455566 and perplexity is 33.823757001062326
At time: 168.3665337562561 and batch: 400, loss is 3.446250329017639 and perplexity is 31.382497373517484
At time: 168.91135478019714 and batch: 450, loss is 3.4759255838394165 and perplexity is 32.327736723884605
At time: 169.45662999153137 and batch: 500, loss is 3.349006161689758 and perplexity is 28.474420606746353
At time: 170.0064685344696 and batch: 550, loss is 3.4044147634506228 and perplexity is 30.096676893496014
At time: 170.56307435035706 and batch: 600, loss is 3.4166286611557006 and perplexity is 30.466528694526655
At time: 171.1112380027771 and batch: 650, loss is 3.269142689704895 and perplexity is 26.288792027572832
At time: 171.68766355514526 and batch: 700, loss is 3.2529234504699707 and perplexity is 25.86584701539838
At time: 172.24300050735474 and batch: 750, loss is 3.3669697713851927 and perplexity is 28.990545841951377
At time: 172.79165935516357 and batch: 800, loss is 3.312787766456604 and perplexity is 27.461575318751283
At time: 173.33980226516724 and batch: 850, loss is 3.3773484325408933 and perplexity is 29.29299569131426
At time: 173.88847494125366 and batch: 900, loss is 3.341559591293335 and perplexity is 28.26317134661578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27390487879923 and perplexity of 71.80146493203895
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 175.35463905334473 and batch: 50, loss is 3.6314260530471802 and perplexity is 37.76663545937777
At time: 175.9068922996521 and batch: 100, loss is 3.5053202438354494 and perplexity is 33.292103736422305
At time: 176.4552707672119 and batch: 150, loss is 3.5186308336257937 and perplexity is 33.73820361240235
At time: 177.00277256965637 and batch: 200, loss is 3.4102638244628904 and perplexity is 30.27323002458485
At time: 177.5513322353363 and batch: 250, loss is 3.5626881647109987 and perplexity is 35.25784877338361
At time: 178.0998272895813 and batch: 300, loss is 3.526542148590088 and perplexity is 34.006175775931005
At time: 178.6485240459442 and batch: 350, loss is 3.520678129196167 and perplexity is 33.807346440963144
At time: 179.19652009010315 and batch: 400, loss is 3.4457978343963624 and perplexity is 31.368300174574063
At time: 179.74435567855835 and batch: 450, loss is 3.4754647827148437 and perplexity is 32.31284349811847
At time: 180.29237461090088 and batch: 500, loss is 3.3485057020187377 and perplexity is 28.460173872832236
At time: 180.84214878082275 and batch: 550, loss is 3.4039070415496826 and perplexity is 30.081400030018923
At time: 181.39034748077393 and batch: 600, loss is 3.4160861206054687 and perplexity is 30.450003850389425
At time: 181.97286224365234 and batch: 650, loss is 3.2685961961746215 and perplexity is 26.27442929773798
At time: 182.52793836593628 and batch: 700, loss is 3.252507457733154 and perplexity is 25.855089248639917
At time: 183.07617092132568 and batch: 750, loss is 3.3663932609558107 and perplexity is 28.973837306705207
At time: 183.62414026260376 and batch: 800, loss is 3.3122044229507446 and perplexity is 27.445560458664872
At time: 184.17121386528015 and batch: 850, loss is 3.3766831445693968 and perplexity is 29.27351389485272
At time: 184.72031235694885 and batch: 900, loss is 3.3408283805847168 and perplexity is 28.242512566948555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273865582191781 and perplexity of 71.79864343349527
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 186.15262842178345 and batch: 50, loss is 3.6313034534454345 and perplexity is 37.76200556872834
At time: 186.7157702445984 and batch: 100, loss is 3.5051968717575073 and perplexity is 33.28799667375892
At time: 187.26237964630127 and batch: 150, loss is 3.518539323806763 and perplexity is 33.7351163767537
At time: 187.8124463558197 and batch: 200, loss is 3.410154633522034 and perplexity is 30.26992464257787
At time: 188.36213994026184 and batch: 250, loss is 3.5625714015960694 and perplexity is 35.25373219747191
At time: 188.9123752117157 and batch: 300, loss is 3.5264241695404053 and perplexity is 34.00216399628725
At time: 189.4628119468689 and batch: 350, loss is 3.5205549812316894 and perplexity is 33.80318339140536
At time: 190.01265287399292 and batch: 400, loss is 3.4456846475601197 and perplexity is 31.364749896845133
At time: 190.5623219013214 and batch: 450, loss is 3.4753489971160887 and perplexity is 32.309102352776115
At time: 191.11262893676758 and batch: 500, loss is 3.3483780813217163 and perplexity is 28.456541997361406
At time: 191.66256737709045 and batch: 550, loss is 3.403775701522827 and perplexity is 30.077449397574888
At time: 192.21240663528442 and batch: 600, loss is 3.415946478843689 and perplexity is 30.445752055076564
At time: 192.76241731643677 and batch: 650, loss is 3.268456468582153 and perplexity is 26.270758291465135
At time: 193.31232476234436 and batch: 700, loss is 3.2523998069763183 and perplexity is 25.85230607852212
At time: 193.86210942268372 and batch: 750, loss is 3.366248173713684 and perplexity is 28.969633877495898
At time: 194.41070318222046 and batch: 800, loss is 3.312057089805603 and perplexity is 27.441517115788855
At time: 194.95944356918335 and batch: 850, loss is 3.3765158081054687 and perplexity is 29.26861577837898
At time: 195.5085644721985 and batch: 900, loss is 3.3406440591812134 and perplexity is 28.237307347125203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273855967064426 and perplexity of 71.79795308371367
Annealing...
Model not improving. Stopping early with 71.78276706776143 lossat 17 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8858900b70>
ELAPSED
430.80848956108093


RESULTS SO FAR:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.78276706776143, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.22372141185209682, 'rnn_dropout': 0.5465833484205133, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8543455600738525 and batch: 50, loss is 6.867841053009033 and perplexity is 960.8718534659716
At time: 1.4861693382263184 and batch: 100, loss is 6.006622953414917 and perplexity is 406.10955106558816
At time: 2.1054847240448 and batch: 150, loss is 5.76515760421753 and perplexity is 318.9893141360081
At time: 2.7385993003845215 and batch: 200, loss is 5.518526830673218 and perplexity is 249.26755325950316
At time: 3.3570611476898193 and batch: 250, loss is 5.517188224792481 and perplexity is 248.9341054742073
At time: 3.975931406021118 and batch: 300, loss is 5.404811153411865 and perplexity is 222.47420302232473
At time: 4.595499515533447 and batch: 350, loss is 5.357544317245483 and perplexity is 212.20320234828375
At time: 5.217243671417236 and batch: 400, loss is 5.189905710220337 and perplexity is 179.4516316792832
At time: 5.8375020027160645 and batch: 450, loss is 5.175224046707154 and perplexity is 176.83622941335167
At time: 6.459162712097168 and batch: 500, loss is 5.097729043960571 and perplexity is 163.64984338921488
At time: 7.078696012496948 and batch: 550, loss is 5.1457914447784425 and perplexity is 171.70732777580184
At time: 7.701403617858887 and batch: 600, loss is 5.063163404464722 and perplexity is 158.08982821264223
At time: 8.320740699768066 and batch: 650, loss is 4.943384256362915 and perplexity is 140.24406922780656
At time: 8.950493812561035 and batch: 700, loss is 5.020713443756104 and perplexity is 151.5193657868415
At time: 9.575957775115967 and batch: 750, loss is 5.0129975414276124 and perplexity is 150.35475594537834
At time: 10.198145627975464 and batch: 800, loss is 4.975107345581055 and perplexity is 144.764364128929
At time: 10.819589614868164 and batch: 850, loss is 5.011115713119507 and perplexity is 150.07208016644287
At time: 11.440591812133789 and batch: 900, loss is 4.919451951980591 and perplexity is 136.92754974672184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.872335773624786 and perplexity of 130.6256728387913
finished 1 epochs...
Completing Train Step...
At time: 12.939697504043579 and batch: 50, loss is 4.835392837524414 and perplexity is 125.88802703543536
At time: 13.491247415542603 and batch: 100, loss is 4.701184043884277 and perplexity is 110.07743183045093
At time: 14.040677070617676 and batch: 150, loss is 4.677959280014038 and perplexity is 107.55036828412773
At time: 14.590555667877197 and batch: 200, loss is 4.567096757888794 and perplexity is 96.264225330504
At time: 15.140186071395874 and batch: 250, loss is 4.677119197845459 and perplexity is 107.46005507807659
At time: 15.689147233963013 and batch: 300, loss is 4.624426603317261 and perplexity is 101.94430179620674
At time: 16.238125801086426 and batch: 350, loss is 4.604425287246704 and perplexity is 99.92553786268051
At time: 16.78735876083374 and batch: 400, loss is 4.483530521392822 and perplexity is 88.54673762696831
At time: 17.360140085220337 and batch: 450, loss is 4.506738300323486 and perplexity is 90.62574196338346
At time: 17.909385919570923 and batch: 500, loss is 4.402076358795166 and perplexity is 81.62016559225286
At time: 18.45936369895935 and batch: 550, loss is 4.4739912986755375 and perplexity is 87.70608652957999
At time: 19.00926685333252 and batch: 600, loss is 4.448646335601807 and perplexity is 85.51111227291035
At time: 19.558234453201294 and batch: 650, loss is 4.298673410415649 and perplexity is 73.60208914215791
At time: 20.10928177833557 and batch: 700, loss is 4.336631989479065 and perplexity is 76.4496221165342
At time: 20.658896684646606 and batch: 750, loss is 4.394255456924438 and perplexity is 80.98431200182596
At time: 21.20760679244995 and batch: 800, loss is 4.34593900680542 and perplexity is 77.16446142510631
At time: 21.757427215576172 and batch: 850, loss is 4.414898118972778 and perplexity is 82.67341762431771
At time: 22.307180404663086 and batch: 900, loss is 4.350847330093384 and perplexity is 77.54414057988362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5130677941727315 and perplexity of 91.20117622241597
finished 2 epochs...
Completing Train Step...
At time: 23.743441104888916 and batch: 50, loss is 4.402423734664917 and perplexity is 81.64852339338744
At time: 24.305835247039795 and batch: 100, loss is 4.2755688238143925 and perplexity is 71.92103807567351
At time: 24.85543656349182 and batch: 150, loss is 4.266876301765442 and perplexity is 71.29857218510867
At time: 25.40597105026245 and batch: 200, loss is 4.1712294816970825 and perplexity is 64.79506750409497
At time: 25.956579446792603 and batch: 250, loss is 4.312449045181275 and perplexity is 74.62302048384241
At time: 26.507558584213257 and batch: 300, loss is 4.279770617485046 and perplexity is 72.22387121388353
At time: 27.05735683441162 and batch: 350, loss is 4.267891330718994 and perplexity is 71.37097904153471
At time: 27.60746932029724 and batch: 400, loss is 4.178712282180786 and perplexity is 65.28173461219328
At time: 28.15761423110962 and batch: 450, loss is 4.212417979240417 and perplexity is 67.51960358805103
At time: 28.707470893859863 and batch: 500, loss is 4.090139293670655 and perplexity is 59.74821367253049
At time: 29.257447719573975 and batch: 550, loss is 4.175207710266113 and perplexity is 65.05335050654298
At time: 29.8078191280365 and batch: 600, loss is 4.176467566490174 and perplexity is 65.13536002432798
At time: 30.358445644378662 and batch: 650, loss is 4.021146864891052 and perplexity is 55.765024115275644
At time: 30.908596754074097 and batch: 700, loss is 4.041418142318726 and perplexity is 56.90698781655577
At time: 31.47440767288208 and batch: 750, loss is 4.133440580368042 and perplexity is 62.392219516645774
At time: 32.02433109283447 and batch: 800, loss is 4.089919075965882 and perplexity is 59.73505750671473
At time: 32.573625564575195 and batch: 850, loss is 4.165231161117553 and perplexity is 64.40756924845724
At time: 33.12223553657532 and batch: 900, loss is 4.112069401741028 and perplexity is 61.07297140296596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393448503050085 and perplexity of 80.91898775786495
finished 3 epochs...
Completing Train Step...
At time: 34.55594348907471 and batch: 50, loss is 4.188026928901673 and perplexity is 65.89265172893266
At time: 35.11956214904785 and batch: 100, loss is 4.061434650421143 and perplexity is 58.057543639302146
At time: 35.701051473617554 and batch: 150, loss is 4.059184718132019 and perplexity is 57.92706493640657
At time: 36.25797724723816 and batch: 200, loss is 3.965956382751465 and perplexity is 52.77071426440319
At time: 36.80820274353027 and batch: 250, loss is 4.112832074165344 and perplexity is 61.119567840776504
At time: 37.35843539237976 and batch: 300, loss is 4.088158740997314 and perplexity is 59.62999629472904
At time: 37.90785193443298 and batch: 350, loss is 4.077949571609497 and perplexity is 59.02432054576397
At time: 38.45838642120361 and batch: 400, loss is 4.00311354637146 and perplexity is 54.768408821842115
At time: 39.02345108985901 and batch: 450, loss is 4.037432713508606 and perplexity is 56.68064041314754
At time: 39.597477436065674 and batch: 500, loss is 3.9129625082015993 and perplexity is 50.04699721222138
At time: 40.14725065231323 and batch: 550, loss is 3.99970290184021 and perplexity is 54.581931432618944
At time: 40.69739270210266 and batch: 600, loss is 4.010517144203186 and perplexity is 55.1753968233588
At time: 41.24726438522339 and batch: 650, loss is 3.8541851568222047 and perplexity is 47.19014869988393
At time: 41.79568886756897 and batch: 700, loss is 3.866950011253357 and perplexity is 47.79638510663
At time: 42.34539532661438 and batch: 750, loss is 3.9734314346313475 and perplexity is 53.16665609046505
At time: 42.89545559883118 and batch: 800, loss is 3.9334494924545287 and perplexity is 51.08288412412283
At time: 43.44466543197632 and batch: 850, loss is 4.0072861003875735 and perplexity is 54.997410394496704
At time: 43.99414587020874 and batch: 900, loss is 3.9585702180862428 and perplexity is 52.38237700630242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350138625053511 and perplexity of 77.48920412581234
finished 4 epochs...
Completing Train Step...
At time: 45.43813943862915 and batch: 50, loss is 4.043880982398987 and perplexity is 57.04731335574875
At time: 45.985204458236694 and batch: 100, loss is 3.9193385648727417 and perplexity is 50.367119176036724
At time: 46.53343343734741 and batch: 150, loss is 3.9221627378463744 and perplexity is 50.50956568488479
At time: 47.08266282081604 and batch: 200, loss is 3.828532824516296 and perplexity is 45.995005960362526
At time: 47.63253664970398 and batch: 250, loss is 3.9766457366943357 and perplexity is 53.33782472930085
At time: 48.18239140510559 and batch: 300, loss is 3.9563046884536743 and perplexity is 52.26383750704593
At time: 48.73294925689697 and batch: 350, loss is 3.9466603994369507 and perplexity is 51.762212748181575
At time: 49.28204941749573 and batch: 400, loss is 3.8765186738967894 and perplexity is 48.2559276889834
At time: 49.831262826919556 and batch: 450, loss is 3.9129407835006713 and perplexity is 50.04590996798466
At time: 50.37990665435791 and batch: 500, loss is 3.791520915031433 and perplexity is 44.323761712926284
At time: 50.92737102508545 and batch: 550, loss is 3.8750516843795775 and perplexity is 48.18518864832209
At time: 51.47713255882263 and batch: 600, loss is 3.889678235054016 and perplexity is 48.89515124663761
At time: 52.02648711204529 and batch: 650, loss is 3.7383732557296754 and perplexity is 42.029563172402725
At time: 52.57548713684082 and batch: 700, loss is 3.7450700330734255 and perplexity is 42.31197035240439
At time: 53.124414682388306 and batch: 750, loss is 3.855387978553772 and perplexity is 47.24694418683148
At time: 53.67558717727661 and batch: 800, loss is 3.819431018829346 and perplexity is 45.578267765555175
At time: 54.226109743118286 and batch: 850, loss is 3.8916511821746824 and perplexity is 48.99171401980413
At time: 54.7743079662323 and batch: 900, loss is 3.8448724699020387 and perplexity is 46.752721591540094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336115588880565 and perplexity of 76.41015367755801
finished 5 epochs...
Completing Train Step...
At time: 56.20530438423157 and batch: 50, loss is 3.932841181755066 and perplexity is 51.05181930863847
At time: 56.766114950180054 and batch: 100, loss is 3.8120471620559693 and perplexity is 45.24296380603577
At time: 57.31409239768982 and batch: 150, loss is 3.8203583717346192 and perplexity is 45.62055450892156
At time: 57.864277601242065 and batch: 200, loss is 3.7229922103881834 and perplexity is 41.38805076843423
At time: 58.41323494911194 and batch: 250, loss is 3.875120882987976 and perplexity is 48.188523111690756
At time: 58.97468590736389 and batch: 300, loss is 3.8546000337600708 and perplexity is 47.209730866087675
At time: 59.52415871620178 and batch: 350, loss is 3.8493562269210817 and perplexity is 46.96282009847685
At time: 60.07115197181702 and batch: 400, loss is 3.7820873355865476 and perplexity is 43.90759603778523
At time: 60.62134790420532 and batch: 450, loss is 3.815267343521118 and perplexity is 45.38888918651654
At time: 61.169968128204346 and batch: 500, loss is 3.69923433303833 and perplexity is 40.41634704843803
At time: 61.718486070632935 and batch: 550, loss is 3.7811467170715334 and perplexity is 43.86631515782598
At time: 62.268335819244385 and batch: 600, loss is 3.798792853355408 and perplexity is 44.647256164679625
At time: 62.816370248794556 and batch: 650, loss is 3.6470067405700686 and perplexity is 38.35967357907215
At time: 63.36462736129761 and batch: 700, loss is 3.65097044467926 and perplexity is 38.51202170685372
At time: 63.91309928894043 and batch: 750, loss is 3.760925965309143 and perplexity is 42.98821314922863
At time: 64.46141648292542 and batch: 800, loss is 3.728429980278015 and perplexity is 41.61372248405167
At time: 65.01014876365662 and batch: 850, loss is 3.802841167449951 and perplexity is 44.82836863382256
At time: 65.55901026725769 and batch: 900, loss is 3.758017168045044 and perplexity is 42.863350840042685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335704646698416 and perplexity of 76.37875997320792
finished 6 epochs...
Completing Train Step...
At time: 67.00435137748718 and batch: 50, loss is 3.8465858554840087 and perplexity is 46.832895695596
At time: 67.5663902759552 and batch: 100, loss is 3.7314035654067994 and perplexity is 41.73764859148365
At time: 68.1135790348053 and batch: 150, loss is 3.7377271366119387 and perplexity is 42.00241583927681
At time: 68.66128516197205 and batch: 200, loss is 3.6409733200073244 and perplexity is 38.12893032124068
At time: 69.20937657356262 and batch: 250, loss is 3.7951754474639894 and perplexity is 44.486040683909216
At time: 69.75927090644836 and batch: 300, loss is 3.7725026607513428 and perplexity is 43.488766386323036
At time: 70.30944204330444 and batch: 350, loss is 3.7692937898635863 and perplexity is 43.349440209329885
At time: 70.85909819602966 and batch: 400, loss is 3.702714829444885 and perplexity is 40.55726108224937
At time: 71.40924763679504 and batch: 450, loss is 3.739333610534668 and perplexity is 42.06994585309412
At time: 71.96115207672119 and batch: 500, loss is 3.6251810216903686 and perplexity is 37.53151656327612
At time: 72.51236295700073 and batch: 550, loss is 3.704702796936035 and perplexity is 40.63796779339049
At time: 73.07498240470886 and batch: 600, loss is 3.719527869224548 and perplexity is 41.24491651656887
At time: 73.63242483139038 and batch: 650, loss is 3.5758654499053955 and perplexity is 35.72552609341975
At time: 74.18224287033081 and batch: 700, loss is 3.5790434312820434 and perplexity is 35.83924174742438
At time: 74.73245692253113 and batch: 750, loss is 3.68519832611084 and perplexity is 39.85302556170363
At time: 75.2826280593872 and batch: 800, loss is 3.656061205863953 and perplexity is 38.7085770958841
At time: 75.83227038383484 and batch: 850, loss is 3.7290070486068725 and perplexity is 41.63774337552434
At time: 76.38225293159485 and batch: 900, loss is 3.686607518196106 and perplexity is 39.90922571900723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344834419145976 and perplexity of 77.07927357065648
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 77.82893753051758 and batch: 50, loss is 3.803800272941589 and perplexity is 44.871384393388595
At time: 78.37816953659058 and batch: 100, loss is 3.6756004095077515 and perplexity is 39.47234931772314
At time: 78.92865252494812 and batch: 150, loss is 3.686108260154724 and perplexity is 39.88930569017275
At time: 79.47842049598694 and batch: 200, loss is 3.5692888498306274 and perplexity is 35.49134449924896
At time: 80.02798843383789 and batch: 250, loss is 3.716721453666687 and perplexity is 41.12932841116675
At time: 80.57715630531311 and batch: 300, loss is 3.682624659538269 and perplexity is 39.750589037271936
At time: 81.12499809265137 and batch: 350, loss is 3.6704380559921264 and perplexity is 39.26910415950297
At time: 81.67394304275513 and batch: 400, loss is 3.593366560935974 and perplexity is 36.35626571532111
At time: 82.22441673278809 and batch: 450, loss is 3.6202123260498045 and perplexity is 37.34549640203086
At time: 82.7742691040039 and batch: 500, loss is 3.4934527254104615 and perplexity is 32.899344231863076
At time: 83.3235912322998 and batch: 550, loss is 3.559482202529907 and perplexity is 35.14499444364758
At time: 83.87329077720642 and batch: 600, loss is 3.562668619155884 and perplexity is 35.25715964589188
At time: 84.42339372634888 and batch: 650, loss is 3.4065402221679686 and perplexity is 30.16071416794628
At time: 84.9731912612915 and batch: 700, loss is 3.3934481716156006 and perplexity is 29.768422130238857
At time: 85.52187991142273 and batch: 750, loss is 3.485997714996338 and perplexity is 32.65499123676146
At time: 86.07014012336731 and batch: 800, loss is 3.443991432189941 and perplexity is 31.311687555901003
At time: 86.63145422935486 and batch: 850, loss is 3.5018343782424926 and perplexity is 33.17625397315225
At time: 87.18041586875916 and batch: 900, loss is 3.4489255475997926 and perplexity is 31.46656481289572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.290968803510274 and perplexity of 73.03719292395108
finished 8 epochs...
Completing Train Step...
At time: 88.61254477500916 and batch: 50, loss is 3.707878737449646 and perplexity is 40.767236728263065
At time: 89.17390489578247 and batch: 100, loss is 3.5856654405593873 and perplexity is 36.07735706957255
At time: 89.72289776802063 and batch: 150, loss is 3.594611792564392 and perplexity is 36.401565886030106
At time: 90.27197766304016 and batch: 200, loss is 3.4820163249969482 and perplexity is 32.52523745282906
At time: 90.82113289833069 and batch: 250, loss is 3.633574628829956 and perplexity is 37.847867172719056
At time: 91.37142395973206 and batch: 300, loss is 3.6057076930999754 and perplexity is 36.80772321396519
At time: 91.92110800743103 and batch: 350, loss is 3.5958073997497557 and perplexity is 36.44511388772624
At time: 92.47138929367065 and batch: 400, loss is 3.5238016986846925 and perplexity is 33.913111132483124
At time: 93.01949405670166 and batch: 450, loss is 3.5558861827850343 and perplexity is 35.01883931381008
At time: 93.56743717193604 and batch: 500, loss is 3.433730893135071 and perplexity is 30.99205536634105
At time: 94.11626648902893 and batch: 550, loss is 3.5019199514389037 and perplexity is 33.179093092724216
At time: 94.6655342578888 and batch: 600, loss is 3.5121424055099486 and perplexity is 33.52000435411642
At time: 95.21455335617065 and batch: 650, loss is 3.362155361175537 and perplexity is 28.851308902879286
At time: 95.76420092582703 and batch: 700, loss is 3.3539555215835573 and perplexity is 28.61570009521323
At time: 96.3139865398407 and batch: 750, loss is 3.451852226257324 and perplexity is 31.55879223073642
At time: 96.86426162719727 and batch: 800, loss is 3.414359016418457 and perplexity is 30.397458909599827
At time: 97.41335082054138 and batch: 850, loss is 3.4799385595321657 and perplexity is 32.45772779661904
At time: 97.96342420578003 and batch: 900, loss is 3.4343851232528686 and perplexity is 31.012337936384817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.296625424737799 and perplexity of 73.45150736502782
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 99.40070700645447 and batch: 50, loss is 3.6833694791793823 and perplexity is 39.78020708541548
At time: 99.97414493560791 and batch: 100, loss is 3.5674242067337034 and perplexity is 35.425227470215596
At time: 100.52386283874512 and batch: 150, loss is 3.580603909492493 and perplexity is 35.89521176188659
At time: 101.0864200592041 and batch: 200, loss is 3.463355207443237 and perplexity is 31.923908360038084
At time: 101.63693475723267 and batch: 250, loss is 3.6159067583084106 and perplexity is 37.185048494966395
At time: 102.18742036819458 and batch: 300, loss is 3.583720464706421 and perplexity is 36.00725567609123
At time: 102.73742628097534 and batch: 350, loss is 3.5660604095458983 and perplexity is 35.37694757409079
At time: 103.28837823867798 and batch: 400, loss is 3.4969756984710694 and perplexity is 33.015452137727976
At time: 103.83871960639954 and batch: 450, loss is 3.523005723953247 and perplexity is 33.88612789336988
At time: 104.3884208202362 and batch: 500, loss is 3.395583610534668 and perplexity is 29.832058699235628
At time: 104.93708395957947 and batch: 550, loss is 3.4572835397720336 and perplexity is 31.73066424838669
At time: 105.48667502403259 and batch: 600, loss is 3.473908200263977 and perplexity is 32.26258501887291
At time: 106.03750586509705 and batch: 650, loss is 3.314448480606079 and perplexity is 27.507219035542818
At time: 106.5888192653656 and batch: 700, loss is 3.3001367950439455 and perplexity is 27.11634804897992
At time: 107.13960146903992 and batch: 750, loss is 3.395052599906921 and perplexity is 29.816221764181314
At time: 107.68948101997375 and batch: 800, loss is 3.3508089208602905 and perplexity is 28.52579942752608
At time: 108.23954510688782 and batch: 850, loss is 3.416020922660828 and perplexity is 30.44801863744068
At time: 108.79086184501648 and batch: 900, loss is 3.3668770170211793 and perplexity is 28.987856967013773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2809820305811215 and perplexity of 72.31141717377614
finished 10 epochs...
Completing Train Step...
At time: 110.23719358444214 and batch: 50, loss is 3.657920255661011 and perplexity is 38.78060519944978
At time: 110.7866439819336 and batch: 100, loss is 3.5353465986251833 and perplexity is 34.30690337935792
At time: 111.3363528251648 and batch: 150, loss is 3.5488132762908937 and perplexity is 34.7720282026792
At time: 111.88642239570618 and batch: 200, loss is 3.4328101873397827 and perplexity is 30.96353393329651
At time: 112.43643498420715 and batch: 250, loss is 3.585767502784729 and perplexity is 36.081039392829375
At time: 112.98648476600647 and batch: 300, loss is 3.555185070037842 and perplexity is 34.994295764078466
At time: 113.53743505477905 and batch: 350, loss is 3.5391320896148684 and perplexity is 34.43701797141456
At time: 114.09066700935364 and batch: 400, loss is 3.472208471298218 and perplexity is 32.20779394678352
At time: 114.65215468406677 and batch: 450, loss is 3.500808448791504 and perplexity is 33.142234930680345
At time: 115.20238089561462 and batch: 500, loss is 3.374902148246765 and perplexity is 29.221424273736105
At time: 115.75240230560303 and batch: 550, loss is 3.439214673042297 and perplexity is 31.16247582391193
At time: 116.30240368843079 and batch: 600, loss is 3.4585986614227293 and perplexity is 31.772421383768705
At time: 116.85213017463684 and batch: 650, loss is 3.302275314331055 and perplexity is 27.17439893162121
At time: 117.40023446083069 and batch: 700, loss is 3.2904458808898926 and perplexity is 26.854835044541097
At time: 117.95036673545837 and batch: 750, loss is 3.388214530944824 and perplexity is 29.613031888508623
At time: 118.50050473213196 and batch: 800, loss is 3.346753239631653 and perplexity is 28.41034216543993
At time: 119.05056619644165 and batch: 850, loss is 3.4155048608779905 and perplexity is 30.432309632415983
At time: 119.59916591644287 and batch: 900, loss is 3.3686226415634155 and perplexity is 29.038503073244474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.281560610418451 and perplexity of 72.35326720738695
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 121.03297901153564 and batch: 50, loss is 3.651588182449341 and perplexity is 38.535819386870344
At time: 121.59542798995972 and batch: 100, loss is 3.533158097267151 and perplexity is 34.231904771933465
At time: 122.14536714553833 and batch: 150, loss is 3.550256538391113 and perplexity is 34.822249585723654
At time: 122.6943907737732 and batch: 200, loss is 3.432868466377258 and perplexity is 30.965338510834993
At time: 123.24425601959229 and batch: 250, loss is 3.5860422039031983 and perplexity is 36.090952256181104
At time: 123.79413366317749 and batch: 300, loss is 3.5523935747146607 and perplexity is 34.89674556991662
At time: 124.34333372116089 and batch: 350, loss is 3.5344305801391602 and perplexity is 34.275492010557876
At time: 124.89308953285217 and batch: 400, loss is 3.4666375064849855 and perplexity is 32.028864328065275
At time: 125.44320249557495 and batch: 450, loss is 3.491846809387207 and perplexity is 32.846553048255934
At time: 125.99285745620728 and batch: 500, loss is 3.366151599884033 and perplexity is 28.96683630409712
At time: 126.54314351081848 and batch: 550, loss is 3.426909508705139 and perplexity is 30.78136605577457
At time: 127.092355966568 and batch: 600, loss is 3.450944538116455 and perplexity is 31.530159685964954
At time: 127.64138650894165 and batch: 650, loss is 3.2880087900161743 and perplexity is 26.789467057355285
At time: 128.191241979599 and batch: 700, loss is 3.2735185956954957 and perplexity is 26.404081373793712
At time: 128.75384902954102 and batch: 750, loss is 3.3698453378677367 and perplexity is 29.074030058565945
At time: 129.30116987228394 and batch: 800, loss is 3.3266778707504274 and perplexity is 27.84568092184691
At time: 129.851083278656 and batch: 850, loss is 3.393104214668274 and perplexity is 29.75818483533149
At time: 130.40123677253723 and batch: 900, loss is 3.34826774597168 and perplexity is 28.45340240804633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.275704997859589 and perplexity of 71.9308325212225
finished 12 epochs...
Completing Train Step...
At time: 131.8276584148407 and batch: 50, loss is 3.64084312915802 and perplexity is 38.12396660654122
At time: 132.38790559768677 and batch: 100, loss is 3.520863265991211 and perplexity is 33.813606004152035
At time: 132.93442821502686 and batch: 150, loss is 3.538005523681641 and perplexity is 34.39824424480658
At time: 133.4988899230957 and batch: 200, loss is 3.421333198547363 and perplexity is 30.610197300147043
At time: 134.0547091960907 and batch: 250, loss is 3.574744291305542 and perplexity is 35.68549455764941
At time: 134.61359286308289 and batch: 300, loss is 3.542265386581421 and perplexity is 34.54508859562531
At time: 135.17036318778992 and batch: 350, loss is 3.5245661211013792 and perplexity is 33.939044985798944
At time: 135.71897220611572 and batch: 400, loss is 3.4581899309158324 and perplexity is 31.75943767946995
At time: 136.2680914402008 and batch: 450, loss is 3.4843695259094236 and perplexity is 32.60186599710046
At time: 136.81704139709473 and batch: 500, loss is 3.359118022918701 and perplexity is 28.763810666726165
At time: 137.3651146888733 and batch: 550, loss is 3.420954079627991 and perplexity is 30.598594594766375
At time: 137.91509914398193 and batch: 600, loss is 3.4465552520751954 and perplexity is 31.39206807966071
At time: 138.46393203735352 and batch: 650, loss is 3.2851566219329835 and perplexity is 26.71316785524674
At time: 139.0142171382904 and batch: 700, loss is 3.271499824523926 and perplexity is 26.350831343402394
At time: 139.56357097625732 and batch: 750, loss is 3.368940734863281 and perplexity is 29.047741495772456
At time: 140.1131443977356 and batch: 800, loss is 3.3271254110336304 and perplexity is 27.858145774826514
At time: 140.66191220283508 and batch: 850, loss is 3.3951904249191283 and perplexity is 29.82033146851345
At time: 141.21004939079285 and batch: 900, loss is 3.3513785886764524 and perplexity is 28.542054286885765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.275273989324701 and perplexity of 71.89983639876074
finished 13 epochs...
Completing Train Step...
At time: 142.65821814537048 and batch: 50, loss is 3.6355718660354612 and perplexity is 37.923533878212545
At time: 143.20824313163757 and batch: 100, loss is 3.5150413513183594 and perplexity is 33.61731801560116
At time: 143.7586784362793 and batch: 150, loss is 3.5319351148605347 and perplexity is 34.190065344309254
At time: 144.3092656135559 and batch: 200, loss is 3.415271348953247 and perplexity is 30.42520415485848
At time: 144.8594105243683 and batch: 250, loss is 3.5686385107040404 and perplexity is 35.46827059301117
At time: 145.40915846824646 and batch: 300, loss is 3.536465730667114 and perplexity is 34.34531882615842
At time: 145.95950841903687 and batch: 350, loss is 3.5189697837829588 and perplexity is 33.74964112008226
At time: 146.5095591545105 and batch: 400, loss is 3.4531248950958253 and perplexity is 31.59898169070135
At time: 147.05942463874817 and batch: 450, loss is 3.4797273445129395 and perplexity is 32.450872960966045
At time: 147.6091663837433 and batch: 500, loss is 3.3548487520217893 and perplexity is 28.641271928619346
At time: 148.15951490402222 and batch: 550, loss is 3.417246336936951 and perplexity is 30.485352944483186
At time: 148.70945286750793 and batch: 600, loss is 3.443590078353882 and perplexity is 31.29912301156965
At time: 149.25959134101868 and batch: 650, loss is 3.2828758478164675 and perplexity is 26.652310580661958
At time: 149.80853605270386 and batch: 700, loss is 3.269801917076111 and perplexity is 26.306128032390223
At time: 150.35851120948792 and batch: 750, loss is 3.367817759513855 and perplexity is 29.015139906932593
At time: 150.9083640575409 and batch: 800, loss is 3.3265910959243774 and perplexity is 27.843264722562825
At time: 151.4604687690735 and batch: 850, loss is 3.3954063844680786 and perplexity is 29.826772149285166
At time: 152.01022577285767 and batch: 900, loss is 3.3520331382751465 and perplexity is 28.560742592609426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2754842679794525 and perplexity of 71.91495698934855
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 153.450541973114 and batch: 50, loss is 3.6334651803970335 and perplexity is 37.8437250096483
At time: 154.00810146331787 and batch: 100, loss is 3.5143278884887694 and perplexity is 33.59334186283047
At time: 154.55254888534546 and batch: 150, loss is 3.532713670730591 and perplexity is 34.21669458521148
At time: 155.09883093833923 and batch: 200, loss is 3.416534118652344 and perplexity is 30.463648448790252
At time: 155.64423537254333 and batch: 250, loss is 3.569897789955139 and perplexity is 35.51296318456185
At time: 156.2028911113739 and batch: 300, loss is 3.536463851928711 and perplexity is 34.34525430034959
At time: 156.7487027645111 and batch: 350, loss is 3.5186042022705077 and perplexity is 33.737305130279196
At time: 157.29412388801575 and batch: 400, loss is 3.4531846714019774 and perplexity is 31.60087061756096
At time: 157.84215140342712 and batch: 450, loss is 3.47701593875885 and perplexity is 32.363004654442335
At time: 158.3932240009308 and batch: 500, loss is 3.3524612474441526 and perplexity is 28.57297232603067
At time: 158.94354009628296 and batch: 550, loss is 3.413173699378967 and perplexity is 30.36144962902003
At time: 159.49226474761963 and batch: 600, loss is 3.439260573387146 and perplexity is 31.163906225126297
At time: 160.04139852523804 and batch: 650, loss is 3.277436556816101 and perplexity is 26.50773445982893
At time: 160.59224462509155 and batch: 700, loss is 3.264091787338257 and perplexity is 26.15634467660264
At time: 161.14141464233398 and batch: 750, loss is 3.361065249443054 and perplexity is 28.819874888953326
At time: 161.69147777557373 and batch: 800, loss is 3.318270788192749 and perplexity is 27.61256128453486
At time: 162.2412829399109 and batch: 850, loss is 3.385099244117737 and perplexity is 29.520922348667966
At time: 162.7913327217102 and batch: 900, loss is 3.3430328369140625 and perplexity is 28.30484062703574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.275247234187714 and perplexity of 71.89791273452262
finished 15 epochs...
Completing Train Step...
At time: 164.221604347229 and batch: 50, loss is 3.630669174194336 and perplexity is 37.738061506529895
At time: 164.7842617034912 and batch: 100, loss is 3.511198697090149 and perplexity is 33.48838616529779
At time: 165.33426880836487 and batch: 150, loss is 3.5293471717834475 and perplexity is 34.10169779580874
At time: 165.88384556770325 and batch: 200, loss is 3.413319664001465 and perplexity is 30.36588165000488
At time: 166.43317985534668 and batch: 250, loss is 3.5662863063812256 and perplexity is 35.38494001729114
At time: 166.98318219184875 and batch: 300, loss is 3.5330733251571655 and perplexity is 34.229002984134205
At time: 167.53269457817078 and batch: 350, loss is 3.515375714302063 and perplexity is 33.62856028175106
At time: 168.08193111419678 and batch: 400, loss is 3.4504289865493774 and perplexity is 31.5139084522631
At time: 168.6312108039856 and batch: 450, loss is 3.4748019647598265 and perplexity is 32.291433061664954
At time: 169.18062806129456 and batch: 500, loss is 3.3502756261825564 and perplexity is 28.51059082620679
At time: 169.73018145561218 and batch: 550, loss is 3.4115496397018434 and perplexity is 30.31218084149325
At time: 170.29225325584412 and batch: 600, loss is 3.4384485197067263 and perplexity is 31.138609732824868
At time: 170.84212636947632 and batch: 650, loss is 3.276711468696594 and perplexity is 26.488520983070728
At time: 171.39053750038147 and batch: 700, loss is 3.2636859941482546 and perplexity is 26.14573276332421
At time: 171.9402403831482 and batch: 750, loss is 3.3610378217697146 and perplexity is 28.81908443767935
At time: 172.49027729034424 and batch: 800, loss is 3.318897018432617 and perplexity is 27.629858520872396
At time: 173.03899359703064 and batch: 850, loss is 3.3864856863021853 and perplexity is 29.56187978673516
At time: 173.58898496627808 and batch: 900, loss is 3.344844727516174 and perplexity is 28.356172391489643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2749876257491435 and perplexity of 71.87924985228682
finished 16 epochs...
Completing Train Step...
At time: 175.1323173046112 and batch: 50, loss is 3.6289869499206544 and perplexity is 37.67463099055714
At time: 175.71424436569214 and batch: 100, loss is 3.5093405199050904 and perplexity is 33.42621658909343
At time: 176.26545238494873 and batch: 150, loss is 3.5272576379776 and perplexity is 34.03051554019173
At time: 176.816260099411 and batch: 200, loss is 3.4113094139099123 and perplexity is 30.30489994840961
At time: 177.36620497703552 and batch: 250, loss is 3.5641593980789184 and perplexity is 35.309759473994006
At time: 177.9164433479309 and batch: 300, loss is 3.5310588932037352 and perplexity is 34.16012038973193
At time: 178.46688413619995 and batch: 350, loss is 3.5134951782226564 and perplexity is 33.56537998587499
At time: 179.0164213180542 and batch: 400, loss is 3.4487078619003295 and perplexity is 31.459715737224684
At time: 179.56641697883606 and batch: 450, loss is 3.473387751579285 and perplexity is 32.24579836760926
At time: 180.1165361404419 and batch: 500, loss is 3.348952031135559 and perplexity is 28.472879312294353
At time: 180.66640710830688 and batch: 550, loss is 3.4105079221725463 and perplexity is 30.28062055266653
At time: 181.21694374084473 and batch: 600, loss is 3.437815933227539 and perplexity is 31.118918098314722
At time: 181.76654243469238 and batch: 650, loss is 3.2762474822998047 and perplexity is 26.476233520491693
At time: 182.31641936302185 and batch: 700, loss is 3.263394846916199 and perplexity is 26.138121613636446
At time: 182.86710286140442 and batch: 750, loss is 3.3609797286987306 and perplexity is 28.817410297189873
At time: 183.41738939285278 and batch: 800, loss is 3.3191726398468018 and perplexity is 27.637474951129068
At time: 183.97990155220032 and batch: 850, loss is 3.3871361351013185 and perplexity is 29.58111453087252
At time: 184.5313436985016 and batch: 900, loss is 3.3456560802459716 and perplexity is 28.37918858522928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274884785691353 and perplexity of 71.8718581661653
finished 17 epochs...
Completing Train Step...
At time: 185.9672453403473 and batch: 50, loss is 3.627560591697693 and perplexity is 37.6209317771083
At time: 186.52920818328857 and batch: 100, loss is 3.507804441452026 and perplexity is 33.374910713037
At time: 187.07941818237305 and batch: 150, loss is 3.5255965042114257 and perplexity is 33.974033227074294
At time: 187.62944793701172 and batch: 200, loss is 3.4096861028671266 and perplexity is 30.25574557688394
At time: 188.17927193641663 and batch: 250, loss is 3.5624901723861693 and perplexity is 35.250868680961645
At time: 188.7280776500702 and batch: 300, loss is 3.5294931983947753 and perplexity is 34.10667791478449
At time: 189.27730679512024 and batch: 350, loss is 3.512016897201538 and perplexity is 33.51579757907015
At time: 189.82750821113586 and batch: 400, loss is 3.4473501205444337 and perplexity is 31.41703056439312
At time: 190.37734603881836 and batch: 450, loss is 3.472214741706848 and perplexity is 32.20799590344581
At time: 190.9273340702057 and batch: 500, loss is 3.347879605293274 and perplexity is 28.442360628153306
At time: 191.47740817070007 and batch: 550, loss is 3.4096264171600343 and perplexity is 30.253939795205795
At time: 192.02739334106445 and batch: 600, loss is 3.4371997833251955 and perplexity is 31.09975008575836
At time: 192.57739639282227 and batch: 650, loss is 3.2757941198349 and perplexity is 26.46423291052006
At time: 193.12736916542053 and batch: 700, loss is 3.2630774068832396 and perplexity is 26.12982564425598
At time: 193.69456720352173 and batch: 750, loss is 3.360831789970398 and perplexity is 28.81314740148813
At time: 194.24325132369995 and batch: 800, loss is 3.3192260599136354 and perplexity is 27.63895138632335
At time: 194.79317355155945 and batch: 850, loss is 3.387427339553833 and perplexity is 29.589729937495747
At time: 195.3434431552887 and batch: 900, loss is 3.346055669784546 and perplexity is 28.390530878081027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274874752514983 and perplexity of 71.87113706675376
finished 18 epochs...
Completing Train Step...
At time: 196.77425909042358 and batch: 50, loss is 3.626256446838379 and perplexity is 37.57190061114942
At time: 197.33645844459534 and batch: 100, loss is 3.5064216327667235 and perplexity is 33.328791491011145
At time: 197.88625526428223 and batch: 150, loss is 3.5241474866867066 and perplexity is 33.92483990714162
At time: 198.44636368751526 and batch: 200, loss is 3.4082561206817625 and perplexity is 30.21251131918054
At time: 198.99391722679138 and batch: 250, loss is 3.5610368871688842 and perplexity is 35.1996763221703
At time: 199.5434226989746 and batch: 300, loss is 3.5281332397460936 and perplexity is 34.06032576882135
At time: 200.0923571586609 and batch: 350, loss is 3.510719223022461 and perplexity is 33.4723332013603
At time: 200.64221811294556 and batch: 400, loss is 3.4461631298065187 and perplexity is 31.37976096381165
At time: 201.19210171699524 and batch: 450, loss is 3.471150679588318 and perplexity is 32.17374282202623
At time: 201.74027752876282 and batch: 500, loss is 3.3469131278991697 and perplexity is 28.41488500899234
At time: 202.28987669944763 and batch: 550, loss is 3.4088088035583497 and perplexity is 30.22921387202766
At time: 202.84039211273193 and batch: 600, loss is 3.4365830755233766 and perplexity is 31.08057654009114
At time: 203.38936948776245 and batch: 650, loss is 3.275321574211121 and perplexity is 26.45173030732938
At time: 203.93821597099304 and batch: 700, loss is 3.262722306251526 and perplexity is 26.120548573907396
At time: 204.48742032051086 and batch: 750, loss is 3.360614151954651 and perplexity is 28.80687724759692
At time: 205.03717398643494 and batch: 800, loss is 3.3191501808166506 and perplexity is 27.636854247216068
At time: 205.58535742759705 and batch: 850, loss is 3.387529230117798 and perplexity is 29.59274500536753
At time: 206.13308453559875 and batch: 900, loss is 3.346252269744873 and perplexity is 28.396113004030212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27491697546554 and perplexity of 71.87417174228659
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 207.57949304580688 and batch: 50, loss is 3.6256443643569947 and perplexity is 37.548910545618966
At time: 208.12931156158447 and batch: 100, loss is 3.5061327171325685 and perplexity is 33.319163672962446
At time: 208.67861413955688 and batch: 150, loss is 3.524130969047546 and perplexity is 33.92427955350533
At time: 209.22740745544434 and batch: 200, loss is 3.408539400100708 and perplexity is 30.22107111418155
At time: 209.77664303779602 and batch: 250, loss is 3.5610516834259034 and perplexity is 35.20019714948129
At time: 210.32435059547424 and batch: 300, loss is 3.5279061508178713 and perplexity is 34.05259192411609
At time: 210.8719675540924 and batch: 350, loss is 3.5101179695129394 and perplexity is 33.45221389255973
At time: 211.42193841934204 and batch: 400, loss is 3.4459363508224485 and perplexity is 31.372645500349194
At time: 211.98326110839844 and batch: 450, loss is 3.4700966835021974 and perplexity is 32.13984968776907
At time: 212.532799243927 and batch: 500, loss is 3.3460147380828857 and perplexity is 28.389368829123637
At time: 213.0809199810028 and batch: 550, loss is 3.407156891822815 and perplexity is 30.179319101101402
At time: 213.6302468776703 and batch: 600, loss is 3.4347087383270263 and perplexity is 31.022375620511248
At time: 214.17936301231384 and batch: 650, loss is 3.273351912498474 and perplexity is 26.399680623871628
At time: 214.72714114189148 and batch: 700, loss is 3.2604785490036012 and perplexity is 26.06200610582919
At time: 215.27615308761597 and batch: 750, loss is 3.3583112478256227 and perplexity is 28.740614099162897
At time: 215.82501411437988 and batch: 800, loss is 3.316419830322266 and perplexity is 27.56149886866983
At time: 216.37441611289978 and batch: 850, loss is 3.3841426515579225 and perplexity is 29.4926963565298
At time: 216.9227819442749 and batch: 900, loss is 3.3428835010528566 and perplexity is 28.300614014884644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274914049122431 and perplexity of 71.8739614141072
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8858900b70>
ELAPSED
654.5915813446045


RESULTS SO FAR:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.78276706776143, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.87113706675376, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.22372141185209682, 'rnn_dropout': 0.5465833484205133, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.7150731284420337, 'rnn_dropout': 0.7542520583353851, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.856334924697876 and batch: 50, loss is 6.901208543777466 and perplexity is 993.4746479774589
At time: 1.474308967590332 and batch: 100, loss is 6.080244626998901 and perplexity is 437.13611693622505
At time: 2.092073678970337 and batch: 150, loss is 5.999736757278442 and perplexity is 403.322607776103
At time: 2.7101833820343018 and batch: 200, loss is 5.8585504531860355 and perplexity is 350.21612124472074
At time: 3.3279032707214355 and batch: 250, loss is 5.913487815856934 and perplexity is 369.99437982359035
At time: 3.9584619998931885 and batch: 300, loss is 5.828845262527466 and perplexity is 339.9658809839049
At time: 4.57616662979126 and batch: 350, loss is 5.818852853775025 and perplexity is 336.5857190443244
At time: 5.193860292434692 and batch: 400, loss is 5.691703767776489 and perplexity is 296.39818436425173
At time: 5.8122878074646 and batch: 450, loss is 5.692482986450195 and perplexity is 296.6292333715208
At time: 6.431451320648193 and batch: 500, loss is 5.64942777633667 and perplexity is 284.1288340552045
At time: 7.053628921508789 and batch: 550, loss is 5.696350708007812 and perplexity is 297.7787341936322
At time: 7.674816370010376 and batch: 600, loss is 5.627175140380859 and perplexity is 277.87604706178723
At time: 8.295100450515747 and batch: 650, loss is 5.537055749893188 and perplexity is 253.92926650086414
At time: 8.914885759353638 and batch: 700, loss is 5.629143648147583 and perplexity is 278.4235869600752
At time: 9.535059928894043 and batch: 750, loss is 5.600648431777954 and perplexity is 270.6018173668695
At time: 10.16844654083252 and batch: 800, loss is 5.594236640930176 and perplexity is 268.8723256057067
At time: 10.803558826446533 and batch: 850, loss is 5.629985389709472 and perplexity is 278.65804632828383
At time: 11.435460090637207 and batch: 900, loss is 5.519561910629273 and perplexity is 249.52569868510696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.378398738495291 and perplexity of 216.6750440879247
finished 1 epochs...
Completing Train Step...
At time: 13.035214185714722 and batch: 50, loss is 5.259869575500488 and perplexity is 192.4563886321148
At time: 13.599828243255615 and batch: 100, loss is 5.064499835968018 and perplexity is 158.30124568043163
At time: 14.149587392807007 and batch: 150, loss is 5.004662408828735 and perplexity is 149.10673754257783
At time: 14.698788166046143 and batch: 200, loss is 4.85537748336792 and perplexity is 128.42916193828611
At time: 15.248583316802979 and batch: 250, loss is 4.925468521118164 and perplexity is 137.75386712175884
At time: 15.798534393310547 and batch: 300, loss is 4.848391771316528 and perplexity is 127.53511919235983
At time: 16.34845757484436 and batch: 350, loss is 4.822383947372437 and perplexity is 124.26096957186849
At time: 16.897448778152466 and batch: 400, loss is 4.676514272689819 and perplexity is 107.39506944523713
At time: 17.449140071868896 and batch: 450, loss is 4.686792640686035 and perplexity is 108.50460784389064
At time: 17.99836254119873 and batch: 500, loss is 4.586771249771118 and perplexity is 98.1769290896717
At time: 18.547430276870728 and batch: 550, loss is 4.654181337356567 and perplexity is 105.02320621994392
At time: 19.096863508224487 and batch: 600, loss is 4.59322340965271 and perplexity is 98.81243030589042
At time: 19.645649194717407 and batch: 650, loss is 4.4555934715270995 and perplexity is 86.10723787824708
At time: 20.19586730003357 and batch: 700, loss is 4.507087078094482 and perplexity is 90.65735572042769
At time: 20.745333433151245 and batch: 750, loss is 4.537861795425415 and perplexity is 93.49068404286007
At time: 21.294822931289673 and batch: 800, loss is 4.483857288360595 and perplexity is 88.5756765038059
At time: 21.844561100006104 and batch: 850, loss is 4.542073078155518 and perplexity is 93.88522993504371
At time: 22.41060972213745 and batch: 900, loss is 4.476096658706665 and perplexity is 87.89093393553755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.585548296366652 and perplexity of 98.05693666752728
finished 2 epochs...
Completing Train Step...
At time: 23.837544918060303 and batch: 50, loss is 4.514402523040771 and perplexity is 91.32298633877483
At time: 24.40073823928833 and batch: 100, loss is 4.378017377853394 and perplexity is 79.67990156156023
At time: 24.94974112510681 and batch: 150, loss is 4.375311861038208 and perplexity is 79.4646176065532
At time: 25.499525785446167 and batch: 200, loss is 4.2750989532470705 and perplexity is 71.88725243477121
At time: 26.048423528671265 and batch: 250, loss is 4.406583232879639 and perplexity is 81.98884757892365
At time: 26.633220195770264 and batch: 300, loss is 4.369164266586304 and perplexity is 78.97759989175063
At time: 27.18239951133728 and batch: 350, loss is 4.357739763259888 and perplexity is 78.08045451710926
At time: 27.732572555541992 and batch: 400, loss is 4.267768445014954 and perplexity is 71.36220910738797
At time: 28.281548976898193 and batch: 450, loss is 4.295883502960205 and perplexity is 73.39703230273291
At time: 28.833104372024536 and batch: 500, loss is 4.178422646522522 and perplexity is 65.26282943195557
At time: 29.38389277458191 and batch: 550, loss is 4.260320348739624 and perplexity is 70.83267097699458
At time: 29.933274030685425 and batch: 600, loss is 4.250260252952575 and perplexity is 70.12365986162442
At time: 30.483471155166626 and batch: 650, loss is 4.096802530288696 and perplexity is 60.147659480798275
At time: 31.048662424087524 and batch: 700, loss is 4.131118555068969 and perplexity is 62.24751127757872
At time: 31.62232542037964 and batch: 750, loss is 4.212273187637329 and perplexity is 67.50982802413452
At time: 32.17191410064697 and batch: 800, loss is 4.164057755470276 and perplexity is 64.33203736640871
At time: 32.72139358520508 and batch: 850, loss is 4.238141493797302 and perplexity is 69.27897671030478
At time: 33.27057957649231 and batch: 900, loss is 4.186922674179077 and perplexity is 65.81992961633442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430004381153681 and perplexity of 83.93178462751052
finished 3 epochs...
Completing Train Step...
At time: 34.698660135269165 and batch: 50, loss is 4.254611377716064 and perplexity is 70.42944141897951
At time: 35.26123929023743 and batch: 100, loss is 4.1196613788604735 and perplexity is 61.538400532589186
At time: 35.81180477142334 and batch: 150, loss is 4.130148153305054 and perplexity is 62.1871354819628
At time: 36.37340521812439 and batch: 200, loss is 4.025794658660889 and perplexity is 56.02481169882908
At time: 36.92329931259155 and batch: 250, loss is 4.172147369384765 and perplexity is 64.85456940263198
At time: 37.47272038459778 and batch: 300, loss is 4.145877685546875 and perplexity is 63.17304364411468
At time: 38.022974491119385 and batch: 350, loss is 4.132380685806274 and perplexity is 62.32612537508147
At time: 38.573012828826904 and batch: 400, loss is 4.060660357475281 and perplexity is 58.012607491924165
At time: 39.12237906455994 and batch: 450, loss is 4.092146549224854 and perplexity is 59.86826405186584
At time: 39.67391586303711 and batch: 500, loss is 3.970602650642395 and perplexity is 53.01647162482433
At time: 40.22392773628235 and batch: 550, loss is 4.0518625497817995 and perplexity is 57.50446228690235
At time: 40.77490186691284 and batch: 600, loss is 4.059095239639282 and perplexity is 57.921881941834116
At time: 41.324822425842285 and batch: 650, loss is 3.9031041765213015 and perplexity is 49.55604129343675
At time: 41.874550342559814 and batch: 700, loss is 3.9227328443527223 and perplexity is 50.53836972682025
At time: 42.4244499206543 and batch: 750, loss is 4.0249015951156615 and perplexity is 55.97480031686761
At time: 42.97460460662842 and batch: 800, loss is 3.981416354179382 and perplexity is 53.592887007107684
At time: 43.52479028701782 and batch: 850, loss is 4.0564560079574585 and perplexity is 57.7692142272947
At time: 44.07459616661072 and batch: 900, loss is 4.014840321540833 and perplexity is 55.41444620261412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36573728796554 and perplexity of 78.70740857975848
finished 4 epochs...
Completing Train Step...
At time: 45.515613079071045 and batch: 50, loss is 4.086398668289185 and perplexity is 59.525135474069444
At time: 46.06457257270813 and batch: 100, loss is 3.956862630844116 and perplexity is 52.29300585385017
At time: 46.61462712287903 and batch: 150, loss is 3.9691768884658813 and perplexity is 52.9409366051321
At time: 47.16439247131348 and batch: 200, loss is 3.8693520498275755 and perplexity is 47.91133186537035
At time: 47.711987018585205 and batch: 250, loss is 4.0174074411392215 and perplexity is 55.55688446329644
At time: 48.26203989982605 and batch: 300, loss is 3.995168514251709 and perplexity is 54.33499607358427
At time: 48.81176686286926 and batch: 350, loss is 3.9864601993560793 and perplexity is 53.8638840908887
At time: 49.361695528030396 and batch: 400, loss is 3.91897566318512 and perplexity is 50.34884417970116
At time: 49.92391920089722 and batch: 450, loss is 3.949756646156311 and perplexity is 51.92272970139864
At time: 50.47329092025757 and batch: 500, loss is 3.8303355836868285 and perplexity is 46.07799866460881
At time: 51.023439168930054 and batch: 550, loss is 3.911803994178772 and perplexity is 49.98905063659195
At time: 51.57371497154236 and batch: 600, loss is 3.926972494125366 and perplexity is 50.7530895613655
At time: 52.1229202747345 and batch: 650, loss is 3.771462984085083 and perplexity is 43.443575626626604
At time: 52.67174553871155 and batch: 700, loss is 3.7873546743392943 and perplexity is 44.139482395797316
At time: 53.221614837646484 and batch: 750, loss is 3.8914913749694824 and perplexity is 48.98388541645894
At time: 53.7704393863678 and batch: 800, loss is 3.855455322265625 and perplexity is 47.250126078565714
At time: 54.319488286972046 and batch: 850, loss is 3.927634611129761 and perplexity is 50.78670517249544
At time: 54.86984825134277 and batch: 900, loss is 3.8900793266296385 and perplexity is 48.91476661340763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33891275484268 and perplexity of 76.6241847592624
finished 5 epochs...
Completing Train Step...
At time: 56.29848861694336 and batch: 50, loss is 3.964227628707886 and perplexity is 52.67956548834417
At time: 56.8606595993042 and batch: 100, loss is 3.8386055755615236 and perplexity is 46.46064339272176
At time: 57.42682886123657 and batch: 150, loss is 3.852516827583313 and perplexity is 47.1114856312144
At time: 57.999616384506226 and batch: 200, loss is 3.756022458076477 and perplexity is 42.777936103969786
At time: 58.59052586555481 and batch: 250, loss is 3.903153004646301 and perplexity is 49.558461081091885
At time: 59.156280755996704 and batch: 300, loss is 3.886019477844238 and perplexity is 48.716582628301424
At time: 59.70260310173035 and batch: 350, loss is 3.8776121044158938 and perplexity is 48.30872105072276
At time: 60.252442836761475 and batch: 400, loss is 3.814945068359375 and perplexity is 45.37426383173348
At time: 60.802053928375244 and batch: 450, loss is 3.845420031547546 and perplexity is 46.77832859877442
At time: 61.35011005401611 and batch: 500, loss is 3.7260474157333374 and perplexity is 41.51469312305726
At time: 61.89768099784851 and batch: 550, loss is 3.8053496360778807 and perplexity is 44.9409603475331
At time: 62.447295904159546 and batch: 600, loss is 3.8273111534118653 and perplexity is 45.93884949997948
At time: 62.997349977493286 and batch: 650, loss is 3.6715245485305785 and perplexity is 39.311792934482334
At time: 63.5465407371521 and batch: 700, loss is 3.681381287574768 and perplexity is 39.70119498328478
At time: 64.1083345413208 and batch: 750, loss is 3.792580795288086 and perplexity is 44.370764497127865
At time: 64.65773415565491 and batch: 800, loss is 3.756668963432312 and perplexity is 42.805601210628794
At time: 65.20744848251343 and batch: 850, loss is 3.8247490167617797 and perplexity is 45.82129854512336
At time: 65.75558757781982 and batch: 900, loss is 3.7924305534362794 and perplexity is 44.36409865206073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33428495224208 and perplexity of 76.27040240661026
finished 6 epochs...
Completing Train Step...
At time: 67.19321537017822 and batch: 50, loss is 3.871088008880615 and perplexity is 47.99457620914622
At time: 67.75547575950623 and batch: 100, loss is 3.7475462198257445 and perplexity is 42.416872517931175
At time: 68.30792951583862 and batch: 150, loss is 3.766225152015686 and perplexity is 43.216620368638544
At time: 68.85677790641785 and batch: 200, loss is 3.6682697534561157 and perplexity is 39.18404910723394
At time: 69.4054946899414 and batch: 250, loss is 3.814592704772949 and perplexity is 45.35827840990398
At time: 69.95754170417786 and batch: 300, loss is 3.7983810567855834 and perplexity is 44.628874362780174
At time: 70.5047037601471 and batch: 350, loss is 3.7892207956314086 and perplexity is 44.22192892740703
At time: 71.05379438400269 and batch: 400, loss is 3.725818796157837 and perplexity is 41.50520313637813
At time: 71.60028648376465 and batch: 450, loss is 3.761580271720886 and perplexity is 43.0163498167187
At time: 72.1476502418518 and batch: 500, loss is 3.6444682693481445 and perplexity is 38.26242213882315
At time: 72.69654703140259 and batch: 550, loss is 3.719988431930542 and perplexity is 41.263916761994594
At time: 73.24631118774414 and batch: 600, loss is 3.7473376989364624 and perplexity is 42.40802863605227
At time: 73.79534459114075 and batch: 650, loss is 3.593667130470276 and perplexity is 36.367194943590356
At time: 74.34266114234924 and batch: 700, loss is 3.6003853034973146 and perplexity is 36.61233858841833
At time: 74.89047527313232 and batch: 750, loss is 3.709354181289673 and perplexity is 40.827430892203004
At time: 75.43910694122314 and batch: 800, loss is 3.6742090463638304 and perplexity is 39.417467135055425
At time: 75.98733901977539 and batch: 850, loss is 3.7422889614105226 and perplexity is 42.194461207070994
At time: 76.53577399253845 and batch: 900, loss is 3.7127617835998534 and perplexity is 40.966791848536324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334693804179152 and perplexity of 76.301592083921
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 77.97646570205688 and batch: 50, loss is 3.8179410886764527 and perplexity is 45.510409894391906
At time: 78.52576875686646 and batch: 100, loss is 3.6887202310562133 and perplexity is 39.99363158470381
At time: 79.0759072303772 and batch: 150, loss is 3.703948802947998 and perplexity is 40.607338558568756
At time: 79.62386322021484 and batch: 200, loss is 3.5874759674072267 and perplexity is 36.14273525977123
At time: 80.17243456840515 and batch: 250, loss is 3.725439233779907 and perplexity is 41.48945231217847
At time: 80.72138714790344 and batch: 300, loss is 3.703273906707764 and perplexity is 40.579942064385015
At time: 81.2715106010437 and batch: 350, loss is 3.6808785343170167 and perplexity is 39.681240094783234
At time: 81.82136225700378 and batch: 400, loss is 3.6116117668151855 and perplexity is 37.02568151290765
At time: 82.37051129341125 and batch: 450, loss is 3.639467926025391 and perplexity is 38.07157444142738
At time: 82.92032384872437 and batch: 500, loss is 3.509931859970093 and perplexity is 33.44598869562723
At time: 83.47099542617798 and batch: 550, loss is 3.5680372095108033 and perplexity is 35.44694989030592
At time: 84.0195050239563 and batch: 600, loss is 3.5930451488494874 and perplexity is 36.34458224980477
At time: 84.56845569610596 and batch: 650, loss is 3.4227721452713014 and perplexity is 30.654275448719368
At time: 85.11855912208557 and batch: 700, loss is 3.4121475124359133 and perplexity is 30.330309086580304
At time: 85.66863083839417 and batch: 750, loss is 3.5130492496490477 and perplexity is 33.550415560635635
At time: 86.21863985061646 and batch: 800, loss is 3.459342360496521 and perplexity is 31.796059292779965
At time: 86.76803350448608 and batch: 850, loss is 3.508797388076782 and perplexity is 33.4080666763078
At time: 87.31759452819824 and batch: 900, loss is 3.4765415620803832 and perplexity is 32.34765604058922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2803277838720035 and perplexity of 72.26412313972858
finished 8 epochs...
Completing Train Step...
At time: 88.75658178329468 and batch: 50, loss is 3.7259333896636964 and perplexity is 41.509959635643696
At time: 89.32530570030212 and batch: 100, loss is 3.59714102268219 and perplexity is 36.49375035152529
At time: 89.87331962585449 and batch: 150, loss is 3.615843825340271 and perplexity is 37.18270840312943
At time: 90.42333364486694 and batch: 200, loss is 3.5041632413864137 and perplexity is 33.25360696558927
At time: 90.96990919113159 and batch: 250, loss is 3.6434795236587525 and perplexity is 38.22460903072207
At time: 91.52932047843933 and batch: 300, loss is 3.625547556877136 and perplexity is 37.54527570616027
At time: 92.11333227157593 and batch: 350, loss is 3.607949905395508 and perplexity is 36.890346538427266
At time: 92.67338538169861 and batch: 400, loss is 3.542892827987671 and perplexity is 34.56677041591737
At time: 93.22337889671326 and batch: 450, loss is 3.5758829259872438 and perplexity is 35.726150441093395
At time: 93.77179527282715 and batch: 500, loss is 3.4480743074417113 and perplexity is 31.439790606546673
At time: 94.32233357429504 and batch: 550, loss is 3.510595455169678 and perplexity is 33.468190658914416
At time: 94.87187170982361 and batch: 600, loss is 3.542086181640625 and perplexity is 34.538898499733186
At time: 95.42161917686462 and batch: 650, loss is 3.3774368381500244 and perplexity is 29.29558547091546
At time: 95.97087407112122 and batch: 700, loss is 3.371323518753052 and perplexity is 29.117038513364307
At time: 96.53287076950073 and batch: 750, loss is 3.4777679681777953 and perplexity is 32.3873517397427
At time: 97.10351705551147 and batch: 800, loss is 3.429394359588623 and perplexity is 30.857948268698195
At time: 97.65355515480042 and batch: 850, loss is 3.4868288040161133 and perplexity is 32.68214172209156
At time: 98.20291781425476 and batch: 900, loss is 3.461533408164978 and perplexity is 31.865802351641555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.28404214937393 and perplexity of 72.5330376199064
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 99.62700033187866 and batch: 50, loss is 3.7013595962524413 and perplexity is 40.502333763905476
At time: 100.18859100341797 and batch: 100, loss is 3.5820741653442383 and perplexity is 35.948025722532705
At time: 100.738210439682 and batch: 150, loss is 3.601715168952942 and perplexity is 36.66106046234478
At time: 101.28767776489258 and batch: 200, loss is 3.484385838508606 and perplexity is 32.60239782261079
At time: 101.83726239204407 and batch: 250, loss is 3.6253682279586794 and perplexity is 37.53854335614529
At time: 102.38760113716125 and batch: 300, loss is 3.601343832015991 and perplexity is 36.64744938375185
At time: 102.9370219707489 and batch: 350, loss is 3.5781185007095337 and perplexity is 35.80610826248535
At time: 103.48629379272461 and batch: 400, loss is 3.515428647994995 and perplexity is 33.630340412748794
At time: 104.03604984283447 and batch: 450, loss is 3.5425222873687745 and perplexity is 34.553964396135974
At time: 104.58543419837952 and batch: 500, loss is 3.409711661338806 and perplexity is 30.256518877382558
At time: 105.13546180725098 and batch: 550, loss is 3.4648153162002564 and perplexity is 31.970554784331956
At time: 105.6972918510437 and batch: 600, loss is 3.4974934339523314 and perplexity is 33.032549834389776
At time: 106.2467029094696 and batch: 650, loss is 3.322144637107849 and perplexity is 27.71973562969266
At time: 106.79618334770203 and batch: 700, loss is 3.31687668800354 and perplexity is 27.57409342787691
At time: 107.34480619430542 and batch: 750, loss is 3.4199131536483764 and perplexity is 30.566760294162705
At time: 107.8931028842926 and batch: 800, loss is 3.365784010887146 and perplexity is 28.956190370580995
At time: 108.44151616096497 and batch: 850, loss is 3.4234305572509767 and perplexity is 30.674465236772868
At time: 108.99049830436707 and batch: 900, loss is 3.3967128086090086 and perplexity is 29.865764028887654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267853201252141 and perplexity of 71.36825775603597
finished 10 epochs...
Completing Train Step...
At time: 110.43349552154541 and batch: 50, loss is 3.6737452220916746 and perplexity is 39.39918859639393
At time: 110.98326778411865 and batch: 100, loss is 3.5498145055770873 and perplexity is 34.80686041025942
At time: 111.53256893157959 and batch: 150, loss is 3.5680741977691652 and perplexity is 35.44826103549495
At time: 112.08240532875061 and batch: 200, loss is 3.453842730522156 and perplexity is 31.62167270242658
At time: 112.63236379623413 and batch: 250, loss is 3.595554780960083 and perplexity is 36.43590832996383
At time: 113.1826479434967 and batch: 300, loss is 3.572660026550293 and perplexity is 35.61119399698701
At time: 113.7315411567688 and batch: 350, loss is 3.5521392154693605 and perplexity is 34.88787038884005
At time: 114.28160548210144 and batch: 400, loss is 3.4912399768829347 and perplexity is 32.82662673879057
At time: 114.83058190345764 and batch: 450, loss is 3.5209942722320555 and perplexity is 33.818036087741994
At time: 115.38049697875977 and batch: 500, loss is 3.3900483322143553 and perplexity is 29.667386126183473
At time: 115.93134593963623 and batch: 550, loss is 3.4469490575790407 and perplexity is 31.404432883351642
At time: 116.48180389404297 and batch: 600, loss is 3.482096247673035 and perplexity is 32.52783706072907
At time: 117.03170824050903 and batch: 650, loss is 3.3099498891830446 and perplexity is 27.38375321527613
At time: 117.58155465126038 and batch: 700, loss is 3.3076257038116457 and perplexity is 27.320182200666977
At time: 118.132164478302 and batch: 750, loss is 3.413262701034546 and perplexity is 30.364151968557348
At time: 118.68262100219727 and batch: 800, loss is 3.361853313446045 and perplexity is 28.842595746493096
At time: 119.24511981010437 and batch: 850, loss is 3.422377109527588 and perplexity is 30.642168305754048
At time: 119.79402136802673 and batch: 900, loss is 3.3979675006866454 and perplexity is 29.90325988436468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2681341301904965 and perplexity of 71.38830998141299
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 121.21862745285034 and batch: 50, loss is 3.667055048942566 and perplexity is 39.13648096239327
At time: 121.77964162826538 and batch: 100, loss is 3.548662281036377 and perplexity is 34.76677818780432
At time: 122.32962083816528 and batch: 150, loss is 3.567813377380371 and perplexity is 35.43901661188908
At time: 122.87875533103943 and batch: 200, loss is 3.4493969011306764 and perplexity is 31.481400185403
At time: 123.4268856048584 and batch: 250, loss is 3.593460521697998 and perplexity is 36.359681938245
At time: 123.97563624382019 and batch: 300, loss is 3.568255763053894 and perplexity is 35.454697793431215
At time: 124.52432703971863 and batch: 350, loss is 3.5482551145553587 and perplexity is 34.75262520257937
At time: 125.07416033744812 and batch: 400, loss is 3.4873942852020265 and perplexity is 32.70062808470326
At time: 125.62463593482971 and batch: 450, loss is 3.5128722620010375 and perplexity is 33.54447807694193
At time: 126.17472767829895 and batch: 500, loss is 3.3810420560836794 and perplexity is 29.401393056130328
At time: 126.72428679466248 and batch: 550, loss is 3.4343952083587648 and perplexity is 31.01265070067412
At time: 127.27459669113159 and batch: 600, loss is 3.470729274749756 and perplexity is 32.16018750746455
At time: 127.8243978023529 and batch: 650, loss is 3.2952756547927855 and perplexity is 26.98485154842635
At time: 128.3746018409729 and batch: 700, loss is 3.292087264060974 and perplexity is 26.8989503139714
At time: 128.92419719696045 and batch: 750, loss is 3.39510630607605 and perplexity is 29.817823122231182
At time: 129.47338891029358 and batch: 800, loss is 3.3432845735549925 and perplexity is 28.3119668894703
At time: 130.04422402381897 and batch: 850, loss is 3.400477046966553 and perplexity is 29.978397740494522
At time: 130.61016964912415 and batch: 900, loss is 3.3754845380783083 and perplexity is 29.238447490689406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264002133722174 and perplexity of 71.09394231858306
finished 12 epochs...
Completing Train Step...
At time: 132.13499307632446 and batch: 50, loss is 3.65758722782135 and perplexity is 38.76769232857156
At time: 132.69428062438965 and batch: 100, loss is 3.536173253059387 and perplexity is 34.33527505833171
At time: 133.2443664073944 and batch: 150, loss is 3.555758013725281 and perplexity is 35.014351269721914
At time: 133.8051507472992 and batch: 200, loss is 3.4385777759552 and perplexity is 31.142634852831975
At time: 134.3551619052887 and batch: 250, loss is 3.582133560180664 and perplexity is 35.95016091304934
At time: 134.90393209457397 and batch: 300, loss is 3.5582571172714235 and perplexity is 35.10196519167796
At time: 135.4520366191864 and batch: 350, loss is 3.538328151702881 and perplexity is 34.40934387271063
At time: 135.9995653629303 and batch: 400, loss is 3.4786176252365113 and perplexity is 32.41488157556496
At time: 136.54729771614075 and batch: 450, loss is 3.5058335399627687 and perplexity is 33.309196830874825
At time: 137.0967891216278 and batch: 500, loss is 3.374558115005493 and perplexity is 29.211372861537725
At time: 137.64925146102905 and batch: 550, loss is 3.428633704185486 and perplexity is 30.834484928507447
At time: 138.19943165779114 and batch: 600, loss is 3.46629337310791 and perplexity is 32.0178440231563
At time: 138.74830222129822 and batch: 650, loss is 3.291870450973511 and perplexity is 26.893118901690407
At time: 139.2963728904724 and batch: 700, loss is 3.289669647216797 and perplexity is 26.833997505740022
At time: 139.8463478088379 and batch: 750, loss is 3.3941581058502197 and perplexity is 29.78956325573614
At time: 140.39534640312195 and batch: 800, loss is 3.3436486673355104 and perplexity is 28.32227697733452
At time: 140.94443249702454 and batch: 850, loss is 3.4024233436584472 and perplexity is 30.036801413846824
At time: 141.49413990974426 and batch: 900, loss is 3.378504376411438 and perplexity is 29.326876328418294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263606659353596 and perplexity of 71.06583204544751
finished 13 epochs...
Completing Train Step...
At time: 142.93777418136597 and batch: 50, loss is 3.6526194190979004 and perplexity is 38.57557943358945
At time: 143.48672771453857 and batch: 100, loss is 3.5304910564422607 and perplexity is 34.14072852382647
At time: 144.0337028503418 and batch: 150, loss is 3.549590563774109 and perplexity is 34.799066571898805
At time: 144.57936596870422 and batch: 200, loss is 3.4327255058288575 and perplexity is 30.96091200547543
At time: 145.12860083580017 and batch: 250, loss is 3.5760634994506835 and perplexity is 35.73260221830626
At time: 145.67520403862 and batch: 300, loss is 3.552565951347351 and perplexity is 34.90276147189538
At time: 146.22285270690918 and batch: 350, loss is 3.5328996992111206 and perplexity is 34.22306045701228
At time: 146.7713179588318 and batch: 400, loss is 3.4735914278030395 and perplexity is 32.25236673894056
At time: 147.33151960372925 and batch: 450, loss is 3.5013225746154784 and perplexity is 33.15927859043981
At time: 147.8806312084198 and batch: 500, loss is 3.3705318069458006 and perplexity is 29.09399533316364
At time: 148.42934918403625 and batch: 550, loss is 3.424950304031372 and perplexity is 30.72111809784527
At time: 148.978449344635 and batch: 600, loss is 3.46330246925354 and perplexity is 31.922224795297588
At time: 149.53668665885925 and batch: 650, loss is 3.2893883848190306 and perplexity is 26.826451172559533
At time: 150.08408665657043 and batch: 700, loss is 3.2878420877456667 and perplexity is 26.785001564585368
At time: 150.63162565231323 and batch: 750, loss is 3.393008408546448 and perplexity is 29.755333955617875
At time: 151.17968463897705 and batch: 800, loss is 3.343160881996155 and perplexity is 28.30846515472396
At time: 151.73032546043396 and batch: 850, loss is 3.4025647830963135 and perplexity is 30.04105010261309
At time: 152.2774794101715 and batch: 900, loss is 3.379079203605652 and perplexity is 29.343739060567398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263781821891053 and perplexity of 71.07828120719661
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 153.7096185684204 and batch: 50, loss is 3.650501594543457 and perplexity is 38.49396957244993
At time: 154.27230429649353 and batch: 100, loss is 3.530207052230835 and perplexity is 34.13103378988236
At time: 154.82154369354248 and batch: 150, loss is 3.5504374933242797 and perplexity is 34.82855141372639
At time: 155.3701457977295 and batch: 200, loss is 3.4321512603759765 and perplexity is 30.943137946363976
At time: 155.91830611228943 and batch: 250, loss is 3.5761847257614137 and perplexity is 35.7369342124165
At time: 156.46453738212585 and batch: 300, loss is 3.551491742134094 and perplexity is 34.86528873433752
At time: 157.01162147521973 and batch: 350, loss is 3.53164888381958 and perplexity is 34.18028048674723
At time: 157.56021118164062 and batch: 400, loss is 3.47334969997406 and perplexity is 32.24457138656406
At time: 158.10960745811462 and batch: 450, loss is 3.4989199829101563 and perplexity is 33.07970601123072
At time: 158.6596806049347 and batch: 500, loss is 3.3676333141326906 and perplexity is 29.00978869191154
At time: 159.2091088294983 and batch: 550, loss is 3.4210614204406737 and perplexity is 30.60187924906271
At time: 159.7578248977661 and batch: 600, loss is 3.4600150632858275 and perplexity is 31.817455786488793
At time: 160.3047182559967 and batch: 650, loss is 3.284836530685425 and perplexity is 26.704618572370016
At time: 160.8518886566162 and batch: 700, loss is 3.281973776817322 and perplexity is 26.628279144887156
At time: 161.41242337226868 and batch: 750, loss is 3.3849945402145387 and perplexity is 29.51783155468397
At time: 161.96034216880798 and batch: 800, loss is 3.3365613126754763 and perplexity is 28.122256600552372
At time: 162.51038932800293 and batch: 850, loss is 3.392887568473816 and perplexity is 29.751738536141247
At time: 163.05845594406128 and batch: 900, loss is 3.368984432220459 and perplexity is 29.049010833040953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264028888859161 and perplexity of 71.0958444721948
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 164.48136043548584 and batch: 50, loss is 3.648558897972107 and perplexity is 38.419260062195164
At time: 165.04355597496033 and batch: 100, loss is 3.5282697057724 and perplexity is 34.06497416330047
At time: 165.5917284488678 and batch: 150, loss is 3.5490868663787842 and perplexity is 34.781542786419855
At time: 166.13974833488464 and batch: 200, loss is 3.430859308242798 and perplexity is 30.9031867063888
At time: 166.68757581710815 and batch: 250, loss is 3.575325574874878 and perplexity is 35.7062439793641
At time: 167.23654174804688 and batch: 300, loss is 3.5500399923324584 and perplexity is 34.81470978120932
At time: 167.78612065315247 and batch: 350, loss is 3.529632453918457 and perplexity is 34.111427788793975
At time: 168.334046125412 and batch: 400, loss is 3.4719486093521117 and perplexity is 32.1994254541422
At time: 168.88171887397766 and batch: 450, loss is 3.4975687074661255 and perplexity is 33.03503640407063
At time: 169.43072271347046 and batch: 500, loss is 3.366076068878174 and perplexity is 28.964648492439366
At time: 169.97983503341675 and batch: 550, loss is 3.419305930137634 and perplexity is 30.54820507281833
At time: 170.5286009311676 and batch: 600, loss is 3.458358368873596 and perplexity is 31.764787624846694
At time: 171.0782904624939 and batch: 650, loss is 3.283272395133972 and perplexity is 26.662881578739956
At time: 171.6283564567566 and batch: 700, loss is 3.280037045478821 and perplexity is 26.576757230334746
At time: 172.1777741909027 and batch: 750, loss is 3.3827171468734742 and perplexity is 29.450684331000414
At time: 172.72821044921875 and batch: 800, loss is 3.334786682128906 and perplexity is 28.072394241669162
At time: 173.27791118621826 and batch: 850, loss is 3.39031551361084 and perplexity is 29.675313758849533
At time: 173.8274736404419 and batch: 900, loss is 3.3660796451568604 and perplexity is 28.96475207827966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263864177547089 and perplexity of 71.08413514672456
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 175.2644739151001 and batch: 50, loss is 3.647996301651001 and perplexity is 38.39765160681326
At time: 175.81417202949524 and batch: 100, loss is 3.527734246253967 and perplexity is 34.04673863126986
At time: 176.3633997440338 and batch: 150, loss is 3.5486378192901613 and perplexity is 34.76592774210128
At time: 176.91251683235168 and batch: 200, loss is 3.430508937835693 and perplexity is 30.892361040888815
At time: 177.46034455299377 and batch: 250, loss is 3.5750525760650635 and perplexity is 35.696497547697454
At time: 178.00960183143616 and batch: 300, loss is 3.5496338033676147 and perplexity is 34.800571301923476
At time: 178.55873799324036 and batch: 350, loss is 3.529124689102173 and perplexity is 34.09411160257672
At time: 179.10752987861633 and batch: 400, loss is 3.4715472984313966 and perplexity is 32.18650606558595
At time: 179.65670728683472 and batch: 450, loss is 3.497188591957092 and perplexity is 33.02248166066947
At time: 180.20647311210632 and batch: 500, loss is 3.365660214424133 and perplexity is 28.952605918506613
At time: 180.75404453277588 and batch: 550, loss is 3.418832416534424 and perplexity is 30.53374350630714
At time: 181.30103421211243 and batch: 600, loss is 3.457898807525635 and perplexity is 31.75019311002243
At time: 181.84551286697388 and batch: 650, loss is 3.2828419256210326 and perplexity is 26.651406491108087
At time: 182.39055585861206 and batch: 700, loss is 3.2795157718658445 and perplexity is 26.56290707824513
At time: 182.93484950065613 and batch: 750, loss is 3.382121968269348 and perplexity is 29.433161129044446
At time: 183.47871828079224 and batch: 800, loss is 3.3343086004257203 and perplexity is 28.058976551249312
At time: 184.0231478214264 and batch: 850, loss is 3.3896726179122925 and perplexity is 29.656241758591285
At time: 184.56899094581604 and batch: 900, loss is 3.365350294113159 and perplexity is 28.943634308190777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263812757518194 and perplexity of 71.08048009241362
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 185.98352456092834 and batch: 50, loss is 3.6478509426116945 and perplexity is 38.392070566701214
At time: 186.54397773742676 and batch: 100, loss is 3.5275961685180666 and perplexity is 34.04203785922831
At time: 187.09305620193481 and batch: 150, loss is 3.5485181617736816 and perplexity is 34.761767986407534
At time: 187.64256191253662 and batch: 200, loss is 3.430417060852051 and perplexity is 30.889522874321873
At time: 188.19269967079163 and batch: 250, loss is 3.5749769973754884 and perplexity is 35.69379975513947
At time: 188.75468564033508 and batch: 300, loss is 3.5495285892486574 and perplexity is 34.79690998308933
At time: 189.3020842075348 and batch: 350, loss is 3.528998351097107 and perplexity is 34.08980449261392
At time: 189.8528459072113 and batch: 400, loss is 3.471443200111389 and perplexity is 32.18315567876546
At time: 190.40243363380432 and batch: 450, loss is 3.497089991569519 and perplexity is 33.019225791696705
At time: 190.95146369934082 and batch: 500, loss is 3.3655539321899415 and perplexity is 28.949528934381192
At time: 191.49837040901184 and batch: 550, loss is 3.4187116050720214 and perplexity is 30.53005490291879
At time: 192.04348874092102 and batch: 600, loss is 3.457781267166138 and perplexity is 31.746461400227727
At time: 192.58916401863098 and batch: 650, loss is 3.2827322721481322 and perplexity is 26.648484232049512
At time: 193.1380100250244 and batch: 700, loss is 3.279383072853088 and perplexity is 26.55938244056305
At time: 193.68644404411316 and batch: 750, loss is 3.381971001625061 and perplexity is 29.428718038865686
At time: 194.23643493652344 and batch: 800, loss is 3.3341862201690673 and perplexity is 28.0555428966076
At time: 194.78665161132812 and batch: 850, loss is 3.3895109605789187 and perplexity is 29.65144799711449
At time: 195.3354787826538 and batch: 900, loss is 3.365166983604431 and perplexity is 28.938329122124344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2638006340967465 and perplexity of 71.07961835902039
Annealing...
Model not improving. Stopping early with 71.06583204544751 lossat 17 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8858900b70>
ELAPSED
856.8586447238922


RESULTS SO FAR:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.78276706776143, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.87113706675376, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.22372141185209682, 'rnn_dropout': 0.5465833484205133, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.06583204544751, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.7150731284420337, 'rnn_dropout': 0.7542520583353851, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.29830252145929015, 'rnn_dropout': 0.8293988567732831, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8476321697235107 and batch: 50, loss is 6.917507429122924 and perplexity is 1009.7998573133606
At time: 1.4802882671356201 and batch: 100, loss is 6.008575229644776 and perplexity is 406.9031635121463
At time: 2.0997228622436523 and batch: 150, loss is 5.840829019546509 and perplexity is 344.0644586259155
At time: 2.72245192527771 and batch: 200, loss is 5.641892547607422 and perplexity is 281.99590445048557
At time: 3.365074872970581 and batch: 250, loss is 5.668339834213257 and perplexity is 289.5534284490766
At time: 3.9871726036071777 and batch: 300, loss is 5.566711797714233 and perplexity is 261.5725800362502
At time: 4.602346897125244 and batch: 350, loss is 5.5321555519104 and perplexity is 252.6880065149794
At time: 5.217451333999634 and batch: 400, loss is 5.387332992553711 and perplexity is 218.6195473888638
At time: 5.833911418914795 and batch: 450, loss is 5.385953187942505 and perplexity is 218.31810314418192
At time: 6.453026533126831 and batch: 500, loss is 5.326159515380859 and perplexity is 205.64667275478158
At time: 7.066847085952759 and batch: 550, loss is 5.380012083053589 and perplexity is 217.0248977325741
At time: 7.690706253051758 and batch: 600, loss is 5.29589614868164 and perplexity is 199.51634218347277
At time: 8.305987119674683 and batch: 650, loss is 5.185426855087281 and perplexity is 178.64969104614818
At time: 8.941604852676392 and batch: 700, loss is 5.274621238708496 and perplexity is 195.31648417692034
At time: 9.564035892486572 and batch: 750, loss is 5.249256982803344 and perplexity is 190.42472703443065
At time: 10.197099447250366 and batch: 800, loss is 5.228019046783447 and perplexity is 186.42314197720188
At time: 10.817585468292236 and batch: 850, loss is 5.267430696487427 and perplexity is 193.91708998200255
At time: 11.436365127563477 and batch: 900, loss is 5.166035680770874 and perplexity is 175.2188354098623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.076097044226241 and perplexity of 160.14778484791734
finished 1 epochs...
Completing Train Step...
At time: 12.921285152435303 and batch: 50, loss is 5.009844198226928 and perplexity is 149.8813825446766
At time: 13.46999454498291 and batch: 100, loss is 4.862810230255127 and perplexity is 129.3872997788545
At time: 14.01887583732605 and batch: 150, loss is 4.832435760498047 and perplexity is 125.51631630229154
At time: 14.567803859710693 and batch: 200, loss is 4.716955518722534 and perplexity is 111.82727783175899
At time: 15.115961790084839 and batch: 250, loss is 4.801421737670898 and perplexity is 121.68329632176066
At time: 15.664041757583618 and batch: 300, loss is 4.742739086151123 and perplexity is 114.74807648602302
At time: 16.21217393875122 and batch: 350, loss is 4.722434787750244 and perplexity is 112.44169130280754
At time: 16.759238958358765 and batch: 400, loss is 4.585522527694702 and perplexity is 98.05440990304969
At time: 17.306994676589966 and batch: 450, loss is 4.602831916809082 and perplexity is 99.76644624428636
At time: 17.87335467338562 and batch: 500, loss is 4.502839021682739 and perplexity is 90.27305500267539
At time: 18.421675205230713 and batch: 550, loss is 4.572791786193847 and perplexity is 96.81401687175574
At time: 18.973705768585205 and batch: 600, loss is 4.533107414245605 and perplexity is 93.04724865986816
At time: 19.52456045150757 and batch: 650, loss is 4.385388622283935 and perplexity is 80.26941163433023
At time: 20.0759699344635 and batch: 700, loss is 4.437281904220581 and perplexity is 84.54482813965093
At time: 20.647526741027832 and batch: 750, loss is 4.481639337539673 and perplexity is 88.37943771382973
At time: 21.19392466545105 and batch: 800, loss is 4.42964651107788 and perplexity is 83.9017533273561
At time: 21.740370750427246 and batch: 850, loss is 4.49438549041748 and perplexity is 89.51314539500851
At time: 22.2857506275177 and batch: 900, loss is 4.420506858825684 and perplexity is 83.13841411972453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.557502642069777 and perplexity of 95.34507148810478
finished 2 epochs...
Completing Train Step...
At time: 23.744463443756104 and batch: 50, loss is 4.468739080429077 and perplexity is 87.24664262758301
At time: 24.308910369873047 and batch: 100, loss is 4.335180788040161 and perplexity is 76.33875877689047
At time: 24.852705240249634 and batch: 150, loss is 4.330281343460083 and perplexity is 75.96565600347856
At time: 25.396918535232544 and batch: 200, loss is 4.235906100273132 and perplexity is 69.12428389847678
At time: 25.94319248199463 and batch: 250, loss is 4.367301740646362 and perplexity is 78.830638965031
At time: 26.487414121627808 and batch: 300, loss is 4.336190228462219 and perplexity is 76.41585711231096
At time: 27.032118558883667 and batch: 350, loss is 4.325058698654175 and perplexity is 75.56994858381775
At time: 27.590423107147217 and batch: 400, loss is 4.227576637268067 and perplexity is 68.55090700885583
At time: 28.171684980392456 and batch: 450, loss is 4.266495413780213 and perplexity is 71.27142058678642
At time: 28.72266411781311 and batch: 500, loss is 4.146726465225219 and perplexity is 63.22668640199989
At time: 29.26898765563965 and batch: 550, loss is 4.230107779502869 and perplexity is 68.72463888210717
At time: 29.81554079055786 and batch: 600, loss is 4.224917492866516 and perplexity is 68.36886239698777
At time: 30.36213731765747 and batch: 650, loss is 4.072357006072998 and perplexity is 58.69514449215464
At time: 30.910149097442627 and batch: 700, loss is 4.1020139741897585 and perplexity is 60.461933834259
At time: 31.457255125045776 and batch: 750, loss is 4.1855881690979 and perplexity is 65.73215116923764
At time: 32.00460648536682 and batch: 800, loss is 4.1399396848678585 and perplexity is 62.79903360296048
At time: 32.55110716819763 and batch: 850, loss is 4.219189476966858 and perplexity is 67.97836392455845
At time: 33.097989559173584 and batch: 900, loss is 4.159999852180481 and perplexity is 64.07151312891567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.421661376953125 and perplexity of 83.23445435534119
finished 3 epochs...
Completing Train Step...
At time: 34.52136254310608 and batch: 50, loss is 4.232361660003662 and perplexity is 68.87971069663035
At time: 35.08256816864014 and batch: 100, loss is 4.104270935058594 and perplexity is 60.59854816158115
At time: 35.63005566596985 and batch: 150, loss is 4.101449165344238 and perplexity is 60.42779404134824
At time: 36.17480278015137 and batch: 200, loss is 4.008168988227844 and perplexity is 55.045988380681514
At time: 36.72204566001892 and batch: 250, loss is 4.152955784797668 and perplexity is 63.6217749258445
At time: 37.269885540008545 and batch: 300, loss is 4.126879787445068 and perplexity is 61.9842169580461
At time: 37.81856989860535 and batch: 350, loss is 4.115916156768799 and perplexity is 61.30835660712024
At time: 38.37890410423279 and batch: 400, loss is 4.034443225860596 and perplexity is 56.51144736502189
At time: 38.92670202255249 and batch: 450, loss is 4.075338568687439 and perplexity is 58.87040889167472
At time: 39.47373366355896 and batch: 500, loss is 3.950994277000427 and perplexity is 51.987030655402144
At time: 40.02159667015076 and batch: 550, loss is 4.036727228164673 and perplexity is 56.64066715398349
At time: 40.56910729408264 and batch: 600, loss is 4.044438910484314 and perplexity is 57.07915053466467
At time: 41.11847472190857 and batch: 650, loss is 3.888683943748474 and perplexity is 48.84655938410634
At time: 41.66615056991577 and batch: 700, loss is 3.9122046184539796 and perplexity is 50.00908147592388
At time: 42.213876485824585 and batch: 750, loss is 4.007867841720581 and perplexity is 55.02941396933016
At time: 42.76155376434326 and batch: 800, loss is 3.968102397918701 and perplexity is 52.884082619196214
At time: 43.30949306488037 and batch: 850, loss is 4.045510377883911 and perplexity is 57.14034176000661
At time: 43.856356620788574 and batch: 900, loss is 3.997728352546692 and perplexity is 54.47426305171438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360495789410317 and perplexity of 78.29594310128385
finished 4 epochs...
Completing Train Step...
At time: 45.28719139099121 and batch: 50, loss is 4.0758654451370235 and perplexity is 58.901434496309655
At time: 45.83540391921997 and batch: 100, loss is 3.950073490142822 and perplexity is 51.93918371260737
At time: 46.413694620132446 and batch: 150, loss is 3.9488603401184084 and perplexity is 51.87621189547228
At time: 46.98646545410156 and batch: 200, loss is 3.860635938644409 and perplexity is 47.4955460200146
At time: 47.5525643825531 and batch: 250, loss is 4.005486965179443 and perplexity is 54.89855157395856
At time: 48.12655973434448 and batch: 300, loss is 3.987204852104187 and perplexity is 53.90400891787795
At time: 48.69044041633606 and batch: 350, loss is 3.9732163524627686 and perplexity is 53.15522212044264
At time: 49.237067222595215 and batch: 400, loss is 3.900271487236023 and perplexity is 49.41586306066702
At time: 49.784265756607056 and batch: 450, loss is 3.9418529987335207 and perplexity is 51.51396823373173
At time: 50.33015465736389 and batch: 500, loss is 3.8176124620437624 and perplexity is 45.4954564188257
At time: 50.87710499763489 and batch: 550, loss is 3.9019876813888548 and perplexity is 49.50074309038096
At time: 51.42555856704712 and batch: 600, loss is 3.916575608253479 and perplexity is 50.22814908329223
At time: 51.985976457595825 and batch: 650, loss is 3.764438624382019 and perplexity is 43.139481607896975
At time: 52.53244376182556 and batch: 700, loss is 3.77990439414978 and perplexity is 43.81185286589398
At time: 53.078967809677124 and batch: 750, loss is 3.8826106214523315 and perplexity is 48.55079752356346
At time: 53.633814334869385 and batch: 800, loss is 3.8470163679122926 and perplexity is 46.853062179895076
At time: 54.17980766296387 and batch: 850, loss is 3.920829162597656 and perplexity is 50.44225227199057
At time: 54.72716760635376 and batch: 900, loss is 3.8780379247665406 and perplexity is 48.32929626762202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335443784112799 and perplexity of 76.35883821092926
finished 5 epochs...
Completing Train Step...
At time: 56.14468765258789 and batch: 50, loss is 3.9637404584884646 and perplexity is 52.65390782319882
At time: 56.705151319503784 and batch: 100, loss is 3.835388913154602 and perplexity is 46.311435292416164
At time: 57.25213885307312 and batch: 150, loss is 3.8331590557098387 and perplexity is 46.2082824444666
At time: 57.79866909980774 and batch: 200, loss is 3.7514547014236452 and perplexity is 42.582982491033455
At time: 58.34581160545349 and batch: 250, loss is 3.897939682006836 and perplexity is 49.30076913323843
At time: 58.89114427566528 and batch: 300, loss is 3.881572470664978 and perplexity is 48.5004206288187
At time: 59.43787503242493 and batch: 350, loss is 3.8666778707504275 and perplexity is 47.78337954409929
At time: 59.98418951034546 and batch: 400, loss is 3.795307087898254 and perplexity is 44.49189723109432
At time: 60.52812576293945 and batch: 450, loss is 3.8415113735198974 and perplexity is 46.59584497454425
At time: 61.075185775756836 and batch: 500, loss is 3.7133652067184446 and perplexity is 40.991519617735726
At time: 61.624138832092285 and batch: 550, loss is 3.800324196815491 and perplexity is 44.71567882434325
At time: 62.17034673690796 and batch: 600, loss is 3.816185474395752 and perplexity is 45.430581263506824
At time: 62.717891454696655 and batch: 650, loss is 3.667688069343567 and perplexity is 39.161262996204805
At time: 63.268184185028076 and batch: 700, loss is 3.6795115280151367 and perplexity is 39.627032648908795
At time: 63.814748764038086 and batch: 750, loss is 3.782787036895752 and perplexity is 43.938328990906804
At time: 64.36288619041443 and batch: 800, loss is 3.751833076477051 and perplexity is 42.599097877945596
At time: 64.9099428653717 and batch: 850, loss is 3.8235119676589964 and perplexity is 45.764650394365795
At time: 65.45689582824707 and batch: 900, loss is 3.7821295833587647 and perplexity is 43.90945107508656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325534768300514 and perplexity of 75.60593370754684
finished 6 epochs...
Completing Train Step...
At time: 66.88584780693054 and batch: 50, loss is 3.8729336881637573 and perplexity is 48.08324060200531
At time: 67.44469094276428 and batch: 100, loss is 3.744984016418457 and perplexity is 42.30833097477534
At time: 67.99523186683655 and batch: 150, loss is 3.746672945022583 and perplexity is 42.379847100967204
At time: 68.54157257080078 and batch: 200, loss is 3.664310483932495 and perplexity is 39.02921561182313
At time: 69.08784365653992 and batch: 250, loss is 3.8097391176223754 and perplexity is 45.13866144880362
At time: 69.63510513305664 and batch: 300, loss is 3.799590353965759 and perplexity is 44.682876580485235
At time: 70.18172192573547 and batch: 350, loss is 3.782651357650757 and perplexity is 43.9323678760119
At time: 70.72798466682434 and batch: 400, loss is 3.715111494064331 and perplexity is 41.06316512835009
At time: 71.27365565299988 and batch: 450, loss is 3.7558488750457766 and perplexity is 42.77051122460893
At time: 71.82064247131348 and batch: 500, loss is 3.6350653266906736 and perplexity is 37.904328980638596
At time: 72.3642692565918 and batch: 550, loss is 3.716624183654785 and perplexity is 41.12532795546802
At time: 72.9088704586029 and batch: 600, loss is 3.737105255126953 and perplexity is 41.976303434793685
At time: 73.45647597312927 and batch: 650, loss is 3.593734693527222 and perplexity is 36.36965210545904
At time: 74.00570368766785 and batch: 700, loss is 3.599385142326355 and perplexity is 36.57573865495131
At time: 74.55495357513428 and batch: 750, loss is 3.7017000770568846 and perplexity is 40.516126379014345
At time: 75.1042730808258 and batch: 800, loss is 3.674420466423035 and perplexity is 39.42580165930257
At time: 75.65312314033508 and batch: 850, loss is 3.7482512998580932 and perplexity is 42.44679035377314
At time: 76.20186972618103 and batch: 900, loss is 3.7086848258972167 and perplexity is 40.80011197522737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332030831950984 and perplexity of 76.09867336660605
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 77.63173246383667 and batch: 50, loss is 3.821298656463623 and perplexity is 45.66347099335045
At time: 78.17652797698975 and batch: 100, loss is 3.6867590665817263 and perplexity is 39.91527435605531
At time: 78.72181916236877 and batch: 150, loss is 3.6871397924423217 and perplexity is 39.930474026505635
At time: 79.26813864707947 and batch: 200, loss is 3.5879274368286134 and perplexity is 36.159056283490635
At time: 79.8256425857544 and batch: 250, loss is 3.7237331008911134 and perplexity is 41.41872614432749
At time: 80.37115931510925 and batch: 300, loss is 3.703447489738464 and perplexity is 40.58698666510808
At time: 80.91696071624756 and batch: 350, loss is 3.673884687423706 and perplexity is 39.40468380049962
At time: 81.46231603622437 and batch: 400, loss is 3.6004918670654296 and perplexity is 36.61624033774382
At time: 82.00811123847961 and batch: 450, loss is 3.6326841259002687 and perplexity is 37.81417853825942
At time: 82.55336618423462 and batch: 500, loss is 3.5020461654663086 and perplexity is 33.18328102397137
At time: 83.09816646575928 and batch: 550, loss is 3.5670455837249757 and perplexity is 35.411817202884045
At time: 83.6433436870575 and batch: 600, loss is 3.579666590690613 and perplexity is 35.8615822682452
At time: 84.1887354850769 and batch: 650, loss is 3.422432985305786 and perplexity is 30.643880508588694
At time: 84.73142862319946 and batch: 700, loss is 3.415627369880676 and perplexity is 30.43603809269878
At time: 85.27683138847351 and batch: 750, loss is 3.504542279243469 and perplexity is 33.266213730582535
At time: 85.82279515266418 and batch: 800, loss is 3.4597762393951417 and perplexity is 31.80985792521765
At time: 86.36780834197998 and batch: 850, loss is 3.516414918899536 and perplexity is 33.663525401011896
At time: 86.91198515892029 and batch: 900, loss is 3.4732699394226074 and perplexity is 32.241999644332076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.278566961419092 and perplexity of 72.13699081074166
finished 8 epochs...
Completing Train Step...
At time: 88.3292875289917 and batch: 50, loss is 3.7296794652938843 and perplexity is 41.665750704221864
At time: 88.91390204429626 and batch: 100, loss is 3.595521483421326 and perplexity is 36.43469512409255
At time: 89.46785593032837 and batch: 150, loss is 3.599079704284668 and perplexity is 36.56456873890876
At time: 90.01089262962341 and batch: 200, loss is 3.503897614479065 and perplexity is 33.244775085857185
At time: 90.55561685562134 and batch: 250, loss is 3.640468020439148 and perplexity is 38.109668656079705
At time: 91.09943175315857 and batch: 300, loss is 3.6271568822860716 and perplexity is 37.60574691821738
At time: 91.64241456985474 and batch: 350, loss is 3.6009692478179933 and perplexity is 36.63372439905744
At time: 92.18818545341492 and batch: 400, loss is 3.5321837186813356 and perplexity is 34.198566181813845
At time: 92.73881793022156 and batch: 450, loss is 3.568929409980774 and perplexity is 35.4785897881241
At time: 93.31789755821228 and batch: 500, loss is 3.44203800201416 and perplexity is 31.250582062662858
At time: 93.90021705627441 and batch: 550, loss is 3.5111323547363282 and perplexity is 33.48616454062864
At time: 94.44730043411255 and batch: 600, loss is 3.5297295951843264 and perplexity is 34.114741577020176
At time: 94.99367570877075 and batch: 650, loss is 3.378405284881592 and perplexity is 29.323970427354777
At time: 95.54072952270508 and batch: 700, loss is 3.3754751253128052 and perplexity is 29.238172277334765
At time: 96.0880663394928 and batch: 750, loss is 3.4707251262664793 and perplexity is 32.160054091741245
At time: 96.63242316246033 and batch: 800, loss is 3.431061487197876 and perplexity is 30.909435312032677
At time: 97.17871284484863 and batch: 850, loss is 3.4945287036895754 and perplexity is 32.93476226275257
At time: 97.72498989105225 and batch: 900, loss is 3.458894143104553 and perplexity is 31.781810939427324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.283082726883562 and perplexity of 72.46348116466841
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 99.13918137550354 and batch: 50, loss is 3.7020767307281495 and perplexity is 40.53138980109196
At time: 99.69896268844604 and batch: 100, loss is 3.576777114868164 and perplexity is 35.75811065468017
At time: 100.24553680419922 and batch: 150, loss is 3.5872555065155027 and perplexity is 36.13476807838474
At time: 100.79191422462463 and batch: 200, loss is 3.4845916032791138 and perplexity is 32.60910693774285
At time: 101.39020919799805 and batch: 250, loss is 3.617903847694397 and perplexity is 37.25938456381351
At time: 101.96603584289551 and batch: 300, loss is 3.6021667432785036 and perplexity is 36.67761939451036
At time: 102.517582654953 and batch: 350, loss is 3.57249059677124 and perplexity is 35.60516091136291
At time: 103.06470346450806 and batch: 400, loss is 3.5041779613494874 and perplexity is 33.254096461058545
At time: 103.63296604156494 and batch: 450, loss is 3.5328580570220947 and perplexity is 34.221635363531895
At time: 104.19170784950256 and batch: 500, loss is 3.4040446424484254 and perplexity is 30.08553954249228
At time: 104.75315499305725 and batch: 550, loss is 3.4681377840042114 and perplexity is 32.076952577022546
At time: 105.30771660804749 and batch: 600, loss is 3.487283887863159 and perplexity is 32.69701822164672
At time: 105.85623025894165 and batch: 650, loss is 3.3274768352508546 and perplexity is 27.86793752232658
At time: 106.40455889701843 and batch: 700, loss is 3.319184193611145 and perplexity is 27.637794269846353
At time: 106.9534068107605 and batch: 750, loss is 3.4140845680236818 and perplexity is 30.389117520489627
At time: 107.51475548744202 and batch: 800, loss is 3.3676704597473144 and perplexity is 29.01086629835666
At time: 108.06481266021729 and batch: 850, loss is 3.426690950393677 and perplexity is 30.77463926750944
At time: 108.61040496826172 and batch: 900, loss is 3.3938338947296143 and perplexity is 29.779906713514425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269007852632705 and perplexity of 71.45071080648604
finished 10 epochs...
Completing Train Step...
At time: 110.04300999641418 and batch: 50, loss is 3.6746710681915284 and perplexity is 39.43568307302083
At time: 110.59254550933838 and batch: 100, loss is 3.543677082061768 and perplexity is 34.59389017944693
At time: 111.1413962841034 and batch: 150, loss is 3.5544912576675416 and perplexity is 34.97002470953996
At time: 111.68984842300415 and batch: 200, loss is 3.454581575393677 and perplexity is 31.645044846262763
At time: 112.23887753486633 and batch: 250, loss is 3.5883398151397703 and perplexity is 36.17397056900643
At time: 112.78873205184937 and batch: 300, loss is 3.5746859073638917 and perplexity is 35.68341115863652
At time: 113.33798384666443 and batch: 350, loss is 3.546148600578308 and perplexity is 34.67949536340731
At time: 113.8868887424469 and batch: 400, loss is 3.480074362754822 and perplexity is 32.462135959968535
At time: 114.43715858459473 and batch: 450, loss is 3.51158588886261 and perplexity is 33.501355103465045
At time: 114.98731923103333 and batch: 500, loss is 3.384220185279846 and perplexity is 29.494983123697526
At time: 115.5369029045105 and batch: 550, loss is 3.4510637807846067 and perplexity is 31.5339196505028
At time: 116.08520293235779 and batch: 600, loss is 3.4725947618484496 and perplexity is 32.220237916568195
At time: 116.63337779045105 and batch: 650, loss is 3.3152615070343017 and perplexity is 27.529592225350513
At time: 117.18189072608948 and batch: 700, loss is 3.3102419137954713 and perplexity is 27.391751112930255
At time: 117.73161673545837 and batch: 750, loss is 3.4077941465377806 and perplexity is 30.198557143608646
At time: 118.28089261054993 and batch: 800, loss is 3.3637372922897337 and perplexity is 28.896985805449177
At time: 118.830983877182 and batch: 850, loss is 3.425570240020752 and perplexity is 30.74016912918796
At time: 119.3787305355072 and batch: 900, loss is 3.395395293235779 and perplexity is 29.826441335461077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269638270547945 and perplexity of 71.495768815831
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 120.80139756202698 and batch: 50, loss is 3.6666133499145506 and perplexity is 39.1191982339554
At time: 121.36127090454102 and batch: 100, loss is 3.5405071592330932 and perplexity is 34.4844038405373
At time: 121.91004300117493 and batch: 150, loss is 3.553491163253784 and perplexity is 34.93506886566462
At time: 122.45810103416443 and batch: 200, loss is 3.451310324668884 and perplexity is 31.541695103995792
At time: 123.00712537765503 and batch: 250, loss is 3.5860132074356077 and perplexity is 36.0899057612261
At time: 123.55713391304016 and batch: 300, loss is 3.571562638282776 and perplexity is 35.572136125247205
At time: 124.10571074485779 and batch: 350, loss is 3.540662703514099 and perplexity is 34.48976810951846
At time: 124.65499973297119 and batch: 400, loss is 3.4743949031829833 and perplexity is 32.27829113497252
At time: 125.20421838760376 and batch: 450, loss is 3.5049903106689455 and perplexity is 33.281121379038574
At time: 125.78388357162476 and batch: 500, loss is 3.3726245307922365 and perplexity is 29.15494478392599
At time: 126.36097693443298 and batch: 550, loss is 3.4386327266693115 and perplexity is 31.144346209876165
At time: 126.91332769393921 and batch: 600, loss is 3.4626566314697267 and perplexity is 31.901614872431228
At time: 127.47696900367737 and batch: 650, loss is 3.300366048812866 and perplexity is 27.12256528660486
At time: 128.05062198638916 and batch: 700, loss is 3.292903838157654 and perplexity is 26.92092427048651
At time: 128.59974837303162 and batch: 750, loss is 3.388124361038208 and perplexity is 29.61036180457104
At time: 129.14868879318237 and batch: 800, loss is 3.342259855270386 and perplexity is 28.282969958705404
At time: 129.69830894470215 and batch: 850, loss is 3.4016690921783446 and perplexity is 30.01415465367271
At time: 130.2516577243805 and batch: 900, loss is 3.3720805931091307 and perplexity is 29.139090623037813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264707800460188 and perplexity of 71.14412865428288
finished 12 epochs...
Completing Train Step...
At time: 131.73777198791504 and batch: 50, loss is 3.6583809423446656 and perplexity is 38.79847502372824
At time: 132.31182646751404 and batch: 100, loss is 3.528432459831238 and perplexity is 34.070518827305804
At time: 132.8615608215332 and batch: 150, loss is 3.541690649986267 and perplexity is 34.5252399734377
At time: 133.40909814834595 and batch: 200, loss is 3.439901385307312 and perplexity is 31.183882827653203
At time: 133.95822072029114 and batch: 250, loss is 3.5747098445892336 and perplexity is 35.6842653307136
At time: 134.50769591331482 and batch: 300, loss is 3.561512966156006 and perplexity is 35.216438138078345
At time: 135.05546498298645 and batch: 350, loss is 3.530717806816101 and perplexity is 34.14847082453405
At time: 135.61594939231873 and batch: 400, loss is 3.465917692184448 and perplexity is 32.00581778910122
At time: 136.1644582748413 and batch: 450, loss is 3.497256121635437 and perplexity is 33.02471173353137
At time: 136.71285772323608 and batch: 500, loss is 3.3658887910842896 and perplexity is 28.959224564875516
At time: 137.26218557357788 and batch: 550, loss is 3.43304518699646 and perplexity is 30.970811208185413
At time: 137.8118073940277 and batch: 600, loss is 3.4582191419601442 and perplexity is 31.760365419361385
At time: 138.36085987091064 and batch: 650, loss is 3.297119164466858 and perplexity is 27.034644265885618
At time: 138.90960955619812 and batch: 700, loss is 3.2907976388931273 and perplexity is 26.86428310931234
At time: 139.45792293548584 and batch: 750, loss is 3.3875462436676025 and perplexity is 29.593248487291525
At time: 140.00633573532104 and batch: 800, loss is 3.342835659980774 and perplexity is 28.299260115555832
At time: 140.5546760559082 and batch: 850, loss is 3.4036372232437135 and perplexity is 30.073284612514982
At time: 141.10413479804993 and batch: 900, loss is 3.375390067100525 and perplexity is 29.235685436435134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26420655969071 and perplexity of 71.10847725220691
finished 13 epochs...
Completing Train Step...
At time: 142.5435073375702 and batch: 50, loss is 3.65335458278656 and perplexity is 38.60394922580084
At time: 143.09486627578735 and batch: 100, loss is 3.522738523483276 and perplexity is 33.877074713632084
At time: 143.6453025341034 and batch: 150, loss is 3.535705213546753 and perplexity is 34.31920855310018
At time: 144.19537329673767 and batch: 200, loss is 3.4339112520217894 and perplexity is 30.997645563049712
At time: 144.74601459503174 and batch: 250, loss is 3.5686777877807616 and perplexity is 35.469663710355036
At time: 145.2935106754303 and batch: 300, loss is 3.5559109115600585 and perplexity is 35.019705297516396
At time: 145.84287214279175 and batch: 350, loss is 3.5252149152755736 and perplexity is 33.96107158505544
At time: 146.39351677894592 and batch: 400, loss is 3.4609650278091433 and perplexity is 31.847695601809882
At time: 146.94437408447266 and batch: 450, loss is 3.4926830768585204 and perplexity is 32.87403304082542
At time: 147.4946792125702 and batch: 500, loss is 3.36176176071167 and perplexity is 28.839955248860264
At time: 148.0460467338562 and batch: 550, loss is 3.429514741897583 and perplexity is 30.861663243364674
At time: 148.59626698493958 and batch: 600, loss is 3.4552776050567626 and perplexity is 31.66707840328055
At time: 149.15852618217468 and batch: 650, loss is 3.294692850112915 and perplexity is 26.969129232622954
At time: 149.70902824401855 and batch: 700, loss is 3.2891504859924314 and perplexity is 26.82006995037649
At time: 150.25909781455994 and batch: 750, loss is 3.3865155220031737 and perplexity is 29.5627617992988
At time: 150.80916714668274 and batch: 800, loss is 3.3424070358276365 and perplexity is 28.287132968334152
At time: 151.35880208015442 and batch: 850, loss is 3.4037904834747312 and perplexity is 30.077894004271357
At time: 151.90815782546997 and batch: 900, loss is 3.376091079711914 and perplexity is 29.256187205788535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26434786025792 and perplexity of 71.11852563028009
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 153.32710599899292 and batch: 50, loss is 3.6513675928115843 and perplexity is 38.52731972193461
At time: 153.88953757286072 and batch: 100, loss is 3.522267532348633 and perplexity is 33.86112266870508
At time: 154.43835544586182 and batch: 150, loss is 3.5354122352600097 and perplexity is 34.30915524294862
At time: 154.98665046691895 and batch: 200, loss is 3.432904362678528 and perplexity is 30.96645007190544
At time: 155.53599762916565 and batch: 250, loss is 3.567797136306763 and perplexity is 35.43844104888558
At time: 156.0851776599884 and batch: 300, loss is 3.5550447607040407 and perplexity is 34.989386082198116
At time: 156.63513779640198 and batch: 350, loss is 3.5239278841018677 and perplexity is 33.9173907425656
At time: 157.1835596561432 and batch: 400, loss is 3.4602111196517944 and perplexity is 31.823694412785823
At time: 157.73086619377136 and batch: 450, loss is 3.4913077116012574 and perplexity is 32.82885031641206
At time: 158.28022933006287 and batch: 500, loss is 3.3589671230316163 and perplexity is 28.759470538415098
At time: 158.8296196460724 and batch: 550, loss is 3.4250393104553223 and perplexity is 30.723852596399077
At time: 159.37901377677917 and batch: 600, loss is 3.451612105369568 and perplexity is 31.55121521526571
At time: 159.92862129211426 and batch: 650, loss is 3.2899621963500976 and perplexity is 26.841848916859078
At time: 160.47787165641785 and batch: 700, loss is 3.2829702615737917 and perplexity is 26.65482704423777
At time: 161.02714848518372 and batch: 750, loss is 3.3787018251419068 and perplexity is 29.3326674546244
At time: 161.5772783756256 and batch: 800, loss is 3.334071383476257 and perplexity is 28.052321275830042
At time: 162.12627744674683 and batch: 850, loss is 3.394420065879822 and perplexity is 29.797367952823155
At time: 162.67629098892212 and batch: 900, loss is 3.3659588289260864 and perplexity is 28.961252877492637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263397216796875 and perplexity of 71.05094939447304
finished 15 epochs...
Completing Train Step...
At time: 164.13191676139832 and batch: 50, loss is 3.6488692712783815 and perplexity is 38.43118622565094
At time: 164.69461607933044 and batch: 100, loss is 3.5192556953430176 and perplexity is 33.7592919121971
At time: 165.2423279285431 and batch: 150, loss is 3.5325449228286745 and perplexity is 34.21092107693711
At time: 165.7904109954834 and batch: 200, loss is 3.4301074743270874 and perplexity is 30.879961374409568
At time: 166.33816289901733 and batch: 250, loss is 3.564961314201355 and perplexity is 35.33808629574508
At time: 166.8865818977356 and batch: 300, loss is 3.5522818756103516 and perplexity is 34.892847852382985
At time: 167.43529748916626 and batch: 350, loss is 3.521254286766052 and perplexity is 33.82683041191253
At time: 168.0032660961151 and batch: 400, loss is 3.4579316139221192 and perplexity is 31.751234736531963
At time: 168.56935358047485 and batch: 450, loss is 3.4891911792755126 and perplexity is 32.75944047349979
At time: 169.11644053459167 and batch: 500, loss is 3.3572385025024416 and perplexity is 28.70979927098917
At time: 169.66351342201233 and batch: 550, loss is 3.423667449951172 and perplexity is 30.68173265443496
At time: 170.21165585517883 and batch: 600, loss is 3.450603337287903 and perplexity is 31.519403404486148
At time: 170.780588388443 and batch: 650, loss is 3.289191861152649 and perplexity is 26.82117965802474
At time: 171.34213161468506 and batch: 700, loss is 3.282585325241089 and perplexity is 26.644568607415728
At time: 171.89061284065247 and batch: 750, loss is 3.378779134750366 and perplexity is 29.334935239320032
At time: 172.4384958744049 and batch: 800, loss is 3.334793529510498 and perplexity is 28.072586464722846
At time: 172.98663854599 and batch: 850, loss is 3.3953339052200318 and perplexity is 29.824610405609846
At time: 173.5355179309845 and batch: 900, loss is 3.3673285675048827 and perplexity is 29.000949403574328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263107508829195 and perplexity of 71.03036834971253
finished 16 epochs...
Completing Train Step...
At time: 174.97189784049988 and batch: 50, loss is 3.6472880363464357 and perplexity is 38.370465511024044
At time: 175.51970052719116 and batch: 100, loss is 3.517435073852539 and perplexity is 33.697884936230416
At time: 176.06783628463745 and batch: 150, loss is 3.530628509521484 and perplexity is 34.14542159462013
At time: 176.6158766746521 and batch: 200, loss is 3.4282319164276123 and perplexity is 30.82209849846744
At time: 177.17691349983215 and batch: 250, loss is 3.5630263662338257 and perplexity is 35.269775048158834
At time: 177.72470331192017 and batch: 300, loss is 3.550512266159058 and perplexity is 34.83115574061205
At time: 178.27232027053833 and batch: 350, loss is 3.5195249414443968 and perplexity is 33.7683826937036
At time: 178.81894445419312 and batch: 400, loss is 3.456388487815857 and perplexity is 31.702276361492412
At time: 179.36580872535706 and batch: 450, loss is 3.4877616930007935 and perplexity is 32.7126447578607
At time: 179.9138059616089 and batch: 500, loss is 3.356025261878967 and perplexity is 28.674988497414112
At time: 180.46183848381042 and batch: 550, loss is 3.4226764917373655 and perplexity is 30.651343399175143
At time: 181.01031064987183 and batch: 600, loss is 3.4498970460891725 and perplexity is 31.497149387105743
At time: 181.5566565990448 and batch: 650, loss is 3.2886102962493897 and perplexity is 26.805585936091823
At time: 182.10412549972534 and batch: 700, loss is 3.282266788482666 and perplexity is 26.636082684512893
At time: 182.65267372131348 and batch: 750, loss is 3.3787604188919067 and perplexity is 29.334386215961914
At time: 183.19996333122253 and batch: 800, loss is 3.3350790786743163 and perplexity is 28.080603712918847
At time: 183.74990558624268 and batch: 850, loss is 3.3958120203018187 and perplexity is 29.838873411060494
At time: 184.29831671714783 and batch: 900, loss is 3.3680347394943237 and perplexity is 29.02143629449322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2630067590164815 and perplexity of 71.02321241388995
finished 17 epochs...
Completing Train Step...
At time: 185.72207951545715 and batch: 50, loss is 3.6459255409240723 and perplexity is 38.31822152658826
At time: 186.28613090515137 and batch: 100, loss is 3.5159504890441893 and perplexity is 33.64789468474604
At time: 186.8352813720703 and batch: 150, loss is 3.5290642309188844 and perplexity is 34.092050396837394
At time: 187.40312004089355 and batch: 200, loss is 3.42667977809906 and perplexity is 30.77429544609345
At time: 187.9531512260437 and batch: 250, loss is 3.56144567489624 and perplexity is 35.2140684593218
At time: 188.50307655334473 and batch: 300, loss is 3.5490652179718016 and perplexity is 34.78078982957632
At time: 189.05216097831726 and batch: 350, loss is 3.5181079053878785 and perplexity is 33.72056556515532
At time: 189.6015875339508 and batch: 400, loss is 3.455115647315979 and perplexity is 31.66195009010136
At time: 190.1508674621582 and batch: 450, loss is 3.4865786981582643 and perplexity is 32.67396874909662
At time: 190.70069479942322 and batch: 500, loss is 3.3549981546401977 and perplexity is 28.6455513293089
At time: 191.2620713710785 and batch: 550, loss is 3.4218111276626586 and perplexity is 30.6248303011518
At time: 191.8113031387329 and batch: 600, loss is 3.4492589139938357 and perplexity is 31.477056456823906
At time: 192.36104083061218 and batch: 650, loss is 3.2880681562423706 and perplexity is 26.79105749412494
At time: 192.90922808647156 and batch: 700, loss is 3.28193253993988 and perplexity is 26.627181100443693
At time: 193.45719742774963 and batch: 750, loss is 3.3786286401748655 and perplexity is 29.330520822875048
At time: 194.00583958625793 and batch: 800, loss is 3.3351462507247924 and perplexity is 28.082490008001297
At time: 194.5543291568756 and batch: 850, loss is 3.3960392093658447 and perplexity is 29.84565324690546
At time: 195.10327100753784 and batch: 900, loss is 3.368417835235596 and perplexity is 29.03255641304227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262996307791096 and perplexity of 71.02247013816826
finished 18 epochs...
Completing Train Step...
At time: 196.52475094795227 and batch: 50, loss is 3.6446700525283813 and perplexity is 38.270143631053315
At time: 197.08585786819458 and batch: 100, loss is 3.514619951248169 and perplexity is 33.60315465985891
At time: 197.63398241996765 and batch: 150, loss is 3.5276806259155276 and perplexity is 34.044913082565394
At time: 198.18114519119263 and batch: 200, loss is 3.4252969551086427 and perplexity is 30.73176945257398
At time: 198.72872042655945 and batch: 250, loss is 3.560053963661194 and perplexity is 35.16509473116373
At time: 199.27579021453857 and batch: 300, loss is 3.547776484489441 and perplexity is 34.73599553133563
At time: 199.82470154762268 and batch: 350, loss is 3.516849007606506 and perplexity is 33.67814152934388
At time: 200.3731608390808 and batch: 400, loss is 3.453983020782471 and perplexity is 31.626109226314867
At time: 200.92134284973145 and batch: 450, loss is 3.485520811080933 and perplexity is 32.63942165647512
At time: 201.4694037437439 and batch: 500, loss is 3.3540647888183592 and perplexity is 28.618827024466903
At time: 202.0177047252655 and batch: 550, loss is 3.421006212234497 and perplexity is 30.600189820839304
At time: 202.56670546531677 and batch: 600, loss is 3.4486399269104004 and perplexity is 31.457578594347115
At time: 203.11619305610657 and batch: 650, loss is 3.287533655166626 and perplexity is 26.776741471379605
At time: 203.66585063934326 and batch: 700, loss is 3.2815748596191407 and perplexity is 26.617658784841268
At time: 204.2168447971344 and batch: 750, loss is 3.3784177827835085 and perplexity is 29.324336917751154
At time: 204.77873349189758 and batch: 800, loss is 3.3350902891159055 and perplexity is 28.08091851065107
At time: 205.32607340812683 and batch: 850, loss is 3.3961157274246214 and perplexity is 29.847937065730413
At time: 205.87311172485352 and batch: 900, loss is 3.3686198472976683 and perplexity is 29.03842193206335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263038112692637 and perplexity of 71.02543928760169
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 207.30760025978088 and batch: 50, loss is 3.6441036796569826 and perplexity is 38.24847459687195
At time: 207.85512733459473 and batch: 100, loss is 3.514485869407654 and perplexity is 33.59864938907926
At time: 208.4036989212036 and batch: 150, loss is 3.5275777864456175 and perplexity is 34.04141210177344
At time: 208.95315647125244 and batch: 200, loss is 3.4250062561035155 and perplexity is 30.7228370561506
At time: 209.50258827209473 and batch: 250, loss is 3.5597476243972777 and perplexity is 35.15432393177185
At time: 210.05116319656372 and batch: 300, loss is 3.5474078464508056 and perplexity is 34.723192881989675
At time: 210.6009418964386 and batch: 350, loss is 3.5161427783966066 and perplexity is 33.654365438733784
At time: 211.1500165462494 and batch: 400, loss is 3.4537247467041015 and perplexity is 31.617942076828932
At time: 211.69903469085693 and batch: 450, loss is 3.484989547729492 and perplexity is 32.62208613320864
At time: 212.2488672733307 and batch: 500, loss is 3.3530406379699706 and perplexity is 28.58953203229604
At time: 212.7976312637329 and batch: 550, loss is 3.419429750442505 and perplexity is 30.551987795067785
At time: 213.34627652168274 and batch: 600, loss is 3.4470819139480593 and perplexity is 31.408605439444802
At time: 213.89538955688477 and batch: 650, loss is 3.285778241157532 and perplexity is 26.729778436127475
At time: 214.44580245018005 and batch: 700, loss is 3.2794191884994506 and perplexity is 26.560341667148315
At time: 214.9940526485443 and batch: 750, loss is 3.3760261726379395 and perplexity is 29.25428833390713
At time: 215.5411558151245 and batch: 800, loss is 3.332194194793701 and perplexity is 27.999711170807615
At time: 216.0887315273285 and batch: 850, loss is 3.3932450819015503 and perplexity is 29.762377083764854
At time: 216.63704633712769 and batch: 900, loss is 3.365458216667175 and perplexity is 28.946758147691163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262776832058005 and perplexity of 71.00688413990554
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8858900b70>
ELAPSED
1080.1204223632812


RESULTS SO FAR:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.78276706776143, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.87113706675376, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.22372141185209682, 'rnn_dropout': 0.5465833484205133, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.06583204544751, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.7150731284420337, 'rnn_dropout': 0.7542520583353851, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.00688413990554, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.29830252145929015, 'rnn_dropout': 0.8293988567732831, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.26287240546702223, 'rnn_dropout': 0.03117097696540194, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.86614990234375 and batch: 50, loss is 6.880011463165284 and perplexity is 972.6375092325095
At time: 1.486349105834961 and batch: 100, loss is 5.996194171905517 and perplexity is 401.89633085074934
At time: 2.1061205863952637 and batch: 150, loss is 5.737736721038818 and perplexity is 310.3611815468253
At time: 2.7274129390716553 and batch: 200, loss is 5.482162456512452 and perplexity is 240.3659266408171
At time: 3.3470358848571777 and batch: 250, loss is 5.469619436264038 and perplexity is 237.3698412164185
At time: 3.968327522277832 and batch: 300, loss is 5.341916303634644 and perplexity is 208.91266705059655
At time: 4.592010736465454 and batch: 350, loss is 5.288052320480347 and perplexity is 197.95749194121117
At time: 5.214948654174805 and batch: 400, loss is 5.113579244613647 and perplexity is 166.26439206708113
At time: 5.830026865005493 and batch: 450, loss is 5.0995241355896 and perplexity is 163.94387367987557
At time: 6.444958209991455 and batch: 500, loss is 5.015470933914185 and perplexity is 150.7271025588877
At time: 7.061007976531982 and batch: 550, loss is 5.0576902103424075 and perplexity is 157.22693543795884
At time: 7.6747353076934814 and batch: 600, loss is 4.977839679718017 and perplexity is 145.16044961538245
At time: 8.290237426757812 and batch: 650, loss is 4.8461079597473145 and perplexity is 127.24418535715088
At time: 8.90658950805664 and batch: 700, loss is 4.913494176864624 and perplexity is 136.11419150674197
At time: 9.521819353103638 and batch: 750, loss is 4.916242980957032 and perplexity is 136.48885745912978
At time: 10.136991500854492 and batch: 800, loss is 4.867046327590942 and perplexity is 129.93655951223272
At time: 10.75129222869873 and batch: 850, loss is 4.905759992599488 and perplexity is 135.06551980602626
At time: 11.366372346878052 and batch: 900, loss is 4.817410202026367 and perplexity is 123.6444616005269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.800322650230094 and perplexity of 121.54962920839631
finished 1 epochs...
Completing Train Step...
At time: 12.897716999053955 and batch: 50, loss is 4.766828260421753 and perplexity is 117.54582533709463
At time: 13.446620464324951 and batch: 100, loss is 4.636544151306152 and perplexity is 103.1871315826705
At time: 13.995046138763428 and batch: 150, loss is 4.619586849212647 and perplexity is 101.45210845125634
At time: 14.542783260345459 and batch: 200, loss is 4.510876197814941 and perplexity is 91.00151892080262
At time: 15.092723369598389 and batch: 250, loss is 4.625712633132935 and perplexity is 102.07548954545256
At time: 15.639228343963623 and batch: 300, loss is 4.571092891693115 and perplexity is 96.64967970617833
At time: 16.22828221321106 and batch: 350, loss is 4.558069400787353 and perplexity is 95.39912445459666
At time: 16.777167320251465 and batch: 400, loss is 4.436981711387634 and perplexity is 84.51945219720933
At time: 17.32826042175293 and batch: 450, loss is 4.465900592803955 and perplexity is 86.99934525345905
At time: 17.877817630767822 and batch: 500, loss is 4.3582306003570555 and perplexity is 78.11878870790181
At time: 18.426984786987305 and batch: 550, loss is 4.432977247238159 and perplexity is 84.18167384298413
At time: 18.98192524909973 and batch: 600, loss is 4.404466648101806 and perplexity is 81.81549475487775
At time: 19.540414810180664 and batch: 650, loss is 4.257746238708496 and perplexity is 70.65057435701746
At time: 20.132371187210083 and batch: 700, loss is 4.29924174785614 and perplexity is 73.64393185437945
At time: 20.68557906150818 and batch: 750, loss is 4.367162127494812 and perplexity is 78.81963393932737
At time: 21.23544478416443 and batch: 800, loss is 4.317282876968384 and perplexity is 74.98460883687427
At time: 21.785407543182373 and batch: 850, loss is 4.383388986587525 and perplexity is 80.10906242692612
At time: 22.33560800552368 and batch: 900, loss is 4.321644854545593 and perplexity is 75.31240441800105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.492271109803082 and perplexity of 89.32408048367938
finished 2 epochs...
Completing Train Step...
At time: 23.79913568496704 and batch: 50, loss is 4.387072696685791 and perplexity is 80.40470518594353
At time: 24.368470191955566 and batch: 100, loss is 4.259633574485779 and perplexity is 70.78404162284208
At time: 24.915738821029663 and batch: 150, loss is 4.2547793674469 and perplexity is 70.44127383572078
At time: 25.466017484664917 and batch: 200, loss is 4.155688629150391 and perplexity is 63.79588112837854
At time: 26.022671222686768 and batch: 250, loss is 4.295177001953125 and perplexity is 73.34519553901336
At time: 26.61993408203125 and batch: 300, loss is 4.26103244304657 and perplexity is 70.88312848186192
At time: 27.17351198196411 and batch: 350, loss is 4.247180614471436 and perplexity is 69.90803653164582
At time: 27.747365474700928 and batch: 400, loss is 4.161918964385986 and perplexity is 64.19459161472399
At time: 28.31678056716919 and batch: 450, loss is 4.198950366973877 and perplexity is 66.6163715877729
At time: 28.864388704299927 and batch: 500, loss is 4.076521043777466 and perplexity is 58.94006285765255
At time: 29.413084983825684 and batch: 550, loss is 4.154299530982971 and perplexity is 63.707323908390144
At time: 29.961573600769043 and batch: 600, loss is 4.15272301197052 and perplexity is 63.6069672289084
At time: 30.545918703079224 and batch: 650, loss is 4.002477707862854 and perplexity is 54.73359602728869
At time: 31.107107639312744 and batch: 700, loss is 4.02743766784668 and perplexity is 56.116936639420196
At time: 31.679341793060303 and batch: 750, loss is 4.120475854873657 and perplexity is 61.5885425006559
At time: 32.22755312919617 and batch: 800, loss is 4.078184728622436 and perplexity is 59.03820216078989
At time: 32.77596378326416 and batch: 850, loss is 4.150706186294555 and perplexity is 63.47881234070552
At time: 33.32353138923645 and batch: 900, loss is 4.0964567804336545 and perplexity is 60.1268670309521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387510534835188 and perplexity of 80.43991714127117
finished 3 epochs...
Completing Train Step...
At time: 34.856427907943726 and batch: 50, loss is 4.1848071098327635 and perplexity is 65.68083050839739
At time: 35.448952436447144 and batch: 100, loss is 4.058761568069458 and perplexity is 57.902558280616894
At time: 36.00442600250244 and batch: 150, loss is 4.056295013427734 and perplexity is 57.75991444844447
At time: 36.5637526512146 and batch: 200, loss is 3.959484577178955 and perplexity is 52.43029521290146
At time: 37.1208381652832 and batch: 250, loss is 4.107297015190125 and perplexity is 60.78220195958154
At time: 37.67677450180054 and batch: 300, loss is 4.078748230934143 and perplexity is 59.07147969929087
At time: 38.23546600341797 and batch: 350, loss is 4.064349012374878 and perplexity is 58.22699113134023
At time: 38.7892849445343 and batch: 400, loss is 3.9944329166412356 and perplexity is 54.29504207714123
At time: 39.33871555328369 and batch: 450, loss is 4.031997094154358 and perplexity is 56.37338185391265
At time: 39.9207239151001 and batch: 500, loss is 3.9057466268539427 and perplexity is 49.68716383734446
At time: 40.47498416900635 and batch: 550, loss is 3.9844401597976686 and perplexity is 53.7551867376943
At time: 41.02248692512512 and batch: 600, loss is 3.9940480947494508 and perplexity is 54.274152176037994
At time: 41.57200241088867 and batch: 650, loss is 3.8471388912200926 and perplexity is 46.85880312374645
At time: 42.120885133743286 and batch: 700, loss is 3.8580043745040893 and perplexity is 47.37072275653021
At time: 42.66893982887268 and batch: 750, loss is 3.9619611930847167 and perplexity is 52.560305842713326
At time: 43.21623516082764 and batch: 800, loss is 3.920592155456543 and perplexity is 50.43029851460715
At time: 43.76664447784424 and batch: 850, loss is 3.996343607902527 and perplexity is 54.398882311292304
At time: 44.3410849571228 and batch: 900, loss is 3.9498616313934325 and perplexity is 51.92818110764192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3510938670537245 and perplexity of 77.56326043337286
finished 4 epochs...
Completing Train Step...
At time: 45.821561336517334 and batch: 50, loss is 4.044040956497192 and perplexity is 57.05644017826603
At time: 46.36994814872742 and batch: 100, loss is 3.9204209852218628 and perplexity is 50.42166708731821
At time: 46.91833710670471 and batch: 150, loss is 3.9206858539581297 and perplexity is 50.435023979393584
At time: 47.46678686141968 and batch: 200, loss is 3.825501408576965 and perplexity is 45.85578708793157
At time: 48.01638388633728 and batch: 250, loss is 3.974109830856323 and perplexity is 53.20273638623588
At time: 48.57045245170593 and batch: 300, loss is 3.948135018348694 and perplexity is 51.838598592177036
At time: 49.12382173538208 and batch: 350, loss is 3.9374182081222533 and perplexity is 51.28602039512129
At time: 49.67469072341919 and batch: 400, loss is 3.8731224393844603 and perplexity is 48.092317228949554
At time: 50.224116802215576 and batch: 450, loss is 3.9103832817077637 and perplexity is 49.91808099460363
At time: 50.7718243598938 and batch: 500, loss is 3.7848170089721678 and perplexity is 44.02761316337111
At time: 51.32213521003723 and batch: 550, loss is 3.859131350517273 and perplexity is 47.42413851829162
At time: 51.88399386405945 and batch: 600, loss is 3.8768563938140868 and perplexity is 48.27222742911015
At time: 52.43229007720947 and batch: 650, loss is 3.733619146347046 and perplexity is 41.83022424675272
At time: 52.9766948223114 and batch: 700, loss is 3.737079372406006 and perplexity is 41.97521698790566
At time: 53.521973609924316 and batch: 750, loss is 3.8446710300445557 and perplexity is 46.74330467846841
At time: 54.06947684288025 and batch: 800, loss is 3.8081608200073243 and perplexity is 45.06747539825645
At time: 54.63263273239136 and batch: 850, loss is 3.8828465366363525 and perplexity is 48.56225274507281
At time: 55.19397783279419 and batch: 900, loss is 3.840774664878845 and perplexity is 46.56153005451542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337067904537672 and perplexity of 76.48295492264786
finished 5 epochs...
Completing Train Step...
At time: 56.63302779197693 and batch: 50, loss is 3.9391510915756225 and perplexity is 51.3749701387638
At time: 57.194499254226685 and batch: 100, loss is 3.8151507568359375 and perplexity is 45.383597754843485
At time: 57.74412941932678 and batch: 150, loss is 3.8166234731674193 and perplexity is 45.450484160700455
At time: 58.31865644454956 and batch: 200, loss is 3.724528946876526 and perplexity is 41.45170219143721
At time: 58.86697506904602 and batch: 250, loss is 3.872350082397461 and perplexity is 48.05518713240731
At time: 59.413865089416504 and batch: 300, loss is 3.8501509952545168 and perplexity is 47.000159496860164
At time: 59.962480545043945 and batch: 350, loss is 3.8395936250686646 and perplexity is 46.506571494410096
At time: 60.51012873649597 and batch: 400, loss is 3.776139245033264 and perplexity is 43.647204863432435
At time: 61.05846571922302 and batch: 450, loss is 3.8165543842315675 and perplexity is 45.44734414358731
At time: 61.60684823989868 and batch: 500, loss is 3.6931082010269165 and perplexity is 40.16950802719319
At time: 62.1584267616272 and batch: 550, loss is 3.7641033697128297 and perplexity is 43.12502131933658
At time: 62.70576310157776 and batch: 600, loss is 3.7878329706192018 and perplexity is 44.160599195665114
At time: 63.25429725646973 and batch: 650, loss is 3.6451638031005857 and perplexity is 38.28904420206939
At time: 63.80157709121704 and batch: 700, loss is 3.6436149072647095 and perplexity is 38.229784366448854
At time: 64.35281300544739 and batch: 750, loss is 3.7518973922729493 and perplexity is 42.6018377609381
At time: 64.90190196037292 and batch: 800, loss is 3.7164565038681032 and perplexity is 41.11843264736728
At time: 65.45054244995117 and batch: 850, loss is 3.7922273015975954 and perplexity is 44.35508248374557
At time: 66.03328490257263 and batch: 900, loss is 3.751986570358276 and perplexity is 42.60563708066654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334677918316567 and perplexity of 76.30037997694181
finished 6 epochs...
Completing Train Step...
At time: 67.5333731174469 and batch: 50, loss is 3.8539214515686036 and perplexity is 47.17770605042167
At time: 68.0942497253418 and batch: 100, loss is 3.7304992961883543 and perplexity is 41.69992357996608
At time: 68.63991379737854 and batch: 150, loss is 3.735260133743286 and perplexity is 41.89892346938644
At time: 69.1864800453186 and batch: 200, loss is 3.643407793045044 and perplexity is 38.221867254393366
At time: 69.7713258266449 and batch: 250, loss is 3.7916553354263307 and perplexity is 44.329720130936565
At time: 70.35360836982727 and batch: 300, loss is 3.7710434198379517 and perplexity is 43.42535207876803
At time: 70.90205192565918 and batch: 350, loss is 3.7623984098434446 and perplexity is 43.05155753282059
At time: 71.45120120048523 and batch: 400, loss is 3.7001408529281616 and perplexity is 40.453001882569694
At time: 71.9980525970459 and batch: 450, loss is 3.744242672920227 and perplexity is 42.27697759193544
At time: 72.55776262283325 and batch: 500, loss is 3.6188937711715696 and perplexity is 37.296286765502614
At time: 73.13743662834167 and batch: 550, loss is 3.689382166862488 and perplexity is 40.020113565191295
At time: 73.70197820663452 and batch: 600, loss is 3.713446774482727 and perplexity is 40.994863340713636
At time: 74.27563905715942 and batch: 650, loss is 3.570590686798096 and perplexity is 35.53757853159976
At time: 74.86317157745361 and batch: 700, loss is 3.5665496969223023 and perplexity is 35.39426130330366
At time: 75.42885756492615 and batch: 750, loss is 3.6748435926437377 and perplexity is 39.44248727956963
At time: 75.98968124389648 and batch: 800, loss is 3.6475517463684084 and perplexity is 38.38058552164252
At time: 76.59746265411377 and batch: 850, loss is 3.718321957588196 and perplexity is 41.19520876940037
At time: 77.17950963973999 and batch: 900, loss is 3.678708848953247 and perplexity is 39.59523762182507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340207034594392 and perplexity of 76.72342209667278
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 78.86372113227844 and batch: 50, loss is 3.8006831502914427 and perplexity is 44.73173255378541
At time: 79.49739170074463 and batch: 100, loss is 3.675769720077515 and perplexity is 39.4790329694665
At time: 80.08604860305786 and batch: 150, loss is 3.681945366859436 and perplexity is 39.723595922310274
At time: 80.71167421340942 and batch: 200, loss is 3.573215374946594 and perplexity is 35.630976108937254
At time: 81.34105896949768 and batch: 250, loss is 3.7166626262664795 and perplexity is 41.12690895087002
At time: 81.94776344299316 and batch: 300, loss is 3.6824273920059203 and perplexity is 39.74274831004903
At time: 82.56948661804199 and batch: 350, loss is 3.6583268547058108 and perplexity is 38.79637656257395
At time: 83.14401865005493 and batch: 400, loss is 3.5916067218780516 and perplexity is 36.29234080419089
At time: 83.7189610004425 and batch: 450, loss is 3.6177303552627564 and perplexity is 37.2529209032984
At time: 84.28149199485779 and batch: 500, loss is 3.4901951932907105 and perplexity is 32.79234792787357
At time: 84.82952952384949 and batch: 550, loss is 3.540570502281189 and perplexity is 34.48658825697139
At time: 85.41139149665833 and batch: 600, loss is 3.561832013130188 and perplexity is 35.22767562865613
At time: 85.97853446006775 and batch: 650, loss is 3.405171957015991 and perplexity is 30.1194745336027
At time: 86.5277168750763 and batch: 700, loss is 3.383583011627197 and perplexity is 29.476195683632312
At time: 87.12165951728821 and batch: 750, loss is 3.4737425088882445 and perplexity is 32.257239829614555
At time: 87.68070006370544 and batch: 800, loss is 3.430673394203186 and perplexity is 30.897441904147584
At time: 88.22954678535461 and batch: 850, loss is 3.4923225593566896 and perplexity is 32.862183512667926
At time: 88.7785005569458 and batch: 900, loss is 3.437227177619934 and perplexity is 31.10060205314797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.289317927948416 and perplexity of 72.91671707972176
finished 8 epochs...
Completing Train Step...
At time: 90.25565385818481 and batch: 50, loss is 3.7135692691802977 and perplexity is 40.999885301676024
At time: 90.83660674095154 and batch: 100, loss is 3.585468306541443 and perplexity is 36.07024569618645
At time: 91.41786527633667 and batch: 150, loss is 3.5881206083297728 and perplexity is 36.16604185735859
At time: 91.98238444328308 and batch: 200, loss is 3.486523871421814 and perplexity is 32.67217739113083
At time: 92.56947422027588 and batch: 250, loss is 3.634612560272217 and perplexity is 37.887171057920746
At time: 93.12077212333679 and batch: 300, loss is 3.603190336227417 and perplexity is 36.71518156801386
At time: 93.66778564453125 and batch: 350, loss is 3.5842883682250974 and perplexity is 36.02771013081624
At time: 94.21751046180725 and batch: 400, loss is 3.523160834312439 and perplexity is 33.89138439049708
At time: 94.76454567909241 and batch: 450, loss is 3.552052130699158 and perplexity is 34.88483231895111
At time: 95.31289052963257 and batch: 500, loss is 3.4284444856643677 and perplexity is 30.82865102482855
At time: 95.86276173591614 and batch: 550, loss is 3.4833966064453126 and perplexity is 32.57016243211408
At time: 96.41177606582642 and batch: 600, loss is 3.5104627656936644 and perplexity is 33.463750076848534
At time: 96.99347019195557 and batch: 650, loss is 3.3601752281188966 and perplexity is 28.794235997015342
At time: 97.54802393913269 and batch: 700, loss is 3.3432389307022095 and perplexity is 28.31067468002385
At time: 98.13067245483398 and batch: 750, loss is 3.4387350559234617 and perplexity is 31.147533350660904
At time: 98.7102267742157 and batch: 800, loss is 3.400143322944641 and perplexity is 29.96839489821731
At time: 99.28717875480652 and batch: 850, loss is 3.4698516798019408 and perplexity is 32.13197627021748
At time: 99.8452455997467 and batch: 900, loss is 3.4224356508255003 and perplexity is 30.643962190565176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.293200767203553 and perplexity of 73.20039134554231
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 101.52033472061157 and batch: 50, loss is 3.691876926422119 and perplexity is 40.12007876881211
At time: 102.12917232513428 and batch: 100, loss is 3.569695997238159 and perplexity is 35.505797650233454
At time: 102.70021438598633 and batch: 150, loss is 3.575807285308838 and perplexity is 35.723448193038436
At time: 103.26437830924988 and batch: 200, loss is 3.469161114692688 and perplexity is 32.10979470830179
At time: 103.83530259132385 and batch: 250, loss is 3.61683030128479 and perplexity is 37.219406348365276
At time: 104.39214086532593 and batch: 300, loss is 3.585574703216553 and perplexity is 36.0740836545683
At time: 104.93574142456055 and batch: 350, loss is 3.558972563743591 and perplexity is 35.12708775469343
At time: 105.48008060455322 and batch: 400, loss is 3.497283601760864 and perplexity is 33.025619269221544
At time: 106.02793717384338 and batch: 450, loss is 3.5191156578063967 and perplexity is 33.75456467512269
At time: 106.57527041435242 and batch: 500, loss is 3.3885068130493163 and perplexity is 29.621688512815947
At time: 107.12921833992004 and batch: 550, loss is 3.4410143995285036 and perplexity is 31.218610255187677
At time: 107.68341851234436 and batch: 600, loss is 3.468050298690796 and perplexity is 32.07414643752272
At time: 108.26404786109924 and batch: 650, loss is 3.3097150325775146 and perplexity is 27.377322715101602
At time: 108.81773352622986 and batch: 700, loss is 3.2893653392791746 and perplexity is 26.825832949633508
At time: 109.38928961753845 and batch: 750, loss is 3.3806166362762453 and perplexity is 29.388887781342493
At time: 109.97079944610596 and batch: 800, loss is 3.335741229057312 and perplexity is 28.09920345265492
At time: 110.52793264389038 and batch: 850, loss is 3.404257130622864 and perplexity is 30.091933043114246
At time: 111.07567620277405 and batch: 900, loss is 3.3575702619552614 and perplexity is 28.71932559842309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274166995531892 and perplexity of 71.82028776420914
finished 10 epochs...
Completing Train Step...
At time: 112.51542520523071 and batch: 50, loss is 3.665800313949585 and perplexity is 39.08740584479978
At time: 113.0625331401825 and batch: 100, loss is 3.535187439918518 and perplexity is 34.301443571485876
At time: 113.6103618144989 and batch: 150, loss is 3.5416489171981813 and perplexity is 34.52379916897888
At time: 114.15720844268799 and batch: 200, loss is 3.4360967588424685 and perplexity is 31.065465212010736
At time: 114.70450401306152 and batch: 250, loss is 3.5849222898483277 and perplexity is 36.05055611582052
At time: 115.2496337890625 and batch: 300, loss is 3.5559585523605346 and perplexity is 35.021373704051
At time: 115.80634713172913 and batch: 350, loss is 3.5326579999923706 and perplexity is 34.2147897695862
At time: 116.35199666023254 and batch: 400, loss is 3.472356686592102 and perplexity is 32.21256798821293
At time: 116.91593933105469 and batch: 450, loss is 3.4966873836517336 and perplexity is 33.005934665689644
At time: 117.48083734512329 and batch: 500, loss is 3.3681513500213622 and perplexity is 29.02482069679957
At time: 118.02761554718018 and batch: 550, loss is 3.422938961982727 and perplexity is 30.659389520685338
At time: 118.57412886619568 and batch: 600, loss is 3.4521660089492796 and perplexity is 31.568696387312634
At time: 119.12798547744751 and batch: 650, loss is 3.2975716686248777 and perplexity is 27.046880323051102
At time: 119.72788596153259 and batch: 700, loss is 3.279891028404236 and perplexity is 26.57287685329957
At time: 120.2859218120575 and batch: 750, loss is 3.373744487762451 and perplexity is 29.187615358957185
At time: 120.83397459983826 and batch: 800, loss is 3.3317791414260864 and perplexity is 27.988092207805593
At time: 121.38226914405823 and batch: 850, loss is 3.403058762550354 and perplexity is 30.05589342998913
At time: 121.93057227134705 and batch: 900, loss is 3.359278621673584 and perplexity is 28.76843046986231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274747247565283 and perplexity of 71.86197372523525
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 123.50606942176819 and batch: 50, loss is 3.6599429750442507 and perplexity is 38.85912686815921
At time: 124.06671190261841 and batch: 100, loss is 3.5333913660049436 and perplexity is 34.23989093657691
At time: 124.64783835411072 and batch: 150, loss is 3.5420323467254637 and perplexity is 34.537039151112076
At time: 125.21638631820679 and batch: 200, loss is 3.433606209754944 and perplexity is 30.988191413011272
At time: 125.76079821586609 and batch: 250, loss is 3.583385534286499 and perplexity is 35.99519777021986
At time: 126.30484962463379 and batch: 300, loss is 3.554571967124939 and perplexity is 34.97284723516017
At time: 126.84857130050659 and batch: 350, loss is 3.5296545886993407 and perplexity is 34.112182846130196
At time: 127.39127326011658 and batch: 400, loss is 3.4708822441101073 and perplexity is 32.1651074070637
At time: 127.9361183643341 and batch: 450, loss is 3.4911906147003173 and perplexity is 32.82500638483921
At time: 128.48513054847717 and batch: 500, loss is 3.3589370536804197 and perplexity is 28.758605772796784
At time: 129.0351598262787 and batch: 550, loss is 3.409517912864685 and perplexity is 30.2506572908729
At time: 129.5863299369812 and batch: 600, loss is 3.439015278816223 and perplexity is 31.15626282560104
At time: 130.15526342391968 and batch: 650, loss is 3.2806601238250734 and perplexity is 26.593321792252606
At time: 130.70523238182068 and batch: 700, loss is 3.2633033275604246 and perplexity is 26.13572957904562
At time: 131.2557430267334 and batch: 750, loss is 3.3545557689666747 and perplexity is 28.63288175041737
At time: 131.83443903923035 and batch: 800, loss is 3.311640615463257 and perplexity is 27.43009080754234
At time: 132.38407373428345 and batch: 850, loss is 3.3812051582336426 and perplexity is 29.406188877643466
At time: 132.93418860435486 and batch: 900, loss is 3.336139426231384 and perplexity is 28.110394704077923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269936339495933 and perplexity of 71.51708266076737
finished 12 epochs...
Completing Train Step...
At time: 134.4450650215149 and batch: 50, loss is 3.6493031358718873 and perplexity is 38.44786377427799
At time: 135.0095443725586 and batch: 100, loss is 3.5198013830184935 and perplexity is 33.77771896897801
At time: 135.55724906921387 and batch: 150, loss is 3.52984881401062 and perplexity is 34.11880893891854
At time: 136.1069405078888 and batch: 200, loss is 3.4221153020858766 and perplexity is 30.634147008124952
At time: 136.65457224845886 and batch: 250, loss is 3.5712037801742555 and perplexity is 35.559373065961886
At time: 137.2032129764557 and batch: 300, loss is 3.5432851982116698 and perplexity is 34.58033604856949
At time: 137.75226259231567 and batch: 350, loss is 3.5198550987243653 and perplexity is 33.77953341172677
At time: 138.3012523651123 and batch: 400, loss is 3.461727375984192 and perplexity is 31.87198389132171
At time: 138.85021424293518 and batch: 450, loss is 3.4828094148635866 and perplexity is 32.55104312081717
At time: 139.4000961780548 and batch: 500, loss is 3.3522211837768556 and perplexity is 28.566113816780884
At time: 139.95011496543884 and batch: 550, loss is 3.40389573097229 and perplexity is 30.081059793939946
At time: 140.49985122680664 and batch: 600, loss is 3.435073981285095 and perplexity is 31.033708394231116
At time: 141.0499393939972 and batch: 650, loss is 3.2774999380111693 and perplexity is 26.50941460496179
At time: 141.59998846054077 and batch: 700, loss is 3.2612962532043457 and perplexity is 26.083325833129106
At time: 142.14940786361694 and batch: 750, loss is 3.3537905263900756 and perplexity is 28.61097903172651
At time: 142.6992654800415 and batch: 800, loss is 3.311984405517578 and perplexity is 27.43952262114005
At time: 143.24823689460754 and batch: 850, loss is 3.383367257118225 and perplexity is 29.469836747515465
At time: 143.84087562561035 and batch: 900, loss is 3.3394748258590696 and perplexity is 28.204310640630872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269688018380779 and perplexity of 71.49932566385837
finished 13 epochs...
Completing Train Step...
At time: 145.36732935905457 and batch: 50, loss is 3.6438403654098512 and perplexity is 38.23840455443056
At time: 145.91501450538635 and batch: 100, loss is 3.513725028038025 and perplexity is 33.5730958689807
At time: 146.46635603904724 and batch: 150, loss is 3.5235583066940306 and perplexity is 33.90485795727416
At time: 147.01642060279846 and batch: 200, loss is 3.4158805990219117 and perplexity is 30.443746360425425
At time: 147.57638549804688 and batch: 250, loss is 3.5647712087631227 and perplexity is 35.33136897188367
At time: 148.1474404335022 and batch: 300, loss is 3.537059998512268 and perplexity is 34.36573521056791
At time: 148.710520029068 and batch: 350, loss is 3.514123492240906 and perplexity is 33.5864762114811
At time: 149.2595715522766 and batch: 400, loss is 3.456415491104126 and perplexity is 31.703132438758182
At time: 149.8250377178192 and batch: 450, loss is 3.4780160903930666 and perplexity is 32.395388758243314
At time: 150.39065527915955 and batch: 500, loss is 3.3479639434814454 and perplexity is 28.44475950647311
At time: 150.9672646522522 and batch: 550, loss is 3.4001711559295655 and perplexity is 29.96922901970871
At time: 151.51903343200684 and batch: 600, loss is 3.432115616798401 and perplexity is 30.942035041881997
At time: 152.09751653671265 and batch: 650, loss is 3.275181632041931 and perplexity is 26.44802885381211
At time: 152.6557629108429 and batch: 700, loss is 3.259645113945007 and perplexity is 26.04029416524632
At time: 153.20210528373718 and batch: 750, loss is 3.3527651023864746 and perplexity is 28.58165568406258
At time: 153.75381755828857 and batch: 800, loss is 3.311487512588501 and perplexity is 27.42589150325587
At time: 154.30133485794067 and batch: 850, loss is 3.3836464643478394 and perplexity is 29.478066087783112
At time: 154.8492476940155 and batch: 900, loss is 3.340257797241211 and perplexity is 28.2264024562166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2699388477900255 and perplexity of 71.5172620468683
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 156.30604577064514 and batch: 50, loss is 3.64192816734314 and perplexity is 38.165355016019326
At time: 156.87635684013367 and batch: 100, loss is 3.513457374572754 and perplexity is 33.56411111598527
At time: 157.42439436912537 and batch: 150, loss is 3.5241612148284913 and perplexity is 33.92530563535067
At time: 157.99063181877136 and batch: 200, loss is 3.4167701148986818 and perplexity is 30.470838603865356
At time: 158.54023385047913 and batch: 250, loss is 3.565905442237854 and perplexity is 35.371465728522814
At time: 159.0888876914978 and batch: 300, loss is 3.5367714262008665 and perplexity is 34.35581964167331
At time: 159.65713214874268 and batch: 350, loss is 3.5138290977478026 and perplexity is 33.57658999313722
At time: 160.2383575439453 and batch: 400, loss is 3.4576347398757936 and perplexity is 31.74181001804627
At time: 160.79009342193604 and batch: 450, loss is 3.4768797063827517 and perplexity is 32.35859606572366
At time: 161.34209942817688 and batch: 500, loss is 3.3443062686920166 and perplexity is 28.340907870273004
At time: 161.89319109916687 and batch: 550, loss is 3.3954469776153564 and perplexity is 29.827982936414507
At time: 162.4700891971588 and batch: 600, loss is 3.4266708660125733 and perplexity is 30.774021184132998
At time: 163.03841519355774 and batch: 650, loss is 3.269349193572998 and perplexity is 26.29422132538049
At time: 163.58800673484802 and batch: 700, loss is 3.2534505844116213 and perplexity is 25.879485375595213
At time: 164.13776803016663 and batch: 750, loss is 3.345739870071411 and perplexity is 28.381566572111108
At time: 164.6883749961853 and batch: 800, loss is 3.3033939504623415 and perplexity is 27.204814204752182
At time: 165.23854207992554 and batch: 850, loss is 3.374627833366394 and perplexity is 29.213409501568073
At time: 165.79061317443848 and batch: 900, loss is 3.331264729499817 and perplexity is 27.97369850184478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268367401541096 and perplexity of 71.40496477136651
finished 15 epochs...
Completing Train Step...
At time: 167.2993769645691 and batch: 50, loss is 3.639047541618347 and perplexity is 38.055573108771625
At time: 167.86738300323486 and batch: 100, loss is 3.5100014877319334 and perplexity is 33.44831754603803
At time: 168.41753149032593 and batch: 150, loss is 3.5206191158294677 and perplexity is 33.80535141449769
At time: 168.97798323631287 and batch: 200, loss is 3.413637237548828 and perplexity is 30.375526582165886
At time: 169.52690625190735 and batch: 250, loss is 3.562033734321594 and perplexity is 35.23478251413502
At time: 170.09292101860046 and batch: 300, loss is 3.5335766887664795 and perplexity is 34.2462369557327
At time: 170.6401436328888 and batch: 350, loss is 3.5110886240005494 and perplexity is 33.484700198033494
At time: 171.1909613609314 and batch: 400, loss is 3.454479007720947 and perplexity is 31.641799254108715
At time: 171.73831057548523 and batch: 450, loss is 3.474324359893799 and perplexity is 32.276014198458846
At time: 172.3000123500824 and batch: 500, loss is 3.342698402404785 and perplexity is 28.29537609427186
At time: 172.84903287887573 and batch: 550, loss is 3.3941513776779173 and perplexity is 29.789362827096003
At time: 173.399240732193 and batch: 600, loss is 3.425938720703125 and perplexity is 30.751498374860844
At time: 173.94914531707764 and batch: 650, loss is 3.268520908355713 and perplexity is 26.27245122772631
At time: 174.5046968460083 and batch: 700, loss is 3.253070511817932 and perplexity is 25.869651161441197
At time: 175.05233812332153 and batch: 750, loss is 3.3457055377960203 and perplexity is 28.380592185078097
At time: 175.60257959365845 and batch: 800, loss is 3.3041066408157347 and perplexity is 27.224209724080566
At time: 176.15031123161316 and batch: 850, loss is 3.375927333831787 and perplexity is 29.251397017862832
At time: 176.69882464408875 and batch: 900, loss is 3.3329924154281616 and perplexity is 28.022070040491485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268172172650899 and perplexity of 71.39102582002691
finished 16 epochs...
Completing Train Step...
At time: 178.15469098091125 and batch: 50, loss is 3.6373570728302003 and perplexity is 37.991295695004126
At time: 178.7014970779419 and batch: 100, loss is 3.5080300331115724 and perplexity is 33.38244066384708
At time: 179.2492868900299 and batch: 150, loss is 3.518507328033447 and perplexity is 33.73403701288496
At time: 179.79696083068848 and batch: 200, loss is 3.4117050647735594 and perplexity is 30.316892480518337
At time: 180.344642162323 and batch: 250, loss is 3.5598607206344606 and perplexity is 35.15829997836296
At time: 180.89261841773987 and batch: 300, loss is 3.5316051340103147 and perplexity is 34.17878513870614
At time: 181.44135332107544 and batch: 350, loss is 3.5093084669113157 and perplexity is 33.425145195951956
At time: 181.98811650276184 and batch: 400, loss is 3.4526639413833617 and perplexity is 31.58441937931455
At time: 182.5352373123169 and batch: 450, loss is 3.47274206161499 and perplexity is 32.224984299653165
At time: 183.08354878425598 and batch: 500, loss is 3.341532435417175 and perplexity is 28.262403845855935
At time: 183.62880730628967 and batch: 550, loss is 3.3931678342819214 and perplexity is 29.7600780997773
At time: 184.17701601982117 and batch: 600, loss is 3.4252979707717897 and perplexity is 30.731800665715504
At time: 184.72537207603455 and batch: 650, loss is 3.267984471321106 and perplexity is 26.258361491363925
At time: 185.2723217010498 and batch: 700, loss is 3.252780637741089 and perplexity is 25.86215330696184
At time: 185.84021139144897 and batch: 750, loss is 3.3456824922561648 and perplexity is 28.379938146546134
At time: 186.38549733161926 and batch: 800, loss is 3.3043903923034668 and perplexity is 27.231935730173127
At time: 186.93296337127686 and batch: 850, loss is 3.3765592527389527 and perplexity is 29.26988737028578
At time: 187.48092317581177 and batch: 900, loss is 3.333826060295105 and perplexity is 28.045440235209853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268152524347174 and perplexity of 71.38962312124876
finished 17 epochs...
Completing Train Step...
At time: 188.89975762367249 and batch: 50, loss is 3.6359246873855593 and perplexity is 37.936916471329404
At time: 189.4622130393982 and batch: 100, loss is 3.506434636116028 and perplexity is 33.32922487974654
At time: 190.00916409492493 and batch: 150, loss is 3.5168428134918215 and perplexity is 33.67793292371895
At time: 190.5571973323822 and batch: 200, loss is 3.4101204681396484 and perplexity is 30.268890476694114
At time: 191.10401368141174 and batch: 250, loss is 3.558170280456543 and perplexity is 35.09891718116634
At time: 191.65134453773499 and batch: 300, loss is 3.529999828338623 and perplexity is 34.123961756987605
At time: 192.1989929676056 and batch: 350, loss is 3.5078422260284423 and perplexity is 33.376171793725774
At time: 192.75533723831177 and batch: 400, loss is 3.4512532424926756 and perplexity is 31.539894686784308
At time: 193.30219268798828 and batch: 450, loss is 3.471470422744751 and perplexity is 32.184031800938044
At time: 193.8505618572235 and batch: 500, loss is 3.3405031824111937 and perplexity is 28.23332964666146
At time: 194.39808702468872 and batch: 550, loss is 3.3922760915756225 and perplexity is 29.733551596358744
At time: 194.97397327423096 and batch: 600, loss is 3.4246619272232057 and perplexity is 30.7122601171444
At time: 195.5237717628479 and batch: 650, loss is 3.2674835109710694 and perplexity is 26.245210387765592
At time: 196.06981015205383 and batch: 700, loss is 3.2524629068374633 and perplexity is 25.85393740691371
At time: 196.61708855628967 and batch: 750, loss is 3.3455674266815185 and perplexity is 28.37667278052409
At time: 197.1649899482727 and batch: 800, loss is 3.3044492483139036 and perplexity is 27.233538540433724
At time: 197.71251153945923 and batch: 850, loss is 3.3768646001815794 and perplexity is 29.278826220198294
At time: 198.2609269618988 and batch: 900, loss is 3.334264807701111 and perplexity is 28.05774779912178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268196001444777 and perplexity of 71.39272700233442
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 199.6897485256195 and batch: 50, loss is 3.635308470726013 and perplexity is 37.913546312668025
At time: 200.25298595428467 and batch: 100, loss is 3.506184139251709 and perplexity is 33.3208770590191
At time: 200.80315732955933 and batch: 150, loss is 3.5167289781570434 and perplexity is 33.674099403149135
At time: 201.35532402992249 and batch: 200, loss is 3.4102949476242066 and perplexity is 30.27417223786872
At time: 201.9052107334137 and batch: 250, loss is 3.558266954421997 and perplexity is 35.10231049669339
At time: 202.4553554058075 and batch: 300, loss is 3.529650821685791 and perplexity is 34.11205434531724
At time: 203.00625348091125 and batch: 350, loss is 3.507361307144165 and perplexity is 33.36012442147676
At time: 203.5561876296997 and batch: 400, loss is 3.451275897026062 and perplexity is 31.540609216475136
At time: 204.11403465270996 and batch: 450, loss is 3.4710526037216187 and perplexity is 32.17058750904656
At time: 204.68207097053528 and batch: 500, loss is 3.339042167663574 and perplexity is 28.1921104539296
At time: 205.2310767173767 and batch: 550, loss is 3.3907004451751708 and perplexity is 29.686738922608743
At time: 205.77994799613953 and batch: 600, loss is 3.4229684686660766 and perplexity is 30.660294190930458
At time: 206.3300426006317 and batch: 650, loss is 3.265606679916382 and perplexity is 26.195998757276843
At time: 206.87933564186096 and batch: 700, loss is 3.2502730798721315 and perplexity is 25.797383701633983
At time: 207.42896580696106 and batch: 750, loss is 3.343240900039673 and perplexity is 28.310730433351008
At time: 207.97589087486267 and batch: 800, loss is 3.301448483467102 and perplexity is 27.1519395861947
At time: 208.52491998672485 and batch: 850, loss is 3.3736993169784544 and perplexity is 29.186296961265178
At time: 209.07323122024536 and batch: 900, loss is 3.3311979293823244 and perplexity is 27.971829917909663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267660898705051 and perplexity of 71.35453477781225
finished 19 epochs...
Completing Train Step...
At time: 210.50968408584595 and batch: 50, loss is 3.634787750244141 and perplexity is 37.89380909179617
At time: 211.05699706077576 and batch: 100, loss is 3.5054871034622193 and perplexity is 33.297659307913555
At time: 211.6063392162323 and batch: 150, loss is 3.516058883666992 and perplexity is 33.651542133276834
At time: 212.15506839752197 and batch: 200, loss is 3.4096932506561277 and perplexity is 30.255961839342294
At time: 212.70398545265198 and batch: 250, loss is 3.5575889587402343 and perplexity is 35.07851934781619
At time: 213.25357222557068 and batch: 300, loss is 3.528906373977661 and perplexity is 34.08666915478609
At time: 213.81523442268372 and batch: 350, loss is 3.506838026046753 and perplexity is 33.34267226554957
At time: 214.36424851417542 and batch: 400, loss is 3.4506599044799806 and perplexity is 31.52118641906229
At time: 214.9139437675476 and batch: 450, loss is 3.470524396896362 and perplexity is 32.15359927219808
At time: 215.46296429634094 and batch: 500, loss is 3.3387606716156006 and perplexity is 28.184175603119755
At time: 216.01228189468384 and batch: 550, loss is 3.3904339838027955 and perplexity is 29.678829607224394
At time: 216.56225633621216 and batch: 600, loss is 3.4227939510345458 and perplexity is 30.654943895880205
At time: 217.11175417900085 and batch: 650, loss is 3.265460648536682 and perplexity is 26.192173598739096
At time: 217.66081428527832 and batch: 700, loss is 3.250233941078186 and perplexity is 25.796374042907498
At time: 218.21007633209229 and batch: 750, loss is 3.3432684564590454 and perplexity is 28.31151058646064
At time: 218.75924110412598 and batch: 800, loss is 3.3016547346115113 and perplexity is 27.157540282362422
At time: 219.3092167377472 and batch: 850, loss is 3.3740125417709352 and perplexity is 29.1954402649541
At time: 219.8567259311676 and batch: 900, loss is 3.331602759361267 and perplexity is 27.983156045649658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267439332726884 and perplexity of 71.33872679183814
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8858900b70>
ELAPSED
1307.0602445602417


RESULTS SO FAR:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.78276706776143, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.87113706675376, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.22372141185209682, 'rnn_dropout': 0.5465833484205133, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.06583204544751, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.7150731284420337, 'rnn_dropout': 0.7542520583353851, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.00688413990554, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.29830252145929015, 'rnn_dropout': 0.8293988567732831, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.33872679183814, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.26287240546702223, 'rnn_dropout': 0.03117097696540194, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -70.26161923907547, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.27548570386339943, 'rnn_dropout': 0.038612554378629316, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.78276706776143, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.5372430548707944, 'rnn_dropout': 0.183763689811286, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.87113706675376, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.22372141185209682, 'rnn_dropout': 0.5465833484205133, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.06583204544751, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.7150731284420337, 'rnn_dropout': 0.7542520583353851, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.00688413990554, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.29830252145929015, 'rnn_dropout': 0.8293988567732831, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}, {'best_accuracy': -71.33872679183814, 'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.26287240546702223, 'rnn_dropout': 0.03117097696540194, 'wordvec_source': 'glove', 'batch_size': 32, 'tie_weights': 'FALSE', 'num_layers': 2}}]
