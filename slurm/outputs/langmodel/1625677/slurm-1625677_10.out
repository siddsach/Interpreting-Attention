TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0656132698059082 and batch: 50, loss is 6.9373563957214355 and perplexity is 1030.0435848548157
At time: 1.6729583740234375 and batch: 100, loss is 6.118933267593384 and perplexity is 454.3797342966461
At time: 2.2770137786865234 and batch: 150, loss is 6.024485340118408 and perplexity is 413.42881203674926
At time: 2.879793167114258 and batch: 200, loss is 5.882437238693237 and perplexity is 358.6823718185781
At time: 3.4834532737731934 and batch: 250, loss is 5.9357848072052 and perplexity is 378.33680113659653
At time: 4.086753845214844 and batch: 300, loss is 5.853866815567017 and perplexity is 348.5796711056812
At time: 4.690204620361328 and batch: 350, loss is 5.851698989868164 and perplexity is 347.82482961456503
At time: 5.293614625930786 and batch: 400, loss is 5.722485418319702 and perplexity is 305.66368176412266
At time: 5.897937536239624 and batch: 450, loss is 5.726629695892334 and perplexity is 306.93306542754067
At time: 6.503731966018677 and batch: 500, loss is 5.680815229415893 and perplexity is 293.1883482886005
At time: 7.125885725021362 and batch: 550, loss is 5.7308479690551755 and perplexity is 308.2305275411282
At time: 7.730057239532471 and batch: 600, loss is 5.6619619941711425 and perplexity is 287.71257955865417
At time: 8.3348388671875 and batch: 650, loss is 5.5717941093444825 and perplexity is 262.9053573278979
At time: 8.93953251838684 and batch: 700, loss is 5.670965795516968 and perplexity is 290.31478375482146
At time: 9.54615569114685 and batch: 750, loss is 5.637605791091919 and perplexity is 280.7896439893973
At time: 10.151939868927002 and batch: 800, loss is 5.625692434310913 and perplexity is 277.4643438529642
At time: 10.756270170211792 and batch: 850, loss is 5.6631935024261475 and perplexity is 288.06711823929726
At time: 11.362032890319824 and batch: 900, loss is 5.5577223587036135 and perplexity is 259.23172652478013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.424791727980522 and perplexity of 226.9640711269568
finished 1 epochs...
Completing Train Step...
At time: 12.905606508255005 and batch: 50, loss is 5.236102180480957 and perplexity is 187.93613176965263
At time: 13.506837606430054 and batch: 100, loss is 5.037928438186645 and perplexity is 154.150352056571
At time: 14.104459047317505 and batch: 150, loss is 4.991518754959106 and perplexity is 147.15975346637438
At time: 14.69956350326538 and batch: 200, loss is 4.856798067092895 and perplexity is 128.61173596569012
At time: 15.294623136520386 and batch: 250, loss is 4.9293438339233395 and perplexity is 138.28874218166445
At time: 15.89030647277832 and batch: 300, loss is 4.849969043731689 and perplexity is 127.73643554145222
At time: 16.486136198043823 and batch: 350, loss is 4.832420930862427 and perplexity is 125.51445495485801
At time: 17.080907106399536 and batch: 400, loss is 4.699221563339234 and perplexity is 109.86161884569121
At time: 17.676105737686157 and batch: 450, loss is 4.712795505523681 and perplexity is 111.36304116481331
At time: 18.271629095077515 and batch: 500, loss is 4.615182027816773 and perplexity is 101.00621279960257
At time: 18.872583627700806 and batch: 550, loss is 4.680375289916992 and perplexity is 107.81052518324395
At time: 19.482930183410645 and batch: 600, loss is 4.626331424713134 and perplexity is 102.13867254546619
At time: 20.103354454040527 and batch: 650, loss is 4.487768754959107 and perplexity is 88.92281577312953
At time: 20.722954511642456 and batch: 700, loss is 4.528045825958252 and perplexity is 92.57747170760122
At time: 21.347593069076538 and batch: 750, loss is 4.565387983322143 and perplexity is 96.09987193202598
At time: 21.953773498535156 and batch: 800, loss is 4.525856523513794 and perplexity is 92.37501332467241
At time: 22.5593318939209 and batch: 850, loss is 4.573933725357056 and perplexity is 96.92463573715557
At time: 23.173253774642944 and batch: 900, loss is 4.507342205047608 and perplexity is 90.68048780605484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.607740010300728 and perplexity of 100.25731291414724
finished 2 epochs...
Completing Train Step...
At time: 24.69539976119995 and batch: 50, loss is 4.536393175125122 and perplexity is 93.35348249953253
At time: 25.317888736724854 and batch: 100, loss is 4.399345850944519 and perplexity is 81.39760507922429
At time: 25.918402910232544 and batch: 150, loss is 4.396746740341187 and perplexity is 81.18631839844663
At time: 26.51892352104187 and batch: 200, loss is 4.2899059009552 and perplexity is 72.95960274768
At time: 27.117867708206177 and batch: 250, loss is 4.424423723220825 and perplexity is 83.46469459490187
At time: 27.717525720596313 and batch: 300, loss is 4.378834357261658 and perplexity is 79.74502499901999
At time: 28.316930055618286 and batch: 350, loss is 4.381194581985474 and perplexity is 79.93346346971549
At time: 28.917032480239868 and batch: 400, loss is 4.2849358510971065 and perplexity is 72.59788949541456
At time: 29.51676630973816 and batch: 450, loss is 4.324023838043213 and perplexity is 75.49178467200842
At time: 30.117527961730957 and batch: 500, loss is 4.203965682983398 and perplexity is 66.95131295902827
At time: 30.71692943572998 and batch: 550, loss is 4.277404518127441 and perplexity is 72.05318436924878
At time: 31.316821098327637 and batch: 600, loss is 4.274523077011108 and perplexity is 71.84586619224594
At time: 31.916543006896973 and batch: 650, loss is 4.1209051179885865 and perplexity is 61.6149858654282
At time: 32.517090797424316 and batch: 700, loss is 4.138591947555542 and perplexity is 62.714454010535164
At time: 33.117637634277344 and batch: 750, loss is 4.229326558113098 and perplexity is 68.67097069030741
At time: 33.71836566925049 and batch: 800, loss is 4.199585871696472 and perplexity is 66.65872006142133
At time: 34.32703709602356 and batch: 850, loss is 4.2555081129074095 and perplexity is 70.49262630342731
At time: 34.96571969985962 and batch: 900, loss is 4.205819764137268 and perplexity is 67.07556127424911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442374869568707 and perplexity of 84.97651035792765
finished 3 epochs...
Completing Train Step...
At time: 36.551218032836914 and batch: 50, loss is 4.268547739982605 and perplexity is 71.41784299261366
At time: 37.15726852416992 and batch: 100, loss is 4.138545761108398 and perplexity is 62.711557519609656
At time: 37.76297974586487 and batch: 150, loss is 4.139326109886169 and perplexity is 62.76051350575988
At time: 38.37026357650757 and batch: 200, loss is 4.0315836238861085 and perplexity is 56.35007795466213
At time: 38.976917028427124 and batch: 250, loss is 4.181458892822266 and perplexity is 65.46128458322883
At time: 39.58362531661987 and batch: 300, loss is 4.143532814979554 and perplexity is 63.0250845736553
At time: 40.1909294128418 and batch: 350, loss is 4.146069278717041 and perplexity is 63.1851483273658
At time: 40.81268119812012 and batch: 400, loss is 4.066991143226623 and perplexity is 58.381037877227776
At time: 41.42949962615967 and batch: 450, loss is 4.1090168952941895 and perplexity is 60.88683000747131
At time: 42.041924238204956 and batch: 500, loss is 3.9860149002075196 and perplexity is 53.83990388874297
At time: 42.659621238708496 and batch: 550, loss is 4.060483288764954 and perplexity is 58.002336183723344
At time: 43.26569485664368 and batch: 600, loss is 4.069171171188355 and perplexity is 58.508449001681676
At time: 43.87118101119995 and batch: 650, loss is 3.916211233139038 and perplexity is 50.20985052969308
At time: 44.478004455566406 and batch: 700, loss is 3.9261330366134644 and perplexity is 50.710502376647945
At time: 45.085429668426514 and batch: 750, loss is 4.0288135433197025 and perplexity is 56.19419969614742
At time: 45.69494938850403 and batch: 800, loss is 4.0026667070388795 and perplexity is 54.743941609461196
At time: 46.302000761032104 and batch: 850, loss is 4.058464317321778 and perplexity is 57.88534925969927
At time: 46.908143281936646 and batch: 900, loss is 4.020484209060669 and perplexity is 55.72808333777676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388888842438998 and perplexity of 80.55086453296943
finished 4 epochs...
Completing Train Step...
At time: 48.463154792785645 and batch: 50, loss is 4.091748785972595 and perplexity is 59.844455391868436
At time: 49.071372985839844 and batch: 100, loss is 3.9656840705871583 and perplexity is 52.75634611338978
At time: 49.68003487586975 and batch: 150, loss is 3.9711208152770996 and perplexity is 53.043950004024474
At time: 50.29009699821472 and batch: 200, loss is 3.8662980222702026 and perplexity is 47.76523254677233
At time: 50.912049531936646 and batch: 250, loss is 4.017150263786316 and perplexity is 55.54259832792849
At time: 51.52135705947876 and batch: 300, loss is 3.983402051925659 and perplexity is 53.69941201028125
At time: 52.13055229187012 and batch: 350, loss is 3.986886854171753 and perplexity is 53.886870279662354
At time: 52.738627433776855 and batch: 400, loss is 3.9144411182403562 and perplexity is 50.12105194024907
At time: 53.34950613975525 and batch: 450, loss is 3.957000012397766 and perplexity is 52.30019044174303
At time: 53.95739769935608 and batch: 500, loss is 3.837081036567688 and perplexity is 46.3898662951219
At time: 54.5705292224884 and batch: 550, loss is 3.909335331916809 and perplexity is 49.86579675247565
At time: 55.181528091430664 and batch: 600, loss is 3.9242061042785643 and perplexity is 50.61288075523511
At time: 55.7910270690918 and batch: 650, loss is 3.772799906730652 and perplexity is 43.501695168695576
At time: 56.399341106414795 and batch: 700, loss is 3.7777625942230224 and perplexity is 43.71811706013855
At time: 57.008480072021484 and batch: 750, loss is 3.8844725561141966 and perplexity is 48.641280146559374
At time: 57.62061047554016 and batch: 800, loss is 3.863492045402527 and perplexity is 47.631392273338555
At time: 58.236492395401 and batch: 850, loss is 3.9173011207580566 and perplexity is 50.264603455986524
At time: 58.844361305236816 and batch: 900, loss is 3.881770191192627 and perplexity is 48.510011105662464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369694957994435 and perplexity of 79.01952374877943
finished 5 epochs...
Completing Train Step...
At time: 60.412264347076416 and batch: 50, loss is 3.9566629838943483 and perplexity is 52.28256675683887
At time: 61.04009389877319 and batch: 100, loss is 3.8408746194839476 and perplexity is 46.56618432646908
At time: 61.6428599357605 and batch: 150, loss is 3.8465513086318968 and perplexity is 46.8312777944213
At time: 62.24422883987427 and batch: 200, loss is 3.7439914608001708 and perplexity is 42.2663584366511
At time: 62.84653663635254 and batch: 250, loss is 3.890550527572632 and perplexity is 48.937820728696046
At time: 63.448342084884644 and batch: 300, loss is 3.8636223220825197 and perplexity is 47.63759793720522
At time: 64.04881834983826 and batch: 350, loss is 3.8637431049346924 and perplexity is 47.64335208964921
At time: 64.65634679794312 and batch: 400, loss is 3.7994011306762694 and perplexity is 44.67442233948968
At time: 65.26695203781128 and batch: 450, loss is 3.838850998878479 and perplexity is 46.47204731726833
At time: 65.87642240524292 and batch: 500, loss is 3.723664011955261 and perplexity is 41.41586466746316
At time: 66.49010753631592 and batch: 550, loss is 3.796352334022522 and perplexity is 44.53842652728343
At time: 67.09281396865845 and batch: 600, loss is 3.8119586944580077 and perplexity is 45.23896144674535
At time: 67.69620108604431 and batch: 650, loss is 3.6634192991256715 and perplexity is 38.994448861948285
At time: 68.29794073104858 and batch: 700, loss is 3.6668141412734987 and perplexity is 39.12705381957022
At time: 68.90010714530945 and batch: 750, loss is 3.774588632583618 and perplexity is 43.579577409715064
At time: 69.50187540054321 and batch: 800, loss is 3.757026572227478 and perplexity is 42.820911607507206
At time: 70.10395050048828 and batch: 850, loss is 3.808654770851135 and perplexity is 45.08974201461208
At time: 70.71841716766357 and batch: 900, loss is 3.7723892021179197 and perplexity is 43.48383249022144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372310690683861 and perplexity of 79.22648826407084
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.25069618225098 and batch: 50, loss is 3.879387826919556 and perplexity is 48.394580142215275
At time: 72.86529088020325 and batch: 100, loss is 3.7726338863372804 and perplexity is 43.494473599631796
At time: 73.47471618652344 and batch: 150, loss is 3.7756134700775146 and perplexity is 43.624262288071066
At time: 74.08906531333923 and batch: 200, loss is 3.6577590560913085 and perplexity is 38.774354286414614
At time: 74.7009048461914 and batch: 250, loss is 3.7960555267333986 and perplexity is 44.52520915924667
At time: 75.31132459640503 and batch: 300, loss is 3.7553674364089966 and perplexity is 42.749924803937354
At time: 75.91199088096619 and batch: 350, loss is 3.740456991195679 and perplexity is 42.11723297241998
At time: 76.51391887664795 and batch: 400, loss is 3.6719261503219602 and perplexity is 39.32758379155326
At time: 77.1157386302948 and batch: 450, loss is 3.6983805227279665 and perplexity is 40.38185388202684
At time: 77.71739029884338 and batch: 500, loss is 3.5779993772506713 and perplexity is 35.8018431690621
At time: 78.31832504272461 and batch: 550, loss is 3.626017279624939 and perplexity is 37.562915718865256
At time: 78.91982889175415 and batch: 600, loss is 3.6382958126068115 and perplexity is 38.026976380257025
At time: 79.51961135864258 and batch: 650, loss is 3.4802804803848266 and perplexity is 32.46882766811331
At time: 80.1228277683258 and batch: 700, loss is 3.4696327829360962 and perplexity is 32.124943451077655
At time: 80.72834253311157 and batch: 750, loss is 3.5593250131607057 and perplexity is 35.139470458307656
At time: 81.35013723373413 and batch: 800, loss is 3.5357433271408083 and perplexity is 34.32051660641043
At time: 81.95342659950256 and batch: 850, loss is 3.566887583732605 and perplexity is 35.406222578023126
At time: 82.5537040233612 and batch: 900, loss is 3.5203677558898927 and perplexity is 33.796855171261164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32780979104238 and perplexity of 75.77813473250337
finished 7 epochs...
Completing Train Step...
At time: 84.08370876312256 and batch: 50, loss is 3.7858707237243654 and perplexity is 44.07403015970751
At time: 84.68404078483582 and batch: 100, loss is 3.6758001041412354 and perplexity is 39.48023252114341
At time: 85.28394436836243 and batch: 150, loss is 3.678903422355652 and perplexity is 39.602942551491054
At time: 85.88243508338928 and batch: 200, loss is 3.5680343914031982 and perplexity is 35.44684999712762
At time: 86.48128747940063 and batch: 250, loss is 3.707522463798523 and perplexity is 40.75271502299179
At time: 87.07919836044312 and batch: 300, loss is 3.672060589790344 and perplexity is 39.332871326429746
At time: 87.67803406715393 and batch: 350, loss is 3.6593526458740233 and perplexity is 38.83619396168718
At time: 88.27900791168213 and batch: 400, loss is 3.59595787525177 and perplexity is 36.45059839716627
At time: 88.88088726997375 and batch: 450, loss is 3.6283034610748293 and perplexity is 37.6488895984838
At time: 89.48319244384766 and batch: 500, loss is 3.5123511505126954 and perplexity is 33.527002217876536
At time: 90.08570504188538 and batch: 550, loss is 3.5631757164001465 and perplexity is 35.27504298830245
At time: 90.6885998249054 and batch: 600, loss is 3.5823193359375 and perplexity is 35.9568402018071
At time: 91.28983807563782 and batch: 650, loss is 3.4289747953414915 and perplexity is 30.845004092511466
At time: 91.8950526714325 and batch: 700, loss is 3.423055639266968 and perplexity is 30.66296698369137
At time: 92.5045690536499 and batch: 750, loss is 3.5209322786331176 and perplexity is 33.8159396509594
At time: 93.10605764389038 and batch: 800, loss is 3.501826219558716 and perplexity is 33.17598329969136
At time: 93.70662474632263 and batch: 850, loss is 3.5417855167388916 and perplexity is 34.52851542620089
At time: 94.30813574790955 and batch: 900, loss is 3.5033866453170774 and perplexity is 33.22779237018261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332945105147688 and perplexity of 76.1682801589335
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 95.85223031044006 and batch: 50, loss is 3.761748056411743 and perplexity is 43.0235679072001
At time: 96.45520806312561 and batch: 100, loss is 3.6633095264434816 and perplexity is 38.99016857163997
At time: 97.07922768592834 and batch: 150, loss is 3.6683230018615722 and perplexity is 39.1861356509203
At time: 97.67989945411682 and batch: 200, loss is 3.552581787109375 and perplexity is 34.90331418809635
At time: 98.2816653251648 and batch: 250, loss is 3.691980504989624 and perplexity is 40.124234564321114
At time: 98.881844997406 and batch: 300, loss is 3.647631497383118 and perplexity is 38.38364653434083
At time: 99.48255467414856 and batch: 350, loss is 3.632817921638489 and perplexity is 37.8192382526687
At time: 100.08388876914978 and batch: 400, loss is 3.5654488611221313 and perplexity is 35.35531947155424
At time: 100.68580675125122 and batch: 450, loss is 3.5958308362960816 and perplexity is 36.445968045335434
At time: 101.28848695755005 and batch: 500, loss is 3.477823705673218 and perplexity is 32.38915697992136
At time: 101.89009523391724 and batch: 550, loss is 3.5244110679626464 and perplexity is 33.93378303829958
At time: 102.49278259277344 and batch: 600, loss is 3.5422040939331056 and perplexity is 34.54297130054697
At time: 103.09392714500427 and batch: 650, loss is 3.3800019931793215 and perplexity is 29.370829654561415
At time: 103.69551229476929 and batch: 700, loss is 3.3716891527175905 and perplexity is 29.127686638130978
At time: 104.29720711708069 and batch: 750, loss is 3.462856159210205 and perplexity is 31.907980764629258
At time: 104.89844489097595 and batch: 800, loss is 3.4371230602264404 and perplexity is 31.09736410809224
At time: 105.50092220306396 and batch: 850, loss is 3.4767641496658324 and perplexity is 32.354857028637774
At time: 106.11453318595886 and batch: 900, loss is 3.4416296005249025 and perplexity is 31.237821884223596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326679386504709 and perplexity of 75.69252318210428
finished 9 epochs...
Completing Train Step...
At time: 107.65522050857544 and batch: 50, loss is 3.734507608413696 and perplexity is 41.86740532878013
At time: 108.27126359939575 and batch: 100, loss is 3.626013855934143 and perplexity is 37.5627871152766
At time: 108.8735716342926 and batch: 150, loss is 3.6325866556167603 and perplexity is 37.81049295917679
At time: 109.47721767425537 and batch: 200, loss is 3.5196060943603515 and perplexity is 33.771123207625116
At time: 110.07960557937622 and batch: 250, loss is 3.6608946895599366 and perplexity is 38.89612726738224
At time: 110.68259358406067 and batch: 300, loss is 3.619250512123108 and perplexity is 37.30959425185373
At time: 111.28515267372131 and batch: 350, loss is 3.604227890968323 and perplexity is 36.75329534786027
At time: 111.8868510723114 and batch: 400, loss is 3.5392507553100585 and perplexity is 34.44110470656525
At time: 112.50256419181824 and batch: 450, loss is 3.5722791242599485 and perplexity is 35.59763219465641
At time: 113.10384917259216 and batch: 500, loss is 3.4554835844039915 and perplexity is 31.673601839242178
At time: 113.70596671104431 and batch: 550, loss is 3.5040425729751585 and perplexity is 33.249594547759145
At time: 114.30849981307983 and batch: 600, loss is 3.525157341957092 and perplexity is 33.959116389749184
At time: 114.91129088401794 and batch: 650, loss is 3.365467987060547 and perplexity is 28.947040970286743
At time: 115.5150978565216 and batch: 700, loss is 3.3597694349288942 and perplexity is 28.78255386256204
At time: 116.11862277984619 and batch: 750, loss is 3.4551026391983033 and perplexity is 31.661538230407505
At time: 116.72065043449402 and batch: 800, loss is 3.432436776161194 and perplexity is 30.951973962042775
At time: 117.32476282119751 and batch: 850, loss is 3.4753994798660277 and perplexity is 32.310733446281645
At time: 117.92711639404297 and batch: 900, loss is 3.4425827407836915 and perplexity is 31.26761010377882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32773788661173 and perplexity of 75.77268614476081
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 119.47088861465454 and batch: 50, loss is 3.730431709289551 and perplexity is 41.69710530669122
At time: 120.09975481033325 and batch: 100, loss is 3.628303289413452 and perplexity is 37.64888313562412
At time: 120.70547485351562 and batch: 150, loss is 3.636735167503357 and perplexity is 37.967676051190146
At time: 121.31140208244324 and batch: 200, loss is 3.5184978580474855 and perplexity is 33.733717553540664
At time: 121.92018246650696 and batch: 250, loss is 3.661471629142761 and perplexity is 38.91857445753499
At time: 122.52648258209229 and batch: 300, loss is 3.6167560148239137 and perplexity is 37.21664155308643
At time: 123.13268065452576 and batch: 350, loss is 3.598809962272644 and perplexity is 36.554707068681516
At time: 123.73900270462036 and batch: 400, loss is 3.53365026473999 and perplexity is 34.24875674865277
At time: 124.34578204154968 and batch: 450, loss is 3.562474527359009 and perplexity is 35.2503171844778
At time: 124.95273804664612 and batch: 500, loss is 3.446451120376587 and perplexity is 31.38879934048138
At time: 125.55823922157288 and batch: 550, loss is 3.4933254289627076 and perplexity is 32.89515652875445
At time: 126.16419672966003 and batch: 600, loss is 3.513607177734375 and perplexity is 33.56913950257215
At time: 126.77006888389587 and batch: 650, loss is 3.3497697639465334 and perplexity is 28.496172042247732
At time: 127.38944792747498 and batch: 700, loss is 3.343736643791199 and perplexity is 28.324768780488412
At time: 127.99445867538452 and batch: 750, loss is 3.438929977416992 and perplexity is 31.153605266135596
At time: 128.60207533836365 and batch: 800, loss is 3.4104028129577637 and perplexity is 30.277437947680596
At time: 129.2090175151825 and batch: 850, loss is 3.4515427446365354 and perplexity is 31.549026875743603
At time: 129.819993019104 and batch: 900, loss is 3.421178493499756 and perplexity is 30.60546211440447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3225816700556505 and perplexity of 75.38299130484234
finished 11 epochs...
Completing Train Step...
At time: 131.47687673568726 and batch: 50, loss is 3.717246823310852 and perplexity is 41.1509421889131
At time: 132.0911157131195 and batch: 100, loss is 3.6116172218322755 and perplexity is 37.02588348918397
At time: 132.6989450454712 and batch: 150, loss is 3.62199471950531 and perplexity is 37.41212012761846
At time: 133.3063428401947 and batch: 200, loss is 3.5061187028884886 and perplexity is 33.31869673334211
At time: 133.9147698879242 and batch: 250, loss is 3.6501970529556274 and perplexity is 38.48224834272602
At time: 134.52277398109436 and batch: 300, loss is 3.6067138147354125 and perplexity is 36.84477489676623
At time: 135.13124680519104 and batch: 350, loss is 3.588865065574646 and perplexity is 36.192975953635305
At time: 135.74012303352356 and batch: 400, loss is 3.5244218254089357 and perplexity is 33.934148081111466
At time: 136.34824109077454 and batch: 450, loss is 3.5549381399154663 and perplexity is 34.985655685134766
At time: 136.95702815055847 and batch: 500, loss is 3.439199242591858 and perplexity is 31.16199497658302
At time: 137.56527185440063 and batch: 550, loss is 3.486966676712036 and perplexity is 32.68664800771904
At time: 138.1735999584198 and batch: 600, loss is 3.5087491703033447 and perplexity is 33.40645585255329
At time: 138.782452583313 and batch: 650, loss is 3.3459905099868776 and perplexity is 28.388681017102257
At time: 139.39986419677734 and batch: 700, loss is 3.3410215187072754 and perplexity is 28.24796779959181
At time: 140.0098192691803 and batch: 750, loss is 3.4375224828720095 and perplexity is 31.109787580477455
At time: 140.61857795715332 and batch: 800, loss is 3.410655279159546 and perplexity is 30.28508294244973
At time: 141.2286388874054 and batch: 850, loss is 3.453544521331787 and perplexity is 31.612244234909582
At time: 141.83819484710693 and batch: 900, loss is 3.4247549676895144 and perplexity is 30.71511773308199
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32206788781571 and perplexity of 75.34427081051925
finished 12 epochs...
Completing Train Step...
At time: 143.39382648468018 and batch: 50, loss is 3.7115102767944337 and perplexity is 40.9155536988759
At time: 143.99595832824707 and batch: 100, loss is 3.6050246953964233 and perplexity is 36.782592206724125
At time: 144.5977578163147 and batch: 150, loss is 3.6154077196121217 and perplexity is 37.16649634634908
At time: 145.21215629577637 and batch: 200, loss is 3.499709186553955 and perplexity is 33.10582294018276
At time: 145.8135724067688 and batch: 250, loss is 3.6439116048812865 and perplexity is 38.241128735192994
At time: 146.41283440589905 and batch: 300, loss is 3.6008376026153566 and perplexity is 36.6289020624113
At time: 147.01496028900146 and batch: 350, loss is 3.582917809486389 and perplexity is 35.9783658601956
At time: 147.62040781974792 and batch: 400, loss is 3.519037857055664 and perplexity is 33.75193864680513
At time: 148.22011303901672 and batch: 450, loss is 3.5500736474990844 and perplexity is 34.81588149578505
At time: 148.82126379013062 and batch: 500, loss is 3.4346242237091062 and perplexity is 31.01975388707767
At time: 149.42409706115723 and batch: 550, loss is 3.482870645523071 and perplexity is 32.55303630367569
At time: 150.02677536010742 and batch: 600, loss is 3.5052833366394043 and perplexity is 33.29087504089695
At time: 150.6291208267212 and batch: 650, loss is 3.343170781135559 and perplexity is 28.308745385553866
At time: 151.23085737228394 and batch: 700, loss is 3.338704228401184 and perplexity is 28.18258484254728
At time: 151.83334398269653 and batch: 750, loss is 3.4361634826660157 and perplexity is 31.067538087784282
At time: 152.434641122818 and batch: 800, loss is 3.4099850034713746 and perplexity is 30.264790389201437
At time: 153.03722310066223 and batch: 850, loss is 3.4536706447601317 and perplexity is 31.616231530970598
At time: 153.63898921012878 and batch: 900, loss is 3.4253645277023317 and perplexity is 30.733846148107606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322284437205694 and perplexity of 75.36058833311301
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 155.1934220790863 and batch: 50, loss is 3.7100212717056276 and perplexity is 40.85467556637942
At time: 155.80945420265198 and batch: 100, loss is 3.6057875251770017 and perplexity is 36.81066176825386
At time: 156.4106481075287 and batch: 150, loss is 3.617891578674316 and perplexity is 37.25892743048039
At time: 157.01292419433594 and batch: 200, loss is 3.4997753429412843 and perplexity is 33.10801317427624
At time: 157.6140832901001 and batch: 250, loss is 3.644667692184448 and perplexity is 38.27005330045858
At time: 158.23785090446472 and batch: 300, loss is 3.601465549468994 and perplexity is 36.651910289429416
At time: 158.84190702438354 and batch: 350, loss is 3.58274742603302 and perplexity is 35.97223626417946
At time: 159.4428882598877 and batch: 400, loss is 3.5185468912124636 and perplexity is 33.73537166503162
At time: 160.04561829566956 and batch: 450, loss is 3.547426505088806 and perplexity is 34.723840775520266
At time: 160.6474883556366 and batch: 500, loss is 3.432109861373901 and perplexity is 30.94185695784792
At time: 161.25029754638672 and batch: 550, loss is 3.4794480323791506 and perplexity is 32.44181030410993
At time: 161.85190272331238 and batch: 600, loss is 3.4999232482910156 and perplexity is 33.11291038869686
At time: 162.4533236026764 and batch: 650, loss is 3.33698646068573 and perplexity is 28.13421526391078
At time: 163.05509972572327 and batch: 700, loss is 3.3311845016479493 and perplexity is 27.971454322129244
At time: 163.6573371887207 and batch: 750, loss is 3.430737557411194 and perplexity is 30.89942444674185
At time: 164.25885152816772 and batch: 800, loss is 3.4031281042099 and perplexity is 30.05797762777873
At time: 164.8594377040863 and batch: 850, loss is 3.4428579235076904 and perplexity is 31.276215593886914
At time: 165.45717573165894 and batch: 900, loss is 3.4153893613815307 and perplexity is 30.428794918955056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320417848351884 and perplexity of 75.22005230121333
finished 14 epochs...
Completing Train Step...
At time: 166.99114322662354 and batch: 50, loss is 3.7067641544342043 and perplexity is 40.72182357168714
At time: 167.6056251525879 and batch: 100, loss is 3.6013626623153687 and perplexity is 36.648139472692435
At time: 168.20761561393738 and batch: 150, loss is 3.614020276069641 and perplexity is 37.11496568720321
At time: 168.80620288848877 and batch: 200, loss is 3.496376886367798 and perplexity is 32.99568800348491
At time: 169.40432858467102 and batch: 250, loss is 3.641494770050049 and perplexity is 38.1488178383084
At time: 170.003169298172 and batch: 300, loss is 3.5985604667663575 and perplexity is 36.545587971168494
At time: 170.60331511497498 and batch: 350, loss is 3.579792046546936 and perplexity is 35.86608159598537
At time: 171.20223450660706 and batch: 400, loss is 3.5156409168243408 and perplexity is 33.6374798434512
At time: 171.80093336105347 and batch: 450, loss is 3.5451629877090456 and perplexity is 34.64533164534335
At time: 172.39960384368896 and batch: 500, loss is 3.4299865198135375 and perplexity is 30.87622652958081
At time: 172.99951720237732 and batch: 550, loss is 3.4774745512008667 and perplexity is 32.37785013493875
At time: 173.61405658721924 and batch: 600, loss is 3.4990172004699707 and perplexity is 33.08292209585593
At time: 174.2175304889679 and batch: 650, loss is 3.3363605642318728 and perplexity is 28.11661166793387
At time: 174.820734500885 and batch: 700, loss is 3.330958013534546 and perplexity is 27.965119837580477
At time: 175.4229073524475 and batch: 750, loss is 3.43018292427063 and perplexity is 30.882291353649162
At time: 176.02622509002686 and batch: 800, loss is 3.4032235145568848 and perplexity is 30.060845606669112
At time: 176.62789177894592 and batch: 850, loss is 3.44396993637085 and perplexity is 31.311014492763903
At time: 177.22994947433472 and batch: 900, loss is 3.417440457344055 and perplexity is 30.491271348030356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320074212061216 and perplexity of 75.19420840216179
finished 15 epochs...
Completing Train Step...
At time: 178.78893399238586 and batch: 50, loss is 3.7049597883224488 and perplexity is 40.64841274314832
At time: 179.3901915550232 and batch: 100, loss is 3.5991309785842898 and perplexity is 36.566443609627754
At time: 179.9932985305786 and batch: 150, loss is 3.6118513441085813 and perplexity is 37.03455308814221
At time: 180.59544014930725 and batch: 200, loss is 3.494389934539795 and perplexity is 32.93019225089081
At time: 181.19881463050842 and batch: 250, loss is 3.6394795513153078 and perplexity is 38.0720170370905
At time: 181.8009970188141 and batch: 300, loss is 3.5966447830200194 and perplexity is 36.47564519779874
At time: 182.4031639099121 and batch: 350, loss is 3.5779221296310424 and perplexity is 35.7990776687145
At time: 183.00451469421387 and batch: 400, loss is 3.5138832187652587 and perplexity is 33.578407241525575
At time: 183.6065685749054 and batch: 450, loss is 3.543731517791748 and perplexity is 34.595773374367774
At time: 184.2090244293213 and batch: 500, loss is 3.428615026473999 and perplexity is 30.833909016272486
At time: 184.81061387062073 and batch: 550, loss is 3.4763141775131228 and perplexity is 32.34030151900258
At time: 185.41265869140625 and batch: 600, loss is 3.4982103872299195 and perplexity is 33.05624112101861
At time: 186.01346373558044 and batch: 650, loss is 3.3358637380599974 and perplexity is 28.1026460689176
At time: 186.61513018608093 and batch: 700, loss is 3.3306966400146485 and perplexity is 27.95781145092516
At time: 187.21608591079712 and batch: 750, loss is 3.4299100542068484 and perplexity is 30.87386565045113
At time: 187.818519115448 and batch: 800, loss is 3.403291883468628 and perplexity is 30.062900904227746
At time: 188.41989636421204 and batch: 850, loss is 3.4444848918914794 and perplexity is 31.327142424760876
At time: 189.03589153289795 and batch: 900, loss is 3.41834578037262 and perplexity is 30.518888297395804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31996175687607 and perplexity of 75.18575289897558
finished 16 epochs...
Completing Train Step...
At time: 190.59665727615356 and batch: 50, loss is 3.7034224700927734 and perplexity is 40.585971205785334
At time: 191.19943070411682 and batch: 100, loss is 3.5973776721954347 and perplexity is 36.502387601742676
At time: 191.801335811615 and batch: 150, loss is 3.6101268577575683 and perplexity is 36.970742542846644
At time: 192.40128660202026 and batch: 200, loss is 3.4927589178085325 and perplexity is 32.876526333267606
At time: 193.00404238700867 and batch: 250, loss is 3.6378315830230714 and perplexity is 38.009327229786976
At time: 193.60698795318604 and batch: 300, loss is 3.5950731945037844 and perplexity is 36.41836551451939
At time: 194.21056818962097 and batch: 350, loss is 3.576377105712891 and perplexity is 35.74380994344142
At time: 194.82050371170044 and batch: 400, loss is 3.5124677276611327 and perplexity is 33.53091092801946
At time: 195.42874813079834 and batch: 450, loss is 3.54252281665802 and perplexity is 34.55398268518255
At time: 196.0308322906494 and batch: 500, loss is 3.427474231719971 and perplexity is 30.798753910813087
At time: 196.63348174095154 and batch: 550, loss is 3.475340142250061 and perplexity is 32.30881626126997
At time: 197.23692083358765 and batch: 600, loss is 3.4974318504333497 and perplexity is 33.03051563636721
At time: 197.83952140808105 and batch: 650, loss is 3.3353261137008667 and perplexity is 28.087541462500916
At time: 198.4415500164032 and batch: 700, loss is 3.3303172636032103 and perplexity is 27.947206928425278
At time: 199.044828414917 and batch: 750, loss is 3.4296378898620605 and perplexity is 30.86546402839815
At time: 199.6472351551056 and batch: 800, loss is 3.4032395029067994 and perplexity is 30.06132623382961
At time: 200.2500457763672 and batch: 850, loss is 3.444703335762024 and perplexity is 31.333986394489646
At time: 200.85367488861084 and batch: 900, loss is 3.4187803411483766 and perplexity is 30.532153491227263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319959666630993 and perplexity of 75.18559574248994
finished 17 epochs...
Completing Train Step...
At time: 202.3967626094818 and batch: 50, loss is 3.7020201110839843 and perplexity is 40.52909499319012
At time: 203.01792192459106 and batch: 100, loss is 3.5958415031433106 and perplexity is 36.44635681098213
At time: 203.6264407634735 and batch: 150, loss is 3.608613886833191 and perplexity is 36.91484917750703
At time: 204.25799179077148 and batch: 200, loss is 3.491299147605896 and perplexity is 32.828569171494095
At time: 204.86392998695374 and batch: 250, loss is 3.6363737964630127 and perplexity is 37.953958111378455
At time: 205.4718668460846 and batch: 300, loss is 3.593679037094116 and perplexity is 36.367627956678525
At time: 206.07862949371338 and batch: 350, loss is 3.574994535446167 and perplexity is 35.69442576101183
At time: 206.68460631370544 and batch: 400, loss is 3.5112139558792115 and perplexity is 33.48889716141691
At time: 207.29135036468506 and batch: 450, loss is 3.5414166927337645 and perplexity is 34.51578282903838
At time: 207.8984317779541 and batch: 500, loss is 3.4264358949661253 and perplexity is 30.766791029643695
At time: 208.50565886497498 and batch: 550, loss is 3.474434609413147 and perplexity is 32.27957280967468
At time: 209.1123583316803 and batch: 600, loss is 3.4966700792312624 and perplexity is 33.00536352205982
At time: 209.72042989730835 and batch: 650, loss is 3.3347548246383667 and perplexity is 28.071499939880372
At time: 210.3271028995514 and batch: 700, loss is 3.329861936569214 and perplexity is 27.934484706191732
At time: 210.93348169326782 and batch: 750, loss is 3.429329447746277 and perplexity is 30.855945287434356
At time: 211.53934001922607 and batch: 800, loss is 3.4030922746658323 and perplexity is 30.05690068343806
At time: 212.1452672481537 and batch: 850, loss is 3.4447528743743896 and perplexity is 31.335538675144107
At time: 212.75295400619507 and batch: 900, loss is 3.4189765882492065 and perplexity is 30.538145925811733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320019865689212 and perplexity of 75.19012198078156
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 214.28934025764465 and batch: 50, loss is 3.701534867286682 and perplexity is 40.50943327198433
At time: 214.91371536254883 and batch: 100, loss is 3.5958797311782837 and perplexity is 36.447750110216326
At time: 215.53161311149597 and batch: 150, loss is 3.6093869638442992 and perplexity is 36.94339823266195
At time: 216.14864778518677 and batch: 200, loss is 3.491496319770813 and perplexity is 32.8350426897266
At time: 216.75788688659668 and batch: 250, loss is 3.6366865062713623 and perplexity is 37.96582854224892
At time: 217.36744379997253 and batch: 300, loss is 3.5939146089553833 and perplexity is 36.37619615565982
At time: 217.97822284698486 and batch: 350, loss is 3.5750664949417112 and perplexity is 35.69699440630143
At time: 218.58738017082214 and batch: 400, loss is 3.5110990524291994 and perplexity is 33.485049392661146
At time: 219.1960210800171 and batch: 450, loss is 3.54069372177124 and perplexity is 34.49083793860645
At time: 219.82815265655518 and batch: 500, loss is 3.4254455614089965 and perplexity is 30.736336726490087
At time: 220.43908286094666 and batch: 550, loss is 3.473098373413086 and perplexity is 32.23646848760691
At time: 221.04744696617126 and batch: 600, loss is 3.4946758937835694 and perplexity is 32.93961029028776
At time: 221.65558338165283 and batch: 650, loss is 3.3325953912734985 and perplexity is 28.010946810063437
At time: 222.26481699943542 and batch: 700, loss is 3.327531976699829 and perplexity is 27.869474243149607
At time: 222.8744306564331 and batch: 750, loss is 3.4276482009887697 and perplexity is 30.804112413604756
At time: 223.48243880271912 and batch: 800, loss is 3.400769600868225 and perplexity is 29.98716932073683
At time: 224.08994317054749 and batch: 850, loss is 3.4412809658050536 and perplexity is 31.226933193140475
At time: 224.69754338264465 and batch: 900, loss is 3.415152759552002 and perplexity is 30.42159626204816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3193380277450775 and perplexity of 75.13887197667518
finished 19 epochs...
Completing Train Step...
At time: 226.24656295776367 and batch: 50, loss is 3.700788583755493 and perplexity is 40.47921302691382
At time: 226.84829354286194 and batch: 100, loss is 3.5950763988494874 and perplexity is 36.4184822117394
At time: 227.4498748779297 and batch: 150, loss is 3.608523058891296 and perplexity is 36.91149642999476
At time: 228.05177903175354 and batch: 200, loss is 3.4907235288619995 and perplexity is 32.8096778693591
At time: 228.6521942615509 and batch: 250, loss is 3.635962495803833 and perplexity is 37.93835083325067
At time: 229.25293040275574 and batch: 300, loss is 3.5932486772537233 and perplexity is 36.351980157449326
At time: 229.8523030281067 and batch: 350, loss is 3.5744431686401366 and perplexity is 35.67475046413813
At time: 230.45253825187683 and batch: 400, loss is 3.5105056667327883 and perplexity is 33.46518573729525
At time: 231.05574321746826 and batch: 450, loss is 3.5402189970016478 and perplexity is 34.47446816938788
At time: 231.65757870674133 and batch: 500, loss is 3.425109486579895 and perplexity is 30.726008752960706
At time: 232.26142859458923 and batch: 550, loss is 3.4727372550964355 and perplexity is 32.22482941004045
At time: 232.86541104316711 and batch: 600, loss is 3.494532151222229 and perplexity is 32.93487580661663
At time: 233.46530842781067 and batch: 650, loss is 3.3325355291366576 and perplexity is 28.009270065119722
At time: 234.06371665000916 and batch: 700, loss is 3.3274395656585694 and perplexity is 27.86689891501165
At time: 234.66053557395935 and batch: 750, loss is 3.427551989555359 and perplexity is 30.80114884836121
At time: 235.27793908119202 and batch: 800, loss is 3.400886549949646 and perplexity is 29.99067649771988
At time: 235.87522506713867 and batch: 850, loss is 3.441457552909851 and perplexity is 31.23244795376823
At time: 236.47561073303223 and batch: 900, loss is 3.4155590295791627 and perplexity is 30.43395815575122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319039540748074 and perplexity of 75.11644734731726
Finished Training.
Improved accuracyfrom -10000000 to -75.11644734731726
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f39b9e46b70>
ELAPSED
247.955153465271


RESULTS SO FAR:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8064701557159424 and batch: 50, loss is 6.896988000869751 and perplexity is 989.2904815359807
At time: 1.4219391345977783 and batch: 100, loss is 5.975442609786987 and perplexity is 393.64229247006693
At time: 2.0255637168884277 and batch: 150, loss is 5.701621732711792 and perplexity is 299.35247723340797
At time: 2.6285438537597656 and batch: 200, loss is 5.4049950695037845 and perplexity is 222.5151233711375
At time: 3.23115873336792 and batch: 250, loss is 5.382443609237671 and perplexity is 217.55324153459165
At time: 3.8335301876068115 and batch: 300, loss is 5.24667947769165 and perplexity is 189.93453833043355
At time: 4.438412189483643 and batch: 350, loss is 5.199433584213256 and perplexity is 181.1695954886659
At time: 5.043501377105713 and batch: 400, loss is 5.034293212890625 and perplexity is 153.59099810218876
At time: 5.649854421615601 and batch: 450, loss is 5.016877994537354 and perplexity is 150.9393340059838
At time: 6.255741834640503 and batch: 500, loss is 4.9352402496337895 and perplexity is 139.10655881760619
At time: 6.860439777374268 and batch: 550, loss is 4.982810745239258 and perplexity is 145.88384826286276
At time: 7.465353965759277 and batch: 600, loss is 4.891603422164917 and perplexity is 133.1669257595406
At time: 8.069829225540161 and batch: 650, loss is 4.760582294464111 and perplexity is 116.81392620152691
At time: 8.675175428390503 and batch: 700, loss is 4.8204872608184814 and perplexity is 124.0255088298993
At time: 9.280391216278076 and batch: 750, loss is 4.824340944290161 and perplexity is 124.50438601123773
At time: 9.88550353050232 and batch: 800, loss is 4.777512512207031 and perplexity is 118.80844760741539
At time: 10.490679502487183 and batch: 850, loss is 4.8121339702606205 and perplexity is 122.99380278737391
At time: 11.097858667373657 and batch: 900, loss is 4.73390606880188 and perplexity is 113.73896803292932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.770937984936858 and perplexity of 118.02990032322089
finished 1 epochs...
Completing Train Step...
At time: 12.634712934494019 and batch: 50, loss is 4.74521469116211 and perplexity is 115.03249931320106
At time: 13.234700679779053 and batch: 100, loss is 4.625765180587768 and perplexity is 102.08085349355875
At time: 13.837006330490112 and batch: 150, loss is 4.615693521499634 and perplexity is 101.05789005454734
At time: 14.436729192733765 and batch: 200, loss is 4.4936280441284175 and perplexity is 89.44536966668406
At time: 15.051499843597412 and batch: 250, loss is 4.6152201271057125 and perplexity is 101.01006113779773
At time: 15.65324068069458 and batch: 300, loss is 4.560522661209107 and perplexity is 95.63345066497583
At time: 16.251742601394653 and batch: 350, loss is 4.560665006637573 and perplexity is 95.6470646184055
At time: 16.84968662261963 and batch: 400, loss is 4.446565375328064 and perplexity is 85.33335206541958
At time: 17.447954416275024 and batch: 450, loss is 4.4776326942443845 and perplexity is 88.02604127177638
At time: 18.046975135803223 and batch: 500, loss is 4.372388000488281 and perplexity is 79.23261348515022
At time: 18.645447254180908 and batch: 550, loss is 4.442784805297851 and perplexity is 85.01135240667377
At time: 19.244934797286987 and batch: 600, loss is 4.418500337600708 and perplexity is 82.97176237822416
At time: 19.84265923500061 and batch: 650, loss is 4.267741260528564 and perplexity is 71.36026918875369
At time: 20.44478678703308 and batch: 700, loss is 4.2978104209899906 and perplexity is 73.53859871724491
At time: 21.042680978775024 and batch: 750, loss is 4.370077605247498 and perplexity is 79.04976613819484
At time: 21.640596866607666 and batch: 800, loss is 4.3340365505218506 and perplexity is 76.25145906032716
At time: 22.240742444992065 and batch: 850, loss is 4.387796430587769 and perplexity is 80.46291785966696
At time: 22.838368892669678 and batch: 900, loss is 4.328246283531189 and perplexity is 75.81121853902127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5263956148330475 and perplexity of 92.42482531787185
finished 2 epochs...
Completing Train Step...
At time: 24.378295183181763 and batch: 50, loss is 4.404180793762207 and perplexity is 81.79211078301978
At time: 24.97959017753601 and batch: 100, loss is 4.291629176139832 and perplexity is 73.08544061607212
At time: 25.59083127975464 and batch: 150, loss is 4.287518453598023 and perplexity is 72.78562330294093
At time: 26.191654682159424 and batch: 200, loss is 4.171242651939392 and perplexity is 64.79592087645399
At time: 26.792114973068237 and batch: 250, loss is 4.318546085357666 and perplexity is 75.07938987532206
At time: 27.389015674591064 and batch: 300, loss is 4.27351544380188 and perplexity is 71.77350837271808
At time: 27.985517263412476 and batch: 350, loss is 4.279617519378662 and perplexity is 72.21281472235049
At time: 28.58222508430481 and batch: 400, loss is 4.1886620473861695 and perplexity is 65.9345146625925
At time: 29.178977727890015 and batch: 450, loss is 4.229636516571045 and perplexity is 68.69225913758498
At time: 29.77747631072998 and batch: 500, loss is 4.115763330459595 and perplexity is 61.29898779317546
At time: 30.389323234558105 and batch: 550, loss is 4.187162594795227 and perplexity is 65.8357230689146
At time: 30.988877296447754 and batch: 600, loss is 4.181916403770447 and perplexity is 65.49124068970843
At time: 31.58785891532898 and batch: 650, loss is 4.033452730178833 and perplexity is 56.455500732459704
At time: 32.1940279006958 and batch: 700, loss is 4.044498910903931 and perplexity is 57.08257541039408
At time: 32.79328179359436 and batch: 750, loss is 4.13593891620636 and perplexity is 62.54829111314877
At time: 33.38904047012329 and batch: 800, loss is 4.110998749732971 and perplexity is 61.00761849484935
At time: 33.984522342681885 and batch: 850, loss is 4.169012675285339 and perplexity is 64.6515884743765
At time: 34.580485582351685 and batch: 900, loss is 4.11826479434967 and perplexity is 61.45251694139167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426128230682791 and perplexity of 83.60708210546296
finished 3 epochs...
Completing Train Step...
At time: 36.06760883331299 and batch: 50, loss is 4.203836317062378 and perplexity is 66.94265230097291
At time: 36.677390575408936 and batch: 100, loss is 4.0924376058578495 and perplexity is 59.8856916433091
At time: 37.27564764022827 and batch: 150, loss is 4.093664231300354 and perplexity is 59.95919402705174
At time: 37.87469959259033 and batch: 200, loss is 3.9802236223220824 and perplexity is 53.52900316916746
At time: 38.47285985946655 and batch: 250, loss is 4.133140802383423 and perplexity is 62.37351850603885
At time: 39.071115016937256 and batch: 300, loss is 4.090390329360962 and perplexity is 59.763214489385824
At time: 39.66971206665039 and batch: 350, loss is 4.097186183929443 and perplexity is 60.170739776477134
At time: 40.26823663711548 and batch: 400, loss is 4.0187561988830565 and perplexity is 55.631867797248674
At time: 40.867273569107056 and batch: 450, loss is 4.062126598358154 and perplexity is 58.097730338799785
At time: 41.46567940711975 and batch: 500, loss is 3.9464422988891603 and perplexity is 51.75092461224542
At time: 42.06373977661133 and batch: 550, loss is 4.0171797609329225 and perplexity is 55.54423670025779
At time: 42.662827253341675 and batch: 600, loss is 4.020036244392395 and perplexity is 55.703124716118545
At time: 43.26089119911194 and batch: 650, loss is 3.8749481534957884 and perplexity is 48.18020025138681
At time: 43.85812830924988 and batch: 700, loss is 3.87949107170105 and perplexity is 48.399576888007076
At time: 44.45556974411011 and batch: 750, loss is 3.975083341598511 and perplexity is 53.25455504053749
At time: 45.052520990371704 and batch: 800, loss is 3.9564696884155275 and perplexity is 52.27246174972118
At time: 45.66100335121155 and batch: 850, loss is 4.014060487747193 and perplexity is 55.37124899032161
At time: 46.25897407531738 and batch: 900, loss is 3.969567656517029 and perplexity is 52.961628274313526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.395568011558219 and perplexity of 81.09067812621161
finished 4 epochs...
Completing Train Step...
At time: 47.752360343933105 and batch: 50, loss is 4.058041663169861 and perplexity is 57.86088894599018
At time: 48.365339279174805 and batch: 100, loss is 3.948353419303894 and perplexity is 51.84992142803987
At time: 48.965771436691284 and batch: 150, loss is 3.952737765312195 and perplexity is 52.0777484954803
At time: 49.567259550094604 and batch: 200, loss is 3.843326187133789 and perplexity is 46.68048452763108
At time: 50.170273780822754 and batch: 250, loss is 3.9943843030929567 and perplexity is 54.29240266664798
At time: 50.76982927322388 and batch: 300, loss is 3.955097255706787 and perplexity is 52.200770520396475
At time: 51.36923623085022 and batch: 350, loss is 3.962485809326172 and perplexity is 52.58788706696078
At time: 51.9690318107605 and batch: 400, loss is 3.892276349067688 and perplexity is 49.02235159324135
At time: 52.56825542449951 and batch: 450, loss is 3.9348430395126344 and perplexity is 51.1541201508631
At time: 53.16967821121216 and batch: 500, loss is 3.8210050582885744 and perplexity is 45.650066249500874
At time: 53.77021384239197 and batch: 550, loss is 3.8887026739120483 and perplexity is 48.847474296721856
At time: 54.37394118309021 and batch: 600, loss is 3.894748320579529 and perplexity is 49.14368335236474
At time: 54.976245641708374 and batch: 650, loss is 3.7521929597854613 and perplexity is 42.61443134118846
At time: 55.577972412109375 and batch: 700, loss is 3.7568282461166382 and perplexity is 42.81241994473254
At time: 56.18163442611694 and batch: 750, loss is 3.85769745349884 and perplexity is 47.35618591762712
At time: 56.784027338027954 and batch: 800, loss is 3.8370267009735106 and perplexity is 46.38734574265142
At time: 57.38507556915283 and batch: 850, loss is 3.893309097290039 and perplexity is 49.07300549156432
At time: 57.98634147644043 and batch: 900, loss is 3.8534982872009276 and perplexity is 47.15774634968736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388398888992937 and perplexity of 80.51140802602333
finished 5 epochs...
Completing Train Step...
At time: 59.50850534439087 and batch: 50, loss is 3.9445904111862182 and perplexity is 51.65517639617602
At time: 60.1128876209259 and batch: 100, loss is 3.8359735107421873 and perplexity is 46.33851676087502
At time: 60.729844093322754 and batch: 150, loss is 3.841599631309509 and perplexity is 46.59995760230984
At time: 61.334924936294556 and batch: 200, loss is 3.736717872619629 and perplexity is 41.96004569830559
At time: 61.939435958862305 and batch: 250, loss is 3.885340762138367 and perplexity is 48.683529136766
At time: 62.54156541824341 and batch: 300, loss is 3.8500554609298705 and perplexity is 46.99566958283823
At time: 63.14452815055847 and batch: 350, loss is 3.858308219909668 and perplexity is 47.38511831990087
At time: 63.74770665168762 and batch: 400, loss is 3.7883008480072022 and perplexity is 44.18126577580507
At time: 64.35143256187439 and batch: 450, loss is 3.8326973247528078 and perplexity is 46.18695157493113
At time: 64.95537686347961 and batch: 500, loss is 3.7223501205444336 and perplexity is 41.361484451277164
At time: 65.56028032302856 and batch: 550, loss is 3.7883927488327025 and perplexity is 44.185326257179455
At time: 66.16543936729431 and batch: 600, loss is 3.7930182600021363 and perplexity is 44.39017938728585
At time: 66.76935410499573 and batch: 650, loss is 3.6564335012435913 and perplexity is 38.72299080320112
At time: 67.37388157844543 and batch: 700, loss is 3.659955077171326 and perplexity is 38.859597149096295
At time: 67.97805047035217 and batch: 750, loss is 3.76165123462677 and perplexity is 43.019402490214226
At time: 68.58307528495789 and batch: 800, loss is 3.7420654344558715 and perplexity is 42.185030661684046
At time: 69.18575644493103 and batch: 850, loss is 3.7954177856445312 and perplexity is 44.496822656457105
At time: 69.78923892974854 and batch: 900, loss is 3.7574097537994384 and perplexity is 42.83732293578823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382759407775043 and perplexity of 80.05864333159683
finished 6 epochs...
Completing Train Step...
At time: 71.29363965988159 and batch: 50, loss is 3.853091607093811 and perplexity is 47.1385721315013
At time: 71.89059829711914 and batch: 100, loss is 3.742443904876709 and perplexity is 42.20099946966173
At time: 72.4888334274292 and batch: 150, loss is 3.7501876401901244 and perplexity is 42.5290614125794
At time: 73.08633160591125 and batch: 200, loss is 3.6501026821136473 and perplexity is 38.478616911901916
At time: 73.68547821044922 and batch: 250, loss is 3.795403771400452 and perplexity is 44.49619907149318
At time: 74.29968214035034 and batch: 300, loss is 3.7635583114624023 and perplexity is 43.10152207547677
At time: 74.90195608139038 and batch: 350, loss is 3.7754661893844603 and perplexity is 43.61783774960415
At time: 75.50140285491943 and batch: 400, loss is 3.705336918830872 and perplexity is 40.66374539073559
At time: 76.11431241035461 and batch: 450, loss is 3.749280891418457 and perplexity is 42.490515716655594
At time: 76.71278429031372 and batch: 500, loss is 3.6423604488372803 and perplexity is 38.18185675915217
At time: 77.31145453453064 and batch: 550, loss is 3.7055915784835816 and perplexity is 40.67410212467984
At time: 77.90815567970276 and batch: 600, loss is 3.711643385887146 and perplexity is 40.92100029359417
At time: 78.50536489486694 and batch: 650, loss is 3.578961420059204 and perplexity is 35.836302647904255
At time: 79.10124135017395 and batch: 700, loss is 3.5866775035858156 and perplexity is 36.1138881114942
At time: 79.698810338974 and batch: 750, loss is 3.686736960411072 and perplexity is 39.914391991941564
At time: 80.29650950431824 and batch: 800, loss is 3.663899092674255 and perplexity is 39.01316263595785
At time: 80.89427304267883 and batch: 850, loss is 3.7176863288879396 and perplexity is 41.16903223255387
At time: 81.49123072624207 and batch: 900, loss is 3.684962821006775 and perplexity is 39.84364107586196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3965516808914815 and perplexity of 81.170483784252
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 82.9886679649353 and batch: 50, loss is 3.8098510551452636 and perplexity is 45.14371444155716
At time: 83.60010290145874 and batch: 100, loss is 3.699105806350708 and perplexity is 40.411152803032834
At time: 84.19625568389893 and batch: 150, loss is 3.70442723274231 and perplexity is 40.62677096735389
At time: 84.79203820228577 and batch: 200, loss is 3.584838013648987 and perplexity is 36.04751803998749
At time: 85.38732385635376 and batch: 250, loss is 3.726992688179016 and perplexity is 41.5539543719222
At time: 85.98358416557312 and batch: 300, loss is 3.6846887969970705 and perplexity is 39.83272445734922
At time: 86.57968211174011 and batch: 350, loss is 3.681844491958618 and perplexity is 39.7195890106133
At time: 87.1769425868988 and batch: 400, loss is 3.600904703140259 and perplexity is 36.63135996342858
At time: 87.78477835655212 and batch: 450, loss is 3.6409444570541383 and perplexity is 38.12782982359166
At time: 88.383620262146 and batch: 500, loss is 3.520161747932434 and perplexity is 33.78989346726664
At time: 88.98135995864868 and batch: 550, loss is 3.5649795293807984 and perplexity is 35.33872999119064
At time: 89.57853507995605 and batch: 600, loss is 3.5594401359558105 and perplexity is 35.14351604523049
At time: 90.17707514762878 and batch: 650, loss is 3.418057532310486 and perplexity is 30.51009255472439
At time: 90.7758936882019 and batch: 700, loss is 3.4089211511611937 and perplexity is 30.23261024252551
At time: 91.4021008014679 and batch: 750, loss is 3.49776038646698 and perplexity is 33.04136913374802
At time: 92.00690340995789 and batch: 800, loss is 3.4526818370819092 and perplexity is 31.58498460962014
At time: 92.60507822036743 and batch: 850, loss is 3.4982025051116943 and perplexity is 33.055980568844866
At time: 93.20694589614868 and batch: 900, loss is 3.4468871545791626 and perplexity is 31.402488914916052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353810349555864 and perplexity of 77.77424611285883
finished 8 epochs...
Completing Train Step...
At time: 94.71688675880432 and batch: 50, loss is 3.7230650568008423 and perplexity is 41.39106584927723
At time: 95.32824683189392 and batch: 100, loss is 3.6035704755783082 and perplexity is 36.72914110641981
At time: 95.92698669433594 and batch: 150, loss is 3.607182388305664 and perplexity is 36.862043429957886
At time: 96.53356528282166 and batch: 200, loss is 3.4949174547195434 and perplexity is 32.947568174498386
At time: 97.13344597816467 and batch: 250, loss is 3.639143257141113 and perplexity is 38.059215792173774
At time: 97.72983264923096 and batch: 300, loss is 3.6035675048828124 and perplexity is 36.72903199548782
At time: 98.32460474967957 and batch: 350, loss is 3.6034641790390016 and perplexity is 36.725237133321585
At time: 98.9225742816925 and batch: 400, loss is 3.527416830062866 and perplexity is 34.03593336014883
At time: 99.523428440094 and batch: 450, loss is 3.5747140741348264 and perplexity is 35.68441625925995
At time: 100.13697671890259 and batch: 500, loss is 3.45772403717041 and perplexity is 31.744644602365582
At time: 100.7428412437439 and batch: 550, loss is 3.5052638053894043 and perplexity is 33.29022483484349
At time: 101.34792494773865 and batch: 600, loss is 3.5055120611190795 and perplexity is 33.2984903498392
At time: 101.94571733474731 and batch: 650, loss is 3.3701775789260866 and perplexity is 29.083691249916452
At time: 102.54382228851318 and batch: 700, loss is 3.365391216278076 and perplexity is 28.944818768602413
At time: 103.1429340839386 and batch: 750, loss is 3.4619311046600343 and perplexity is 31.878477789870573
At time: 103.7437961101532 and batch: 800, loss is 3.423860878944397 and perplexity is 30.687667965104794
At time: 104.34483337402344 and batch: 850, loss is 3.4756130313873292 and perplexity is 32.31763418936933
At time: 104.94508409500122 and batch: 900, loss is 3.4292175340652467 and perplexity is 30.852492278238657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357510083342252 and perplexity of 78.06252306407295
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 106.52463126182556 and batch: 50, loss is 3.708521690368652 and perplexity is 40.79345657027606
At time: 107.12131261825562 and batch: 100, loss is 3.599742908477783 and perplexity is 36.58882655727015
At time: 107.71925330162048 and batch: 150, loss is 3.607292399406433 and perplexity is 36.86609888700077
At time: 108.31567239761353 and batch: 200, loss is 3.487667055130005 and perplexity is 32.709549049300904
At time: 108.91522860527039 and batch: 250, loss is 3.63528874874115 and perplexity is 37.91279858965486
At time: 109.51391434669495 and batch: 300, loss is 3.5910429906845094 and perplexity is 36.2718874452328
At time: 110.11352896690369 and batch: 350, loss is 3.5864967632293703 and perplexity is 36.107361464316334
At time: 110.71301198005676 and batch: 400, loss is 3.5079271459579466 and perplexity is 33.37900621622939
At time: 111.31204843521118 and batch: 450, loss is 3.548478879928589 and perplexity is 34.7604025068418
At time: 111.91006374359131 and batch: 500, loss is 3.4281609153747556 and perplexity is 30.819910174710362
At time: 112.50720596313477 and batch: 550, loss is 3.4716080713272093 and perplexity is 32.18846219220479
At time: 113.10425019264221 and batch: 600, loss is 3.468726887702942 and perplexity is 32.09585479556645
At time: 113.70223450660706 and batch: 650, loss is 3.327664008140564 and perplexity is 27.87315413291169
At time: 114.30012536048889 and batch: 700, loss is 3.3189892244338988 and perplexity is 27.63240627709999
At time: 114.89864802360535 and batch: 750, loss is 3.4100026941299437 and perplexity is 30.265325798010732
At time: 115.49722719192505 and batch: 800, loss is 3.362643280029297 and perplexity is 28.865389435247916
At time: 116.09620022773743 and batch: 850, loss is 3.419316067695618 and perplexity is 30.54851475858828
At time: 116.70012712478638 and batch: 900, loss is 3.3773503732681274 and perplexity is 29.29305254108393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338429490180864 and perplexity of 76.58716394467407
finished 10 epochs...
Completing Train Step...
At time: 118.24174213409424 and batch: 50, loss is 3.6805824613571168 and perplexity is 39.66949329161707
At time: 118.8422064781189 and batch: 100, loss is 3.562445111274719 and perplexity is 35.24928027342727
At time: 119.44004559516907 and batch: 150, loss is 3.569263973236084 and perplexity is 35.49046160644378
At time: 120.0375394821167 and batch: 200, loss is 3.452420449256897 and perplexity is 31.57672975809583
At time: 120.63501858711243 and batch: 250, loss is 3.6006007528305055 and perplexity is 36.62022754215816
At time: 121.23327589035034 and batch: 300, loss is 3.5582964181900025 and perplexity is 35.103344758262864
At time: 121.84622621536255 and batch: 350, loss is 3.555168662071228 and perplexity is 34.99372158355248
At time: 122.44689106941223 and batch: 400, loss is 3.4804582929611207 and perplexity is 32.474601547329186
At time: 123.04735565185547 and batch: 450, loss is 3.5240730905532835 and perplexity is 33.922316124106544
At time: 123.64723181724548 and batch: 500, loss is 3.4059753036499023 and perplexity is 30.143680633727154
At time: 124.24719834327698 and batch: 550, loss is 3.4507229375839232 and perplexity is 31.523173359903105
At time: 124.84676718711853 and batch: 600, loss is 3.4513988494873047 and perplexity is 31.54448745042171
At time: 125.44816136360168 and batch: 650, loss is 3.3136971282958982 and perplexity is 27.486559185358825
At time: 126.04686188697815 and batch: 700, loss is 3.3081399536132814 and perplexity is 27.334235212013773
At time: 126.64500331878662 and batch: 750, loss is 3.402553505897522 and perplexity is 30.040711325629406
At time: 127.24240303039551 and batch: 800, loss is 3.358654022216797 and perplexity is 28.75046733428457
At time: 127.83941888809204 and batch: 850, loss is 3.4188326025009155 and perplexity is 30.53374918456082
At time: 128.43614721298218 and batch: 900, loss is 3.379556279182434 and perplexity is 29.357741581669497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3391614940068495 and perplexity of 76.64324656554523
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 129.99092936515808 and batch: 50, loss is 3.678072772026062 and perplexity is 39.57006001304432
At time: 130.60755252838135 and batch: 100, loss is 3.567527651786804 and perplexity is 35.42889222429943
At time: 131.21134614944458 and batch: 150, loss is 3.574635844230652 and perplexity is 35.681624779985455
At time: 131.81512713432312 and batch: 200, loss is 3.457668299674988 and perplexity is 31.742875284691504
At time: 132.4188916683197 and batch: 250, loss is 3.6055473041534425 and perplexity is 36.80182013542156
At time: 133.02415871620178 and batch: 300, loss is 3.559693465232849 and perplexity is 35.152420054518316
At time: 133.62949347496033 and batch: 350, loss is 3.5536919593811036 and perplexity is 34.942084396522624
At time: 134.23256087303162 and batch: 400, loss is 3.4802930688858034 and perplexity is 32.46923640455481
At time: 134.8346803188324 and batch: 450, loss is 3.514950337409973 and perplexity is 33.61425851132993
At time: 135.4364070892334 and batch: 500, loss is 3.3998328638076782 and perplexity is 29.959092380301605
At time: 136.03783178329468 and batch: 550, loss is 3.442263627052307 and perplexity is 31.25763377192403
At time: 136.64135265350342 and batch: 600, loss is 3.4420737648010253 and perplexity is 31.251699690553206
At time: 137.25808000564575 and batch: 650, loss is 3.297746934890747 and perplexity is 27.051621144209616
At time: 137.86152267456055 and batch: 700, loss is 3.290633111000061 and perplexity is 26.859863548994067
At time: 138.46538543701172 and batch: 750, loss is 3.38580512046814 and perplexity is 29.541767825895125
At time: 139.0689458847046 and batch: 800, loss is 3.338326110839844 and perplexity is 28.17193052671325
At time: 139.67235660552979 and batch: 850, loss is 3.3957024002075196 and perplexity is 29.835602650217222
At time: 140.28962397575378 and batch: 900, loss is 3.3609803247451784 and perplexity is 28.817427473710033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332381575074915 and perplexity of 76.12536913443445
finished 12 epochs...
Completing Train Step...
At time: 141.826491355896 and batch: 50, loss is 3.665714478492737 and perplexity is 39.08405090345061
At time: 142.44427824020386 and batch: 100, loss is 3.55042209148407 and perplexity is 34.828014994073705
At time: 143.0492250919342 and batch: 150, loss is 3.5590605545043945 and perplexity is 35.130178749857286
At time: 143.65535044670105 and batch: 200, loss is 3.443145542144775 and perplexity is 31.285212510167575
At time: 144.26179790496826 and batch: 250, loss is 3.5908271217346193 and perplexity is 36.264058316042814
At time: 144.8681285381317 and batch: 300, loss is 3.5475545930862427 and perplexity is 34.72828876760963
At time: 145.47438669204712 and batch: 350, loss is 3.541467061042786 and perplexity is 34.51752137443746
At time: 146.0819172859192 and batch: 400, loss is 3.4697846746444703 and perplexity is 32.12982333421737
At time: 146.68791818618774 and batch: 450, loss is 3.5067155170440674 and perplexity is 33.338587738224234
At time: 147.2925055027008 and batch: 500, loss is 3.3917484664916993 and perplexity is 29.717867566709966
At time: 147.89665579795837 and batch: 550, loss is 3.43481559753418 and perplexity is 31.025690824100934
At time: 148.50063705444336 and batch: 600, loss is 3.4375487184524536 and perplexity is 31.110603774518736
At time: 149.1072223186493 and batch: 650, loss is 3.293692526817322 and perplexity is 26.94216487317663
At time: 149.71297001838684 and batch: 700, loss is 3.2881843519210814 and perplexity is 26.79417068009972
At time: 150.31849122047424 and batch: 750, loss is 3.3851714181900023 and perplexity is 29.52305307074142
At time: 150.92510724067688 and batch: 800, loss is 3.3391659545898436 and perplexity is 28.19560048463337
At time: 151.53787469863892 and batch: 850, loss is 3.3984576749801634 and perplexity is 29.917921286680084
At time: 152.1593246459961 and batch: 900, loss is 3.3653723192214966 and perplexity is 28.94427180189251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331685523464255 and perplexity of 76.07240038526653
finished 13 epochs...
Completing Train Step...
At time: 153.67100548744202 and batch: 50, loss is 3.6597842025756835 and perplexity is 38.85295759842804
At time: 154.2684509754181 and batch: 100, loss is 3.5439272928237915 and perplexity is 34.60254702604302
At time: 154.8650679588318 and batch: 150, loss is 3.5521918392181395 and perplexity is 34.88970636767445
At time: 155.46167612075806 and batch: 200, loss is 3.436320505142212 and perplexity is 31.072416772565706
At time: 156.05772352218628 and batch: 250, loss is 3.583772587776184 and perplexity is 36.009132533704175
At time: 156.654314994812 and batch: 300, loss is 3.540939016342163 and perplexity is 34.49929939163118
At time: 157.25402426719666 and batch: 350, loss is 3.5351291036605836 and perplexity is 34.299442611990976
At time: 157.85202932357788 and batch: 400, loss is 3.464159994125366 and perplexity is 31.949610637365748
At time: 158.4486358165741 and batch: 450, loss is 3.5016185903549193 and perplexity is 33.169095711750614
At time: 159.04506945610046 and batch: 500, loss is 3.3871597719192503 and perplexity is 29.58181374255444
At time: 159.64128971099854 and batch: 550, loss is 3.4305088329315185 and perplexity is 30.892357800151345
At time: 160.23761820793152 and batch: 600, loss is 3.4342541790008543 and perplexity is 31.008277314853846
At time: 160.83378911018372 and batch: 650, loss is 3.290928211212158 and perplexity is 26.867791070073324
At time: 161.43065571784973 and batch: 700, loss is 3.2862090349197386 and perplexity is 26.741295938598917
At time: 162.02725744247437 and batch: 750, loss is 3.384122958183289 and perplexity is 29.492115551530517
At time: 162.62663507461548 and batch: 800, loss is 3.3388228368759156 and perplexity is 28.185927734193164
At time: 163.2251799106598 and batch: 850, loss is 3.3988400745391845 and perplexity is 29.92936407430597
At time: 163.82682728767395 and batch: 900, loss is 3.3663426542282107 and perplexity is 28.97237107271406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3319200489619005 and perplexity of 76.09024339506254
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 165.34750390052795 and batch: 50, loss is 3.6586906719207763 and perplexity is 38.81049392015884
At time: 165.95105695724487 and batch: 100, loss is 3.5455649375915526 and perplexity is 34.65926013141675
At time: 166.54926109313965 and batch: 150, loss is 3.553839616775513 and perplexity is 34.94724423459459
At time: 167.15346026420593 and batch: 200, loss is 3.437224478721619 and perplexity is 31.10051811589876
At time: 167.76403856277466 and batch: 250, loss is 3.5845031118392945 and perplexity is 36.035447682266124
At time: 168.36199188232422 and batch: 300, loss is 3.5429508590698244 and perplexity is 34.56877642121592
At time: 168.96116304397583 and batch: 350, loss is 3.5352226543426513 and perplexity is 34.30265149833628
At time: 169.55958247184753 and batch: 400, loss is 3.4643175172805787 and perplexity is 31.954643837253517
At time: 170.15922117233276 and batch: 450, loss is 3.4998325443267824 and perplexity is 33.10990705266661
At time: 170.75707602500916 and batch: 500, loss is 3.386129312515259 and perplexity is 29.55134658467693
At time: 171.3551368713379 and batch: 550, loss is 3.428015260696411 and perplexity is 30.815421437517973
At time: 171.95554065704346 and batch: 600, loss is 3.431291699409485 and perplexity is 30.916551860622207
At time: 172.55505418777466 and batch: 650, loss is 3.28546603679657 and perplexity is 26.72143458529325
At time: 173.15190148353577 and batch: 700, loss is 3.2790866708755493 and perplexity is 26.551511353645942
At time: 173.74906396865845 and batch: 750, loss is 3.375968599319458 and perplexity is 29.252604115931405
At time: 174.3452250957489 and batch: 800, loss is 3.3315761041641236 and perplexity is 27.98241015904949
At time: 174.942476272583 and batch: 850, loss is 3.387856993675232 and perplexity is 29.602446018477163
At time: 175.54001426696777 and batch: 900, loss is 3.356167502403259 and perplexity is 28.679067532907244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331182610498716 and perplexity of 76.0341522073515
finished 15 epochs...
Completing Train Step...
At time: 177.04623699188232 and batch: 50, loss is 3.655504026412964 and perplexity is 38.687015479551846
At time: 177.65616416931152 and batch: 100, loss is 3.5409141969680786 and perplexity is 34.498443151239655
At time: 178.2530553340912 and batch: 150, loss is 3.5498220825195315 and perplexity is 34.80712414083654
At time: 178.85180687904358 and batch: 200, loss is 3.4334229230880737 and perplexity is 30.982512211171738
At time: 179.44972729682922 and batch: 250, loss is 3.5808267116546633 and perplexity is 35.90321018367845
At time: 180.04560327529907 and batch: 300, loss is 3.5390396308898926 and perplexity is 34.433834115830834
At time: 180.64172267913818 and batch: 350, loss is 3.53180136680603 and perplexity is 34.185492795377655
At time: 181.23804593086243 and batch: 400, loss is 3.46102783203125 and perplexity is 31.84969583436892
At time: 181.83277916908264 and batch: 450, loss is 3.497293643951416 and perplexity is 33.025950920448594
At time: 182.42983055114746 and batch: 500, loss is 3.3835433959960937 and perplexity is 29.475027988667414
At time: 183.03858375549316 and batch: 550, loss is 3.4258986282348634 and perplexity is 30.75026549610299
At time: 183.63376879692078 and batch: 600, loss is 3.4302006006240844 and perplexity is 30.882837244771277
At time: 184.23102974891663 and batch: 650, loss is 3.2846259117126464 and perplexity is 26.698994665309417
At time: 184.8310694694519 and batch: 700, loss is 3.278655014038086 and perplexity is 26.540052685509302
At time: 185.4288191795349 and batch: 750, loss is 3.376097168922424 and perplexity is 29.256365353413543
At time: 186.02542734146118 and batch: 800, loss is 3.3319696950912476 and perplexity is 27.99342594952254
At time: 186.6235134601593 and batch: 850, loss is 3.3892451667785646 and perplexity is 29.64356787335575
At time: 187.21947026252747 and batch: 900, loss is 3.358334264755249 and perplexity is 28.74127562746818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330701854130993 and perplexity of 75.99760708986626
finished 16 epochs...
Completing Train Step...
At time: 188.70703291893005 and batch: 50, loss is 3.6536213731765748 and perplexity is 38.61424976245189
At time: 189.314612865448 and batch: 100, loss is 3.5386273288726806 and perplexity is 34.41963990291692
At time: 189.91090416908264 and batch: 150, loss is 3.5475711011886597 and perplexity is 34.72886207048943
At time: 190.50690054893494 and batch: 200, loss is 3.431183066368103 and perplexity is 30.913193483983182
At time: 191.10395860671997 and batch: 250, loss is 3.578554420471191 and perplexity is 35.82172025520586
At time: 191.70158648490906 and batch: 300, loss is 3.536879734992981 and perplexity is 34.3595408805186
At time: 192.29788947105408 and batch: 350, loss is 3.5297077083587647 and perplexity is 34.113994921793186
At time: 192.89391040802002 and batch: 400, loss is 3.4591832256317137 and perplexity is 31.79099983376184
At time: 193.4905970096588 and batch: 450, loss is 3.4956681871414186 and perplexity is 32.97231226908381
At time: 194.0866677761078 and batch: 500, loss is 3.382006773948669 and perplexity is 29.429770791320333
At time: 194.6828579902649 and batch: 550, loss is 3.4245489645004272 and perplexity is 30.708790972564582
At time: 195.28006386756897 and batch: 600, loss is 3.4294184112548827 and perplexity is 30.858690462696874
At time: 195.87659001350403 and batch: 650, loss is 3.284002718925476 and perplexity is 26.682361227866746
At time: 196.47245287895203 and batch: 700, loss is 3.278340458869934 and perplexity is 26.53170568763854
At time: 197.06808829307556 and batch: 750, loss is 3.3761431217193603 and perplexity is 29.25770979611993
At time: 197.67640709877014 and batch: 800, loss is 3.332188968658447 and perplexity is 27.999564840912335
At time: 198.2729046344757 and batch: 850, loss is 3.3899521017074585 and perplexity is 29.664531355938443
At time: 198.86929845809937 and batch: 900, loss is 3.359383454322815 and perplexity is 28.771446498729077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330539233063998 and perplexity of 75.9852492827594
finished 17 epochs...
Completing Train Step...
At time: 200.3693459033966 and batch: 50, loss is 3.652055034637451 and perplexity is 38.553814118584825
At time: 200.9672338962555 and batch: 100, loss is 3.5368733167648316 and perplexity is 34.35932035385382
At time: 201.56479692459106 and batch: 150, loss is 3.545792274475098 and perplexity is 34.66714035529984
At time: 202.17847347259521 and batch: 200, loss is 3.429401168823242 and perplexity is 30.85815838842319
At time: 202.77870321273804 and batch: 250, loss is 3.5767258691787718 and perplexity is 35.75627825260006
At time: 203.3763289451599 and batch: 300, loss is 3.535160231590271 and perplexity is 34.30051029924628
At time: 203.97240900993347 and batch: 350, loss is 3.5280537605285645 and perplexity is 34.05761878835622
At time: 204.5691785812378 and batch: 400, loss is 3.4577318716049192 and perplexity is 31.744893304678957
At time: 205.16618156433105 and batch: 450, loss is 3.4943323945999145 and perplexity is 32.92829750412078
At time: 205.76342010498047 and batch: 500, loss is 3.380792021751404 and perplexity is 29.39404261741888
At time: 206.3601357936859 and batch: 550, loss is 3.4234593963623046 and perplexity is 30.675349873846766
At time: 206.95773005485535 and batch: 600, loss is 3.428703556060791 and perplexity is 30.836638850330623
At time: 207.55473971366882 and batch: 650, loss is 3.283412446975708 and perplexity is 26.66661602591305
At time: 208.1519796848297 and batch: 700, loss is 3.277990107536316 and perplexity is 26.52241189730869
At time: 208.75032353401184 and batch: 750, loss is 3.3760556602478027 and perplexity is 29.25515098566705
At time: 209.34981441497803 and batch: 800, loss is 3.3322473669052126 and perplexity is 28.0012000141544
At time: 209.9454128742218 and batch: 850, loss is 3.3903013038635255 and perplexity is 29.674892083135507
At time: 210.54236888885498 and batch: 900, loss is 3.359930191040039 and perplexity is 28.787181205916703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330518748662243 and perplexity of 75.98369278632761
finished 18 epochs...
Completing Train Step...
At time: 212.08682107925415 and batch: 50, loss is 3.6506378555297854 and perplexity is 38.49921515608765
At time: 212.68719363212585 and batch: 100, loss is 3.535348415374756 and perplexity is 34.306965706466585
At time: 213.300119638443 and batch: 150, loss is 3.5442360782623292 and perplexity is 34.613233438520346
At time: 213.90073704719543 and batch: 200, loss is 3.4278491353988647 and perplexity is 30.81030264165517
At time: 214.50254082679749 and batch: 250, loss is 3.575129270553589 and perplexity is 35.699235377305904
At time: 215.1060471534729 and batch: 300, loss is 3.5336492967605593 and perplexity is 34.248723596576745
At time: 215.7067723274231 and batch: 350, loss is 3.526609711647034 and perplexity is 34.00847341473835
At time: 216.30754590034485 and batch: 400, loss is 3.4564544200897216 and perplexity is 31.704366633567044
At time: 216.90842199325562 and batch: 450, loss is 3.493139133453369 and perplexity is 32.88902887960863
At time: 217.50915479660034 and batch: 500, loss is 3.379722638130188 and perplexity is 29.362625910932127
At time: 218.1097412109375 and batch: 550, loss is 3.4224849367141723 and perplexity is 30.645472542693387
At time: 218.71053504943848 and batch: 600, loss is 3.4280043745040896 and perplexity is 30.815085976739688
At time: 219.31163382530212 and batch: 650, loss is 3.282824058532715 and perplexity is 26.650930312328494
At time: 219.91263580322266 and batch: 700, loss is 3.277595839500427 and perplexity is 26.511957019210467
At time: 220.51350235939026 and batch: 750, loss is 3.3758731651306153 and perplexity is 29.24981255059357
At time: 221.11445307731628 and batch: 800, loss is 3.3321896457672118 and perplexity is 27.999583799669512
At time: 221.7154176235199 and batch: 850, loss is 3.390445518493652 and perplexity is 29.67917194532328
At time: 222.31750416755676 and batch: 900, loss is 3.3602126359939577 and perplexity is 28.795313148344842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330578947720462 and perplexity of 75.9882670707558
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 223.821368932724 and batch: 50, loss is 3.6502871465682984 and perplexity is 38.48571550368511
At time: 224.43555641174316 and batch: 100, loss is 3.5356618642807005 and perplexity is 34.31772087284307
At time: 225.0387864112854 and batch: 150, loss is 3.5446618604660034 and perplexity is 34.627974275300375
At time: 225.64233016967773 and batch: 200, loss is 3.4278933429718017 and perplexity is 30.811664720463288
At time: 226.2454068660736 and batch: 250, loss is 3.5748807287216184 and perplexity is 35.69036372647913
At time: 226.84954190254211 and batch: 300, loss is 3.5342944955825804 and perplexity is 34.2708279627857
At time: 227.45299196243286 and batch: 350, loss is 3.526357731819153 and perplexity is 33.9999050450343
At time: 228.0569167137146 and batch: 400, loss is 3.456341896057129 and perplexity is 31.70079933109004
At time: 228.6736180782318 and batch: 450, loss is 3.492908754348755 and perplexity is 32.881452807301606
At time: 229.27960228919983 and batch: 500, loss is 3.3794235754013062 and perplexity is 29.353845956844026
At time: 229.88388061523438 and batch: 550, loss is 3.421125741004944 and perplexity is 30.603847642507148
At time: 230.48733282089233 and batch: 600, loss is 3.4263784646987916 and perplexity is 30.765024135346955
At time: 231.09031581878662 and batch: 650, loss is 3.280856547355652 and perplexity is 26.598545859456916
At time: 231.69434595108032 and batch: 700, loss is 3.275384950637817 and perplexity is 26.45340677659995
At time: 232.297687292099 and batch: 750, loss is 3.3731678199768065 and perplexity is 29.170788653615645
At time: 232.90071415901184 and batch: 800, loss is 3.329646553993225 and perplexity is 27.928468752805813
At time: 233.5042655467987 and batch: 850, loss is 3.3870312070846555 and perplexity is 29.578010806030807
At time: 234.10727071762085 and batch: 900, loss is 3.3563414096832274 and perplexity is 28.684055465240217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330341077830694 and perplexity of 75.9701938996608
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f39b9e46b70>
ELAPSED
488.80402302742004


RESULTS SO FAR:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.9701938996608, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5835303149491065, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7493984943579154, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8098812103271484 and batch: 50, loss is 6.84702389717102 and perplexity is 941.0759959566715
At time: 1.4234638214111328 and batch: 100, loss is 6.0195973682403565 and perplexity is 411.4129144689431
At time: 2.025110960006714 and batch: 150, loss is 5.82101146697998 and perplexity is 337.31306216585836
At time: 2.6273961067199707 and batch: 200, loss is 5.642015647888184 and perplexity is 282.03062036222246
At time: 3.2330851554870605 and batch: 250, loss is 5.6735241413116455 and perplexity is 291.0584602461638
At time: 3.8353421688079834 and batch: 300, loss is 5.565060663223266 and perplexity is 261.1410448867851
At time: 4.437333583831787 and batch: 350, loss is 5.5349382972717285 and perplexity is 253.39215216760712
At time: 5.039306402206421 and batch: 400, loss is 5.393708896636963 and perplexity is 220.01789779059766
At time: 5.641636371612549 and batch: 450, loss is 5.3861000347137455 and perplexity is 218.35016480675023
At time: 6.243362188339233 and batch: 500, loss is 5.3244126033782955 and perplexity is 205.2877397173542
At time: 6.845276832580566 and batch: 550, loss is 5.384291019439697 and perplexity is 217.95552308759596
At time: 7.449680328369141 and batch: 600, loss is 5.287677030563355 and perplexity is 197.88321442916512
At time: 8.05481743812561 and batch: 650, loss is 5.185752229690552 and perplexity is 178.7078285762218
At time: 8.659335851669312 and batch: 700, loss is 5.268961963653564 and perplexity is 194.21425631730514
At time: 9.264095067977905 and batch: 750, loss is 5.255057020187378 and perplexity is 191.5324067567404
At time: 9.870743036270142 and batch: 800, loss is 5.226570529937744 and perplexity is 186.15330039783754
At time: 10.475695848464966 and batch: 850, loss is 5.2499995803833 and perplexity is 190.56618849385814
At time: 11.089911699295044 and batch: 900, loss is 5.164572763442993 and perplexity is 174.96269214326318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.024875588613014 and perplexity of 152.15132557766006
finished 1 epochs...
Completing Train Step...
At time: 12.606860637664795 and batch: 50, loss is 4.956571207046509 and perplexity is 142.1057085478258
At time: 13.205469369888306 and batch: 100, loss is 4.81944709777832 and perplexity is 123.89656915034715
At time: 13.803910493850708 and batch: 150, loss is 4.800017461776734 and perplexity is 121.51253932504162
At time: 14.401554584503174 and batch: 200, loss is 4.6807561779022215 and perplexity is 107.85159673830188
At time: 14.998526573181152 and batch: 250, loss is 4.780370769500732 and perplexity is 119.1485184928657
At time: 15.595446109771729 and batch: 300, loss is 4.71186595916748 and perplexity is 111.25957215275488
At time: 16.196369171142578 and batch: 350, loss is 4.700551595687866 and perplexity is 110.0078355675595
At time: 16.793776035308838 and batch: 400, loss is 4.585219602584839 and perplexity is 98.02471125861693
At time: 17.389856100082397 and batch: 450, loss is 4.605937910079956 and perplexity is 100.07680188674361
At time: 17.985917329788208 and batch: 500, loss is 4.50219841003418 and perplexity is 90.21524355141128
At time: 18.582018613815308 and batch: 550, loss is 4.573131303787232 and perplexity is 96.84689251438364
At time: 19.17930006980896 and batch: 600, loss is 4.532464256286621 and perplexity is 92.98742382180362
At time: 19.77686333656311 and batch: 650, loss is 4.386362571716308 and perplexity is 80.34762806546988
At time: 20.374910354614258 and batch: 700, loss is 4.42950213432312 and perplexity is 83.88964073890115
At time: 20.972501039505005 and batch: 750, loss is 4.478358306884766 and perplexity is 88.08993725907041
At time: 21.572111129760742 and batch: 800, loss is 4.437014455795288 and perplexity is 84.52221978191797
At time: 22.170252323150635 and batch: 850, loss is 4.491077260971069 and perplexity is 89.21750466489006
At time: 22.769253492355347 and batch: 900, loss is 4.428560276031494 and perplexity is 83.81066578263656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.560564433058647 and perplexity of 95.63744553452398
finished 2 epochs...
Completing Train Step...
At time: 24.283830404281616 and batch: 50, loss is 4.466628980636597 and perplexity is 87.06273760229455
At time: 24.880186557769775 and batch: 100, loss is 4.335889630317688 and perplexity is 76.39289009954388
At time: 25.47699213027954 and batch: 150, loss is 4.338573274612426 and perplexity is 76.59817677822484
At time: 26.072644233703613 and batch: 200, loss is 4.223871126174926 and perplexity is 68.29736091154388
At time: 26.68810272216797 and batch: 250, loss is 4.366723852157593 and perplexity is 78.78509680662273
At time: 27.286404848098755 and batch: 300, loss is 4.324384765625 and perplexity is 75.51903665699557
At time: 27.884614944458008 and batch: 350, loss is 4.3237064170837405 and perplexity is 75.46782580001077
At time: 28.48309016227722 and batch: 400, loss is 4.241372766494751 and perplexity is 69.503198041295
At time: 29.08138108253479 and batch: 450, loss is 4.273480682373047 and perplexity is 71.77101346637818
At time: 29.67906165122986 and batch: 500, loss is 4.153940172195434 and perplexity is 63.68443423476221
At time: 30.27684760093689 and batch: 550, loss is 4.231832509040832 and perplexity is 68.84327257288014
At time: 30.87318229675293 and batch: 600, loss is 4.2240789270401 and perplexity is 68.31155463691351
At time: 31.481375217437744 and batch: 650, loss is 4.075801959037781 and perplexity is 58.89769519269185
At time: 32.08468794822693 and batch: 700, loss is 4.0901299619674685 and perplexity is 59.74765612253604
At time: 32.68439602851868 and batch: 750, loss is 4.17801221370697 and perplexity is 65.23604892129707
At time: 33.282644510269165 and batch: 800, loss is 4.151498341560364 and perplexity is 63.529117338222264
At time: 33.89522671699524 and batch: 850, loss is 4.207603521347046 and perplexity is 67.19531456391641
At time: 34.49898147583008 and batch: 900, loss is 4.158293261528015 and perplexity is 63.962262533058926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4312940623662245 and perplexity of 84.0400997042264
finished 3 epochs...
Completing Train Step...
At time: 36.009583473205566 and batch: 50, loss is 4.221521754264831 and perplexity is 68.13709334805758
At time: 36.62049198150635 and batch: 100, loss is 4.102441740036011 and perplexity is 60.48780291711226
At time: 37.219523429870605 and batch: 150, loss is 4.107841110229492 and perplexity is 60.81528225274518
At time: 37.817285776138306 and batch: 200, loss is 3.9917120838165285 and perplexity is 54.14751513352523
At time: 38.41531944274902 and batch: 250, loss is 4.145922904014587 and perplexity is 63.17590029693524
At time: 39.01243710517883 and batch: 300, loss is 4.109384980201721 and perplexity is 60.90924565584235
At time: 39.612202167510986 and batch: 350, loss is 4.10930296421051 and perplexity is 60.90425032853713
At time: 40.20799446105957 and batch: 400, loss is 4.039912285804749 and perplexity is 56.82135854713908
At time: 40.804020404815674 and batch: 450, loss is 4.077191739082337 and perplexity is 58.97960694061302
At time: 41.40056753158569 and batch: 500, loss is 3.9562036752700807 and perplexity is 52.25855843706484
At time: 42.00972390174866 and batch: 550, loss is 4.033802490234375 and perplexity is 56.47525006509405
At time: 42.606372594833374 and batch: 600, loss is 4.034469337463379 and perplexity is 56.512922988753544
At time: 43.20667052268982 and batch: 650, loss is 3.8917823028564453 and perplexity is 48.99813826791383
At time: 43.819860219955444 and batch: 700, loss is 3.899036293029785 and perplexity is 49.35486255440991
At time: 44.41787528991699 and batch: 750, loss is 3.993254904747009 and perplexity is 54.23111952992947
At time: 45.01521587371826 and batch: 800, loss is 3.9769204902648925 and perplexity is 53.35248150049886
At time: 45.61265540122986 and batch: 850, loss is 4.029989008903503 and perplexity is 56.26029288142159
At time: 46.218822956085205 and batch: 900, loss is 3.987331500053406 and perplexity is 53.91083618238248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380933369675728 and perplexity of 79.91258659186404
finished 4 epochs...
Completing Train Step...
At time: 47.747233629226685 and batch: 50, loss is 4.057336874008179 and perplexity is 57.82012358575526
At time: 48.357887506484985 and batch: 100, loss is 3.9465967988967896 and perplexity is 51.758920748178454
At time: 48.955857276916504 and batch: 150, loss is 3.948873243331909 and perplexity is 51.87688126962851
At time: 49.55448317527771 and batch: 200, loss is 3.8357174396514893 and perplexity is 46.326652325481014
At time: 50.154255867004395 and batch: 250, loss is 3.9938930463790894 and perplexity is 54.26573770953187
At time: 50.75218987464905 and batch: 300, loss is 3.959952878952026 and perplexity is 52.45485416316345
At time: 51.3492214679718 and batch: 350, loss is 3.956283392906189 and perplexity is 52.26272453186328
At time: 51.946205854415894 and batch: 400, loss is 3.893443078994751 and perplexity is 49.07958081697224
At time: 52.54400634765625 and batch: 450, loss is 3.9357772970199587 and perplexity is 51.201933603205234
At time: 53.142059326171875 and batch: 500, loss is 3.816932158470154 and perplexity is 45.46451622279728
At time: 53.739930152893066 and batch: 550, loss is 3.892821249961853 and perplexity is 49.04907119556506
At time: 54.33692121505737 and batch: 600, loss is 3.8965422534942626 and perplexity is 49.231922947774734
At time: 54.93987584114075 and batch: 650, loss is 3.756882863044739 and perplexity is 42.814758291450545
At time: 55.55400276184082 and batch: 700, loss is 3.7636606454849244 and perplexity is 43.105933053300284
At time: 56.15369725227356 and batch: 750, loss is 3.858419804573059 and perplexity is 47.39040606738861
At time: 56.763285875320435 and batch: 800, loss is 3.8474537467956544 and perplexity is 46.873559202068975
At time: 57.36105823516846 and batch: 850, loss is 3.898888339996338 and perplexity is 49.34756089294443
At time: 57.95985436439514 and batch: 900, loss is 3.8581914377212523 and perplexity is 47.37958490519357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368295747939855 and perplexity of 78.90903615234588
finished 5 epochs...
Completing Train Step...
At time: 59.480103731155396 and batch: 50, loss is 3.931967921257019 and perplexity is 51.007257231465516
At time: 60.0826952457428 and batch: 100, loss is 3.8244253158569337 and perplexity is 45.806468549694046
At time: 60.68658256530762 and batch: 150, loss is 3.828685040473938 and perplexity is 46.00200766711376
At time: 61.290507793426514 and batch: 200, loss is 3.7209107303619384 and perplexity is 41.301991963347696
At time: 61.891483545303345 and batch: 250, loss is 3.8753324222564696 and perplexity is 48.198717954886405
At time: 62.4926917552948 and batch: 300, loss is 3.8463983726501465 and perplexity is 46.82411615462535
At time: 63.09342646598816 and batch: 350, loss is 3.8383757638931275 and perplexity is 46.44996742152728
At time: 63.69491934776306 and batch: 400, loss is 3.7813559341430665 and perplexity is 43.875493699942616
At time: 64.29607200622559 and batch: 450, loss is 3.823203544616699 and perplexity is 45.750537698113725
At time: 64.89834928512573 and batch: 500, loss is 3.710418767929077 and perplexity is 40.870918373640706
At time: 65.4996383190155 and batch: 550, loss is 3.783705163002014 and perplexity is 43.978688442516855
At time: 66.10131430625916 and batch: 600, loss is 3.7894373893737794 and perplexity is 44.23150815785151
At time: 66.70231556892395 and batch: 650, loss is 3.653863549232483 and perplexity is 38.62360234160087
At time: 67.30354738235474 and batch: 700, loss is 3.6593979454040526 and perplexity is 38.83795326286923
At time: 67.90553426742554 and batch: 750, loss is 3.7542030715942385 and perplexity is 42.70017726342541
At time: 68.50657057762146 and batch: 800, loss is 3.749711203575134 and perplexity is 42.50880383662992
At time: 69.10923385620117 and batch: 850, loss is 3.793578667640686 and perplexity is 44.415062954710464
At time: 69.71134090423584 and batch: 900, loss is 3.75440176486969 and perplexity is 42.70866234444459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377093171420163 and perplexity of 79.6062949030508
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 71.23582172393799 and batch: 50, loss is 3.8615827989578246 and perplexity is 47.54053896526055
At time: 71.84022355079651 and batch: 100, loss is 3.751592755317688 and perplexity is 42.588861643396896
At time: 72.45791792869568 and batch: 150, loss is 3.76322114944458 and perplexity is 43.086992328899576
At time: 73.0625250339508 and batch: 200, loss is 3.6386204528808594 and perplexity is 38.03932347236357
At time: 73.66735982894897 and batch: 250, loss is 3.7798615980148313 and perplexity is 43.80997792804671
At time: 74.27297806739807 and batch: 300, loss is 3.742675838470459 and perplexity is 42.21078843427957
At time: 74.8896095752716 and batch: 350, loss is 3.727038221359253 and perplexity is 41.55584649869312
At time: 75.49990463256836 and batch: 400, loss is 3.6590684509277343 and perplexity is 38.82515847981853
At time: 76.11890840530396 and batch: 450, loss is 3.691401023864746 and perplexity is 40.10099006326619
At time: 76.73971009254456 and batch: 500, loss is 3.5681427335739135 and perplexity is 35.450690593846865
At time: 77.36072564125061 and batch: 550, loss is 3.6224597024917604 and perplexity is 37.42952017201364
At time: 77.969979763031 and batch: 600, loss is 3.625741443634033 and perplexity is 37.552555943651754
At time: 78.57505798339844 and batch: 650, loss is 3.4791586875915526 and perplexity is 32.43242479328799
At time: 79.17881965637207 and batch: 700, loss is 3.4650615787506105 and perplexity is 31.978428904200005
At time: 79.78226065635681 and batch: 750, loss is 3.5472567796707155 and perplexity is 34.71794775723857
At time: 80.40038990974426 and batch: 800, loss is 3.526076021194458 and perplexity is 33.990328259548946
At time: 81.00413799285889 and batch: 850, loss is 3.556219367980957 and perplexity is 35.03050901662599
At time: 81.60690689086914 and batch: 900, loss is 3.505678343772888 and perplexity is 33.30402777155822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331479007250642 and perplexity of 76.05669182327068
finished 7 epochs...
Completing Train Step...
At time: 83.10621690750122 and batch: 50, loss is 3.768289852142334 and perplexity is 43.305941909554
At time: 83.71547889709473 and batch: 100, loss is 3.6538114500045777 and perplexity is 38.62159013415764
At time: 84.31163334846497 and batch: 150, loss is 3.665992465019226 and perplexity is 39.09491725328183
At time: 84.90859150886536 and batch: 200, loss is 3.549267735481262 and perplexity is 34.78783426179603
At time: 85.50489926338196 and batch: 250, loss is 3.692227644920349 and perplexity is 40.13415209032961
At time: 86.10166883468628 and batch: 300, loss is 3.660959014892578 and perplexity is 38.898629354180095
At time: 86.69965195655823 and batch: 350, loss is 3.6474964809417725 and perplexity is 38.37846446082034
At time: 87.29732728004456 and batch: 400, loss is 3.585467133522034 and perplexity is 36.07020338511297
At time: 87.91304206848145 and batch: 450, loss is 3.622330513000488 and perplexity is 37.42468498367854
At time: 88.51062393188477 and batch: 500, loss is 3.505045461654663 and perplexity is 33.28295691630379
At time: 89.10933709144592 and batch: 550, loss is 3.559723000526428 and perplexity is 35.153458306897114
At time: 89.70688891410828 and batch: 600, loss is 3.5719003248214722 and perplexity is 35.58415038518213
At time: 90.30298972129822 and batch: 650, loss is 3.4306665897369384 and perplexity is 30.897231664262293
At time: 90.89941692352295 and batch: 700, loss is 3.4215960025787355 and perplexity is 30.618242840552547
At time: 91.50189018249512 and batch: 750, loss is 3.5096179246902466 and perplexity is 33.4354904677746
At time: 92.10061287879944 and batch: 800, loss is 3.493014054298401 and perplexity is 32.88491540492898
At time: 92.69892621040344 and batch: 850, loss is 3.5302351093292237 and perplexity is 34.13199142108962
At time: 93.29590797424316 and batch: 900, loss is 3.4882256269454954 and perplexity is 32.72782478517755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336927022019478 and perplexity of 76.47218057033622
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 94.7996096611023 and batch: 50, loss is 3.7450806951522826 and perplexity is 42.31242148837391
At time: 95.41073441505432 and batch: 100, loss is 3.640273022651672 and perplexity is 38.102238079506876
At time: 96.00791764259338 and batch: 150, loss is 3.656273193359375 and perplexity is 38.716783700012
At time: 96.60341501235962 and batch: 200, loss is 3.533880100250244 and perplexity is 34.25662923378932
At time: 97.19929480552673 and batch: 250, loss is 3.676556634902954 and perplexity is 39.51011183240942
At time: 97.79414105415344 and batch: 300, loss is 3.6391164875030517 and perplexity is 38.05819697437887
At time: 98.38905882835388 and batch: 350, loss is 3.622227749824524 and perplexity is 37.420839301790785
At time: 98.98450422286987 and batch: 400, loss is 3.558999733924866 and perplexity is 35.12804217700123
At time: 99.57862901687622 and batch: 450, loss is 3.5886253786087035 and perplexity is 36.18430200859786
At time: 100.17352628707886 and batch: 500, loss is 3.470020775794983 and perplexity is 32.1374101180613
At time: 100.77024149894714 and batch: 550, loss is 3.5157878971099854 and perplexity is 33.64242425320344
At time: 101.36694025993347 and batch: 600, loss is 3.5298516654968264 and perplexity is 34.118906228370314
At time: 101.96468830108643 and batch: 650, loss is 3.381075611114502 and perplexity is 29.40237963733285
At time: 102.5761787891388 and batch: 700, loss is 3.3710635137557983 and perplexity is 29.109468921954043
At time: 103.17383575439453 and batch: 750, loss is 3.4521875286102297 and perplexity is 31.569375742265244
At time: 103.77273416519165 and batch: 800, loss is 3.428895573616028 and perplexity is 30.842560594855648
At time: 104.370924949646 and batch: 850, loss is 3.465213656425476 and perplexity is 31.98329247912482
At time: 104.96866297721863 and batch: 900, loss is 3.4202192783355714 and perplexity is 30.576118966483552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3248182322880995 and perplexity of 75.5517787376033
finished 9 epochs...
Completing Train Step...
At time: 106.48609399795532 and batch: 50, loss is 3.7157767057418822 and perplexity is 41.09048991268588
At time: 107.08461022377014 and batch: 100, loss is 3.605717830657959 and perplexity is 36.80809635628489
At time: 107.6803560256958 and batch: 150, loss is 3.620070538520813 and perplexity is 37.34020165175071
At time: 108.27602934837341 and batch: 200, loss is 3.5018607425689696 and perplexity is 33.17712865427342
At time: 108.87270998954773 and batch: 250, loss is 3.644263300895691 and perplexity is 38.25458035305704
At time: 109.46852731704712 and batch: 300, loss is 3.6094181823730467 and perplexity is 36.94455156920434
At time: 110.06394481658936 and batch: 350, loss is 3.592860016822815 and perplexity is 36.337854326430566
At time: 110.66218066215515 and batch: 400, loss is 3.532737584114075 and perplexity is 34.21751283193413
At time: 111.26034879684448 and batch: 450, loss is 3.5655584144592285 and perplexity is 35.35919297696033
At time: 111.85879898071289 and batch: 500, loss is 3.4493248748779295 and perplexity is 31.479132779773764
At time: 112.45648193359375 and batch: 550, loss is 3.495999712944031 and perplexity is 32.983245253555886
At time: 113.05373001098633 and batch: 600, loss is 3.5131402444839477 and perplexity is 33.553468614064336
At time: 113.65092277526855 and batch: 650, loss is 3.36754714012146 and perplexity is 29.007288909764675
At time: 114.24892663955688 and batch: 700, loss is 3.3603511953353884 and perplexity is 28.799303284399898
At time: 114.84616780281067 and batch: 750, loss is 3.4444199275970457 and perplexity is 31.325107345161097
At time: 115.44300031661987 and batch: 800, loss is 3.4236082792282105 and perplexity is 30.67991724784208
At time: 116.04122686386108 and batch: 850, loss is 3.463782272338867 and perplexity is 31.937544852256266
At time: 116.63802313804626 and batch: 900, loss is 3.4214146184921264 and perplexity is 30.612689682183838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325928152424016 and perplexity of 75.63568173232662
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 118.15687966346741 and batch: 50, loss is 3.709616026878357 and perplexity is 40.83812277462947
At time: 118.75330686569214 and batch: 100, loss is 3.605292911529541 and perplexity is 36.79245921455805
At time: 119.35036301612854 and batch: 150, loss is 3.6224838590621946 and perplexity is 37.430424351774896
At time: 119.94822263717651 and batch: 200, loss is 3.5012565660476684 and perplexity is 33.15708986618852
At time: 120.5456383228302 and batch: 250, loss is 3.643819055557251 and perplexity is 38.23758970834799
At time: 121.14311408996582 and batch: 300, loss is 3.6088992977142333 and perplexity is 36.925386580807576
At time: 121.7636706829071 and batch: 350, loss is 3.5866576623916626 and perplexity is 36.11317157593704
At time: 122.36705589294434 and batch: 400, loss is 3.527650752067566 and perplexity is 34.04389604519959
At time: 122.97270822525024 and batch: 450, loss is 3.5571994495391848 and perplexity is 35.06485860243674
At time: 123.57781553268433 and batch: 500, loss is 3.440996298789978 and perplexity is 31.21804518040046
At time: 124.17561054229736 and batch: 550, loss is 3.4827626514434815 and perplexity is 32.54952095830389
At time: 124.77205038070679 and batch: 600, loss is 3.5065956687927247 and perplexity is 33.334592406203136
At time: 125.3695855140686 and batch: 650, loss is 3.352295227050781 and perplexity is 28.56822902367866
At time: 125.966801404953 and batch: 700, loss is 3.3424188470840455 and perplexity is 28.287467076887832
At time: 126.56450939178467 and batch: 750, loss is 3.426566653251648 and perplexity is 30.770814305522368
At time: 127.16289114952087 and batch: 800, loss is 3.402913007736206 and perplexity is 30.05151295807338
At time: 127.76066827774048 and batch: 850, loss is 3.4383657360076905 and perplexity is 31.136032070224047
At time: 128.35815286636353 and batch: 900, loss is 3.3994327545166017 and perplexity is 29.947107866805506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320666587516053 and perplexity of 75.23876480132081
finished 11 epochs...
Completing Train Step...
At time: 129.8734998703003 and batch: 50, loss is 3.7000127840042114 and perplexity is 40.44782144188185
At time: 130.4990839958191 and batch: 100, loss is 3.591806583404541 and perplexity is 36.29959497171425
At time: 131.09736275672913 and batch: 150, loss is 3.6071653890609743 and perplexity is 36.86141680838792
At time: 131.69473719596863 and batch: 200, loss is 3.4893758153915404 and perplexity is 32.7654896077792
At time: 132.29116129875183 and batch: 250, loss is 3.632528657913208 and perplexity is 37.80830010100597
At time: 132.90087151527405 and batch: 300, loss is 3.597903118133545 and perplexity is 36.5215726729568
At time: 133.49999594688416 and batch: 350, loss is 3.575883946418762 and perplexity is 35.72618689720194
At time: 134.0985825061798 and batch: 400, loss is 3.518984136581421 and perplexity is 33.7501255253557
At time: 134.69664669036865 and batch: 450, loss is 3.549127416610718 and perplexity is 34.78295321464326
At time: 135.29572916030884 and batch: 500, loss is 3.4342356300354004 and perplexity is 31.007702148723535
At time: 135.89445304870605 and batch: 550, loss is 3.4766665506362915 and perplexity is 32.351699380085066
At time: 136.49210047721863 and batch: 600, loss is 3.5015812635421755 and perplexity is 33.16785763823292
At time: 137.08997654914856 and batch: 650, loss is 3.348720827102661 and perplexity is 28.466297028723705
At time: 137.68731474876404 and batch: 700, loss is 3.3400772285461424 and perplexity is 28.221306111692588
At time: 138.2860324382782 and batch: 750, loss is 3.425434112548828 and perplexity is 30.735984832483215
At time: 138.88390254974365 and batch: 800, loss is 3.4031715202331543 and perplexity is 30.059282653963717
At time: 139.48233008384705 and batch: 850, loss is 3.4406115293502806 and perplexity is 31.20603574122871
At time: 140.08015322685242 and batch: 900, loss is 3.4028201961517333 and perplexity is 30.04872395896765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320273203392551 and perplexity of 75.20917288665241
finished 12 epochs...
Completing Train Step...
At time: 141.59372329711914 and batch: 50, loss is 3.6943333530426026 and perplexity is 40.218751940402
At time: 142.20789337158203 and batch: 100, loss is 3.5854513025283814 and perplexity is 36.06963236247208
At time: 142.80928134918213 and batch: 150, loss is 3.600304112434387 and perplexity is 36.609366114402874
At time: 143.4113893508911 and batch: 200, loss is 3.4829708242416384 and perplexity is 32.556297588491006
At time: 144.0127251148224 and batch: 250, loss is 3.6260660028457643 and perplexity is 37.56474594968969
At time: 144.61559224128723 and batch: 300, loss is 3.5917928075790404 and perplexity is 36.2990949182725
At time: 145.21739435195923 and batch: 350, loss is 3.569778561592102 and perplexity is 35.50872928450026
At time: 145.82860374450684 and batch: 400, loss is 3.5137133502960207 and perplexity is 33.57270381331803
At time: 146.43769121170044 and batch: 450, loss is 3.544107584953308 and perplexity is 34.60878615534909
At time: 147.03979992866516 and batch: 500, loss is 3.4298535108566286 and perplexity is 30.872119988006286
At time: 147.64218974113464 and batch: 550, loss is 3.472678017616272 and perplexity is 32.22292054888612
At time: 148.25918769836426 and batch: 600, loss is 3.4982965660095213 and perplexity is 33.05908999029092
At time: 148.86081981658936 and batch: 650, loss is 3.3460085868835447 and perplexity is 28.38919420099389
At time: 149.46228504180908 and batch: 700, loss is 3.338157639503479 and perplexity is 28.167184763703176
At time: 150.06328082084656 and batch: 750, loss is 3.424151678085327 and perplexity is 30.696593210250104
At time: 150.66480565071106 and batch: 800, loss is 3.402516450881958 and perplexity is 30.03959818722749
At time: 151.26610255241394 and batch: 850, loss is 3.440803632736206 and perplexity is 31.21203110220204
At time: 151.88420057296753 and batch: 900, loss is 3.4035064029693602 and perplexity is 30.069350674496302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320597191379495 and perplexity of 75.23354370288833
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 153.4427661895752 and batch: 50, loss is 3.6926984071731566 and perplexity is 40.153050182087284
At time: 154.04923558235168 and batch: 100, loss is 3.585155935287476 and perplexity is 36.05898014791575
At time: 154.65423369407654 and batch: 150, loss is 3.601782789230347 and perplexity is 36.66353957724148
At time: 155.25932383537292 and batch: 200, loss is 3.483404679298401 and perplexity is 32.57042536731177
At time: 155.86524748802185 and batch: 250, loss is 3.6266345024108886 and perplexity is 37.58610756288467
At time: 156.4677836894989 and batch: 300, loss is 3.5921545934677126 and perplexity is 36.31222979444866
At time: 157.0720579624176 and batch: 350, loss is 3.5697036170959473 and perplexity is 35.50606820039299
At time: 157.67579746246338 and batch: 400, loss is 3.5126429843902587 and perplexity is 33.53678796077304
At time: 158.2875850200653 and batch: 450, loss is 3.5417770624160765 and perplexity is 34.52822351221912
At time: 158.9013500213623 and batch: 500, loss is 3.428067765235901 and perplexity is 30.81703942950533
At time: 159.50856471061707 and batch: 550, loss is 3.4689059400558473 and perplexity is 32.10160214840927
At time: 160.11191177368164 and batch: 600, loss is 3.49561797618866 and perplexity is 32.97065673943332
At time: 160.71505737304688 and batch: 650, loss is 3.3403027391433717 and perplexity is 28.227671032940048
At time: 161.31785941123962 and batch: 700, loss is 3.33095618724823 and perplexity is 27.965068765311432
At time: 161.9207684993744 and batch: 750, loss is 3.4173487520217893 and perplexity is 30.488475264374927
At time: 162.52448201179504 and batch: 800, loss is 3.3941278219223023 and perplexity is 29.788661124409924
At time: 163.12868857383728 and batch: 850, loss is 3.4313352966308592 and perplexity is 30.917899765760055
At time: 163.74744868278503 and batch: 900, loss is 3.394117622375488 and perplexity is 29.788357295115716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320102221345248 and perplexity of 75.19631456759849
finished 14 epochs...
Completing Train Step...
At time: 165.2703025341034 and batch: 50, loss is 3.689905676841736 and perplexity is 40.04106997898057
At time: 165.86789393424988 and batch: 100, loss is 3.5819817066192625 and perplexity is 35.9447021675572
At time: 166.46569347381592 and batch: 150, loss is 3.5979377317428587 and perplexity is 36.52283683828336
At time: 167.0635244846344 and batch: 200, loss is 3.480279817581177 and perplexity is 32.46880614766297
At time: 167.66131234169006 and batch: 250, loss is 3.6232260704040526 and perplexity is 37.45821594960308
At time: 168.25963759422302 and batch: 300, loss is 3.5886490678787233 and perplexity is 36.18515919845168
At time: 168.85767650604248 and batch: 350, loss is 3.566373515129089 and perplexity is 35.388026028164894
At time: 169.45561861991882 and batch: 400, loss is 3.510065712928772 and perplexity is 33.450465839802895
At time: 170.05346298217773 and batch: 450, loss is 3.5395361852645872 and perplexity is 34.45093663261192
At time: 170.6506745815277 and batch: 500, loss is 3.426304512023926 and perplexity is 30.762749063644694
At time: 171.24954628944397 and batch: 550, loss is 3.467036347389221 and perplexity is 32.04164129709509
At time: 171.84622979164124 and batch: 600, loss is 3.49464714050293 and perplexity is 32.938663182045225
At time: 172.44326996803284 and batch: 650, loss is 3.3397222805023192 and perplexity is 28.21129079185835
At time: 173.03885221481323 and batch: 700, loss is 3.3305557918548585 and perplexity is 27.95387392193391
At time: 173.63516306877136 and batch: 750, loss is 3.4171365690231323 and perplexity is 30.48200681454066
At time: 174.2307267189026 and batch: 800, loss is 3.394596190452576 and perplexity is 29.802616463705345
At time: 174.82560396194458 and batch: 850, loss is 3.432415828704834 and perplexity is 30.951325603709698
At time: 175.4221158027649 and batch: 900, loss is 3.3959495401382447 and perplexity is 29.842977130216042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319816275818707 and perplexity of 75.17481559174996
finished 15 epochs...
Completing Train Step...
At time: 176.92850518226624 and batch: 50, loss is 3.688107757568359 and perplexity is 39.96914404542496
At time: 177.53894567489624 and batch: 100, loss is 3.580004873275757 and perplexity is 35.87371566914566
At time: 178.13791465759277 and batch: 150, loss is 3.595668911933899 and perplexity is 36.44006703297729
At time: 178.74825716018677 and batch: 200, loss is 3.4783221769332884 and perplexity is 32.40530606840762
At time: 179.34646797180176 and batch: 250, loss is 3.621134238243103 and perplexity is 37.37994154578806
At time: 179.94439601898193 and batch: 300, loss is 3.5865813207626345 and perplexity is 36.11041474282146
At time: 180.54308938980103 and batch: 350, loss is 3.564392123222351 and perplexity is 35.31797789911216
At time: 181.1416289806366 and batch: 400, loss is 3.508405442237854 and perplexity is 33.3949750893515
At time: 181.73883509635925 and batch: 450, loss is 3.538030228614807 and perplexity is 34.39909406162897
At time: 182.3371503353119 and batch: 500, loss is 3.425102400779724 and perplexity is 30.72579103537398
At time: 182.94024229049683 and batch: 550, loss is 3.4658768701553346 and perplexity is 32.00451127334313
At time: 183.54344511032104 and batch: 600, loss is 3.4938292407989504 and perplexity is 32.911733673498915
At time: 184.15150690078735 and batch: 650, loss is 3.3391761493682863 and perplexity is 28.195887933998613
At time: 184.7576196193695 and batch: 700, loss is 3.3302100801467898 and perplexity is 27.944211610716508
At time: 185.36243557929993 and batch: 750, loss is 3.4169661664962767 and perplexity is 30.476813046084036
At time: 185.96507835388184 and batch: 800, loss is 3.3947813940048217 and perplexity is 29.808136525292372
At time: 186.57380938529968 and batch: 850, loss is 3.4329507112503053 and perplexity is 30.967885355900716
At time: 187.18338823318481 and batch: 900, loss is 3.3967881631851196 and perplexity is 29.868014635671976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319742699191995 and perplexity of 75.1692846858802
finished 16 epochs...
Completing Train Step...
At time: 188.68890810012817 and batch: 50, loss is 3.686593623161316 and perplexity is 39.90867118278009
At time: 189.3003375530243 and batch: 100, loss is 3.578355097770691 and perplexity is 35.814580884730965
At time: 189.90108180046082 and batch: 150, loss is 3.5938790369033815 and perplexity is 36.37490220273296
At time: 190.50119876861572 and batch: 200, loss is 3.4767006921768187 and perplexity is 32.35280393579608
At time: 191.10157656669617 and batch: 250, loss is 3.6194563055038453 and perplexity is 37.31727310949069
At time: 191.70345950126648 and batch: 300, loss is 3.584957728385925 and perplexity is 36.05183371744687
At time: 192.3093822002411 and batch: 350, loss is 3.562806038856506 and perplexity is 35.262005007131634
At time: 192.93117666244507 and batch: 400, loss is 3.507047429084778 and perplexity is 33.34965505350083
At time: 193.53109693527222 and batch: 450, loss is 3.5367760038375855 and perplexity is 34.35597691049477
At time: 194.14382576942444 and batch: 500, loss is 3.424061822891235 and perplexity is 30.693835085826993
At time: 194.74467825889587 and batch: 550, loss is 3.4649108457565307 and perplexity is 31.973609063128972
At time: 195.34558415412903 and batch: 600, loss is 3.4930793952941896 and perplexity is 32.887064208249676
At time: 195.94544005393982 and batch: 650, loss is 3.3386078214645387 and perplexity is 28.179867976840313
At time: 196.54691076278687 and batch: 700, loss is 3.3298243570327757 and perplexity is 27.933434960930427
At time: 197.14710783958435 and batch: 750, loss is 3.416731495857239 and perplexity is 30.469661872008786
At time: 197.74800372123718 and batch: 800, loss is 3.3947816467285157 and perplexity is 29.808144058515698
At time: 198.349946975708 and batch: 850, loss is 3.4331993770599367 and perplexity is 30.975586967709397
At time: 198.9522578716278 and batch: 900, loss is 3.3971989583969116 and perplexity is 29.880286793569315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319771544574058 and perplexity of 75.17145300388923
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 200.49257397651672 and batch: 50, loss is 3.6860573720932006 and perplexity is 39.88727585237835
At time: 201.09306979179382 and batch: 100, loss is 3.578094034194946 and perplexity is 35.805232222532084
At time: 201.70253491401672 and batch: 150, loss is 3.594291214942932 and perplexity is 36.389898228915754
At time: 202.3086977005005 and batch: 200, loss is 3.476898226737976 and perplexity is 32.359195363966954
At time: 202.9073839187622 and batch: 250, loss is 3.6195859813690188 and perplexity is 37.3221125729411
At time: 203.50765347480774 and batch: 300, loss is 3.5846958494186403 and perplexity is 36.04239373658439
At time: 204.10830330848694 and batch: 350, loss is 3.5626902294158938 and perplexity is 35.257921570511705
At time: 204.70874309539795 and batch: 400, loss is 3.5064895725250245 and perplexity is 33.33105591797095
At time: 205.30870461463928 and batch: 450, loss is 3.5361095476150513 and perplexity is 34.33308778404911
At time: 205.90861248970032 and batch: 500, loss is 3.4238340997695924 and perplexity is 30.686846185683343
At time: 206.51184177398682 and batch: 550, loss is 3.463622817993164 and perplexity is 31.93245267793393
At time: 207.11425304412842 and batch: 600, loss is 3.491350712776184 and perplexity is 32.83026202589952
At time: 207.7136619091034 and batch: 650, loss is 3.336334648132324 and perplexity is 28.115883004469012
At time: 208.31529569625854 and batch: 700, loss is 3.327586359977722 and perplexity is 27.870989917725407
At time: 208.91556859016418 and batch: 750, loss is 3.4144823789596557 and perplexity is 30.401209048685487
At time: 209.52596402168274 and batch: 800, loss is 3.3919491720199586 and perplexity is 29.723832705616836
At time: 210.1244080066681 and batch: 850, loss is 3.4301637172698975 and perplexity is 30.88169820315285
At time: 210.9178810119629 and batch: 900, loss is 3.393782253265381 and perplexity is 29.77836887523546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319361856538955 and perplexity of 75.14066246670023
finished 18 epochs...
Completing Train Step...
At time: 212.49553894996643 and batch: 50, loss is 3.6853655338287354 and perplexity is 39.85968985230392
At time: 213.09694981575012 and batch: 100, loss is 3.5773806762695313 and perplexity is 35.77969938446378
At time: 213.69924187660217 and batch: 150, loss is 3.5934322929382323 and perplexity is 36.35865556400511
At time: 214.31067442893982 and batch: 200, loss is 3.4761805868148805 and perplexity is 32.335981444108626
At time: 214.9208106994629 and batch: 250, loss is 3.618924283981323 and perplexity is 37.29742479736739
At time: 215.52142596244812 and batch: 300, loss is 3.5840070152282717 and perplexity is 36.017575052436364
At time: 216.12281394004822 and batch: 350, loss is 3.5620518350601196 and perplexity is 35.235420295492446
At time: 216.73650527000427 and batch: 400, loss is 3.50602424621582 and perplexity is 33.315549708742104
At time: 217.33562064170837 and batch: 450, loss is 3.535615267753601 and perplexity is 34.316121823487784
At time: 217.9389932155609 and batch: 500, loss is 3.423402466773987 and perplexity is 30.67360358851507
At time: 218.54349756240845 and batch: 550, loss is 3.4632766342163084 and perplexity is 31.921400094084618
At time: 219.14570593833923 and batch: 600, loss is 3.4912786197662355 and perplexity is 32.827895278806665
At time: 219.74857640266418 and batch: 650, loss is 3.3363407325744627 and perplexity is 28.11605407445276
At time: 220.34958696365356 and batch: 700, loss is 3.327538461685181 and perplexity is 27.869654976867864
At time: 220.95161890983582 and batch: 750, loss is 3.4144453048706054 and perplexity is 30.40008197244677
At time: 221.5528826713562 and batch: 800, loss is 3.392049984931946 and perplexity is 29.72682940279766
At time: 222.15632677078247 and batch: 850, loss is 3.430485625267029 and perplexity is 30.891640868995403
At time: 222.77092218399048 and batch: 900, loss is 3.3941797733306887 and perplexity is 29.79020872750901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319153668129281 and perplexity of 75.12502067995527
finished 19 epochs...
Completing Train Step...
At time: 224.3337206840515 and batch: 50, loss is 3.6848328590393065 and perplexity is 39.83846325434385
At time: 224.9568169116974 and batch: 100, loss is 3.5768200826644896 and perplexity is 35.759647134905116
At time: 225.56281661987305 and batch: 150, loss is 3.5927788066864013 and perplexity is 36.33490344414613
At time: 226.1696331501007 and batch: 200, loss is 3.475619649887085 and perplexity is 32.31784808433115
At time: 226.77636623382568 and batch: 250, loss is 3.618370590209961 and perplexity is 37.276779161776616
At time: 227.39602327346802 and batch: 300, loss is 3.583459401130676 and perplexity is 35.99785672008759
At time: 228.00672507286072 and batch: 350, loss is 3.5615133476257324 and perplexity is 35.21645157208593
At time: 228.6146605014801 and batch: 400, loss is 3.505618348121643 and perplexity is 33.302029734660344
At time: 229.22341966629028 and batch: 450, loss is 3.5352187585830688 and perplexity is 34.302517863713305
At time: 229.83151078224182 and batch: 500, loss is 3.423061423301697 and perplexity is 30.663144339870215
At time: 230.44013237953186 and batch: 550, loss is 3.4629906368255616 and perplexity is 31.91227196232188
At time: 231.04658150672913 and batch: 600, loss is 3.4911763763427732 and perplexity is 32.824539013989224
At time: 231.6567828655243 and batch: 650, loss is 3.3362917613983156 and perplexity is 28.114677231929193
At time: 232.2667019367218 and batch: 700, loss is 3.3274860048294066 and perplexity is 27.868193060740364
At time: 232.87469220161438 and batch: 750, loss is 3.4144139957427977 and perplexity is 30.39913018729479
At time: 233.48523616790771 and batch: 800, loss is 3.3921212482452394 and perplexity is 29.72894791063966
At time: 234.09300589561462 and batch: 850, loss is 3.430709595680237 and perplexity is 30.898560457428125
At time: 234.7025899887085 and batch: 900, loss is 3.394470477104187 and perplexity is 29.79887011248705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319039122699058 and perplexity of 75.11641594496695
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f39b9e46b70>
ELAPSED
729.8834505081177


RESULTS SO FAR:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.9701938996608, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.11641594496695, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5835303149491065, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7493984943579154, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.9121007976102171, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.8055971500675865, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8522682189941406 and batch: 50, loss is 7.247198429107666 and perplexity is 1404.1654634580505
At time: 1.4663758277893066 and batch: 100, loss is 6.470525970458985 and perplexity is 645.8233216377547
At time: 2.0803093910217285 and batch: 150, loss is 6.357082185745239 and perplexity is 576.5616000306271
At time: 2.693748712539673 and batch: 200, loss is 6.2034258460998535 and perplexity is 494.4400183339498
At time: 3.3204731941223145 and batch: 250, loss is 6.248761634826661 and perplexity is 517.3717326618653
At time: 3.9350035190582275 and batch: 300, loss is 6.148527030944824 and perplexity is 468.0274887999888
At time: 4.549170732498169 and batch: 350, loss is 6.152951564788818 and perplexity is 470.1028801981037
At time: 5.163911581039429 and batch: 400, loss is 6.039864645004273 and perplexity is 419.8362041129831
At time: 5.7787394523620605 and batch: 450, loss is 6.045218086242675 and perplexity is 422.0897994242841
At time: 6.393918991088867 and batch: 500, loss is 6.019078121185303 and perplexity is 411.19934497715747
At time: 7.008688688278198 and batch: 550, loss is 6.0570846366882325 and perplexity is 427.1283855141344
At time: 7.622596979141235 and batch: 600, loss is 5.99200475692749 and perplexity is 400.21614230364435
At time: 8.236922979354858 and batch: 650, loss is 5.920050077438354 and perplexity is 372.43036376779173
At time: 8.850741147994995 and batch: 700, loss is 6.02687406539917 and perplexity is 414.4175603457472
At time: 9.465311050415039 and batch: 750, loss is 5.977228498458862 and perplexity is 394.3459218958575
At time: 10.092081069946289 and batch: 800, loss is 5.987644109725952 and perplexity is 398.4747404816195
At time: 10.72780442237854 and batch: 850, loss is 6.025253438949585 and perplexity is 413.74648821178044
At time: 11.358186483383179 and batch: 900, loss is 5.903211402893066 and perplexity is 366.211634597821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.830332716850386 and perplexity of 340.4719409804788
finished 1 epochs...
Completing Train Step...
At time: 12.94230031967163 and batch: 50, loss is 5.576847906112671 and perplexity is 264.237390650847
At time: 13.544004440307617 and batch: 100, loss is 5.2866973781585695 and perplexity is 197.68945258740644
At time: 14.145006656646729 and batch: 150, loss is 5.197278051376343 and perplexity is 180.77949906041286
At time: 14.748063564300537 and batch: 200, loss is 5.0258948802948 and perplexity is 152.3064912242863
At time: 15.349364757537842 and batch: 250, loss is 5.068384962081909 and perplexity is 158.91746224831797
At time: 15.953209400177002 and batch: 300, loss is 4.977897357940674 and perplexity is 145.1688224535792
At time: 16.5571129322052 and batch: 350, loss is 4.955999774932861 and perplexity is 142.02452797922987
At time: 17.16595458984375 and batch: 400, loss is 4.805018787384033 and perplexity is 122.12178534850995
At time: 17.776578903198242 and batch: 450, loss is 4.808313474655152 and perplexity is 122.52480198255022
At time: 18.37797260284424 and batch: 500, loss is 4.718779678344727 and perplexity is 112.0314548057177
At time: 18.99255108833313 and batch: 550, loss is 4.776033983230591 and perplexity is 118.63291567148552
At time: 19.595155954360962 and batch: 600, loss is 4.708390684127807 and perplexity is 110.87358563186903
At time: 20.19756579399109 and batch: 650, loss is 4.567684049606323 and perplexity is 96.32077711730756
At time: 20.800092458724976 and batch: 700, loss is 4.622690629959107 and perplexity is 101.7674827252763
At time: 21.40175485610962 and batch: 750, loss is 4.644695739746094 and perplexity is 104.0317082556274
At time: 22.003864288330078 and batch: 800, loss is 4.591935606002807 and perplexity is 98.68526119948162
At time: 22.6074640750885 and batch: 850, loss is 4.642529401779175 and perplexity is 103.80658435154268
At time: 23.209079265594482 and batch: 900, loss is 4.5732575702667235 and perplexity is 96.85912180260948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.666857575717038 and perplexity of 106.36297911535972
finished 2 epochs...
Completing Train Step...
At time: 24.745604753494263 and batch: 50, loss is 4.620910940170288 and perplexity is 101.58652924372909
At time: 25.362324237823486 and batch: 100, loss is 4.485681910514831 and perplexity is 88.73744118030788
At time: 25.96489405632019 and batch: 150, loss is 4.487180509567261 and perplexity is 88.87052271860313
At time: 26.567893505096436 and batch: 200, loss is 4.365820341110229 and perplexity is 78.71394574901387
At time: 27.169893741607666 and batch: 250, loss is 4.497800350189209 and perplexity is 89.81934274721492
At time: 27.770470142364502 and batch: 300, loss is 4.452173566818237 and perplexity is 85.81326230071303
At time: 28.373023748397827 and batch: 350, loss is 4.448160600662232 and perplexity is 85.4695866240032
At time: 28.97460436820984 and batch: 400, loss is 4.351038227081299 and perplexity is 77.55894493575929
At time: 29.58787226676941 and batch: 450, loss is 4.386493492126465 and perplexity is 80.35814789850672
At time: 30.20099639892578 and batch: 500, loss is 4.269036989212037 and perplexity is 71.45279266611874
At time: 30.807308435440063 and batch: 550, loss is 4.338978939056396 and perplexity is 76.62925623850646
At time: 31.40884304046631 and batch: 600, loss is 4.326032390594483 and perplexity is 75.64356626834007
At time: 32.01067233085632 and batch: 650, loss is 4.177504062652588 and perplexity is 65.20290757537205
At time: 32.611581563949585 and batch: 700, loss is 4.199290509223938 and perplexity is 66.6390344843815
At time: 33.21358585357666 and batch: 750, loss is 4.280002655982972 and perplexity is 72.24063187695869
At time: 33.81549596786499 and batch: 800, loss is 4.244246253967285 and perplexity is 69.70320182673555
At time: 34.42752385139465 and batch: 850, loss is 4.306880331039428 and perplexity is 74.2086211210591
At time: 35.027036905288696 and batch: 900, loss is 4.253008193969727 and perplexity is 70.31662054369406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.475861013752141 and perplexity of 87.87022532044335
finished 3 epochs...
Completing Train Step...
At time: 36.53898906707764 and batch: 50, loss is 4.328573093414307 and perplexity is 75.83599844342933
At time: 37.152822494506836 and batch: 100, loss is 4.202861247062683 and perplexity is 66.87741034192182
At time: 37.754746198654175 and batch: 150, loss is 4.210430417060852 and perplexity is 67.3855374541035
At time: 38.35817241668701 and batch: 200, loss is 4.089868674278259 and perplexity is 59.73204683487827
At time: 38.96082544326782 and batch: 250, loss is 4.238886833190918 and perplexity is 69.33063230888125
At time: 39.56273555755615 and batch: 300, loss is 4.200079064369202 and perplexity is 66.69160376206273
At time: 40.16305112838745 and batch: 350, loss is 4.199855136871338 and perplexity is 66.67667135005496
At time: 40.763431549072266 and batch: 400, loss is 4.122500438690185 and perplexity is 61.71335997617181
At time: 41.36453366279602 and batch: 450, loss is 4.162943859100341 and perplexity is 64.26041803917529
At time: 41.96558141708374 and batch: 500, loss is 4.04341826915741 and perplexity is 57.02092291453683
At time: 42.5668830871582 and batch: 550, loss is 4.109882984161377 and perplexity is 60.93958625560097
At time: 43.1699378490448 and batch: 600, loss is 4.111735072135925 and perplexity is 61.05255631342893
At time: 43.77202272415161 and batch: 650, loss is 3.9641816139221193 and perplexity is 52.6771415051939
At time: 44.37447166442871 and batch: 700, loss is 3.9712207746505737 and perplexity is 53.04925250904659
At time: 44.9761438369751 and batch: 750, loss is 4.070259304046631 and perplexity is 58.57214861806477
At time: 45.57788157463074 and batch: 800, loss is 4.0441176271438595 and perplexity is 57.060814900135256
At time: 46.17919182777405 and batch: 850, loss is 4.101734304428101 and perplexity is 60.44502682393419
At time: 46.777591705322266 and batch: 900, loss is 4.057545509338379 and perplexity is 57.83218816483672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.406734100759846 and perplexity of 82.00121799568434
finished 4 epochs...
Completing Train Step...
At time: 48.3162624835968 and batch: 50, loss is 4.138067345619202 and perplexity is 62.68156251475023
At time: 48.91703462600708 and batch: 100, loss is 4.021306509971619 and perplexity is 55.773927437710235
At time: 49.53093457221985 and batch: 150, loss is 4.028850679397583 and perplexity is 56.196286567072626
At time: 50.1328239440918 and batch: 200, loss is 3.9173111963272094 and perplexity is 50.26510990302595
At time: 50.73447108268738 and batch: 250, loss is 4.062986068725586 and perplexity is 58.1476850806761
At time: 51.33572745323181 and batch: 300, loss is 4.029406719207763 and perplexity is 56.22754262859017
At time: 51.937055349349976 and batch: 350, loss is 4.031113262176514 and perplexity is 56.32357926813188
At time: 52.538689374923706 and batch: 400, loss is 3.9595929384231567 and perplexity is 52.43597693275815
At time: 53.140122175216675 and batch: 450, loss is 4.004018268585205 and perplexity is 54.81798143925743
At time: 53.7410683631897 and batch: 500, loss is 3.8877854681015016 and perplexity is 48.802691650057746
At time: 54.34055018424988 and batch: 550, loss is 3.9516170120239256 and perplexity is 52.01941488250862
At time: 54.94187307357788 and batch: 600, loss is 3.9587714290618896 and perplexity is 52.39291797593039
At time: 55.54267954826355 and batch: 650, loss is 3.8123957395553587 and perplexity is 45.2587372341956
At time: 56.14997577667236 and batch: 700, loss is 3.817050561904907 and perplexity is 45.46989969638204
At time: 56.75125694274902 and batch: 750, loss is 3.9191753005981447 and perplexity is 50.35889669609779
At time: 57.35340189933777 and batch: 800, loss is 3.8978289985656738 and perplexity is 49.29531265643521
At time: 57.95409631729126 and batch: 850, loss is 3.954937391281128 and perplexity is 52.192426141200734
At time: 58.55608367919922 and batch: 900, loss is 3.915821933746338 and perplexity is 50.1903076696329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379288764849101 and perplexity of 79.78126997781341
finished 5 epochs...
Completing Train Step...
At time: 60.0974006652832 and batch: 50, loss is 3.9968098545074464 and perplexity is 54.42425151917531
At time: 60.697866916656494 and batch: 100, loss is 3.8869110441207884 and perplexity is 48.76003605840964
At time: 61.29776358604431 and batch: 150, loss is 3.8907956314086913 and perplexity is 48.94981704639672
At time: 61.898972511291504 and batch: 200, loss is 3.7868430376052857 and perplexity is 44.1169047914419
At time: 62.50091314315796 and batch: 250, loss is 3.9291103887557983 and perplexity is 50.861710387585354
At time: 63.10317778587341 and batch: 300, loss is 3.901080513000488 and perplexity is 49.45585794332017
At time: 63.71082520484924 and batch: 350, loss is 3.9033292293548585 and perplexity is 49.56719527602032
At time: 64.32291531562805 and batch: 400, loss is 3.836327061653137 and perplexity is 46.3549026821459
At time: 64.94276142120361 and batch: 450, loss is 3.8785334157943727 and perplexity is 48.35324893397965
At time: 65.54530000686646 and batch: 500, loss is 3.7674252080917356 and perplexity is 43.26851386782495
At time: 66.1448061466217 and batch: 550, loss is 3.829253091812134 and perplexity is 46.028146592551664
At time: 66.74442338943481 and batch: 600, loss is 3.837585201263428 and perplexity is 46.41326032467436
At time: 67.3446717262268 and batch: 650, loss is 3.69550904750824 and perplexity is 40.26606471170499
At time: 67.94387674331665 and batch: 700, loss is 3.7017873907089234 and perplexity is 40.51966414442032
At time: 68.5445454120636 and batch: 750, loss is 3.803724856376648 and perplexity is 44.86800047531684
At time: 69.14510846138 and batch: 800, loss is 3.786697978973389 and perplexity is 44.11050571772098
At time: 69.74569010734558 and batch: 850, loss is 3.8386153030395507 and perplexity is 46.46109533980763
At time: 70.34634375572205 and batch: 900, loss is 3.804525098800659 and perplexity is 44.90392012307385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371390564800942 and perplexity of 79.15362344914818
finished 6 epochs...
Completing Train Step...
At time: 71.866868019104 and batch: 50, loss is 3.8863981199264526 and perplexity is 48.73503226927099
At time: 72.48405575752258 and batch: 100, loss is 3.780139856338501 and perplexity is 43.82217011527511
At time: 73.08844470977783 and batch: 150, loss is 3.785978693962097 and perplexity is 44.07878910012899
At time: 73.69235897064209 and batch: 200, loss is 3.685504789352417 and perplexity is 39.8652409207876
At time: 74.29685473442078 and batch: 250, loss is 3.8242657375335694 and perplexity is 45.79915941344901
At time: 74.90087008476257 and batch: 300, loss is 3.7966892194747923 and perplexity is 44.553433402904595
At time: 75.50375390052795 and batch: 350, loss is 3.80003212928772 and perplexity is 44.702620733591395
At time: 76.10772514343262 and batch: 400, loss is 3.735160231590271 and perplexity is 41.89473788580072
At time: 76.71297478675842 and batch: 450, loss is 3.7751948690414427 and perplexity is 43.6060049482167
At time: 77.3181619644165 and batch: 500, loss is 3.6702165031433105 and perplexity is 39.26040494130986
At time: 77.922287940979 and batch: 550, loss is 3.731210503578186 and perplexity is 41.72959142251547
At time: 78.52708172798157 and batch: 600, loss is 3.740279264450073 and perplexity is 42.10974827880469
At time: 79.13309693336487 and batch: 650, loss is 3.5999075508117677 and perplexity is 36.59485112300797
At time: 79.73886680603027 and batch: 700, loss is 3.6097937774658204 and perplexity is 36.95843036771961
At time: 80.35774660110474 and batch: 750, loss is 3.7118471384048464 and perplexity is 40.929338899907975
At time: 80.97421145439148 and batch: 800, loss is 3.6946287298202516 and perplexity is 40.23063338041557
At time: 81.58335709571838 and batch: 850, loss is 3.7447997331619263 and perplexity is 42.300534976123
At time: 82.18766355514526 and batch: 900, loss is 3.711970319747925 and perplexity is 40.934380941381335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377876177226027 and perplexity of 79.66865150374754
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 83.75011348724365 and batch: 50, loss is 3.8310317516326906 and perplexity is 46.110087858726764
At time: 84.38748240470886 and batch: 100, loss is 3.7236069107055663 and perplexity is 41.41349983735147
At time: 84.99776840209961 and batch: 150, loss is 3.7238117456436157 and perplexity is 41.421983637884786
At time: 85.60811185836792 and batch: 200, loss is 3.6109767150878906 and perplexity is 37.00217575438486
At time: 86.22976732254028 and batch: 250, loss is 3.741542572975159 and perplexity is 42.16297949944241
At time: 86.84491562843323 and batch: 300, loss is 3.70139853477478 and perplexity is 40.50391089563891
At time: 87.45521283149719 and batch: 350, loss is 3.6950961923599244 and perplexity is 40.2494440907769
At time: 88.06586980819702 and batch: 400, loss is 3.6235019731521607 and perplexity is 37.468552200157205
At time: 88.67568945884705 and batch: 450, loss is 3.6534801006317137 and perplexity is 38.60879501443219
At time: 89.28298282623291 and batch: 500, loss is 3.538965497016907 and perplexity is 34.43128149696501
At time: 89.90269470214844 and batch: 550, loss is 3.5833064889907837 and perplexity is 35.99235263161668
At time: 90.51375317573547 and batch: 600, loss is 3.5882749366760254 and perplexity is 36.17162373349873
At time: 91.12450695037842 and batch: 650, loss is 3.432476978302002 and perplexity is 30.953218322671095
At time: 91.73560905456543 and batch: 700, loss is 3.4225826692581176 and perplexity is 30.648467749047573
At time: 92.34655833244324 and batch: 750, loss is 3.5141820907592773 and perplexity is 33.58844438688993
At time: 92.95791530609131 and batch: 800, loss is 3.4814444255828856 and perplexity is 32.50664160657117
At time: 93.56745886802673 and batch: 850, loss is 3.508466753959656 and perplexity is 33.39702265554302
At time: 94.17746043205261 and batch: 900, loss is 3.470662498474121 and perplexity is 32.15804004161977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340417731298159 and perplexity of 76.73958917192525
finished 8 epochs...
Completing Train Step...
At time: 95.7246265411377 and batch: 50, loss is 3.7442917585372926 and perplexity is 42.27905283440007
At time: 96.32529950141907 and batch: 100, loss is 3.634394292831421 and perplexity is 37.87890242447449
At time: 96.92460250854492 and batch: 150, loss is 3.6313038301467895 and perplexity is 37.762019793729685
At time: 97.52374649047852 and batch: 200, loss is 3.522579312324524 and perplexity is 33.871681534650314
At time: 98.12133932113647 and batch: 250, loss is 3.654523253440857 and perplexity is 38.649090901138784
At time: 98.719731092453 and batch: 300, loss is 3.6204647779464723 and perplexity is 37.35492553358055
At time: 99.32184600830078 and batch: 350, loss is 3.6182371759414673 and perplexity is 37.27180623928973
At time: 99.92160868644714 and batch: 400, loss is 3.5529017496109008 and perplexity is 34.91448372663697
At time: 100.52134466171265 and batch: 450, loss is 3.5855326652526855 and perplexity is 36.07256720541753
At time: 101.12226128578186 and batch: 500, loss is 3.476336693763733 and perplexity is 32.341029709534425
At time: 101.72263050079346 and batch: 550, loss is 3.5243430805206297 and perplexity is 33.93147604561701
At time: 102.3239996433258 and batch: 600, loss is 3.5356005859375 and perplexity is 34.315618004196374
At time: 102.92519235610962 and batch: 650, loss is 3.3849351739883424 and perplexity is 29.51607924443361
At time: 103.5267345905304 and batch: 700, loss is 3.380162796974182 and perplexity is 29.375552975181837
At time: 104.1280472278595 and batch: 750, loss is 3.4781970453262328 and perplexity is 32.401251394071394
At time: 104.7298731803894 and batch: 800, loss is 3.451027960777283 and perplexity is 31.532790125499886
At time: 105.33171272277832 and batch: 850, loss is 3.4862181329727173 and perplexity is 32.66218977716326
At time: 105.93620419502258 and batch: 900, loss is 3.4541607332229614 and perplexity is 31.631730078801606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346458957619863 and perplexity of 77.20459358214008
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 107.46276879310608 and batch: 50, loss is 3.7230191040039062 and perplexity is 41.389163857734545
At time: 108.06304860115051 and batch: 100, loss is 3.6281947612762453 and perplexity is 37.644797394182454
At time: 108.66431093215942 and batch: 150, loss is 3.628988184928894 and perplexity is 37.67467751906557
At time: 109.26493167877197 and batch: 200, loss is 3.5044728565216063 and perplexity is 33.26390437964093
At time: 109.86622548103333 and batch: 250, loss is 3.6408785486221316 and perplexity is 38.1253169609225
At time: 110.46730899810791 and batch: 300, loss is 3.6067794370651245 and perplexity is 36.84719281606655
At time: 111.08505606651306 and batch: 350, loss is 3.5956284523010256 and perplexity is 36.438592711068715
At time: 111.68722367286682 and batch: 400, loss is 3.526977262496948 and perplexity is 34.02097555549171
At time: 112.28931140899658 and batch: 450, loss is 3.5564838314056395 and perplexity is 35.03977453014962
At time: 112.89118528366089 and batch: 500, loss is 3.4455065155029296 and perplexity is 31.359163327011117
At time: 113.49300646781921 and batch: 550, loss is 3.482559771537781 and perplexity is 32.542917984389284
At time: 114.09572696685791 and batch: 600, loss is 3.502520914077759 and perplexity is 33.199038480680244
At time: 114.69831371307373 and batch: 650, loss is 3.3440017223358156 and perplexity is 28.332278064204377
At time: 115.29827332496643 and batch: 700, loss is 3.334215831756592 and perplexity is 28.056373678071957
At time: 115.89794445037842 and batch: 750, loss is 3.421343297958374 and perplexity is 30.610506446671803
At time: 116.51196432113647 and batch: 800, loss is 3.388871636390686 and perplexity is 29.632497167701114
At time: 117.11365580558777 and batch: 850, loss is 3.426074061393738 and perplexity is 30.755660585540156
At time: 117.7123920917511 and batch: 900, loss is 3.395427985191345 and perplexity is 29.8274164360948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333810466609589 and perplexity of 76.23422178075637
finished 10 epochs...
Completing Train Step...
At time: 119.24625492095947 and batch: 50, loss is 3.69570698261261 and perplexity is 40.27403556825641
At time: 119.85908699035645 and batch: 100, loss is 3.592671413421631 and perplexity is 36.33100152976356
At time: 120.45819449424744 and batch: 150, loss is 3.5937853956222536 and perplexity is 36.37149616976493
At time: 121.05757308006287 and batch: 200, loss is 3.472750635147095 and perplexity is 32.22526058277499
At time: 121.6568353176117 and batch: 250, loss is 3.6096622228622435 and perplexity is 36.953568635862375
At time: 122.2561423778534 and batch: 300, loss is 3.578038787841797 and perplexity is 35.803254168668765
At time: 122.8550443649292 and batch: 350, loss is 3.5675001907348634 and perplexity is 35.42791932300836
At time: 123.45445680618286 and batch: 400, loss is 3.501772904396057 and perplexity is 33.174214563896
At time: 124.05422854423523 and batch: 450, loss is 3.5337119054794313 and perplexity is 34.250867932410486
At time: 124.65350794792175 and batch: 500, loss is 3.4251134729385377 and perplexity is 30.726131238095387
At time: 125.25372767448425 and batch: 550, loss is 3.4633974123001097 and perplexity is 31.925255732453856
At time: 125.85849523544312 and batch: 600, loss is 3.4866983890533447 and perplexity is 32.67787975971355
At time: 126.47196245193481 and batch: 650, loss is 3.3309000730514526 and perplexity is 27.963499571967272
At time: 127.07266044616699 and batch: 700, loss is 3.324072413444519 and perplexity is 27.773224620938944
At time: 127.6726610660553 and batch: 750, loss is 3.414052734375 and perplexity is 30.38815013939644
At time: 128.2722179889679 and batch: 800, loss is 3.3844062185287473 and perplexity is 29.500470681652605
At time: 128.87498116493225 and batch: 850, loss is 3.4253891277313233 and perplexity is 30.73460221091342
At time: 129.47682690620422 and batch: 900, loss is 3.3969334888458254 and perplexity is 29.872355540047632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335082589763484 and perplexity of 76.33126281038642
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 131.00307393074036 and batch: 50, loss is 3.69134560585022 and perplexity is 40.09876780759341
At time: 131.6191771030426 and batch: 100, loss is 3.59666796207428 and perplexity is 36.476490678556644
At time: 132.21900749206543 and batch: 150, loss is 3.6027764463424683 and perplexity is 36.69998867004908
At time: 132.82070803642273 and batch: 200, loss is 3.4751588773727415 and perplexity is 32.30296033840663
At time: 133.42203068733215 and batch: 250, loss is 3.6123260354995725 and perplexity is 37.05213724485608
At time: 134.0227451324463 and batch: 300, loss is 3.578747482299805 and perplexity is 35.82863672965454
At time: 134.62317562103271 and batch: 350, loss is 3.5669701957702635 and perplexity is 35.40914767903871
At time: 135.22406363487244 and batch: 400, loss is 3.4991306829452515 and perplexity is 33.086676640778386
At time: 135.824049949646 and batch: 450, loss is 3.5253464126586915 and perplexity is 33.96553767072961
At time: 136.42507648468018 and batch: 500, loss is 3.4161681747436523 and perplexity is 30.452502501724
At time: 137.0250964164734 and batch: 550, loss is 3.4541110944747926 and perplexity is 31.630159958287813
At time: 137.62531042099 and batch: 600, loss is 3.4764208936691285 and perplexity is 32.34375293582245
At time: 138.2268784046173 and batch: 650, loss is 3.3185589647293092 and perplexity is 27.620519723474004
At time: 138.82740759849548 and batch: 700, loss is 3.3102192163467405 and perplexity is 27.391129397119432
At time: 139.42791485786438 and batch: 750, loss is 3.3948230504989625 and perplexity is 29.80937825361973
At time: 140.0303180217743 and batch: 800, loss is 3.363963828086853 and perplexity is 28.90353274869346
At time: 140.63070368766785 and batch: 850, loss is 3.40155264377594 and perplexity is 30.010659756805286
At time: 141.24377942085266 and batch: 900, loss is 3.380850853919983 and perplexity is 29.395771983560046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326721609455266 and perplexity of 75.69571921124057
finished 12 epochs...
Completing Train Step...
At time: 142.77674293518066 and batch: 50, loss is 3.679021611213684 and perplexity is 39.60762345465579
At time: 143.37836456298828 and batch: 100, loss is 3.5794136142730713 and perplexity is 35.852511281056906
At time: 143.97989296913147 and batch: 150, loss is 3.5860773801803587 and perplexity is 36.092221823849854
At time: 144.58289623260498 and batch: 200, loss is 3.4622611379623414 and perplexity is 31.889000485492566
At time: 145.18516397476196 and batch: 250, loss is 3.6003865385055542 and perplexity is 36.61238380498608
At time: 145.78621888160706 and batch: 300, loss is 3.5681075286865234 and perplexity is 35.449442578244856
At time: 146.3880603313446 and batch: 350, loss is 3.556358857154846 and perplexity is 35.03539573420382
At time: 146.98919200897217 and batch: 400, loss is 3.4892995071411135 and perplexity is 32.76298942598631
At time: 147.58904838562012 and batch: 450, loss is 3.517480959892273 and perplexity is 33.69943123419403
At time: 148.1878056526184 and batch: 500, loss is 3.4092477703094484 and perplexity is 30.242486404716484
At time: 148.787691116333 and batch: 550, loss is 3.447404341697693 and perplexity is 31.418734078205127
At time: 149.3880820274353 and batch: 600, loss is 3.472194209098816 and perplexity is 32.20733459607964
At time: 150.0002248287201 and batch: 650, loss is 3.3149858617782595 and perplexity is 27.522004869610168
At time: 150.61464142799377 and batch: 700, loss is 3.3077727651596067 and perplexity is 27.32420023892977
At time: 151.2215428352356 and batch: 750, loss is 3.3936257410049437 and perplexity is 29.773708560118404
At time: 151.82492995262146 and batch: 800, loss is 3.3646400690078737 and perplexity is 28.923085110608696
At time: 152.42837405204773 and batch: 850, loss is 3.4041117572784425 and perplexity is 30.087558796124824
At time: 153.03214645385742 and batch: 900, loss is 3.3848319816589356 and perplexity is 29.513033568609313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32611083984375 and perplexity of 75.64950068209782
finished 13 epochs...
Completing Train Step...
At time: 154.63109183311462 and batch: 50, loss is 3.673194236755371 and perplexity is 39.37748620061501
At time: 155.23844718933105 and batch: 100, loss is 3.572755331993103 and perplexity is 35.614588099335535
At time: 155.84490203857422 and batch: 150, loss is 3.579236693382263 and perplexity is 35.84616878390001
At time: 156.45065116882324 and batch: 200, loss is 3.4560220193862916 and perplexity is 31.690660606594292
At time: 157.06891202926636 and batch: 250, loss is 3.5939922666549684 and perplexity is 36.379021157063065
At time: 157.67457032203674 and batch: 300, loss is 3.562201724052429 and perplexity is 35.2407020929659
At time: 158.28091502189636 and batch: 350, loss is 3.550397443771362 and perplexity is 34.82715657374504
At time: 158.8872516155243 and batch: 400, loss is 3.483946123123169 and perplexity is 32.58806519805474
At time: 159.49196791648865 and batch: 450, loss is 3.512602515220642 and perplexity is 33.53543078227479
At time: 160.09789562225342 and batch: 500, loss is 3.404816083908081 and perplexity is 30.108757729616087
At time: 160.7031707763672 and batch: 550, loss is 3.4433902978897093 and perplexity is 31.292870682813568
At time: 161.3091812133789 and batch: 600, loss is 3.4691667270660402 and perplexity is 32.109974920963666
At time: 161.91605877876282 and batch: 650, loss is 3.3123674058914183 and perplexity is 27.450033981360903
At time: 162.52568411827087 and batch: 700, loss is 3.3057918691635133 and perplexity is 27.27012741408272
At time: 163.13318538665771 and batch: 750, loss is 3.3923584270477294 and perplexity is 29.735999823153417
At time: 163.7394824028015 and batch: 800, loss is 3.3642434024810792 and perplexity is 28.91161456603463
At time: 164.34437680244446 and batch: 850, loss is 3.404425411224365 and perplexity is 30.096997357808082
At time: 164.95280504226685 and batch: 900, loss is 3.3855721616744994 and perplexity is 29.53488661285106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326490428349743 and perplexity of 75.67822181380173
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 166.48927855491638 and batch: 50, loss is 3.671906394958496 and perplexity is 39.32680686851551
At time: 167.11070203781128 and batch: 100, loss is 3.5736949634552 and perplexity is 35.64806841393845
At time: 167.71907472610474 and batch: 150, loss is 3.5816217041015626 and perplexity is 35.9317643132487
At time: 168.32552313804626 and batch: 200, loss is 3.458690938949585 and perplexity is 31.775353399513715
At time: 168.93361949920654 and batch: 250, loss is 3.597224655151367 and perplexity is 36.496802541606044
At time: 169.54184937477112 and batch: 300, loss is 3.562779006958008 and perplexity is 35.26105182107471
At time: 170.1506199836731 and batch: 350, loss is 3.5514254713058473 and perplexity is 34.86297825933542
At time: 170.7592649459839 and batch: 400, loss is 3.484214539527893 and perplexity is 32.5968135433993
At time: 171.36732149124146 and batch: 450, loss is 3.511383919715881 and perplexity is 33.49458954660041
At time: 171.97593903541565 and batch: 500, loss is 3.401984701156616 and perplexity is 30.02362888535428
At time: 172.59805345535278 and batch: 550, loss is 3.4399952411651613 and perplexity is 31.186809755079555
At time: 173.2062737941742 and batch: 600, loss is 3.464530143737793 and perplexity is 31.961438962350268
At time: 173.815500497818 and batch: 650, loss is 3.3067113828659056 and perplexity is 27.29521420196097
At time: 174.42516708374023 and batch: 700, loss is 3.301356792449951 and perplexity is 27.149450111369404
At time: 175.03334951400757 and batch: 750, loss is 3.3860686206817627 and perplexity is 29.5495531136955
At time: 175.64193677902222 and batch: 800, loss is 3.3567613554000855 and perplexity is 28.696103741108825
At time: 176.24957418441772 and batch: 850, loss is 3.394382438659668 and perplexity is 29.796246781792597
At time: 176.85793256759644 and batch: 900, loss is 3.3762031507492067 and perplexity is 29.259466160770426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324224184637201 and perplexity of 75.50691070911593
finished 15 epochs...
Completing Train Step...
At time: 178.38199663162231 and batch: 50, loss is 3.6684292125701905 and perplexity is 39.190297859187424
At time: 178.99522519111633 and batch: 100, loss is 3.5693812370300293 and perplexity is 35.49462359664123
At time: 179.59591698646545 and batch: 150, loss is 3.5777477073669433 and perplexity is 35.79283405706313
At time: 180.1967921257019 and batch: 200, loss is 3.45486741065979 and perplexity is 31.654091408929283
At time: 180.7983901500702 and batch: 250, loss is 3.5932933855056763 and perplexity is 36.35360542726841
At time: 181.41244220733643 and batch: 300, loss is 3.5592719411849973 and perplexity is 35.13760558667174
At time: 182.01594281196594 and batch: 350, loss is 3.54791775226593 and perplexity is 34.74090295481072
At time: 182.61577701568604 and batch: 400, loss is 3.48122549533844 and perplexity is 32.49952569855029
At time: 183.21718311309814 and batch: 450, loss is 3.509002718925476 and perplexity is 33.41492708729457
At time: 183.81690382957458 and batch: 500, loss is 3.4000694513320924 and perplexity is 29.966181166327658
At time: 184.41818189620972 and batch: 550, loss is 3.4381518888473512 and perplexity is 31.129374430065692
At time: 185.01952862739563 and batch: 600, loss is 3.463426127433777 and perplexity is 31.92617248360183
At time: 185.6206727027893 and batch: 650, loss is 3.3059993982315063 and perplexity is 27.275787345488848
At time: 186.22267079353333 and batch: 700, loss is 3.3008829593658446 and perplexity is 27.136588850977308
At time: 186.82469749450684 and batch: 750, loss is 3.3857146644592286 and perplexity is 29.539095716337417
At time: 187.439444065094 and batch: 800, loss is 3.3574353742599485 and perplexity is 28.71545197604005
At time: 188.04078269004822 and batch: 850, loss is 3.3956554555892944 and perplexity is 29.834202062116585
At time: 188.6440987586975 and batch: 900, loss is 3.3787121963500977 and perplexity is 29.332971671402913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323493853007277 and perplexity of 75.45178575614587
finished 16 epochs...
Completing Train Step...
At time: 190.17410445213318 and batch: 50, loss is 3.666397728919983 and perplexity is 39.11076422283312
At time: 190.77606105804443 and batch: 100, loss is 3.566930875778198 and perplexity is 35.407755419004936
At time: 191.37756991386414 and batch: 150, loss is 3.575377206802368 and perplexity is 35.70808760915887
At time: 191.9918441772461 and batch: 200, loss is 3.452681450843811 and perplexity is 31.584972410298104
At time: 192.60149216651917 and batch: 250, loss is 3.590972456932068 and perplexity is 36.269329143127536
At time: 193.20208859443665 and batch: 300, loss is 3.557178678512573 and perplexity is 35.06413027688964
At time: 193.80278301239014 and batch: 350, loss is 3.545836238861084 and perplexity is 34.66866450834343
At time: 194.40498566627502 and batch: 400, loss is 3.479436240196228 and perplexity is 32.441427746604084
At time: 195.0061309337616 and batch: 450, loss is 3.507461748123169 and perplexity is 33.36347531331395
At time: 195.60757756233215 and batch: 500, loss is 3.3987975454330446 and perplexity is 29.928091232271164
At time: 196.20892715454102 and batch: 550, loss is 3.4369817972183228 and perplexity is 31.092971511156833
At time: 196.80917286872864 and batch: 600, loss is 3.462702221870422 and perplexity is 31.90306931299011
At time: 197.41074585914612 and batch: 650, loss is 3.305499219894409 and perplexity is 27.262147998868574
At time: 198.02244544029236 and batch: 700, loss is 3.300616054534912 and perplexity is 27.129346930812677
At time: 198.62781238555908 and batch: 750, loss is 3.3855306053161622 and perplexity is 29.53365927602153
At time: 199.22844767570496 and batch: 800, loss is 3.357752113342285 and perplexity is 28.72454872251933
At time: 199.83900356292725 and batch: 850, loss is 3.3963012170791624 and perplexity is 29.853474062777458
At time: 200.44646072387695 and batch: 900, loss is 3.3798828744888305 and perplexity is 29.367331248161264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323246367990154 and perplexity of 75.43311488013235
finished 17 epochs...
Completing Train Step...
At time: 201.9777410030365 and batch: 50, loss is 3.6647780418395994 and perplexity is 39.047468296951635
At time: 202.5790309906006 and batch: 100, loss is 3.5650918865203858 and perplexity is 35.34270077287776
At time: 203.19561123847961 and batch: 150, loss is 3.5735365676879884 and perplexity is 35.64242235795989
At time: 203.8098635673523 and batch: 200, loss is 3.4509830904006957 and perplexity is 31.53137526907492
At time: 204.4150676727295 and batch: 250, loss is 3.58921865940094 and perplexity is 36.205775829332865
At time: 205.0166199207306 and batch: 300, loss is 3.555559182167053 and perplexity is 35.00739000378123
At time: 205.6186981201172 and batch: 350, loss is 3.544201135635376 and perplexity is 34.61202398234757
At time: 206.22104692459106 and batch: 400, loss is 3.4780208444595337 and perplexity is 32.39554276844079
At time: 206.82348561286926 and batch: 450, loss is 3.5062136125564574 and perplexity is 33.3218591498558
At time: 207.42795729637146 and batch: 500, loss is 3.3977266120910645 and perplexity is 29.89605739762113
At time: 208.038161277771 and batch: 550, loss is 3.436001014709473 and perplexity is 31.062491018360973
At time: 208.64602279663086 and batch: 600, loss is 3.4620403337478636 and perplexity is 31.88196003709883
At time: 209.2501471042633 and batch: 650, loss is 3.30499135017395 and perplexity is 27.248305894675596
At time: 209.85390782356262 and batch: 700, loss is 3.3002926397323606 and perplexity is 27.120574317104733
At time: 210.46731114387512 and batch: 750, loss is 3.3853084897994994 and perplexity is 29.527100120504567
At time: 211.07129049301147 and batch: 800, loss is 3.3578419733047484 and perplexity is 28.72713002536547
At time: 211.67572855949402 and batch: 850, loss is 3.3966113996505736 and perplexity is 29.86273552642585
At time: 212.277978181839 and batch: 900, loss is 3.38046133518219 and perplexity is 29.384324009305548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323203308941567 and perplexity of 75.42986687190222
finished 18 epochs...
Completing Train Step...
At time: 213.8757996559143 and batch: 50, loss is 3.6633475017547608 and perplexity is 38.99164926354299
At time: 214.49866199493408 and batch: 100, loss is 3.5635294914245605 and perplexity is 35.28752462521236
At time: 215.11144304275513 and batch: 150, loss is 3.5719528198242188 and perplexity is 35.58601842428526
At time: 215.7352900505066 and batch: 200, loss is 3.4495119714736937 and perplexity is 31.485022969354496
At time: 216.34631204605103 and batch: 250, loss is 3.587725820541382 and perplexity is 36.15176676368061
At time: 216.951313495636 and batch: 300, loss is 3.5541655921936037 and perplexity is 34.958638034093475
At time: 217.55589413642883 and batch: 350, loss is 3.5427830410003662 and perplexity is 34.56297564264472
At time: 218.1581437587738 and batch: 400, loss is 3.476782131195068 and perplexity is 32.355438823676124
At time: 218.7718324661255 and batch: 450, loss is 3.5050987911224367 and perplexity is 33.284731926011816
At time: 219.3738386631012 and batch: 500, loss is 3.3967437171936035 and perplexity is 29.866687151647767
At time: 219.9762167930603 and batch: 550, loss is 3.435102310180664 and perplexity is 31.034587557368134
At time: 220.57828879356384 and batch: 600, loss is 3.461388711929321 and perplexity is 31.86119182356616
At time: 221.18127632141113 and batch: 650, loss is 3.3044597339630126 and perplexity is 27.23382410326001
At time: 221.78472018241882 and batch: 700, loss is 3.2999129581451414 and perplexity is 27.110279088979674
At time: 222.388099193573 and batch: 750, loss is 3.385034646987915 and perplexity is 29.51901544340551
At time: 222.99127554893494 and batch: 800, loss is 3.357794680595398 and perplexity is 28.725771473679753
At time: 223.59399318695068 and batch: 850, loss is 3.3967300844192505 and perplexity is 29.866279988616544
At time: 224.200049161911 and batch: 900, loss is 3.380740909576416 and perplexity is 29.392540262363656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323265180195848 and perplexity of 75.43453395675354
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 225.7522373199463 and batch: 50, loss is 3.6629433631896973 and perplexity is 38.975894418144904
At time: 226.36802625656128 and batch: 100, loss is 3.563701877593994 and perplexity is 35.29360823076095
At time: 226.97111225128174 and batch: 150, loss is 3.572667818069458 and perplexity is 35.61147146336847
At time: 227.572124004364 and batch: 200, loss is 3.450320839881897 and perplexity is 31.510500512366697
At time: 228.1766152381897 and batch: 250, loss is 3.588829307556152 and perplexity is 36.191681787670355
At time: 228.7811987400055 and batch: 300, loss is 3.5544721841812135 and perplexity is 34.96935771561274
At time: 229.3856086730957 and batch: 350, loss is 3.5427173662185667 and perplexity is 34.56070580129753
At time: 229.98947525024414 and batch: 400, loss is 3.4767625904083252 and perplexity is 32.35480657912339
At time: 230.59419775009155 and batch: 450, loss is 3.505474395751953 and perplexity is 33.29723617359507
At time: 231.1973214149475 and batch: 500, loss is 3.3959876441955568 and perplexity is 29.844114290392046
At time: 231.80192136764526 and batch: 550, loss is 3.433640789985657 and perplexity is 30.989263010347653
At time: 232.40847373008728 and batch: 600, loss is 3.4595347166061403 and perplexity is 31.802176047325442
At time: 233.01134157180786 and batch: 650, loss is 3.3021903228759766 and perplexity is 27.172089438060134
At time: 233.6299171447754 and batch: 700, loss is 3.298147020339966 and perplexity is 27.062446269550364
At time: 234.23148703575134 and batch: 750, loss is 3.3832126665115356 and perplexity is 29.465281339694325
At time: 234.84071469306946 and batch: 800, loss is 3.354889459609985 and perplexity is 28.64243786945358
At time: 235.44237089157104 and batch: 850, loss is 3.3934760427474977 and perplexity is 29.76925182142058
At time: 236.04493761062622 and batch: 900, loss is 3.377050366401672 and perplexity is 29.284265742298064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322406089469178 and perplexity of 75.36975667692482
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f39b9e46b70>
ELAPSED
972.584657907486


RESULTS SO FAR:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.9701938996608, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.11641594496695, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5835303149491065, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7493984943579154, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.36975667692482, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.9121007976102171, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.8055971500675865, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5989062690656494, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.730432796799109, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.829843282699585 and batch: 50, loss is 6.876118602752686 and perplexity is 968.8585274735924
At time: 1.4531359672546387 and batch: 100, loss is 5.996919498443604 and perplexity is 402.1879426691856
At time: 2.064419984817505 and batch: 150, loss is 5.83136137008667 and perplexity is 340.8223487375711
At time: 2.676039695739746 and batch: 200, loss is 5.645065250396729 and perplexity is 282.89201443674517
At time: 3.287970781326294 and batch: 250, loss is 5.6778811264038085 and perplexity is 292.3293642628339
At time: 3.9002859592437744 and batch: 300, loss is 5.575666818618775 and perplexity is 263.92548740210054
At time: 4.512035608291626 and batch: 350, loss is 5.547375564575195 and perplexity is 256.56333765957237
At time: 5.1240622997283936 and batch: 400, loss is 5.403314323425293 and perplexity is 222.14144606642557
At time: 5.736162185668945 and batch: 450, loss is 5.397823677062989 and perplexity is 220.92508829466465
At time: 6.347905397415161 and batch: 500, loss is 5.345899152755737 and perplexity is 209.74639388491573
At time: 6.959900379180908 and batch: 550, loss is 5.392374439239502 and perplexity is 219.72448909356152
At time: 7.572407484054565 and batch: 600, loss is 5.302253656387329 and perplexity is 200.78880944064258
At time: 8.182489395141602 and batch: 650, loss is 5.200533046722412 and perplexity is 181.36889420736287
At time: 8.794200897216797 and batch: 700, loss is 5.292057495117188 and perplexity is 198.75193614900147
At time: 9.404851913452148 and batch: 750, loss is 5.268325023651123 and perplexity is 194.09059287568945
At time: 10.016987800598145 and batch: 800, loss is 5.246695442199707 and perplexity is 189.93757056610502
At time: 10.62802791595459 and batch: 850, loss is 5.277539319992066 and perplexity is 195.8872659425366
At time: 11.237413883209229 and batch: 900, loss is 5.179657125473023 and perplexity is 177.62189852685643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.039168266400899 and perplexity of 154.34159053922386
finished 1 epochs...
Completing Train Step...
At time: 12.778584957122803 and batch: 50, loss is 4.968926601409912 and perplexity is 143.8723720561772
At time: 13.386192798614502 and batch: 100, loss is 4.830494356155396 and perplexity is 125.27287476643808
At time: 14.006446838378906 and batch: 150, loss is 4.8077458572387695 and perplexity is 122.45527450537637
At time: 14.615286350250244 and batch: 200, loss is 4.692626066207886 and perplexity is 109.13941113123983
At time: 15.22542142868042 and batch: 250, loss is 4.790939264297485 and perplexity is 120.41441652633921
At time: 15.83534288406372 and batch: 300, loss is 4.720204849243164 and perplexity is 112.19123260310602
At time: 16.445614337921143 and batch: 350, loss is 4.718617515563965 and perplexity is 112.01328894642656
At time: 17.057300329208374 and batch: 400, loss is 4.590993242263794 and perplexity is 98.59230759268533
At time: 17.674001216888428 and batch: 450, loss is 4.611336708068848 and perplexity is 100.61855742197217
At time: 18.284900426864624 and batch: 500, loss is 4.510980529785156 and perplexity is 91.01101378386447
At time: 18.89560580253601 and batch: 550, loss is 4.58442494392395 and perplexity is 97.94684601507672
At time: 19.50564455986023 and batch: 600, loss is 4.539759016036987 and perplexity is 93.66822485941039
At time: 20.11693239212036 and batch: 650, loss is 4.394533739089966 and perplexity is 81.0068516275861
At time: 20.729425191879272 and batch: 700, loss is 4.434666042327881 and perplexity is 84.32395955230142
At time: 21.344788312911987 and batch: 750, loss is 4.490624656677246 and perplexity is 89.17713357594782
At time: 21.963390588760376 and batch: 800, loss is 4.443729543685913 and perplexity is 85.09170384428091
At time: 22.572295904159546 and batch: 850, loss is 4.50079303741455 and perplexity is 90.0885465673401
At time: 23.180600881576538 and batch: 900, loss is 4.435507802963257 and perplexity is 84.3949700247889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.554532403815283 and perplexity of 95.06229407543351
finished 2 epochs...
Completing Train Step...
At time: 24.715264320373535 and batch: 50, loss is 4.468818683624267 and perplexity is 87.25358801553965
At time: 25.314225673675537 and batch: 100, loss is 4.344432392120361 and perplexity is 77.04829184772856
At time: 25.91414785385132 and batch: 150, loss is 4.343748569488525 and perplexity is 76.99562249231981
At time: 26.51763129234314 and batch: 200, loss is 4.233719396591186 and perplexity is 68.9732947168289
At time: 27.125153064727783 and batch: 250, loss is 4.370388708114624 and perplexity is 79.0743625728984
At time: 27.726048707962036 and batch: 300, loss is 4.32912061214447 and perplexity is 75.8775314420075
At time: 28.328365802764893 and batch: 350, loss is 4.330613479614258 and perplexity is 75.99089113483072
At time: 28.937073230743408 and batch: 400, loss is 4.242698197364807 and perplexity is 69.59538080299023
At time: 29.559273719787598 and batch: 450, loss is 4.2715510606765745 and perplexity is 71.63265609324061
At time: 30.16065812110901 and batch: 500, loss is 4.1628471899032595 and perplexity is 64.2542063364043
At time: 30.762786626815796 and batch: 550, loss is 4.235351891517639 and perplexity is 69.08598522883234
At time: 31.369266510009766 and batch: 600, loss is 4.2323588514328 and perplexity is 68.87951724335358
At time: 31.971763610839844 and batch: 650, loss is 4.079668641090393 and perplexity is 59.12587471817465
At time: 32.573220014572144 and batch: 700, loss is 4.099300713539123 and perplexity is 60.29810720100863
At time: 33.17409038543701 and batch: 750, loss is 4.18676872253418 and perplexity is 65.8097973098656
At time: 33.776084184646606 and batch: 800, loss is 4.1536139345169065 and perplexity is 63.663661361410334
At time: 34.37704133987427 and batch: 850, loss is 4.2189567232131955 and perplexity is 67.9625435463848
At time: 34.98544383049011 and batch: 900, loss is 4.165466256141663 and perplexity is 64.42271292753459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43617812901327 and perplexity of 84.45156113681585
finished 3 epochs...
Completing Train Step...
At time: 36.50017428398132 and batch: 50, loss is 4.225380291938782 and perplexity is 68.40051076592998
At time: 37.11528730392456 and batch: 100, loss is 4.106595339775086 and perplexity is 60.73956754230504
At time: 37.71689414978027 and batch: 150, loss is 4.112763633728028 and perplexity is 61.1153849339665
At time: 38.31858992576599 and batch: 200, loss is 3.9985564374923706 and perplexity is 54.519391051204174
At time: 38.919389486312866 and batch: 250, loss is 4.149987735748291 and perplexity is 63.433222332347356
At time: 39.520551681518555 and batch: 300, loss is 4.112785863876343 and perplexity is 61.11674355313901
At time: 40.121280908584595 and batch: 350, loss is 4.113201928138733 and perplexity is 61.14217733664244
At time: 40.7231662273407 and batch: 400, loss is 4.04109787940979 and perplexity is 56.88876553721472
At time: 41.32505750656128 and batch: 450, loss is 4.070196633338928 and perplexity is 58.56847797508132
At time: 41.92597055435181 and batch: 500, loss is 3.965513005256653 and perplexity is 52.74732210347526
At time: 42.52545785903931 and batch: 550, loss is 4.033024282455444 and perplexity is 56.43131768265433
At time: 43.12751317024231 and batch: 600, loss is 4.0430191612243656 and perplexity is 56.99816995259795
At time: 43.729196548461914 and batch: 650, loss is 3.892217764854431 and perplexity is 49.019479741464664
At time: 44.33053731918335 and batch: 700, loss is 3.9044763708114623 and perplexity is 49.6240884866503
At time: 44.948795795440674 and batch: 750, loss is 3.996856412887573 and perplexity is 54.42678548315382
At time: 45.549439668655396 and batch: 800, loss is 3.972553405761719 and perplexity is 53.11999471952913
At time: 46.15195918083191 and batch: 850, loss is 4.038621206283569 and perplexity is 56.74804499175201
At time: 46.755003213882446 and batch: 900, loss is 3.9919154930114744 and perplexity is 54.158530356247674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3797628324325775 and perplexity of 79.81910065811955
finished 4 epochs...
Completing Train Step...
At time: 48.279417991638184 and batch: 50, loss is 4.060666193962097 and perplexity is 58.01294608273107
At time: 48.89369559288025 and batch: 100, loss is 3.945990514755249 and perplexity is 51.72754964620735
At time: 49.49486994743347 and batch: 150, loss is 3.9586376619338988 and perplexity is 52.38590999449492
At time: 50.09639358520508 and batch: 200, loss is 3.844235348701477 and perplexity is 46.72294392842768
At time: 50.70571684837341 and batch: 250, loss is 3.9937074613571166 and perplexity is 54.25566773585373
At time: 51.31315541267395 and batch: 300, loss is 3.961983528137207 and perplexity is 52.56147979301329
At time: 51.91700863838196 and batch: 350, loss is 3.959636149406433 and perplexity is 52.43824279183512
At time: 52.518189907073975 and batch: 400, loss is 3.8932719469070434 and perplexity is 49.07118244447921
At time: 53.119168758392334 and batch: 450, loss is 3.9224821805953978 and perplexity is 50.5257031767643
At time: 53.72022533416748 and batch: 500, loss is 3.823941388130188 and perplexity is 45.784306892251294
At time: 54.321820974349976 and batch: 550, loss is 3.887877902984619 and perplexity is 48.80720292965283
At time: 54.92178559303284 and batch: 600, loss is 3.904764938354492 and perplexity is 49.638410454268104
At time: 55.5243718624115 and batch: 650, loss is 3.758485450744629 and perplexity is 42.88342770614497
At time: 56.126277446746826 and batch: 700, loss is 3.764355239868164 and perplexity is 43.13588459316489
At time: 56.727149963378906 and batch: 750, loss is 3.8612153053283693 and perplexity is 47.5230713298689
At time: 57.328871726989746 and batch: 800, loss is 3.8426909017562867 and perplexity is 46.65083851623178
At time: 57.932809829711914 and batch: 850, loss is 3.9035250997543334 and perplexity is 49.57690497324998
At time: 58.5343451499939 and batch: 900, loss is 3.862197661399841 and perplexity is 47.5697788454744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358334476000642 and perplexity of 78.12690376882632
finished 5 epochs...
Completing Train Step...
At time: 60.050880670547485 and batch: 50, loss is 3.9348272848129273 and perplexity is 51.15331423940981
At time: 60.64979863166809 and batch: 100, loss is 3.8252705574035644 and perplexity is 45.845202447460444
At time: 61.2659547328949 and batch: 150, loss is 3.840791931152344 and perplexity is 46.562334005568474
At time: 61.87073612213135 and batch: 200, loss is 3.726544246673584 and perplexity is 41.53532403168812
At time: 62.47527074813843 and batch: 250, loss is 3.877808918952942 and perplexity is 48.31822984499553
At time: 63.09119486808777 and batch: 300, loss is 3.8474933242797853 and perplexity is 46.87541437632578
At time: 63.70766353607178 and batch: 350, loss is 3.8445213174819948 and perplexity is 46.73630714236503
At time: 64.31775164604187 and batch: 400, loss is 3.7825572299957275 and perplexity is 43.92823281985846
At time: 64.91900873184204 and batch: 450, loss is 3.8104429769515993 and perplexity is 45.17044390064852
At time: 65.51607584953308 and batch: 500, loss is 3.7147231769561766 and perplexity is 41.04722269437604
At time: 66.11448884010315 and batch: 550, loss is 3.77814266204834 and perplexity is 43.73473606779157
At time: 66.71319365501404 and batch: 600, loss is 3.7984450578689577 and perplexity is 44.63173075049415
At time: 67.3130292892456 and batch: 650, loss is 3.6564138317108155 and perplexity is 38.72222914755506
At time: 67.91168808937073 and batch: 700, loss is 3.6560154485702516 and perplexity is 38.70680593667521
At time: 68.51125288009644 and batch: 750, loss is 3.756218514442444 and perplexity is 42.78632381287089
At time: 69.1127724647522 and batch: 800, loss is 3.741702427864075 and perplexity is 42.16971999658302
At time: 69.71414613723755 and batch: 850, loss is 3.800731773376465 and perplexity is 44.733907601498906
At time: 70.31776857376099 and batch: 900, loss is 3.7603182220458984 and perplexity is 42.96209528956877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364800022072988 and perplexity of 78.63367337035179
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 71.83953213691711 and batch: 50, loss is 3.8626490211486817 and perplexity is 47.59125477522702
At time: 72.44102501869202 and batch: 100, loss is 3.7509387922286987 and perplexity is 42.56101920483589
At time: 73.04191541671753 and batch: 150, loss is 3.7694015884399414 and perplexity is 43.354113469151095
At time: 73.6413803100586 and batch: 200, loss is 3.639028787612915 and perplexity is 38.05485942103909
At time: 74.24243235588074 and batch: 250, loss is 3.782975172996521 and perplexity is 43.946596154449026
At time: 74.84359073638916 and batch: 300, loss is 3.744490647315979 and perplexity is 42.287462499848644
At time: 75.46591758728027 and batch: 350, loss is 3.7284995222091677 and perplexity is 41.61661648330165
At time: 76.06929278373718 and batch: 400, loss is 3.6580871629714964 and perplexity is 38.787078506168704
At time: 76.6747477054596 and batch: 450, loss is 3.6751280879974364 and perplexity is 39.45371008028062
At time: 77.29403281211853 and batch: 500, loss is 3.5765179586410523 and perplexity is 35.74884491832275
At time: 77.90043663978577 and batch: 550, loss is 3.620995407104492 and perplexity is 37.37475240615746
At time: 78.50816893577576 and batch: 600, loss is 3.630593900680542 and perplexity is 37.73522093694769
At time: 79.1154158115387 and batch: 650, loss is 3.4774047470092775 and perplexity is 32.375590104165404
At time: 79.72234582901001 and batch: 700, loss is 3.461804361343384 and perplexity is 31.87443766190074
At time: 80.3280279636383 and batch: 750, loss is 3.5441109991073607 and perplexity is 34.608904315278316
At time: 80.9333713054657 and batch: 800, loss is 3.5157226276397706 and perplexity is 33.640228501654235
At time: 81.5412244796753 and batch: 850, loss is 3.5581010675430296 and perplexity is 35.09648796691454
At time: 82.14513087272644 and batch: 900, loss is 3.5117609453201295 and perplexity is 33.50722024536206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325045650952483 and perplexity of 75.56896257610435
finished 7 epochs...
Completing Train Step...
At time: 83.68460702896118 and batch: 50, loss is 3.770163073539734 and perplexity is 43.38713955341223
At time: 84.30718278884888 and batch: 100, loss is 3.6550417232513426 and perplexity is 38.66913448352092
At time: 84.91744256019592 and batch: 150, loss is 3.672799172401428 and perplexity is 39.36193263200184
At time: 85.52885389328003 and batch: 200, loss is 3.5488355922698975 and perplexity is 34.772804183188846
At time: 86.13833522796631 and batch: 250, loss is 3.693915991783142 and perplexity is 40.20196969381161
At time: 86.74873542785645 and batch: 300, loss is 3.661388487815857 and perplexity is 38.915338850121586
At time: 87.35984539985657 and batch: 350, loss is 3.6493763446807863 and perplexity is 38.450678599623366
At time: 87.96365642547607 and batch: 400, loss is 3.5846117830276487 and perplexity is 36.039363909975336
At time: 88.56929039955139 and batch: 450, loss is 3.60731285572052 and perplexity is 36.86685303921232
At time: 89.17152905464172 and batch: 500, loss is 3.5119685077667238 and perplexity is 33.51417580780657
At time: 89.77234721183777 and batch: 550, loss is 3.5596399307250977 and perplexity is 35.15053823738596
At time: 90.37555623054504 and batch: 600, loss is 3.5763774347305297 and perplexity is 35.74382170378731
At time: 90.99828672409058 and batch: 650, loss is 3.4300621271133425 and perplexity is 30.878561085950516
At time: 91.60086584091187 and batch: 700, loss is 3.4170343494415283 and perplexity is 30.47889111580305
At time: 92.20405912399292 and batch: 750, loss is 3.5062794637680055 and perplexity is 33.3240535069016
At time: 92.80742907524109 and batch: 800, loss is 3.4835817289352415 and perplexity is 32.57619245981055
At time: 93.41202163696289 and batch: 850, loss is 3.532914218902588 and perplexity is 34.22355736889868
At time: 94.01544523239136 and batch: 900, loss is 3.4932135725021363 and perplexity is 32.89147719875745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328697727151113 and perplexity of 75.84545075632604
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 95.5253529548645 and batch: 50, loss is 3.7442983388900757 and perplexity is 42.279331046398426
At time: 96.14205861091614 and batch: 100, loss is 3.6384600734710695 and perplexity is 38.03322323730539
At time: 96.74677586555481 and batch: 150, loss is 3.657153787612915 and perplexity is 38.750892493053236
At time: 97.35197305679321 and batch: 200, loss is 3.5310108041763306 and perplexity is 34.1584777022643
At time: 97.9575788974762 and batch: 250, loss is 3.678580164909363 and perplexity is 39.59014267435558
At time: 98.56409215927124 and batch: 300, loss is 3.637160964012146 and perplexity is 37.983845997408594
At time: 99.16925287246704 and batch: 350, loss is 3.6248673677444456 and perplexity is 37.51974650096931
At time: 99.77438688278198 and batch: 400, loss is 3.559535193443298 and perplexity is 35.146856858349366
At time: 100.37910413742065 and batch: 450, loss is 3.574525923728943 and perplexity is 35.67770285344187
At time: 100.98520231246948 and batch: 500, loss is 3.4740980768203737 and perplexity is 32.26871150903677
At time: 101.60583019256592 and batch: 550, loss is 3.517464823722839 and perplexity is 33.69888745884904
At time: 102.21591854095459 and batch: 600, loss is 3.5376732683181764 and perplexity is 34.386817142125686
At time: 102.82046413421631 and batch: 650, loss is 3.384318070411682 and perplexity is 29.49787038531678
At time: 103.42537140846252 and batch: 700, loss is 3.3634582233428953 and perplexity is 28.888922679190053
At time: 104.03099989891052 and batch: 750, loss is 3.449535298347473 and perplexity is 31.485757425077484
At time: 104.63556218147278 and batch: 800, loss is 3.420699005126953 and perplexity is 30.590790668851223
At time: 105.24015522003174 and batch: 850, loss is 3.4700280380249025 and perplexity is 32.137643508170065
At time: 105.86688137054443 and batch: 900, loss is 3.4255963277816774 and perplexity is 30.740971081831535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319184603756422 and perplexity of 75.12734475553216
finished 9 epochs...
Completing Train Step...
At time: 107.3828284740448 and batch: 50, loss is 3.7170542669296265 and perplexity is 41.1430190752487
At time: 107.98003602027893 and batch: 100, loss is 3.6045854663848877 and perplexity is 36.76643977267595
At time: 108.5787365436554 and batch: 150, loss is 3.624192543029785 and perplexity is 37.49443578984988
At time: 109.17894744873047 and batch: 200, loss is 3.4978875875473023 and perplexity is 33.04557229891507
At time: 109.77714800834656 and batch: 250, loss is 3.646673922538757 and perplexity is 38.34690891230328
At time: 110.37395429611206 and batch: 300, loss is 3.6067300033569336 and perplexity is 36.84537136771007
At time: 110.98378252983093 and batch: 350, loss is 3.5966658782958985 and perplexity is 36.47641466971314
At time: 111.60550212860107 and batch: 400, loss is 3.534348421096802 and perplexity is 34.2726760846364
At time: 112.2229163646698 and batch: 450, loss is 3.5510944652557375 and perplexity is 34.85144031227722
At time: 112.8267433643341 and batch: 500, loss is 3.4528429651260377 and perplexity is 31.59007424644472
At time: 113.4271011352539 and batch: 550, loss is 3.497720522880554 and perplexity is 33.04005201252669
At time: 114.02789640426636 and batch: 600, loss is 3.5212557220458987 and perplexity is 33.82687896291534
At time: 114.62914705276489 and batch: 650, loss is 3.3706157207489014 and perplexity is 29.096436823395376
At time: 115.23118185997009 and batch: 700, loss is 3.3526179075241087 and perplexity is 28.57744892080254
At time: 115.83329272270203 and batch: 750, loss is 3.442064905166626 and perplexity is 31.251422813146103
At time: 116.43479776382446 and batch: 800, loss is 3.4153318786621094 and perplexity is 30.42704583934581
At time: 117.03741693496704 and batch: 850, loss is 3.4682611274719237 and perplexity is 32.080909303600215
At time: 117.63838601112366 and batch: 900, loss is 3.4268954372406006 and perplexity is 30.780932919921106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320735983652611 and perplexity of 75.24398626208993
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 119.17304301261902 and batch: 50, loss is 3.7103962802886965 and perplexity is 40.86999929346031
At time: 119.77418327331543 and batch: 100, loss is 3.6018264484405518 and perplexity is 36.66514031336592
At time: 120.37582015991211 and batch: 150, loss is 3.6243087100982665 and perplexity is 37.49879166153948
At time: 120.9768853187561 and batch: 200, loss is 3.4969423484802244 and perplexity is 33.01435109106149
At time: 121.59499430656433 and batch: 250, loss is 3.644366731643677 and perplexity is 38.25853725754611
At time: 122.19608092308044 and batch: 300, loss is 3.604991607666016 and perplexity is 36.78137517436403
At time: 122.79790377616882 and batch: 350, loss is 3.5930072116851806 and perplexity is 36.34320346557004
At time: 123.39911007881165 and batch: 400, loss is 3.529701075553894 and perplexity is 34.113768651071915
At time: 124.00047874450684 and batch: 450, loss is 3.5441031074523925 and perplexity is 34.60863119482432
At time: 124.6008517742157 and batch: 500, loss is 3.4428104400634765 and perplexity is 31.274730526706872
At time: 125.19870734214783 and batch: 550, loss is 3.485404510498047 and perplexity is 32.63562589344042
At time: 125.79770374298096 and batch: 600, loss is 3.510854573249817 and perplexity is 33.476863995884365
At time: 126.41298961639404 and batch: 650, loss is 3.3546806669235227 and perplexity is 28.63645816218528
At time: 127.01489567756653 and batch: 700, loss is 3.3362274980545044 and perplexity is 28.11287054681253
At time: 127.6172513961792 and batch: 750, loss is 3.423427629470825 and perplexity is 30.674375428813853
At time: 128.218359708786 and batch: 800, loss is 3.3942547845840454 and perplexity is 29.792443412215626
At time: 128.81963181495667 and batch: 850, loss is 3.4449664640426634 and perplexity is 31.342232337278002
At time: 129.42096638679504 and batch: 900, loss is 3.405854640007019 and perplexity is 30.14004360684485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314613655821918 and perplexity of 74.78472521877414
finished 11 epochs...
Completing Train Step...
At time: 130.9343454837799 and batch: 50, loss is 3.7000101709365847 and perplexity is 40.44771574912716
At time: 131.54872274398804 and batch: 100, loss is 3.5893541049957274 and perplexity is 36.2106800742965
At time: 132.1469566822052 and batch: 150, loss is 3.611665964126587 and perplexity is 37.02768825967811
At time: 132.74511528015137 and batch: 200, loss is 3.484752049446106 and perplexity is 32.61433936371018
At time: 133.34359765052795 and batch: 250, loss is 3.6335393238067626 and perplexity is 37.84653097647806
At time: 133.94163608551025 and batch: 300, loss is 3.5952382278442383 and perplexity is 36.42437625500677
At time: 134.5401964187622 and batch: 350, loss is 3.5828408098220823 and perplexity is 35.97559564475626
At time: 135.1383411884308 and batch: 400, loss is 3.520867443084717 and perplexity is 33.81374724704107
At time: 135.7400426864624 and batch: 450, loss is 3.5365506076812743 and perplexity is 34.34823407798795
At time: 136.33939838409424 and batch: 500, loss is 3.435814371109009 and perplexity is 31.056693944208156
At time: 136.94995284080505 and batch: 550, loss is 3.4796656656265257 and perplexity is 32.44887148898357
At time: 137.5501000881195 and batch: 600, loss is 3.5061031007766723 and perplexity is 33.3181768953654
At time: 138.147700548172 and batch: 650, loss is 3.3508550214767454 and perplexity is 28.527114514777494
At time: 138.74356031417847 and batch: 700, loss is 3.333682026863098 and perplexity is 28.041401045096816
At time: 139.3392653465271 and batch: 750, loss is 3.422502136230469 and perplexity is 30.64599963453065
At time: 139.95415568351746 and batch: 800, loss is 3.3944140243530274 and perplexity is 29.79718793177007
At time: 140.55641722679138 and batch: 850, loss is 3.4471137619018553 and perplexity is 31.40960575518855
At time: 141.15437293052673 and batch: 900, loss is 3.4092943334579466 and perplexity is 30.243894622887183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314435148892337 and perplexity of 74.7713768185222
finished 12 epochs...
Completing Train Step...
At time: 142.6678228378296 and batch: 50, loss is 3.6944537925720216 and perplexity is 40.22359615967141
At time: 143.27800369262695 and batch: 100, loss is 3.5832793283462525 and perplexity is 35.99137506939668
At time: 143.87667798995972 and batch: 150, loss is 3.6051777172088624 and perplexity is 36.788221176316384
At time: 144.4906039237976 and batch: 200, loss is 3.4783480739593506 and perplexity is 32.40614528032992
At time: 145.0925624370575 and batch: 250, loss is 3.6271879148483275 and perplexity is 37.606913939007526
At time: 145.6917324066162 and batch: 300, loss is 3.589271059036255 and perplexity is 36.207673048488985
At time: 146.29029297828674 and batch: 350, loss is 3.576919283866882 and perplexity is 35.76319471085702
At time: 146.89374136924744 and batch: 400, loss is 3.5154840898513795 and perplexity is 33.632204992939535
At time: 147.4991729259491 and batch: 450, loss is 3.5316368627548216 and perplexity is 34.17986960585166
At time: 148.09889602661133 and batch: 500, loss is 3.4313658142089842 and perplexity is 30.918843319579032
At time: 148.69719004631042 and batch: 550, loss is 3.475754828453064 and perplexity is 32.322217059980005
At time: 149.29581999778748 and batch: 600, loss is 3.502855682373047 and perplexity is 33.210154326710224
At time: 149.8946521282196 and batch: 650, loss is 3.3481527853012083 and perplexity is 28.45013157384055
At time: 150.49367880821228 and batch: 700, loss is 3.3316406345367433 and perplexity is 27.984215932666846
At time: 151.0920889377594 and batch: 750, loss is 3.421265482902527 and perplexity is 30.608124581076844
At time: 151.7112169265747 and batch: 800, loss is 3.3937371921539308 and perplexity is 29.77702705906886
At time: 152.30989265441895 and batch: 850, loss is 3.4472529315948486 and perplexity is 31.41397732456647
At time: 152.91012287139893 and batch: 900, loss is 3.4100098276138304 and perplexity is 30.26554169599469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314842746682363 and perplexity of 74.80185967842174
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 154.42168855667114 and batch: 50, loss is 3.692387132644653 and perplexity is 40.140553505373354
At time: 155.0209732055664 and batch: 100, loss is 3.5826402139663696 and perplexity is 35.96837981312028
At time: 155.62076449394226 and batch: 150, loss is 3.605116772651672 and perplexity is 36.78597920278572
At time: 156.21898436546326 and batch: 200, loss is 3.4785792589187623 and perplexity is 32.41363795977527
At time: 156.8182008266449 and batch: 250, loss is 3.6271904039382936 and perplexity is 37.607007546116165
At time: 157.41750407218933 and batch: 300, loss is 3.589724860191345 and perplexity is 36.224107861127955
At time: 158.01650071144104 and batch: 350, loss is 3.576114630699158 and perplexity is 35.734429317579455
At time: 158.61618065834045 and batch: 400, loss is 3.5142191219329835 and perplexity is 33.58968822943887
At time: 159.2158706188202 and batch: 450, loss is 3.5292069721221924 and perplexity is 34.09691708446443
At time: 159.81422305107117 and batch: 500, loss is 3.4279548215866087 and perplexity is 30.813559037159585
At time: 160.4132363796234 and batch: 550, loss is 3.4724034881591797 and perplexity is 32.21407562215392
At time: 161.01168489456177 and batch: 600, loss is 3.499707808494568 and perplexity is 33.10577731842413
At time: 161.61063742637634 and batch: 650, loss is 3.341689472198486 and perplexity is 28.266842431289465
At time: 162.2095649242401 and batch: 700, loss is 3.3252296018600465 and perplexity is 27.805382077261633
At time: 162.80820608139038 and batch: 750, loss is 3.414299840927124 and perplexity is 30.395660178254513
At time: 163.4071650505066 and batch: 800, loss is 3.3861590957641603 and perplexity is 29.552226732894407
At time: 164.01939797401428 and batch: 850, loss is 3.4367203855514528 and perplexity is 31.08484450793917
At time: 164.61736536026 and batch: 900, loss is 3.400274386405945 and perplexity is 29.97232291718665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3136575777236725 and perplexity of 74.71325934982902
finished 14 epochs...
Completing Train Step...
At time: 166.1279799938202 and batch: 50, loss is 3.689423689842224 and perplexity is 40.021775354056864
At time: 166.72991728782654 and batch: 100, loss is 3.579341688156128 and perplexity is 35.84993264187465
At time: 167.3435845375061 and batch: 150, loss is 3.601967577934265 and perplexity is 36.670315211212035
At time: 167.94418334960938 and batch: 200, loss is 3.4758215236663816 and perplexity is 32.32437286903198
At time: 168.54512119293213 and batch: 250, loss is 3.624243001937866 and perplexity is 37.496327765872074
At time: 169.14655590057373 and batch: 300, loss is 3.5871668529510496 and perplexity is 36.13156474438949
At time: 169.74787712097168 and batch: 350, loss is 3.573282742500305 and perplexity is 35.63337656148945
At time: 170.34916138648987 and batch: 400, loss is 3.511623330116272 and perplexity is 33.50260945968168
At time: 170.94986844062805 and batch: 450, loss is 3.5272121953964235 and perplexity is 34.02896914086327
At time: 171.55128622055054 and batch: 500, loss is 3.4263210010528566 and perplexity is 30.76325631568603
At time: 172.15320920944214 and batch: 550, loss is 3.4707013988494873 and perplexity is 32.15929102578015
At time: 172.75519561767578 and batch: 600, loss is 3.4983712339401247 and perplexity is 33.06155853628759
At time: 173.35573434829712 and batch: 650, loss is 3.341091275215149 and perplexity is 28.24993834790852
At time: 173.95673298835754 and batch: 700, loss is 3.324813098907471 and perplexity is 27.79380346495945
At time: 174.5581238269806 and batch: 750, loss is 3.4141331815719607 and perplexity is 30.390594879230896
At time: 175.15953755378723 and batch: 800, loss is 3.386466884613037 and perplexity is 29.561323978685856
At time: 175.76152443885803 and batch: 850, loss is 3.437820453643799 and perplexity is 31.119058769096032
At time: 176.36302852630615 and batch: 900, loss is 3.401878523826599 and perplexity is 30.020441225833395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31333986047196 and perplexity of 74.68952542893965
finished 15 epochs...
Completing Train Step...
At time: 177.86633586883545 and batch: 50, loss is 3.6876646518707275 and perplexity is 39.95143741321349
At time: 178.48281836509705 and batch: 100, loss is 3.5773820161819456 and perplexity is 35.77974732615929
At time: 179.10473108291626 and batch: 150, loss is 3.599868850708008 and perplexity is 36.593434925876096
At time: 179.72389817237854 and batch: 200, loss is 3.47392107963562 and perplexity is 32.263000543371376
At time: 180.33848524093628 and batch: 250, loss is 3.622316975593567 and perplexity is 37.42417835391825
At time: 180.9582552909851 and batch: 300, loss is 3.585463533401489 and perplexity is 36.070073528266455
At time: 181.57347416877747 and batch: 350, loss is 3.5714273595809938 and perplexity is 35.56732429832874
At time: 182.1842761039734 and batch: 400, loss is 3.5100225496292112 and perplexity is 33.449022038485246
At time: 182.8046817779541 and batch: 450, loss is 3.525814428329468 and perplexity is 33.98143779508931
At time: 183.41030406951904 and batch: 500, loss is 3.4250999784469602 and perplexity is 30.72571660737381
At time: 184.01550889015198 and batch: 550, loss is 3.4695918464660647 and perplexity is 32.12362839620984
At time: 184.62116765975952 and batch: 600, loss is 3.4975001621246338 and perplexity is 33.03277208382431
At time: 185.22631788253784 and batch: 650, loss is 3.3405390691757204 and perplexity is 28.2343428676948
At time: 185.8316969871521 and batch: 700, loss is 3.3244284629821776 and perplexity is 27.78311502535699
At time: 186.43729543685913 and batch: 750, loss is 3.413999972343445 and perplexity is 30.386546841156896
At time: 187.04287123680115 and batch: 800, loss is 3.3865734004974364 and perplexity is 29.56447289695539
At time: 187.65110850334167 and batch: 850, loss is 3.4383335447311403 and perplexity is 31.13502977773762
At time: 188.25917840003967 and batch: 900, loss is 3.4026365232467652 and perplexity is 30.043205329374402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313276735070634 and perplexity of 74.68481077148094
finished 16 epochs...
Completing Train Step...
At time: 189.81939482688904 and batch: 50, loss is 3.6861805868148805 and perplexity is 39.89219085476518
At time: 190.43360900878906 and batch: 100, loss is 3.5757689762115477 and perplexity is 35.72207968619936
At time: 191.03562545776367 and batch: 150, loss is 3.5981561279296876 and perplexity is 36.53081415765738
At time: 191.63800430297852 and batch: 200, loss is 3.472310528755188 and perplexity is 32.2110811600678
At time: 192.2404670715332 and batch: 250, loss is 3.620708131790161 and perplexity is 37.36401710447935
At time: 192.84110713005066 and batch: 300, loss is 3.5839923572540284 and perplexity is 36.01704711161822
At time: 193.45745134353638 and batch: 350, loss is 3.5699084043502807 and perplexity is 35.51334013518628
At time: 194.0599808692932 and batch: 400, loss is 3.5086842489242556 and perplexity is 33.404287129767866
At time: 194.6641731262207 and batch: 450, loss is 3.5246085691452027 and perplexity is 33.94048566244456
At time: 195.26422119140625 and batch: 500, loss is 3.424020380973816 and perplexity is 30.692563100805025
At time: 195.86376571655273 and batch: 550, loss is 3.46865234375 and perplexity is 32.09346233284986
At time: 196.4636561870575 and batch: 600, loss is 3.4967423915863036 and perplexity is 33.00775030392096
At time: 197.06106567382812 and batch: 650, loss is 3.339976305961609 and perplexity is 28.218458088258245
At time: 197.6820740699768 and batch: 700, loss is 3.324010157585144 and perplexity is 27.77149562879305
At time: 198.2803134918213 and batch: 750, loss is 3.4137985563278197 and perplexity is 30.38042712028913
At time: 198.8791790008545 and batch: 800, loss is 3.3865300941467287 and perplexity is 29.563192595246424
At time: 199.4768717288971 and batch: 850, loss is 3.4385543632507325 and perplexity is 31.141905728061246
At time: 200.07590889930725 and batch: 900, loss is 3.4030166053771973 and perplexity is 30.05462638519312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3133260648544525 and perplexity of 74.68849504792237
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 201.59375643730164 and batch: 50, loss is 3.685505690574646 and perplexity is 39.86527684824507
At time: 202.19286394119263 and batch: 100, loss is 3.575426378250122 and perplexity is 35.70984347069191
At time: 202.79211163520813 and batch: 150, loss is 3.5980114698410035 and perplexity is 36.52553006210605
At time: 203.3915615081787 and batch: 200, loss is 3.472470841407776 and perplexity is 32.21624541786817
At time: 203.99109268188477 and batch: 250, loss is 3.6207804679870605 and perplexity is 37.36671997313402
At time: 204.5906262397766 and batch: 300, loss is 3.5841121912002563 and perplexity is 36.02136343512095
At time: 205.1899209022522 and batch: 350, loss is 3.5697586488723756 and perplexity is 35.508022216166026
At time: 205.79007744789124 and batch: 400, loss is 3.508019948005676 and perplexity is 33.38210400009556
At time: 206.38985872268677 and batch: 450, loss is 3.523876495361328 and perplexity is 33.91564781535674
At time: 206.99069023132324 and batch: 500, loss is 3.4225874757766723 and perplexity is 30.648615061830515
At time: 207.59988737106323 and batch: 550, loss is 3.4674347972869874 and perplexity is 32.054410829634705
At time: 208.1991572380066 and batch: 600, loss is 3.4954070711135863 and perplexity is 32.96370379383
At time: 208.7997932434082 and batch: 650, loss is 3.3376749420166014 and perplexity is 28.153591815310264
At time: 209.39959239959717 and batch: 700, loss is 3.3220824575424195 and perplexity is 27.71801208216266
At time: 209.99947929382324 and batch: 750, loss is 3.411901092529297 and perplexity is 30.322836015442107
At time: 210.60007190704346 and batch: 800, loss is 3.3840527391433715 and perplexity is 29.490044716198245
At time: 211.1991548538208 and batch: 850, loss is 3.4353271770477294 and perplexity is 31.041566992535277
At time: 211.79869771003723 and batch: 900, loss is 3.3994286918640135 and perplexity is 29.946986202357365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312903835348887 and perplexity of 74.65696601829859
finished 18 epochs...
Completing Train Step...
At time: 213.31808614730835 and batch: 50, loss is 3.684872965812683 and perplexity is 39.84006107860284
At time: 213.91786432266235 and batch: 100, loss is 3.5747592306137084 and perplexity is 35.6860276782319
At time: 214.51830387115479 and batch: 150, loss is 3.597331237792969 and perplexity is 36.50069267453759
At time: 215.11687326431274 and batch: 200, loss is 3.471886463165283 and perplexity is 32.19742444481035
At time: 215.71644949913025 and batch: 250, loss is 3.620125970840454 and perplexity is 37.34227156311359
At time: 216.32915019989014 and batch: 300, loss is 3.58343542098999 and perplexity is 35.9969934967692
At time: 216.9318778514862 and batch: 350, loss is 3.5691637420654296 and perplexity is 35.486904534197635
At time: 217.5309488773346 and batch: 400, loss is 3.507460341453552 and perplexity is 33.36342838195993
At time: 218.13034176826477 and batch: 450, loss is 3.523445529937744 and perplexity is 33.90103449297439
At time: 218.7294852733612 and batch: 500, loss is 3.422319560050964 and perplexity is 30.640404915747006
At time: 219.32839441299438 and batch: 550, loss is 3.4671419763565066 and perplexity is 32.04502600133369
At time: 219.927654504776 and batch: 600, loss is 3.4952038621902464 and perplexity is 32.95700595562605
At time: 220.52763485908508 and batch: 650, loss is 3.3376590251922607 and perplexity is 28.153143703101048
At time: 221.12815594673157 and batch: 700, loss is 3.3220037031173706 and perplexity is 27.715829252012526
At time: 221.72718858718872 and batch: 750, loss is 3.4118289804458617 and perplexity is 30.320649451401156
At time: 222.3270788192749 and batch: 800, loss is 3.384151887893677 and perplexity is 29.49296876223367
At time: 222.9261839389801 and batch: 850, loss is 3.43553608417511 and perplexity is 31.0480524745332
At time: 223.5262610912323 and batch: 900, loss is 3.399737486839294 and perplexity is 29.95623510915593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312709442556721 and perplexity of 74.64245465271775
finished 19 epochs...
Completing Train Step...
At time: 225.03799033164978 and batch: 50, loss is 3.684373197555542 and perplexity is 39.82015525527683
At time: 225.65094804763794 and batch: 100, loss is 3.5742227983474733 and perplexity is 35.666889675111655
At time: 226.25091791152954 and batch: 150, loss is 3.596787118911743 and perplexity is 36.48083736079097
At time: 226.8499665260315 and batch: 200, loss is 3.471405391693115 and perplexity is 32.18193890755642
At time: 227.44992232322693 and batch: 250, loss is 3.6196036767959594 and perplexity is 37.32277300950074
At time: 228.0618977546692 and batch: 300, loss is 3.5829315328598024 and perplexity is 35.978859608133035
At time: 228.66119074821472 and batch: 350, loss is 3.5686697959899902 and perplexity is 35.469380245356625
At time: 229.26100611686707 and batch: 400, loss is 3.5070209741592406 and perplexity is 33.348772802529695
At time: 229.86278200149536 and batch: 450, loss is 3.523091688156128 and perplexity is 33.88904101255314
At time: 230.4616403579712 and batch: 500, loss is 3.4220602893829346 and perplexity is 30.632461787250417
At time: 231.06219005584717 and batch: 550, loss is 3.466871199607849 and perplexity is 32.03635012804779
At time: 231.66273665428162 and batch: 600, loss is 3.495014672279358 and perplexity is 32.95077141238109
At time: 232.26170349121094 and batch: 650, loss is 3.33758629322052 and perplexity is 28.15109614391123
At time: 232.86140990257263 and batch: 700, loss is 3.321927170753479 and perplexity is 27.7137081752492
At time: 233.4602129459381 and batch: 750, loss is 3.4117862129211427 and perplexity is 30.319352740005016
At time: 234.06046509742737 and batch: 800, loss is 3.384216408729553 and perplexity is 29.494871734620705
At time: 234.66106390953064 and batch: 850, loss is 3.435693831443787 and perplexity is 31.052950606332093
At time: 235.2671344280243 and batch: 900, loss is 3.3999735593795775 and perplexity is 29.963307788475312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31261203713613 and perplexity of 74.63518442711383
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f39b9e46b70>
ELAPSED
1214.6509392261505


RESULTS SO FAR:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.9701938996608, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.11641594496695, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5835303149491065, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7493984943579154, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.36975667692482, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.9121007976102171, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.8055971500675865, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -74.63518442711383, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5989062690656494, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.730432796799109, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.606213139937303, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7214203749085344, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8276407718658447 and batch: 50, loss is 6.811080846786499 and perplexity is 907.8515252526703
At time: 1.4531142711639404 and batch: 100, loss is 5.99842791557312 and perplexity is 402.79506763492464
At time: 2.061976432800293 and batch: 150, loss is 5.824195737838745 and perplexity is 338.38887024312874
At time: 2.6704490184783936 and batch: 200, loss is 5.652610893249512 and perplexity is 285.0346903106332
At time: 3.278585433959961 and batch: 250, loss is 5.690539131164551 and perplexity is 296.0531891230688
At time: 3.886235475540161 and batch: 300, loss is 5.585249290466309 and perplexity is 266.46670206868305
At time: 4.494445323944092 and batch: 350, loss is 5.5559139060974125 and perplexity is 258.76334188670046
At time: 5.101140975952148 and batch: 400, loss is 5.414795780181885 and perplexity is 224.70665144103234
At time: 5.708081483840942 and batch: 450, loss is 5.416350812911987 and perplexity is 225.0563494641627
At time: 6.318933725357056 and batch: 500, loss is 5.358282117843628 and perplexity is 212.35982376848864
At time: 6.939900159835815 and batch: 550, loss is 5.402304821014404 and perplexity is 221.91730689461528
At time: 7.558866500854492 and batch: 600, loss is 5.3188448333740235 and perplexity is 204.14792086705012
At time: 8.165124893188477 and batch: 650, loss is 5.217959966659546 and perplexity is 184.55729673146712
At time: 8.771782398223877 and batch: 700, loss is 5.304385719299316 and perplexity is 201.2173605009654
At time: 9.37815523147583 and batch: 750, loss is 5.281325626373291 and perplexity is 196.63036105269637
At time: 9.984769582748413 and batch: 800, loss is 5.2550741481781005 and perplexity is 191.53568735012132
At time: 10.592066764831543 and batch: 850, loss is 5.288026933670044 and perplexity is 197.95246649570552
At time: 11.199291706085205 and batch: 900, loss is 5.2038080024719235 and perplexity is 181.9638429939525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.061940232368364 and perplexity of 157.89657536089712
finished 1 epochs...
Completing Train Step...
At time: 12.727960586547852 and batch: 50, loss is 4.975830011367798 and perplexity is 144.86901819242553
At time: 13.346284866333008 and batch: 100, loss is 4.834513883590699 and perplexity is 125.77742587264896
At time: 13.953016757965088 and batch: 150, loss is 4.813420953750611 and perplexity is 123.15219568359103
At time: 14.557238817214966 and batch: 200, loss is 4.697514476776123 and perplexity is 109.67423553762704
At time: 15.162577867507935 and batch: 250, loss is 4.795101490020752 and perplexity is 120.91665299396267
At time: 15.765375137329102 and batch: 300, loss is 4.724973173141479 and perplexity is 112.72747420950292
At time: 16.369516134262085 and batch: 350, loss is 4.717643575668335 and perplexity is 111.90424784384028
At time: 16.971860885620117 and batch: 400, loss is 4.594697484970093 and perplexity is 98.95819467789211
At time: 17.575237274169922 and batch: 450, loss is 4.6190564060211186 and perplexity is 101.39830814132772
At time: 18.177842378616333 and batch: 500, loss is 4.515112733840942 and perplexity is 91.38786794705875
At time: 18.781699419021606 and batch: 550, loss is 4.583702669143677 and perplexity is 97.8761270207402
At time: 19.384990692138672 and batch: 600, loss is 4.544405183792114 and perplexity is 94.1044357151362
At time: 19.999067544937134 and batch: 650, loss is 4.402035903930664 and perplexity is 81.61686372630193
At time: 20.625330448150635 and batch: 700, loss is 4.442521667480468 and perplexity is 84.98898564784783
At time: 21.237035751342773 and batch: 750, loss is 4.494974861145019 and perplexity is 89.56591737223602
At time: 21.840454816818237 and batch: 800, loss is 4.452672119140625 and perplexity is 85.85605536833002
At time: 22.445032835006714 and batch: 850, loss is 4.51133505821228 and perplexity is 91.04328549571383
At time: 23.061190605163574 and batch: 900, loss is 4.439181470870972 and perplexity is 84.70557930608823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5701620023544525 and perplexity of 96.55975141289342
finished 2 epochs...
Completing Train Step...
At time: 24.580747365951538 and batch: 50, loss is 4.473610792160034 and perplexity is 87.67272014067296
At time: 25.199084281921387 and batch: 100, loss is 4.345057544708252 and perplexity is 77.09647384575088
At time: 25.808131217956543 and batch: 150, loss is 4.346848955154419 and perplexity is 77.23470905539199
At time: 26.414531469345093 and batch: 200, loss is 4.238086981773376 and perplexity is 69.27520027600026
At time: 27.020496606826782 and batch: 250, loss is 4.375603036880493 and perplexity is 79.48775915248275
At time: 27.626359939575195 and batch: 300, loss is 4.336278228759766 and perplexity is 76.42258202636702
At time: 28.232858657836914 and batch: 350, loss is 4.336614332199097 and perplexity is 76.4482722360707
At time: 28.839073419570923 and batch: 400, loss is 4.2461984348297115 and perplexity is 69.83940798966694
At time: 29.445027828216553 and batch: 450, loss is 4.289034605026245 and perplexity is 72.89606102866814
At time: 30.051179885864258 and batch: 500, loss is 4.164186654090881 and perplexity is 64.34033021174261
At time: 30.65677499771118 and batch: 550, loss is 4.2387320423126225 and perplexity is 69.31990138995899
At time: 31.262979745864868 and batch: 600, loss is 4.234868202209473 and perplexity is 69.05257715664803
At time: 31.869118213653564 and batch: 650, loss is 4.08276216506958 and perplexity is 59.30906523544194
At time: 32.474571228027344 and batch: 700, loss is 4.108323359489441 and perplexity is 60.84461745047535
At time: 33.080923557281494 and batch: 750, loss is 4.1946017980575565 and perplexity is 66.32731465247686
At time: 33.68596696853638 and batch: 800, loss is 4.164175901412964 and perplexity is 64.33963838461428
At time: 34.291892766952515 and batch: 850, loss is 4.225879411697388 and perplexity is 68.43465933375555
At time: 34.89767050743103 and batch: 900, loss is 4.16625985622406 and perplexity is 64.47385908993714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436806038634418 and perplexity of 84.50460573643902
finished 3 epochs...
Completing Train Step...
At time: 36.402979135513306 and batch: 50, loss is 4.23047209739685 and perplexity is 68.74968105918897
At time: 37.01480317115784 and batch: 100, loss is 4.106003804206848 and perplexity is 60.703648552431815
At time: 37.62219595909119 and batch: 150, loss is 4.1169852447509765 and perplexity is 61.373935683030794
At time: 38.24112629890442 and batch: 200, loss is 4.007210745811462 and perplexity is 54.993266244091515
At time: 38.84098505973816 and batch: 250, loss is 4.149735760688782 and perplexity is 63.4172407559392
At time: 39.43993926048279 and batch: 300, loss is 4.12022379398346 and perplexity is 61.57302039414739
At time: 40.039470195770264 and batch: 350, loss is 4.119612560272217 and perplexity is 61.535396388081274
At time: 40.65016317367554 and batch: 400, loss is 4.042930655479431 and perplexity is 56.993125510340725
At time: 41.25697588920593 and batch: 450, loss is 4.090055341720581 and perplexity is 59.743197904023994
At time: 41.85634136199951 and batch: 500, loss is 3.966148519515991 and perplexity is 52.78085443282207
At time: 42.45659279823303 and batch: 550, loss is 4.038837118148804 and perplexity is 56.7602988908281
At time: 43.056840658187866 and batch: 600, loss is 4.045116491317749 and perplexity is 57.11783937898519
At time: 43.656726360321045 and batch: 650, loss is 3.89706253528595 and perplexity is 49.25754408538194
At time: 44.256508111953735 and batch: 700, loss is 3.9136470317840577 and perplexity is 50.08126729004442
At time: 44.85728406906128 and batch: 750, loss is 4.006974925994873 and perplexity is 54.980299271126896
At time: 45.457563161849976 and batch: 800, loss is 3.984786939620972 and perplexity is 53.773831184424964
At time: 46.05704879760742 and batch: 850, loss is 4.044117426872253 and perplexity is 57.060803472475335
At time: 46.65588021278381 and batch: 900, loss is 3.9890987920761107 and perplexity is 54.006196613149136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388124230789812 and perplexity of 80.48929794386056
finished 4 epochs...
Completing Train Step...
At time: 48.16244459152222 and batch: 50, loss is 4.064823989868164 and perplexity is 58.254654210779364
At time: 48.79116129875183 and batch: 100, loss is 3.946415367126465 and perplexity is 51.749530887392304
At time: 49.40609622001648 and batch: 150, loss is 3.961876993179321 and perplexity is 52.55588045624498
At time: 50.014867305755615 and batch: 200, loss is 3.850506010055542 and perplexity is 47.01684821132714
At time: 50.6144757270813 and batch: 250, loss is 3.995495891571045 and perplexity is 54.35278703096405
At time: 51.21298432350159 and batch: 300, loss is 3.9694808197021483 and perplexity is 52.9570294548796
At time: 51.81054496765137 and batch: 350, loss is 3.9708279943466187 and perplexity is 53.02841989910877
At time: 52.4078905582428 and batch: 400, loss is 3.9003183889389037 and perplexity is 49.41818080314649
At time: 53.0050048828125 and batch: 450, loss is 3.945321717262268 and perplexity is 51.69296595671741
At time: 53.62734317779541 and batch: 500, loss is 3.828515214920044 and perplexity is 45.994196014009376
At time: 54.235788345336914 and batch: 550, loss is 3.8972305393218996 and perplexity is 49.26582024678405
At time: 54.83381414413452 and batch: 600, loss is 3.905717897415161 and perplexity is 49.68573637351797
At time: 55.43922781944275 and batch: 650, loss is 3.764641480445862 and perplexity is 43.148233600999596
At time: 56.046339988708496 and batch: 700, loss is 3.774302530288696 and perplexity is 43.5671109760291
At time: 56.644593238830566 and batch: 750, loss is 3.873155355453491 and perplexity is 48.09390026503684
At time: 57.24405026435852 and batch: 800, loss is 3.8520516395568847 and perplexity is 47.08957502886156
At time: 57.842365026474 and batch: 850, loss is 3.9099368333816527 and perplexity is 49.89580012489948
At time: 58.44631552696228 and batch: 900, loss is 3.858858094215393 and perplexity is 47.41118134397565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371999662216395 and perplexity of 79.20185040258015
finished 5 epochs...
Completing Train Step...
At time: 59.9612557888031 and batch: 50, loss is 3.943135266304016 and perplexity is 51.58006529262648
At time: 60.559704542160034 and batch: 100, loss is 3.8284692335128785 and perplexity is 45.992081184777
At time: 61.158968687057495 and batch: 150, loss is 3.8397162055969236 and perplexity is 46.512272643929215
At time: 61.7575249671936 and batch: 200, loss is 3.732397174835205 and perplexity is 41.77914012241822
At time: 62.35708141326904 and batch: 250, loss is 3.877376494407654 and perplexity is 48.297340373311336
At time: 62.95729947090149 and batch: 300, loss is 3.8544017839431763 and perplexity is 47.200372473268324
At time: 63.56012225151062 and batch: 350, loss is 3.858148832321167 and perplexity is 47.377566322024386
At time: 64.16508913040161 and batch: 400, loss is 3.790208306312561 and perplexity is 44.26562012377499
At time: 64.76490378379822 and batch: 450, loss is 3.8314988946914674 and perplexity is 46.13163289812556
At time: 65.36538338661194 and batch: 500, loss is 3.724436936378479 and perplexity is 41.44788837513194
At time: 65.96523571014404 and batch: 550, loss is 3.783326907157898 and perplexity is 43.9620563923803
At time: 66.56546688079834 and batch: 600, loss is 3.7974217176437377 and perplexity is 44.58608066686324
At time: 67.16611886024475 and batch: 650, loss is 3.661750135421753 and perplexity is 38.92941503440512
At time: 67.76649832725525 and batch: 700, loss is 3.668737416267395 and perplexity is 39.20237831540718
At time: 68.36544871330261 and batch: 750, loss is 3.767429986000061 and perplexity is 43.268720601311465
At time: 68.97782111167908 and batch: 800, loss is 3.7478817749023436 and perplexity is 42.43110810311589
At time: 69.57724070549011 and batch: 850, loss is 3.8049117136001587 and perplexity is 44.9212839994985
At time: 70.17669582366943 and batch: 900, loss is 3.758686580657959 and perplexity is 42.8920537136878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381115220997431 and perplexity of 79.92712012278734
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 71.68584990501404 and batch: 50, loss is 3.8741507959365844 and perplexity is 48.14179871642408
At time: 72.28456974029541 and batch: 100, loss is 3.7611684560775758 and perplexity is 42.99863865806247
At time: 72.88530397415161 and batch: 150, loss is 3.7726930570602417 and perplexity is 43.49704727522188
At time: 73.48492574691772 and batch: 200, loss is 3.6519559144973757 and perplexity is 38.5499928485145
At time: 74.08381867408752 and batch: 250, loss is 3.786266551017761 and perplexity is 44.091479316973185
At time: 74.68198370933533 and batch: 300, loss is 3.7525899600982666 and perplexity is 42.63135264241962
At time: 75.28334021568298 and batch: 350, loss is 3.74739239692688 and perplexity is 42.41034833343704
At time: 75.88200187683105 and batch: 400, loss is 3.66443274974823 and perplexity is 39.033987842441796
At time: 76.48156571388245 and batch: 450, loss is 3.6902396965026854 and perplexity is 40.054446717521465
At time: 77.08132433891296 and batch: 500, loss is 3.5840779447555544 and perplexity is 36.02012985261301
At time: 77.68118095397949 and batch: 550, loss is 3.6230367374420167 and perplexity is 37.45112454596418
At time: 78.2809066772461 and batch: 600, loss is 3.63446635723114 and perplexity is 37.881632243199924
At time: 78.8971655368805 and batch: 650, loss is 3.485320219993591 and perplexity is 32.632875136003626
At time: 79.49700856208801 and batch: 700, loss is 3.4750101947784424 and perplexity is 32.2981578074952
At time: 80.09689402580261 and batch: 750, loss is 3.55637451171875 and perplexity is 35.035944202338236
At time: 80.69661402702332 and batch: 800, loss is 3.5177107286453246 and perplexity is 33.70717520011388
At time: 81.29656863212585 and batch: 850, loss is 3.5656542444229125 and perplexity is 35.36258160950288
At time: 81.90105724334717 and batch: 900, loss is 3.509995927810669 and perplexity is 33.44813157654304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329240354773116 and perplexity of 75.88661776107655
finished 7 epochs...
Completing Train Step...
At time: 83.40637254714966 and batch: 50, loss is 3.780275249481201 and perplexity is 43.828103738283815
At time: 84.01989412307739 and batch: 100, loss is 3.6629475784301757 and perplexity is 38.97605871125901
At time: 84.62336707115173 and batch: 150, loss is 3.6768998098373413 and perplexity is 39.52367303924519
At time: 85.223623752594 and batch: 200, loss is 3.562650318145752 and perplexity is 35.256514410160236
At time: 85.82365417480469 and batch: 250, loss is 3.698296055793762 and perplexity is 40.378443094683334
At time: 86.42427825927734 and batch: 300, loss is 3.6693145036697388 and perplexity is 39.225008043112275
At time: 87.0285792350769 and batch: 350, loss is 3.6685035753250124 and perplexity is 39.19321226605884
At time: 87.63232421875 and batch: 400, loss is 3.5905875062942503 and perplexity is 36.25536992871729
At time: 88.23580956459045 and batch: 450, loss is 3.6226100969314574 and perplexity is 37.43514978704883
At time: 88.83735394477844 and batch: 500, loss is 3.5215227460861205 and perplexity is 33.835912758870776
At time: 89.4405837059021 and batch: 550, loss is 3.5616098499298094 and perplexity is 35.21985020478936
At time: 90.04156994819641 and batch: 600, loss is 3.5787906408309937 and perplexity is 35.830183074359034
At time: 90.64357376098633 and batch: 650, loss is 3.436083269119263 and perplexity is 31.06504615031029
At time: 91.24498701095581 and batch: 700, loss is 3.4304517316818237 and perplexity is 30.890593858277057
At time: 91.84622049331665 and batch: 750, loss is 3.5180297470092774 and perplexity is 33.717930123417496
At time: 92.44658422470093 and batch: 800, loss is 3.484976110458374 and perplexity is 32.621647784337945
At time: 93.04667925834656 and batch: 850, loss is 3.539894289970398 and perplexity is 34.46327588437983
At time: 93.64672231674194 and batch: 900, loss is 3.4923198890686034 and perplexity is 32.86209576128796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333173777959118 and perplexity of 76.18569976531847
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 95.1480507850647 and batch: 50, loss is 3.755451488494873 and perplexity is 42.753518175301245
At time: 95.76522755622864 and batch: 100, loss is 3.647025074958801 and perplexity is 38.360376886686545
At time: 96.36806607246399 and batch: 150, loss is 3.6690495204925537 and perplexity is 39.21461545284751
At time: 96.9707579612732 and batch: 200, loss is 3.5500401401519777 and perplexity is 34.81471492750336
At time: 97.58332085609436 and batch: 250, loss is 3.6851250076293947 and perplexity is 39.85010370550279
At time: 98.20460534095764 and batch: 300, loss is 3.6468080568313597 and perplexity is 38.3520528927881
At time: 98.82365894317627 and batch: 350, loss is 3.642654314041138 and perplexity is 38.19307872706454
At time: 99.44471621513367 and batch: 400, loss is 3.5615246200561526 and perplexity is 35.216848549323366
At time: 100.04906797409058 and batch: 450, loss is 3.5858414125442506 and perplexity is 36.0837062323257
At time: 100.65344905853271 and batch: 500, loss is 3.4830725526809694 and perplexity is 32.55960965829807
At time: 101.25725245475769 and batch: 550, loss is 3.517031865119934 and perplexity is 33.68430039364096
At time: 101.86172461509705 and batch: 600, loss is 3.5410999250411987 and perplexity is 34.50485107565908
At time: 102.46515107154846 and batch: 650, loss is 3.3893046712875368 and perplexity is 29.64533185178805
At time: 103.06849336624146 and batch: 700, loss is 3.379050726890564 and perplexity is 29.342903459168205
At time: 103.6719286441803 and batch: 750, loss is 3.4609488105773925 and perplexity is 31.847179124537508
At time: 104.27523517608643 and batch: 800, loss is 3.423628373146057 and perplexity is 30.680533733772585
At time: 104.87921524047852 and batch: 850, loss is 3.4759377956390383 and perplexity is 32.3281315061382
At time: 105.48325324058533 and batch: 900, loss is 3.429045639038086 and perplexity is 30.84718934402758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321127695580051 and perplexity of 75.27346600238295
finished 9 epochs...
Completing Train Step...
At time: 107.01689720153809 and batch: 50, loss is 3.730312852859497 and perplexity is 41.69214963212361
At time: 107.62325239181519 and batch: 100, loss is 3.6135732173919677 and perplexity is 37.0983768280942
At time: 108.22977423667908 and batch: 150, loss is 3.632867259979248 and perplexity is 37.82110423716477
At time: 108.83629584312439 and batch: 200, loss is 3.517491202354431 and perplexity is 33.69977640111087
At time: 109.44471192359924 and batch: 250, loss is 3.652960710525513 and perplexity is 38.58874719505956
At time: 110.05032658576965 and batch: 300, loss is 3.617564444541931 and perplexity is 37.246740757028924
At time: 110.65726065635681 and batch: 350, loss is 3.613714823722839 and perplexity is 37.10363056509054
At time: 111.26279067993164 and batch: 400, loss is 3.5356691598892214 and perplexity is 34.31797124241318
At time: 111.86881613731384 and batch: 450, loss is 3.563101978302002 and perplexity is 35.2724419696188
At time: 112.47498440742493 and batch: 500, loss is 3.462320170402527 and perplexity is 31.89088302657125
At time: 113.07988548278809 and batch: 550, loss is 3.4982602071762083 and perplexity is 33.0578880221997
At time: 113.68597841262817 and batch: 600, loss is 3.5246216583251955 and perplexity is 33.940929918477906
At time: 114.2925488948822 and batch: 650, loss is 3.3753669834136963 and perplexity is 29.23501057681745
At time: 114.9103524684906 and batch: 700, loss is 3.3680753993988035 and perplexity is 29.02261632731067
At time: 115.51690554618835 and batch: 750, loss is 3.45296591758728 and perplexity is 31.593958562612375
At time: 116.1236481666565 and batch: 800, loss is 3.4186014461517336 and perplexity is 30.526691930268473
At time: 116.7303581237793 and batch: 850, loss is 3.4739981126785278 and perplexity is 32.265485956204785
At time: 117.3357048034668 and batch: 900, loss is 3.430018353462219 and perplexity is 30.877209448173623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321823329141695 and perplexity of 75.32584696850222
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 118.84895038604736 and batch: 50, loss is 3.7250617647171023 and perplexity is 41.473794282894644
At time: 119.44930124282837 and batch: 100, loss is 3.6146805667877198 and perplexity is 37.13948044709182
At time: 120.04962229728699 and batch: 150, loss is 3.633596634864807 and perplexity is 37.848700063367374
At time: 120.64852833747864 and batch: 200, loss is 3.5143016242980956 and perplexity is 33.592459572480784
At time: 121.24860715866089 and batch: 250, loss is 3.653213715553284 and perplexity is 38.5985115772821
At time: 121.85633730888367 and batch: 300, loss is 3.6163899946212767 and perplexity is 37.203022003070984
At time: 122.45595908164978 and batch: 350, loss is 3.609465093612671 and perplexity is 36.94628472456774
At time: 123.05475783348083 and batch: 400, loss is 3.5329099178314207 and perplexity is 34.223410171259395
At time: 123.66092777252197 and batch: 450, loss is 3.5560247182846068 and perplexity is 35.02369100226665
At time: 124.27337956428528 and batch: 500, loss is 3.450776424407959 and perplexity is 31.52485947942184
At time: 124.87327885627747 and batch: 550, loss is 3.4843396282196046 and perplexity is 32.60089129119416
At time: 125.47312569618225 and batch: 600, loss is 3.516207995414734 and perplexity is 33.65656034766659
At time: 126.0739324092865 and batch: 650, loss is 3.362351999282837 and perplexity is 28.85698272748185
At time: 126.67453908920288 and batch: 700, loss is 3.350616693496704 and perplexity is 28.520316515307698
At time: 127.27387714385986 and batch: 750, loss is 3.433253812789917 and perplexity is 30.977273192292564
At time: 127.87332487106323 and batch: 800, loss is 3.3973378419876097 and perplexity is 29.88443696327886
At time: 128.47388339042664 and batch: 850, loss is 3.4536128854751587 and perplexity is 31.61440545278082
At time: 129.07375049591064 and batch: 900, loss is 3.4082488203048706 and perplexity is 30.212290757266153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318127775845462 and perplexity of 75.04799002025364
finished 11 epochs...
Completing Train Step...
At time: 130.57060480117798 and batch: 50, loss is 3.7161323976516725 and perplexity is 41.10510806714266
At time: 131.18281507492065 and batch: 100, loss is 3.6014960289001463 and perplexity is 36.653027435830595
At time: 131.78223323822021 and batch: 150, loss is 3.6197809267044065 and perplexity is 37.329389053928985
At time: 132.38117933273315 and batch: 200, loss is 3.5032670068740845 and perplexity is 33.22381728663049
At time: 132.97942638397217 and batch: 250, loss is 3.641819543838501 and perplexity is 38.16120958655131
At time: 133.58183240890503 and batch: 300, loss is 3.6051764059066773 and perplexity is 36.7881729358732
At time: 134.19212460517883 and batch: 350, loss is 3.5984683561325075 and perplexity is 36.54222188892441
At time: 134.79209804534912 and batch: 400, loss is 3.522882399559021 and perplexity is 33.881949164849644
At time: 135.39217066764832 and batch: 450, loss is 3.54784300327301 and perplexity is 34.73830620435517
At time: 135.99066543579102 and batch: 500, loss is 3.4440332317352294 and perplexity is 31.31299639755735
At time: 136.58998918533325 and batch: 550, loss is 3.4787349796295164 and perplexity is 32.41868582753507
At time: 137.1890685558319 and batch: 600, loss is 3.511237382888794 and perplexity is 33.48968171532145
At time: 137.78857851028442 and batch: 650, loss is 3.358773512840271 and perplexity is 28.753902950809405
At time: 138.387127161026 and batch: 700, loss is 3.3480243825912477 and perplexity is 28.446478734369997
At time: 138.98602962493896 and batch: 750, loss is 3.432028193473816 and perplexity is 30.939330104548244
At time: 139.5855016708374 and batch: 800, loss is 3.3977749013900755 and perplexity is 29.897501092133275
At time: 140.18343114852905 and batch: 850, loss is 3.4558178329467775 and perplexity is 31.68419046401935
At time: 140.78256392478943 and batch: 900, loss is 3.41165744304657 and perplexity is 30.315448772117684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317641166791524 and perplexity of 75.01147987263452
finished 12 epochs...
Completing Train Step...
At time: 142.28125643730164 and batch: 50, loss is 3.710707082748413 and perplexity is 40.88270376395741
At time: 142.8922598361969 and batch: 100, loss is 3.5952787733078004 and perplexity is 36.42585312816704
At time: 143.49994778633118 and batch: 150, loss is 3.6129684495925902 and perplexity is 37.07594770726803
At time: 144.09997034072876 and batch: 200, loss is 3.496787815093994 and perplexity is 33.0092496657736
At time: 144.7046980857849 and batch: 250, loss is 3.635356135368347 and perplexity is 37.91535349136155
At time: 145.31793522834778 and batch: 300, loss is 3.598944444656372 and perplexity is 36.55962336339471
At time: 145.91918230056763 and batch: 350, loss is 3.592315225601196 and perplexity is 36.31806317389364
At time: 146.51934599876404 and batch: 400, loss is 3.517247505187988 and perplexity is 33.69156486169724
At time: 147.1195352077484 and batch: 450, loss is 3.542894568443298 and perplexity is 34.56683057789981
At time: 147.71918058395386 and batch: 500, loss is 3.4396060514450073 and perplexity is 31.1746745309239
At time: 148.31927585601807 and batch: 550, loss is 3.474840350151062 and perplexity is 32.2926726047466
At time: 148.91955542564392 and batch: 600, loss is 3.508029036521912 and perplexity is 33.382407395268444
At time: 149.51939821243286 and batch: 650, loss is 3.3561427068710326 and perplexity is 28.67835642898014
At time: 150.12032008171082 and batch: 700, loss is 3.3459393405914306 and perplexity is 28.38722842262158
At time: 150.72037172317505 and batch: 750, loss is 3.4306104707717897 and perplexity is 30.89549779224734
At time: 151.32032585144043 and batch: 800, loss is 3.39728467464447 and perplexity is 29.882848129401708
At time: 151.92049264907837 and batch: 850, loss is 3.4560006141662596 and perplexity is 31.689982268291065
At time: 152.52025818824768 and batch: 900, loss is 3.412326464653015 and perplexity is 30.335737248313027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317858552279538 and perplexity of 75.02778805230987
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 154.03103613853455 and batch: 50, loss is 3.7093471002578737 and perplexity is 40.827141792890146
At time: 154.63097476959229 and batch: 100, loss is 3.5962694311141967 and perplexity is 36.46195656404573
At time: 155.23049759864807 and batch: 150, loss is 3.6143408966064454 and perplexity is 37.126867415292594
At time: 155.82954049110413 and batch: 200, loss is 3.496423544883728 and perplexity is 32.99722756923588
At time: 156.42886519432068 and batch: 250, loss is 3.6346065330505373 and perplexity is 37.88694270423014
At time: 157.02811765670776 and batch: 300, loss is 3.5995144653320312 and perplexity is 36.580469045276544
At time: 157.62688326835632 and batch: 350, loss is 3.592231945991516 and perplexity is 36.31503874570654
At time: 158.2270348072052 and batch: 400, loss is 3.515980405807495 and perplexity is 33.64890133590486
At time: 158.82705950737 and batch: 450, loss is 3.5410779428482058 and perplexity is 34.50409259170014
At time: 159.42634773254395 and batch: 500, loss is 3.4360737562179566 and perplexity is 31.064750632997807
At time: 160.02582454681396 and batch: 550, loss is 3.4708662128448484 and perplexity is 32.164591763828
At time: 160.6377465724945 and batch: 600, loss is 3.505123267173767 and perplexity is 33.285546614789084
At time: 161.23724102973938 and batch: 650, loss is 3.350390839576721 and perplexity is 28.513875817379372
At time: 161.83688259124756 and batch: 700, loss is 3.3393502187728883 and perplexity is 28.200796402618217
At time: 162.43715238571167 and batch: 750, loss is 3.4235868501663207 and perplexity is 30.679259813040733
At time: 163.03800225257874 and batch: 800, loss is 3.389325704574585 and perplexity is 29.645955397120105
At time: 163.6371030807495 and batch: 850, loss is 3.4463480138778686 and perplexity is 31.385563118123052
At time: 164.2361934185028 and batch: 900, loss is 3.402997441291809 and perplexity is 30.054050421285687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317417510568279 and perplexity of 74.99470496432247
finished 14 epochs...
Completing Train Step...
At time: 165.75425505638123 and batch: 50, loss is 3.7059120082855226 and perplexity is 40.687137407502796
At time: 166.35472583770752 and batch: 100, loss is 3.592645764350891 and perplexity is 36.3300696852858
At time: 166.9545774459839 and batch: 150, loss is 3.6111537551879884 and perplexity is 37.008727203201715
At time: 167.5553846359253 and batch: 200, loss is 3.4937941598892213 and perplexity is 32.910579120192395
At time: 168.15968775749207 and batch: 250, loss is 3.6318358182907104 and perplexity is 37.78211408503892
At time: 168.77076840400696 and batch: 300, loss is 3.596630291938782 and perplexity is 36.47511663009075
At time: 169.372083902359 and batch: 350, loss is 3.589102683067322 and perplexity is 36.20157705968004
At time: 169.97229719161987 and batch: 400, loss is 3.5133519744873047 and perplexity is 33.56057364223391
At time: 170.57303881645203 and batch: 450, loss is 3.538747272491455 and perplexity is 34.42376856668228
At time: 171.172922372818 and batch: 500, loss is 3.434343123435974 and perplexity is 31.011035451222277
At time: 171.77315473556519 and batch: 550, loss is 3.4691810178756715 and perplexity is 32.110433801781404
At time: 172.37385082244873 and batch: 600, loss is 3.503854169845581 and perplexity is 33.24333081016158
At time: 172.97370147705078 and batch: 650, loss is 3.349894118309021 and perplexity is 28.49971588589703
At time: 173.57471680641174 and batch: 700, loss is 3.3387255573272707 and perplexity is 28.183185953226815
At time: 174.17562222480774 and batch: 750, loss is 3.423559765815735 and perplexity is 30.67842889646471
At time: 174.77637147903442 and batch: 800, loss is 3.389640154838562 and perplexity is 29.65527904145497
At time: 175.39355635643005 and batch: 850, loss is 3.4476068782806397 and perplexity is 31.42509816570435
At time: 176.0021572113037 and batch: 900, loss is 3.4046805810928347 and perplexity is 30.104678184581307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317168771404109 and perplexity of 74.97605316390526
finished 15 epochs...
Completing Train Step...
At time: 177.68184542655945 and batch: 50, loss is 3.7040070486068726 and perplexity is 40.60970382864094
At time: 178.28590273857117 and batch: 100, loss is 3.590510091781616 and perplexity is 36.25256334556043
At time: 178.89030265808105 and batch: 150, loss is 3.6091313934326172 and perplexity is 36.93395779956576
At time: 179.4946026802063 and batch: 200, loss is 3.4919162130355836 and perplexity is 32.84883279798464
At time: 180.09879684448242 and batch: 250, loss is 3.629918727874756 and perplexity is 37.70975174097252
At time: 180.70257949829102 and batch: 300, loss is 3.5947086811065674 and perplexity is 36.40509295154507
At time: 181.30589175224304 and batch: 350, loss is 3.587167510986328 and perplexity is 36.13158852024157
At time: 181.9084393978119 and batch: 400, loss is 3.511635103225708 and perplexity is 33.50300389189108
At time: 182.51238441467285 and batch: 450, loss is 3.537194542884827 and perplexity is 34.37035923791078
At time: 183.12928748130798 and batch: 500, loss is 3.4331404113769532 and perplexity is 30.973760524917285
At time: 183.73369240760803 and batch: 550, loss is 3.468076753616333 and perplexity is 32.07499496790224
At time: 184.33690810203552 and batch: 600, loss is 3.5029842901229857 and perplexity is 33.21442568459228
At time: 184.94130945205688 and batch: 650, loss is 3.3493613815307617 and perplexity is 28.484537082587043
At time: 185.54584002494812 and batch: 700, loss is 3.338266191482544 and perplexity is 28.170242533314436
At time: 186.14955949783325 and batch: 750, loss is 3.423450984954834 and perplexity is 30.675091852064842
At time: 186.7530312538147 and batch: 800, loss is 3.3897817659378053 and perplexity is 29.65947885548152
At time: 187.35691666603088 and batch: 850, loss is 3.4482168769836425 and perplexity is 31.44427328263081
At time: 187.96090817451477 and batch: 900, loss is 3.405475072860718 and perplexity is 30.12860560738534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31711317088506 and perplexity of 74.97188457232208
finished 16 epochs...
Completing Train Step...
At time: 189.48548436164856 and batch: 50, loss is 3.702474374771118 and perplexity is 40.54751007165198
At time: 190.09157848358154 and batch: 100, loss is 3.588820610046387 and perplexity is 36.191367011533465
At time: 190.69627356529236 and batch: 150, loss is 3.607462077140808 and perplexity is 36.8723547738625
At time: 191.31468176841736 and batch: 200, loss is 3.4903076219558717 and perplexity is 32.79603493503737
At time: 191.92110466957092 and batch: 250, loss is 3.6282992696762086 and perplexity is 37.648731797310575
At time: 192.52722239494324 and batch: 300, loss is 3.593117094039917 and perplexity is 36.347197161759865
At time: 193.1339271068573 and batch: 350, loss is 3.585588421821594 and perplexity is 36.07457854406877
At time: 193.74061012268066 and batch: 400, loss is 3.510208258628845 and perplexity is 33.455234399734586
At time: 194.34742259979248 and batch: 450, loss is 3.5359123659133913 and perplexity is 34.32631859477867
At time: 194.96166348457336 and batch: 500, loss is 3.4320837116241454 and perplexity is 30.94104784661057
At time: 195.5719599723816 and batch: 550, loss is 3.4671217918396 and perplexity is 32.04437919449235
At time: 196.17763328552246 and batch: 600, loss is 3.5022232294082642 and perplexity is 33.18915710672233
At time: 196.78419160842896 and batch: 650, loss is 3.3488062810897827 and perplexity is 28.468729691242274
At time: 197.39018416404724 and batch: 700, loss is 3.3378117179870603 and perplexity is 28.157442813508712
At time: 197.99652433395386 and batch: 750, loss is 3.4232308769226076 and perplexity is 30.668340760970864
At time: 198.60326766967773 and batch: 800, loss is 3.389779863357544 and perplexity is 29.65942242599617
At time: 199.20852994918823 and batch: 850, loss is 3.4484986019134523 and perplexity is 31.453133166280725
At time: 199.8148970603943 and batch: 900, loss is 3.4058769750595093 and perplexity is 30.14071679381867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317146614806293 and perplexity of 74.97439196805277
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 201.36453652381897 and batch: 50, loss is 3.701921372413635 and perplexity is 40.52509340179919
At time: 201.96426129341125 and batch: 100, loss is 3.588846640586853 and perplexity is 36.19230910463853
At time: 202.5846917629242 and batch: 150, loss is 3.607911777496338 and perplexity is 36.88894001382819
At time: 203.20444798469543 and batch: 200, loss is 3.4905025577545166 and perplexity is 32.802428679464406
At time: 203.8184950351715 and batch: 250, loss is 3.627987308502197 and perplexity is 37.636988686531595
At time: 204.42528676986694 and batch: 300, loss is 3.593135747909546 and perplexity is 36.347875183960944
At time: 205.03133249282837 and batch: 350, loss is 3.585809464454651 and perplexity is 36.08255344526068
At time: 205.6294777393341 and batch: 400, loss is 3.5099810361862183 and perplexity is 33.44763348323774
At time: 206.2270085811615 and batch: 450, loss is 3.535550241470337 and perplexity is 34.313890446179435
At time: 206.82411742210388 and batch: 500, loss is 3.4310080194473267 and perplexity is 30.90778269823696
At time: 207.4229428768158 and batch: 550, loss is 3.465701451301575 and perplexity is 31.998897571049422
At time: 208.0207748413086 and batch: 600, loss is 3.5003379487991335 and perplexity is 33.12664517717536
At time: 208.6240303516388 and batch: 650, loss is 3.346536545753479 and perplexity is 28.40418648518905
At time: 209.23363065719604 and batch: 700, loss is 3.3358033895492554 and perplexity is 28.100950167252517
At time: 209.83041191101074 and batch: 750, loss is 3.421035690307617 and perplexity is 30.60109186876759
At time: 210.42835140228271 and batch: 800, loss is 3.3869826793670654 and perplexity is 29.576575487502183
At time: 211.02549076080322 and batch: 850, loss is 3.4452996349334715 and perplexity is 31.35267639647737
At time: 211.62347674369812 and batch: 900, loss is 3.4026345682144163 and perplexity is 30.043146593993534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316785002407962 and perplexity of 74.94728519972692
finished 18 epochs...
Completing Train Step...
At time: 213.14552474021912 and batch: 50, loss is 3.701155481338501 and perplexity is 40.49406747719978
At time: 213.76078081130981 and batch: 100, loss is 3.5881475257873534 and perplexity is 36.167015368358314
At time: 214.36046028137207 and batch: 150, loss is 3.6071730709075926 and perplexity is 36.86169997322559
At time: 214.9601547718048 and batch: 200, loss is 3.4898752641677855 and perplexity is 32.78185837880794
At time: 215.55829071998596 and batch: 250, loss is 3.6273615312576295 and perplexity is 37.613443683189516
At time: 216.15642929077148 and batch: 300, loss is 3.592407531738281 and perplexity is 36.32141570873902
At time: 216.75594854354858 and batch: 350, loss is 3.585121121406555 and perplexity is 36.057724816726356
At time: 217.35549306869507 and batch: 400, loss is 3.509369854927063 and perplexity is 33.42719716227403
At time: 217.97319841384888 and batch: 450, loss is 3.5350195455551146 and perplexity is 34.295685035879984
At time: 218.57490301132202 and batch: 500, loss is 3.4306294107437134 and perplexity is 30.896082957649586
At time: 219.17553806304932 and batch: 550, loss is 3.4653939723968508 and perplexity is 31.98906009755719
At time: 219.77507758140564 and batch: 600, loss is 3.5002453470230104 and perplexity is 33.12357773302253
At time: 220.37495374679565 and batch: 650, loss is 3.3465039491653443 and perplexity is 28.40326062071099
At time: 220.97454714775085 and batch: 700, loss is 3.3356447267532348 and perplexity is 28.09649194561445
At time: 221.59055495262146 and batch: 750, loss is 3.4210211706161497 and perplexity is 30.600647553580757
At time: 222.19113874435425 and batch: 800, loss is 3.3871904516220095 and perplexity is 29.58272131772914
At time: 222.79056239128113 and batch: 850, loss is 3.445581555366516 and perplexity is 31.361516602643
At time: 223.39096903800964 and batch: 900, loss is 3.403059720993042 and perplexity is 30.055922236854222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316611930115582 and perplexity of 74.93431502369116
finished 19 epochs...
Completing Train Step...
At time: 224.90170669555664 and batch: 50, loss is 3.7005710315704348 and perplexity is 40.4704076435203
At time: 225.51332640647888 and batch: 100, loss is 3.5875658273696898 and perplexity is 36.14598319053226
At time: 226.11137056350708 and batch: 150, loss is 3.6065897607803343 and perplexity is 36.84020444021379
At time: 226.71121406555176 and batch: 200, loss is 3.4893540477752687 and perplexity is 32.76477638893703
At time: 227.3109908103943 and batch: 250, loss is 3.626849775314331 and perplexity is 37.59419970436697
At time: 227.9107208251953 and batch: 300, loss is 3.5918714952468873 and perplexity is 36.30195132177697
At time: 228.51046586036682 and batch: 350, loss is 3.5845785617828367 and perplexity is 36.03816665733124
At time: 229.10989546775818 and batch: 400, loss is 3.508898072242737 and perplexity is 33.411430508976764
At time: 229.72541451454163 and batch: 450, loss is 3.5346008586883544 and perplexity is 34.281328888543804
At time: 230.32737565040588 and batch: 500, loss is 3.4303137397766115 and perplexity is 30.88633150046962
At time: 230.926331281662 and batch: 550, loss is 3.4651339197158815 and perplexity is 31.980742338291805
At time: 231.52696418762207 and batch: 600, loss is 3.5001136922836302 and perplexity is 33.119217144081155
At time: 232.12642407417297 and batch: 650, loss is 3.3464424228668213 and perplexity is 28.401513126977974
At time: 232.74637055397034 and batch: 700, loss is 3.3355304288864134 and perplexity is 28.093280760039228
At time: 233.3453722000122 and batch: 750, loss is 3.4210199069976808 and perplexity is 30.600608886061778
At time: 233.9434404373169 and batch: 800, loss is 3.387301859855652 and perplexity is 29.58601726005163
At time: 234.544038772583 and batch: 850, loss is 3.4457945728302004 and perplexity is 31.3681978649545
At time: 235.14422845840454 and batch: 900, loss is 3.4033624505996705 and perplexity is 30.06502243174955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316518705185145 and perplexity of 74.9273296029985
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f39b9e46b70>
ELAPSED
1456.8295028209686


RESULTS SO FAR:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.9701938996608, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.11641594496695, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5835303149491065, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7493984943579154, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.36975667692482, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.9121007976102171, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.8055971500675865, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -74.63518442711383, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5989062690656494, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.730432796799109, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -74.9273296029985, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.606213139937303, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7214203749085344, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -75.11644734731726, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.8013891730739058, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7228941564734824, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.9701938996608, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.10560296698712279, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.862953234225491, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.11641594496695, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5835303149491065, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7493984943579154, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -75.36975667692482, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.9121007976102171, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.8055971500675865, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -74.63518442711383, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.5989062690656494, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.730432796799109, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}, {'best_accuracy': -74.9273296029985, 'params': {'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'dropout': 0.606213139937303, 'tie_weights': 'FALSE', 'data': 'ptb', 'seq_len': 35, 'rnn_dropout': 0.7214203749085344, 'wordvec_source': 'gigavec', 'num_layers': 1, 'batch_size': 32}}]
