TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'rnn_dropout', 'domain': [0, 1], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9332079887390137 and batch: 50, loss is 6.672626104354858 and perplexity is 790.4687345293792
At time: 1.4012980461120605 and batch: 100, loss is 5.930791244506836 and perplexity is 376.4522617977145
At time: 1.8697459697723389 and batch: 150, loss is 5.781389255523681 and perplexity is 324.2092872382281
At time: 2.3372890949249268 and batch: 200, loss is 5.6096954441070555 and perplexity is 273.06106298381286
At time: 2.8050692081451416 and batch: 250, loss is 5.644877872467041 and perplexity is 282.839011682683
At time: 3.271832227706909 and batch: 300, loss is 5.556375856399536 and perplexity is 258.8829053047159
At time: 3.7415695190429688 and batch: 350, loss is 5.5306904506683345 and perplexity is 252.3180640705207
At time: 4.2104246616363525 and batch: 400, loss is 5.390001888275147 and perplexity is 219.20379947065032
At time: 4.678786277770996 and batch: 450, loss is 5.384672765731811 and perplexity is 218.038742683756
At time: 5.1458580493927 and batch: 500, loss is 5.324809150695801 and perplexity is 205.36916216271507
At time: 5.613146066665649 and batch: 550, loss is 5.376338062286377 and perplexity is 216.22900670665433
At time: 6.078946113586426 and batch: 600, loss is 5.302416906356812 and perplexity is 200.82159088336826
At time: 6.5461061000823975 and batch: 650, loss is 5.1967909717559815 and perplexity is 180.69146649181687
At time: 7.014196395874023 and batch: 700, loss is 5.2829665851593015 and perplexity is 196.9532882539129
At time: 7.480358839035034 and batch: 750, loss is 5.279707288742065 and perplexity is 196.31240409021657
At time: 7.946418046951294 and batch: 800, loss is 5.249905233383179 and perplexity is 190.54820999377122
At time: 8.413504123687744 and batch: 850, loss is 5.283944339752197 and perplexity is 197.1459544108498
At time: 8.881303787231445 and batch: 900, loss is 5.191885452270508 and perplexity is 179.80725152211812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.028138043129281 and perplexity of 152.64852295786832
finished 1 epochs...
Completing Train Step...
At time: 10.136265993118286 and batch: 50, loss is 4.969416570663452 and perplexity is 143.94288236745032
At time: 10.596989393234253 and batch: 100, loss is 4.823686971664428 and perplexity is 124.42299016922995
At time: 11.057432174682617 and batch: 150, loss is 4.797648992538452 and perplexity is 121.22508116586661
At time: 11.518510580062866 and batch: 200, loss is 4.683859701156616 and perplexity is 108.18683662010547
At time: 11.982446432113647 and batch: 250, loss is 4.781165676116943 and perplexity is 119.24326809207994
At time: 12.443119764328003 and batch: 300, loss is 4.7265667152404784 and perplexity is 112.90725339017214
At time: 12.903470754623413 and batch: 350, loss is 4.7076286029815675 and perplexity is 110.7891231503631
At time: 13.364577531814575 and batch: 400, loss is 4.585567684173584 and perplexity is 98.05883779491296
At time: 13.832701683044434 and batch: 450, loss is 4.603603496551513 and perplexity is 99.84345371806576
At time: 14.305745601654053 and batch: 500, loss is 4.5093121910095215 and perplexity is 90.8593031680992
At time: 14.769362926483154 and batch: 550, loss is 4.588335494995118 and perplexity is 98.3306220575138
At time: 15.230014562606812 and batch: 600, loss is 4.5487779521942135 and perplexity is 94.51683362127021
At time: 15.714695692062378 and batch: 650, loss is 4.410204906463623 and perplexity is 82.28632277626038
At time: 16.17550754547119 and batch: 700, loss is 4.446966581344604 and perplexity is 85.36759518849391
At time: 16.636653900146484 and batch: 750, loss is 4.508843765258789 and perplexity is 90.81675229754148
At time: 17.097442626953125 and batch: 800, loss is 4.459072074890137 and perplexity is 86.40729238825405
At time: 17.559208393096924 and batch: 850, loss is 4.518542928695679 and perplexity is 91.70188440239153
At time: 18.020812511444092 and batch: 900, loss is 4.453465204238892 and perplexity is 85.92417353461948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5848104398544525 and perplexity of 97.98461140435533
finished 2 epochs...
Completing Train Step...
At time: 19.257365703582764 and batch: 50, loss is 4.4868742370605466 and perplexity is 88.84330828856568
At time: 19.731866121292114 and batch: 100, loss is 4.3629205560684206 and perplexity is 78.48602284996808
At time: 20.192727088928223 and batch: 150, loss is 4.358930568695069 and perplexity is 78.17348852843917
At time: 20.653871774673462 and batch: 200, loss is 4.2661286020278935 and perplexity is 71.24528218634
At time: 21.114686965942383 and batch: 250, loss is 4.3986063480377195 and perplexity is 81.33743356490872
At time: 21.575796842575073 and batch: 300, loss is 4.370237903594971 and perplexity is 79.06243870074321
At time: 22.036662101745605 and batch: 350, loss is 4.360283451080322 and perplexity is 78.27931963644221
At time: 22.498835563659668 and batch: 400, loss is 4.271697883605957 and perplexity is 71.64317418177706
At time: 22.96167016029358 and batch: 450, loss is 4.302891273498535 and perplexity is 73.91318830239149
At time: 23.424140453338623 and batch: 500, loss is 4.193141570091248 and perplexity is 66.23053233201378
At time: 23.885016441345215 and batch: 550, loss is 4.280353531837464 and perplexity is 72.26598381782942
At time: 24.348458290100098 and batch: 600, loss is 4.273006405830383 and perplexity is 71.73698222899456
At time: 24.813958406448364 and batch: 650, loss is 4.129396958351135 and perplexity is 62.14043836110675
At time: 25.276780128479004 and batch: 700, loss is 4.149007902145386 and perplexity is 63.37109876991285
At time: 25.738396883010864 and batch: 750, loss is 4.243888602256775 and perplexity is 69.67827681487645
At time: 26.201051712036133 and batch: 800, loss is 4.199498991966248 and perplexity is 66.65292902137095
At time: 26.66473388671875 and batch: 850, loss is 4.270360832214355 and perplexity is 71.54744758598852
At time: 27.15011167526245 and batch: 900, loss is 4.21594575881958 and perplexity is 67.75821850973809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.47206680088827 and perplexity of 87.53745867413674
finished 3 epochs...
Completing Train Step...
At time: 28.3833224773407 and batch: 50, loss is 4.279888429641724 and perplexity is 72.2323805651575
At time: 28.842604160308838 and batch: 100, loss is 4.157428102493286 and perplexity is 63.90694893472662
At time: 29.301460027694702 and batch: 150, loss is 4.157139687538147 and perplexity is 63.88851987265285
At time: 29.76104211807251 and batch: 200, loss is 4.0690656709671025 and perplexity is 58.50227667296361
At time: 30.22196674346924 and batch: 250, loss is 4.211123676300049 and perplexity is 67.43226929730116
At time: 30.682576417922974 and batch: 300, loss is 4.187878112792969 and perplexity is 65.88284657051106
At time: 31.14510941505432 and batch: 350, loss is 4.176198430061341 and perplexity is 65.11783208494958
At time: 31.607417821884155 and batch: 400, loss is 4.102899484634399 and perplexity is 60.51549722014118
At time: 32.06877112388611 and batch: 450, loss is 4.141192593574524 and perplexity is 62.8777643698643
At time: 32.52854943275452 and batch: 500, loss is 4.02397087097168 and perplexity is 55.92272745525192
At time: 32.98740911483765 and batch: 550, loss is 4.111031470298767 and perplexity is 61.00961473130315
At time: 33.44576334953308 and batch: 600, loss is 4.114731631278992 and perplexity is 61.235778289900495
At time: 33.904273986816406 and batch: 650, loss is 3.9705388879776002 and perplexity is 53.013091261087155
At time: 34.36342239379883 and batch: 700, loss is 3.980995764732361 and perplexity is 53.570351143902435
At time: 34.822373390197754 and batch: 750, loss is 4.087364444732666 and perplexity is 59.58265121690957
At time: 35.28207755088806 and batch: 800, loss is 4.047224793434143 and perplexity is 57.238388072511974
At time: 35.74175763130188 and batch: 850, loss is 4.118716640472412 and perplexity is 61.48029029707336
At time: 36.20148038864136 and batch: 900, loss is 4.069944453239441 and perplexity is 58.55371003264762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43150392297196 and perplexity of 84.05773826121083
finished 4 epochs...
Completing Train Step...
At time: 37.43938851356506 and batch: 50, loss is 4.144618434906006 and perplexity is 63.09354301453498
At time: 37.89913296699524 and batch: 100, loss is 4.0252976942062375 and perplexity is 55.99697627601698
At time: 38.35940456390381 and batch: 150, loss is 4.026822137832641 and perplexity is 56.08240560915574
At time: 38.818724632263184 and batch: 200, loss is 3.941192455291748 and perplexity is 51.47995225560769
At time: 39.29084515571594 and batch: 250, loss is 4.084881176948548 and perplexity is 59.43487509842732
At time: 39.75088405609131 and batch: 300, loss is 4.068460736274719 and perplexity is 58.466897318399134
At time: 40.210429191589355 and batch: 350, loss is 4.051321640014648 and perplexity is 57.47336597250486
At time: 40.67036724090576 and batch: 400, loss is 3.9859163188934326 and perplexity is 53.83459654187421
At time: 41.13020586967468 and batch: 450, loss is 4.030822954177856 and perplexity is 56.30723045576765
At time: 41.590532302856445 and batch: 500, loss is 3.910029149055481 and perplexity is 49.90040650192634
At time: 42.049379110336304 and batch: 550, loss is 3.994276041984558 and perplexity is 54.28652522911237
At time: 42.50902271270752 and batch: 600, loss is 4.00408676147461 and perplexity is 54.82173620978361
At time: 42.96799302101135 and batch: 650, loss is 3.859074993133545 and perplexity is 47.421465893230945
At time: 43.42667818069458 and batch: 700, loss is 3.869233498573303 and perplexity is 47.90565225355305
At time: 43.88591527938843 and batch: 750, loss is 3.9780469846725466 and perplexity is 53.41261663713196
At time: 44.34645056724548 and batch: 800, loss is 3.938290491104126 and perplexity is 51.33077583478634
At time: 44.80569505691528 and batch: 850, loss is 4.012081408500672 and perplexity is 55.261773266889506
At time: 45.264928579330444 and batch: 900, loss is 3.966800503730774 and perplexity is 52.81527793732639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.409292142685145 and perplexity of 82.21124906885875
finished 5 epochs...
Completing Train Step...
At time: 46.4769184589386 and batch: 50, loss is 4.046711640357971 and perplexity is 57.2090235524887
At time: 46.94902491569519 and batch: 100, loss is 3.930486350059509 and perplexity is 50.931742302472564
At time: 47.40734386444092 and batch: 150, loss is 3.930678086280823 and perplexity is 50.941508698542584
At time: 47.86526942253113 and batch: 200, loss is 3.8477481698989866 and perplexity is 46.88736189264927
At time: 48.323413610458374 and batch: 250, loss is 3.9913624048233034 and perplexity is 54.12858419501635
At time: 48.78205490112305 and batch: 300, loss is 3.9800542879104612 and perplexity is 53.51993963431697
At time: 49.24047088623047 and batch: 350, loss is 3.962900824546814 and perplexity is 52.609716369948146
At time: 49.69971942901611 and batch: 400, loss is 3.89733606338501 and perplexity is 49.27101925061413
At time: 50.15806436538696 and batch: 450, loss is 3.944663100242615 and perplexity is 51.658931298674766
At time: 50.616562604904175 and batch: 500, loss is 3.8256365728378294 and perplexity is 45.86198557039687
At time: 51.09751892089844 and batch: 550, loss is 3.9059968900680544 and perplexity is 49.69960026279148
At time: 51.55541777610779 and batch: 600, loss is 3.9187334823608397 and perplexity is 50.336652131515905
At time: 52.03083372116089 and batch: 650, loss is 3.775877356529236 and perplexity is 43.635775658899696
At time: 52.49948692321777 and batch: 700, loss is 3.7827251529693604 and perplexity is 43.93560999872158
At time: 52.96142625808716 and batch: 750, loss is 3.8930277156829836 and perplexity is 49.0591991929264
At time: 53.42406129837036 and batch: 800, loss is 3.856808362007141 and perplexity is 47.314100647248665
At time: 53.884262561798096 and batch: 850, loss is 3.92827347278595 and perplexity is 50.819161217431734
At time: 54.34285855293274 and batch: 900, loss is 3.887263159751892 and perplexity is 48.77720825240053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.403835766936002 and perplexity of 81.76389517847781
finished 6 epochs...
Completing Train Step...
At time: 55.55778169631958 and batch: 50, loss is 3.969775857925415 and perplexity is 52.97265610787607
At time: 56.02804183959961 and batch: 100, loss is 3.8555276155471803 and perplexity is 47.25354206870892
At time: 56.48524880409241 and batch: 150, loss is 3.858883500099182 and perplexity is 47.4123858822403
At time: 56.95158815383911 and batch: 200, loss is 3.7746029233932497 and perplexity is 43.58020020160974
At time: 57.410616636276245 and batch: 250, loss is 3.917866153717041 and perplexity is 50.293012638915954
At time: 57.868072271347046 and batch: 300, loss is 3.9097209310531618 and perplexity is 49.88502866830372
At time: 58.32443070411682 and batch: 350, loss is 3.8930304193496705 and perplexity is 49.05933183282825
At time: 58.781362533569336 and batch: 400, loss is 3.8256912183761598 and perplexity is 45.86449179176355
At time: 59.23932242393494 and batch: 450, loss is 3.8748489379882813 and perplexity is 48.17542026549536
At time: 59.69842171669006 and batch: 500, loss is 3.7579096221923827 and perplexity is 42.85874131230092
At time: 60.15705060958862 and batch: 550, loss is 3.83680682182312 and perplexity is 46.377147253739274
At time: 60.617177963256836 and batch: 600, loss is 3.8510719442367556 and perplexity is 47.0434641835567
At time: 61.07610082626343 and batch: 650, loss is 3.710584354400635 and perplexity is 40.87768660515186
At time: 61.53523063659668 and batch: 700, loss is 3.714976849555969 and perplexity is 41.05763657087291
At time: 61.99395251274109 and batch: 750, loss is 3.826188912391663 and perplexity is 45.88732395609688
At time: 62.466100215911865 and batch: 800, loss is 3.789164423942566 and perplexity is 44.21943613285169
At time: 62.925524950027466 and batch: 850, loss is 3.861874976158142 and perplexity is 47.55443125624352
At time: 63.38441443443298 and batch: 900, loss is 3.824324746131897 and perplexity is 45.80186203738882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404823198710402 and perplexity of 81.8446713204772
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 64.62256574630737 and batch: 50, loss is 3.9225131177902224 and perplexity is 50.52726632446671
At time: 65.0813102722168 and batch: 100, loss is 3.7934115362167358 and perplexity is 44.407640422280494
At time: 65.54017925262451 and batch: 150, loss is 3.8011114168167115 and perplexity is 44.75089376021426
At time: 65.99786424636841 and batch: 200, loss is 3.6921665239334107 and perplexity is 40.13169912630847
At time: 66.45568227767944 and batch: 250, loss is 3.833390598297119 and perplexity is 46.21898286849169
At time: 66.91768455505371 and batch: 300, loss is 3.8120433378219603 and perplexity is 45.242790786685745
At time: 67.38131785392761 and batch: 350, loss is 3.777323355674744 and perplexity is 43.69891859452933
At time: 67.84472894668579 and batch: 400, loss is 3.708711919784546 and perplexity is 40.801217423839624
At time: 68.30768418312073 and batch: 450, loss is 3.7440995883941652 and perplexity is 42.27092884338486
At time: 68.77027487754822 and batch: 500, loss is 3.6197779464721678 and perplexity is 37.32927780384605
At time: 69.23281979560852 and batch: 550, loss is 3.6849914169311524 and perplexity is 39.844780457899844
At time: 69.69652605056763 and batch: 600, loss is 3.6941552782058715 and perplexity is 40.21159063036017
At time: 70.15888833999634 and batch: 650, loss is 3.5414384412765503 and perplexity is 34.51653350518105
At time: 70.621990442276 and batch: 700, loss is 3.5316738271713257 and perplexity is 34.18113306813926
At time: 71.08745741844177 and batch: 750, loss is 3.6329979419708254 and perplexity is 37.82604709735451
At time: 71.54947113990784 and batch: 800, loss is 3.583884334564209 and perplexity is 36.01315666344199
At time: 72.01231741905212 and batch: 850, loss is 3.642331085205078 and perplexity is 38.18073561761397
At time: 72.47574639320374 and batch: 900, loss is 3.592551007270813 and perplexity is 36.32662731706035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331966034353596 and perplexity of 76.09374251516286
finished 8 epochs...
Completing Train Step...
At time: 73.71313977241516 and batch: 50, loss is 3.8331919002532957 and perplexity is 46.20980015933161
At time: 74.17631840705872 and batch: 100, loss is 3.713060669898987 and perplexity is 40.97903809136524
At time: 74.65230751037598 and batch: 150, loss is 3.722161641120911 and perplexity is 41.35368939715847
At time: 75.11368560791016 and batch: 200, loss is 3.621904730796814 and perplexity is 37.408753610722606
At time: 75.5745906829834 and batch: 250, loss is 3.764395332336426 and perplexity is 43.13761405191779
At time: 76.03679490089417 and batch: 300, loss is 3.749837818145752 and perplexity is 42.514186411324175
At time: 76.51134419441223 and batch: 350, loss is 3.7178973245620726 and perplexity is 41.1777196367333
At time: 76.98104190826416 and batch: 400, loss is 3.654817109107971 and perplexity is 38.66044982438914
At time: 77.45381498336792 and batch: 450, loss is 3.696159062385559 and perplexity is 40.292246761257694
At time: 77.92635893821716 and batch: 500, loss is 3.5730871963500976 and perplexity is 35.62640927311937
At time: 78.38970375061035 and batch: 550, loss is 3.641452765464783 and perplexity is 38.14721544669085
At time: 78.85144209861755 and batch: 600, loss is 3.656003084182739 and perplexity is 38.706327353685936
At time: 79.31206893920898 and batch: 650, loss is 3.508819131851196 and perplexity is 33.4087931016708
At time: 79.7728066444397 and batch: 700, loss is 3.502802267074585 and perplexity is 33.208380443781564
At time: 80.23703789710999 and batch: 750, loss is 3.6101048374176026 and perplexity is 36.969928443490474
At time: 80.70010662078857 and batch: 800, loss is 3.5654290819168093 and perplexity is 35.35462017834694
At time: 81.16350865364075 and batch: 850, loss is 3.6301027011871336 and perplexity is 37.7166899671139
At time: 81.62680768966675 and batch: 900, loss is 3.5871354389190673 and perplexity is 36.130429724086916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331171741224315 and perplexity of 76.03332577577872
finished 9 epochs...
Completing Train Step...
At time: 82.86950731277466 and batch: 50, loss is 3.794551439285278 and perplexity is 44.458289690008044
At time: 83.34492182731628 and batch: 100, loss is 3.675052275657654 and perplexity is 39.45071911558379
At time: 83.80817985534668 and batch: 150, loss is 3.6845251750946044 and perplexity is 39.8262074843674
At time: 84.27215385437012 and batch: 200, loss is 3.586428656578064 and perplexity is 36.10490239657988
At time: 84.73416376113892 and batch: 250, loss is 3.7291665363311766 and perplexity is 41.64438461404439
At time: 85.1975646018982 and batch: 300, loss is 3.7174512052536013 and perplexity is 41.15935355796032
At time: 85.66053438186646 and batch: 350, loss is 3.685785174369812 and perplexity is 39.87642010422116
At time: 86.12396621704102 and batch: 400, loss is 3.624729800224304 and perplexity is 37.51458535749783
At time: 86.62639856338501 and batch: 450, loss is 3.667995185852051 and perplexity is 39.17329191360903
At time: 87.09483194351196 and batch: 500, loss is 3.5456253814697267 and perplexity is 34.66135513482817
At time: 87.5585572719574 and batch: 550, loss is 3.615125217437744 and perplexity is 37.15599821325977
At time: 88.0212812423706 and batch: 600, loss is 3.6317539596557618 and perplexity is 37.779021419336864
At time: 88.48419427871704 and batch: 650, loss is 3.4873155307769776 and perplexity is 32.69805286694594
At time: 88.94462704658508 and batch: 700, loss is 3.4826207256317137 and perplexity is 32.54490166892518
At time: 89.40626740455627 and batch: 750, loss is 3.592784814834595 and perplexity is 36.33512175028643
At time: 89.86835503578186 and batch: 800, loss is 3.549348831176758 and perplexity is 34.79065551980467
At time: 90.33144402503967 and batch: 850, loss is 3.616613073348999 and perplexity is 37.21132213164349
At time: 90.79289245605469 and batch: 900, loss is 3.5759447526931765 and perplexity is 35.72835933957458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332611502033391 and perplexity of 76.14287442141398
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 92.02679705619812 and batch: 50, loss is 3.7842312908172606 and perplexity is 44.00183294173304
At time: 92.50131964683533 and batch: 100, loss is 3.6662258768081664 and perplexity is 39.104043532905585
At time: 92.96104097366333 and batch: 150, loss is 3.681285834312439 and perplexity is 39.697405555564764
At time: 93.4240128993988 and batch: 200, loss is 3.571483178138733 and perplexity is 35.569309670483506
At time: 93.88653063774109 and batch: 250, loss is 3.712631049156189 and perplexity is 40.96143642787189
At time: 94.3488609790802 and batch: 300, loss is 3.6988088369369505 and perplexity is 40.39915370844589
At time: 94.8116307258606 and batch: 350, loss is 3.6603262996673585 and perplexity is 38.87402538362173
At time: 95.27383685112 and batch: 400, loss is 3.6034926319122316 and perplexity is 36.72628208670397
At time: 95.7383713722229 and batch: 450, loss is 3.635466332435608 and perplexity is 37.919531882339456
At time: 96.20409560203552 and batch: 500, loss is 3.5113108062744143 and perplexity is 33.49214073140988
At time: 96.66758489608765 and batch: 550, loss is 3.5744633197784426 and perplexity is 35.675469358212005
At time: 97.1330304145813 and batch: 600, loss is 3.5927873611450196 and perplexity is 36.33521427090352
At time: 97.59767246246338 and batch: 650, loss is 3.4420407104492186 and perplexity is 31.250666702949534
At time: 98.08467483520508 and batch: 700, loss is 3.432337107658386 and perplexity is 30.948889178869447
At time: 98.55032777786255 and batch: 750, loss is 3.538623185157776 and perplexity is 34.41949727803755
At time: 99.01540398597717 and batch: 800, loss is 3.4928157043457033 and perplexity is 32.878393330361924
At time: 99.4790313243866 and batch: 850, loss is 3.5573370027542115 and perplexity is 35.06968221821629
At time: 99.94173955917358 and batch: 900, loss is 3.5144050550460815 and perplexity is 33.595934245391966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31859348244863 and perplexity of 75.08294850432718
finished 11 epochs...
Completing Train Step...
At time: 101.18468713760376 and batch: 50, loss is 3.7624099349975584 and perplexity is 43.052053711515256
At time: 101.64649653434753 and batch: 100, loss is 3.6408784580230713 and perplexity is 38.125313506804765
At time: 102.1116259098053 and batch: 150, loss is 3.6560041761398314 and perplexity is 38.70636961935768
At time: 102.5740110874176 and batch: 200, loss is 3.5499091720581055 and perplexity is 34.81015560921973
At time: 103.03501033782959 and batch: 250, loss is 3.692522964477539 and perplexity is 40.14600624064818
At time: 103.49839782714844 and batch: 300, loss is 3.6805415534973145 and perplexity is 39.66787053073914
At time: 103.96200489997864 and batch: 350, loss is 3.6433866882324217 and perplexity is 38.22106059755908
At time: 104.42489838600159 and batch: 400, loss is 3.587832727432251 and perplexity is 36.15563184326289
At time: 104.88675117492676 and batch: 450, loss is 3.6219808435440064 and perplexity is 37.41160100208898
At time: 105.34856343269348 and batch: 500, loss is 3.4992023658752442 and perplexity is 33.08904847571274
At time: 105.81161952018738 and batch: 550, loss is 3.5635580348968507 and perplexity is 35.28853186806873
At time: 106.27377986907959 and batch: 600, loss is 3.5832162141799926 and perplexity is 35.98910357544911
At time: 106.73641419410706 and batch: 650, loss is 3.434321870803833 and perplexity is 31.01037639209692
At time: 107.20022416114807 and batch: 700, loss is 3.4269384574890136 and perplexity is 30.782257151785892
At time: 107.66360330581665 and batch: 750, loss is 3.535151596069336 and perplexity is 34.30021409775044
At time: 108.1279182434082 and batch: 800, loss is 3.4910278797149656 and perplexity is 32.81966504252942
At time: 108.58981800079346 and batch: 850, loss is 3.557898774147034 and perplexity is 35.08938889724785
At time: 109.05328631401062 and batch: 900, loss is 3.517123599052429 and perplexity is 33.68739052871237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317984385033176 and perplexity of 75.03722959949536
finished 12 epochs...
Completing Train Step...
At time: 110.30530142784119 and batch: 50, loss is 3.7506554412841795 and perplexity is 42.548961208247526
At time: 110.76854872703552 and batch: 100, loss is 3.62886568069458 and perplexity is 37.67006249422889
At time: 111.2307436466217 and batch: 150, loss is 3.643629484176636 and perplexity is 38.2303416427102
At time: 111.69382333755493 and batch: 200, loss is 3.538323426246643 and perplexity is 34.40918127324616
At time: 112.16009616851807 and batch: 250, loss is 3.6809613847732545 and perplexity is 39.68452783982288
At time: 112.62287330627441 and batch: 300, loss is 3.6696545457839966 and perplexity is 39.23834846580326
At time: 113.08425664901733 and batch: 350, loss is 3.6331067419052125 and perplexity is 37.83016279268646
At time: 113.54628872871399 and batch: 400, loss is 3.578350191116333 and perplexity is 35.81440515539271
At time: 114.00874376296997 and batch: 450, loss is 3.613256096839905 and perplexity is 37.08661403556405
At time: 114.4706437587738 and batch: 500, loss is 3.4913241720199584 and perplexity is 32.82939069748117
At time: 114.93357610702515 and batch: 550, loss is 3.556312990188599 and perplexity is 35.03378880374301
At time: 115.39664101600647 and batch: 600, loss is 3.5766533327102663 and perplexity is 35.75368471251298
At time: 115.8600115776062 and batch: 650, loss is 3.4286718702316286 and perplexity is 30.835661781339777
At time: 116.322683095932 and batch: 700, loss is 3.422437047958374 and perplexity is 30.644005004282043
At time: 116.78532600402832 and batch: 750, loss is 3.5315274381637574 and perplexity is 34.1761296922204
At time: 117.24892401695251 and batch: 800, loss is 3.488114595413208 and perplexity is 32.72419116637008
At time: 117.71114325523376 and batch: 850, loss is 3.556278872489929 and perplexity is 35.032593551883075
At time: 118.17291212081909 and batch: 900, loss is 3.5164748430252075 and perplexity is 33.66554271878096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318284126177226 and perplexity of 75.05972471572939
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 119.40873003005981 and batch: 50, loss is 3.749819850921631 and perplexity is 42.513422556270804
At time: 119.88279271125793 and batch: 100, loss is 3.632342505455017 and perplexity is 37.80126264804204
At time: 120.344970703125 and batch: 150, loss is 3.6469340324401855 and perplexity is 38.35688462033451
At time: 120.80796241760254 and batch: 200, loss is 3.5388816690444944 and perplexity is 34.428395313422705
At time: 121.26904320716858 and batch: 250, loss is 3.6782961225509645 and perplexity is 39.5788989937774
At time: 121.74421381950378 and batch: 300, loss is 3.6672430658340454 and perplexity is 39.1438399736748
At time: 122.20667338371277 and batch: 350, loss is 3.6293452167510987 and perplexity is 37.68813097934498
At time: 122.66852307319641 and batch: 400, loss is 3.5767515754699706 and perplexity is 35.757197425715226
At time: 123.1320412158966 and batch: 450, loss is 3.6040486192703245 and perplexity is 36.746707112755914
At time: 123.59426689147949 and batch: 500, loss is 3.484340662956238 and perplexity is 32.600925024548104
At time: 124.05631828308105 and batch: 550, loss is 3.5460914373397827 and perplexity is 34.67751302780081
At time: 124.51797795295715 and batch: 600, loss is 3.5670229959487916 and perplexity is 35.41101733771641
At time: 124.97953796386719 and batch: 650, loss is 3.4150842380523683 and perplexity is 30.419511800067074
At time: 125.44075918197632 and batch: 700, loss is 3.4089003133773805 and perplexity is 30.231980268492823
At time: 125.90200471878052 and batch: 750, loss is 3.514916777610779 and perplexity is 33.61313044249321
At time: 126.36494064331055 and batch: 800, loss is 3.470404920578003 and perplexity is 32.14975790801563
At time: 126.8281922340393 and batch: 850, loss is 3.536093511581421 and perplexity is 34.3325372219132
At time: 127.29070806503296 and batch: 900, loss is 3.5003339529037474 and perplexity is 33.12651280683121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313490358117509 and perplexity of 74.70076687254802
finished 14 epochs...
Completing Train Step...
At time: 128.52619123458862 and batch: 50, loss is 3.740518579483032 and perplexity is 42.119826980546236
At time: 129.00174832344055 and batch: 100, loss is 3.6223926448822024 and perplexity is 37.42701032201721
At time: 129.46403884887695 and batch: 150, loss is 3.6371268701553343 and perplexity is 37.9825510036778
At time: 129.92730593681335 and batch: 200, loss is 3.530631365776062 and perplexity is 34.14551912277615
At time: 130.39099597930908 and batch: 250, loss is 3.6715688943862914 and perplexity is 39.313536288234594
At time: 130.8525333404541 and batch: 300, loss is 3.6607922458648683 and perplexity is 38.89214280847595
At time: 131.32333374023438 and batch: 350, loss is 3.6231085062026978 and perplexity is 37.45381246321204
At time: 131.78732228279114 and batch: 400, loss is 3.5711360692977907 and perplexity is 35.556965391158855
At time: 132.25058126449585 and batch: 450, loss is 3.599371976852417 and perplexity is 36.57525712118709
At time: 132.71437430381775 and batch: 500, loss is 3.4803768157958985 and perplexity is 32.47195571664225
At time: 133.1775689125061 and batch: 550, loss is 3.542873673439026 and perplexity is 34.566108311373114
At time: 133.65415930747986 and batch: 600, loss is 3.5645743370056153 and perplexity is 35.32441390782934
At time: 134.11756873130798 and batch: 650, loss is 3.413650813102722 and perplexity is 30.37593894956311
At time: 134.57899737358093 and batch: 700, loss is 3.4081420421600344 and perplexity is 30.209064917135514
At time: 135.04139065742493 and batch: 750, loss is 3.514654459953308 and perplexity is 33.60431428122322
At time: 135.50358366966248 and batch: 800, loss is 3.470779747962952 and perplexity is 32.16181077643049
At time: 135.96537733078003 and batch: 850, loss is 3.5373956251144407 and perplexity is 34.37727120129219
At time: 136.4279580116272 and batch: 900, loss is 3.502681279182434 and perplexity is 33.20436287487342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313155500856165 and perplexity of 74.67575696594086
finished 15 epochs...
Completing Train Step...
At time: 137.66684556007385 and batch: 50, loss is 3.7369286394119263 and perplexity is 41.96889041459557
At time: 138.1289792060852 and batch: 100, loss is 3.6186373090744017 and perplexity is 37.286722908018056
At time: 138.59229016304016 and batch: 150, loss is 3.6329262399673463 and perplexity is 37.82333499122682
At time: 139.05376529693604 and batch: 200, loss is 3.5267940282821657 and perplexity is 34.01474231983812
At time: 139.51456832885742 and batch: 250, loss is 3.6679086303710937 and perplexity is 39.16990139722277
At time: 139.97659134864807 and batch: 300, loss is 3.657364220619202 and perplexity is 38.75904781790152
At time: 140.4403522014618 and batch: 350, loss is 3.6198579454421997 and perplexity is 37.332264227076195
At time: 140.90285682678223 and batch: 400, loss is 3.568182373046875 and perplexity is 35.45209586839011
At time: 141.36657524108887 and batch: 450, loss is 3.596609773635864 and perplexity is 36.47436823027675
At time: 141.83018469810486 and batch: 500, loss is 3.4780385637283326 and perplexity is 32.396116798856674
At time: 142.2948727607727 and batch: 550, loss is 3.5408085060119627 and perplexity is 34.49479717047548
At time: 142.75968956947327 and batch: 600, loss is 3.5629154443740845 and perplexity is 35.265863076083455
At time: 143.22542452812195 and batch: 650, loss is 3.4124008798599244 and perplexity is 30.33799477247313
At time: 143.6916561126709 and batch: 700, loss is 3.407312340736389 and perplexity is 30.184010808119012
At time: 144.15867352485657 and batch: 750, loss is 3.514132080078125 and perplexity is 33.586764647910094
At time: 144.6241853237152 and batch: 800, loss is 3.470535454750061 and perplexity is 32.15395482396054
At time: 145.08943057060242 and batch: 850, loss is 3.5375723600387574 and perplexity is 34.38334740263884
At time: 145.5670940876007 and batch: 900, loss is 3.5032873868942263 and perplexity is 33.224494395595705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313148394022902 and perplexity of 74.6752262596732
finished 16 epochs...
Completing Train Step...
At time: 146.80278897285461 and batch: 50, loss is 3.7338079833984374 and perplexity is 41.83812408882648
At time: 147.26617455482483 and batch: 100, loss is 3.615532245635986 and perplexity is 37.171124830537444
At time: 147.728994846344 and batch: 150, loss is 3.629624772071838 and perplexity is 37.69866836971213
At time: 148.193274974823 and batch: 200, loss is 3.5237031507492067 and perplexity is 33.909769230066615
At time: 148.65599656105042 and batch: 250, loss is 3.66491108417511 and perplexity is 39.0526636089202
At time: 149.11774492263794 and batch: 300, loss is 3.654561071395874 and perplexity is 38.65055255835819
At time: 149.57831931114197 and batch: 350, loss is 3.6171698713302614 and perplexity is 37.232047089958535
At time: 150.03986144065857 and batch: 400, loss is 3.5657514572143554 and perplexity is 35.36601947187316
At time: 150.50216388702393 and batch: 450, loss is 3.594292621612549 and perplexity is 36.38994941751596
At time: 150.96396708488464 and batch: 500, loss is 3.476061315536499 and perplexity is 32.33212492025491
At time: 151.42661809921265 and batch: 550, loss is 3.539004979133606 and perplexity is 34.43264094367592
At time: 151.88964986801147 and batch: 600, loss is 3.5613648557662962 and perplexity is 35.21122260394846
At time: 152.35288977622986 and batch: 650, loss is 3.411125159263611 and perplexity is 30.299316644179086
At time: 152.81588292121887 and batch: 700, loss is 3.406375641822815 and perplexity is 30.155750715652992
At time: 153.279376745224 and batch: 750, loss is 3.513409399986267 and perplexity is 33.56250093025798
At time: 153.74168586730957 and batch: 800, loss is 3.4699952363967896 and perplexity is 32.136589358428296
At time: 154.20389652252197 and batch: 850, loss is 3.5373472452163695 and perplexity is 34.37560807264683
At time: 154.66630101203918 and batch: 900, loss is 3.5033673620224 and perplexity is 33.227151635048614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313248725786601 and perplexity of 74.6827189326987
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 155.89066171646118 and batch: 50, loss is 3.7332127857208253 and perplexity is 41.81322954385577
At time: 156.36598420143127 and batch: 100, loss is 3.617079482078552 and perplexity is 37.228681865174835
At time: 156.82946467399597 and batch: 150, loss is 3.630967922210693 and perplexity is 37.74933736178197
At time: 157.30553698539734 and batch: 200, loss is 3.5250609064102174 and perplexity is 33.95584168169042
At time: 157.76977491378784 and batch: 250, loss is 3.66587815284729 and perplexity is 39.09044848380119
At time: 158.23421907424927 and batch: 300, loss is 3.6533815717697142 and perplexity is 38.604991121195965
At time: 158.69895815849304 and batch: 350, loss is 3.6165532302856445 and perplexity is 37.209095358764785
At time: 159.1628861427307 and batch: 400, loss is 3.5668876552581787 and perplexity is 35.406225110473606
At time: 159.6281533241272 and batch: 450, loss is 3.591948890686035 and perplexity is 36.30476103596956
At time: 160.0929057598114 and batch: 500, loss is 3.4730009174346925 and perplexity is 32.23332700411117
At time: 160.55688071250916 and batch: 550, loss is 3.5354919052124023 and perplexity is 34.31188876060162
At time: 161.02143788337708 and batch: 600, loss is 3.5593178129196166 and perplexity is 35.139217446559485
At time: 161.48629570007324 and batch: 650, loss is 3.4061725521087647 and perplexity is 30.14962701471456
At time: 161.9499387741089 and batch: 700, loss is 3.400964016914368 and perplexity is 29.992999874389415
At time: 162.41533136367798 and batch: 750, loss is 3.507977623939514 and perplexity is 33.380691163615936
At time: 162.87951850891113 and batch: 800, loss is 3.463041052818298 and perplexity is 31.913880891750885
At time: 163.34294176101685 and batch: 850, loss is 3.5294325160980224 and perplexity is 34.10460830602895
At time: 163.80674290657043 and batch: 900, loss is 3.4951277685165407 and perplexity is 32.95449823138073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312135043209547 and perplexity of 74.59959238670103
finished 18 epochs...
Completing Train Step...
At time: 165.05817461013794 and batch: 50, loss is 3.7305276727676393 and perplexity is 41.70110689794291
At time: 165.53733229637146 and batch: 100, loss is 3.6142595052719115 and perplexity is 37.12384573297764
At time: 166.00271582603455 and batch: 150, loss is 3.6283317470550536 and perplexity is 37.64995454929197
At time: 166.46815419197083 and batch: 200, loss is 3.5227694129943847 and perplexity is 33.878121176070046
At time: 166.93266558647156 and batch: 250, loss is 3.6635263919830323 and perplexity is 38.99862511251744
At time: 167.39854335784912 and batch: 300, loss is 3.6515336418151856 and perplexity is 38.533717676158126
At time: 167.86319255828857 and batch: 350, loss is 3.614360146522522 and perplexity is 37.127582111253425
At time: 168.3269658088684 and batch: 400, loss is 3.564599232673645 and perplexity is 35.32529334365836
At time: 168.79196691513062 and batch: 450, loss is 3.5905286026000978 and perplexity is 36.25323441639103
At time: 169.27102661132812 and batch: 500, loss is 3.4719066762924196 and perplexity is 32.198075262021625
At time: 169.73649716377258 and batch: 550, loss is 3.5346190834045412 and perplexity is 34.28195366172645
At time: 170.2012529373169 and batch: 600, loss is 3.5584844160079956 and perplexity is 35.10994473085246
At time: 170.66705465316772 and batch: 650, loss is 3.4059304666519163 and perplexity is 30.14232911187861
At time: 171.13337326049805 and batch: 700, loss is 3.4012706327438353 and perplexity is 30.00219761293818
At time: 171.5986967086792 and batch: 750, loss is 3.5082862043380736 and perplexity is 33.39099338004867
At time: 172.06551003456116 and batch: 800, loss is 3.4633267307281494 and perplexity is 31.922999284939024
At time: 172.52740025520325 and batch: 850, loss is 3.5303367233276366 and perplexity is 34.135459885431
At time: 172.9903335571289 and batch: 900, loss is 3.4967963886260987 and perplexity is 33.00953267284855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311872090378853 and perplexity of 74.57997879155255
finished 19 epochs...
Completing Train Step...
At time: 174.22722005844116 and batch: 50, loss is 3.7292414140701293 and perplexity is 41.64750296815056
At time: 174.6893138885498 and batch: 100, loss is 3.6128538274765014 and perplexity is 37.07169822723267
At time: 175.15098881721497 and batch: 150, loss is 3.626857304573059 and perplexity is 37.59448276188883
At time: 175.61390352249146 and batch: 200, loss is 3.5214767122268675 and perplexity is 33.8343551970757
At time: 176.07788491249084 and batch: 250, loss is 3.66223237991333 and perplexity is 38.94819305780103
At time: 176.54112839698792 and batch: 300, loss is 3.6504461765289307 and perplexity is 38.4918363721944
At time: 177.0036883354187 and batch: 350, loss is 3.6131713819503783 and perplexity is 37.083472380227484
At time: 177.46647906303406 and batch: 400, loss is 3.5634065866470337 and perplexity is 35.283187886357254
At time: 177.92854690551758 and batch: 450, loss is 3.5896704530715944 and perplexity is 36.22213706536683
At time: 178.39019417762756 and batch: 500, loss is 3.4712636280059814 and perplexity is 32.17737700060179
At time: 178.85128450393677 and batch: 550, loss is 3.534064979553223 and perplexity is 34.26296316101245
At time: 179.3127875328064 and batch: 600, loss is 3.5580134439468383 and perplexity is 35.09341282115483
At time: 179.7729630470276 and batch: 650, loss is 3.4057564449310305 and perplexity is 30.137084148277083
At time: 180.23243117332458 and batch: 700, loss is 3.4013162946701048 and perplexity is 30.003567602351442
At time: 180.69137239456177 and batch: 750, loss is 3.5083694648742676 and perplexity is 33.39377364780323
At time: 181.16361141204834 and batch: 800, loss is 3.4634807443618776 and perplexity is 31.92791624068779
At time: 181.62317967414856 and batch: 850, loss is 3.530750608444214 and perplexity is 34.14959096834583
At time: 182.08066320419312 and batch: 900, loss is 3.497539367675781 and perplexity is 33.034067177247046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3117872264287245 and perplexity of 74.5736499085026
Finished Training.
Improved accuracyfrom -10000000 to -74.5736499085026
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe3d26db70>
ELAPSED
194.13000988960266


RESULTS SO FAR:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6868875026702881 and batch: 50, loss is 6.6740333938598635 and perplexity is 791.5819359981679
At time: 1.1681938171386719 and batch: 100, loss is 5.840910444259643 and perplexity is 344.09247511636084
At time: 1.6351244449615479 and batch: 150, loss is 5.611736698150635 and perplexity is 273.619019254282
At time: 2.1033730506896973 and batch: 200, loss is 5.375164728164673 and perplexity is 215.97544661942447
At time: 2.5723190307617188 and batch: 250, loss is 5.378058919906616 and perplexity is 216.60142638928122
At time: 3.046070098876953 and batch: 300, loss is 5.273820877075195 and perplexity is 195.16022289774168
At time: 3.5132052898406982 and batch: 350, loss is 5.226543397903442 and perplexity is 186.14824974862324
At time: 3.981337308883667 and batch: 400, loss is 5.066555786132812 and perplexity is 158.62703994620352
At time: 4.450256586074829 and batch: 450, loss is 5.063268547058105 and perplexity is 158.10645106103746
At time: 4.920618295669556 and batch: 500, loss is 4.978790016174316 and perplexity is 145.29846645349372
At time: 5.390658617019653 and batch: 550, loss is 5.031133575439453 and perplexity is 153.1064720987731
At time: 5.861261606216431 and batch: 600, loss is 4.95062403678894 and perplexity is 141.26308978667228
At time: 6.332293510437012 and batch: 650, loss is 4.830412902832031 and perplexity is 125.26267129002011
At time: 6.8030149936676025 and batch: 700, loss is 4.896086015701294 and perplexity is 133.76519886599922
At time: 7.273562431335449 and batch: 750, loss is 4.911168565750122 and perplexity is 135.79801062951805
At time: 7.744047403335571 and batch: 800, loss is 4.858034191131591 and perplexity is 128.77081432421124
At time: 8.214184761047363 and batch: 850, loss is 4.894452533721924 and perplexity is 133.54687418744442
At time: 8.685590744018555 and batch: 900, loss is 4.825393733978271 and perplexity is 124.63553196739684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.801004070125214 and perplexity of 121.63248377015073
finished 1 epochs...
Completing Train Step...
At time: 9.939465522766113 and batch: 50, loss is 4.771514091491699 and perplexity is 118.0979177132152
At time: 10.401885986328125 and batch: 100, loss is 4.642189464569092 and perplexity is 103.77130262799506
At time: 10.864571809768677 and batch: 150, loss is 4.624028558731079 and perplexity is 101.903731493715
At time: 11.324026107788086 and batch: 200, loss is 4.521251707077027 and perplexity is 91.95062121875323
At time: 11.798559188842773 and batch: 250, loss is 4.63532717704773 and perplexity is 103.06163188017663
At time: 12.260607481002808 and batch: 300, loss is 4.590000286102295 and perplexity is 98.49445834143283
At time: 12.721663236618042 and batch: 350, loss is 4.574852590560913 and perplexity is 97.01373734224411
At time: 13.184324026107788 and batch: 400, loss is 4.466294355392456 and perplexity is 87.03360908630903
At time: 13.646841287612915 and batch: 450, loss is 4.497778635025025 and perplexity is 89.81739232661718
At time: 14.110429525375366 and batch: 500, loss is 4.393163022994995 and perplexity is 80.8958902978709
At time: 14.571961641311646 and batch: 550, loss is 4.46951005935669 and perplexity is 87.31393388729227
At time: 15.03389835357666 and batch: 600, loss is 4.4474134349823 and perplexity is 85.40575053323762
At time: 15.495707035064697 and batch: 650, loss is 4.301029644012451 and perplexity is 73.77571733134624
At time: 15.957292556762695 and batch: 700, loss is 4.336822619438172 and perplexity is 76.46419709404158
At time: 16.419151306152344 and batch: 750, loss is 4.408214197158814 and perplexity is 82.12267756691442
At time: 16.882492542266846 and batch: 800, loss is 4.356844091415406 and perplexity is 78.01055136221095
At time: 17.34589648246765 and batch: 850, loss is 4.426018810272216 and perplexity is 83.59793428470029
At time: 17.807738780975342 and batch: 900, loss is 4.371933641433716 and perplexity is 79.19662160703344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.566306336285317 and perplexity of 96.18816607056185
finished 2 epochs...
Completing Train Step...
At time: 19.055342435836792 and batch: 50, loss is 4.432060747146607 and perplexity is 84.10455667554353
At time: 19.518051624298096 and batch: 100, loss is 4.308948769569397 and perplexity is 74.36227595024268
At time: 19.98133158683777 and batch: 150, loss is 4.304492220878601 and perplexity is 74.03161419907697
At time: 20.442925930023193 and batch: 200, loss is 4.213360157012939 and perplexity is 67.58324903571608
At time: 20.90536618232727 and batch: 250, loss is 4.3504344081878665 and perplexity is 77.51212751549632
At time: 21.3670494556427 and batch: 300, loss is 4.320555505752563 and perplexity is 75.23040761081822
At time: 21.828449249267578 and batch: 350, loss is 4.305035066604614 and perplexity is 74.07181285428172
At time: 22.29103183746338 and batch: 400, loss is 4.22087375164032 and perplexity is 68.09295463528684
At time: 22.75418710708618 and batch: 450, loss is 4.2607487869262695 and perplexity is 70.86302490002878
At time: 23.216482877731323 and batch: 500, loss is 4.142848415374756 and perplexity is 62.98196498787758
At time: 23.693495512008667 and batch: 550, loss is 4.228946604728699 and perplexity is 68.64488387878363
At time: 24.154601573944092 and batch: 600, loss is 4.226484785079956 and perplexity is 68.47610039735412
At time: 24.61677122116089 and batch: 650, loss is 4.082107200622558 and perplexity is 59.27023262470454
At time: 25.07739019393921 and batch: 700, loss is 4.098169050216675 and perplexity is 60.22990864085903
At time: 25.538167476654053 and batch: 750, loss is 4.19420554637909 and perplexity is 66.30103754923458
At time: 26.00081205368042 and batch: 800, loss is 4.147511548995972 and perplexity is 63.276344137563136
At time: 26.46436834335327 and batch: 850, loss is 4.223625383377075 and perplexity is 68.28057938904122
At time: 26.92630386352539 and batch: 900, loss is 4.180545606613159 and perplexity is 65.4015269868365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.474069673721105 and perplexity of 87.71296076748911
finished 3 epochs...
Completing Train Step...
At time: 28.152568340301514 and batch: 50, loss is 4.257409710884094 and perplexity is 70.62680247311089
At time: 28.628090143203735 and batch: 100, loss is 4.1315849494934085 and perplexity is 62.27654994098276
At time: 29.08906888961792 and batch: 150, loss is 4.131441593170166 and perplexity is 62.26762284365221
At time: 29.54929208755493 and batch: 200, loss is 4.044766397476196 and perplexity is 57.09784627511125
At time: 30.010817289352417 and batch: 250, loss is 4.189257130622864 and perplexity is 65.97376286380288
At time: 30.47130036354065 and batch: 300, loss is 4.164308037757873 and perplexity is 64.34814055097344
At time: 30.932560920715332 and batch: 350, loss is 4.146690049171448 and perplexity is 63.22438397751094
At time: 31.393846035003662 and batch: 400, loss is 4.072970380783081 and perplexity is 58.73115765304276
At time: 31.855116367340088 and batch: 450, loss is 4.118222455978394 and perplexity is 61.44991519699073
At time: 32.317299127578735 and batch: 500, loss is 3.996901659965515 and perplexity is 54.42924819187346
At time: 32.779552936553955 and batch: 550, loss is 4.084081645011902 and perplexity is 59.38737400948482
At time: 33.24274206161499 and batch: 600, loss is 4.087424564361572 and perplexity is 59.586233411469
At time: 33.70626211166382 and batch: 650, loss is 3.945553946495056 and perplexity is 51.704971968561566
At time: 34.16848111152649 and batch: 700, loss is 3.9570273399353026 and perplexity is 52.30161969668941
At time: 34.63098192214966 and batch: 750, loss is 4.058020033836365 and perplexity is 57.859637467061184
At time: 35.09516620635986 and batch: 800, loss is 4.015165758132935 and perplexity is 55.43248302590149
At time: 35.573763370513916 and batch: 850, loss is 4.095294647216797 and perplexity is 60.057032188104756
At time: 36.03886437416077 and batch: 900, loss is 4.05343719959259 and perplexity is 57.595083006787085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442819255672089 and perplexity of 85.01428113002615
finished 4 epochs...
Completing Train Step...
At time: 37.27852654457092 and batch: 50, loss is 4.137770590782165 and perplexity is 62.66296421758475
At time: 37.75716280937195 and batch: 100, loss is 4.009810194969178 and perplexity is 55.136404403293106
At time: 38.22314643859863 and batch: 150, loss is 4.015036873817444 and perplexity is 55.42533910865005
At time: 38.689082860946655 and batch: 200, loss is 3.929154806137085 and perplexity is 50.86396958174191
At time: 39.15482425689697 and batch: 250, loss is 4.075575976371765 and perplexity is 58.88438683829537
At time: 39.62142729759216 and batch: 300, loss is 4.054651494026184 and perplexity is 57.665062874967965
At time: 40.087120056152344 and batch: 350, loss is 4.039079036712646 and perplexity is 56.77403192188966
At time: 40.55334758758545 and batch: 400, loss is 3.967312903404236 and perplexity is 52.84234740309573
At time: 41.018502950668335 and batch: 450, loss is 4.013075351715088 and perplexity is 55.31672763768249
At time: 41.484190464019775 and batch: 500, loss is 3.89250186920166 and perplexity is 49.033408367256015
At time: 41.9489209651947 and batch: 550, loss is 3.982442240715027 and perplexity is 53.64789543967533
At time: 42.41345930099487 and batch: 600, loss is 3.9888948965072633 and perplexity is 53.995186111503806
At time: 42.875640630722046 and batch: 650, loss is 3.847615976333618 and perplexity is 46.88116409477348
At time: 43.33944582939148 and batch: 700, loss is 3.856391191482544 and perplexity is 47.29436671555432
At time: 43.80438423156738 and batch: 750, loss is 3.959884943962097 and perplexity is 52.45129076421526
At time: 44.26709222793579 and batch: 800, loss is 3.918892526626587 and perplexity is 50.34465852406286
At time: 44.72932195663452 and batch: 850, loss is 4.000189657211304 and perplexity is 54.6085059480278
At time: 45.192830085754395 and batch: 900, loss is 3.9601946115493774 and perplexity is 52.46753574401798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4278020989404965 and perplexity of 83.74714653831963
finished 5 epochs...
Completing Train Step...
At time: 46.43349075317383 and batch: 50, loss is 4.046555910110474 and perplexity is 57.20011507076982
At time: 46.896080017089844 and batch: 100, loss is 3.9206381034851074 and perplexity is 50.4326157406394
At time: 47.37291121482849 and batch: 150, loss is 3.925348062515259 and perplexity is 50.67071156519666
At time: 47.83773493766785 and batch: 200, loss is 3.8413003969192503 and perplexity is 46.58601537851108
At time: 48.301790952682495 and batch: 250, loss is 3.9900831604003906 and perplexity is 54.0593847764867
At time: 48.76534414291382 and batch: 300, loss is 3.968589448928833 and perplexity is 52.909846138620566
At time: 49.23012089729309 and batch: 350, loss is 3.9560416173934936 and perplexity is 52.25009021224083
At time: 49.69451546669006 and batch: 400, loss is 3.885213007926941 and perplexity is 48.6773100081601
At time: 50.15819025039673 and batch: 450, loss is 3.934475235939026 and perplexity is 51.135308942293754
At time: 50.624062299728394 and batch: 500, loss is 3.8117404079437254 and perplexity is 45.22908746925866
At time: 51.087822675704956 and batch: 550, loss is 3.902564787864685 and perplexity is 49.52931853452066
At time: 51.55143904685974 and batch: 600, loss is 3.9114559745788573 and perplexity is 49.9716564941161
At time: 52.01344537734985 and batch: 650, loss is 3.7724244832992553 and perplexity is 43.48536667826451
At time: 52.476622104644775 and batch: 700, loss is 3.779443588256836 and perplexity is 43.79166875674795
At time: 52.94068241119385 and batch: 750, loss is 3.883044595718384 and perplexity is 48.57187189282122
At time: 53.40572547912598 and batch: 800, loss is 3.8444345140457155 and perplexity is 46.73225044637619
At time: 53.87144923210144 and batch: 850, loss is 3.9261029243469237 and perplexity is 50.708975391474574
At time: 54.33612585067749 and batch: 900, loss is 3.8878247356414795 and perplexity is 48.80460804932904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423778795216181 and perplexity of 83.41088323005178
finished 6 epochs...
Completing Train Step...
At time: 55.57990074157715 and batch: 50, loss is 3.9740143156051637 and perplexity is 53.19765495618847
At time: 56.04367470741272 and batch: 100, loss is 3.8485389232635496 and perplexity is 46.924452894806016
At time: 56.506641149520874 and batch: 150, loss is 3.8565332221984865 and perplexity is 47.301084445369675
At time: 56.97012686729431 and batch: 200, loss is 3.774990553855896 and perplexity is 43.59709648932246
At time: 57.433120012283325 and batch: 250, loss is 3.9225763988494875 and perplexity is 50.530463844571656
At time: 57.89936089515686 and batch: 300, loss is 3.9020954895019533 and perplexity is 49.50607995976424
At time: 58.363773584365845 and batch: 350, loss is 3.8883049583435056 and perplexity is 48.82805075848787
At time: 58.839120388031006 and batch: 400, loss is 3.818493824005127 and perplexity is 45.53557205913106
At time: 59.32710528373718 and batch: 450, loss is 3.873585205078125 and perplexity is 48.114577853821444
At time: 59.794379472732544 and batch: 500, loss is 3.74895140171051 and perplexity is 42.476517835247165
At time: 60.26197791099548 and batch: 550, loss is 3.8390657997131346 and perplexity is 46.482030623993154
At time: 60.72770547866821 and batch: 600, loss is 3.846261029243469 and perplexity is 46.8176856126044
At time: 61.19423961639404 and batch: 650, loss is 3.709943900108337 and perplexity is 40.8515146971564
At time: 61.65859842300415 and batch: 700, loss is 3.717402024269104 and perplexity is 41.15732935020774
At time: 62.12454319000244 and batch: 750, loss is 3.8170082712173463 and perplexity is 45.4679767837215
At time: 62.58938241004944 and batch: 800, loss is 3.785539298057556 and perplexity is 44.0594253152167
At time: 63.06183195114136 and batch: 850, loss is 3.864084882736206 and perplexity is 47.659638312759384
At time: 63.52926063537598 and batch: 900, loss is 3.825289936065674 and perplexity is 45.84609087475626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425978569135274 and perplexity of 83.59457027646482
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 64.77589392662048 and batch: 50, loss is 3.9250698041915895 and perplexity is 50.65661397941365
At time: 65.25692224502563 and batch: 100, loss is 3.792096276283264 and perplexity is 44.34927122584021
At time: 65.72454762458801 and batch: 150, loss is 3.7947527980804443 and perplexity is 44.46724265900445
At time: 66.19212770462036 and batch: 200, loss is 3.6957503604888915 and perplexity is 40.27578260827982
At time: 66.66027116775513 and batch: 250, loss is 3.8360843420028687 and perplexity is 46.34365280171741
At time: 67.1267032623291 and batch: 300, loss is 3.806675362586975 and perplexity is 45.00057928047796
At time: 67.59256958961487 and batch: 350, loss is 3.779756436347961 and perplexity is 43.805371039980514
At time: 68.05916714668274 and batch: 400, loss is 3.706878638267517 and perplexity is 40.72648582902057
At time: 68.52407789230347 and batch: 450, loss is 3.749746127128601 and perplexity is 42.510288421036854
At time: 68.98868489265442 and batch: 500, loss is 3.613216519355774 and perplexity is 37.08514626973101
At time: 69.45433259010315 and batch: 550, loss is 3.6914506769180297 and perplexity is 40.10298124929635
At time: 69.91959071159363 and batch: 600, loss is 3.6953577423095703 and perplexity is 40.25997270767171
At time: 70.38408613204956 and batch: 650, loss is 3.5418296432495118 and perplexity is 34.530039082720116
At time: 70.84775876998901 and batch: 700, loss is 3.533795075416565 and perplexity is 34.2537166934072
At time: 71.33420181274414 and batch: 750, loss is 3.625023913383484 and perplexity is 37.52562051342539
At time: 71.79903221130371 and batch: 800, loss is 3.5818364095687865 and perplexity is 35.93947988775213
At time: 72.26289939880371 and batch: 850, loss is 3.6449407577514648 and perplexity is 38.280504961192214
At time: 72.72747421264648 and batch: 900, loss is 3.5948378086090087 and perplexity is 36.40979415379476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346114485231165 and perplexity of 77.17800331144034
finished 8 epochs...
Completing Train Step...
At time: 73.96184277534485 and batch: 50, loss is 3.8353711652755735 and perplexity is 46.310613369958666
At time: 74.43844819068909 and batch: 100, loss is 3.7085328578948973 and perplexity is 40.79391213481669
At time: 74.90262126922607 and batch: 150, loss is 3.717544469833374 and perplexity is 41.16319244678702
At time: 75.36745691299438 and batch: 200, loss is 3.6251804494857787 and perplexity is 37.53149508757622
At time: 75.83375358581543 and batch: 250, loss is 3.7664313650131227 and perplexity is 43.225533116394466
At time: 76.29844832420349 and batch: 300, loss is 3.7436904096603394 and perplexity is 42.25363601641288
At time: 76.76319193840027 and batch: 350, loss is 3.7210629653930662 and perplexity is 41.308280052001415
At time: 77.22839593887329 and batch: 400, loss is 3.6518823289871216 and perplexity is 38.547156231988666
At time: 77.69308066368103 and batch: 450, loss is 3.7002032661437987 and perplexity is 40.4555267632915
At time: 78.15759682655334 and batch: 500, loss is 3.566881160736084 and perplexity is 35.40599516470903
At time: 78.62077951431274 and batch: 550, loss is 3.649124736785889 and perplexity is 38.44100532231104
At time: 79.08561706542969 and batch: 600, loss is 3.657595992088318 and perplexity is 38.768032100465774
At time: 79.5501070022583 and batch: 650, loss is 3.5090377044677736 and perplexity is 33.416096147089526
At time: 80.01445007324219 and batch: 700, loss is 3.5063093376159666 and perplexity is 33.32504903947963
At time: 80.47856569290161 and batch: 750, loss is 3.6027213191986083 and perplexity is 36.69796556025865
At time: 80.94449090957642 and batch: 800, loss is 3.564636960029602 and perplexity is 35.326626098715124
At time: 81.40948247909546 and batch: 850, loss is 3.6337853384017946 and perplexity is 37.85584292085975
At time: 81.87347102165222 and batch: 900, loss is 3.5911924695968627 and perplexity is 36.277309732765836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344081512869221 and perplexity of 77.02126194317704
finished 9 epochs...
Completing Train Step...
At time: 83.11531043052673 and batch: 50, loss is 3.7970814180374144 and perplexity is 44.57091062249127
At time: 83.57885789871216 and batch: 100, loss is 3.67120454788208 and perplexity is 39.29921514780694
At time: 84.04355335235596 and batch: 150, loss is 3.681070547103882 and perplexity is 39.68886013182896
At time: 84.50919270515442 and batch: 200, loss is 3.590413703918457 and perplexity is 36.2490692068445
At time: 84.97418570518494 and batch: 250, loss is 3.7313385152816774 and perplexity is 41.73493364052538
At time: 85.43843150138855 and batch: 300, loss is 3.7110952615737913 and perplexity is 40.89857664444152
At time: 85.90267300605774 and batch: 350, loss is 3.6896704244613647 and perplexity is 40.031651329878656
At time: 86.36713576316833 and batch: 400, loss is 3.621468915939331 and perplexity is 37.39245387219124
At time: 86.83183217048645 and batch: 450, loss is 3.672052888870239 and perplexity is 39.332568428296454
At time: 87.29551959037781 and batch: 500, loss is 3.5403294467926028 and perplexity is 34.47827607747793
At time: 87.76238322257996 and batch: 550, loss is 3.6235898208618162 and perplexity is 37.47184387123286
At time: 88.22881603240967 and batch: 600, loss is 3.633855619430542 and perplexity is 37.85850356193953
At time: 88.6955349445343 and batch: 650, loss is 3.4874097108840942 and perplexity is 32.70113251808611
At time: 89.1625235080719 and batch: 700, loss is 3.4868537282943723 and perplexity is 32.682956311037415
At time: 89.62838411331177 and batch: 750, loss is 3.5849417877197265 and perplexity is 36.05125903178018
At time: 90.09429454803467 and batch: 800, loss is 3.549369225502014 and perplexity is 34.79136505898448
At time: 90.55788111686707 and batch: 850, loss is 3.6207969856262205 and perplexity is 37.36733718822858
At time: 91.02307176589966 and batch: 900, loss is 3.5812988233566285 and perplexity is 35.920164511197584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34591173145869 and perplexity of 77.16235676636852
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 92.27360200881958 and batch: 50, loss is 3.7881765747070313 and perplexity is 44.17577556525174
At time: 92.73880791664124 and batch: 100, loss is 3.6636480617523195 and perplexity is 39.00337035490781
At time: 93.20319509506226 and batch: 150, loss is 3.679043107032776 and perplexity is 39.60847486211504
At time: 93.66709399223328 and batch: 200, loss is 3.5754376649856567 and perplexity is 35.7102465205257
At time: 94.13160371780396 and batch: 250, loss is 3.719645199775696 and perplexity is 41.24975608926511
At time: 94.59525990486145 and batch: 300, loss is 3.6937078714370726 and perplexity is 40.193603716561505
At time: 95.07316732406616 and batch: 350, loss is 3.6680111265182496 and perplexity is 39.17391636695641
At time: 95.53814315795898 and batch: 400, loss is 3.6001399183273315 and perplexity is 36.60335556568569
At time: 96.00319504737854 and batch: 450, loss is 3.6436280679702757 and perplexity is 38.23028750069555
At time: 96.46857762336731 and batch: 500, loss is 3.5080103874206543 and perplexity is 33.38178484917769
At time: 96.93362641334534 and batch: 550, loss is 3.5862029361724854 and perplexity is 36.096753703065346
At time: 97.39866495132446 and batch: 600, loss is 3.59555703163147 and perplexity is 36.43599033531243
At time: 97.86395525932312 and batch: 650, loss is 3.440780534744263 and perplexity is 31.211310175285128
At time: 98.32871675491333 and batch: 700, loss is 3.4384244680404663 and perplexity is 31.137860806382275
At time: 98.79544997215271 and batch: 750, loss is 3.5309148597717286 and perplexity is 34.155200544674024
At time: 99.26080775260925 and batch: 800, loss is 3.487791666984558 and perplexity is 32.71362530083888
At time: 99.72517418861389 and batch: 850, loss is 3.563061876296997 and perplexity is 35.27102750233607
At time: 100.19011211395264 and batch: 900, loss is 3.5220227432250977 and perplexity is 33.852834848590575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325513447800728 and perplexity of 75.60432176843713
finished 11 epochs...
Completing Train Step...
At time: 101.42331528663635 and batch: 50, loss is 3.7633215045928954 and perplexity is 43.09131654738037
At time: 101.90133666992188 and batch: 100, loss is 3.6366438627243043 and perplexity is 37.96420957917229
At time: 102.36689519882202 and batch: 150, loss is 3.6543711185455323 and perplexity is 38.64321147298458
At time: 102.8324842453003 and batch: 200, loss is 3.553988699913025 and perplexity is 34.9524546677967
At time: 103.29876685142517 and batch: 250, loss is 3.69794819355011 and perplexity is 40.36439940165014
At time: 103.76611733436584 and batch: 300, loss is 3.674177532196045 and perplexity is 39.41622494595583
At time: 104.23178815841675 and batch: 350, loss is 3.6498650026321413 and perplexity is 38.46947242095721
At time: 104.69689869880676 and batch: 400, loss is 3.5836297988891603 and perplexity is 36.00399119681863
At time: 105.16322016716003 and batch: 450, loss is 3.6290694952011107 and perplexity is 37.67774098189413
At time: 105.62774181365967 and batch: 500, loss is 3.4952063083648683 and perplexity is 32.95708657431624
At time: 106.09379744529724 and batch: 550, loss is 3.5749494457244873 and perplexity is 35.69281634557306
At time: 106.5588026046753 and batch: 600, loss is 3.586128911972046 and perplexity is 36.09408176862914
At time: 107.04559469223022 and batch: 650, loss is 3.433374810218811 and perplexity is 30.981021589471617
At time: 107.50845313072205 and batch: 700, loss is 3.433176693916321 and perplexity is 30.97488435199042
At time: 107.97217464447021 and batch: 750, loss is 3.5278009128570558 and perplexity is 34.04900848734161
At time: 108.43584156036377 and batch: 800, loss is 3.4864690828323366 and perplexity is 32.67038737765303
At time: 108.89916181564331 and batch: 850, loss is 3.5638843822479247 and perplexity is 35.300050066331686
At time: 109.36233592033386 and batch: 900, loss is 3.5248308086395266 and perplexity is 33.948029417044346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3245912316727315 and perplexity of 75.53463038375479
finished 12 epochs...
Completing Train Step...
At time: 110.59015369415283 and batch: 50, loss is 3.751112976074219 and perplexity is 42.56843329251864
At time: 111.06682419776917 and batch: 100, loss is 3.624437003135681 and perplexity is 37.50360280403255
At time: 111.53020691871643 and batch: 150, loss is 3.6422014617919922 and perplexity is 38.17578682109596
At time: 111.99299430847168 and batch: 200, loss is 3.542292275428772 and perplexity is 34.54601748572762
At time: 112.45556688308716 and batch: 250, loss is 3.68629994392395 and perplexity is 39.89695255550583
At time: 112.91897010803223 and batch: 300, loss is 3.663408832550049 and perplexity is 38.99404072573631
At time: 113.38239169120789 and batch: 350, loss is 3.639392991065979 and perplexity is 38.068721656429865
At time: 113.84490585327148 and batch: 400, loss is 3.5740527725219726 and perplexity is 35.66082589826554
At time: 114.30783009529114 and batch: 450, loss is 3.6199956798553465 and perplexity is 37.33740651870808
At time: 114.77080512046814 and batch: 500, loss is 3.487078881263733 and perplexity is 32.69031580417311
At time: 115.2338433265686 and batch: 550, loss is 3.567448625564575 and perplexity is 35.42609252341553
At time: 115.69791007041931 and batch: 600, loss is 3.579428687095642 and perplexity is 35.85305168367085
At time: 116.16064357757568 and batch: 650, loss is 3.427872405052185 and perplexity is 30.811019595057928
At time: 116.62347412109375 and batch: 700, loss is 3.4288827991485595 and perplexity is 30.842166600085235
At time: 117.08569574356079 and batch: 750, loss is 3.524283013343811 and perplexity is 33.92943793885793
At time: 117.54929423332214 and batch: 800, loss is 3.483816785812378 and perplexity is 32.58385061789525
At time: 118.01303720474243 and batch: 850, loss is 3.562326602935791 and perplexity is 35.245103187288315
At time: 118.4891586303711 and batch: 900, loss is 3.524174680709839 and perplexity is 33.925762472566895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324743401514341 and perplexity of 75.54612535106779
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 119.73244094848633 and batch: 50, loss is 3.7495770931243895 and perplexity is 42.50310334404314
At time: 120.19538497924805 and batch: 100, loss is 3.6263701915740967 and perplexity is 37.57617446011415
At time: 120.65878057479858 and batch: 150, loss is 3.645728921890259 and perplexity is 38.31068817551863
At time: 121.12067031860352 and batch: 200, loss is 3.5424681186676024 and perplexity is 34.55209270345832
At time: 121.58331799507141 and batch: 250, loss is 3.6854643487930296 and perplexity is 39.863628780742786
At time: 122.04582381248474 and batch: 300, loss is 3.6643764638900755 and perplexity is 39.03179084276949
At time: 122.50795817375183 and batch: 350, loss is 3.636500563621521 and perplexity is 37.95876973177365
At time: 122.97031807899475 and batch: 400, loss is 3.5704391288757322 and perplexity is 35.53219293815667
At time: 123.43489170074463 and batch: 450, loss is 3.6113193702697752 and perplexity is 37.01485691415658
At time: 123.89716053009033 and batch: 500, loss is 3.4798533296585084 and perplexity is 32.45496154646498
At time: 124.35928869247437 and batch: 550, loss is 3.556644248962402 and perplexity is 35.045395976046514
At time: 124.82138895988464 and batch: 600, loss is 3.570066967010498 and perplexity is 35.5189716713352
At time: 125.28332209587097 and batch: 650, loss is 3.4158055639266967 and perplexity is 30.441462096719615
At time: 125.74565243721008 and batch: 700, loss is 3.415026707649231 and perplexity is 30.417761803629457
At time: 126.20859336853027 and batch: 750, loss is 3.5093529987335206 and perplexity is 33.42663371171791
At time: 126.67099332809448 and batch: 800, loss is 3.466714315414429 and perplexity is 32.03132452532695
At time: 127.13312315940857 and batch: 850, loss is 3.543242835998535 and perplexity is 34.57887118003129
At time: 127.59601426124573 and batch: 900, loss is 3.506068162918091 and perplexity is 33.31701284994759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320400290293236 and perplexity of 75.21873159471808
finished 14 epochs...
Completing Train Step...
At time: 128.83641958236694 and batch: 50, loss is 3.740701951980591 and perplexity is 42.12755130660924
At time: 129.29944515228271 and batch: 100, loss is 3.6154972887039185 and perplexity is 37.16982546476292
At time: 129.76288843154907 and batch: 150, loss is 3.6360349798202516 and perplexity is 37.941100856960546
At time: 130.22502326965332 and batch: 200, loss is 3.5349997997283937 and perplexity is 34.295007845911854
At time: 130.69957041740417 and batch: 250, loss is 3.6776846837997437 and perplexity is 39.55470631812536
At time: 131.16153931617737 and batch: 300, loss is 3.657276973724365 and perplexity is 38.75566635884562
At time: 131.6224558353424 and batch: 350, loss is 3.6300848531723022 and perplexity is 37.7160168050793
At time: 132.0846390724182 and batch: 400, loss is 3.5653839111328125 and perplexity is 35.35302321850381
At time: 132.54694771766663 and batch: 450, loss is 3.6074495697021485 and perplexity is 36.87189359803101
At time: 133.00879001617432 and batch: 500, loss is 3.475965976715088 and perplexity is 32.3290425605079
At time: 133.47135972976685 and batch: 550, loss is 3.553060998916626 and perplexity is 34.92004427667532
At time: 133.9342541694641 and batch: 600, loss is 3.567536401748657 and perplexity is 35.42920222711113
At time: 134.39644360542297 and batch: 650, loss is 3.4140951299667357 and perplexity is 30.389438490313367
At time: 134.85873794555664 and batch: 700, loss is 3.4140095043182375 and perplexity is 30.386836486335863
At time: 135.32143354415894 and batch: 750, loss is 3.508966007232666 and perplexity is 33.41370039127325
At time: 135.78373646736145 and batch: 800, loss is 3.467048807144165 and perplexity is 32.04204053058081
At time: 136.24813747406006 and batch: 850, loss is 3.5447650957107544 and perplexity is 34.63154928723036
At time: 136.71089100837708 and batch: 900, loss is 3.5086159133911132 and perplexity is 33.40200450799059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319987257866011 and perplexity of 75.18767023455077
finished 15 epochs...
Completing Train Step...
At time: 137.96017980575562 and batch: 50, loss is 3.7370271921157836 and perplexity is 41.97302676604478
At time: 138.43629121780396 and batch: 100, loss is 3.6115296125411986 and perplexity is 37.022639819869845
At time: 138.90156483650208 and batch: 150, loss is 3.6318101358413695 and perplexity is 37.781143760268144
At time: 139.36991024017334 and batch: 200, loss is 3.5311605644226076 and perplexity is 34.16359366737198
At time: 139.83706521987915 and batch: 250, loss is 3.6738457107543945 and perplexity is 39.40314796710083
At time: 140.30063319206238 and batch: 300, loss is 3.6536373329162597 and perplexity is 38.61486604074404
At time: 140.76370573043823 and batch: 350, loss is 3.6266459941864015 and perplexity is 37.58653949647702
At time: 141.22952818870544 and batch: 400, loss is 3.562431311607361 and perplexity is 35.24879384844114
At time: 141.69634222984314 and batch: 450, loss is 3.604759707450867 and perplexity is 36.77284655448023
At time: 142.16318464279175 and batch: 500, loss is 3.4736377429962157 and perplexity is 32.253860548129225
At time: 142.64282870292664 and batch: 550, loss is 3.5508246088027953 and perplexity is 34.842036695085085
At time: 143.10926961898804 and batch: 600, loss is 3.565820894241333 and perplexity is 35.368475268381935
At time: 143.57646203041077 and batch: 650, loss is 3.412840733528137 and perplexity is 30.351341985955234
At time: 144.0434808731079 and batch: 700, loss is 3.4131132555007935 and perplexity is 30.359614520718566
At time: 144.51057124137878 and batch: 750, loss is 3.508466944694519 and perplexity is 33.39702902552017
At time: 144.97758555412292 and batch: 800, loss is 3.4669364643096925 and perplexity is 32.03844103911765
At time: 145.4443690776825 and batch: 850, loss is 3.545045771598816 and perplexity is 34.64127089232775
At time: 145.91099500656128 and batch: 900, loss is 3.5092980432510377 and perplexity is 33.42479678540955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319916189533391 and perplexity of 75.18232696206462
finished 16 epochs...
Completing Train Step...
At time: 147.1581265926361 and batch: 50, loss is 3.7338455963134765 and perplexity is 41.839697772228455
At time: 147.63745999336243 and batch: 100, loss is 3.6082872200012206 and perplexity is 36.90279229007338
At time: 148.10338354110718 and batch: 150, loss is 3.628452863693237 and perplexity is 37.654514861374004
At time: 148.56946206092834 and batch: 200, loss is 3.5280218839645388 and perplexity is 34.056533165793404
At time: 149.0358259677887 and batch: 250, loss is 3.6707663202285765 and perplexity is 39.28199691799596
At time: 149.5021195411682 and batch: 300, loss is 3.6507352924346925 and perplexity is 38.50296658321457
At time: 149.9681761264801 and batch: 350, loss is 3.6238599157333375 and perplexity is 37.481966191020724
At time: 150.43471121788025 and batch: 400, loss is 3.5599978017807006 and perplexity is 35.16311984877297
At time: 150.90123057365417 and batch: 450, loss is 3.6024486923217776 and perplexity is 36.687962072193734
At time: 151.36749529838562 and batch: 500, loss is 3.4716244745254516 and perplexity is 32.18899019026166
At time: 151.83133554458618 and batch: 550, loss is 3.5488942098617553 and perplexity is 34.77484254097343
At time: 152.2934832572937 and batch: 600, loss is 3.5642373752593994 and perplexity is 35.31251293683349
At time: 152.75532245635986 and batch: 650, loss is 3.4116064500808716 and perplexity is 30.31390293689201
At time: 153.21758127212524 and batch: 700, loss is 3.4121515941619873 and perplexity is 30.330432886846395
At time: 153.67870140075684 and batch: 750, loss is 3.50777195930481 and perplexity is 33.37382664188046
At time: 154.15352630615234 and batch: 800, loss is 3.4665320920944214 and perplexity is 32.02548820280089
At time: 154.6181879043579 and batch: 850, loss is 3.544882559776306 and perplexity is 34.63561748873506
At time: 155.0805995464325 and batch: 900, loss is 3.5094054079055788 and perplexity is 33.42838561982306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31996175687607 and perplexity of 75.18575289897558
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 156.33180737495422 and batch: 50, loss is 3.733158674240112 and perplexity is 41.81096702930631
At time: 156.7934374809265 and batch: 100, loss is 3.6087681102752684 and perplexity is 36.92054275164097
At time: 157.25511360168457 and batch: 150, loss is 3.6297547721862795 and perplexity is 37.70356951948259
At time: 157.71719932556152 and batch: 200, loss is 3.5293195104599 and perplexity is 34.100754510758804
At time: 158.17953872680664 and batch: 250, loss is 3.670758023262024 and perplexity is 39.281670997933496
At time: 158.64179873466492 and batch: 300, loss is 3.65129346370697 and perplexity is 38.52446383207403
At time: 159.1048493385315 and batch: 350, loss is 3.624192576408386 and perplexity is 37.49443704136172
At time: 159.57617235183716 and batch: 400, loss is 3.559497060775757 and perplexity is 35.14551664049486
At time: 160.04860472679138 and batch: 450, loss is 3.599559717178345 and perplexity is 36.58212441649388
At time: 160.5112853050232 and batch: 500, loss is 3.4694629096984864 and perplexity is 32.119486746412754
At time: 160.9727976322174 and batch: 550, loss is 3.544579153060913 and perplexity is 34.62511040383785
At time: 161.43484902381897 and batch: 600, loss is 3.561038866043091 and perplexity is 35.19974597797078
At time: 161.89668345451355 and batch: 650, loss is 3.407495379447937 and perplexity is 30.189536156227554
At time: 162.35824370384216 and batch: 700, loss is 3.4073326539993287 and perplexity is 30.18462395009457
At time: 162.81989550590515 and batch: 750, loss is 3.5025654554367067 and perplexity is 33.20051724390275
At time: 163.28255558013916 and batch: 800, loss is 3.4596551942825315 and perplexity is 31.80600773041131
At time: 163.74534106254578 and batch: 850, loss is 3.5366975021362306 and perplexity is 34.353280013712464
At time: 164.2071545124054 and batch: 900, loss is 3.5009692525863647 and perplexity is 33.14756475634397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319491869782748 and perplexity of 75.1504323830638
finished 18 epochs...
Completing Train Step...
At time: 165.44875597953796 and batch: 50, loss is 3.730956406593323 and perplexity is 41.71898940618267
At time: 165.91030550003052 and batch: 100, loss is 3.6059110307693483 and perplexity is 36.8152083715999
At time: 166.38608860969543 and batch: 150, loss is 3.627220811843872 and perplexity is 37.60815111383737
At time: 166.8481068611145 and batch: 200, loss is 3.526950926780701 and perplexity is 34.02007960053088
At time: 167.31103897094727 and batch: 250, loss is 3.668634281158447 and perplexity is 39.19833538233636
At time: 167.77418303489685 and batch: 300, loss is 3.6487726545333863 and perplexity is 38.427473308899366
At time: 168.24886441230774 and batch: 350, loss is 3.621995892524719 and perplexity is 37.412164012787244
At time: 168.71245503425598 and batch: 400, loss is 3.557727026939392 and perplexity is 35.08336291017493
At time: 169.17532658576965 and batch: 450, loss is 3.598321123123169 and perplexity is 36.53684206368143
At time: 169.63834190368652 and batch: 500, loss is 3.4682775068283083 and perplexity is 32.081434772550246
At time: 170.10142850875854 and batch: 550, loss is 3.5435192680358885 and perplexity is 34.588431209129254
At time: 170.56458950042725 and batch: 600, loss is 3.5603429889678955 and perplexity is 35.175259802364245
At time: 171.02744150161743 and batch: 650, loss is 3.4071281957626343 and perplexity is 30.17845308596994
At time: 171.49057340621948 and batch: 700, loss is 3.407200436592102 and perplexity is 30.180633281201533
At time: 171.95345377922058 and batch: 750, loss is 3.502621030807495 and perplexity is 33.20236242623182
At time: 172.41571402549744 and batch: 800, loss is 3.460100226402283 and perplexity is 31.820165575566655
At time: 172.87876272201538 and batch: 850, loss is 3.5376540756225587 and perplexity is 34.38615717274433
At time: 173.34182214736938 and batch: 900, loss is 3.5024669408798217 and perplexity is 33.19724667076029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319328412617723 and perplexity of 75.13814951032515
finished 19 epochs...
Completing Train Step...
At time: 174.58415579795837 and batch: 50, loss is 3.729755177497864 and perplexity is 41.668905429462235
At time: 175.06001377105713 and batch: 100, loss is 3.6045034313201905 and perplexity is 36.763423759121636
At time: 175.52347445487976 and batch: 150, loss is 3.625778470039368 and perplexity is 37.55394640555122
At time: 175.98832297325134 and batch: 200, loss is 3.525660662651062 and perplexity is 33.97621301795953
At time: 176.4511137008667 and batch: 250, loss is 3.667385387420654 and perplexity is 39.14941138354236
At time: 176.91460347175598 and batch: 300, loss is 3.6474386930465696 and perplexity is 38.376246714218105
At time: 177.37817931175232 and batch: 350, loss is 3.6208002233505248 and perplexity is 37.36745817356024
At time: 177.84118366241455 and batch: 400, loss is 3.556779761314392 and perplexity is 35.05014538187596
At time: 178.31738710403442 and batch: 450, loss is 3.5975671291351317 and perplexity is 36.50930388753782
At time: 178.7810754776001 and batch: 500, loss is 3.467588963508606 and perplexity is 32.05935291798027
At time: 179.24485969543457 and batch: 550, loss is 3.542930564880371 and perplexity is 34.56807488303665
At time: 179.70817589759827 and batch: 600, loss is 3.559933362007141 and perplexity is 35.16085401829786
At time: 180.17216730117798 and batch: 650, loss is 3.4069074630737304 and perplexity is 30.1717924500104
At time: 180.63560009002686 and batch: 700, loss is 3.407148909568787 and perplexity is 30.179078203071402
At time: 181.09999418258667 and batch: 750, loss is 3.5026728916168213 and perplexity is 33.20408437226916
At time: 181.56216526031494 and batch: 800, loss is 3.4602964305877686 and perplexity is 31.826409437751348
At time: 182.02454900741577 and batch: 850, loss is 3.5381008625030517 and perplexity is 34.40152388920771
At time: 182.4927430152893 and batch: 900, loss is 3.5031240224838256 and perplexity is 33.219067138980265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319243548667594 and perplexity of 75.13177326071305
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe3d26db70>
ELAPSED
383.04437279701233


RESULTS SO FAR:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.13177326071305, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.9680256792332449, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5177066140482257, 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6899635791778564 and batch: 50, loss is 6.63736743927002 and perplexity is 763.0834828597468
At time: 1.1691210269927979 and batch: 100, loss is 5.928130626678467 and perplexity is 375.4519974490882
At time: 1.634378433227539 and batch: 150, loss is 5.757742824554444 and perplexity is 316.63282587386107
At time: 2.100351333618164 and batch: 200, loss is 5.579185848236084 and perplexity is 264.85588509741734
At time: 2.5656161308288574 and batch: 250, loss is 5.6169823551177975 and perplexity is 275.0581019361046
At time: 3.0308079719543457 and batch: 300, loss is 5.518476572036743 and perplexity is 249.2550257269699
At time: 3.5064079761505127 and batch: 350, loss is 5.502236442565918 and perplexity is 245.23978410831674
At time: 3.9811294078826904 and batch: 400, loss is 5.352718610763549 and perplexity is 211.18163884450155
At time: 4.446088552474976 and batch: 450, loss is 5.347815017700196 and perplexity is 210.14862483524143
At time: 4.911154270172119 and batch: 500, loss is 5.290143470764161 and perplexity is 198.37188393362518
At time: 5.376319169998169 and batch: 550, loss is 5.341163206100464 and perplexity is 208.75539466433804
At time: 5.842405557632446 and batch: 600, loss is 5.2660799312591555 and perplexity is 193.65533034739343
At time: 6.309638738632202 and batch: 650, loss is 5.156652908325196 and perplexity is 173.5824857023348
At time: 6.776762247085571 and batch: 700, loss is 5.245187759399414 and perplexity is 189.65142072363508
At time: 7.243675947189331 and batch: 750, loss is 5.230626344680786 and perplexity is 186.9098368468368
At time: 7.710711479187012 and batch: 800, loss is 5.205714445114136 and perplexity is 182.3110775097516
At time: 8.17816948890686 and batch: 850, loss is 5.237650623321533 and perplexity is 188.2273655486166
At time: 8.65790867805481 and batch: 900, loss is 5.147239694595337 and perplexity is 171.95618304061207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.004458492749358 and perplexity of 149.07633538109963
finished 1 epochs...
Completing Train Step...
At time: 9.903142213821411 and batch: 50, loss is 4.94875994682312 and perplexity is 141.00000795873845
At time: 10.364641904830933 and batch: 100, loss is 4.8025245857238765 and perplexity is 121.81756853550455
At time: 10.826347827911377 and batch: 150, loss is 4.776854581832886 and perplexity is 118.7303056298385
At time: 11.287869215011597 and batch: 200, loss is 4.666683034896851 and perplexity is 106.3444160538005
At time: 11.753044128417969 and batch: 250, loss is 4.769512891769409 and perplexity is 117.86181651470575
At time: 12.21518063545227 and batch: 300, loss is 4.719490966796875 and perplexity is 112.11116983262657
At time: 12.677428960800171 and batch: 350, loss is 4.696375494003296 and perplexity is 109.54938958491805
At time: 13.139753103256226 and batch: 400, loss is 4.586732635498047 and perplexity is 98.17313813211553
At time: 13.601920366287231 and batch: 450, loss is 4.604277954101563 and perplexity is 99.91081660339866
At time: 14.064984560012817 and batch: 500, loss is 4.49793643951416 and perplexity is 89.83156703271547
At time: 14.527092933654785 and batch: 550, loss is 4.579851121902466 and perplexity is 97.49987753019667
At time: 14.98935055732727 and batch: 600, loss is 4.541432504653931 and perplexity is 93.82510880261486
At time: 15.451310873031616 and batch: 650, loss is 4.405489673614502 and perplexity is 81.89923692122858
At time: 15.913196802139282 and batch: 700, loss is 4.4434450817108155 and perplexity is 85.06750193256593
At time: 16.3745174407959 and batch: 750, loss is 4.49908260345459 and perplexity is 89.93458776360596
At time: 16.836395263671875 and batch: 800, loss is 4.453268508911133 and perplexity is 85.90727431319712
At time: 17.298582315444946 and batch: 850, loss is 4.518053894042969 and perplexity is 91.65704996688979
At time: 17.760931730270386 and batch: 900, loss is 4.456083030700683 and perplexity is 86.14940278672908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.589250956496147 and perplexity of 98.42068117291656
finished 2 epochs...
Completing Train Step...
At time: 19.006516933441162 and batch: 50, loss is 4.49041485786438 and perplexity is 89.15842628164144
At time: 19.46930456161499 and batch: 100, loss is 4.366339936256408 and perplexity is 78.75485576056299
At time: 19.931366443634033 and batch: 150, loss is 4.35675274848938 and perplexity is 78.00342597562044
At time: 20.39336848258972 and batch: 200, loss is 4.265667672157288 and perplexity is 71.21245067473406
At time: 20.8684561252594 and batch: 250, loss is 4.40113356590271 and perplexity is 81.5432509432408
At time: 21.3304500579834 and batch: 300, loss is 4.378382472991944 and perplexity is 79.70899761735154
At time: 21.793349266052246 and batch: 350, loss is 4.362107152938843 and perplexity is 78.42220803045922
At time: 22.255558252334595 and batch: 400, loss is 4.2805521249771115 and perplexity is 72.28033677159745
At time: 22.717328310012817 and batch: 450, loss is 4.310904178619385 and perplexity is 74.50782687703557
At time: 23.179691076278687 and batch: 500, loss is 4.196282558441162 and perplexity is 66.43888871366062
At time: 23.657119035720825 and batch: 550, loss is 4.281705002784729 and perplexity is 72.36371522113558
At time: 24.12253212928772 and batch: 600, loss is 4.2742025089263915 and perplexity is 71.82283839171205
At time: 24.58462357521057 and batch: 650, loss is 4.131316289901734 and perplexity is 62.25982099580112
At time: 25.047081232070923 and batch: 700, loss is 4.148515062332153 and perplexity is 63.33987466430393
At time: 25.509969234466553 and batch: 750, loss is 4.24146116733551 and perplexity is 69.50934245401888
At time: 25.972581386566162 and batch: 800, loss is 4.200242319107056 and perplexity is 66.70249237113627
At time: 26.43476915359497 and batch: 850, loss is 4.2735643529891965 and perplexity is 71.77701884252983
At time: 26.89725613594055 and batch: 900, loss is 4.2242546558380125 and perplexity is 68.32355999910642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.480721251605308 and perplexity of 88.29833503049066
finished 3 epochs...
Completing Train Step...
At time: 28.118582725524902 and batch: 50, loss is 4.287934856414795 and perplexity is 72.8159377525779
At time: 28.59184718132019 and batch: 100, loss is 4.165151925086975 and perplexity is 64.40246605051202
At time: 29.053903102874756 and batch: 150, loss is 4.159773817062378 and perplexity is 64.05703235352209
At time: 29.515397787094116 and batch: 200, loss is 4.072347040176392 and perplexity is 58.69455954532809
At time: 29.977339506149292 and batch: 250, loss is 4.213617796897888 and perplexity is 67.60066341944571
At time: 30.439457893371582 and batch: 300, loss is 4.202720308303833 and perplexity is 66.8679853868995
At time: 30.901514530181885 and batch: 350, loss is 4.186026735305786 and perplexity is 65.7609853919078
At time: 31.363356590270996 and batch: 400, loss is 4.112175350189209 and perplexity is 61.07944233229831
At time: 31.825592279434204 and batch: 450, loss is 4.145069327354431 and perplexity is 63.121997831173516
At time: 32.2882981300354 and batch: 500, loss is 4.030947031974793 and perplexity is 56.314217366325465
At time: 32.76718497276306 and batch: 550, loss is 4.116085486412048 and perplexity is 61.31873880825512
At time: 33.229487895965576 and batch: 600, loss is 4.1188635873794555 and perplexity is 61.489325299393215
At time: 33.691669940948486 and batch: 650, loss is 3.974811191558838 and perplexity is 53.24006378325801
At time: 34.15464544296265 and batch: 700, loss is 3.9841106128692627 and perplexity is 53.7374747996367
At time: 34.61875367164612 and batch: 750, loss is 4.086707682609558 and perplexity is 59.54353243566897
At time: 35.080249547958374 and batch: 800, loss is 4.049122896194458 and perplexity is 57.347135589239635
At time: 35.5425443649292 and batch: 850, loss is 4.122092390060425 and perplexity is 61.688183061247805
At time: 36.00411009788513 and batch: 900, loss is 4.079725832939148 and perplexity is 59.1292563329585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.438240782855308 and perplexity of 84.6259352487533
finished 4 epochs...
Completing Train Step...
At time: 37.22683119773865 and batch: 50, loss is 4.154693431854248 and perplexity is 63.732423221780564
At time: 37.70235466957092 and batch: 100, loss is 4.034563207626343 and perplexity is 56.51822811503674
At time: 38.164549112319946 and batch: 150, loss is 4.032786583900451 and perplexity is 56.417905634060254
At time: 38.626595973968506 and batch: 200, loss is 3.9465831899642945 and perplexity is 51.758216369312905
At time: 39.08864712715149 and batch: 250, loss is 4.093102769851685 and perplexity is 59.92553870008068
At time: 39.55158877372742 and batch: 300, loss is 4.082212357521057 and perplexity is 59.27646562625649
At time: 40.01420545578003 and batch: 350, loss is 4.063305945396423 and perplexity is 58.16628814378093
At time: 40.476739168167114 and batch: 400, loss is 3.9948368501663207 and perplexity is 54.316978094930334
At time: 40.939369201660156 and batch: 450, loss is 4.031857862472534 and perplexity is 56.36553343953368
At time: 41.40175676345825 and batch: 500, loss is 3.9179661226272584 and perplexity is 50.29804062789812
At time: 41.86412739753723 and batch: 550, loss is 4.00024582862854 and perplexity is 54.61157347135279
At time: 42.326207399368286 and batch: 600, loss is 4.010541939735413 and perplexity is 55.17676494365041
At time: 42.78882884979248 and batch: 650, loss is 3.865109477043152 and perplexity is 47.70849513178092
At time: 43.251644134521484 and batch: 700, loss is 3.871738085746765 and perplexity is 48.02578651629065
At time: 43.71453595161438 and batch: 750, loss is 3.975251932144165 and perplexity is 53.26353401189392
At time: 44.19407796859741 and batch: 800, loss is 3.944229521751404 and perplexity is 51.63653795217222
At time: 44.65673232078552 and batch: 850, loss is 4.0141145658493045 and perplexity is 55.37424344334501
At time: 45.11897850036621 and batch: 900, loss is 3.9774554681777956 and perplexity is 53.38103153583844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425396644905822 and perplexity of 83.54593872188269
finished 5 epochs...
Completing Train Step...
At time: 46.351582288742065 and batch: 50, loss is 4.056995177268982 and perplexity is 57.80037001312393
At time: 46.8133499622345 and batch: 100, loss is 3.939505491256714 and perplexity is 51.39318063850411
At time: 47.28032159805298 and batch: 150, loss is 3.9401790285110474 and perplexity is 51.4278075202212
At time: 47.743797063827515 and batch: 200, loss is 3.8533497524261473 and perplexity is 47.15074230463917
At time: 48.2081196308136 and batch: 250, loss is 3.999866042137146 and perplexity is 54.59083667150211
At time: 48.67210912704468 and batch: 300, loss is 3.991788778305054 and perplexity is 54.15166810875643
At time: 49.13430953025818 and batch: 350, loss is 3.971218104362488 and perplexity is 53.049110852448784
At time: 49.59766149520874 and batch: 400, loss is 3.9077573871612548 and perplexity is 49.787173328032445
At time: 50.061267614364624 and batch: 450, loss is 3.9462505912780763 and perplexity is 51.74100451702588
At time: 50.52462339401245 and batch: 500, loss is 3.833878779411316 and perplexity is 46.241551611414295
At time: 50.988701581954956 and batch: 550, loss is 3.9124673128128054 and perplexity is 50.02222030519027
At time: 51.45203971862793 and batch: 600, loss is 3.9252883052825926 and perplexity is 50.6676837141652
At time: 51.91541266441345 and batch: 650, loss is 3.7845591163635253 and perplexity is 44.016260231341654
At time: 52.37889361381531 and batch: 700, loss is 3.785349016189575 and perplexity is 44.05104240304894
At time: 52.8415253162384 and batch: 750, loss is 3.891158313751221 and perplexity is 48.96757350049104
At time: 53.30391478538513 and batch: 800, loss is 3.8616798448562624 and perplexity is 47.545152803450065
At time: 53.77300310134888 and batch: 850, loss is 3.935012168884277 and perplexity is 51.162772546727666
At time: 54.245373249053955 and batch: 900, loss is 3.8974269914627073 and perplexity is 49.27549957337126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.419588689934717 and perplexity of 83.06211404777203
finished 6 epochs...
Completing Train Step...
At time: 55.48060131072998 and batch: 50, loss is 3.978220639228821 and perplexity is 53.42189278677305
At time: 55.94433569908142 and batch: 100, loss is 3.864099078178406 and perplexity is 47.6603148672023
At time: 56.4203577041626 and batch: 150, loss is 3.8659099435806272 and perplexity is 47.74669947429616
At time: 56.884111404418945 and batch: 200, loss is 3.77800395488739 and perplexity is 43.72867016741835
At time: 57.34734606742859 and batch: 250, loss is 3.924749150276184 and perplexity is 50.64037334175116
At time: 57.81061267852783 and batch: 300, loss is 3.9210949659347536 and perplexity is 50.455661773041285
At time: 58.274436235427856 and batch: 350, loss is 3.90199098110199 and perplexity is 49.50090642890262
At time: 58.738147258758545 and batch: 400, loss is 3.8392043590545653 and perplexity is 46.48847158976246
At time: 59.20163345336914 and batch: 450, loss is 3.87890043258667 and perplexity is 48.37099864532236
At time: 59.66436815261841 and batch: 500, loss is 3.7678411912918093 and perplexity is 43.286516586840854
At time: 60.12757110595703 and batch: 550, loss is 3.8444502782821655 and perplexity is 46.73298715042884
At time: 60.590893268585205 and batch: 600, loss is 3.858604226112366 and perplexity is 47.39914668497824
At time: 61.05436730384827 and batch: 650, loss is 3.7174275159835815 and perplexity is 41.15837853446889
At time: 61.51630163192749 and batch: 700, loss is 3.717687029838562 and perplexity is 41.16906109002275
At time: 61.9797899723053 and batch: 750, loss is 3.825932002067566 and perplexity is 45.875536543046266
At time: 62.443116188049316 and batch: 800, loss is 3.797775387763977 and perplexity is 44.60185222017287
At time: 62.906572103500366 and batch: 850, loss is 3.869126853942871 and perplexity is 47.90054364538053
At time: 63.369117975234985 and batch: 900, loss is 3.834104228019714 and perplexity is 46.25197788012499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424616147394049 and perplexity of 83.48075676507801
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 64.58750009536743 and batch: 50, loss is 3.9341846990585325 and perplexity is 51.12045440715023
At time: 65.06217050552368 and batch: 100, loss is 3.80208758354187 and perplexity is 44.79459942215994
At time: 65.52526545524597 and batch: 150, loss is 3.8069218969345093 and perplexity is 45.01167483659119
At time: 65.98802828788757 and batch: 200, loss is 3.699759578704834 and perplexity is 40.43758113564911
At time: 66.45063924789429 and batch: 250, loss is 3.8385015726089478 and perplexity is 46.45581159989506
At time: 66.91349744796753 and batch: 300, loss is 3.8239756488800047 and perplexity is 45.785875523806354
At time: 67.3756206035614 and batch: 350, loss is 3.7955856275558473 and perplexity is 44.50429171501225
At time: 67.83745670318604 and batch: 400, loss is 3.7212671279907226 and perplexity is 41.31671451873357
At time: 68.31655168533325 and batch: 450, loss is 3.747865290641785 and perplexity is 42.43040866343901
At time: 68.77864193916321 and batch: 500, loss is 3.629502592086792 and perplexity is 37.694062628344895
At time: 69.24126672744751 and batch: 550, loss is 3.6903403568267823 and perplexity is 40.058478814042246
At time: 69.70418334007263 and batch: 600, loss is 3.699671630859375 and perplexity is 40.43402489389684
At time: 70.16726183891296 and batch: 650, loss is 3.5497764110565186 and perplexity is 34.805534484854995
At time: 70.63016557693481 and batch: 700, loss is 3.535880846977234 and perplexity is 34.32523668278481
At time: 71.09346556663513 and batch: 750, loss is 3.627579073905945 and perplexity is 37.621627101429574
At time: 71.55637502670288 and batch: 800, loss is 3.5935462856292726 and perplexity is 36.362800421232684
At time: 72.01978468894958 and batch: 850, loss is 3.6509602069854736 and perplexity is 38.511627434586615
At time: 72.48281455039978 and batch: 900, loss is 3.6023762893676756 and perplexity is 36.68530585152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341992103890197 and perplexity of 76.860501033027
finished 8 epochs...
Completing Train Step...
At time: 73.77070879936218 and batch: 50, loss is 3.843948817253113 and perplexity is 46.70955825343204
At time: 74.247891664505 and batch: 100, loss is 3.7201294755935668 and perplexity is 41.2697371864228
At time: 74.71050500869751 and batch: 150, loss is 3.7300390243530273 and perplexity is 41.68073469599712
At time: 75.17312240600586 and batch: 200, loss is 3.6274426794052124 and perplexity is 37.61649606831464
At time: 75.63534998893738 and batch: 250, loss is 3.7700141954421995 and perplexity is 43.3806806394254
At time: 76.09796142578125 and batch: 300, loss is 3.7612066507339477 and perplexity is 43.00028100765476
At time: 76.56121397018433 and batch: 350, loss is 3.7353908109664915 and perplexity is 41.90439906212055
At time: 77.02356195449829 and batch: 400, loss is 3.6659853410720826 and perplexity is 39.09463874414978
At time: 77.48662757873535 and batch: 450, loss is 3.698240694999695 and perplexity is 40.376207773885554
At time: 77.94913387298584 and batch: 500, loss is 3.5820813179016113 and perplexity is 35.94828284376867
At time: 78.41165137290955 and batch: 550, loss is 3.6468538093566893 and perplexity is 38.35380763620119
At time: 78.87534213066101 and batch: 600, loss is 3.662380938529968 and perplexity is 38.953979577290276
At time: 79.33831977844238 and batch: 650, loss is 3.5163996696472166 and perplexity is 33.66301206133313
At time: 79.81341004371643 and batch: 700, loss is 3.5084719371795656 and perplexity is 33.397195760104395
At time: 80.27620935440063 and batch: 750, loss is 3.604606618881226 and perplexity is 36.76721748288383
At time: 80.73950457572937 and batch: 800, loss is 3.5750003480911254 and perplexity is 35.694633240638765
At time: 81.20229363441467 and batch: 850, loss is 3.6397168684005736 and perplexity is 38.081053249385306
At time: 81.6652843952179 and batch: 900, loss is 3.59803382396698 and perplexity is 36.52634656753252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340515554767766 and perplexity of 76.7470964719833
finished 9 epochs...
Completing Train Step...
At time: 82.89491128921509 and batch: 50, loss is 3.8051450490951537 and perplexity is 44.93176695251129
At time: 83.35710763931274 and batch: 100, loss is 3.682587823867798 and perplexity is 39.749124824640916
At time: 83.81902265548706 and batch: 150, loss is 3.693488893508911 and perplexity is 40.184803168092415
At time: 84.28146409988403 and batch: 200, loss is 3.5916964912414553 and perplexity is 36.295598890757304
At time: 84.74399495124817 and batch: 250, loss is 3.735025124549866 and perplexity is 41.889077994110636
At time: 85.20595026016235 and batch: 300, loss is 3.727990174293518 and perplexity is 41.595424543932715
At time: 85.66821002960205 and batch: 350, loss is 3.702309374809265 and perplexity is 40.54082028595928
At time: 86.13080954551697 and batch: 400, loss is 3.634790997505188 and perplexity is 37.893932143086154
At time: 86.59206748008728 and batch: 450, loss is 3.669428997039795 and perplexity is 39.229499303578486
At time: 87.05410552024841 and batch: 500, loss is 3.554052209854126 and perplexity is 34.95467456662607
At time: 87.5161497592926 and batch: 550, loss is 3.6203989505767824 and perplexity is 37.35246663801991
At time: 87.97852206230164 and batch: 600, loss is 3.638449387550354 and perplexity is 38.032816819468806
At time: 88.44037127494812 and batch: 650, loss is 3.4937366771698 and perplexity is 32.9086873849784
At time: 88.9026608467102 and batch: 700, loss is 3.4890695667266844 and perplexity is 32.755456756685504
At time: 89.36519169807434 and batch: 750, loss is 3.5867726469039916 and perplexity is 36.117324270102536
At time: 89.82813620567322 and batch: 800, loss is 3.5589229345321653 and perplexity is 35.1253444682878
At time: 90.29098296165466 and batch: 850, loss is 3.6267260313034058 and perplexity is 37.58954793512826
At time: 90.75314378738403 and batch: 900, loss is 3.5879054498672485 and perplexity is 36.158261264457195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34258573349208 and perplexity of 76.9061412470065
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 91.98720026016235 and batch: 50, loss is 3.7959323740005493 and perplexity is 44.51972609569208
At time: 92.46230483055115 and batch: 100, loss is 3.673951654434204 and perplexity is 39.40732270273206
At time: 92.9302146434784 and batch: 150, loss is 3.687800259590149 and perplexity is 39.95685550388787
At time: 93.39281964302063 and batch: 200, loss is 3.578722710609436 and perplexity is 35.82774920475193
At time: 93.85509729385376 and batch: 250, loss is 3.721873178482056 and perplexity is 41.34176212315775
At time: 94.31723856925964 and batch: 300, loss is 3.711443772315979 and perplexity is 40.91283272179606
At time: 94.78043699264526 and batch: 350, loss is 3.6778255605697634 and perplexity is 39.56027904991555
At time: 95.24257946014404 and batch: 400, loss is 3.6102283143997194 and perplexity is 36.97449366052756
At time: 95.70504808425903 and batch: 450, loss is 3.6377971839904784 and perplexity is 38.008019768188596
At time: 96.16739130020142 and batch: 500, loss is 3.522371602058411 and perplexity is 33.86464676928684
At time: 96.64340901374817 and batch: 550, loss is 3.5820619249343872 and perplexity is 35.94758570665752
At time: 97.10788893699646 and batch: 600, loss is 3.598590669631958 and perplexity is 36.54669176931913
At time: 97.58511924743652 and batch: 650, loss is 3.4486037635803224 and perplexity is 31.456441004118602
At time: 98.05096101760864 and batch: 700, loss is 3.442299542427063 and perplexity is 31.25875642171501
At time: 98.51296901702881 and batch: 750, loss is 3.534137191772461 and perplexity is 34.26543745495603
At time: 98.97456049919128 and batch: 800, loss is 3.5023556423187254 and perplexity is 33.19355207057914
At time: 99.43636012077332 and batch: 850, loss is 3.5669019508361814 and perplexity is 35.40673126654435
At time: 99.89712595939636 and batch: 900, loss is 3.5264697456359864 and perplexity is 34.00371371747836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327598676289598 and perplexity of 75.76213853889864
finished 11 epochs...
Completing Train Step...
At time: 101.12748265266418 and batch: 50, loss is 3.7725272226333617 and perplexity is 43.48983456539035
At time: 101.60435032844543 and batch: 100, loss is 3.647138447761536 and perplexity is 38.364726156667956
At time: 102.06762075424194 and batch: 150, loss is 3.663811674118042 and perplexity is 39.00975231067194
At time: 102.53060030937195 and batch: 200, loss is 3.5565683126449583 and perplexity is 35.04273485877173
At time: 102.99431610107422 and batch: 250, loss is 3.7015918684005737 and perplexity is 40.51174242061535
At time: 103.47025489807129 and batch: 300, loss is 3.6922984743118286 and perplexity is 40.13699486857465
At time: 103.93281650543213 and batch: 350, loss is 3.6595228147506713 and perplexity is 38.84280323551733
At time: 104.39818334579468 and batch: 400, loss is 3.5935824584960936 and perplexity is 36.36411579175978
At time: 104.86121273040771 and batch: 450, loss is 3.623770718574524 and perplexity is 37.478623055231054
At time: 105.32375502586365 and batch: 500, loss is 3.5091351795196535 and perplexity is 33.41935354154985
At time: 105.78637146949768 and batch: 550, loss is 3.5707245445251465 and perplexity is 35.54233582948033
At time: 106.2495367527008 and batch: 600, loss is 3.5892995262145995 and perplexity is 36.208703793446226
At time: 106.71260690689087 and batch: 650, loss is 3.4409638595581056 and perplexity is 31.21703250741951
At time: 107.17560982704163 and batch: 700, loss is 3.437230830192566 and perplexity is 31.100715650563323
At time: 107.63857889175415 and batch: 750, loss is 3.5306457710266113 and perplexity is 34.146011001077056
At time: 108.101478099823 and batch: 800, loss is 3.500454831123352 and perplexity is 33.130517322745504
At time: 108.56421136856079 and batch: 850, loss is 3.567622618675232 and perplexity is 35.43225695572092
At time: 109.02710485458374 and batch: 900, loss is 3.5293348455429077 and perplexity is 34.10127745266953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326885066620291 and perplexity of 75.70809323019088
finished 12 epochs...
Completing Train Step...
At time: 110.25099396705627 and batch: 50, loss is 3.7604034614562987 and perplexity is 42.96575750932133
At time: 110.7280924320221 and batch: 100, loss is 3.635023317337036 and perplexity is 37.9027366777241
At time: 111.1915373802185 and batch: 150, loss is 3.6517129230499266 and perplexity is 38.54062666795006
At time: 111.65398669242859 and batch: 200, loss is 3.5449806356430056 and perplexity is 34.63901457352223
At time: 112.11678862571716 and batch: 250, loss is 3.690194773674011 and perplexity is 40.05264739888948
At time: 112.57975554466248 and batch: 300, loss is 3.6813762378692627 and perplexity is 39.70099450444809
At time: 113.04275393486023 and batch: 350, loss is 3.648777976036072 and perplexity is 38.42767780134589
At time: 113.50648069381714 and batch: 400, loss is 3.5838710117340087 and perplexity is 36.0126768694669
At time: 113.97460389137268 and batch: 450, loss is 3.614881000518799 and perplexity is 37.146925197792825
At time: 114.44258689880371 and batch: 500, loss is 3.5006805753707884 and perplexity is 33.13799719068252
At time: 114.90614652633667 and batch: 550, loss is 3.563160891532898 and perplexity is 35.274520044349266
At time: 115.38221883773804 and batch: 600, loss is 3.5827871131896973 and perplexity is 35.97366392828589
At time: 115.84634780883789 and batch: 650, loss is 3.4352349042892456 and perplexity is 31.03870283366517
At time: 116.30937457084656 and batch: 700, loss is 3.433001103401184 and perplexity is 30.969445933572068
At time: 116.77223038673401 and batch: 750, loss is 3.5268290138244627 and perplexity is 34.015932364861335
At time: 117.23589897155762 and batch: 800, loss is 3.4975222635269163 and perplexity is 33.033502162476495
At time: 117.69894599914551 and batch: 850, loss is 3.565982322692871 and perplexity is 35.374185207438686
At time: 118.16154289245605 and batch: 900, loss is 3.5286773300170897 and perplexity is 34.07886270312463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327193168744649 and perplexity of 75.73142264828324
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 119.39940404891968 and batch: 50, loss is 3.7597569751739504 and perplexity is 42.93798971319761
At time: 119.86214518547058 and batch: 100, loss is 3.6370652961730956 and perplexity is 37.98021233875811
At time: 120.32535433769226 and batch: 150, loss is 3.6529855298995972 and perplexity is 38.5897049554971
At time: 120.78894066810608 and batch: 200, loss is 3.5466490888595583 and perplexity is 34.6968563885688
At time: 121.2525224685669 and batch: 250, loss is 3.689442629814148 and perplexity is 40.02253337253683
At time: 121.7149612903595 and batch: 300, loss is 3.679444069862366 and perplexity is 39.62435957264791
At time: 122.17834901809692 and batch: 350, loss is 3.6454142332077026 and perplexity is 38.29863413226382
At time: 122.64286589622498 and batch: 400, loss is 3.5825290298461914 and perplexity is 35.9643809227672
At time: 123.1059958934784 and batch: 450, loss is 3.6095903539657592 and perplexity is 36.95091291909622
At time: 123.57957053184509 and batch: 500, loss is 3.4933281993865966 and perplexity is 32.89524766240817
At time: 124.05149555206299 and batch: 550, loss is 3.5519747734069824 and perplexity is 34.882133827160274
At time: 124.51339769363403 and batch: 600, loss is 3.5726401090621946 and perplexity is 35.61048471851795
At time: 124.97747015953064 and batch: 650, loss is 3.4227153348922728 and perplexity is 30.652534017178443
At time: 125.44145917892456 and batch: 700, loss is 3.4203687953948974 and perplexity is 30.580690959663993
At time: 125.90465378761292 and batch: 750, loss is 3.511178722381592 and perplexity is 33.487717251224794
At time: 126.36728024482727 and batch: 800, loss is 3.480435276031494 and perplexity is 32.473854090312834
At time: 126.83045029640198 and batch: 850, loss is 3.545965061187744 and perplexity is 34.67313089404652
At time: 127.30597305297852 and batch: 900, loss is 3.509622473716736 and perplexity is 33.43564256705237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323898524454195 and perplexity of 75.48232511824753
finished 14 epochs...
Completing Train Step...
At time: 128.53995323181152 and batch: 50, loss is 3.750111608505249 and perplexity is 42.52582797930727
At time: 129.00259280204773 and batch: 100, loss is 3.627324118614197 and perplexity is 37.612036491156275
At time: 129.46610021591187 and batch: 150, loss is 3.6441376781463624 and perplexity is 38.24977500933523
At time: 129.9288625717163 and batch: 200, loss is 3.5380723762512205 and perplexity is 34.40054393269254
At time: 130.39187932014465 and batch: 250, loss is 3.68150288105011 and perplexity is 39.70602268306047
At time: 130.8552565574646 and batch: 300, loss is 3.671953406333923 and perplexity is 39.32865571925573
At time: 131.3175458908081 and batch: 350, loss is 3.638862929344177 and perplexity is 38.04854823133442
At time: 131.77967166900635 and batch: 400, loss is 3.5767719459533693 and perplexity is 35.757925824530666
At time: 132.24208450317383 and batch: 450, loss is 3.604874305725098 and perplexity is 36.77706090070773
At time: 132.70481181144714 and batch: 500, loss is 3.4890790462493895 and perplexity is 32.75576726425327
At time: 133.16734409332275 and batch: 550, loss is 3.5487122631072996 and perplexity is 34.768515946805486
At time: 133.62964463233948 and batch: 600, loss is 3.5701831436157225 and perplexity is 35.52309838459417
At time: 134.09264659881592 and batch: 650, loss is 3.4209656000137327 and perplexity is 30.598947104409785
At time: 134.55582785606384 and batch: 700, loss is 3.4191947412490844 and perplexity is 30.544808640675353
At time: 135.0191216468811 and batch: 750, loss is 3.510604019165039 and perplexity is 33.46847728157128
At time: 135.48175024986267 and batch: 800, loss is 3.480850381851196 and perplexity is 32.48733697435209
At time: 135.94375085830688 and batch: 850, loss is 3.547406625747681 and perplexity is 34.72315049530549
At time: 136.40531396865845 and batch: 900, loss is 3.5121538496017455 and perplexity is 33.5203879623183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32355739645762 and perplexity of 75.45658037527902
finished 15 epochs...
Completing Train Step...
At time: 137.62752747535706 and batch: 50, loss is 3.7465098237991334 and perplexity is 42.37293461226062
At time: 138.1032555103302 and batch: 100, loss is 3.6235738229751586 and perplexity is 37.47124440571686
At time: 138.56510996818542 and batch: 150, loss is 3.6402092790603637 and perplexity is 38.099809383422524
At time: 139.04055857658386 and batch: 200, loss is 3.534302439689636 and perplexity is 34.271100214993325
At time: 139.50236678123474 and batch: 250, loss is 3.677723431587219 and perplexity is 39.55623900517336
At time: 139.96481847763062 and batch: 300, loss is 3.6682529163360598 and perplexity is 39.183389366248925
At time: 140.42703294754028 and batch: 350, loss is 3.6354661130905153 and perplexity is 37.919523564877124
At time: 140.8892548084259 and batch: 400, loss is 3.573657555580139 and perplexity is 35.64673492039078
At time: 141.35119533538818 and batch: 450, loss is 3.6021607160568236 and perplexity is 36.67739833103378
At time: 141.81246495246887 and batch: 500, loss is 3.4866232967376707 and perplexity is 32.675425994181694
At time: 142.27455973625183 and batch: 550, loss is 3.5466240501403807 and perplexity is 34.69598763460163
At time: 142.7369420528412 and batch: 600, loss is 3.5685243034362792 and perplexity is 35.46422009003737
At time: 143.1992290019989 and batch: 650, loss is 3.419633460044861 and perplexity is 30.558212162312866
At time: 143.66153383255005 and batch: 700, loss is 3.418307385444641 and perplexity is 30.517716549372366
At time: 144.1243348121643 and batch: 750, loss is 3.509948477745056 and perplexity is 33.44654449815888
At time: 144.58702731132507 and batch: 800, loss is 3.480644955635071 and perplexity is 32.48066390908021
At time: 145.04879522323608 and batch: 850, loss is 3.5476362895965576 and perplexity is 34.73112606350816
At time: 145.51158809661865 and batch: 900, loss is 3.512839288711548 and perplexity is 33.54337202339074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323487164223031 and perplexity of 75.45128107711777
finished 16 epochs...
Completing Train Step...
At time: 146.73748111724854 and batch: 50, loss is 3.743324694633484 and perplexity is 42.23818605209665
At time: 147.21282410621643 and batch: 100, loss is 3.6204185771942137 and perplexity is 37.353199747786945
At time: 147.67485427856445 and batch: 150, loss is 3.637001404762268 and perplexity is 37.97778580692635
At time: 148.13833284378052 and batch: 200, loss is 3.5312685918807984 and perplexity is 34.16728447290911
At time: 148.60119771957397 and batch: 250, loss is 3.6746991539001463 and perplexity is 39.43679066767849
At time: 149.06382608413696 and batch: 300, loss is 3.6652928447723387 and perplexity is 39.067575223255204
At time: 149.5252275466919 and batch: 350, loss is 3.6326764965057374 and perplexity is 37.813890040073005
At time: 149.98658967018127 and batch: 400, loss is 3.5711505603790283 and perplexity is 35.55748065376625
At time: 150.44945430755615 and batch: 450, loss is 3.5999023485183717 and perplexity is 36.59466074635084
At time: 150.928790807724 and batch: 500, loss is 3.4845251131057737 and perplexity is 32.60693882465001
At time: 151.39084887504578 and batch: 550, loss is 3.544762134552002 and perplexity is 34.63144673786691
At time: 151.9983057975769 and batch: 600, loss is 3.566992402076721 and perplexity is 35.409933994154024
At time: 152.46120405197144 and batch: 650, loss is 3.4183226585388184 and perplexity is 30.518182652890715
At time: 152.92411255836487 and batch: 700, loss is 3.4173605680465697 and perplexity is 30.488835519082553
At time: 153.386310338974 and batch: 750, loss is 3.5091282320022583 and perplexity is 33.419121360816334
At time: 153.8478720188141 and batch: 800, loss is 3.4801242923736573 and perplexity is 32.46375682250779
At time: 154.30957508087158 and batch: 850, loss is 3.547422127723694 and perplexity is 34.72368877692377
At time: 154.78653192520142 and batch: 900, loss is 3.5129681634902954 and perplexity is 33.5476951966067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3235364940068495 and perplexity of 75.45500316430628
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 156.02819800376892 and batch: 50, loss is 3.742839698791504 and perplexity is 42.21770567433908
At time: 156.5104513168335 and batch: 100, loss is 3.621406955718994 and perplexity is 37.39013709928499
At time: 156.98284482955933 and batch: 150, loss is 3.637902135848999 and perplexity is 38.01200898983642
At time: 157.44335222244263 and batch: 200, loss is 3.5327206087112426 and perplexity is 34.2169319808
At time: 157.90411138534546 and batch: 250, loss is 3.6750867223739623 and perplexity is 39.45207808671923
At time: 158.36509680747986 and batch: 300, loss is 3.6646445322036745 and perplexity is 39.04225543166714
At time: 158.82619547843933 and batch: 350, loss is 3.63273099899292 and perplexity is 37.81595104729474
At time: 159.28743076324463 and batch: 400, loss is 3.5707531499862672 and perplexity is 35.54335254892783
At time: 159.7487428188324 and batch: 450, loss is 3.599067063331604 and perplexity is 36.564106530832916
At time: 160.20974898338318 and batch: 500, loss is 3.481758737564087 and perplexity is 32.51686043936339
At time: 160.6710648536682 and batch: 550, loss is 3.5417332315444945 and perplexity is 34.52671014325478
At time: 161.1321461200714 and batch: 600, loss is 3.5643258905410766 and perplexity is 35.31563877220288
At time: 161.59327673912048 and batch: 650, loss is 3.41313955783844 and perplexity is 30.36041306005219
At time: 162.05455613136292 and batch: 700, loss is 3.4128062200546263 and perplexity is 30.350294473794335
At time: 162.51472640037537 and batch: 750, loss is 3.503335704803467 and perplexity is 33.226099772484396
At time: 162.97581553459167 and batch: 800, loss is 3.4726142072677613 and perplexity is 32.220864458696475
At time: 163.43738102912903 and batch: 850, loss is 3.539989433288574 and perplexity is 34.46655499079276
At time: 163.8985230922699 and batch: 900, loss is 3.50455237865448 and perplexity is 33.26654970144433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32341525979238 and perplexity of 75.44585599075629
finished 18 epochs...
Completing Train Step...
At time: 165.13481974601746 and batch: 50, loss is 3.7398634719848634 and perplexity is 42.09224300229172
At time: 165.59708976745605 and batch: 100, loss is 3.6186731910705565 and perplexity is 37.28806085407001
At time: 166.059556722641 and batch: 150, loss is 3.6356625986099242 and perplexity is 37.926974934179626
At time: 166.52175784111023 and batch: 200, loss is 3.5302276039123535 and perplexity is 34.13173524722674
At time: 166.9842541217804 and batch: 250, loss is 3.6728649950027465 and perplexity is 39.36452362207251
At time: 167.4468822479248 and batch: 300, loss is 3.662200937271118 and perplexity is 38.94696844295457
At time: 167.90928173065186 and batch: 350, loss is 3.6304023122787474 and perplexity is 37.727991998789484
At time: 168.37125635147095 and batch: 400, loss is 3.568869848251343 and perplexity is 35.47647668488921
At time: 168.83355045318604 and batch: 450, loss is 3.5974262809753417 and perplexity is 36.504161981392556
At time: 169.29655027389526 and batch: 500, loss is 3.4805595207214357 and perplexity is 32.47788904490115
At time: 169.75890946388245 and batch: 550, loss is 3.5406913471221926 and perplexity is 34.49075603506823
At time: 170.22158193588257 and batch: 600, loss is 3.563511400222778 and perplexity is 35.286886237258585
At time: 170.68436884880066 and batch: 650, loss is 3.4128396987915037 and perplexity is 30.35131058032606
At time: 171.14756298065186 and batch: 700, loss is 3.412556128501892 and perplexity is 30.34270507058582
At time: 171.6221091747284 and batch: 750, loss is 3.503582172393799 and perplexity is 33.23428993849523
At time: 172.08431839942932 and batch: 800, loss is 3.4731728076934814 and perplexity is 32.23886807524596
At time: 172.5470907688141 and batch: 850, loss is 3.540925393104553 and perplexity is 34.49882940267958
At time: 173.0105447769165 and batch: 900, loss is 3.5060774278640747 and perplexity is 33.317321531701936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323177807951627 and perplexity of 75.42794336015177
finished 19 epochs...
Completing Train Step...
At time: 174.24104571342468 and batch: 50, loss is 3.7384539270401 and perplexity is 42.03295388910536
At time: 174.71681427955627 and batch: 100, loss is 3.6173039627075196 and perplexity is 37.2370399211713
At time: 175.17982935905457 and batch: 150, loss is 3.634423060417175 and perplexity is 37.879992124722214
At time: 175.6434473991394 and batch: 200, loss is 3.5288960361480712 and perplexity is 34.086316774430514
At time: 176.10632991790771 and batch: 250, loss is 3.6715342664718627 and perplexity is 39.31217496603412
At time: 176.56954956054688 and batch: 300, loss is 3.6609109115600584 and perplexity is 38.89675824548136
At time: 177.03230237960815 and batch: 350, loss is 3.62916898727417 and perplexity is 37.6814898049386
At time: 177.49469995498657 and batch: 400, loss is 3.5677983713150025 and perplexity is 35.4384848156793
At time: 177.95834851264954 and batch: 450, loss is 3.596459994316101 and perplexity is 36.468905533324154
At time: 178.42165184020996 and batch: 500, loss is 3.479814262390137 and perplexity is 32.45369364453915
At time: 178.884694814682 and batch: 550, loss is 3.5400822830200194 and perplexity is 34.46975534974143
At time: 179.34764409065247 and batch: 600, loss is 3.5630539083480834 and perplexity is 35.27074646571044
At time: 179.81051516532898 and batch: 650, loss is 3.412615237236023 and perplexity is 30.344498642480005
At time: 180.2736873626709 and batch: 700, loss is 3.4123909950256346 and perplexity is 30.33769488790428
At time: 180.73649525642395 and batch: 750, loss is 3.503650636672974 and perplexity is 33.23656537809203
At time: 181.21045541763306 and batch: 800, loss is 3.473427176475525 and perplexity is 32.24706967992459
At time: 181.6840922832489 and batch: 850, loss is 3.5413663673400877 and perplexity is 34.51404585238683
At time: 182.14694046974182 and batch: 900, loss is 3.506783480644226 and perplexity is 33.3408536256692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323085001070205 and perplexity of 75.42094345278127
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe3d26db70>
ELAPSED
571.6208307743073


RESULTS SO FAR:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.13177326071305, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.42094345278127, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.9680256792332449, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5177066140482257, 'tune_wordvecs': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.11885787751686028, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.207900223189446, 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6822555065155029 and batch: 50, loss is 6.6405691719055175 and perplexity is 765.5305875474412
At time: 1.1609389781951904 and batch: 100, loss is 5.9208801651000975 and perplexity is 372.73964196386237
At time: 1.6278448104858398 and batch: 150, loss is 5.640414638519287 and perplexity is 281.5794479587009
At time: 2.0953338146209717 and batch: 200, loss is 5.387932157516479 and perplexity is 218.7505758117356
At time: 2.562196731567383 and batch: 250, loss is 5.387525653839111 and perplexity is 218.66167096953862
At time: 3.0424728393554688 and batch: 300, loss is 5.277988748550415 and perplexity is 195.97532306027773
At time: 3.509216070175171 and batch: 350, loss is 5.236173763275146 and perplexity is 187.9495852446067
At time: 3.976163148880005 and batch: 400, loss is 5.071152935028076 and perplexity is 159.35795083349825
At time: 4.443198204040527 and batch: 450, loss is 5.063735246658325 and perplexity is 158.18025649968456
At time: 4.911108732223511 and batch: 500, loss is 4.984825487136841 and perplexity is 146.17806284784166
At time: 5.390340805053711 and batch: 550, loss is 5.035370349884033 and perplexity is 153.75652578008769
At time: 5.86545467376709 and batch: 600, loss is 4.961505680084229 and perplexity is 142.80865825199777
At time: 6.333220720291138 and batch: 650, loss is 4.83617527961731 and perplexity is 125.9865656721408
At time: 6.800203561782837 and batch: 700, loss is 4.899387645721435 and perplexity is 134.20757193648
At time: 7.269466400146484 and batch: 750, loss is 4.9068489837646485 and perplexity is 135.2126850801109
At time: 7.738942623138428 and batch: 800, loss is 4.8608536052703855 and perplexity is 129.13438486590573
At time: 8.208775758743286 and batch: 850, loss is 4.902049551010132 and perplexity is 134.5652956841406
At time: 8.678342580795288 and batch: 900, loss is 4.829802474975586 and perplexity is 125.18623079916803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.800527494247645 and perplexity of 121.57453047312569
finished 1 epochs...
Completing Train Step...
At time: 9.937190294265747 and batch: 50, loss is 4.775768985748291 and perplexity is 118.60148241256586
At time: 10.400226593017578 and batch: 100, loss is 4.646541061401368 and perplexity is 104.223857453719
At time: 10.86374044418335 and batch: 150, loss is 4.626307764053345 and perplexity is 102.13625590567348
At time: 11.327674150466919 and batch: 200, loss is 4.520643692016602 and perplexity is 91.89473084905188
At time: 11.791602611541748 and batch: 250, loss is 4.634232406616211 and perplexity is 102.94886479127025
At time: 12.271512031555176 and batch: 300, loss is 4.593735418319702 and perplexity is 98.86303608080573
At time: 12.738924503326416 and batch: 350, loss is 4.581426343917847 and perplexity is 97.65358251173193
At time: 13.20229959487915 and batch: 400, loss is 4.4664112567901615 and perplexity is 87.04378403157962
At time: 13.666261911392212 and batch: 450, loss is 4.50086802482605 and perplexity is 90.09530232754828
At time: 14.130364656448364 and batch: 500, loss is 4.388281211853028 and perplexity is 80.50193423123226
At time: 14.593814611434937 and batch: 550, loss is 4.467090635299683 and perplexity is 87.10293980014173
At time: 15.070185661315918 and batch: 600, loss is 4.448555994033813 and perplexity is 85.50338741388994
At time: 15.532013416290283 and batch: 650, loss is 4.30522967338562 and perplexity is 74.08622913405193
At time: 15.995001792907715 and batch: 700, loss is 4.338492136001587 and perplexity is 76.59196196070249
At time: 16.45703649520874 and batch: 750, loss is 4.410581645965576 and perplexity is 82.31732912481208
At time: 16.91949224472046 and batch: 800, loss is 4.365324773788452 and perplexity is 78.67494735369581
At time: 17.382179975509644 and batch: 850, loss is 4.429953880310059 and perplexity is 83.92754610860538
At time: 17.845244884490967 and batch: 900, loss is 4.365117530822754 and perplexity is 78.658644213694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5587400671553935 and perplexity of 95.4631268986532
finished 2 epochs...
Completing Train Step...
At time: 19.15813422203064 and batch: 50, loss is 4.433296184539795 and perplexity is 84.20852680086418
At time: 19.617750644683838 and batch: 100, loss is 4.311966319084167 and perplexity is 74.58700669750647
At time: 20.07795763015747 and batch: 150, loss is 4.302641181945801 and perplexity is 73.89470554964794
At time: 20.538087844848633 and batch: 200, loss is 4.209194927215576 and perplexity is 67.30233471551345
At time: 20.99872922897339 and batch: 250, loss is 4.345841770172119 and perplexity is 77.1569585774486
At time: 21.459614038467407 and batch: 300, loss is 4.321092281341553 and perplexity is 75.27080029710758
At time: 21.920574188232422 and batch: 350, loss is 4.310807008743286 and perplexity is 74.500587312469
At time: 22.381227254867554 and batch: 400, loss is 4.221632962226868 and perplexity is 68.14467115669642
At time: 22.84114980697632 and batch: 450, loss is 4.259896211624145 and perplexity is 70.80263458246988
At time: 23.30331540107727 and batch: 500, loss is 4.1385881710052494 and perplexity is 62.71421716669272
At time: 23.763578414916992 and batch: 550, loss is 4.221689109802246 and perplexity is 68.14849742217359
At time: 24.223539352416992 and batch: 600, loss is 4.227718152999878 and perplexity is 68.5606087270842
At time: 24.685248613357544 and batch: 650, loss is 4.078015222549438 and perplexity is 59.028195675088746
At time: 25.15043568611145 and batch: 700, loss is 4.097887654304504 and perplexity is 60.212962575165704
At time: 25.618661403656006 and batch: 750, loss is 4.192789716720581 and perplexity is 66.20723299518717
At time: 26.083174228668213 and batch: 800, loss is 4.156479382514954 and perplexity is 63.84634788677587
At time: 26.56311845779419 and batch: 850, loss is 4.226979913711548 and perplexity is 68.51001327016498
At time: 27.02588200569153 and batch: 900, loss is 4.167867426872253 and perplexity is 64.57758872740726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477578777156464 and perplexity of 88.02129529191569
finished 3 epochs...
Completing Train Step...
At time: 28.24786353111267 and batch: 50, loss is 4.255334506034851 and perplexity is 70.4803893612757
At time: 28.72324275970459 and batch: 100, loss is 4.134138655662537 and perplexity is 62.435789189345776
At time: 29.1855046749115 and batch: 150, loss is 4.133030843734741 and perplexity is 62.366660375292724
At time: 29.64748239517212 and batch: 200, loss is 4.036546006202697 and perplexity is 56.63040355117746
At time: 30.109764099121094 and batch: 250, loss is 4.1798025274276736 and perplexity is 65.35294652523389
At time: 30.572448015213013 and batch: 300, loss is 4.162557053565979 and perplexity is 64.23556656049189
At time: 31.033376216888428 and batch: 350, loss is 4.1513102340698245 and perplexity is 63.517168159283834
At time: 31.49623203277588 and batch: 400, loss is 4.073863086700439 and perplexity is 58.78361071410848
At time: 31.95871353149414 and batch: 450, loss is 4.11512942314148 and perplexity is 61.26014222976457
At time: 32.42103362083435 and batch: 500, loss is 3.9917542839050295 and perplexity is 54.149800211670886
At time: 32.883309841156006 and batch: 550, loss is 4.07484682559967 and perplexity is 58.841466891625586
At time: 33.3454327583313 and batch: 600, loss is 4.0859645652770995 and perplexity is 59.499301041273604
At time: 33.80784368515015 and batch: 650, loss is 3.9380190563201904 and perplexity is 51.31684476751202
At time: 34.269747257232666 and batch: 700, loss is 3.95405020236969 and perplexity is 52.14614213383213
At time: 34.73248362541199 and batch: 750, loss is 4.0570441198349 and perplexity is 57.80319898077125
At time: 35.195542097091675 and batch: 800, loss is 4.022635869979858 and perplexity is 55.84812036998077
At time: 35.65734624862671 and batch: 850, loss is 4.09414689540863 and perplexity is 59.988141163279614
At time: 36.120182514190674 and batch: 900, loss is 4.0407716751098635 and perplexity is 56.87021120369518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.446078783845248 and perplexity of 85.29183968370916
finished 4 epochs...
Completing Train Step...
At time: 37.34156632423401 and batch: 50, loss is 4.131607580184936 and perplexity is 62.27795931832143
At time: 37.817365884780884 and batch: 100, loss is 4.014079079627991 and perplexity is 55.37227845555232
At time: 38.28015756607056 and batch: 150, loss is 4.015862526893616 and perplexity is 55.47112010739348
At time: 38.75536775588989 and batch: 200, loss is 3.9194210624694823 and perplexity is 50.371274513723826
At time: 39.21755051612854 and batch: 250, loss is 4.06734055519104 and perplexity is 58.40144047460541
At time: 39.67975091934204 and batch: 300, loss is 4.055383644104004 and perplexity is 57.70729781450984
At time: 40.15695643424988 and batch: 350, loss is 4.040382876396179 and perplexity is 56.848104436552006
At time: 40.62373328208923 and batch: 400, loss is 3.9694927597045897 and perplexity is 52.95766176571548
At time: 41.085423946380615 and batch: 450, loss is 4.013220663070679 and perplexity is 55.3247663704078
At time: 41.54811930656433 and batch: 500, loss is 3.887437243461609 and perplexity is 48.78570030890564
At time: 42.01048517227173 and batch: 550, loss is 3.971833052635193 and perplexity is 53.08174334415487
At time: 42.47320508956909 and batch: 600, loss is 3.985887231826782 and perplexity is 53.83303067414986
At time: 42.936200857162476 and batch: 650, loss is 3.8413253736495974 and perplexity is 46.5871789593863
At time: 43.39853048324585 and batch: 700, loss is 3.8516586780548097 and perplexity is 47.07107427400642
At time: 43.86175847053528 and batch: 750, loss is 3.9568493700027467 and perplexity is 52.29231240919265
At time: 44.32392191886902 and batch: 800, loss is 3.9266769599914553 and perplexity is 50.73809250717855
At time: 44.786951780319214 and batch: 850, loss is 3.9976668167114258 and perplexity is 54.470911035572605
At time: 45.249446868896484 and batch: 900, loss is 3.944600110054016 and perplexity is 51.65567739533252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43719482421875 and perplexity of 84.53746629640108
finished 5 epochs...
Completing Train Step...
At time: 46.56255626678467 and batch: 50, loss is 4.038674826622009 and perplexity is 56.75108792271107
At time: 47.02417063713074 and batch: 100, loss is 3.924104108810425 and perplexity is 50.60771873402445
At time: 47.49375796318054 and batch: 150, loss is 3.9285366249084475 and perplexity is 50.83253614731306
At time: 47.96757102012634 and batch: 200, loss is 3.834172601699829 and perplexity is 46.25514040618079
At time: 48.43021035194397 and batch: 250, loss is 3.9775225353240966 and perplexity is 53.38461176934683
At time: 48.892231464385986 and batch: 300, loss is 3.9728203296661375 and perplexity is 53.134175608447286
At time: 49.35476326942444 and batch: 350, loss is 3.955268688201904 and perplexity is 52.20972019584449
At time: 49.81764817237854 and batch: 400, loss is 3.8905938720703124 and perplexity is 48.93994195992462
At time: 50.280545711517334 and batch: 450, loss is 3.9347402572631838 and perplexity is 51.14886268551701
At time: 50.77631855010986 and batch: 500, loss is 3.8111622667312623 and perplexity is 45.20294622718342
At time: 51.24258279800415 and batch: 550, loss is 3.891565284729004 and perplexity is 48.987505937444766
At time: 51.70601439476013 and batch: 600, loss is 3.9089298677444457 and perplexity is 49.845582056910516
At time: 52.16993808746338 and batch: 650, loss is 3.764843215942383 and perplexity is 43.15693900939452
At time: 52.63325595855713 and batch: 700, loss is 3.7741988372802733 and perplexity is 43.56259360543762
At time: 53.09624409675598 and batch: 750, loss is 3.880673952102661 and perplexity is 48.456861672799356
At time: 53.55941724777222 and batch: 800, loss is 3.849862198829651 and perplexity is 46.986587978626645
At time: 54.022353649139404 and batch: 850, loss is 3.922511487007141 and perplexity is 50.52718392552283
At time: 54.484150648117065 and batch: 900, loss is 3.870694317817688 and perplexity is 47.975684892339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426985649213399 and perplexity of 83.67879910830014
finished 6 epochs...
Completing Train Step...
At time: 55.72128415107727 and batch: 50, loss is 3.965647368431091 and perplexity is 52.754409877273424
At time: 56.184940814971924 and batch: 100, loss is 3.850900273323059 and perplexity is 47.03538888224387
At time: 56.65666890144348 and batch: 150, loss is 3.8596402263641356 and perplexity is 47.448277658333595
At time: 57.1313111782074 and batch: 200, loss is 3.7688666439056395 and perplexity is 43.33092762523442
At time: 57.596543312072754 and batch: 250, loss is 3.9074860620498657 and perplexity is 49.77366665011677
At time: 58.05956792831421 and batch: 300, loss is 3.904801859855652 and perplexity is 49.64024321273116
At time: 58.52377939224243 and batch: 350, loss is 3.886050601005554 and perplexity is 48.718098865956264
At time: 58.986523151397705 and batch: 400, loss is 3.8247016954421995 and perplexity is 45.8191302721145
At time: 59.45016694068909 and batch: 450, loss is 3.86866427898407 and perplexity is 47.87839117736071
At time: 59.91338920593262 and batch: 500, loss is 3.7481395864486693 and perplexity is 42.4420487429593
At time: 60.377580881118774 and batch: 550, loss is 3.82559353351593 and perplexity is 45.860011744113336
At time: 60.841389179229736 and batch: 600, loss is 3.8467796802520753 and perplexity is 46.841973950508994
At time: 61.304558753967285 and batch: 650, loss is 3.703536772727966 and perplexity is 40.59061055438585
At time: 61.76816129684448 and batch: 700, loss is 3.711103777885437 and perplexity is 40.89892495094923
At time: 62.24574375152588 and batch: 750, loss is 3.8162344217300417 and perplexity is 45.43280502377803
At time: 62.70956540107727 and batch: 800, loss is 3.7890170001983643 and perplexity is 44.2129176185142
At time: 63.17323112487793 and batch: 850, loss is 3.8595258474349974 and perplexity is 47.44285088550573
At time: 63.648553133010864 and batch: 900, loss is 3.8098996019363405 and perplexity is 45.145906077228595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4306268561376285 and perplexity of 83.98404632791224
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 64.91054010391235 and batch: 50, loss is 3.9210966300964354 and perplexity is 50.455745739490105
At time: 65.38757228851318 and batch: 100, loss is 3.790411376953125 and perplexity is 44.27461008437612
At time: 65.85125279426575 and batch: 150, loss is 3.79914927482605 and perplexity is 44.66317224162925
At time: 66.31482982635498 and batch: 200, loss is 3.690539755821228 and perplexity is 40.06646723085146
At time: 66.79343008995056 and batch: 250, loss is 3.8256615352630616 and perplexity is 45.86313041107161
At time: 67.26074361801147 and batch: 300, loss is 3.8099506664276124 and perplexity is 45.14821148881727
At time: 67.72487998008728 and batch: 350, loss is 3.778097767829895 and perplexity is 43.73277267506973
At time: 68.1962537765503 and batch: 400, loss is 3.708350238800049 and perplexity is 40.78646306769896
At time: 68.67605137825012 and batch: 450, loss is 3.7400083637237547 and perplexity is 42.09834226242969
At time: 69.14656639099121 and batch: 500, loss is 3.6111928701400755 and perplexity is 37.01017482610474
At time: 69.61682868003845 and batch: 550, loss is 3.6777790117263796 and perplexity is 39.558437607540675
At time: 70.08097958564758 and batch: 600, loss is 3.6880311393737792 and perplexity is 39.96608179908269
At time: 70.55818247795105 and batch: 650, loss is 3.5316014385223387 and perplexity is 34.178658831650004
At time: 71.025550365448 and batch: 700, loss is 3.527084059715271 and perplexity is 34.024609095068165
At time: 71.4897358417511 and batch: 750, loss is 3.6231863355636595 and perplexity is 37.45672758294112
At time: 71.96500134468079 and batch: 800, loss is 3.5788853216171264 and perplexity is 35.83357566486382
At time: 72.43804478645325 and batch: 850, loss is 3.6322417497634887 and perplexity is 37.797454147550575
At time: 72.90397000312805 and batch: 900, loss is 3.5819878673553465 and perplexity is 35.94492361406301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347621969980736 and perplexity of 77.29443571241963
finished 8 epochs...
Completing Train Step...
At time: 74.12609243392944 and batch: 50, loss is 3.8304375505447386 and perplexity is 46.08269733290158
At time: 74.60254001617432 and batch: 100, loss is 3.708468508720398 and perplexity is 40.791287164704464
At time: 75.0649802684784 and batch: 150, loss is 3.7208381843566896 and perplexity is 41.298995777503904
At time: 75.5274670124054 and batch: 200, loss is 3.6178683280944823 and perplexity is 37.25806114888446
At time: 75.98920845985413 and batch: 250, loss is 3.756412401199341 and perplexity is 42.794620318699835
At time: 76.45093894004822 and batch: 300, loss is 3.7468025064468384 and perplexity is 42.385338250030195
At time: 76.91362166404724 and batch: 350, loss is 3.7178424739837648 and perplexity is 41.175461076940046
At time: 77.37594294548035 and batch: 400, loss is 3.6539489603042603 and perplexity is 38.62690136575736
At time: 77.83799815177917 and batch: 450, loss is 3.691075177192688 and perplexity is 40.08792541775907
At time: 78.30098581314087 and batch: 500, loss is 3.5632622146606447 and perplexity is 35.278094350126786
At time: 78.76327562332153 and batch: 550, loss is 3.6350202560424805 and perplexity is 37.90262064646027
At time: 79.22603869438171 and batch: 600, loss is 3.651033754348755 and perplexity is 38.514459967401486
At time: 79.68753004074097 and batch: 650, loss is 3.4983347988128664 and perplexity is 33.06035395613961
At time: 80.15087866783142 and batch: 700, loss is 3.498333525657654 and perplexity is 33.06031186520444
At time: 80.61419558525085 and batch: 750, loss is 3.60081298828125 and perplexity is 36.62800047747398
At time: 81.0761570930481 and batch: 800, loss is 3.56043993473053 and perplexity is 35.178670060053996
At time: 81.53850483894348 and batch: 850, loss is 3.6207892751693724 and perplexity is 37.367049070098425
At time: 82.00068807601929 and batch: 900, loss is 3.578069124221802 and perplexity is 35.8043403262676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345928453419306 and perplexity of 77.16364708304769
finished 9 epochs...
Completing Train Step...
At time: 83.22935748100281 and batch: 50, loss is 3.791489601135254 and perplexity is 44.32237378498456
At time: 83.69376802444458 and batch: 100, loss is 3.6717302989959717 and perplexity is 39.319882186329174
At time: 84.16593837738037 and batch: 150, loss is 3.6832305335998536 and perplexity is 39.77468018546628
At time: 84.62860536575317 and batch: 200, loss is 3.582458944320679 and perplexity is 35.961860428556605
At time: 85.09180188179016 and batch: 250, loss is 3.7220861387252806 and perplexity is 41.350567212408535
At time: 85.55378079414368 and batch: 300, loss is 3.714493579864502 and perplexity is 41.037799453239856
At time: 86.02862167358398 and batch: 350, loss is 3.6861261129379272 and perplexity is 39.890017831656195
At time: 86.5217936038971 and batch: 400, loss is 3.6238289213180543 and perplexity is 37.48080447739838
At time: 86.98924255371094 and batch: 450, loss is 3.662811336517334 and perplexity is 38.97074890018278
At time: 87.45173192024231 and batch: 500, loss is 3.535432467460632 and perplexity is 34.30984939968282
At time: 87.91419625282288 and batch: 550, loss is 3.6094660329818726 and perplexity is 36.94631943078602
At time: 88.37651920318604 and batch: 600, loss is 3.6273758935928346 and perplexity is 37.613983903955315
At time: 88.83873796463013 and batch: 650, loss is 3.4765013742446897 and perplexity is 32.34635608442458
At time: 89.30146861076355 and batch: 700, loss is 3.4783583641052247 and perplexity is 32.40647874600778
At time: 89.76431894302368 and batch: 750, loss is 3.583783597946167 and perplexity is 36.00952900255692
At time: 90.22725987434387 and batch: 800, loss is 3.5444034814834593 and perplexity is 34.61902829031658
At time: 90.68985104560852 and batch: 850, loss is 3.6077791929244993 and perplexity is 36.88404943372577
At time: 91.15271329879761 and batch: 900, loss is 3.5682993078231813 and perplexity is 35.45624169368092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347917948683647 and perplexity of 77.31731660520548
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 92.39299011230469 and batch: 50, loss is 3.7816620111465453 and perplexity is 43.88892503498699
At time: 92.87547969818115 and batch: 100, loss is 3.666064963340759 and perplexity is 39.097751671907226
At time: 93.3478102684021 and batch: 150, loss is 3.6791525363922117 and perplexity is 39.61280942930756
At time: 93.81018495559692 and batch: 200, loss is 3.5681130838394166 and perplexity is 35.44963950586534
At time: 94.27309536933899 and batch: 250, loss is 3.707979846000671 and perplexity is 40.771358852885484
At time: 94.73471331596375 and batch: 300, loss is 3.695381941795349 and perplexity is 40.26094699009722
At time: 95.19663906097412 and batch: 350, loss is 3.6668879652023314 and perplexity is 39.12994243903014
At time: 95.65922665596008 and batch: 400, loss is 3.5976692771911623 and perplexity is 36.5130334324366
At time: 96.12183499336243 and batch: 450, loss is 3.6313819599151613 and perplexity is 37.76497024684703
At time: 96.59773135185242 and batch: 500, loss is 3.5052610540390017 and perplexity is 33.290133241895994
At time: 97.06650876998901 and batch: 550, loss is 3.5714812660217286 and perplexity is 35.56924165786668
At time: 97.52911972999573 and batch: 600, loss is 3.5896150493621826 and perplexity is 36.220130280202774
At time: 98.01098728179932 and batch: 650, loss is 3.4312233686447144 and perplexity is 30.914439381163998
At time: 98.47291874885559 and batch: 700, loss is 3.429932532310486 and perplexity is 30.874559644202716
At time: 98.93548798561096 and batch: 750, loss is 3.5308752965927126 and perplexity is 34.15384928309083
At time: 99.39805173873901 and batch: 800, loss is 3.487274398803711 and perplexity is 32.696707959169096
At time: 99.86011695861816 and batch: 850, loss is 3.5460028982162477 and perplexity is 34.674442847108516
At time: 100.33654522895813 and batch: 900, loss is 3.5071526861190794 and perplexity is 33.35316552403436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326367103890197 and perplexity of 75.6688894134656
finished 11 epochs...
Completing Train Step...
At time: 101.56361150741577 and batch: 50, loss is 3.7568075037002564 and perplexity is 42.81153192090164
At time: 102.0386643409729 and batch: 100, loss is 3.6382699728012087 and perplexity is 38.025993783274814
At time: 102.50078082084656 and batch: 150, loss is 3.6540936040878296 and perplexity is 38.63248891101056
At time: 102.96319103240967 and batch: 200, loss is 3.546433324813843 and perplexity is 34.689370862043575
At time: 103.44077181816101 and batch: 250, loss is 3.6872109079360964 and perplexity is 39.93331380285754
At time: 103.90639662742615 and batch: 300, loss is 3.6764004707336424 and perplexity is 39.50394225036207
At time: 104.36857962608337 and batch: 350, loss is 3.648798956871033 and perplexity is 38.42848405456967
At time: 104.83038854598999 and batch: 400, loss is 3.5814918851852418 and perplexity is 35.92709999330909
At time: 105.29293036460876 and batch: 450, loss is 3.617659692764282 and perplexity is 37.25028861183515
At time: 105.75653338432312 and batch: 500, loss is 3.4921795082092286 and perplexity is 32.857482875832126
At time: 106.22274661064148 and batch: 550, loss is 3.5597196292877196 and perplexity is 35.15333979639749
At time: 106.6849410533905 and batch: 600, loss is 3.5800321769714354 and perplexity is 35.87469516753305
At time: 107.14730405807495 and batch: 650, loss is 3.4241880226135253 and perplexity is 30.697708883721813
At time: 107.6099123954773 and batch: 700, loss is 3.4246373319625856 and perplexity is 30.711504750391843
At time: 108.07127070426941 and batch: 750, loss is 3.5274189424514772 and perplexity is 34.03600525734277
At time: 108.54360866546631 and batch: 800, loss is 3.4856935214996336 and perplexity is 32.64505931148243
At time: 109.01452088356018 and batch: 850, loss is 3.5468711996078492 and perplexity is 34.70456378922118
At time: 109.47624707221985 and batch: 900, loss is 3.510011353492737 and perplexity is 33.44864754076604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325275577910959 and perplexity of 75.58633991550747
finished 12 epochs...
Completing Train Step...
At time: 110.72644710540771 and batch: 50, loss is 3.7446599531173708 and perplexity is 42.29462261868373
At time: 111.20578217506409 and batch: 100, loss is 3.626160430908203 and perplexity is 37.568293283347
At time: 111.66857194900513 and batch: 150, loss is 3.6416745376586914 and perplexity is 38.15567637651677
At time: 112.13185143470764 and batch: 200, loss is 3.534829306602478 and perplexity is 34.2891612812346
At time: 112.59473633766174 and batch: 250, loss is 3.675486283302307 and perplexity is 39.46784474532531
At time: 113.05784225463867 and batch: 300, loss is 3.6655369853973387 and perplexity is 39.07711436988667
At time: 113.52098393440247 and batch: 350, loss is 3.6384224033355714 and perplexity is 38.03179054761759
At time: 113.98397207260132 and batch: 400, loss is 3.572018151283264 and perplexity is 35.5883433867363
At time: 114.4467523097992 and batch: 450, loss is 3.6090899753570556 and perplexity is 36.932428097789696
At time: 114.90999484062195 and batch: 500, loss is 3.483907518386841 and perplexity is 32.58680716867343
At time: 115.37259769439697 and batch: 550, loss is 3.5521647930145264 and perplexity is 34.88876274633277
At time: 115.8357412815094 and batch: 600, loss is 3.5734461545944214 and perplexity is 35.639199961967876
At time: 116.29860210418701 and batch: 650, loss is 3.4188578796386717 and perplexity is 30.534521000099776
At time: 116.76199531555176 and batch: 700, loss is 3.4202519989013673 and perplexity is 30.577119450764098
At time: 117.22785711288452 and batch: 750, loss is 3.5238339471817017 and perplexity is 33.91420479698048
At time: 117.69376301765442 and batch: 800, loss is 3.4828733158111573 and perplexity is 32.55312322977676
At time: 118.16056752204895 and batch: 850, loss is 3.5452934741973876 and perplexity is 34.64985268796826
At time: 118.62559700012207 and batch: 900, loss is 3.509529404640198 and perplexity is 33.43253088747802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325293972067637 and perplexity of 75.58773027527384
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 119.8738420009613 and batch: 50, loss is 3.74294725894928 and perplexity is 42.222246861643384
At time: 120.33672189712524 and batch: 100, loss is 3.628191409111023 and perplexity is 37.64467120281334
At time: 120.80131983757019 and batch: 150, loss is 3.646124005317688 and perplexity is 38.32582708387916
At time: 121.26507878303528 and batch: 200, loss is 3.5358438301086426 and perplexity is 34.323966093525925
At time: 121.7431173324585 and batch: 250, loss is 3.6758996772766115 and perplexity is 39.4841638874069
At time: 122.2067461013794 and batch: 300, loss is 3.665140247344971 and perplexity is 39.06161406662278
At time: 122.66919660568237 and batch: 350, loss is 3.6350301837921144 and perplexity is 37.90299693605637
At time: 123.13206028938293 and batch: 400, loss is 3.5692807245254516 and perplexity is 35.49105612241538
At time: 123.59573459625244 and batch: 450, loss is 3.600785284042358 and perplexity is 36.626985740654945
At time: 124.05928158760071 and batch: 500, loss is 3.4776265811920166 and perplexity is 32.38277291340371
At time: 124.52341771125793 and batch: 550, loss is 3.5410641479492186 and perplexity is 34.50361661451124
At time: 124.98721623420715 and batch: 600, loss is 3.5646575784683225 and perplexity is 35.32735448609962
At time: 125.45083022117615 and batch: 650, loss is 3.408128890991211 and perplexity is 30.208667635235162
At time: 125.91406321525574 and batch: 700, loss is 3.4085632944107056 and perplexity is 30.221793234450466
At time: 126.37831521034241 and batch: 750, loss is 3.5072937059402465 and perplexity is 33.35786931312787
At time: 126.84224128723145 and batch: 800, loss is 3.4660915899276734 and perplexity is 32.011384012547644
At time: 127.30515027046204 and batch: 850, loss is 3.5248658800125123 and perplexity is 33.94922004192446
At time: 127.76817631721497 and batch: 900, loss is 3.4918109607696532 and perplexity is 32.84537556584344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320202971157962 and perplexity of 75.2038909638618
finished 14 epochs...
Completing Train Step...
At time: 129.00529408454895 and batch: 50, loss is 3.7358410692214967 and perplexity is 41.9232711120486
At time: 129.46764826774597 and batch: 100, loss is 3.6173004722595214 and perplexity is 37.236909947446684
At time: 129.9302523136139 and batch: 150, loss is 3.6361928176879883 and perplexity is 37.947089872053645
At time: 130.39326333999634 and batch: 200, loss is 3.526654682159424 and perplexity is 34.01000282760239
At time: 130.85626649856567 and batch: 250, loss is 3.668554635047913 and perplexity is 39.19521351170781
At time: 131.31945133209229 and batch: 300, loss is 3.658395767211914 and perplexity is 38.799050210233425
At time: 131.782968044281 and batch: 350, loss is 3.62743625164032 and perplexity is 37.616254279098925
At time: 132.2457628250122 and batch: 400, loss is 3.5631465244293214 and perplexity is 35.27401325530673
At time: 132.7095229625702 and batch: 450, loss is 3.5962369346618654 and perplexity is 36.4607716990644
At time: 133.17200589179993 and batch: 500, loss is 3.473559470176697 and perplexity is 32.25133604632499
At time: 133.64906311035156 and batch: 550, loss is 3.537391939163208 and perplexity is 34.377144488580555
At time: 134.11344647407532 and batch: 600, loss is 3.56208637714386 and perplexity is 35.23663742135183
At time: 134.57688069343567 and batch: 650, loss is 3.4060625791549684 and perplexity is 30.146311553484768
At time: 135.040118932724 and batch: 700, loss is 3.4076296710968017 and perplexity is 30.193590631051393
At time: 135.50416922569275 and batch: 750, loss is 3.507120475769043 and perplexity is 33.35209122419988
At time: 135.9692370891571 and batch: 800, loss is 3.4667248821258543 and perplexity is 32.03166299287802
At time: 136.4317591190338 and batch: 850, loss is 3.5266982746124267 and perplexity is 34.01148543936739
At time: 136.89474606513977 and batch: 900, loss is 3.494624934196472 and perplexity is 32.9379317441176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3196168664383565 and perplexity of 75.159826522885
finished 15 epochs...
Completing Train Step...
At time: 138.12796235084534 and batch: 50, loss is 3.732190713882446 and perplexity is 41.770515251723246
At time: 138.60172033309937 and batch: 100, loss is 3.6132012701034544 and perplexity is 37.084580753290105
At time: 139.06358671188354 and batch: 150, loss is 3.6319847202301023 and perplexity is 37.7877403339698
At time: 139.52526354789734 and batch: 200, loss is 3.52276743888855 and perplexity is 33.87805429713937
At time: 139.98720741271973 and batch: 250, loss is 3.664761395454407 and perplexity is 39.04681830316361
At time: 140.44904017448425 and batch: 300, loss is 3.6548204040527343 and perplexity is 38.66057720864569
At time: 140.9108121395111 and batch: 350, loss is 3.623931818008423 and perplexity is 37.48466132655656
At time: 141.37189078330994 and batch: 400, loss is 3.560026936531067 and perplexity is 35.16414433241584
At time: 141.83260869979858 and batch: 450, loss is 3.593612060546875 and perplexity is 36.36519226009482
At time: 142.2931764125824 and batch: 500, loss is 3.4711413288116457 and perplexity is 32.17344197394898
At time: 142.75401854515076 and batch: 550, loss is 3.5352096366882324 and perplexity is 34.30220496117987
At time: 143.2165801525116 and batch: 600, loss is 3.560319395065308 and perplexity is 35.174429890501436
At time: 143.6789424419403 and batch: 650, loss is 3.404743027687073 and perplexity is 30.106558177903555
At time: 144.14072442054749 and batch: 700, loss is 3.406802463531494 and perplexity is 30.168624591929433
At time: 144.60288858413696 and batch: 750, loss is 3.506607856750488 and perplexity is 33.33499868928332
At time: 145.06486773490906 and batch: 800, loss is 3.4666603088378904 and perplexity is 32.02959466985955
At time: 145.5407474040985 and batch: 850, loss is 3.5270708084106444 and perplexity is 34.02415822759554
At time: 146.005380153656 and batch: 900, loss is 3.495387306213379 and perplexity is 32.96305227595267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319483090753424 and perplexity of 75.14977263811016
finished 16 epochs...
Completing Train Step...
At time: 147.22946667671204 and batch: 50, loss is 3.729010581970215 and perplexity is 41.637890497060354
At time: 147.70418548583984 and batch: 100, loss is 3.609905104637146 and perplexity is 36.96254507426415
At time: 148.16622805595398 and batch: 150, loss is 3.6286628007888795 and perplexity is 37.66242077070461
At time: 148.62917494773865 and batch: 200, loss is 3.519678258895874 and perplexity is 33.77356037298286
At time: 149.09148716926575 and batch: 250, loss is 3.6616911697387695 and perplexity is 38.92711960253598
At time: 149.55397152900696 and batch: 300, loss is 3.6519606637954714 and perplexity is 38.550175934356886
At time: 150.01620602607727 and batch: 350, loss is 3.621165838241577 and perplexity is 37.381122770547115
At time: 150.47884106636047 and batch: 400, loss is 3.55754873752594 and perplexity is 35.07710847554606
At time: 150.9417281150818 and batch: 450, loss is 3.591424069404602 and perplexity is 36.28571252373007
At time: 151.40415620803833 and batch: 500, loss is 3.469070415496826 and perplexity is 32.10688250781158
At time: 151.866441488266 and batch: 550, loss is 3.5333261680603028 and perplexity is 34.237658638834525
At time: 152.3403389453888 and batch: 600, loss is 3.5587121725082396 and perplexity is 35.11794215968675
At time: 152.8113031387329 and batch: 650, loss is 3.403504800796509 and perplexity is 30.06930249823759
At time: 153.27400016784668 and batch: 700, loss is 3.4058711338043213 and perplexity is 30.14054073471453
At time: 153.73577046394348 and batch: 750, loss is 3.5058654022216795 and perplexity is 33.31025815403635
At time: 154.19782304763794 and batch: 800, loss is 3.4662340307235717 and perplexity is 32.01594406432519
At time: 154.66077756881714 and batch: 850, loss is 3.5269343519210814 and perplexity is 34.01951572716033
At time: 155.124187707901 and batch: 900, loss is 3.4955387687683106 and perplexity is 32.96804532218895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319494378076841 and perplexity of 75.15062088268579
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 156.36749601364136 and batch: 50, loss is 3.728689022064209 and perplexity is 41.62450357337015
At time: 156.82923460006714 and batch: 100, loss is 3.6103577613830566 and perplexity is 36.97928020698765
At time: 157.30448198318481 and batch: 150, loss is 3.6304443502426147 and perplexity is 37.72957804009065
At time: 157.76537775993347 and batch: 200, loss is 3.520639781951904 and perplexity is 33.80605004724802
At time: 158.22615575790405 and batch: 250, loss is 3.663160300254822 and perplexity is 38.98435065149264
At time: 158.68765234947205 and batch: 300, loss is 3.653428177833557 and perplexity is 38.6067903898049
At time: 159.14886617660522 and batch: 350, loss is 3.6209750270843504 and perplexity is 37.37399071571229
At time: 159.60972714424133 and batch: 400, loss is 3.5588754510879514 and perplexity is 35.12367663555078
At time: 160.071147441864 and batch: 450, loss is 3.5887690591812134 and perplexity is 36.18950136334051
At time: 160.5331027507782 and batch: 500, loss is 3.4664418125152587 and perplexity is 32.02259708570984
At time: 160.99547338485718 and batch: 550, loss is 3.530062184333801 and perplexity is 34.126089656925345
At time: 161.45809745788574 and batch: 600, loss is 3.554666614532471 and perplexity is 34.97615748113561
At time: 161.9204773902893 and batch: 650, loss is 3.3999948263168336 and perplexity is 29.963945023038026
At time: 162.38356757164001 and batch: 700, loss is 3.4014606571197508 and perplexity is 30.00789930352861
At time: 162.84569334983826 and batch: 750, loss is 3.499366211891174 and perplexity is 33.09447042864696
At time: 163.3082253932953 and batch: 800, loss is 3.459254517555237 and perplexity is 31.79326635608213
At time: 163.77093982696533 and batch: 850, loss is 3.5196205282211306 and perplexity is 33.77161065883374
At time: 164.2340211868286 and batch: 900, loss is 3.4873902130126955 and perplexity is 32.70049492182559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319170808138913 and perplexity of 75.1263083345685
finished 18 epochs...
Completing Train Step...
At time: 165.48672652244568 and batch: 50, loss is 3.7264594411849976 and perplexity is 41.53180175759622
At time: 165.9485833644867 and batch: 100, loss is 3.607356548309326 and perplexity is 36.868463882653444
At time: 166.41129636764526 and batch: 150, loss is 3.627527770996094 and perplexity is 37.61969705199494
At time: 166.87371706962585 and batch: 200, loss is 3.517746396064758 and perplexity is 33.70837746951046
At time: 167.33626222610474 and batch: 250, loss is 3.6603889417648316 and perplexity is 38.87646061038205
At time: 167.79865980148315 and batch: 300, loss is 3.6511117696762083 and perplexity is 38.517464802817614
At time: 168.26164960861206 and batch: 350, loss is 3.6186329412460325 and perplexity is 37.28656004636762
At time: 168.72449207305908 and batch: 400, loss is 3.556579360961914 and perplexity is 35.0431220241522
At time: 169.19965386390686 and batch: 450, loss is 3.5874571800231934 and perplexity is 36.142056238702416
At time: 169.66169786453247 and batch: 500, loss is 3.465298600196838 and perplexity is 31.986009375998677
At time: 170.12342047691345 and batch: 550, loss is 3.5289694118499755 and perplexity is 34.08881797361167
At time: 170.58543252944946 and batch: 600, loss is 3.5540162420272825 and perplexity is 34.953417345553774
At time: 171.0478813648224 and batch: 650, loss is 3.3996881914138792 and perplexity is 29.954758440199207
At time: 171.5105209350586 and batch: 700, loss is 3.4014646911621096 and perplexity is 30.008020356909668
At time: 171.97280263900757 and batch: 750, loss is 3.4997122478485108 and perplexity is 33.105924287013416
At time: 172.43238925933838 and batch: 800, loss is 3.4598580741882325 and perplexity is 31.812461184876362
At time: 172.89815855026245 and batch: 850, loss is 3.5207377910614013 and perplexity is 33.80936351048091
At time: 173.35987329483032 and batch: 900, loss is 3.4890164709091187 and perplexity is 32.75371762509999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318864378210616 and perplexity of 75.10329091208602
finished 19 epochs...
Completing Train Step...
At time: 174.59343791007996 and batch: 50, loss is 3.7252316808700563 and perplexity is 41.48084194920681
At time: 175.06930541992188 and batch: 100, loss is 3.6058276653289796 and perplexity is 36.81213938346729
At time: 175.53236937522888 and batch: 150, loss is 3.625943512916565 and perplexity is 37.56014492841306
At time: 175.9952871799469 and batch: 200, loss is 3.516227822303772 and perplexity is 33.657227659169344
At time: 176.45835781097412 and batch: 250, loss is 3.658970232009888 and perplexity is 38.82134530203359
At time: 176.92111349105835 and batch: 300, loss is 3.6498168325424194 and perplexity is 38.46761938764989
At time: 177.38426780700684 and batch: 350, loss is 3.6173744869232176 and perplexity is 37.23966612681111
At time: 177.84649229049683 and batch: 400, loss is 3.555391535758972 and perplexity is 35.00152163251023
At time: 178.30920553207397 and batch: 450, loss is 3.586643934249878 and perplexity is 36.112675812600315
At time: 178.77197670936584 and batch: 500, loss is 3.4646175479888917 and perplexity is 31.964232650075118
At time: 179.23488306999207 and batch: 550, loss is 3.528362717628479 and perplexity is 34.068142757130786
At time: 179.69772458076477 and batch: 600, loss is 3.553653817176819 and perplexity is 34.940751653818474
At time: 180.15983629226685 and batch: 650, loss is 3.399481763839722 and perplexity is 29.94857559025724
At time: 180.62183737754822 and batch: 700, loss is 3.4014617300033567 and perplexity is 30.007931498529093
At time: 181.09772849082947 and batch: 750, loss is 3.49985155582428 and perplexity is 33.110536527565294
At time: 181.56051778793335 and batch: 800, loss is 3.4601074838638306 and perplexity is 31.82039651003276
At time: 182.02398538589478 and batch: 850, loss is 3.5212927293777465 and perplexity is 33.828130828614455
At time: 182.48781442642212 and batch: 900, loss is 3.4897913932800293 and perplexity is 32.779109050539326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31871555276113 and perplexity of 75.09211446274884
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe3d26db70>
ELAPSED
760.5517091751099


RESULTS SO FAR:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.13177326071305, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.42094345278127, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.9680256792332449, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5177066140482257, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.09211446274884, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.11885787751686028, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.207900223189446, 'tune_wordvecs': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.4107464825280964, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.25800213299985186, 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6949069499969482 and batch: 50, loss is 6.613248844146728 and perplexity is 744.8991530227357
At time: 1.1733160018920898 and batch: 100, loss is 5.896673030853272 and perplexity is 363.82501748320396
At time: 1.6387572288513184 and batch: 150, loss is 5.629655933380127 and perplexity is 278.56625579251465
At time: 2.10439133644104 and batch: 200, loss is 5.401150207519532 and perplexity is 221.66122604298172
At time: 2.571396589279175 and batch: 250, loss is 5.413840379714966 and perplexity is 224.49206912367114
At time: 3.036928176879883 and batch: 300, loss is 5.303177070617676 and perplexity is 200.974306316621
At time: 3.503509998321533 and batch: 350, loss is 5.267636356353759 and perplexity is 193.95697504604573
At time: 3.9695725440979004 and batch: 400, loss is 5.103043622970581 and perplexity is 164.52188863600745
At time: 4.435791015625 and batch: 450, loss is 5.094693441390991 and perplexity is 163.15382074881117
At time: 4.9018754959106445 and batch: 500, loss is 5.022476348876953 and perplexity is 151.7867156396674
At time: 5.368434906005859 and batch: 550, loss is 5.0751236629486085 and perplexity is 159.99197583558183
At time: 5.834731101989746 and batch: 600, loss is 5.002064065933228 and perplexity is 148.71981001330155
At time: 6.302415370941162 and batch: 650, loss is 4.873479633331299 and perplexity is 130.7751757714771
At time: 6.770535707473755 and batch: 700, loss is 4.94102029800415 and perplexity is 139.91292964239264
At time: 7.238724946975708 and batch: 750, loss is 4.955901079177856 and perplexity is 142.0105114529089
At time: 7.7080559730529785 and batch: 800, loss is 4.90155062675476 and perplexity is 134.49817453976843
At time: 8.17585563659668 and batch: 850, loss is 4.943935289382934 and perplexity is 140.32136963639581
At time: 8.64480996131897 and batch: 900, loss is 4.870326528549194 and perplexity is 130.36347734460523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.824222512441139 and perplexity of 124.48964159971227
finished 1 epochs...
Completing Train Step...
At time: 9.89066219329834 and batch: 50, loss is 4.7897937202453615 and perplexity is 120.27655548572102
At time: 10.352963209152222 and batch: 100, loss is 4.66003191947937 and perplexity is 105.63945405921902
At time: 10.81598973274231 and batch: 150, loss is 4.6415619468688964 and perplexity is 103.70620472600271
At time: 11.277952432632446 and batch: 200, loss is 4.536826286315918 and perplexity is 93.39392369463779
At time: 11.753696918487549 and batch: 250, loss is 4.651934957504272 and perplexity is 104.78754899239026
At time: 12.216360330581665 and batch: 300, loss is 4.603261833190918 and perplexity is 99.80934669502683
At time: 12.679489612579346 and batch: 350, loss is 4.5928576374053955 and perplexity is 98.77629407041374
At time: 13.142301797866821 and batch: 400, loss is 4.486501941680908 and perplexity is 88.81023849162631
At time: 13.605794668197632 and batch: 450, loss is 4.50940936088562 and perplexity is 90.8681323842904
At time: 14.070042848587036 and batch: 500, loss is 4.403744764328003 and perplexity is 81.756454789329
At time: 14.532609462738037 and batch: 550, loss is 4.48522572517395 and perplexity is 88.69696969240574
At time: 14.995227098464966 and batch: 600, loss is 4.462458972930908 and perplexity is 86.70044122994325
At time: 15.457720756530762 and batch: 650, loss is 4.321025323867798 and perplexity is 75.26576052319933
At time: 15.920865774154663 and batch: 700, loss is 4.359118318557739 and perplexity is 78.18816696806921
At time: 16.383572816848755 and batch: 750, loss is 4.416582975387573 and perplexity is 82.81282787248139
At time: 16.846880674362183 and batch: 800, loss is 4.378935108184814 and perplexity is 79.75305978865521
At time: 17.319023847579956 and batch: 850, loss is 4.441259059906006 and perplexity is 84.88174562610149
At time: 17.782704830169678 and batch: 900, loss is 4.386483602523803 and perplexity is 80.35735319228307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.562708188409674 and perplexity of 95.84268873715136
finished 2 epochs...
Completing Train Step...
At time: 19.0224187374115 and batch: 50, loss is 4.436319065093994 and perplexity is 84.4634642476227
At time: 19.484086275100708 and batch: 100, loss is 4.316309571266174 and perplexity is 74.91166139535453
At time: 19.947721242904663 and batch: 150, loss is 4.318465304374695 and perplexity is 75.07332513336826
At time: 20.409342050552368 and batch: 200, loss is 4.219447779655456 and perplexity is 67.99592518668965
At time: 20.871403694152832 and batch: 250, loss is 4.356637287139892 and perplexity is 77.99442011471707
At time: 21.33280897140503 and batch: 300, loss is 4.326919403076172 and perplexity is 75.71069282242401
At time: 21.794456005096436 and batch: 350, loss is 4.3172891902923585 and perplexity is 74.98508224049735
At time: 22.25535535812378 and batch: 400, loss is 4.233337960243225 and perplexity is 68.94699081213686
At time: 22.717214584350586 and batch: 450, loss is 4.264918689727783 and perplexity is 71.15913376962173
At time: 23.18181848526001 and batch: 500, loss is 4.154329476356506 and perplexity is 63.70923167656574
At time: 23.662015438079834 and batch: 550, loss is 4.234728169441223 and perplexity is 69.0429082101145
At time: 24.123881340026855 and batch: 600, loss is 4.235400886535644 and perplexity is 69.08937018084477
At time: 24.5857093334198 and batch: 650, loss is 4.093939666748047 and perplexity is 59.97571118910339
At time: 25.047414779663086 and batch: 700, loss is 4.115596599578858 and perplexity is 61.28876821093605
At time: 25.509308099746704 and batch: 750, loss is 4.197418894767761 and perplexity is 66.51442854760508
At time: 25.97156548500061 and batch: 800, loss is 4.165065279006958 and perplexity is 64.39688607103051
At time: 26.43407416343689 and batch: 850, loss is 4.236662497520447 and perplexity is 69.17658909580003
At time: 26.896335124969482 and batch: 900, loss is 4.191816120147705 and perplexity is 66.14280522845912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.478626407989084 and perplexity of 88.11355743468498
finished 3 epochs...
Completing Train Step...
At time: 28.124292135238647 and batch: 50, loss is 4.252830510139465 and perplexity is 70.30412752716119
At time: 28.599701166152954 and batch: 100, loss is 4.139780497550964 and perplexity is 62.789037588938086
At time: 29.061872959136963 and batch: 150, loss is 4.1449579238891605 and perplexity is 63.11496621356088
At time: 29.52393341064453 and batch: 200, loss is 4.04761664390564 and perplexity is 57.26082135682535
At time: 29.985438108444214 and batch: 250, loss is 4.194875807762146 and perplexity is 66.34549147056953
At time: 30.447457551956177 and batch: 300, loss is 4.1691414785385135 and perplexity is 64.65991634561182
At time: 30.9090838432312 and batch: 350, loss is 4.156189107894898 and perplexity is 63.82781760196686
At time: 31.382790088653564 and batch: 400, loss is 4.085544981956482 and perplexity is 59.474341363666674
At time: 31.851388216018677 and batch: 450, loss is 4.122118573188782 and perplexity is 61.68979827200855
At time: 32.31348705291748 and batch: 500, loss is 4.010219402313233 and perplexity is 55.158971241842856
At time: 32.77580142021179 and batch: 550, loss is 4.083227605819702 and perplexity is 59.33667651646336
At time: 33.2377986907959 and batch: 600, loss is 4.097075834274292 and perplexity is 60.164100322430066
At time: 33.70029306411743 and batch: 650, loss is 3.956008234024048 and perplexity is 52.24834595729042
At time: 34.16244316101074 and batch: 700, loss is 3.9682641315460203 and perplexity is 52.89263644540769
At time: 34.6249897480011 and batch: 750, loss is 4.060276508331299 and perplexity is 57.99034367544503
At time: 35.086820125579834 and batch: 800, loss is 4.030665130615234 and perplexity is 56.29834454927688
At time: 35.563851833343506 and batch: 850, loss is 4.101348381042481 and perplexity is 60.42170417520565
At time: 36.026068687438965 and batch: 900, loss is 4.063887763023376 and perplexity is 58.20014016241305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442687152183219 and perplexity of 85.00305118865867
finished 4 epochs...
Completing Train Step...
At time: 37.268189907073975 and batch: 50, loss is 4.129838538169861 and perplexity is 62.16788438397271
At time: 37.743327140808105 and batch: 100, loss is 4.021973872184754 and perplexity is 55.81116127220549
At time: 38.205743074417114 and batch: 150, loss is 4.02777428150177 and perplexity is 56.13582954620166
At time: 38.6682014465332 and batch: 200, loss is 3.9324398946762087 and perplexity is 51.031336983118905
At time: 39.130338191986084 and batch: 250, loss is 4.0811195564270015 and perplexity is 59.211723621278274
At time: 39.59241080284119 and batch: 300, loss is 4.059430718421936 and perplexity is 57.94131676407998
At time: 40.05507946014404 and batch: 350, loss is 4.044996747970581 and perplexity is 57.11100030718976
At time: 40.517566204071045 and batch: 400, loss is 3.9804020643234255 and perplexity is 53.538555843896724
At time: 40.979756355285645 and batch: 450, loss is 4.0173835182189945 and perplexity is 55.55555539627901
At time: 41.44208908081055 and batch: 500, loss is 3.9078311824798586 and perplexity is 49.79084752391813
At time: 41.90454292297363 and batch: 550, loss is 3.9788151168823243 and perplexity is 53.45366034986026
At time: 42.37507772445679 and batch: 600, loss is 3.995403037071228 and perplexity is 54.347740364417106
At time: 42.847301959991455 and batch: 650, loss is 3.856915006637573 and perplexity is 47.31914671108943
At time: 43.31076240539551 and batch: 700, loss is 3.864608874320984 and perplexity is 47.68461810619806
At time: 43.773388624191284 and batch: 750, loss is 3.960929136276245 and perplexity is 52.50608860366077
At time: 44.23522996902466 and batch: 800, loss is 3.9312077522277833 and perplexity is 50.96849782796741
At time: 44.69721794128418 and batch: 850, loss is 4.0040802383422855 and perplexity is 54.82137860151044
At time: 45.15987205505371 and batch: 900, loss is 3.9699893045425414 and perplexity is 52.98396414891111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428680001872859 and perplexity of 83.8207006858194
finished 5 epochs...
Completing Train Step...
At time: 46.407047510147095 and batch: 50, loss is 4.041977424621582 and perplexity is 56.9388237895687
At time: 46.87006878852844 and batch: 100, loss is 3.9363560009002687 and perplexity is 51.23157293623212
At time: 47.34502363204956 and batch: 150, loss is 3.9376500988006593 and perplexity is 51.297914524198994
At time: 47.805720806121826 and batch: 200, loss is 3.8432408285140993 and perplexity is 46.67650011595971
At time: 48.26837658882141 and batch: 250, loss is 3.9942430210113526 and perplexity is 54.284732664813625
At time: 48.73084259033203 and batch: 300, loss is 3.974955987930298 and perplexity is 53.24777330945236
At time: 49.209587812423706 and batch: 350, loss is 3.958766222000122 and perplexity is 52.39264516348058
At time: 49.67509937286377 and batch: 400, loss is 3.8969596672058104 and perplexity is 49.25247731699833
At time: 50.138076066970825 and batch: 450, loss is 3.936182780265808 and perplexity is 51.22269933923093
At time: 50.60072684288025 and batch: 500, loss is 3.828614492416382 and perplexity is 45.99876242930311
At time: 51.06335258483887 and batch: 550, loss is 3.8977595663070677 and perplexity is 49.2918900903584
At time: 51.526164531707764 and batch: 600, loss is 3.9152806329727174 and perplexity is 50.163146968980406
At time: 51.988028049468994 and batch: 650, loss is 3.7802638483047484 and perplexity is 43.82760404918803
At time: 52.45126390457153 and batch: 700, loss is 3.78607367515564 and perplexity is 44.08297595496072
At time: 52.91386032104492 and batch: 750, loss is 3.8858687496185302 and perplexity is 48.70924021760748
At time: 53.37653875350952 and batch: 800, loss is 3.8531818103790285 and perplexity is 47.14282437734842
At time: 53.83924412727356 and batch: 850, loss is 3.929981107711792 and perplexity is 50.906015928992275
At time: 54.30194449424744 and batch: 900, loss is 3.8963394451141355 and perplexity is 49.2219393136477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.419317376123716 and perplexity of 83.03958120593369
finished 6 epochs...
Completing Train Step...
At time: 55.53431177139282 and batch: 50, loss is 3.9669343614578247 and perplexity is 52.82234814357495
At time: 55.995036125183105 and batch: 100, loss is 3.868265833854675 and perplexity is 47.8593180656394
At time: 56.458200216293335 and batch: 150, loss is 3.8683808851242065 and perplexity is 47.86482465770589
At time: 56.91835904121399 and batch: 200, loss is 3.7740461921691892 and perplexity is 43.555944495987546
At time: 57.37951588630676 and batch: 250, loss is 3.925338416099548 and perplexity is 50.67022277680608
At time: 57.83978629112244 and batch: 300, loss is 3.9065542316436765 and perplexity is 49.72730763682826
At time: 58.30056548118591 and batch: 350, loss is 3.8927790546417236 and perplexity is 49.047001597967885
At time: 58.762343645095825 and batch: 400, loss is 3.8281593561172484 and perplexity is 45.97783148638359
At time: 59.23491072654724 and batch: 450, loss is 3.8701663780212403 and perplexity is 47.9503633037478
At time: 59.69833993911743 and batch: 500, loss is 3.76595175743103 and perplexity is 43.20480679361992
At time: 60.158384561538696 and batch: 550, loss is 3.832557144165039 and perplexity is 46.180477514691496
At time: 60.618730306625366 and batch: 600, loss is 3.8515969467163087 and perplexity is 47.06816860327321
At time: 61.08016276359558 and batch: 650, loss is 3.716352915763855 and perplexity is 41.11417348748264
At time: 61.542917251586914 and batch: 700, loss is 3.7217979764938356 and perplexity is 41.33865325734747
At time: 62.00549602508545 and batch: 750, loss is 3.8250599479675294 and perplexity is 45.83554803191927
At time: 62.467647314071655 and batch: 800, loss is 3.7895381927490233 and perplexity is 44.23596706789876
At time: 62.93060064315796 and batch: 850, loss is 3.8684578227996824 and perplexity is 47.86850740772141
At time: 63.39459848403931 and batch: 900, loss is 3.8378420877456665 and perplexity is 46.42518479540048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.420408902102953 and perplexity of 83.13027055200945
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 64.62450170516968 and batch: 50, loss is 3.9234403944015503 and perplexity is 50.5741408062103
At time: 65.10145664215088 and batch: 100, loss is 3.808754353523254 and perplexity is 45.094232395185394
At time: 65.5787365436554 and batch: 150, loss is 3.804637560844421 and perplexity is 44.90897039368038
At time: 66.04754424095154 and batch: 200, loss is 3.6935979080200196 and perplexity is 40.189184133553844
At time: 66.51068234443665 and batch: 250, loss is 3.841871337890625 and perplexity is 46.61262083773441
At time: 66.9734582901001 and batch: 300, loss is 3.813329620361328 and perplexity is 45.30102324217338
At time: 67.43639159202576 and batch: 350, loss is 3.786072835922241 and perplexity is 44.08293895907051
At time: 67.89982485771179 and batch: 400, loss is 3.715660262107849 and perplexity is 41.08570546528087
At time: 68.36287927627563 and batch: 450, loss is 3.7433141899108886 and perplexity is 42.23774235399971
At time: 68.82562947273254 and batch: 500, loss is 3.634926257133484 and perplexity is 37.899058008916036
At time: 69.2892689704895 and batch: 550, loss is 3.687173538208008 and perplexity is 39.931821533662074
At time: 69.75274968147278 and batch: 600, loss is 3.696701717376709 and perplexity is 40.314117483658976
At time: 70.21637272834778 and batch: 650, loss is 3.547687191963196 and perplexity is 34.732894005016625
At time: 70.6797730922699 and batch: 700, loss is 3.5403126764297483 and perplexity is 34.47769786912591
At time: 71.16694474220276 and batch: 750, loss is 3.632791528701782 and perplexity is 37.818240105079276
At time: 71.63757491111755 and batch: 800, loss is 3.5850774097442626 and perplexity is 36.05614870808355
At time: 72.10040402412415 and batch: 850, loss is 3.650118737220764 and perplexity is 38.47923469517741
At time: 72.56251406669617 and batch: 900, loss is 3.6029731369018556 and perplexity is 36.707207921306114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33874051864833 and perplexity of 76.61098843775906
finished 8 epochs...
Completing Train Step...
At time: 73.78783631324768 and batch: 50, loss is 3.832682685852051 and perplexity is 46.186275453679606
At time: 74.2638292312622 and batch: 100, loss is 3.7259040307998657 and perplexity is 41.50874096828056
At time: 74.72751712799072 and batch: 150, loss is 3.7256408405303953 and perplexity is 41.497817709070574
At time: 75.19161748886108 and batch: 200, loss is 3.6217201471328737 and perplexity is 37.401849203157724
At time: 75.65479636192322 and batch: 250, loss is 3.7722914171218873 and perplexity is 43.47958063172128
At time: 76.11832547187805 and batch: 300, loss is 3.7506570768356324 and perplexity is 42.549030799319766
At time: 76.58083915710449 and batch: 350, loss is 3.7282163095474243 and perplexity is 41.60483179943927
At time: 77.06205201148987 and batch: 400, loss is 3.6612446689605713 and perplexity is 38.90974249307558
At time: 77.53598761558533 and batch: 450, loss is 3.6940421295166015 and perplexity is 40.20704099898415
At time: 77.99982905387878 and batch: 500, loss is 3.589375901222229 and perplexity is 36.21146933908264
At time: 78.46321773529053 and batch: 550, loss is 3.6451467800140382 and perplexity is 38.28839240990389
At time: 78.9259569644928 and batch: 600, loss is 3.65960382938385 and perplexity is 38.845950198446396
At time: 79.38856315612793 and batch: 650, loss is 3.515054564476013 and perplexity is 33.61776220945859
At time: 79.8517997264862 and batch: 700, loss is 3.512793292999268 and perplexity is 33.541829207583966
At time: 80.31457424163818 and batch: 750, loss is 3.6104863834381105 and perplexity is 36.984036863901224
At time: 80.77742433547974 and batch: 800, loss is 3.568007831573486 and perplexity is 35.4459085473303
At time: 81.24052023887634 and batch: 850, loss is 3.638736505508423 and perplexity is 38.04373829197403
At time: 81.70406293869019 and batch: 900, loss is 3.597993493080139 and perplexity is 36.524873457288514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337919470382063 and perplexity of 76.54811293396908
finished 9 epochs...
Completing Train Step...
At time: 82.94157266616821 and batch: 50, loss is 3.793952536582947 and perplexity is 44.4316714718287
At time: 83.40321516990662 and batch: 100, loss is 3.6884576320648192 and perplexity is 39.983130676211815
At time: 83.8741443157196 and batch: 150, loss is 3.687315249443054 and perplexity is 39.9374807223851
At time: 84.35019397735596 and batch: 200, loss is 3.5862743282318115 and perplexity is 36.099330816638826
At time: 84.82815670967102 and batch: 250, loss is 3.736857681274414 and perplexity is 41.96591248595367
At time: 85.29933404922485 and batch: 300, loss is 3.7179297971725465 and perplexity is 41.17905680649384
At time: 85.76257252693176 and batch: 350, loss is 3.696390013694763 and perplexity is 40.301553383044855
At time: 86.2250804901123 and batch: 400, loss is 3.63082754611969 and perplexity is 37.74403862928153
At time: 86.68766784667969 and batch: 450, loss is 3.666033353805542 and perplexity is 39.09651582968114
At time: 87.15006232261658 and batch: 500, loss is 3.5627209329605103 and perplexity is 35.25900413029886
At time: 87.61297273635864 and batch: 550, loss is 3.6197404050827027 and perplexity is 37.327876437194355
At time: 88.0757737159729 and batch: 600, loss is 3.6362697172164915 and perplexity is 37.95000809757652
At time: 88.53712129592896 and batch: 650, loss is 3.4939128446578978 and perplexity is 32.91448533646189
At time: 88.99915361404419 and batch: 700, loss is 3.4934812641143798 and perplexity is 32.9002831499049
At time: 89.46169018745422 and batch: 750, loss is 3.593040976524353 and perplexity is 36.34443060870711
At time: 89.92493009567261 and batch: 800, loss is 3.5529818391799926 and perplexity is 34.91728012457336
At time: 90.38757967948914 and batch: 850, loss is 3.625589232444763 and perplexity is 37.546840459442805
At time: 90.84995317459106 and batch: 900, loss is 3.587540001869202 and perplexity is 36.14504971447953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340810697372645 and perplexity of 76.76975115296929
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 92.09782862663269 and batch: 50, loss is 3.7865818881988527 and perplexity is 44.105385192175675
At time: 92.56075763702393 and batch: 100, loss is 3.6824337530136106 and perplexity is 39.74300111478071
At time: 93.02335548400879 and batch: 150, loss is 3.6815001153945923 and perplexity is 39.70591287003161
At time: 93.48561692237854 and batch: 200, loss is 3.573548173904419 and perplexity is 35.64283603402849
At time: 93.94836664199829 and batch: 250, loss is 3.7237518215179444 and perplexity is 41.41950153610134
At time: 94.41100215911865 and batch: 300, loss is 3.697231817245483 and perplexity is 40.33549365729966
At time: 94.89223051071167 and batch: 350, loss is 3.6710426902770994 and perplexity is 39.29285478571587
At time: 95.35380458831787 and batch: 400, loss is 3.60487181186676 and perplexity is 36.77696918404213
At time: 95.81580853462219 and batch: 450, loss is 3.635686812400818 and perplexity is 37.92789330113844
At time: 96.27754878997803 and batch: 500, loss is 3.5305069494247436 and perplexity is 34.141271126138626
At time: 96.74658370018005 and batch: 550, loss is 3.5820116567611695 and perplexity is 35.94577873260948
At time: 97.20839595794678 and batch: 600, loss is 3.599926404953003 and perplexity is 36.59554109400391
At time: 97.68033576011658 and batch: 650, loss is 3.446966700553894 and perplexity is 31.40498695585902
At time: 98.15283226966858 and batch: 700, loss is 3.4439988327026367 and perplexity is 31.311919279299715
At time: 98.61537098884583 and batch: 750, loss is 3.53767418384552 and perplexity is 34.38684862421145
At time: 99.07676815986633 and batch: 800, loss is 3.494986276626587 and perplexity is 32.94983576700172
At time: 99.53934121131897 and batch: 850, loss is 3.5646638679504394 and perplexity is 35.32757667756263
At time: 100.00106525421143 and batch: 900, loss is 3.5271111822128294 and perplexity is 34.025531939960146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324352943733947 and perplexity of 75.51663353667476
finished 11 epochs...
Completing Train Step...
At time: 101.22102427482605 and batch: 50, loss is 3.762008981704712 and perplexity is 43.0347953089537
At time: 101.69583010673523 and batch: 100, loss is 3.6548505210876465 and perplexity is 38.661741568132655
At time: 102.15782380104065 and batch: 150, loss is 3.6565931940078737 and perplexity is 38.72917507842262
At time: 102.62103867530823 and batch: 200, loss is 3.550957007408142 and perplexity is 34.846650037544265
At time: 103.0833010673523 and batch: 250, loss is 3.703012285232544 and perplexity is 40.56932686871992
At time: 103.54496049880981 and batch: 300, loss is 3.6781906652450562 and perplexity is 39.574725329794255
At time: 104.00659656524658 and batch: 350, loss is 3.6539445543289184 and perplexity is 38.626731176957335
At time: 104.4690306186676 and batch: 400, loss is 3.589420757293701 and perplexity is 36.213093679769926
At time: 104.9438169002533 and batch: 450, loss is 3.621881113052368 and perplexity is 37.40787011077296
At time: 105.41236090660095 and batch: 500, loss is 3.5176176118850706 and perplexity is 33.70403664329032
At time: 105.87461447715759 and batch: 550, loss is 3.571197214126587 and perplexity is 35.5591395821898
At time: 106.33657240867615 and batch: 600, loss is 3.5913045024871826 and perplexity is 36.28137421230167
At time: 106.8119068145752 and batch: 650, loss is 3.439873375892639 and perplexity is 31.18300939758016
At time: 107.27352237701416 and batch: 700, loss is 3.4390101337432863 and perplexity is 31.15610252476875
At time: 107.73571968078613 and batch: 750, loss is 3.5341348028182984 and perplexity is 34.26535559649437
At time: 108.19768714904785 and batch: 800, loss is 3.4935493755340574 and perplexity is 32.9025241112147
At time: 108.65972900390625 and batch: 850, loss is 3.565401496887207 and perplexity is 35.35364493355388
At time: 109.12199115753174 and batch: 900, loss is 3.529863090515137 and perplexity is 34.119296039725505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32367946677012 and perplexity of 75.46579194584382
finished 12 epochs...
Completing Train Step...
At time: 110.34785056114197 and batch: 50, loss is 3.7498977327346803 and perplexity is 42.51673370763598
At time: 110.8236038684845 and batch: 100, loss is 3.6428775691986086 and perplexity is 38.201606480767126
At time: 111.28527021408081 and batch: 150, loss is 3.6444160556793213 and perplexity is 38.260424369541106
At time: 111.7471272945404 and batch: 200, loss is 3.539259395599365 and perplexity is 34.44140228895956
At time: 112.20954513549805 and batch: 250, loss is 3.691395869255066 and perplexity is 40.10078335884737
At time: 112.67157340049744 and batch: 300, loss is 3.667193603515625 and perplexity is 39.14190387648016
At time: 113.13407135009766 and batch: 350, loss is 3.6437206506729125 and perplexity is 38.23382712788654
At time: 113.5962438583374 and batch: 400, loss is 3.580017800331116 and perplexity is 35.87417941365147
At time: 114.05902028083801 and batch: 450, loss is 3.6132131671905516 and perplexity is 37.08502195440179
At time: 114.52120280265808 and batch: 500, loss is 3.50938024520874 and perplexity is 33.42754448207259
At time: 114.98376488685608 and batch: 550, loss is 3.5641153287887573 and perplexity is 35.30820343224531
At time: 115.44578504562378 and batch: 600, loss is 3.585251879692078 and perplexity is 36.062439971269285
At time: 115.92416572570801 and batch: 650, loss is 3.4345208978652955 and perplexity is 31.016548910413793
At time: 116.3889799118042 and batch: 700, loss is 3.434641189575195 and perplexity is 31.020280168532626
At time: 116.85085654258728 and batch: 750, loss is 3.5304092502593996 and perplexity is 34.13793571538189
At time: 117.31344485282898 and batch: 800, loss is 3.4908532905578613 and perplexity is 32.81393558503834
At time: 117.77498602867126 and batch: 850, loss is 3.5637908029556273 and perplexity is 35.29674686718634
At time: 118.2488124370575 and batch: 900, loss is 3.5292134284973145 and perplexity is 34.0971372276623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323899778601241 and perplexity of 75.48241978424194
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 119.47979283332825 and batch: 50, loss is 3.749859938621521 and perplexity is 42.51512685575602
At time: 119.94155859947205 and batch: 100, loss is 3.6454878664016723 and perplexity is 38.30145428684688
At time: 120.40293574333191 and batch: 150, loss is 3.6474739694595337 and perplexity is 38.37760051442367
At time: 120.86468839645386 and batch: 200, loss is 3.5400344133377075 and perplexity is 34.46810533299668
At time: 121.32600331306458 and batch: 250, loss is 3.690600624084473 and perplexity is 40.0689060813497
At time: 121.78786754608154 and batch: 300, loss is 3.6646241760253906 and perplexity is 39.04146068864396
At time: 122.24951696395874 and batch: 350, loss is 3.639577031135559 and perplexity is 38.075728471359916
At time: 122.71171712875366 and batch: 400, loss is 3.577178225517273 and perplexity is 35.772456490597214
At time: 123.17343306541443 and batch: 450, loss is 3.6056855583190917 and perplexity is 36.806908492093896
At time: 123.63565254211426 and batch: 500, loss is 3.502083492279053 and perplexity is 33.184519673205656
At time: 124.09822130203247 and batch: 550, loss is 3.552432804107666 and perplexity is 34.898114574915624
At time: 124.56128454208374 and batch: 600, loss is 3.5773755598068235 and perplexity is 35.779516319434514
At time: 125.0242133140564 and batch: 650, loss is 3.423436374664307 and perplexity is 30.674643683334878
At time: 125.48749947547913 and batch: 700, loss is 3.420276041030884 and perplexity is 30.5778545986674
At time: 125.94975638389587 and batch: 750, loss is 3.515009045600891 and perplexity is 33.616232001565635
At time: 126.41283464431763 and batch: 800, loss is 3.4737686491012574 and perplexity is 32.25808305175586
At time: 126.87526869773865 and batch: 850, loss is 3.542352991104126 and perplexity is 34.548115034186445
At time: 127.33846998214722 and batch: 900, loss is 3.511220488548279 and perplexity is 33.48911593401408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318657025898973 and perplexity of 75.08771968552408
finished 14 epochs...
Completing Train Step...
At time: 128.58069252967834 and batch: 50, loss is 3.740156774520874 and perplexity is 42.10459057460928
At time: 129.043452501297 and batch: 100, loss is 3.6346326589584352 and perplexity is 37.88793254793599
At time: 129.50643396377563 and batch: 150, loss is 3.638191041946411 and perplexity is 38.022992477530345
At time: 129.96966791152954 and batch: 200, loss is 3.5316282606124876 and perplexity is 34.17957558701295
At time: 130.44532704353333 and batch: 250, loss is 3.682571334838867 and perplexity is 39.74846940557534
At time: 130.90854001045227 and batch: 300, loss is 3.658021855354309 and perplexity is 38.78454549720709
At time: 131.3714258670807 and batch: 350, loss is 3.633235764503479 and perplexity is 37.83504405347252
At time: 131.83487057685852 and batch: 400, loss is 3.5719011449813842 and perplexity is 35.58417956988774
At time: 132.2980453968048 and batch: 450, loss is 3.60140154838562 and perplexity is 36.64956460252722
At time: 132.76157903671265 and batch: 500, loss is 3.498123025894165 and perplexity is 33.05335340977825
At time: 133.22534918785095 and batch: 550, loss is 3.549271364212036 and perplexity is 34.78796049770982
At time: 133.68885040283203 and batch: 600, loss is 3.5751074361801147 and perplexity is 35.698455915377494
At time: 134.15140557289124 and batch: 650, loss is 3.421509780883789 and perplexity is 30.615602997566057
At time: 134.61465644836426 and batch: 700, loss is 3.4192022466659546 and perplexity is 30.54503789305774
At time: 135.07790064811707 and batch: 750, loss is 3.514295415878296 and perplexity is 33.59225101703706
At time: 135.55398082733154 and batch: 800, loss is 3.4741904878616334 and perplexity is 32.27169363205585
At time: 136.02188229560852 and batch: 850, loss is 3.5438109922409056 and perplexity is 34.59852296365949
At time: 136.48526811599731 and batch: 900, loss is 3.513837203979492 and perplexity is 33.576862173858224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318210131501498 and perplexity of 75.0541709012177
finished 15 epochs...
Completing Train Step...
At time: 137.80558514595032 and batch: 50, loss is 3.7363841009140013 and perplexity is 41.94604295927369
At time: 138.281574010849 and batch: 100, loss is 3.630627365112305 and perplexity is 37.73648374580327
At time: 138.74993753433228 and batch: 150, loss is 3.634252734184265 and perplexity is 37.873540717798456
At time: 139.2198610305786 and batch: 200, loss is 3.5277087259292603 and perplexity is 34.045869758531786
At time: 139.68333339691162 and batch: 250, loss is 3.6787031507492065 and perplexity is 39.59501200072488
At time: 140.14540338516235 and batch: 300, loss is 3.6546626806259157 and perplexity is 38.65448001077367
At time: 140.62155866622925 and batch: 350, loss is 3.629980893135071 and perplexity is 37.71209605037246
At time: 141.08957386016846 and batch: 400, loss is 3.569013543128967 and perplexity is 35.48157483914554
At time: 141.55379509925842 and batch: 450, loss is 3.598768739700317 and perplexity is 36.553200220683784
At time: 142.01658964157104 and batch: 500, loss is 3.49570839881897 and perplexity is 32.97363816773102
At time: 142.49248909950256 and batch: 550, loss is 3.547292275428772 and perplexity is 34.719180118984056
At time: 142.9556107521057 and batch: 600, loss is 3.573485870361328 and perplexity is 35.640615428234284
At time: 143.41807794570923 and batch: 650, loss is 3.4202038764953615 and perplexity is 30.575648041611558
At time: 143.88086366653442 and batch: 700, loss is 3.418411684036255 and perplexity is 30.520899670222366
At time: 144.34473323822021 and batch: 750, loss is 3.5136605882644654 and perplexity is 33.570932495989595
At time: 144.80803847312927 and batch: 800, loss is 3.4739912033081053 and perplexity is 32.26526302278062
At time: 145.27130365371704 and batch: 850, loss is 3.54400749206543 and perplexity is 34.60532223535662
At time: 145.73444247245789 and batch: 900, loss is 3.514525270462036 and perplexity is 33.59997323737135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318120250963185 and perplexity of 75.04742529508833
finished 16 epochs...
Completing Train Step...
At time: 146.96380043029785 and batch: 50, loss is 3.73316912651062 and perplexity is 41.81140405112782
At time: 147.43934082984924 and batch: 100, loss is 3.6274143838882447 and perplexity is 37.615431705170295
At time: 147.90192437171936 and batch: 150, loss is 3.631051049232483 and perplexity is 37.75247548220088
At time: 148.36414909362793 and batch: 200, loss is 3.524595980644226 and perplexity is 33.94005840529692
At time: 148.82615876197815 and batch: 250, loss is 3.675616898536682 and perplexity is 39.47300018379907
At time: 149.2882432937622 and batch: 300, loss is 3.651875696182251 and perplexity is 38.546900557070984
At time: 149.75031900405884 and batch: 350, loss is 3.627315707206726 and perplexity is 37.61172012232209
At time: 150.21194624900818 and batch: 400, loss is 3.5666096925735475 and perplexity is 35.3963848687626
At time: 150.67401814460754 and batch: 450, loss is 3.5965444469451904 and perplexity is 36.47198555833273
At time: 151.1365671157837 and batch: 500, loss is 3.493635654449463 and perplexity is 32.90536302777668
At time: 151.59810853004456 and batch: 550, loss is 3.5455276679992678 and perplexity is 34.657968418993704
At time: 152.0600140094757 and batch: 600, loss is 3.572009868621826 and perplexity is 35.5880486217576
At time: 152.52160930633545 and batch: 650, loss is 3.418980212211609 and perplexity is 30.538256595104944
At time: 152.9833722114563 and batch: 700, loss is 3.4175199842453003 and perplexity is 30.493696320779698
At time: 153.4449498653412 and batch: 750, loss is 3.5128802347183226 and perplexity is 33.54474551864823
At time: 153.91963624954224 and batch: 800, loss is 3.473501663208008 and perplexity is 32.2494717482372
At time: 154.3817548751831 and batch: 850, loss is 3.543763990402222 and perplexity is 34.59689680768091
At time: 154.84266352653503 and batch: 900, loss is 3.51464262008667 and perplexity is 33.60391641297902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318150350492295 and perplexity of 75.04968422124676
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 156.0825846195221 and batch: 50, loss is 3.733015785217285 and perplexity is 41.804993127896665
At time: 156.54561138153076 and batch: 100, loss is 3.6280890274047852 and perplexity is 37.640817274434085
At time: 157.0108082294464 and batch: 150, loss is 3.631863303184509 and perplexity is 37.783152536702836
At time: 157.4732756614685 and batch: 200, loss is 3.5252575874328613 and perplexity is 33.9625208081643
At time: 157.936119556427 and batch: 250, loss is 3.675772981643677 and perplexity is 39.479161733154534
At time: 158.3984501361847 and batch: 300, loss is 3.6519011878967285 and perplexity is 38.5478831961785
At time: 158.86084270477295 and batch: 350, loss is 3.627096486091614 and perplexity is 37.603475742799645
At time: 159.32297730445862 and batch: 400, loss is 3.566187272071838 and perplexity is 35.3814358677122
At time: 159.78568124771118 and batch: 450, loss is 3.5952588510513306 and perplexity is 36.42512745020748
At time: 160.24773120880127 and batch: 500, loss is 3.4915226554870604 and perplexity is 32.83590743548064
At time: 160.71000456809998 and batch: 550, loss is 3.541668257713318 and perplexity is 34.52446688349623
At time: 161.17222595214844 and batch: 600, loss is 3.5679041862487795 and perplexity is 35.44223493500904
At time: 161.6330087184906 and batch: 650, loss is 3.4145052242279053 and perplexity is 30.401903580394666
At time: 162.0943534374237 and batch: 700, loss is 3.4122552490234375 and perplexity is 30.333576946610727
At time: 162.55598759651184 and batch: 750, loss is 3.507437195777893 and perplexity is 33.36265617180442
At time: 163.01806163787842 and batch: 800, loss is 3.4666175079345702 and perplexity is 32.028223803612065
At time: 163.47984766960144 and batch: 850, loss is 3.5354803800582886 and perplexity is 34.31149331307452
At time: 163.94172263145447 and batch: 900, loss is 3.5063570070266725 and perplexity is 33.32663766279315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317790828339041 and perplexity of 75.02270704691104
finished 18 epochs...
Completing Train Step...
At time: 165.1889340877533 and batch: 50, loss is 3.730639319419861 and perplexity is 41.705762946833296
At time: 165.651136636734 and batch: 100, loss is 3.6252954816818237 and perplexity is 37.53581266620251
At time: 166.13168692588806 and batch: 150, loss is 3.6292574882507322 and perplexity is 37.684824801157696
At time: 166.59321522712708 and batch: 200, loss is 3.5228889083862303 and perplexity is 33.88216969731997
At time: 167.05541563034058 and batch: 250, loss is 3.6733918619155883 and perplexity is 39.38526895164274
At time: 167.5173783302307 and batch: 300, loss is 3.6494212532043457 and perplexity is 38.45240540160291
At time: 167.97910261154175 and batch: 350, loss is 3.6250624465942383 and perplexity is 37.52706652392885
At time: 168.44141364097595 and batch: 400, loss is 3.5644975328445434 and perplexity is 35.32170095003835
At time: 168.90404868125916 and batch: 450, loss is 3.5938033628463746 and perplexity is 36.37214967045901
At time: 169.36588168144226 and batch: 500, loss is 3.4902520513534547 and perplexity is 32.79421249025678
At time: 169.8275601863861 and batch: 550, loss is 3.5406192207336424 and perplexity is 34.48826843110908
At time: 170.28946828842163 and batch: 600, loss is 3.5675975847244263 and perplexity is 35.431369957445945
At time: 170.7519016265869 and batch: 650, loss is 3.4143474006652834 and perplexity is 30.39710582227072
At time: 171.21457052230835 and batch: 700, loss is 3.4120745944976805 and perplexity is 30.328097543607303
At time: 171.6769118309021 and batch: 750, loss is 3.5073455381393432 and perplexity is 33.35959836966156
At time: 172.13896012306213 and batch: 800, loss is 3.4670368385314942 and perplexity is 32.041657034103494
At time: 172.60055685043335 and batch: 850, loss is 3.536414232254028 and perplexity is 34.34355014228454
At time: 173.0634410381317 and batch: 900, loss is 3.507883896827698 and perplexity is 33.37756263445903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317547105763056 and perplexity of 75.00442454751162
finished 19 epochs...
Completing Train Step...
At time: 174.292551279068 and batch: 50, loss is 3.72939688205719 and perplexity is 41.65397832494542
At time: 174.76733350753784 and batch: 100, loss is 3.6238988399505616 and perplexity is 37.48342517560946
At time: 175.22942543029785 and batch: 150, loss is 3.6278226709365846 and perplexity is 37.63079273439449
At time: 175.69200372695923 and batch: 200, loss is 3.521532440185547 and perplexity is 33.83624076916312
At time: 176.15405106544495 and batch: 250, loss is 3.6720463275909423 and perplexity is 39.332310357176176
At time: 176.6276912689209 and batch: 300, loss is 3.648134512901306 and perplexity is 38.4029589610102
At time: 177.091126203537 and batch: 350, loss is 3.6239123249053957 and perplexity is 37.483930641313066
At time: 177.55170059204102 and batch: 400, loss is 3.5635068655014037 and perplexity is 35.286726221424146
At time: 178.02709031105042 and batch: 450, loss is 3.5929069662094117 and perplexity is 36.33956040645082
At time: 178.4895725250244 and batch: 500, loss is 3.489535775184631 and perplexity is 32.77073118792724
At time: 178.9513599872589 and batch: 550, loss is 3.5400427913665773 and perplexity is 34.46839410898793
At time: 179.41419458389282 and batch: 600, loss is 3.5673252010345458 and perplexity is 35.42172034441767
At time: 179.87677335739136 and batch: 650, loss is 3.4141719579696654 and perplexity is 30.391773339872497
At time: 180.33939504623413 and batch: 700, loss is 3.4119689750671385 and perplexity is 30.32489447637137
At time: 180.80162930488586 and batch: 750, loss is 3.507314052581787 and perplexity is 33.358548040642226
At time: 181.26412558555603 and batch: 800, loss is 3.4672410011291506 and perplexity is 32.048199409868275
At time: 181.72716069221497 and batch: 850, loss is 3.536853761672974 and perplexity is 34.358648460757614
At time: 182.18902969360352 and batch: 900, loss is 3.5085807704925536 and perplexity is 33.40083068536036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317432560332834 and perplexity of 74.99583362546804
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe3d26db70>
ELAPSED
949.1902785301208


RESULTS SO FAR:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.13177326071305, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.42094345278127, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.9680256792332449, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5177066140482257, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.09211446274884, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.11885787751686028, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.207900223189446, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -74.99583362546804, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.4107464825280964, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.25800213299985186, 'tune_wordvecs': 'TRUE'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.09730286041528366, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.6732898409058357, 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6958081722259521 and batch: 50, loss is 6.7435955047607425 and perplexity is 848.6064256171782
At time: 1.1751456260681152 and batch: 100, loss is 6.0103906059265135 and perplexity is 407.64251676327785
At time: 1.6398720741271973 and batch: 150, loss is 5.892376909255981 and perplexity is 362.26533366251215
At time: 2.1055102348327637 and batch: 200, loss is 5.739283056259155 and perplexity is 310.8414752246871
At time: 2.5700454711914062 and batch: 250, loss is 5.79406400680542 and perplexity is 328.3447116927931
At time: 3.0357179641723633 and batch: 300, loss is 5.70630373954773 and perplexity is 300.7573337858179
At time: 3.5027692317962646 and batch: 350, loss is 5.6908901405334475 and perplexity is 296.15712480627496
At time: 3.968569278717041 and batch: 400, loss is 5.554999284744262 and perplexity is 258.52677960774525
At time: 4.433885812759399 and batch: 450, loss is 5.556791639328003 and perplexity is 258.9905667775636
At time: 4.899891138076782 and batch: 500, loss is 5.5103613471984865 and perplexity is 247.24045057241958
At time: 5.365246772766113 and batch: 550, loss is 5.562282657623291 and perplexity is 260.41660032313064
At time: 5.843949556350708 and batch: 600, loss is 5.493388576507568 and perplexity is 243.079506363461
At time: 6.309886932373047 and batch: 650, loss is 5.391338882446289 and perplexity is 219.49706967942828
At time: 6.77740740776062 and batch: 700, loss is 5.488541326522827 and perplexity is 241.90409029914707
At time: 7.246099948883057 and batch: 750, loss is 5.47035659790039 and perplexity is 237.54488566709043
At time: 7.714375019073486 and batch: 800, loss is 5.453524036407471 and perplexity is 233.5798611500573
At time: 8.184162855148315 and batch: 850, loss is 5.487407627105713 and perplexity is 241.62999917081055
At time: 8.651392936706543 and batch: 900, loss is 5.3853633594512935 and perplexity is 218.1893708755123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.1974729773116435 and perplexity of 180.81474110803333
finished 1 epochs...
Completing Train Step...
At time: 9.902537107467651 and batch: 50, loss is 5.097877712249756 and perplexity is 163.67417474006217
At time: 10.365175485610962 and batch: 100, loss is 4.930494995117187 and perplexity is 138.44802647857077
At time: 10.828808069229126 and batch: 150, loss is 4.891349172592163 and perplexity is 133.13307242934096
At time: 11.297837495803833 and batch: 200, loss is 4.77338583946228 and perplexity is 118.31917425466938
At time: 11.765349626541138 and batch: 250, loss is 4.85765302658081 and perplexity is 128.7217408077522
At time: 12.228826522827148 and batch: 300, loss is 4.796008090972901 and perplexity is 121.02632585394541
At time: 12.692981719970703 and batch: 350, loss is 4.771630687713623 and perplexity is 118.11168828702044
At time: 13.156301975250244 and batch: 400, loss is 4.647666025161743 and perplexity is 104.34117149096905
At time: 13.620319604873657 and batch: 450, loss is 4.661968488693237 and perplexity is 105.84423039149249
At time: 14.084212303161621 and batch: 500, loss is 4.561555509567261 and perplexity is 95.73227654476815
At time: 14.547238826751709 and batch: 550, loss is 4.638908061981201 and perplexity is 103.43134528050645
At time: 15.010254383087158 and batch: 600, loss is 4.596050214767456 and perplexity is 99.0921489580727
At time: 15.473788499832153 and batch: 650, loss is 4.455877666473389 and perplexity is 86.13171259772206
At time: 15.953975200653076 and batch: 700, loss is 4.49650089263916 and perplexity is 89.70270212532317
At time: 16.42097496986389 and batch: 750, loss is 4.549211072921753 and perplexity is 94.55777968766789
At time: 16.885615348815918 and batch: 800, loss is 4.4938591194152835 and perplexity is 89.46604066932558
At time: 17.34946632385254 and batch: 850, loss is 4.554394969940185 and perplexity is 95.0492301937136
At time: 17.826001167297363 and batch: 900, loss is 4.494064550399781 and perplexity is 89.4844216540866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6043809864619005 and perplexity of 99.92111118098478
finished 2 epochs...
Completing Train Step...
At time: 19.074190616607666 and batch: 50, loss is 4.523023405075073 and perplexity is 92.11367434808623
At time: 19.53673481941223 and batch: 100, loss is 4.39361312866211 and perplexity is 80.93231019232586
At time: 19.999308824539185 and batch: 150, loss is 4.383944025039673 and perplexity is 80.15353837872854
At time: 20.463003873825073 and batch: 200, loss is 4.294700617790222 and perplexity is 73.31026337066012
At time: 20.92803931236267 and batch: 250, loss is 4.426306943893433 and perplexity is 83.62202513075665
At time: 21.39150333404541 and batch: 300, loss is 4.399752697944641 and perplexity is 81.43072818822979
At time: 21.85459065437317 and batch: 350, loss is 4.38100866317749 and perplexity is 79.91860371686577
At time: 22.317795515060425 and batch: 400, loss is 4.300392837524414 and perplexity is 73.72875143157202
At time: 22.79478669166565 and batch: 450, loss is 4.3265891361236575 and perplexity is 75.68569221129788
At time: 23.262139081954956 and batch: 500, loss is 4.213797154426575 and perplexity is 67.61278919476625
At time: 23.734167337417603 and batch: 550, loss is 4.306475129127502 and perplexity is 74.17855773718111
At time: 24.196303129196167 and batch: 600, loss is 4.292775230407715 and perplexity is 73.16924851223575
At time: 24.6608829498291 and batch: 650, loss is 4.151299662590027 and perplexity is 63.51649669237306
At time: 25.123865127563477 and batch: 700, loss is 4.167909469604492 and perplexity is 64.58030380275294
At time: 25.587580919265747 and batch: 750, loss is 4.259826140403748 and perplexity is 70.79767352927287
At time: 26.05122971534729 and batch: 800, loss is 4.216451334953308 and perplexity is 67.79248410908527
At time: 26.514968156814575 and batch: 850, loss is 4.285600185394287 and perplexity is 72.64613478703586
At time: 26.978206872940063 and batch: 900, loss is 4.238131942749024 and perplexity is 69.27831502661343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.480510554901541 and perplexity of 88.27973282213244
finished 3 epochs...
Completing Train Step...
At time: 28.225528717041016 and batch: 50, loss is 4.302348108291626 and perplexity is 73.87305213144649
At time: 28.706243753433228 and batch: 100, loss is 4.173334980010987 and perplexity is 64.93163713258292
At time: 29.168756008148193 and batch: 150, loss is 4.1710692977905275 and perplexity is 64.78468920829685
At time: 29.652809381484985 and batch: 200, loss is 4.080238404273987 and perplexity is 59.15957206362868
At time: 30.12638258934021 and batch: 250, loss is 4.226074953079223 and perplexity is 68.44804245004097
At time: 30.589168071746826 and batch: 300, loss is 4.2065176486969 and perplexity is 67.12238861092807
At time: 31.0520076751709 and batch: 350, loss is 4.187154402732849 and perplexity is 65.83518374077364
At time: 31.5147705078125 and batch: 400, loss is 4.119874501228333 and perplexity is 61.55151713989653
At time: 31.978285789489746 and batch: 450, loss is 4.154221067428589 and perplexity is 63.70232540141896
At time: 32.4408905506134 and batch: 500, loss is 4.035634269714356 and perplexity is 56.57879507615025
At time: 32.904388427734375 and batch: 550, loss is 4.125065836906433 and perplexity is 61.87188256962263
At time: 33.36745476722717 and batch: 600, loss is 4.126423306465149 and perplexity is 61.95592879894409
At time: 33.830594062805176 and batch: 650, loss is 3.984022135734558 and perplexity is 53.732720472167905
At time: 34.29359793663025 and batch: 700, loss is 3.988454132080078 and perplexity is 53.97139219836706
At time: 34.756550312042236 and batch: 750, loss is 4.093419108390808 and perplexity is 59.944498456141915
At time: 35.21912097930908 and batch: 800, loss is 4.058760786056519 and perplexity is 57.90251300008483
At time: 35.6815345287323 and batch: 850, loss is 4.129801640510559 and perplexity is 62.1655905768735
At time: 36.144758462905884 and batch: 900, loss is 4.090257391929627 and perplexity is 59.75527024921842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433933623849529 and perplexity of 84.26222173787545
finished 4 epochs...
Completing Train Step...
At time: 37.38226580619812 and batch: 50, loss is 4.163968653678894 and perplexity is 64.32630552199868
At time: 37.85685086250305 and batch: 100, loss is 4.034849925041199 and perplexity is 56.53443519860962
At time: 38.31970930099487 and batch: 150, loss is 4.0357478952407835 and perplexity is 56.58522423677689
At time: 38.793819427490234 and batch: 200, loss is 3.946446108818054 and perplexity is 51.75112177996397
At time: 39.26564073562622 and batch: 250, loss is 4.0991415309906 and perplexity is 60.28850955854212
At time: 39.735397815704346 and batch: 300, loss is 4.08206259727478 and perplexity is 59.267589032862865
At time: 40.20150637626648 and batch: 350, loss is 4.060655236244202 and perplexity is 58.01231039671644
At time: 40.674959659576416 and batch: 400, loss is 3.9978970193862917 and perplexity is 54.483451828402025
At time: 41.146615982055664 and batch: 450, loss is 4.03729733467102 and perplexity is 56.67296757331659
At time: 41.62273073196411 and batch: 500, loss is 3.9167888021469115 and perplexity is 50.2388585595121
At time: 42.0873806476593 and batch: 550, loss is 4.0048774719238285 and perplexity is 54.86510147187252
At time: 42.55026721954346 and batch: 600, loss is 4.010939455032348 and perplexity is 55.198702911801
At time: 43.012791872024536 and batch: 650, loss is 3.871358857154846 and perplexity is 48.00757721785581
At time: 43.475183725357056 and batch: 700, loss is 3.8708642148971557 and perplexity is 47.983836513536374
At time: 43.93791961669922 and batch: 750, loss is 3.9799595499038696 and perplexity is 53.514869502094015
At time: 44.400866985321045 and batch: 800, loss is 3.9492460680007935 and perplexity is 51.89622585655615
At time: 44.863635540008545 and batch: 850, loss is 4.018333458900452 and perplexity is 55.608354952683726
At time: 45.32653069496155 and batch: 900, loss is 3.982900133132935 and perplexity is 53.672466029147884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.412205108224529 and perplexity of 82.45107673963072
finished 5 epochs...
Completing Train Step...
At time: 46.57899308204651 and batch: 50, loss is 4.064985637664795 and perplexity is 58.2640717084144
At time: 47.0411958694458 and batch: 100, loss is 3.9352611017227175 and perplexity is 51.17551022626778
At time: 47.50476622581482 and batch: 150, loss is 3.939628200531006 and perplexity is 51.39948744534775
At time: 47.967684268951416 and batch: 200, loss is 3.8463064670562743 and perplexity is 46.81981295416975
At time: 48.43071460723877 and batch: 250, loss is 4.003172602653503 and perplexity is 54.77164333594881
At time: 48.89343285560608 and batch: 300, loss is 3.990812635421753 and perplexity is 54.09883413426256
At time: 49.355605125427246 and batch: 350, loss is 3.968332290649414 and perplexity is 52.89624168294743
At time: 49.83077263832092 and batch: 400, loss is 3.9072723722457887 and perplexity is 49.76303166137692
At time: 50.299954652786255 and batch: 450, loss is 3.9494943523406985 and perplexity is 51.90911247644326
At time: 50.762884855270386 and batch: 500, loss is 3.8278003215789793 and perplexity is 45.96132681993521
At time: 51.22533822059631 and batch: 550, loss is 3.9151133918762206 and perplexity is 50.15475833075966
At time: 51.68794250488281 and batch: 600, loss is 3.924084391593933 and perplexity is 50.60672090051529
At time: 52.15052652359009 and batch: 650, loss is 3.7843275690078735 and perplexity is 44.00606956253615
At time: 52.612905979156494 and batch: 700, loss is 3.782102818489075 and perplexity is 43.90827586007769
At time: 53.07466959953308 and batch: 750, loss is 3.8949515199661255 and perplexity is 49.15367033331687
At time: 53.550384283065796 and batch: 800, loss is 3.8635473251342773 and perplexity is 47.63402539670483
At time: 54.012574195861816 and batch: 850, loss is 3.9329955434799193 and perplexity is 51.05970036377484
At time: 54.47367763519287 and batch: 900, loss is 3.8986565923690795 and perplexity is 49.33612603784793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.406620391427654 and perplexity of 81.99189422205721
finished 6 epochs...
Completing Train Step...
At time: 55.72062659263611 and batch: 50, loss is 3.9859057760238645 and perplexity is 53.83402897373652
At time: 56.19161081314087 and batch: 100, loss is 3.8575867128372194 and perplexity is 47.350941952632205
At time: 56.65458798408508 and batch: 150, loss is 3.8613840246200564 and perplexity is 47.53109006524112
At time: 57.11759686470032 and batch: 200, loss is 3.772813277244568 and perplexity is 43.502276812604634
At time: 57.580907106399536 and batch: 250, loss is 3.9272465896606445 and perplexity is 50.76700266328831
At time: 58.042741775512695 and batch: 300, loss is 3.9171557331085207 and perplexity is 50.257296134645216
At time: 58.50589895248413 and batch: 350, loss is 3.895169677734375 and perplexity is 49.16439475810381
At time: 58.96829128265381 and batch: 400, loss is 3.837308168411255 and perplexity is 46.40040410766742
At time: 59.4309606552124 and batch: 450, loss is 3.8790294456481935 and perplexity is 48.377239538516264
At time: 59.909361362457275 and batch: 500, loss is 3.7588985347747803 and perplexity is 42.90114582457246
At time: 60.37419581413269 and batch: 550, loss is 3.8426207971572874 and perplexity is 46.64756819253833
At time: 60.83634805679321 and batch: 600, loss is 3.8541895294189454 and perplexity is 47.190355043825456
At time: 61.298993825912476 and batch: 650, loss is 3.716860876083374 and perplexity is 41.135063161297616
At time: 61.761192083358765 and batch: 700, loss is 3.712208380699158 and perplexity is 40.94412697907585
At time: 62.22346615791321 and batch: 750, loss is 3.827344470024109 and perplexity is 45.94038005231147
At time: 62.69891595840454 and batch: 800, loss is 3.7948544454574584 and perplexity is 44.47176286731355
At time: 63.17196083068848 and batch: 850, loss is 3.8677451753616334 and perplexity is 47.83440619106965
At time: 63.63450622558594 and batch: 900, loss is 3.8305795621871948 and perplexity is 46.08924207714258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.407101565844392 and perplexity of 82.03135611720253
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 64.8690242767334 and batch: 50, loss is 3.932538056373596 and perplexity is 51.03634655164697
At time: 65.34479212760925 and batch: 100, loss is 3.7938160848617555 and perplexity is 44.42560910740039
At time: 65.80772972106934 and batch: 150, loss is 3.796921453475952 and perplexity is 44.56378142654412
At time: 66.27089834213257 and batch: 200, loss is 3.688102865219116 and perplexity is 39.96894850289168
At time: 66.73363280296326 and batch: 250, loss is 3.838162589073181 and perplexity is 46.44006651343532
At time: 67.19649028778076 and batch: 300, loss is 3.8160819673538207 and perplexity is 45.42587912178361
At time: 67.65977764129639 and batch: 350, loss is 3.781641526222229 and perplexity is 43.888025982887854
At time: 68.12402844429016 and batch: 400, loss is 3.717340669631958 and perplexity is 41.15480423466413
At time: 68.58589696884155 and batch: 450, loss is 3.743885498046875 and perplexity is 42.261880014216544
At time: 69.04786586761475 and batch: 500, loss is 3.6191038417816164 and perplexity is 37.3041224422098
At time: 69.51053190231323 and batch: 550, loss is 3.6928334712982176 and perplexity is 40.1584737849375
At time: 69.97360801696777 and batch: 600, loss is 3.6917355728149412 and perplexity is 40.11440805175547
At time: 70.43748641014099 and batch: 650, loss is 3.54410813331604 and perplexity is 34.60880513352282
At time: 70.90059447288513 and batch: 700, loss is 3.5287361335754395 and perplexity is 34.08086672043718
At time: 71.36385321617126 and batch: 750, loss is 3.6293090105056764 and perplexity is 37.686766458327476
At time: 71.82719016075134 and batch: 800, loss is 3.588067932128906 and perplexity is 36.16413681784873
At time: 72.2902307510376 and batch: 850, loss is 3.6440885639190674 and perplexity is 38.24789644732388
At time: 72.7537841796875 and batch: 900, loss is 3.594938039779663 and perplexity is 36.41344373298383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3355340827001285 and perplexity of 76.36573361746785
finished 8 epochs...
Completing Train Step...
At time: 73.98017859458923 and batch: 50, loss is 3.8450118827819826 and perplexity is 46.759239977465306
At time: 74.4573347568512 and batch: 100, loss is 3.7127551555633547 and perplexity is 40.966520320044566
At time: 74.92295241355896 and batch: 150, loss is 3.7197355222702027 and perplexity is 41.25348203839887
At time: 75.38678860664368 and batch: 200, loss is 3.6165613746643066 and perplexity is 37.20939840496112
At time: 75.84956908226013 and batch: 250, loss is 3.767022104263306 and perplexity is 43.251075679170796
At time: 76.31263470649719 and batch: 300, loss is 3.75294527053833 and perplexity is 42.64650269841464
At time: 76.77637219429016 and batch: 350, loss is 3.7219884729385377 and perplexity is 41.346528873936435
At time: 77.25374341011047 and batch: 400, loss is 3.6626267290115355 and perplexity is 38.96355527144857
At time: 77.71752285957336 and batch: 450, loss is 3.6943961811065673 and perplexity is 40.22127888610222
At time: 78.18124341964722 and batch: 500, loss is 3.5729510974884033 and perplexity is 35.621560889308284
At time: 78.64479160308838 and batch: 550, loss is 3.6486023092269897 and perplexity is 38.42092792668792
At time: 79.10873627662659 and batch: 600, loss is 3.6543311929702758 and perplexity is 38.64166865133598
At time: 79.57194781303406 and batch: 650, loss is 3.5109812116622923 and perplexity is 33.48110372124583
At time: 80.03585886955261 and batch: 700, loss is 3.499788188934326 and perplexity is 33.108438482314845
At time: 80.49939584732056 and batch: 750, loss is 3.6064282655715942 and perplexity is 36.83425540409103
At time: 80.96334052085876 and batch: 800, loss is 3.5688545227050783 and perplexity is 35.47593299267067
At time: 81.42798829078674 and batch: 850, loss is 3.6319654750823975 and perplexity is 37.78701311032343
At time: 81.89253830909729 and batch: 900, loss is 3.590211720466614 and perplexity is 36.241748234102566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333830532962328 and perplexity of 76.23575153888973
finished 9 epochs...
Completing Train Step...
At time: 83.1402530670166 and batch: 50, loss is 3.806305661201477 and perplexity is 44.98394557891048
At time: 83.60390448570251 and batch: 100, loss is 3.6749633169174194 and perplexity is 39.44720978540503
At time: 84.06721353530884 and batch: 150, loss is 3.6821031093597414 and perplexity is 39.72986251589326
At time: 84.53147220611572 and batch: 200, loss is 3.5809963226318358 and perplexity is 35.909300278700215
At time: 84.99579954147339 and batch: 250, loss is 3.7315640592575074 and perplexity is 41.74434776499929
At time: 85.45983552932739 and batch: 300, loss is 3.7196385526657103 and perplexity is 41.2494818985108
At time: 85.92317414283752 and batch: 350, loss is 3.689300150871277 and perplexity is 40.0168314105053
At time: 86.3876268863678 and batch: 400, loss is 3.6318270444869993 and perplexity is 37.781782593640365
At time: 86.85145831108093 and batch: 450, loss is 3.6651885509490967 and perplexity is 39.06350092893593
At time: 87.3231418132782 and batch: 500, loss is 3.5455300331115724 and perplexity is 34.658050389078205
At time: 87.7880129814148 and batch: 550, loss is 3.621543436050415 and perplexity is 37.39524046583483
At time: 88.25201058387756 and batch: 600, loss is 3.630150547027588 and perplexity is 37.71849459701621
At time: 88.71558737754822 and batch: 650, loss is 3.489602217674255 and perplexity is 32.77290862923067
At time: 89.19225907325745 and batch: 700, loss is 3.479256815910339 and perplexity is 32.43560748876085
At time: 89.65556764602661 and batch: 750, loss is 3.588458709716797 and perplexity is 36.17827171362293
At time: 90.1195900440216 and batch: 800, loss is 3.55192138671875 and perplexity is 34.88027163526534
At time: 90.58357739448547 and batch: 850, loss is 3.618099265098572 and perplexity is 37.26666640750245
At time: 91.04794716835022 and batch: 900, loss is 3.579368200302124 and perplexity is 35.85088311312226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335239776193279 and perplexity of 76.34326199209889
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 92.28112006187439 and batch: 50, loss is 3.7962070226669313 and perplexity is 44.531955058349226
At time: 92.74110722541809 and batch: 100, loss is 3.666461009979248 and perplexity is 39.113239271732844
At time: 93.20176005363464 and batch: 150, loss is 3.6734912824630737 and perplexity is 39.38918485130199
At time: 93.66204953193665 and batch: 200, loss is 3.5644491481781007 and perplexity is 35.31999196266441
At time: 94.1225802898407 and batch: 250, loss is 3.714150824546814 and perplexity is 41.02373593956079
At time: 94.58233976364136 and batch: 300, loss is 3.70205114364624 and perplexity is 40.53035273436904
At time: 95.04272270202637 and batch: 350, loss is 3.6644757747650147 and perplexity is 39.035667316553344
At time: 95.50349378585815 and batch: 400, loss is 3.6065858602523804 and perplexity is 36.840060744246784
At time: 95.96666717529297 and batch: 450, loss is 3.630650968551636 and perplexity is 37.73737446711993
At time: 96.42686200141907 and batch: 500, loss is 3.509698166847229 and perplexity is 33.438173511294664
At time: 96.88746523857117 and batch: 550, loss is 3.5830858325958252 and perplexity is 35.9844115649947
At time: 97.34944558143616 and batch: 600, loss is 3.5906267261505125 and perplexity is 36.25679188699895
At time: 97.8121109008789 and batch: 650, loss is 3.4450326585769653 and perplexity is 31.344307090419452
At time: 98.2742440700531 and batch: 700, loss is 3.428247332572937 and perplexity is 30.822573660079676
At time: 98.73737621307373 and batch: 750, loss is 3.5317566537857057 and perplexity is 34.183964292915725
At time: 99.2069206237793 and batch: 800, loss is 3.4933284425735476 and perplexity is 32.89525566210412
At time: 99.68010687828064 and batch: 850, loss is 3.556680369377136 and perplexity is 35.046661853145544
At time: 100.14725112915039 and batch: 900, loss is 3.5177359342575074 and perplexity is 33.70802482080732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319597636183647 and perplexity of 75.1583811941741
finished 11 epochs...
Completing Train Step...
At time: 101.4019706249237 and batch: 50, loss is 3.771882348060608 and perplexity is 43.46179811787354
At time: 101.88360619544983 and batch: 100, loss is 3.6407584714889527 and perplexity is 38.12073925700458
At time: 102.35752177238464 and batch: 150, loss is 3.6495685815811156 and perplexity is 38.45807094941136
At time: 102.82152986526489 and batch: 200, loss is 3.542906346321106 and perplexity is 34.56723770420409
At time: 103.28359389305115 and batch: 250, loss is 3.693607096672058 and perplexity is 40.18955341967917
At time: 103.7460868358612 and batch: 300, loss is 3.68352641582489 and perplexity is 39.78645054757426
At time: 104.20724058151245 and batch: 350, loss is 3.647131128311157 and perplexity is 38.36444534898624
At time: 104.66861653327942 and batch: 400, loss is 3.590797338485718 and perplexity is 36.26297827065152
At time: 105.1301498413086 and batch: 450, loss is 3.616753134727478 and perplexity is 37.21653436572411
At time: 105.59245228767395 and batch: 500, loss is 3.496681056022644 and perplexity is 33.00572581703808
At time: 106.05531215667725 and batch: 550, loss is 3.5718746280670164 and perplexity is 35.58323599975557
At time: 106.51684927940369 and batch: 600, loss is 3.58164324760437 and perplexity is 35.93253841765249
At time: 106.97935461997986 and batch: 650, loss is 3.4377195167541506 and perplexity is 31.115917866614225
At time: 107.45351266860962 and batch: 700, loss is 3.4227701902389525 and perplexity is 30.654215518677816
At time: 107.91525483131409 and batch: 750, loss is 3.5282132053375244 and perplexity is 34.06304953181798
At time: 108.37678599357605 and batch: 800, loss is 3.4916271448135374 and perplexity is 32.83933861659062
At time: 108.84029722213745 and batch: 850, loss is 3.557120876312256 and perplexity is 35.06210355158258
At time: 109.30158424377441 and batch: 900, loss is 3.5204811239242555 and perplexity is 33.80068687149163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318925831415882 and perplexity of 75.10790639185515
finished 12 epochs...
Completing Train Step...
At time: 110.62167572975159 and batch: 50, loss is 3.759942479133606 and perplexity is 42.94595561913975
At time: 111.09827709197998 and batch: 100, loss is 3.628843221664429 and perplexity is 37.66921647066
At time: 111.56233620643616 and batch: 150, loss is 3.6373606061935426 and perplexity is 37.99142993229282
At time: 112.02610421180725 and batch: 200, loss is 3.5312421798706053 and perplexity is 34.166382058160686
At time: 112.49664616584778 and batch: 250, loss is 3.6818302965164182 and perplexity is 39.71902517748523
At time: 112.97614049911499 and batch: 300, loss is 3.672608151435852 and perplexity is 39.35441439571677
At time: 113.43962025642395 and batch: 350, loss is 3.6367476844787596 and perplexity is 37.96815129463168
At time: 114.0613362789154 and batch: 400, loss is 3.581381769180298 and perplexity is 35.923144062398265
At time: 114.53083515167236 and batch: 450, loss is 3.6078757143020628 and perplexity is 36.887609704805605
At time: 115.00414299964905 and batch: 500, loss is 3.488378143310547 and perplexity is 32.7328166947141
At time: 115.46646118164062 and batch: 550, loss is 3.5644285774230955 and perplexity is 35.31926541123585
At time: 115.92692971229553 and batch: 600, loss is 3.5753963661193846 and perplexity is 35.70877175828332
At time: 116.3897979259491 and batch: 650, loss is 3.4323191976547243 and perplexity is 30.9483348891146
At time: 116.85226774215698 and batch: 700, loss is 3.4180886793136596 and perplexity is 30.511042867473638
At time: 117.31484007835388 and batch: 750, loss is 3.5243513870239256 and perplexity is 33.93175789870522
At time: 117.78747391700745 and batch: 800, loss is 3.488647289276123 and perplexity is 32.74162778595106
At time: 118.25865578651428 and batch: 850, loss is 3.5553123855590822 and perplexity is 34.998751364711644
At time: 118.73356580734253 and batch: 900, loss is 3.519781002998352 and perplexity is 33.77703058539974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3192117769424225 and perplexity of 75.12938623258154
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 119.9869544506073 and batch: 50, loss is 3.7591834115982055 and perplexity is 42.913369107698635
At time: 120.44921875 and batch: 100, loss is 3.6308688068389894 and perplexity is 37.7455960075937
At time: 120.9112617969513 and batch: 150, loss is 3.642737584114075 and perplexity is 38.196259199933095
At time: 121.37384939193726 and batch: 200, loss is 3.529975175857544 and perplexity is 34.12312052703529
At time: 121.83640670776367 and batch: 250, loss is 3.6793326997756957 and perplexity is 39.619946850015275
At time: 122.29912281036377 and batch: 300, loss is 3.6712507104873655 and perplexity is 39.30102934383752
At time: 122.76137614250183 and batch: 350, loss is 3.631367259025574 and perplexity is 37.76441507226995
At time: 123.22331690788269 and batch: 400, loss is 3.5784897661209105 and perplexity is 35.81940430002592
At time: 123.68532276153564 and batch: 450, loss is 3.6012673425674437 and perplexity is 36.64464634776052
At time: 124.14720845222473 and batch: 500, loss is 3.4813832330703733 and perplexity is 32.50465250435763
At time: 124.622731924057 and batch: 550, loss is 3.553381071090698 and perplexity is 34.93122300006936
At time: 125.0842649936676 and batch: 600, loss is 3.566598687171936 and perplexity is 35.39599531947511
At time: 125.54631471633911 and batch: 650, loss is 3.420275721549988 and perplexity is 30.577844829628578
At time: 126.00840640068054 and batch: 700, loss is 3.4037840795516967 and perplexity is 30.077701388369864
At time: 126.47059202194214 and batch: 750, loss is 3.5082987689971925 and perplexity is 33.39141292913387
At time: 126.93326783180237 and batch: 800, loss is 3.4705717849731443 and perplexity is 32.155123005532324
At time: 127.39522552490234 and batch: 850, loss is 3.5362612771987916 and perplexity is 34.33829752439298
At time: 127.85716104507446 and batch: 900, loss is 3.5036066579818725 and perplexity is 33.23510370959137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313596960616438 and perplexity of 74.70873058543688
finished 14 epochs...
Completing Train Step...
At time: 129.17377090454102 and batch: 50, loss is 3.74986123085022 and perplexity is 42.51518179505859
At time: 129.64938378334045 and batch: 100, loss is 3.6208513021469115 and perplexity is 37.36936690709526
At time: 130.1121847629547 and batch: 150, loss is 3.6336197662353515 and perplexity is 37.84957556579891
At time: 130.57490611076355 and batch: 200, loss is 3.5221254205703736 and perplexity is 33.856310946258084
At time: 131.03838634490967 and batch: 250, loss is 3.6716948127746583 and perplexity is 39.31848689704481
At time: 131.50099754333496 and batch: 300, loss is 3.66480122089386 and perplexity is 39.04837339082759
At time: 131.9635043144226 and batch: 350, loss is 3.6255172443389894 and perplexity is 37.54413763080726
At time: 132.42625451087952 and batch: 400, loss is 3.5730701637268067 and perplexity is 35.62580246707878
At time: 132.8884928226471 and batch: 450, loss is 3.5963681745529175 and perplexity is 36.4655571207821
At time: 133.35201930999756 and batch: 500, loss is 3.477139892578125 and perplexity is 32.36701642110423
At time: 133.82854223251343 and batch: 550, loss is 3.5496470022201536 and perplexity is 34.80103063256367
At time: 134.29186749458313 and batch: 600, loss is 3.5638954401016236 and perplexity is 35.30044041127907
At time: 134.76739406585693 and batch: 650, loss is 3.4183132648468018 and perplexity is 30.517895975828445
At time: 135.23758363723755 and batch: 700, loss is 3.4028069162368775 and perplexity is 30.048324917121576
At time: 135.70024371147156 and batch: 750, loss is 3.5077310609817505 and perplexity is 33.372461736248106
At time: 136.16285872459412 and batch: 800, loss is 3.471000213623047 and perplexity is 32.168902132944744
At time: 136.62598824501038 and batch: 850, loss is 3.5377254724502563 and perplexity is 34.388612322927095
At time: 137.08768916130066 and batch: 900, loss is 3.5062576007843016 and perplexity is 33.32332495162705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313323974609375 and perplexity of 74.6883389308264
finished 15 epochs...
Completing Train Step...
At time: 138.32755541801453 and batch: 50, loss is 3.7460662364959716 and perplexity is 42.354142684707284
At time: 138.802316904068 and batch: 100, loss is 3.6169991540908812 and perplexity is 37.22569148018438
At time: 139.2655267715454 and batch: 150, loss is 3.6296822929382326 and perplexity is 37.70083689214572
At time: 139.72868752479553 and batch: 200, loss is 3.5182752990722657 and perplexity is 33.72621064732726
At time: 140.19147777557373 and batch: 250, loss is 3.6679267406463625 and perplexity is 39.17061078134288
At time: 140.6536250114441 and batch: 300, loss is 3.6613476181030276 and perplexity is 38.91374842389847
At time: 141.11601567268372 and batch: 350, loss is 3.622306089401245 and perplexity is 37.42377094933275
At time: 141.5805060863495 and batch: 400, loss is 3.5700585794448854 and perplexity is 35.51867375487921
At time: 142.04814052581787 and batch: 450, loss is 3.5936095762252807 and perplexity is 36.36510191737463
At time: 142.5420618057251 and batch: 500, loss is 3.474665060043335 and perplexity is 32.287012514780315
At time: 143.01561903953552 and batch: 550, loss is 3.547445287704468 and perplexity is 34.72449298620095
At time: 143.4851303100586 and batch: 600, loss is 3.562190556526184 and perplexity is 35.24030854369787
At time: 143.9594428539276 and batch: 650, loss is 3.417010045051575 and perplexity is 30.478150353949943
At time: 144.42225575447083 and batch: 700, loss is 3.4019379377365113 and perplexity is 30.022224910611236
At time: 144.88574481010437 and batch: 750, loss is 3.5071302127838133 and perplexity is 33.352415975585814
At time: 145.36234092712402 and batch: 800, loss is 3.470769414901733 and perplexity is 32.16147844818782
At time: 145.8439428806305 and batch: 850, loss is 3.53789568901062 and perplexity is 34.39446633244293
At time: 146.31917715072632 and batch: 900, loss is 3.506993794441223 and perplexity is 33.34786640460656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313292202884203 and perplexity of 74.68596599114477
finished 16 epochs...
Completing Train Step...
At time: 147.5817642211914 and batch: 50, loss is 3.7428699731826782 and perplexity is 42.218983809022426
At time: 148.0573925971985 and batch: 100, loss is 3.6138400554656984 and perplexity is 37.108277408372686
At time: 148.51971793174744 and batch: 150, loss is 3.6264567708969118 and perplexity is 37.57942792069206
At time: 148.9830038547516 and batch: 200, loss is 3.5151365756988526 and perplexity is 33.62051935630346
At time: 149.4461169242859 and batch: 250, loss is 3.664839587211609 and perplexity is 39.04987156186816
At time: 149.90890097618103 and batch: 300, loss is 3.658520064353943 and perplexity is 38.803873121018796
At time: 150.3712501525879 and batch: 350, loss is 3.619633665084839 and perplexity is 37.32389227238185
At time: 150.83293890953064 and batch: 400, loss is 3.5676078748703004 and perplexity is 35.43173455328719
At time: 151.29645586013794 and batch: 450, loss is 3.5913133811950684 and perplexity is 36.28169634545506
At time: 151.7591872215271 and batch: 500, loss is 3.4725673007965088 and perplexity is 32.2193531270899
At time: 152.22172021865845 and batch: 550, loss is 3.545566687583923 and perplexity is 34.65932078491062
At time: 152.69464540481567 and batch: 600, loss is 3.56068039894104 and perplexity is 35.18713028832706
At time: 153.16197276115417 and batch: 650, loss is 3.415757489204407 and perplexity is 30.43999866706049
At time: 153.6253538131714 and batch: 700, loss is 3.4009538745880126 and perplexity is 29.992695677138954
At time: 154.1176414489746 and batch: 750, loss is 3.506355710029602 and perplexity is 33.326594438269765
At time: 154.58119106292725 and batch: 800, loss is 3.470224370956421 and perplexity is 32.143953805371424
At time: 155.04296731948853 and batch: 850, loss is 3.5376204586029054 and perplexity is 34.3850012320526
At time: 155.5048496723175 and batch: 900, loss is 3.50712996006012 and perplexity is 33.352407546641125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313358254628639 and perplexity of 74.69089929240813
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 156.75300478935242 and batch: 50, loss is 3.7424594926834107 and perplexity is 42.20165729581111
At time: 157.21589422225952 and batch: 100, loss is 3.614586338996887 and perplexity is 37.13598104076987
At time: 157.691632270813 and batch: 150, loss is 3.629558358192444 and perplexity is 37.69616473803657
At time: 158.15495896339417 and batch: 200, loss is 3.5162171792984007 and perplexity is 33.65686944702082
At time: 158.61768078804016 and batch: 250, loss is 3.665122184753418 and perplexity is 39.06090851901454
At time: 159.08010053634644 and batch: 300, loss is 3.6584062910079957 and perplexity is 38.79945852567451
At time: 159.5432939529419 and batch: 350, loss is 3.6190876579284668 and perplexity is 37.30351872265559
At time: 160.006982088089 and batch: 400, loss is 3.566882219314575 and perplexity is 35.40603264475381
At time: 160.46975541114807 and batch: 450, loss is 3.5902911806106568 and perplexity is 36.24462812305429
At time: 160.93319272994995 and batch: 500, loss is 3.471243462562561 and perplexity is 32.17672813606881
At time: 161.3964822292328 and batch: 550, loss is 3.541775736808777 and perplexity is 34.52817774138433
At time: 161.86026072502136 and batch: 600, loss is 3.5573254299163817 and perplexity is 35.06927636481968
At time: 162.32476377487183 and batch: 650, loss is 3.410917959213257 and perplexity is 30.293039274602823
At time: 162.78951621055603 and batch: 700, loss is 3.3954699563980104 and perplexity is 29.828668355026423
At time: 163.25355696678162 and batch: 750, loss is 3.501676254272461 and perplexity is 33.17100842689747
At time: 163.71837377548218 and batch: 800, loss is 3.4632897996902465 and perplexity is 31.921820357212102
At time: 164.18332862854004 and batch: 850, loss is 3.529580383300781 and perplexity is 34.1096516319224
At time: 164.647038936615 and batch: 900, loss is 3.4996162700653075 and perplexity is 33.10274700626607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31275877234054 and perplexity of 74.64613683968946
finished 18 epochs...
Completing Train Step...
At time: 165.90751361846924 and batch: 50, loss is 3.7395919513702394 and perplexity is 42.080815642052976
At time: 166.37098264694214 and batch: 100, loss is 3.611608080863953 and perplexity is 37.02554503830276
At time: 166.83427333831787 and batch: 150, loss is 3.626031923294067 and perplexity is 37.563465781802
At time: 167.29685497283936 and batch: 200, loss is 3.513783779144287 and perplexity is 33.575068383446784
At time: 167.76145362854004 and batch: 250, loss is 3.66250590801239 and perplexity is 38.95884794014838
At time: 168.2248637676239 and batch: 300, loss is 3.6561897087097166 and perplexity is 38.71355157780709
At time: 168.68776679039001 and batch: 350, loss is 3.616973900794983 and perplexity is 37.22475142065226
At time: 169.15075492858887 and batch: 400, loss is 3.564820818901062 and perplexity is 35.33312180945085
At time: 169.627583026886 and batch: 450, loss is 3.5884324359893798 and perplexity is 36.177321188060475
At time: 170.09025955200195 and batch: 500, loss is 3.4697994422912597 and perplexity is 32.130297819603285
At time: 170.55413937568665 and batch: 550, loss is 3.540634956359863 and perplexity is 34.488811129879956
At time: 171.01787567138672 and batch: 600, loss is 3.556722502708435 and perplexity is 35.04813851686849
At time: 171.48161578178406 and batch: 650, loss is 3.410657858848572 and perplexity is 30.28516106864662
At time: 171.94499707221985 and batch: 700, loss is 3.395501585006714 and perplexity is 29.829611809225966
At time: 172.4080150127411 and batch: 750, loss is 3.50155179977417 and perplexity is 33.16688040256682
At time: 172.86911368370056 and batch: 800, loss is 3.4637414503097532 and perplexity is 31.93624112348109
At time: 173.33150720596313 and batch: 850, loss is 3.5307872438430787 and perplexity is 34.15084207514928
At time: 173.79476404190063 and batch: 900, loss is 3.5012639570236206 and perplexity is 33.15733493034799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312483696088399 and perplexity of 74.62560628399441
finished 19 epochs...
Completing Train Step...
At time: 175.0503056049347 and batch: 50, loss is 3.738214898109436 and perplexity is 42.02290799776175
At time: 175.53553295135498 and batch: 100, loss is 3.6101436376571656 and perplexity is 36.97136291339942
At time: 175.99911999702454 and batch: 150, loss is 3.6243798971176147 and perplexity is 37.50146118376356
At time: 176.46248507499695 and batch: 200, loss is 3.5124866485595705 and perplexity is 33.53154536898173
At time: 176.92578864097595 and batch: 250, loss is 3.6611296701431275 and perplexity is 38.905268175977355
At time: 177.38917994499207 and batch: 300, loss is 3.6549947261810303 and perplexity is 38.66731719019275
At time: 177.85200786590576 and batch: 350, loss is 3.6158523511886598 and perplexity is 37.183025418615365
At time: 178.31552028656006 and batch: 400, loss is 3.563776717185974 and perplexity is 35.29624968884204
At time: 178.77890419960022 and batch: 450, loss is 3.5874701642990114 and perplexity is 36.14252552017589
At time: 179.2421932220459 and batch: 500, loss is 3.469015941619873 and perplexity is 32.105133569080664
At time: 179.70575094223022 and batch: 550, loss is 3.539980788230896 and perplexity is 34.46625702672486
At time: 180.16918659210205 and batch: 600, loss is 3.5563340187072754 and perplexity is 35.03452552017118
At time: 180.63229775428772 and batch: 650, loss is 3.410462188720703 and perplexity is 30.27923574703091
At time: 181.09554195404053 and batch: 700, loss is 3.3954846477508545 and perplexity is 29.829106581737157
At time: 181.57327795028687 and batch: 750, loss is 3.5015294790267943 and perplexity is 33.16614010127018
At time: 182.03673815727234 and batch: 800, loss is 3.463971118927002 and perplexity is 31.94357671816574
At time: 182.5006387233734 and batch: 850, loss is 3.531321506500244 and perplexity is 34.16909246959879
At time: 182.964026927948 and batch: 900, loss is 3.5019799947738646 and perplexity is 33.18108533593434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312385872618793 and perplexity of 74.61830650531695
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe3d26db70>
ELAPSED
1138.8439524173737


RESULTS SO FAR:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.13177326071305, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.42094345278127, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.9680256792332449, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5177066140482257, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.09211446274884, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.11885787751686028, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.207900223189446, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -74.99583362546804, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.4107464825280964, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.25800213299985186, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -74.61830650531695, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.09730286041528366, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.6732898409058357, 'tune_wordvecs': 'TRUE'}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -74.5736499085026, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.026072552246488634, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5477859567509309, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.13177326071305, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.3437062113450484, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.20156453519302608, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.42094345278127, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.9680256792332449, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.5177066140482257, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -75.09211446274884, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.11885787751686028, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.207900223189446, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -74.99583362546804, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.4107464825280964, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.25800213299985186, 'tune_wordvecs': 'TRUE'}}, {'best_accuracy': -74.61830650531695, 'params': {'tie_weights': 'FALSE', 'wordvec_dim': 300, 'rnn_dropout': 0.09730286041528366, 'num_layers': 1, 'data': 'ptb', 'seq_len': 35, 'batch_size': 32, 'wordvec_source': 'glove', 'dropout': 0.6732898409058357, 'tune_wordvecs': 'TRUE'}}]
