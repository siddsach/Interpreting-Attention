TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.192946195602417 and batch: 50, loss is 6.965682744979858 and perplexity is 1059.638133127983
At time: 1.9822218418121338 and batch: 100, loss is 6.0971529674530025 and perplexity is 444.5902037572501
At time: 2.7903428077697754 and batch: 150, loss is 5.925639839172363 and perplexity is 374.51798999463296
At time: 3.5815460681915283 and batch: 200, loss is 5.732341804504395 and perplexity is 308.69131731606143
At time: 4.371119976043701 and batch: 250, loss is 5.771294631958008 and perplexity is 320.95297977789323
At time: 5.158975839614868 and batch: 300, loss is 5.658989963531494 and perplexity is 286.8587583766314
At time: 5.946200370788574 and batch: 350, loss is 5.619349384307862 and perplexity is 275.7099436521893
At time: 6.736873388290405 and batch: 400, loss is 5.468754663467407 and perplexity is 237.16465896579507
At time: 7.531852722167969 and batch: 450, loss is 5.4552281665802 and perplexity is 233.978250996852
At time: 8.330370903015137 and batch: 500, loss is 5.398302612304687 and perplexity is 221.030922447054
At time: 9.130789995193481 and batch: 550, loss is 5.43653790473938 and perplexity is 229.6457501289784
At time: 9.92888617515564 and batch: 600, loss is 5.351407794952393 and perplexity is 210.90499996419834
At time: 10.72682499885559 and batch: 650, loss is 5.236842260360718 and perplexity is 188.0752710001772
At time: 11.525558233261108 and batch: 700, loss is 5.3336998558044435 and perplexity is 207.20317959523678
At time: 12.317892789840698 and batch: 750, loss is 5.305787010192871 and perplexity is 201.49952220500234
At time: 13.108367681503296 and batch: 800, loss is 5.286178569793702 and perplexity is 197.58691624641355
At time: 13.900486707687378 and batch: 850, loss is 5.317534656524658 and perplexity is 203.88062612720975
At time: 14.691455125808716 and batch: 900, loss is 5.215135250091553 and perplexity is 184.0367102783829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.13633163661173 and perplexity of 170.0906680953958
finished 1 epochs...
Completing Train Step...
At time: 16.387998819351196 and batch: 50, loss is 5.07470965385437 and perplexity is 159.9257514122834
At time: 17.023637771606445 and batch: 100, loss is 4.936562757492066 and perplexity is 139.290650038992
At time: 17.64447569847107 and batch: 150, loss is 4.9025305557250975 and perplexity is 134.63003779521753
At time: 18.265596389770508 and batch: 200, loss is 4.780230236053467 and perplexity is 119.13177531734102
At time: 18.896904706954956 and batch: 250, loss is 4.868543691635132 and perplexity is 130.1312675825243
At time: 19.530928134918213 and batch: 300, loss is 4.804932546615601 and perplexity is 122.11125392602467
At time: 20.16395115852356 and batch: 350, loss is 4.7881143856048585 and perplexity is 120.0747404046111
At time: 20.79467797279358 and batch: 400, loss is 4.644406652450561 and perplexity is 104.00163835706034
At time: 21.427361011505127 and batch: 450, loss is 4.658850421905518 and perplexity is 105.51471500450089
At time: 22.060762405395508 and batch: 500, loss is 4.560855159759521 and perplexity is 95.66525393567076
At time: 22.695713996887207 and batch: 550, loss is 4.626414909362793 and perplexity is 102.14719991270739
At time: 23.34875750541687 and batch: 600, loss is 4.575796928405762 and perplexity is 97.1053943566689
At time: 23.989076137542725 and batch: 650, loss is 4.435567960739136 and perplexity is 84.40004719119496
At time: 24.623812675476074 and batch: 700, loss is 4.4919203186035155 and perplexity is 89.29275187755727
At time: 25.256601333618164 and batch: 750, loss is 4.530649242401123 and perplexity is 92.81880342685956
At time: 25.891502380371094 and batch: 800, loss is 4.479379243850708 and perplexity is 88.17991745658507
At time: 26.578414916992188 and batch: 850, loss is 4.533549690246582 and perplexity is 93.0884103266451
At time: 27.236640691757202 and batch: 900, loss is 4.466353721618653 and perplexity is 87.03877609660417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.591616277825342 and perplexity of 98.65375324583657
finished 2 epochs...
Completing Train Step...
At time: 28.84565782546997 and batch: 50, loss is 4.516851234436035 and perplexity is 91.54688399456496
At time: 29.503008604049683 and batch: 100, loss is 4.38820182800293 and perplexity is 80.49554393139933
At time: 30.154752492904663 and batch: 150, loss is 4.379940309524536 and perplexity is 79.83326797713721
At time: 30.787319660186768 and batch: 200, loss is 4.287276039123535 and perplexity is 72.76798115276037
At time: 31.423617839813232 and batch: 250, loss is 4.420608711242676 and perplexity is 83.14688239939757
At time: 32.060853004455566 and batch: 300, loss is 4.381615200042725 and perplexity is 79.96709199971714
At time: 32.70666527748108 and batch: 350, loss is 4.375713844299316 and perplexity is 79.4965674739071
At time: 33.383784770965576 and batch: 400, loss is 4.277683157920837 and perplexity is 72.07326405102422
At time: 34.03458833694458 and batch: 450, loss is 4.309825849533081 and perplexity is 74.42752622320322
At time: 34.685691118240356 and batch: 500, loss is 4.191848669052124 and perplexity is 66.14495813934175
At time: 35.33005213737488 and batch: 550, loss is 4.2708303737640385 and perplexity is 71.58104997364515
At time: 35.96301341056824 and batch: 600, loss is 4.263169350624085 and perplexity is 71.03476113101028
At time: 36.59631705284119 and batch: 650, loss is 4.11287368774414 and perplexity is 61.122111297649646
At time: 37.228976249694824 and batch: 700, loss is 4.147586541175842 and perplexity is 63.281089546476295
At time: 37.86161661148071 and batch: 750, loss is 4.2233385181427 and perplexity is 68.26099487382412
At time: 38.50713539123535 and batch: 800, loss is 4.174364624023437 and perplexity is 64.99852803497403
At time: 39.1410493850708 and batch: 850, loss is 4.244952592849732 and perplexity is 69.75245330048456
At time: 39.77425026893616 and batch: 900, loss is 4.192333316802978 and perplexity is 66.17702291396715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42421983692744 and perplexity of 83.44767902237233
finished 3 epochs...
Completing Train Step...
At time: 41.43155837059021 and batch: 50, loss is 4.265932025909424 and perplexity is 71.23127844175454
At time: 42.070048332214355 and batch: 100, loss is 4.138016691207886 and perplexity is 62.678387497515665
At time: 42.70232105255127 and batch: 150, loss is 4.135814771652222 and perplexity is 62.540526565409564
At time: 43.334874629974365 and batch: 200, loss is 4.045359663963318 and perplexity is 57.13173056400577
At time: 43.98388600349426 and batch: 250, loss is 4.1897550344467165 and perplexity is 66.0066196316831
At time: 44.63276672363281 and batch: 300, loss is 4.157009749412537 and perplexity is 63.880218857453436
At time: 45.30215787887573 and batch: 350, loss is 4.152828669548034 and perplexity is 63.61368814203031
At time: 45.94647526741028 and batch: 400, loss is 4.069817905426025 and perplexity is 58.546300657505604
At time: 46.579726457595825 and batch: 450, loss is 4.110358948707581 and perplexity is 60.968598241909696
At time: 47.220601081848145 and batch: 500, loss is 3.9836981439590455 and perplexity is 53.715314332534305
At time: 47.85433006286621 and batch: 550, loss is 4.068609952926636 and perplexity is 58.475622203997844
At time: 48.48770308494568 and batch: 600, loss is 4.0747019481658935 and perplexity is 58.83294270839846
At time: 49.1301372051239 and batch: 650, loss is 3.9198968744277956 and perplexity is 50.39524747135061
At time: 49.80061411857605 and batch: 700, loss is 3.9453235244750977 and perplexity is 51.6930593769931
At time: 50.44032955169678 and batch: 750, loss is 4.032289891242981 and perplexity is 56.38989023268002
At time: 51.07278776168823 and batch: 800, loss is 3.993269624710083 and perplexity is 54.23191781588176
At time: 51.704184770584106 and batch: 850, loss is 4.065429458618164 and perplexity is 58.289936263458365
At time: 52.340659856796265 and batch: 900, loss is 4.020189986228943 and perplexity is 55.7116892751625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355813222388699 and perplexity of 77.93017413736912
finished 4 epochs...
Completing Train Step...
At time: 53.975724935531616 and batch: 50, loss is 4.09990177154541 and perplexity is 60.334360755258786
At time: 54.60948991775513 and batch: 100, loss is 3.977617974281311 and perplexity is 53.38970698416253
At time: 55.256829261779785 and batch: 150, loss is 3.9742580032348633 and perplexity is 53.21062014629439
At time: 55.87721633911133 and batch: 200, loss is 3.886028518676758 and perplexity is 48.71702306875688
At time: 56.50382852554321 and batch: 250, loss is 4.031064896583557 and perplexity is 56.32085521069901
At time: 57.12782907485962 and batch: 300, loss is 4.007573738098144 and perplexity is 55.01323199904587
At time: 57.7474570274353 and batch: 350, loss is 4.002667541503906 and perplexity is 54.74398729138496
At time: 58.36120891571045 and batch: 400, loss is 3.928486747741699 and perplexity is 50.830000827659205
At time: 58.98395037651062 and batch: 450, loss is 3.967988452911377 and perplexity is 52.878057085311106
At time: 59.615708351135254 and batch: 500, loss is 3.8414609384536744 and perplexity is 46.59349496927908
At time: 60.245798110961914 and batch: 550, loss is 3.9255751085281374 and perplexity is 50.68221745436096
At time: 60.87285041809082 and batch: 600, loss is 3.9406498718261718 and perplexity is 51.45202766110186
At time: 61.49895977973938 and batch: 650, loss is 3.7837103414535522 and perplexity is 43.97891618461335
At time: 62.12638258934021 and batch: 700, loss is 3.8072059392929076 and perplexity is 45.02446187481151
At time: 62.75308275222778 and batch: 750, loss is 3.8985401010513305 and perplexity is 49.33037914225138
At time: 63.379904985427856 and batch: 800, loss is 3.860406813621521 and perplexity is 47.48466484856761
At time: 64.0139422416687 and batch: 850, loss is 3.9386685705184936 and perplexity is 51.35018661362971
At time: 64.64506793022156 and batch: 900, loss is 3.895460801124573 and perplexity is 49.17870974699576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32558702442744 and perplexity of 75.60988468404548
finished 5 epochs...
Completing Train Step...
At time: 66.25310778617859 and batch: 50, loss is 3.9783748960494996 and perplexity is 53.430134113733075
At time: 66.89899063110352 and batch: 100, loss is 3.8552082490921022 and perplexity is 47.23845328204296
At time: 67.53327250480652 and batch: 150, loss is 3.856133885383606 and perplexity is 47.28219915201427
At time: 68.16643261909485 and batch: 200, loss is 3.770516333580017 and perplexity is 43.40246920359564
At time: 68.798588514328 and batch: 250, loss is 3.9168405818939207 and perplexity is 50.24145998224827
At time: 69.43154859542847 and batch: 300, loss is 3.8910748672485354 and perplexity is 48.9634874982211
At time: 70.06176447868347 and batch: 350, loss is 3.8911303663253785 and perplexity is 48.966205001985045
At time: 70.70441198348999 and batch: 400, loss is 3.817558751106262 and perplexity is 45.49301288083238
At time: 71.33656764030457 and batch: 450, loss is 3.8585358810424806 and perplexity is 47.39590729768489
At time: 71.97110486030579 and batch: 500, loss is 3.736071434020996 and perplexity is 41.932929870468946
At time: 72.60538959503174 and batch: 550, loss is 3.8169941186904905 and perplexity is 45.46733330151245
At time: 73.28505325317383 and batch: 600, loss is 3.8299660301208496 and perplexity is 46.06097352193399
At time: 73.97172331809998 and batch: 650, loss is 3.67761935710907 and perplexity is 39.552122424460556
At time: 74.64376068115234 and batch: 700, loss is 3.6994331312179565 and perplexity is 40.42438254335283
At time: 75.2857940196991 and batch: 750, loss is 3.7929550981521607 and perplexity is 44.387375709978656
At time: 75.9491319656372 and batch: 800, loss is 3.759969515800476 and perplexity is 42.94711675033174
At time: 76.58662223815918 and batch: 850, loss is 3.8354549837112426 and perplexity is 46.31449521580906
At time: 77.22480630874634 and batch: 900, loss is 3.7986441087722778 and perplexity is 44.640615621058195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315910025818707 and perplexity of 74.88173676060568
finished 6 epochs...
Completing Train Step...
At time: 78.85046863555908 and batch: 50, loss is 3.8773321104049683 and perplexity is 48.29519679159722
At time: 79.47885870933533 and batch: 100, loss is 3.7599232006073 and perplexity is 42.94512769238524
At time: 80.10800766944885 and batch: 150, loss is 3.763316297531128 and perplexity is 43.09109216881764
At time: 80.73990249633789 and batch: 200, loss is 3.6736485147476197 and perplexity is 39.39537858973763
At time: 81.37615180015564 and batch: 250, loss is 3.8223992443084716 and perplexity is 45.71375532056322
At time: 82.00234508514404 and batch: 300, loss is 3.8011431217193605 and perplexity is 44.752312605436444
At time: 82.63178730010986 and batch: 350, loss is 3.7982823944091795 and perplexity is 44.6244713891868
At time: 83.26049327850342 and batch: 400, loss is 3.727203035354614 and perplexity is 41.562696048220495
At time: 83.92363476753235 and batch: 450, loss is 3.768308310508728 and perplexity is 43.30674127387426
At time: 84.57220339775085 and batch: 500, loss is 3.6493185806274413 and perplexity is 38.44845759672127
At time: 85.20497608184814 and batch: 550, loss is 3.7259973096847534 and perplexity is 41.512613037939545
At time: 85.8340060710907 and batch: 600, loss is 3.74619589805603 and perplexity is 42.35963474496958
At time: 86.46539211273193 and batch: 650, loss is 3.5930037975311278 and perplexity is 36.34307938448646
At time: 87.15497326850891 and batch: 700, loss is 3.613589162826538 and perplexity is 37.098968382550865
At time: 87.79864621162415 and batch: 750, loss is 3.7078558015823364 and perplexity is 40.7663017070537
At time: 88.44221377372742 and batch: 800, loss is 3.6748453617095946 and perplexity is 39.44255705598891
At time: 89.09026169776917 and batch: 850, loss is 3.7508505392074585 and perplexity is 42.557263232044406
At time: 89.73745656013489 and batch: 900, loss is 3.7170318078994753 and perplexity is 41.14209505331914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31687989953446 and perplexity of 74.95439781920946
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 91.4220380783081 and batch: 50, loss is 3.8210129594802855 and perplexity is 45.65042694085088
At time: 92.05275344848633 and batch: 100, loss is 3.6956303787231444 and perplexity is 40.27095053865155
At time: 92.68084025382996 and batch: 150, loss is 3.7032966899871824 and perplexity is 40.58086661907601
At time: 93.32817840576172 and batch: 200, loss is 3.593910603523254 and perplexity is 36.376050453566805
At time: 93.96277523040771 and batch: 250, loss is 3.7392609786987303 and perplexity is 42.06689034665387
At time: 94.59428668022156 and batch: 300, loss is 3.7041714906692507 and perplexity is 40.616382321188574
At time: 95.22806477546692 and batch: 350, loss is 3.692334008216858 and perplexity is 40.13842111807843
At time: 95.85882043838501 and batch: 400, loss is 3.6208841514587404 and perplexity is 37.370594485244084
At time: 96.48718070983887 and batch: 450, loss is 3.6458094835281374 and perplexity is 38.31377467163122
At time: 97.1146879196167 and batch: 500, loss is 3.5155422687530518 and perplexity is 33.63416173460683
At time: 97.73942852020264 and batch: 550, loss is 3.5740472316741942 and perplexity is 35.660628307605
At time: 98.36249256134033 and batch: 600, loss is 3.590563941001892 and perplexity is 36.25451557039201
At time: 98.98669075965881 and batch: 650, loss is 3.425715937614441 and perplexity is 30.744648224148428
At time: 99.60977339744568 and batch: 700, loss is 3.4262971591949465 and perplexity is 30.76252287124347
At time: 100.23747706413269 and batch: 750, loss is 3.514468331336975 and perplexity is 33.59806013875873
At time: 100.86122703552246 and batch: 800, loss is 3.463740267753601 and perplexity is 31.936203357105004
At time: 101.48573064804077 and batch: 850, loss is 3.51912784576416 and perplexity is 33.754976076838346
At time: 102.11248207092285 and batch: 900, loss is 3.4796049547195436 and perplexity is 32.446901548363975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271242742669092 and perplexity of 71.61057385979332
finished 8 epochs...
Completing Train Step...
At time: 103.68573427200317 and batch: 50, loss is 3.7303473806381224 and perplexity is 41.69358919428883
At time: 104.3560585975647 and batch: 100, loss is 3.6035147428512575 and perplexity is 36.72709414826553
At time: 104.99173188209534 and batch: 150, loss is 3.609764742851257 and perplexity is 36.95735730951701
At time: 105.62145042419434 and batch: 200, loss is 3.5056891536712644 and perplexity is 33.30438778665981
At time: 106.24947237968445 and batch: 250, loss is 3.65147274017334 and perplexity is 38.53137098094476
At time: 106.8750147819519 and batch: 300, loss is 3.6232302951812745 and perplexity is 37.458374202554886
At time: 107.5001916885376 and batch: 350, loss is 3.61491379737854 and perplexity is 37.14814352026682
At time: 108.13171362876892 and batch: 400, loss is 3.5458091163635252 and perplexity is 34.667724220326484
At time: 108.75867319107056 and batch: 450, loss is 3.577340350151062 and perplexity is 35.778256557159615
At time: 109.40714645385742 and batch: 500, loss is 3.4501131105422975 and perplexity is 31.503955536720085
At time: 110.04286575317383 and batch: 550, loss is 3.512190833091736 and perplexity is 33.521627686175506
At time: 110.67790293693542 and batch: 600, loss is 3.534473204612732 and perplexity is 34.27695301649832
At time: 111.31581497192383 and batch: 650, loss is 3.373700919151306 and perplexity is 29.18634372279527
At time: 111.9534523487091 and batch: 700, loss is 3.3796509599685667 and perplexity is 29.360521327313702
At time: 112.58960914611816 and batch: 750, loss is 3.473501691818237 and perplexity is 32.249472670901994
At time: 113.23367929458618 and batch: 800, loss is 3.427285943031311 and perplexity is 30.79295539974102
At time: 113.87104201316833 and batch: 850, loss is 3.4885664987564087 and perplexity is 32.738982679677136
At time: 114.50938010215759 and batch: 900, loss is 3.4553252124786376 and perplexity is 31.6685860271284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2769537102686215 and perplexity of 72.02070954798445
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 116.27812695503235 and batch: 50, loss is 3.6990026330947874 and perplexity is 40.40698366789797
At time: 116.91523337364197 and batch: 100, loss is 3.5855133390426634 and perplexity is 36.07187006614423
At time: 117.54347705841064 and batch: 150, loss is 3.5958779096603393 and perplexity is 36.44768372004593
At time: 118.17204689979553 and batch: 200, loss is 3.4864195919036867 and perplexity is 32.66877052985233
At time: 118.80300259590149 and batch: 250, loss is 3.6302399158477785 and perplexity is 37.72186560500696
At time: 119.46059799194336 and batch: 300, loss is 3.6015402460098267 and perplexity is 36.6546481625965
At time: 120.09474539756775 and batch: 350, loss is 3.583949475288391 and perplexity is 36.015502662956344
At time: 120.73178315162659 and batch: 400, loss is 3.5159886837005616 and perplexity is 33.649179879064796
At time: 121.36722183227539 and batch: 450, loss is 3.542931761741638 and perplexity is 34.568116256251315
At time: 122.00741028785706 and batch: 500, loss is 3.411528482437134 and perplexity is 30.311539525443536
At time: 122.6413733959198 and batch: 550, loss is 3.4666449308395384 and perplexity is 32.029102122592704
At time: 123.31969118118286 and batch: 600, loss is 3.49146586894989 and perplexity is 32.834042850944684
At time: 123.98912715911865 and batch: 650, loss is 3.323043851852417 and perplexity is 27.74467283495741
At time: 124.64286637306213 and batch: 700, loss is 3.3200949525833128 and perplexity is 27.66297710495433
At time: 125.2840940952301 and batch: 750, loss is 3.4148095989227296 and perplexity is 30.41115855893821
At time: 125.92112064361572 and batch: 800, loss is 3.361541495323181 and perplexity is 28.83360350447377
At time: 126.55800414085388 and batch: 850, loss is 3.4173775625228884 and perplexity is 30.489353665278564
At time: 127.19732332229614 and batch: 900, loss is 3.3846689891815185 and perplexity is 29.508223558160278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2642537992294525 and perplexity of 71.11183646321612
finished 10 epochs...
Completing Train Step...
At time: 128.83687591552734 and batch: 50, loss is 3.6722242069244384 and perplexity is 39.33930738462221
At time: 129.47217869758606 and batch: 100, loss is 3.5522653341293333 and perplexity is 34.89227067777624
At time: 130.10806441307068 and batch: 150, loss is 3.5614884424209596 and perplexity is 35.21557451006991
At time: 130.74366068840027 and batch: 200, loss is 3.454403142929077 and perplexity is 31.63939884664827
At time: 131.38447904586792 and batch: 250, loss is 3.597991623878479 and perplexity is 36.52480518499822
At time: 132.0256905555725 and batch: 300, loss is 3.570549793243408 and perplexity is 35.53612530340265
At time: 132.66794061660767 and batch: 350, loss is 3.5550162601470947 and perplexity is 34.98838887941806
At time: 133.30983090400696 and batch: 400, loss is 3.4898746109008787 and perplexity is 32.781836963511715
At time: 133.94900345802307 and batch: 450, loss is 3.517641429901123 and perplexity is 33.70483941613631
At time: 134.59263586997986 and batch: 500, loss is 3.38874165058136 and perplexity is 29.62864561390351
At time: 135.25703835487366 and batch: 550, loss is 3.4458969974517824 and perplexity is 31.371410905294937
At time: 135.89600014686584 and batch: 600, loss is 3.4736619901657106 and perplexity is 32.25464262243431
At time: 136.5375783443451 and batch: 650, loss is 3.3085213184356688 and perplexity is 27.34466151575687
At time: 137.18000769615173 and batch: 700, loss is 3.3084010314941406 and perplexity is 27.34137250787233
At time: 137.8378713130951 and batch: 750, loss is 3.406164565086365 and perplexity is 30.1493862099299
At time: 138.51223278045654 and batch: 800, loss is 3.3552556610107422 and perplexity is 28.65292869108202
At time: 139.16137433052063 and batch: 850, loss is 3.4150108766555785 and perplexity is 30.417280264047058
At time: 139.80335974693298 and batch: 900, loss is 3.3846077489852906 and perplexity is 29.506416524091367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265780514233733 and perplexity of 71.22048688893999
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 141.4463667869568 and batch: 50, loss is 3.6623235034942625 and perplexity is 38.95174231833151
At time: 142.0987319946289 and batch: 100, loss is 3.550309963226318 and perplexity is 34.82411000836519
At time: 142.73785543441772 and batch: 150, loss is 3.5599486684799193 and perplexity is 35.16139221107166
At time: 143.37725639343262 and batch: 200, loss is 3.4523164892196654 and perplexity is 31.573447210724353
At time: 144.02167415618896 and batch: 250, loss is 3.595003967285156 and perplexity is 36.41584445963164
At time: 144.66084623336792 and batch: 300, loss is 3.566757287979126 and perplexity is 35.40160959810685
At time: 145.30052590370178 and batch: 350, loss is 3.5507052421569822 and perplexity is 34.83787796624319
At time: 145.94300484657288 and batch: 400, loss is 3.484460544586182 and perplexity is 32.6048335108509
At time: 146.5793900489807 and batch: 450, loss is 3.5081935596466063 and perplexity is 33.38790002506286
At time: 147.21812343597412 and batch: 500, loss is 3.3771392345428466 and perplexity is 29.286868296200588
At time: 147.85757184028625 and batch: 550, loss is 3.43270929813385 and perplexity is 30.96041020452302
At time: 148.49632167816162 and batch: 600, loss is 3.464883394241333 and perplexity is 31.972731351161162
At time: 149.13285493850708 and batch: 650, loss is 3.2950382137298586 and perplexity is 26.978444997211028
At time: 149.77399039268494 and batch: 700, loss is 3.2908426189422606 and perplexity is 26.86549149326291
At time: 150.4093508720398 and batch: 750, loss is 3.3866928386688233 and perplexity is 29.56800423442027
At time: 151.04221272468567 and batch: 800, loss is 3.332201223373413 and perplexity is 27.9999079697011
At time: 151.68841218948364 and batch: 850, loss is 3.3899101305007933 and perplexity is 29.663286325890166
At time: 152.32417798042297 and batch: 900, loss is 3.3607940006256105 and perplexity is 28.812058592099387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260597542540668 and perplexity of 70.8523080758161
finished 12 epochs...
Completing Train Step...
At time: 153.96816205978394 and batch: 50, loss is 3.6532175397872924 and perplexity is 38.598659187305
At time: 154.64284086227417 and batch: 100, loss is 3.5364581871032716 and perplexity is 34.345059741030376
At time: 155.3109118938446 and batch: 150, loss is 3.5470599222183226 and perplexity is 34.711113943156455
At time: 155.95831727981567 and batch: 200, loss is 3.4404586601257323 and perplexity is 31.201265663352064
At time: 156.60210180282593 and batch: 250, loss is 3.582899045944214 and perplexity is 35.97769078494382
At time: 157.24665021896362 and batch: 300, loss is 3.555473132133484 and perplexity is 35.00437774630052
At time: 157.88985633850098 and batch: 350, loss is 3.5398288345336915 and perplexity is 34.46102014943236
At time: 158.53666639328003 and batch: 400, loss is 3.474909863471985 and perplexity is 32.29491745368338
At time: 159.22116708755493 and batch: 450, loss is 3.4996525287628173 and perplexity is 33.10394729051676
At time: 159.8966567516327 and batch: 500, loss is 3.369692096710205 and perplexity is 29.0695750618988
At time: 160.5513424873352 and batch: 550, loss is 3.4263720417022707 and perplexity is 30.76482653233857
At time: 161.2315707206726 and batch: 600, loss is 3.45957190990448 and perplexity is 31.803358897143934
At time: 161.88835263252258 and batch: 650, loss is 3.2909774112701418 and perplexity is 26.869112999471103
At time: 162.53331184387207 and batch: 700, loss is 3.2880530166625976 and perplexity is 26.790651891843137
At time: 163.18513083457947 and batch: 750, loss is 3.385281310081482 and perplexity is 29.5262975931557
At time: 163.8303680419922 and batch: 800, loss is 3.3322300910949707 and perplexity is 28.000716274914915
At time: 164.5158941745758 and batch: 850, loss is 3.391998643875122 and perplexity is 29.725303235137968
At time: 165.1949486732483 and batch: 900, loss is 3.3637895488739016 and perplexity is 28.898495902676036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260567861060574 and perplexity of 70.8502051056541
finished 13 epochs...
Completing Train Step...
At time: 166.8608157634735 and batch: 50, loss is 3.647556300163269 and perplexity is 38.380760299353575
At time: 167.5074746608734 and batch: 100, loss is 3.5300416707992555 and perplexity is 34.125389617386425
At time: 168.16062664985657 and batch: 150, loss is 3.540509653091431 and perplexity is 34.48448983986257
At time: 168.80506539344788 and batch: 200, loss is 3.434076523780823 and perplexity is 31.002769021827675
At time: 169.44734740257263 and batch: 250, loss is 3.5762009239196777 and perplexity is 35.7375130896211
At time: 170.08074808120728 and batch: 300, loss is 3.549094738960266 and perplexity is 34.78181660802735
At time: 170.718341588974 and batch: 350, loss is 3.533669800758362 and perplexity is 34.249425839529486
At time: 171.35579657554626 and batch: 400, loss is 3.4693208837509157 and perplexity is 32.1149252698038
At time: 171.9994626045227 and batch: 450, loss is 3.4942430067062378 and perplexity is 32.92535424451237
At time: 172.64437890052795 and batch: 500, loss is 3.3649576807022097 and perplexity is 28.932272879670315
At time: 173.29079484939575 and batch: 550, loss is 3.4222111177444456 and perplexity is 30.637082379720272
At time: 173.93361377716064 and batch: 600, loss is 3.4560017490386965 and perplexity is 31.690018232398874
At time: 174.5749912261963 and batch: 650, loss is 3.2880784463882446 and perplexity is 26.791333179433092
At time: 175.21666979789734 and batch: 700, loss is 3.2858199214935304 and perplexity is 26.730892565492343
At time: 175.91033720970154 and batch: 750, loss is 3.3837382555007935 and perplexity is 29.480772037645394
At time: 176.5806028842926 and batch: 800, loss is 3.3313009548187256 and perplexity is 27.974711876348856
At time: 177.261785030365 and batch: 850, loss is 3.3919652986526487 and perplexity is 29.724312054814156
At time: 177.91709566116333 and batch: 900, loss is 3.3641250801086424 and perplexity is 28.908193877584175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261003886183647 and perplexity of 70.8811043109804
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 179.5438449382782 and batch: 50, loss is 3.644744381904602 and perplexity is 38.27298833267842
At time: 180.1979682445526 and batch: 100, loss is 3.5290846490859984 and perplexity is 34.092746501126214
At time: 180.83902883529663 and batch: 150, loss is 3.5406736278533937 and perplexity is 34.49014488950551
At time: 181.47551226615906 and batch: 200, loss is 3.433608636856079 and perplexity is 30.9882666245771
At time: 182.10844349861145 and batch: 250, loss is 3.576139030456543 and perplexity is 35.7353012396224
At time: 182.74136877059937 and batch: 300, loss is 3.5475852060317994 and perplexity is 34.72935191909596
At time: 183.384850025177 and batch: 350, loss is 3.532966179847717 and perplexity is 34.22533570348683
At time: 184.04310584068298 and batch: 400, loss is 3.4689974546432496 and perplexity is 32.10454004771311
At time: 184.68711495399475 and batch: 450, loss is 3.4917637968063353 and perplexity is 32.843826484285785
At time: 185.3314151763916 and batch: 500, loss is 3.3611334323883058 and perplexity is 28.82183997989712
At time: 185.97740244865417 and batch: 550, loss is 3.4172634887695312 and perplexity is 30.485875828637244
At time: 186.63519859313965 and batch: 600, loss is 3.4512637424468995 and perplexity is 31.54022585597337
At time: 187.29507756233215 and batch: 650, loss is 3.2840629196166993 and perplexity is 26.68396757280718
At time: 187.92865777015686 and batch: 700, loss is 3.279242043495178 and perplexity is 26.555637052022046
At time: 188.55939316749573 and batch: 750, loss is 3.375862507820129 and perplexity is 29.249500827920624
At time: 189.1911165714264 and batch: 800, loss is 3.3221854162216187 and perplexity is 27.720866038993957
At time: 189.82375645637512 and batch: 850, loss is 3.3820414972305297 and perplexity is 29.430792707288646
At time: 190.457612991333 and batch: 900, loss is 3.353677282333374 and perplexity is 28.607739191844516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260495120531892 and perplexity of 70.84505161171369
finished 15 epochs...
Completing Train Step...
At time: 192.05399990081787 and batch: 50, loss is 3.642261610031128 and perplexity is 38.17808309650864
At time: 192.69651651382446 and batch: 100, loss is 3.5258349800109863 and perplexity is 33.98213617795286
At time: 193.33151602745056 and batch: 150, loss is 3.5376856184005736 and perplexity is 34.38724182477319
At time: 194.00069189071655 and batch: 200, loss is 3.430735192298889 and perplexity is 30.899351366219307
At time: 194.64436101913452 and batch: 250, loss is 3.5730113124847414 and perplexity is 35.62370590604723
At time: 195.302725315094 and batch: 300, loss is 3.5444970417022703 and perplexity is 34.62226740570225
At time: 195.9698576927185 and batch: 350, loss is 3.5298663568496704 and perplexity is 34.11940748494243
At time: 196.6240439414978 and batch: 400, loss is 3.4660638427734374 and perplexity is 32.01049580006088
At time: 197.31176948547363 and batch: 450, loss is 3.4895759344100954 and perplexity is 32.77204726153571
At time: 198.01343202590942 and batch: 500, loss is 3.359147071838379 and perplexity is 28.764646236487987
At time: 198.67153453826904 and batch: 550, loss is 3.4156230688095093 and perplexity is 30.43590718541443
At time: 199.35152983665466 and batch: 600, loss is 3.4502793550491333 and perplexity is 31.509193331636915
At time: 200.0305142402649 and batch: 650, loss is 3.283146448135376 and perplexity is 26.659523680294242
At time: 200.72414422035217 and batch: 700, loss is 3.278864846229553 and perplexity is 26.545622227240095
At time: 201.37290334701538 and batch: 750, loss is 3.3756471252441407 and perplexity is 29.243201673474335
At time: 202.02005887031555 and batch: 800, loss is 3.322653636932373 and perplexity is 27.733848561699283
At time: 202.6731140613556 and batch: 850, loss is 3.383218083381653 and perplexity is 29.465440949728606
At time: 203.3264923095703 and batch: 900, loss is 3.355096960067749 and perplexity is 28.648381805086316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260322884337543 and perplexity of 70.83285058039542
finished 16 epochs...
Completing Train Step...
At time: 205.02657437324524 and batch: 50, loss is 3.6405512952804564 and perplexity is 38.112842364832574
At time: 205.6776053905487 and batch: 100, loss is 3.523753185272217 and perplexity is 33.91146593164189
At time: 206.34944987297058 and batch: 150, loss is 3.5356173086166383 and perplexity is 34.31619185806386
At time: 207.0124671459198 and batch: 200, loss is 3.428739657402039 and perplexity is 30.837752114447742
At time: 207.69483470916748 and batch: 250, loss is 3.5709013509750367 and perplexity is 35.54862049926785
At time: 208.3884139060974 and batch: 300, loss is 3.5425187730789185 and perplexity is 34.553842963702785
At time: 209.06702518463135 and batch: 350, loss is 3.527896900177002 and perplexity is 34.05227691727454
At time: 209.71686792373657 and batch: 400, loss is 3.4642657232284546 and perplexity is 31.952988819625407
At time: 210.37209486961365 and batch: 450, loss is 3.4880517864227296 and perplexity is 32.722135857505315
At time: 211.02418041229248 and batch: 500, loss is 3.357826728820801 and perplexity is 28.726692098430927
At time: 211.6780686378479 and batch: 550, loss is 3.414487810134888 and perplexity is 30.401374163427484
At time: 212.32884311676025 and batch: 600, loss is 3.449446859359741 and perplexity is 31.48297297969142
At time: 213.00177145004272 and batch: 650, loss is 3.282477412223816 and perplexity is 26.641693466758714
At time: 213.69444251060486 and batch: 700, loss is 3.2784859561920165 and perplexity is 26.535566260611073
At time: 214.36449909210205 and batch: 750, loss is 3.375442318916321 and perplexity is 29.23721309399632
At time: 215.05349564552307 and batch: 800, loss is 3.322788462638855 and perplexity is 27.737588049508954
At time: 215.69893288612366 and batch: 850, loss is 3.383777265548706 and perplexity is 29.48192210640705
At time: 216.3552074432373 and batch: 900, loss is 3.3557391691207887 and perplexity is 28.666785964262765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260324556533605 and perplexity of 70.83296902690822
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 218.05707001686096 and batch: 50, loss is 3.6396242904663088 and perplexity is 38.077527947325954
At time: 218.71930360794067 and batch: 100, loss is 3.5231360578536988 and perplexity is 33.8905446924125
At time: 219.369802236557 and batch: 150, loss is 3.535531177520752 and perplexity is 34.31323629413729
At time: 220.02029061317444 and batch: 200, loss is 3.4284837198257447 and perplexity is 30.829860584825774
At time: 220.6845326423645 and batch: 250, loss is 3.5707101726531985 and perplexity is 35.54182502325168
At time: 221.36477088928223 and batch: 300, loss is 3.5416755294799804 and perplexity is 34.52471793827636
At time: 222.06055092811584 and batch: 350, loss is 3.527074861526489 and perplexity is 34.024296131729834
At time: 222.73453664779663 and batch: 400, loss is 3.4635706520080567 and perplexity is 31.930786933531078
At time: 223.38426208496094 and batch: 450, loss is 3.487403030395508 and perplexity is 32.70091405927327
At time: 224.04262781143188 and batch: 500, loss is 3.356748733520508 and perplexity is 28.695741544628866
At time: 224.71287274360657 and batch: 550, loss is 3.4127746486663817 and perplexity is 30.34933628798987
At time: 225.371723651886 and batch: 600, loss is 3.4474199295043944 and perplexity is 31.419223831175817
At time: 226.02539443969727 and batch: 650, loss is 3.280704588890076 and perplexity is 26.594504292324505
At time: 226.67653465270996 and batch: 700, loss is 3.2762929582595826 and perplexity is 26.477437580000014
At time: 227.35369968414307 and batch: 750, loss is 3.3731693029403687 and perplexity is 29.170831912864376
At time: 228.03491854667664 and batch: 800, loss is 3.319964346885681 and perplexity is 27.659364398456134
At time: 228.7120177745819 and batch: 850, loss is 3.380724730491638 and perplexity is 29.392064721809835
At time: 229.36079263687134 and batch: 900, loss is 3.352395839691162 and perplexity is 28.571103493233302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260115532025899 and perplexity of 70.8181647477105
finished 18 epochs...
Completing Train Step...
At time: 230.9952220916748 and batch: 50, loss is 3.6390916776657103 and perplexity is 38.057252768415275
At time: 231.67271947860718 and batch: 100, loss is 3.5224593925476073 and perplexity is 33.867619893697295
At time: 232.3563950061798 and batch: 150, loss is 3.534837551116943 and perplexity is 34.289443979886144
At time: 233.02772068977356 and batch: 200, loss is 3.4278568744659426 and perplexity is 30.810541085576666
At time: 233.68671679496765 and batch: 250, loss is 3.570101065635681 and perplexity is 35.52018284008659
At time: 234.34981966018677 and batch: 300, loss is 3.5410566234588625 and perplexity is 34.50335699335753
At time: 235.00095462799072 and batch: 350, loss is 3.5264876556396483 and perplexity is 34.004322729569246
At time: 235.66718912124634 and batch: 400, loss is 3.463017683029175 and perplexity is 31.913135079799105
At time: 236.30491542816162 and batch: 450, loss is 3.4869182920455932 and perplexity is 32.685066513418406
At time: 236.94391322135925 and batch: 500, loss is 3.3563208627700805 and perplexity is 28.683466102498688
At time: 237.61407351493835 and batch: 550, loss is 3.412483253479004 and perplexity is 30.3404939258289
At time: 238.28738474845886 and batch: 600, loss is 3.447263069152832 and perplexity is 31.4142957871973
At time: 238.95750093460083 and batch: 650, loss is 3.280625057220459 and perplexity is 26.592389271102206
At time: 239.63510465621948 and batch: 700, loss is 3.2762966632843016 and perplexity is 26.477535679742477
At time: 240.30039954185486 and batch: 750, loss is 3.3731361532211306 and perplexity is 29.169864924004315
At time: 240.9690456390381 and batch: 800, loss is 3.3200838804244994 and perplexity is 27.662670817774206
At time: 241.60780572891235 and batch: 850, loss is 3.3810060691833494 and perplexity is 29.400335010166867
At time: 242.22467017173767 and batch: 900, loss is 3.3526828336715697 and perplexity is 28.57930440469929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260012691968108 and perplexity of 70.81088217803159
finished 19 epochs...
Completing Train Step...
At time: 243.8145468235016 and batch: 50, loss is 3.6386150312423706 and perplexity is 38.03911723746241
At time: 244.4524965286255 and batch: 100, loss is 3.5218835496902465 and perplexity is 33.848123080774684
At time: 245.13666129112244 and batch: 150, loss is 3.5342567682266237 and perplexity is 34.269535039449664
At time: 245.79672694206238 and batch: 200, loss is 3.4273177766799927 and perplexity is 30.79393566746775
At time: 246.43287444114685 and batch: 250, loss is 3.5695557975769043 and perplexity is 35.50082009836402
At time: 247.06831240653992 and batch: 300, loss is 3.540528402328491 and perplexity is 34.48513640379875
At time: 247.70435786247253 and batch: 350, loss is 3.5259787797927857 and perplexity is 33.987023153084884
At time: 248.3390407562256 and batch: 400, loss is 3.4625380516052244 and perplexity is 31.89783220754063
At time: 248.9741246700287 and batch: 450, loss is 3.486507411003113 and perplexity is 32.67163959783726
At time: 249.6087772846222 and batch: 500, loss is 3.355960569381714 and perplexity is 28.67313350080233
At time: 250.27231311798096 and batch: 550, loss is 3.4122129917144775 and perplexity is 30.332295158360484
At time: 250.911523103714 and batch: 600, loss is 3.447101602554321 and perplexity is 31.409223837198205
At time: 251.5746295452118 and batch: 650, loss is 3.280519814491272 and perplexity is 26.589590762743626
At time: 252.25367784500122 and batch: 700, loss is 3.2762648487091064 and perplexity is 26.476693321592315
At time: 252.90305948257446 and batch: 750, loss is 3.3730979585647582 and perplexity is 29.16875081231381
At time: 253.55320000648499 and batch: 800, loss is 3.320169711112976 and perplexity is 27.665045225752674
At time: 254.20020937919617 and batch: 850, loss is 3.381227259635925 and perplexity is 29.40683880283555
At time: 254.84014582633972 and batch: 900, loss is 3.352912940979004 and perplexity is 28.58588146817036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259960017792166 and perplexity of 70.80715237139826
Finished Training.
Improved accuracyfrom -10000000 to -70.80715237139826
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f74eca12b70>
ELAPSED
266.9952573776245


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0538067817687988 and batch: 50, loss is 7.04499059677124 and perplexity is 1147.0980493397753
At time: 1.8535199165344238 and batch: 100, loss is 6.054420156478882 and perplexity is 425.99182522768814
At time: 2.6534297466278076 and batch: 150, loss is 5.846438684463501 and perplexity is 345.99996865440016
At time: 3.450392484664917 and batch: 200, loss is 5.6347504901885985 and perplexity is 279.9890485795904
At time: 4.242403984069824 and batch: 250, loss is 5.640766954421997 and perplexity is 281.6786703538805
At time: 5.033627033233643 and batch: 300, loss is 5.515404949188232 and perplexity is 248.4905829358472
At time: 5.823738098144531 and batch: 350, loss is 5.456214981079102 and perplexity is 234.20925808919105
At time: 6.621649980545044 and batch: 400, loss is 5.2860298919677735 and perplexity is 197.55754163700485
At time: 7.426825046539307 and batch: 450, loss is 5.266290216445923 and perplexity is 193.69605747671
At time: 8.22364854812622 and batch: 500, loss is 5.1948653411865235 and perplexity is 180.34385627214576
At time: 9.018565654754639 and batch: 550, loss is 5.227369756698608 and perplexity is 186.30213856693985
At time: 9.816274404525757 and batch: 600, loss is 5.132216262817383 and perplexity is 169.3921197968358
At time: 10.608299732208252 and batch: 650, loss is 5.012472143173218 and perplexity is 150.27578056764426
At time: 11.401647329330444 and batch: 700, loss is 5.091890888214111 and perplexity is 162.69721362188955
At time: 12.200682163238525 and batch: 750, loss is 5.077096948623657 and perplexity is 160.30799740749612
At time: 12.998245000839233 and batch: 800, loss is 5.03676983833313 and perplexity is 153.971856903455
At time: 13.798501968383789 and batch: 850, loss is 5.068796339035035 and perplexity is 158.98285067846052
At time: 14.597358465194702 and batch: 900, loss is 4.973700971603393 and perplexity is 144.56091439106729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.937037637788955 and perplexity of 139.35681213253037
finished 1 epochs...
Completing Train Step...
At time: 16.322457790374756 and batch: 50, loss is 4.894900226593018 and perplexity is 133.60667555629948
At time: 16.941736221313477 and batch: 100, loss is 4.762991323471069 and perplexity is 117.0956735706973
At time: 17.56924057006836 and batch: 150, loss is 4.7467845439910885 and perplexity is 115.21322522710037
At time: 18.202123641967773 and batch: 200, loss is 4.6252145576477055 and perplexity is 102.02466090576769
At time: 18.836728811264038 and batch: 250, loss is 4.732431001663208 and perplexity is 113.5713190959143
At time: 19.470679759979248 and batch: 300, loss is 4.673757553100586 and perplexity is 107.09941905335678
At time: 20.106654405593872 and batch: 350, loss is 4.659006147384644 and perplexity is 105.53114761350456
At time: 20.742965936660767 and batch: 400, loss is 4.530747022628784 and perplexity is 92.82787971432339
At time: 21.385088443756104 and batch: 450, loss is 4.552038478851318 and perplexity is 94.82551122915851
At time: 22.021926879882812 and batch: 500, loss is 4.451375999450684 and perplexity is 85.74484772924015
At time: 22.64899468421936 and batch: 550, loss is 4.524539670944214 and perplexity is 92.2534491096523
At time: 23.278972625732422 and batch: 600, loss is 4.4845536518096925 and perplexity is 88.63737884853758
At time: 23.907712697982788 and batch: 650, loss is 4.340370302200317 and perplexity is 76.73594956875428
At time: 24.533390760421753 and batch: 700, loss is 4.388090200424195 and perplexity is 80.4865589102287
At time: 25.1601243019104 and batch: 750, loss is 4.435062637329102 and perplexity is 84.3574086455762
At time: 25.789209842681885 and batch: 800, loss is 4.389627590179443 and perplexity is 80.61039328779329
At time: 26.420912265777588 and batch: 850, loss is 4.455818405151367 and perplexity is 86.12660846980575
At time: 27.05574655532837 and batch: 900, loss is 4.390492963790893 and perplexity is 80.68018158707426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.533383304125642 and perplexity of 93.07292299562052
finished 2 epochs...
Completing Train Step...
At time: 28.688930988311768 and batch: 50, loss is 4.440854969024659 and perplexity is 84.84745261591412
At time: 29.356146812438965 and batch: 100, loss is 4.3120121574401855 and perplexity is 74.59042572163445
At time: 30.011937141418457 and batch: 150, loss is 4.314713435173035 and perplexity is 74.79218756241762
At time: 30.665693044662476 and batch: 200, loss is 4.210051312446594 and perplexity is 67.35999612764364
At time: 31.301804065704346 and batch: 250, loss is 4.349896383285523 and perplexity is 77.47043527739622
At time: 31.967397689819336 and batch: 300, loss is 4.319854745864868 and perplexity is 75.17770762600391
At time: 32.61169648170471 and batch: 350, loss is 4.307509574890137 and perplexity is 74.25533113402207
At time: 33.24795579910278 and batch: 400, loss is 4.2148705577850345 and perplexity is 67.68540395525442
At time: 33.89918112754822 and batch: 450, loss is 4.248291192054748 and perplexity is 69.98571795755214
At time: 34.55273485183716 and batch: 500, loss is 4.131020951271057 and perplexity is 62.241435980558286
At time: 35.226322412490845 and batch: 550, loss is 4.213592915534973 and perplexity is 67.59898144373089
At time: 35.8963828086853 and batch: 600, loss is 4.210282769203186 and perplexity is 67.37558885832321
At time: 36.53392028808594 and batch: 650, loss is 4.058174514770508 and perplexity is 57.86857636833343
At time: 37.168967723846436 and batch: 700, loss is 4.083257007598877 and perplexity is 59.338421145970756
At time: 37.80532884597778 and batch: 750, loss is 4.165983386039734 and perplexity is 64.4560364540576
At time: 38.43783450126648 and batch: 800, loss is 4.12197928905487 and perplexity is 61.68120646025046
At time: 39.07348823547363 and batch: 850, loss is 4.2027050352096555 and perplexity is 66.86696411366023
At time: 39.707459688186646 and batch: 900, loss is 4.143396816253662 and perplexity is 63.01651382527272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.401763079917594 and perplexity of 81.59459972322135
finished 3 epochs...
Completing Train Step...
At time: 41.322279930114746 and batch: 50, loss is 4.22185444355011 and perplexity is 68.15976560014307
At time: 41.96742033958435 and batch: 100, loss is 4.0894093465805055 and perplexity is 59.70461655155021
At time: 42.60132932662964 and batch: 150, loss is 4.099470987319946 and perplexity is 60.30837526186416
At time: 43.23648810386658 and batch: 200, loss is 3.993119626045227 and perplexity is 54.2237837106845
At time: 43.870664834976196 and batch: 250, loss is 4.143647766113281 and perplexity is 63.032329794995526
At time: 44.50563097000122 and batch: 300, loss is 4.11834988117218 and perplexity is 61.457745963250694
At time: 45.14034676551819 and batch: 350, loss is 4.10952977180481 and perplexity is 60.91806544166383
At time: 45.77668380737305 and batch: 400, loss is 4.025980153083801 and perplexity is 56.035204952859466
At time: 46.411360025405884 and batch: 450, loss is 4.067569074630737 and perplexity is 58.41478786407103
At time: 47.05228567123413 and batch: 500, loss is 3.943823595046997 and perplexity is 51.61558155616247
At time: 47.68942379951477 and batch: 550, loss is 4.025911817550659 and perplexity is 56.03137588808636
At time: 48.331620931625366 and batch: 600, loss is 4.0408722448349 and perplexity is 56.87593091280868
At time: 48.964452505111694 and batch: 650, loss is 3.8834703254699705 and perplexity is 48.592554786125376
At time: 49.59678030014038 and batch: 700, loss is 3.902779712677002 and perplexity is 49.5399647580387
At time: 50.23006248474121 and batch: 750, loss is 3.99355016708374 and perplexity is 54.24713430116846
At time: 50.86669850349426 and batch: 800, loss is 3.9557277870178225 and perplexity is 52.23369511957428
At time: 51.50431990623474 and batch: 850, loss is 4.0376524209976195 and perplexity is 56.69309494245498
At time: 52.14891242980957 and batch: 900, loss is 3.9847820377349854 and perplexity is 53.773567591881495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340322416122645 and perplexity of 76.73227507309228
finished 4 epochs...
Completing Train Step...
At time: 53.90016460418701 and batch: 50, loss is 4.067316856384277 and perplexity is 58.400056446552874
At time: 54.53610610961914 and batch: 100, loss is 3.93368284702301 and perplexity is 51.094805939455334
At time: 55.17404055595398 and batch: 150, loss is 3.949008812904358 and perplexity is 51.88391467298919
At time: 55.81429934501648 and batch: 200, loss is 3.8506616306304933 and perplexity is 47.02416556962917
At time: 56.47030210494995 and batch: 250, loss is 3.9978602361679076 and perplexity is 54.48144778855285
At time: 57.149622678756714 and batch: 300, loss is 3.9773580408096314 and perplexity is 53.375831015766614
At time: 57.794020652770996 and batch: 350, loss is 3.9698733043670655 and perplexity is 52.977818356235844
At time: 58.42933702468872 and batch: 400, loss is 3.8922623109817507 and perplexity is 49.021663418087186
At time: 59.06588292121887 and batch: 450, loss is 3.934233021736145 and perplexity is 51.122924744074126
At time: 59.703528881073 and batch: 500, loss is 3.808316717147827 and perplexity is 45.07450183648729
At time: 60.33825969696045 and batch: 550, loss is 3.8922267532348633 and perplexity is 49.019920349177355
At time: 60.97335886955261 and batch: 600, loss is 3.91570529460907 and perplexity is 50.18445385684558
At time: 61.61071443557739 and batch: 650, loss is 3.7555920553207396 and perplexity is 42.75952832404965
At time: 62.24838042259216 and batch: 700, loss is 3.7708065271377564 and perplexity is 43.41506614823619
At time: 62.88609576225281 and batch: 750, loss is 3.870024838447571 and perplexity is 47.943576910051455
At time: 63.52210831642151 and batch: 800, loss is 3.832176938056946 and perplexity is 46.162922752491134
At time: 64.1765809059143 and batch: 850, loss is 3.9130680322647096 and perplexity is 50.05227865336834
At time: 64.810720205307 and batch: 900, loss is 3.867300133705139 and perplexity is 47.813122624089196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324928597228168 and perplexity of 75.56011746527888
finished 5 epochs...
Completing Train Step...
At time: 66.48299741744995 and batch: 50, loss is 3.9510452938079834 and perplexity is 51.98968293539537
At time: 67.14826202392578 and batch: 100, loss is 3.8202942752838136 and perplexity is 45.61763048700447
At time: 67.78197717666626 and batch: 150, loss is 3.83776939868927 and perplexity is 46.42181031517004
At time: 68.41592693328857 and batch: 200, loss is 3.7411489486694336 and perplexity is 42.146386391849106
At time: 69.048588514328 and batch: 250, loss is 3.886820511817932 and perplexity is 48.75562189987291
At time: 69.68031120300293 and batch: 300, loss is 3.867712206840515 and perplexity is 47.83282918743483
At time: 70.31431102752686 and batch: 350, loss is 3.862900333404541 and perplexity is 47.603216543830335
At time: 70.94943618774414 and batch: 400, loss is 3.7887382316589355 and perplexity is 44.200594165821066
At time: 71.58653330802917 and batch: 450, loss is 3.832030735015869 and perplexity is 46.15617408614952
At time: 72.22186279296875 and batch: 500, loss is 3.703309512138367 and perplexity is 40.58138695641891
At time: 72.86097478866577 and batch: 550, loss is 3.7857674741744995 and perplexity is 44.06947977084959
At time: 73.51484656333923 and batch: 600, loss is 3.8127378320693968 and perplexity is 45.274222557953884
At time: 74.1606764793396 and batch: 650, loss is 3.658187232017517 and perplexity is 38.790960086322514
At time: 74.78877544403076 and batch: 700, loss is 3.6694486236572263 and perplexity is 39.23026925350907
At time: 75.4238760471344 and batch: 750, loss is 3.771832242012024 and perplexity is 43.45962047346253
At time: 76.0541479587555 and batch: 800, loss is 3.737296471595764 and perplexity is 41.984330762764415
At time: 76.6871485710144 and batch: 850, loss is 3.812762222290039 and perplexity is 45.275326819698016
At time: 77.31989860534668 and batch: 900, loss is 3.7726902055740354 and perplexity is 43.4969232441684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3110895026220035 and perplexity of 74.52163624530301
finished 6 epochs...
Completing Train Step...
At time: 78.91354012489319 and batch: 50, loss is 3.859541726112366 and perplexity is 47.443604221209355
At time: 79.54677200317383 and batch: 100, loss is 3.7282070970535277 and perplexity is 41.604448516945745
At time: 80.16844511032104 and batch: 150, loss is 3.7472101211547852 and perplexity is 42.40261865893736
At time: 80.79891896247864 and batch: 200, loss is 3.654138789176941 and perplexity is 38.63423456290302
At time: 81.41973114013672 and batch: 250, loss is 3.798685574531555 and perplexity is 44.642466716457804
At time: 82.05191612243652 and batch: 300, loss is 3.776805210113525 and perplexity is 43.676282058846205
At time: 82.678701877594 and batch: 350, loss is 3.7760381412506105 and perplexity is 43.64279218899129
At time: 83.30695843696594 and batch: 400, loss is 3.70226478099823 and perplexity is 40.539012456589546
At time: 83.93457198143005 and batch: 450, loss is 3.748595471382141 and perplexity is 42.46140184458544
At time: 84.56313133239746 and batch: 500, loss is 3.6215462493896484 and perplexity is 37.39534567147997
At time: 85.19072484970093 and batch: 550, loss is 3.6976560401916503 and perplexity is 40.35260852925809
At time: 85.82434821128845 and batch: 600, loss is 3.728790307044983 and perplexity is 41.62871972392706
At time: 86.4563341140747 and batch: 650, loss is 3.5732061100006103 and perplexity is 35.63064599139752
At time: 87.08750915527344 and batch: 700, loss is 3.585746946334839 and perplexity is 36.080297702374395
At time: 87.72062587738037 and batch: 750, loss is 3.6917945957183838 and perplexity is 40.11677579046328
At time: 88.35000133514404 and batch: 800, loss is 3.654165196418762 and perplexity is 38.63525479994846
At time: 89.01719069480896 and batch: 850, loss is 3.7315149021148684 and perplexity is 41.74229578257704
At time: 89.66611957550049 and batch: 900, loss is 3.6966709232330324 and perplexity is 40.31287606404731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3194559175674225 and perplexity of 75.14773060710465
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 91.2893431186676 and batch: 50, loss is 3.805079560279846 and perplexity is 44.92882452067313
At time: 91.91667628288269 and batch: 100, loss is 3.6704153490066527 and perplexity is 39.2682124866489
At time: 92.55051636695862 and batch: 150, loss is 3.6949679756164553 and perplexity is 40.24428376895604
At time: 93.18217515945435 and batch: 200, loss is 3.574174427986145 and perplexity is 35.665164496494704
At time: 93.81235408782959 and batch: 250, loss is 3.717266631126404 and perplexity is 41.15175730725859
At time: 94.44538521766663 and batch: 300, loss is 3.6806936359405515 and perplexity is 39.67390377617119
At time: 95.07699990272522 and batch: 350, loss is 3.6735264444351197 and perplexity is 39.39056987706862
At time: 95.706791639328 and batch: 400, loss is 3.5981212711334227 and perplexity is 36.529540832703034
At time: 96.35377717018127 and batch: 450, loss is 3.6255126190185547 and perplexity is 37.54396397754188
At time: 96.98177695274353 and batch: 500, loss is 3.492487063407898 and perplexity is 32.867589919663054
At time: 97.62122678756714 and batch: 550, loss is 3.552418885231018 and perplexity is 34.897628835744094
At time: 98.29962420463562 and batch: 600, loss is 3.571327977180481 and perplexity is 35.563789707901115
At time: 98.9202151298523 and batch: 650, loss is 3.4058033514022825 and perplexity is 30.138497805702887
At time: 99.55050420761108 and batch: 700, loss is 3.4026595544815064 and perplexity is 30.043897269456806
At time: 100.18574714660645 and batch: 750, loss is 3.500328788757324 and perplexity is 33.126341737110295
At time: 100.82165360450745 and batch: 800, loss is 3.442819366455078 and perplexity is 31.275009698444777
At time: 101.45567893981934 and batch: 850, loss is 3.501857051849365 and perplexity is 33.177006207020234
At time: 102.09023022651672 and batch: 900, loss is 3.4595771169662477 and perplexity is 31.803524499629276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267184322827483 and perplexity of 71.32053702967887
finished 8 epochs...
Completing Train Step...
At time: 103.71492886543274 and batch: 50, loss is 3.711160135269165 and perplexity is 40.90122997230864
At time: 104.36028289794922 and batch: 100, loss is 3.574179253578186 and perplexity is 35.6653366024439
At time: 105.00084280967712 and batch: 150, loss is 3.5982403802871703 and perplexity is 36.53389209453082
At time: 105.63692474365234 and batch: 200, loss is 3.4860525560379028 and perplexity is 32.65678211959967
At time: 106.2719497680664 and batch: 250, loss is 3.6320623922348023 and perplexity is 37.790675497503166
At time: 106.90721774101257 and batch: 300, loss is 3.59875771522522 and perplexity is 36.55279724305954
At time: 107.54386782646179 and batch: 350, loss is 3.594840545654297 and perplexity is 36.40989380918668
At time: 108.1791558265686 and batch: 400, loss is 3.5222247314453123 and perplexity is 33.85967341308235
At time: 108.81526112556458 and batch: 450, loss is 3.5538089990615847 and perplexity is 34.946174246248425
At time: 109.45139193534851 and batch: 500, loss is 3.424756555557251 and perplexity is 30.71516650466518
At time: 110.08704662322998 and batch: 550, loss is 3.489933142662048 and perplexity is 32.7837557983194
At time: 110.72223901748657 and batch: 600, loss is 3.5136082220077514 and perplexity is 33.5691745579491
At time: 111.3563928604126 and batch: 650, loss is 3.3543202304840087 and perplexity is 28.626138399086066
At time: 111.99102091789246 and batch: 700, loss is 3.3554050302505494 and perplexity is 28.65720887691495
At time: 112.64286780357361 and batch: 750, loss is 3.458269648551941 and perplexity is 31.76196956768263
At time: 113.27698016166687 and batch: 800, loss is 3.4058300399780275 and perplexity is 30.139302170018034
At time: 113.91366386413574 and batch: 850, loss is 3.4715984869003296 and perplexity is 32.18815368572097
At time: 114.55055618286133 and batch: 900, loss is 3.4358141136169436 and perplexity is 31.056685947356915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2730169426904965 and perplexity of 71.7377381155236
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 116.15813183784485 and batch: 50, loss is 3.6815167760848997 and perplexity is 39.706574403460095
At time: 116.80117797851562 and batch: 100, loss is 3.554831008911133 and perplexity is 34.98190783746288
At time: 117.43463635444641 and batch: 150, loss is 3.5845835781097413 and perplexity is 36.038347437009655
At time: 118.07639455795288 and batch: 200, loss is 3.466579236984253 and perplexity is 32.026998076505144
At time: 118.73925137519836 and batch: 250, loss is 3.6141704893112183 and perplexity is 37.12054126526246
At time: 119.37783813476562 and batch: 300, loss is 3.5759512042999266 and perplexity is 35.72858984564243
At time: 120.01132154464722 and batch: 350, loss is 3.568095812797546 and perplexity is 35.44902725894423
At time: 120.64540982246399 and batch: 400, loss is 3.4947850418090822 and perplexity is 32.94320577992884
At time: 121.28373908996582 and batch: 450, loss is 3.517985215187073 and perplexity is 33.71642863598037
At time: 121.92069387435913 and batch: 500, loss is 3.3893480777740477 and perplexity is 29.64661867941332
At time: 122.55516171455383 and batch: 550, loss is 3.447018084526062 and perplexity is 31.406600710294892
At time: 123.19237208366394 and batch: 600, loss is 3.4695574140548704 and perplexity is 32.122522321270374
At time: 123.83130240440369 and batch: 650, loss is 3.3024183988571165 and perplexity is 27.17828744579982
At time: 124.46803212165833 and batch: 700, loss is 3.2980094289779665 and perplexity is 27.058722966862117
At time: 125.10408067703247 and batch: 750, loss is 3.397434573173523 and perplexity is 29.88732786012435
At time: 125.73774337768555 and batch: 800, loss is 3.341000714302063 and perplexity is 28.24738012353643
At time: 126.37246799468994 and batch: 850, loss is 3.3983016538619997 and perplexity is 29.9132538232687
At time: 127.01160907745361 and batch: 900, loss is 3.367561283111572 and perplexity is 29.007699162466004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2613721873662245 and perplexity of 70.90721471347145
finished 10 epochs...
Completing Train Step...
At time: 128.64165925979614 and batch: 50, loss is 3.6581522178649903 and perplexity is 38.7896018775078
At time: 129.2743034362793 and batch: 100, loss is 3.5208123683929444 and perplexity is 33.81188501661512
At time: 129.92911529541016 and batch: 150, loss is 3.5497040796279906 and perplexity is 34.80301704187124
At time: 130.57018113136292 and batch: 200, loss is 3.4325549268722533 and perplexity is 30.955631175822028
At time: 131.2062783241272 and batch: 250, loss is 3.5809840869903566 and perplexity is 35.908860908064234
At time: 131.84339666366577 and batch: 300, loss is 3.544479794502258 and perplexity is 34.62167027368088
At time: 132.48107600212097 and batch: 350, loss is 3.5382216119766237 and perplexity is 34.4056781059121
At time: 133.1190309524536 and batch: 400, loss is 3.4672109603881838 and perplexity is 32.04723667267209
At time: 133.75732135772705 and batch: 450, loss is 3.492901382446289 and perplexity is 32.88121040933204
At time: 134.39444780349731 and batch: 500, loss is 3.365362024307251 and perplexity is 28.943973824630234
At time: 135.03113913536072 and batch: 550, loss is 3.4261030387878417 and perplexity is 30.75655181735059
At time: 135.66899371147156 and batch: 600, loss is 3.4516379261016845 and perplexity is 31.552029901259584
At time: 136.30636167526245 and batch: 650, loss is 3.2873110485076906 and perplexity is 26.770781453817893
At time: 136.94328546524048 and batch: 700, loss is 3.2861811780929564 and perplexity is 26.740551021325587
At time: 137.58368253707886 and batch: 750, loss is 3.388636922836304 and perplexity is 29.62554283513565
At time: 138.22310400009155 and batch: 800, loss is 3.3349250459671023 and perplexity is 28.076278714612894
At time: 138.85905265808105 and batch: 850, loss is 3.3955291938781738 and perplexity is 29.830435382513013
At time: 139.49758911132812 and batch: 900, loss is 3.3675061225891114 and perplexity is 29.006099126754627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262933600438784 and perplexity of 71.01801664674605
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 141.1330065727234 and batch: 50, loss is 3.649804973602295 and perplexity is 38.46716320515976
At time: 141.80007982254028 and batch: 100, loss is 3.520278902053833 and perplexity is 33.79385232444165
At time: 142.4481885433197 and batch: 150, loss is 3.5495703983306885 and perplexity is 34.798364840366155
At time: 143.0858509540558 and batch: 200, loss is 3.4314111423492433 and perplexity is 30.920244845009698
At time: 143.7251205444336 and batch: 250, loss is 3.5803805923461915 and perplexity is 35.88719664062036
At time: 144.37600994110107 and batch: 300, loss is 3.5407499504089355 and perplexity is 34.49277736596187
At time: 145.0132701396942 and batch: 350, loss is 3.5318154859542847 and perplexity is 34.185975468826044
At time: 145.65029621124268 and batch: 400, loss is 3.460540075302124 and perplexity is 31.834164718916035
At time: 146.2863793373108 and batch: 450, loss is 3.483507266044617 and perplexity is 32.57376683266518
At time: 146.92445921897888 and batch: 500, loss is 3.357211666107178 and perplexity is 28.70902881380618
At time: 147.56635761260986 and batch: 550, loss is 3.4153188848495484 and perplexity is 30.426650478584012
At time: 148.204345703125 and batch: 600, loss is 3.440300073623657 and perplexity is 31.196317956100355
At time: 148.84007287025452 and batch: 650, loss is 3.272934775352478 and perplexity is 26.388670632938727
At time: 149.474778175354 and batch: 700, loss is 3.269174919128418 and perplexity is 26.289639313838695
At time: 150.11237239837646 and batch: 750, loss is 3.367827410697937 and perplexity is 29.015419938740322
At time: 150.75494360923767 and batch: 800, loss is 3.313226971626282 and perplexity is 27.473639233671744
At time: 151.39466452598572 and batch: 850, loss is 3.3718289232254026 and perplexity is 29.131758114213294
At time: 152.02883768081665 and batch: 900, loss is 3.344515910148621 and perplexity is 28.34684992230887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260404821944563 and perplexity of 70.83865469245467
finished 12 epochs...
Completing Train Step...
At time: 153.64978051185608 and batch: 50, loss is 3.639614758491516 and perplexity is 38.07716499501923
At time: 154.29492616653442 and batch: 100, loss is 3.5079715490341186 and perplexity is 33.38048837969102
At time: 154.93395805358887 and batch: 150, loss is 3.5357805347442626 and perplexity is 34.321793614339725
At time: 155.5708577632904 and batch: 200, loss is 3.4186846351623537 and perplexity is 30.529231521199222
At time: 156.2061252593994 and batch: 250, loss is 3.5683222341537477 and perplexity is 35.457054584516904
At time: 156.84474158287048 and batch: 300, loss is 3.5297343492507935 and perplexity is 34.114903761154665
At time: 157.4820785522461 and batch: 350, loss is 3.5210839557647704 and perplexity is 33.82106914469342
At time: 158.12052702903748 and batch: 400, loss is 3.4509225273132325 and perplexity is 31.529465689462263
At time: 158.75866794586182 and batch: 450, loss is 3.4750088691711425 and perplexity is 32.29811499284981
At time: 159.39146280288696 and batch: 500, loss is 3.349075517654419 and perplexity is 28.47639554614181
At time: 160.0240113735199 and batch: 550, loss is 3.407854895591736 and perplexity is 30.20039173311016
At time: 160.67168736457825 and batch: 600, loss is 3.4347974967956545 and perplexity is 31.025129241266306
At time: 161.3037235736847 and batch: 650, loss is 3.268577790260315 and perplexity is 26.273945697294444
At time: 161.9365749359131 and batch: 700, loss is 3.266150050163269 and perplexity is 26.210236751489948
At time: 162.5650451183319 and batch: 750, loss is 3.366397833824158 and perplexity is 28.973969800551657
At time: 163.1938898563385 and batch: 800, loss is 3.3133847951889037 and perplexity is 27.4779755634723
At time: 163.8205006122589 and batch: 850, loss is 3.3739012479782104 and perplexity is 29.1921911744819
At time: 164.4479534626007 and batch: 900, loss is 3.3477411556243895 and perplexity is 28.43842306532547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260300727739726 and perplexity of 70.8312811827992
finished 13 epochs...
Completing Train Step...
At time: 166.03424859046936 and batch: 50, loss is 3.6337290048599242 and perplexity is 37.85371042721356
At time: 166.6611671447754 and batch: 100, loss is 3.501457781791687 and perplexity is 33.16376226596887
At time: 167.28563976287842 and batch: 150, loss is 3.5288204383850097 and perplexity is 34.083740022530925
At time: 167.91027665138245 and batch: 200, loss is 3.4119713068008424 and perplexity is 30.324965186032326
At time: 168.53449392318726 and batch: 250, loss is 3.5614466381073 and perplexity is 35.21410237791834
At time: 169.15901684761047 and batch: 300, loss is 3.5231855726242065 and perplexity is 33.89222281650095
At time: 169.78422570228577 and batch: 350, loss is 3.5148143768310547 and perplexity is 33.60968860795321
At time: 170.41177320480347 and batch: 400, loss is 3.44529146194458 and perplexity is 31.35242015244861
At time: 171.04356861114502 and batch: 450, loss is 3.469801483154297 and perplexity is 32.130363393207396
At time: 171.67659616470337 and batch: 500, loss is 3.3441455793380737 and perplexity is 28.336354153973836
At time: 172.3046293258667 and batch: 550, loss is 3.403380522727966 and perplexity is 30.065565775601822
At time: 172.93357586860657 and batch: 600, loss is 3.4311415767669677 and perplexity is 30.911910934522133
At time: 173.56358122825623 and batch: 650, loss is 3.265515832901001 and perplexity is 26.193619037071695
At time: 174.1944191455841 and batch: 700, loss is 3.26392849445343 and perplexity is 26.152073880328686
At time: 174.82331728935242 and batch: 750, loss is 3.3647845697402956 and perplexity is 28.92726481956931
At time: 175.45322108268738 and batch: 800, loss is 3.312478814125061 and perplexity is 27.45309231151862
At time: 176.0919704437256 and batch: 850, loss is 3.373791561126709 and perplexity is 29.188989350545743
At time: 176.72354221343994 and batch: 900, loss is 3.348119034767151 and perplexity is 28.449171382909505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260729646029538 and perplexity of 70.86166853116818
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 178.3208932876587 and batch: 50, loss is 3.630966739654541 and perplexity is 37.74929272109723
At time: 178.9623064994812 and batch: 100, loss is 3.5015125608444215 and perplexity is 33.165578995209806
At time: 179.59578919410706 and batch: 150, loss is 3.529018359184265 and perplexity is 34.090486571216935
At time: 180.2287154197693 and batch: 200, loss is 3.4126627969741823 and perplexity is 30.345941853209105
At time: 180.86159300804138 and batch: 250, loss is 3.5613520097732545 and perplexity is 35.210770283733154
At time: 181.49753618240356 and batch: 300, loss is 3.522537922859192 and perplexity is 33.87027963287389
At time: 182.1303675174713 and batch: 350, loss is 3.5142578792572023 and perplexity is 33.59099010110436
At time: 182.76170182228088 and batch: 400, loss is 3.444160056114197 and perplexity is 31.31696790071547
At time: 183.39357995986938 and batch: 450, loss is 3.4678560876846314 and perplexity is 32.06791789011702
At time: 184.02595233917236 and batch: 500, loss is 3.3408906507492064 and perplexity is 28.244271287609045
At time: 184.66295051574707 and batch: 550, loss is 3.398985643386841 and perplexity is 29.933721174466687
At time: 185.29675793647766 and batch: 600, loss is 3.4265553760528564 and perplexity is 30.770467298889095
At time: 185.94123816490173 and batch: 650, loss is 3.261196699142456 and perplexity is 26.0807292613466
At time: 186.58755898475647 and batch: 700, loss is 3.257560257911682 and perplexity is 25.98606045522958
At time: 187.24508810043335 and batch: 750, loss is 3.3569314956665037 and perplexity is 28.70098651921131
At time: 187.87949562072754 and batch: 800, loss is 3.3034920120239257 and perplexity is 27.207482082121683
At time: 188.51831817626953 and batch: 850, loss is 3.3639534378051756 and perplexity is 28.903232434406913
At time: 189.15906047821045 and batch: 900, loss is 3.337674322128296 and perplexity is 28.153574363233346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260477562473245 and perplexity of 70.84380772106282
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 190.77765417099 and batch: 50, loss is 3.6286055183410646 and perplexity is 37.6602634368415
At time: 191.41679739952087 and batch: 100, loss is 3.499201979637146 and perplexity is 33.08903569546405
At time: 192.04664206504822 and batch: 150, loss is 3.527080497741699 and perplexity is 34.02448790052563
At time: 192.6846055984497 and batch: 200, loss is 3.411339616775513 and perplexity is 30.305815257052192
At time: 193.31300044059753 and batch: 250, loss is 3.560096011161804 and perplexity is 35.16657336659214
At time: 193.9428551197052 and batch: 300, loss is 3.521673355102539 and perplexity is 33.841009136181576
At time: 194.57473516464233 and batch: 350, loss is 3.5128072118759155 and perplexity is 33.542296075416374
At time: 195.20359754562378 and batch: 400, loss is 3.4424445915222166 and perplexity is 31.263290804895743
At time: 195.8328559398651 and batch: 450, loss is 3.4665788078308104 and perplexity is 32.026984332011615
At time: 196.464453458786 and batch: 500, loss is 3.3390944623947143 and perplexity is 28.193584791315764
At time: 197.09608507156372 and batch: 550, loss is 3.3969030475616453 and perplexity is 29.871446201024305
At time: 197.7288498878479 and batch: 600, loss is 3.4246220874786375 and perplexity is 30.711036572919227
At time: 198.36533951759338 and batch: 650, loss is 3.25932363986969 and perplexity is 26.03192423118905
At time: 199.01669025421143 and batch: 700, loss is 3.255449299812317 and perplexity is 25.931262828536806
At time: 199.6540732383728 and batch: 750, loss is 3.354813480377197 and perplexity is 28.64026172167115
At time: 200.28929328918457 and batch: 800, loss is 3.30104172706604 and perplexity is 27.1408976068169
At time: 200.92048597335815 and batch: 850, loss is 3.361421947479248 and perplexity is 28.830156715374194
At time: 201.59252166748047 and batch: 900, loss is 3.334711375236511 and perplexity is 28.07028027649581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260175731084118 and perplexity of 70.82242806285562
finished 16 epochs...
Completing Train Step...
At time: 203.37065529823303 and batch: 50, loss is 3.6278101873397826 and perplexity is 37.63032296968283
At time: 204.04428577423096 and batch: 100, loss is 3.4981638479232786 and perplexity is 33.0547027422745
At time: 204.68630361557007 and batch: 150, loss is 3.52601309299469 and perplexity is 33.98818937668078
At time: 205.32528924942017 and batch: 200, loss is 3.4104140186309815 and perplexity is 30.27777722865704
At time: 205.96590566635132 and batch: 250, loss is 3.5592210149765013 and perplexity is 35.13581620720712
At time: 206.60669827461243 and batch: 300, loss is 3.5208324337005616 and perplexity is 33.81256346929575
At time: 207.25309419631958 and batch: 350, loss is 3.5118627071380617 and perplexity is 33.51063017450544
At time: 207.88639450073242 and batch: 400, loss is 3.4416272687911986 and perplexity is 31.237749046026394
At time: 208.54101514816284 and batch: 450, loss is 3.465824213027954 and perplexity is 32.00282605208609
At time: 209.24170088768005 and batch: 500, loss is 3.3384500837326048 and perplexity is 28.17542329893537
At time: 209.8966429233551 and batch: 550, loss is 3.3963741636276246 and perplexity is 29.85565185010001
At time: 210.5300998687744 and batch: 600, loss is 3.4243405532836912 and perplexity is 30.70239158294895
At time: 211.16704940795898 and batch: 650, loss is 3.259041919708252 and perplexity is 26.024591546223196
At time: 211.8043282032013 and batch: 700, loss is 3.2554336977005005 and perplexity is 25.930858249230774
At time: 212.44786667823792 and batch: 750, loss is 3.3549013900756837 and perplexity is 28.642779589114536
At time: 213.08663702011108 and batch: 800, loss is 3.3013551473617553 and perplexity is 27.149405448166238
At time: 213.72393035888672 and batch: 850, loss is 3.3619256019592285 and perplexity is 28.84468081021813
At time: 214.36189651489258 and batch: 900, loss is 3.3353367280960082 and perplexity is 28.087839596346804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.25999513390946 and perplexity of 70.80963888732433
finished 17 epochs...
Completing Train Step...
At time: 216.08858275413513 and batch: 50, loss is 3.6271282911300657 and perplexity is 37.60467174181086
At time: 216.74450850486755 and batch: 100, loss is 3.4973212480545044 and perplexity is 33.026862584785164
At time: 217.38142371177673 and batch: 150, loss is 3.5251368379592893 and perplexity is 33.95842009923974
At time: 218.01959657669067 and batch: 200, loss is 3.4096356725692747 and perplexity is 30.254219809095552
At time: 218.656991481781 and batch: 250, loss is 3.558459982872009 and perplexity is 35.109086895278224
At time: 219.29514050483704 and batch: 300, loss is 3.5201221990585325 and perplexity is 33.78855714145602
At time: 219.93805861473083 and batch: 350, loss is 3.5110814714431764 and perplexity is 33.48446069765073
At time: 220.57699990272522 and batch: 400, loss is 3.440930323600769 and perplexity is 31.215985631903255
At time: 221.2139389514923 and batch: 450, loss is 3.4651976156234743 and perplexity is 31.982779445577542
At time: 221.86626505851746 and batch: 500, loss is 3.3379169511795044 and perplexity is 28.160406067020617
At time: 222.50510382652283 and batch: 550, loss is 3.395924515724182 and perplexity is 29.842230336543548
At time: 223.16470217704773 and batch: 600, loss is 3.424089493751526 and perplexity is 30.69468442240032
At time: 223.82020449638367 and batch: 650, loss is 3.258820662498474 and perplexity is 26.018834054678806
At time: 224.48756670951843 and batch: 700, loss is 3.2554169607162478 and perplexity is 25.930424248496536
At time: 225.20299768447876 and batch: 750, loss is 3.3549792861938474 and perplexity is 28.64501083735961
At time: 225.85169792175293 and batch: 800, loss is 3.30161301612854 and perplexity is 27.156407334613206
At time: 226.49470853805542 and batch: 850, loss is 3.3623354959487917 and perplexity is 28.85650649498609
At time: 227.1409101486206 and batch: 900, loss is 3.335843200683594 and perplexity is 28.102068920223928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259876407989084 and perplexity of 70.80123244681694
finished 18 epochs...
Completing Train Step...
At time: 228.8313970565796 and batch: 50, loss is 3.62651948928833 and perplexity is 37.581784915874366
At time: 229.51277327537537 and batch: 100, loss is 3.496595482826233 and perplexity is 33.00290153242331
At time: 230.1552276611328 and batch: 150, loss is 3.5243747520446775 and perplexity is 33.93255072419483
At time: 230.80195832252502 and batch: 200, loss is 3.4089475297927856 and perplexity is 30.233407747931665
At time: 231.44607067108154 and batch: 250, loss is 3.557776894569397 and perplexity is 35.0851124779589
At time: 232.10456490516663 and batch: 300, loss is 3.5194932508468626 and perplexity is 33.76731257043479
At time: 232.7853045463562 and batch: 350, loss is 3.5104038238525392 and perplexity is 33.461777719935796
At time: 233.46022582054138 and batch: 400, loss is 3.4403146743774413 and perplexity is 31.19677344918307
At time: 234.1211416721344 and batch: 450, loss is 3.4646501255035402 and perplexity is 31.96527398229442
At time: 234.77884674072266 and batch: 500, loss is 3.3374499559402464 and perplexity is 28.147258361648596
At time: 235.44554662704468 and batch: 550, loss is 3.3955257511138917 and perplexity is 29.830332683532344
At time: 236.1144347190857 and batch: 600, loss is 3.4238560676574705 and perplexity is 30.68752031828429
At time: 236.75735592842102 and batch: 650, loss is 3.2586299276351927 and perplexity is 26.013871829172395
At time: 237.3988335132599 and batch: 700, loss is 3.2553888177871704 and perplexity is 25.929694500674632
At time: 238.0427324771881 and batch: 750, loss is 3.355041017532349 and perplexity is 28.646779186800714
At time: 238.706303358078 and batch: 800, loss is 3.3018225145339968 and perplexity is 27.162097154631045
At time: 239.3714587688446 and batch: 850, loss is 3.3626713275909426 and perplexity is 28.86619905039134
At time: 240.0388147830963 and batch: 900, loss is 3.3362567901611326 and perplexity is 28.11369404407514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259797396725172 and perplexity of 70.79563857294723
finished 19 epochs...
Completing Train Step...
At time: 241.78371787071228 and batch: 50, loss is 3.625963191986084 and perplexity is 37.56088408438917
At time: 242.43050932884216 and batch: 100, loss is 3.495947866439819 and perplexity is 32.98153523192163
At time: 243.08657050132751 and batch: 150, loss is 3.5236901330947874 and perplexity is 33.90932780728249
At time: 243.7455928325653 and batch: 200, loss is 3.4083218431472777 and perplexity is 30.214497025165873
At time: 244.40633082389832 and batch: 250, loss is 3.557151823043823 and perplexity is 35.063188625879036
At time: 245.05433773994446 and batch: 300, loss is 3.5189204931259157 and perplexity is 33.74797761909435
At time: 245.69468212127686 and batch: 350, loss is 3.5097983121871947 and perplexity is 33.44152235623159
At time: 246.3316125869751 and batch: 400, loss is 3.4397590255737303 and perplexity is 31.17944381437768
At time: 247.00684213638306 and batch: 450, loss is 3.464157724380493 and perplexity is 31.949538119983107
At time: 247.6535964012146 and batch: 500, loss is 3.3370276737213134 and perplexity is 28.135374784219074
At time: 248.29352498054504 and batch: 550, loss is 3.395162863731384 and perplexity is 29.819509596085197
At time: 248.93194127082825 and batch: 600, loss is 3.4236341857910157 and perplexity is 30.680712069340622
At time: 249.58313250541687 and batch: 650, loss is 3.258455948829651 and perplexity is 26.009346360503272
At time: 250.22103834152222 and batch: 700, loss is 3.2553484344482424 and perplexity is 25.928647394176284
At time: 250.85607242584229 and batch: 750, loss is 3.355086421966553 and perplexity is 28.64807990713047
At time: 251.4972882270813 and batch: 800, loss is 3.3019911575317384 and perplexity is 27.166678238393143
At time: 252.1407482624054 and batch: 850, loss is 3.3629480504989626 and perplexity is 28.874188094260795
At time: 252.80632996559143 and batch: 900, loss is 3.336596999168396 and perplexity is 28.12326020317086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2597417962061215 and perplexity of 70.7917024081235
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f74eca12b70>
ELAPSED
527.1669476032257


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.7917024081235}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.41628114957778306, 'tune_wordvecs': 'TRUE', 'dropout': 0.7387750863627058, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0512199401855469 and batch: 50, loss is 7.055090713500976 and perplexity is 1158.7425801082375
At time: 1.8502087593078613 and batch: 100, loss is 6.188528451919556 and perplexity is 487.12874515838183
At time: 2.6601428985595703 and batch: 150, loss is 6.044046678543091 and perplexity is 421.5956496652445
At time: 3.4568843841552734 and batch: 200, loss is 5.893452863693238 and perplexity is 362.65532442425217
At time: 4.249507665634155 and batch: 250, loss is 5.934243373870849 and perplexity is 377.7540694161439
At time: 5.045609951019287 and batch: 300, loss is 5.84496072769165 and perplexity is 345.4889733651577
At time: 5.839376211166382 and batch: 350, loss is 5.831901893615723 and perplexity is 341.0066210335602
At time: 6.632701873779297 and batch: 400, loss is 5.686136684417725 and perplexity is 294.7526955011472
At time: 7.431853532791138 and batch: 450, loss is 5.684522161483764 and perplexity is 294.2771944713629
At time: 8.239200830459595 and batch: 500, loss is 5.62568398475647 and perplexity is 277.46199941278934
At time: 9.036654710769653 and batch: 550, loss is 5.6646022129058835 and perplexity is 288.4732073714806
At time: 9.838590860366821 and batch: 600, loss is 5.589464626312256 and perplexity is 267.5923194705932
At time: 10.634068965911865 and batch: 650, loss is 5.477173051834106 and perplexity is 239.16963064606796
At time: 11.436219692230225 and batch: 700, loss is 5.581913928985596 and perplexity is 265.5794198207842
At time: 12.231690406799316 and batch: 750, loss is 5.544483070373535 and perplexity is 255.8223019304659
At time: 13.027753353118896 and batch: 800, loss is 5.531580028533935 and perplexity is 252.54262050081547
At time: 13.82225775718689 and batch: 850, loss is 5.55943042755127 and perplexity is 259.67489053075354
At time: 14.61819314956665 and batch: 900, loss is 5.450314521789551 and perplexity is 232.83138493642227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.335816683834547 and perplexity of 207.64225765614623
finished 1 epochs...
Completing Train Step...
At time: 16.372993230819702 and batch: 50, loss is 5.2309972858428955 and perplexity is 186.9791822596653
At time: 17.040361404418945 and batch: 100, loss is 5.053938465118408 and perplexity is 156.63816518244985
At time: 17.693960189819336 and batch: 150, loss is 4.998434953689575 and perplexity is 148.1810673000874
At time: 18.33753228187561 and batch: 200, loss is 4.86101092338562 and perplexity is 129.15470164200133
At time: 18.977181673049927 and batch: 250, loss is 4.93254204750061 and perplexity is 138.7317271171193
At time: 19.611247777938843 and batch: 300, loss is 4.856337804794311 and perplexity is 128.55255445302393
At time: 20.246329069137573 and batch: 350, loss is 4.8316561794281006 and perplexity is 125.41850428928349
At time: 20.881287813186646 and batch: 400, loss is 4.680928506851196 and perplexity is 107.87018429215387
At time: 21.518020153045654 and batch: 450, loss is 4.692140941619873 and perplexity is 109.08647776005687
At time: 22.1520516872406 and batch: 500, loss is 4.598238763809204 and perplexity is 99.30925447209522
At time: 22.788298845291138 and batch: 550, loss is 4.657204084396362 and perplexity is 105.34114508795719
At time: 23.42028832435608 and batch: 600, loss is 4.60031023979187 and perplexity is 99.5151844234275
At time: 24.09018611907959 and batch: 650, loss is 4.465004644393921 and perplexity is 86.92143323619104
At time: 24.728963136672974 and batch: 700, loss is 4.519679164886474 and perplexity is 91.80613861970475
At time: 25.37898898124695 and batch: 750, loss is 4.5555336380004885 and perplexity is 95.15752135843628
At time: 26.01124906539917 and batch: 800, loss is 4.49614294052124 and perplexity is 89.67059859921966
At time: 26.64301109313965 and batch: 850, loss is 4.547817897796631 and perplexity is 94.42613586386334
At time: 27.275448322296143 and batch: 900, loss is 4.478459825515747 and perplexity is 88.09888048284846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.589430299523759 and perplexity of 98.4383338187499
finished 2 epochs...
Completing Train Step...
At time: 28.8942391872406 and batch: 50, loss is 4.525249137878418 and perplexity is 92.3189231044327
At time: 29.537912130355835 and batch: 100, loss is 4.390747890472412 and perplexity is 80.7007517398596
At time: 30.173843145370483 and batch: 150, loss is 4.389922580718994 and perplexity is 80.63417609888235
At time: 30.80854344367981 and batch: 200, loss is 4.290546908378601 and perplexity is 73.00638538705729
At time: 31.472332239151 and batch: 250, loss is 4.419781436920166 and perplexity is 83.0781255628958
At time: 32.12951111793518 and batch: 300, loss is 4.382583532333374 and perplexity is 80.0445642204627
At time: 32.78576397895813 and batch: 350, loss is 4.374108371734619 and perplexity is 79.36904031391067
At time: 33.44004154205322 and batch: 400, loss is 4.269953107833862 and perplexity is 71.51828189343165
At time: 34.095696449279785 and batch: 450, loss is 4.307280883789063 and perplexity is 74.2383515421992
At time: 34.74952459335327 and batch: 500, loss is 4.1883040809631344 and perplexity is 65.91091654414319
At time: 35.42656970024109 and batch: 550, loss is 4.26662516117096 and perplexity is 71.2806684675736
At time: 36.06842088699341 and batch: 600, loss is 4.256394982337952 and perplexity is 70.55517178951474
At time: 36.711132526397705 and batch: 650, loss is 4.112139201164245 and perplexity is 61.07723440992004
At time: 37.35392189025879 and batch: 700, loss is 4.141569781303406 and perplexity is 62.90148556439464
At time: 37.99763321876526 and batch: 750, loss is 4.2171952390670775 and perplexity is 67.84293397950319
At time: 38.64166212081909 and batch: 800, loss is 4.169636836051941 and perplexity is 64.69195405539624
At time: 39.309107542037964 and batch: 850, loss is 4.241076669692993 and perplexity is 69.48262141313023
At time: 39.98681712150574 and batch: 900, loss is 4.185364875793457 and perplexity is 65.71747525857218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.408323941165453 and perplexity of 82.13169053313705
finished 3 epochs...
Completing Train Step...
At time: 41.73796892166138 and batch: 50, loss is 4.263266940116882 and perplexity is 71.04169371558822
At time: 42.41751003265381 and batch: 100, loss is 4.12502923488617 and perplexity is 61.86961797516769
At time: 43.083213567733765 and batch: 150, loss is 4.131950278282165 and perplexity is 62.2993055139205
At time: 43.72454309463501 and batch: 200, loss is 4.037098693847656 and perplexity is 56.66171112640728
At time: 44.365416526794434 and batch: 250, loss is 4.175366005897522 and perplexity is 65.06364898281709
At time: 45.000781297683716 and batch: 300, loss is 4.149682197570801 and perplexity is 63.41384402176121
At time: 45.63648700714111 and batch: 350, loss is 4.142815599441528 and perplexity is 62.9798982098318
At time: 46.27791881561279 and batch: 400, loss is 4.05779417514801 and perplexity is 57.846570840895275
At time: 46.94591808319092 and batch: 450, loss is 4.097290182113648 and perplexity is 60.176997749559426
At time: 47.63333177566528 and batch: 500, loss is 3.9717956256866453 and perplexity is 53.07975669365529
At time: 48.27756690979004 and batch: 550, loss is 4.054974756240845 and perplexity is 57.683706824181264
At time: 48.93899202346802 and batch: 600, loss is 4.058778944015503 and perplexity is 57.9035644010866
At time: 49.62765717506409 and batch: 650, loss is 3.9137454271316527 and perplexity is 50.08619529618989
At time: 50.283398151397705 and batch: 700, loss is 3.9294068956375123 and perplexity is 50.87679347073949
At time: 50.93535017967224 and batch: 750, loss is 4.022448220252991 and perplexity is 55.83764146865952
At time: 51.599326372146606 and batch: 800, loss is 3.9807509899139406 and perplexity is 53.55724007562361
At time: 52.280107736587524 and batch: 850, loss is 4.056363825798035 and perplexity is 57.76388918181986
At time: 52.94270062446594 and batch: 900, loss is 4.007683024406433 and perplexity is 55.01924452061526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338663597629495 and perplexity of 76.60509566911622
finished 4 epochs...
Completing Train Step...
At time: 54.57828140258789 and batch: 50, loss is 4.094603462219238 and perplexity is 60.01553601087806
At time: 55.216869592666626 and batch: 100, loss is 3.9587964725494387 and perplexity is 52.394230093749314
At time: 55.85630536079407 and batch: 150, loss is 3.9704332447052 and perplexity is 53.007491080462195
At time: 56.5136239528656 and batch: 200, loss is 3.8686706399917603 and perplexity is 47.87869573314383
At time: 57.190046310424805 and batch: 250, loss is 4.014850401878357 and perplexity is 55.41500480175102
At time: 57.850637674331665 and batch: 300, loss is 3.99567232131958 and perplexity is 54.36237732549386
At time: 58.498210430145264 and batch: 350, loss is 3.9879026651382445 and perplexity is 53.941636965032785
At time: 59.13765740394592 and batch: 400, loss is 3.906706805229187 and perplexity is 49.734895289275144
At time: 59.778573989868164 and batch: 450, loss is 3.952607970237732 and perplexity is 52.07098949888819
At time: 60.421472787857056 and batch: 500, loss is 3.829798946380615 and perplexity is 46.053278125104846
At time: 61.07111167907715 and batch: 550, loss is 3.9079955434799194 and perplexity is 49.799031869986266
At time: 61.71179485321045 and batch: 600, loss is 3.922028160095215 and perplexity is 50.502768678496054
At time: 62.354180335998535 and batch: 650, loss is 3.7739589977264405 and perplexity is 43.552146825249125
At time: 62.998393535614014 and batch: 700, loss is 3.7853038692474366 and perplexity is 44.04905367807919
At time: 63.6314914226532 and batch: 750, loss is 3.883994736671448 and perplexity is 48.618043948980734
At time: 64.26453042030334 and batch: 800, loss is 3.845086765289307 and perplexity is 46.76274155769731
At time: 64.89790225028992 and batch: 850, loss is 3.9209229707717896 and perplexity is 50.446984389527564
At time: 65.53224539756775 and batch: 900, loss is 3.876967568397522 and perplexity is 48.27759437221438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311026795269692 and perplexity of 74.51696333731851
finished 5 epochs...
Completing Train Step...
At time: 67.17697620391846 and batch: 50, loss is 3.966839065551758 and perplexity is 52.817314629888465
At time: 67.85434532165527 and batch: 100, loss is 3.834701280593872 and perplexity is 46.27960098798147
At time: 68.51483178138733 and batch: 150, loss is 3.8493272066116333 and perplexity is 46.96145724268037
At time: 69.18239498138428 and batch: 200, loss is 3.746705312728882 and perplexity is 42.38121886161141
At time: 69.81718897819519 and batch: 250, loss is 3.893668007850647 and perplexity is 49.09062147256772
At time: 70.45344805717468 and batch: 300, loss is 3.879175567626953 and perplexity is 48.38430903297623
At time: 71.08474063873291 and batch: 350, loss is 3.870449357032776 and perplexity is 47.9639341702029
At time: 71.71880388259888 and batch: 400, loss is 3.796316213607788 and perplexity is 44.53681780989961
At time: 72.35655760765076 and batch: 450, loss is 3.839153695106506 and perplexity is 46.48611635991562
At time: 72.9918303489685 and batch: 500, loss is 3.716940846443176 and perplexity is 41.1383528786373
At time: 73.62578010559082 and batch: 550, loss is 3.794359874725342 and perplexity is 44.44977387299847
At time: 74.26207327842712 and batch: 600, loss is 3.8137414169311525 and perplexity is 45.31968188967389
At time: 74.90783834457397 and batch: 650, loss is 3.6663907527923585 and perplexity is 39.1104913821021
At time: 75.54550528526306 and batch: 700, loss is 3.670986742973328 and perplexity is 39.290656517927275
At time: 76.18081164360046 and batch: 750, loss is 3.7756572914123536 and perplexity is 43.62617400336255
At time: 76.81963610649109 and batch: 800, loss is 3.7387513875961305 and perplexity is 42.04545889472157
At time: 77.4544358253479 and batch: 850, loss is 3.8156083726882937 and perplexity is 45.40437076127907
At time: 78.08737516403198 and batch: 900, loss is 3.7796741914749146 and perplexity is 43.80176842095107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306640625 and perplexity of 74.19083499820444
finished 6 epochs...
Completing Train Step...
At time: 79.73956370353699 and batch: 50, loss is 3.8669936418533326 and perplexity is 47.79847053708282
At time: 80.38417601585388 and batch: 100, loss is 3.736279878616333 and perplexity is 41.94167147410546
At time: 81.05289340019226 and batch: 150, loss is 3.753181495666504 and perplexity is 42.656578063961156
At time: 81.69834542274475 and batch: 200, loss is 3.649778437614441 and perplexity is 38.46614245452756
At time: 82.3375768661499 and batch: 250, loss is 3.796416563987732 and perplexity is 44.54128732074313
At time: 82.97324776649475 and batch: 300, loss is 3.7841232776641847 and perplexity is 43.99708042168785
At time: 83.61513090133667 and batch: 350, loss is 3.776513833999634 and perplexity is 43.663557687389655
At time: 84.2456886768341 and batch: 400, loss is 3.7028021955490114 and perplexity is 40.56080456693213
At time: 84.89781832695007 and batch: 450, loss is 3.7490560483932494 and perplexity is 42.4809630945197
At time: 85.55781888961792 and batch: 500, loss is 3.6286960315704344 and perplexity is 37.66367234317732
At time: 86.19061636924744 and batch: 550, loss is 3.70356822013855 and perplexity is 40.59188704405283
At time: 86.82057666778564 and batch: 600, loss is 3.724450345039368 and perplexity is 41.448444139537735
At time: 87.45448684692383 and batch: 650, loss is 3.5797393560409545 and perplexity is 35.864191843784944
At time: 88.08366441726685 and batch: 700, loss is 3.580902781486511 and perplexity is 35.90594143872166
At time: 88.71159482002258 and batch: 750, loss is 3.68967173576355 and perplexity is 40.031703823504934
At time: 89.33890557289124 and batch: 800, loss is 3.6539497232437133 and perplexity is 38.6269308357556
At time: 89.9648687839508 and batch: 850, loss is 3.7299366998672485 and perplexity is 41.676469954449935
At time: 90.60610675811768 and batch: 900, loss is 3.691729950904846 and perplexity is 40.114182532793805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312721984027183 and perplexity of 74.64339078472823
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 92.20709252357483 and batch: 50, loss is 3.80599916934967 and perplexity is 44.97016047874676
At time: 92.83937621116638 and batch: 100, loss is 3.677767825126648 and perplexity is 39.55799508560832
At time: 93.47406506538391 and batch: 150, loss is 3.6937142181396485 and perplexity is 40.19385881421927
At time: 94.10654520988464 and batch: 200, loss is 3.5735736894607544 and perplexity is 35.643745492421886
At time: 94.74298214912415 and batch: 250, loss is 3.723005185127258 and perplexity is 41.38858777107749
At time: 95.3780460357666 and batch: 300, loss is 3.6910516834259033 and perplexity is 40.08698361245173
At time: 96.01268911361694 and batch: 350, loss is 3.6703234577178954 and perplexity is 39.2646042457818
At time: 96.64751386642456 and batch: 400, loss is 3.5893906354904175 and perplexity is 36.212002892514136
At time: 97.27785730361938 and batch: 450, loss is 3.6256881666183474 and perplexity is 37.5505553088341
At time: 97.90809226036072 and batch: 500, loss is 3.4947022676467894 and perplexity is 32.94047904652025
At time: 98.53739500045776 and batch: 550, loss is 3.550782208442688 and perplexity is 34.84055941150123
At time: 99.164794921875 and batch: 600, loss is 3.5628919887542723 and perplexity is 35.26503590310776
At time: 99.79421639442444 and batch: 650, loss is 3.4096270561218263 and perplexity is 30.253959126323554
At time: 100.42722272872925 and batch: 700, loss is 3.4004242038726806 and perplexity is 29.97681363106357
At time: 101.06420421600342 and batch: 750, loss is 3.4959667110443116 and perplexity is 32.982156761764855
At time: 101.6995165348053 and batch: 800, loss is 3.4384857034683227 and perplexity is 31.139767604992507
At time: 102.34167742729187 and batch: 850, loss is 3.5000743579864504 and perplexity is 33.117914448572115
At time: 102.99334001541138 and batch: 900, loss is 3.4586893558502196 and perplexity is 31.775303096011733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273095953954409 and perplexity of 71.7434064288097
finished 8 epochs...
Completing Train Step...
At time: 104.63009524345398 and batch: 50, loss is 3.7148569059371948 and perplexity is 41.052712264689724
At time: 105.27598237991333 and batch: 100, loss is 3.5817343950271607 and perplexity is 35.93581372518924
At time: 105.90973424911499 and batch: 150, loss is 3.598829436302185 and perplexity is 36.55541894305834
At time: 106.55129027366638 and batch: 200, loss is 3.4827262783050537 and perplexity is 32.548337051603646
At time: 107.19149851799011 and batch: 250, loss is 3.633939151763916 and perplexity is 37.86166610316549
At time: 107.82466793060303 and batch: 300, loss is 3.6058358669281008 and perplexity is 36.81244130311541
At time: 108.45894312858582 and batch: 350, loss is 3.5898308420181273 and perplexity is 36.22794716169708
At time: 109.0963544845581 and batch: 400, loss is 3.513348937034607 and perplexity is 33.56047170373378
At time: 109.73276710510254 and batch: 450, loss is 3.5558525848388673 and perplexity is 35.017662772496784
At time: 110.36634349822998 and batch: 500, loss is 3.427955026626587 and perplexity is 30.81356535517171
At time: 111.00120210647583 and batch: 550, loss is 3.486246953010559 and perplexity is 32.66313111627328
At time: 111.64358043670654 and batch: 600, loss is 3.504335799217224 and perplexity is 33.259345630985464
At time: 112.27510690689087 and batch: 650, loss is 3.356822929382324 and perplexity is 28.697870728890578
At time: 112.90340614318848 and batch: 700, loss is 3.3519917726516724 and perplexity is 28.559561184120216
At time: 113.53207731246948 and batch: 750, loss is 3.4536749505996704 and perplexity is 31.616367665683477
At time: 114.16628122329712 and batch: 800, loss is 3.4007858753204347 and perplexity is 29.98765734946204
At time: 114.79556918144226 and batch: 850, loss is 3.4690735864639284 and perplexity is 32.10698431784118
At time: 115.42467188835144 and batch: 900, loss is 3.4362111949920653 and perplexity is 31.069020427653747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280033477365154 and perplexity of 72.24285846739599
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 117.04397058486938 and batch: 50, loss is 3.684508547782898 and perplexity is 39.82554528710676
At time: 117.68272185325623 and batch: 100, loss is 3.5648461771011353 and perplexity is 35.33401780518327
At time: 118.31426191329956 and batch: 150, loss is 3.585568222999573 and perplexity is 36.073849887436296
At time: 118.94949674606323 and batch: 200, loss is 3.466370620727539 and perplexity is 32.020317420923554
At time: 119.58487391471863 and batch: 250, loss is 3.615831522941589 and perplexity is 37.182250969440354
At time: 120.21956300735474 and batch: 300, loss is 3.5829954528808594 and perplexity is 35.98115945109902
At time: 120.85786986351013 and batch: 350, loss is 3.5652928018569945 and perplexity is 35.34980237688689
At time: 121.49323916435242 and batch: 400, loss is 3.484085354804993 and perplexity is 32.59260280506165
At time: 122.13625454902649 and batch: 450, loss is 3.521222038269043 and perplexity is 33.82573956506234
At time: 122.78780603408813 and batch: 500, loss is 3.389628348350525 and perplexity is 29.654928918824595
At time: 123.42356610298157 and batch: 550, loss is 3.441112504005432 and perplexity is 31.22167309085368
At time: 124.08977031707764 and batch: 600, loss is 3.4594226837158204 and perplexity is 31.79861335719736
At time: 124.7440185546875 and batch: 650, loss is 3.3058492088317872 and perplexity is 27.271691118973163
At time: 125.38006567955017 and batch: 700, loss is 3.2950233316421507 and perplexity is 26.97804350461389
At time: 126.08112096786499 and batch: 750, loss is 3.3943128633499144 and perplexity is 29.794173770809287
At time: 126.74316501617432 and batch: 800, loss is 3.336370768547058 and perplexity is 28.11689858016514
At time: 127.4159197807312 and batch: 850, loss is 3.401719818115234 and perplexity is 30.01567718840314
At time: 128.0613808631897 and batch: 900, loss is 3.3701439619064333 and perplexity is 29.082713559329726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266777561135488 and perplexity of 71.29153246673512
finished 10 epochs...
Completing Train Step...
At time: 129.75204229354858 and batch: 50, loss is 3.6595366859436034 and perplexity is 38.84334203527192
At time: 130.38206338882446 and batch: 100, loss is 3.5310706758499144 and perplexity is 34.160522888715164
At time: 131.01994943618774 and batch: 150, loss is 3.5494627952575684 and perplexity is 34.79462063081761
At time: 131.6536247730255 and batch: 200, loss is 3.430790214538574 and perplexity is 30.901051564510215
At time: 132.29953837394714 and batch: 250, loss is 3.580966582298279 and perplexity is 35.908232340012646
At time: 132.9331398010254 and batch: 300, loss is 3.5497312355041504 and perplexity is 34.80396216112473
At time: 133.56378865242004 and batch: 350, loss is 3.533626503944397 and perplexity is 34.247942980612265
At time: 134.1996202468872 and batch: 400, loss is 3.4562351655960084 and perplexity is 31.69741607071122
At time: 134.8472216129303 and batch: 450, loss is 3.495728645324707 and perplexity is 32.974305775443696
At time: 135.54007959365845 and batch: 500, loss is 3.366247549057007 and perplexity is 28.969615781426317
At time: 136.17249751091003 and batch: 550, loss is 3.419943370819092 and perplexity is 30.56768394913178
At time: 136.7995638847351 and batch: 600, loss is 3.440897855758667 and perplexity is 31.214972132663856
At time: 137.4266595840454 and batch: 650, loss is 3.289984636306763 and perplexity is 26.842451253543768
At time: 138.0658528804779 and batch: 700, loss is 3.2828557777404783 and perplexity is 26.651775672131162
At time: 138.76954889297485 and batch: 750, loss is 3.385255813598633 and perplexity is 29.525544786012528
At time: 139.43318605422974 and batch: 800, loss is 3.3304337644577027 and perplexity is 27.9504629915765
At time: 140.07678651809692 and batch: 850, loss is 3.398989462852478 and perplexity is 29.933835505504444
At time: 140.73095703125 and batch: 900, loss is 3.3700631713867186 and perplexity is 29.080364046697007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268179279484161 and perplexity of 71.39153318594671
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 142.32502341270447 and batch: 50, loss is 3.649839882850647 and perplexity is 38.468506088352875
At time: 142.95919919013977 and batch: 100, loss is 3.5292365646362303 and perplexity is 34.09792611289167
At time: 143.5816090106964 and batch: 150, loss is 3.5512762689590454 and perplexity is 34.857777009191636
At time: 144.2094657421112 and batch: 200, loss is 3.433409128189087 and perplexity is 30.98208481349339
At time: 144.83533120155334 and batch: 250, loss is 3.5799304628372193 and perplexity is 35.87104638954311
At time: 145.46551084518433 and batch: 300, loss is 3.5467878913879396 and perplexity is 34.70167273421516
At time: 146.09116291999817 and batch: 350, loss is 3.5298243236541746 and perplexity is 34.1179733673579
At time: 146.7194743156433 and batch: 400, loss is 3.4531548595428467 and perplexity is 31.599928550900152
At time: 147.35869455337524 and batch: 450, loss is 3.487040638923645 and perplexity is 32.68906567390265
At time: 147.98731470108032 and batch: 500, loss is 3.35570113658905 and perplexity is 28.66569571454834
At time: 148.6134934425354 and batch: 550, loss is 3.408767833709717 and perplexity is 30.227975411080987
At time: 149.24296522140503 and batch: 600, loss is 3.429623804092407 and perplexity is 30.86502926764358
At time: 149.87035942077637 and batch: 650, loss is 3.2760014390945433 and perplexity is 26.469720024466643
At time: 150.50276517868042 and batch: 700, loss is 3.2651868772506716 and perplexity is 26.185003915161083
At time: 151.13722848892212 and batch: 750, loss is 3.3659255838394166 and perplexity is 28.96029007413499
At time: 151.76746249198914 and batch: 800, loss is 3.309497060775757 and perplexity is 27.371355881066492
At time: 152.40069842338562 and batch: 850, loss is 3.3741150188446043 and perplexity is 29.198432281540867
At time: 153.03627943992615 and batch: 900, loss is 3.345412278175354 and perplexity is 28.37227052364289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2645284574325775 and perplexity of 71.13137059492198
finished 12 epochs...
Completing Train Step...
At time: 154.691082239151 and batch: 50, loss is 3.6418975257873534 and perplexity is 38.164185588081125
At time: 155.33702397346497 and batch: 100, loss is 3.516347675323486 and perplexity is 33.66126182128793
At time: 155.95836305618286 and batch: 150, loss is 3.536762399673462 and perplexity is 34.355509529325424
At time: 156.58515620231628 and batch: 200, loss is 3.418546233177185 and perplexity is 30.525006507332918
At time: 157.2106418609619 and batch: 250, loss is 3.5656167173385622 and perplexity is 35.361254579819914
At time: 157.839506149292 and batch: 300, loss is 3.534339680671692 and perplexity is 34.272376528186896
At time: 158.4764974117279 and batch: 350, loss is 3.518042316436768 and perplexity is 33.71835394115888
At time: 159.1103127002716 and batch: 400, loss is 3.442750163078308 and perplexity is 31.272845437053093
At time: 159.74650692939758 and batch: 450, loss is 3.477569499015808 and perplexity is 32.38092448701075
At time: 160.37961173057556 and batch: 500, loss is 3.3478964042663573 and perplexity is 28.44283843461724
At time: 161.01879787445068 and batch: 550, loss is 3.401552228927612 and perplexity is 30.010647306935855
At time: 161.66247248649597 and batch: 600, loss is 3.424072570800781 and perplexity is 30.69416498216296
At time: 162.31793475151062 and batch: 650, loss is 3.271764941215515 and perplexity is 26.35781831476922
At time: 162.97295713424683 and batch: 700, loss is 3.262682614326477 and perplexity is 26.119511819626688
At time: 163.6294960975647 and batch: 750, loss is 3.3646618700027466 and perplexity is 28.923715669512283
At time: 164.28608298301697 and batch: 800, loss is 3.309784779548645 and perplexity is 27.379232267030588
At time: 164.9280002117157 and batch: 850, loss is 3.376608338356018 and perplexity is 29.271324136030756
At time: 165.57672929763794 and batch: 900, loss is 3.349056935310364 and perplexity is 28.475866392878793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264224535798373 and perplexity of 71.10975551733881
finished 13 epochs...
Completing Train Step...
At time: 167.3081932067871 and batch: 50, loss is 3.6363620948791504 and perplexity is 37.953513992553155
At time: 167.93151378631592 and batch: 100, loss is 3.509785351753235 and perplexity is 33.441088942398196
At time: 168.55354928970337 and batch: 150, loss is 3.529707193374634 and perplexity is 34.11397735363168
At time: 169.18209886550903 and batch: 200, loss is 3.41142719745636 and perplexity is 30.308469577217917
At time: 169.8172926902771 and batch: 250, loss is 3.5583728551864624 and perplexity is 35.10602805505215
At time: 170.45068860054016 and batch: 300, loss is 3.5275677108764647 and perplexity is 34.041069116899635
At time: 171.09602761268616 and batch: 350, loss is 3.511477198600769 and perplexity is 33.497714030285
At time: 171.747487783432 and batch: 400, loss is 3.4369249820709227 and perplexity is 31.09120500957982
At time: 172.40117502212524 and batch: 450, loss is 3.4720582580566406 and perplexity is 32.20295627300114
At time: 173.040189743042 and batch: 500, loss is 3.3430366134643554 and perplexity is 28.304947521891748
At time: 173.67710280418396 and batch: 550, loss is 3.397127585411072 and perplexity is 29.87815422438793
At time: 174.30664563179016 and batch: 600, loss is 3.4203944206237793 and perplexity is 30.58147460690973
At time: 174.93352222442627 and batch: 650, loss is 3.268667778968811 and perplexity is 26.27631016212081
At time: 175.5604054927826 and batch: 700, loss is 3.2604384517669676 and perplexity is 26.060961112354033
At time: 176.19221591949463 and batch: 750, loss is 3.3630905103683473 and perplexity is 28.87830180033735
At time: 176.84469151496887 and batch: 800, loss is 3.3089588260650635 and perplexity is 27.356627631235767
At time: 177.51368188858032 and batch: 850, loss is 3.376563320159912 and perplexity is 29.270006423481263
At time: 178.15478610992432 and batch: 900, loss is 3.3495163106918335 and perplexity is 28.488950509896235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264605796500428 and perplexity of 71.13687204155434
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 179.81500840187073 and batch: 50, loss is 3.6338786125183105 and perplexity is 37.859374055842345
At time: 180.46628403663635 and batch: 100, loss is 3.509066467285156 and perplexity is 33.41705730198334
At time: 181.09492945671082 and batch: 150, loss is 3.530638771057129 and perplexity is 34.14577198087868
At time: 181.73427724838257 and batch: 200, loss is 3.4131194496154786 and perplexity is 30.35980257223511
At time: 182.36017537117004 and batch: 250, loss is 3.558713846206665 and perplexity is 35.11800093658044
At time: 182.98935985565186 and batch: 300, loss is 3.527665419578552 and perplexity is 34.04439538808089
At time: 183.61824131011963 and batch: 350, loss is 3.5108942461013792 and perplexity is 33.478192144885995
At time: 184.24654936790466 and batch: 400, loss is 3.4365565729141236 and perplexity is 31.079752834630963
At time: 184.8781135082245 and batch: 450, loss is 3.470022349357605 and perplexity is 32.13746068832842
At time: 185.50749111175537 and batch: 500, loss is 3.3392961168289186 and perplexity is 28.19927072598275
At time: 186.1372139453888 and batch: 550, loss is 3.392105989456177 and perplexity is 29.728494286355332
At time: 186.76711511611938 and batch: 600, loss is 3.414873728752136 and perplexity is 30.413108883884984
At time: 187.4372673034668 and batch: 650, loss is 3.2631179666519166 and perplexity is 26.130885485432994
At time: 188.06815028190613 and batch: 700, loss is 3.252600908279419 and perplexity is 25.857505533753756
At time: 188.69895315170288 and batch: 750, loss is 3.355896954536438 and perplexity is 28.671309521867784
At time: 189.32381749153137 and batch: 800, loss is 3.300869221687317 and perplexity is 27.136216059803985
At time: 189.95554447174072 and batch: 850, loss is 3.3663443183898925 and perplexity is 28.97241928746395
At time: 190.59225988388062 and batch: 900, loss is 3.3387884426116945 and perplexity is 28.184958316618655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264056898143194 and perplexity of 71.09783584378447
finished 15 epochs...
Completing Train Step...
At time: 192.216881275177 and batch: 50, loss is 3.631427068710327 and perplexity is 37.766673817577065
At time: 192.85388255119324 and batch: 100, loss is 3.5058844661712647 and perplexity is 33.310893185171544
At time: 193.4818994998932 and batch: 150, loss is 3.5269634771347045 and perplexity is 34.02050656725238
At time: 194.11247634887695 and batch: 200, loss is 3.409317684173584 and perplexity is 30.24460084771569
At time: 194.74499440193176 and batch: 250, loss is 3.5550972557067873 and perplexity is 34.99122289832801
At time: 195.37516140937805 and batch: 300, loss is 3.5244724941253662 and perplexity is 33.935867524398525
At time: 195.9956018924713 and batch: 350, loss is 3.50767578125 and perplexity is 33.37061696650513
At time: 196.61780261993408 and batch: 400, loss is 3.4338938426971435 and perplexity is 30.997105919672272
At time: 197.238046169281 and batch: 450, loss is 3.467692642211914 and perplexity is 32.06267696243303
At time: 197.85731720924377 and batch: 500, loss is 3.3374181842803954 and perplexity is 28.1463640907365
At time: 198.47572946548462 and batch: 550, loss is 3.390421199798584 and perplexity is 29.678450195366903
At time: 199.09647965431213 and batch: 600, loss is 3.4138037252426146 and perplexity is 30.380584154534194
At time: 199.7160165309906 and batch: 650, loss is 3.2624192953109743 and perplexity is 26.112634960932493
At time: 200.33478999137878 and batch: 700, loss is 3.2524262046813965 and perplexity is 25.852988529081095
At time: 200.95401430130005 and batch: 750, loss is 3.3560809183120726 and perplexity is 28.676584489406412
At time: 201.5849735736847 and batch: 800, loss is 3.301472015380859 and perplexity is 27.152578530813145
At time: 202.20678639411926 and batch: 850, loss is 3.3676171875 and perplexity is 29.009320865477118
At time: 202.83610081672668 and batch: 900, loss is 3.3403383779525755 and perplexity is 28.228677051448802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263879645360659 and perplexity of 71.08523467137842
finished 16 epochs...
Completing Train Step...
At time: 204.40382838249207 and batch: 50, loss is 3.62963041305542 and perplexity is 37.69888102788126
At time: 205.02352571487427 and batch: 100, loss is 3.5036451292037962 and perplexity is 33.23638232923671
At time: 205.64800381660461 and batch: 150, loss is 3.5246120738983153 and perplexity is 33.94060461567578
At time: 206.2679226398468 and batch: 200, loss is 3.406902756690979 and perplexity is 30.171650450340987
At time: 206.88712453842163 and batch: 250, loss is 3.552659502029419 and perplexity is 34.906026801771674
At time: 207.53726148605347 and batch: 300, loss is 3.522251749038696 and perplexity is 33.86058823232879
At time: 208.1654555797577 and batch: 350, loss is 3.505570092201233 and perplexity is 33.300422753337465
At time: 208.81540155410767 and batch: 400, loss is 3.432009291648865 and perplexity is 30.938745300273457
At time: 209.47541856765747 and batch: 450, loss is 3.465987277030945 and perplexity is 32.00804498650778
At time: 210.0976448059082 and batch: 500, loss is 3.33604736328125 and perplexity is 28.107806897334424
At time: 210.71855282783508 and batch: 550, loss is 3.3892028760910033 and perplexity is 29.642314252997036
At time: 211.3434407711029 and batch: 600, loss is 3.4130055141448974 and perplexity is 30.35634371088953
At time: 211.96344566345215 and batch: 650, loss is 3.261858973503113 and perplexity is 26.098007580505985
At time: 212.5910894870758 and batch: 700, loss is 3.252195143699646 and perplexity is 25.84701560225212
At time: 213.21948432922363 and batch: 750, loss is 3.356052279472351 and perplexity is 28.675763237059375
At time: 213.85253620147705 and batch: 800, loss is 3.301721134185791 and perplexity is 27.15934359134447
At time: 214.48426127433777 and batch: 850, loss is 3.3682559537887573 and perplexity is 29.027856961191997
At time: 215.11884689331055 and batch: 900, loss is 3.3411075496673583 and perplexity is 28.25039810392121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2638570707138275 and perplexity of 71.08362996542365
finished 17 epochs...
Completing Train Step...
At time: 216.7432882785797 and batch: 50, loss is 3.6280542993545533 and perplexity is 37.63951010493886
At time: 217.42139720916748 and batch: 100, loss is 3.501791763305664 and perplexity is 33.17484019930912
At time: 218.04826498031616 and batch: 150, loss is 3.52269953250885 and perplexity is 33.87575383922955
At time: 218.68612241744995 and batch: 200, loss is 3.4049777507781984 and perplexity is 30.113625711726463
At time: 219.31590819358826 and batch: 250, loss is 3.5507068061828613 and perplexity is 34.83793245362851
At time: 219.95043230056763 and batch: 300, loss is 3.5204496812820434 and perplexity is 33.79962410529598
At time: 220.5842649936676 and batch: 350, loss is 3.503867540359497 and perplexity is 33.24377529355027
At time: 221.21698904037476 and batch: 400, loss is 3.430473222732544 and perplexity is 30.891257736730147
At time: 221.86039805412292 and batch: 450, loss is 3.464561185836792 and perplexity is 31.962431127902054
At time: 222.53567934036255 and batch: 500, loss is 3.3348653507232666 and perplexity is 28.07460274433325
At time: 223.2128336429596 and batch: 550, loss is 3.388146924972534 and perplexity is 29.611029938367995
At time: 223.8556625843048 and batch: 600, loss is 3.412251563072205 and perplexity is 30.333465138731448
At time: 224.4964771270752 and batch: 650, loss is 3.261280031204224 and perplexity is 26.082902712846202
At time: 225.1372184753418 and batch: 700, loss is 3.2518545627593993 and perplexity is 25.838214100272726
At time: 225.77617287635803 and batch: 750, loss is 3.355874276161194 and perplexity is 28.67065931052461
At time: 226.41685557365417 and batch: 800, loss is 3.3017432069778443 and perplexity is 27.15994308050404
At time: 227.0572373867035 and batch: 850, loss is 3.3685347509384154 and perplexity is 29.035950973214774
At time: 227.69591903686523 and batch: 900, loss is 3.3414754915237426 and perplexity is 28.260794520364254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263921868311216 and perplexity of 71.0882361630927
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 229.33508253097534 and batch: 50, loss is 3.627273859977722 and perplexity is 37.610146208989086
At time: 229.9844651222229 and batch: 100, loss is 3.5014165258407592 and perplexity is 33.1623940916431
At time: 230.62076711654663 and batch: 150, loss is 3.522776417732239 and perplexity is 33.87835848425902
At time: 231.25469708442688 and batch: 200, loss is 3.4053719806671143 and perplexity is 30.12549974344097
At time: 231.88768696784973 and batch: 250, loss is 3.5506943368911745 and perplexity is 34.837498051995325
At time: 232.5223515033722 and batch: 300, loss is 3.5205143690109253 and perplexity is 33.80181059693524
At time: 233.15743613243103 and batch: 350, loss is 3.5034751749038695 and perplexity is 33.230734143126206
At time: 233.7935619354248 and batch: 400, loss is 3.430025701522827 and perplexity is 30.87743633661365
At time: 234.43622064590454 and batch: 450, loss is 3.4640445709228516 and perplexity is 31.945923123803297
At time: 235.07289695739746 and batch: 500, loss is 3.333442544937134 and perplexity is 28.034686440413125
At time: 235.6986792087555 and batch: 550, loss is 3.386478590965271 and perplexity is 29.561670035982377
At time: 236.32549214363098 and batch: 600, loss is 3.4102479124069216 and perplexity is 30.272748319086816
At time: 236.95169043540955 and batch: 650, loss is 3.2592606258392336 and perplexity is 26.03028390640499
At time: 237.57927060127258 and batch: 700, loss is 3.249241714477539 and perplexity is 25.770790888623896
At time: 238.2104685306549 and batch: 750, loss is 3.353357572555542 and perplexity is 28.598594479808042
At time: 238.84218406677246 and batch: 800, loss is 3.298744249343872 and perplexity is 27.07861357469041
At time: 239.47661352157593 and batch: 850, loss is 3.3651982593536376 and perplexity is 28.939234204201906
At time: 240.12909317016602 and batch: 900, loss is 3.338179936408997 and perplexity is 28.167812811762296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263393454355736 and perplexity of 71.05068206996096
finished 19 epochs...
Completing Train Step...
At time: 241.75554704666138 and batch: 50, loss is 3.6265923452377318 and perplexity is 37.58452307223891
At time: 242.4114089012146 and batch: 100, loss is 3.5007238960266114 and perplexity is 33.13943278154863
At time: 243.04631209373474 and batch: 150, loss is 3.521967206001282 and perplexity is 33.85095480833139
At time: 243.67980074882507 and batch: 200, loss is 3.404574770927429 and perplexity is 30.10149297212002
At time: 244.31509637832642 and batch: 250, loss is 3.5499636459350588 and perplexity is 34.81205190500193
At time: 244.94854974746704 and batch: 300, loss is 3.5198048830032347 and perplexity is 33.77783719068588
At time: 245.58181500434875 and batch: 350, loss is 3.5028667402267457 and perplexity is 33.210521561768495
At time: 246.21324920654297 and batch: 400, loss is 3.429527997970581 and perplexity is 30.862072350537037
At time: 246.8459436893463 and batch: 450, loss is 3.463546895980835 and perplexity is 31.930028393897448
At time: 247.4818525314331 and batch: 500, loss is 3.3331317138671874 and perplexity is 28.02597374298992
At time: 248.11591386795044 and batch: 550, loss is 3.3861608505249023 and perplexity is 29.552278590027214
At time: 248.7486011981964 and batch: 600, loss is 3.410051155090332 and perplexity is 30.266792520304463
At time: 249.37874007225037 and batch: 650, loss is 3.2591598796844483 and perplexity is 26.02766158748994
At time: 250.0102858543396 and batch: 700, loss is 3.249193062782288 and perplexity is 25.769537126458275
At time: 250.64099764823914 and batch: 750, loss is 3.35347373008728 and perplexity is 28.601916614895767
At time: 251.25969004631042 and batch: 800, loss is 3.2989741373062134 and perplexity is 27.08483933757468
At time: 251.87760639190674 and batch: 850, loss is 3.365489845275879 and perplexity is 28.947673707856726
At time: 252.49677419662476 and batch: 900, loss is 3.3384693336486815 and perplexity is 28.175965678689668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263184429848031 and perplexity of 71.03583228815741
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f74eca12b70>
ELAPSED
786.6585190296173


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.7917024081235}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.41628114957778306, 'tune_wordvecs': 'TRUE', 'dropout': 0.7387750863627058, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.03583228815741}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.09083670716130898, 'tune_wordvecs': 'TRUE', 'dropout': 0.021912257622493048, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.024888277053833 and batch: 50, loss is 7.065455551147461 and perplexity is 1170.8152162017254
At time: 1.8238632678985596 and batch: 100, loss is 6.0907525634765625 and perplexity is 441.7537338090368
At time: 2.6197478771209717 and batch: 150, loss is 5.788867568969726 and perplexity is 326.6429142814942
At time: 3.4159700870513916 and batch: 200, loss is 5.545988960266113 and perplexity is 256.20783235957157
At time: 4.212464094161987 and batch: 250, loss is 5.508925323486328 and perplexity is 246.88566222600127
At time: 5.009570121765137 and batch: 300, loss is 5.354228210449219 and perplexity is 211.50067933124143
At time: 5.806979656219482 and batch: 350, loss is 5.271075820922851 and perplexity is 194.62523175349403
At time: 6.617289781570435 and batch: 400, loss is 5.071286659240723 and perplexity is 159.37926227489862
At time: 7.41016697883606 and batch: 450, loss is 5.044581260681152 and perplexity is 155.1793059148301
At time: 8.207108497619629 and batch: 500, loss is 4.951435413360596 and perplexity is 141.3777538597455
At time: 9.002165794372559 and batch: 550, loss is 4.988967695236206 and perplexity is 146.7848185904415
At time: 9.792068004608154 and batch: 600, loss is 4.890449657440185 and perplexity is 133.01337105813113
At time: 10.585710287094116 and batch: 650, loss is 4.7525492286682125 and perplexity is 115.87931118433448
At time: 11.379063129425049 and batch: 700, loss is 4.810264272689819 and perplexity is 122.76405641812134
At time: 12.174004077911377 and batch: 750, loss is 4.810199222564697 and perplexity is 122.75607086062443
At time: 12.967689514160156 and batch: 800, loss is 4.749000511169434 and perplexity is 115.46881703965822
At time: 13.762245655059814 and batch: 850, loss is 4.788351955413819 and perplexity is 120.10326992649235
At time: 14.554507970809937 and batch: 900, loss is 4.699331693649292 and perplexity is 109.87371860610112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.767892497859589 and perplexity of 117.6709885949788
finished 1 epochs...
Completing Train Step...
At time: 16.26386022567749 and batch: 50, loss is 4.735480909347534 and perplexity is 113.9182298887786
At time: 16.915884494781494 and batch: 100, loss is 4.614391040802002 and perplexity is 100.92634978633407
At time: 17.550787925720215 and batch: 150, loss is 4.6046835231781005 and perplexity is 99.95134555911466
At time: 18.187227725982666 and batch: 200, loss is 4.497192659378052 and perplexity is 89.76477693921252
At time: 18.82175898551941 and batch: 250, loss is 4.617278079986573 and perplexity is 101.21814912831124
At time: 19.45648980140686 and batch: 300, loss is 4.566301040649414 and perplexity is 96.18765669440488
At time: 20.089518547058105 and batch: 350, loss is 4.550097389221191 and perplexity is 94.64162494026202
At time: 20.717552185058594 and batch: 400, loss is 4.434832372665405 and perplexity is 84.33798635146394
At time: 21.39019751548767 and batch: 450, loss is 4.457093744277954 and perplexity is 86.23651917524167
At time: 22.042893886566162 and batch: 500, loss is 4.348308591842652 and perplexity is 77.34752598616974
At time: 22.705525636672974 and batch: 550, loss is 4.424309711456299 and perplexity is 83.45517918024038
At time: 23.347136974334717 and batch: 600, loss is 4.399951057434082 and perplexity is 81.4468823480105
At time: 23.977397441864014 and batch: 650, loss is 4.249995212554932 and perplexity is 70.10507672168067
At time: 24.61055898666382 and batch: 700, loss is 4.28896876335144 and perplexity is 72.89126158792669
At time: 25.245126724243164 and batch: 750, loss is 4.350310640335083 and perplexity is 77.5025345995686
At time: 25.87825036048889 and batch: 800, loss is 4.300819220542908 and perplexity is 73.76019482214475
At time: 26.51175093650818 and batch: 850, loss is 4.372401285171509 and perplexity is 79.23366607231327
At time: 27.14975118637085 and batch: 900, loss is 4.3063972425460815 and perplexity is 74.1727804478938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4802789557470035 and perplexity of 88.25928967804684
finished 2 epochs...
Completing Train Step...
At time: 28.800766229629517 and batch: 50, loss is 4.385887155532837 and perplexity is 80.30943858145268
At time: 29.44421362876892 and batch: 100, loss is 4.264944353103638 and perplexity is 71.16095997665037
At time: 30.07918381690979 and batch: 150, loss is 4.260523056983947 and perplexity is 70.84703079874701
At time: 30.699488401412964 and batch: 200, loss is 4.161396951675415 and perplexity is 64.16108996685493
At time: 31.319273710250854 and batch: 250, loss is 4.305961275100708 and perplexity is 74.14045057818315
At time: 31.940738677978516 and batch: 300, loss is 4.271605868339538 and perplexity is 71.63658221930287
At time: 32.56015157699585 and batch: 350, loss is 4.25937087059021 and perplexity is 70.76544882167124
At time: 33.17764949798584 and batch: 400, loss is 4.167253441810608 and perplexity is 64.53795122227478
At time: 33.79624152183533 and batch: 450, loss is 4.201554083824158 and perplexity is 66.79004776065199
At time: 34.42143940925598 and batch: 500, loss is 4.084854984283448 and perplexity is 59.43331836103624
At time: 35.047832012176514 and batch: 550, loss is 4.160984673500061 and perplexity is 64.1346432018413
At time: 35.66744637489319 and batch: 600, loss is 4.162895073890686 and perplexity is 64.25728315767728
At time: 36.2884624004364 and batch: 650, loss is 4.00796989440918 and perplexity is 55.0350301555463
At time: 36.913965702056885 and batch: 700, loss is 4.0309940481185915 and perplexity is 56.316865105909834
At time: 37.545130252838135 and batch: 750, loss is 4.119049911499023 and perplexity is 61.50078331121128
At time: 38.17852306365967 and batch: 800, loss is 4.074251270294189 and perplexity is 58.80643397689057
At time: 38.8115348815918 and batch: 850, loss is 4.149178190231323 and perplexity is 63.38189103189685
At time: 39.444931507110596 and batch: 900, loss is 4.094284081459046 and perplexity is 59.99637126395217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379955553028681 and perplexity of 79.83448492516379
finished 3 epochs...
Completing Train Step...
At time: 41.08678913116455 and batch: 50, loss is 4.188616580963135 and perplexity is 65.93151692420501
At time: 41.72511029243469 and batch: 100, loss is 4.061432070732117 and perplexity is 58.05739386908712
At time: 42.36582803726196 and batch: 150, loss is 4.0640620470046995 and perplexity is 58.21028439851738
At time: 43.00651431083679 and batch: 200, loss is 3.9664831352233887 and perplexity is 52.79851869096945
At time: 43.62993574142456 and batch: 250, loss is 4.119413342475891 and perplexity is 61.52313866303581
At time: 44.25326061248779 and batch: 300, loss is 4.085824174880981 and perplexity is 59.49094849715388
At time: 44.89382243156433 and batch: 350, loss is 4.079488549232483 and perplexity is 59.11522758830557
At time: 45.520392656326294 and batch: 400, loss is 3.997097816467285 and perplexity is 54.439925890008475
At time: 46.14306688308716 and batch: 450, loss is 4.037605662345886 and perplexity is 56.69044411174797
At time: 46.76648283004761 and batch: 500, loss is 3.910686912536621 and perplexity is 49.93323996415977
At time: 47.38738822937012 and batch: 550, loss is 3.9883371019363403 and perplexity is 53.96507628816351
At time: 48.01663279533386 and batch: 600, loss is 4.0040263652801515 and perplexity is 54.81842528552757
At time: 48.64096188545227 and batch: 650, loss is 3.851119947433472 and perplexity is 47.045722474424274
At time: 49.268221616744995 and batch: 700, loss is 3.8668060064315797 and perplexity is 47.78950269227344
At time: 49.8927481174469 and batch: 750, loss is 3.960874981880188 and perplexity is 52.50324524513398
At time: 50.51639723777771 and batch: 800, loss is 3.9207106018066407 and perplexity is 50.43627215317141
At time: 51.14128613471985 and batch: 850, loss is 3.9954758739471434 and perplexity is 54.35169902820492
At time: 51.76594018936157 and batch: 900, loss is 3.9505968952178954 and perplexity is 51.96637606064341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326303560439855 and perplexity of 75.66408130391224
finished 4 epochs...
Completing Train Step...
At time: 53.424179792404175 and batch: 50, loss is 4.043664584159851 and perplexity is 57.034969753206234
At time: 54.1171658039093 and batch: 100, loss is 3.9202133560180665 and perplexity is 50.411199163487744
At time: 54.75431489944458 and batch: 150, loss is 3.926385622024536 and perplexity is 50.72331272752217
At time: 55.390066146850586 and batch: 200, loss is 3.8319514274597166 and perplexity is 46.15251369793154
At time: 56.02498745918274 and batch: 250, loss is 3.985412335395813 and perplexity is 53.807471629444365
At time: 56.65196442604065 and batch: 300, loss is 3.953876008987427 and perplexity is 52.13705941205985
At time: 57.278077602386475 and batch: 350, loss is 3.950210089683533 and perplexity is 51.94627906584737
At time: 57.907439947128296 and batch: 400, loss is 3.8713881254196165 and perplexity is 48.008982336899415
At time: 58.53941082954407 and batch: 450, loss is 3.9123525428771972 and perplexity is 50.01647958762406
At time: 59.169519901275635 and batch: 500, loss is 3.787188940048218 and perplexity is 44.132167576148944
At time: 59.799341917037964 and batch: 550, loss is 3.8630947971343996 and perplexity is 47.61247454301606
At time: 60.45016384124756 and batch: 600, loss is 3.8817488288879396 and perplexity is 48.50897483109348
At time: 61.08303785324097 and batch: 650, loss is 3.731324338912964 and perplexity is 41.73434199491155
At time: 61.71991324424744 and batch: 700, loss is 3.7458718156814577 and perplexity is 42.345908958218324
At time: 62.344857931137085 and batch: 750, loss is 3.8421181631088257 and perplexity is 46.6241274280435
At time: 62.96815824508667 and batch: 800, loss is 3.8058152770996094 and perplexity is 44.96189157506769
At time: 63.5943398475647 and batch: 850, loss is 3.8799602222442626 and perplexity is 48.42228890305587
At time: 64.22364687919617 and batch: 900, loss is 3.8388254404067994 and perplexity is 46.470859577941546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318161219766695 and perplexity of 75.0504999612915
finished 5 epochs...
Completing Train Step...
At time: 65.8754825592041 and batch: 50, loss is 3.933983464241028 and perplexity is 51.11016822684037
At time: 66.51543736457825 and batch: 100, loss is 3.8144070959091185 and perplexity is 45.349860292645765
At time: 67.15436244010925 and batch: 150, loss is 3.8200828647613525 and perplexity is 45.60798745926445
At time: 67.79320168495178 and batch: 200, loss is 3.727479887008667 and perplexity is 41.574204342339975
At time: 68.43182253837585 and batch: 250, loss is 3.879492316246033 and perplexity is 48.39963712349516
At time: 69.06081318855286 and batch: 300, loss is 3.848252296447754 and perplexity is 46.911005015643276
At time: 69.69765543937683 and batch: 350, loss is 3.844473719596863 and perplexity is 46.73408264592726
At time: 70.37235116958618 and batch: 400, loss is 3.7677927350997926 and perplexity is 43.28441913789899
At time: 71.01979184150696 and batch: 450, loss is 3.8138689661026 and perplexity is 45.32546274621307
At time: 71.66631627082825 and batch: 500, loss is 3.6900169134140013 and perplexity is 40.04552425808964
At time: 72.32937741279602 and batch: 550, loss is 3.7674777841567995 and perplexity is 43.270788815828666
At time: 72.9718861579895 and batch: 600, loss is 3.7901402568817137 and perplexity is 44.2626079760081
At time: 73.62923526763916 and batch: 650, loss is 3.637171034812927 and perplexity is 37.984228527080724
At time: 74.30662178993225 and batch: 700, loss is 3.648913950920105 and perplexity is 38.43290335564261
At time: 74.94656586647034 and batch: 750, loss is 3.7475001049041747 and perplexity is 42.414916512282645
At time: 75.58210587501526 and batch: 800, loss is 3.7112471532821654 and perplexity is 40.90478927092939
At time: 76.24411296844482 and batch: 850, loss is 3.78492702960968 and perplexity is 44.03245737591628
At time: 76.90359735488892 and batch: 900, loss is 3.7485724973678587 and perplexity is 42.4604263469386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314030895494435 and perplexity of 74.74115634415377
finished 6 epochs...
Completing Train Step...
At time: 78.58658647537231 and batch: 50, loss is 3.8425206470489504 and perplexity is 46.64289666746096
At time: 79.22825121879578 and batch: 100, loss is 3.724499750137329 and perplexity is 41.45049195456662
At time: 79.87799596786499 and batch: 150, loss is 3.7349003076553347 and perplexity is 41.88384985576742
At time: 80.53885459899902 and batch: 200, loss is 3.641846537590027 and perplexity is 38.16223971466427
At time: 81.17749619483948 and batch: 250, loss is 3.792703838348389 and perplexity is 44.376224347671034
At time: 81.81528282165527 and batch: 300, loss is 3.763231453895569 and perplexity is 43.08743631898782
At time: 82.47422313690186 and batch: 350, loss is 3.759435520172119 and perplexity is 42.9241892998605
At time: 83.1304075717926 and batch: 400, loss is 3.6901475524902345 and perplexity is 40.050756110120716
At time: 83.81480264663696 and batch: 450, loss is 3.733843126296997 and perplexity is 41.83959442761309
At time: 84.45984864234924 and batch: 500, loss is 3.608732967376709 and perplexity is 36.91924527955088
At time: 85.08544611930847 and batch: 550, loss is 3.6868344831466673 and perplexity is 39.918284742451
At time: 85.71266913414001 and batch: 600, loss is 3.7075254201889036 and perplexity is 40.75283550410455
At time: 86.3384222984314 and batch: 650, loss is 3.5603587675094603 and perplexity is 35.17581482104177
At time: 86.96960949897766 and batch: 700, loss is 3.565567650794983 and perplexity is 35.359519567846924
At time: 87.59778165817261 and batch: 750, loss is 3.6690226125717165 and perplexity is 39.21356028327551
At time: 88.22861456871033 and batch: 800, loss is 3.6323959827423096 and perplexity is 37.80328421107807
At time: 88.85850381851196 and batch: 850, loss is 3.7042506551742553 and perplexity is 40.61959782426529
At time: 89.48773121833801 and batch: 900, loss is 3.6723057079315184 and perplexity is 39.342513708449424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322933249277611 and perplexity of 75.40949905778886
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 91.07136106491089 and batch: 50, loss is 3.79000762462616 and perplexity is 44.25673771577708
At time: 91.71989583969116 and batch: 100, loss is 3.670143389701843 and perplexity is 39.25753458292337
At time: 92.37000775337219 and batch: 150, loss is 3.6798721504211427 and perplexity is 39.64132562179376
At time: 93.0412323474884 and batch: 200, loss is 3.5714929628372194 and perplexity is 35.569657707156715
At time: 93.685800075531 and batch: 250, loss is 3.715646653175354 and perplexity is 41.08514633649325
At time: 94.32205414772034 and batch: 300, loss is 3.677407732009888 and perplexity is 39.543753088241665
At time: 94.95705890655518 and batch: 350, loss is 3.658326678276062 and perplexity is 38.79636971773959
At time: 95.58761167526245 and batch: 400, loss is 3.5819361782073975 and perplexity is 35.943065699605725
At time: 96.21802067756653 and batch: 450, loss is 3.612737874984741 and perplexity is 37.06739992065394
At time: 96.84952926635742 and batch: 500, loss is 3.4787008810043334 and perplexity is 32.41758041376476
At time: 97.47939991950989 and batch: 550, loss is 3.5420639944076537 and perplexity is 34.53813218564682
At time: 98.11334562301636 and batch: 600, loss is 3.5563482570648195 and perplexity is 35.035024357823225
At time: 98.74821543693542 and batch: 650, loss is 3.3990907669067383 and perplexity is 29.936868078004064
At time: 99.37972807884216 and batch: 700, loss is 3.382942543029785 and perplexity is 29.457323150203404
At time: 100.01217150688171 and batch: 750, loss is 3.4730019569396973 and perplexity is 32.23336051083332
At time: 100.643869638443 and batch: 800, loss is 3.4233257818222045 and perplexity is 30.671251474889907
At time: 101.28608393669128 and batch: 850, loss is 3.478090372085571 and perplexity is 32.397795231926985
At time: 101.97781157493591 and batch: 900, loss is 3.44361487865448 and perplexity is 31.299899248854214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.278087459198416 and perplexity of 72.10240925508398
finished 8 epochs...
Completing Train Step...
At time: 103.63745498657227 and batch: 50, loss is 3.69864652633667 and perplexity is 40.39259702968027
At time: 104.29013347625732 and batch: 100, loss is 3.571468515396118 and perplexity is 35.568788130674434
At time: 104.92249321937561 and batch: 150, loss is 3.5831717967987062 and perplexity is 35.987505069214436
At time: 105.55622625350952 and batch: 200, loss is 3.4797881603240968 and perplexity is 32.452846547139934
At time: 106.18789219856262 and batch: 250, loss is 3.62666766166687 and perplexity is 37.587353910910565
At time: 106.81925582885742 and batch: 300, loss is 3.591547737121582 and perplexity is 36.29020017243978
At time: 107.45290803909302 and batch: 350, loss is 3.5760651302337645 and perplexity is 35.73266049047691
At time: 108.08338141441345 and batch: 400, loss is 3.506346049308777 and perplexity is 33.326272480900016
At time: 108.70841836929321 and batch: 450, loss is 3.5409042739868166 and perplexity is 34.49810082553314
At time: 109.36245894432068 and batch: 500, loss is 3.41013219833374 and perplexity is 30.26924553873681
At time: 110.00140428543091 and batch: 550, loss is 3.4792004442214965 and perplexity is 32.43377909032351
At time: 110.64010000228882 and batch: 600, loss is 3.4988453722000123 and perplexity is 33.07723800294494
At time: 111.27645921707153 and batch: 650, loss is 3.3460092973709106 and perplexity is 28.38921437116486
At time: 111.91175127029419 and batch: 700, loss is 3.3341265296936036 and perplexity is 28.053868297892006
At time: 112.54676032066345 and batch: 750, loss is 3.432779107093811 and perplexity is 30.962571594000664
At time: 113.18580985069275 and batch: 800, loss is 3.3871042490005494 and perplexity is 29.58017131951147
At time: 113.8252739906311 and batch: 850, loss is 3.44768506526947 and perplexity is 31.42755529556017
At time: 114.46270704269409 and batch: 900, loss is 3.419864926338196 and perplexity is 30.565286177079425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.286780788473887 and perplexity of 72.73195168536591
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 116.16210842132568 and batch: 50, loss is 3.671184010505676 and perplexity is 39.2984080533209
At time: 116.82410216331482 and batch: 100, loss is 3.5553600358963013 and perplexity is 35.00041910675034
At time: 117.44941520690918 and batch: 150, loss is 3.5654980993270873 and perplexity is 35.357060346879095
At time: 118.07793426513672 and batch: 200, loss is 3.464507007598877 and perplexity is 31.96069950661258
At time: 118.70129728317261 and batch: 250, loss is 3.609380478858948 and perplexity is 36.9431586560424
At time: 119.32281613349915 and batch: 300, loss is 3.5691703605651854 and perplexity is 35.48713940504388
At time: 119.94714164733887 and batch: 350, loss is 3.5480875158309937 and perplexity is 34.746801194988834
At time: 120.56925415992737 and batch: 400, loss is 3.479291319847107 and perplexity is 32.43672666421854
At time: 121.18985366821289 and batch: 450, loss is 3.5066664171218873 and perplexity is 33.33695085634641
At time: 121.81005597114563 and batch: 500, loss is 3.3715185022354124 and perplexity is 29.122716408459556
At time: 122.43067622184753 and batch: 550, loss is 3.436591205596924 and perplexity is 31.080829228491492
At time: 123.05437898635864 and batch: 600, loss is 3.4577978086471557 and perplexity is 31.746986538059627
At time: 123.68652200698853 and batch: 650, loss is 3.2992377424240114 and perplexity is 27.091979980952523
At time: 124.31035876274109 and batch: 700, loss is 3.278605065345764 and perplexity is 26.538727077689973
At time: 124.95588231086731 and batch: 750, loss is 3.372837572097778 and perplexity is 29.16115665309462
At time: 125.57724046707153 and batch: 800, loss is 3.3203223514556885 and perplexity is 27.66926835003846
At time: 126.19917464256287 and batch: 850, loss is 3.3806384086608885 and perplexity is 29.389527654477263
At time: 126.82226467132568 and batch: 900, loss is 3.3510640335083006 and perplexity is 28.53307764809823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2711549523758565 and perplexity of 71.60428742246351
finished 10 epochs...
Completing Train Step...
At time: 128.3858609199524 and batch: 50, loss is 3.6453292989730834 and perplexity is 38.295381405222734
At time: 129.01635265350342 and batch: 100, loss is 3.5200242376327515 and perplexity is 33.78524732834301
At time: 129.63880848884583 and batch: 150, loss is 3.5302500534057617 and perplexity is 34.132501495993104
At time: 130.2665684223175 and batch: 200, loss is 3.4299259567260743 and perplexity is 30.874356626597084
At time: 130.90202927589417 and batch: 250, loss is 3.574329681396484 and perplexity is 35.67070206476516
At time: 131.5339560508728 and batch: 300, loss is 3.5367112827301024 and perplexity is 34.35375342557443
At time: 132.16492795944214 and batch: 350, loss is 3.517213463783264 and perplexity is 33.69041797302296
At time: 132.79495453834534 and batch: 400, loss is 3.450723853111267 and perplexity is 31.523202220243487
At time: 133.43171286582947 and batch: 450, loss is 3.4808332300186158 and perplexity is 32.486779761765945
At time: 134.07324600219727 and batch: 500, loss is 3.347774600982666 and perplexity is 28.439374214479386
At time: 134.71575331687927 and batch: 550, loss is 3.414845199584961 and perplexity is 30.412241235594028
At time: 135.35895419120789 and batch: 600, loss is 3.439468750953674 and perplexity is 31.170394526623724
At time: 135.9984438419342 and batch: 650, loss is 3.283045711517334 and perplexity is 26.65683822530418
At time: 136.64083194732666 and batch: 700, loss is 3.2662732124328615 and perplexity is 26.21346506253379
At time: 137.2888331413269 and batch: 750, loss is 3.363912763595581 and perplexity is 28.902056842181217
At time: 137.93482494354248 and batch: 800, loss is 3.314309196472168 and perplexity is 27.503387983171756
At time: 138.5795135498047 and batch: 850, loss is 3.378011713027954 and perplexity is 29.312431608783708
At time: 139.22258281707764 and batch: 900, loss is 3.350658974647522 and perplexity is 28.52152241260484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273171202777183 and perplexity of 71.74880523880975
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 140.89639568328857 and batch: 50, loss is 3.637462086677551 and perplexity is 37.99528551662049
At time: 141.58324241638184 and batch: 100, loss is 3.5198751068115235 and perplexity is 33.78020928233685
At time: 142.23109793663025 and batch: 150, loss is 3.5323328828811644 and perplexity is 34.203667764050905
At time: 142.87238574028015 and batch: 200, loss is 3.4301017475128175 and perplexity is 30.879784531112488
At time: 143.52718782424927 and batch: 250, loss is 3.5760071849823 and perplexity is 35.730590012467054
At time: 144.2160882949829 and batch: 300, loss is 3.5367199039459227 and perplexity is 34.354049597973635
At time: 144.8699917793274 and batch: 350, loss is 3.5135759258270265 and perplexity is 33.56809041932765
At time: 145.50630068778992 and batch: 400, loss is 3.4462930011749267 and perplexity is 31.383836560954293
At time: 146.14824676513672 and batch: 450, loss is 3.4755406332015992 and perplexity is 32.31529453598129
At time: 146.7869050502777 and batch: 500, loss is 3.3390573024749757 and perplexity is 28.192537139433227
At time: 147.4220621585846 and batch: 550, loss is 3.4024835634231567 and perplexity is 30.038610277424716
At time: 148.06419038772583 and batch: 600, loss is 3.425482807159424 and perplexity is 30.73748154573655
At time: 148.7005205154419 and batch: 650, loss is 3.2697707128524782 and perplexity is 26.30530718289525
At time: 149.3366711139679 and batch: 700, loss is 3.250691170692444 and perplexity is 25.808171605952452
At time: 149.97584557533264 and batch: 750, loss is 3.344086112976074 and perplexity is 28.334669144181166
At time: 150.61363697052002 and batch: 800, loss is 3.2925960302352903 and perplexity is 26.912639071909002
At time: 151.25240683555603 and batch: 850, loss is 3.3549733924865723 and perplexity is 28.644842012548345
At time: 151.89046359062195 and batch: 900, loss is 3.32866108417511 and perplexity is 27.900959646706934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2679464261825775 and perplexity of 71.37491136703706
finished 12 epochs...
Completing Train Step...
At time: 153.53028750419617 and batch: 50, loss is 3.6257169246673584 and perplexity is 37.55163520507185
At time: 154.16258478164673 and batch: 100, loss is 3.5052769947052003 and perplexity is 33.29066391302732
At time: 154.802081823349 and batch: 150, loss is 3.518303117752075 and perplexity is 33.72714887903256
At time: 155.43854212760925 and batch: 200, loss is 3.417368187904358 and perplexity is 30.489067840558466
At time: 156.07522296905518 and batch: 250, loss is 3.562268466949463 and perplexity is 35.243054238010714
At time: 156.7134952545166 and batch: 300, loss is 3.5236718893051147 and perplexity is 33.908709178281114
At time: 157.36361169815063 and batch: 350, loss is 3.501019515991211 and perplexity is 33.14923090768386
At time: 158.00146055221558 and batch: 400, loss is 3.435391182899475 and perplexity is 31.04355389805627
At time: 158.64037227630615 and batch: 450, loss is 3.4660153865814207 and perplexity is 32.0089447309096
At time: 159.28043508529663 and batch: 500, loss is 3.3306263971328733 and perplexity is 27.9558476826519
At time: 159.91496205329895 and batch: 550, loss is 3.395349454879761 and perplexity is 29.8250741717589
At time: 160.55573892593384 and batch: 600, loss is 3.4205314922332763 and perplexity is 30.585666746159966
At time: 161.19282913208008 and batch: 650, loss is 3.265876350402832 and perplexity is 26.20306399760535
At time: 161.83101081848145 and batch: 700, loss is 3.2478173685073854 and perplexity is 25.734110495460424
At time: 162.46921586990356 and batch: 750, loss is 3.3427358722686766 and perplexity is 28.296436338026343
At time: 163.1207263469696 and batch: 800, loss is 3.29301992893219 and perplexity is 26.92404972285088
At time: 163.79210567474365 and batch: 850, loss is 3.3573390007019044 and perplexity is 28.71268469911058
At time: 164.43206977844238 and batch: 900, loss is 3.3321308755874632 and perplexity is 27.997938307450134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268093997485017 and perplexity of 71.38544503288325
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 166.07143759727478 and batch: 50, loss is 3.6224711513519288 and perplexity is 37.42994869980933
At time: 166.71153116226196 and batch: 100, loss is 3.5039635610580446 and perplexity is 33.2469675373346
At time: 167.3465974330902 and batch: 150, loss is 3.5185831880569456 and perplexity is 33.73659617479327
At time: 167.9841651916504 and batch: 200, loss is 3.417413659095764 and perplexity is 30.490454246318556
At time: 168.61152052879333 and batch: 250, loss is 3.5628452825546266 and perplexity is 35.26338884576455
At time: 169.23420572280884 and batch: 300, loss is 3.524007964134216 and perplexity is 33.92010695706926
At time: 169.86255359649658 and batch: 350, loss is 3.4998341178894044 and perplexity is 33.109959153219755
At time: 170.49327158927917 and batch: 400, loss is 3.434340949058533 and perplexity is 31.01096802159967
At time: 171.12681818008423 and batch: 450, loss is 3.4653778505325317 and perplexity is 31.988544378427786
At time: 171.765953540802 and batch: 500, loss is 3.3292517137527464 and perplexity is 27.917443646208362
At time: 172.40602588653564 and batch: 550, loss is 3.3912569093704223 and perplexity is 29.70326312703062
At time: 173.0483078956604 and batch: 600, loss is 3.414843044281006 and perplexity is 30.41217568804084
At time: 173.69349336624146 and batch: 650, loss is 3.2591992950439455 and perplexity is 26.028687497346453
At time: 174.3295407295227 and batch: 700, loss is 3.2422226905822753 and perplexity is 25.59053842977106
At time: 175.01167035102844 and batch: 750, loss is 3.33455677986145 and perplexity is 28.065941076407174
At time: 175.66979217529297 and batch: 800, loss is 3.2844559288024904 and perplexity is 26.69445667819902
At time: 176.33773493766785 and batch: 850, loss is 3.3470408296585084 and perplexity is 28.418513871500522
At time: 177.01610589027405 and batch: 900, loss is 3.3225979518890383 and perplexity is 27.732304244138383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267698523116438 and perplexity of 71.35721950068833
finished 14 epochs...
Completing Train Step...
At time: 178.67704796791077 and batch: 50, loss is 3.619103441238403 and perplexity is 37.30410750029972
At time: 179.32723832130432 and batch: 100, loss is 3.4996554136276243 and perplexity is 33.10404279106702
At time: 179.9658510684967 and batch: 150, loss is 3.5142252969741823 and perplexity is 33.589895647787955
At time: 180.64194512367249 and batch: 200, loss is 3.413567137718201 and perplexity is 30.373397337527976
At time: 181.28416180610657 and batch: 250, loss is 3.558560929298401 and perplexity is 35.11263121102417
At time: 181.91545176506042 and batch: 300, loss is 3.519738178253174 and perplexity is 33.77558412364445
At time: 182.546484708786 and batch: 350, loss is 3.4959111833572387 and perplexity is 32.9803253897316
At time: 183.20460963249207 and batch: 400, loss is 3.43076865196228 and perplexity is 30.900385265411884
At time: 183.85545206069946 and batch: 450, loss is 3.4621883487701415 and perplexity is 31.886679395383325
At time: 184.48818564414978 and batch: 500, loss is 3.326424865722656 and perplexity is 27.838636715719325
At time: 185.12098479270935 and batch: 550, loss is 3.3889522647857664 and perplexity is 29.634886484712077
At time: 185.75425457954407 and batch: 600, loss is 3.41346330165863 and perplexity is 30.370243647368884
At time: 186.38631415367126 and batch: 650, loss is 3.258346633911133 and perplexity is 26.006503306322113
At time: 187.01952695846558 and batch: 700, loss is 3.2416644430160524 and perplexity is 25.576256560754516
At time: 187.65340423583984 and batch: 750, loss is 3.33471586227417 and perplexity is 28.07040622918308
At time: 188.28765892982483 and batch: 800, loss is 3.285458197593689 and perplexity is 26.721225111367332
At time: 188.91829538345337 and batch: 850, loss is 3.348822932243347 and perplexity is 28.4692037323788
At time: 189.56132888793945 and batch: 900, loss is 3.3249613666534423 and perplexity is 27.79792469506637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267419684423159 and perplexity of 71.3373251206371
finished 15 epochs...
Completing Train Step...
At time: 191.1723792552948 and batch: 50, loss is 3.617062544822693 and perplexity is 37.22805131880467
At time: 191.80351281166077 and batch: 100, loss is 3.4971685886383055 and perplexity is 33.02182110804833
At time: 192.42545771598816 and batch: 150, loss is 3.511624426841736 and perplexity is 33.502646202866735
At time: 193.0443034172058 and batch: 200, loss is 3.411048674583435 and perplexity is 30.29699929925783
At time: 193.66290974617004 and batch: 250, loss is 3.5557929611206056 and perplexity is 35.0155749514799
At time: 194.28298020362854 and batch: 300, loss is 3.517041006088257 and perplexity is 33.68460830217112
At time: 194.90571641921997 and batch: 350, loss is 3.493448872566223 and perplexity is 32.899217476058396
At time: 195.53095531463623 and batch: 400, loss is 3.428548936843872 and perplexity is 30.83187128196752
At time: 196.1511595249176 and batch: 450, loss is 3.4601247549057006 and perplexity is 31.820946086179067
At time: 196.78142094612122 and batch: 500, loss is 3.324634518623352 and perplexity is 27.788840482798502
At time: 197.41707587242126 and batch: 550, loss is 3.387471990585327 and perplexity is 29.591051178956274
At time: 198.08928394317627 and batch: 600, loss is 3.412547459602356 and perplexity is 30.34244203386403
At time: 198.73613333702087 and batch: 650, loss is 3.2577336740493776 and perplexity is 25.990567248231674
At time: 199.37324023246765 and batch: 700, loss is 3.241272768974304 and perplexity is 25.566240966526752
At time: 200.01123142242432 and batch: 750, loss is 3.334719624519348 and perplexity is 28.070511837132223
At time: 200.64934492111206 and batch: 800, loss is 3.285935606956482 and perplexity is 26.73398512005224
At time: 201.29176807403564 and batch: 850, loss is 3.349744324684143 and perplexity is 28.495447129870684
At time: 201.93109965324402 and batch: 900, loss is 3.3261030673980714 and perplexity is 27.829679730314226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267376207325556 and perplexity of 71.33422364821209
finished 16 epochs...
Completing Train Step...
At time: 203.61534643173218 and batch: 50, loss is 3.6153459692001344 and perplexity is 37.16420137074614
At time: 204.29279470443726 and batch: 100, loss is 3.495201449394226 and perplexity is 32.956926437189175
At time: 204.92852449417114 and batch: 150, loss is 3.5095702028274536 and perplexity is 33.43389490195807
At time: 205.57598781585693 and batch: 200, loss is 3.409023723602295 and perplexity is 30.235711434204628
At time: 206.21209049224854 and batch: 250, loss is 3.5536242151260375 and perplexity is 34.9397173512225
At time: 206.84901070594788 and batch: 300, loss is 3.514950642585754 and perplexity is 33.61426876958909
At time: 207.48918676376343 and batch: 350, loss is 3.491483421325684 and perplexity is 32.83461917146151
At time: 208.14710211753845 and batch: 400, loss is 3.426798677444458 and perplexity is 30.777954707214647
At time: 208.78527355194092 and batch: 450, loss is 3.4584861850738524 and perplexity is 31.76884793878427
At time: 209.42060327529907 and batch: 500, loss is 3.3232000875473022 and perplexity is 27.74900788183286
At time: 210.05118036270142 and batch: 550, loss is 3.3862370586395265 and perplexity is 29.55453079927863
At time: 210.6818768978119 and batch: 600, loss is 3.411708478927612 and perplexity is 30.316995987236364
At time: 211.3122181892395 and batch: 650, loss is 3.2571043300628664 and perplexity is 25.97421538704045
At time: 211.94286823272705 and batch: 700, loss is 3.2408459520339967 and perplexity is 25.555331190186557
At time: 212.61004614830017 and batch: 750, loss is 3.3345593786239625 and perplexity is 28.066014013217487
At time: 213.25531268119812 and batch: 800, loss is 3.2860877990722654 and perplexity is 26.73805413143886
At time: 213.92227339744568 and batch: 850, loss is 3.350201659202576 and perplexity is 28.50848206189634
At time: 214.5821213722229 and batch: 900, loss is 3.326698145866394 and perplexity is 27.846245501980214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26745145614833 and perplexity of 71.33959166653109
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 216.2450520992279 and batch: 50, loss is 3.614393448829651 and perplexity is 37.128818565984126
At time: 216.9195957183838 and batch: 100, loss is 3.4946808433532714 and perplexity is 32.939773327588334
At time: 217.58487510681152 and batch: 150, loss is 3.5094723463058473 and perplexity is 33.43062333737379
At time: 218.22452545166016 and batch: 200, loss is 3.4089738178253173 and perplexity is 30.234202535184735
At time: 218.8643765449524 and batch: 250, loss is 3.5536669445037843 and perplexity is 34.94121033550048
At time: 219.50268077850342 and batch: 300, loss is 3.5147104930877684 and perplexity is 33.606197289038704
At time: 220.14157676696777 and batch: 350, loss is 3.490757079124451 and perplexity is 32.81077866112838
At time: 220.77834296226501 and batch: 400, loss is 3.4261626625061035 and perplexity is 30.75838569200152
At time: 221.41433119773865 and batch: 450, loss is 3.4581824684143068 and perplexity is 31.759200675502136
At time: 222.0677661895752 and batch: 500, loss is 3.3227648019790648 and perplexity is 27.73693176763878
At time: 222.7370309829712 and batch: 550, loss is 3.38488178730011 and perplexity is 29.514503520774852
At time: 223.40224289894104 and batch: 600, loss is 3.4097722339630128 and perplexity is 30.25835164963767
At time: 224.08490657806396 and batch: 650, loss is 3.2546937561035154 and perplexity is 25.911678025564427
At time: 224.72703075408936 and batch: 700, loss is 3.238871693611145 and perplexity is 25.50492813303694
At time: 225.36765933036804 and batch: 750, loss is 3.331844482421875 and perplexity is 27.9899210373688
At time: 226.0040442943573 and batch: 800, loss is 3.2828491163253783 and perplexity is 26.651598134181587
At time: 226.64264822006226 and batch: 850, loss is 3.346594686508179 and perplexity is 28.405837974036878
At time: 227.28076839447021 and batch: 900, loss is 3.322930054664612 and perplexity is 27.74151574884939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267197700395976 and perplexity of 71.32149113142971
finished 18 epochs...
Completing Train Step...
At time: 228.9422481060028 and batch: 50, loss is 3.6138018798828124 and perplexity is 37.10686080529272
At time: 229.5907280445099 and batch: 100, loss is 3.4940238189697266 and perplexity is 32.91813820150465
At time: 230.22212886810303 and batch: 150, loss is 3.508733158111572 and perplexity is 33.40592094625826
At time: 230.8532111644745 and batch: 200, loss is 3.4082805252075197 and perplexity is 30.213248650188287
At time: 231.48343992233276 and batch: 250, loss is 3.552936358451843 and perplexity is 34.91569209736089
At time: 232.11311888694763 and batch: 300, loss is 3.5139871501922606 and perplexity is 33.58189727466556
At time: 232.7454526424408 and batch: 350, loss is 3.4901526594161987 and perplexity is 32.79095317192426
At time: 233.38325333595276 and batch: 400, loss is 3.4255411529541018 and perplexity is 30.739275000843506
At time: 234.02480673789978 and batch: 450, loss is 3.4575905704498293 and perplexity is 31.740408031481405
At time: 234.6539912223816 and batch: 500, loss is 3.3222680044174195 and perplexity is 27.72315554984814
At time: 235.31122946739197 and batch: 550, loss is 3.384494252204895 and perplexity is 29.50306783085141
At time: 235.95514225959778 and batch: 600, loss is 3.4095568323135375 and perplexity is 30.251834652693077
At time: 236.58793473243713 and batch: 650, loss is 3.254562668800354 and perplexity is 25.908281556193543
At time: 237.22095203399658 and batch: 700, loss is 3.2387963151931762 and perplexity is 25.503005684360343
At time: 237.86839938163757 and batch: 750, loss is 3.331870608329773 and perplexity is 27.99065230902042
At time: 238.51001596450806 and batch: 800, loss is 3.2830809593200683 and perplexity is 26.657777836838992
At time: 239.14793705940247 and batch: 850, loss is 3.346910390853882 and perplexity is 28.414807236271656
At time: 239.79936170578003 and batch: 900, loss is 3.3233351421356203 and perplexity is 27.752755765747338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267079392524614 and perplexity of 71.31305373674655
finished 19 epochs...
Completing Train Step...
At time: 241.44935607910156 and batch: 50, loss is 3.6132873773574827 and perplexity is 37.08777414219056
At time: 242.0990903377533 and batch: 100, loss is 3.493442625999451 and perplexity is 32.899011969541526
At time: 242.74228715896606 and batch: 150, loss is 3.5080951261520386 and perplexity is 33.38461369913204
At time: 243.39421463012695 and batch: 200, loss is 3.4076657772064207 and perplexity is 30.194680823825703
At time: 244.03718042373657 and batch: 250, loss is 3.552282676696777 and perplexity is 34.892875804580946
At time: 244.6807839870453 and batch: 300, loss is 3.513357243537903 and perplexity is 33.5607504750604
At time: 245.32211089134216 and batch: 350, loss is 3.4896069288253786 and perplexity is 32.77306302771968
At time: 245.96242809295654 and batch: 400, loss is 3.4250107479095457 and perplexity is 30.722975057485293
At time: 246.59914231300354 and batch: 450, loss is 3.457087893486023 and perplexity is 31.724456869019004
At time: 247.23839402198792 and batch: 500, loss is 3.321841530799866 and perplexity is 27.711334876192684
At time: 247.87941193580627 and batch: 550, loss is 3.384150562286377 and perplexity is 29.492929666164894
At time: 248.51763153076172 and batch: 600, loss is 3.409355812072754 and perplexity is 30.245754032791368
At time: 249.15774035453796 and batch: 650, loss is 3.2544386863708494 and perplexity is 25.905069583620108
At time: 249.7966058254242 and batch: 700, loss is 3.238728656768799 and perplexity is 25.501280249549612
At time: 250.43447160720825 and batch: 750, loss is 3.331888780593872 and perplexity is 27.9911609671682
At time: 251.07267808914185 and batch: 800, loss is 3.28324725151062 and perplexity is 26.662211185716156
At time: 251.70996046066284 and batch: 850, loss is 3.347162561416626 and perplexity is 28.42197351772733
At time: 252.35079050064087 and batch: 900, loss is 3.323655939102173 and perplexity is 27.761660193790856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2670221198095035 and perplexity of 71.30896956149321
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f74eca12b70>
ELAPSED
1045.7815651893616


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.7917024081235}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.41628114957778306, 'tune_wordvecs': 'TRUE', 'dropout': 0.7387750863627058, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.03583228815741}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.09083670716130898, 'tune_wordvecs': 'TRUE', 'dropout': 0.021912257622493048, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.30896956149321}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.9388904092742721, 'tune_wordvecs': 'TRUE', 'dropout': 0.7397063209606896, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0340676307678223 and batch: 50, loss is 7.075768823623657 and perplexity is 1182.952633208653
At time: 1.8304474353790283 and batch: 100, loss is 6.400710325241089 and perplexity is 602.2726954637162
At time: 2.632213830947876 and batch: 150, loss is 6.216975450515747 and perplexity is 501.1850782420397
At time: 3.4457571506500244 and batch: 200, loss is 6.091012907028198 and perplexity is 441.8687565171056
At time: 4.247037649154663 and batch: 250, loss is 6.164802837371826 and perplexity is 475.7073419788665
At time: 5.053534030914307 and batch: 300, loss is 6.074050798416137 and perplexity is 434.43693851397614
At time: 5.852121829986572 and batch: 350, loss is 6.108293628692627 and perplexity is 449.5709253590071
At time: 6.65157675743103 and batch: 400, loss is 6.016268720626831 and perplexity is 410.0457425321681
At time: 7.457282543182373 and batch: 450, loss is 6.011939325332642 and perplexity is 408.2743297639944
At time: 8.272894382476807 and batch: 500, loss is 5.999424657821655 and perplexity is 403.1967506502981
At time: 9.077584743499756 and batch: 550, loss is 6.031896953582764 and perplexity is 416.5043699308728
At time: 9.88014030456543 and batch: 600, loss is 5.968802165985108 and perplexity is 391.03699269573696
At time: 10.682583808898926 and batch: 650, loss is 5.9028042125701905 and perplexity is 366.0625471196374
At time: 11.481777429580688 and batch: 700, loss is 6.000079336166382 and perplexity is 403.4608012562889
At time: 12.279955625534058 and batch: 750, loss is 5.955702800750732 and perplexity is 385.94805997118766
At time: 13.082034587860107 and batch: 800, loss is 5.964915561676025 and perplexity is 389.5201362547352
At time: 13.877064228057861 and batch: 850, loss is 6.015318641662597 and perplexity is 409.65635170312277
At time: 14.67140817642212 and batch: 900, loss is 5.885182180404663 and perplexity is 359.66828654231267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.921219499143835 and perplexity of 372.8661466762953
finished 1 epochs...
Completing Train Step...
At time: 16.384108304977417 and batch: 50, loss is 5.774463901519775 and perplexity is 321.9717798607495
At time: 17.01197338104248 and batch: 100, loss is 5.591272649765014 and perplexity is 268.0765702964832
At time: 17.64108633995056 and batch: 150, loss is 5.500934839248657 and perplexity is 244.92078684078416
At time: 18.27513575553894 and batch: 200, loss is 5.310854415893555 and perplexity is 202.5231935209037
At time: 18.914079189300537 and batch: 250, loss is 5.338013038635254 and perplexity is 208.09881492291635
At time: 19.546983003616333 and batch: 300, loss is 5.222374458312988 and perplexity is 185.37382432857868
At time: 20.17941117286682 and batch: 350, loss is 5.159863586425781 and perplexity is 174.14069882963
At time: 20.83482050895691 and batch: 400, loss is 4.9823223495483395 and perplexity is 145.81261661602724
At time: 21.483093976974487 and batch: 450, loss is 4.973332271575928 and perplexity is 144.50762460253785
At time: 22.11476731300354 and batch: 500, loss is 4.887756814956665 and perplexity is 132.6556688356936
At time: 22.758461236953735 and batch: 550, loss is 4.930572862625122 and perplexity is 138.4588075011114
At time: 23.42198872566223 and batch: 600, loss is 4.843502445220947 and perplexity is 126.91308032061282
At time: 24.060033559799194 and batch: 650, loss is 4.713264083862304 and perplexity is 111.4152357012859
At time: 24.693135261535645 and batch: 700, loss is 4.779786767959595 and perplexity is 119.07895588875277
At time: 25.327102184295654 and batch: 750, loss is 4.776041831970215 and perplexity is 118.63384679400548
At time: 25.960517406463623 and batch: 800, loss is 4.7199003124237064 and perplexity is 112.15707144388749
At time: 26.593879461288452 and batch: 850, loss is 4.7627015209198 and perplexity is 117.06174386244938
At time: 27.231053829193115 and batch: 900, loss is 4.685179538726807 and perplexity is 108.32971994232362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.777264320687072 and perplexity of 118.77896401715962
finished 2 epochs...
Completing Train Step...
At time: 28.839188814163208 and batch: 50, loss is 4.738099451065064 and perplexity is 114.21692042227001
At time: 29.471852779388428 and batch: 100, loss is 4.613001194000244 and perplexity is 100.78617505513567
At time: 30.096508026123047 and batch: 150, loss is 4.608395195007324 and perplexity is 100.32302149557125
At time: 30.723047256469727 and batch: 200, loss is 4.508801794052124 and perplexity is 90.81294068885157
At time: 31.349299669265747 and batch: 250, loss is 4.635862579345703 and perplexity is 103.11682608895283
At time: 31.984290838241577 and batch: 300, loss is 4.596191730499267 and perplexity is 99.10617304834054
At time: 32.617016077041626 and batch: 350, loss is 4.586214666366577 and perplexity is 98.12230064428489
At time: 33.2470498085022 and batch: 400, loss is 4.464534850120544 and perplexity is 86.88060763519063
At time: 33.88192272186279 and batch: 450, loss is 4.496994581222534 and perplexity is 89.7469982586082
At time: 34.50640845298767 and batch: 500, loss is 4.389842691421509 and perplexity is 80.62773454850942
At time: 35.13079810142517 and batch: 550, loss is 4.468167238235473 and perplexity is 87.19676557833809
At time: 35.76878905296326 and batch: 600, loss is 4.440312786102295 and perplexity is 84.80146224482323
At time: 36.423903703689575 and batch: 650, loss is 4.298251810073853 and perplexity is 73.5710650165692
At time: 37.047285318374634 and batch: 700, loss is 4.3348035669326785 and perplexity is 76.30996761641892
At time: 37.683971881866455 and batch: 750, loss is 4.405013990402222 and perplexity is 81.86028809351275
At time: 38.3184449672699 and batch: 800, loss is 4.351219644546509 and perplexity is 77.57301675935261
At time: 38.95897054672241 and batch: 850, loss is 4.423632392883301 and perplexity is 83.39867257601519
At time: 39.58984160423279 and batch: 900, loss is 4.360962867736816 and perplexity is 78.33252198129532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.540518564720676 and perplexity of 93.7393974624126
finished 3 epochs...
Completing Train Step...
At time: 41.1798734664917 and batch: 50, loss is 4.449013433456421 and perplexity is 85.54250898125095
At time: 41.822243213653564 and batch: 100, loss is 4.315179061889649 and perplexity is 74.82702091218047
At time: 42.442439556121826 and batch: 150, loss is 4.317102861404419 and perplexity is 74.97111165511367
At time: 43.0682532787323 and batch: 200, loss is 4.221190605163574 and perplexity is 68.1145335463683
At time: 43.70184826850891 and batch: 250, loss is 4.362239112854004 and perplexity is 78.4325573012071
At time: 44.32640361785889 and batch: 300, loss is 4.333170375823975 and perplexity is 76.18544057171277
At time: 44.95328402519226 and batch: 350, loss is 4.327110013961792 and perplexity is 75.72512548010094
At time: 45.58813834190369 and batch: 400, loss is 4.2284184217453005 and perplexity is 68.60863639271686
At time: 46.220287561416626 and batch: 450, loss is 4.270559849739075 and perplexity is 71.56168819892588
At time: 46.85184383392334 and batch: 500, loss is 4.146250810623169 and perplexity is 63.19661948894004
At time: 47.48374390602112 and batch: 550, loss is 4.22829421043396 and perplexity is 68.60011495326178
At time: 48.11697340011597 and batch: 600, loss is 4.230172090530395 and perplexity is 68.72905877637251
At time: 48.74802303314209 and batch: 650, loss is 4.076076164245605 and perplexity is 58.91384746185948
At time: 49.4002525806427 and batch: 700, loss is 4.100101547241211 and perplexity is 60.34641529822165
At time: 50.0502610206604 and batch: 750, loss is 4.195764951705932 and perplexity is 66.40450839589087
At time: 50.687031745910645 and batch: 800, loss is 4.1423976564407345 and perplexity is 62.95358170196752
At time: 51.32690763473511 and batch: 850, loss is 4.225790185928345 and perplexity is 68.42855347105147
At time: 51.96468544006348 and batch: 900, loss is 4.167843327522278 and perplexity is 64.57603246824846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425815111970248 and perplexity of 83.58090726168516
finished 4 epochs...
Completing Train Step...
At time: 53.64496636390686 and batch: 50, loss is 4.26572934627533 and perplexity is 71.2168427752611
At time: 54.28068280220032 and batch: 100, loss is 4.134292488098144 and perplexity is 62.445394577657055
At time: 54.92452788352966 and batch: 150, loss is 4.13794632434845 and perplexity is 62.673977171405156
At time: 55.55828332901001 and batch: 200, loss is 4.039303007125855 and perplexity is 56.786749049355706
At time: 56.195167541503906 and batch: 250, loss is 4.190351672172547 and perplexity is 66.04601342185197
At time: 56.829753398895264 and batch: 300, loss is 4.160036916732788 and perplexity is 64.0738879548761
At time: 57.46300983428955 and batch: 350, loss is 4.16146068572998 and perplexity is 64.16517934357876
At time: 58.10053300857544 and batch: 400, loss is 4.072874603271484 and perplexity is 58.725532798281115
At time: 58.775293827056885 and batch: 450, loss is 4.1185699605941775 and perplexity is 61.47127303692024
At time: 59.40894937515259 and batch: 500, loss is 3.9893793487548828 and perplexity is 54.02135053797204
At time: 60.069904088974 and batch: 550, loss is 4.0747271919250485 and perplexity is 58.8344278917803
At time: 60.72401976585388 and batch: 600, loss is 4.083773608207703 and perplexity is 59.36908332983104
At time: 61.358067750930786 and batch: 650, loss is 3.9279960203170776 and perplexity is 50.805063271531324
At time: 61.995245933532715 and batch: 700, loss is 3.9450998592376707 and perplexity is 51.68149872949989
At time: 62.63511657714844 and batch: 750, loss is 4.053040947914123 and perplexity is 57.57226537953432
At time: 63.267189025878906 and batch: 800, loss is 4.002494969367981 and perplexity is 54.73454081969136
At time: 63.90351963043213 and batch: 850, loss is 4.086319904327393 and perplexity is 59.520447223208336
At time: 64.54547810554504 and batch: 900, loss is 4.033462128639221 and perplexity is 56.456031329740405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378110284674658 and perplexity of 79.68730471182919
finished 5 epochs...
Completing Train Step...
At time: 66.18776082992554 and batch: 50, loss is 4.135910739898682 and perplexity is 62.54652875808207
At time: 66.84064412117004 and batch: 100, loss is 4.00411376953125 and perplexity is 54.823216858334895
At time: 67.4798800945282 and batch: 150, loss is 4.011144213676452 and perplexity is 55.21000648057774
At time: 68.13543248176575 and batch: 200, loss is 3.912353367805481 and perplexity is 50.01652084764975
At time: 68.78628253936768 and batch: 250, loss is 4.063727283477784 and perplexity is 58.19080097976029
At time: 69.42393088340759 and batch: 300, loss is 4.036617498397828 and perplexity is 56.63445232776468
At time: 70.0915093421936 and batch: 350, loss is 4.041089382171631 and perplexity is 56.88828214187912
At time: 70.76092553138733 and batch: 400, loss is 3.9592292499542237 and perplexity is 52.41691004000514
At time: 71.40224385261536 and batch: 450, loss is 4.00304811000824 and perplexity is 54.7648250936038
At time: 72.05406212806702 and batch: 500, loss is 3.874406409263611 and perplexity is 48.154105974645624
At time: 72.7259886264801 and batch: 550, loss is 3.956915078163147 and perplexity is 52.29574855373426
At time: 73.3997974395752 and batch: 600, loss is 3.9711159610748292 and perplexity is 53.04369251858688
At time: 74.05798172950745 and batch: 650, loss is 3.815728197097778 and perplexity is 45.40981163916204
At time: 74.70364117622375 and batch: 700, loss is 3.826612739562988 and perplexity is 45.90677637274905
At time: 75.35630941390991 and batch: 750, loss is 3.939474310874939 and perplexity is 51.39157820449355
At time: 75.99787759780884 and batch: 800, loss is 3.8939937925338746 and perplexity is 49.106617050549275
At time: 76.63692879676819 and batch: 850, loss is 3.9764762687683106 and perplexity is 53.32878644463659
At time: 77.27713871002197 and batch: 900, loss is 3.928753151893616 and perplexity is 50.84354395481417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346765387548159 and perplexity of 77.22825500531458
finished 6 epochs...
Completing Train Step...
At time: 78.96209216117859 and batch: 50, loss is 4.029996504783631 and perplexity is 56.26071460341355
At time: 79.632972240448 and batch: 100, loss is 3.901619143486023 and perplexity is 49.48250355152094
At time: 80.28558301925659 and batch: 150, loss is 3.909574465751648 and perplexity is 49.877722777581596
At time: 80.95613527297974 and batch: 200, loss is 3.8082727289199827 and perplexity is 45.07251913263868
At time: 81.61649751663208 and batch: 250, loss is 3.9654352188110353 and perplexity is 52.74321923634878
At time: 82.26190876960754 and batch: 300, loss is 3.9367598199844362 and perplexity is 51.25226540082055
At time: 82.90477919578552 and batch: 350, loss is 3.945731210708618 and perplexity is 51.714138222155185
At time: 83.57386922836304 and batch: 400, loss is 3.86790801525116 and perplexity is 47.84219617473219
At time: 84.22422289848328 and batch: 450, loss is 3.912851805686951 and perplexity is 50.041457190432205
At time: 84.86262106895447 and batch: 500, loss is 3.782594804763794 and perplexity is 43.929883444028945
At time: 85.5012834072113 and batch: 550, loss is 3.865581612586975 and perplexity is 47.73102532630925
At time: 86.14270639419556 and batch: 600, loss is 3.880933756828308 and perplexity is 48.46945262997625
At time: 86.7809853553772 and batch: 650, loss is 3.7273591232299803 and perplexity is 41.56918398747231
At time: 87.43007016181946 and batch: 700, loss is 3.7346161794662476 and perplexity is 41.87195116381285
At time: 88.0663492679596 and batch: 750, loss is 3.8495372581481933 and perplexity is 46.97132260501427
At time: 88.69824194908142 and batch: 800, loss is 3.8050913190841675 and perplexity is 44.92935283303521
At time: 89.33688879013062 and batch: 850, loss is 3.8886880445480347 and perplexity is 48.84675969446633
At time: 89.97731924057007 and batch: 900, loss is 3.8457506942749022 and perplexity is 46.793799006092634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333844746628853 and perplexity of 76.23683513614029
finished 7 epochs...
Completing Train Step...
At time: 91.5912697315216 and batch: 50, loss is 3.943584890365601 and perplexity is 51.603262145620995
At time: 92.21738982200623 and batch: 100, loss is 3.822078981399536 and perplexity is 45.699117244447436
At time: 92.84753608703613 and batch: 150, loss is 3.8298223400115967 and perplexity is 46.0543554911005
At time: 93.47637748718262 and batch: 200, loss is 3.730723652839661 and perplexity is 41.70928028476045
At time: 94.09806799888611 and batch: 250, loss is 3.8838028573989867 and perplexity is 48.608716049023265
At time: 94.724280834198 and batch: 300, loss is 3.855758328437805 and perplexity is 47.26444532770469
At time: 95.35551834106445 and batch: 350, loss is 3.8634587955474853 and perplexity is 47.62980856277925
At time: 95.98036527633667 and batch: 400, loss is 3.7879404401779175 and perplexity is 44.1653453708034
At time: 96.61715531349182 and batch: 450, loss is 3.8349474239349366 and perplexity is 46.29099380566737
At time: 97.24253010749817 and batch: 500, loss is 3.706310305595398 and perplexity is 40.703346212626194
At time: 97.86374521255493 and batch: 550, loss is 3.7890591430664062 and perplexity is 44.21478091692924
At time: 98.48688840866089 and batch: 600, loss is 3.8066054582595825 and perplexity is 44.99743365519878
At time: 99.10361790657043 and batch: 650, loss is 3.652288875579834 and perplexity is 38.5628306329848
At time: 99.71847176551819 and batch: 700, loss is 3.6594368982315064 and perplexity is 38.83946614042657
At time: 100.40651035308838 and batch: 750, loss is 3.777597413063049 and perplexity is 43.71089624723825
At time: 101.01869249343872 and batch: 800, loss is 3.730369682312012 and perplexity is 41.694519041486856
At time: 101.63037991523743 and batch: 850, loss is 3.814977684020996 and perplexity is 45.375743767503344
At time: 102.24172902107239 and batch: 900, loss is 3.772383728027344 and perplexity is 43.483594456435306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324647250240797 and perplexity of 75.53886184410729
finished 8 epochs...
Completing Train Step...
At time: 103.77394938468933 and batch: 50, loss is 3.8688389968872072 and perplexity is 47.88675712029153
At time: 104.40289115905762 and batch: 100, loss is 3.752047371864319 and perplexity is 42.60822764632131
At time: 105.01810050010681 and batch: 150, loss is 3.7587240886688233 and perplexity is 42.893662539476225
At time: 105.6290352344513 and batch: 200, loss is 3.6623138856887816 and perplexity is 38.951367689852304
At time: 106.2408561706543 and batch: 250, loss is 3.811810393333435 and perplexity is 45.23225295533901
At time: 106.8521740436554 and batch: 300, loss is 3.7830384159088135 and perplexity is 43.94937555306288
At time: 107.4645528793335 and batch: 350, loss is 3.795194973945618 and perplexity is 44.48690934824631
At time: 108.07658243179321 and batch: 400, loss is 3.7181464433670044 and perplexity is 41.187979058893454
At time: 108.6885769367218 and batch: 450, loss is 3.766032004356384 and perplexity is 43.208273985639515
At time: 109.30013751983643 and batch: 500, loss is 3.6401935482025145 and perplexity is 38.099210045451095
At time: 109.91119694709778 and batch: 550, loss is 3.7221462392807005 and perplexity is 41.35305247914714
At time: 110.52242088317871 and batch: 600, loss is 3.741669526100159 and perplexity is 42.16833256123595
At time: 111.13378858566284 and batch: 650, loss is 3.5867813348770143 and perplexity is 36.117638057804534
At time: 111.74403953552246 and batch: 700, loss is 3.59216495513916 and perplexity is 36.31260605179264
At time: 112.35612916946411 and batch: 750, loss is 3.712030644416809 and perplexity is 40.93685036884055
At time: 112.96848630905151 and batch: 800, loss is 3.6693422603607178 and perplexity is 39.22609681464945
At time: 113.58050560951233 and batch: 850, loss is 3.7491219902038573 and perplexity is 42.483764458505014
At time: 114.19221496582031 and batch: 900, loss is 3.7090820837020875 and perplexity is 40.81632335798509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326991251070205 and perplexity of 75.71613267924923
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 115.76979184150696 and batch: 50, loss is 3.8247455167770386 and perplexity is 45.82113817155828
At time: 116.40681982040405 and batch: 100, loss is 3.703525266647339 and perplexity is 40.59014351823498
At time: 117.0338191986084 and batch: 150, loss is 3.7102845096588135 and perplexity is 40.865431483174234
At time: 117.68775129318237 and batch: 200, loss is 3.591342525482178 and perplexity is 36.28275376503896
At time: 118.33911967277527 and batch: 250, loss is 3.7372659730911253 and perplexity is 41.98305032298374
At time: 119.03659963607788 and batch: 300, loss is 3.6988262224197386 and perplexity is 40.3998560733428
At time: 119.67506670951843 and batch: 350, loss is 3.696754803657532 and perplexity is 40.31625766702752
At time: 120.30666422843933 and batch: 400, loss is 3.6112480545043946 and perplexity is 37.01221726543069
At time: 120.9242627620697 and batch: 450, loss is 3.651582760810852 and perplexity is 38.53561046015512
At time: 121.5425238609314 and batch: 500, loss is 3.5168327856063843 and perplexity is 33.677595206959126
At time: 122.16094064712524 and batch: 550, loss is 3.5813538932800295 and perplexity is 35.922142686374244
At time: 122.77961707115173 and batch: 600, loss is 3.5948327732086183 and perplexity is 36.409610816364655
At time: 123.39979720115662 and batch: 650, loss is 3.431311635971069 and perplexity is 30.917168236506537
At time: 124.0201141834259 and batch: 700, loss is 3.423106760978699 and perplexity is 30.664534567118757
At time: 124.63961338996887 and batch: 750, loss is 3.5285467100143433 and perplexity is 34.074411612691804
At time: 125.26670098304749 and batch: 800, loss is 3.472883114814758 and perplexity is 32.229530057390406
At time: 125.95917272567749 and batch: 850, loss is 3.5364436531066894 and perplexity is 34.344560573676944
At time: 126.58045148849487 and batch: 900, loss is 3.490901846885681 and perplexity is 32.81552894793527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2826517183486725 and perplexity of 72.4322555155627
finished 10 epochs...
Completing Train Step...
At time: 128.1611044406891 and batch: 50, loss is 3.737756595611572 and perplexity is 42.003653206655336
At time: 128.7829234600067 and batch: 100, loss is 3.615133295059204 and perplexity is 37.156298346560476
At time: 129.408753156662 and batch: 150, loss is 3.622506718635559 and perplexity is 37.43128000508632
At time: 130.03354382514954 and batch: 200, loss is 3.509695625305176 and perplexity is 33.438088526878495
At time: 130.66062927246094 and batch: 250, loss is 3.6577871322631834 and perplexity is 38.7754429371324
At time: 131.28725695610046 and batch: 300, loss is 3.6248722743988036 and perplexity is 37.51993059784864
At time: 131.9148030281067 and batch: 350, loss is 3.624780559539795 and perplexity is 37.51648962050063
At time: 132.5418131351471 and batch: 400, loss is 3.5446712017059325 and perplexity is 34.62829774502714
At time: 133.16884684562683 and batch: 450, loss is 3.5902361249923707 and perplexity is 36.24263270757334
At time: 133.7965123653412 and batch: 500, loss is 3.4574689149856566 and perplexity is 31.73654687227962
At time: 134.43443703651428 and batch: 550, loss is 3.526581220626831 and perplexity is 34.00750449243809
At time: 135.06368613243103 and batch: 600, loss is 3.544526038169861 and perplexity is 34.623271343712254
At time: 135.6916298866272 and batch: 650, loss is 3.3837702465057373 and perplexity is 29.481715172255228
At time: 136.31994152069092 and batch: 700, loss is 3.3809363937377928 and perplexity is 29.39828660008829
At time: 136.94747281074524 and batch: 750, loss is 3.4923541259765627 and perplexity is 32.863220877096026
At time: 137.5735743045807 and batch: 800, loss is 3.4417610645294188 and perplexity is 31.241928803330964
At time: 138.2003037929535 and batch: 850, loss is 3.5110204124450686 and perplexity is 33.48241623244549
At time: 138.82710671424866 and batch: 900, loss is 3.472673053741455 and perplexity is 32.22276059873927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.286949262227098 and perplexity of 72.74420614249287
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 140.4247531890869 and batch: 50, loss is 3.714216103553772 and perplexity is 41.026414015714764
At time: 141.05842447280884 and batch: 100, loss is 3.604025278091431 and perplexity is 36.745849411301364
At time: 141.70712518692017 and batch: 150, loss is 3.616426010131836 and perplexity is 37.2043619130314
At time: 142.37513279914856 and batch: 200, loss is 3.4961716175079345 and perplexity is 32.98891571132214
At time: 143.00298023223877 and batch: 250, loss is 3.6420241928100587 and perplexity is 38.169020038019774
At time: 143.64181780815125 and batch: 300, loss is 3.6039781093597414 and perplexity is 36.744116197066866
At time: 144.26905035972595 and batch: 350, loss is 3.598633303642273 and perplexity is 36.54824993456818
At time: 144.89537453651428 and batch: 400, loss is 3.519592909812927 and perplexity is 33.77067795358485
At time: 145.5567524433136 and batch: 450, loss is 3.5564230632781983 and perplexity is 35.03764529336098
At time: 146.20240902900696 and batch: 500, loss is 3.4191814422607423 and perplexity is 30.54440242832244
At time: 146.83095455169678 and batch: 550, loss is 3.4841828298568727 and perplexity is 32.59577992555345
At time: 147.4580180644989 and batch: 600, loss is 3.5051906538009643 and perplexity is 33.287789691085706
At time: 148.08728647232056 and batch: 650, loss is 3.336679286956787 and perplexity is 28.125574499273167
At time: 148.71678280830383 and batch: 700, loss is 3.3265103816986086 and perplexity is 27.841017465701885
At time: 149.34489917755127 and batch: 750, loss is 3.440118741989136 and perplexity is 31.19066158962892
At time: 149.9712746143341 and batch: 800, loss is 3.380282163619995 and perplexity is 29.379059645694973
At time: 150.60589957237244 and batch: 850, loss is 3.4460635805130004 and perplexity is 31.376637286259456
At time: 151.22748684883118 and batch: 900, loss is 3.4086724948883056 and perplexity is 30.225093648905734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.276446616812928 and perplexity of 71.98419757577173
finished 12 epochs...
Completing Train Step...
At time: 152.85775089263916 and batch: 50, loss is 3.691044816970825 and perplexity is 40.08670835792454
At time: 153.489750623703 and batch: 100, loss is 3.5711747074127196 and perplexity is 35.55833927181607
At time: 154.1408770084381 and batch: 150, loss is 3.580793409347534 and perplexity is 35.90201454385488
At time: 154.8108468055725 and batch: 200, loss is 3.4630256414413454 and perplexity is 31.91338905869236
At time: 155.4333245754242 and batch: 250, loss is 3.6110757207870483 and perplexity is 37.0058393620218
At time: 156.0558683872223 and batch: 300, loss is 3.5765745639801025 and perplexity is 35.750868551083684
At time: 156.68128085136414 and batch: 350, loss is 3.571878361701965 and perplexity is 35.58336885481711
At time: 157.3002588748932 and batch: 400, loss is 3.4950954961776732 and perplexity is 32.95343472980754
At time: 157.92187309265137 and batch: 450, loss is 3.5340217113494874 and perplexity is 34.261480696213845
At time: 158.5437033176422 and batch: 500, loss is 3.3991119956970213 and perplexity is 29.93750360824397
At time: 159.16697216033936 and batch: 550, loss is 3.465492854118347 and perplexity is 31.992223387281868
At time: 159.7900207042694 and batch: 600, loss is 3.4887490463256836 and perplexity is 32.74495964690967
At time: 160.4126889705658 and batch: 650, loss is 3.3236328315734864 and perplexity is 27.761018697843262
At time: 161.03672003746033 and batch: 700, loss is 3.3157084798812866 and perplexity is 27.5418999559696
At time: 161.66049480438232 and batch: 750, loss is 3.4322767162323 and perplexity is 30.94702018775224
At time: 162.28344058990479 and batch: 800, loss is 3.375155429840088 and perplexity is 29.228826460014513
At time: 162.9055950641632 and batch: 850, loss is 3.4442690992355347 and perplexity is 31.320382986838638
At time: 163.5273470878601 and batch: 900, loss is 3.40905876159668 and perplexity is 30.236770851451908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277383464656464 and perplexity of 72.05166741559148
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 165.11453938484192 and batch: 50, loss is 3.6850524711608887 and perplexity is 39.847213224544305
At time: 165.73744177818298 and batch: 100, loss is 3.5720680475234987 and perplexity is 35.5901191555692
At time: 166.3702483177185 and batch: 150, loss is 3.5838758182525634 and perplexity is 36.012849965482474
At time: 166.99324488639832 and batch: 200, loss is 3.462513871192932 and perplexity is 31.89706091413175
At time: 167.6181356906891 and batch: 250, loss is 3.611689953804016 and perplexity is 37.028576552630284
At time: 168.24086499214172 and batch: 300, loss is 3.5767983293533323 and perplexity is 35.7588692526349
At time: 168.8631591796875 and batch: 350, loss is 3.568539566993713 and perplexity is 35.46476140432875
At time: 169.48826360702515 and batch: 400, loss is 3.49156858921051 and perplexity is 32.83741574561289
At time: 170.10964846611023 and batch: 450, loss is 3.5256006288528443 and perplexity is 33.974173358067894
At time: 170.76028728485107 and batch: 500, loss is 3.3885811519622804 and perplexity is 29.62389063879096
At time: 171.3829493522644 and batch: 550, loss is 3.4521671295166017 and perplexity is 31.568731762182026
At time: 172.0069980621338 and batch: 600, loss is 3.478978853225708 and perplexity is 32.426592853149835
At time: 172.63187503814697 and batch: 650, loss is 3.3094611024856566 and perplexity is 27.370371671606634
At time: 173.25603866577148 and batch: 700, loss is 3.2993853378295896 and perplexity is 27.095978927831563
At time: 173.8809630870819 and batch: 750, loss is 3.4148895835876463 and perplexity is 30.413591082546272
At time: 174.50501132011414 and batch: 800, loss is 3.3571967124938964 and perplexity is 28.70859951330142
At time: 175.12872052192688 and batch: 850, loss is 3.4198855638504027 and perplexity is 30.565916975055032
At time: 175.7518355846405 and batch: 900, loss is 3.3858007478713987 and perplexity is 29.541638651939813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272687938115368 and perplexity of 71.71413995363869
finished 14 epochs...
Completing Train Step...
At time: 177.41807389259338 and batch: 50, loss is 3.676079535484314 and perplexity is 39.49126607703072
At time: 178.0506501197815 and batch: 100, loss is 3.5587805461883546 and perplexity is 35.12034338471963
At time: 178.67404341697693 and batch: 150, loss is 3.569923243522644 and perplexity is 35.51386712767181
At time: 179.29785585403442 and batch: 200, loss is 3.4499417781829833 and perplexity is 31.498558352059643
At time: 179.92227053642273 and batch: 250, loss is 3.599894413948059 and perplexity is 36.59437038459402
At time: 180.54768681526184 and batch: 300, loss is 3.5655775451660157 and perplexity is 35.35986942978389
At time: 181.17236232757568 and batch: 350, loss is 3.5581035804748535 and perplexity is 35.09657616210688
At time: 181.81077909469604 and batch: 400, loss is 3.482138843536377 and perplexity is 32.529222641541125
At time: 182.43487167358398 and batch: 450, loss is 3.517483129501343 and perplexity is 33.699504348865
At time: 183.05971932411194 and batch: 500, loss is 3.3818789863586427 and perplexity is 29.42601027211484
At time: 183.68209600448608 and batch: 550, loss is 3.446184029579163 and perplexity is 31.380416800534814
At time: 184.30409812927246 and batch: 600, loss is 3.474011392593384 and perplexity is 32.2659144419562
At time: 184.92760157585144 and batch: 650, loss is 3.305740785598755 and perplexity is 27.268734394343507
At time: 185.5532410144806 and batch: 700, loss is 3.2971118879318237 and perplexity is 27.034447548065195
At time: 186.17902398109436 and batch: 750, loss is 3.4139382600784303 and perplexity is 30.384671676386283
At time: 186.80227184295654 and batch: 800, loss is 3.3574885320663452 and perplexity is 28.716978467048882
At time: 187.42617225646973 and batch: 850, loss is 3.422092332839966 and perplexity is 30.633443372950076
At time: 188.0496802330017 and batch: 900, loss is 3.3890225982666013 and perplexity is 29.636970882733323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272412025765197 and perplexity of 71.69435586620615
finished 15 epochs...
Completing Train Step...
At time: 189.6326003074646 and batch: 50, loss is 3.6710031509399412 and perplexity is 39.29130120299661
At time: 190.26603078842163 and batch: 100, loss is 3.552665195465088 and perplexity is 34.906225537555464
At time: 190.89026737213135 and batch: 150, loss is 3.5632446241378783 and perplexity is 35.27747379546292
At time: 191.51372861862183 and batch: 200, loss is 3.443494281768799 and perplexity is 31.29612480608026
At time: 192.1378812789917 and batch: 250, loss is 3.593716082572937 and perplexity is 36.36897523782495
At time: 192.76112508773804 and batch: 300, loss is 3.5595685768127443 and perplexity is 35.14803019844165
At time: 193.38281631469727 and batch: 350, loss is 3.552349076271057 and perplexity is 34.89519275360114
At time: 194.0084421634674 and batch: 400, loss is 3.476963758468628 and perplexity is 32.36131598752496
At time: 194.63295102119446 and batch: 450, loss is 3.512637667655945 and perplexity is 33.53660965505572
At time: 195.2662751674652 and batch: 500, loss is 3.3777135372161866 and perplexity is 29.303692653631156
At time: 195.8906533718109 and batch: 550, loss is 3.4423334217071533 and perplexity is 31.259815463818786
At time: 196.51212906837463 and batch: 600, loss is 3.4707506704330444 and perplexity is 32.160875604012084
At time: 197.13508868217468 and batch: 650, loss is 3.303063488006592 and perplexity is 27.19582552033503
At time: 197.7667155265808 and batch: 700, loss is 3.2951132011413575 and perplexity is 26.98046811682094
At time: 198.3899884223938 and batch: 750, loss is 3.412623414993286 and perplexity is 30.344746793438826
At time: 199.01362895965576 and batch: 800, loss is 3.3567607593536377 and perplexity is 28.696086636903225
At time: 199.63499975204468 and batch: 850, loss is 3.42217963218689 and perplexity is 30.63611776928539
At time: 200.25809240341187 and batch: 900, loss is 3.3895240783691407 and perplexity is 29.651836961140265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272704242026969 and perplexity of 71.71530918416855
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 201.85212349891663 and batch: 50, loss is 3.669357213973999 and perplexity is 39.226683390917465
At time: 202.47595810890198 and batch: 100, loss is 3.5531052207946776 and perplexity is 34.92158854075975
At time: 203.09742712974548 and batch: 150, loss is 3.564660220146179 and perplexity is 35.32744780971296
At time: 203.73571634292603 and batch: 200, loss is 3.443825750350952 and perplexity is 31.30650020766145
At time: 204.35723233222961 and batch: 250, loss is 3.5954385375976563 and perplexity is 36.43167314362739
At time: 204.97806644439697 and batch: 300, loss is 3.561098461151123 and perplexity is 35.20184377314357
At time: 205.6084086894989 and batch: 350, loss is 3.553042798042297 and perplexity is 34.91940870712185
At time: 206.27101254463196 and batch: 400, loss is 3.477135462760925 and perplexity is 32.36687304145576
At time: 206.89403128623962 and batch: 450, loss is 3.5114246892929075 and perplexity is 33.49595513468593
At time: 207.5159649848938 and batch: 500, loss is 3.3736663627624512 and perplexity is 29.185335165578483
At time: 208.139142036438 and batch: 550, loss is 3.4382097816467283 and perplexity is 31.131176648861548
At time: 208.76505088806152 and batch: 600, loss is 3.467655506134033 and perplexity is 32.061486302472645
At time: 209.38704442977905 and batch: 650, loss is 3.2975529527664182 and perplexity is 27.046374122204213
At time: 210.00835156440735 and batch: 700, loss is 3.288020315170288 and perplexity is 26.789775811870964
At time: 210.63124060630798 and batch: 750, loss is 3.405014228820801 and perplexity is 30.114724217883648
At time: 211.25062012672424 and batch: 800, loss is 3.3490807104110716 and perplexity is 28.476543417518158
At time: 211.87231016159058 and batch: 850, loss is 3.412491116523743 and perplexity is 30.34073249542798
At time: 212.49398183822632 and batch: 900, loss is 3.380185227394104 and perplexity is 29.37621188856033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270659982341609 and perplexity of 71.56885421577694
finished 17 epochs...
Completing Train Step...
At time: 214.04253721237183 and batch: 50, loss is 3.6659001684188843 and perplexity is 39.09130909184179
At time: 214.67127203941345 and batch: 100, loss is 3.5491775465011597 and perplexity is 34.784696923982764
At time: 215.29339170455933 and batch: 150, loss is 3.5609362077713014 and perplexity is 35.196132618355044
At time: 215.91643404960632 and batch: 200, loss is 3.440818552970886 and perplexity is 31.212496796505057
At time: 216.54223942756653 and batch: 250, loss is 3.5923533821105957 and perplexity is 36.31944897085093
At time: 217.1659550666809 and batch: 300, loss is 3.5577465867996216 and perplexity is 35.08404914256113
At time: 217.78880882263184 and batch: 350, loss is 3.5497123336791994 and perplexity is 34.803304308941684
At time: 218.41134095191956 and batch: 400, loss is 3.474329209327698 and perplexity is 32.276170719235736
At time: 219.03401494026184 and batch: 450, loss is 3.508873291015625 and perplexity is 33.41060254298823
At time: 219.65760493278503 and batch: 500, loss is 3.37196014881134 and perplexity is 29.13558119707888
At time: 220.2819321155548 and batch: 550, loss is 3.436652970314026 and perplexity is 31.08274898640213
At time: 220.9051103591919 and batch: 600, loss is 3.4664211320877074 and perplexity is 32.02193485155847
At time: 221.52902746200562 and batch: 650, loss is 3.2969531869888304 and perplexity is 27.03015749617266
At time: 222.15023255348206 and batch: 700, loss is 3.287969455718994 and perplexity is 26.788413333220635
At time: 222.7713761329651 and batch: 750, loss is 3.4053174591064455 and perplexity is 30.12385729895375
At time: 223.39532232284546 and batch: 800, loss is 3.3498493576049806 and perplexity is 28.49844024709848
At time: 224.02413845062256 and batch: 850, loss is 3.4135695552825926 and perplexity is 30.373470767260596
At time: 224.6520824432373 and batch: 900, loss is 3.3817405462265016 and perplexity is 29.42193681333597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27013156838613 and perplexity of 71.53104622444654
finished 18 epochs...
Completing Train Step...
At time: 226.22553086280823 and batch: 50, loss is 3.6639540004730224 and perplexity is 39.01530482165195
At time: 226.85866904258728 and batch: 100, loss is 3.5469108629226684 and perplexity is 34.705940314559015
At time: 227.47924780845642 and batch: 150, loss is 3.5586137866973875 and perplexity is 35.1144872224331
At time: 228.1009693145752 and batch: 200, loss is 3.4387096977233886 and perplexity is 31.14674351529286
At time: 228.7236626148224 and batch: 250, loss is 3.590266184806824 and perplexity is 36.243722170762275
At time: 229.35529232025146 and batch: 300, loss is 3.5556363582611086 and perplexity is 35.01009184166211
At time: 229.97716879844666 and batch: 350, loss is 3.547734603881836 and perplexity is 34.7345407971998
At time: 230.6014130115509 and batch: 400, loss is 3.4725372076034544 and perplexity is 32.21838355846494
At time: 231.2227509021759 and batch: 450, loss is 3.5072689342498777 and perplexity is 33.35704299255261
At time: 231.84400391578674 and batch: 500, loss is 3.3707995319366457 and perplexity is 29.10178556557128
At time: 232.46252751350403 and batch: 550, loss is 3.435665192604065 and perplexity is 31.052061298591216
At time: 233.08067679405212 and batch: 600, loss is 3.4656571197509765 and perplexity is 31.997479041745688
At time: 233.73382353782654 and batch: 650, loss is 3.296487083435059 and perplexity is 27.017561579433504
At time: 234.40445351600647 and batch: 700, loss is 3.287785110473633 and perplexity is 26.78347547174151
At time: 235.05631589889526 and batch: 750, loss is 3.405409336090088 and perplexity is 30.126625115245414
At time: 235.67742443084717 and batch: 800, loss is 3.350160336494446 and perplexity is 28.507304038552594
At time: 236.29752254486084 and batch: 850, loss is 3.4141156339645384 and perplexity is 30.390061601681513
At time: 236.91929054260254 and batch: 900, loss is 3.382481927871704 and perplexity is 29.443757785090177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269976472201413 and perplexity of 71.51995289237726
finished 19 epochs...
Completing Train Step...
At time: 238.50076866149902 and batch: 50, loss is 3.662445640563965 and perplexity is 38.95650006054051
At time: 239.12503266334534 and batch: 100, loss is 3.5451593351364137 and perplexity is 34.64520510098426
At time: 239.74899101257324 and batch: 150, loss is 3.556792368888855 and perplexity is 35.05058728197944
At time: 240.3995373249054 and batch: 200, loss is 3.436989731788635 and perplexity is 31.09321822150428
At time: 241.06916666030884 and batch: 250, loss is 3.588613314628601 and perplexity is 36.18386548453152
At time: 241.70773267745972 and batch: 300, loss is 3.5540000867843626 and perplexity is 34.95285266916693
At time: 242.36784839630127 and batch: 350, loss is 3.5461745834350586 and perplexity is 34.68039644747384
At time: 243.02198934555054 and batch: 400, loss is 3.4711180257797243 and perplexity is 32.17269224393916
At time: 243.67486000061035 and batch: 450, loss is 3.505976939201355 and perplexity is 33.31397368682885
At time: 244.30027556419373 and batch: 500, loss is 3.3697840213775634 and perplexity is 29.072247395741435
At time: 244.937109708786 and batch: 550, loss is 3.434804081916809 and perplexity is 31.025333546173883
At time: 245.55958151817322 and batch: 600, loss is 3.4649716138839723 and perplexity is 31.975552098516
At time: 246.18443155288696 and batch: 650, loss is 3.295992832183838 and perplexity is 27.004211415257373
At time: 246.80672669410706 and batch: 700, loss is 3.287482671737671 and perplexity is 26.775376336083085
At time: 247.42907428741455 and batch: 750, loss is 3.405321192741394 and perplexity is 30.12396977065012
At time: 248.05284762382507 and batch: 800, loss is 3.350210542678833 and perplexity is 28.50873531744476
At time: 248.7061083316803 and batch: 850, loss is 3.4143653774261473 and perplexity is 30.3976522686847
At time: 249.3370542526245 and batch: 900, loss is 3.3828443717956542 and perplexity is 29.45443143037973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269963512681935 and perplexity of 71.51902603416052
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f74eca12b70>
ELAPSED
1302.151937007904


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.7917024081235}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.41628114957778306, 'tune_wordvecs': 'TRUE', 'dropout': 0.7387750863627058, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.03583228815741}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.09083670716130898, 'tune_wordvecs': 'TRUE', 'dropout': 0.021912257622493048, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.30896956149321}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.9388904092742721, 'tune_wordvecs': 'TRUE', 'dropout': 0.7397063209606896, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.51902603416052}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.5901454392767163, 'tune_wordvecs': 'TRUE', 'dropout': 0.30661194625441784, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0165200233459473 and batch: 50, loss is 7.071221971511841 and perplexity is 1177.5861321190987
At time: 1.8114464282989502 and batch: 100, loss is 6.135786256790161 and perplexity is 462.1022822777678
At time: 2.6071012020111084 and batch: 150, loss is 5.8872900390625 and perplexity is 360.42721603109914
At time: 3.403441905975342 and batch: 200, loss is 5.686471090316773 and perplexity is 294.85127902382015
At time: 4.201093435287476 and batch: 250, loss is 5.7065299606323245 and perplexity is 300.8253791324226
At time: 4.9979188442230225 and batch: 300, loss is 5.5839122295379635 and perplexity is 266.11065793203653
At time: 5.794136047363281 and batch: 350, loss is 5.535748462677002 and perplexity is 253.59752490497138
At time: 6.588219165802002 and batch: 400, loss is 5.37347092628479 and perplexity is 215.6099366400633
At time: 7.39290452003479 and batch: 450, loss is 5.3578190517425535 and perplexity is 212.2615098975385
At time: 8.199232578277588 and batch: 500, loss is 5.2857170581817625 and perplexity is 197.49574862927372
At time: 8.99574899673462 and batch: 550, loss is 5.328296880722046 and perplexity is 206.08668489149335
At time: 9.790206909179688 and batch: 600, loss is 5.236670560836792 and perplexity is 188.0429813378234
At time: 10.586941957473755 and batch: 650, loss is 5.120801868438721 and perplexity is 167.4696043848025
At time: 11.385244369506836 and batch: 700, loss is 5.2055085754394534 and perplexity is 182.273549050652
At time: 12.182428121566772 and batch: 750, loss is 5.178610191345215 and perplexity is 177.43603740859297
At time: 12.986321687698364 and batch: 800, loss is 5.150298042297363 and perplexity is 172.4828898536036
At time: 13.781537771224976 and batch: 850, loss is 5.187175064086914 and perplexity is 178.96228120066507
At time: 14.577269077301025 and batch: 900, loss is 5.081240634918213 and perplexity is 160.97364161722257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.028870883053297 and perplexity of 152.7604308901258
finished 1 epochs...
Completing Train Step...
At time: 16.26608419418335 and batch: 50, loss is 4.9715384674072265 and perplexity is 144.24863857766783
At time: 16.89059042930603 and batch: 100, loss is 4.8394391727447506 and perplexity is 126.39844415601941
At time: 17.513460636138916 and batch: 150, loss is 4.8131555080413815 and perplexity is 123.11950980002035
At time: 18.136292457580566 and batch: 200, loss is 4.69796459197998 and perplexity is 109.72361269038385
At time: 18.756940841674805 and batch: 250, loss is 4.798992652893066 and perplexity is 121.38807598172721
At time: 19.37736988067627 and batch: 300, loss is 4.736488723754883 and perplexity is 114.03309619435669
At time: 19.997939109802246 and batch: 350, loss is 4.719807062149048 and perplexity is 112.14661325379258
At time: 20.6246817111969 and batch: 400, loss is 4.58610878944397 and perplexity is 98.11191230700585
At time: 21.247737169265747 and batch: 450, loss is 4.604831066131592 and perplexity is 99.96609376381394
At time: 21.870718002319336 and batch: 500, loss is 4.50541356086731 and perplexity is 90.50576595328232
At time: 22.49975347518921 and batch: 550, loss is 4.572242746353149 and perplexity is 96.76087670872333
At time: 23.166662216186523 and batch: 600, loss is 4.527704305648804 and perplexity is 92.54586001913982
At time: 23.789801597595215 and batch: 650, loss is 4.391730127334594 and perplexity is 80.78005793538246
At time: 24.414244651794434 and batch: 700, loss is 4.445401630401611 and perplexity is 85.23410357106313
At time: 25.03722858428955 and batch: 750, loss is 4.484661064147949 and perplexity is 88.64690010799787
At time: 25.65949058532715 and batch: 800, loss is 4.436599206924439 and perplexity is 84.48712931173512
At time: 26.283971309661865 and batch: 850, loss is 4.492446355819702 and perplexity is 89.33973554467586
At time: 26.907915115356445 and batch: 900, loss is 4.430942459106445 and perplexity is 84.0105561253319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.563897955907534 and perplexity of 95.95678711492975
finished 2 epochs...
Completing Train Step...
At time: 28.46988606452942 and batch: 50, loss is 4.479611864089966 and perplexity is 88.20043227607015
At time: 29.102248430252075 and batch: 100, loss is 4.350097370147705 and perplexity is 77.48600738193635
At time: 29.725637674331665 and batch: 150, loss is 4.3482514047622685 and perplexity is 77.34310283345853
At time: 30.347885131835938 and batch: 200, loss is 4.252563462257386 and perplexity is 70.28535546543482
At time: 30.971145153045654 and batch: 250, loss is 4.386491460800171 and perplexity is 80.35798466505376
At time: 31.594460248947144 and batch: 300, loss is 4.349054970741272 and perplexity is 77.40527809722377
At time: 32.218427658081055 and batch: 350, loss is 4.345489330291748 and perplexity is 77.12977017961707
At time: 32.84113144874573 and batch: 400, loss is 4.244881730079651 and perplexity is 69.74751062355176
At time: 33.464017391204834 and batch: 450, loss is 4.28292818069458 and perplexity is 72.4522830751474
At time: 34.08739757537842 and batch: 500, loss is 4.161630868911743 and perplexity is 64.17610010719662
At time: 34.7104127407074 and batch: 550, loss is 4.239947576522827 and perplexity is 69.40421333319107
At time: 35.35182285308838 and batch: 600, loss is 4.233727164268494 and perplexity is 68.97383048120594
At time: 35.98497772216797 and batch: 650, loss is 4.085924258232117 and perplexity is 59.49690284860246
At time: 36.60722064971924 and batch: 700, loss is 4.121083388328552 and perplexity is 61.625970969035386
At time: 37.22991228103638 and batch: 750, loss is 4.1999396705627445 and perplexity is 66.68230801345553
At time: 37.852890968322754 and batch: 800, loss is 4.150874166488648 and perplexity is 63.489476419575205
At time: 38.47871947288513 and batch: 850, loss is 4.2214592170715335 and perplexity is 68.13283237871605
At time: 39.10453224182129 and batch: 900, loss is 4.168862404823304 and perplexity is 64.64187398022416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411821757277397 and perplexity of 82.41947509891699
finished 3 epochs...
Completing Train Step...
At time: 40.67327809333801 and batch: 50, loss is 4.247021155357361 and perplexity is 69.89688994680967
At time: 41.31016945838928 and batch: 100, loss is 4.114159746170044 and perplexity is 61.20076847191263
At time: 41.9361846446991 and batch: 150, loss is 4.120563325881958 and perplexity is 61.59392994818749
At time: 42.563512563705444 and batch: 200, loss is 4.020446572303772 and perplexity is 55.725985952921484
At time: 43.190377950668335 and batch: 250, loss is 4.169783201217651 and perplexity is 64.70142339694563
At time: 43.81600332260132 and batch: 300, loss is 4.138270201683045 and perplexity is 62.6942791395761
At time: 44.441081047058105 and batch: 350, loss is 4.136728959083557 and perplexity is 62.59772647047821
At time: 45.073824882507324 and batch: 400, loss is 4.048521938323975 and perplexity is 57.312682730152936
At time: 45.69627642631531 and batch: 450, loss is 4.095829935073852 and perplexity is 60.0891885938648
At time: 46.32010626792908 and batch: 500, loss is 3.959251070022583 and perplexity is 52.41805379304374
At time: 46.97478246688843 and batch: 550, loss is 4.044966716766357 and perplexity is 57.10928522082929
At time: 47.602479696273804 and batch: 600, loss is 4.05548360824585 and perplexity is 57.713066763353325
At time: 48.2277934551239 and batch: 650, loss is 3.902212390899658 and perplexity is 49.511867627993084
At time: 48.85373568534851 and batch: 700, loss is 3.9283125019073486 and perplexity is 50.82114468335048
At time: 49.48082876205444 and batch: 750, loss is 4.01956814289093 and perplexity is 55.6770561016574
At time: 50.108747720718384 and batch: 800, loss is 3.975548715591431 and perplexity is 53.279344093100676
At time: 50.7356858253479 and batch: 850, loss is 4.053223934173584 and perplexity is 57.58280127695762
At time: 51.36107778549194 and batch: 900, loss is 4.001835398674011 and perplexity is 54.698451423681114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346859866625642 and perplexity of 77.23555180429504
finished 4 epochs...
Completing Train Step...
At time: 52.95582699775696 and batch: 50, loss is 4.090831093788147 and perplexity is 59.78956179443783
At time: 53.58050537109375 and batch: 100, loss is 3.956764750480652 and perplexity is 52.28788764592067
At time: 54.20621633529663 and batch: 150, loss is 3.9671341609954833 and perplexity is 52.83290307871254
At time: 54.8314950466156 and batch: 200, loss is 3.8651835107803345 and perplexity is 47.7120273007191
At time: 55.45629382133484 and batch: 250, loss is 4.017738919258118 and perplexity is 55.57530340741991
At time: 56.08151125907898 and batch: 300, loss is 3.9950917530059815 and perplexity is 54.33082541167372
At time: 56.70701503753662 and batch: 350, loss is 3.9920460033416747 and perplexity is 54.165599065187365
At time: 57.333510875701904 and batch: 400, loss is 3.9114244890213015 and perplexity is 49.97008313341859
At time: 57.95881986618042 and batch: 450, loss is 3.959973955154419 and perplexity is 52.45595972393674
At time: 58.58146691322327 and batch: 500, loss is 3.8239575624465942 and perplexity is 45.785047428106225
At time: 59.20652985572815 and batch: 550, loss is 3.905263915061951 and perplexity is 49.663185045337976
At time: 59.832385540008545 and batch: 600, loss is 3.9270881843566894 and perplexity is 50.758961537695825
At time: 60.46704125404358 and batch: 650, loss is 3.7722912549972536 and perplexity is 43.47957358261076
At time: 61.09350299835205 and batch: 700, loss is 3.792787103652954 and perplexity is 44.37991950134371
At time: 61.720130920410156 and batch: 750, loss is 3.8911198234558104 and perplexity is 48.96568876039381
At time: 62.34659194946289 and batch: 800, loss is 3.846166729927063 and perplexity is 46.81327094500875
At time: 62.97259569168091 and batch: 850, loss is 3.924876961708069 and perplexity is 50.64684617402132
At time: 63.59869861602783 and batch: 900, loss is 3.8798437547683715 and perplexity is 48.41664960969397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32402101281571 and perplexity of 75.49157139084343
finished 5 epochs...
Completing Train Step...
At time: 65.16315174102783 and batch: 50, loss is 3.9733310079574586 and perplexity is 53.161317008129714
At time: 65.796062707901 and batch: 100, loss is 3.8426647281646726 and perplexity is 46.6496175122151
At time: 66.42151522636414 and batch: 150, loss is 3.8499804592132567 and perplexity is 46.99214495912417
At time: 67.0741057395935 and batch: 200, loss is 3.7510015869140627 and perplexity is 42.5636918945601
At time: 67.70956182479858 and batch: 250, loss is 3.9063013696670534 and perplexity is 49.71473508115469
At time: 68.33337044715881 and batch: 300, loss is 3.885690007209778 and perplexity is 48.70053458873821
At time: 68.95956468582153 and batch: 350, loss is 3.8807209396362303 and perplexity is 48.45913859470693
At time: 69.58641719818115 and batch: 400, loss is 3.8038507652282716 and perplexity is 44.87365010939333
At time: 70.21181917190552 and batch: 450, loss is 3.852710108757019 and perplexity is 47.12059227449523
At time: 70.83758544921875 and batch: 500, loss is 3.7192974710464477 and perplexity is 41.23541485757204
At time: 71.46170592308044 and batch: 550, loss is 3.795864682197571 and perplexity is 44.51671257716096
At time: 72.0866801738739 and batch: 600, loss is 3.8222111558914182 and perplexity is 45.70515790125026
At time: 72.7138774394989 and batch: 650, loss is 3.6699068355560303 and perplexity is 39.24824914866067
At time: 73.33736181259155 and batch: 700, loss is 3.6876222896575928 and perplexity is 39.94974501775382
At time: 73.96242332458496 and batch: 750, loss is 3.788898677825928 and perplexity is 44.20768655069153
At time: 74.58765625953674 and batch: 800, loss is 3.7468241930007933 and perplexity is 42.38625745192217
At time: 75.21406435966492 and batch: 850, loss is 3.8251734161376953 and perplexity is 45.84074920276109
At time: 75.84068870544434 and batch: 900, loss is 3.7800245761871336 and perplexity is 43.81711858004743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316951385916096 and perplexity of 74.95975622942163
finished 6 epochs...
Completing Train Step...
At time: 77.41906094551086 and batch: 50, loss is 3.877551636695862 and perplexity is 48.30580002081806
At time: 78.05273175239563 and batch: 100, loss is 3.7459211111068726 and perplexity is 42.34799646926695
At time: 78.67866086959839 and batch: 150, loss is 3.7579510736465456 and perplexity is 42.860517906272854
At time: 79.30381870269775 and batch: 200, loss is 3.6616142654418944 and perplexity is 38.92412605488339
At time: 79.92843699455261 and batch: 250, loss is 3.811818881034851 and perplexity is 45.23263687482576
At time: 80.55847954750061 and batch: 300, loss is 3.7930833101272583 and perplexity is 44.393067067930154
At time: 81.1842520236969 and batch: 350, loss is 3.788855233192444 and perplexity is 44.205766005671144
At time: 81.80947422981262 and batch: 400, loss is 3.715176138877869 and perplexity is 41.06581973480543
At time: 82.43567895889282 and batch: 450, loss is 3.765355830192566 and perplexity is 43.179067542541155
At time: 83.06061673164368 and batch: 500, loss is 3.6371189403533934 and perplexity is 37.98224981076533
At time: 83.68609952926636 and batch: 550, loss is 3.7100541734695436 and perplexity is 40.85601977938313
At time: 84.30897831916809 and batch: 600, loss is 3.7405397272109986 and perplexity is 42.120717728607836
At time: 84.93425130844116 and batch: 650, loss is 3.5884762525558473 and perplexity is 36.1789063887877
At time: 85.55901551246643 and batch: 700, loss is 3.603701868057251 and perplexity is 36.73396735637875
At time: 86.18490314483643 and batch: 750, loss is 3.7054586601257324 and perplexity is 40.6686961491031
At time: 86.81093525886536 and batch: 800, loss is 3.66702157497406 and perplexity is 39.1351709309882
At time: 87.48484468460083 and batch: 850, loss is 3.740809578895569 and perplexity is 42.13208560899407
At time: 88.15578985214233 and batch: 900, loss is 3.7008713817596437 and perplexity is 40.48256476371868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318853090887201 and perplexity of 75.10244320173614
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 89.75901794433594 and batch: 50, loss is 3.821211395263672 and perplexity is 45.65948651792522
At time: 90.38540053367615 and batch: 100, loss is 3.6909980010986327 and perplexity is 40.08483170763831
At time: 91.01351356506348 and batch: 150, loss is 3.69869179725647 and perplexity is 40.39442568109296
At time: 91.63894009590149 and batch: 200, loss is 3.582210450172424 and perplexity is 35.952925226898394
At time: 92.27292656898499 and batch: 250, loss is 3.7290036821365358 and perplexity is 41.637603203532315
At time: 92.89803791046143 and batch: 300, loss is 3.7011652278900145 and perplexity is 40.49446215663781
At time: 93.5246467590332 and batch: 350, loss is 3.6894204378128053 and perplexity is 40.02164520227766
At time: 94.14946937561035 and batch: 400, loss is 3.607093930244446 and perplexity is 36.858782829278866
At time: 94.77398872375488 and batch: 450, loss is 3.6408725118637086 and perplexity is 38.1250868082889
At time: 95.39838314056396 and batch: 500, loss is 3.50041944026947 and perplexity is 33.129344826195855
At time: 96.02400875091553 and batch: 550, loss is 3.560629754066467 and perplexity is 35.18534828565207
At time: 96.65099263191223 and batch: 600, loss is 3.5857111167907716 and perplexity is 36.079004984916786
At time: 97.27494406700134 and batch: 650, loss is 3.420850925445557 and perplexity is 30.59543838454826
At time: 97.90027499198914 and batch: 700, loss is 3.4240587663650515 and perplexity is 30.69374126945975
At time: 98.52684998512268 and batch: 750, loss is 3.508165783882141 and perplexity is 33.386972663494916
At time: 99.15439295768738 and batch: 800, loss is 3.456662640571594 and perplexity is 31.710968819396253
At time: 99.77999353408813 and batch: 850, loss is 3.518283977508545 and perplexity is 33.726503339367355
At time: 100.40604066848755 and batch: 900, loss is 3.4673233318328855 and perplexity is 32.05083806929898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270559232528895 and perplexity of 71.56164403033706
finished 8 epochs...
Completing Train Step...
At time: 101.97555994987488 and batch: 50, loss is 3.731628623008728 and perplexity is 41.74704302369032
At time: 102.61502766609192 and batch: 100, loss is 3.5941569232940673 and perplexity is 36.385011697598095
At time: 103.24219131469727 and batch: 150, loss is 3.606140432357788 and perplexity is 36.823654807654975
At time: 103.86968970298767 and batch: 200, loss is 3.4928218698501587 and perplexity is 32.8785960428674
At time: 104.49737119674683 and batch: 250, loss is 3.641920576095581 and perplexity is 38.165065294460895
At time: 105.12627029418945 and batch: 300, loss is 3.618165168762207 and perplexity is 37.26912249828197
At time: 105.75542998313904 and batch: 350, loss is 3.61159140586853 and perplexity is 37.02492764265627
At time: 106.3873884677887 and batch: 400, loss is 3.5328970432281492 and perplexity is 34.22296956126719
At time: 107.02099442481995 and batch: 450, loss is 3.571858525276184 and perplexity is 35.58266301496248
At time: 107.65205001831055 and batch: 500, loss is 3.4345447635650634 and perplexity is 31.01728915089107
At time: 108.28985500335693 and batch: 550, loss is 3.4973265647888185 and perplexity is 33.02703818030555
At time: 108.9209372997284 and batch: 600, loss is 3.5296950197219847 and perplexity is 34.1135620644487
At time: 109.54492807388306 and batch: 650, loss is 3.368570146560669 and perplexity is 29.036978736956257
At time: 110.16928052902222 and batch: 700, loss is 3.377355933189392 and perplexity is 29.29321540860243
At time: 110.79325985908508 and batch: 750, loss is 3.4661226511001586 and perplexity is 32.01237833911034
At time: 111.41896867752075 and batch: 800, loss is 3.4213891220092774 and perplexity is 30.611909176216518
At time: 112.04888367652893 and batch: 850, loss is 3.488310179710388 and perplexity is 32.73059213024375
At time: 112.6808819770813 and batch: 900, loss is 3.444300036430359 and perplexity is 31.32135196661775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.276974194670377 and perplexity of 72.02218486424397
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 114.27975702285767 and batch: 50, loss is 3.7020140075683594 and perplexity is 40.528847623980475
At time: 114.92280435562134 and batch: 100, loss is 3.577872200012207 and perplexity is 35.79729027903406
At time: 115.5551164150238 and batch: 150, loss is 3.5929233741760256 and perplexity is 36.34015666963646
At time: 116.18542742729187 and batch: 200, loss is 3.473073134422302 and perplexity is 32.235654881943184
At time: 116.81730008125305 and batch: 250, loss is 3.623203339576721 and perplexity is 37.45736450304126
At time: 117.45488739013672 and batch: 300, loss is 3.595239591598511 and perplexity is 36.424425928939385
At time: 118.08927488327026 and batch: 350, loss is 3.585076537132263 and perplexity is 36.05611724506926
At time: 118.72592854499817 and batch: 400, loss is 3.504757113456726 and perplexity is 33.27336121917243
At time: 119.36329126358032 and batch: 450, loss is 3.5362525177001953 and perplexity is 34.33799673944138
At time: 120.00043797492981 and batch: 500, loss is 3.398077974319458 and perplexity is 29.906563588599784
At time: 120.63728737831116 and batch: 550, loss is 3.4559839868545534 and perplexity is 31.689455353458527
At time: 121.27366614341736 and batch: 600, loss is 3.4855435609817507 and perplexity is 32.64016420852702
At time: 121.91073942184448 and batch: 650, loss is 3.3188865566253662 and perplexity is 27.629569464130217
At time: 122.54231119155884 and batch: 700, loss is 3.32141685962677 and perplexity is 27.6995691695578
At time: 123.17252254486084 and batch: 750, loss is 3.404155035018921 and perplexity is 30.088860945862876
At time: 123.85746550559998 and batch: 800, loss is 3.355716004371643 and perplexity is 28.66612191304841
At time: 124.50330758094788 and batch: 850, loss is 3.4168773651123048 and perplexity is 30.47410678306822
At time: 125.1597797870636 and batch: 900, loss is 3.3780402088165284 and perplexity is 29.313266901538544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263142624946489 and perplexity of 71.03286270425471
finished 10 epochs...
Completing Train Step...
At time: 126.82505512237549 and batch: 50, loss is 3.6752432441711425 and perplexity is 39.45825368017874
At time: 127.50406837463379 and batch: 100, loss is 3.5432235527038576 and perplexity is 34.57820439189767
At time: 128.17381310462952 and batch: 150, loss is 3.556374773979187 and perplexity is 35.035953390881474
At time: 128.8498113155365 and batch: 200, loss is 3.4391768169403076 and perplexity is 31.161296156377837
At time: 129.4952757358551 and batch: 250, loss is 3.589279851913452 and perplexity is 36.20799141951139
At time: 130.15895104408264 and batch: 300, loss is 3.563098945617676 and perplexity is 35.27233499959909
At time: 130.8184516429901 and batch: 350, loss is 3.5549095058441162 and perplexity is 34.98465391771606
At time: 131.45604872703552 and batch: 400, loss is 3.4768583250045775 and perplexity is 32.357904201740524
At time: 132.11690831184387 and batch: 450, loss is 3.510832462310791 and perplexity is 33.47612379916903
At time: 132.79588627815247 and batch: 500, loss is 3.374367108345032 and perplexity is 29.20579382759324
At time: 133.4283058643341 and batch: 550, loss is 3.434330997467041 and perplexity is 31.010659414649723
At time: 134.05589199066162 and batch: 600, loss is 3.4678948068618776 and perplexity is 32.06915955755174
At time: 134.68236875534058 and batch: 650, loss is 3.303899359703064 and perplexity is 27.218567244407318
At time: 135.30660343170166 and batch: 700, loss is 3.3096839809417724 and perplexity is 27.37647261764758
At time: 135.92915534973145 and batch: 750, loss is 3.3954103708267214 and perplexity is 29.8268910497331
At time: 136.55167412757874 and batch: 800, loss is 3.35023756980896 and perplexity is 28.509505837156357
At time: 137.17365789413452 and batch: 850, loss is 3.414407343864441 and perplexity is 30.39892797665118
At time: 137.79582500457764 and batch: 900, loss is 3.3783146381378173 and perplexity is 29.321312425392424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264805205880779 and perplexity of 71.15105881556917
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 139.36365723609924 and batch: 50, loss is 3.6664664220809935 and perplexity is 39.1134509571362
At time: 139.99444270133972 and batch: 100, loss is 3.5409727621078493 and perplexity is 34.500463616548515
At time: 140.6207869052887 and batch: 150, loss is 3.558005862236023 and perplexity is 35.09314675405592
At time: 141.2451207637787 and batch: 200, loss is 3.4379411602020262 and perplexity is 31.122815270287926
At time: 141.86812543869019 and batch: 250, loss is 3.587385721206665 and perplexity is 36.13947366241172
At time: 142.49138498306274 and batch: 300, loss is 3.55859637260437 and perplexity is 35.11387574081056
At time: 143.11870670318604 and batch: 350, loss is 3.5505551290512085 and perplexity is 34.83264873668087
At time: 143.74479722976685 and batch: 400, loss is 3.470879702568054 and perplexity is 32.165025658194466
At time: 144.3708724975586 and batch: 450, loss is 3.502491865158081 and perplexity is 33.19807409848524
At time: 144.99721479415894 and batch: 500, loss is 3.3673260164260865 and perplexity is 29.000875419961602
At time: 145.62349343299866 and batch: 550, loss is 3.422202854156494 and perplexity is 30.636829208541496
At time: 146.24941611289978 and batch: 600, loss is 3.45624915599823 and perplexity is 31.69785953341354
At time: 146.8751926422119 and batch: 650, loss is 3.287968626022339 and perplexity is 26.78839110697292
At time: 147.50044894218445 and batch: 700, loss is 3.2919487857818606 and perplexity is 26.89522565152034
At time: 148.12622594833374 and batch: 750, loss is 3.375226812362671 and perplexity is 29.230912961848368
At time: 148.752756357193 and batch: 800, loss is 3.328396396636963 and perplexity is 27.893575587663427
At time: 149.37846994400024 and batch: 850, loss is 3.388231496810913 and perplexity is 29.61353430350407
At time: 150.004798412323 and batch: 900, loss is 3.3536052942276 and perplexity is 28.60567984901461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2621961619755995 and perplexity of 70.96566453529444
finished 12 epochs...
Completing Train Step...
At time: 151.57369232177734 and batch: 50, loss is 3.6587979316711428 and perplexity is 38.81465694730695
At time: 152.21041321754456 and batch: 100, loss is 3.527899594306946 and perplexity is 34.05236865865703
At time: 152.835116147995 and batch: 150, loss is 3.542692279815674 and perplexity is 34.55983880838245
At time: 153.46046781539917 and batch: 200, loss is 3.424429121017456 and perplexity is 30.705110944614685
At time: 154.0863094329834 and batch: 250, loss is 3.5747690629959106 and perplexity is 35.686378558620305
At time: 154.71107959747314 and batch: 300, loss is 3.5472890281677247 and perplexity is 34.71906737692591
At time: 155.33835554122925 and batch: 350, loss is 3.53967511177063 and perplexity is 34.45572311334501
At time: 155.97372817993164 and batch: 400, loss is 3.4608841800689696 and perplexity is 31.84512089167189
At time: 156.61645698547363 and batch: 450, loss is 3.4938704538345338 and perplexity is 32.91309009390082
At time: 157.24259614944458 and batch: 500, loss is 3.3592141914367675 and perplexity is 28.76657697278557
At time: 157.86719250679016 and batch: 550, loss is 3.4154261445999143 and perplexity is 30.42991420854912
At time: 158.49053764343262 and batch: 600, loss is 3.451186456680298 and perplexity is 31.537788339633178
At time: 159.11599683761597 and batch: 650, loss is 3.283945140838623 and perplexity is 26.68082495278284
At time: 159.74074053764343 and batch: 700, loss is 3.2892723751068114 and perplexity is 26.823339224190747
At time: 160.3658802509308 and batch: 750, loss is 3.3739885759353636 and perplexity is 29.19474058021757
At time: 160.9900770187378 and batch: 800, loss is 3.328557744026184 and perplexity is 27.898076506356517
At time: 161.61651515960693 and batch: 850, loss is 3.3901071691513063 and perplexity is 29.669131715662544
At time: 162.24312543869019 and batch: 900, loss is 3.3567055654525757 and perplexity is 28.694502831645117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262279353729666 and perplexity of 70.97156853898446
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 163.82149958610535 and batch: 50, loss is 3.6560610151290893 and perplexity is 38.70856971280964
At time: 164.44585347175598 and batch: 100, loss is 3.526309080123901 and perplexity is 33.99825093225349
At time: 165.07235193252563 and batch: 150, loss is 3.5418399286270144 and perplexity is 34.53039423903372
At time: 165.69675254821777 and batch: 200, loss is 3.4230345392227175 and perplexity is 30.662320000556875
At time: 166.32152104377747 and batch: 250, loss is 3.573771824836731 and perplexity is 35.65080847902689
At time: 166.94807505607605 and batch: 300, loss is 3.5452616930007936 and perplexity is 34.648751491686795
At time: 167.57341885566711 and batch: 350, loss is 3.5381908321380617 and perplexity is 34.404619120992166
At time: 168.19793009757996 and batch: 400, loss is 3.4592777824401857 and perplexity is 31.794006031370245
At time: 168.82951068878174 and batch: 450, loss is 3.491078953742981 and perplexity is 32.82134131782796
At time: 169.45448541641235 and batch: 500, loss is 3.3554004049301147 and perplexity is 28.657076328447673
At time: 170.08019495010376 and batch: 550, loss is 3.4103465986251833 and perplexity is 30.27573596955235
At time: 170.7048168182373 and batch: 600, loss is 3.446155323982239 and perplexity is 31.37951601986762
At time: 171.33778262138367 and batch: 650, loss is 3.2785661363601686 and perplexity is 26.53769397207485
At time: 171.96197748184204 and batch: 700, loss is 3.282304997444153 and perplexity is 26.637100441013928
At time: 172.59041500091553 and batch: 750, loss is 3.3661505699157717 and perplexity is 28.966806469190463
At time: 173.21525716781616 and batch: 800, loss is 3.3202918100357057 and perplexity is 27.66842330419768
At time: 173.8431797027588 and batch: 850, loss is 3.38097309589386 and perplexity is 29.399365600391892
At time: 174.50846600532532 and batch: 900, loss is 3.3462102031707763 and perplexity is 28.394918501962955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26176515344071 and perplexity of 70.9350843188362
finished 14 epochs...
Completing Train Step...
At time: 176.0606987476349 and batch: 50, loss is 3.653152756690979 and perplexity is 38.596158727643946
At time: 176.69164395332336 and batch: 100, loss is 3.5228241062164307 and perplexity is 33.87997413034559
At time: 177.31361961364746 and batch: 150, loss is 3.5378057527542115 and perplexity is 34.391373161996
At time: 177.93459177017212 and batch: 200, loss is 3.41959361076355 and perplexity is 30.55699446378241
At time: 178.554913520813 and batch: 250, loss is 3.570366415977478 and perplexity is 35.52960938335683
At time: 179.17513275146484 and batch: 300, loss is 3.5421861743927003 and perplexity is 34.54235231192304
At time: 179.79558420181274 and batch: 350, loss is 3.5351096487045286 and perplexity is 34.2987753243333
At time: 180.41665983200073 and batch: 400, loss is 3.4562466096878053 and perplexity is 31.697778820926125
At time: 181.0367591381073 and batch: 450, loss is 3.4885585641860963 and perplexity is 32.73872291094769
At time: 181.65776324272156 and batch: 500, loss is 3.353237280845642 and perplexity is 28.595154512881226
At time: 182.2799699306488 and batch: 550, loss is 3.408546876907349 and perplexity is 30.221297072131474
At time: 182.9006004333496 and batch: 600, loss is 3.444999895095825 and perplexity is 31.343280158628268
At time: 183.52142930030823 and batch: 650, loss is 3.277651605606079 and perplexity is 26.513435529031625
At time: 184.14315557479858 and batch: 700, loss is 3.2818238067626955 and perplexity is 26.624285999842765
At time: 184.7659785747528 and batch: 750, loss is 3.366163387298584 and perplexity is 28.967177750217253
At time: 185.38827633857727 and batch: 800, loss is 3.3207910537719725 and perplexity is 27.682240039897078
At time: 186.01002168655396 and batch: 850, loss is 3.3822291564941405 and perplexity is 29.43631618642538
At time: 186.63114070892334 and batch: 900, loss is 3.347949352264404 and perplexity is 28.444344465841443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261550694295805 and perplexity of 70.91987327244166
finished 15 epochs...
Completing Train Step...
At time: 188.20935034751892 and batch: 50, loss is 3.6512558031082154 and perplexity is 38.52301300501909
At time: 188.84139704704285 and batch: 100, loss is 3.5205458974838257 and perplexity is 33.80287633320506
At time: 189.52711629867554 and batch: 150, loss is 3.5352501010894777 and perplexity is 34.30359300744806
At time: 190.20388555526733 and batch: 200, loss is 3.4172447061538698 and perplexity is 30.485303229525922
At time: 190.8420865535736 and batch: 250, loss is 3.5680635261535643 and perplexity is 35.44788274729793
At time: 191.518456697464 and batch: 300, loss is 3.5399685525894165 and perplexity is 34.46583531254071
At time: 192.15321230888367 and batch: 350, loss is 3.5330392026901247 and perplexity is 34.22783502603495
At time: 192.82488083839417 and batch: 400, loss is 3.4541912412643434 and perplexity is 31.632695115652453
At time: 193.47209811210632 and batch: 450, loss is 3.4867762184143065 and perplexity is 32.68042315718704
At time: 194.0971393585205 and batch: 500, loss is 3.351720404624939 and perplexity is 28.551812083833898
At time: 194.7206254005432 and batch: 550, loss is 3.4072479581832886 and perplexity is 30.182067546997104
At time: 195.34553718566895 and batch: 600, loss is 3.444070091247559 and perplexity is 31.314150600605686
At time: 195.96800231933594 and batch: 650, loss is 3.2769580984115603 and perplexity is 26.495054645115143
At time: 196.59040331840515 and batch: 700, loss is 3.2814305782318116 and perplexity is 26.613818629142497
At time: 197.2127718925476 and batch: 750, loss is 3.366089940071106 and perplexity is 28.96505026945337
At time: 197.8362250328064 and batch: 800, loss is 3.321033978462219 and perplexity is 27.68896555634803
At time: 198.45813512802124 and batch: 850, loss is 3.3828859233856203 and perplexity is 29.455655334264613
At time: 199.08057165145874 and batch: 900, loss is 3.348850269317627 and perplexity is 28.469982007753757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261526447452911 and perplexity of 70.91815371026345
finished 16 epochs...
Completing Train Step...
At time: 200.73784017562866 and batch: 50, loss is 3.6496382665634157 and perplexity is 38.46075099278307
At time: 201.36206936836243 and batch: 100, loss is 3.518700566291809 and perplexity is 33.74055634931717
At time: 202.04671168327332 and batch: 150, loss is 3.533223958015442 and perplexity is 34.234159385041615
At time: 202.67778205871582 and batch: 200, loss is 3.4153274202346804 and perplexity is 30.42691018287246
At time: 203.32267332077026 and batch: 250, loss is 3.5661638832092284 and perplexity is 35.380608345847186
At time: 203.9699215888977 and batch: 300, loss is 3.538110809326172 and perplexity is 34.401866076782746
At time: 204.63438415527344 and batch: 350, loss is 3.5313282871246336 and perplexity is 34.169324158166056
At time: 205.26849484443665 and batch: 400, loss is 3.4525156736373903 and perplexity is 31.579736775793474
At time: 205.89450407028198 and batch: 450, loss is 3.485283875465393 and perplexity is 32.63168913110558
At time: 206.5188820362091 and batch: 500, loss is 3.3504207134246826 and perplexity is 28.514727649294024
At time: 207.14286255836487 and batch: 550, loss is 3.4061211824417112 and perplexity is 30.148078278192408
At time: 207.76646733283997 and batch: 600, loss is 3.4432182931900024 and perplexity is 31.287488624871703
At time: 208.43139791488647 and batch: 650, loss is 3.2763031578063964 and perplexity is 26.47770763924136
At time: 209.06715273857117 and batch: 700, loss is 3.280997552871704 and perplexity is 26.602296665578027
At time: 209.74755597114563 and batch: 750, loss is 3.3658946132659913 and perplexity is 28.9593931712337
At time: 210.40051245689392 and batch: 800, loss is 3.3210835742950437 and perplexity is 27.69033884770933
At time: 211.0498161315918 and batch: 850, loss is 3.3832099294662474 and perplexity is 29.46520069199524
At time: 211.6713936328888 and batch: 900, loss is 3.349332933425903 and perplexity is 28.483726763015355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261595843589469 and perplexity of 70.9230753269114
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 213.2300202846527 and batch: 50, loss is 3.6488469648361206 and perplexity is 38.43032897217555
At time: 213.86521339416504 and batch: 100, loss is 3.518237338066101 and perplexity is 33.72493039073703
At time: 214.48560619354248 and batch: 150, loss is 3.5329641342163085 and perplexity is 34.22526569113675
At time: 215.10740089416504 and batch: 200, loss is 3.4150560522079467 and perplexity is 30.41865441252325
At time: 215.73552632331848 and batch: 250, loss is 3.5658595514297486 and perplexity is 35.36984254062164
At time: 216.42475390434265 and batch: 300, loss is 3.5377063941955567 and perplexity is 34.38795625448078
At time: 217.07556915283203 and batch: 350, loss is 3.530738310813904 and perplexity is 34.1491710118831
At time: 217.70000624656677 and batch: 400, loss is 3.452051725387573 and perplexity is 31.565088810398358
At time: 218.3250675201416 and batch: 450, loss is 3.4846507120132446 and perplexity is 32.61103447774175
At time: 218.9532482624054 and batch: 500, loss is 3.34901424407959 and perplexity is 28.474650749043974
At time: 219.5919804573059 and batch: 550, loss is 3.4043345737457273 and perplexity is 30.094263546621672
At time: 220.21677803993225 and batch: 600, loss is 3.44111825466156 and perplexity is 31.221852636475624
At time: 220.84078073501587 and batch: 650, loss is 3.274346709251404 and perplexity is 26.42595600760096
At time: 221.46386671066284 and batch: 700, loss is 3.278726716041565 and perplexity is 26.54195572868448
At time: 222.08636045455933 and batch: 750, loss is 3.363323211669922 and perplexity is 28.885022600692718
At time: 222.70763301849365 and batch: 800, loss is 3.318376750946045 and perplexity is 27.615487342577975
At time: 223.32717633247375 and batch: 850, loss is 3.3802732944488527 and perplexity is 29.378799078942478
At time: 223.94762420654297 and batch: 900, loss is 3.34588716506958 and perplexity is 28.385747342803132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261316168798159 and perplexity of 70.9032427040919
finished 18 epochs...
Completing Train Step...
At time: 225.5081844329834 and batch: 50, loss is 3.648161044120789 and perplexity is 38.40397785185933
At time: 226.18496942520142 and batch: 100, loss is 3.5175620126724243 and perplexity is 33.70216277748317
At time: 226.81827664375305 and batch: 150, loss is 3.5322281646728517 and perplexity is 34.20008620477544
At time: 227.43927788734436 and batch: 200, loss is 3.414421920776367 and perplexity is 30.399371102376634
At time: 228.05878734588623 and batch: 250, loss is 3.56525194644928 and perplexity is 35.348358175800065
At time: 228.6794035434723 and batch: 300, loss is 3.537082371711731 and perplexity is 34.36650409061758
At time: 229.29964995384216 and batch: 350, loss is 3.5302056789398195 and perplexity is 34.13098691807249
At time: 229.91983604431152 and batch: 400, loss is 3.451514482498169 and perplexity is 31.548135245380465
At time: 230.54178929328918 and batch: 450, loss is 3.4841078615188597 and perplexity is 32.59333636570215
At time: 231.1607539653778 and batch: 500, loss is 3.348646392822266 and perplexity is 28.46417823924512
At time: 231.78008913993835 and batch: 550, loss is 3.4040181159973146 and perplexity is 30.084741490483257
At time: 232.4054787158966 and batch: 600, loss is 3.4409557151794434 and perplexity is 31.216778265121384
At time: 233.029629945755 and batch: 650, loss is 3.274217290878296 and perplexity is 26.422536224662778
At time: 233.65781426429749 and batch: 700, loss is 3.2786547231674192 and perplexity is 26.540044965787608
At time: 234.27936625480652 and batch: 750, loss is 3.3633752202987672 and perplexity is 28.886524910178537
At time: 234.91308903694153 and batch: 800, loss is 3.3185062408447266 and perplexity is 27.619063500769187
At time: 235.53376936912537 and batch: 850, loss is 3.3805574560165406 and perplexity is 29.387148590794542
At time: 236.1533694267273 and batch: 900, loss is 3.3461567687988283 and perplexity is 28.393401277862605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261186155554366 and perplexity of 70.89402494274098
finished 19 epochs...
Completing Train Step...
At time: 237.71258807182312 and batch: 50, loss is 3.647605886459351 and perplexity is 38.38266350628372
At time: 238.33169198036194 and batch: 100, loss is 3.516987347602844 and perplexity is 33.68280088559996
At time: 238.95163559913635 and batch: 150, loss is 3.531603636741638 and perplexity is 34.17873396392006
At time: 239.57096982002258 and batch: 200, loss is 3.4138621997833254 and perplexity is 30.38236069717991
At time: 240.19286704063416 and batch: 250, loss is 3.5647092390060426 and perplexity is 35.32917956337043
At time: 240.81775856018066 and batch: 300, loss is 3.5365418863296507 and perplexity is 34.3479345162672
At time: 241.44266295433044 and batch: 350, loss is 3.5297210121154787 and perplexity is 34.1144487691011
At time: 242.06781458854675 and batch: 400, loss is 3.451026463508606 and perplexity is 31.53274291247628
At time: 242.69389653205872 and batch: 450, loss is 3.4836553812026976 and perplexity is 32.5785918586098
At time: 243.32087540626526 and batch: 500, loss is 3.348311424255371 and perplexity is 28.454645230970677
At time: 243.9477264881134 and batch: 550, loss is 3.4037320804595947 and perplexity is 30.076137415868082
At time: 244.57492327690125 and batch: 600, loss is 3.4407887840270996 and perplexity is 31.21156764727245
At time: 245.202045917511 and batch: 650, loss is 3.274092035293579 and perplexity is 26.419226861700682
At time: 245.8293526172638 and batch: 700, loss is 3.2785817337036134 and perplexity is 26.53810789282999
At time: 246.49811506271362 and batch: 750, loss is 3.3633981418609618 and perplexity is 28.88718704204437
At time: 247.1312654018402 and batch: 800, loss is 3.3185990858078003 and perplexity is 27.62162791074448
At time: 247.80379104614258 and batch: 850, loss is 3.380769510269165 and perplexity is 29.393380921398478
At time: 248.43425011634827 and batch: 900, loss is 3.3463771677017213 and perplexity is 28.39965984201865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261125120398116 and perplexity of 70.88969804689914
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f74eca12b70>
ELAPSED
1557.4052064418793


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.7917024081235}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.41628114957778306, 'tune_wordvecs': 'TRUE', 'dropout': 0.7387750863627058, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.03583228815741}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.09083670716130898, 'tune_wordvecs': 'TRUE', 'dropout': 0.021912257622493048, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.30896956149321}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.9388904092742721, 'tune_wordvecs': 'TRUE', 'dropout': 0.7397063209606896, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.51902603416052}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.5901454392767163, 'tune_wordvecs': 'TRUE', 'dropout': 0.30661194625441784, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.88969804689914}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.6799297876075221, 'tune_wordvecs': 'TRUE', 'dropout': 0.397233241774987, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.80715237139826}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.4958912749571752, 'tune_wordvecs': 'TRUE', 'dropout': 0.2296058494733163, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.7917024081235}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.41628114957778306, 'tune_wordvecs': 'TRUE', 'dropout': 0.7387750863627058, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.03583228815741}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.09083670716130898, 'tune_wordvecs': 'TRUE', 'dropout': 0.021912257622493048, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.30896956149321}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.9388904092742721, 'tune_wordvecs': 'TRUE', 'dropout': 0.7397063209606896, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -71.51902603416052}, {'params': {'seq_len': 35, 'num_layers': 3, 'data': 'ptb', 'wordvec_source': 'glove', 'rnn_dropout': 0.5901454392767163, 'tune_wordvecs': 'TRUE', 'dropout': 0.30661194625441784, 'batch_size': 32, 'tie_weights': 'FALSE', 'wordvec_dim': 300}, 'best_accuracy': -70.88969804689914}]
