FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.6888210773468018 and batch: 50, loss is 6.988662071228028 and perplexity is 1084.26982950106
At time: 2.8442673683166504 and batch: 100, loss is 6.037453269958496 and perplexity is 418.8250412035447
At time: 3.9771728515625 and batch: 150, loss is 5.805351486206055 and perplexity is 332.071891522734
At time: 5.112563610076904 and batch: 200, loss is 5.585038757324218 and perplexity is 266.41060790168183
At time: 6.246962785720825 and batch: 250, loss is 5.60950011253357 and perplexity is 273.0077307456271
At time: 7.379695892333984 and batch: 300, loss is 5.501246509552002 and perplexity is 244.99713327355408
At time: 8.508311986923218 and batch: 350, loss is 5.455896492004395 and perplexity is 234.13467687657553
At time: 9.635170459747314 and batch: 400, loss is 5.292943773269653 and perplexity is 198.9281637295623
At time: 10.761451482772827 and batch: 450, loss is 5.292918872833252 and perplexity is 198.9232103931434
At time: 11.887741327285767 and batch: 500, loss is 5.225752792358398 and perplexity is 186.00113807148838
At time: 13.011667013168335 and batch: 550, loss is 5.278141021728516 and perplexity is 196.00516711771644
At time: 14.138620138168335 and batch: 600, loss is 5.185412158966065 and perplexity is 178.64706560792527
At time: 15.272119283676147 and batch: 650, loss is 5.079307889938354 and perplexity is 160.66282108466513
At time: 16.40494966506958 and batch: 700, loss is 5.1632825756073 and perplexity is 174.73710296366025
At time: 17.53817129135132 and batch: 750, loss is 5.147702808380127 and perplexity is 172.03583676230272
At time: 18.67167592048645 and batch: 800, loss is 5.103334951400757 and perplexity is 164.56982552190306
At time: 19.805352687835693 and batch: 850, loss is 5.144438972473145 and perplexity is 171.4752553415137
At time: 20.938050746917725 and batch: 900, loss is 5.055593566894531 and perplexity is 156.89763195057526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.023575874224101 and perplexity of 151.95370076624005
finished 1 epochs...
Completing Train Step...
At time: 23.251781702041626 and batch: 50, loss is 4.980410251617432 and perplexity is 145.53407499778652
At time: 24.171690940856934 and batch: 100, loss is 4.844599046707153 and perplexity is 127.05232972971153
At time: 25.080711364746094 and batch: 150, loss is 4.830598974227906 and perplexity is 125.28598125870994
At time: 25.991511344909668 and batch: 200, loss is 4.711304998397827 and perplexity is 111.19717739966931
At time: 26.902421474456787 and batch: 250, loss is 4.816633234024048 and perplexity is 123.54843112133506
At time: 27.81381320953369 and batch: 300, loss is 4.756392869949341 and perplexity is 116.32556676221269
At time: 28.72383165359497 and batch: 350, loss is 4.739852132797242 and perplexity is 114.41728186582998
At time: 29.633727312088013 and batch: 400, loss is 4.610182552337647 and perplexity is 100.50249492723884
At time: 30.543703079223633 and batch: 450, loss is 4.626552600860595 and perplexity is 102.16126568200592
At time: 31.455054759979248 and batch: 500, loss is 4.531076860427857 and perplexity is 92.85850290792669
At time: 32.36533522605896 and batch: 550, loss is 4.601211862564087 and perplexity is 99.60495004116652
At time: 33.275115966796875 and batch: 600, loss is 4.558901395797729 and perplexity is 95.47852907768181
At time: 34.18493866920471 and batch: 650, loss is 4.418008842468262 and perplexity is 82.93099218088226
At time: 35.09458136558533 and batch: 700, loss is 4.467600727081299 and perplexity is 87.14738162763027
At time: 36.00294852256775 and batch: 750, loss is 4.517220869064331 and perplexity is 91.5807291477869
At time: 36.92927265167236 and batch: 800, loss is 4.453505296707153 and perplexity is 85.9276185158782
At time: 37.838664531707764 and batch: 850, loss is 4.518474683761597 and perplexity is 91.69562642687532
At time: 38.74822950363159 and batch: 900, loss is 4.454997243881226 and perplexity is 86.05591366449225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.577497090378853 and perplexity of 97.27062967914033
finished 2 epochs...
Completing Train Step...
At time: 40.90034508705139 and batch: 50, loss is 4.490944538116455 and perplexity is 89.20566424875368
At time: 41.815271615982056 and batch: 100, loss is 4.356516017913818 and perplexity is 77.98496236523022
At time: 42.72062540054321 and batch: 150, loss is 4.356160168647766 and perplexity is 77.95721641059103
At time: 43.6286084651947 and batch: 200, loss is 4.250371341705322 and perplexity is 70.13145024424043
At time: 44.53501605987549 and batch: 250, loss is 4.3967705345153805 and perplexity is 81.18825018283127
At time: 45.44122266769409 and batch: 300, loss is 4.357780909538269 and perplexity is 78.08366730332358
At time: 46.349611043930054 and batch: 350, loss is 4.3479641342163085 and perplexity is 77.32088762912103
At time: 47.25938606262207 and batch: 400, loss is 4.254071345329285 and perplexity is 70.39141750762636
At time: 48.16806721687317 and batch: 450, loss is 4.279645533561706 and perplexity is 72.21483773369664
At time: 49.07696461677551 and batch: 500, loss is 4.163638544082642 and perplexity is 64.30507429576299
At time: 49.99185514450073 and batch: 550, loss is 4.236871752738953 and perplexity is 69.19106617271588
At time: 50.90145444869995 and batch: 600, loss is 4.237424578666687 and perplexity is 69.22932736297828
At time: 51.811199426651 and batch: 650, loss is 4.082875008583069 and perplexity is 59.315758256369755
At time: 52.71907162666321 and batch: 700, loss is 4.113970670700073 and perplexity is 61.18919800173176
At time: 53.62842035293579 and batch: 750, loss is 4.200664415359497 and perplexity is 66.73065318606372
At time: 54.53658676147461 and batch: 800, loss is 4.1467059707641605 and perplexity is 63.22539061841578
At time: 55.44527459144592 and batch: 850, loss is 4.222299494743347 and perplexity is 68.19010693637732
At time: 56.35320997238159 and batch: 900, loss is 4.168803868293762 and perplexity is 64.63809018000462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4297071483037245 and perplexity of 83.9068410511725
finished 3 epochs...
Completing Train Step...
At time: 58.5147225856781 and batch: 50, loss is 4.232304458618164 and perplexity is 68.87577079443078
At time: 59.42957949638367 and batch: 100, loss is 4.098691506385803 and perplexity is 60.26138434981768
At time: 60.34266209602356 and batch: 150, loss is 4.098971486091614 and perplexity is 60.278258676603976
At time: 61.249579429626465 and batch: 200, loss is 3.990458846092224 and perplexity is 54.079697929298064
At time: 62.15687704086304 and batch: 250, loss is 4.145117831230164 and perplexity is 63.125059566964744
At time: 63.066160440444946 and batch: 300, loss is 4.118121552467346 and perplexity is 61.44371499760996
At time: 63.98042583465576 and batch: 350, loss is 4.1049982357025145 and perplexity is 60.64263755585867
At time: 64.89035248756409 and batch: 400, loss is 4.02595175743103 and perplexity is 56.033613819227405
At time: 65.80464053153992 and batch: 450, loss is 4.055185294151306 and perplexity is 57.69585270982351
At time: 66.71606588363647 and batch: 500, loss is 3.9319879817962646 and perplexity is 51.00828047481439
At time: 67.62490606307983 and batch: 550, loss is 4.008088817596436 and perplexity is 55.04157548593119
At time: 68.53328013420105 and batch: 600, loss is 4.0231080245971675 and perplexity is 55.87449554397161
At time: 69.44287443161011 and batch: 650, loss is 3.863784327507019 and perplexity is 47.64531611165736
At time: 70.35110211372375 and batch: 700, loss is 3.8874200916290285 and perplexity is 48.784863551917596
At time: 71.25960564613342 and batch: 750, loss is 3.988617420196533 and perplexity is 53.98020580490062
At time: 72.16923117637634 and batch: 800, loss is 3.9389986324310304 and perplexity is 51.36713815180719
At time: 73.08612012863159 and batch: 850, loss is 4.015029997825622 and perplexity is 55.42495800578188
At time: 73.9986982345581 and batch: 900, loss is 3.9670098352432253 and perplexity is 52.826334996592635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369861341502569 and perplexity of 79.03267238817986
finished 4 epochs...
Completing Train Step...
At time: 76.15974545478821 and batch: 50, loss is 4.040715041160584 and perplexity is 56.8669905102395
At time: 77.06745767593384 and batch: 100, loss is 3.9080796813964844 and perplexity is 49.80322203304809
At time: 77.97290325164795 and batch: 150, loss is 3.9115070581436155 and perplexity is 49.97420928966908
At time: 78.88053226470947 and batch: 200, loss is 3.802565221786499 and perplexity is 44.81600014649213
At time: 79.78531455993652 and batch: 250, loss is 3.9575758028030394 and perplexity is 52.33031306091867
At time: 80.68976521492004 and batch: 300, loss is 3.9332892084121704 and perplexity is 51.07469700910957
At time: 81.59887409210205 and batch: 350, loss is 3.919850616455078 and perplexity is 50.39291634328503
At time: 82.5027437210083 and batch: 400, loss is 3.849029459953308 and perplexity is 46.94747670714843
At time: 83.4071786403656 and batch: 450, loss is 3.8819326257705686 and perplexity is 48.51789144884514
At time: 84.31212401390076 and batch: 500, loss is 3.7542210483551024 and perplexity is 42.70094488120054
At time: 85.22521138191223 and batch: 550, loss is 3.82995719909668 and perplexity is 46.06056675815961
At time: 86.13283276557922 and batch: 600, loss is 3.858066635131836 and perplexity is 47.37367217928136
At time: 87.03670120239258 and batch: 650, loss is 3.6947021102905273 and perplexity is 40.23358563152998
At time: 87.94341492652893 and batch: 700, loss is 3.7146555519104005 and perplexity is 41.04444696791773
At time: 88.85098361968994 and batch: 750, loss is 3.8214532852172853 and perplexity is 45.670532424894155
At time: 89.75744795799255 and batch: 800, loss is 3.7753890800476073 and perplexity is 43.61447453672954
At time: 90.66523265838623 and batch: 850, loss is 3.85042498588562 and perplexity is 47.013038864555135
At time: 91.57322597503662 and batch: 900, loss is 3.802920627593994 and perplexity is 44.83193084397641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356006779082834 and perplexity of 77.94525950413825
finished 5 epochs...
Completing Train Step...
At time: 93.71200323104858 and batch: 50, loss is 3.8831752824783323 and perplexity is 48.5782200081818
At time: 94.62634873390198 and batch: 100, loss is 3.7534037590026856 and perplexity is 42.666060111028415
At time: 95.53494167327881 and batch: 150, loss is 3.7566291236877443 and perplexity is 42.80389588038068
At time: 96.44394779205322 and batch: 200, loss is 3.64978732585907 and perplexity is 38.46648435253105
At time: 97.35302472114563 and batch: 250, loss is 3.8063491487503054 and perplexity is 44.98590186297702
At time: 98.2628071308136 and batch: 300, loss is 3.7802294635772706 and perplexity is 43.8260970748754
At time: 99.16827988624573 and batch: 350, loss is 3.7667406797409058 and perplexity is 43.238905478431285
At time: 100.07708072662354 and batch: 400, loss is 3.7013599920272826 and perplexity is 40.502349793713364
At time: 100.98582363128662 and batch: 450, loss is 3.7344674825668336 and perplexity is 41.86572539738993
At time: 101.89495348930359 and batch: 500, loss is 3.6080703210830687 and perplexity is 36.894788982334646
At time: 102.80456233024597 and batch: 550, loss is 3.682435002326965 and perplexity is 39.74305076627377
At time: 103.7185230255127 and batch: 600, loss is 3.7199074268341064 and perplexity is 41.26057430981764
At time: 104.62766408920288 and batch: 650, loss is 3.554239134788513 and perplexity is 34.96120907758845
At time: 105.53676772117615 and batch: 700, loss is 3.576149401664734 and perplexity is 35.73567185979321
At time: 106.44555640220642 and batch: 750, loss is 3.6812855195999146 and perplexity is 39.697393062296015
At time: 107.35339069366455 and batch: 800, loss is 3.6355919551849367 and perplexity is 37.92429573740579
At time: 108.26061201095581 and batch: 850, loss is 3.7116980218887328 and perplexity is 40.92323611450875
At time: 109.16879963874817 and batch: 900, loss is 3.663886504173279 and perplexity is 39.01267152181312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365443399507705 and perplexity of 78.68428077949406
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 111.31004405021667 and batch: 50, loss is 3.772690010070801 and perplexity is 43.49691474038005
At time: 112.22312426567078 and batch: 100, loss is 3.6513105249404907 and perplexity is 38.525121112554714
At time: 113.12975144386292 and batch: 150, loss is 3.6620208024978638 and perplexity is 38.93995337147329
At time: 114.03686499595642 and batch: 200, loss is 3.5316039180755614 and perplexity is 34.17874357955873
At time: 114.9524507522583 and batch: 250, loss is 3.6796395349502564 and perplexity is 39.632105508579755
At time: 115.85921621322632 and batch: 300, loss is 3.6462254238128664 and perplexity is 38.32971422869933
At time: 116.76422238349915 and batch: 350, loss is 3.6147998762130737 and perplexity is 37.14391180150783
At time: 117.67117476463318 and batch: 400, loss is 3.543259711265564 and perplexity is 34.57945471263964
At time: 118.57833480834961 and batch: 450, loss is 3.56023072719574 and perplexity is 35.17131118700622
At time: 119.48814725875854 and batch: 500, loss is 3.425441093444824 and perplexity is 30.73619939794559
At time: 120.39805221557617 and batch: 550, loss is 3.478354802131653 and perplexity is 32.406363315192515
At time: 121.30700135231018 and batch: 600, loss is 3.5047386932373046 and perplexity is 33.27274832220275
At time: 122.21599841117859 and batch: 650, loss is 3.32325300693512 and perplexity is 27.75047638119821
At time: 123.12502717971802 and batch: 700, loss is 3.32744957447052 and perplexity is 27.867177830958347
At time: 124.03242111206055 and batch: 750, loss is 3.415241365432739 and perplexity is 30.42429191380192
At time: 124.94068884849548 and batch: 800, loss is 3.3507008790969848 and perplexity is 28.52271761634138
At time: 125.86259031295776 and batch: 850, loss is 3.401059489250183 and perplexity is 29.995863512842483
At time: 126.78677034378052 and batch: 900, loss is 3.344768075942993 and perplexity is 28.353998929567144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.339339582887415 and perplexity of 76.65689709099628
finished 7 epochs...
Completing Train Step...
At time: 128.9384160041809 and batch: 50, loss is 3.65823561668396 and perplexity is 38.79283701939431
At time: 129.84538388252258 and batch: 100, loss is 3.5264320516586305 and perplexity is 34.00243200642002
At time: 130.75480890274048 and batch: 150, loss is 3.53291184425354 and perplexity is 34.22347610005725
At time: 131.66307306289673 and batch: 200, loss is 3.40826952457428 and perplexity is 30.212916287149003
At time: 132.5702154636383 and batch: 250, loss is 3.557265586853027 and perplexity is 35.06717777468604
At time: 133.4788897037506 and batch: 300, loss is 3.530004539489746 and perplexity is 34.12412252050704
At time: 134.38802456855774 and batch: 350, loss is 3.5013992595672607 and perplexity is 33.161821505620075
At time: 135.2970428466797 and batch: 400, loss is 3.4368278408050537 and perplexity is 31.088184917257966
At time: 136.20441365242004 and batch: 450, loss is 3.4569301176071168 and perplexity is 31.719451909794977
At time: 137.11288785934448 and batch: 500, loss is 3.328822512626648 and perplexity is 27.905464018976268
At time: 138.02583932876587 and batch: 550, loss is 3.3846173763275145 and perplexity is 29.50670059382846
At time: 138.9357078075409 and batch: 600, loss is 3.418079442977905 and perplexity is 30.51076105853894
At time: 139.8450961112976 and batch: 650, loss is 3.243033933639526 and perplexity is 25.611306999443492
At time: 140.7550721168518 and batch: 700, loss is 3.252563796043396 and perplexity is 25.856545921712204
At time: 141.661141872406 and batch: 750, loss is 3.3460639905929566 and perplexity is 28.390767111231966
At time: 142.5699667930603 and batch: 800, loss is 3.2896879291534424 and perplexity is 26.834488087666745
At time: 143.48570132255554 and batch: 850, loss is 3.346440997123718 and perplexity is 28.40147263374538
At time: 144.39720177650452 and batch: 900, loss is 3.299320659637451 and perplexity is 27.094226465573946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360459837194991 and perplexity of 78.29312823927891
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 146.53724431991577 and batch: 50, loss is 3.603748927116394 and perplexity is 36.73569606299646
At time: 147.45992279052734 and batch: 100, loss is 3.483844895362854 and perplexity is 32.58476654816204
At time: 148.36702919006348 and batch: 150, loss is 3.4958023500442503 and perplexity is 32.97673622696978
At time: 149.28008723258972 and batch: 200, loss is 3.3660505294799803 and perplexity is 28.96390876219415
At time: 150.192800283432 and batch: 250, loss is 3.5150288438796995 and perplexity is 33.616897551687636
At time: 151.1022789478302 and batch: 300, loss is 3.489768190383911 and perplexity is 32.77834848910083
At time: 152.00993728637695 and batch: 350, loss is 3.4513604974746706 and perplexity is 31.543277679039203
At time: 152.91755199432373 and batch: 400, loss is 3.3873001956939697 and perplexity is 29.585968024176342
At time: 153.82584691047668 and batch: 450, loss is 3.398769745826721 and perplexity is 29.927259254681267
At time: 154.73234724998474 and batch: 500, loss is 3.270199942588806 and perplexity is 26.316600626528906
At time: 155.63758635520935 and batch: 550, loss is 3.3163033962249755 and perplexity is 27.558289957246064
At time: 156.55360889434814 and batch: 600, loss is 3.345674977302551 and perplexity is 28.379724873428774
At time: 157.46327328681946 and batch: 650, loss is 3.162591605186462 and perplexity is 23.631760831130528
At time: 158.37030386924744 and batch: 700, loss is 3.1626640939712525 and perplexity is 23.63347393084514
At time: 159.2878339290619 and batch: 750, loss is 3.252024416923523 and perplexity is 25.842603201274162
At time: 160.19638204574585 and batch: 800, loss is 3.1880335807800293 and perplexity is 24.240713141083017
At time: 161.1032428741455 and batch: 850, loss is 3.236897449493408 and perplexity is 25.454624850515472
At time: 162.01264929771423 and batch: 900, loss is 3.192252836227417 and perplexity is 24.34320697383262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3588975880244005 and perplexity of 78.17091035687416
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 164.15599536895752 and batch: 50, loss is 3.583828692436218 and perplexity is 36.01115287051773
At time: 165.06940031051636 and batch: 100, loss is 3.4605267572402956 and perplexity is 31.833740752365262
At time: 165.97828769683838 and batch: 150, loss is 3.473207697868347 and perplexity is 32.23999291461338
At time: 166.88790488243103 and batch: 200, loss is 3.341712737083435 and perplexity is 28.26750006377634
At time: 167.79799818992615 and batch: 250, loss is 3.490822615623474 and perplexity is 32.81292903515528
At time: 168.71539449691772 and batch: 300, loss is 3.4646735334396364 and perplexity is 31.966022232142553
At time: 169.63484716415405 and batch: 350, loss is 3.4288290882110597 and perplexity is 30.840510082889562
At time: 170.54407167434692 and batch: 400, loss is 3.3636235857009886 and perplexity is 28.89370021456843
At time: 171.45528721809387 and batch: 450, loss is 3.3733147811889648 and perplexity is 29.17507594310071
At time: 172.3715591430664 and batch: 500, loss is 3.247825388908386 and perplexity is 25.73431689417369
At time: 173.29000186920166 and batch: 550, loss is 3.2898480987548826 and perplexity is 26.838786501157
At time: 174.1984260082245 and batch: 600, loss is 3.317929549217224 and perplexity is 27.60314040988673
At time: 175.10847687721252 and batch: 650, loss is 3.13448570728302 and perplexity is 22.976815993739415
At time: 176.01634168624878 and batch: 700, loss is 3.133538408279419 and perplexity is 22.955060385002774
At time: 176.92592072486877 and batch: 750, loss is 3.221171612739563 and perplexity is 25.057460630255203
At time: 177.83697843551636 and batch: 800, loss is 3.155538659095764 and perplexity is 23.465673686286447
At time: 178.74677753448486 and batch: 850, loss is 3.203565731048584 and perplexity is 24.62016274593375
At time: 179.6579098701477 and batch: 900, loss is 3.159901666641235 and perplexity is 23.56827826715594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355792319937928 and perplexity of 77.92854522276484
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 181.82107734680176 and batch: 50, loss is 3.572764377593994 and perplexity is 35.61491025614242
At time: 182.72927284240723 and batch: 100, loss is 3.449619164466858 and perplexity is 31.48839812409967
At time: 183.63556623458862 and batch: 150, loss is 3.463355107307434 and perplexity is 31.923905163312035
At time: 184.54402709007263 and batch: 200, loss is 3.331461410522461 and perplexity is 27.979200938567956
At time: 185.45379662513733 and batch: 250, loss is 3.4812547016143798 and perplexity is 32.50047490252705
At time: 186.36203598976135 and batch: 300, loss is 3.4546376514434813 and perplexity is 31.64681942512885
At time: 187.27102828025818 and batch: 350, loss is 3.420122284889221 and perplexity is 30.573153427150245
At time: 188.179993391037 and batch: 400, loss is 3.3541141891479493 and perplexity is 28.620240838875556
At time: 189.08976554870605 and batch: 450, loss is 3.363308672904968 and perplexity is 28.884602651191226
At time: 189.997638463974 and batch: 500, loss is 3.237783417701721 and perplexity is 25.477186832010904
At time: 190.90691351890564 and batch: 550, loss is 3.280641884803772 and perplexity is 26.59283676051323
At time: 191.81479573249817 and batch: 600, loss is 3.3094541215896607 and perplexity is 27.370180602555543
At time: 192.72021913528442 and batch: 650, loss is 3.12588876247406 and perplexity is 22.780132225869195
At time: 193.6263530254364 and batch: 700, loss is 3.124964003562927 and perplexity is 22.759075833140525
At time: 194.54112482070923 and batch: 750, loss is 3.211930584907532 and perplexity is 24.82697056199216
At time: 195.45002007484436 and batch: 800, loss is 3.145239205360413 and perplexity is 23.22523040848326
At time: 196.35723233222961 and batch: 850, loss is 3.1931690263748167 and perplexity is 24.365520200236432
At time: 197.2660574913025 and batch: 900, loss is 3.1493556308746338 and perplexity is 23.32103238508078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354801125722389 and perplexity of 77.85134116794184
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 199.40920400619507 and batch: 50, loss is 3.5693311977386473 and perplexity is 35.49284751526588
At time: 200.3232500553131 and batch: 100, loss is 3.44640558719635 and perplexity is 31.38737014116183
At time: 201.23261141777039 and batch: 150, loss is 3.460743741989136 and perplexity is 31.840648938065428
At time: 202.14056301116943 and batch: 200, loss is 3.32856568813324 and perplexity is 27.898298132543243
At time: 203.0449924468994 and batch: 250, loss is 3.4783250188827513 and perplexity is 32.405398162780656
At time: 203.94989013671875 and batch: 300, loss is 3.451710653305054 and perplexity is 31.554324675600174
At time: 204.86371731758118 and batch: 350, loss is 3.417186150550842 and perplexity is 30.48351819647419
At time: 205.77121877670288 and batch: 400, loss is 3.3511683177947997 and perplexity is 28.536053354899423
At time: 206.67722940444946 and batch: 450, loss is 3.3607704305648802 and perplexity is 28.81137949813178
At time: 207.58233213424683 and batch: 500, loss is 3.2347914457321165 and perplexity is 25.401073724067604
At time: 208.49487924575806 and batch: 550, loss is 3.2776971435546876 and perplexity is 26.514642923987115
At time: 209.40437817573547 and batch: 600, loss is 3.3070590686798096 and perplexity is 27.30470601071447
At time: 210.31251907348633 and batch: 650, loss is 3.1234638690948486 and perplexity is 22.72495975477207
At time: 211.22026777267456 and batch: 700, loss is 3.122641534805298 and perplexity is 22.70627992272293
At time: 212.12793898582458 and batch: 750, loss is 3.2094300317764284 and perplexity is 24.764966956979716
At time: 213.03604078292847 and batch: 800, loss is 3.142516026496887 and perplexity is 23.16206998952489
At time: 213.94460678100586 and batch: 850, loss is 3.1903470659255984 and perplexity is 24.29685859164398
At time: 214.85240006446838 and batch: 900, loss is 3.146376566886902 and perplexity is 23.251660919667795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354314098619435 and perplexity of 77.81343468628295
Annealing...
Model not improving. Stopping early with 76.65689709099628 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -76.65689709099628
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb19ab91b70>
ELAPSED
226.84920835494995


RESULTS SO FAR:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.353879690170288 and batch: 50, loss is 7.0573358154296875 and perplexity is 1161.3469978073576
At time: 2.4885008335113525 and batch: 100, loss is 6.217256031036377 and perplexity is 501.325720742075
At time: 3.623793840408325 and batch: 150, loss is 6.1338791656494145 and perplexity is 461.2218509073942
At time: 4.770390272140503 and batch: 200, loss is 6.014250040054321 and perplexity is 409.2188260787797
At time: 5.901076316833496 and batch: 250, loss is 6.073749217987061 and perplexity is 434.3059405898503
At time: 7.034287452697754 and batch: 300, loss is 5.994000873565674 and perplexity is 401.01581826188453
At time: 8.169260263442993 and batch: 350, loss is 5.999635591506958 and perplexity is 403.2818073971659
At time: 9.304397344589233 and batch: 400, loss is 5.894051113128662 and perplexity is 362.87234767788937
At time: 10.440673351287842 and batch: 450, loss is 5.905658121109009 and perplexity is 367.1087483197819
At time: 11.575803756713867 and batch: 500, loss is 5.8782258796691895 and perplexity is 357.17500782855024
At time: 12.70896029472351 and batch: 550, loss is 5.928840055465698 and perplexity is 375.71844840711685
At time: 13.844255924224854 and batch: 600, loss is 5.867743864059448 and perplexity is 353.45064732393905
At time: 14.976564168930054 and batch: 650, loss is 5.797946987152099 and perplexity is 329.6221462768776
At time: 16.11112904548645 and batch: 700, loss is 5.902749853134155 and perplexity is 366.0426487068605
At time: 17.24880623817444 and batch: 750, loss is 5.878929281234742 and perplexity is 357.42633366936536
At time: 18.384023189544678 and batch: 800, loss is 5.884801464080811 and perplexity is 359.53138101719173
At time: 19.5183002948761 and batch: 850, loss is 5.931451377868652 and perplexity is 376.70085253735107
At time: 20.652063131332397 and batch: 900, loss is 5.806615629196167 and perplexity is 332.4919433234465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.888867103890197 and perplexity of 360.9960815674574
finished 1 epochs...
Completing Train Step...
At time: 22.948622941970825 and batch: 50, loss is 5.763687791824341 and perplexity is 318.52080408408204
At time: 23.857218027114868 and batch: 100, loss is 5.509570093154907 and perplexity is 247.04489794227507
At time: 24.770653247833252 and batch: 150, loss is 5.419483509063721 and perplexity is 225.76248810546338
At time: 25.681397676467896 and batch: 200, loss is 5.235182437896729 and perplexity is 187.76335837185405
At time: 26.590507984161377 and batch: 250, loss is 5.260556478500366 and perplexity is 192.58863291711003
At time: 27.499542236328125 and batch: 300, loss is 5.1694676971435545 and perplexity is 175.82122243157178
At time: 28.409080266952515 and batch: 350, loss is 5.122892484664917 and perplexity is 167.82008529000453
At time: 29.317838191986084 and batch: 400, loss is 4.965696220397949 and perplexity is 143.40835934977056
At time: 30.235851287841797 and batch: 450, loss is 4.954920959472656 and perplexity is 141.8713923401172
At time: 31.144245862960815 and batch: 500, loss is 4.868505668640137 and perplexity is 130.12631969605562
At time: 32.05208086967468 and batch: 550, loss is 4.92155948638916 and perplexity is 137.21643357867416
At time: 32.96177911758423 and batch: 600, loss is 4.839422740936279 and perplexity is 126.39636721805796
At time: 33.86827564239502 and batch: 650, loss is 4.706891212463379 and perplexity is 110.70745841453494
At time: 34.777138233184814 and batch: 700, loss is 4.76447455406189 and perplexity is 117.26948232315485
At time: 35.68647599220276 and batch: 750, loss is 4.7729166316986085 and perplexity is 118.2636710018147
At time: 36.60264229774475 and batch: 800, loss is 4.713135662078858 and perplexity is 111.40092847671333
At time: 37.51341652870178 and batch: 850, loss is 4.758248147964477 and perplexity is 116.54158335227832
At time: 38.421950578689575 and batch: 900, loss is 4.680211067199707 and perplexity is 107.79282169954328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.773028230013913 and perplexity of 118.27686976472637
finished 2 epochs...
Completing Train Step...
At time: 40.56760573387146 and batch: 50, loss is 4.730968971252441 and perplexity is 113.40539569568918
At time: 41.48058843612671 and batch: 100, loss is 4.597640180587769 and perplexity is 99.24982740643163
At time: 42.3877158164978 and batch: 150, loss is 4.5892485332489015 and perplexity is 98.42044267556098
At time: 43.29625749588013 and batch: 200, loss is 4.4846094036102295 and perplexity is 88.64232067975999
At time: 44.207634925842285 and batch: 250, loss is 4.612421970367432 and perplexity is 100.7278142243009
At time: 45.121543169021606 and batch: 300, loss is 4.573508901596069 and perplexity is 96.88346859387893
At time: 46.03300189971924 and batch: 350, loss is 4.565630922317505 and perplexity is 96.12322117447343
At time: 46.942620277404785 and batch: 400, loss is 4.462116136550903 and perplexity is 86.6707222591904
At time: 47.85156464576721 and batch: 450, loss is 4.47802152633667 and perplexity is 88.0602752767883
At time: 48.76051044464111 and batch: 500, loss is 4.368886985778809 and perplexity is 78.95570395488043
At time: 49.67926621437073 and batch: 550, loss is 4.445246095657349 and perplexity is 85.22084773745982
At time: 50.5877149105072 and batch: 600, loss is 4.423355007171631 and perplexity is 83.37554218403884
At time: 51.4956591129303 and batch: 650, loss is 4.281997447013855 and perplexity is 72.38488066675545
At time: 52.40148854255676 and batch: 700, loss is 4.317898845672607 and perplexity is 75.03081123737176
At time: 53.33204913139343 and batch: 750, loss is 4.379028558731079 and perplexity is 79.76051310391152
At time: 54.24205446243286 and batch: 800, loss is 4.328005685806274 and perplexity is 75.79298072639386
At time: 55.151960372924805 and batch: 850, loss is 4.395432920455932 and perplexity is 81.07972423702151
At time: 56.060129165649414 and batch: 900, loss is 4.335742130279541 and perplexity is 76.38162297631102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.542115093910531 and perplexity of 93.88917467673407
finished 3 epochs...
Completing Train Step...
At time: 58.20405864715576 and batch: 50, loss is 4.417467164993286 and perplexity is 82.88608249482125
At time: 59.116689682006836 and batch: 100, loss is 4.2823868751525875 and perplexity is 72.41307486555675
At time: 60.024441719055176 and batch: 150, loss is 4.276790657043457 and perplexity is 72.00896729634603
At time: 60.93083357810974 and batch: 200, loss is 4.170663142204285 and perplexity is 64.75838188766618
At time: 61.837499141693115 and batch: 250, loss is 4.320254936218261 and perplexity is 75.2077990401313
At time: 62.74585580825806 and batch: 300, loss is 4.290698428153991 and perplexity is 73.01744813626469
At time: 63.653682231903076 and batch: 350, loss is 4.293496294021606 and perplexity is 73.22202722111021
At time: 64.56225991249084 and batch: 400, loss is 4.203544602394104 and perplexity is 66.92312699541105
At time: 65.47991800308228 and batch: 450, loss is 4.225897026062012 and perplexity is 68.4358647774145
At time: 66.39073061943054 and batch: 500, loss is 4.102595806121826 and perplexity is 60.49712275406423
At time: 67.29909300804138 and batch: 550, loss is 4.1856620454788205 and perplexity is 65.73700740205437
At time: 68.20787787437439 and batch: 600, loss is 4.189784531593323 and perplexity is 66.0085666673352
At time: 69.12168288230896 and batch: 650, loss is 4.036679711341858 and perplexity is 56.63797583338022
At time: 70.03431415557861 and batch: 700, loss is 4.05879358291626 and perplexity is 57.904412051823634
At time: 70.94912767410278 and batch: 750, loss is 4.147303838729858 and perplexity is 63.26320235617231
At time: 71.85390853881836 and batch: 800, loss is 4.099224400520325 and perplexity is 60.29350584599417
At time: 72.7599983215332 and batch: 850, loss is 4.175909585952759 and perplexity is 65.09902589894702
At time: 73.66539192199707 and batch: 900, loss is 4.117794609069824 and perplexity is 61.42362966423474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442547941861087 and perplexity of 84.99121871014117
finished 4 epochs...
Completing Train Step...
At time: 75.7919409275055 and batch: 50, loss is 4.211685647964478 and perplexity is 67.47017497187433
At time: 76.69494080543518 and batch: 100, loss is 4.07479811668396 and perplexity is 58.838600857375745
At time: 77.59886908531189 and batch: 150, loss is 4.073492774963379 and perplexity is 58.76184648313805
At time: 78.5054235458374 and batch: 200, loss is 3.968118371963501 and perplexity is 52.88492739864842
At time: 79.41527485847473 and batch: 250, loss is 4.122055377960205 and perplexity is 61.6858998942866
At time: 80.3261513710022 and batch: 300, loss is 4.099224286079407 and perplexity is 60.2934989459504
At time: 81.23588061332703 and batch: 350, loss is 4.095315117835998 and perplexity is 60.058261605324475
At time: 82.14107537269592 and batch: 400, loss is 4.016454277038574 and perplexity is 55.50395486478608
At time: 83.0461196899414 and batch: 450, loss is 4.0434732913970945 and perplexity is 57.02406041973995
At time: 83.95452308654785 and batch: 500, loss is 3.9211645221710203 and perplexity is 50.45917140102941
At time: 84.86217093467712 and batch: 550, loss is 3.9988168382644655 and perplexity is 54.53358979132927
At time: 85.77039432525635 and batch: 600, loss is 4.020751843452453 and perplexity is 55.743000085494494
At time: 86.68460726737976 and batch: 650, loss is 3.855942234992981 and perplexity is 47.27313836835628
At time: 87.5935411453247 and batch: 700, loss is 3.875919795036316 and perplexity is 48.22703688590703
At time: 88.50003409385681 and batch: 750, loss is 3.9775566387176515 and perplexity is 53.38643239681639
At time: 89.40727233886719 and batch: 800, loss is 3.930789613723755 and perplexity is 50.94719039157329
At time: 90.3132894039154 and batch: 850, loss is 4.00962128162384 and perplexity is 55.12598938448638
At time: 91.2301664352417 and batch: 900, loss is 3.9583095359802245 and perplexity is 52.36872363761795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39162246495077 and perplexity of 80.77136142993133
finished 5 epochs...
Completing Train Step...
At time: 93.3788857460022 and batch: 50, loss is 4.048368873596192 and perplexity is 57.30391085132207
At time: 94.29385423660278 and batch: 100, loss is 3.9161579418182373 and perplexity is 50.20717485173699
At time: 95.20424175262451 and batch: 150, loss is 3.920185580253601 and perplexity is 50.409798973339136
At time: 96.11384987831116 and batch: 200, loss is 3.8093767499923707 and perplexity is 45.122307622260465
At time: 97.02182865142822 and batch: 250, loss is 3.9635144662857056 and perplexity is 52.642009795067416
At time: 97.94379568099976 and batch: 300, loss is 3.942649412155151 and perplexity is 51.55501099076644
At time: 98.85276699066162 and batch: 350, loss is 3.9412862491607665 and perplexity is 51.48478098595575
At time: 99.76699161529541 and batch: 400, loss is 3.8668106174468995 and perplexity is 47.78972305091051
At time: 100.67483377456665 and batch: 450, loss is 3.8951451063156126 and perplexity is 49.16318673401351
At time: 101.58256983757019 and batch: 500, loss is 3.777303524017334 and perplexity is 43.69805198113979
At time: 102.49146795272827 and batch: 550, loss is 3.854823799133301 and perplexity is 47.22029595114104
At time: 103.40089011192322 and batch: 600, loss is 3.8778926515579224 and perplexity is 48.322275825636375
At time: 104.31144785881042 and batch: 650, loss is 3.718303990364075 and perplexity is 41.194468612501005
At time: 105.220041513443 and batch: 700, loss is 3.7291412258148195 and perplexity is 41.643330586505485
At time: 106.1361620426178 and batch: 750, loss is 3.834868173599243 and perplexity is 46.287325374232815
At time: 107.04591131210327 and batch: 800, loss is 3.793165335655212 and perplexity is 44.39670858204041
At time: 107.9553952217102 and batch: 850, loss is 3.8687598800659178 and perplexity is 47.882968622155246
At time: 108.86274337768555 and batch: 900, loss is 3.8215830945968627 and perplexity is 45.67646127317494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3729268949325775 and perplexity of 79.27532300729294
finished 6 epochs...
Completing Train Step...
At time: 111.00299525260925 and batch: 50, loss is 3.916227045059204 and perplexity is 50.2106444501179
At time: 111.91477680206299 and batch: 100, loss is 3.781821722984314 and perplexity is 43.89593517564834
At time: 112.83688473701477 and batch: 150, loss is 3.790656204223633 and perplexity is 44.28545104334293
At time: 113.74668550491333 and batch: 200, loss is 3.679350190162659 and perplexity is 39.62063982427731
At time: 114.653400182724 and batch: 250, loss is 3.8303564977645874 and perplexity is 46.07896235353316
At time: 115.57012414932251 and batch: 300, loss is 3.8145954465866088 and perplexity is 45.3584027740218
At time: 116.48002624511719 and batch: 350, loss is 3.8104149150848388 and perplexity is 45.16917635145525
At time: 117.3880205154419 and batch: 400, loss is 3.7414730405807495 and perplexity is 42.16004790844383
At time: 118.29261708259583 and batch: 450, loss is 3.770515480041504 and perplexity is 43.40243215793242
At time: 119.2010018825531 and batch: 500, loss is 3.6561543273925783 and perplexity is 38.71218186559242
At time: 120.11015319824219 and batch: 550, loss is 3.728065600395203 and perplexity is 41.598562042977164
At time: 121.03615927696228 and batch: 600, loss is 3.755173544883728 and perplexity is 42.74163675932877
At time: 121.94611477851868 and batch: 650, loss is 3.5952578926086427 and perplexity is 36.42509253882715
At time: 122.8536856174469 and batch: 700, loss is 3.607848114967346 and perplexity is 36.8865916453674
At time: 123.76152563095093 and batch: 750, loss is 3.716014575958252 and perplexity is 41.10026527900031
At time: 124.6690616607666 and batch: 800, loss is 3.6771658658981323 and perplexity is 39.53418995098402
At time: 125.57688474655151 and batch: 850, loss is 3.7484993600845335 and perplexity is 42.45732102026572
At time: 126.48413801193237 and batch: 900, loss is 3.705606207847595 and perplexity is 40.67469716527827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382210927466824 and perplexity of 80.01474478207219
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 128.63451051712036 and batch: 50, loss is 3.824438467025757 and perplexity is 45.80707096225635
At time: 129.55009651184082 and batch: 100, loss is 3.69234525680542 and perplexity is 40.1388726212025
At time: 130.4616003036499 and batch: 150, loss is 3.7026909160614014 and perplexity is 40.5562912325083
At time: 131.37043285369873 and batch: 200, loss is 3.5794368267059324 and perplexity is 35.853343514726966
At time: 132.27884721755981 and batch: 250, loss is 3.7155024862289427 and perplexity is 41.07922364334162
At time: 133.18770003318787 and batch: 300, loss is 3.683728790283203 and perplexity is 39.794503123742416
At time: 134.09734201431274 and batch: 350, loss is 3.667908802032471 and perplexity is 39.169908121182566
At time: 135.01415848731995 and batch: 400, loss is 3.5966780233383178 and perplexity is 36.47685768000679
At time: 135.92154812812805 and batch: 450, loss is 3.6094972324371337 and perplexity is 36.947472153808235
At time: 136.8291311264038 and batch: 500, loss is 3.4859279441833495 and perplexity is 32.652712950954594
At time: 137.7374186515808 and batch: 550, loss is 3.5410854244232177 and perplexity is 34.50435073762276
At time: 138.64623546600342 and batch: 600, loss is 3.5547183656692507 and perplexity is 34.97796758388248
At time: 139.5551254749298 and batch: 650, loss is 3.3815431070327757 and perplexity is 29.416128343282384
At time: 140.46360325813293 and batch: 700, loss is 3.3747513103485107 and perplexity is 29.21701690792191
At time: 141.3730170726776 and batch: 750, loss is 3.465255494117737 and perplexity is 31.9846306142651
At time: 142.28126192092896 and batch: 800, loss is 3.40888888835907 and perplexity is 30.231634869537793
At time: 143.19468593597412 and batch: 850, loss is 3.462493271827698 and perplexity is 31.896403861691542
At time: 144.1034812927246 and batch: 900, loss is 3.407382378578186 and perplexity is 30.186124905125315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365342231645976 and perplexity of 78.67632086170666
finished 8 epochs...
Completing Train Step...
At time: 146.24018001556396 and batch: 50, loss is 3.7103384923934937 and perplexity is 40.86763757046436
At time: 147.1533443927765 and batch: 100, loss is 3.5732678747177125 and perplexity is 35.63284677613207
At time: 148.06094765663147 and batch: 150, loss is 3.5849704217910765 and perplexity is 36.05229134088309
At time: 148.9665126800537 and batch: 200, loss is 3.4659498882293702 and perplexity is 32.00684826643708
At time: 149.8798954486847 and batch: 250, loss is 3.602277145385742 and perplexity is 36.68166890451311
At time: 150.7904040813446 and batch: 300, loss is 3.57686541557312 and perplexity is 35.76126826046623
At time: 151.69659972190857 and batch: 350, loss is 3.564052491188049 and perplexity is 35.3059848191632
At time: 152.60186505317688 and batch: 400, loss is 3.496587529182434 and perplexity is 33.00263904014408
At time: 153.50928711891174 and batch: 450, loss is 3.514728264808655 and perplexity is 33.60679453430401
At time: 154.41571879386902 and batch: 500, loss is 3.397129383087158 and perplexity is 29.87820793567956
At time: 155.32318305969238 and batch: 550, loss is 3.456742262840271 and perplexity is 31.71349381919737
At time: 156.22872853279114 and batch: 600, loss is 3.4759268760681152 and perplexity is 32.327778498740756
At time: 157.13696932792664 and batch: 650, loss is 3.308573913574219 and perplexity is 27.346099749839627
At time: 158.04390001296997 and batch: 700, loss is 3.3052642583847045 and perplexity is 27.255743195887522
At time: 158.95082807540894 and batch: 750, loss is 3.4020883512496947 and perplexity is 30.026740998565433
At time: 159.85822987556458 and batch: 800, loss is 3.3537643337249756 and perplexity is 28.610229643748827
At time: 160.76682829856873 and batch: 850, loss is 3.412107548713684 and perplexity is 30.32909699875277
At time: 161.67326498031616 and batch: 900, loss is 3.3660215663909914 and perplexity is 28.963069890075428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381792460402397 and perplexity of 79.9812682516136
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.82693815231323 and batch: 50, loss is 3.668218421936035 and perplexity is 39.18203778205298
At time: 164.74317979812622 and batch: 100, loss is 3.548149824142456 and perplexity is 34.74896627695061
At time: 165.6508891582489 and batch: 150, loss is 3.56687313079834 and perplexity is 35.405710857913576
At time: 166.575133562088 and batch: 200, loss is 3.4357191705703736 and perplexity is 31.05373747094759
At time: 167.48356556892395 and batch: 250, loss is 3.576966814994812 and perplexity is 35.764894616238905
At time: 168.40083050727844 and batch: 300, loss is 3.5448346757888793 and perplexity is 34.63395903696967
At time: 169.31740593910217 and batch: 350, loss is 3.5231442260742187 and perplexity is 33.890821518985675
At time: 170.22849297523499 and batch: 400, loss is 3.4549999618530274 and perplexity is 31.658287474607
At time: 171.1386682987213 and batch: 450, loss is 3.4677252435684203 and perplexity is 32.0637222662343
At time: 172.04932379722595 and batch: 500, loss is 3.348005623817444 and perplexity is 28.445945118314917
At time: 172.95911860466003 and batch: 550, loss is 3.3996750593185423 and perplexity is 29.954365074038442
At time: 173.86892175674438 and batch: 600, loss is 3.418350267410278 and perplexity is 30.51902523710411
At time: 174.78284001350403 and batch: 650, loss is 3.2425701808929444 and perplexity is 25.59943243912175
At time: 175.68896436691284 and batch: 700, loss is 3.2340500783920287 and perplexity is 25.382249176420032
At time: 176.59507060050964 and batch: 750, loss is 3.3242661142349244 and perplexity is 27.778604837559406
At time: 177.50114393234253 and batch: 800, loss is 3.2666234731674195 and perplexity is 26.222648218217447
At time: 178.4069058895111 and batch: 850, loss is 3.3228642082214357 and perplexity is 27.739689128847886
At time: 179.3117983341217 and batch: 900, loss is 3.2836291646957396 and perplexity is 26.672395780404127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377114491919949 and perplexity of 79.60799216713744
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 181.45541906356812 and batch: 50, loss is 3.655137758255005 and perplexity is 38.67284825231571
At time: 182.36286902427673 and batch: 100, loss is 3.5389743280410766 and perplexity is 34.431585561786704
At time: 183.27107167243958 and batch: 150, loss is 3.5554697275161744 and perplexity is 35.00425856999302
At time: 184.17894625663757 and batch: 200, loss is 3.42614173412323 and perplexity is 30.75774197546519
At time: 185.09617400169373 and batch: 250, loss is 3.567682671546936 and perplexity is 35.43438482839385
At time: 186.00283646583557 and batch: 300, loss is 3.531981725692749 and perplexity is 34.19165900884914
At time: 186.90796875953674 and batch: 350, loss is 3.5083546876907348 and perplexity is 33.39328018552718
At time: 187.81597518920898 and batch: 400, loss is 3.442568483352661 and perplexity is 31.267164311162215
At time: 188.73095679283142 and batch: 450, loss is 3.448590235710144 and perplexity is 31.45601546834673
At time: 189.63816046714783 and batch: 500, loss is 3.331605978012085 and perplexity is 27.983246113802696
At time: 190.54432463645935 and batch: 550, loss is 3.3836145973205567 and perplexity is 29.477126724414287
At time: 191.45340609550476 and batch: 600, loss is 3.409336552619934 and perplexity is 30.245171521728004
At time: 192.36115741729736 and batch: 650, loss is 3.224942669868469 and perplexity is 25.152132139439427
At time: 193.2684564590454 and batch: 700, loss is 3.2125971269607545 and perplexity is 24.843524298193632
At time: 194.18408489227295 and batch: 750, loss is 3.3088443660736084 and perplexity is 27.353496571064834
At time: 195.0928852558136 and batch: 800, loss is 3.2441933727264405 and perplexity is 25.64101897112779
At time: 196.00080704689026 and batch: 850, loss is 3.2979299592971802 and perplexity is 27.056572704226934
At time: 196.91108918190002 and batch: 900, loss is 3.2630731058120728 and perplexity is 26.129713258258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36729410249893 and perplexity of 78.8300368472879
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 199.0644567012787 and batch: 50, loss is 3.6458116245269774 and perplexity is 38.313856701466165
At time: 199.9841501712799 and batch: 100, loss is 3.5293053245544432 and perplexity is 34.10027076411052
At time: 200.89206957817078 and batch: 150, loss is 3.5455628347396853 and perplexity is 34.659187248203494
At time: 201.80030488967896 and batch: 200, loss is 3.4151497364044188 and perplexity is 30.421504293211957
At time: 202.7087118625641 and batch: 250, loss is 3.559958896636963 and perplexity is 35.161751849152274
At time: 203.62308764457703 and batch: 300, loss is 3.526213221549988 and perplexity is 33.99499206460128
At time: 204.53444504737854 and batch: 350, loss is 3.5042259979248045 and perplexity is 33.25569391233554
At time: 205.4434540271759 and batch: 400, loss is 3.4390811014175413 and perplexity is 31.158313679363104
At time: 206.3519742488861 and batch: 450, loss is 3.4446699666976928 and perplexity is 31.33294082612681
At time: 207.26087069511414 and batch: 500, loss is 3.3251320552825927 and perplexity is 27.80266988968966
At time: 208.1685667037964 and batch: 550, loss is 3.3762381505966186 and perplexity is 29.26049025554288
At time: 209.07541418075562 and batch: 600, loss is 3.403206272125244 and perplexity is 30.06032728906222
At time: 209.98429918289185 and batch: 650, loss is 3.2188660383224486 and perplexity is 24.999755337553395
At time: 210.89030385017395 and batch: 700, loss is 3.2044438457489015 and perplexity is 24.641791567668513
At time: 211.81296181678772 and batch: 750, loss is 3.300722212791443 and perplexity is 27.132227087857203
At time: 212.72197198867798 and batch: 800, loss is 3.2366804027557374 and perplexity is 25.449100606764265
At time: 213.6295804977417 and batch: 850, loss is 3.2891086769104003 and perplexity is 26.81894865131226
At time: 214.54701328277588 and batch: 900, loss is 3.2545502758026124 and perplexity is 25.907960476908293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364481886772261 and perplexity of 78.60866120186466
finished 12 epochs...
Completing Train Step...
At time: 216.6857931613922 and batch: 50, loss is 3.6398321485519407 and perplexity is 38.08544349201717
At time: 217.59767985343933 and batch: 100, loss is 3.521779580116272 and perplexity is 33.844604088775355
At time: 218.50316834449768 and batch: 150, loss is 3.5395136308670043 and perplexity is 34.450159621252546
At time: 219.40973591804504 and batch: 200, loss is 3.408362102508545 and perplexity is 30.21571346600352
At time: 220.3170943260193 and batch: 250, loss is 3.5530232429504394 and perplexity is 34.91872586155354
At time: 221.22272443771362 and batch: 300, loss is 3.518984246253967 and perplexity is 33.7501292268181
At time: 222.13569903373718 and batch: 350, loss is 3.4968003368377687 and perplexity is 33.00966300172796
At time: 223.0507071018219 and batch: 400, loss is 3.432328505516052 and perplexity is 30.948622953264707
At time: 223.95790314674377 and batch: 450, loss is 3.4394428014755247 and perplexity is 31.169585681646645
At time: 224.8655309677124 and batch: 500, loss is 3.3205480670928953 and perplexity is 27.67551444146887
At time: 225.77290749549866 and batch: 550, loss is 3.3723386669158937 and perplexity is 29.146611629526156
At time: 226.68616199493408 and batch: 600, loss is 3.4002238130569458 and perplexity is 29.970807154768355
At time: 227.59326314926147 and batch: 650, loss is 3.2164310121536257 and perplexity is 24.9389543351508
At time: 228.49895000457764 and batch: 700, loss is 3.2025397205352784 and perplexity is 24.594915154479096
At time: 229.40583205223083 and batch: 750, loss is 3.2998388147354127 and perplexity is 27.108269114963406
At time: 230.31259202957153 and batch: 800, loss is 3.2367414474487304 and perplexity is 25.450654186716175
At time: 231.21886038780212 and batch: 850, loss is 3.291581645011902 and perplexity is 26.88535313007989
At time: 232.12294816970825 and batch: 900, loss is 3.25896888256073 and perplexity is 26.02269085370266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363437182282748 and perplexity of 78.52658126269822
finished 13 epochs...
Completing Train Step...
At time: 234.257426738739 and batch: 50, loss is 3.636539878845215 and perplexity is 37.960262118633366
At time: 235.16530084609985 and batch: 100, loss is 3.5177618885040283 and perplexity is 33.70889969854659
At time: 236.07280135154724 and batch: 150, loss is 3.535614867210388 and perplexity is 34.31610807840085
At time: 236.9936125278473 and batch: 200, loss is 3.404132380485535 and perplexity is 30.08817930447919
At time: 237.90178990364075 and batch: 250, loss is 3.548626298904419 and perplexity is 34.765527227510056
At time: 238.80943727493286 and batch: 300, loss is 3.5148075342178347 and perplexity is 33.60945863064044
At time: 239.71890306472778 and batch: 350, loss is 3.4925376415252685 and perplexity is 32.86925234252444
At time: 240.6343731880188 and batch: 400, loss is 3.4282512044906617 and perplexity is 30.822693002779992
At time: 241.5443572998047 and batch: 450, loss is 3.4360995197296145 and perplexity is 31.065550980372723
At time: 242.45703268051147 and batch: 500, loss is 3.317513680458069 and perplexity is 27.59166351274258
At time: 243.36642265319824 and batch: 550, loss is 3.369621138572693 and perplexity is 29.067512412175898
At time: 244.2821340560913 and batch: 600, loss is 3.397911581993103 and perplexity is 29.901587779890676
At time: 245.19429683685303 and batch: 650, loss is 3.2146008682250975 and perplexity is 24.893354199424113
At time: 246.10418677330017 and batch: 700, loss is 3.201225447654724 and perplexity is 24.562611956750207
At time: 247.0229024887085 and batch: 750, loss is 3.299234080314636 and perplexity is 27.091880767340303
At time: 247.93238592147827 and batch: 800, loss is 3.2367401027679445 and perplexity is 25.450619963733512
At time: 248.84019017219543 and batch: 850, loss is 3.292848868370056 and perplexity is 26.919444473669856
At time: 249.75512766838074 and batch: 900, loss is 3.261317834854126 and perplexity is 26.08388876040677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363108595756636 and perplexity of 78.50078272491167
finished 14 epochs...
Completing Train Step...
At time: 251.88874793052673 and batch: 50, loss is 3.6338700342178343 and perplexity is 37.85904928814883
At time: 252.80922293663025 and batch: 100, loss is 3.5147115802764892 and perplexity is 33.60623382533721
At time: 253.7149076461792 and batch: 150, loss is 3.5324378395080567 and perplexity is 34.20725785404534
At time: 254.62186241149902 and batch: 200, loss is 3.400801296234131 and perplexity is 29.988119790103596
At time: 255.5282895565033 and batch: 250, loss is 3.5452134084701536 and perplexity is 34.64707853337309
At time: 256.4396929740906 and batch: 300, loss is 3.5115618228912355 and perplexity is 33.50054887051355
At time: 257.34768772125244 and batch: 350, loss is 3.489284996986389 and perplexity is 32.76251403338952
At time: 258.25873732566833 and batch: 400, loss is 3.4250936365127562 and perplexity is 30.72552174751861
At time: 259.16921496391296 and batch: 450, loss is 3.4334230852127074 and perplexity is 30.98251723420059
At time: 260.0777871608734 and batch: 500, loss is 3.3150486612319945 and perplexity is 27.523733290753054
At time: 260.9871575832367 and batch: 550, loss is 3.367327289581299 and perplexity is 29.00091234260081
At time: 261.89505076408386 and batch: 600, loss is 3.3959111404418945 and perplexity is 29.841831190958057
At time: 262.80195593833923 and batch: 650, loss is 3.2130200624465943 and perplexity is 24.854033728461506
At time: 263.7100865840912 and batch: 700, loss is 3.200066633224487 and perplexity is 24.534164933164472
At time: 264.6179029941559 and batch: 750, loss is 3.2986163330078124 and perplexity is 27.075149999185033
At time: 265.5226249694824 and batch: 800, loss is 3.2366052532196044 and perplexity is 25.44718819051816
At time: 266.4298007488251 and batch: 850, loss is 3.2934801626205443 and perplexity is 26.9364439294572
At time: 267.3378360271454 and batch: 900, loss is 3.262652907371521 and perplexity is 26.11873589999075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363077660129495 and perplexity of 78.49835429152978
finished 15 epochs...
Completing Train Step...
At time: 269.47314643859863 and batch: 50, loss is 3.6314901304244995 and perplexity is 37.76905552386304
At time: 270.385981798172 and batch: 100, loss is 3.512103123664856 and perplexity is 33.51868765235929
At time: 271.2928864955902 and batch: 150, loss is 3.529651050567627 and perplexity is 34.11206215294775
At time: 272.1991205215454 and batch: 200, loss is 3.397933797836304 and perplexity is 29.90225207625517
At time: 273.10601234436035 and batch: 250, loss is 3.5423025369644163 and perplexity is 34.546371982736254
At time: 274.0138649940491 and batch: 300, loss is 3.508765754699707 and perplexity is 33.40700988305233
At time: 274.920982837677 and batch: 350, loss is 3.4865184879302977 and perplexity is 32.67200150121448
At time: 275.8314130306244 and batch: 400, loss is 3.42239794254303 and perplexity is 30.64280668116916
At time: 276.73945665359497 and batch: 450, loss is 3.431081838607788 and perplexity is 30.91006436902194
At time: 277.6479196548462 and batch: 500, loss is 3.3128703594207765 and perplexity is 27.46384354532619
At time: 278.55497455596924 and batch: 550, loss is 3.365272183418274 and perplexity is 28.941373589096784
At time: 279.478253364563 and batch: 600, loss is 3.3941030406951906 and perplexity is 29.787922933979917
At time: 280.386780500412 and batch: 650, loss is 3.211573724746704 and perplexity is 24.81811238594361
At time: 281.29554510116577 and batch: 700, loss is 3.198963146209717 and perplexity is 24.507106732674625
At time: 282.20417070388794 and batch: 750, loss is 3.297955722808838 and perplexity is 27.057269785532814
At time: 283.1171009540558 and batch: 800, loss is 3.236349606513977 and perplexity is 25.44068353217244
At time: 284.02844738960266 and batch: 850, loss is 3.293741068840027 and perplexity is 26.943472732098428
At time: 284.94392442703247 and batch: 900, loss is 3.2634255027770998 and perplexity is 26.138922912539037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363200148491011 and perplexity of 78.50797001522373
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 287.0879600048065 and batch: 50, loss is 3.630242872238159 and perplexity is 37.72197712573242
At time: 287.99281072616577 and batch: 100, loss is 3.5120244455337524 and perplexity is 33.516050568399535
At time: 288.91061449050903 and batch: 150, loss is 3.5299513292312623 and perplexity is 34.12230681543469
At time: 289.8190886974335 and batch: 200, loss is 3.39781400680542 and perplexity is 29.898670269191676
At time: 290.7250053882599 and batch: 250, loss is 3.542582097053528 and perplexity is 34.5560311196579
At time: 291.63077116012573 and batch: 300, loss is 3.508629312515259 and perplexity is 33.402452068594165
At time: 292.53910851478577 and batch: 350, loss is 3.486962175369263 and perplexity is 32.68650087424341
At time: 293.44402599334717 and batch: 400, loss is 3.4229894876480103 and perplexity is 30.660938645873006
At time: 294.3497145175934 and batch: 450, loss is 3.431409840583801 and perplexity is 30.920204594129697
At time: 295.2574636936188 and batch: 500, loss is 3.312002410888672 and perplexity is 27.440016684375372
At time: 296.1630940437317 and batch: 550, loss is 3.3634104347229004 and perplexity is 28.887542150429116
At time: 297.07048988342285 and batch: 600, loss is 3.391769599914551 and perplexity is 29.71849561360787
At time: 297.97831177711487 and batch: 650, loss is 3.2093633794784546 and perplexity is 24.763316370031106
At time: 298.8850965499878 and batch: 700, loss is 3.1962535858154295 and perplexity is 24.440793127823163
At time: 299.7943892478943 and batch: 750, loss is 3.295355296134949 and perplexity is 26.98700074380301
At time: 300.7096574306488 and batch: 800, loss is 3.2330051708221434 and perplexity is 25.35574092386364
At time: 301.6229326725006 and batch: 850, loss is 3.2894448947906496 and perplexity is 26.827967177388224
At time: 302.53018736839294 and batch: 900, loss is 3.259100527763367 and perplexity is 26.02611684161574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362695981378424 and perplexity of 78.46839885474321
finished 17 epochs...
Completing Train Step...
At time: 304.6728341579437 and batch: 50, loss is 3.6290220403671265 and perplexity is 37.67595303337467
At time: 305.5877101421356 and batch: 100, loss is 3.5106884098052977 and perplexity is 33.471301826978475
At time: 306.4971363544464 and batch: 150, loss is 3.5286708545684813 and perplexity is 34.078642027915045
At time: 307.4059884548187 and batch: 200, loss is 3.3967100238800048 and perplexity is 29.86568086094414
At time: 308.323703289032 and batch: 250, loss is 3.5414131355285643 and perplexity is 34.51566004953459
At time: 309.23925137519836 and batch: 300, loss is 3.5072900390625 and perplexity is 33.35774699412348
At time: 310.1556673049927 and batch: 350, loss is 3.4855169343948362 and perplexity is 32.63929512392825
At time: 311.0669000148773 and batch: 400, loss is 3.421712293624878 and perplexity is 30.621803675086095
At time: 311.97639441490173 and batch: 450, loss is 3.4304107427597046 and perplexity is 30.889327712080348
At time: 312.8842992782593 and batch: 500, loss is 3.311164288520813 and perplexity is 27.41702822753339
At time: 313.79449462890625 and batch: 550, loss is 3.362759575843811 and perplexity is 28.868746554429222
At time: 314.70431995391846 and batch: 600, loss is 3.391376910209656 and perplexity is 29.706827757408877
At time: 315.6121561527252 and batch: 650, loss is 3.209079828262329 and perplexity is 24.756295696966312
At time: 316.51918625831604 and batch: 700, loss is 3.196029863357544 and perplexity is 24.435325785118412
At time: 317.42857241630554 and batch: 750, loss is 3.2951013040542603 and perplexity is 26.980147129751238
At time: 318.340713262558 and batch: 800, loss is 3.233204598426819 and perplexity is 25.360798062790646
At time: 319.24910378456116 and batch: 850, loss is 3.290086197853088 and perplexity is 26.845177552841484
At time: 320.1580982208252 and batch: 900, loss is 3.259889235496521 and perplexity is 26.046651938262958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362386207058005 and perplexity of 78.44409512434375
finished 18 epochs...
Completing Train Step...
At time: 322.28830099105835 and batch: 50, loss is 3.6281384325027464 and perplexity is 37.64267696863767
At time: 323.20250606536865 and batch: 100, loss is 3.50968523979187 and perplexity is 33.43774125696847
At time: 324.10870814323425 and batch: 150, loss is 3.5277124786376954 and perplexity is 34.04599752299414
At time: 325.0238993167877 and batch: 200, loss is 3.3957999324798585 and perplexity is 29.838512726251157
At time: 325.93269205093384 and batch: 250, loss is 3.5404570484161377 and perplexity is 34.48267584218469
At time: 326.84101700782776 and batch: 300, loss is 3.5062663316726685 and perplexity is 33.32361589512732
At time: 327.74911880493164 and batch: 350, loss is 3.4844621658325194 and perplexity is 32.60488637136067
At time: 328.65686559677124 and batch: 400, loss is 3.420738306045532 and perplexity is 30.591992938649568
At time: 329.56309032440186 and batch: 450, loss is 3.429615979194641 and perplexity is 30.864787752889924
At time: 330.47146439552307 and batch: 500, loss is 3.310496602058411 and perplexity is 27.398728358913694
At time: 331.37779355049133 and batch: 550, loss is 3.3622257804870603 and perplexity is 28.853340663725533
At time: 332.28665947914124 and batch: 600, loss is 3.3910112762451172 and perplexity is 29.695967917686183
At time: 333.1944930553436 and batch: 650, loss is 3.208801474571228 and perplexity is 24.749405649660606
At time: 334.1015727519989 and batch: 700, loss is 3.1958144092559815 and perplexity is 24.430061661063917
At time: 335.0094051361084 and batch: 750, loss is 3.294938817024231 and perplexity is 26.975763561920477
At time: 335.91685461997986 and batch: 800, loss is 3.2332898664474485 and perplexity is 25.362960620040226
At time: 336.8338451385498 and batch: 850, loss is 3.290498623847961 and perplexity is 26.8562514853331
At time: 337.74771189689636 and batch: 900, loss is 3.2604606533050537 and perplexity is 26.06153971219761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362193904510916 and perplexity of 78.42901157539617
finished 19 epochs...
Completing Train Step...
At time: 339.9033441543579 and batch: 50, loss is 3.627393012046814 and perplexity is 37.61462780271901
At time: 340.80414867401123 and batch: 100, loss is 3.5088395500183105 and perplexity is 33.409475254955574
At time: 341.71026253700256 and batch: 150, loss is 3.526892137527466 and perplexity is 34.01807964424472
At time: 342.6161985397339 and batch: 200, loss is 3.394984426498413 and perplexity is 29.814189160000378
At time: 343.52394104003906 and batch: 250, loss is 3.539608511924744 and perplexity is 34.453428443908926
At time: 344.4336311817169 and batch: 300, loss is 3.5053992223739625 and perplexity is 33.29473320195375
At time: 345.3424651622772 and batch: 350, loss is 3.483584222793579 and perplexity is 32.576273700321025
At time: 346.25246691703796 and batch: 400, loss is 3.419912714958191 and perplexity is 30.566746884827904
At time: 347.17705488204956 and batch: 450, loss is 3.428921551704407 and perplexity is 30.84336183602793
At time: 348.08512115478516 and batch: 500, loss is 3.309902114868164 and perplexity is 27.38244500647705
At time: 348.99355697631836 and batch: 550, loss is 3.3617358779907227 and perplexity is 28.839208802006503
At time: 349.90155959129333 and batch: 600, loss is 3.390645155906677 and perplexity is 29.685097609903657
At time: 350.80866050720215 and batch: 650, loss is 3.208513479232788 and perplexity is 24.74227896247995
At time: 351.716943025589 and batch: 700, loss is 3.1955926036834716 and perplexity is 24.424643538157873
At time: 352.6254725456238 and batch: 750, loss is 3.2948013162612915 and perplexity is 26.97205462884624
At time: 353.53781270980835 and batch: 800, loss is 3.2333109951019288 and perplexity is 25.363496510933068
At time: 354.4516222476959 and batch: 850, loss is 3.2907762002944945 and perplexity is 26.86370718290275
At time: 355.35952138900757 and batch: 900, loss is 3.26089063167572 and perplexity is 26.07274802007241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36208144932577 and perplexity of 78.4201923222733
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb19ab91b70>
ELAPSED
588.9164798259735


RESULTS SO FAR:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.4201923222733, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2343070501481508, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.08771921670537419, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.341034173965454 and batch: 50, loss is 6.948516473770142 and perplexity is 1041.6033355375166
At time: 2.477999687194824 and batch: 100, loss is 6.029532632827759 and perplexity is 415.52078321992104
At time: 3.6119472980499268 and batch: 150, loss is 5.7617259693145755 and perplexity is 317.8965353532464
At time: 4.74999475479126 and batch: 200, loss is 5.5061284446716305 and perplexity is 246.19611768342887
At time: 5.886298418045044 and batch: 250, loss is 5.503853330612182 and perplexity is 245.63663012504315
At time: 7.019931793212891 and batch: 300, loss is 5.383786087036133 and perplexity is 217.8454980613913
At time: 8.152191162109375 and batch: 350, loss is 5.322521171569824 and perplexity is 204.89981893506638
At time: 9.282336711883545 and batch: 400, loss is 5.149630146026611 and perplexity is 172.36772763719034
At time: 10.414260625839233 and batch: 450, loss is 5.140545377731323 and perplexity is 170.80889829294898
At time: 11.546228170394897 and batch: 500, loss is 5.060016527175903 and perplexity is 157.59312087107372
At time: 12.676721096038818 and batch: 550, loss is 5.107944965362549 and perplexity is 165.3302461433707
At time: 13.812616348266602 and batch: 600, loss is 5.02610013961792 and perplexity is 152.33775676024348
At time: 14.95119047164917 and batch: 650, loss is 4.898279438018799 and perplexity is 134.05892445286412
At time: 16.08958864212036 and batch: 700, loss is 4.974666996002197 and perplexity is 144.70063123555747
At time: 17.226250648498535 and batch: 750, loss is 4.968387441635132 and perplexity is 143.79482276806928
At time: 18.363473892211914 and batch: 800, loss is 4.909342851638794 and perplexity is 135.55030847068886
At time: 19.5016131401062 and batch: 850, loss is 4.953717432022095 and perplexity is 141.70074893259684
At time: 20.65316152572632 and batch: 900, loss is 4.8682013034820555 and perplexity is 130.08671980491005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.884728000588613 and perplexity of 132.25448729881572
finished 1 epochs...
Completing Train Step...
At time: 22.962791204452515 and batch: 50, loss is 4.848222894668579 and perplexity is 127.51358330744002
At time: 23.871521949768066 and batch: 100, loss is 4.716877717971801 and perplexity is 111.81857792402538
At time: 24.779841661453247 and batch: 150, loss is 4.702580881118775 and perplexity is 110.2312995249537
At time: 25.688568115234375 and batch: 200, loss is 4.581685037612915 and perplexity is 97.67884814571774
At time: 26.598143815994263 and batch: 250, loss is 4.7028600788116455 and perplexity is 110.26208014620249
At time: 27.507735013961792 and batch: 300, loss is 4.656733093261718 and perplexity is 105.29154202472603
At time: 28.41550374031067 and batch: 350, loss is 4.636452007293701 and perplexity is 103.17762394437584
At time: 29.324962377548218 and batch: 400, loss is 4.524401636123657 and perplexity is 92.24071580019896
At time: 30.23393988609314 and batch: 450, loss is 4.539147911071777 and perplexity is 93.61100122871966
At time: 31.144114017486572 and batch: 500, loss is 4.437732419967651 and perplexity is 84.5829254971466
At time: 32.051172494888306 and batch: 550, loss is 4.5089123249053955 and perplexity is 90.82297887542848
At time: 32.96216368675232 and batch: 600, loss is 4.481156663894653 and perplexity is 88.33678958187626
At time: 33.87308692932129 and batch: 650, loss is 4.332205619812012 and perplexity is 76.1119756534561
At time: 34.78292179107666 and batch: 700, loss is 4.372174434661865 and perplexity is 79.21569391335761
At time: 35.69306421279907 and batch: 750, loss is 4.4388790035247805 and perplexity is 84.67996250862556
At time: 36.601460456848145 and batch: 800, loss is 4.372887849807739 and perplexity is 79.27222775283722
At time: 37.511054039001465 and batch: 850, loss is 4.44185209274292 and perplexity is 84.93209821740668
At time: 38.42081904411316 and batch: 900, loss is 4.376285657882691 and perplexity is 79.54203769002501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.536356886772261 and perplexity of 93.35009491688403
finished 2 epochs...
Completing Train Step...
At time: 40.559181451797485 and batch: 50, loss is 4.436785898208618 and perplexity is 84.5029037948364
At time: 41.47815465927124 and batch: 100, loss is 4.296519474983215 and perplexity is 73.44372560809357
At time: 42.38586211204529 and batch: 150, loss is 4.294289064407349 and perplexity is 73.2800984914254
At time: 43.30602145195007 and batch: 200, loss is 4.184789209365845 and perplexity is 65.67965480138658
At time: 44.21283936500549 and batch: 250, loss is 4.329405832290649 and perplexity is 75.89917632924933
At time: 45.12081050872803 and batch: 300, loss is 4.301451673507691 and perplexity is 73.80685943107261
At time: 46.0292432308197 and batch: 350, loss is 4.290269603729248 and perplexity is 72.9861431837153
At time: 46.93598437309265 and batch: 400, loss is 4.202017493247986 and perplexity is 66.82100607083453
At time: 47.84279465675354 and batch: 450, loss is 4.223555822372436 and perplexity is 68.27582988853351
At time: 48.75195646286011 and batch: 500, loss is 4.10443067073822 and perplexity is 60.60822868499803
At time: 49.665879249572754 and batch: 550, loss is 4.185402183532715 and perplexity is 65.71992707473939
At time: 50.575196504592896 and batch: 600, loss is 4.191915740966797 and perplexity is 66.14939475711468
At time: 51.482048749923706 and batch: 650, loss is 4.024775981903076 and perplexity is 55.967769583961704
At time: 52.38982081413269 and batch: 700, loss is 4.052239637374878 and perplexity is 57.52615059511639
At time: 53.29783320426941 and batch: 750, loss is 4.148783788681031 and perplexity is 63.3568980447738
At time: 54.20564651489258 and batch: 800, loss is 4.0857575368881225 and perplexity is 59.48698427183823
At time: 55.113585472106934 and batch: 850, loss is 4.165931658744812 and perplexity is 64.45270240388187
At time: 56.02172660827637 and batch: 900, loss is 4.109371638298034 and perplexity is 60.90843301597427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.409473157908819 and perplexity of 82.2261319034666
finished 3 epochs...
Completing Train Step...
At time: 58.169018507003784 and batch: 50, loss is 4.18484703540802 and perplexity is 65.68345290568878
At time: 59.0839364528656 and batch: 100, loss is 4.05003743648529 and perplexity is 57.39960584472407
At time: 59.99049472808838 and batch: 150, loss is 4.050409302711487 and perplexity is 57.42095478876435
At time: 60.898513317108154 and batch: 200, loss is 3.9420474910736085 and perplexity is 51.52398828034029
At time: 61.80580711364746 and batch: 250, loss is 4.091980137825012 and perplexity is 59.85830211915147
At time: 62.72112011909485 and batch: 300, loss is 4.070469369888306 and perplexity is 58.584453918179314
At time: 63.6359498500824 and batch: 350, loss is 4.059013981819152 and perplexity is 57.91717552718882
At time: 64.54165530204773 and batch: 400, loss is 3.9856717729568483 and perplexity is 53.82143311963855
At time: 65.4451515674591 and batch: 450, loss is 4.009481644630432 and perplexity is 55.11829229448185
At time: 66.35811853408813 and batch: 500, loss is 3.8831553220748902 and perplexity is 48.57725037698909
At time: 67.26100873947144 and batch: 550, loss is 3.9676962089538574 and perplexity is 52.8626060504883
At time: 68.16119456291199 and batch: 600, loss is 3.983602204322815 and perplexity is 53.710161152018316
At time: 69.06195330619812 and batch: 650, loss is 3.8174701595306395 and perplexity is 45.48898276166139
At time: 69.96550440788269 and batch: 700, loss is 3.8405908870697023 and perplexity is 46.55297386477466
At time: 70.8692398071289 and batch: 750, loss is 3.9417230558395384 and perplexity is 51.50727479451078
At time: 71.77650284767151 and batch: 800, loss is 3.8876620435714724 and perplexity is 48.79666857248208
At time: 72.68527746200562 and batch: 850, loss is 3.9672946882247926 and perplexity is 52.84138487902187
At time: 73.59322166442871 and batch: 900, loss is 3.917350959777832 and perplexity is 50.26710865698004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371003033363656 and perplexity of 79.12295487458874
finished 4 epochs...
Completing Train Step...
At time: 75.74236679077148 and batch: 50, loss is 3.9960292625427245 and perplexity is 54.381784962436235
At time: 76.6470513343811 and batch: 100, loss is 3.8676217126846315 and perplexity is 47.8285007917842
At time: 77.55423998832703 and batch: 150, loss is 3.8675901174545286 and perplexity is 47.82698966316857
At time: 78.47227454185486 and batch: 200, loss is 3.7612047529220582 and perplexity is 43.00019940128765
At time: 79.37763667106628 and batch: 250, loss is 3.9109045028686524 and perplexity is 49.94410613656713
At time: 80.28491449356079 and batch: 300, loss is 3.890699281692505 and perplexity is 48.94510097261676
At time: 81.19086742401123 and batch: 350, loss is 3.8820544719696044 and perplexity is 48.52380352967839
At time: 82.10446405410767 and batch: 400, loss is 3.8126047039031983 and perplexity is 45.26819568491086
At time: 83.02154636383057 and batch: 450, loss is 3.837735457420349 and perplexity is 46.42023472676123
At time: 83.92894744873047 and batch: 500, loss is 3.713360896110535 and perplexity is 40.99134291974787
At time: 84.83630228042603 and batch: 550, loss is 3.7991009283065797 and perplexity is 44.66101298489957
At time: 85.74524784088135 and batch: 600, loss is 3.8184110832214357 and perplexity is 45.53180456607788
At time: 86.65891170501709 and batch: 650, loss is 3.6547689199447633 and perplexity is 38.658586854550705
At time: 87.56756663322449 and batch: 700, loss is 3.6773839569091797 and perplexity is 39.54281294270568
At time: 88.49699139595032 and batch: 750, loss is 3.777170329093933 and perplexity is 43.6922320100573
At time: 89.40374684333801 and batch: 800, loss is 3.7279155254364014 and perplexity is 41.5923196089206
At time: 90.31009721755981 and batch: 850, loss is 3.808734130859375 and perplexity is 45.093320478901504
At time: 91.21717953681946 and batch: 900, loss is 3.7634116315841677 and perplexity is 43.095200413108685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369695794092466 and perplexity of 79.01958981687525
finished 5 epochs...
Completing Train Step...
At time: 93.43541169166565 and batch: 50, loss is 3.8436706352233885 and perplexity is 46.69656630085723
At time: 94.35146117210388 and batch: 100, loss is 3.7186728620529177 and perplexity is 41.20966688864364
At time: 95.27108883857727 and batch: 150, loss is 3.716346459388733 and perplexity is 41.113908039812685
At time: 96.18814277648926 and batch: 200, loss is 3.613442621231079 and perplexity is 37.093532238854515
At time: 97.09866523742676 and batch: 250, loss is 3.7610876417160033 and perplexity is 42.995163890938265
At time: 98.01497316360474 and batch: 300, loss is 3.743940019607544 and perplexity is 42.26418426068671
At time: 98.92734098434448 and batch: 350, loss is 3.7385059642791747 and perplexity is 42.035141224886864
At time: 99.84507083892822 and batch: 400, loss is 3.6688582038879396 and perplexity is 39.20711376338958
At time: 100.76458215713501 and batch: 450, loss is 3.696324920654297 and perplexity is 40.298930117778724
At time: 101.6729097366333 and batch: 500, loss is 3.57227605342865 and perplexity is 35.59752288050115
At time: 102.58230519294739 and batch: 550, loss is 3.654324450492859 and perplexity is 38.641408111636096
At time: 103.49192094802856 and batch: 600, loss is 3.6796259593963625 and perplexity is 39.63156748444749
At time: 104.4011583328247 and batch: 650, loss is 3.517206072807312 and perplexity is 33.6901689688741
At time: 105.30970191955566 and batch: 700, loss is 3.5415456247329713 and perplexity is 34.52023330482092
At time: 106.22023868560791 and batch: 750, loss is 3.6431320476531983 and perplexity is 38.21132920360647
At time: 107.12717294692993 and batch: 800, loss is 3.590029010772705 and perplexity is 36.23512712026524
At time: 108.0358715057373 and batch: 850, loss is 3.674434332847595 and perplexity is 39.42634835799738
At time: 108.94559478759766 and batch: 900, loss is 3.6295094776153562 and perplexity is 37.69432217278337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.390022591368793 and perplexity of 80.64224077850021
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 111.08048176765442 and batch: 50, loss is 3.73688645362854 and perplexity is 41.967119961419826
At time: 111.99460792541504 and batch: 100, loss is 3.62100652217865 and perplexity is 37.37516783161083
At time: 112.90213394165039 and batch: 150, loss is 3.6221174240112304 and perplexity is 37.416711044991516
At time: 113.81063199043274 and batch: 200, loss is 3.5059010219573974 and perplexity is 33.31144467776019
At time: 114.7182948589325 and batch: 250, loss is 3.6439402723312377 and perplexity is 38.242225026550955
At time: 115.6260142326355 and batch: 300, loss is 3.607712440490723 and perplexity is 36.88158741583229
At time: 116.53321599960327 and batch: 350, loss is 3.5976323175430296 and perplexity is 36.511683948507056
At time: 117.44103169441223 and batch: 400, loss is 3.520189208984375 and perplexity is 33.79082138602697
At time: 118.3492341041565 and batch: 450, loss is 3.5287896394729614 and perplexity is 34.08269029658499
At time: 119.26421451568604 and batch: 500, loss is 3.398574390411377 and perplexity is 29.921413373550322
At time: 120.1718864440918 and batch: 550, loss is 3.455860333442688 and perplexity is 31.68553708644245
At time: 121.08465123176575 and batch: 600, loss is 3.4697638511657716 and perplexity is 32.129154286491556
At time: 121.99290299415588 and batch: 650, loss is 3.289176092147827 and perplexity is 26.820756718048063
At time: 122.90086388587952 and batch: 700, loss is 3.289873843193054 and perplexity is 26.839477459530805
At time: 123.80879974365234 and batch: 750, loss is 3.378157353401184 and perplexity is 29.316700993153315
At time: 124.71570420265198 and batch: 800, loss is 3.309041018486023 and perplexity is 27.358876231097735
At time: 125.62261867523193 and batch: 850, loss is 3.361335711479187 and perplexity is 28.82767062517471
At time: 126.53015279769897 and batch: 900, loss is 3.308940763473511 and perplexity is 27.356133504107227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366981819884418 and perplexity of 78.80542344064872
finished 7 epochs...
Completing Train Step...
At time: 128.67788410186768 and batch: 50, loss is 3.6209102630615235 and perplexity is 37.371570304103095
At time: 129.58651280403137 and batch: 100, loss is 3.496175513267517 and perplexity is 32.989044228456976
At time: 130.49449348449707 and batch: 150, loss is 3.4951816415786743 and perplexity is 32.95627363893443
At time: 131.4035291671753 and batch: 200, loss is 3.3824176168441773 and perplexity is 29.441864287659797
At time: 132.31166195869446 and batch: 250, loss is 3.5211113119125366 and perplexity is 33.82199437151381
At time: 133.22579383850098 and batch: 300, loss is 3.4897392416000366 and perplexity is 32.777399609509175
At time: 134.13903141021729 and batch: 350, loss is 3.4814463329315184 and perplexity is 32.506703608128724
At time: 135.0466775894165 and batch: 400, loss is 3.4090254497528076 and perplexity is 30.23576362563847
At time: 135.95899176597595 and batch: 450, loss is 3.424603977203369 and perplexity is 30.71048039262942
At time: 136.86850118637085 and batch: 500, loss is 3.2989274358749388 and perplexity is 27.083574466347653
At time: 137.77661108970642 and batch: 550, loss is 3.3616031312942507 and perplexity is 28.83538074639566
At time: 138.6831991672516 and batch: 600, loss is 3.3825228691101072 and perplexity is 29.444963273674066
At time: 139.59757781028748 and batch: 650, loss is 3.206815128326416 and perplexity is 24.70029355364876
At time: 140.50610160827637 and batch: 700, loss is 3.2155508184432984 and perplexity is 24.917012882183887
At time: 141.41498446464539 and batch: 750, loss is 3.3099990320205688 and perplexity is 27.38509896367787
At time: 142.33306503295898 and batch: 800, loss is 3.246977672576904 and perplexity is 25.71251073748641
At time: 143.2410671710968 and batch: 850, loss is 3.3079961776733398 and perplexity is 27.33030548916015
At time: 144.14832091331482 and batch: 900, loss is 3.2629804706573484 and perplexity is 26.12729284033704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391568536627783 and perplexity of 80.7670056833141
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 146.29833579063416 and batch: 50, loss is 3.566189503669739 and perplexity is 35.3815148249383
At time: 147.21095061302185 and batch: 100, loss is 3.4530896854400637 and perplexity is 31.597869121020317
At time: 148.11797738075256 and batch: 150, loss is 3.4620853900909423 and perplexity is 31.88339655399014
At time: 149.025484085083 and batch: 200, loss is 3.349338927268982 and perplexity is 28.483897490515528
At time: 149.9308557510376 and batch: 250, loss is 3.4896957445144654 and perplexity is 32.77597391916047
At time: 150.83804178237915 and batch: 300, loss is 3.4534766340255736 and perplexity is 31.610098237648923
At time: 151.7453978061676 and batch: 350, loss is 3.436190657615662 and perplexity is 31.068382358038896
At time: 152.6597020626068 and batch: 400, loss is 3.36280375957489 and perplexity is 28.870022111542806
At time: 153.56651973724365 and batch: 450, loss is 3.370342297554016 and perplexity is 29.088482270208573
At time: 154.47150301933289 and batch: 500, loss is 3.238775987625122 and perplexity is 25.50248727554572
At time: 155.37824511528015 and batch: 550, loss is 3.2930139541625976 and perplexity is 26.923888858337857
At time: 156.28474307060242 and batch: 600, loss is 3.311327815055847 and perplexity is 27.421512005758746
At time: 157.21702527999878 and batch: 650, loss is 3.1279526472091677 and perplexity is 22.827196343784898
At time: 158.12821245193481 and batch: 700, loss is 3.1277424812316896 and perplexity is 22.82239934785255
At time: 159.03631234169006 and batch: 750, loss is 3.2180296325683595 and perplexity is 24.978854140497486
At time: 159.94434189796448 and batch: 800, loss is 3.147906084060669 and perplexity is 23.287251945967476
At time: 160.85219931602478 and batch: 850, loss is 3.2019921016693114 and perplexity is 24.58145020210112
At time: 161.7679545879364 and batch: 900, loss is 3.1569612216949463 and perplexity is 23.49907883085836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.389575278922303 and perplexity of 80.60617656707377
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.9060935974121 and batch: 50, loss is 3.5454698991775513 and perplexity is 34.65596632682487
At time: 164.82663989067078 and batch: 100, loss is 3.4271279430389403 and perplexity is 30.788090497360177
At time: 165.735018491745 and batch: 150, loss is 3.4375480699539183 and perplexity is 31.1105835993443
At time: 166.6445689201355 and batch: 200, loss is 3.32724271774292 and perplexity is 27.86141391391784
At time: 167.55464386940002 and batch: 250, loss is 3.4687292432785033 and perplexity is 32.09593039986667
At time: 168.46377730369568 and batch: 300, loss is 3.432658338546753 and perplexity is 30.958832515002182
At time: 169.37275624275208 and batch: 350, loss is 3.4124808645248415 and perplexity is 30.34042144386622
At time: 170.28115129470825 and batch: 400, loss is 3.3390890836715696 and perplexity is 28.193433146236547
At time: 171.18954849243164 and batch: 450, loss is 3.345153865814209 and perplexity is 28.364939725451062
At time: 172.09893369674683 and batch: 500, loss is 3.2114778566360473 and perplexity is 24.815733234443307
At time: 173.0059039592743 and batch: 550, loss is 3.2643931102752686 and perplexity is 26.164227370760717
At time: 173.91649103164673 and batch: 600, loss is 3.2848773050308226 and perplexity is 26.70570745791055
At time: 174.8268599510193 and batch: 650, loss is 3.1029566669464113 and perplexity is 22.263680351875543
At time: 175.73646807670593 and batch: 700, loss is 3.1021443748474122 and perplexity is 22.245603083236297
At time: 176.64623260498047 and batch: 750, loss is 3.186383957862854 and perplexity is 24.200758069618253
At time: 177.55520224571228 and batch: 800, loss is 3.114411458969116 and perplexity is 22.520172406970495
At time: 178.46556448936462 and batch: 850, loss is 3.1669008255004885 and perplexity is 23.73381502403731
At time: 179.37954473495483 and batch: 900, loss is 3.1239162397384646 and perplexity is 22.735242185001923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385546122511772 and perplexity of 80.28205508059764
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 181.529545545578 and batch: 50, loss is 3.5352264738082884 and perplexity is 34.30278251638515
At time: 182.43484091758728 and batch: 100, loss is 3.41565016746521 and perplexity is 30.436731968759396
At time: 183.34289526939392 and batch: 150, loss is 3.426799988746643 and perplexity is 30.77799506644037
At time: 184.25103735923767 and batch: 200, loss is 3.316249580383301 and perplexity is 27.556806924582588
At time: 185.16720294952393 and batch: 250, loss is 3.4586471128463745 and perplexity is 31.773960840111528
At time: 186.08345890045166 and batch: 300, loss is 3.4222919750213623 and perplexity is 30.639559710927944
At time: 186.9945034980774 and batch: 350, loss is 3.4018179416656493 and perplexity is 30.018622577720645
At time: 187.90351033210754 and batch: 400, loss is 3.3289469289779663 and perplexity is 27.908936130980628
At time: 188.81132340431213 and batch: 450, loss is 3.336234450340271 and perplexity is 28.1130659962017
At time: 189.71866917610168 and batch: 500, loss is 3.202120471000671 and perplexity is 24.584605908971074
At time: 190.6337833404541 and batch: 550, loss is 3.2537217807769774 and perplexity is 25.886504749737718
At time: 191.54908633232117 and batch: 600, loss is 3.2751023149490357 and perplexity is 26.44593115624335
At time: 192.45572328567505 and batch: 650, loss is 3.094446029663086 and perplexity is 22.07500625104623
At time: 193.37464904785156 and batch: 700, loss is 3.09392343044281 and perplexity is 22.063472883918674
At time: 194.2873511314392 and batch: 750, loss is 3.1763474273681642 and perplexity is 23.959081250352384
At time: 195.20507287979126 and batch: 800, loss is 3.1042950439453123 and perplexity is 22.29349749841009
At time: 196.1162850856781 and batch: 850, loss is 3.1564038801193237 and perplexity is 23.485985466314233
At time: 197.0240979194641 and batch: 900, loss is 3.11268235206604 and perplexity is 22.48126626753377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385306580425942 and perplexity of 80.26282645279368
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 199.16175198554993 and batch: 50, loss is 3.5318178510665894 and perplexity is 34.18605632259289
At time: 200.0799605846405 and batch: 100, loss is 3.4126908445358275 and perplexity is 30.346792994820085
At time: 200.9980685710907 and batch: 150, loss is 3.423758239746094 and perplexity is 30.684518369105827
At time: 201.9087257385254 and batch: 200, loss is 3.313222360610962 and perplexity is 27.47351255259241
At time: 202.8491449356079 and batch: 250, loss is 3.455775694847107 and perplexity is 31.682855380572736
At time: 203.76045989990234 and batch: 300, loss is 3.419109053611755 and perplexity is 30.542191440322718
At time: 204.6675786972046 and batch: 350, loss is 3.398173909187317 and perplexity is 29.90943280845269
At time: 205.5745816230774 and batch: 400, loss is 3.3258423709869387 and perplexity is 27.822425578292346
At time: 206.4801173210144 and batch: 450, loss is 3.3337103748321533 and perplexity is 28.042195973133154
At time: 207.38428902626038 and batch: 500, loss is 3.199237651824951 and perplexity is 24.51383499451647
At time: 208.29055428504944 and batch: 550, loss is 3.2506109571456907 and perplexity is 25.80610152399814
At time: 209.19734907150269 and batch: 600, loss is 3.272572431564331 and perplexity is 26.379110594172356
At time: 210.10297870635986 and batch: 650, loss is 3.0917227983474733 and perplexity is 22.01497268247868
At time: 211.00692558288574 and batch: 700, loss is 3.091368708610535 and perplexity is 22.007178786543548
At time: 211.9106969833374 and batch: 750, loss is 3.17367495059967 and perplexity is 23.895136645610073
At time: 212.8174183368683 and batch: 800, loss is 3.1016308164596555 and perplexity is 22.234181600232418
At time: 213.72939920425415 and batch: 850, loss is 3.153753066062927 and perplexity is 23.42381092888809
At time: 214.6378698348999 and batch: 900, loss is 3.1093499517440795 and perplexity is 22.406474376060352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384753083529538 and perplexity of 80.21841351979957
Annealing...
Model not improving. Stopping early with 78.80542344064872 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb19ab91b70>
ELAPSED
810.4156923294067


RESULTS SO FAR:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.4201923222733, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.80542344064872, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2343070501481508, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.08771921670537419, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.562999421480413, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.8319819592781227, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.351912260055542 and batch: 50, loss is 7.01779914855957 and perplexity is 1116.3270421802965
At time: 2.485924005508423 and batch: 100, loss is 6.145601034164429 and perplexity is 466.66004342062246
At time: 3.6164815425872803 and batch: 150, loss is 6.001192283630371 and perplexity is 403.91008189836646
At time: 4.753566265106201 and batch: 200, loss is 5.84643250465393 and perplexity is 345.99783044708926
At time: 5.8838067054748535 and batch: 250, loss is 5.89507529258728 and perplexity is 363.2441844637776
At time: 7.012006759643555 and batch: 300, loss is 5.800302562713623 and perplexity is 330.3995113608319
At time: 8.14006781578064 and batch: 350, loss is 5.7922287940979 and perplexity is 327.7426819007798
At time: 9.271160364151001 and batch: 400, loss is 5.655728826522827 and perplexity is 285.92479638016965
At time: 10.406760692596436 and batch: 450, loss is 5.66001178741455 and perplexity is 287.1520273160904
At time: 11.539788246154785 and batch: 500, loss is 5.617606983184815 and perplexity is 275.2299646161734
At time: 12.67396354675293 and batch: 550, loss is 5.672385034561157 and perplexity is 290.7271023511479
At time: 13.804048776626587 and batch: 600, loss is 5.592351570129394 and perplexity is 268.36595965335204
At time: 14.934236526489258 and batch: 650, loss is 5.508899068832397 and perplexity is 246.87918041346828
At time: 16.063625812530518 and batch: 700, loss is 5.609174890518188 and perplexity is 272.91895705758066
At time: 17.194164037704468 and batch: 750, loss is 5.5686397361755375 and perplexity is 262.0773623119526
At time: 18.32437562942505 and batch: 800, loss is 5.569270391464233 and perplexity is 262.24269491500246
At time: 19.4712917804718 and batch: 850, loss is 5.605847492218017 and perplexity is 272.0123561314316
At time: 20.6046085357666 and batch: 900, loss is 5.497261753082276 and perplexity is 244.02282184775584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.45624782614512 and perplexity of 234.21695083406868
finished 1 epochs...
Completing Train Step...
At time: 22.941627025604248 and batch: 50, loss is 5.3643260669708255 and perplexity is 213.6472022460996
At time: 23.86038064956665 and batch: 100, loss is 5.183857231140137 and perplexity is 178.36949816918346
At time: 24.765682220458984 and batch: 150, loss is 5.131398267745972 and perplexity is 169.25361453374046
At time: 25.680460214614868 and batch: 200, loss is 4.977100486755371 and perplexity is 145.05318768108975
At time: 26.586061000823975 and batch: 250, loss is 5.0435176181793215 and perplexity is 155.0143383584595
At time: 27.49269938468933 and batch: 300, loss is 4.973161764144898 and perplexity is 144.48298707920267
At time: 28.396768808364868 and batch: 350, loss is 4.93063985824585 and perplexity is 138.46808394560225
At time: 29.3035089969635 and batch: 400, loss is 4.794877872467041 and perplexity is 120.88961693079912
At time: 30.21252751350403 and batch: 450, loss is 4.798116378784179 and perplexity is 121.28175334430742
At time: 31.120365142822266 and batch: 500, loss is 4.703923816680908 and perplexity is 110.37943250140412
At time: 32.0283465385437 and batch: 550, loss is 4.767976026535035 and perplexity is 117.68081790730353
At time: 32.94421339035034 and batch: 600, loss is 4.703619737625122 and perplexity is 110.34587353033953
At time: 33.855515480041504 and batch: 650, loss is 4.558112163543701 and perplexity is 95.40320407133869
At time: 34.77314877510071 and batch: 700, loss is 4.609161338806152 and perplexity is 100.39991280750962
At time: 35.682501554489136 and batch: 750, loss is 4.640751495361328 and perplexity is 103.62218992559775
At time: 36.590986490249634 and batch: 800, loss is 4.574553804397583 and perplexity is 96.98475530980409
At time: 37.500998973846436 and batch: 850, loss is 4.632567567825317 and perplexity is 102.77761411962231
At time: 38.40948271751404 and batch: 900, loss is 4.5581006336212155 and perplexity is 95.40210408613228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6736822258936215 and perplexity of 107.09135185709547
finished 2 epochs...
Completing Train Step...
At time: 40.54075217247009 and batch: 50, loss is 4.604666919708252 and perplexity is 99.9496860337393
At time: 41.450631618499756 and batch: 100, loss is 4.46455554485321 and perplexity is 86.88240562474391
At time: 42.35501003265381 and batch: 150, loss is 4.46509635925293 and perplexity is 86.92940558877113
At time: 43.26178050041199 and batch: 200, loss is 4.356604704856872 and perplexity is 77.9918789198461
At time: 44.16841244697571 and batch: 250, loss is 4.493312177658081 and perplexity is 89.41712133506473
At time: 45.07501792907715 and batch: 300, loss is 4.451986889839173 and perplexity is 85.79724443527297
At time: 45.98075747489929 and batch: 350, loss is 4.446959524154663 and perplexity is 85.36699273528568
At time: 46.88670778274536 and batch: 400, loss is 4.343773593902588 and perplexity is 76.99754928676643
At time: 47.79354786872864 and batch: 450, loss is 4.370755529403686 and perplexity is 79.10337405319815
At time: 48.700098752975464 and batch: 500, loss is 4.25154908657074 and perplexity is 70.21409585784038
At time: 49.60673928260803 and batch: 550, loss is 4.3305216836929326 and perplexity is 75.98391580112514
At time: 50.51379442214966 and batch: 600, loss is 4.324477257728577 and perplexity is 75.52602189459076
At time: 51.42015790939331 and batch: 650, loss is 4.167431607246399 and perplexity is 64.54945067885176
At time: 52.32794809341431 and batch: 700, loss is 4.1950248432159425 and perplexity is 66.35538003785346
At time: 53.234129905700684 and batch: 750, loss is 4.282037138938904 and perplexity is 72.38775381903363
At time: 54.14030718803406 and batch: 800, loss is 4.221060085296631 and perplexity is 68.10564382666804
At time: 55.04695963859558 and batch: 850, loss is 4.298447308540344 and perplexity is 73.58544945297984
At time: 55.95987343788147 and batch: 900, loss is 4.232096238136291 and perplexity is 68.86143094122401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4765382531571065 and perplexity of 87.92975465508388
finished 3 epochs...
Completing Train Step...
At time: 58.11502981185913 and batch: 50, loss is 4.3094343662261965 and perplexity is 74.39839479172072
At time: 59.020577907562256 and batch: 100, loss is 4.170900921821595 and perplexity is 64.77378194176595
At time: 59.93011498451233 and batch: 150, loss is 4.177768487930297 and perplexity is 65.22015115203344
At time: 60.87886619567871 and batch: 200, loss is 4.06685366153717 and perplexity is 58.37301210521937
At time: 61.78936743736267 and batch: 250, loss is 4.219027285575867 and perplexity is 67.96733931322889
At time: 62.69427704811096 and batch: 300, loss is 4.18671745300293 and perplexity is 65.80642335889706
At time: 63.59986615180969 and batch: 350, loss is 4.183300089836121 and perplexity is 65.58192273014416
At time: 64.50782704353333 and batch: 400, loss is 4.096375799179077 and perplexity is 60.121998078975686
At time: 65.40992021560669 and batch: 450, loss is 4.130012636184692 and perplexity is 62.17870863144288
At time: 66.31174087524414 and batch: 500, loss is 3.994919810295105 and perplexity is 54.32148442534856
At time: 67.21335625648499 and batch: 550, loss is 4.080646896362305 and perplexity is 59.183743217291344
At time: 68.11694312095642 and batch: 600, loss is 4.095449671745301 and perplexity is 60.06634322290381
At time: 69.02170157432556 and batch: 650, loss is 3.930141625404358 and perplexity is 50.91418790106243
At time: 69.92958521842957 and batch: 700, loss is 3.9475861024856567 and perplexity is 51.81015137137183
At time: 70.8536286354065 and batch: 750, loss is 4.057446250915527 and perplexity is 57.82644811792814
At time: 71.76911187171936 and batch: 800, loss is 3.9971070957183836 and perplexity is 54.44043105409437
At time: 72.6773738861084 and batch: 850, loss is 4.077211470603943 and perplexity is 58.9807707094831
At time: 73.58509516716003 and batch: 900, loss is 4.021718621253967 and perplexity is 55.79691723932115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.395944673721105 and perplexity of 81.12122766949183
finished 4 epochs...
Completing Train Step...
At time: 75.69874858856201 and batch: 50, loss is 4.104096827507019 and perplexity is 60.58799841515364
At time: 76.61475491523743 and batch: 100, loss is 3.967817497253418 and perplexity is 52.869018054929214
At time: 77.52114987373352 and batch: 150, loss is 3.977674651145935 and perplexity is 53.39273303111019
At time: 78.42865657806396 and batch: 200, loss is 3.866620855331421 and perplexity is 47.78065523235767
At time: 79.33523774147034 and batch: 250, loss is 4.023463659286499 and perplexity is 55.894369986647085
At time: 80.25009083747864 and batch: 300, loss is 3.9946333599090575 and perplexity is 54.30592624359421
At time: 81.15684533119202 and batch: 350, loss is 3.9869308423995973 and perplexity is 53.88924071972537
At time: 82.06113195419312 and batch: 400, loss is 3.9132172679901123 and perplexity is 50.05974879887366
At time: 82.96460008621216 and batch: 450, loss is 3.948220157623291 and perplexity is 51.84301228074376
At time: 83.8710069656372 and batch: 500, loss is 3.8096974372863768 and perplexity is 45.13678009343594
At time: 84.78177952766418 and batch: 550, loss is 3.8948696517944335 and perplexity is 49.14964637691395
At time: 85.69492149353027 and batch: 600, loss is 3.9203583002090454 and perplexity is 50.41850650353223
At time: 86.61044836044312 and batch: 650, loss is 3.7521262407302856 and perplexity is 42.61158824143804
At time: 87.51595735549927 and batch: 700, loss is 3.764807162284851 and perplexity is 43.15538307194414
At time: 88.42063403129578 and batch: 750, loss is 3.8825609636306764 and perplexity is 48.548386656578515
At time: 89.32574558258057 and batch: 800, loss is 3.8269955778121947 and perplexity is 45.92435460723787
At time: 90.2335033416748 and batch: 850, loss is 3.9043765306472777 and perplexity is 49.61913425682796
At time: 91.13799691200256 and batch: 900, loss is 3.8569272422790526 and perplexity is 47.31972569474582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370102137735445 and perplexity of 79.05170544941787
finished 5 epochs...
Completing Train Step...
At time: 93.28513932228088 and batch: 50, loss is 3.9447730922698976 and perplexity is 51.664613681758254
At time: 94.2012848854065 and batch: 100, loss is 3.8081756687164305 and perplexity is 45.06814459705715
At time: 95.11154699325562 and batch: 150, loss is 3.817339496612549 and perplexity is 45.48303942672798
At time: 96.01905012130737 and batch: 200, loss is 3.7085470151901245 and perplexity is 40.794489670362424
At time: 96.92513513565063 and batch: 250, loss is 3.8651955127716064 and perplexity is 47.71259994349075
At time: 97.83235502243042 and batch: 300, loss is 3.838805675506592 and perplexity is 46.46994109511632
At time: 98.73918533325195 and batch: 350, loss is 3.8301123762130738 and perplexity is 46.06771485868508
At time: 99.6491072177887 and batch: 400, loss is 3.7625383043289187 and perplexity is 43.05758062959969
At time: 100.56466317176819 and batch: 450, loss is 3.796209006309509 and perplexity is 44.53204339391897
At time: 101.48469305038452 and batch: 500, loss is 3.6594916868209837 and perplexity is 38.84159415828748
At time: 102.39310693740845 and batch: 550, loss is 3.7435259771347047 and perplexity is 42.24668871552222
At time: 103.30246686935425 and batch: 600, loss is 3.7759984731674194 and perplexity is 43.641060997416794
At time: 104.20947480201721 and batch: 650, loss is 3.609176778793335 and perplexity is 36.935634098602655
At time: 105.11733984947205 and batch: 700, loss is 3.61788547039032 and perplexity is 37.25869984306533
At time: 106.03351712226868 and batch: 750, loss is 3.739529709815979 and perplexity is 42.078196548191954
At time: 106.9434323310852 and batch: 800, loss is 3.686216559410095 and perplexity is 39.89362590621013
At time: 107.85158920288086 and batch: 850, loss is 3.7640618753433226 and perplexity is 43.1232319108924
At time: 108.76136302947998 and batch: 900, loss is 3.7167649984359743 and perplexity is 41.131119417277546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370496357956978 and perplexity of 79.08287537375624
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.90089774131775 and batch: 50, loss is 3.8268700790405275 and perplexity is 45.91859151878288
At time: 111.80620694160461 and batch: 100, loss is 3.697009844779968 and perplexity is 40.326541281951954
At time: 112.71294212341309 and batch: 150, loss is 3.708158025741577 and perplexity is 40.778624130285664
At time: 113.62026572227478 and batch: 200, loss is 3.5821827173233034 and perplexity is 35.951928163673394
At time: 114.52693486213684 and batch: 250, loss is 3.728222451210022 and perplexity is 41.60508732306328
At time: 115.43432879447937 and batch: 300, loss is 3.696128935813904 and perplexity is 40.29103291228314
At time: 116.3398323059082 and batch: 350, loss is 3.675235652923584 and perplexity is 39.45795414394376
At time: 117.24671459197998 and batch: 400, loss is 3.598213768005371 and perplexity is 36.53291985723597
At time: 118.16276288032532 and batch: 450, loss is 3.610732426643372 and perplexity is 36.99313765442265
At time: 119.06876850128174 and batch: 500, loss is 3.470702385902405 and perplexity is 32.15932276871785
At time: 119.97730755805969 and batch: 550, loss is 3.5314139461517335 and perplexity is 34.17225119459186
At time: 120.8848443031311 and batch: 600, loss is 3.5617115545272826 and perplexity is 35.22343240763767
At time: 121.79321146011353 and batch: 650, loss is 3.379870362281799 and perplexity is 29.366963800331526
At time: 122.70000839233398 and batch: 700, loss is 3.3652111721038818 and perplexity is 28.939607891718104
At time: 123.6125659942627 and batch: 750, loss is 3.470725836753845 and perplexity is 32.160076941061476
At time: 124.51896142959595 and batch: 800, loss is 3.400459675788879 and perplexity is 29.977876984944082
At time: 125.42503881454468 and batch: 850, loss is 3.456257367134094 and perplexity is 31.69811980991335
At time: 126.33311343193054 and batch: 900, loss is 3.4002923822402953 and perplexity is 29.97286229899826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342505886130137 and perplexity of 76.90000073966566
finished 7 epochs...
Completing Train Step...
At time: 128.47166109085083 and batch: 50, loss is 3.7100647020339967 and perplexity is 40.85644993688515
At time: 129.38600993156433 and batch: 100, loss is 3.574751071929932 and perplexity is 35.68573652840453
At time: 130.29389810562134 and batch: 150, loss is 3.5852530908584597 and perplexity is 36.06248364891068
At time: 131.1994490623474 and batch: 200, loss is 3.4641354513168334 and perplexity is 31.948826513811518
At time: 132.10620713233948 and batch: 250, loss is 3.60960738658905 and perplexity is 36.95154229543618
At time: 133.0141441822052 and batch: 300, loss is 3.582011785507202 and perplexity is 35.945783360486175
At time: 133.92191290855408 and batch: 350, loss is 3.565025153160095 and perplexity is 35.340342314388245
At time: 134.82976770401 and batch: 400, loss is 3.493386025428772 and perplexity is 32.89714991938633
At time: 135.73745369911194 and batch: 450, loss is 3.5110007238388063 and perplexity is 33.481757016825114
At time: 136.65354466438293 and batch: 500, loss is 3.376453833580017 and perplexity is 29.26680192601239
At time: 137.56420135498047 and batch: 550, loss is 3.440663628578186 and perplexity is 31.20766159394929
At time: 138.47180676460266 and batch: 600, loss is 3.476886706352234 and perplexity is 32.35882257570139
At time: 139.38030886650085 and batch: 650, loss is 3.3000738286972044 and perplexity is 27.114640685360104
At time: 140.28902888298035 and batch: 700, loss is 3.289644923210144 and perplexity is 26.83333407000859
At time: 141.19516921043396 and batch: 750, loss is 3.403054084777832 and perplexity is 30.055752835685546
At time: 142.10332822799683 and batch: 800, loss is 3.3391055345535277 and perplexity is 28.193896956892264
At time: 143.01397967338562 and batch: 850, loss is 3.4021557998657226 and perplexity is 30.028766328991715
At time: 143.92997241020203 and batch: 900, loss is 3.3538563203811647 and perplexity is 28.61286152415364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3612955171767975 and perplexity of 78.35858358531337
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 146.05705952644348 and batch: 50, loss is 3.6534526491165162 and perplexity is 38.60773515905648
At time: 146.96757793426514 and batch: 100, loss is 3.5317083024978637 and perplexity is 34.18231149417642
At time: 147.87550711631775 and batch: 150, loss is 3.5506032037734987 and perplexity is 34.83432334684841
At time: 148.78156805038452 and batch: 200, loss is 3.4282119131088256 and perplexity is 30.821481960371923
At time: 149.68701887130737 and batch: 250, loss is 3.566960883140564 and perplexity is 35.40881792829383
At time: 150.59410452842712 and batch: 300, loss is 3.5377273178100586 and perplexity is 34.388675782348486
At time: 151.49976015090942 and batch: 350, loss is 3.5177175188064576 and perplexity is 33.70740407804188
At time: 152.40597939491272 and batch: 400, loss is 3.442320694923401 and perplexity is 31.259417629438826
At time: 153.30977201461792 and batch: 450, loss is 3.450372633934021 and perplexity is 31.512132611138828
At time: 154.22449493408203 and batch: 500, loss is 3.312667951583862 and perplexity is 27.458285210705046
At time: 155.1304919719696 and batch: 550, loss is 3.3714565896987914 and perplexity is 29.120913403028307
At time: 156.0368480682373 and batch: 600, loss is 3.403811507225037 and perplexity is 30.078526361051853
At time: 156.94360899925232 and batch: 650, loss is 3.2209136199951174 and perplexity is 25.050996821062153
At time: 157.84991812705994 and batch: 700, loss is 3.2025686168670653 and perplexity is 24.595625867576125
At time: 158.75772285461426 and batch: 750, loss is 3.3094029998779297 and perplexity is 27.368781427837163
At time: 159.663738489151 and batch: 800, loss is 3.241658301353455 and perplexity is 25.576099480498577
At time: 160.57079219818115 and batch: 850, loss is 3.29930956363678 and perplexity is 27.093925829686828
At time: 161.47632956504822 and batch: 900, loss is 3.2523600912094115 and perplexity is 25.851279354748595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355673175968536 and perplexity of 77.91926105964508
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.61541414260864 and batch: 50, loss is 3.6339035987854005 and perplexity is 37.860320032092524
At time: 164.52555418014526 and batch: 100, loss is 3.5076190423965454 and perplexity is 33.36872360967333
At time: 165.43398761749268 and batch: 150, loss is 3.5294614028930664 and perplexity is 34.10559349308852
At time: 166.34054493904114 and batch: 200, loss is 3.406944842338562 and perplexity is 30.172920270509263
At time: 167.24845433235168 and batch: 250, loss is 3.545394515991211 and perplexity is 34.6533539481234
At time: 168.15765404701233 and batch: 300, loss is 3.513762755393982 and perplexity is 33.57436251701262
At time: 169.0716187953949 and batch: 350, loss is 3.4947564125061037 and perplexity is 32.942262652410086
At time: 169.98038005828857 and batch: 400, loss is 3.421530785560608 and perplexity is 30.616246075166426
At time: 170.8883535861969 and batch: 450, loss is 3.426261377334595 and perplexity is 30.761422150639063
At time: 171.79737448692322 and batch: 500, loss is 3.288238859176636 and perplexity is 26.795631196612366
At time: 172.70554876327515 and batch: 550, loss is 3.346611289978027 and perplexity is 28.40630961342661
At time: 173.61496114730835 and batch: 600, loss is 3.3790930843353273 and perplexity is 29.344146375903875
At time: 174.52457809448242 and batch: 650, loss is 3.194960217475891 and perplexity is 24.409202613287814
At time: 175.43401718139648 and batch: 700, loss is 3.1765387868881225 and perplexity is 23.963666487339474
At time: 176.34291982650757 and batch: 750, loss is 3.2782826137542727 and perplexity is 26.53017100244179
At time: 177.25263595581055 and batch: 800, loss is 3.2113459157943725 and perplexity is 24.812459241705003
At time: 178.16107559204102 and batch: 850, loss is 3.264754309654236 and perplexity is 26.173679580401558
At time: 179.06859135627747 and batch: 900, loss is 3.22080451965332 and perplexity is 25.048263897830754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354432824539812 and perplexity of 77.82267370637848
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 181.20399117469788 and batch: 50, loss is 3.625663104057312 and perplexity is 37.549614207543016
At time: 182.11536717414856 and batch: 100, loss is 3.4983954191207887 and perplexity is 33.06235814572313
At time: 183.02188563346863 and batch: 150, loss is 3.519571108818054 and perplexity is 33.76994172723319
At time: 183.9274182319641 and batch: 200, loss is 3.3972961997985838 and perplexity is 29.883192535816423
At time: 184.83293843269348 and batch: 250, loss is 3.5364599561691286 and perplexity is 34.34512049975666
At time: 185.73822021484375 and batch: 300, loss is 3.503785343170166 and perplexity is 33.24104286095907
At time: 186.64477229118347 and batch: 350, loss is 3.484718246459961 and perplexity is 32.61323692028157
At time: 187.55147576332092 and batch: 400, loss is 3.4123420667648316 and perplexity is 30.336210553569888
At time: 188.46682453155518 and batch: 450, loss is 3.4172082424163817 and perplexity is 30.48419164169816
At time: 189.37363409996033 and batch: 500, loss is 3.279792127609253 and perplexity is 26.57024890460904
At time: 190.2798707485199 and batch: 550, loss is 3.337062969207764 and perplexity is 28.136367853483875
At time: 191.18513536453247 and batch: 600, loss is 3.3698318099975584 and perplexity is 29.073636751522063
At time: 192.09817504882812 and batch: 650, loss is 3.185885872840881 and perplexity is 24.18870703597221
At time: 193.00501132011414 and batch: 700, loss is 3.1681607294082643 and perplexity is 23.76373619527484
At time: 193.91290378570557 and batch: 750, loss is 3.2697055530548096 and perplexity is 26.303593190243912
At time: 194.820054769516 and batch: 800, loss is 3.202185940742493 and perplexity is 24.58621550946221
At time: 195.72762203216553 and batch: 850, loss is 3.2548414993286134 and perplexity is 25.91550658325839
At time: 196.63342666625977 and batch: 900, loss is 3.209642724990845 and perplexity is 24.770234857610212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3542501371200775 and perplexity of 77.80845778149711
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 198.76173424720764 and batch: 50, loss is 3.6228154373168944 and perplexity is 37.442837524409455
At time: 199.67406916618347 and batch: 100, loss is 3.495640573501587 and perplexity is 32.97140179609919
At time: 200.58204793930054 and batch: 150, loss is 3.51677170753479 and perplexity is 33.67553830720433
At time: 201.4887752532959 and batch: 200, loss is 3.394751257896423 and perplexity is 29.807238237594404
At time: 202.39601469039917 and batch: 250, loss is 3.5338956785202025 and perplexity is 34.257162896964154
At time: 203.3015239238739 and batch: 300, loss is 3.501362428665161 and perplexity is 33.16060014831074
At time: 204.20698070526123 and batch: 350, loss is 3.481629753112793 and perplexity is 32.51266654044151
At time: 205.13395977020264 and batch: 400, loss is 3.409599475860596 and perplexity is 30.253124725734143
At time: 206.04061436653137 and batch: 450, loss is 3.414851508140564 and perplexity is 30.41243309351404
At time: 206.9463245868683 and batch: 500, loss is 3.2771688413619997 and perplexity is 26.500638879501455
At time: 207.85287404060364 and batch: 550, loss is 3.334257788658142 and perplexity is 28.057550861275523
At time: 208.7588291168213 and batch: 600, loss is 3.3674116039276125 and perplexity is 29.003357638652794
At time: 209.66372323036194 and batch: 650, loss is 3.1833209133148195 and perplexity is 24.126743482412266
At time: 210.56882333755493 and batch: 700, loss is 3.1655579090118406 and perplexity is 23.701963884000342
At time: 211.4728603363037 and batch: 750, loss is 3.2672653293609617 and perplexity is 26.23948479013834
At time: 212.37747859954834 and batch: 800, loss is 3.1996809434890747 and perplexity is 24.524704182157393
At time: 213.28161454200745 and batch: 850, loss is 3.2523621845245363 and perplexity is 25.851333469679304
At time: 214.19553422927856 and batch: 900, loss is 3.2066086626052854 and perplexity is 24.695194316155057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353662778253424 and perplexity of 77.76276971287814
Annealing...
Model not improving. Stopping early with 76.90000073966566 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb19ab91b70>
ELAPSED
1031.502109527588


RESULTS SO FAR:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.4201923222733, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.80542344064872, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2343070501481508, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.08771921670537419, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -76.90000073966566, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.562999421480413, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.8319819592781227, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.01804147960545366, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.6597197598977973, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3383970260620117 and batch: 50, loss is 6.959417924880982 and perplexity is 1053.0204418210128
At time: 2.4680733680725098 and batch: 100, loss is 5.97708680152893 and perplexity is 394.29004824804946
At time: 3.5984249114990234 and batch: 150, loss is 5.747450160980224 and perplexity is 313.3905451962939
At time: 4.729351282119751 and batch: 200, loss is 5.531981954574585 and perplexity is 252.6441443575428
At time: 5.858738422393799 and batch: 250, loss is 5.545047254562378 and perplexity is 255.9666735506316
At time: 6.9893927574157715 and batch: 300, loss is 5.439272432327271 and perplexity is 230.27458215556211
At time: 8.120450973510742 and batch: 350, loss is 5.381656427383422 and perplexity is 217.3820549568462
At time: 9.251376867294312 and batch: 400, loss is 5.214722366333008 and perplexity is 183.96074019421903
At time: 10.380805253982544 and batch: 450, loss is 5.2137211894989015 and perplexity is 183.77665512897954
At time: 11.510819435119629 and batch: 500, loss is 5.139838905334472 and perplexity is 170.6882691367703
At time: 12.653005599975586 and batch: 550, loss is 5.192495603561401 and perplexity is 179.9169946252896
At time: 13.783385992050171 and batch: 600, loss is 5.101678037643433 and perplexity is 164.29737329121178
At time: 14.91574740409851 and batch: 650, loss is 4.986783962249756 and perplexity is 146.4646294711865
At time: 16.047735452651978 and batch: 700, loss is 5.06898832321167 and perplexity is 159.0133758002236
At time: 17.1822566986084 and batch: 750, loss is 5.058324499130249 and perplexity is 157.32669435489046
At time: 18.315877676010132 and batch: 800, loss is 5.00259485244751 and perplexity is 148.79876943630794
At time: 19.4480242729187 and batch: 850, loss is 5.045288248062134 and perplexity is 155.28905451676837
At time: 20.58622145652771 and batch: 900, loss is 4.95979793548584 and perplexity is 142.56498566176003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.96764133401113 and perplexity of 143.6875763681635
finished 1 epochs...
Completing Train Step...
At time: 22.887455224990845 and batch: 50, loss is 4.93386700630188 and perplexity is 138.91566276664886
At time: 23.795011043548584 and batch: 100, loss is 4.805644845962524 and perplexity is 122.19826467762005
At time: 24.708980321884155 and batch: 150, loss is 4.78948826789856 and perplexity is 120.23982233998106
At time: 25.618390560150146 and batch: 200, loss is 4.666188821792603 and perplexity is 106.29187223481455
At time: 26.526278018951416 and batch: 250, loss is 4.778812685012817 and perplexity is 118.96301958345745
At time: 27.4329354763031 and batch: 300, loss is 4.726459913253784 and perplexity is 112.89519531512249
At time: 28.345568895339966 and batch: 350, loss is 4.709566946029663 and perplexity is 111.00407873855912
At time: 29.253058671951294 and batch: 400, loss is 4.58891544342041 and perplexity is 98.38766528640062
At time: 30.16881561279297 and batch: 450, loss is 4.610492067337036 and perplexity is 100.53360677143773
At time: 31.077386379241943 and batch: 500, loss is 4.510951814651489 and perplexity is 91.00840042796008
At time: 31.985092639923096 and batch: 550, loss is 4.579248971939087 and perplexity is 97.44118565494112
At time: 32.8944993019104 and batch: 600, loss is 4.543867540359497 and perplexity is 94.05385468179244
At time: 33.80227756500244 and batch: 650, loss is 4.400245037078857 and perplexity is 81.47082959338009
At time: 34.7100989818573 and batch: 700, loss is 4.448783912658691 and perplexity is 85.5228774493584
At time: 35.6190881729126 and batch: 750, loss is 4.500122442245483 and perplexity is 90.02815387501082
At time: 36.52679753303528 and batch: 800, loss is 4.437720165252686 and perplexity is 84.58188896385491
At time: 37.43504810333252 and batch: 850, loss is 4.504471111297607 and perplexity is 90.42050901459714
At time: 38.34320616722107 and batch: 900, loss is 4.442059164047241 and perplexity is 84.94968703877053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.580391243712543 and perplexity of 97.55255356498763
finished 2 epochs...
Completing Train Step...
At time: 40.477829694747925 and batch: 50, loss is 4.493108644485473 and perplexity is 89.39892383663411
At time: 41.389227628707886 and batch: 100, loss is 4.35088321685791 and perplexity is 77.54692343813059
At time: 42.29504680633545 and batch: 150, loss is 4.351946153640747 and perplexity is 77.62939473857331
At time: 43.19869661331177 and batch: 200, loss is 4.243469896316529 and perplexity is 69.64910821342743
At time: 44.10834527015686 and batch: 250, loss is 4.386782398223877 and perplexity is 80.3813672113504
At time: 45.01005935668945 and batch: 300, loss is 4.355549559593201 and perplexity is 77.909629558342
At time: 45.91354966163635 and batch: 350, loss is 4.34615343093872 and perplexity is 77.18100912191838
At time: 46.81742024421692 and batch: 400, loss is 4.254843740463257 and perplexity is 70.44580849894706
At time: 47.72218918800354 and batch: 450, loss is 4.280965404510498 and perplexity is 72.31021492904166
At time: 48.62639594078064 and batch: 500, loss is 4.161836986541748 and perplexity is 64.18932929619137
At time: 49.53360462188721 and batch: 550, loss is 4.241416745185852 and perplexity is 69.50625476818708
At time: 50.43812084197998 and batch: 600, loss is 4.238405933380127 and perplexity is 69.2972992364984
At time: 51.34939217567444 and batch: 650, loss is 4.087060766220093 and perplexity is 59.56455999310986
At time: 52.255115270614624 and batch: 700, loss is 4.113078789710999 and perplexity is 61.13464884858955
At time: 53.161017656326294 and batch: 750, loss is 4.200083699226379 and perplexity is 66.69191286883742
At time: 54.07304120063782 and batch: 800, loss is 4.141777496337891 and perplexity is 62.914552505690935
At time: 54.97828960418701 and batch: 850, loss is 4.220595221519471 and perplexity is 68.0739913374507
At time: 55.8860650062561 and batch: 900, loss is 4.166964387893676 and perplexity is 64.51929897056631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.449143501177226 and perplexity of 85.55363602404498
finished 3 epochs...
Completing Train Step...
At time: 58.01805138587952 and batch: 50, loss is 4.243595004081726 and perplexity is 69.657822402799
At time: 58.93022632598877 and batch: 100, loss is 4.0981253719329835 and perplexity is 60.227277959274964
At time: 59.836310386657715 and batch: 150, loss is 4.1051583576202395 and perplexity is 60.65234854873067
At time: 60.74201941490173 and batch: 200, loss is 3.9976559114456176 and perplexity is 54.47031701904791
At time: 61.64633822441101 and batch: 250, loss is 4.148744430541992 and perplexity is 63.35440448424278
At time: 62.55327343940735 and batch: 300, loss is 4.121815857887268 and perplexity is 61.67112665235934
At time: 63.460225343704224 and batch: 350, loss is 4.112946667671204 and perplexity is 61.126572147648375
At time: 64.36559653282166 and batch: 400, loss is 4.035391054153442 and perplexity is 56.56503590605996
At time: 65.26964521408081 and batch: 450, loss is 4.062673020362854 and perplexity is 58.12948489198322
At time: 66.17522859573364 and batch: 500, loss is 3.9376144361495973 and perplexity is 51.29608513719369
At time: 67.08162331581116 and batch: 550, loss is 4.020497522354126 and perplexity is 55.72882526704279
At time: 67.98853182792664 and batch: 600, loss is 4.031891269683838 and perplexity is 56.36741648627305
At time: 68.89587616920471 and batch: 650, loss is 3.8743083381652834 and perplexity is 48.1493836801478
At time: 69.80323839187622 and batch: 700, loss is 3.8889739751815795 and perplexity is 48.860728476368536
At time: 70.7104287147522 and batch: 750, loss is 3.994377546310425 and perplexity is 54.292035825929375
At time: 71.61780643463135 and batch: 800, loss is 3.9376997089385988 and perplexity is 51.30045948394192
At time: 72.52391147613525 and batch: 850, loss is 4.0195734357833866 and perplexity is 55.67735079510754
At time: 73.43822240829468 and batch: 900, loss is 3.9714632129669187 and perplexity is 53.062115239654624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3930597174657535 and perplexity of 80.88753373675716
finished 4 epochs...
Completing Train Step...
At time: 75.57779383659363 and batch: 50, loss is 4.053701148033142 and perplexity is 57.610287145596295
At time: 76.48280835151672 and batch: 100, loss is 3.909407606124878 and perplexity is 49.86940089368733
At time: 77.38835000991821 and batch: 150, loss is 3.922265944480896 and perplexity is 50.51477887618361
At time: 78.29479575157166 and batch: 200, loss is 3.8147468328475953 and perplexity is 45.36526993280571
At time: 79.19981074333191 and batch: 250, loss is 3.966928553581238 and perplexity is 52.82204135878679
At time: 80.10605549812317 and batch: 300, loss is 3.942643084526062 and perplexity is 51.554684770811306
At time: 81.01204991340637 and batch: 350, loss is 3.9356471204757693 and perplexity is 51.195268746246384
At time: 81.91789817810059 and batch: 400, loss is 3.866191110610962 and perplexity is 47.76012615947777
At time: 82.82397770881653 and batch: 450, loss is 3.8886236333847046 and perplexity is 48.843613519174994
At time: 83.729008436203 and batch: 500, loss is 3.7625568962097167 and perplexity is 43.05838115844785
At time: 84.63473343849182 and batch: 550, loss is 3.8458765506744386 and perplexity is 46.799688675774696
At time: 85.54114818572998 and batch: 600, loss is 3.8680081462860105 and perplexity is 47.84698690319099
At time: 86.44651174545288 and batch: 650, loss is 3.708154726028442 and perplexity is 40.778489572746004
At time: 87.35909605026245 and batch: 700, loss is 3.71904435634613 and perplexity is 41.22497888870218
At time: 88.26715660095215 and batch: 750, loss is 3.8330549383163453 and perplexity is 46.20347160899084
At time: 89.17570495605469 and batch: 800, loss is 3.779820032119751 and perplexity is 43.808156964945915
At time: 90.08131670951843 and batch: 850, loss is 3.859677448272705 and perplexity is 47.45004380665588
At time: 90.9868733882904 and batch: 900, loss is 3.8125478792190552 and perplexity is 45.26562340707447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376453138377569 and perplexity of 79.55536054548901
finished 5 epochs...
Completing Train Step...
At time: 93.14254856109619 and batch: 50, loss is 3.9024772119522093 and perplexity is 49.524981149183716
At time: 94.05600333213806 and batch: 100, loss is 3.7605466175079347 and perplexity is 42.97190875780563
At time: 94.96431994438171 and batch: 150, loss is 3.7748737859725954 and perplexity is 43.59200604585328
At time: 95.88321113586426 and batch: 200, loss is 3.666065254211426 and perplexity is 39.09776304429797
At time: 96.79126048088074 and batch: 250, loss is 3.819512438774109 and perplexity is 45.58197889667704
At time: 97.69995546340942 and batch: 300, loss is 3.7925570678710936 and perplexity is 44.36971170598642
At time: 98.61419868469238 and batch: 350, loss is 3.7852276134490968 and perplexity is 44.04569481039305
At time: 99.52189517021179 and batch: 400, loss is 3.721056132316589 and perplexity is 41.30799779032905
At time: 100.42906284332275 and batch: 450, loss is 3.741814522743225 and perplexity is 42.17444727119651
At time: 101.33619523048401 and batch: 500, loss is 3.619517421722412 and perplexity is 37.31955386980539
At time: 102.24467635154724 and batch: 550, loss is 3.7004037570953368 and perplexity is 40.46363854348934
At time: 103.15202593803406 and batch: 600, loss is 3.7292708492279054 and perplexity is 41.64872888701388
At time: 104.06101155281067 and batch: 650, loss is 3.5705662822723387 and perplexity is 35.53671126443181
At time: 104.96972799301147 and batch: 700, loss is 3.5767528915405276 and perplexity is 35.75724448474092
At time: 105.88251900672913 and batch: 750, loss is 3.694114980697632 and perplexity is 40.20997023610456
At time: 106.7920470237732 and batch: 800, loss is 3.646322040557861 and perplexity is 38.33341769983047
At time: 107.7003824710846 and batch: 850, loss is 3.7240523290634155 and perplexity is 41.431950279219535
At time: 108.606281042099 and batch: 900, loss is 3.6777246952056886 and perplexity is 39.556288989199125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384647735177654 and perplexity of 80.20996308727192
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.7393946647644 and batch: 50, loss is 3.7922448682785035 and perplexity is 44.355861662169985
At time: 111.66094613075256 and batch: 100, loss is 3.6563544702529907 and perplexity is 38.71993060780565
At time: 112.5627064704895 and batch: 150, loss is 3.6768083810806274 and perplexity is 39.5200596041469
At time: 113.46829557418823 and batch: 200, loss is 3.553945903778076 and perplexity is 34.95095886983736
At time: 114.37745261192322 and batch: 250, loss is 3.6995954847335817 and perplexity is 40.43094611677091
At time: 115.28323745727539 and batch: 300, loss is 3.6583682918548583 and perplexity is 38.79798420711995
At time: 116.18908739089966 and batch: 350, loss is 3.6362139415740966 and perplexity is 37.94789147052465
At time: 117.09563159942627 and batch: 400, loss is 3.558840413093567 and perplexity is 35.122445993925794
At time: 118.0001232624054 and batch: 450, loss is 3.566087775230408 and perplexity is 35.377915701723694
At time: 118.91106653213501 and batch: 500, loss is 3.4393791913986207 and perplexity is 31.167603044962874
At time: 119.81750702857971 and batch: 550, loss is 3.498557333946228 and perplexity is 33.06771186508248
At time: 120.7240035533905 and batch: 600, loss is 3.5222538042068483 and perplexity is 33.86065782160284
At time: 121.63180899620056 and batch: 650, loss is 3.345195760726929 and perplexity is 28.366128097018354
At time: 122.53820943832397 and batch: 700, loss is 3.332293109893799 and perplexity is 28.002480902022647
At time: 123.44479060173035 and batch: 750, loss is 3.4301731634140014 and perplexity is 30.881989917502032
At time: 124.35009789466858 and batch: 800, loss is 3.3662508010864256 and perplexity is 28.96970999162227
At time: 125.25652503967285 and batch: 850, loss is 3.4220043277740477 and perplexity is 30.630747593369605
At time: 126.16945600509644 and batch: 900, loss is 3.3612404584884645 and perplexity is 28.824924834106582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355184894718536 and perplexity of 77.88122383264391
finished 7 epochs...
Completing Train Step...
At time: 128.31191492080688 and batch: 50, loss is 3.6788941478729247 and perplexity is 39.60257525638765
At time: 129.21883296966553 and batch: 100, loss is 3.5310126399993895 and perplexity is 34.15854041124289
At time: 130.12406063079834 and batch: 150, loss is 3.5502554273605345 and perplexity is 34.82221089716104
At time: 131.03159046173096 and batch: 200, loss is 3.4327824687957764 and perplexity is 30.962675681113396
At time: 131.93625473976135 and batch: 250, loss is 3.5790310287475586 and perplexity is 35.83879725274913
At time: 132.84174966812134 and batch: 300, loss is 3.542914514541626 and perplexity is 34.567520058177585
At time: 133.74800658226013 and batch: 350, loss is 3.5237091398239135 and perplexity is 33.90997231881598
At time: 134.6533555984497 and batch: 400, loss is 3.454042525291443 and perplexity is 31.62799117840684
At time: 135.55908823013306 and batch: 450, loss is 3.4646567821502687 and perplexity is 31.965486764539108
At time: 136.47037148475647 and batch: 500, loss is 3.3434743738174437 and perplexity is 28.31734101820582
At time: 137.38172030448914 and batch: 550, loss is 3.4062814617156985 and perplexity is 30.152910777555345
At time: 138.2888584136963 and batch: 600, loss is 3.4350440073013306 and perplexity is 31.032778204300378
At time: 139.1963667869568 and batch: 650, loss is 3.264956150054932 and perplexity is 26.178963019563554
At time: 140.10263442993164 and batch: 700, loss is 3.2572688961029055 and perplexity is 25.978490212545722
At time: 141.01429390907288 and batch: 750, loss is 3.3611763429641726 and perplexity is 28.823076768183657
At time: 141.92005348205566 and batch: 800, loss is 3.304478783607483 and perplexity is 27.2343429028682
At time: 142.826651096344 and batch: 850, loss is 3.3677343416213987 and perplexity is 29.012719626061003
At time: 143.73327374458313 and batch: 900, loss is 3.3147807216644285 and perplexity is 27.51635958145568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376091525979238 and perplexity of 79.5265975416049
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 145.86224007606506 and batch: 50, loss is 3.6234647703170775 and perplexity is 37.467158289717766
At time: 146.7703878879547 and batch: 100, loss is 3.4902608394622803 and perplexity is 32.79450069063136
At time: 147.6752049922943 and batch: 150, loss is 3.513572082519531 and perplexity is 33.56796140708206
At time: 148.58064889907837 and batch: 200, loss is 3.3922272968292235 and perplexity is 29.732100790645198
At time: 149.48601388931274 and batch: 250, loss is 3.540708608627319 and perplexity is 34.4913514025688
At time: 150.39123463630676 and batch: 300, loss is 3.5040568685531617 and perplexity is 33.250069873329096
At time: 151.2968180179596 and batch: 350, loss is 3.477513084411621 and perplexity is 32.379097781499524
At time: 152.20405459403992 and batch: 400, loss is 3.4054499292373657 and perplexity is 30.127848074597416
At time: 153.10972118377686 and batch: 450, loss is 3.4090986347198484 and perplexity is 30.23797650997681
At time: 154.01495242118835 and batch: 500, loss is 3.2869152498245238 and perplexity is 26.760187710404267
At time: 154.92154145240784 and batch: 550, loss is 3.340536046028137 and perplexity is 28.23425751123841
At time: 155.83330655097961 and batch: 600, loss is 3.36631760597229 and perplexity is 28.971645374437582
At time: 156.73808693885803 and batch: 650, loss is 3.1886604022979737 and perplexity is 24.255912504836637
At time: 157.65402269363403 and batch: 700, loss is 3.1803237915039064 and perplexity is 24.054540947239346
At time: 158.55994272232056 and batch: 750, loss is 3.272618794441223 and perplexity is 26.380333633980953
At time: 159.46559858322144 and batch: 800, loss is 3.204225387573242 and perplexity is 24.63640895479696
At time: 160.3709135055542 and batch: 850, loss is 3.2644893312454224 and perplexity is 26.166745039225965
At time: 161.27373838424683 and batch: 900, loss is 3.2109795093536375 and perplexity is 24.80336946220694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373451128397902 and perplexity of 79.31689267974251
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.42229533195496 and batch: 50, loss is 3.6033662271499636 and perplexity is 36.72164000314473
At time: 164.34157872200012 and batch: 100, loss is 3.4659185075759886 and perplexity is 32.00584388638493
At time: 165.25038385391235 and batch: 150, loss is 3.490680232048035 and perplexity is 32.808257345592445
At time: 166.15772771835327 and batch: 200, loss is 3.366306200027466 and perplexity is 28.971314927333513
At time: 167.0651354789734 and batch: 250, loss is 3.5172767353057863 and perplexity is 33.69254968450012
At time: 167.97942423820496 and batch: 300, loss is 3.482079553604126 and perplexity is 32.52729404330831
At time: 168.8895092010498 and batch: 350, loss is 3.456808395385742 and perplexity is 31.71559118262064
At time: 169.7973906993866 and batch: 400, loss is 3.3856096172332766 and perplexity is 29.53599287925035
At time: 170.71392178535461 and batch: 450, loss is 3.3885943603515627 and perplexity is 29.624281925254703
At time: 171.62137460708618 and batch: 500, loss is 3.265466642379761 and perplexity is 26.192330590988032
At time: 172.5298092365265 and batch: 550, loss is 3.3160758066177367 and perplexity is 27.552018690523155
At time: 173.4375741481781 and batch: 600, loss is 3.3420212411880494 and perplexity is 28.27622204888813
At time: 174.35260105133057 and batch: 650, loss is 3.161991739273071 and perplexity is 23.617589194300418
At time: 175.26110911369324 and batch: 700, loss is 3.153065409660339 and perplexity is 23.407708932284876
At time: 176.17031168937683 and batch: 750, loss is 3.244607925415039 and perplexity is 25.65165072804012
At time: 177.078688621521 and batch: 800, loss is 3.1709222984313965 and perplexity is 23.829452090752348
At time: 177.98701524734497 and batch: 850, loss is 3.231916656494141 and perplexity is 25.32815585266722
At time: 178.8956801891327 and batch: 900, loss is 3.1808863258361817 and perplexity is 24.068076259051153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37002061817744 and perplexity of 79.04526145198955
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 181.03407287597656 and batch: 50, loss is 3.5934033012390136 and perplexity is 36.35760148007892
At time: 181.938960313797 and batch: 100, loss is 3.455313119888306 and perplexity is 31.668203074206637
At time: 182.84397387504578 and batch: 150, loss is 3.4813163805007936 and perplexity is 32.50247955744877
At time: 183.74920344352722 and batch: 200, loss is 3.356070022583008 and perplexity is 28.676272038813504
At time: 184.6565113067627 and batch: 250, loss is 3.5067768621444704 and perplexity is 33.340632959967856
At time: 185.5632884502411 and batch: 300, loss is 3.4709395790100097 and perplexity is 32.16695164314628
At time: 186.47495889663696 and batch: 350, loss is 3.4460832405090334 and perplexity is 31.377254156887847
At time: 187.38084435462952 and batch: 400, loss is 3.376715941429138 and perplexity is 29.274473989926033
At time: 188.28645706176758 and batch: 450, loss is 3.3806775856018065 and perplexity is 29.390679068820088
At time: 189.1927149295807 and batch: 500, loss is 3.256971683502197 and perplexity is 25.970770225202763
At time: 190.09761452674866 and batch: 550, loss is 3.3067707872390746 and perplexity is 27.296835705212864
At time: 191.00366258621216 and batch: 600, loss is 3.333768939971924 and perplexity is 28.043838316351355
At time: 191.90836095809937 and batch: 650, loss is 3.15378098487854 and perplexity is 23.424464903075414
At time: 192.8147838115692 and batch: 700, loss is 3.143956732749939 and perplexity is 23.195463778136357
At time: 193.7203860282898 and batch: 750, loss is 3.2358408164978028 and perplexity is 25.427742858708513
At time: 194.62496304512024 and batch: 800, loss is 3.1618789768218996 and perplexity is 23.61492616719966
At time: 195.53126192092896 and batch: 850, loss is 3.2210681343078615 and perplexity is 25.054867857677095
At time: 196.43536233901978 and batch: 900, loss is 3.169849715232849 and perplexity is 23.80390672302435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368731773062928 and perplexity of 78.94344997664653
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 198.55886816978455 and batch: 50, loss is 3.5900541973114013 and perplexity is 36.23603976918981
At time: 199.46715712547302 and batch: 100, loss is 3.4522250843048097 and perplexity is 31.57056137436218
At time: 200.3696050643921 and batch: 150, loss is 3.4794249677658082 and perplexity is 32.44106205492822
At time: 201.2725646495819 and batch: 200, loss is 3.353819603919983 and perplexity is 28.611810980420433
At time: 202.1764588356018 and batch: 250, loss is 3.5040870332717895 and perplexity is 33.25107286745862
At time: 203.083336353302 and batch: 300, loss is 3.4677579164505006 and perplexity is 32.064769897565434
At time: 203.9967029094696 and batch: 350, loss is 3.4421706199645996 and perplexity is 31.25472672562833
At time: 204.9023470878601 and batch: 400, loss is 3.3737200784683226 and perplexity is 29.186902918564098
At time: 205.81407046318054 and batch: 450, loss is 3.3783062219619753 and perplexity is 29.321065653109574
At time: 206.7215051651001 and batch: 500, loss is 3.2543885326385498 and perplexity is 25.90377038027523
At time: 207.62779903411865 and batch: 550, loss is 3.303944511413574 and perplexity is 27.219796237021384
At time: 208.53156399726868 and batch: 600, loss is 3.3315145587921142 and perplexity is 27.98068802420179
At time: 209.44149899482727 and batch: 650, loss is 3.151228370666504 and perplexity is 23.364747531186307
At time: 210.34650039672852 and batch: 700, loss is 3.141005444526672 and perplexity is 23.127108197180597
At time: 211.25324153900146 and batch: 750, loss is 3.2331892108917235 and perplexity is 25.360407825622808
At time: 212.15859937667847 and batch: 800, loss is 3.159385094642639 and perplexity is 23.55610669856442
At time: 213.0653326511383 and batch: 850, loss is 3.2182475328445435 and perplexity is 24.984297632761155
At time: 213.97118639945984 and batch: 900, loss is 3.1667250108718874 and perplexity is 23.729642638957486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368111388324058 and perplexity of 78.89448985367416
Annealing...
Model not improving. Stopping early with 77.88122383264391 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb19ab91b70>
ELAPSED
1252.1945114135742


RESULTS SO FAR:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.4201923222733, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.80542344064872, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2343070501481508, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.08771921670537419, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -76.90000073966566, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.562999421480413, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.8319819592781227, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -77.88122383264391, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.01804147960545366, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.6597197598977973, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2793966501442918, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5662880211767414, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.347506046295166 and batch: 50, loss is 6.971767129898072 and perplexity is 1066.105033007765
At time: 2.478478193283081 and batch: 100, loss is 6.065670108795166 and perplexity is 430.8112713888201
At time: 3.61153507232666 and batch: 150, loss is 5.83216326713562 and perplexity is 341.0957627835469
At time: 4.74355936050415 and batch: 200, loss is 5.59950894355774 and perplexity is 270.2936453961216
At time: 5.87684965133667 and batch: 250, loss is 5.631091384887696 and perplexity is 278.9664112775362
At time: 7.009284734725952 and batch: 300, loss is 5.517189960479737 and perplexity is 248.93453754633677
At time: 8.142791509628296 and batch: 350, loss is 5.468526210784912 and perplexity is 237.11048425167633
At time: 9.276313543319702 and batch: 400, loss is 5.304192667007446 and perplexity is 201.17851877771918
At time: 10.411272525787354 and batch: 450, loss is 5.30508059501648 and perplexity is 201.35723014903232
At time: 11.545756578445435 and batch: 500, loss is 5.237986450195312 and perplexity is 188.29058797164862
At time: 12.679255723953247 and batch: 550, loss is 5.290423517227173 and perplexity is 198.42744505756687
At time: 13.81508207321167 and batch: 600, loss is 5.201371030807495 and perplexity is 181.52094215221922
At time: 14.948797464370728 and batch: 650, loss is 5.091499090194702 and perplexity is 162.63348166167225
At time: 16.081428289413452 and batch: 700, loss is 5.175278606414795 and perplexity is 176.84587780953336
At time: 17.216296434402466 and batch: 750, loss is 5.159539127349854 and perplexity is 174.08420646463225
At time: 18.353464365005493 and batch: 800, loss is 5.114413480758667 and perplexity is 166.40315370451245
At time: 19.485565185546875 and batch: 850, loss is 5.161275472640991 and perplexity is 174.38673933153916
At time: 20.629762887954712 and batch: 900, loss is 5.073544006347657 and perplexity is 159.73944296497424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0338548634150255 and perplexity of 153.5236863228333
finished 1 epochs...
Completing Train Step...
At time: 22.929978370666504 and batch: 50, loss is 4.99363073348999 and perplexity is 147.47088013708284
At time: 23.837024927139282 and batch: 100, loss is 4.851766414642334 and perplexity is 127.96623174756819
At time: 24.745853185653687 and batch: 150, loss is 4.832548570632935 and perplexity is 125.53047661356251
At time: 25.662168741226196 and batch: 200, loss is 4.709912986755371 and perplexity is 111.04249731733539
At time: 26.571540117263794 and batch: 250, loss is 4.81268277168274 and perplexity is 123.06132048646549
At time: 27.48008418083191 and batch: 300, loss is 4.7613060188293455 and perplexity is 116.89849788572047
At time: 28.401628255844116 and batch: 350, loss is 4.736601963043213 and perplexity is 114.0460099521743
At time: 29.31292414665222 and batch: 400, loss is 4.614983377456665 and perplexity is 100.98614987187666
At time: 30.232208490371704 and batch: 450, loss is 4.630769424438476 and perplexity is 102.59297128937067
At time: 31.15363049507141 and batch: 500, loss is 4.531341075897217 and perplexity is 92.88304080235949
At time: 32.06560444831848 and batch: 550, loss is 4.599456367492675 and perplexity is 99.43024743193179
At time: 32.973376750946045 and batch: 600, loss is 4.56031943321228 and perplexity is 95.61401724514194
At time: 33.881351470947266 and batch: 650, loss is 4.4153915309906 and perplexity is 82.71421974742914
At time: 34.7904736995697 and batch: 700, loss is 4.462330656051636 and perplexity is 86.68931681363355
At time: 35.69739890098572 and batch: 750, loss is 4.516049699783325 and perplexity is 91.47353539434508
At time: 36.60328245162964 and batch: 800, loss is 4.450483875274658 and perplexity is 85.6683867890091
At time: 37.511746406555176 and batch: 850, loss is 4.516112403869629 and perplexity is 91.47927133863465
At time: 38.41795063018799 and batch: 900, loss is 4.453754463195801 and perplexity is 85.94903146644575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.579908815148759 and perplexity of 97.50550277691238
finished 2 epochs...
Completing Train Step...
At time: 40.56084132194519 and batch: 50, loss is 4.494666557312012 and perplexity is 89.53830811284229
At time: 41.472564697265625 and batch: 100, loss is 4.349709496498108 and perplexity is 77.45595842943557
At time: 42.378716230392456 and batch: 150, loss is 4.359365968704224 and perplexity is 78.2075326769336
At time: 43.28913497924805 and batch: 200, loss is 4.249729065895081 and perplexity is 70.08642097236539
At time: 44.19568681716919 and batch: 250, loss is 4.392919759750367 and perplexity is 80.87621369451419
At time: 45.10123586654663 and batch: 300, loss is 4.356969089508056 and perplexity is 78.02030314180209
At time: 46.01848292350769 and batch: 350, loss is 4.348497171401977 and perplexity is 77.36211352394761
At time: 46.925233364105225 and batch: 400, loss is 4.25482964515686 and perplexity is 70.44481555068987
At time: 47.831547021865845 and batch: 450, loss is 4.281105003356934 and perplexity is 72.32031005624891
At time: 48.7381854057312 and batch: 500, loss is 4.159286699295044 and perplexity is 64.02583663355158
At time: 49.64428925514221 and batch: 550, loss is 4.240000195503235 and perplexity is 69.4078654082161
At time: 50.5522882938385 and batch: 600, loss is 4.2421743154525755 and perplexity is 69.55893059044986
At time: 51.46320176124573 and batch: 650, loss is 4.085696349143982 and perplexity is 59.48334450882048
At time: 52.36989092826843 and batch: 700, loss is 4.111449337005615 and perplexity is 61.03511394536226
At time: 53.276572704315186 and batch: 750, loss is 4.201376066207886 and perplexity is 66.77815901379353
At time: 54.18364858627319 and batch: 800, loss is 4.138336162567139 and perplexity is 62.69841464604514
At time: 55.0881142616272 and batch: 850, loss is 4.218208527565002 and perplexity is 67.91171328492466
At time: 55.992883920669556 and batch: 900, loss is 4.16408992767334 and perplexity is 64.33410710307216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428949643487799 and perplexity of 83.84330528235074
finished 3 epochs...
Completing Train Step...
At time: 58.136470556259155 and batch: 50, loss is 4.231910939216614 and perplexity is 68.8486721745925
At time: 59.0500271320343 and batch: 100, loss is 4.08723548412323 and perplexity is 59.574967897330275
At time: 59.96537137031555 and batch: 150, loss is 4.101309642791748 and perplexity is 60.41936358941501
At time: 60.8725266456604 and batch: 200, loss is 3.990389919281006 and perplexity is 54.07597051662897
At time: 61.779040813446045 and batch: 250, loss is 4.14435359954834 and perplexity is 63.07683582592908
At time: 62.68517255783081 and batch: 300, loss is 4.115066118240357 and perplexity is 61.25626428526568
At time: 63.591689586639404 and batch: 350, loss is 4.107089252471924 and perplexity is 60.76957499583575
At time: 64.497483253479 and batch: 400, loss is 4.024686679840088 and perplexity is 55.96277176983789
At time: 65.40269613265991 and batch: 450, loss is 4.050196485519409 and perplexity is 57.40873592263828
At time: 66.3310317993164 and batch: 500, loss is 3.9263095569610598 and perplexity is 50.71945460225595
At time: 67.23686337471008 and batch: 550, loss is 4.012651553153992 and perplexity is 55.29328945498995
At time: 68.14830541610718 and batch: 600, loss is 4.0276179599761965 and perplexity is 56.127054993531246
At time: 69.05574154853821 and batch: 650, loss is 3.863353576660156 and perplexity is 47.62479727096447
At time: 69.95963430404663 and batch: 700, loss is 3.885589632987976 and perplexity is 48.695646555797886
At time: 70.86452460289001 and batch: 750, loss is 3.9866801261901856 and perplexity is 53.87573150712411
At time: 71.77035450935364 and batch: 800, loss is 3.930591459274292 and perplexity is 50.93709597926896
At time: 72.68354797363281 and batch: 850, loss is 4.013795299530029 and perplexity is 55.3565671343334
At time: 73.58945918083191 and batch: 900, loss is 3.9630821323394776 and perplexity is 52.619255786254975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367575867535317 and perplexity of 78.85225152500136
finished 4 epochs...
Completing Train Step...
At time: 75.72995638847351 and batch: 50, loss is 4.03953450679779 and perplexity is 56.79989668490884
At time: 76.63387894630432 and batch: 100, loss is 3.895897192955017 and perplexity is 49.20017561758576
At time: 77.54047393798828 and batch: 150, loss is 3.909128398895264 and perplexity is 49.85547894006677
At time: 78.44624996185303 and batch: 200, loss is 3.8006659078598024 and perplexity is 44.73096127659407
At time: 79.36018991470337 and batch: 250, loss is 3.95595844745636 and perplexity is 52.2457447562308
At time: 80.26748871803284 and batch: 300, loss is 3.9333406782150266 and perplexity is 51.077325881348756
At time: 81.17237687110901 and batch: 350, loss is 3.9242710638046265 and perplexity is 50.616168650770526
At time: 82.07876420021057 and batch: 400, loss is 3.8468635511398315 and perplexity is 46.84590279320394
At time: 82.98930644989014 and batch: 450, loss is 3.8727871131896974 and perplexity is 48.07619331875165
At time: 83.89427900314331 and batch: 500, loss is 3.752204723358154 and perplexity is 42.61493264209784
At time: 84.79755687713623 and batch: 550, loss is 3.8350490665435792 and perplexity is 46.29569918216375
At time: 85.7045624256134 and batch: 600, loss is 3.859279890060425 and perplexity is 47.4311834013704
At time: 86.6102340221405 and batch: 650, loss is 3.694648485183716 and perplexity is 40.23142815905094
At time: 87.51507568359375 and batch: 700, loss is 3.7166848611831664 and perplexity is 41.12782341443061
At time: 88.45272564888 and batch: 750, loss is 3.820113172531128 and perplexity is 45.60936975659537
At time: 89.3586835861206 and batch: 800, loss is 3.768945255279541 and perplexity is 43.334334062878085
At time: 90.2633306980133 and batch: 850, loss is 3.8464338684082033 and perplexity is 46.82577824162195
At time: 91.16828417778015 and batch: 900, loss is 3.801235809326172 and perplexity is 44.756460782430445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3613084766962755 and perplexity of 78.35959908148378
finished 5 epochs...
Completing Train Step...
At time: 93.30415916442871 and batch: 50, loss is 3.8841086435317993 and perplexity is 48.623582193139406
At time: 94.21807670593262 and batch: 100, loss is 3.742497272491455 and perplexity is 42.20325169644078
At time: 95.13416934013367 and batch: 150, loss is 3.7547144222259523 and perplexity is 42.72201760960486
At time: 96.04235601425171 and batch: 200, loss is 3.6481130790710448 and perplexity is 38.402135847327564
At time: 96.9477436542511 and batch: 250, loss is 3.805027551651001 and perplexity is 44.926487894877056
At time: 97.85720229148865 and batch: 300, loss is 3.783790512084961 and perplexity is 43.982442143429815
At time: 98.77431297302246 and batch: 350, loss is 3.775050497055054 and perplexity is 43.59970991708785
At time: 99.6816418170929 and batch: 400, loss is 3.6990170907974242 and perplexity is 40.40756786427535
At time: 100.58907008171082 and batch: 450, loss is 3.7301905393600463 and perplexity is 41.687050431259394
At time: 101.49732375144958 and batch: 500, loss is 3.610914134979248 and perplexity is 36.99986022665988
At time: 102.41098594665527 and batch: 550, loss is 3.690160698890686 and perplexity is 40.051282636859874
At time: 103.31978154182434 and batch: 600, loss is 3.7190273332595827 and perplexity is 41.22427711829181
At time: 104.22824549674988 and batch: 650, loss is 3.560110158920288 and perplexity is 35.167070898298306
At time: 105.13753724098206 and batch: 700, loss is 3.572268524169922 and perplexity is 35.597254858550315
At time: 106.0457968711853 and batch: 750, loss is 3.677591519355774 and perplexity is 39.5510213975593
At time: 106.95204496383667 and batch: 800, loss is 3.631153130531311 and perplexity is 37.756329500640334
At time: 107.86018705368042 and batch: 850, loss is 3.711882667541504 and perplexity is 40.930793109816314
At time: 108.77542233467102 and batch: 900, loss is 3.6663702058792116 and perplexity is 39.10968779048823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371607114190924 and perplexity of 79.17076597405566
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.91473913192749 and batch: 50, loss is 3.7716585493087766 and perplexity is 43.45207251003245
At time: 111.82577013969421 and batch: 100, loss is 3.6402360153198243 and perplexity is 38.10082804342912
At time: 112.7319827079773 and batch: 150, loss is 3.653582954406738 and perplexity is 38.612766278974846
At time: 113.63988876342773 and batch: 200, loss is 3.5312525749206545 and perplexity is 34.166737221258145
At time: 114.54737496376038 and batch: 250, loss is 3.6810847520828247 and perplexity is 39.68942391525566
At time: 115.4586136341095 and batch: 300, loss is 3.6446128368377684 and perplexity is 38.267954040995654
At time: 116.36532258987427 and batch: 350, loss is 3.6253503513336183 and perplexity is 37.537872299678504
At time: 117.27268266677856 and batch: 400, loss is 3.5427452421188352 and perplexity is 34.561669225513754
At time: 118.1792562007904 and batch: 450, loss is 3.5585342979431154 and perplexity is 35.11169612651872
At time: 119.08517646789551 and batch: 500, loss is 3.429706974029541 and perplexity is 30.867596416940746
At time: 119.99102020263672 and batch: 550, loss is 3.492084431648254 and perplexity is 32.854359047861784
At time: 120.89647698402405 and batch: 600, loss is 3.5064144706726075 and perplexity is 33.32855278792452
At time: 121.80071592330933 and batch: 650, loss is 3.3319949102401734 and perplexity is 27.994131816826034
At time: 122.7168288230896 and batch: 700, loss is 3.3259783029556274 and perplexity is 27.826207792430978
At time: 123.63250589370728 and batch: 750, loss is 3.417428135871887 and perplexity is 30.49089565299364
At time: 124.54544496536255 and batch: 800, loss is 3.3462845849990845 and perplexity is 28.397030646467403
At time: 125.45401620864868 and batch: 850, loss is 3.4043411684036253 and perplexity is 30.094462008648847
At time: 126.36136674880981 and batch: 900, loss is 3.349811592102051 and perplexity is 28.497364009492287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351661159567637 and perplexity of 77.60727397346692
finished 7 epochs...
Completing Train Step...
At time: 128.50858521461487 and batch: 50, loss is 3.6534469175338744 and perplexity is 38.607513876265955
At time: 129.41575646400452 and batch: 100, loss is 3.5146251201629637 and perplexity is 33.60332835215109
At time: 130.32148480415344 and batch: 150, loss is 3.525810284614563 and perplexity is 33.98129698599077
At time: 131.22625279426575 and batch: 200, loss is 3.4100470209121703 and perplexity is 30.266667392250454
At time: 132.14094972610474 and batch: 250, loss is 3.5601156330108643 and perplexity is 35.167263406556614
At time: 133.06015634536743 and batch: 300, loss is 3.528568458557129 and perplexity is 34.075152689549306
At time: 133.9732415676117 and batch: 350, loss is 3.5137082862854006 and perplexity is 33.57253380121984
At time: 134.87969088554382 and batch: 400, loss is 3.434688286781311 and perplexity is 31.021741171465806
At time: 135.78555607795715 and batch: 450, loss is 3.4562672710418703 and perplexity is 31.69843374672322
At time: 136.6924786567688 and batch: 500, loss is 3.3340042448043823 and perplexity is 28.0504379434595
At time: 137.5982723236084 and batch: 550, loss is 3.3990529918670656 and perplexity is 29.935737232983733
At time: 138.50469326972961 and batch: 600, loss is 3.419736180305481 and perplexity is 30.561351271052537
At time: 139.41123580932617 and batch: 650, loss is 3.2523167181015014 and perplexity is 25.85015812873523
At time: 140.3178288936615 and batch: 700, loss is 3.2520975780487063 and perplexity is 25.844493944365638
At time: 141.22317838668823 and batch: 750, loss is 3.3501774978637697 and perplexity is 28.507793267123528
At time: 142.13101148605347 and batch: 800, loss is 3.2843700361251833 and perplexity is 26.692163918312726
At time: 143.03736448287964 and batch: 850, loss is 3.3499285364151 and perplexity is 28.500696809022273
At time: 143.94412422180176 and batch: 900, loss is 3.3040502500534057 and perplexity is 27.222674573424985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374127949753853 and perplexity of 79.37059421771968
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 146.07605814933777 and batch: 50, loss is 3.5963328886032104 and perplexity is 36.464270421668836
At time: 146.98645997047424 and batch: 100, loss is 3.4713627099990845 and perplexity is 32.18056535719959
At time: 147.89968609809875 and batch: 150, loss is 3.4924064445495606 and perplexity is 32.864940278894366
At time: 148.8067364692688 and batch: 200, loss is 3.371822257041931 and perplexity is 29.131563917216138
At time: 149.71150493621826 and batch: 250, loss is 3.5202473640441894 and perplexity is 33.79278655040743
At time: 150.61623120307922 and batch: 300, loss is 3.4879526853561402 and perplexity is 32.71889321961785
At time: 151.52213382720947 and batch: 350, loss is 3.4627942180633546 and perplexity is 31.90600440891652
At time: 152.428142786026 and batch: 400, loss is 3.3841313982009886 and perplexity is 29.4923644665582
At time: 153.33372855186462 and batch: 450, loss is 3.399825067520142 and perplexity is 29.958858811513565
At time: 154.24048352241516 and batch: 500, loss is 3.2750857210159303 and perplexity is 26.44549231787187
At time: 155.15526056289673 and batch: 550, loss is 3.328473105430603 and perplexity is 27.8957153522654
At time: 156.06521248817444 and batch: 600, loss is 3.3512541818618775 and perplexity is 28.538503681694827
At time: 156.98227953910828 and batch: 650, loss is 3.173924446105957 and perplexity is 23.901099118599443
At time: 157.88532638549805 and batch: 700, loss is 3.1685925674438478 and perplexity is 23.774000496529865
At time: 158.79150986671448 and batch: 750, loss is 3.260069880485535 and perplexity is 26.051357560426922
At time: 159.69680333137512 and batch: 800, loss is 3.1833653926849363 and perplexity is 24.127816648632038
At time: 160.60158443450928 and batch: 850, loss is 3.241184868812561 and perplexity is 25.56399378857573
At time: 161.50883841514587 and batch: 900, loss is 3.1990948724746704 and perplexity is 24.510335174939993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371677764474529 and perplexity of 79.17635960871854
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.64856028556824 and batch: 50, loss is 3.577288007736206 and perplexity is 35.77638388582258
At time: 164.56249117851257 and batch: 100, loss is 3.4481365299224853 and perplexity is 31.441746929176222
At time: 165.47039675712585 and batch: 150, loss is 3.470082335472107 and perplexity is 32.13938854754689
At time: 166.3784294128418 and batch: 200, loss is 3.3478068208694456 and perplexity is 28.44029054265857
At time: 167.28578448295593 and batch: 250, loss is 3.498295240402222 and perplexity is 33.05904616694896
At time: 168.19269132614136 and batch: 300, loss is 3.468439373970032 and perplexity is 32.08662812300433
At time: 169.09847283363342 and batch: 350, loss is 3.4406796407699587 and perplexity is 31.208161301012193
At time: 170.00608944892883 and batch: 400, loss is 3.3628114223480225 and perplexity is 28.870243336820174
At time: 170.92077112197876 and batch: 450, loss is 3.3756490087509157 and perplexity is 29.24325675329468
At time: 171.83254599571228 and batch: 500, loss is 3.250678424835205 and perplexity is 25.80784266077791
At time: 172.73919773101807 and batch: 550, loss is 3.301851944923401 and perplexity is 27.16289655749065
At time: 173.64656901359558 and batch: 600, loss is 3.326368932723999 and perplexity is 27.837079660835066
At time: 174.5546109676361 and batch: 650, loss is 3.1451989603042603 and perplexity is 23.224295726589602
At time: 175.46291494369507 and batch: 700, loss is 3.1396084451675415 and perplexity is 23.094822198860868
At time: 176.37269353866577 and batch: 750, loss is 3.2310103464126585 and perplexity is 25.30521108877857
At time: 177.2829873561859 and batch: 800, loss is 3.1507642316818236 and perplexity is 23.35390555727589
At time: 178.19248509407043 and batch: 850, loss is 3.2074871301651 and perplexity is 24.716897774741547
At time: 179.10555148124695 and batch: 900, loss is 3.1663574028015136 and perplexity is 23.720921033980723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366895701787243 and perplexity of 78.79863715974992
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 181.2552616596222 and batch: 50, loss is 3.568817057609558 and perplexity is 35.47460390834972
At time: 182.16211795806885 and batch: 100, loss is 3.437916669845581 and perplexity is 31.122053070781693
At time: 183.07036685943604 and batch: 150, loss is 3.4591821527481077 and perplexity is 31.7909657257376
At time: 183.97774362564087 and batch: 200, loss is 3.337717514038086 and perplexity is 28.154790396138715
At time: 184.88474106788635 and batch: 250, loss is 3.488974370956421 and perplexity is 32.75233872416333
At time: 185.79125905036926 and batch: 300, loss is 3.4575434494018555 and perplexity is 31.738912425429383
At time: 186.69645762443542 and batch: 350, loss is 3.4306293392181395 and perplexity is 30.8960807477896
At time: 187.60097861289978 and batch: 400, loss is 3.353240213394165 and perplexity is 28.595238369682317
At time: 188.50671648979187 and batch: 450, loss is 3.3661077308654783 and perplexity is 28.965565585290626
At time: 189.41288590431213 and batch: 500, loss is 3.2420599460601807 and perplexity is 25.586374048698456
At time: 190.31845426559448 and batch: 550, loss is 3.292089037895203 and perplexity is 26.898998028292507
At time: 191.224041223526 and batch: 600, loss is 3.3173248672485354 and perplexity is 27.586454333994727
At time: 192.1291298866272 and batch: 650, loss is 3.136020007133484 and perplexity is 23.012096377505458
At time: 193.03717231750488 and batch: 700, loss is 3.1304195022583006 and perplexity is 22.88357724150265
At time: 193.94410872459412 and batch: 750, loss is 3.22234347820282 and perplexity is 25.0868418150002
At time: 194.85020875930786 and batch: 800, loss is 3.141112594604492 and perplexity is 23.129586401391187
At time: 195.75533843040466 and batch: 850, loss is 3.1970723485946655 and perplexity is 24.460812533985173
At time: 196.66244196891785 and batch: 900, loss is 3.1555181074142458 and perplexity is 23.465191432189815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366349311724101 and perplexity of 78.75559412763009
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 198.79244375228882 and batch: 50, loss is 3.5661787939071656 and perplexity is 35.38113589934415
At time: 199.70479774475098 and batch: 100, loss is 3.434822473526001 and perplexity is 31.025904157230727
At time: 200.61153864860535 and batch: 150, loss is 3.455903434753418 and perplexity is 31.686902804053954
At time: 201.5234489440918 and batch: 200, loss is 3.334810743331909 and perplexity is 28.07306970537199
At time: 202.42987418174744 and batch: 250, loss is 3.4860124492645266 and perplexity is 32.655472387704734
At time: 203.33743357658386 and batch: 300, loss is 3.4546294593811036 and perplexity is 31.646560173471965
At time: 204.24413585662842 and batch: 350, loss is 3.427316837310791 and perplexity is 30.793906740606573
At time: 205.15016293525696 and batch: 400, loss is 3.3502449321746828 and perplexity is 28.50971573533758
At time: 206.05714416503906 and batch: 450, loss is 3.3636853075027466 and perplexity is 28.895483640842702
At time: 206.96341753005981 and batch: 500, loss is 3.239560241699219 and perplexity is 25.522495549850976
At time: 207.87015581130981 and batch: 550, loss is 3.2891420412063597 and perplexity is 26.819843461579655
At time: 208.7765293121338 and batch: 600, loss is 3.314677424430847 and perplexity is 27.51351736443179
At time: 209.68142318725586 and batch: 650, loss is 3.133401355743408 and perplexity is 22.951914551339897
At time: 210.58818006515503 and batch: 700, loss is 3.127659993171692 and perplexity is 22.820516850048715
At time: 211.49498438835144 and batch: 750, loss is 3.219810094833374 and perplexity is 25.023367663282585
At time: 212.40231490135193 and batch: 800, loss is 3.1386714458465574 and perplexity is 23.0731925012308
At time: 213.3089039325714 and batch: 850, loss is 3.1944185590744016 and perplexity is 24.39598474372689
At time: 214.21653079986572 and batch: 900, loss is 3.1523696327209474 and perplexity is 23.391428052792854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365892384150257 and perplexity of 78.71961674523367
Annealing...
Model not improving. Stopping early with 77.60727397346692 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb19ab91b70>
ELAPSED
1473.4915244579315


RESULTS SO FAR:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.4201923222733, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.80542344064872, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2343070501481508, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.08771921670537419, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -76.90000073966566, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.562999421480413, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.8319819592781227, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -77.88122383264391, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.01804147960545366, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.6597197598977973, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -77.60727397346692, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2793966501442918, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5662880211767414, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -76.65689709099628, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2615595918734831, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5660413221619177, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.4201923222733, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.04020629548167309, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.9634513575363356, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -78.80542344064872, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2343070501481508, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.08771921670537419, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -76.90000073966566, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.562999421480413, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.8319819592781227, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -77.88122383264391, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.01804147960545366, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.6597197598977973, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}, {'best_accuracy': -77.60727397346692, 'params': {'data': 'ptb', 'wordvec_dim': 300, 'dropout': 0.2793966501442918, 'seq_len': 35, 'num_layers': 3, 'rnn_dropout': 0.5662880211767414, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'batch_size': 32, 'tie_weights': 'FALSE'}}]
Exception ignored in: <bound method DropoutDescriptor.__del__ of <torch.backends.cudnn.DropoutDescriptor object at 0x7fb1924dd780>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 215, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroyDropoutDescriptor'
Exception ignored in: <bound method CuDNNHandle.__del__ of <torch.backends.cudnn.CuDNNHandle object at 0x7fb19306e5c0>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 91, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroy'
