FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'rnn_dropout', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2135329246520996 and batch: 50, loss is 7.120991954803467 and perplexity is 1237.6775449312943
At time: 2.0062978267669678 and batch: 100, loss is 6.366926145553589 and perplexity is 582.2652765708792
At time: 2.7877893447875977 and batch: 150, loss is 6.183650531768799 and perplexity is 484.75835602093565
At time: 3.5703649520874023 and batch: 200, loss is 6.046712350845337 and perplexity is 422.7209847321961
At time: 4.352803945541382 and batch: 250, loss is 6.1053822803497315 and perplexity is 448.2639712129826
At time: 5.135659217834473 and batch: 300, loss is 6.00736249923706 and perplexity is 406.4099987711403
At time: 5.917337656021118 and batch: 350, loss is 6.010719480514527 and perplexity is 407.77660207545244
At time: 6.6998069286346436 and batch: 400, loss is 5.884463253021241 and perplexity is 359.40980408835907
At time: 7.483232259750366 and batch: 450, loss is 5.88774845123291 and perplexity is 360.59247812965936
At time: 8.263118743896484 and batch: 500, loss is 5.847636661529541 and perplexity is 346.41471706163657
At time: 9.044352769851685 and batch: 550, loss is 5.8852170467376705 and perplexity is 359.6808270751834
At time: 9.82689356803894 and batch: 600, loss is 5.823039083480835 and perplexity is 337.9976975511041
At time: 10.610034704208374 and batch: 650, loss is 5.742549457550049 and perplexity is 311.85846827009914
At time: 11.392828226089478 and batch: 700, loss is 5.84067102432251 and perplexity is 344.0101023788316
At time: 12.174659967422485 and batch: 750, loss is 5.79457498550415 and perplexity is 328.5125317188867
At time: 12.956979274749756 and batch: 800, loss is 5.8016260051727295 and perplexity is 330.8370655776847
At time: 13.739795684814453 and batch: 850, loss is 5.844486312866211 and perplexity is 345.3251071475224
At time: 14.521447896957397 and batch: 900, loss is 5.716959829330444 and perplexity is 303.9793675891539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.649641063115368 and perplexity of 284.18944144210275
finished 1 epochs...
Completing Train Step...
At time: 16.19133234024048 and batch: 50, loss is 5.486086883544922 and perplexity is 241.31107855784413
At time: 16.815204620361328 and batch: 100, loss is 5.274316310882568 and perplexity is 195.25693582546847
At time: 17.42944860458374 and batch: 150, loss is 5.187171354293823 and perplexity is 178.96161728886216
At time: 18.043664932250977 and batch: 200, loss is 5.019547901153564 and perplexity is 151.34286638967163
At time: 18.65621328353882 and batch: 250, loss is 5.062878618240356 and perplexity is 158.0448128175414
At time: 19.268301248550415 and batch: 300, loss is 4.980183038711548 and perplexity is 145.50101153406627
At time: 19.881710529327393 and batch: 350, loss is 4.947742681503296 and perplexity is 140.85664647124472
At time: 20.49510431289673 and batch: 400, loss is 4.785808219909668 and perplexity is 119.79814721487999
At time: 21.11001443862915 and batch: 450, loss is 4.789998579025268 and perplexity is 120.30119771813187
At time: 21.722336292266846 and batch: 500, loss is 4.68880841255188 and perplexity is 108.72354897324527
At time: 22.335166215896606 and batch: 550, loss is 4.739452867507935 and perplexity is 114.3716081352585
At time: 22.948578357696533 and batch: 600, loss is 4.6790565013885494 and perplexity is 107.66843961039163
At time: 23.561408281326294 and batch: 650, loss is 4.544500894546509 and perplexity is 94.11344295270827
At time: 24.177002668380737 and batch: 700, loss is 4.5968496608734135 and perplexity is 99.17139946472179
At time: 24.79164171218872 and batch: 750, loss is 4.617696084976196 and perplexity is 101.26046766375065
At time: 25.42274761199951 and batch: 800, loss is 4.563547344207763 and perplexity is 95.92314943992064
At time: 26.036362648010254 and batch: 850, loss is 4.616939363479614 and perplexity is 101.1838706760642
At time: 26.65040636062622 and batch: 900, loss is 4.539069414138794 and perplexity is 93.60365334062685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.634154437339469 and perplexity of 102.94083825565676
finished 2 epochs...
Completing Train Step...
At time: 28.204362869262695 and batch: 50, loss is 4.590973901748657 and perplexity is 98.59040078510733
At time: 28.82775568962097 and batch: 100, loss is 4.459666471481324 and perplexity is 86.45866785549201
At time: 29.442724227905273 and batch: 150, loss is 4.451181936264038 and perplexity is 85.72820942534472
At time: 30.05661964416504 and batch: 200, loss is 4.348196120262146 and perplexity is 77.33882707687306
At time: 30.670243978500366 and batch: 250, loss is 4.476344766616822 and perplexity is 87.91274307687635
At time: 31.285035848617554 and batch: 300, loss is 4.4397411012649535 and perplexity is 84.75299638959075
At time: 31.898563385009766 and batch: 350, loss is 4.4273314285278325 and perplexity is 83.70773850912319
At time: 32.51283812522888 and batch: 400, loss is 4.323219132423401 and perplexity is 75.43106044447364
At time: 33.12731719017029 and batch: 450, loss is 4.352668313980103 and perplexity is 77.68547585596382
At time: 33.741464614868164 and batch: 500, loss is 4.23727560043335 and perplexity is 69.2190144683101
At time: 34.35626983642578 and batch: 550, loss is 4.3113424110412595 and perplexity is 74.54048577802847
At time: 34.970428466796875 and batch: 600, loss is 4.3008679676055905 and perplexity is 73.76379050262393
At time: 35.58369779586792 and batch: 650, loss is 4.149524297714233 and perplexity is 63.4038317753727
At time: 36.20525145530701 and batch: 700, loss is 4.178757019042969 and perplexity is 65.28465517748566
At time: 36.81965351104736 and batch: 750, loss is 4.255911273956299 and perplexity is 70.52105191425127
At time: 37.4342679977417 and batch: 800, loss is 4.21115481376648 and perplexity is 67.43436900001231
At time: 38.04880452156067 and batch: 850, loss is 4.285377507209778 and perplexity is 72.62995987859632
At time: 38.66473412513733 and batch: 900, loss is 4.223838667869568 and perplexity is 68.29514413092492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430476776541096 and perplexity of 83.97144298190037
finished 3 epochs...
Completing Train Step...
At time: 40.23425817489624 and batch: 50, loss is 4.304308042526245 and perplexity is 74.01798043391227
At time: 40.84983968734741 and batch: 100, loss is 4.1762973403930665 and perplexity is 65.12427322986503
At time: 41.46555829048157 and batch: 150, loss is 4.17569739818573 and perplexity is 65.08521414739994
At time: 42.08048629760742 and batch: 200, loss is 4.071417818069458 and perplexity is 58.64004459521282
At time: 42.693479776382446 and batch: 250, loss is 4.218096308708191 and perplexity is 67.90409273768827
At time: 43.35357594490051 and batch: 300, loss is 4.190464329719544 and perplexity is 66.0534544228476
At time: 43.96901488304138 and batch: 350, loss is 4.185375938415527 and perplexity is 65.71820227018569
At time: 44.58394169807434 and batch: 400, loss is 4.097451643943787 and perplexity is 60.186714822195384
At time: 45.19615459442139 and batch: 450, loss is 4.130747442245483 and perplexity is 62.22441471389408
At time: 45.81008577346802 and batch: 500, loss is 4.0096051168441775 and perplexity is 55.125098292216485
At time: 46.425503730773926 and batch: 550, loss is 4.084277095794678 and perplexity is 59.39898245262469
At time: 47.0406539440155 and batch: 600, loss is 4.095151491165161 and perplexity is 60.04843527586847
At time: 47.65558624267578 and batch: 650, loss is 3.9381825494766236 and perplexity is 51.325235406328574
At time: 48.270875215530396 and batch: 700, loss is 3.9551190996170043 and perplexity is 52.201910801795044
At time: 48.8853862285614 and batch: 750, loss is 4.056572322845459 and perplexity is 57.775934037776835
At time: 49.52612543106079 and batch: 800, loss is 4.010933051109314 and perplexity is 55.1983494246878
At time: 50.166285276412964 and batch: 850, loss is 4.09036274433136 and perplexity is 59.76156594208275
At time: 50.82765221595764 and batch: 900, loss is 4.037407460212708 and perplexity is 56.67920905823679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355441576813998 and perplexity of 77.90121711422337
finished 4 epochs...
Completing Train Step...
At time: 52.38208198547363 and batch: 50, loss is 4.128516583442688 and perplexity is 62.085755552591884
At time: 52.99722766876221 and batch: 100, loss is 4.000024862289429 and perplexity is 54.59950748502726
At time: 53.61078238487244 and batch: 150, loss is 4.001198654174805 and perplexity is 54.663633571836186
At time: 54.2254433631897 and batch: 200, loss is 3.9001166915893553 and perplexity is 49.408214292202196
At time: 54.84065580368042 and batch: 250, loss is 4.051674566268921 and perplexity is 57.49365341205243
At time: 55.48219347000122 and batch: 300, loss is 4.02979202747345 and perplexity is 56.24921173989996
At time: 56.12966752052307 and batch: 350, loss is 4.022813010215759 and perplexity is 55.85801419546915
At time: 56.76098585128784 and batch: 400, loss is 3.9454170513153075 and perplexity is 51.69789429159103
At time: 57.40636587142944 and batch: 450, loss is 3.9835086727142333 and perplexity is 53.705137789173854
At time: 58.05565643310547 and batch: 500, loss is 3.8571821594238282 and perplexity is 47.33178984172511
At time: 58.67127180099487 and batch: 550, loss is 3.933791184425354 and perplexity is 51.100341717864595
At time: 59.286200761795044 and batch: 600, loss is 3.9569561004638674 and perplexity is 52.29789388966084
At time: 59.8996319770813 and batch: 650, loss is 3.7958304643630982 and perplexity is 44.51518933771985
At time: 60.51343846321106 and batch: 700, loss is 3.8095105504989624 and perplexity is 45.128345413799686
At time: 61.12823939323425 and batch: 750, loss is 3.9154639291763305 and perplexity is 50.17234252611071
At time: 61.7447452545166 and batch: 800, loss is 3.8744274854660032 and perplexity is 48.15512089102442
At time: 62.36167120933533 and batch: 850, loss is 3.9525233125686645 and perplexity is 52.066581476880245
At time: 62.97626328468323 and batch: 900, loss is 3.907309150695801 and perplexity is 49.76486190220584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31639078218643 and perplexity of 74.91774528735019
finished 5 epochs...
Completing Train Step...
At time: 64.52683544158936 and batch: 50, loss is 4.001275157928466 and perplexity is 54.667815704965626
At time: 65.14993929862976 and batch: 100, loss is 3.8751521635055544 and perplexity is 48.190030497210806
At time: 65.76453852653503 and batch: 150, loss is 3.8735773181915283 and perplexity is 48.114198381098696
At time: 66.37876772880554 and batch: 200, loss is 3.7746495389938355 and perplexity is 43.58223176616672
At time: 66.99235582351685 and batch: 250, loss is 3.927406916618347 and perplexity is 50.77514263488799
At time: 67.60724806785583 and batch: 300, loss is 3.9053009033203123 and perplexity is 49.665022034030784
At time: 68.22089838981628 and batch: 350, loss is 3.901272850036621 and perplexity is 49.46537105128857
At time: 68.83451509475708 and batch: 400, loss is 3.828248744010925 and perplexity is 45.98194153158739
At time: 69.44942951202393 and batch: 450, loss is 3.8704160261154175 and perplexity is 47.96233551491935
At time: 70.06530666351318 and batch: 500, loss is 3.740719223022461 and perplexity is 42.12827889959487
At time: 70.67826890945435 and batch: 550, loss is 3.8181718444824218 and perplexity is 45.520912897474936
At time: 71.30215191841125 and batch: 600, loss is 3.8467103958129885 and perplexity is 46.83872864304407
At time: 71.91629528999329 and batch: 650, loss is 3.686770315170288 and perplexity is 39.915723349079144
At time: 72.53063917160034 and batch: 700, loss is 3.6974152326583862 and perplexity is 40.34289248703242
At time: 73.14635133743286 and batch: 750, loss is 3.80716148853302 and perplexity is 45.022460547748224
At time: 73.76130795478821 and batch: 800, loss is 3.768728404045105 and perplexity is 43.324937977856216
At time: 74.37606501579285 and batch: 850, loss is 3.845962071418762 and perplexity is 46.803691191130966
At time: 74.99013996124268 and batch: 900, loss is 3.8013190984725953 and perplexity is 44.76018866508987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311204466101241 and perplexity of 74.53020400436452
finished 6 epochs...
Completing Train Step...
At time: 76.55196404457092 and batch: 50, loss is 3.8982951402664185 and perplexity is 49.31829661378993
At time: 77.16644477844238 and batch: 100, loss is 3.775065312385559 and perplexity is 43.60035586598516
At time: 77.77974700927734 and batch: 150, loss is 3.772645173072815 and perplexity is 43.494964513022936
At time: 78.39504599571228 and batch: 200, loss is 3.678211226463318 and perplexity is 39.57553904272484
At time: 79.01039290428162 and batch: 250, loss is 3.8269952487945558 and perplexity is 45.92433949731764
At time: 79.62484359741211 and batch: 300, loss is 3.809524750709534 and perplexity is 45.12898625035729
At time: 80.23978328704834 and batch: 350, loss is 3.8032730293273924 and perplexity is 44.84773247821554
At time: 80.85500144958496 and batch: 400, loss is 3.7345892906188967 and perplexity is 41.87082529044651
At time: 81.46906423568726 and batch: 450, loss is 3.7773696422576903 and perplexity is 43.700941314961575
At time: 82.08372235298157 and batch: 500, loss is 3.6503013372421265 and perplexity is 38.486261645795906
At time: 82.69809007644653 and batch: 550, loss is 3.7259564638137816 and perplexity is 41.51091745373274
At time: 83.31128287315369 and batch: 600, loss is 3.7592563676834105 and perplexity is 42.91650001331961
At time: 83.92537832260132 and batch: 650, loss is 3.5975802040100096 and perplexity is 36.50978124523871
At time: 84.54046893119812 and batch: 700, loss is 3.604025082588196 and perplexity is 36.745842227369636
At time: 85.15468168258667 and batch: 750, loss is 3.7187894344329835 and perplexity is 41.214471077607
At time: 85.76907730102539 and batch: 800, loss is 3.6816933441162107 and perplexity is 39.71358593412049
At time: 86.38356494903564 and batch: 850, loss is 3.7538675832748414 and perplexity is 42.68585425545262
At time: 87.00656127929688 and batch: 900, loss is 3.7159060430526734 and perplexity is 41.09580478984883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312355773089683 and perplexity of 74.61606056322923
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.56053471565247 and batch: 50, loss is 3.8339267587661743 and perplexity is 46.24377030445357
At time: 89.17503666877747 and batch: 100, loss is 3.7081559228897096 and perplexity is 40.77853837896993
At time: 89.79083514213562 and batch: 150, loss is 3.7058492851257325 and perplexity is 40.68458546171567
At time: 90.40582227706909 and batch: 200, loss is 3.5968283939361574 and perplexity is 36.482343139319
At time: 91.02080774307251 and batch: 250, loss is 3.740266695022583 and perplexity is 42.10921898670353
At time: 91.6352686882019 and batch: 300, loss is 3.707976846694946 and perplexity is 40.77123656729884
At time: 92.24917769432068 and batch: 350, loss is 3.692692174911499 and perplexity is 40.152799938552
At time: 92.86409258842468 and batch: 400, loss is 3.615490322113037 and perplexity is 37.169566518697756
At time: 93.47803401947021 and batch: 450, loss is 3.642780213356018 and perplexity is 38.197887512214464
At time: 94.09379839897156 and batch: 500, loss is 3.511827688217163 and perplexity is 33.50945668894531
At time: 94.7077169418335 and batch: 550, loss is 3.5743758201599123 and perplexity is 35.67234790481726
At time: 95.32178902626038 and batch: 600, loss is 3.597438168525696 and perplexity is 36.50459592903553
At time: 95.94183778762817 and batch: 650, loss is 3.430313324928284 and perplexity is 30.886318687329304
At time: 96.55624389648438 and batch: 700, loss is 3.420037260055542 and perplexity is 30.57055406037199
At time: 97.17108678817749 and batch: 750, loss is 3.5141313076019287 and perplexity is 33.58673870294391
At time: 97.78436398506165 and batch: 800, loss is 3.4648297739028933 and perplexity is 31.971017008447504
At time: 98.39911270141602 and batch: 850, loss is 3.52334933757782 and perplexity is 33.89777362930025
At time: 99.0131607055664 and batch: 900, loss is 3.4763820123672486 and perplexity is 32.34249539304823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265626672196062 and perplexity of 71.2095310268707
finished 8 epochs...
Completing Train Step...
At time: 100.56387591362 and batch: 50, loss is 3.7414085149765013 and perplexity is 42.15732759364333
At time: 101.18779110908508 and batch: 100, loss is 3.612120342254639 and perplexity is 37.04451665429332
At time: 101.80240869522095 and batch: 150, loss is 3.612921280860901 and perplexity is 37.07419892308278
At time: 102.42580652236938 and batch: 200, loss is 3.5085284757614135 and perplexity is 33.3990840435703
At time: 103.0518102645874 and batch: 250, loss is 3.651816415786743 and perplexity is 38.544615549289105
At time: 103.70421767234802 and batch: 300, loss is 3.626234016418457 and perplexity is 37.57105786709303
At time: 104.31860327720642 and batch: 350, loss is 3.6114428758621218 and perplexity is 37.0194287383025
At time: 104.93334436416626 and batch: 400, loss is 3.5395773124694823 and perplexity is 34.452353532478
At time: 105.54667353630066 and batch: 450, loss is 3.5727487325668337 and perplexity is 35.6143530642628
At time: 106.16205883026123 and batch: 500, loss is 3.446287627220154 and perplexity is 31.38366790608919
At time: 106.77710556983948 and batch: 550, loss is 3.5110918140411376 and perplexity is 33.48480701575659
At time: 107.39021587371826 and batch: 600, loss is 3.541076831817627 and perplexity is 34.50405425661948
At time: 108.00521469116211 and batch: 650, loss is 3.377729363441467 and perplexity is 29.3041564241425
At time: 108.62000226974487 and batch: 700, loss is 3.3728636932373046 and perplexity is 29.161918385684928
At time: 109.23529839515686 and batch: 750, loss is 3.4725453233718873 and perplexity is 32.218645036466235
At time: 109.84998917579651 and batch: 800, loss is 3.429382743835449 and perplexity is 30.857589832469408
At time: 110.46516847610474 and batch: 850, loss is 3.493596067428589 and perplexity is 32.904060428266824
At time: 111.08043909072876 and batch: 900, loss is 3.4538400650024412 and perplexity is 31.621588414346917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273009417808219 and perplexity of 71.73719829952047
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 112.62008213996887 and batch: 50, loss is 3.7107392501831056 and perplexity is 40.884018876812576
At time: 113.24624752998352 and batch: 100, loss is 3.593529505729675 and perplexity is 36.362190262211755
At time: 113.85988330841064 and batch: 150, loss is 3.6000931644439698 and perplexity is 36.601644256674405
At time: 114.47441339492798 and batch: 200, loss is 3.487845230102539 and perplexity is 32.71537759153911
At time: 115.08800959587097 and batch: 250, loss is 3.629558033943176 and perplexity is 37.696152515084734
At time: 115.73861837387085 and batch: 300, loss is 3.6001005029678343 and perplexity is 36.601912859699844
At time: 116.3973593711853 and batch: 350, loss is 3.5835991144180297 and perplexity is 36.00288645033953
At time: 117.05140829086304 and batch: 400, loss is 3.505892443656921 and perplexity is 33.31115892340409
At time: 117.7091474533081 and batch: 450, loss is 3.536614875793457 and perplexity is 34.35044164508628
At time: 118.3420901298523 and batch: 500, loss is 3.4058336782455445 and perplexity is 30.139411825061583
At time: 118.95796632766724 and batch: 550, loss is 3.4652565717697144 and perplexity is 31.9846650825841
At time: 119.57251620292664 and batch: 600, loss is 3.4978316020965576 and perplexity is 33.04372227944236
At time: 120.18745541572571 and batch: 650, loss is 3.326966881752014 and perplexity is 27.853729793030134
At time: 120.80236911773682 and batch: 700, loss is 3.317930636405945 and perplexity is 27.603170419725956
At time: 121.4167811870575 and batch: 750, loss is 3.410827441215515 and perplexity is 30.290297333441558
At time: 122.03098106384277 and batch: 800, loss is 3.3629763889312745 and perplexity is 28.875006355079723
At time: 122.64346694946289 and batch: 850, loss is 3.420445203781128 and perplexity is 30.58302767018104
At time: 123.2559609413147 and batch: 900, loss is 3.387384648323059 and perplexity is 29.588466742470324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261222525818707 and perplexity of 70.89660342405807
finished 10 epochs...
Completing Train Step...
At time: 124.80975890159607 and batch: 50, loss is 3.6870042276382446 and perplexity is 39.9250612265184
At time: 125.42470169067383 and batch: 100, loss is 3.5588929986953737 and perplexity is 35.124292977447254
At time: 126.03979778289795 and batch: 150, loss is 3.5625139331817626 and perplexity is 35.251706279597826
At time: 126.65536594390869 and batch: 200, loss is 3.452983846664429 and perplexity is 31.594525018210874
At time: 127.26987767219543 and batch: 250, loss is 3.596516637802124 and perplexity is 36.47097131777107
At time: 127.88538670539856 and batch: 300, loss is 3.569786338806152 and perplexity is 35.509005444562426
At time: 128.49991130828857 and batch: 350, loss is 3.5539029121398924 and perplexity is 34.94945630315859
At time: 129.11277437210083 and batch: 400, loss is 3.4789638471603395 and perplexity is 32.426106261228725
At time: 129.7259430885315 and batch: 450, loss is 3.511613898277283 and perplexity is 33.50229346995373
At time: 130.33963632583618 and batch: 500, loss is 3.3835440731048583 and perplexity is 29.47504794647396
At time: 130.95569777488708 and batch: 550, loss is 3.445359568595886 and perplexity is 31.354555533511537
At time: 131.5716998577118 and batch: 600, loss is 3.480329518318176 and perplexity is 32.47041991136028
At time: 132.1862382888794 and batch: 650, loss is 3.3119671726226807 and perplexity is 27.43904976280506
At time: 132.80073714256287 and batch: 700, loss is 3.3057332849502563 and perplexity is 27.26852986191887
At time: 133.42320942878723 and batch: 750, loss is 3.4020344686508177 and perplexity is 30.025123123312678
At time: 134.03609251976013 and batch: 800, loss is 3.3571195125579836 and perplexity is 28.706383296805832
At time: 134.64827132225037 and batch: 850, loss is 3.4178088235855104 and perplexity is 30.502505372044592
At time: 135.26043796539307 and batch: 900, loss is 3.38771915435791 and perplexity is 29.598365918732462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262818636959547 and perplexity of 71.00985263775421
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 136.79224562644958 and batch: 50, loss is 3.6782677793502807 and perplexity is 39.57777721699782
At time: 137.41445994377136 and batch: 100, loss is 3.556470308303833 and perplexity is 35.0393006869153
At time: 138.02669477462769 and batch: 150, loss is 3.5623929357528685 and perplexity is 35.2474411718126
At time: 138.6408805847168 and batch: 200, loss is 3.452092089653015 and perplexity is 31.566362937736237
At time: 139.26125574111938 and batch: 250, loss is 3.5961516952514647 and perplexity is 36.45766393683581
At time: 139.87526893615723 and batch: 300, loss is 3.56735924243927 and perplexity is 35.42292617005983
At time: 140.48870968818665 and batch: 350, loss is 3.5483995962142942 and perplexity is 34.757646682268124
At time: 141.10266208648682 and batch: 400, loss is 3.473341159820557 and perplexity is 32.24429601415064
At time: 141.71588349342346 and batch: 450, loss is 3.5058059978485105 and perplexity is 33.30827943780339
At time: 142.33076763153076 and batch: 500, loss is 3.3748249006271362 and perplexity is 29.219167075451512
At time: 142.94422364234924 and batch: 550, loss is 3.4316933822631834 and perplexity is 30.92897300391323
At time: 143.56012845039368 and batch: 600, loss is 3.4689327383041384 and perplexity is 32.10246242664111
At time: 144.1742832660675 and batch: 650, loss is 3.295426483154297 and perplexity is 26.98892193632828
At time: 144.78773093223572 and batch: 700, loss is 3.2884078788757325 and perplexity is 26.800160568900107
At time: 145.4028444290161 and batch: 750, loss is 3.383777675628662 and perplexity is 29.481934196354857
At time: 146.0184030532837 and batch: 800, loss is 3.336505079269409 and perplexity is 28.120675234740638
At time: 146.63238167762756 and batch: 850, loss is 3.39098464012146 and perplexity is 29.6951769427469
At time: 147.24727368354797 and batch: 900, loss is 3.365169506072998 and perplexity is 28.938402118242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260890594900471 and perplexity of 70.87307455457938
finished 12 epochs...
Completing Train Step...
At time: 148.83935809135437 and batch: 50, loss is 3.6706716871261595 and perplexity is 39.2782797166464
At time: 149.49023866653442 and batch: 100, loss is 3.543589267730713 and perplexity is 34.59085247350124
At time: 150.10604071617126 and batch: 150, loss is 3.54836669921875 and perplexity is 34.75650327892747
At time: 150.7264482975006 and batch: 200, loss is 3.438519697189331 and perplexity is 31.140826179557077
At time: 151.35792207717896 and batch: 250, loss is 3.5825408840179445 and perplexity is 35.96480725324254
At time: 152.00526595115662 and batch: 300, loss is 3.55600332736969 and perplexity is 35.02294182148521
At time: 152.62103366851807 and batch: 350, loss is 3.5371936655044554 and perplexity is 34.37032908204546
At time: 153.23702096939087 and batch: 400, loss is 3.463393449783325 and perplexity is 31.925129228342804
At time: 153.85105633735657 and batch: 450, loss is 3.4965302419662474 and perplexity is 33.00074846498006
At time: 154.4655590057373 and batch: 500, loss is 3.366682515144348 and perplexity is 28.982219322712346
At time: 155.08092403411865 and batch: 550, loss is 3.4253736543655395 and perplexity is 30.734126646850488
At time: 155.69641852378845 and batch: 600, loss is 3.463885564804077 and perplexity is 31.940843930378836
At time: 156.3173005580902 and batch: 650, loss is 3.291848726272583 and perplexity is 26.892534663071473
At time: 156.93030619621277 and batch: 700, loss is 3.2859823179244994 and perplexity is 26.735233919542253
At time: 157.54334688186646 and batch: 750, loss is 3.3824682569503786 and perplexity is 29.443355264545385
At time: 158.15721678733826 and batch: 800, loss is 3.3363770914077757 and perplexity is 28.117076359960713
At time: 158.77268290519714 and batch: 850, loss is 3.3927015256881714 and perplexity is 29.746203954676652
At time: 159.3878529071808 and batch: 900, loss is 3.3682955646514894 and perplexity is 29.029006802422447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260705817235659 and perplexity of 70.85998000319273
finished 13 epochs...
Completing Train Step...
At time: 160.9439513683319 and batch: 50, loss is 3.6651155138015747 and perplexity is 39.060647946443986
At time: 161.5580608844757 and batch: 100, loss is 3.537311940193176 and perplexity is 34.374394462429926
At time: 162.17272639274597 and batch: 150, loss is 3.5415844202041624 and perplexity is 34.52157255951595
At time: 162.78625679016113 and batch: 200, loss is 3.4318065214157105 and perplexity is 30.932472479667556
At time: 163.40007138252258 and batch: 250, loss is 3.5757316064834597 and perplexity is 35.72074478673731
At time: 164.01489520072937 and batch: 300, loss is 3.5495718908309937 and perplexity is 34.79841677697505
At time: 164.63750410079956 and batch: 350, loss is 3.530978813171387 and perplexity is 34.15738495571438
At time: 165.25237655639648 and batch: 400, loss is 3.4576206159591676 and perplexity is 31.741361702534007
At time: 165.8682689666748 and batch: 450, loss is 3.491084237098694 and perplexity is 32.82151472510721
At time: 166.4834496974945 and batch: 500, loss is 3.361847410202026 and perplexity is 28.84242548211482
At time: 167.09802079200745 and batch: 550, loss is 3.4212508249282836 and perplexity is 30.607675931263255
At time: 167.71277451515198 and batch: 600, loss is 3.4603934478759766 and perplexity is 31.829497299473957
At time: 168.32926487922668 and batch: 650, loss is 3.2890005016326906 and perplexity is 26.816047661004568
At time: 168.9452519416809 and batch: 700, loss is 3.2837678480148313 and perplexity is 26.676095053286875
At time: 169.5594482421875 and batch: 750, loss is 3.3809746313095093 and perplexity is 29.399410740672568
At time: 170.17368936538696 and batch: 800, loss is 3.3354667139053347 and perplexity is 28.091490854209678
At time: 170.7879605293274 and batch: 850, loss is 3.392486686706543 and perplexity is 29.739813996942306
At time: 171.40410327911377 and batch: 900, loss is 3.3685635805130003 and perplexity is 29.03678807939565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261134317476455 and perplexity of 70.89035002800365
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 172.9469497203827 and batch: 50, loss is 3.6628475427627563 and perplexity is 38.97215991022528
At time: 173.570654630661 and batch: 100, loss is 3.5371782207489013 and perplexity is 34.369798244813815
At time: 174.18548607826233 and batch: 150, loss is 3.5422098350524904 and perplexity is 34.543169616438384
At time: 174.7996847629547 and batch: 200, loss is 3.43216534614563 and perplexity is 30.94357380734715
At time: 175.41320848464966 and batch: 250, loss is 3.5769398736953737 and perplexity is 35.763931076483225
At time: 176.06183528900146 and batch: 300, loss is 3.5499313163757322 and perplexity is 34.81092646489718
At time: 176.67618036270142 and batch: 350, loss is 3.5299536895751955 and perplexity is 34.12238735590962
At time: 177.29043340682983 and batch: 400, loss is 3.456495137214661 and perplexity is 31.705657570505824
At time: 177.90585732460022 and batch: 450, loss is 3.4895642518997194 and perplexity is 32.77166440398991
At time: 178.51942658424377 and batch: 500, loss is 3.3591586923599244 and perplexity is 28.764980498621473
At time: 179.13327074050903 and batch: 550, loss is 3.4160983562469482 and perplexity is 30.45037642799895
At time: 179.74709010124207 and batch: 600, loss is 3.455758285522461 and perplexity is 31.68230380825897
At time: 180.36988282203674 and batch: 650, loss is 3.2825771808624267 and perplexity is 26.644351604843376
At time: 180.9844889640808 and batch: 700, loss is 3.2760515069961547 and perplexity is 26.47104534098214
At time: 181.5976278781891 and batch: 750, loss is 3.373543872833252 and perplexity is 29.181760474876672
At time: 182.21204733848572 and batch: 800, loss is 3.3278729152679443 and perplexity is 27.87897764173769
At time: 182.8272979259491 and batch: 850, loss is 3.383060255050659 and perplexity is 29.460790835331068
At time: 183.44367694854736 and batch: 900, loss is 3.3584416723251342 and perplexity is 28.74436282382989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259566633668665 and perplexity of 70.77930343984788
finished 15 epochs...
Completing Train Step...
At time: 184.98592877388 and batch: 50, loss is 3.6596247720718384 and perplexity is 38.84676374557991
At time: 185.61079239845276 and batch: 100, loss is 3.533370862007141 and perplexity is 34.2391888891258
At time: 186.2259624004364 and batch: 150, loss is 3.5385756492614746 and perplexity is 34.41786115527178
At time: 186.84035086631775 and batch: 200, loss is 3.4287372398376466 and perplexity is 30.837677562286412
At time: 187.45521783828735 and batch: 250, loss is 3.5733298873901367 and perplexity is 35.635056532702485
At time: 188.07063245773315 and batch: 300, loss is 3.5468799448013306 and perplexity is 34.704867288673285
At time: 188.68556761741638 and batch: 350, loss is 3.5268127059936525 and perplexity is 34.01537764331464
At time: 189.3004584312439 and batch: 400, loss is 3.4537609243392944 and perplexity is 31.619085959894306
At time: 189.91488432884216 and batch: 450, loss is 3.4871129608154297 and perplexity is 32.69142989446428
At time: 190.52898144721985 and batch: 500, loss is 3.357097511291504 and perplexity is 28.70575172696494
At time: 191.14397883415222 and batch: 550, loss is 3.414487442970276 and perplexity is 30.40136300112078
At time: 191.75829148292542 and batch: 600, loss is 3.4545313215255735 and perplexity is 31.643454600311276
At time: 192.37240934371948 and batch: 650, loss is 3.2820105600357055 and perplexity is 26.629258636708443
At time: 192.98748517036438 and batch: 700, loss is 3.276014332771301 and perplexity is 26.470061318680774
At time: 193.60201597213745 and batch: 750, loss is 3.3736351203918455 and perplexity is 29.184423360764534
At time: 194.21876859664917 and batch: 800, loss is 3.3283779573440553 and perplexity is 27.893061254594908
At time: 194.83731245994568 and batch: 850, loss is 3.3841501569747923 and perplexity is 29.492917712341253
At time: 195.46571326255798 and batch: 900, loss is 3.3601583862304687 and perplexity is 28.793751051789023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259216726642766 and perplexity of 70.75454159671014
finished 16 epochs...
Completing Train Step...
At time: 197.0258972644806 and batch: 50, loss is 3.6577534532546996 and perplexity is 38.774137040651524
At time: 197.6451916694641 and batch: 100, loss is 3.5311734867095947 and perplexity is 34.16403514198628
At time: 198.26328492164612 and batch: 150, loss is 3.5362783098220825 and perplexity is 34.33888240066013
At time: 198.88113451004028 and batch: 200, loss is 3.4265313577651977 and perplexity is 30.769728253829452
At time: 199.49992895126343 and batch: 250, loss is 3.5710175752639772 and perplexity is 35.55275235251439
At time: 200.11842370033264 and batch: 300, loss is 3.544832305908203 and perplexity is 34.63387695871666
At time: 200.7368927001953 and batch: 350, loss is 3.5248115921020506 and perplexity is 33.947377059732865
At time: 201.35838508605957 and batch: 400, loss is 3.4519063901901244 and perplexity is 31.56050162533141
At time: 201.97665762901306 and batch: 450, loss is 3.485432062149048 and perplexity is 32.636525071202094
At time: 202.59708499908447 and batch: 500, loss is 3.3556601285934446 and perplexity is 28.66452021592704
At time: 203.21610140800476 and batch: 550, loss is 3.4133768224716188 and perplexity is 30.367617366949858
At time: 203.83548951148987 and batch: 600, loss is 3.453678021430969 and perplexity is 31.616464754363925
At time: 204.45526337623596 and batch: 650, loss is 3.281510157585144 and perplexity is 26.615936623891916
At time: 205.07388043403625 and batch: 700, loss is 3.2758121156692503 and perplexity is 26.46470916075697
At time: 205.6929693222046 and batch: 750, loss is 3.373576073646545 and perplexity is 29.182700166426624
At time: 206.3126242160797 and batch: 800, loss is 3.3285510206222533 and perplexity is 27.897888936949823
At time: 206.95055055618286 and batch: 850, loss is 3.3846354818344118 and perplexity is 29.507234832435895
At time: 207.60576915740967 and batch: 900, loss is 3.3609180355072024 and perplexity is 28.815632514016183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259141895869007 and perplexity of 70.74924717771071
finished 17 epochs...
Completing Train Step...
At time: 209.15838193893433 and batch: 50, loss is 3.6561856842041016 and perplexity is 38.7133957752149
At time: 209.78460097312927 and batch: 100, loss is 3.5294358587265013 and perplexity is 34.10472230525446
At time: 210.40361881256104 and batch: 150, loss is 3.5344665670394897 and perplexity is 34.27672550146723
At time: 211.0310287475586 and batch: 200, loss is 3.424774212837219 and perplexity is 30.715708855747636
At time: 211.64815425872803 and batch: 250, loss is 3.569198307991028 and perplexity is 35.488131193099655
At time: 212.2654309272766 and batch: 300, loss is 3.543142328262329 and perplexity is 34.575395910617075
At time: 212.88337540626526 and batch: 350, loss is 3.5231883764266967 and perplexity is 33.8923178437329
At time: 213.50060987472534 and batch: 400, loss is 3.4503871774673462 and perplexity is 31.512590912222258
At time: 214.11861276626587 and batch: 450, loss is 3.4840381002426146 and perplexity is 32.591062692268295
At time: 214.73699355125427 and batch: 500, loss is 3.354445848464966 and perplexity is 28.629734582662294
At time: 215.3542823791504 and batch: 550, loss is 3.412402558326721 and perplexity is 30.338045693832772
At time: 215.98360323905945 and batch: 600, loss is 3.4529069662094116 and perplexity is 31.592096110120394
At time: 216.63927483558655 and batch: 650, loss is 3.2809646368026733 and perplexity is 26.60142103695581
At time: 217.26027250289917 and batch: 700, loss is 3.2754736948013305 and perplexity is 26.455754466226757
At time: 217.87948393821716 and batch: 750, loss is 3.373373351097107 and perplexity is 29.1767847746613
At time: 218.49908757209778 and batch: 800, loss is 3.3285142087936403 and perplexity is 27.896861983545747
At time: 219.11962842941284 and batch: 850, loss is 3.38480881690979 and perplexity is 29.51234991450854
At time: 219.7392361164093 and batch: 900, loss is 3.36125186920166 and perplexity is 28.825253748933324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259185372966609 and perplexity of 70.75232321650367
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 221.29562401771545 and batch: 50, loss is 3.6554284620285036 and perplexity is 38.68409222948871
At time: 221.92424702644348 and batch: 100, loss is 3.529174304008484 and perplexity is 34.09580322069402
At time: 222.54339599609375 and batch: 150, loss is 3.5345800733566284 and perplexity is 34.280616347156084
At time: 223.20098447799683 and batch: 200, loss is 3.4247384119033812 and perplexity is 30.714609224371138
At time: 223.83530521392822 and batch: 250, loss is 3.569306139945984 and perplexity is 35.49195815399457
At time: 224.45870065689087 and batch: 300, loss is 3.543216609954834 and perplexity is 34.577964324936254
At time: 225.07920455932617 and batch: 350, loss is 3.5228865909576417 and perplexity is 33.882091177902254
At time: 225.69809579849243 and batch: 400, loss is 3.449843897819519 and perplexity is 31.495475412601625
At time: 226.3176395893097 and batch: 450, loss is 3.4836039209365843 and perplexity is 32.57691539873906
At time: 226.95539331436157 and batch: 500, loss is 3.353537530899048 and perplexity is 28.603741498607828
At time: 227.57465863227844 and batch: 550, loss is 3.410911550521851 and perplexity is 30.29284513648445
At time: 228.19429087638855 and batch: 600, loss is 3.4512655687332154 and perplexity is 31.540283457508853
At time: 228.81439232826233 and batch: 650, loss is 3.278638758659363 and perplexity is 26.539621270407988
At time: 229.43479084968567 and batch: 700, loss is 3.2726927185058594 and perplexity is 26.38228384755261
At time: 230.0546088218689 and batch: 750, loss is 3.3705722284317017 and perplexity is 29.095171379454417
At time: 230.6756522655487 and batch: 800, loss is 3.325797924995422 and perplexity is 27.821189010481568
At time: 231.2954626083374 and batch: 850, loss is 3.3817590475082397 and perplexity is 29.422481161913797
At time: 231.91607761383057 and batch: 900, loss is 3.357941060066223 and perplexity is 28.729976644674956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258587144825556 and perplexity of 70.71000984349784
finished 19 epochs...
Completing Train Step...
At time: 233.48407769203186 and batch: 50, loss is 3.6546741485595704 and perplexity is 38.654923300327695
At time: 234.1031038761139 and batch: 100, loss is 3.528394827842712 and perplexity is 34.069236710056686
At time: 234.7230043411255 and batch: 150, loss is 3.5337376976013184 and perplexity is 34.25175134636345
At time: 235.34278774261475 and batch: 200, loss is 3.424068956375122 and perplexity is 30.694054040585954
At time: 235.98807740211487 and batch: 250, loss is 3.56864990234375 and perplexity is 35.468674637072255
At time: 236.6059591770172 and batch: 300, loss is 3.5425256299972534 and perplexity is 34.55407989739446
At time: 237.2239863872528 and batch: 350, loss is 3.522256212234497 and perplexity is 33.86073935910126
At time: 237.84402084350586 and batch: 400, loss is 3.4493030214309695 and perplexity is 31.47844485973195
At time: 238.46444511413574 and batch: 450, loss is 3.483054337501526 and perplexity is 32.559016584567026
At time: 239.08433866500854 and batch: 500, loss is 3.3531260013580324 and perplexity is 28.59197263578103
At time: 239.7052459716797 and batch: 550, loss is 3.410557188987732 and perplexity is 30.282112419152458
At time: 240.32439494132996 and batch: 600, loss is 3.451031379699707 and perplexity is 31.53289793384744
At time: 240.9447627067566 and batch: 650, loss is 3.2785916423797605 and perplexity is 26.538370851649443
At time: 241.56406593322754 and batch: 700, loss is 3.272736768722534 and perplexity is 26.383446018469222
At time: 242.193217754364 and batch: 750, loss is 3.370671281814575 and perplexity is 29.098053497344235
At time: 242.81329131126404 and batch: 800, loss is 3.3259388828277587 and perplexity is 27.82511090138164
At time: 243.4331555366516 and batch: 850, loss is 3.382092180252075 and perplexity is 29.432284386590617
At time: 244.05452370643616 and batch: 900, loss is 3.3583325481414796 and perplexity is 28.741226289841034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258297436857877 and perplexity of 70.6895275573355
Finished Training.
Improved accuracyfrom -10000000 to -70.6895275573355
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f67603d0b70>
ELAPSED
255.24836468696594


RESULTS SO FAR:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0128309726715088 and batch: 50, loss is 7.057760009765625 and perplexity is 1161.8397391275353
At time: 1.7983033657073975 and batch: 100, loss is 6.119906902313232 and perplexity is 454.8223496198106
At time: 2.5877552032470703 and batch: 150, loss is 5.894429445266724 and perplexity is 363.0096599222094
At time: 3.376715660095215 and batch: 200, loss is 5.683149938583374 and perplexity is 293.8736575007043
At time: 4.163447141647339 and batch: 250, loss is 5.704089374542236 and perplexity is 300.09208409538695
At time: 4.952755928039551 and batch: 300, loss is 5.582849454879761 and perplexity is 265.8279924999434
At time: 5.742212295532227 and batch: 350, loss is 5.533254728317261 and perplexity is 252.9659079133878
At time: 6.5305821895599365 and batch: 400, loss is 5.375713548660278 and perplexity is 216.0940109033552
At time: 7.3189356327056885 and batch: 450, loss is 5.368534297943115 and perplexity is 214.5481734375477
At time: 8.1077561378479 and batch: 500, loss is 5.29571572303772 and perplexity is 199.4803475662353
At time: 8.896262407302856 and batch: 550, loss is 5.336102991104126 and perplexity is 207.7017156552076
At time: 9.682034015655518 and batch: 600, loss is 5.249544229507446 and perplexity is 190.47943376643883
At time: 10.46999478340149 and batch: 650, loss is 5.135631809234619 and perplexity is 169.9716756312201
At time: 11.25878620147705 and batch: 700, loss is 5.221819047927856 and perplexity is 185.2708943682455
At time: 12.047430038452148 and batch: 750, loss is 5.2026076507568355 and perplexity is 181.7455534212723
At time: 12.836631059646606 and batch: 800, loss is 5.172475061416626 and perplexity is 176.3507767766513
At time: 13.625996589660645 and batch: 850, loss is 5.20332365989685 and perplexity is 181.87573149746362
At time: 14.415553569793701 and batch: 900, loss is 5.100323085784912 and perplexity is 164.0749090081527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.076245869675728 and perplexity of 160.17162068762744
finished 1 epochs...
Completing Train Step...
At time: 16.102881908416748 and batch: 50, loss is 5.011787919998169 and perplexity is 150.1729935645662
At time: 16.720923900604248 and batch: 100, loss is 4.880018253326416 and perplexity is 131.63306660523318
At time: 17.338566303253174 and batch: 150, loss is 4.8499796676635745 and perplexity is 127.73779261185139
At time: 17.95764970779419 and batch: 200, loss is 4.7304041290283205 and perplexity is 113.34135762716228
At time: 18.60918354988098 and batch: 250, loss is 4.831925725936889 and perplexity is 125.45231496582237
At time: 19.2340145111084 and batch: 300, loss is 4.770976514816284 and perplexity is 118.03444808866082
At time: 19.858413219451904 and batch: 350, loss is 4.75489896774292 and perplexity is 116.15191748118058
At time: 20.489007711410522 and batch: 400, loss is 4.616691179275513 and perplexity is 101.15876155362528
At time: 21.117169618606567 and batch: 450, loss is 4.633503608703613 and perplexity is 102.8738632073247
At time: 21.753751039505005 and batch: 500, loss is 4.534510250091553 and perplexity is 93.17787027457533
At time: 22.392473459243774 and batch: 550, loss is 4.602550535202027 and perplexity is 99.73837775047608
At time: 23.03717064857483 and batch: 600, loss is 4.557546319961548 and perplexity is 95.34923605076713
At time: 23.650509119033813 and batch: 650, loss is 4.419277353286743 and perplexity is 83.03625779281946
At time: 24.255034923553467 and batch: 700, loss is 4.468567628860473 and perplexity is 87.23168533611128
At time: 24.860594034194946 and batch: 750, loss is 4.513726081848144 and perplexity is 91.26123259772343
At time: 25.526191234588623 and batch: 800, loss is 4.459705486297607 and perplexity is 86.46204109033715
At time: 26.150896310806274 and batch: 850, loss is 4.52220048904419 and perplexity is 92.03790370951413
At time: 26.75681161880493 and batch: 900, loss is 4.4477946567535405 and perplexity is 85.43831527152925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.583615237719392 and perplexity of 97.86756994563532
finished 2 epochs...
Completing Train Step...
At time: 28.27827286720276 and batch: 50, loss is 4.501444072723388 and perplexity is 90.14721648810007
At time: 28.892279624938965 and batch: 100, loss is 4.3758846330642704 and perplexity is 79.51014575395993
At time: 29.498658180236816 and batch: 150, loss is 4.372996044158936 and perplexity is 79.28080502408463
At time: 30.103983879089355 and batch: 200, loss is 4.271147913932801 and perplexity is 71.60378344153754
At time: 30.710241556167603 and batch: 250, loss is 4.412772159576416 and perplexity is 82.49784399262437
At time: 31.31497812271118 and batch: 300, loss is 4.375693769454956 and perplexity is 79.49497160870632
At time: 31.920713901519775 and batch: 350, loss is 4.372041206359864 and perplexity is 79.20514084396505
At time: 32.530781745910645 and batch: 400, loss is 4.268919835090637 and perplexity is 71.44442216730118
At time: 33.13784694671631 and batch: 450, loss is 4.303114757537842 and perplexity is 73.92970856621028
At time: 33.74553561210632 and batch: 500, loss is 4.182049384117127 and perplexity is 65.49995031669164
At time: 34.36894369125366 and batch: 550, loss is 4.2653682947158815 and perplexity is 71.19113446441025
At time: 34.97581744194031 and batch: 600, loss is 4.256275868415832 and perplexity is 70.54676818678006
At time: 35.58310008049011 and batch: 650, loss is 4.107389516830445 and perplexity is 60.787824673016154
At time: 36.19265937805176 and batch: 700, loss is 4.13357177734375 and perplexity is 62.40040572414517
At time: 36.804288387298584 and batch: 750, loss is 4.222047057151794 and perplexity is 68.1728953625301
At time: 37.4165518283844 and batch: 800, loss is 4.168805565834045 and perplexity is 64.63819990585968
At time: 38.02599740028381 and batch: 850, loss is 4.244631280899048 and perplexity is 69.73004460393342
At time: 38.636906147003174 and batch: 900, loss is 4.1877175283432 and perplexity is 65.8722676592724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42930665734696 and perplexity of 83.8732438482579
finished 3 epochs...
Completing Train Step...
At time: 40.17589807510376 and batch: 50, loss is 4.263030190467834 and perplexity is 71.02487661033372
At time: 40.79144549369812 and batch: 100, loss is 4.1354045391082765 and perplexity is 62.514875667873156
At time: 41.397053241729736 and batch: 150, loss is 4.141335768699646 and perplexity is 62.886767546145414
At time: 42.00657033920288 and batch: 200, loss is 4.03586416721344 and perplexity is 56.5918038949296
At time: 42.61870718002319 and batch: 250, loss is 4.18480345249176 and perplexity is 65.6805902916421
At time: 43.22929787635803 and batch: 300, loss is 4.156740007400512 and perplexity is 63.862990002467726
At time: 43.838974714279175 and batch: 350, loss is 4.160333251953125 and perplexity is 64.09287811816917
At time: 44.47548961639404 and batch: 400, loss is 4.0699642419815065 and perplexity is 58.55486874837715
At time: 45.085240602493286 and batch: 450, loss is 4.110869364738464 and perplexity is 60.99972553509181
At time: 45.695165157318115 and batch: 500, loss is 3.980211730003357 and perplexity is 53.52836658898594
At time: 46.30571007728577 and batch: 550, loss is 4.06630955696106 and perplexity is 58.341259721303636
At time: 46.91648244857788 and batch: 600, loss is 4.073061552047729 and perplexity is 58.736512491060196
At time: 47.527568340301514 and batch: 650, loss is 3.9204215955734254 and perplexity is 50.421697862270904
At time: 48.13719081878662 and batch: 700, loss is 3.9389799642562866 and perplexity is 51.36617923004677
At time: 48.74765133857727 and batch: 750, loss is 4.038895101547241 and perplexity is 56.76359014127213
At time: 49.41400718688965 and batch: 800, loss is 3.990160388946533 and perplexity is 54.063559865394296
At time: 50.02590346336365 and batch: 850, loss is 4.0687153673172 and perplexity is 58.48178670098297
At time: 50.67581653594971 and batch: 900, loss is 4.020871086120605 and perplexity is 55.74964742587091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355528531009203 and perplexity of 77.90799124637837
finished 4 epochs...
Completing Train Step...
At time: 52.26087737083435 and batch: 50, loss is 4.10382999420166 and perplexity is 60.571833676012744
At time: 52.87348198890686 and batch: 100, loss is 3.976175503730774 and perplexity is 53.31274942198854
At time: 53.53401041030884 and batch: 150, loss is 3.9854289436340333 and perplexity is 53.80836528417221
At time: 54.15922546386719 and batch: 200, loss is 3.8803320646286013 and perplexity is 48.440297710428155
At time: 54.77142548561096 and batch: 250, loss is 4.032207455635071 and perplexity is 56.385241889395814
At time: 55.39995622634888 and batch: 300, loss is 4.008014016151428 and perplexity is 55.03745845053135
At time: 56.046955585479736 and batch: 350, loss is 4.008171000480652 and perplexity is 55.04609914723765
At time: 56.65933060646057 and batch: 400, loss is 3.9269376516342165 and perplexity is 50.75132122809839
At time: 57.273709774017334 and batch: 450, loss is 3.9723681449890136 and perplexity is 53.11015457978538
At time: 57.889588832855225 and batch: 500, loss is 3.8400644731521605 and perplexity is 46.528474180482434
At time: 58.50575518608093 and batch: 550, loss is 3.921358165740967 and perplexity is 50.468943441231886
At time: 59.121190786361694 and batch: 600, loss is 3.938636202812195 and perplexity is 51.3485245527697
At time: 59.736042499542236 and batch: 650, loss is 3.7849679231643676 and perplexity is 44.03425805643787
At time: 60.370304584503174 and batch: 700, loss is 3.80016047000885 and perplexity is 44.70835826834455
At time: 61.01596117019653 and batch: 750, loss is 3.9033491468429564 and perplexity is 49.56818253987416
At time: 61.631380558013916 and batch: 800, loss is 3.8602806377410888 and perplexity is 47.47867380714372
At time: 62.24452614784241 and batch: 850, loss is 3.9368681383132933 and perplexity is 51.2578172612376
At time: 62.877715826034546 and batch: 900, loss is 3.897018542289734 and perplexity is 49.25537714609682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329119956656678 and perplexity of 75.87748170522823
finished 5 epochs...
Completing Train Step...
At time: 64.4706826210022 and batch: 50, loss is 3.983809266090393 and perplexity is 53.72128362440302
At time: 65.10096096992493 and batch: 100, loss is 3.8557391500473024 and perplexity is 47.26353888040743
At time: 65.71421694755554 and batch: 150, loss is 3.8669546937942503 and perplexity is 47.7966089156818
At time: 66.32746767997742 and batch: 200, loss is 3.765521559715271 and perplexity is 43.18622418181266
At time: 66.94210433959961 and batch: 250, loss is 3.9172551345825197 and perplexity is 50.262292032255885
At time: 67.55597853660583 and batch: 300, loss is 3.895845346450806 and perplexity is 49.197624826598776
At time: 68.1706292629242 and batch: 350, loss is 3.897101459503174 and perplexity is 49.25946143404325
At time: 68.81267380714417 and batch: 400, loss is 3.8218447017669677 and perplexity is 45.688412126095265
At time: 69.44099998474121 and batch: 450, loss is 3.864465184211731 and perplexity is 47.67776679045762
At time: 70.10546040534973 and batch: 500, loss is 3.7313129568099974 and perplexity is 41.73386697303712
At time: 70.76776242256165 and batch: 550, loss is 3.808698048591614 and perplexity is 45.09169343899156
At time: 71.42336750030518 and batch: 600, loss is 3.8307621717453 and perplexity is 46.09765918177019
At time: 72.0606141090393 and batch: 650, loss is 3.682261438369751 and perplexity is 39.736153403695106
At time: 72.67438912391663 and batch: 700, loss is 3.6944457626342775 and perplexity is 40.22327316799521
At time: 73.2877585887909 and batch: 750, loss is 3.799627914428711 and perplexity is 44.68455492153506
At time: 73.90145683288574 and batch: 800, loss is 3.7620789861679076 and perplexity is 43.037808042149194
At time: 74.51648020744324 and batch: 850, loss is 3.8353675556182862 and perplexity is 46.31044620481734
At time: 75.13072419166565 and batch: 900, loss is 3.801625633239746 and perplexity is 44.77391132222825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32740804593857 and perplexity of 75.74769735234236
finished 6 epochs...
Completing Train Step...
At time: 76.6830039024353 and batch: 50, loss is 3.8850345993041993 and perplexity is 48.66862633096724
At time: 77.30702781677246 and batch: 100, loss is 3.7629176235198973 and perplexity is 43.07391629426929
At time: 77.92235708236694 and batch: 150, loss is 3.775465054512024 and perplexity is 43.61778824895044
At time: 78.53656387329102 and batch: 200, loss is 3.671484351158142 and perplexity is 39.31021273546074
At time: 79.15060997009277 and batch: 250, loss is 3.8268778038024904 and perplexity is 45.91894623034207
At time: 79.76495909690857 and batch: 300, loss is 3.802979302406311 and perplexity is 44.8345614262782
At time: 80.3800220489502 and batch: 350, loss is 3.8042650556564332 and perplexity is 44.89224468462164
At time: 81.00375723838806 and batch: 400, loss is 3.731186308860779 and perplexity is 41.72858179905731
At time: 81.61743927001953 and batch: 450, loss is 3.774677724838257 and perplexity is 43.583460185482764
At time: 82.23247170448303 and batch: 500, loss is 3.6436763525009157 and perplexity is 38.23213347674944
At time: 82.84694361686707 and batch: 550, loss is 3.7233259057998658 and perplexity is 41.40186407566458
At time: 83.46043634414673 and batch: 600, loss is 3.7467940187454225 and perplexity is 42.384978497461454
At time: 84.07518148422241 and batch: 650, loss is 3.600679650306702 and perplexity is 36.62311689966898
At time: 84.6897087097168 and batch: 700, loss is 3.612992401123047 and perplexity is 37.07683574359363
At time: 85.33584904670715 and batch: 750, loss is 3.71717538356781 and perplexity is 41.14800248118441
At time: 85.97424173355103 and batch: 800, loss is 3.6783703184127807 and perplexity is 39.58183569324211
At time: 86.58578753471375 and batch: 850, loss is 3.748381567001343 and perplexity is 42.4523201360593
At time: 87.19783735275269 and batch: 900, loss is 3.7204364490509034 and perplexity is 41.28240784500873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325276414008989 and perplexity of 75.58640311312384
finished 7 epochs...
Completing Train Step...
At time: 88.76390814781189 and batch: 50, loss is 3.8032448196411135 and perplexity is 44.84646735559645
At time: 89.37619376182556 and batch: 100, loss is 3.6802814531326296 and perplexity is 39.65755424484084
At time: 89.98966240882874 and batch: 150, loss is 3.695365333557129 and perplexity is 40.26027833225126
At time: 90.64580702781677 and batch: 200, loss is 3.592599196434021 and perplexity is 36.32837790901236
At time: 91.27757501602173 and batch: 250, loss is 3.752715802192688 and perplexity is 42.636717798698335
At time: 91.89249157905579 and batch: 300, loss is 3.724203667640686 and perplexity is 41.43822100611781
At time: 92.50708627700806 and batch: 350, loss is 3.726771740913391 and perplexity is 41.54477415353723
At time: 93.11990904808044 and batch: 400, loss is 3.655325174331665 and perplexity is 38.68009684503862
At time: 93.73430252075195 and batch: 450, loss is 3.699212522506714 and perplexity is 40.41546555603587
At time: 94.34523510932922 and batch: 500, loss is 3.567560796737671 and perplexity is 35.43006653265255
At time: 94.95584154129028 and batch: 550, loss is 3.6483311605453492 and perplexity is 38.41051155498973
At time: 95.565181016922 and batch: 600, loss is 3.6735888719558716 and perplexity is 39.39302900944502
At time: 96.18752360343933 and batch: 650, loss is 3.53030038356781 and perplexity is 34.13421943355618
At time: 96.80140018463135 and batch: 700, loss is 3.5449784851074218 and perplexity is 34.638940081168904
At time: 97.41661977767944 and batch: 750, loss is 3.644702000617981 and perplexity is 38.27136630856203
At time: 98.03100728988647 and batch: 800, loss is 3.608943781852722 and perplexity is 36.92702921135322
At time: 98.64572930335999 and batch: 850, loss is 3.676629967689514 and perplexity is 39.513009325246706
At time: 99.25955963134766 and batch: 900, loss is 3.6514365243911744 and perplexity is 38.52997556247501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332187600331764 and perplexity of 76.11060416757098
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.80700945854187 and batch: 50, loss is 3.756746997833252 and perplexity is 42.80894165040906
At time: 101.43297290802002 and batch: 100, loss is 3.6257384490966795 and perplexity is 37.55244349128863
At time: 102.04674124717712 and batch: 150, loss is 3.648633017539978 and perplexity is 38.422107786683654
At time: 102.66024661064148 and batch: 200, loss is 3.5257724571228026 and perplexity is 33.98001158307096
At time: 103.27478528022766 and batch: 250, loss is 3.6832420206069947 and perplexity is 39.77513708012577
At time: 103.88898229598999 and batch: 300, loss is 3.6404749393463134 and perplexity is 38.109932334251425
At time: 104.52878189086914 and batch: 350, loss is 3.6316131067276003 and perplexity is 37.77370050828862
At time: 105.14801144599915 and batch: 400, loss is 3.5553137350082396 and perplexity is 34.99879859377905
At time: 105.76185297966003 and batch: 450, loss is 3.581054267883301 and perplexity is 35.91138111242144
At time: 106.37538552284241 and batch: 500, loss is 3.4452064085006713 and perplexity is 31.349753634539642
At time: 106.98856115341187 and batch: 550, loss is 3.5112033700942993 and perplexity is 33.48854265703097
At time: 107.60062956809998 and batch: 600, loss is 3.5305768823623658 and perplexity is 34.143658809010496
At time: 108.21383309364319 and batch: 650, loss is 3.375092477798462 and perplexity is 29.226986503629643
At time: 108.82900166511536 and batch: 700, loss is 3.372186846733093 and perplexity is 29.142186921509033
At time: 109.44224119186401 and batch: 750, loss is 3.46070827960968 and perplexity is 31.839519812911522
At time: 110.05645775794983 and batch: 800, loss is 3.4117033433914186 and perplexity is 30.316840293605974
At time: 110.67125749588013 and batch: 850, loss is 3.4566871881484986 and perplexity is 31.711747256396396
At time: 111.28461694717407 and batch: 900, loss is 3.4249381589889527 and perplexity is 30.720744990828507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.28758218843643 and perplexity of 72.79026243071596
finished 9 epochs...
Completing Train Step...
At time: 112.84398460388184 and batch: 50, loss is 3.669848051071167 and perplexity is 39.24594202836478
At time: 113.46993803977966 and batch: 100, loss is 3.5355175256729128 and perplexity is 34.31276785825405
At time: 114.08472633361816 and batch: 150, loss is 3.5570703983306884 and perplexity is 35.06033373203462
At time: 114.69848728179932 and batch: 200, loss is 3.438187656402588 and perplexity is 31.130487871597524
At time: 115.31109666824341 and batch: 250, loss is 3.5968341064453124 and perplexity is 36.48255154563344
At time: 115.92563056945801 and batch: 300, loss is 3.5577424430847167 and perplexity is 35.083903764564965
At time: 116.54002857208252 and batch: 350, loss is 3.5529969120025635 and perplexity is 34.91780643050779
At time: 117.15429449081421 and batch: 400, loss is 3.481246862411499 and perplexity is 32.500220125709205
At time: 117.76861071586609 and batch: 450, loss is 3.512259078025818 and perplexity is 33.523915445510376
At time: 118.38327097892761 and batch: 500, loss is 3.380629510879517 and perplexity is 29.389266154048965
At time: 118.99717926979065 and batch: 550, loss is 3.4498498964309694 and perplexity is 31.49566434228772
At time: 119.61107778549194 and batch: 600, loss is 3.47526734828949 and perplexity is 32.306464460172435
At time: 120.22524285316467 and batch: 650, loss is 3.324837908744812 and perplexity is 27.794493033256508
At time: 120.83966565132141 and batch: 700, loss is 3.325685224533081 and perplexity is 27.81805372629451
At time: 121.45378565788269 and batch: 750, loss is 3.421278438568115 and perplexity is 30.60852113227199
At time: 122.06896376609802 and batch: 800, loss is 3.374608597755432 and perplexity is 29.212847569192604
At time: 122.68407726287842 and batch: 850, loss is 3.426437349319458 and perplexity is 30.76683577546131
At time: 123.29812169075012 and batch: 900, loss is 3.4016445684432983 and perplexity is 30.01341860352173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.295869174068922 and perplexity of 73.39598061217131
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 124.86338305473328 and batch: 50, loss is 3.644803228378296 and perplexity is 38.27524062934874
At time: 125.4761905670166 and batch: 100, loss is 3.522397871017456 and perplexity is 33.8655363699903
At time: 126.08921504020691 and batch: 150, loss is 3.5486889934539794 and perplexity is 34.76770690490604
At time: 126.7039577960968 and batch: 200, loss is 3.4271724128723147 and perplexity is 30.7894596690577
At time: 127.3267970085144 and batch: 250, loss is 3.5824073266983034 and perplexity is 35.96000421073227
At time: 127.94060039520264 and batch: 300, loss is 3.5408276414871214 and perplexity is 34.495457251125295
At time: 128.5538625717163 and batch: 350, loss is 3.529742078781128 and perplexity is 34.11516745435725
At time: 129.1672658920288 and batch: 400, loss is 3.4595066022872927 and perplexity is 31.801281963376336
At time: 129.78100037574768 and batch: 450, loss is 3.482231922149658 and perplexity is 32.53225055739062
At time: 130.39529299736023 and batch: 500, loss is 3.3513305377960205 and perplexity is 28.540682848997605
At time: 131.0099287033081 and batch: 550, loss is 3.4137802267074586 and perplexity is 30.379870263697107
At time: 131.62380027770996 and batch: 600, loss is 3.4382851791381834 and perplexity is 31.133523949976112
At time: 132.2374141216278 and batch: 650, loss is 3.2796234560012816 and perplexity is 26.56576763594412
At time: 132.8778305053711 and batch: 700, loss is 3.2801865005493163 and perplexity is 26.580729558295033
At time: 133.51842856407166 and batch: 750, loss is 3.368173780441284 and perplexity is 29.025471743017548
At time: 134.1322934627533 and batch: 800, loss is 3.313489031791687 and perplexity is 27.480839923579392
At time: 134.74724769592285 and batch: 850, loss is 3.3625732135772703 and perplexity is 28.863367010676708
At time: 135.3606185913086 and batch: 900, loss is 3.3353761720657347 and perplexity is 28.088947514091714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280296012146832 and perplexity of 72.26182722034126
finished 11 epochs...
Completing Train Step...
At time: 136.9087872505188 and batch: 50, loss is 3.6218867826461794 and perplexity is 37.408082198803065
At time: 137.52925992012024 and batch: 100, loss is 3.489508295059204 and perplexity is 32.76983065649727
At time: 138.1390242576599 and batch: 150, loss is 3.5107052373886107 and perplexity is 33.47186507283758
At time: 138.75141954421997 and batch: 200, loss is 3.3914653873443603 and perplexity is 29.709456248688987
At time: 139.36606979370117 and batch: 250, loss is 3.548413143157959 and perplexity is 34.75811754533901
At time: 139.97978615760803 and batch: 300, loss is 3.5080353593826294 and perplexity is 33.38261846824812
At time: 140.5950963497162 and batch: 350, loss is 3.5000133895874024 and perplexity is 33.115895363899185
At time: 141.2094554901123 and batch: 400, loss is 3.431339769363403 and perplexity is 30.918038053565787
At time: 141.82395839691162 and batch: 450, loss is 3.4565672397613527 and perplexity is 31.707943711578505
At time: 142.43591570854187 and batch: 500, loss is 3.327709131240845 and perplexity is 27.87441188441738
At time: 143.0571792125702 and batch: 550, loss is 3.3922641849517823 and perplexity is 29.733197572252074
At time: 143.67112755775452 and batch: 600, loss is 3.4199612855911257 and perplexity is 30.568231567126542
At time: 144.28401827812195 and batch: 650, loss is 3.2648709774017335 and perplexity is 26.176733382778583
At time: 144.8980851173401 and batch: 700, loss is 3.2676576280593874 and perplexity is 26.249780525238574
At time: 145.5423309803009 and batch: 750, loss is 3.3593804025650025 and perplexity is 28.77135869537642
At time: 146.1827507019043 and batch: 800, loss is 3.308013710975647 and perplexity is 27.330784683869364
At time: 146.79708886146545 and batch: 850, loss is 3.3600984954833986 and perplexity is 28.792026624166724
At time: 147.41176915168762 and batch: 900, loss is 3.3349541330337527 and perplexity is 28.077095383080348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.282352813302654 and perplexity of 72.41060838427416
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 148.96034264564514 and batch: 50, loss is 3.614508376121521 and perplexity is 37.1330859257656
At time: 149.58370184898376 and batch: 100, loss is 3.490700445175171 and perplexity is 32.808920509771575
At time: 150.19665479660034 and batch: 150, loss is 3.5123476934432984 and perplexity is 33.52688631290354
At time: 150.81173610687256 and batch: 200, loss is 3.392348165512085 and perplexity is 29.735694687696895
At time: 151.42365503311157 and batch: 250, loss is 3.5475205516815187 and perplexity is 34.72710658799796
At time: 152.0381247997284 and batch: 300, loss is 3.509008183479309 and perplexity is 33.415109685461374
At time: 152.65174055099487 and batch: 350, loss is 3.498798508644104 and perplexity is 33.07568792227392
At time: 153.26560378074646 and batch: 400, loss is 3.429990124702454 and perplexity is 30.876337835148224
At time: 153.87972736358643 and batch: 450, loss is 3.4531188106536863 and perplexity is 31.598789429110504
At time: 154.49307823181152 and batch: 500, loss is 3.319729218482971 and perplexity is 27.652861660804263
At time: 155.10677886009216 and batch: 550, loss is 3.3837426280975342 and perplexity is 29.48090094545495
At time: 155.72023677825928 and batch: 600, loss is 3.4132126998901366 and perplexity is 30.362633764166194
At time: 156.33240580558777 and batch: 650, loss is 3.251170582771301 and perplexity is 25.820547321450277
At time: 156.94541382789612 and batch: 700, loss is 3.2507906341552735 and perplexity is 25.81073870373378
At time: 157.55978798866272 and batch: 750, loss is 3.3407797956466676 and perplexity is 28.241140439557768
At time: 158.1837248802185 and batch: 800, loss is 3.2887075567245483 and perplexity is 26.808193186906042
At time: 158.79704189300537 and batch: 850, loss is 3.3364476776123047 and perplexity is 28.119061107710493
At time: 159.43063640594482 and batch: 900, loss is 3.3098666524887084 and perplexity is 27.381473977039416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27867105562393 and perplexity of 72.14450024427693
finished 13 epochs...
Completing Train Step...
At time: 161.0241825580597 and batch: 50, loss is 3.6049022483825683 and perplexity is 36.778088563881
At time: 161.6393768787384 and batch: 100, loss is 3.4774271774291994 and perplexity is 32.37631631039119
At time: 162.2534213066101 and batch: 150, loss is 3.4978009939193724 and perplexity is 33.0427108868145
At time: 162.86732244491577 and batch: 200, loss is 3.377862582206726 and perplexity is 29.308060547723763
At time: 163.52926540374756 and batch: 250, loss is 3.5338582801818847 and perplexity is 34.25588175995267
At time: 164.14360213279724 and batch: 300, loss is 3.495350885391235 and perplexity is 32.96185175635049
At time: 164.7697069644928 and batch: 350, loss is 3.4859150075912475 and perplexity is 32.652290538858416
At time: 165.38227438926697 and batch: 400, loss is 3.4187495708465576 and perplexity is 30.5312140221031
At time: 166.04063153266907 and batch: 450, loss is 3.4435657405853273 and perplexity is 31.298361270027414
At time: 166.66373324394226 and batch: 500, loss is 3.3111927366256713 and perplexity is 27.417808201121648
At time: 167.2776234149933 and batch: 550, loss is 3.37651563167572 and perplexity is 29.268610614524913
At time: 167.8909718990326 and batch: 600, loss is 3.407319898605347 and perplexity is 30.184238935779398
At time: 168.5043752193451 and batch: 650, loss is 3.247003426551819 and perplexity is 25.713172945370147
At time: 169.11870074272156 and batch: 700, loss is 3.248463954925537 and perplexity is 25.7507552023308
At time: 169.73288822174072 and batch: 750, loss is 3.339916524887085 and perplexity is 28.216771208934553
At time: 170.34684872627258 and batch: 800, loss is 3.2894752836227417 and perplexity is 26.828782460365833
At time: 170.96121978759766 and batch: 850, loss is 3.3388826656341553 and perplexity is 28.187614113695826
At time: 171.57402968406677 and batch: 900, loss is 3.3130918455123903 and perplexity is 27.469927078377847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.278607512173587 and perplexity of 72.13991607945654
finished 14 epochs...
Completing Train Step...
At time: 173.12534022331238 and batch: 50, loss is 3.5991841411590575 and perplexity is 36.568387627594184
At time: 173.74865245819092 and batch: 100, loss is 3.471086573600769 and perplexity is 32.17168035857832
At time: 174.36076760292053 and batch: 150, loss is 3.4907475566864012 and perplexity is 32.810466224008806
At time: 174.97542023658752 and batch: 200, loss is 3.3708111095428466 and perplexity is 29.102122496534722
At time: 175.58894443511963 and batch: 250, loss is 3.5268296098709104 and perplexity is 34.01595263994303
At time: 176.2026870250702 and batch: 300, loss is 3.488415174484253 and perplexity is 32.734028851778675
At time: 176.81345582008362 and batch: 350, loss is 3.479350666999817 and perplexity is 32.438651748712864
At time: 177.42658877372742 and batch: 400, loss is 3.412750678062439 and perplexity is 30.34860880478893
At time: 178.0420787334442 and batch: 450, loss is 3.438108162879944 and perplexity is 31.12801329781258
At time: 178.65681624412537 and batch: 500, loss is 3.306275362968445 and perplexity is 27.283315539678725
At time: 179.27381014823914 and batch: 550, loss is 3.3720536708831785 and perplexity is 29.13830614441602
At time: 179.88513326644897 and batch: 600, loss is 3.4036254119873046 and perplexity is 30.072929411337054
At time: 180.49580931663513 and batch: 650, loss is 3.2441116189956665 and perplexity is 25.638922807851802
At time: 181.10577249526978 and batch: 700, loss is 3.2462862300872803 and perplexity is 25.694738160113253
At time: 181.71720099449158 and batch: 750, loss is 3.3384511423110963 and perplexity is 28.17545312484825
At time: 182.3317289352417 and batch: 800, loss is 3.2887966728210447 and perplexity is 26.81058233489117
At time: 182.9453411102295 and batch: 850, loss is 3.3388174390792846 and perplexity is 28.185775592698015
At time: 183.55826044082642 and batch: 900, loss is 3.313373398780823 and perplexity is 27.47766241503393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279142614913313 and perplexity of 72.17852867612606
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 185.11131072044373 and batch: 50, loss is 3.596823740005493 and perplexity is 36.482173353418645
At time: 185.7346806526184 and batch: 100, loss is 3.4713828897476198 and perplexity is 32.18121475946859
At time: 186.34799981117249 and batch: 150, loss is 3.4916641998291014 and perplexity is 32.840555501339864
At time: 186.96283388137817 and batch: 200, loss is 3.371392331123352 and perplexity is 29.11904219473877
At time: 187.57689476013184 and batch: 250, loss is 3.5274957370758058 and perplexity is 34.038619139944934
At time: 188.1919538974762 and batch: 300, loss is 3.4881642055511475 and perplexity is 32.7258146582783
At time: 188.8057234287262 and batch: 350, loss is 3.4795506381988526 and perplexity is 32.445139193427586
At time: 189.42894101142883 and batch: 400, loss is 3.4135645818710327 and perplexity is 30.37331970786561
At time: 190.04437446594238 and batch: 450, loss is 3.439446611404419 and perplexity is 31.169704435777977
At time: 190.65722846984863 and batch: 500, loss is 3.30490394115448 and perplexity is 27.245924251065205
At time: 191.28518342971802 and batch: 550, loss is 3.3688323307037353 and perplexity is 29.044592770439447
At time: 191.92973566055298 and batch: 600, loss is 3.3998686361312864 and perplexity is 29.960164105818183
At time: 192.5446162223816 and batch: 650, loss is 3.240039138793945 and perplexity is 25.534721125982756
At time: 193.15918731689453 and batch: 700, loss is 3.2387354660034178 and perplexity is 25.501453894341108
At time: 193.77038717269897 and batch: 750, loss is 3.328813982009888 and perplexity is 27.905225969172566
At time: 194.38331818580627 and batch: 800, loss is 3.279307837486267 and perplexity is 26.557384310848548
At time: 194.9980924129486 and batch: 850, loss is 3.328961148262024 and perplexity is 27.909332978892664
At time: 195.61179327964783 and batch: 900, loss is 3.3030786228179934 and perplexity is 27.196237127139977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277814055142337 and perplexity of 72.08269885851452
finished 16 epochs...
Completing Train Step...
At time: 197.16924858093262 and batch: 50, loss is 3.5939441204071043 and perplexity is 36.3772696858571
At time: 197.78344202041626 and batch: 100, loss is 3.467637887001038 and perplexity is 32.06092141185791
At time: 198.39682745933533 and batch: 150, loss is 3.4881120824813845 and perplexity is 32.72410893281203
At time: 199.00984001159668 and batch: 200, loss is 3.367860403060913 and perplexity is 29.016377241798594
At time: 199.6239333152771 and batch: 250, loss is 3.523542070388794 and perplexity is 33.904307472120294
At time: 200.23778653144836 and batch: 300, loss is 3.4845504188537597 and perplexity is 32.60776397806701
At time: 200.8543906211853 and batch: 350, loss is 3.4758400344848632 and perplexity is 32.32497122516871
At time: 201.4686324596405 and batch: 400, loss is 3.4103077125549315 and perplexity is 30.27455868804659
At time: 202.08079504966736 and batch: 450, loss is 3.4363262605667115 and perplexity is 31.072595608029097
At time: 202.69502639770508 and batch: 500, loss is 3.302271399497986 and perplexity is 27.174292548593883
At time: 203.30899310112 and batch: 550, loss is 3.366927261352539 and perplexity is 28.98931347909507
At time: 203.92175126075745 and batch: 600, loss is 3.3986245298385622 and perplexity is 29.922913653688724
At time: 204.54531407356262 and batch: 650, loss is 3.238921465873718 and perplexity is 25.50619760260881
At time: 205.16704511642456 and batch: 700, loss is 3.238445382118225 and perplexity is 25.494057406370224
At time: 205.80061960220337 and batch: 750, loss is 3.329366192817688 and perplexity is 27.920639791994937
At time: 206.44413781166077 and batch: 800, loss is 3.280354962348938 and perplexity is 26.585207773025143
At time: 207.05897450447083 and batch: 850, loss is 3.3305188035964965 and perplexity is 27.952839975945114
At time: 207.67282915115356 and batch: 900, loss is 3.304755973815918 and perplexity is 27.241893042418067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277395588077911 and perplexity of 72.05254093361653
finished 17 epochs...
Completing Train Step...
At time: 209.22237968444824 and batch: 50, loss is 3.5921515226364136 and perplexity is 36.31211828588808
At time: 209.8462438583374 and batch: 100, loss is 3.46541711807251 and perplexity is 31.989800514535542
At time: 210.46108150482178 and batch: 150, loss is 3.485876979827881 and perplexity is 32.65104886888954
At time: 211.07365250587463 and batch: 200, loss is 3.3655348587036134 and perplexity is 28.948976771202712
At time: 211.688472032547 and batch: 250, loss is 3.5211420679092407 and perplexity is 33.823034616658035
At time: 212.30227661132812 and batch: 300, loss is 3.482171106338501 and perplexity is 32.53027214234427
At time: 212.91791033744812 and batch: 350, loss is 3.473548622131348 and perplexity is 32.25098618426665
At time: 213.53241539001465 and batch: 400, loss is 3.4082518100738524 and perplexity is 30.212381085170954
At time: 214.14661407470703 and batch: 450, loss is 3.4344421148300173 and perplexity is 31.01410542880049
At time: 214.76187109947205 and batch: 500, loss is 3.3006696128845214 and perplexity is 27.130799972771193
At time: 215.3763608932495 and batch: 550, loss is 3.365652151107788 and perplexity is 28.952372465427278
At time: 215.9904236793518 and batch: 600, loss is 3.3976776599884033 and perplexity is 29.89459395856974
At time: 216.60491967201233 and batch: 650, loss is 3.2382043838500976 and perplexity is 25.48791412297781
At time: 217.22001600265503 and batch: 700, loss is 3.2381805276870725 and perplexity is 25.48730608639606
At time: 217.83437037467957 and batch: 750, loss is 3.3295086145401003 and perplexity is 27.9246165807888
At time: 218.44748830795288 and batch: 800, loss is 3.2808072566986084 and perplexity is 26.59723483196607
At time: 219.06199145317078 and batch: 850, loss is 3.3312748432159425 and perplexity is 27.97398142132109
At time: 219.67692589759827 and batch: 900, loss is 3.305530962944031 and perplexity is 27.2630133963204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277319085108091 and perplexity of 72.04702891109774
finished 18 epochs...
Completing Train Step...
At time: 221.23521614074707 and batch: 50, loss is 3.5906033945083617 and perplexity is 36.2559459663735
At time: 221.86002016067505 and batch: 100, loss is 3.463645830154419 and perplexity is 31.933187521139363
At time: 222.47397565841675 and batch: 150, loss is 3.4840307569503786 and perplexity is 32.59082336744938
At time: 223.0837390422821 and batch: 200, loss is 3.363671131134033 and perplexity is 28.895074010715994
At time: 223.69372820854187 and batch: 250, loss is 3.519260597229004 and perplexity is 33.75945739680263
At time: 224.304589509964 and batch: 300, loss is 3.48030499458313 and perplexity is 32.46962362514953
At time: 224.94122195243835 and batch: 350, loss is 3.471769528388977 and perplexity is 32.193659666306324
At time: 225.56021332740784 and batch: 400, loss is 3.4066624593734742 and perplexity is 30.164401154701114
At time: 226.17392110824585 and batch: 450, loss is 3.432992687225342 and perplexity is 30.969185290366173
At time: 226.81826615333557 and batch: 500, loss is 3.299409894943237 and perplexity is 27.09664433503569
At time: 227.44968247413635 and batch: 550, loss is 3.3645724868774414 and perplexity is 28.921130492947604
At time: 228.06210207939148 and batch: 600, loss is 3.3968363857269286 and perplexity is 29.8694549819848
At time: 228.67445755004883 and batch: 650, loss is 3.237570285797119 and perplexity is 25.471757409269657
At time: 229.2872188091278 and batch: 700, loss is 3.2378296756744387 and perplexity is 25.478365382282764
At time: 229.89980721473694 and batch: 750, loss is 3.329406318664551 and perplexity is 27.921760153789126
At time: 230.51371264457703 and batch: 800, loss is 3.280927743911743 and perplexity is 26.600439651734085
At time: 231.1271402835846 and batch: 850, loss is 3.331601185798645 and perplexity is 27.983112012435903
At time: 231.74079871177673 and batch: 900, loss is 3.3058727979660034 and perplexity is 27.27233444214296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27738388270548 and perplexity of 72.05169753672642
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 233.30124139785767 and batch: 50, loss is 3.5898742151260374 and perplexity is 36.22951851443567
At time: 233.91648483276367 and batch: 100, loss is 3.4634935569763186 and perplexity is 31.92832532338919
At time: 234.5292706489563 and batch: 150, loss is 3.4841462326049806 and perplexity is 32.594587031413354
At time: 235.14335584640503 and batch: 200, loss is 3.363648114204407 and perplexity is 28.894408942484887
At time: 235.76772546768188 and batch: 250, loss is 3.519190273284912 and perplexity is 33.7570833820841
At time: 236.38108444213867 and batch: 300, loss is 3.479948444366455 and perplexity is 32.45804863746544
At time: 236.99456477165222 and batch: 350, loss is 3.471475396156311 and perplexity is 32.1841918657719
At time: 237.60819721221924 and batch: 400, loss is 3.406490807533264 and perplexity is 30.159223824095946
At time: 238.22195434570312 and batch: 450, loss is 3.432920331954956 and perplexity is 30.966944587655163
At time: 238.83599948883057 and batch: 500, loss is 3.2985605716705324 and perplexity is 27.07364029470598
At time: 239.45010423660278 and batch: 550, loss is 3.363345808982849 and perplexity is 28.885675331962293
At time: 240.06376481056213 and batch: 600, loss is 3.394972605705261 and perplexity is 29.813836734720304
At time: 240.67840957641602 and batch: 650, loss is 3.2362040519714355 and perplexity is 25.436980794600643
At time: 241.29159665107727 and batch: 700, loss is 3.2352529621124266 and perplexity is 25.41279944126588
At time: 241.90400004386902 and batch: 750, loss is 3.325937519073486 and perplexity is 27.825072954793633
At time: 242.5175642967224 and batch: 800, loss is 3.277517490386963 and perplexity is 26.509879912252618
At time: 243.13173389434814 and batch: 850, loss is 3.3282209825515747 and perplexity is 27.888683090732492
At time: 243.7465226650238 and batch: 900, loss is 3.302521758079529 and perplexity is 27.181096717636297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2771669152664815 and perplexity of 72.03606636022589
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f67603d0b70>
ELAPSED
505.7231638431549


RESULTS SO FAR:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.03606636022589, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'dropout': 0.17348567797130798, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.17852101364501027, 'wordvec_source': 'glove', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9891448020935059 and batch: 50, loss is 7.048665361404419 and perplexity is 1151.3211193240654
At time: 1.7717974185943604 and batch: 100, loss is 6.085424280166626 and perplexity is 439.40620446676536
At time: 2.556419849395752 and batch: 150, loss is 5.83472357749939 and perplexity is 341.97019272839367
At time: 3.3414928913116455 and batch: 200, loss is 5.594385681152343 and perplexity is 268.91240138322684
At time: 4.126728534698486 and batch: 250, loss is 5.579029846191406 and perplexity is 264.81457026048054
At time: 4.909489154815674 and batch: 300, loss is 5.436796293258667 and perplexity is 229.70509562108163
At time: 5.693511962890625 and batch: 350, loss is 5.368974876403809 and perplexity is 214.6427195675103
At time: 6.478707790374756 and batch: 400, loss is 5.176755275726318 and perplexity is 177.1072135959439
At time: 7.263600587844849 and batch: 450, loss is 5.147226943969726 and perplexity is 171.95399050567883
At time: 8.048402309417725 and batch: 500, loss is 5.061344738006592 and perplexity is 157.80257683110153
At time: 8.833249568939209 and batch: 550, loss is 5.096122884750367 and perplexity is 163.38720666066834
At time: 9.617091417312622 and batch: 600, loss is 4.999302415847779 and perplexity is 148.3096645370868
At time: 10.402431964874268 and batch: 650, loss is 4.872761268615722 and perplexity is 130.68126523457613
At time: 11.203006267547607 and batch: 700, loss is 4.9368102169036865 and perplexity is 139.325123086459
At time: 11.98739218711853 and batch: 750, loss is 4.924417877197266 and perplexity is 137.60921286205286
At time: 12.770259380340576 and batch: 800, loss is 4.877070150375366 and perplexity is 131.24557024335684
At time: 13.553942203521729 and batch: 850, loss is 4.913659152984619 and perplexity is 136.13664895035296
At time: 14.3386869430542 and batch: 900, loss is 4.824690408706665 and perplexity is 124.54790346730569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.832816764099957 and perplexity of 125.56414758226754
finished 1 epochs...
Completing Train Step...
At time: 16.069355010986328 and batch: 50, loss is 4.793594398498535 and perplexity is 120.73455778288154
At time: 16.679333448410034 and batch: 100, loss is 4.660353908538818 and perplexity is 105.6734742844484
At time: 17.29320788383484 and batch: 150, loss is 4.651739015579223 and perplexity is 104.76701872975482
At time: 17.907875537872314 and batch: 200, loss is 4.5439414787292485 and perplexity is 94.06080912757345
At time: 18.52086591720581 and batch: 250, loss is 4.654665803909301 and perplexity is 105.07409877749396
At time: 19.13518714904785 and batch: 300, loss is 4.603622512817383 and perplexity is 99.84535238577972
At time: 19.748931646347046 and batch: 350, loss is 4.586673231124878 and perplexity is 98.16730639159991
At time: 20.361825466156006 and batch: 400, loss is 4.465619421005249 and perplexity is 86.97488692972445
At time: 20.975101232528687 and batch: 450, loss is 4.492283201217651 and perplexity is 89.32516054469723
At time: 21.58918070793152 and batch: 500, loss is 4.381032295227051 and perplexity is 79.92049237958605
At time: 22.204338312149048 and batch: 550, loss is 4.45577223777771 and perplexity is 86.12263232227552
At time: 22.81831407546997 and batch: 600, loss is 4.425379943847656 and perplexity is 83.54454342795205
At time: 23.432343006134033 and batch: 650, loss is 4.279007892608643 and perplexity is 72.16880527338942
At time: 24.045801401138306 and batch: 700, loss is 4.324853715896606 and perplexity is 75.55445963490567
At time: 24.659192323684692 and batch: 750, loss is 4.382467012405396 and perplexity is 80.03523797696184
At time: 25.2752583026886 and batch: 800, loss is 4.3327710151672365 and perplexity is 76.15502117869086
At time: 25.88904333114624 and batch: 850, loss is 4.398748269081116 and perplexity is 81.34897787751851
At time: 26.503252506256104 and batch: 900, loss is 4.338518114089966 and perplexity is 76.59395169930406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.505044179419949 and perplexity of 90.47234097612326
finished 2 epochs...
Completing Train Step...
At time: 28.066293239593506 and batch: 50, loss is 4.40232813835144 and perplexity is 81.64071846861717
At time: 28.732693910598755 and batch: 100, loss is 4.272263588905335 and perplexity is 71.68371457095128
At time: 29.34600853919983 and batch: 150, loss is 4.275360627174377 and perplexity is 71.90606591583051
At time: 29.959348917007446 and batch: 200, loss is 4.1790007209777835 and perplexity is 65.30056711307209
At time: 30.57397747039795 and batch: 250, loss is 4.31758560180664 and perplexity is 75.00731197668493
At time: 31.1886887550354 and batch: 300, loss is 4.2854458475112915 and perplexity is 72.63492360156259
At time: 31.80345845222473 and batch: 350, loss is 4.276268253326416 and perplexity is 71.97135936828612
At time: 32.419353008270264 and batch: 400, loss is 4.180275249481201 and perplexity is 65.38384760755547
At time: 33.03359055519104 and batch: 450, loss is 4.218817229270935 and perplexity is 67.95306384445534
At time: 33.64786338806152 and batch: 500, loss is 4.093389387130737 and perplexity is 59.94271685658931
At time: 34.26345872879028 and batch: 550, loss is 4.171850709915161 and perplexity is 64.83533253401231
At time: 34.88509249687195 and batch: 600, loss is 4.169933805465698 and perplexity is 64.7111684399679
At time: 35.49840021133423 and batch: 650, loss is 4.020927495956421 and perplexity is 55.752792343030286
At time: 36.113621950149536 and batch: 700, loss is 4.046800851821899 and perplexity is 57.214127480890795
At time: 36.72752618789673 and batch: 750, loss is 4.132141518592834 and perplexity is 62.31122079176586
At time: 37.3401358127594 and batch: 800, loss is 4.085725865364075 and perplexity is 59.485100258220285
At time: 37.953399896621704 and batch: 850, loss is 4.16182092666626 and perplexity is 64.188298431833
At time: 38.56785011291504 and batch: 900, loss is 4.112846431732177 and perplexity is 61.12044537535621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388311098699701 and perplexity of 80.50434021615098
finished 3 epochs...
Completing Train Step...
At time: 40.11121869087219 and batch: 50, loss is 4.1939974117279055 and perplexity is 66.28723944189305
At time: 40.73372435569763 and batch: 100, loss is 4.064945473670959 and perplexity is 58.26173163759107
At time: 41.348798751831055 and batch: 150, loss is 4.067732610702515 and perplexity is 58.42434157017854
At time: 41.9628005027771 and batch: 200, loss is 3.9727415800094605 and perplexity is 53.129991475112
At time: 42.57652139663696 and batch: 250, loss is 4.1228455066680905 and perplexity is 61.73465895509498
At time: 43.19953179359436 and batch: 300, loss is 4.093137621879578 and perplexity is 59.927627263022075
At time: 43.81440186500549 and batch: 350, loss is 4.086532754898071 and perplexity is 59.53311753276029
At time: 44.428508281707764 and batch: 400, loss is 4.0036795043945315 and perplexity is 54.79941421527367
At time: 45.04141449928284 and batch: 450, loss is 4.04544780254364 and perplexity is 57.136766295546884
At time: 45.65526342391968 and batch: 500, loss is 3.9177563905715944 and perplexity is 50.287492622607445
At time: 46.274455547332764 and batch: 550, loss is 3.9966257047653198 and perplexity is 54.41423023002984
At time: 46.89262771606445 and batch: 600, loss is 4.005551552772522 and perplexity is 54.90209745377689
At time: 47.51205134391785 and batch: 650, loss is 3.8535939121246336 and perplexity is 47.162256021199255
At time: 48.13101625442505 and batch: 700, loss is 3.8726587677001953 and perplexity is 48.070023352138925
At time: 48.75044775009155 and batch: 750, loss is 3.968448052406311 and perplexity is 52.902365399257114
At time: 49.37032747268677 and batch: 800, loss is 3.924385690689087 and perplexity is 50.62197095703007
At time: 49.98826813697815 and batch: 850, loss is 4.005105738639831 and perplexity is 54.877626777909526
At time: 50.606427907943726 and batch: 900, loss is 3.95915780544281 and perplexity is 52.413165273250634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332699292326627 and perplexity of 76.14955932011758
finished 4 epochs...
Completing Train Step...
At time: 52.17022132873535 and batch: 50, loss is 4.045452923774719 and perplexity is 57.13705890687947
At time: 52.79124307632446 and batch: 100, loss is 3.9205624723434447 and perplexity is 50.428801608569295
At time: 53.41068243980408 and batch: 150, loss is 3.9234963846206665 and perplexity is 50.57697254270969
At time: 54.03009247779846 and batch: 200, loss is 3.830424580574036 and perplexity is 46.082099645543266
At time: 54.64892601966858 and batch: 250, loss is 3.984347128868103 and perplexity is 53.750186075314865
At time: 55.268433570861816 and batch: 300, loss is 3.953585214614868 and perplexity is 52.1219004527579
At time: 55.88764810562134 and batch: 350, loss is 3.9529284191131593 and perplexity is 52.08767826272049
At time: 56.50671672821045 and batch: 400, loss is 3.8745317888259887 and perplexity is 48.160143893887344
At time: 57.12538933753967 and batch: 450, loss is 3.9132437610626223 and perplexity is 50.06107505299662
At time: 57.74360704421997 and batch: 500, loss is 3.786468448638916 and perplexity is 44.10038218046372
At time: 58.369322299957275 and batch: 550, loss is 3.867997212409973 and perplexity is 47.84646375302746
At time: 58.987515926361084 and batch: 600, loss is 3.8854541635513304 and perplexity is 48.689050230802174
At time: 59.60128617286682 and batch: 650, loss is 3.7308874702453614 and perplexity is 41.71611355053899
At time: 60.21957731246948 and batch: 700, loss is 3.7438333892822264 and perplexity is 42.25967785723362
At time: 60.867910623550415 and batch: 750, loss is 3.8423101472854615 and perplexity is 46.6330793820484
At time: 61.52630090713501 and batch: 800, loss is 3.805212907791138 and perplexity is 44.934816067078245
At time: 62.16654586791992 and batch: 850, loss is 3.8858118295669555 and perplexity is 48.70646776404698
At time: 62.791380167007446 and batch: 900, loss is 3.847682957649231 and perplexity is 46.884304361990445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309668972067637 and perplexity of 74.41585113733294
finished 5 epochs...
Completing Train Step...
At time: 64.35157537460327 and batch: 50, loss is 3.9349584770202637 and perplexity is 51.160025595846655
At time: 64.97890329360962 and batch: 100, loss is 3.808166732788086 and perplexity is 45.06774187314577
At time: 65.59575486183167 and batch: 150, loss is 3.815053129196167 and perplexity is 45.37916727758242
At time: 66.21310639381409 and batch: 200, loss is 3.722390685081482 and perplexity is 41.36316229477581
At time: 66.82942795753479 and batch: 250, loss is 3.874362154006958 and perplexity is 48.1519749494817
At time: 67.44744277000427 and batch: 300, loss is 3.847052731513977 and perplexity is 46.854765956963426
At time: 68.06466388702393 and batch: 350, loss is 3.8472495174407957 and perplexity is 46.86398722278605
At time: 68.68190670013428 and batch: 400, loss is 3.776463580131531 and perplexity is 43.6613634798549
At time: 69.29989504814148 and batch: 450, loss is 3.812159872055054 and perplexity is 45.24806342782871
At time: 69.91767024993896 and batch: 500, loss is 3.6885874843597413 and perplexity is 39.98832291459303
At time: 70.536052942276 and batch: 550, loss is 3.7709033250808717 and perplexity is 43.41926884074226
At time: 71.15367603302002 and batch: 600, loss is 3.7879155492782592 and perplexity is 44.16424606930476
At time: 71.80303764343262 and batch: 650, loss is 3.636800184249878 and perplexity is 37.970144666209755
At time: 72.44922113418579 and batch: 700, loss is 3.6469865465164184 and perplexity is 38.35889894958738
At time: 73.0690667629242 and batch: 750, loss is 3.7467730236053467 and perplexity is 42.38408862824228
At time: 73.68760442733765 and batch: 800, loss is 3.711651201248169 and perplexity is 40.92132010723462
At time: 74.31836557388306 and batch: 850, loss is 3.7870963287353514 and perplexity is 44.12808062742183
At time: 74.95015478134155 and batch: 900, loss is 3.7536518287658693 and perplexity is 42.67664558336967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308972084358947 and perplexity of 74.36400971126801
finished 6 epochs...
Completing Train Step...
At time: 76.48972845077515 and batch: 50, loss is 3.8435795879364014 and perplexity is 46.692314898726174
At time: 77.11194658279419 and batch: 100, loss is 3.7164782333374022 and perplexity is 41.11932613879463
At time: 77.79912161827087 and batch: 150, loss is 3.7282779932022097 and perplexity is 41.60739821667357
At time: 78.46347427368164 and batch: 200, loss is 3.6357942533493044 and perplexity is 37.931968528888056
At time: 79.0940146446228 and batch: 250, loss is 3.7888525915145874 and perplexity is 44.2056492284322
At time: 79.71111369132996 and batch: 300, loss is 3.7563256359100343 and perplexity is 42.79090739216601
At time: 80.32808995246887 and batch: 350, loss is 3.760403347015381 and perplexity is 42.96575259228089
At time: 80.94500017166138 and batch: 400, loss is 3.6930730533599854 and perplexity is 40.16809618751584
At time: 81.56092500686646 and batch: 450, loss is 3.730269169807434 and perplexity is 41.69032843155871
At time: 82.18025135993958 and batch: 500, loss is 3.607461886405945 and perplexity is 36.87234774101963
At time: 82.79865980148315 and batch: 550, loss is 3.687644543647766 and perplexity is 39.9506340688793
At time: 83.41560769081116 and batch: 600, loss is 3.708760766983032 and perplexity is 40.80321049768324
At time: 84.0334005355835 and batch: 650, loss is 3.55691379070282 and perplexity is 35.054843446258104
At time: 84.65181851387024 and batch: 700, loss is 3.5660308980941773 and perplexity is 35.375903564415616
At time: 85.27106618881226 and batch: 750, loss is 3.668492374420166 and perplexity is 39.19277326907622
At time: 85.88990378379822 and batch: 800, loss is 3.6342697381973266 and perplexity is 37.87418472545485
At time: 86.5092339515686 and batch: 850, loss is 3.7103481340408324 and perplexity is 40.86803160371294
At time: 87.12963676452637 and batch: 900, loss is 3.6733580684661864 and perplexity is 39.38393801003792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317475619381422 and perplexity of 74.99906294423721
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.70322513580322 and batch: 50, loss is 3.794959154129028 and perplexity is 44.476419690326374
At time: 89.32263374328613 and batch: 100, loss is 3.6665531063079833 and perplexity is 39.11684162335384
At time: 89.94901371002197 and batch: 150, loss is 3.675257124900818 and perplexity is 39.45880139333286
At time: 90.59390902519226 and batch: 200, loss is 3.5676315307617186 and perplexity is 35.43257273246644
At time: 91.23730802536011 and batch: 250, loss is 3.7129631423950196 and perplexity is 40.975041702947266
At time: 91.87033677101135 and batch: 300, loss is 3.671658124923706 and perplexity is 39.31704441271884
At time: 92.49172616004944 and batch: 350, loss is 3.663646879196167 and perplexity is 39.0033242312595
At time: 93.11081194877625 and batch: 400, loss is 3.5884724855422974 and perplexity is 36.17877010261381
At time: 93.74535703659058 and batch: 450, loss is 3.6104726552963258 and perplexity is 36.983529145284415
At time: 94.36287260055542 and batch: 500, loss is 3.481501774787903 and perplexity is 32.50850589008217
At time: 94.98078536987305 and batch: 550, loss is 3.5426989936828615 and perplexity is 34.56007083932915
At time: 95.5985918045044 and batch: 600, loss is 3.561970763206482 and perplexity is 35.232563810447225
At time: 96.21534991264343 and batch: 650, loss is 3.3877601051330566 and perplexity is 29.599578019577965
At time: 96.83238053321838 and batch: 700, loss is 3.383428592681885 and perplexity is 29.471644351997984
At time: 97.4501805305481 and batch: 750, loss is 3.477593126296997 and perplexity is 32.381689569257134
At time: 98.06699419021606 and batch: 800, loss is 3.427474856376648 and perplexity is 30.79877314946637
At time: 98.68302083015442 and batch: 850, loss is 3.483222975730896 and perplexity is 32.56450774247044
At time: 99.30092430114746 and batch: 900, loss is 3.439913592338562 and perplexity is 31.184263492608764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279639675192637 and perplexity of 72.21441467376417
finished 8 epochs...
Completing Train Step...
At time: 100.84936428070068 and batch: 50, loss is 3.7015035581588744 and perplexity is 40.508164976815316
At time: 101.4743926525116 and batch: 100, loss is 3.570505256652832 and perplexity is 35.534542680781925
At time: 102.09061813354492 and batch: 150, loss is 3.579214482307434 and perplexity is 35.84537261080478
At time: 102.7074363231659 and batch: 200, loss is 3.475957293510437 and perplexity is 32.32876184203395
At time: 103.3251428604126 and batch: 250, loss is 3.6228694248199464 and perplexity is 37.44485902428195
At time: 103.94170713424683 and batch: 300, loss is 3.5867902994155885 and perplexity is 36.117961837215375
At time: 104.55966830253601 and batch: 350, loss is 3.5830600309371947 and perplexity is 35.98348311946925
At time: 105.18601560592651 and batch: 400, loss is 3.512732563018799 and perplexity is 33.53979227480337
At time: 105.8038444519043 and batch: 450, loss is 3.5382265043258667 and perplexity is 34.405846430917094
At time: 106.42090129852295 and batch: 500, loss is 3.41320912361145 and perplexity is 30.36252517912036
At time: 107.03700017929077 and batch: 550, loss is 3.477436370849609 and perplexity is 32.37661396084656
At time: 107.65344381332397 and batch: 600, loss is 3.503066158294678 and perplexity is 33.21714500020806
At time: 108.27150416374207 and batch: 650, loss is 3.334911723136902 and perplexity is 28.075904661610647
At time: 108.88768196105957 and batch: 700, loss is 3.335822467803955 and perplexity is 28.101486289451227
At time: 109.50306558609009 and batch: 750, loss is 3.4355787372589113 and perplexity is 31.049376797960303
At time: 110.12170505523682 and batch: 800, loss is 3.3897929430007934 and perplexity is 29.65981036319752
At time: 110.73890209197998 and batch: 850, loss is 3.452859721183777 and perplexity is 31.5906035759875
At time: 111.35600447654724 and batch: 900, loss is 3.4159237432479856 and perplexity is 30.44505986063571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.287979753050085 and perplexity of 72.81920701656743
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 112.90043950080872 and batch: 50, loss is 3.6706310987472532 and perplexity is 39.27668550729988
At time: 113.52781105041504 and batch: 100, loss is 3.552776508331299 and perplexity is 34.91011126583042
At time: 114.14539766311646 and batch: 150, loss is 3.566015377044678 and perplexity is 35.37535449752637
At time: 114.7640209197998 and batch: 200, loss is 3.4565541315078736 and perplexity is 31.707528078539152
At time: 115.38040113449097 and batch: 250, loss is 3.6064000701904297 and perplexity is 36.83321686286111
At time: 115.9982738494873 and batch: 300, loss is 3.564779782295227 and perplexity is 35.33167188780836
At time: 116.61542773246765 and batch: 350, loss is 3.5554035091400147 and perplexity is 35.00194072157476
At time: 117.23239040374756 and batch: 400, loss is 3.4872668600082397 and perplexity is 32.696461466304335
At time: 117.85065340995789 and batch: 450, loss is 3.5082639026641846 and perplexity is 33.390248713307166
At time: 118.46731472015381 and batch: 500, loss is 3.3773858642578123 and perplexity is 29.29409219895864
At time: 119.08480310440063 and batch: 550, loss is 3.4314105463027955 and perplexity is 30.92022641511309
At time: 119.70160031318665 and batch: 600, loss is 3.458205099105835 and perplexity is 31.759919416308577
At time: 120.31928730010986 and batch: 650, loss is 3.2866518306732178 and perplexity is 26.753139492827305
At time: 120.94551301002502 and batch: 700, loss is 3.280061297416687 and perplexity is 26.577401776015495
At time: 121.56378650665283 and batch: 750, loss is 3.3762766790390013 and perplexity is 29.261617638373796
At time: 122.18088865280151 and batch: 800, loss is 3.324309139251709 and perplexity is 27.779800038210368
At time: 122.79530835151672 and batch: 850, loss is 3.3838692045211793 and perplexity is 29.484632768637894
At time: 123.40776228904724 and batch: 900, loss is 3.3488309001922607 and perplexity is 28.469430574443482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.273530724930437 and perplexity of 71.77460516130652
finished 10 epochs...
Completing Train Step...
At time: 124.98484015464783 and batch: 50, loss is 3.6465717649459837 and perplexity is 38.34299168448918
At time: 125.60603547096252 and batch: 100, loss is 3.5182418060302734 and perplexity is 33.72508107285435
At time: 126.24842143058777 and batch: 150, loss is 3.530833239555359 and perplexity is 34.1524129035809
At time: 126.89246606826782 and batch: 200, loss is 3.421923441886902 and perplexity is 30.628270098375115
At time: 127.5138144493103 and batch: 250, loss is 3.57136013507843 and perplexity is 35.564933383010214
At time: 128.13577890396118 and batch: 300, loss is 3.532045078277588 and perplexity is 34.19382520744371
At time: 128.77360939979553 and batch: 350, loss is 3.5245489835739137 and perplexity is 33.93846335946718
At time: 129.42326712608337 and batch: 400, loss is 3.4582724237442015 and perplexity is 31.76205771337706
At time: 130.04919576644897 and batch: 450, loss is 3.4817008447647093 and perplexity is 32.51497800177591
At time: 130.67175817489624 and batch: 500, loss is 3.3529867839813234 and perplexity is 28.587992413420228
At time: 131.29607701301575 and batch: 550, loss is 3.408978509902954 and perplexity is 30.234344396743126
At time: 131.91863679885864 and batch: 600, loss is 3.4391009426116943 and perplexity is 31.15893190364743
At time: 132.5417013168335 and batch: 650, loss is 3.270729465484619 and perplexity is 26.33053955926694
At time: 133.1640157699585 and batch: 700, loss is 3.2677978706359863 and perplexity is 26.253462120246738
At time: 133.78743934631348 and batch: 750, loss is 3.367431631088257 and perplexity is 29.00393849937193
At time: 134.40942454338074 and batch: 800, loss is 3.3188405799865723 and perplexity is 27.628299178596887
At time: 135.0793673992157 and batch: 850, loss is 3.381501383781433 and perplexity is 29.414901032369865
At time: 135.70081400871277 and batch: 900, loss is 3.3489348554611205 and perplexity is 28.47239027558873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27524765223673 and perplexity of 71.89794279138057
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.24906420707703 and batch: 50, loss is 3.637713589668274 and perplexity is 38.00484264633427
At time: 137.87505054473877 and batch: 100, loss is 3.514082283973694 and perplexity is 33.58509219951124
At time: 138.49200582504272 and batch: 150, loss is 3.530647249221802 and perplexity is 34.14606147558361
At time: 139.1073899269104 and batch: 200, loss is 3.421921901702881 and perplexity is 30.628222925239253
At time: 139.7227966785431 and batch: 250, loss is 3.5718617820739746 and perplexity is 35.58277890068947
At time: 140.33946800231934 and batch: 300, loss is 3.5290702867507933 and perplexity is 34.092256853189156
At time: 140.95640873908997 and batch: 350, loss is 3.5180805635452272 and perplexity is 33.71964359536175
At time: 141.5741901397705 and batch: 400, loss is 3.4534232568740846 and perplexity is 31.608411025676396
At time: 142.19214749336243 and batch: 450, loss is 3.47526659488678 and perplexity is 32.30644012040373
At time: 142.80812048912048 and batch: 500, loss is 3.3449798822402954 and perplexity is 28.360005121146276
At time: 143.42566084861755 and batch: 550, loss is 3.3956382608413698 and perplexity is 29.83368907494295
At time: 144.04033017158508 and batch: 600, loss is 3.4269227027893066 and perplexity is 30.78177219038838
At time: 144.65615344047546 and batch: 650, loss is 3.2574291467666625 and perplexity is 25.982653616430895
At time: 145.27211213111877 and batch: 700, loss is 3.250950984954834 and perplexity is 25.814877808167658
At time: 145.88798832893372 and batch: 750, loss is 3.347586646080017 and perplexity is 28.434029396975568
At time: 146.50519466400146 and batch: 800, loss is 3.297421541213989 and perplexity is 27.04282014971487
At time: 147.12909054756165 and batch: 850, loss is 3.35805157661438 and perplexity is 28.733151957981455
At time: 147.76941108703613 and batch: 900, loss is 3.3253726053237913 and perplexity is 27.809358627532884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271851003986516 and perplexity of 71.65414505178268
finished 12 epochs...
Completing Train Step...
At time: 149.3193325996399 and batch: 50, loss is 3.6299407863616944 and perplexity is 37.71058357021316
At time: 149.94580936431885 and batch: 100, loss is 3.501324715614319 and perplexity is 33.15934958449282
At time: 150.56338596343994 and batch: 150, loss is 3.516831388473511 and perplexity is 33.677548154916636
At time: 151.18145370483398 and batch: 200, loss is 3.4087377691268923 and perplexity is 30.227066633271704
At time: 151.79828190803528 and batch: 250, loss is 3.5590924215316773 and perplexity is 35.13129826205962
At time: 152.42415475845337 and batch: 300, loss is 3.5171093130111695 and perplexity is 33.68690927269897
At time: 153.04218173027039 and batch: 350, loss is 3.5067315340042113 and perplexity is 33.33912172533171
At time: 153.6594216823578 and batch: 400, loss is 3.443105206489563 and perplexity is 31.28395062607215
At time: 154.33449745178223 and batch: 450, loss is 3.4658292818069456 and perplexity is 32.00298826774958
At time: 154.97392988204956 and batch: 500, loss is 3.336472725868225 and perplexity is 28.1197654499706
At time: 155.59240579605103 and batch: 550, loss is 3.388007164001465 and perplexity is 29.606891761254015
At time: 156.2094111442566 and batch: 600, loss is 3.4210684633255006 and perplexity is 30.602094775332706
At time: 156.82278084754944 and batch: 650, loss is 3.2529506158828734 and perplexity is 25.866549681356695
At time: 157.4359221458435 and batch: 700, loss is 3.248145432472229 and perplexity is 25.74255431476198
At time: 158.04898142814636 and batch: 750, loss is 3.346315507888794 and perplexity is 28.397908778291274
At time: 158.6620922088623 and batch: 800, loss is 3.297932186126709 and perplexity is 27.056632954669062
At time: 159.27499532699585 and batch: 850, loss is 3.3599812841415404 and perplexity is 28.788652069863172
At time: 159.8874614238739 and batch: 900, loss is 3.3288170289993286 and perplexity is 27.905310996230977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271614388243793 and perplexity of 71.63719255872455
finished 13 epochs...
Completing Train Step...
At time: 161.4382553100586 and batch: 50, loss is 3.6243693447113037 and perplexity is 37.50106545519584
At time: 162.0546109676361 and batch: 100, loss is 3.4947526168823244 and perplexity is 32.942137616211916
At time: 162.67290902137756 and batch: 150, loss is 3.5098037338256836 and perplexity is 33.44170366456782
At time: 163.28981733322144 and batch: 200, loss is 3.401912360191345 and perplexity is 30.021457025617938
At time: 163.90922904014587 and batch: 250, loss is 3.552169976234436 and perplexity is 34.88894358293112
At time: 164.52768087387085 and batch: 300, loss is 3.5104219245910646 and perplexity is 33.462383408306586
At time: 165.14463639259338 and batch: 350, loss is 3.5002520799636843 and perplexity is 33.1238007528571
At time: 165.76174354553223 and batch: 400, loss is 3.437167763710022 and perplexity is 31.098754299671047
At time: 166.3791217803955 and batch: 450, loss is 3.460228567123413 and perplexity is 31.82424966063471
At time: 166.99628734588623 and batch: 500, loss is 3.33141161441803 and perplexity is 27.97780771804489
At time: 167.6307442188263 and batch: 550, loss is 3.3833047103881837 and perplexity is 29.46799356323523
At time: 168.24795532226562 and batch: 600, loss is 3.4173279285430906 and perplexity is 30.487840394869824
At time: 168.86518144607544 and batch: 650, loss is 3.249818468093872 and perplexity is 25.785658572547767
At time: 169.48221945762634 and batch: 700, loss is 3.2458564233779907 and perplexity is 25.683696762262013
At time: 170.10033345222473 and batch: 750, loss is 3.3447625541687014 and perplexity is 28.353842365617044
At time: 170.71846294403076 and batch: 800, loss is 3.2971476030349733 and perplexity is 27.03541310339032
At time: 171.33498406410217 and batch: 850, loss is 3.3598141717910766 and perplexity is 28.783841532510387
At time: 171.9523754119873 and batch: 900, loss is 3.329158010482788 and perplexity is 27.914827813007808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272080512895976 and perplexity of 71.67059220378358
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 173.502836227417 and batch: 50, loss is 3.6220361518859865 and perplexity is 37.413670232933555
At time: 174.13034558296204 and batch: 100, loss is 3.493568811416626 and perplexity is 32.903163607024105
At time: 174.74770212173462 and batch: 150, loss is 3.509539885520935 and perplexity is 33.43288129168326
At time: 175.36656975746155 and batch: 200, loss is 3.401917567253113 and perplexity is 30.02161334960603
At time: 175.98348259925842 and batch: 250, loss is 3.5535651874542236 and perplexity is 34.937655001921854
At time: 176.59915113449097 and batch: 300, loss is 3.509057488441467 and perplexity is 33.41675725679631
At time: 177.21526217460632 and batch: 350, loss is 3.498068022727966 and perplexity is 33.051535420685255
At time: 177.83320903778076 and batch: 400, loss is 3.4355765438079833 and perplexity is 31.049308692750643
At time: 178.44831705093384 and batch: 450, loss is 3.4584181404113767 and perplexity is 31.766686311793457
At time: 179.0646574497223 and batch: 500, loss is 3.3286910343170164 and perplexity is 27.90179529692154
At time: 179.68108558654785 and batch: 550, loss is 3.379172124862671 and perplexity is 29.346465844372673
At time: 180.29799795150757 and batch: 600, loss is 3.413031187057495 and perplexity is 30.357123056652394
At time: 180.9154508113861 and batch: 650, loss is 3.2448783540725707 and perplexity is 25.65858860756842
At time: 181.5334792137146 and batch: 700, loss is 3.24002703666687 and perplexity is 25.53441210341278
At time: 182.15040469169617 and batch: 750, loss is 3.3362747955322267 and perplexity is 28.114200246126394
At time: 182.76772356033325 and batch: 800, loss is 3.2875456190109253 and perplexity is 26.777061826061743
At time: 183.39555287361145 and batch: 850, loss is 3.3504766178131105 and perplexity is 28.516321792263824
At time: 184.01279020309448 and batch: 900, loss is 3.3181142807006836 and perplexity is 27.608240049980328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271417905206549 and perplexity of 71.62311844825825
finished 15 epochs...
Completing Train Step...
At time: 185.57144570350647 and batch: 50, loss is 3.61955379486084 and perplexity is 37.32091132379164
At time: 186.1983323097229 and batch: 100, loss is 3.4904900693893435 and perplexity is 32.8020190333144
At time: 186.81363463401794 and batch: 150, loss is 3.505985131263733 and perplexity is 33.31424659809721
At time: 187.4294764995575 and batch: 200, loss is 3.39883713722229 and perplexity is 29.929276162408343
At time: 188.0459213256836 and batch: 250, loss is 3.550098114013672 and perplexity is 34.816733329478204
At time: 188.6624150276184 and batch: 300, loss is 3.50627863407135 and perplexity is 33.324025858057325
At time: 189.2791771888733 and batch: 350, loss is 3.495246090888977 and perplexity is 32.95839771648754
At time: 189.89540886878967 and batch: 400, loss is 3.4328122663497926 and perplexity is 30.96359830686042
At time: 190.513249874115 and batch: 450, loss is 3.4558160066604615 and perplexity is 31.68413259966871
At time: 191.13122010231018 and batch: 500, loss is 3.3265601110458376 and perplexity is 27.842402015752747
At time: 191.74870371818542 and batch: 550, loss is 3.3771708011627197 and perplexity is 29.287792798230996
At time: 192.36669731140137 and batch: 600, loss is 3.4115415668487548 and perplexity is 30.311936136698254
At time: 192.9840178489685 and batch: 650, loss is 3.243828077316284 and perplexity is 25.63165413515584
At time: 193.60180568695068 and batch: 700, loss is 3.239576749801636 and perplexity is 25.522916881299132
At time: 194.21780133247375 and batch: 750, loss is 3.3364583110809325 and perplexity is 28.119360112454352
At time: 194.86948919296265 and batch: 800, loss is 3.2883939218521117 and perplexity is 26.79978652103631
At time: 195.50428175926208 and batch: 850, loss is 3.351651334762573 and perplexity is 28.549840082206693
At time: 196.12055611610413 and batch: 900, loss is 3.319666094779968 and perplexity is 27.65111616486927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27123563583583 and perplexity of 71.6100649371935
finished 16 epochs...
Completing Train Step...
At time: 197.67868423461914 and batch: 50, loss is 3.617866668701172 and perplexity is 37.25799932315833
At time: 198.29651165008545 and batch: 100, loss is 3.4883762311935427 and perplexity is 32.73275410579854
At time: 198.92400741577148 and batch: 150, loss is 3.503715147972107 and perplexity is 33.238709581265226
At time: 199.54108810424805 and batch: 200, loss is 3.396699686050415 and perplexity is 29.865372116220698
At time: 200.15939807891846 and batch: 250, loss is 3.547925238609314 and perplexity is 34.74116303811324
At time: 200.77576279640198 and batch: 300, loss is 3.5042851114273073 and perplexity is 33.25765983098645
At time: 201.3923795223236 and batch: 350, loss is 3.4933077812194826 and perplexity is 32.894576008601135
At time: 202.01269626617432 and batch: 400, loss is 3.430974621772766 and perplexity is 30.906750467406166
At time: 202.6301715373993 and batch: 450, loss is 3.4541007471084595 and perplexity is 31.629832671128835
At time: 203.24859380722046 and batch: 500, loss is 3.3251031160354616 and perplexity is 27.801865312996796
At time: 203.86381816864014 and batch: 550, loss is 3.375824394226074 and perplexity is 29.248386045564075
At time: 204.4821071624756 and batch: 600, loss is 3.410543580055237 and perplexity is 30.281700314732895
At time: 205.10033798217773 and batch: 650, loss is 3.2430819845199585 and perplexity is 25.61253767486112
At time: 205.7179765701294 and batch: 700, loss is 3.239172782897949 and perplexity is 25.512608549846092
At time: 206.33488202095032 and batch: 750, loss is 3.336399097442627 and perplexity is 28.117695112131123
At time: 206.9534149169922 and batch: 800, loss is 3.2887165355682373 and perplexity is 26.80843389456289
At time: 207.57044339179993 and batch: 850, loss is 3.3521863126754763 and perplexity is 28.565117702297872
At time: 208.18744564056396 and batch: 900, loss is 3.320378646850586 and perplexity is 27.6708260462719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271249849502355 and perplexity of 71.61108278600999
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 209.7366065979004 and batch: 50, loss is 3.6170738172531127 and perplexity is 37.22847097178807
At time: 210.36130595207214 and batch: 100, loss is 3.48778067111969 and perplexity is 32.71326558821341
At time: 210.97542905807495 and batch: 150, loss is 3.503201723098755 and perplexity is 33.2216483812051
At time: 211.61140275001526 and batch: 200, loss is 3.3962950611114504 and perplexity is 29.853290286320696
At time: 212.26610159873962 and batch: 250, loss is 3.5479102754592895 and perplexity is 34.74064320476786
At time: 212.88189697265625 and batch: 300, loss is 3.503733696937561 and perplexity is 33.23932613065914
At time: 213.49830532073975 and batch: 350, loss is 3.4924588346481324 and perplexity is 32.86666212145849
At time: 214.15676927566528 and batch: 400, loss is 3.4302897596359254 and perplexity is 30.885590850775436
At time: 214.79805207252502 and batch: 450, loss is 3.453379168510437 and perplexity is 31.607017493276285
At time: 215.4296154975891 and batch: 500, loss is 3.323863916397095 and perplexity is 27.76743458923126
At time: 216.06172490119934 and batch: 550, loss is 3.3743333530426027 and perplexity is 29.204807993828556
At time: 216.69316148757935 and batch: 600, loss is 3.4088986110687256 and perplexity is 30.231928804374963
At time: 217.32351851463318 and batch: 650, loss is 3.241499810218811 and perplexity is 25.572046216683823
At time: 217.95475840568542 and batch: 700, loss is 3.236870889663696 and perplexity is 25.45394878899214
At time: 218.62053847312927 and batch: 750, loss is 3.3335382556915283 and perplexity is 28.037369789812196
At time: 219.2382025718689 and batch: 800, loss is 3.285589966773987 and perplexity is 26.724746377288874
At time: 219.85449481010437 and batch: 850, loss is 3.349204444885254 and perplexity is 28.480067165644517
At time: 220.469256401062 and batch: 900, loss is 3.316770305633545 and perplexity is 27.571160186492943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270929623956549 and perplexity of 71.588154759205
finished 18 epochs...
Completing Train Step...
At time: 222.01784873008728 and batch: 50, loss is 3.61646653175354 and perplexity is 37.205869524655796
At time: 222.6430585384369 and batch: 100, loss is 3.487179408073425 and perplexity is 32.69360222251254
At time: 223.26107597351074 and batch: 150, loss is 3.50246150970459 and perplexity is 33.19706637118603
At time: 223.87822794914246 and batch: 200, loss is 3.3956252670288087 and perplexity is 29.83330142409764
At time: 224.49536776542664 and batch: 250, loss is 3.5471799993515014 and perplexity is 34.71528220445962
At time: 225.10968232154846 and batch: 300, loss is 3.5031428289413453 and perplexity is 33.2196918778298
At time: 225.72509264945984 and batch: 350, loss is 3.491900334358215 and perplexity is 32.848311206107816
At time: 226.34038615226746 and batch: 400, loss is 3.4296969842910765 and perplexity is 30.86728805926572
At time: 226.95636463165283 and batch: 450, loss is 3.452824773788452 and perplexity is 31.58949958596668
At time: 227.57168984413147 and batch: 500, loss is 3.3234638738632203 and perplexity is 27.75632865590995
At time: 228.18920516967773 and batch: 550, loss is 3.373964762687683 and perplexity is 29.194045366906842
At time: 228.80639600753784 and batch: 600, loss is 3.4086286926269533 and perplexity is 30.223769750449414
At time: 229.42299818992615 and batch: 650, loss is 3.241287126541138 and perplexity is 25.566608038175236
At time: 230.0483250617981 and batch: 700, loss is 3.2369079208374023 and perplexity is 25.454891396044065
At time: 230.66337180137634 and batch: 750, loss is 3.333640818595886 and perplexity is 28.040245531358078
At time: 231.2792067527771 and batch: 800, loss is 3.285798416137695 and perplexity is 26.73031771431714
At time: 231.89536595344543 and batch: 850, loss is 3.34951934337616 and perplexity is 28.48903690802093
At time: 232.51284670829773 and batch: 900, loss is 3.3170656156539917 and perplexity is 27.579303428702683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270777036065924 and perplexity of 71.57723210702997
finished 19 epochs...
Completing Train Step...
At time: 234.07250881195068 and batch: 50, loss is 3.615959434509277 and perplexity is 37.18700731364126
At time: 234.68806910514832 and batch: 100, loss is 3.486634497642517 and perplexity is 32.675791990568804
At time: 235.30466222763062 and batch: 150, loss is 3.5018298625946045 and perplexity is 33.17610416120931
At time: 235.92133927345276 and batch: 200, loss is 3.3950434589385985 and perplexity is 29.81594921628834
At time: 236.53807497024536 and batch: 250, loss is 3.5465568017959597 and perplexity is 34.69365446532691
At time: 237.15510034561157 and batch: 300, loss is 3.5026183414459227 and perplexity is 33.20227313319427
At time: 237.77281498908997 and batch: 350, loss is 3.491400752067566 and perplexity is 32.83190487005022
At time: 238.39040899276733 and batch: 400, loss is 3.4291903257369993 and perplexity is 30.851652844921343
At time: 239.0094439983368 and batch: 450, loss is 3.4523550939559935 and perplexity is 31.574666118856555
At time: 239.6258430480957 and batch: 500, loss is 3.323105688095093 and perplexity is 27.746388514324835
At time: 240.24441289901733 and batch: 550, loss is 3.373632674217224 and perplexity is 29.18435197065608
At time: 240.8609402179718 and batch: 600, loss is 3.4083943605422973 and perplexity is 30.21668818122947
At time: 241.4774351119995 and batch: 650, loss is 3.241108775138855 and perplexity is 25.562048604382802
At time: 242.09384179115295 and batch: 700, loss is 3.2368970775604247 and perplexity is 25.45461538310267
At time: 242.71171617507935 and batch: 750, loss is 3.3337038564682007 and perplexity is 28.042013184489512
At time: 243.32901549339294 and batch: 800, loss is 3.285952916145325 and perplexity is 26.734447867654094
At time: 243.94569444656372 and batch: 850, loss is 3.3497568941116334 and perplexity is 28.4958053035782
At time: 244.56270813941956 and batch: 900, loss is 3.317303042411804 and perplexity is 27.585852270702883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270702623341181 and perplexity of 71.57190604832506
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f67603d0b70>
ELAPSED
756.9838180541992


RESULTS SO FAR:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.03606636022589, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.57190604832506, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.17348567797130798, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.17852101364501027, 'wordvec_source': 'glove', 'num_layers': 3}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'dropout': 0.2729052577440735, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.7700012466142289, 'wordvec_source': 'glove', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0006873607635498 and batch: 50, loss is 7.009526109695434 and perplexity is 1107.1297225244834
At time: 1.7859454154968262 and batch: 100, loss is 6.13353123664856 and perplexity is 461.0614063629092
At time: 2.5725765228271484 and batch: 150, loss is 5.945739545822144 and perplexity is 382.1218534471138
At time: 3.359863042831421 and batch: 200, loss is 5.739094142913818 and perplexity is 310.78275866807815
At time: 4.1626176834106445 and batch: 250, loss is 5.770885105133057 and perplexity is 320.821567833321
At time: 4.955327749252319 and batch: 300, loss is 5.663486642837524 and perplexity is 288.1515747310427
At time: 5.743821382522583 and batch: 350, loss is 5.627673330307007 and perplexity is 278.0145165983514
At time: 6.548011779785156 and batch: 400, loss is 5.472621917724609 and perplexity is 238.08361076767892
At time: 7.34382700920105 and batch: 450, loss is 5.459423818588257 and perplexity is 234.96200461628814
At time: 8.138065576553345 and batch: 500, loss is 5.401254005432129 and perplexity is 221.68423520967985
At time: 8.923405170440674 and batch: 550, loss is 5.4442220973968505 and perplexity is 231.4171896407033
At time: 9.705490589141846 and batch: 600, loss is 5.3650554656982425 and perplexity is 213.80309308991423
At time: 10.48639440536499 and batch: 650, loss is 5.254931287765503 and perplexity is 191.50832643724152
At time: 11.269840955734253 and batch: 700, loss is 5.342514553070068 and perplexity is 209.03768632834002
At time: 12.069865226745605 and batch: 750, loss is 5.3218754863739015 and perplexity is 204.76756085846878
At time: 12.868302583694458 and batch: 800, loss is 5.302896385192871 and perplexity is 200.91790367414777
At time: 13.652821779251099 and batch: 850, loss is 5.338843297958374 and perplexity is 208.271662648425
At time: 14.435286283493042 and batch: 900, loss is 5.238307867050171 and perplexity is 188.3511174673129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.181881421232877 and perplexity of 178.01742187999488
finished 1 epochs...
Completing Train Step...
At time: 16.093689918518066 and batch: 50, loss is 5.120057010650635 and perplexity is 167.34490979134353
At time: 16.714632272720337 and batch: 100, loss is 4.980161561965942 and perplexity is 145.49788667941212
At time: 17.329517602920532 and batch: 150, loss is 4.943949308395386 and perplexity is 140.3233368172129
At time: 17.940789222717285 and batch: 200, loss is 4.820034694671631 and perplexity is 123.96939178255273
At time: 18.559220790863037 and batch: 250, loss is 4.907760906219482 and perplexity is 135.3360448025154
At time: 19.164245128631592 and batch: 300, loss is 4.8454319095611575 and perplexity is 127.1581909734876
At time: 19.768040895462036 and batch: 350, loss is 4.827586078643799 and perplexity is 124.90907575260714
At time: 20.37543511390686 and batch: 400, loss is 4.684002923965454 and perplexity is 108.20233255238445
At time: 20.984744548797607 and batch: 450, loss is 4.694470109939576 and perplexity is 109.34085465646766
At time: 21.590730905532837 and batch: 500, loss is 4.598557929992676 and perplexity is 99.34095568653728
At time: 22.195816040039062 and batch: 550, loss is 4.658396139144897 and perplexity is 105.46679237451903
At time: 22.803255081176758 and batch: 600, loss is 4.611181917190552 and perplexity is 100.60298379245499
At time: 23.432795524597168 and batch: 650, loss is 4.475202169418335 and perplexity is 87.81235158736474
At time: 24.042106866836548 and batch: 700, loss is 4.523384485244751 and perplexity is 92.14694077481157
At time: 24.71102285385132 and batch: 750, loss is 4.564969053268433 and perplexity is 96.05962123921707
At time: 25.32506775856018 and batch: 800, loss is 4.513995733261108 and perplexity is 91.28584463622859
At time: 25.9395170211792 and batch: 850, loss is 4.5708505821228025 and perplexity is 96.62626340092989
At time: 26.5513334274292 and batch: 900, loss is 4.496142101287842 and perplexity is 89.67052334469004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.615281196489726 and perplexity of 101.01622994837143
finished 2 epochs...
Completing Train Step...
At time: 28.10491442680359 and batch: 50, loss is 4.549841337203979 and perplexity is 94.61739486349623
At time: 28.718263864517212 and batch: 100, loss is 4.4253698825836185 and perplexity is 83.54370286847025
At time: 29.331213235855103 and batch: 150, loss is 4.417659645080566 and perplexity is 82.9020379507187
At time: 29.943049430847168 and batch: 200, loss is 4.315469512939453 and perplexity is 74.8487576555353
At time: 30.55562949180603 and batch: 250, loss is 4.452821283340454 and perplexity is 85.86886297332386
At time: 31.204456567764282 and batch: 300, loss is 4.418108448982239 and perplexity is 82.93925305932589
At time: 31.839689016342163 and batch: 350, loss is 4.417381591796875 and perplexity is 82.87898997127348
At time: 32.459065198898315 and batch: 400, loss is 4.311390047073364 and perplexity is 74.54403667557675
At time: 33.11628556251526 and batch: 450, loss is 4.344795045852661 and perplexity is 77.07623876555517
At time: 33.78804969787598 and batch: 500, loss is 4.225884909629822 and perplexity is 68.435035583923
At time: 34.426778078079224 and batch: 550, loss is 4.306604490280152 and perplexity is 74.18815418160203
At time: 35.06229543685913 and batch: 600, loss is 4.2968018436431885 and perplexity is 73.46446674266234
At time: 35.70010685920715 and batch: 650, loss is 4.14376953125 and perplexity is 63.04000540255197
At time: 36.33053183555603 and batch: 700, loss is 4.177034888267517 and perplexity is 65.17232321656546
At time: 36.974273920059204 and batch: 750, loss is 4.262822065353394 and perplexity is 71.0100960879134
At time: 37.616071462631226 and batch: 800, loss is 4.21316885471344 and perplexity is 67.57032144134661
At time: 38.22232103347778 and batch: 850, loss is 4.287684273719788 and perplexity is 72.79769362458323
At time: 38.82854700088501 and batch: 900, loss is 4.22336067199707 and perplexity is 68.26250713471481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452234555597174 and perplexity of 85.81849610639748
finished 3 epochs...
Completing Train Step...
At time: 40.36790633201599 and batch: 50, loss is 4.310266494750977 and perplexity is 74.46032958350439
At time: 40.97364640235901 and batch: 100, loss is 4.181391115188599 and perplexity is 65.45684792261783
At time: 41.58000445365906 and batch: 150, loss is 4.176143198013306 and perplexity is 65.11423559304167
At time: 42.186556339263916 and batch: 200, loss is 4.076885576248169 and perplexity is 58.96155234095871
At time: 42.79341745376587 and batch: 250, loss is 4.226285824775696 and perplexity is 68.46247772681478
At time: 43.4009211063385 and batch: 300, loss is 4.2003183269500735 and perplexity is 66.70756247638676
At time: 44.01022529602051 and batch: 350, loss is 4.199793257713318 and perplexity is 66.67254558142311
At time: 44.62071990966797 and batch: 400, loss is 4.105244913101196 and perplexity is 60.657598569136205
At time: 45.231099128723145 and batch: 450, loss is 4.147586255073548 and perplexity is 63.28107144161395
At time: 45.84173274040222 and batch: 500, loss is 4.01927396774292 and perplexity is 55.660679704320586
At time: 46.451427936553955 and batch: 550, loss is 4.102403621673584 and perplexity is 60.4854972650624
At time: 47.06184101104736 and batch: 600, loss is 4.111100282669067 and perplexity is 61.01381309195185
At time: 47.68086218833923 and batch: 650, loss is 3.950993800163269 and perplexity is 51.987005866060095
At time: 48.29084253311157 and batch: 700, loss is 3.9762185335159304 and perplexity is 53.31504350749891
At time: 48.900532960891724 and batch: 750, loss is 4.075792031288147 and perplexity is 58.89711047402243
At time: 49.51171398162842 and batch: 800, loss is 4.030584421157837 and perplexity is 56.29380092379518
At time: 50.12467408180237 and batch: 850, loss is 4.108826332092285 and perplexity is 60.87522832365339
At time: 50.73352408409119 and batch: 900, loss is 4.052131786346435 and perplexity is 57.51994667516792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37306150671554 and perplexity of 79.28599511814767
finished 4 epochs...
Completing Train Step...
At time: 52.271849155426025 and batch: 50, loss is 4.147315988540649 and perplexity is 63.26397099678039
At time: 52.891236305236816 and batch: 100, loss is 4.018445229530334 and perplexity is 55.614570680904954
At time: 53.50205731391907 and batch: 150, loss is 4.019484133720398 and perplexity is 55.67237891482267
At time: 54.113093852996826 and batch: 200, loss is 3.9200405406951906 and perplexity is 50.40248808855304
At time: 54.72369360923767 and batch: 250, loss is 4.071968560218811 and perplexity is 58.67234903430292
At time: 55.334821939468384 and batch: 300, loss is 4.051191515922547 and perplexity is 57.46588778949411
At time: 55.9454505443573 and batch: 350, loss is 4.051594533920288 and perplexity is 57.48905224406198
At time: 56.55626130104065 and batch: 400, loss is 3.959739718437195 and perplexity is 52.443674051066196
At time: 57.16620087623596 and batch: 450, loss is 4.008102884292603 and perplexity is 55.04234974449572
At time: 57.77665567398071 and batch: 500, loss is 3.875238952636719 and perplexity is 48.19421304958586
At time: 58.38724422454834 and batch: 550, loss is 3.9582051420211792 and perplexity is 52.363256944577095
At time: 58.99859809875488 and batch: 600, loss is 3.977531728744507 and perplexity is 53.38510255878228
At time: 59.607380628585815 and batch: 650, loss is 3.8154275035858154 and perplexity is 45.39615925611706
At time: 60.21760892868042 and batch: 700, loss is 3.8340460014343263 and perplexity is 46.24928486378893
At time: 60.831000089645386 and batch: 750, loss is 3.9435272121429445 and perplexity is 51.600285847011804
At time: 61.44602108001709 and batch: 800, loss is 3.900916814804077 and perplexity is 49.44776277117351
At time: 62.070632457733154 and batch: 850, loss is 3.9757828426361086 and perplexity is 53.29181968885576
At time: 62.69440746307373 and batch: 900, loss is 3.9265762996673583 and perplexity is 50.73298545138597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3421129200556505 and perplexity of 76.86978758500815
finished 5 epochs...
Completing Train Step...
At time: 64.30770444869995 and batch: 50, loss is 4.0256357002258305 and perplexity is 56.01590679021092
At time: 64.91628360748291 and batch: 100, loss is 3.898809037208557 and perplexity is 49.343647648963056
At time: 65.52653503417969 and batch: 150, loss is 3.8992837858200073 and perplexity is 49.36707903873808
At time: 66.13595533370972 and batch: 200, loss is 3.8038235235214235 and perplexity is 44.872427691222306
At time: 66.7473795413971 and batch: 250, loss is 3.9573299169540403 and perplexity is 52.31744735927511
At time: 67.35710573196411 and batch: 300, loss is 3.936501545906067 and perplexity is 51.23902997846663
At time: 67.96796131134033 and batch: 350, loss is 3.936763286590576 and perplexity is 51.252443072546434
At time: 68.61286997795105 and batch: 400, loss is 3.8472088193893432 and perplexity is 46.86207998863341
At time: 69.29287767410278 and batch: 450, loss is 3.8976939964294433 and perplexity is 49.28865813311798
At time: 69.92723488807678 and batch: 500, loss is 3.7656028032302857 and perplexity is 43.189732924994786
At time: 70.54915690422058 and batch: 550, loss is 3.845551152229309 and perplexity is 46.78446260725092
At time: 71.21589612960815 and batch: 600, loss is 3.870986862182617 and perplexity is 47.98972196174053
At time: 71.85502600669861 and batch: 650, loss is 3.708040919303894 and perplexity is 40.77384897048658
At time: 72.46791887283325 and batch: 700, loss is 3.7284980392456055 and perplexity is 41.61655476742158
At time: 73.07950854301453 and batch: 750, loss is 3.837459774017334 and perplexity is 46.40743920232091
At time: 73.68936109542847 and batch: 800, loss is 3.798586854934692 and perplexity is 44.63805984766632
At time: 74.29833483695984 and batch: 850, loss is 3.8732481241226195 and perplexity is 48.0983620791127
At time: 74.90918350219727 and batch: 900, loss is 3.828529505729675 and perplexity is 45.994853313005414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326832810493364 and perplexity of 75.704137121828
finished 6 epochs...
Completing Train Step...
At time: 76.49986028671265 and batch: 50, loss is 3.9266878271102907 and perplexity is 50.73864388705526
At time: 77.11813068389893 and batch: 100, loss is 3.799775466918945 and perplexity is 44.691148725342856
At time: 77.72846031188965 and batch: 150, loss is 3.8041455936431885 and perplexity is 44.88688208701227
At time: 78.33855652809143 and batch: 200, loss is 3.7096478509902955 and perplexity is 40.839422432300125
At time: 78.96798062324524 and batch: 250, loss is 3.8655973625183107 and perplexity is 47.73177709260083
At time: 79.57900905609131 and batch: 300, loss is 3.842911539077759 and perplexity is 46.66113256787066
At time: 80.18965148925781 and batch: 350, loss is 3.8447457361221313 and perplexity is 46.74679681785425
At time: 80.80005359649658 and batch: 400, loss is 3.757477293014526 and perplexity is 42.840216232660175
At time: 81.41066884994507 and batch: 450, loss is 3.8082990264892578 and perplexity is 45.07370444591833
At time: 82.02097296714783 and batch: 500, loss is 3.67693660736084 and perplexity is 39.525127439291595
At time: 82.63621115684509 and batch: 550, loss is 3.7576186800003053 and perplexity is 42.84627370991757
At time: 83.26739740371704 and batch: 600, loss is 3.7833884286880495 and perplexity is 43.96476108855584
At time: 83.88324666023254 and batch: 650, loss is 3.6230070066452025 and perplexity is 37.45001111074158
At time: 84.49812340736389 and batch: 700, loss is 3.6416835403442382 and perplexity is 38.15601988161925
At time: 85.11377334594727 and batch: 750, loss is 3.7482777309417723 and perplexity is 42.447912283267726
At time: 85.72831511497498 and batch: 800, loss is 3.713779525756836 and perplexity is 41.00850670351947
At time: 86.34452939033508 and batch: 850, loss is 3.790100073814392 and perplexity is 44.26082940438644
At time: 86.96106934547424 and batch: 900, loss is 3.7444175148010252 and perplexity is 42.28437002444665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325184861274614 and perplexity of 75.57948328800656
finished 7 epochs...
Completing Train Step...
At time: 88.63918972015381 and batch: 50, loss is 3.8465072298049927 and perplexity is 46.82921357212828
At time: 89.27447843551636 and batch: 100, loss is 3.718448257446289 and perplexity is 41.20041204700176
At time: 89.89529085159302 and batch: 150, loss is 3.7227449226379394 and perplexity is 41.37781727583355
At time: 90.51543068885803 and batch: 200, loss is 3.6308908796310426 and perplexity is 37.74642916748035
At time: 91.16376733779907 and batch: 250, loss is 3.784468994140625 and perplexity is 44.01229356687084
At time: 91.81162238121033 and batch: 300, loss is 3.7642894315719606 and perplexity is 43.13304598749751
At time: 92.42487907409668 and batch: 350, loss is 3.763830723762512 and perplexity is 43.113265059637634
At time: 93.03966093063354 and batch: 400, loss is 3.680274066925049 and perplexity is 39.65726132699482
At time: 93.6545512676239 and batch: 450, loss is 3.730985903739929 and perplexity is 41.720220015479086
At time: 94.2800817489624 and batch: 500, loss is 3.6039930248260497 and perplexity is 36.7446642567813
At time: 94.89549422264099 and batch: 550, loss is 3.681625175476074 and perplexity is 39.7108788052441
At time: 95.51173043251038 and batch: 600, loss is 3.7121534395217894 and perplexity is 40.94187752232776
At time: 96.12834072113037 and batch: 650, loss is 3.550571904182434 and perplexity is 34.83323306383543
At time: 96.74432182312012 and batch: 700, loss is 3.569468283653259 and perplexity is 35.49771341824545
At time: 97.36104655265808 and batch: 750, loss is 3.676382327079773 and perplexity is 39.50322551100955
At time: 97.99299621582031 and batch: 800, loss is 3.6395517778396607 and perplexity is 38.074766945863175
At time: 98.64068102836609 and batch: 850, loss is 3.7172950172424315 and perplexity is 41.152925462395864
At time: 99.25700259208679 and batch: 900, loss is 3.672373900413513 and perplexity is 39.34519666358475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331893293824915 and perplexity of 76.0882076174111
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.81709408760071 and batch: 50, loss is 3.7987980318069456 and perplexity is 44.647487368930605
At time: 101.4421877861023 and batch: 100, loss is 3.6672736406326294 and perplexity is 39.14503680699418
At time: 102.05708956718445 and batch: 150, loss is 3.677738571166992 and perplexity is 39.5568378745426
At time: 102.67202281951904 and batch: 200, loss is 3.566509838104248 and perplexity is 35.39285055799772
At time: 103.28365540504456 and batch: 250, loss is 3.715405111312866 and perplexity is 41.075223752134555
At time: 103.90412783622742 and batch: 300, loss is 3.680952262878418 and perplexity is 39.68416584338434
At time: 104.5343885421753 and batch: 350, loss is 3.669449586868286 and perplexity is 39.23030704055649
At time: 105.19195532798767 and batch: 400, loss is 3.5810321950912476 and perplexity is 35.91058845672189
At time: 105.81869578361511 and batch: 450, loss is 3.6188667821884155 and perplexity is 37.29528019023067
At time: 106.43521285057068 and batch: 500, loss is 3.4835957956314085 and perplexity is 32.57665070243513
At time: 107.05056810379028 and batch: 550, loss is 3.546364984512329 and perplexity is 34.687000260984256
At time: 107.66721868515015 and batch: 600, loss is 3.5651529026031494 and perplexity is 35.34485731182432
At time: 108.28274440765381 and batch: 650, loss is 3.393869423866272 and perplexity is 29.780964786685804
At time: 108.89612317085266 and batch: 700, loss is 3.3990211963653563 and perplexity is 29.93478542633104
At time: 109.50624537467957 and batch: 750, loss is 3.490003991127014 and perplexity is 32.78607855937458
At time: 110.14423131942749 and batch: 800, loss is 3.4380373573303222 and perplexity is 31.125809339749548
At time: 110.75609564781189 and batch: 850, loss is 3.4985972595214845 and perplexity is 33.06903213885728
At time: 111.36713743209839 and batch: 900, loss is 3.4457407855987547 and perplexity is 31.366510701810228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.288636508053297 and perplexity of 72.86704710299394
finished 9 epochs...
Completing Train Step...
At time: 112.9191780090332 and batch: 50, loss is 3.713364863395691 and perplexity is 40.991505544416746
At time: 113.58763670921326 and batch: 100, loss is 3.575506691932678 and perplexity is 35.71271157489701
At time: 114.19775295257568 and batch: 150, loss is 3.587654218673706 and perplexity is 36.14917832233027
At time: 114.80784225463867 and batch: 200, loss is 3.4791778707504273 and perplexity is 32.433046955612994
At time: 115.42002415657043 and batch: 250, loss is 3.6300859117507933 and perplexity is 37.71605673046459
At time: 116.03094148635864 and batch: 300, loss is 3.6021370315551757 and perplexity is 36.67652965541968
At time: 116.66634368896484 and batch: 350, loss is 3.5931616401672364 and perplexity is 36.34881632469578
At time: 117.34119772911072 and batch: 400, loss is 3.5076266145706176 and perplexity is 33.36897628441372
At time: 118.00541067123413 and batch: 450, loss is 3.5500616216659546 and perplexity is 34.815462808321456
At time: 118.68536949157715 and batch: 500, loss is 3.419933309555054 and perplexity is 30.567376401139704
At time: 119.31032991409302 and batch: 550, loss is 3.4847850465774535 and perplexity is 32.615415561105586
At time: 119.92538738250732 and batch: 600, loss is 3.509729151725769 and perplexity is 33.439209605091044
At time: 120.5417013168335 and batch: 650, loss is 3.3417794704437256 and perplexity is 28.269386511986422
At time: 121.15621757507324 and batch: 700, loss is 3.353247294425964 and perplexity is 28.595440854191416
At time: 121.77103877067566 and batch: 750, loss is 3.4505184268951417 and perplexity is 31.516727193184156
At time: 122.387535572052 and batch: 800, loss is 3.4026731348037718 and perplexity is 30.044305278034273
At time: 123.03252363204956 and batch: 850, loss is 3.4689051723480224 and perplexity is 32.10157750376757
At time: 123.68002915382385 and batch: 900, loss is 3.4240044450759886 and perplexity is 30.692073991152593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.296507952964469 and perplexity of 73.44287939298539
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.23883867263794 and batch: 50, loss is 3.6868320751190184 and perplexity is 39.91818861823338
At time: 125.86031174659729 and batch: 100, loss is 3.559808192253113 and perplexity is 35.15645321827824
At time: 126.47354459762573 and batch: 150, loss is 3.5771910429000853 and perplexity is 35.77291500280465
At time: 127.08643913269043 and batch: 200, loss is 3.463537502288818 and perplexity is 31.929728454453404
At time: 127.69986271858215 and batch: 250, loss is 3.614451832771301 and perplexity is 37.13098635604227
At time: 128.31268167495728 and batch: 300, loss is 3.582826738357544 and perplexity is 35.97508941899957
At time: 128.92447876930237 and batch: 350, loss is 3.570142822265625 and perplexity is 35.521666074184196
At time: 129.53714156150818 and batch: 400, loss is 3.488061728477478 and perplexity is 32.7224611843887
At time: 130.15021896362305 and batch: 450, loss is 3.5204850912094114 and perplexity is 33.80082096872091
At time: 130.76448774337769 and batch: 500, loss is 3.3865877628326415 and perplexity is 29.564897514874545
At time: 131.37774682044983 and batch: 550, loss is 3.443309154510498 and perplexity is 31.290331576558362
At time: 131.98918676376343 and batch: 600, loss is 3.4721854543685913 and perplexity is 32.207052630788255
At time: 132.59966158866882 and batch: 650, loss is 3.296592798233032 and perplexity is 27.020417886471904
At time: 133.21264338493347 and batch: 700, loss is 3.305186638832092 and perplexity is 27.253627699397548
At time: 133.82527685165405 and batch: 750, loss is 3.3938448429107666 and perplexity is 29.78023275111258
At time: 134.4375603199005 and batch: 800, loss is 3.341992349624634 and perplexity is 28.27540511642694
At time: 135.04969263076782 and batch: 850, loss is 3.406377658843994 and perplexity is 30.155811540502196
At time: 135.66226840019226 and batch: 900, loss is 3.362314019203186 and perplexity is 28.855886757792025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.282377896243578 and perplexity of 72.41242467806545
finished 11 epochs...
Completing Train Step...
At time: 137.21875405311584 and batch: 50, loss is 3.6613937425613403 and perplexity is 38.91554334085991
At time: 137.84117078781128 and batch: 100, loss is 3.52524619102478 and perplexity is 33.962133759623185
At time: 138.4540936946869 and batch: 150, loss is 3.5414813566207886 and perplexity is 34.5180148258838
At time: 139.0671706199646 and batch: 200, loss is 3.4285986757278444 and perplexity is 30.833404862974852
At time: 139.6796793937683 and batch: 250, loss is 3.580966582298279 and perplexity is 35.908232340012646
At time: 140.29399847984314 and batch: 300, loss is 3.55126145362854 and perplexity is 34.85726058353008
At time: 140.90731000900269 and batch: 350, loss is 3.5402000379562377 and perplexity is 34.473814572576174
At time: 141.52332735061646 and batch: 400, loss is 3.459240684509277 and perplexity is 31.792826561409267
At time: 142.12926959991455 and batch: 450, loss is 3.494739446640015 and perplexity is 32.94170376313429
At time: 142.73800253868103 and batch: 500, loss is 3.3636081266403197 and perplexity is 28.89325354855639
At time: 143.36643934249878 and batch: 550, loss is 3.422525644302368 and perplexity is 30.646720071361493
At time: 144.00209879875183 and batch: 600, loss is 3.454201855659485 and perplexity is 31.63303087935976
At time: 144.6556339263916 and batch: 650, loss is 3.2813132429122924 and perplexity is 26.610696071426755
At time: 145.33077597618103 and batch: 700, loss is 3.2934890604019165 and perplexity is 26.936683605112517
At time: 145.94502186775208 and batch: 750, loss is 3.384896035194397 and perplexity is 29.514924043296723
At time: 146.55739068984985 and batch: 800, loss is 3.3364686298370363 and perplexity is 28.119650270770183
At time: 147.16875624656677 and batch: 850, loss is 3.40395498752594 and perplexity is 30.08284234668691
At time: 147.78278827667236 and batch: 900, loss is 3.362502946853638 and perplexity is 28.86133894769737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.284140390892551 and perplexity of 72.54016372570639
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 149.35441374778748 and batch: 50, loss is 3.6534944009780883 and perplexity is 38.60934713752177
At time: 149.96664881706238 and batch: 100, loss is 3.5240532255172727 and perplexity is 33.92164226276833
At time: 150.57876205444336 and batch: 150, loss is 3.5418741846084596 and perplexity is 34.53157713183853
At time: 151.1917815208435 and batch: 200, loss is 3.4292420530319214 and perplexity is 30.853248758742684
At time: 151.8041524887085 and batch: 250, loss is 3.581793508529663 and perplexity is 35.93793807979221
At time: 152.41706466674805 and batch: 300, loss is 3.5506066513061523 and perplexity is 34.83444343952262
At time: 153.03076481819153 and batch: 350, loss is 3.537568383216858 and perplexity is 34.38321066646181
At time: 153.64366745948792 and batch: 400, loss is 3.4575352478027344 and perplexity is 31.738652116660603
At time: 154.25573801994324 and batch: 450, loss is 3.4878040885925294 and perplexity is 32.7140316591915
At time: 154.868008852005 and batch: 500, loss is 3.3545948314666747 and perplexity is 28.634000244206185
At time: 155.47982621192932 and batch: 550, loss is 3.4106454038619995 and perplexity is 30.284783869721178
At time: 156.09225988388062 and batch: 600, loss is 3.4444426107406616 and perplexity is 31.325817905128627
At time: 156.7134108543396 and batch: 650, loss is 3.2675195026397703 and perplexity is 26.246155013682085
At time: 157.32465362548828 and batch: 700, loss is 3.278053698539734 and perplexity is 26.52409853772132
At time: 157.93723797798157 and batch: 750, loss is 3.368716449737549 and perplexity is 29.04122724997037
At time: 158.55013489723206 and batch: 800, loss is 3.316739602088928 and perplexity is 27.57031366714165
At time: 159.16372561454773 and batch: 850, loss is 3.380924482345581 and perplexity is 29.397936427651768
At time: 159.77562546730042 and batch: 900, loss is 3.339886269569397 and perplexity is 28.215917514471958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.281094067717252 and perplexity of 72.31951919172077
finished 13 epochs...
Completing Train Step...
At time: 161.31467938423157 and batch: 50, loss is 3.6427431535720824 and perplexity is 38.196471932987166
At time: 161.9341959953308 and batch: 100, loss is 3.5106010103225707 and perplexity is 33.46837658034698
At time: 162.54702711105347 and batch: 150, loss is 3.527367033958435 and perplexity is 34.03423854545476
At time: 163.15995836257935 and batch: 200, loss is 3.41533477306366 and perplexity is 30.427133907561913
At time: 163.77246689796448 and batch: 250, loss is 3.5683550310134886 and perplexity is 35.45821748363255
At time: 164.38519430160522 and batch: 300, loss is 3.5381474256515504 and perplexity is 34.40312576976716
At time: 164.9980969429016 and batch: 350, loss is 3.5257786083221436 and perplexity is 33.980220601538676
At time: 165.6110463142395 and batch: 400, loss is 3.4459683799743654 and perplexity is 31.373650355670204
At time: 166.22369980812073 and batch: 450, loss is 3.4789813423156737 and perplexity is 32.42667356595718
At time: 166.8352837562561 and batch: 500, loss is 3.3468653678894045 and perplexity is 28.413527946213755
At time: 167.4471893310547 and batch: 550, loss is 3.403465828895569 and perplexity is 30.06813066319376
At time: 168.05723118782043 and batch: 600, loss is 3.4389024496078493 and perplexity is 31.152747687439405
At time: 168.66823148727417 and batch: 650, loss is 3.263500952720642 and perplexity is 26.140895167199357
At time: 169.31484055519104 and batch: 700, loss is 3.2755300998687744 and perplexity is 26.457246746927403
At time: 169.9510419368744 and batch: 750, loss is 3.3673048067092894 and perplexity is 29.000260326130082
At time: 170.5628662109375 and batch: 800, loss is 3.317219099998474 and perplexity is 27.583536744876287
At time: 171.17727780342102 and batch: 850, loss is 3.383122401237488 and perplexity is 29.46262176803459
At time: 171.79061818122864 and batch: 900, loss is 3.343283076286316 and perplexity is 28.31192449888084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.281141307255993 and perplexity of 72.32293561314383
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 173.35837602615356 and batch: 50, loss is 3.6396447086334227 and perplexity is 38.078305428592216
At time: 173.98154044151306 and batch: 100, loss is 3.5092426252365114 and perplexity is 33.4229445008612
At time: 174.59452438354492 and batch: 150, loss is 3.5261227798461916 and perplexity is 33.99191763862867
At time: 175.20741033554077 and batch: 200, loss is 3.4145797300338745 and perplexity is 30.40416878310825
At time: 175.82153844833374 and batch: 250, loss is 3.568219623565674 and perplexity is 35.453416501950706
At time: 176.4339723587036 and batch: 300, loss is 3.5368439388275146 and perplexity is 34.3583109627212
At time: 177.04733419418335 and batch: 350, loss is 3.5244466066360474 and perplexity is 33.93498902136163
At time: 177.6603455543518 and batch: 400, loss is 3.444291319847107 and perplexity is 31.321078952635645
At time: 178.27374935150146 and batch: 450, loss is 3.4764108419418336 and perplexity is 32.3434278268722
At time: 178.88736271858215 and batch: 500, loss is 3.3432682228088377 and perplexity is 28.311503971471083
At time: 179.49960088729858 and batch: 550, loss is 3.3985888051986692 and perplexity is 29.921844687468226
At time: 180.1143102645874 and batch: 600, loss is 3.4336868953704833 and perplexity is 30.990691815181776
At time: 180.726633310318 and batch: 650, loss is 3.257770366668701 and perplexity is 25.991520927718103
At time: 181.3393633365631 and batch: 700, loss is 3.268472785949707 and perplexity is 26.2711869645815
At time: 181.95154237747192 and batch: 750, loss is 3.3616038751602173 and perplexity is 28.83540219606201
At time: 182.56429386138916 and batch: 800, loss is 3.309693398475647 and perplexity is 27.376730437719836
At time: 183.17526745796204 and batch: 850, loss is 3.372590208053589 and perplexity is 29.153944123548687
At time: 183.8322958946228 and batch: 900, loss is 3.332508521080017 and perplexity is 28.008513599382727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280923085669949 and perplexity of 72.3071549093347
finished 15 epochs...
Completing Train Step...
At time: 185.4696912765503 and batch: 50, loss is 3.636667585372925 and perplexity is 37.96511020145883
At time: 186.10346245765686 and batch: 100, loss is 3.5057861280441283 and perplexity is 33.307617615381815
At time: 186.73218774795532 and batch: 150, loss is 3.5220961427688597 and perplexity is 33.8553197224167
At time: 187.35998964309692 and batch: 200, loss is 3.410776448249817 and perplexity is 30.28875278072965
At time: 187.9973759651184 and batch: 250, loss is 3.564712629318237 and perplexity is 35.32929934052177
At time: 188.62529039382935 and batch: 300, loss is 3.5334098863601686 and perplexity is 34.240525077392164
At time: 189.28698229789734 and batch: 350, loss is 3.5211496496200563 and perplexity is 33.823291054097524
At time: 189.90710997581482 and batch: 400, loss is 3.440882821083069 and perplexity is 31.214502829211956
At time: 190.5245590209961 and batch: 450, loss is 3.4739256429672243 and perplexity is 32.26314777047732
At time: 191.17083525657654 and batch: 500, loss is 3.341112723350525 and perplexity is 28.250544262908427
At time: 191.78389239311218 and batch: 550, loss is 3.3965125846862794 and perplexity is 29.859784787072083
At time: 192.39683532714844 and batch: 600, loss is 3.432446551322937 and perplexity is 30.95227652407332
At time: 193.01053833961487 and batch: 650, loss is 3.256905117034912 and perplexity is 25.969041500314393
At time: 193.62164425849915 and batch: 700, loss is 3.2681173276901245 and perplexity is 26.261850313679048
At time: 194.233717918396 and batch: 750, loss is 3.3614859914779665 and perplexity is 28.832003173020563
At time: 194.84519243240356 and batch: 800, loss is 3.3103419065475466 and perplexity is 27.394490226451666
At time: 195.45866465568542 and batch: 850, loss is 3.374130311012268 and perplexity is 29.198878792276883
At time: 196.07229113578796 and batch: 900, loss is 3.334408288002014 and perplexity is 28.061773822039246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280846164651113 and perplexity of 72.30159318321948
finished 16 epochs...
Completing Train Step...
At time: 197.61676573753357 and batch: 50, loss is 3.634754457473755 and perplexity is 37.892547522911705
At time: 198.24051547050476 and batch: 100, loss is 3.5035359621047975 and perplexity is 33.23275420783596
At time: 198.85403990745544 and batch: 150, loss is 3.5195513153076172 and perplexity is 33.76927330815436
At time: 199.4686667919159 and batch: 200, loss is 3.4082746744155883 and perplexity is 30.213071879273986
At time: 200.08160877227783 and batch: 250, loss is 3.562230887413025 and perplexity is 35.241729845254966
At time: 200.6941864490509 and batch: 300, loss is 3.5310064363479614 and perplexity is 34.15832850422218
At time: 201.30644297599792 and batch: 350, loss is 3.518848180770874 and perplexity is 33.74553731158807
At time: 201.91980266571045 and batch: 400, loss is 3.438674030303955 and perplexity is 31.145632611139572
At time: 202.5555067062378 and batch: 450, loss is 3.472151927947998 and perplexity is 32.20597286169619
At time: 203.1794204711914 and batch: 500, loss is 3.339616494178772 and perplexity is 28.20830658097009
At time: 203.80706787109375 and batch: 550, loss is 3.395135841369629 and perplexity is 29.818703813396652
At time: 204.42040276527405 and batch: 600, loss is 3.4315244150161743 and perplexity is 30.923747461977094
At time: 205.03325986862183 and batch: 650, loss is 3.256274356842041 and perplexity is 25.952666427603905
At time: 205.6470558643341 and batch: 700, loss is 3.2678687286376955 and perplexity is 26.255322454019517
At time: 206.26076865196228 and batch: 750, loss is 3.361433982849121 and perplexity is 28.8305036990618
At time: 206.87404680252075 and batch: 800, loss is 3.3107079696655273 and perplexity is 27.404520174644503
At time: 207.48768162727356 and batch: 850, loss is 3.374921245574951 and perplexity is 29.221982330194198
At time: 208.10148930549622 and batch: 900, loss is 3.3353615951538087 and perplexity is 28.088538066961956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280863722709761 and perplexity of 72.30286266997776
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 209.67047834396362 and batch: 50, loss is 3.6337192821502686 and perplexity is 37.853342388366855
At time: 210.29103875160217 and batch: 100, loss is 3.5029522132873536 and perplexity is 33.21336028800635
At time: 210.90346670150757 and batch: 150, loss is 3.5189033699035646 and perplexity is 33.74739974991718
At time: 211.51600670814514 and batch: 200, loss is 3.4077404069900514 and perplexity is 30.19693433041069
At time: 212.12736868858337 and batch: 250, loss is 3.5621251821517945 and perplexity is 35.23800480587707
At time: 212.73977518081665 and batch: 300, loss is 3.5303120756149293 and perplexity is 34.134618534791336
At time: 213.35200881958008 and batch: 350, loss is 3.5181718349456785 and perplexity is 33.72272137490991
At time: 213.96414756774902 and batch: 400, loss is 3.437736029624939 and perplexity is 31.116431683987713
At time: 214.57597517967224 and batch: 450, loss is 3.471190357208252 and perplexity is 32.17501942489189
At time: 215.18681144714355 and batch: 500, loss is 3.3382564544677735 and perplexity is 28.1699682405822
At time: 215.80004835128784 and batch: 550, loss is 3.3935819911956786 and perplexity is 29.77240599454149
At time: 216.41243648529053 and batch: 600, loss is 3.429710202217102 and perplexity is 30.86769606349238
At time: 217.02662324905396 and batch: 650, loss is 3.2543666696548463 and perplexity is 25.903204052756372
At time: 217.6388430595398 and batch: 700, loss is 3.265447154045105 and perplexity is 26.191820151057886
At time: 218.25177764892578 and batch: 750, loss is 3.359342293739319 and perplexity is 28.770262273575025
At time: 218.87511777877808 and batch: 800, loss is 3.308156590461731 and perplexity is 27.33468997132535
At time: 219.4888780117035 and batch: 850, loss is 3.3716189289093017 and perplexity is 29.125641252866817
At time: 220.10297536849976 and batch: 900, loss is 3.331934061050415 and perplexity is 27.992428448411808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280657206496147 and perplexity of 72.28793249826006
finished 18 epochs...
Completing Train Step...
At time: 221.658203125 and batch: 50, loss is 3.633058199882507 and perplexity is 37.828326484634644
At time: 222.2706573009491 and batch: 100, loss is 3.5022231245040896 and perplexity is 33.18915362504137
At time: 222.8844029903412 and batch: 150, loss is 3.518139100074768 and perplexity is 33.72161748404692
At time: 223.49761843681335 and batch: 200, loss is 3.4070405626296996 and perplexity is 30.175808569454222
At time: 224.1119122505188 and batch: 250, loss is 3.5614005088806153 and perplexity is 35.212478016072815
At time: 224.725647687912 and batch: 300, loss is 3.5296390008926393 and perplexity is 34.111651116162086
At time: 225.33912110328674 and batch: 350, loss is 3.517553672790527 and perplexity is 33.70188170659798
At time: 225.95247268676758 and batch: 400, loss is 3.437127084732056 and perplexity is 31.097489259860556
At time: 226.56558442115784 and batch: 450, loss is 3.4707206296920776 and perplexity is 32.15990948199038
At time: 227.17940878868103 and batch: 500, loss is 3.337865891456604 and perplexity is 28.158968241197876
At time: 227.79227948188782 and batch: 550, loss is 3.393187890052795 and perplexity is 29.760674967070152
At time: 228.40511989593506 and batch: 600, loss is 3.4294874477386474 and perplexity is 30.860820911718513
At time: 229.01894879341125 and batch: 650, loss is 3.254219136238098 and perplexity is 25.899382746449863
At time: 229.63183331489563 and batch: 700, loss is 3.2654254961013796 and perplexity is 26.19125289623379
At time: 230.24461579322815 and batch: 750, loss is 3.3594264936447145 and perplexity is 28.7726848289247
At time: 230.85705733299255 and batch: 800, loss is 3.3083416652679443 and perplexity is 27.339749401946825
At time: 231.46926093101501 and batch: 850, loss is 3.3719319009780886 and perplexity is 29.134758191663654
At time: 232.08066511154175 and batch: 900, loss is 3.3323070907592776 and perplexity is 28.002872403677966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280564399614726 and perplexity of 72.28122399198313
finished 19 epochs...
Completing Train Step...
At time: 233.6228883266449 and batch: 50, loss is 3.6325103521347044 and perplexity is 37.80760799697349
At time: 234.24352836608887 and batch: 100, loss is 3.501605544090271 and perplexity is 33.168662981772584
At time: 234.8544909954071 and batch: 150, loss is 3.5174786329269407 and perplexity is 33.69935281687707
At time: 235.46614480018616 and batch: 200, loss is 3.4064162254333494 and perplexity is 30.15697456972945
At time: 236.07647156715393 and batch: 250, loss is 3.5607582759857177 and perplexity is 35.189870664749435
At time: 236.68674302101135 and batch: 300, loss is 3.529041814804077 and perplexity is 34.09128619408693
At time: 237.29721927642822 and batch: 350, loss is 3.516994676589966 and perplexity is 33.68304774731849
At time: 237.91049075126648 and batch: 400, loss is 3.4365886735916136 and perplexity is 31.080750531766466
At time: 238.5288646221161 and batch: 450, loss is 3.4702882957458496 and perplexity is 32.14600866652742
At time: 239.14387464523315 and batch: 500, loss is 3.337510371208191 and perplexity is 28.148958937174264
At time: 239.75751781463623 and batch: 550, loss is 3.3928458642959596 and perplexity is 29.75049779021812
At time: 240.3712592124939 and batch: 600, loss is 3.429285097122192 and perplexity is 30.854576837350166
At time: 240.98366689682007 and batch: 650, loss is 3.254085073471069 and perplexity is 25.895910836266676
At time: 241.597594499588 and batch: 700, loss is 3.265401644706726 and perplexity is 26.19062820577439
At time: 242.211252450943 and batch: 750, loss is 3.359478178024292 and perplexity is 28.7741719657194
At time: 242.82421565055847 and batch: 800, loss is 3.308489747047424 and perplexity is 27.343798220459526
At time: 243.45723724365234 and batch: 850, loss is 3.372183585166931 and perplexity is 29.142091872493285
At time: 244.0930953025818 and batch: 900, loss is 3.332607946395874 and perplexity is 28.011298493136202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.280524684958262 and perplexity of 72.27835342500566
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f67603d0b70>
ELAPSED
1007.7589001655579


RESULTS SO FAR:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.03606636022589, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.57190604832506, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.17348567797130798, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.17852101364501027, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.27835342500566, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.2729052577440735, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.7700012466142289, 'wordvec_source': 'glove', 'num_layers': 3}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'dropout': 0.6853646424831196, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.08897995277100423, 'wordvec_source': 'glove', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9947011470794678 and batch: 50, loss is 7.026971311569214 and perplexity is 1126.613277181585
At time: 1.7788052558898926 and batch: 100, loss is 6.14434772491455 and perplexity is 466.075540429668
At time: 2.563519239425659 and batch: 150, loss is 5.995935735702514 and perplexity is 401.7924797090288
At time: 3.347611427307129 and batch: 200, loss is 5.820102186203003 and perplexity is 337.00648928414415
At time: 4.132050037384033 and batch: 250, loss is 5.859119443893433 and perplexity is 350.4154476653526
At time: 4.917869567871094 and batch: 300, loss is 5.751235284805298 and perplexity is 314.5790150492974
At time: 5.7026143074035645 and batch: 350, loss is 5.7129716873168945 and perplexity is 302.76946892950497
At time: 6.486617565155029 and batch: 400, loss is 5.555724411010742 and perplexity is 258.7143121503964
At time: 7.271188974380493 and batch: 450, loss is 5.541732950210571 and perplexity is 255.11972638618388
At time: 8.056740999221802 and batch: 500, loss is 5.48082049369812 and perplexity is 240.04358084184568
At time: 8.839514017105103 and batch: 550, loss is 5.516858348846435 and perplexity is 248.85200164344502
At time: 9.639502048492432 and batch: 600, loss is 5.42165512084961 and perplexity is 226.25328930740153
At time: 10.421575546264648 and batch: 650, loss is 5.317032041549683 and perplexity is 203.77817841944372
At time: 11.206606388092041 and batch: 700, loss is 5.409071588516236 and perplexity is 223.42406189597813
At time: 11.993728876113892 and batch: 750, loss is 5.371090478897095 and perplexity is 215.09729892525488
At time: 12.776780366897583 and batch: 800, loss is 5.344404602050782 and perplexity is 209.4331514007679
At time: 13.561352252960205 and batch: 850, loss is 5.370656385421753 and perplexity is 215.003946854456
At time: 14.347166538238525 and batch: 900, loss is 5.272205467224121 and perplexity is 194.84521365415515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.153160199727098 and perplexity of 172.97727019799342
finished 1 epochs...
Completing Train Step...
At time: 16.0153489112854 and batch: 50, loss is 5.070211658477783 and perplexity is 159.20802150482638
At time: 16.623470544815063 and batch: 100, loss is 4.9150386428833 and perplexity is 136.3245776755071
At time: 17.23111343383789 and batch: 150, loss is 4.882753295898437 and perplexity is 131.99358143325088
At time: 17.838913917541504 and batch: 200, loss is 4.755620918273926 and perplexity is 116.23580369688885
At time: 18.447174549102783 and batch: 250, loss is 4.847511310577392 and perplexity is 127.42287894586592
At time: 19.0554039478302 and batch: 300, loss is 4.771634654998779 and perplexity is 118.11215687069767
At time: 19.663554906845093 and batch: 350, loss is 4.761203384399414 and perplexity is 116.88650069070326
At time: 20.271198511123657 and batch: 400, loss is 4.6142636489868165 and perplexity is 100.91349341435041
At time: 20.88018798828125 and batch: 450, loss is 4.6308121013641355 and perplexity is 102.59734973540817
At time: 21.488450527191162 and batch: 500, loss is 4.536641445159912 and perplexity is 93.37666224918004
At time: 22.097026824951172 and batch: 550, loss is 4.5942559242248535 and perplexity is 98.91450826951397
At time: 22.70305824279785 and batch: 600, loss is 4.5483371353149415 and perplexity is 94.4751781875183
At time: 23.314889192581177 and batch: 650, loss is 4.408638668060303 and perplexity is 82.15754365318998
At time: 23.92928457260132 and batch: 700, loss is 4.45559398651123 and perplexity is 86.10728222211957
At time: 24.542095184326172 and batch: 750, loss is 4.504059858322144 and perplexity is 90.38333095653947
At time: 25.155583143234253 and batch: 800, loss is 4.454784002304077 and perplexity is 86.03756492216635
At time: 25.786337852478027 and batch: 850, loss is 4.506132555007935 and perplexity is 90.57086246789747
At time: 26.39635944366455 and batch: 900, loss is 4.4372251510620115 and perplexity is 84.54003008976686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.558027293584118 and perplexity of 95.39510754884571
finished 2 epochs...
Completing Train Step...
At time: 27.923539400100708 and batch: 50, loss is 4.484106779098511 and perplexity is 88.59777807165177
At time: 28.546480655670166 and batch: 100, loss is 4.348416223526001 and perplexity is 77.35585147862986
At time: 29.15895128250122 and batch: 150, loss is 4.3492414045333865 and perplexity is 77.41971040204197
At time: 29.770203590393066 and batch: 200, loss is 4.247024712562561 and perplexity is 69.89713858483226
At time: 30.38246440887451 and batch: 250, loss is 4.386980028152466 and perplexity is 80.39725454506682
At time: 30.994334936141968 and batch: 300, loss is 4.339563312530518 and perplexity is 76.67404942989563
At time: 31.606195211410522 and batch: 350, loss is 4.346680402755737 and perplexity is 77.22169205697318
At time: 32.21422243118286 and batch: 400, loss is 4.233062248229981 and perplexity is 68.92798391882774
At time: 32.8216016292572 and batch: 450, loss is 4.276577739715576 and perplexity is 71.99363697154966
At time: 33.42811870574951 and batch: 500, loss is 4.155790123939514 and perplexity is 63.80235640647854
At time: 34.03395056724548 and batch: 550, loss is 4.231863179206848 and perplexity is 68.84538403985837
At time: 34.64168405532837 and batch: 600, loss is 4.224690670967102 and perplexity is 68.35335660035172
At time: 35.249242544174194 and batch: 650, loss is 4.072846035957337 and perplexity is 58.72385519149968
At time: 35.85583186149597 and batch: 700, loss is 4.103841366767884 and perplexity is 60.57252253711958
At time: 36.46274161338806 and batch: 750, loss is 4.186858081817627 and perplexity is 65.81567828895317
At time: 37.07131791114807 and batch: 800, loss is 4.143142051696778 and perplexity is 63.00046149592073
At time: 37.67961668968201 and batch: 850, loss is 4.213120079040527 and perplexity is 67.56702573382519
At time: 38.28808903694153 and batch: 900, loss is 4.158077211380005 and perplexity is 63.94844496946867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3926981050674225 and perplexity of 80.8582890896197
finished 3 epochs...
Completing Train Step...
At time: 39.82130026817322 and batch: 50, loss is 4.229029889106751 and perplexity is 68.65060116332086
At time: 40.43808197975159 and batch: 100, loss is 4.098824434280395 and perplexity is 60.26939530119216
At time: 41.05127000808716 and batch: 150, loss is 4.103954834938049 and perplexity is 60.57939598036532
At time: 41.67253637313843 and batch: 200, loss is 4.00363664150238 and perplexity is 54.79706540423099
At time: 42.282065629959106 and batch: 250, loss is 4.155347828865051 and perplexity is 63.77414317824669
At time: 42.891257524490356 and batch: 300, loss is 4.112024416923523 and perplexity is 61.070224108286666
At time: 43.50288915634155 and batch: 350, loss is 4.1183245611190795 and perplexity is 61.45618986955974
At time: 44.115463733673096 and batch: 400, loss is 4.026672220230102 and perplexity is 56.07399849956479
At time: 44.7269549369812 and batch: 450, loss is 4.072820229530334 and perplexity is 58.72233975817142
At time: 45.33941984176636 and batch: 500, loss is 3.945231351852417 and perplexity is 51.68829491171594
At time: 45.95148038864136 and batch: 550, loss is 4.022082676887512 and perplexity is 55.8172341193895
At time: 46.563321113586426 and batch: 600, loss is 4.032376184463501 and perplexity is 56.3947565078732
At time: 47.17643737792969 and batch: 650, loss is 3.878860607147217 and perplexity is 48.36907228740381
At time: 47.78959512710571 and batch: 700, loss is 3.9009712076187135 and perplexity is 49.45045244731696
At time: 48.40184545516968 and batch: 750, loss is 3.9981313371658325 and perplexity is 54.496219765675114
At time: 49.01328206062317 and batch: 800, loss is 3.959732027053833 and perplexity is 52.44327068821538
At time: 49.62637257575989 and batch: 850, loss is 4.031373209953308 and perplexity is 56.33822236048094
At time: 50.24048113822937 and batch: 900, loss is 3.9844411706924436 and perplexity is 53.75524107855917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333390745398116 and perplexity of 76.20223137483048
finished 4 epochs...
Completing Train Step...
At time: 51.7973747253418 and batch: 50, loss is 4.06119595527649 and perplexity is 58.04368723931843
At time: 52.410847187042236 and batch: 100, loss is 3.9332207536697386 and perplexity is 51.071200823547635
At time: 53.086448669433594 and batch: 150, loss is 3.9419153547286987 and perplexity is 51.517180538638705
At time: 53.70258021354675 and batch: 200, loss is 3.8436502265930175 and perplexity is 46.695613297620774
At time: 54.31768846511841 and batch: 250, loss is 3.9971040391922 and perplexity is 54.4402646557457
At time: 54.93259024620056 and batch: 300, loss is 3.959083943367004 and perplexity is 52.40929407103327
At time: 55.54339051246643 and batch: 350, loss is 3.9638390731811524 and perplexity is 52.65910052817191
At time: 56.15755844116211 and batch: 400, loss is 3.884671883583069 and perplexity is 48.65097665617195
At time: 56.78791522979736 and batch: 450, loss is 3.9283243227005005 and perplexity is 50.82174543314019
At time: 57.401084184646606 and batch: 500, loss is 3.8047467184066774 and perplexity is 44.9138728149753
At time: 58.01300573348999 and batch: 550, loss is 3.8756219387054442 and perplexity is 48.21267429675162
At time: 58.62530279159546 and batch: 600, loss is 3.8948591852188112 and perplexity is 49.149131951115486
At time: 59.23879146575928 and batch: 650, loss is 3.7441224956512453 and perplexity is 42.27189716550964
At time: 59.85775089263916 and batch: 700, loss is 3.7609523487091066 and perplexity is 42.98934733941169
At time: 60.47135949134827 and batch: 750, loss is 3.861052017211914 and perplexity is 47.51531201058283
At time: 61.08648157119751 and batch: 800, loss is 3.827461361885071 and perplexity is 45.94575042269925
At time: 61.69769072532654 and batch: 850, loss is 3.8991331338882445 and perplexity is 49.359642353104995
At time: 62.30832886695862 and batch: 900, loss is 3.857863998413086 and perplexity is 47.36407350635045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314804286172945 and perplexity of 74.7989828161163
finished 5 epochs...
Completing Train Step...
At time: 63.84583640098572 and batch: 50, loss is 3.940438542366028 and perplexity is 51.4411554807194
At time: 64.4634063243866 and batch: 100, loss is 3.8137607288360598 and perplexity is 45.320557107512
At time: 65.07210230827332 and batch: 150, loss is 3.823750762939453 and perplexity is 45.77558008181867
At time: 65.68045139312744 and batch: 200, loss is 3.726134600639343 and perplexity is 41.51831273546055
At time: 66.287034034729 and batch: 250, loss is 3.879548420906067 and perplexity is 48.40235264485774
At time: 66.89486575126648 and batch: 300, loss is 3.8435996770858765 and perplexity is 46.69325291704147
At time: 67.50366640090942 and batch: 350, loss is 3.847993688583374 and perplexity is 46.898875029342804
At time: 68.11211085319519 and batch: 400, loss is 3.7771649408340453 and perplexity is 43.691996585590424
At time: 68.7222843170166 and batch: 450, loss is 3.8198059368133546 and perplexity is 45.59535908154111
At time: 69.33076524734497 and batch: 500, loss is 3.695229797363281 and perplexity is 40.254821977138015
At time: 69.94065022468567 and batch: 550, loss is 3.7651907634735107 and perplexity is 43.17194070374818
At time: 70.5513288974762 and batch: 600, loss is 3.7902044677734374 and perplexity is 44.26545020878643
At time: 71.15985870361328 and batch: 650, loss is 3.639902853965759 and perplexity is 38.08813643426141
At time: 71.76684069633484 and batch: 700, loss is 3.6522446060180664 and perplexity is 38.56112351115921
At time: 72.46178340911865 and batch: 750, loss is 3.7573615074157716 and perplexity is 42.83525623972631
At time: 73.09216070175171 and batch: 800, loss is 3.7213201570510863 and perplexity is 41.31890556337582
At time: 73.71895122528076 and batch: 850, loss is 3.7936836433410646 and perplexity is 44.419725701784785
At time: 74.37658596038818 and batch: 900, loss is 3.758367099761963 and perplexity is 42.87835271065713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313433503451413 and perplexity of 74.69651990612144
finished 6 epochs...
Completing Train Step...
At time: 75.93457984924316 and batch: 50, loss is 3.8431983900070192 and perplexity is 46.674519277011285
At time: 76.56388974189758 and batch: 100, loss is 3.7177060508728026 and perplexity is 41.16984417559092
At time: 77.17666053771973 and batch: 150, loss is 3.728391270637512 and perplexity is 41.61211166299169
At time: 77.78860783576965 and batch: 200, loss is 3.635623731613159 and perplexity is 37.925500855214246
At time: 78.39951944351196 and batch: 250, loss is 3.7841258239746094 and perplexity is 43.99719245205501
At time: 79.01008939743042 and batch: 300, loss is 3.7532081747055055 and perplexity is 42.65771611565199
At time: 79.62253379821777 and batch: 350, loss is 3.75460590839386 and perplexity is 42.71738193128148
At time: 80.23333859443665 and batch: 400, loss is 3.687143201828003 and perplexity is 39.9306101651241
At time: 80.84535622596741 and batch: 450, loss is 3.7315977811813354 and perplexity is 41.74575548845031
At time: 81.45693778991699 and batch: 500, loss is 3.6091407251358034 and perplexity is 36.93430245790556
At time: 82.07096004486084 and batch: 550, loss is 3.6750852966308596 and perplexity is 39.45202183823111
At time: 82.68463683128357 and batch: 600, loss is 3.706458911895752 and perplexity is 40.709395435784074
At time: 83.29822778701782 and batch: 650, loss is 3.5536631393432616 and perplexity is 34.94107737883925
At time: 83.91214966773987 and batch: 700, loss is 3.568899941444397 and perplexity is 35.47754430141489
At time: 84.5259017944336 and batch: 750, loss is 3.6715046167373657 and perplexity is 39.31100938776351
At time: 85.13947033882141 and batch: 800, loss is 3.636862292289734 and perplexity is 37.972502990702715
At time: 85.75270438194275 and batch: 850, loss is 3.710272789001465 and perplexity is 40.864952516261326
At time: 86.36572337150574 and batch: 900, loss is 3.6772166681289673 and perplexity is 39.53619842704488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316708499438142 and perplexity of 74.94155172914573
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 87.91759157180786 and batch: 50, loss is 3.7876772832870484 and perplexity is 44.15372448495662
At time: 88.53016471862793 and batch: 100, loss is 3.665793890953064 and perplexity is 39.0871547873343
At time: 89.14290428161621 and batch: 150, loss is 3.6765284442901613 and perplexity is 39.50899803384477
At time: 89.75195622444153 and batch: 200, loss is 3.5634804582595825 and perplexity is 35.2857944086151
At time: 90.35968828201294 and batch: 250, loss is 3.7069307470321657 and perplexity is 40.728608091179346
At time: 90.96790719032288 and batch: 300, loss is 3.670527596473694 and perplexity is 39.27262049142482
At time: 91.57772183418274 and batch: 350, loss is 3.648162798881531 and perplexity is 38.404045241711124
At time: 92.18913269042969 and batch: 400, loss is 3.577779674530029 and perplexity is 35.79397827071528
At time: 92.80182480812073 and batch: 450, loss is 3.607473669052124 and perplexity is 36.87278219740638
At time: 93.41825461387634 and batch: 500, loss is 3.4754034376144407 and perplexity is 32.310861324288716
At time: 94.03511166572571 and batch: 550, loss is 3.525027256011963 and perplexity is 33.95469907331949
At time: 94.7012848854065 and batch: 600, loss is 3.552030167579651 and perplexity is 34.88406614762369
At time: 95.31650519371033 and batch: 650, loss is 3.387523627281189 and perplexity is 29.592579202516937
At time: 95.9288387298584 and batch: 700, loss is 3.3884373092651368 and perplexity is 29.61962976491673
At time: 96.53823399543762 and batch: 750, loss is 3.4790614891052245 and perplexity is 32.42927256388858
At time: 97.15008449554443 and batch: 800, loss is 3.426755452156067 and perplexity is 30.77662434999908
At time: 97.75988960266113 and batch: 850, loss is 3.477893829345703 and perplexity is 32.391428306193426
At time: 98.3719527721405 and batch: 900, loss is 3.4377464771270754 and perplexity is 31.116756774672393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272792868418236 and perplexity of 71.7216653348773
finished 8 epochs...
Completing Train Step...
At time: 99.91709613800049 and batch: 50, loss is 3.6966533994674684 and perplexity is 40.3121696368476
At time: 100.53707528114319 and batch: 100, loss is 3.5678430938720704 and perplexity is 35.440069750779784
At time: 101.15053081512451 and batch: 150, loss is 3.5776367807388305 and perplexity is 35.78886389887278
At time: 101.76422667503357 and batch: 200, loss is 3.471211380958557 and perplexity is 32.17569587157704
At time: 102.37724590301514 and batch: 250, loss is 3.615143532752991 and perplexity is 37.15667874331239
At time: 102.99833369255066 and batch: 300, loss is 3.5828107023239135 and perplexity is 35.974512525881345
At time: 103.61106491088867 and batch: 350, loss is 3.5649106073379517 and perplexity is 35.33629445765998
At time: 104.22289705276489 and batch: 400, loss is 3.5000255346298217 and perplexity is 33.11629756029547
At time: 104.8361132144928 and batch: 450, loss is 3.535589771270752 and perplexity is 34.31524689423013
At time: 105.44881105422974 and batch: 500, loss is 3.4072237682342528 and perplexity is 30.181337453151844
At time: 106.06137251853943 and batch: 550, loss is 3.458859748840332 and perplexity is 31.780717846222622
At time: 106.67502760887146 and batch: 600, loss is 3.492960925102234 and perplexity is 32.88316830221899
At time: 107.28861045837402 and batch: 650, loss is 3.3329846906661986 and perplexity is 28.02185357750678
At time: 107.90204048156738 and batch: 700, loss is 3.339430193901062 and perplexity is 28.203051855113966
At time: 108.51348567008972 and batch: 750, loss is 3.4367472171783446 and perplexity is 31.085678576078653
At time: 109.12628936767578 and batch: 800, loss is 3.388104991912842 and perplexity is 29.609788283315424
At time: 109.73907780647278 and batch: 850, loss is 3.4472422647476195 and perplexity is 31.413642238256653
At time: 110.3516845703125 and batch: 900, loss is 3.4139453315734865 and perplexity is 30.38488654220154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2800924222763275 and perplexity of 72.24711694177772
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 111.89033532142639 and batch: 50, loss is 3.6685762500762937 and perplexity is 39.19606072651652
At time: 112.51034879684448 and batch: 100, loss is 3.552334442138672 and perplexity is 34.8946820964673
At time: 113.1848886013031 and batch: 150, loss is 3.566220302581787 and perplexity is 35.38260455388268
At time: 113.80510568618774 and batch: 200, loss is 3.4565581607818605 and perplexity is 31.707655837114615
At time: 114.42262697219849 and batch: 250, loss is 3.5952911138534547 and perplexity is 36.4263026458442
At time: 115.03931736946106 and batch: 300, loss is 3.5612469577789305 and perplexity is 35.20707151637801
At time: 115.65647721290588 and batch: 350, loss is 3.5412349128723144 and perplexity is 34.50950912505167
At time: 116.27332949638367 and batch: 400, loss is 3.477172431945801 and perplexity is 32.36806964048758
At time: 116.8913562297821 and batch: 450, loss is 3.502886562347412 and perplexity is 33.21117987125881
At time: 117.5103280544281 and batch: 500, loss is 3.3713801813125612 and perplexity is 29.118688406034934
At time: 118.12907528877258 and batch: 550, loss is 3.4150943088531496 and perplexity is 30.419818150452873
At time: 118.75614738464355 and batch: 600, loss is 3.4513655424118044 and perplexity is 31.5434368132935
At time: 119.37418866157532 and batch: 650, loss is 3.2827927684783935 and perplexity is 26.6500964163177
At time: 119.99307513237 and batch: 700, loss is 3.285364894866943 and perplexity is 26.718732064508288
At time: 120.61113333702087 and batch: 750, loss is 3.3764129114151 and perplexity is 29.265604289622484
At time: 121.22843384742737 and batch: 800, loss is 3.318929452896118 and perplexity is 27.63075469504352
At time: 121.84318590164185 and batch: 850, loss is 3.3769899129867555 and perplexity is 29.2824954619409
At time: 122.45497226715088 and batch: 900, loss is 3.345832028388977 and perplexity is 28.38418229006452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267309319483091 and perplexity of 71.32945241546881
finished 10 epochs...
Completing Train Step...
At time: 124.0003354549408 and batch: 50, loss is 3.6390312194824217 and perplexity is 38.05495196560382
At time: 124.66331148147583 and batch: 100, loss is 3.515349998474121 and perplexity is 33.627695506600176
At time: 125.31919980049133 and batch: 150, loss is 3.528484230041504 and perplexity is 34.07228271088727
At time: 125.95083594322205 and batch: 200, loss is 3.421159038543701 and perplexity is 30.604866692275966
At time: 126.6212899684906 and batch: 250, loss is 3.5604668617248536 and perplexity is 35.179617328656505
At time: 127.23937129974365 and batch: 300, loss is 3.52743004322052 and perplexity is 34.03638308527336
At time: 127.8564829826355 and batch: 350, loss is 3.5098444032669067 and perplexity is 33.44306374762613
At time: 128.474618434906 and batch: 400, loss is 3.4469275426864625 and perplexity is 31.403757227620034
At time: 129.0926764011383 and batch: 450, loss is 3.4761796855926512 and perplexity is 32.33595230221648
At time: 129.71175479888916 and batch: 500, loss is 3.3474141931533814 and perplexity is 28.429126288179894
At time: 130.33006715774536 and batch: 550, loss is 3.392688579559326 and perplexity is 29.74581885898035
At time: 130.94949173927307 and batch: 600, loss is 3.4325622367858886 and perplexity is 30.955857459639507
At time: 131.56836891174316 and batch: 650, loss is 3.267107162475586 and perplexity is 26.235334900751415
At time: 132.19092297554016 and batch: 700, loss is 3.273205442428589 and perplexity is 26.395814143974405
At time: 132.85062527656555 and batch: 750, loss is 3.3678630018234252 and perplexity is 29.016452648569995
At time: 133.46781301498413 and batch: 800, loss is 3.3135263204574583 and perplexity is 27.48186466653994
At time: 134.0889642238617 and batch: 850, loss is 3.3747486114501952 and perplexity is 29.216938054270603
At time: 134.70175194740295 and batch: 900, loss is 3.3460940074920655 and perplexity is 28.39161932681434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26904756728917 and perplexity of 71.45354850326859
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 136.24209928512573 and batch: 50, loss is 3.6297461080551146 and perplexity is 37.70324285222603
At time: 136.86201548576355 and batch: 100, loss is 3.5114820337295534 and perplexity is 33.497875996437884
At time: 137.47508311271667 and batch: 150, loss is 3.529235119819641 and perplexity is 34.097876847677966
At time: 138.08927130699158 and batch: 200, loss is 3.4219946336746214 and perplexity is 30.6304506572962
At time: 138.70303606987 and batch: 250, loss is 3.559583835601807 and perplexity is 35.148566518912205
At time: 139.3159921169281 and batch: 300, loss is 3.5247928285598755 and perplexity is 33.94674009266757
At time: 139.92873120307922 and batch: 350, loss is 3.506144042015076 and perplexity is 33.31954101071294
At time: 140.54146671295166 and batch: 400, loss is 3.4430396366119385 and perplexity is 31.28189940850777
At time: 141.15602612495422 and batch: 450, loss is 3.4692322969436646 and perplexity is 32.11208043711822
At time: 141.76986384391785 and batch: 500, loss is 3.33989155292511 and perplexity is 28.21606658959476
At time: 142.3838768005371 and batch: 550, loss is 3.3836131381988523 and perplexity is 29.47708371373028
At time: 142.99718189239502 and batch: 600, loss is 3.422049241065979 and perplexity is 30.632123351973036
At time: 143.61042857170105 and batch: 650, loss is 3.2537559175491335 and perplexity is 25.88738844653547
At time: 144.2221019268036 and batch: 700, loss is 3.254664840698242 and perplexity is 25.91092878972529
At time: 144.83580589294434 and batch: 750, loss is 3.3466875648498533 and perplexity is 28.40847638368544
At time: 145.44930601119995 and batch: 800, loss is 3.2898612880706786 and perplexity is 26.839140488722165
At time: 146.06579089164734 and batch: 850, loss is 3.3506492280960085 and perplexity is 28.5212444274721
At time: 146.6806559562683 and batch: 900, loss is 3.3228645277023316 and perplexity is 27.739697991150038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267241177493578 and perplexity of 71.32459205026976
finished 12 epochs...
Completing Train Step...
At time: 148.34621286392212 and batch: 50, loss is 3.6199683475494386 and perplexity is 37.33638601523772
At time: 148.97311806678772 and batch: 100, loss is 3.497692098617554 and perplexity is 33.039112886745784
At time: 149.59187245368958 and batch: 150, loss is 3.515206332206726 and perplexity is 33.62286468812678
At time: 150.21838879585266 and batch: 200, loss is 3.408290104866028 and perplexity is 30.213538084179117
At time: 150.8348891735077 and batch: 250, loss is 3.547573666572571 and perplexity is 34.72895116346771
At time: 151.45024633407593 and batch: 300, loss is 3.5133519124984742 and perplexity is 33.560571561853266
At time: 152.0651216506958 and batch: 350, loss is 3.4952411699295043 and perplexity is 32.958235529947146
At time: 152.68241000175476 and batch: 400, loss is 3.4323980617523193 and perplexity is 30.950775697862525
At time: 153.2990894317627 and batch: 450, loss is 3.4592630815505983 and perplexity is 31.793538634633617
At time: 153.91725969314575 and batch: 500, loss is 3.331035461425781 and perplexity is 27.96728576101206
At time: 154.5353171825409 and batch: 550, loss is 3.3757347536087035 and perplexity is 29.24576431969017
At time: 155.15206050872803 and batch: 600, loss is 3.4160027170181273 and perplexity is 30.447464316738326
At time: 155.7699465751648 and batch: 650, loss is 3.249440531730652 and perplexity is 25.775915075850452
At time: 156.38982629776 and batch: 700, loss is 3.2521479511260987 and perplexity is 25.84579584384933
At time: 157.0071063041687 and batch: 750, loss is 3.3455631256103517 and perplexity is 28.376550730697456
At time: 157.62617826461792 and batch: 800, loss is 3.290394425392151 and perplexity is 26.853453251187847
At time: 158.2426769733429 and batch: 850, loss is 3.3530134105682374 and perplexity is 28.5887536242191
At time: 158.90922498703003 and batch: 900, loss is 3.3265583658218385 and perplexity is 27.842353424566955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267439750775899 and perplexity of 71.33875661492884
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 160.47209239006042 and batch: 50, loss is 3.616638226509094 and perplexity is 37.21225812575798
At time: 161.08771300315857 and batch: 100, loss is 3.4953308868408204 and perplexity is 32.961192573687754
At time: 161.70578455924988 and batch: 150, loss is 3.514469542503357 and perplexity is 33.598100831624315
At time: 162.3216609954834 and batch: 200, loss is 3.4075175619125364 and perplexity is 30.190205841972222
At time: 162.93833208084106 and batch: 250, loss is 3.5479730319976808 and perplexity is 34.74282347568929
At time: 163.55432605743408 and batch: 300, loss is 3.5121813440322875 and perplexity is 33.52130959896676
At time: 164.17077445983887 and batch: 350, loss is 3.493973126411438 and perplexity is 32.916469539159856
At time: 164.7861499786377 and batch: 400, loss is 3.4312887239456176 and perplexity is 30.916459869676103
At time: 165.41122889518738 and batch: 450, loss is 3.4564185905456544 and perplexity is 31.70323070091572
At time: 166.02877259254456 and batch: 500, loss is 3.3274760961532595 and perplexity is 27.867916925208586
At time: 166.6443133354187 and batch: 550, loss is 3.371887946128845 and perplexity is 29.133477605903778
At time: 167.2602972984314 and batch: 600, loss is 3.4109056282043455 and perplexity is 30.292665733168654
At time: 167.87647485733032 and batch: 650, loss is 3.2435311126708983 and perplexity is 25.624043570165238
At time: 168.49280309677124 and batch: 700, loss is 3.2449425411224366 and perplexity is 25.660235609532396
At time: 169.16070175170898 and batch: 750, loss is 3.338771719932556 and perplexity is 28.18448699254511
At time: 169.77890610694885 and batch: 800, loss is 3.282110056877136 and perplexity is 26.631908295646596
At time: 170.39542245864868 and batch: 850, loss is 3.342979407310486 and perplexity is 28.30332835102225
At time: 171.01203989982605 and batch: 900, loss is 3.3161940431594847 and perplexity is 27.555276538525845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267503712275257 and perplexity of 71.34331969469345
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 172.56211709976196 and batch: 50, loss is 3.614393949508667 and perplexity is 37.128837155609126
At time: 173.19088220596313 and batch: 100, loss is 3.493975238800049 and perplexity is 32.91653907160866
At time: 173.80709147453308 and batch: 150, loss is 3.5130110502243044 and perplexity is 33.549133978539324
At time: 174.42480564117432 and batch: 200, loss is 3.4056319904327395 and perplexity is 30.13333368597525
At time: 175.03934526443481 and batch: 250, loss is 3.5462330532073976 and perplexity is 34.68242426164109
At time: 175.65676856040955 and batch: 300, loss is 3.510497784614563 and perplexity is 33.4649219617844
At time: 176.2746171951294 and batch: 350, loss is 3.4922593069076537 and perplexity is 32.860104964817396
At time: 176.89144849777222 and batch: 400, loss is 3.4299134683609007 and perplexity is 30.873971058764592
At time: 177.50786995887756 and batch: 450, loss is 3.454916124343872 and perplexity is 31.65563343389664
At time: 178.12594771385193 and batch: 500, loss is 3.325479979515076 and perplexity is 27.812344795241458
At time: 178.7427167892456 and batch: 550, loss is 3.3703170728683474 and perplexity is 29.08774853164093
At time: 179.35943627357483 and batch: 600, loss is 3.4089769601821898 and perplexity is 30.23429754198813
At time: 180.01870369911194 and batch: 650, loss is 3.2413320398330687 and perplexity is 25.56775634449265
At time: 180.6449854373932 and batch: 700, loss is 3.2427320861816407 and perplexity is 25.603577458162466
At time: 181.27083778381348 and batch: 750, loss is 3.3367921113967896 and perplexity is 28.12874793048273
At time: 181.88684058189392 and batch: 800, loss is 3.279597268104553 and perplexity is 26.56507194347416
At time: 182.50281310081482 and batch: 850, loss is 3.3401699924468993 and perplexity is 28.223924151559878
At time: 183.11877584457397 and batch: 900, loss is 3.3132341384887694 and perplexity is 27.47383613417174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267088171553938 and perplexity of 71.31367979888459
finished 15 epochs...
Completing Train Step...
At time: 184.66175031661987 and batch: 50, loss is 3.6135616159439086 and perplexity is 37.097946435698944
At time: 185.29014253616333 and batch: 100, loss is 3.493191108703613 and perplexity is 32.890738339539354
At time: 185.9084849357605 and batch: 150, loss is 3.5119425201416017 and perplexity is 33.513304865286315
At time: 186.5261926651001 and batch: 200, loss is 3.404596061706543 and perplexity is 30.1021338631804
At time: 187.14409112930298 and batch: 250, loss is 3.5451180219650267 and perplexity is 34.64377382725362
At time: 187.76126050949097 and batch: 300, loss is 3.509523324966431 and perplexity is 33.43232762921489
At time: 188.37906098365784 and batch: 350, loss is 3.4913296794891355 and perplexity is 32.829571504836444
At time: 188.99804782867432 and batch: 400, loss is 3.429058241844177 and perplexity is 30.847578107623093
At time: 189.61681175231934 and batch: 450, loss is 3.454116816520691 and perplexity is 31.630340948032686
At time: 190.23408603668213 and batch: 500, loss is 3.3247838735580446 and perplexity is 27.79299119321082
At time: 190.85152220726013 and batch: 550, loss is 3.3696996545791627 and perplexity is 29.069794766768023
At time: 191.46982789039612 and batch: 600, loss is 3.408598017692566 and perplexity is 30.22284265251479
At time: 192.08768248558044 and batch: 650, loss is 3.241148886680603 and perplexity is 25.563073958126687
At time: 192.74721121788025 and batch: 700, loss is 3.242722873687744 and perplexity is 25.603341586447886
At time: 193.41268301010132 and batch: 750, loss is 3.3368931818008423 and perplexity is 28.13159105807717
At time: 194.06662702560425 and batch: 800, loss is 3.279941077232361 and perplexity is 26.574206827927544
At time: 194.6841917037964 and batch: 850, loss is 3.3407489538192747 and perplexity is 28.24026944461056
At time: 195.30336046218872 and batch: 900, loss is 3.313907713890076 and perplexity is 27.492348068263052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266860752889555 and perplexity of 71.29746358107762
finished 16 epochs...
Completing Train Step...
At time: 196.86701893806458 and batch: 50, loss is 3.612870063781738 and perplexity is 37.07230013952714
At time: 197.484539270401 and batch: 100, loss is 3.492498550415039 and perplexity is 32.86796747207164
At time: 198.1495862007141 and batch: 150, loss is 3.5110542726516725 and perplexity is 33.48354997317095
At time: 198.77200388908386 and batch: 200, loss is 3.403714632987976 and perplexity is 30.075612667891896
At time: 199.41283988952637 and batch: 250, loss is 3.5441749334335326 and perplexity is 34.61111708299041
At time: 200.0358214378357 and batch: 300, loss is 3.5086862659454345 and perplexity is 33.40435450699043
At time: 200.65413427352905 and batch: 350, loss is 3.4905294466018675 and perplexity is 32.803310710820256
At time: 201.27166390419006 and batch: 400, loss is 3.4283116674423217 and perplexity is 30.824556690118502
At time: 201.89248752593994 and batch: 450, loss is 3.453427219390869 and perplexity is 31.608536274783766
At time: 202.5120506286621 and batch: 500, loss is 3.3241875886917116 and perplexity is 27.776423593167664
At time: 203.13100481033325 and batch: 550, loss is 3.369184055328369 and perplexity is 29.054810265696283
At time: 203.74958896636963 and batch: 600, loss is 3.408276400566101 and perplexity is 30.213124031628514
At time: 204.3682360649109 and batch: 650, loss is 3.240985155105591 and perplexity is 25.558888818394475
At time: 204.98679637908936 and batch: 700, loss is 3.2427237701416014 and perplexity is 25.603364538672498
At time: 205.6044406890869 and batch: 750, loss is 3.336989064216614 and perplexity is 28.13428851230446
At time: 206.2215085029602 and batch: 800, loss is 3.2802166652679445 and perplexity is 26.58153137061625
At time: 206.838942527771 and batch: 850, loss is 3.3412253522872923 and perplexity is 28.253726270861574
At time: 207.45713686943054 and batch: 900, loss is 3.314468336105347 and perplexity is 27.50776521053251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266726977204623 and perplexity of 71.2879263519921
finished 17 epochs...
Completing Train Step...
At time: 209.01408505439758 and batch: 50, loss is 3.6122585344314575 and perplexity is 37.04963627042628
At time: 209.6421709060669 and batch: 100, loss is 3.4918616819381714 and perplexity is 32.84704156392288
At time: 210.267174243927 and batch: 150, loss is 3.5102739095687867 and perplexity is 33.45743083941727
At time: 210.9236810207367 and batch: 200, loss is 3.402931933403015 and perplexity is 30.0520817083767
At time: 211.59817337989807 and batch: 250, loss is 3.543348150253296 and perplexity is 34.58251301984555
At time: 212.25017595291138 and batch: 300, loss is 3.507939786911011 and perplexity is 33.37942816134719
At time: 212.88369154930115 and batch: 350, loss is 3.4898149061203 and perplexity is 32.77987978955574
At time: 213.5249330997467 and batch: 400, loss is 3.4276382732391357 and perplexity is 30.803806599607043
At time: 214.16984915733337 and batch: 450, loss is 3.4528090572357177 and perplexity is 31.589003111832028
At time: 214.80055332183838 and batch: 500, loss is 3.323658003807068 and perplexity is 27.761717513485728
At time: 215.42948246002197 and batch: 550, loss is 3.3687305212020875 and perplexity is 29.041635905444956
At time: 216.06627488136292 and batch: 600, loss is 3.407986407279968 and perplexity is 30.20436369878649
At time: 216.73242473602295 and batch: 650, loss is 3.240828905105591 and perplexity is 25.554895553998513
At time: 217.36739468574524 and batch: 700, loss is 3.2427203702926635 and perplexity is 25.60327749124874
At time: 218.00280332565308 and batch: 750, loss is 3.337072491645813 and perplexity is 28.13663578157935
At time: 218.63854146003723 and batch: 800, loss is 3.2804406023025514 and perplexity is 26.58748462647905
At time: 219.27421689033508 and batch: 850, loss is 3.341622161865234 and perplexity is 28.26493984473802
At time: 219.90953278541565 and batch: 900, loss is 3.3149375534057617 and perplexity is 27.520675358460558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266644621548587 and perplexity of 71.28205562979683
finished 18 epochs...
Completing Train Step...
At time: 221.4583203792572 and batch: 50, loss is 3.6116985177993772 and perplexity is 37.028893666546
At time: 222.0836741924286 and batch: 100, loss is 3.491264371871948 and perplexity is 32.827427553757104
At time: 222.69461917877197 and batch: 150, loss is 3.5095655965805053 and perplexity is 33.433740897536396
At time: 223.30815887451172 and batch: 200, loss is 3.4022197246551515 and perplexity is 30.03068597291187
At time: 223.9187135696411 and batch: 250, loss is 3.5426056003570556 and perplexity is 34.55684331009057
At time: 224.53024458885193 and batch: 300, loss is 3.5072590351104735 and perplexity is 33.35671278816828
At time: 225.14042806625366 and batch: 350, loss is 3.489162549972534 and perplexity is 32.758502606978354
At time: 225.7530481815338 and batch: 400, loss is 3.427019324302673 and perplexity is 30.784746515491104
At time: 226.36332273483276 and batch: 450, loss is 3.4522426557540893 and perplexity is 31.571116119753924
At time: 226.97331500053406 and batch: 500, loss is 3.3231761026382447 and perplexity is 27.74834233238398
At time: 227.58395266532898 and batch: 550, loss is 3.368317894935608 and perplexity is 29.029655035629617
At time: 228.22587752342224 and batch: 600, loss is 3.4077158212661742 and perplexity is 30.196191926047618
At time: 228.83574748039246 and batch: 650, loss is 3.2406751823425295 and perplexity is 25.550967486768613
At time: 229.44620728492737 and batch: 700, loss is 3.2427077674865723 and perplexity is 25.602954820140504
At time: 230.05721187591553 and batch: 750, loss is 3.337141399383545 and perplexity is 28.13857468030024
At time: 230.66729760169983 and batch: 800, loss is 3.2806238174438476 and perplexity is 26.592356302500377
At time: 231.27748370170593 and batch: 850, loss is 3.341956386566162 and perplexity is 28.274388264664076
At time: 231.88785529136658 and batch: 900, loss is 3.3153335094451903 and perplexity is 27.531574493724715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266593201519692 and perplexity of 71.2783903986706
finished 19 epochs...
Completing Train Step...
At time: 233.44463634490967 and batch: 50, loss is 3.611174759864807 and perplexity is 37.009504567720214
At time: 234.05534172058105 and batch: 100, loss is 3.490698013305664 and perplexity is 32.80884072285525
At time: 234.66519594192505 and batch: 150, loss is 3.508909649848938 and perplexity is 33.411817335601064
At time: 235.27636098861694 and batch: 200, loss is 3.401560835838318 and perplexity is 30.010905607009033
At time: 235.88724398612976 and batch: 250, loss is 3.5419264316558836 and perplexity is 34.53338135191874
At time: 236.4996156692505 and batch: 300, loss is 3.506628909111023 and perplexity is 33.335700477081275
At time: 237.11058044433594 and batch: 350, loss is 3.4885578203201293 and perplexity is 32.73869855773497
At time: 237.72273993492126 and batch: 400, loss is 3.4264432621002197 and perplexity is 30.767017693553797
At time: 238.3346085548401 and batch: 450, loss is 3.45171555519104 and perplexity is 31.554479351681202
At time: 238.95017385482788 and batch: 500, loss is 3.3227298498153686 and perplexity is 27.735962318801473
At time: 239.56194734573364 and batch: 550, loss is 3.367934613227844 and perplexity is 29.018530631898177
At time: 240.17653703689575 and batch: 600, loss is 3.407458100318909 and perplexity is 30.18841073759133
At time: 240.8214476108551 and batch: 650, loss is 3.2405220556259153 and perplexity is 25.547055250552635
At time: 241.43369221687317 and batch: 700, loss is 3.2426846170425416 and perplexity is 25.60236210722873
At time: 242.04936957359314 and batch: 750, loss is 3.337195348739624 and perplexity is 28.140092779235097
At time: 242.6651291847229 and batch: 800, loss is 3.280773959159851 and perplexity is 26.596349224252773
At time: 243.30236840248108 and batch: 850, loss is 3.3422402048110964 and perplexity is 28.28241419081621
At time: 243.9207639694214 and batch: 900, loss is 3.315670356750488 and perplexity is 27.540849992529214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266563101990582 and perplexity of 71.276244984972
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f67603d0b70>
ELAPSED
1258.4035789966583


RESULTS SO FAR:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.03606636022589, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.57190604832506, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.17348567797130798, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.17852101364501027, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.27835342500566, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.2729052577440735, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.7700012466142289, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.276244984972, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.6853646424831196, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.08897995277100423, 'wordvec_source': 'glove', 'num_layers': 3}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'dropout': 0.9533773978602583, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.37216033776861573, 'wordvec_source': 'glove', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0059540271759033 and batch: 50, loss is 7.4166624641418455 and perplexity is 1663.4723340347543
At time: 1.7894032001495361 and batch: 100, loss is 6.718125343322754 and perplexity is 827.2652186620088
At time: 2.5719382762908936 and batch: 150, loss is 6.49744891166687 and perplexity is 663.4469605331441
At time: 3.352093458175659 and batch: 200, loss is 6.2893821907043455 and perplexity is 538.8203380192307
At time: 4.133840799331665 and batch: 250, loss is 6.311991930007935 and perplexity is 551.1416918062946
At time: 4.916064262390137 and batch: 300, loss is 6.1996557331085205 and perplexity is 492.5794331094814
At time: 5.698505163192749 and batch: 350, loss is 6.209760837554931 and perplexity is 497.582234071427
At time: 6.478811502456665 and batch: 400, loss is 6.0988122177124025 and perplexity is 445.3285025099746
At time: 7.260959625244141 and batch: 450, loss is 6.091880359649658 and perplexity is 442.25222302375505
At time: 8.04297399520874 and batch: 500, loss is 6.067055749893188 and perplexity is 431.4086349621284
At time: 8.82724928855896 and batch: 550, loss is 6.10099536895752 and perplexity is 446.30178400850616
At time: 9.607808828353882 and batch: 600, loss is 6.037925548553467 and perplexity is 419.0228900217466
At time: 10.38967490196228 and batch: 650, loss is 5.972425260543823 and perplexity is 392.4563263336092
At time: 11.171411037445068 and batch: 700, loss is 6.069858989715576 and perplexity is 432.6196734499903
At time: 11.954038619995117 and batch: 750, loss is 6.015014562606812 and perplexity is 409.5318027238279
At time: 12.73632550239563 and batch: 800, loss is 6.030203943252563 and perplexity is 415.79982030319184
At time: 13.518617391586304 and batch: 850, loss is 6.066600618362426 and perplexity is 431.2123319649446
At time: 14.300754308700562 and batch: 900, loss is 5.943432416915893 and perplexity is 381.2412652791272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.951946937874572 and perplexity of 384.50121076505815
finished 1 epochs...
Completing Train Step...
At time: 15.977224349975586 and batch: 50, loss is 5.759890689849853 and perplexity is 317.31364142025
At time: 16.62873387336731 and batch: 100, loss is 5.527262783050537 and perplexity is 251.45468215135966
At time: 17.266178607940674 and batch: 150, loss is 5.4132153415679936 and perplexity is 224.351796859101
At time: 17.879213571548462 and batch: 200, loss is 5.209914379119873 and perplexity is 183.07838219041574
At time: 18.49479079246521 and batch: 250, loss is 5.225961818695068 and perplexity is 186.04002127166072
At time: 19.176533937454224 and batch: 300, loss is 5.131618452072144 and perplexity is 169.29088562991373
At time: 19.785826444625854 and batch: 350, loss is 5.079213857650757 and perplexity is 160.64771430234046
At time: 20.393279552459717 and batch: 400, loss is 4.912058792114258 and perplexity is 135.91895542491292
At time: 21.00228190422058 and batch: 450, loss is 4.899785680770874 and perplexity is 134.2610018868018
At time: 21.61053991317749 and batch: 500, loss is 4.80959942817688 and perplexity is 122.68246453478899
At time: 22.218695878982544 and batch: 550, loss is 4.858651399612427 and perplexity is 128.85031729532636
At time: 22.82660961151123 and batch: 600, loss is 4.779057378768921 and perplexity is 118.9921326533047
At time: 23.436100006103516 and batch: 650, loss is 4.650543823242187 and perplexity is 104.64187679103495
At time: 24.044595956802368 and batch: 700, loss is 4.709189348220825 and perplexity is 110.962171754146
At time: 24.65318727493286 and batch: 750, loss is 4.718446502685547 and perplexity is 111.99413486930635
At time: 25.26210379600525 and batch: 800, loss is 4.658584260940552 and perplexity is 105.4866348432244
At time: 25.871467351913452 and batch: 850, loss is 4.705063447952271 and perplexity is 110.50529605979929
At time: 26.48474907875061 and batch: 900, loss is 4.6310062599182125 and perplexity is 102.61727182244415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7104249719071065 and perplexity of 111.09936398337564
finished 2 epochs...
Completing Train Step...
At time: 28.0275297164917 and batch: 50, loss is 4.684811649322509 and perplexity is 108.28987391610114
At time: 28.645405530929565 and batch: 100, loss is 4.554328031539917 and perplexity is 95.04286796323886
At time: 29.25376534461975 and batch: 150, loss is 4.554341249465942 and perplexity is 95.04412424113947
At time: 29.861680030822754 and batch: 200, loss is 4.44554368019104 and perplexity is 85.24621191750123
At time: 30.471935987472534 and batch: 250, loss is 4.5700219631195065 and perplexity is 96.54623020595082
At time: 31.080459594726562 and batch: 300, loss is 4.533598003387451 and perplexity is 93.09290782876978
At time: 31.688889265060425 and batch: 350, loss is 4.520288124084472 and perplexity is 91.86206183800083
At time: 32.31456780433655 and batch: 400, loss is 4.41157470703125 and perplexity is 82.39911586228727
At time: 32.924015045166016 and batch: 450, loss is 4.434014329910278 and perplexity is 84.26902248428893
At time: 33.53419613838196 and batch: 500, loss is 4.321880021095276 and perplexity is 75.33011745897205
At time: 34.14688730239868 and batch: 550, loss is 4.4028249168396 and perplexity is 81.68128589698883
At time: 34.760035276412964 and batch: 600, loss is 4.38343584060669 and perplexity is 80.11281594640539
At time: 35.37317395210266 and batch: 650, loss is 4.234710540771484 and perplexity is 69.041691086216
At time: 35.984025716781616 and batch: 700, loss is 4.271114945411682 and perplexity is 71.60142280960449
At time: 36.59576869010925 and batch: 750, loss is 4.343026313781738 and perplexity is 76.94003204225359
At time: 37.206573724746704 and batch: 800, loss is 4.284171500205994 and perplexity is 72.54242043549908
At time: 37.820313453674316 and batch: 850, loss is 4.362359223365783 and perplexity is 78.44197844158236
At time: 38.43378210067749 and batch: 900, loss is 4.304246873855591 and perplexity is 74.01345299091481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.482766347388699 and perplexity of 88.47909835930042
finished 3 epochs...
Completing Train Step...
At time: 39.96682024002075 and batch: 50, loss is 4.382642040252685 and perplexity is 80.04924759837341
At time: 40.585721492767334 and batch: 100, loss is 4.252862319946289 and perplexity is 70.30636392344624
At time: 41.193336963653564 and batch: 150, loss is 4.261222348213196 and perplexity is 70.89659083243173
At time: 41.801478147506714 and batch: 200, loss is 4.154615001678467 and perplexity is 63.72742487263756
At time: 42.4394793510437 and batch: 250, loss is 4.295724353790283 and perplexity is 73.38535215543473
At time: 43.088980197906494 and batch: 300, loss is 4.271335635185242 and perplexity is 71.61722625515615
At time: 43.69749927520752 and batch: 350, loss is 4.258220233917236 and perplexity is 70.68407032859535
At time: 44.34601426124573 and batch: 400, loss is 4.16658474445343 and perplexity is 64.4948092909101
At time: 45.02316498756409 and batch: 450, loss is 4.200391149520874 and perplexity is 66.71242046946172
At time: 45.66297459602356 and batch: 500, loss is 4.0811610555648805 and perplexity is 59.214180907748165
At time: 46.28071713447571 and batch: 550, loss is 4.164665055274964 and perplexity is 64.3711180657859
At time: 46.89593553543091 and batch: 600, loss is 4.166721639633178 and perplexity is 64.50363892377412
At time: 47.5130569934845 and batch: 650, loss is 4.00626708984375 and perplexity is 54.9413959978991
At time: 48.148815870285034 and batch: 700, loss is 4.033981375694275 and perplexity is 56.48535356983297
At time: 48.76604223251343 and batch: 750, loss is 4.12742841720581 and perplexity is 62.018232674329525
At time: 49.405688762664795 and batch: 800, loss is 4.076599473953247 and perplexity is 58.94468571842657
At time: 50.055408000946045 and batch: 850, loss is 4.161926875114441 and perplexity is 64.19509944271515
At time: 50.668840646743774 and batch: 900, loss is 4.109852108955383 and perplexity is 60.93770476236806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386327456121576 and perplexity of 80.34480666026298
finished 4 epochs...
Completing Train Step...
At time: 52.2221302986145 and batch: 50, loss is 4.2005655527114865 and perplexity is 66.72405634308235
At time: 52.83587908744812 and batch: 100, loss is 4.070364909172058 and perplexity is 58.5783344637889
At time: 53.45014691352844 and batch: 150, loss is 4.077891063690186 and perplexity is 59.02086725660345
At time: 54.06413388252258 and batch: 200, loss is 3.978629503250122 and perplexity is 53.44373954255524
At time: 54.67784261703491 and batch: 250, loss is 4.120511832237244 and perplexity is 61.59075833390191
At time: 55.29197144508362 and batch: 300, loss is 4.100255136489868 and perplexity is 60.35568457061852
At time: 55.90626120567322 and batch: 350, loss is 4.088844103813171 and perplexity is 59.670878484865355
At time: 56.51928210258484 and batch: 400, loss is 4.0050556135177615 and perplexity is 54.87487609910801
At time: 57.13296389579773 and batch: 450, loss is 4.044483275413513 and perplexity is 57.08168290331063
At time: 57.7473464012146 and batch: 500, loss is 3.9226668071746826 and perplexity is 50.53503242569495
At time: 58.3622510433197 and batch: 550, loss is 4.000019092559814 and perplexity is 54.59919246154077
At time: 58.97640657424927 and batch: 600, loss is 4.015768985748291 and perplexity is 55.46593151796328
At time: 59.589921951293945 and batch: 650, loss is 3.857176022529602 and perplexity is 47.33149937242861
At time: 60.20308542251587 and batch: 700, loss is 3.8785499811172484 and perplexity is 48.35404992779467
At time: 60.81784987449646 and batch: 750, loss is 3.9780347537994385 and perplexity is 53.4119633581906
At time: 61.496068716049194 and batch: 800, loss is 3.93397780418396 and perplexity is 51.10987894119014
At time: 62.13976573944092 and batch: 850, loss is 4.02082670211792 and perplexity is 55.74717308828075
At time: 62.75165152549744 and batch: 900, loss is 3.973913688659668 and perplexity is 53.192302107987594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34805799510381 and perplexity of 77.32814537686029
finished 5 epochs...
Completing Train Step...
At time: 64.29445290565491 and batch: 50, loss is 4.062743511199951 and perplexity is 58.133582632458165
At time: 64.9159529209137 and batch: 100, loss is 3.934783673286438 and perplexity is 51.15108341393605
At time: 65.52960109710693 and batch: 150, loss is 3.9466206884384154 and perplexity is 51.760157259839964
At time: 66.14474034309387 and batch: 200, loss is 3.8483168029785157 and perplexity is 46.914031179434005
At time: 66.76032757759094 and batch: 250, loss is 3.9884050130844115 and perplexity is 53.968741242894225
At time: 67.37562346458435 and batch: 300, loss is 3.9741377067565917 and perplexity is 53.2042194810806
At time: 67.99083495140076 and batch: 350, loss is 3.962831373214722 and perplexity is 52.60606268194349
At time: 68.60505795478821 and batch: 400, loss is 3.884249382019043 and perplexity is 48.63042588411571
At time: 69.21903944015503 and batch: 450, loss is 3.9243575620651243 and perplexity is 50.620547050671135
At time: 69.83326578140259 and batch: 500, loss is 3.8029314994812013 and perplexity is 44.83241825432136
At time: 70.44728660583496 and batch: 550, loss is 3.8766125059127807 and perplexity is 48.26045585240202
At time: 71.06162333488464 and batch: 600, loss is 3.8998560810089113 and perplexity is 49.39533966650151
At time: 71.67685151100159 and batch: 650, loss is 3.7403317213058473 and perplexity is 42.11195728173501
At time: 72.29074931144714 and batch: 700, loss is 3.7582537794113158 and perplexity is 42.87349399599361
At time: 72.9052746295929 and batch: 750, loss is 3.8641675662994386 and perplexity is 47.66357914439621
At time: 73.52029347419739 and batch: 800, loss is 3.823069682121277 and perplexity is 45.744413826858036
At time: 74.13426375389099 and batch: 850, loss is 3.9063564491271974 and perplexity is 49.71747341733651
At time: 74.74795651435852 and batch: 900, loss is 3.866750087738037 and perplexity is 47.78683044043294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332324720408819 and perplexity of 76.12104117502439
finished 6 epochs...
Completing Train Step...
At time: 76.29810452461243 and batch: 50, loss is 3.9537138605117796 and perplexity is 52.12860615271153
At time: 76.93123412132263 and batch: 100, loss is 3.8281666803359986 and perplexity is 45.97816823931228
At time: 77.54651093482971 and batch: 150, loss is 3.8421362686157225 and perplexity is 46.62497158914617
At time: 78.16197490692139 and batch: 200, loss is 3.747592797279358 and perplexity is 42.41884823385487
At time: 78.77684998512268 and batch: 250, loss is 3.8835622024536134 and perplexity is 48.59701952858702
At time: 79.39921593666077 and batch: 300, loss is 3.8729993677139283 and perplexity is 48.0863987913319
At time: 80.01209259033203 and batch: 350, loss is 3.864338607788086 and perplexity is 47.67173229117064
At time: 80.62702894210815 and batch: 400, loss is 3.7844964933395384 and perplexity is 44.013503886327605
At time: 81.24018788337708 and batch: 450, loss is 3.8261420822143553 and perplexity is 45.88517509489601
At time: 81.85466122627258 and batch: 500, loss is 3.708882002830505 and perplexity is 40.80815760936513
At time: 82.47066307067871 and batch: 550, loss is 3.7775465965271 and perplexity is 43.70867506734454
At time: 83.08500337600708 and batch: 600, loss is 3.804352140426636 and perplexity is 44.89615428566476
At time: 83.69913935661316 and batch: 650, loss is 3.6508083629608152 and perplexity is 38.50578011803208
At time: 84.33688378334045 and batch: 700, loss is 3.66284481048584 and perplexity is 38.972053427637846
At time: 84.97118282318115 and batch: 750, loss is 3.7734429121017454 and perplexity is 43.529675987284975
At time: 85.58575344085693 and batch: 800, loss is 3.732966685295105 and perplexity is 41.80294055637831
At time: 86.19995760917664 and batch: 850, loss is 3.8141414165496825 and perplexity is 45.33781337119275
At time: 86.81328654289246 and batch: 900, loss is 3.776880669593811 and perplexity is 43.67957797274359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328781336954195 and perplexity of 75.85179244463846
finished 7 epochs...
Completing Train Step...
At time: 88.36808395385742 and batch: 50, loss is 3.8634807109832763 and perplexity is 47.630852402228605
At time: 89.0067081451416 and batch: 100, loss is 3.740259261131287 and perplexity is 42.10890595251055
At time: 89.61832690238953 and batch: 150, loss is 3.758725528717041 and perplexity is 42.893724308462986
At time: 90.23147106170654 and batch: 200, loss is 3.6642512559890745 and perplexity is 39.02690406010404
At time: 90.84585809707642 and batch: 250, loss is 3.8027131366729736 and perplexity is 44.82262959035068
At time: 91.4601776599884 and batch: 300, loss is 3.787809381484985 and perplexity is 44.15955749765002
At time: 92.07340979576111 and batch: 350, loss is 3.7847156715393067 and perplexity is 44.023151744136335
At time: 92.68809843063354 and batch: 400, loss is 3.7004628658294676 and perplexity is 40.46603036863016
At time: 93.30213713645935 and batch: 450, loss is 3.744360103607178 and perplexity is 42.281942497966725
At time: 93.91642475128174 and batch: 500, loss is 3.6306393480300905 and perplexity is 37.73693594169482
At time: 94.5389347076416 and batch: 550, loss is 3.6995649003982543 and perplexity is 40.429709582066664
At time: 95.17106533050537 and batch: 600, loss is 3.7264798784255984 and perplexity is 41.5326505616949
At time: 95.81894946098328 and batch: 650, loss is 3.570755515098572 and perplexity is 35.5434366130477
At time: 96.43227577209473 and batch: 700, loss is 3.585755033493042 and perplexity is 36.0805894906298
At time: 97.04629373550415 and batch: 750, loss is 3.693876233100891 and perplexity is 40.20037134824697
At time: 97.65925192832947 and batch: 800, loss is 3.6557317209243774 and perplexity is 38.69582530357574
At time: 98.29233288764954 and batch: 850, loss is 3.7354496002197264 and perplexity is 41.90686266286458
At time: 98.93943643569946 and batch: 900, loss is 3.7000852680206298 and perplexity is 40.450753368692965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32883192088506 and perplexity of 75.85562942350734
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.48353672027588 and batch: 50, loss is 3.8119159936904907 and perplexity is 45.237029749612674
At time: 101.10768389701843 and batch: 100, loss is 3.689386668205261 and perplexity is 40.020293709845724
At time: 101.72088932991028 and batch: 150, loss is 3.7019656324386596 and perplexity is 40.526887083141176
At time: 102.39553332328796 and batch: 200, loss is 3.5901195430755615 and perplexity is 36.23840771826554
At time: 103.00915455818176 and batch: 250, loss is 3.7307377862930298 and perplexity is 41.709869785094234
At time: 103.619131565094 and batch: 300, loss is 3.698123426437378 and perplexity is 40.37147319166236
At time: 104.23173499107361 and batch: 350, loss is 3.6812257289886476 and perplexity is 39.69501960185515
At time: 104.84131383895874 and batch: 400, loss is 3.5901603651046754 and perplexity is 36.2398870737954
At time: 105.47181844711304 and batch: 450, loss is 3.6226917839050294 and perplexity is 37.43820787604147
At time: 106.11411738395691 and batch: 500, loss is 3.498686876296997 and perplexity is 33.0719958116823
At time: 106.73013091087341 and batch: 550, loss is 3.548785424232483 and perplexity is 34.77105974360549
At time: 107.34418845176697 and batch: 600, loss is 3.5710008430480955 and perplexity is 35.55215748116361
At time: 107.95909571647644 and batch: 650, loss is 3.410182876586914 and perplexity is 30.270779570096295
At time: 108.57244086265564 and batch: 700, loss is 3.3977910566329954 and perplexity is 29.89798409742764
At time: 109.21454381942749 and batch: 750, loss is 3.4953129291534424 and perplexity is 32.96060067221051
At time: 109.83342146873474 and batch: 800, loss is 3.4414173364639282 and perplexity is 31.231191920970815
At time: 110.46557188034058 and batch: 850, loss is 3.5074483489990236 and perplexity is 33.36302827496127
At time: 111.07918167114258 and batch: 900, loss is 3.467385420799255 and perplexity is 32.05282813448589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.297141715271832 and perplexity of 73.48943947414266
finished 9 epochs...
Completing Train Step...
At time: 112.62257027626038 and batch: 50, loss is 3.719904980659485 and perplexity is 41.26047337937134
At time: 113.24446415901184 and batch: 100, loss is 3.5920686960220336 and perplexity is 36.309110800621134
At time: 113.85843014717102 and batch: 150, loss is 3.606755509376526 and perplexity is 36.84631115845917
At time: 114.4726676940918 and batch: 200, loss is 3.500158219337463 and perplexity is 33.12069187807837
At time: 115.08667206764221 and batch: 250, loss is 3.642080850601196 and perplexity is 38.17118267164945
At time: 115.7007532119751 and batch: 300, loss is 3.61599826335907 and perplexity is 37.188451270395895
At time: 116.3229763507843 and batch: 350, loss is 3.6011774253845217 and perplexity is 36.64135151252513
At time: 116.98105192184448 and batch: 400, loss is 3.5166804695129397 and perplexity is 33.6724659578641
At time: 117.59444284439087 and batch: 450, loss is 3.5536254453659057 and perplexity is 34.939760335482205
At time: 118.2100236415863 and batch: 500, loss is 3.4334541082382204 and perplexity is 30.983478420532578
At time: 118.82428121566772 and batch: 550, loss is 3.4861418533325197 and perplexity is 32.65969841210042
At time: 119.46891379356384 and batch: 600, loss is 3.514157156944275 and perplexity is 33.58760690927217
At time: 120.11463212966919 and batch: 650, loss is 3.359416742324829 and perplexity is 28.77240425863894
At time: 120.72394919395447 and batch: 700, loss is 3.3502481174468994 and perplexity is 28.509806546687646
At time: 121.34403657913208 and batch: 750, loss is 3.4541364908218384 and perplexity is 31.630963259007636
At time: 121.95482730865479 and batch: 800, loss is 3.4062415552139282 and perplexity is 30.151707504377395
At time: 122.56344389915466 and batch: 850, loss is 3.4785022068023683 and perplexity is 32.41114051658832
At time: 123.17167282104492 and batch: 900, loss is 3.445643982887268 and perplexity is 31.363474485483774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30591823630137 and perplexity of 74.13725973087344
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 124.72146987915039 and batch: 50, loss is 3.6940016603469847 and perplexity is 40.20541388634634
At time: 125.37600302696228 and batch: 100, loss is 3.578528118133545 and perplexity is 35.820778072615504
At time: 126.01383519172668 and batch: 150, loss is 3.5890055894851685 and perplexity is 36.19806228951729
At time: 126.62350606918335 and batch: 200, loss is 3.4821348667144774 and perplexity is 32.52909327887337
At time: 127.2324206829071 and batch: 250, loss is 3.624205460548401 and perplexity is 37.49492012805041
At time: 127.84211945533752 and batch: 300, loss is 3.591288366317749 and perplexity is 36.280788774623474
At time: 128.45255041122437 and batch: 350, loss is 3.5747790718078614 and perplexity is 35.68673573865997
At time: 129.06356024742126 and batch: 400, loss is 3.494749593734741 and perplexity is 32.942038027418725
At time: 129.67239332199097 and batch: 450, loss is 3.5201412773132326 and perplexity is 33.789201774304324
At time: 130.28201746940613 and batch: 500, loss is 3.3982783555984497 and perplexity is 29.91255690451602
At time: 130.8910140991211 and batch: 550, loss is 3.444721474647522 and perplexity is 31.334554763235825
At time: 131.50121068954468 and batch: 600, loss is 3.472476987838745 and perplexity is 32.21644343340526
At time: 132.1429407596588 and batch: 650, loss is 3.3109685754776 and perplexity is 27.411662882554257
At time: 132.77530074119568 and batch: 700, loss is 3.2950324773788453 and perplexity is 26.978290239824602
At time: 133.38554763793945 and batch: 750, loss is 3.3937747859954834 and perplexity is 29.778146512948183
At time: 134.04319834709167 and batch: 800, loss is 3.3452719593048097 and perplexity is 28.368289637991456
At time: 134.68936419487 and batch: 850, loss is 3.411528401374817 and perplexity is 30.311537068320014
At time: 135.32165384292603 and batch: 900, loss is 3.382924213409424 and perplexity is 29.45678321360164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.297941861087328 and perplexity of 73.54826527309099
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 136.9019811153412 and batch: 50, loss is 3.683409514427185 and perplexity is 39.78179972774449
At time: 137.51763081550598 and batch: 100, loss is 3.5678437995910643 and perplexity is 35.44009476151898
At time: 138.1274037361145 and batch: 150, loss is 3.5807208824157715 and perplexity is 35.899410775318735
At time: 138.73728728294373 and batch: 200, loss is 3.4680593252182006 and perplexity is 32.074435956991195
At time: 139.3579273223877 and batch: 250, loss is 3.609500222206116 and perplexity is 36.94758261837958
At time: 140.0101249217987 and batch: 300, loss is 3.5787961769104 and perplexity is 35.83038143364675
At time: 140.62170004844666 and batch: 350, loss is 3.5599521255493163 and perplexity is 35.161513766654735
At time: 141.24731516838074 and batch: 400, loss is 3.4824150371551514 and perplexity is 32.5382082460846
At time: 141.8619236946106 and batch: 450, loss is 3.5060968732833864 and perplexity is 33.31796940728855
At time: 142.4797558784485 and batch: 500, loss is 3.383686408996582 and perplexity is 29.47924360229614
At time: 143.09483695030212 and batch: 550, loss is 3.4303728008270262 and perplexity is 30.888155733521526
At time: 143.707013130188 and batch: 600, loss is 3.4615391588211057 and perplexity is 31.865985601440016
At time: 144.3196234703064 and batch: 650, loss is 3.294174842834473 and perplexity is 26.955162645096507
At time: 144.92841601371765 and batch: 700, loss is 3.277479748725891 and perplexity is 26.50887940423045
At time: 145.53709292411804 and batch: 750, loss is 3.375295934677124 and perplexity is 29.232933540038584
At time: 146.14636778831482 and batch: 800, loss is 3.326108179092407 and perplexity is 27.829821987494057
At time: 146.7554268836975 and batch: 850, loss is 3.3914828586578367 and perplexity is 29.709975316446705
At time: 147.36115908622742 and batch: 900, loss is 3.364282965660095 and perplexity is 28.91275842404453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.289361823095034 and perplexity of 72.91991783995728
finished 12 epochs...
Completing Train Step...
At time: 148.88903641700745 and batch: 50, loss is 3.670402069091797 and perplexity is 39.26769101159312
At time: 149.50943779945374 and batch: 100, loss is 3.5510658836364746 and perplexity is 34.85044421591454
At time: 150.11950635910034 and batch: 150, loss is 3.563620619773865 and perplexity is 35.290740465607385
At time: 150.7287359237671 and batch: 200, loss is 3.4524913215637207 and perplexity is 31.578967753080732
At time: 151.33770442008972 and batch: 250, loss is 3.5945984172821044 and perplexity is 36.401079008066745
At time: 151.94788789749146 and batch: 300, loss is 3.563964533805847 and perplexity is 35.30287953373083
At time: 152.56156373023987 and batch: 350, loss is 3.5460136461257936 and perplexity is 34.674815526886555
At time: 153.17545986175537 and batch: 400, loss is 3.4699072885513305 and perplexity is 32.13376313891559
At time: 153.78537464141846 and batch: 450, loss is 3.495279097557068 and perplexity is 32.959485581335066
At time: 154.39522099494934 and batch: 500, loss is 3.37449089050293 and perplexity is 29.20940920753149
At time: 155.00892758369446 and batch: 550, loss is 3.422007555961609 and perplexity is 30.630846475327584
At time: 155.62648105621338 and batch: 600, loss is 3.45411497592926 and perplexity is 31.63028272955176
At time: 156.23910474777222 and batch: 650, loss is 3.2887891578674315 and perplexity is 26.810380855365636
At time: 156.87039279937744 and batch: 700, loss is 3.2738852310180664 and perplexity is 26.413763817539856
At time: 157.48425841331482 and batch: 750, loss is 3.373818235397339 and perplexity is 29.189767955931416
At time: 158.0976824760437 and batch: 800, loss is 3.326740889549255 and perplexity is 27.847435778505044
At time: 158.71162486076355 and batch: 850, loss is 3.394599962234497 and perplexity is 29.80272887288731
At time: 159.32640314102173 and batch: 900, loss is 3.3694952011108397 and perplexity is 29.06385195393957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.288750217385488 and perplexity of 72.87533323735522
finished 13 epochs...
Completing Train Step...
At time: 160.88100242614746 and batch: 50, loss is 3.6640944051742554 and perplexity is 39.02078313845067
At time: 161.4947874546051 and batch: 100, loss is 3.5436193895339967 and perplexity is 34.59189442804757
At time: 162.14089465141296 and batch: 150, loss is 3.5558272409439087 and perplexity is 35.01677529977584
At time: 162.7682068347931 and batch: 200, loss is 3.4445530366897583 and perplexity is 31.329277279300854
At time: 163.3818428516388 and batch: 250, loss is 3.586707925796509 and perplexity is 36.114986792519346
At time: 163.9950556755066 and batch: 300, loss is 3.5560384082794187 and perplexity is 35.024170479696785
At time: 164.60933804512024 and batch: 350, loss is 3.53835391998291 and perplexity is 34.41023055374324
At time: 165.23914003372192 and batch: 400, loss is 3.462879056930542 and perplexity is 31.90871139301416
At time: 165.85481929779053 and batch: 450, loss is 3.488787541389465 and perplexity is 32.746220190482
At time: 166.4676444530487 and batch: 500, loss is 3.3688599920272826 and perplexity is 29.04539619342919
At time: 167.08052110671997 and batch: 550, loss is 3.416774272918701 and perplexity is 30.470965302485688
At time: 167.69505882263184 and batch: 600, loss is 3.449555697441101 and perplexity is 31.486399712542166
At time: 168.3094277381897 and batch: 650, loss is 3.2852562999725343 and perplexity is 26.715830704160318
At time: 168.92534756660461 and batch: 700, loss is 3.2712866973876955 and perplexity is 26.345215864607596
At time: 169.53937292099 and batch: 750, loss is 3.3723398637771607 and perplexity is 29.146646513997556
At time: 170.15287709236145 and batch: 800, loss is 3.3261024284362795 and perplexity is 27.82966194821788
At time: 170.7656524181366 and batch: 850, loss is 3.3950164031982424 and perplexity is 29.81514253462061
At time: 171.40898847579956 and batch: 900, loss is 3.3707434129714966 and perplexity is 29.100152449306172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.289024039490582 and perplexity of 72.89529084680433
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 172.96025013923645 and batch: 50, loss is 3.6616001176834105 and perplexity is 38.92357536964426
At time: 173.58071851730347 and batch: 100, loss is 3.5437529277801514 and perplexity is 34.596514077403725
At time: 174.19283318519592 and batch: 150, loss is 3.5582174682617187 and perplexity is 35.100573461109974
At time: 174.8050193786621 and batch: 200, loss is 3.445629177093506 and perplexity is 31.363010127786477
At time: 175.41712498664856 and batch: 250, loss is 3.5858749866485597 and perplexity is 36.084917730779985
At time: 176.02843403816223 and batch: 300, loss is 3.555514388084412 and perplexity is 35.00582191498116
At time: 176.64050436019897 and batch: 350, loss is 3.5371234035491943 and perplexity is 34.36791424035793
At time: 177.25328373908997 and batch: 400, loss is 3.460753016471863 and perplexity is 31.84094424498343
At time: 177.86665606498718 and batch: 450, loss is 3.4862454605102537 and perplexity is 32.6630823665765
At time: 178.47770404815674 and batch: 500, loss is 3.3657664394378664 and perplexity is 28.955681572820737
At time: 179.0900218486786 and batch: 550, loss is 3.4123069047927856 and perplexity is 30.335143891335505
At time: 179.70233798027039 and batch: 600, loss is 3.446250672340393 and perplexity is 31.382508147844767
At time: 180.3149929046631 and batch: 650, loss is 3.279975037574768 and perplexity is 26.57510931241487
At time: 180.93327140808105 and batch: 700, loss is 3.2645021581649782 and perplexity is 26.167080680112235
At time: 181.58493852615356 and batch: 750, loss is 3.364442048072815 and perplexity is 28.917358301283095
At time: 182.2393922805786 and batch: 800, loss is 3.3177872705459595 and perplexity is 27.599213351121456
At time: 182.8683512210846 and batch: 850, loss is 3.3851991844177247 and perplexity is 29.523872825936742
At time: 183.4973931312561 and batch: 900, loss is 3.3599294662475585 and perplexity is 28.787160341191786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.28618924911708 and perplexity of 72.68894059607686
finished 15 epochs...
Completing Train Step...
At time: 185.0841703414917 and batch: 50, loss is 3.65753267288208 and perplexity is 38.76557741716089
At time: 185.75635504722595 and batch: 100, loss is 3.539273543357849 and perplexity is 34.44188956104788
At time: 186.38230109214783 and batch: 150, loss is 3.553514289855957 and perplexity is 34.93587680444656
At time: 186.9990246295929 and batch: 200, loss is 3.4416575193405152 and perplexity is 31.23869401938736
At time: 187.61620497703552 and batch: 250, loss is 3.5821827173233034 and perplexity is 35.951928163673394
At time: 188.27010250091553 and batch: 300, loss is 3.551928482055664 and perplexity is 34.880519123422246
At time: 188.91624927520752 and batch: 350, loss is 3.5335320472717284 and perplexity is 34.24470818664892
At time: 189.53225302696228 and batch: 400, loss is 3.4577780675888063 and perplexity is 31.746359825131975
At time: 190.14668607711792 and batch: 450, loss is 3.483542966842651 and perplexity is 32.574929762894726
At time: 190.76222229003906 and batch: 500, loss is 3.36341845035553 and perplexity is 28.887773703282587
At time: 191.37728881835938 and batch: 550, loss is 3.410407075881958 and perplexity is 30.277567018378466
At time: 191.9940369129181 and batch: 600, loss is 3.4445829820632934 and perplexity is 31.330215460258586
At time: 192.61122608184814 and batch: 650, loss is 3.278956069946289 and perplexity is 26.548043928019332
At time: 193.22672057151794 and batch: 700, loss is 3.2641105318069457 and perplexity is 26.156834967981553
At time: 193.84379744529724 and batch: 750, loss is 3.364439706802368 and perplexity is 28.91729059800596
At time: 194.4607424736023 and batch: 800, loss is 3.3186244058609007 and perplexity is 27.622327300684155
At time: 195.07725834846497 and batch: 850, loss is 3.38680380821228 and perplexity is 29.57128556441157
At time: 195.69394993782043 and batch: 900, loss is 3.3622084569931032 and perplexity is 28.85284082738237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.285536256555009 and perplexity of 72.64149075240633
finished 16 epochs...
Completing Train Step...
At time: 197.2449586391449 and batch: 50, loss is 3.655363450050354 and perplexity is 38.68157738187844
At time: 197.86244344711304 and batch: 100, loss is 3.536650619506836 and perplexity is 34.35166947937042
At time: 198.4794418811798 and batch: 150, loss is 3.550870084762573 and perplexity is 34.84362120617271
At time: 199.0955629348755 and batch: 200, loss is 3.439214811325073 and perplexity is 31.162480133145888
At time: 199.71297311782837 and batch: 250, loss is 3.5798306703567504 and perplexity is 35.86746690745255
At time: 200.33001232147217 and batch: 300, loss is 3.5495888090133665 and perplexity is 34.79900550791648
At time: 200.94582748413086 and batch: 350, loss is 3.5312461709976195 and perplexity is 34.166518420803214
At time: 201.5624074935913 and batch: 400, loss is 3.455729966163635 and perplexity is 31.681406598433252
At time: 202.17931723594666 and batch: 450, loss is 3.4816913986206055 and perplexity is 32.514670862058814
At time: 202.79684138298035 and batch: 500, loss is 3.361847286224365 and perplexity is 28.842421906298593
At time: 203.42102527618408 and batch: 550, loss is 3.4090582513809204 and perplexity is 30.23675542417884
At time: 204.0377323627472 and batch: 600, loss is 3.443434143066406 and perplexity is 31.294242754337148
At time: 204.653222322464 and batch: 650, loss is 3.278170509338379 and perplexity is 26.527197019819408
At time: 205.2692563533783 and batch: 700, loss is 3.263681330680847 and perplexity is 26.145610833835935
At time: 205.8863296508789 and batch: 750, loss is 3.364329857826233 and perplexity is 28.914114237704318
At time: 206.50286602973938 and batch: 800, loss is 3.319022579193115 and perplexity is 27.63332796472918
At time: 207.12013459205627 and batch: 850, loss is 3.3876033639907837 and perplexity is 29.59493891148743
At time: 207.73662209510803 and batch: 900, loss is 3.3633146619796754 and perplexity is 28.884775643752427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.285366110605736 and perplexity of 72.62913214841868
finished 17 epochs...
Completing Train Step...
At time: 209.27663683891296 and batch: 50, loss is 3.6536138677597045 and perplexity is 38.61395994749789
At time: 209.9011664390564 and batch: 100, loss is 3.5346037197113036 and perplexity is 34.28142696835279
At time: 210.519104719162 and batch: 150, loss is 3.5488172721862794 and perplexity is 34.772167148343854
At time: 211.1362075805664 and batch: 200, loss is 3.4372338819503785 and perplexity is 31.100810562560117
At time: 211.75225067138672 and batch: 250, loss is 3.5779103422164917 and perplexity is 35.7986556926325
At time: 212.36913228034973 and batch: 300, loss is 3.547672028541565 and perplexity is 34.7323673394934
At time: 212.98618388175964 and batch: 350, loss is 3.5293849754333495 and perplexity is 34.10298698882123
At time: 213.60232090950012 and batch: 400, loss is 3.4540349864959716 and perplexity is 31.627752742348935
At time: 214.25963139533997 and batch: 450, loss is 3.4801411390304566 and perplexity is 32.46430373288418
At time: 214.87729215621948 and batch: 500, loss is 3.3605269718170168 and perplexity is 28.804365969542033
At time: 215.4943025112152 and batch: 550, loss is 3.4078824424743654 and perplexity is 30.2012236712152
At time: 216.11040544509888 and batch: 600, loss is 3.4424371004104612 and perplexity is 31.26305660896768
At time: 216.72670459747314 and batch: 650, loss is 3.2774316453933716 and perplexity is 26.50760426945911
At time: 217.34469509124756 and batch: 700, loss is 3.263197407722473 and perplexity is 26.132961433407218
At time: 217.96202039718628 and batch: 750, loss is 3.3641158294677735 and perplexity is 28.907926459501343
At time: 218.5793821811676 and batch: 800, loss is 3.319147343635559 and perplexity is 27.636775836567004
At time: 219.20624446868896 and batch: 850, loss is 3.3879852199554445 and perplexity is 29.606242073387104
At time: 219.82239866256714 and batch: 900, loss is 3.363888816833496 and perplexity is 28.901364739789095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.285368618899828 and perplexity of 72.62931432387026
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 221.37466979026794 and batch: 50, loss is 3.6527344799041748 and perplexity is 38.580018226222215
At time: 222.04367470741272 and batch: 100, loss is 3.5342774057388304 and perplexity is 34.27024228469522
At time: 222.70362305641174 and batch: 150, loss is 3.549232726097107 and perplexity is 34.78661638246116
At time: 223.34517741203308 and batch: 200, loss is 3.4375596141815183 and perplexity is 31.11094274907519
At time: 224.00130462646484 and batch: 250, loss is 3.577536587715149 and perplexity is 35.785278284014424
At time: 224.61877465248108 and batch: 300, loss is 3.5476535511016847 and perplexity is 34.731725580193036
At time: 225.2713861465454 and batch: 350, loss is 3.5290862131118774 and perplexity is 34.09279982310573
At time: 225.9024305343628 and batch: 400, loss is 3.4532602262496948 and perplexity is 31.603258306727817
At time: 226.5222945213318 and batch: 450, loss is 3.479538230895996 and perplexity is 32.4447366392567
At time: 227.14086747169495 and batch: 500, loss is 3.359419569969177 and perplexity is 28.772485616880246
At time: 227.76020002365112 and batch: 550, loss is 3.4064090633392334 and perplexity is 30.156758583412785
At time: 228.37856793403625 and batch: 600, loss is 3.440959248542786 and perplexity is 31.21688856553624
At time: 228.99788904190063 and batch: 650, loss is 3.2752331018447878 and perplexity is 26.44939016367604
At time: 229.61791563034058 and batch: 700, loss is 3.260666399002075 and perplexity is 26.0669023134983
At time: 230.2384638786316 and batch: 750, loss is 3.361325197219849 and perplexity is 28.82736752516308
At time: 230.8587682247162 and batch: 800, loss is 3.316247138977051 and perplexity is 27.556739647304056
At time: 231.47597002983093 and batch: 850, loss is 3.384715127944946 and perplexity is 29.509585062515022
At time: 232.09363460540771 and batch: 900, loss is 3.3600574779510497 and perplexity is 28.790845670503344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.284586867241011 and perplexity of 72.57255842431854
finished 19 epochs...
Completing Train Step...
At time: 233.66790533065796 and batch: 50, loss is 3.6519240665435793 and perplexity is 38.548765129673676
At time: 234.30712270736694 and batch: 100, loss is 3.533375267982483 and perplexity is 34.239339746480105
At time: 234.93385934829712 and batch: 150, loss is 3.548259997367859 and perplexity is 34.7527948935464
At time: 235.5522346496582 and batch: 200, loss is 3.436746416091919 and perplexity is 31.08565367377349
At time: 236.17116975784302 and batch: 250, loss is 3.5768745851516726 and perplexity is 35.761596177728116
At time: 236.78950428962708 and batch: 300, loss is 3.546829743385315 and perplexity is 34.703125098923316
At time: 237.40830397605896 and batch: 350, loss is 3.5283795547485353 and perplexity is 34.06871637136948
At time: 238.0278079509735 and batch: 400, loss is 3.452696738243103 and perplexity is 31.58545526607377
At time: 238.64403200149536 and batch: 450, loss is 3.4789515209197996 and perplexity is 32.42570657170652
At time: 239.25963282585144 and batch: 500, loss is 3.3589904165267943 and perplexity is 28.760140454805722
At time: 239.8778955936432 and batch: 550, loss is 3.4060255432128907 and perplexity is 30.145195077111225
At time: 240.49565410614014 and batch: 600, loss is 3.4406660175323487 and perplexity is 31.207736147711408
At time: 241.11274766921997 and batch: 650, loss is 3.275122838020325 and perplexity is 26.44647391354328
At time: 241.74580931663513 and batch: 700, loss is 3.260658903121948 and perplexity is 26.066706919855605
At time: 242.37428545951843 and batch: 750, loss is 3.3614041948318483 and perplexity is 28.82964490831053
At time: 243.01366782188416 and batch: 800, loss is 3.3164799308776853 and perplexity is 27.56315537983815
At time: 243.63045620918274 and batch: 850, loss is 3.3851454067230224 and perplexity is 29.522285142808833
At time: 244.24189949035645 and batch: 900, loss is 3.3604938125610353 and perplexity is 28.80341085403301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.28420058995077 and perplexity of 72.5445307066885
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f67603d0b70>
ELAPSED
1509.3991088867188


RESULTS SO FAR:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.03606636022589, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.57190604832506, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.17348567797130798, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.17852101364501027, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.27835342500566, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.2729052577440735, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.7700012466142289, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.276244984972, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.6853646424831196, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.08897995277100423, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.5445307066885, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.9533773978602583, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.37216033776861573, 'wordvec_source': 'glove', 'num_layers': 3}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -70.6895275573355, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.8684389293484294, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.4694082358096594, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.03606636022589, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.1974852826886876, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.6994175184177001, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.57190604832506, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.17348567797130798, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.17852101364501027, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.27835342500566, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.2729052577440735, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.7700012466142289, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -71.276244984972, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.6853646424831196, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.08897995277100423, 'wordvec_source': 'glove', 'num_layers': 3}}, {'best_accuracy': -72.5445307066885, 'params': {'batch_size': 32, 'data': 'ptb', 'dropout': 0.9533773978602583, 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'tie_weights': 'FALSE', 'rnn_dropout': 0.37216033776861573, 'wordvec_source': 'glove', 'num_layers': 3}}]
