TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.6965947151184082 and batch: 50, loss is 6.923568334579468 and perplexity is 1015.938743590799
At time: 2.830669641494751 and batch: 100, loss is 5.9806396484375 and perplexity is 395.6933918834281
At time: 3.9614038467407227 and batch: 150, loss is 5.748197832107544 and perplexity is 313.6249458749025
At time: 5.090010166168213 and batch: 200, loss is 5.518330268859863 and perplexity is 249.21856159232715
At time: 6.219916582107544 and batch: 250, loss is 5.527679014205932 and perplexity is 251.55936720932112
At time: 7.3516294956207275 and batch: 300, loss is 5.414965105056763 and perplexity is 224.74470308812604
At time: 8.480819940567017 and batch: 350, loss is 5.363494453430175 and perplexity is 213.46960419649645
At time: 9.616171836853027 and batch: 400, loss is 5.198760871887207 and perplexity is 181.04776145289864
At time: 10.743056535720825 and batch: 450, loss is 5.200395212173462 and perplexity is 181.3438970304132
At time: 11.869779348373413 and batch: 500, loss is 5.122461462020874 and perplexity is 167.74776661973115
At time: 12.996214628219604 and batch: 550, loss is 5.175847396850586 and perplexity is 176.94649466568055
At time: 14.125210285186768 and batch: 600, loss is 5.08846643447876 and perplexity is 162.14101741859102
At time: 15.252583980560303 and batch: 650, loss is 4.968332080841065 and perplexity is 143.78686239284647
At time: 16.381245136260986 and batch: 700, loss is 5.045433759689331 and perplexity is 155.31165252387356
At time: 17.510804176330566 and batch: 750, loss is 5.043028955459595 and perplexity is 154.93860713530032
At time: 18.63944172859192 and batch: 800, loss is 4.987584352493286 and perplexity is 146.58190525857466
At time: 19.765486240386963 and batch: 850, loss is 5.034842338562012 and perplexity is 153.67536202321804
At time: 20.89161705970764 and batch: 900, loss is 4.951859292984008 and perplexity is 141.43769371155395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.9447870123876285 and perplexity of 140.4409354862297
finished 1 epochs...
Completing Train Step...
At time: 23.195175409317017 and batch: 50, loss is 4.913407545089722 and perplexity is 136.1024002035005
At time: 24.120750188827515 and batch: 100, loss is 4.778690500259399 and perplexity is 118.94848500421385
At time: 25.03605079650879 and batch: 150, loss is 4.757788333892822 and perplexity is 116.488008210566
At time: 25.950360536575317 and batch: 200, loss is 4.643610725402832 and perplexity is 103.91889357385753
At time: 26.864778757095337 and batch: 250, loss is 4.753701753616333 and perplexity is 116.01294197306244
At time: 27.77932834625244 and batch: 300, loss is 4.700816764831543 and perplexity is 110.03701011903912
At time: 28.697697639465332 and batch: 350, loss is 4.685274114608765 and perplexity is 108.33996580562756
At time: 29.615719079971313 and batch: 400, loss is 4.566505727767944 and perplexity is 96.20734708380762
At time: 30.555315017700195 and batch: 450, loss is 4.580655822753906 and perplexity is 97.57836734083382
At time: 31.46916699409485 and batch: 500, loss is 4.483769493103027 and perplexity is 88.56790032083397
At time: 32.42232632637024 and batch: 550, loss is 4.555055179595947 and perplexity is 95.11200333269636
At time: 33.36843252182007 and batch: 600, loss is 4.521483144760132 and perplexity is 91.97190452027218
At time: 34.27646231651306 and batch: 650, loss is 4.37708888053894 and perplexity is 79.60595332262763
At time: 35.184407472610474 and batch: 700, loss is 4.417133054733276 and perplexity is 82.85839403000539
At time: 36.11811900138855 and batch: 750, loss is 4.480383863449097 and perplexity is 88.2685492430067
At time: 37.030980348587036 and batch: 800, loss is 4.414922313690186 and perplexity is 82.67541790849229
At time: 37.945895195007324 and batch: 850, loss is 4.489635972976685 and perplexity is 89.08900916827966
At time: 38.85856294631958 and batch: 900, loss is 4.419264879226684 and perplexity is 83.035222000013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.565083542915239 and perplexity of 96.0706197009181
finished 2 epochs...
Completing Train Step...
At time: 41.006290674209595 and batch: 50, loss is 4.46877088546753 and perplexity is 87.24941755453474
At time: 41.93580198287964 and batch: 100, loss is 4.330466518402099 and perplexity is 75.97972424192668
At time: 42.845866441726685 and batch: 150, loss is 4.328735003471374 and perplexity is 75.84827804833401
At time: 43.75520658493042 and batch: 200, loss is 4.224663181304932 and perplexity is 68.35147761549699
At time: 44.66646146774292 and batch: 250, loss is 4.369514989852905 and perplexity is 79.00530403153222
At time: 45.57748198509216 and batch: 300, loss is 4.330510659217834 and perplexity is 75.983078122955
At time: 46.49116277694702 and batch: 350, loss is 4.322227144241333 and perplexity is 75.35627082529408
At time: 47.405035972595215 and batch: 400, loss is 4.230196671485901 and perplexity is 68.73074822307227
At time: 48.312889099121094 and batch: 450, loss is 4.257587070465088 and perplexity is 70.63932992410336
At time: 49.22263240814209 and batch: 500, loss is 4.135752940177918 and perplexity is 62.53665971199612
At time: 50.13418960571289 and batch: 550, loss is 4.218158831596375 and perplexity is 67.90833843041085
At time: 51.046032190322876 and batch: 600, loss is 4.218215627670288 and perplexity is 67.9121954669509
At time: 51.960209608078 and batch: 650, loss is 4.063430757522583 and perplexity is 58.173548454952986
At time: 52.87590932846069 and batch: 700, loss is 4.084892773628235 and perplexity is 59.435564349632614
At time: 53.79316473007202 and batch: 750, loss is 4.183630604743957 and perplexity is 65.60360211577375
At time: 54.73395824432373 and batch: 800, loss is 4.123722186088562 and perplexity is 61.78880419066978
At time: 55.64640522003174 and batch: 850, loss is 4.2045904684066775 and perplexity is 66.99315623360735
At time: 56.55546426773071 and batch: 900, loss is 4.149738597869873 and perplexity is 63.41742068239075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430367665748074 and perplexity of 83.96228129099418
finished 3 epochs...
Completing Train Step...
At time: 58.7126624584198 and batch: 50, loss is 4.221027507781982 and perplexity is 68.1034251501983
At time: 59.6320743560791 and batch: 100, loss is 4.080928950309754 and perplexity is 59.20043858007637
At time: 60.552313804626465 and batch: 150, loss is 4.08531418800354 and perplexity is 59.460616629178524
At time: 61.47393751144409 and batch: 200, loss is 3.978706188201904 and perplexity is 53.44783803028927
At time: 62.39632606506348 and batch: 250, loss is 4.1303945875167845 and perplexity is 62.202462408136974
At time: 63.31780815124512 and batch: 300, loss is 4.097981791496277 and perplexity is 60.2186311211772
At time: 64.23865866661072 and batch: 350, loss is 4.088719491958618 and perplexity is 59.6634432493034
At time: 65.15790510177612 and batch: 400, loss is 4.0147731590271 and perplexity is 55.41072455408908
At time: 66.07867169380188 and batch: 450, loss is 4.041002879142761 and perplexity is 56.88336134600159
At time: 66.99761176109314 and batch: 500, loss is 3.912738790512085 and perplexity is 50.03580206596076
At time: 67.91323351860046 and batch: 550, loss is 3.9974439191818236 and perplexity is 54.45877095711461
At time: 68.83069181442261 and batch: 600, loss is 4.011445016860962 and perplexity is 55.22661632436559
At time: 69.74722743034363 and batch: 650, loss is 3.8482917594909667 and perplexity is 46.91285630318985
At time: 70.66469216346741 and batch: 700, loss is 3.8638118743896483 and perplexity is 47.64662860966565
At time: 71.5877902507782 and batch: 750, loss is 3.9782769203186037 and perplexity is 53.42489951372749
At time: 72.50484800338745 and batch: 800, loss is 3.921536755561829 and perplexity is 50.477957485684485
At time: 73.42217755317688 and batch: 850, loss is 4.004002528190613 and perplexity is 54.81711858938965
At time: 74.33918333053589 and batch: 900, loss is 3.9536276960372927 and perplexity is 52.12411471226073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370221281704837 and perplexity of 79.06112454449534
finished 4 epochs...
Completing Train Step...
At time: 76.49876379966736 and batch: 50, loss is 4.034130883216858 and perplexity is 56.49379918643332
At time: 77.41323447227478 and batch: 100, loss is 3.894728908538818 and perplexity is 49.1427293824421
At time: 78.33239483833313 and batch: 150, loss is 3.902561974525452 and perplexity is 49.529179191941644
At time: 79.25173330307007 and batch: 200, loss is 3.796422801017761 and perplexity is 44.54156512695604
At time: 80.17572927474976 and batch: 250, loss is 3.950087833404541 and perplexity is 51.939928695255546
At time: 81.09021067619324 and batch: 300, loss is 3.91932861328125 and perplexity is 50.3666179455361
At time: 82.0072386264801 and batch: 350, loss is 3.912538833618164 and perplexity is 50.025798062612836
At time: 82.92852115631104 and batch: 400, loss is 3.842959723472595 and perplexity is 46.66338096047411
At time: 83.84956884384155 and batch: 450, loss is 3.870068922042847 and perplexity is 47.94569048187862
At time: 84.76920080184937 and batch: 500, loss is 3.7431678199768066 and perplexity is 42.23156047086753
At time: 85.68989181518555 and batch: 550, loss is 3.825732159614563 and perplexity is 45.86636957929544
At time: 86.61310601234436 and batch: 600, loss is 3.844131278991699 and perplexity is 46.71808173822084
At time: 87.53567171096802 and batch: 650, loss is 3.6859164333343504 and perplexity is 39.881654585362305
At time: 88.45566725730896 and batch: 700, loss is 3.6957636737823485 and perplexity is 40.27631881516223
At time: 89.37563514709473 and batch: 750, loss is 3.8136365032196045 and perplexity is 45.31492748304625
At time: 90.2931272983551 and batch: 800, loss is 3.7600050258636473 and perplexity is 42.94864183223828
At time: 91.21239519119263 and batch: 850, loss is 3.8410357427597046 and perplexity is 46.57368782710533
At time: 92.12908267974854 and batch: 900, loss is 3.7966209173202516 and perplexity is 44.550390411333616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356049838131422 and perplexity of 77.94861582511393
finished 5 epochs...
Completing Train Step...
At time: 94.28650569915771 and batch: 50, loss is 3.8768893003463747 and perplexity is 48.27381592685649
At time: 95.20881986618042 and batch: 100, loss is 3.7439328384399415 and perplexity is 42.26388075558572
At time: 96.12656688690186 and batch: 150, loss is 3.7513809108734133 and perplexity is 42.57984038525521
At time: 97.04612040519714 and batch: 200, loss is 3.6443187379837036 and perplexity is 38.25670113437938
At time: 97.96574807167053 and batch: 250, loss is 3.800002865791321 and perplexity is 44.70131259775094
At time: 98.88494658470154 and batch: 300, loss is 3.772563962936401 and perplexity is 43.491432424444156
At time: 99.8056001663208 and batch: 350, loss is 3.7647155046463014 and perplexity is 43.15142773271235
At time: 100.73842167854309 and batch: 400, loss is 3.6985174417495728 and perplexity is 40.38738330448393
At time: 101.65759611129761 and batch: 450, loss is 3.7260434103012083 and perplexity is 41.51452683910462
At time: 102.57596397399902 and batch: 500, loss is 3.605173697471619 and perplexity is 36.78807329763082
At time: 103.49310326576233 and batch: 550, loss is 3.6831374979019165 and perplexity is 39.77097989246734
At time: 104.41047525405884 and batch: 600, loss is 3.7060632514953613 and perplexity is 40.69329152613605
At time: 105.32863783836365 and batch: 650, loss is 3.547483401298523 and perplexity is 34.72581648665191
At time: 106.24608635902405 and batch: 700, loss is 3.5551318454742433 and perplexity is 34.99243325752397
At time: 107.16389226913452 and batch: 750, loss is 3.675346026420593 and perplexity is 39.46230949668079
At time: 108.08276724815369 and batch: 800, loss is 3.6252353239059447 and perplexity is 37.53355466311561
At time: 109.00346803665161 and batch: 850, loss is 3.705436792373657 and perplexity is 40.66780682586227
At time: 109.92301297187805 and batch: 900, loss is 3.658999366760254 and perplexity is 38.82247636871443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364990652424016 and perplexity of 78.64866476397093
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 112.07826924324036 and batch: 50, loss is 3.7712970638275145 and perplexity is 43.43636805532652
At time: 113.01174426078796 and batch: 100, loss is 3.6458036041259767 and perplexity is 38.313549410203834
At time: 113.93168234825134 and batch: 150, loss is 3.651951050758362 and perplexity is 38.54980535186627
At time: 114.84949231147766 and batch: 200, loss is 3.530354313850403 and perplexity is 34.13606035129649
At time: 115.77094197273254 and batch: 250, loss is 3.6773477745056153 and perplexity is 39.54138221457347
At time: 116.69137072563171 and batch: 300, loss is 3.6366154623031615 and perplexity is 37.9631313949424
At time: 117.6122670173645 and batch: 350, loss is 3.6148005390167235 and perplexity is 37.143936420636294
At time: 118.5301263332367 and batch: 400, loss is 3.5444188213348387 and perplexity is 34.61955934513859
At time: 119.45076036453247 and batch: 450, loss is 3.5505330753326416 and perplexity is 34.83188055571935
At time: 120.3679518699646 and batch: 500, loss is 3.4239296913146973 and perplexity is 30.68977972893336
At time: 121.28802180290222 and batch: 550, loss is 3.479134368896484 and perplexity is 32.43163608862929
At time: 122.20915961265564 and batch: 600, loss is 3.493620343208313 and perplexity is 32.90485920968528
At time: 123.14021515846252 and batch: 650, loss is 3.3169639205932615 and perplexity is 27.576498892371767
At time: 124.06578183174133 and batch: 700, loss is 3.314902744293213 and perplexity is 27.51971740484744
At time: 124.98712611198425 and batch: 750, loss is 3.412996001243591 and perplexity is 30.356054935361335
At time: 125.9049699306488 and batch: 800, loss is 3.3386934185028077 and perplexity is 28.182280193315762
At time: 126.82546329498291 and batch: 850, loss is 3.39989363193512 and perplexity is 29.960912993562477
At time: 127.74844002723694 and batch: 900, loss is 3.3429449796676636 and perplexity is 28.302353950916345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344900470890411 and perplexity of 77.0843649592813
finished 7 epochs...
Completing Train Step...
At time: 129.9211618900299 and batch: 50, loss is 3.6566598224639892 and perplexity is 38.73175562953284
At time: 130.84359455108643 and batch: 100, loss is 3.522268795967102 and perplexity is 33.86116545627209
At time: 131.768004655838 and batch: 150, loss is 3.5241663122177123 and perplexity is 33.92547856627868
At time: 132.6889398097992 and batch: 200, loss is 3.4081607389450075 and perplexity is 30.20962973480663
At time: 133.61247420310974 and batch: 250, loss is 3.5561710929870607 and perplexity is 35.02881795983524
At time: 134.5368800163269 and batch: 300, loss is 3.5194472169876097 and perplexity is 33.76575816649845
At time: 135.45697379112244 and batch: 350, loss is 3.5030329275131225 and perplexity is 33.21604118685906
At time: 136.38025331497192 and batch: 400, loss is 3.437459473609924 and perplexity is 31.10782743747261
At time: 137.30348920822144 and batch: 450, loss is 3.447787113189697 and perplexity is 31.43076257586126
At time: 138.22574400901794 and batch: 500, loss is 3.326972260475159 and perplexity is 27.85387961093415
At time: 139.14872026443481 and batch: 550, loss is 3.386417956352234 and perplexity is 29.559877629900498
At time: 140.06849884986877 and batch: 600, loss is 3.406834354400635 and perplexity is 30.16958671092978
At time: 140.9860816001892 and batch: 650, loss is 3.2356290006637574 and perplexity is 25.422357430526617
At time: 141.90438747406006 and batch: 700, loss is 3.2390490627288817 and perplexity is 25.509452320851572
At time: 142.82390117645264 and batch: 750, loss is 3.3452539825439453 and perplexity is 28.36777967261628
At time: 143.7469220161438 and batch: 800, loss is 3.277099914550781 and perplexity is 26.4988123379178
At time: 144.67009711265564 and batch: 850, loss is 3.3465656566619875 and perplexity is 28.405013368898693
At time: 145.59289813041687 and batch: 900, loss is 3.2964379930496217 and perplexity is 27.016235309475817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367182065362799 and perplexity of 78.82120545044994
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 147.74751615524292 and batch: 50, loss is 3.6049129343032837 and perplexity is 36.77848157371929
At time: 148.6737265586853 and batch: 100, loss is 3.4856782960891723 and perplexity is 32.644562280838635
At time: 149.59639024734497 and batch: 150, loss is 3.489250135421753 and perplexity is 32.76137190079729
At time: 150.51686263084412 and batch: 200, loss is 3.3705286598205566 and perplexity is 29.093903770860557
At time: 151.43662071228027 and batch: 250, loss is 3.5153422260284426 and perplexity is 33.62743413817929
At time: 152.35940527915955 and batch: 300, loss is 3.4770935487747194 and perplexity is 32.36551644521595
At time: 153.2810730934143 and batch: 350, loss is 3.4560578203201295 and perplexity is 31.69179518214726
At time: 154.20437240600586 and batch: 400, loss is 3.3915794610977175 and perplexity is 29.712845511182746
At time: 155.12599802017212 and batch: 450, loss is 3.3922946310043334 and perplexity is 29.734102844528785
At time: 156.0473039150238 and batch: 500, loss is 3.263977551460266 and perplexity is 26.153356854264977
At time: 156.9697344303131 and batch: 550, loss is 3.3152754497528076 and perplexity is 27.529976065381376
At time: 157.89055013656616 and batch: 600, loss is 3.3342862129211426 and perplexity is 28.058348387814856
At time: 158.8118224143982 and batch: 650, loss is 3.154889016151428 and perplexity is 23.450434327548958
At time: 159.73390889167786 and batch: 700, loss is 3.155306086540222 and perplexity is 23.460216849169583
At time: 160.65650868415833 and batch: 750, loss is 3.255072555541992 and perplexity is 25.92149521390583
At time: 161.57943558692932 and batch: 800, loss is 3.1791405487060547 and perplexity is 24.02609541721096
At time: 162.5018711090088 and batch: 850, loss is 3.240929503440857 and perplexity is 25.55746646326157
At time: 163.4239046573639 and batch: 900, loss is 3.193220729827881 and perplexity is 24.36678001433458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362922563944777 and perplexity of 78.48618044035591
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 165.60163855552673 and batch: 50, loss is 3.581688413619995 and perplexity is 35.93416138389529
At time: 166.52590107917786 and batch: 100, loss is 3.4593404579162597 and perplexity is 31.795998798282714
At time: 167.44651532173157 and batch: 150, loss is 3.46555947303772 and perplexity is 31.994354745625873
At time: 168.37032985687256 and batch: 200, loss is 3.34702597618103 and perplexity is 28.418091760879683
At time: 169.29131746292114 and batch: 250, loss is 3.490160136222839 and perplexity is 32.79119834445723
At time: 170.21636700630188 and batch: 300, loss is 3.453675923347473 and perplexity is 31.61639842045061
At time: 171.13702607154846 and batch: 350, loss is 3.434005813598633 and perplexity is 31.000576887885757
At time: 172.05703687667847 and batch: 400, loss is 3.368219895362854 and perplexity is 29.02681028123356
At time: 172.9774923324585 and batch: 450, loss is 3.3690947532653808 and perplexity is 29.05221572705037
At time: 173.89640998840332 and batch: 500, loss is 3.2387650537490846 and perplexity is 25.502208436035605
At time: 174.8140149116516 and batch: 550, loss is 3.2886351919174195 and perplexity is 26.806253287367674
At time: 175.73146963119507 and batch: 600, loss is 3.3101522541046142 and perplexity is 27.389295287089375
At time: 176.65099477767944 and batch: 650, loss is 3.1286884450912478 and perplexity is 22.843998727329527
At time: 177.56806683540344 and batch: 700, loss is 3.1265373754501344 and perplexity is 22.79491250805082
At time: 178.48645901679993 and batch: 750, loss is 3.225199217796326 and perplexity is 25.158585694608682
At time: 179.40287351608276 and batch: 800, loss is 3.147389965057373 and perplexity is 23.275236053785505
At time: 180.31743383407593 and batch: 850, loss is 3.207723922729492 and perplexity is 24.72275124535125
At time: 181.2326431274414 and batch: 900, loss is 3.160660119056702 and perplexity is 23.586160465277004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358599101027397 and perplexity of 78.14758083854086
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 183.3845043182373 and batch: 50, loss is 3.570278830528259 and perplexity is 35.52649764283199
At time: 184.29980182647705 and batch: 100, loss is 3.4463900470733644 and perplexity is 31.386882381359573
At time: 185.21540522575378 and batch: 150, loss is 3.4540834999084473 and perplexity is 31.629287149782776
At time: 186.13012194633484 and batch: 200, loss is 3.337117910385132 and perplexity is 28.13791374112666
At time: 187.04881501197815 and batch: 250, loss is 3.4810440158843994 and perplexity is 32.49362823752014
At time: 187.96849751472473 and batch: 300, loss is 3.44420485496521 and perplexity is 31.31837089632069
At time: 188.893488407135 and batch: 350, loss is 3.424513192176819 and perplexity is 30.707692467404456
At time: 189.81275057792664 and batch: 400, loss is 3.359022707939148 and perplexity is 28.76106917535527
At time: 190.73182797431946 and batch: 450, loss is 3.3599925374984743 and perplexity is 28.788976040663435
At time: 191.6565010547638 and batch: 500, loss is 3.2293430852890013 and perplexity is 25.263055845768346
At time: 192.58869910240173 and batch: 550, loss is 3.2789543533325194 and perplexity is 26.54799835532068
At time: 193.510596036911 and batch: 600, loss is 3.3015571546554567 and perplexity is 27.1548903800659
At time: 194.43157148361206 and batch: 650, loss is 3.120626392364502 and perplexity is 22.66056960624578
At time: 195.35317397117615 and batch: 700, loss is 3.117477684020996 and perplexity is 22.589330296420307
At time: 196.27475929260254 and batch: 750, loss is 3.2155648851394654 and perplexity is 24.917363384698692
At time: 197.19332599639893 and batch: 800, loss is 3.137574305534363 and perplexity is 23.04789185332495
At time: 198.11279010772705 and batch: 850, loss is 3.1975640153884886 and perplexity is 24.472842060275045
At time: 199.03224778175354 and batch: 900, loss is 3.150582790374756 and perplexity is 23.349668578519548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357346626177226 and perplexity of 78.04976422814813
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 201.17917132377625 and batch: 50, loss is 3.5665687561035155 and perplexity is 35.39493589537231
At time: 202.10418057441711 and batch: 100, loss is 3.4425114488601682 and perplexity is 31.265381055168074
At time: 203.02637362480164 and batch: 150, loss is 3.4506365633010865 and perplexity is 31.5204506859976
At time: 203.94849944114685 and batch: 200, loss is 3.333993425369263 and perplexity is 28.05013445520789
At time: 204.87034964561462 and batch: 250, loss is 3.4783070278167725 and perplexity is 32.40481516036867
At time: 205.790287733078 and batch: 300, loss is 3.4411423063278197 and perplexity is 31.222603583085967
At time: 206.7125759124756 and batch: 350, loss is 3.421006302833557 and perplexity is 30.600192593187874
At time: 207.63302063941956 and batch: 400, loss is 3.356006507873535 and perplexity is 28.67445073156671
At time: 208.55573558807373 and batch: 450, loss is 3.357326707839966 and perplexity is 28.712331740211127
At time: 209.4781277179718 and batch: 500, loss is 3.226887249946594 and perplexity is 25.2010900603916
At time: 210.3994836807251 and batch: 550, loss is 3.276142091751099 and perplexity is 26.473443322746125
At time: 211.31975674629211 and batch: 600, loss is 3.2991371726989747 and perplexity is 27.08925548497834
At time: 212.24214053153992 and batch: 650, loss is 3.1181678915023805 and perplexity is 22.604927003052993
At time: 213.16252207756042 and batch: 700, loss is 3.114861750602722 and perplexity is 22.5303153356593
At time: 214.08302235603333 and batch: 750, loss is 3.2130979251861573 and perplexity is 24.85596900695886
At time: 215.00658631324768 and batch: 800, loss is 3.1349701499938964 and perplexity is 22.98794964135621
At time: 215.94278120994568 and batch: 850, loss is 3.194752221107483 and perplexity is 24.404126115753336
At time: 216.86408162117004 and batch: 900, loss is 3.1474836587905886 and perplexity is 23.277416899707056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356736692663741 and perplexity of 78.00217357626792
Annealing...
Model not improving. Stopping early with 77.0843649592813 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -77.0843649592813
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f825458fb70>
ELAPSED
228.98018074035645


RESULTS SO FAR:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3629419803619385 and batch: 50, loss is 7.480670433044434 and perplexity is 1773.4293430938847
At time: 2.5003750324249268 and batch: 100, loss is 6.688967370986939 and perplexity is 803.4921143365258
At time: 3.640054941177368 and batch: 150, loss is 6.601709680557251 and perplexity is 736.3530421516876
At time: 4.780987024307251 and batch: 200, loss is 6.500159950256347 and perplexity is 665.2480311280482
At time: 5.920804977416992 and batch: 250, loss is 6.560179147720337 and perplexity is 706.3982328935767
At time: 7.061058521270752 and batch: 300, loss is 6.486712951660156 and perplexity is 656.3623187619902
At time: 8.200490236282349 and batch: 350, loss is 6.496395931243897 and perplexity is 662.7487315472623
At time: 9.345854997634888 and batch: 400, loss is 6.4243738555908205 and perplexity is 616.6945567790555
At time: 10.485617399215698 and batch: 450, loss is 6.414951362609863 and perplexity is 610.911046976566
At time: 11.625788688659668 and batch: 500, loss is 6.39772250175476 and perplexity is 600.4758965547072
At time: 12.76633620262146 and batch: 550, loss is 6.414833717346191 and perplexity is 610.8391804128275
At time: 13.908527612686157 and batch: 600, loss is 6.370543766021728 and perplexity is 584.3755060565669
At time: 15.04967999458313 and batch: 650, loss is 6.289896650314331 and perplexity is 539.0976106368463
At time: 16.192049980163574 and batch: 700, loss is 6.366772108078003 and perplexity is 582.1755928050624
At time: 17.330673933029175 and batch: 750, loss is 6.319762516021728 and perplexity is 555.4410684324756
At time: 18.46940279006958 and batch: 800, loss is 6.32358136177063 and perplexity is 557.5662675164174
At time: 19.60732913017273 and batch: 850, loss is 6.349759283065796 and perplexity is 572.3549169090152
At time: 20.743828296661377 and batch: 900, loss is 6.237913541793823 and perplexity is 511.7895686335773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 7.500717372110445 and perplexity of 1809.339918999839
finished 1 epochs...
Completing Train Step...
At time: 23.02356743812561 and batch: 50, loss is 6.032000093460083 and perplexity is 416.54733035591903
At time: 23.949180126190186 and batch: 100, loss is 5.674196662902832 and perplexity is 291.25426918046486
At time: 24.871084451675415 and batch: 150, loss is 5.4960862827301025 and perplexity is 243.73614877627708
At time: 25.791158199310303 and batch: 200, loss is 5.277843408584594 and perplexity is 195.94684208328468
At time: 26.71412420272827 and batch: 250, loss is 5.284447383880615 and perplexity is 197.2451524740674
At time: 27.633469104766846 and batch: 300, loss is 5.178317317962646 and perplexity is 177.38407872515657
At time: 28.55509114265442 and batch: 350, loss is 5.118407545089721 and perplexity is 167.06910765144625
At time: 29.4745991230011 and batch: 400, loss is 4.947571353912354 and perplexity is 140.83251590851114
At time: 30.39889645576477 and batch: 450, loss is 4.940791425704956 and perplexity is 139.88091111271538
At time: 31.321399450302124 and batch: 500, loss is 4.848412303924561 and perplexity is 127.5377378478564
At time: 32.24468779563904 and batch: 550, loss is 4.8981485843658445 and perplexity is 134.04138350056223
At time: 33.16408705711365 and batch: 600, loss is 4.819704561233521 and perplexity is 123.92847209587319
At time: 34.08338260650635 and batch: 650, loss is 4.685401458740234 and perplexity is 108.35376314296279
At time: 35.0015344619751 and batch: 700, loss is 4.741585054397583 and perplexity is 114.61572994284532
At time: 35.91803860664368 and batch: 750, loss is 4.7518283653259275 and perplexity is 115.79580813753115
At time: 36.835354804992676 and batch: 800, loss is 4.6848270702362065 and perplexity is 108.29154385777706
At time: 37.75401496887207 and batch: 850, loss is 4.733695106506348 and perplexity is 113.71497592994518
At time: 38.672064542770386 and batch: 900, loss is 4.656778812408447 and perplexity is 105.29635597422916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.750040550754495 and perplexity of 115.58897165216555
finished 2 epochs...
Completing Train Step...
At time: 40.81773781776428 and batch: 50, loss is 4.718106346130371 and perplexity is 111.95604580867844
At time: 41.74264192581177 and batch: 100, loss is 4.578809289932251 and perplexity is 97.39835193618165
At time: 42.660982608795166 and batch: 150, loss is 4.577699117660522 and perplexity is 97.29028298522582
At time: 43.57820510864258 and batch: 200, loss is 4.460650882720947 and perplexity is 86.54382064569717
At time: 44.495572566986084 and batch: 250, loss is 4.587041578292847 and perplexity is 98.20347270136637
At time: 45.41474962234497 and batch: 300, loss is 4.545656604766846 and perplexity is 94.22227369688798
At time: 46.33464217185974 and batch: 350, loss is 4.534028263092041 and perplexity is 93.132970573866
At time: 47.25397801399231 and batch: 400, loss is 4.424886770248413 and perplexity is 83.50335162296282
At time: 48.17006826400757 and batch: 450, loss is 4.446348094940186 and perplexity is 85.31481281576445
At time: 49.08584403991699 and batch: 500, loss is 4.336988573074341 and perplexity is 76.47688765857963
At time: 50.010884046554565 and batch: 550, loss is 4.417547960281372 and perplexity is 82.89277957027714
At time: 50.93058896064758 and batch: 600, loss is 4.392086668014526 and perplexity is 80.80886444720517
At time: 51.847262382507324 and batch: 650, loss is 4.248611559867859 and perplexity is 70.00814272085759
At time: 52.76512908935547 and batch: 700, loss is 4.2805001211166385 and perplexity is 72.27657801278487
At time: 53.686609983444214 and batch: 750, loss is 4.3525374364852905 and perplexity is 77.67530924080556
At time: 54.6087601184845 and batch: 800, loss is 4.292304992675781 and perplexity is 73.13484965922125
At time: 55.53059458732605 and batch: 850, loss is 4.364032173156739 and perplexity is 78.5733177644324
At time: 56.451929807662964 and batch: 900, loss is 4.299100437164307 and perplexity is 73.63352591467236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.500766283845248 and perplexity of 90.08613640940648
finished 3 epochs...
Completing Train Step...
At time: 58.607483863830566 and batch: 50, loss is 4.4016194152832036 and perplexity is 81.58287830688052
At time: 59.53055787086487 and batch: 100, loss is 4.253311142921448 and perplexity is 70.33792611726399
At time: 60.45319843292236 and batch: 150, loss is 4.260493769645691 and perplexity is 70.84495590817568
At time: 61.37373924255371 and batch: 200, loss is 4.145269541740418 and perplexity is 63.134637028443876
At time: 62.2975492477417 and batch: 250, loss is 4.290723390579224 and perplexity is 73.01927085160406
At time: 63.22050976753235 and batch: 300, loss is 4.261134753227234 and perplexity is 70.8903809185356
At time: 64.14314389228821 and batch: 350, loss is 4.258205471038818 and perplexity is 70.68302683596147
At time: 65.06615400314331 and batch: 400, loss is 4.163735971450806 and perplexity is 64.31133967511597
At time: 65.98618006706238 and batch: 450, loss is 4.196277976036072 and perplexity is 66.43858426445635
At time: 66.90844774246216 and batch: 500, loss is 4.067926177978515 and perplexity is 58.43565170542914
At time: 67.8301899433136 and batch: 550, loss is 4.151907682418823 and perplexity is 63.55512772484444
At time: 68.75339341163635 and batch: 600, loss is 4.154583134651184 and perplexity is 63.72539410140798
At time: 69.67659664154053 and batch: 650, loss is 3.9991474103927613 and perplexity is 54.55162005615751
At time: 70.6000075340271 and batch: 700, loss is 4.019863390922547 and perplexity is 55.69349706983996
At time: 71.52196383476257 and batch: 750, loss is 4.115277357101441 and perplexity is 61.26920535554791
At time: 72.45074653625488 and batch: 800, loss is 4.060351643562317 and perplexity is 57.99470095700454
At time: 73.37501001358032 and batch: 850, loss is 4.140919270515442 and perplexity is 62.86058077540179
At time: 74.30049872398376 and batch: 900, loss is 4.084571762084961 and perplexity is 59.416487909437926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.412807516855736 and perplexity of 82.50076094350509
finished 4 epochs...
Completing Train Step...
At time: 76.46668982505798 and batch: 50, loss is 4.191699795722961 and perplexity is 66.13511165217405
At time: 77.3926055431366 and batch: 100, loss is 4.046170201301575 and perplexity is 57.1780567368373
At time: 78.31875777244568 and batch: 150, loss is 4.051902165412903 and perplexity is 57.506740407591415
At time: 79.24446773529053 and batch: 200, loss is 3.937671785354614 and perplexity is 51.29902701125305
At time: 80.17036294937134 and batch: 250, loss is 4.0887686634063725 and perplexity is 59.66637705931523
At time: 81.09524393081665 and batch: 300, loss is 4.068099880218506 and perplexity is 58.44580299065066
At time: 82.02038288116455 and batch: 350, loss is 4.061951427459717 and perplexity is 58.08755419848702
At time: 82.94486451148987 and batch: 400, loss is 3.978464422225952 and perplexity is 53.43491772347358
At time: 83.86946558952332 and batch: 450, loss is 4.012500948905945 and perplexity is 55.2849626777493
At time: 84.79153871536255 and batch: 500, loss is 3.885167064666748 and perplexity is 48.67507366521373
At time: 85.717369556427 and batch: 550, loss is 3.96564257144928 and perplexity is 52.754156815935744
At time: 86.64236950874329 and batch: 600, loss is 3.9778207635879514 and perplexity is 53.40053494368367
At time: 87.56726312637329 and batch: 650, loss is 3.8211003875732423 and perplexity is 45.6544182450945
At time: 88.49204015731812 and batch: 700, loss is 3.832493257522583 and perplexity is 46.177527293276945
At time: 89.42644953727722 and batch: 750, loss is 3.93980263710022 and perplexity is 51.408454177637346
At time: 90.35220837593079 and batch: 800, loss is 3.889546265602112 and perplexity is 48.88869900608551
At time: 91.27714419364929 and batch: 850, loss is 3.9674930715560914 and perplexity is 52.851868768864726
At time: 92.20178723335266 and batch: 900, loss is 3.9169162368774413 and perplexity is 50.245261142861835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373512581603168 and perplexity of 79.32176710680245
finished 5 epochs...
Completing Train Step...
At time: 94.35979413986206 and batch: 50, loss is 4.025362648963928 and perplexity is 56.000613664173954
At time: 95.2902319431305 and batch: 100, loss is 3.884871997833252 and perplexity is 48.66071338408268
At time: 96.212571144104 and batch: 150, loss is 3.8876162433624266 and perplexity is 48.79443372603934
At time: 97.1335015296936 and batch: 200, loss is 3.7721663427352907 and perplexity is 43.47414278991874
At time: 98.05743765830994 and batch: 250, loss is 3.9303304100036622 and perplexity is 50.9238006229626
At time: 98.98204636573792 and batch: 300, loss is 3.9116271495819093 and perplexity is 49.980211124717556
At time: 99.90712213516235 and batch: 350, loss is 3.9048058128356935 and perplexity is 49.64043944000968
At time: 100.83182668685913 and batch: 400, loss is 3.827920060157776 and perplexity is 45.96683049338267
At time: 101.75609421730042 and batch: 450, loss is 3.8605773353576662 and perplexity is 47.49276270646863
At time: 102.67942237854004 and batch: 500, loss is 3.732906908988953 and perplexity is 41.800441805689346
At time: 103.60218286514282 and batch: 550, loss is 3.812490448951721 and perplexity is 45.26302386486807
At time: 104.52638363838196 and batch: 600, loss is 3.8346106481552122 and perplexity is 46.275406744953834
At time: 105.458416223526 and batch: 650, loss is 3.6714455318450927 and perplexity is 39.30868676962519
At time: 106.38370394706726 and batch: 700, loss is 3.6849763584136963 and perplexity is 39.84418045909534
At time: 107.30469417572021 and batch: 750, loss is 3.7951000452041628 and perplexity is 44.48268646237044
At time: 108.22630858421326 and batch: 800, loss is 3.748076906204224 and perplexity is 42.43938854834102
At time: 109.1466805934906 and batch: 850, loss is 3.82245539188385 and perplexity is 45.71632210914472
At time: 110.06572127342224 and batch: 900, loss is 3.779635272026062 and perplexity is 43.80006371343872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368620990073844 and perplexity of 78.93470486970304
finished 6 epochs...
Completing Train Step...
At time: 112.2306957244873 and batch: 50, loss is 3.883758931159973 and perplexity is 48.606580897838874
At time: 113.15718173980713 and batch: 100, loss is 3.748090777397156 and perplexity is 42.4399772373704
At time: 114.07894158363342 and batch: 150, loss is 3.7490904474258424 and perplexity is 42.48242442368778
At time: 115.00035643577576 and batch: 200, loss is 3.6376031970977785 and perplexity is 38.000647425628124
At time: 115.9217643737793 and batch: 250, loss is 3.794722466468811 and perplexity is 44.465893916324575
At time: 116.84371709823608 and batch: 300, loss is 3.7767194080352784 and perplexity is 43.67253470384318
At time: 117.76811456680298 and batch: 350, loss is 3.769637117385864 and perplexity is 43.36432582040285
At time: 118.69703793525696 and batch: 400, loss is 3.698521099090576 and perplexity is 40.38753101518702
At time: 119.62204551696777 and batch: 450, loss is 3.7301613187789915 and perplexity is 41.68583232922025
At time: 120.54653549194336 and batch: 500, loss is 3.603978796005249 and perplexity is 36.744141427257844
At time: 121.47011017799377 and batch: 550, loss is 3.683377547264099 and perplexity is 39.780528036791026
At time: 122.39433789253235 and batch: 600, loss is 3.709594278335571 and perplexity is 40.837234614627135
At time: 123.31799459457397 and batch: 650, loss is 3.5463501977920533 and perplexity is 34.68648735780628
At time: 124.24303531646729 and batch: 700, loss is 3.5608926248550414 and perplexity is 35.19459870168099
At time: 125.16736221313477 and batch: 750, loss is 3.673324818611145 and perplexity is 39.38262852157841
At time: 126.0913348197937 and batch: 800, loss is 3.6242829608917235 and perplexity is 37.49782610983902
At time: 127.0127432346344 and batch: 850, loss is 3.7006777858734132 and perplexity is 40.474728264297866
At time: 127.93472194671631 and batch: 900, loss is 3.657576084136963 and perplexity is 38.767260316050944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385446208797089 and perplexity of 80.27403420295667
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 130.10701298713684 and batch: 50, loss is 3.7861228466033934 and perplexity is 44.08514363200319
At time: 131.02893686294556 and batch: 100, loss is 3.6582660150527953 and perplexity is 38.79401627628585
At time: 131.95185661315918 and batch: 150, loss is 3.6609787368774414 and perplexity is 38.899396519924416
At time: 132.87736916542053 and batch: 200, loss is 3.53294180393219 and perplexity is 34.22450143976283
At time: 133.81191110610962 and batch: 250, loss is 3.6752161407470703 and perplexity is 39.4571842408889
At time: 134.7367742061615 and batch: 300, loss is 3.653197555541992 and perplexity is 38.59788782993906
At time: 135.66023755073547 and batch: 350, loss is 3.6271793746948244 and perplexity is 37.60659277156112
At time: 136.5865159034729 and batch: 400, loss is 3.550378079414368 and perplexity is 34.826482174781724
At time: 137.5114688873291 and batch: 450, loss is 3.5638848829269407 and perplexity is 35.30006774033044
At time: 138.4359838962555 and batch: 500, loss is 3.4249213981628417 and perplexity is 30.720230090078807
At time: 139.3593671321869 and batch: 550, loss is 3.481642680168152 and perplexity is 32.51308683619833
At time: 140.28199100494385 and batch: 600, loss is 3.5025714206695557 and perplexity is 33.20071529330953
At time: 141.21412086486816 and batch: 650, loss is 3.3199279594421385 and perplexity is 27.65835796320657
At time: 142.13982009887695 and batch: 700, loss is 3.3147257566452026 and perplexity is 27.514847185787072
At time: 143.0671591758728 and batch: 750, loss is 3.410977168083191 and perplexity is 30.294832944325194
At time: 144.00275564193726 and batch: 800, loss is 3.3382163524627684 and perplexity is 28.16883859102558
At time: 144.93160319328308 and batch: 850, loss is 3.394811444282532 and perplexity is 29.809032281531774
At time: 145.85949325561523 and batch: 900, loss is 3.337484178543091 and perplexity is 28.148221650575703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373194028253424 and perplexity of 79.29650291639287
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 148.04050064086914 and batch: 50, loss is 3.688823971748352 and perplexity is 39.997780766941354
At time: 148.9658362865448 and batch: 100, loss is 3.5707589960098267 and perplexity is 35.54356033681158
At time: 149.88541293144226 and batch: 150, loss is 3.5807063388824463 and perplexity is 35.89888867483837
At time: 150.80488777160645 and batch: 200, loss is 3.4481914186477662 and perplexity is 31.443472773950045
At time: 151.72378635406494 and batch: 250, loss is 3.591892318725586 and perplexity is 36.30270726255767
At time: 152.64260530471802 and batch: 300, loss is 3.5709992122650145 and perplexity is 35.55209950335397
At time: 153.56569242477417 and batch: 350, loss is 3.5433266496658327 and perplexity is 34.58176948349293
At time: 154.49033617973328 and batch: 400, loss is 3.465755624771118 and perplexity is 32.00063110930787
At time: 155.41534566879272 and batch: 450, loss is 3.476311273574829 and perplexity is 32.34020760489893
At time: 156.3428463935852 and batch: 500, loss is 3.3331335306167604 and perplexity is 28.026024659212002
At time: 157.26733708381653 and batch: 550, loss is 3.3845911121368406 and perplexity is 29.505925634394792
At time: 158.1910960674286 and batch: 600, loss is 3.4130748224258425 and perplexity is 30.358447729800034
At time: 159.11807322502136 and batch: 650, loss is 3.226444330215454 and perplexity is 25.18993047194101
At time: 160.0455939769745 and batch: 700, loss is 3.2149473333358767 and perplexity is 24.901980372392178
At time: 160.97154641151428 and batch: 750, loss is 3.3083383798599244 and perplexity is 27.33965957986243
At time: 161.896582365036 and batch: 800, loss is 3.2307712697982787 and perplexity is 25.299161927720586
At time: 162.82178378105164 and batch: 850, loss is 3.2849216890335082 and perplexity is 26.706892790406787
At time: 163.74506759643555 and batch: 900, loss is 3.2330265998840333 and perplexity is 25.356284279426937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368393153360445 and perplexity of 78.91672269455006
finished 9 epochs...
Completing Train Step...
At time: 165.91696977615356 and batch: 50, loss is 3.6509978008270263 and perplexity is 38.513075261820845
At time: 166.84710240364075 and batch: 100, loss is 3.523644137382507 and perplexity is 33.90776815946588
At time: 167.77190852165222 and batch: 150, loss is 3.5284223985671996 and perplexity is 34.07017603654442
At time: 168.6970179080963 and batch: 200, loss is 3.399087805747986 and perplexity is 29.936779430316385
At time: 169.62093687057495 and batch: 250, loss is 3.542494077682495 and perplexity is 34.552989653389304
At time: 170.54614543914795 and batch: 300, loss is 3.522472562789917 and perplexity is 33.86806594139502
At time: 171.47009229660034 and batch: 350, loss is 3.4969713497161865 and perplexity is 33.01530856193148
At time: 172.39450979232788 and batch: 400, loss is 3.421522002220154 and perplexity is 30.615977163434696
At time: 173.3166995048523 and batch: 450, loss is 3.4345993423461914 and perplexity is 31.018982082925483
At time: 174.24163627624512 and batch: 500, loss is 3.295448899269104 and perplexity is 26.9895269298817
At time: 175.16585683822632 and batch: 550, loss is 3.348851842880249 and perplexity is 28.47002680708854
At time: 176.08726048469543 and batch: 600, loss is 3.3810626602172853 and perplexity is 29.40199885260199
At time: 177.0104968547821 and batch: 650, loss is 3.1987704038619995 and perplexity is 24.50238363057273
At time: 177.93272519111633 and batch: 700, loss is 3.191208553314209 and perplexity is 24.31779904757779
At time: 178.856107711792 and batch: 750, loss is 3.2890842151641846 and perplexity is 26.818292621020436
At time: 179.78213906288147 and batch: 800, loss is 3.2160073423385622 and perplexity is 24.928390690886367
At time: 180.70748615264893 and batch: 850, loss is 3.275644636154175 and perplexity is 26.460277235238845
At time: 181.63239431381226 and batch: 900, loss is 3.2283372020721437 and perplexity is 25.23765693819553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374395501123716 and perplexity of 79.39183277000463
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 183.80852961540222 and batch: 50, loss is 3.631059741973877 and perplexity is 37.7528036561336
At time: 184.73291039466858 and batch: 100, loss is 3.511020607948303 and perplexity is 33.4824227783668
At time: 185.65752458572388 and batch: 150, loss is 3.516975831985474 and perplexity is 33.68241300958632
At time: 186.5836534500122 and batch: 200, loss is 3.3881840467453004 and perplexity is 29.612129172695383
At time: 187.51392889022827 and batch: 250, loss is 3.531105179786682 and perplexity is 34.16170158157153
At time: 188.43872904777527 and batch: 300, loss is 3.5099106788635255 and perplexity is 33.445280280078954
At time: 189.364360332489 and batch: 350, loss is 3.4845049953460694 and perplexity is 32.6062828526884
At time: 190.28815245628357 and batch: 400, loss is 3.4087684535980225 and perplexity is 30.227994149055256
At time: 191.20891904830933 and batch: 450, loss is 3.416459436416626 and perplexity is 30.46137344036839
At time: 192.13374614715576 and batch: 500, loss is 3.275656614303589 and perplexity is 26.460594182291324
At time: 193.05708527565002 and batch: 550, loss is 3.32582359790802 and perplexity is 27.821903270603936
At time: 193.98346781730652 and batch: 600, loss is 3.3574776029586793 and perplexity is 28.716664617814423
At time: 194.90867137908936 and batch: 650, loss is 3.174094305038452 and perplexity is 23.905159278598646
At time: 195.83327651023865 and batch: 700, loss is 3.164481978416443 and perplexity is 23.6764759299801
At time: 196.75781559944153 and batch: 750, loss is 3.2576806354522705 and perplexity is 25.9891887815631
At time: 197.68368101119995 and batch: 800, loss is 3.181676745414734 and perplexity is 24.08710765814878
At time: 198.6079306602478 and batch: 850, loss is 3.2396986436843873 and perplexity is 25.526028158355903
At time: 199.5318944454193 and batch: 900, loss is 3.1959046936035156 and perplexity is 24.432267412812134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368746822827483 and perplexity of 78.94463806592209
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 201.71458292007446 and batch: 50, loss is 3.6220094394683837 and perplexity is 37.412670836698446
At time: 202.64619207382202 and batch: 100, loss is 3.5012714862823486 and perplexity is 33.15758458144126
At time: 203.57259464263916 and batch: 150, loss is 3.505988483428955 and perplexity is 33.314358273143235
At time: 204.4994761943817 and batch: 200, loss is 3.3789181470870973 and perplexity is 29.33901344066909
At time: 205.4268445968628 and batch: 250, loss is 3.5221536922454835 and perplexity is 33.857268134412166
At time: 206.3537700176239 and batch: 300, loss is 3.4995160913467407 and perplexity is 33.09943098159027
At time: 207.27974820137024 and batch: 350, loss is 3.4759169006347657 and perplexity is 32.32745601674945
At time: 208.21172952651978 and batch: 400, loss is 3.40060275554657 and perplexity is 29.982166519184613
At time: 209.13581371307373 and batch: 450, loss is 3.408971838951111 and perplexity is 30.23414270556039
At time: 210.062082529068 and batch: 500, loss is 3.2682855701446534 and perplexity is 26.266269043534844
At time: 210.99392080307007 and batch: 550, loss is 3.3176545524597167 and perplexity is 27.59555067940046
At time: 211.91932845115662 and batch: 600, loss is 3.347687225341797 and perplexity is 28.43688941448855
At time: 212.84560012817383 and batch: 650, loss is 3.1658753299713136 and perplexity is 23.70948857830246
At time: 213.77250814437866 and batch: 700, loss is 3.1579365587234496 and perplexity is 23.522009533341624
At time: 214.70817279815674 and batch: 750, loss is 3.2494320106506347 and perplexity is 25.775695438151352
At time: 215.63402843475342 and batch: 800, loss is 3.1715641021728516 and perplexity is 23.844750831130277
At time: 216.56014394760132 and batch: 850, loss is 3.228892111778259 and perplexity is 25.25166544535898
At time: 217.48648357391357 and batch: 900, loss is 3.1861666536331175 and perplexity is 24.195499713879055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367432476723031 and perplexity of 78.84094564720348
finished 12 epochs...
Completing Train Step...
At time: 219.65589809417725 and batch: 50, loss is 3.618245897293091 and perplexity is 37.27213130123507
At time: 220.58609080314636 and batch: 100, loss is 3.496768617630005 and perplexity is 33.00861597797446
At time: 221.5118007659912 and batch: 150, loss is 3.5007687997817993 and perplexity is 33.14092089993612
At time: 222.4350414276123 and batch: 200, loss is 3.3740349292755125 and perplexity is 29.196093885323105
At time: 223.3589379787445 and batch: 250, loss is 3.5171561241149902 and perplexity is 33.68848623101563
At time: 224.28454685211182 and batch: 300, loss is 3.495149211883545 and perplexity is 32.955204894357344
At time: 225.2104456424713 and batch: 350, loss is 3.470888028144836 and perplexity is 32.16529345170005
At time: 226.13617157936096 and batch: 400, loss is 3.3958969163894652 and perplexity is 29.841406722205452
At time: 227.0633327960968 and batch: 450, loss is 3.404755907058716 and perplexity is 30.10694593395224
At time: 227.98589658737183 and batch: 500, loss is 3.2643468236923217 and perplexity is 26.16301634610759
At time: 228.91014003753662 and batch: 550, loss is 3.314217128753662 and perplexity is 27.500855925551132
At time: 229.8360345363617 and batch: 600, loss is 3.3451345586776733 and perplexity is 28.364392084973655
At time: 230.761732339859 and batch: 650, loss is 3.1635688877105714 and perplexity is 23.654867026806315
At time: 231.68709135055542 and batch: 700, loss is 3.1561312866210938 and perplexity is 23.4795842118864
At time: 232.61301636695862 and batch: 750, loss is 3.248692331314087 and perplexity is 25.756636738382763
At time: 233.5444061756134 and batch: 800, loss is 3.1717477369308473 and perplexity is 23.84912995824629
At time: 234.47004747390747 and batch: 850, loss is 3.2302484798431395 and perplexity is 25.285939236637528
At time: 235.3948495388031 and batch: 900, loss is 3.1887272357940675 and perplexity is 24.257533666443624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36687772567958 and perplexity of 78.79722067969608
finished 13 epochs...
Completing Train Step...
At time: 237.5558955669403 and batch: 50, loss is 3.615511751174927 and perplexity is 37.170363036173384
At time: 238.4808738231659 and batch: 100, loss is 3.493502626419067 and perplexity is 32.9009859832849
At time: 239.40585470199585 and batch: 150, loss is 3.4971339511871338 and perplexity is 33.020677336140885
At time: 240.3295841217041 and batch: 200, loss is 3.3705124473571777 and perplexity is 29.093432090834682
At time: 241.25358057022095 and batch: 250, loss is 3.5134117794036865 and perplexity is 33.56258078955235
At time: 242.17782855033875 and batch: 300, loss is 3.491534333229065 and perplexity is 32.83629088697508
At time: 243.10304975509644 and batch: 350, loss is 3.4672906255722045 and perplexity is 32.04978982337624
At time: 244.0280466079712 and batch: 400, loss is 3.3923827266693114 and perplexity is 29.736722405475685
At time: 244.9526846408844 and batch: 450, loss is 3.4015518617630005 and perplexity is 30.010636288090215
At time: 245.8733730316162 and batch: 500, loss is 3.261403636932373 and perplexity is 26.086126908288676
At time: 246.7979338169098 and batch: 550, loss is 3.311587920188904 and perplexity is 27.428645409464867
At time: 247.72281765937805 and batch: 600, loss is 3.343026866912842 and perplexity is 28.304671647607055
At time: 248.64763355255127 and batch: 650, loss is 3.161822032928467 and perplexity is 23.61358147964681
At time: 249.57327580451965 and batch: 700, loss is 3.1548317766189573 and perplexity is 23.449092074067163
At time: 250.49667191505432 and batch: 750, loss is 3.2480787086486815 and perplexity is 25.74083673041275
At time: 251.41958284378052 and batch: 800, loss is 3.1717199659347535 and perplexity is 23.848467653347853
At time: 252.34153580665588 and batch: 850, loss is 3.2310141134262085 and perplexity is 25.305306414031172
At time: 253.26641845703125 and batch: 900, loss is 3.190118818283081 and perplexity is 24.29131352379826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366833412483947 and perplexity of 78.79372900040508
finished 14 epochs...
Completing Train Step...
At time: 255.43155646324158 and batch: 50, loss is 3.6130644369125364 and perplexity is 37.079506698929215
At time: 256.3617708683014 and batch: 100, loss is 3.4906869173049926 and perplexity is 32.808476677956286
At time: 257.2860598564148 and batch: 150, loss is 3.494049115180969 and perplexity is 32.91897091621452
At time: 258.21131682395935 and batch: 200, loss is 3.3675534534454346 and perplexity is 29.00747204275528
At time: 259.13612604141235 and batch: 250, loss is 3.5102458000183105 and perplexity is 33.45649037929431
At time: 260.0608334541321 and batch: 300, loss is 3.4884106063842775 and perplexity is 32.73387931980382
At time: 260.98697805404663 and batch: 350, loss is 3.4642387866973876 and perplexity is 31.952128128541453
At time: 261.91239762306213 and batch: 400, loss is 3.3893972635269165 and perplexity is 29.648076906534786
At time: 262.8372383117676 and batch: 450, loss is 3.39880482673645 and perplexity is 29.92830914857713
At time: 263.76275634765625 and batch: 500, loss is 3.2589075851440428 and perplexity is 26.021095778865558
At time: 264.6854820251465 and batch: 550, loss is 3.3093111371994017 and perplexity is 27.366267373742794
At time: 265.61253356933594 and batch: 600, loss is 3.341130599975586 and perplexity is 28.251049291810066
At time: 266.5369153022766 and batch: 650, loss is 3.160268044471741 and perplexity is 23.576914743826396
At time: 267.461505651474 and batch: 700, loss is 3.153662338256836 and perplexity is 23.421685834316264
At time: 268.3922688961029 and batch: 750, loss is 3.2474211168289187 and perplexity is 25.723915331041653
At time: 269.32356238365173 and batch: 800, loss is 3.1715024518966675 and perplexity is 23.84328084096911
At time: 270.2508182525635 and batch: 850, loss is 3.231390562057495 and perplexity is 25.31483435527272
At time: 271.17635846138 and batch: 900, loss is 3.190900249481201 and perplexity is 24.31030293252098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367021952589897 and perplexity of 78.808586178962
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 273.35471749305725 and batch: 50, loss is 3.611441740989685 and perplexity is 37.01938672599703
At time: 274.285359621048 and batch: 100, loss is 3.4895512104034423 and perplexity is 32.771237015237496
At time: 275.21065616607666 and batch: 150, loss is 3.4930329847335817 and perplexity is 32.88553793658086
At time: 276.13754200935364 and batch: 200, loss is 3.36689067363739 and perplexity is 28.98825284575432
At time: 277.0639445781708 and batch: 250, loss is 3.5095453882217407 and perplexity is 33.43306526333225
At time: 277.9904706478119 and batch: 300, loss is 3.4871125173568727 and perplexity is 32.69141539717317
At time: 278.9145565032959 and batch: 350, loss is 3.4625628852844237 and perplexity is 31.898624357908787
At time: 279.8456552028656 and batch: 400, loss is 3.3876155185699464 and perplexity is 29.59529862770134
At time: 280.77196764945984 and batch: 450, loss is 3.397449097633362 and perplexity is 29.887761960570078
At time: 281.6977229118347 and batch: 500, loss is 3.2573561763763426 and perplexity is 25.980757721227906
At time: 282.6292450428009 and batch: 550, loss is 3.3072479438781737 and perplexity is 27.309863679541042
At time: 283.5563848018646 and batch: 600, loss is 3.3383227157592774 and perplexity is 28.171834880901763
At time: 284.4813988208771 and batch: 650, loss is 3.157956070899963 and perplexity is 23.522468503421333
At time: 285.40718626976013 and batch: 700, loss is 3.1516447257995606 and perplexity is 23.37447758919273
At time: 286.33237195014954 and batch: 750, loss is 3.244977822303772 and perplexity is 25.661140948928676
At time: 287.25728130340576 and batch: 800, loss is 3.1681711053848267 and perplexity is 23.763982768523853
At time: 288.1825807094574 and batch: 850, loss is 3.22754168510437 and perplexity is 25.217587937548004
At time: 289.10851287841797 and batch: 900, loss is 3.1868203496932983 and perplexity is 24.211321387435554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366479743016909 and perplexity of 78.76586699152492
finished 16 epochs...
Completing Train Step...
At time: 291.2840895652771 and batch: 50, loss is 3.6104862022399904 and perplexity is 36.98403016246388
At time: 292.2079393863678 and batch: 100, loss is 3.4885456657409666 and perplexity is 32.73830063504996
At time: 293.13319969177246 and batch: 150, loss is 3.491931881904602 and perplexity is 32.849347506075596
At time: 294.0594425201416 and batch: 200, loss is 3.365895586013794 and perplexity is 28.959421341433476
At time: 294.98640179634094 and batch: 250, loss is 3.5084941577911377 and perplexity is 33.39793787446406
At time: 295.9119369983673 and batch: 300, loss is 3.486227912902832 and perplexity is 32.6625092126587
At time: 296.8365659713745 and batch: 350, loss is 3.461645450592041 and perplexity is 31.86937287349857
At time: 297.76095819473267 and batch: 400, loss is 3.386801323890686 and perplexity is 29.571212099919528
At time: 298.6861605644226 and batch: 450, loss is 3.396671690940857 and perplexity is 29.864536043559337
At time: 299.6205093860626 and batch: 500, loss is 3.256685566902161 and perplexity is 25.963340619643027
At time: 300.54412364959717 and batch: 550, loss is 3.306632342338562 and perplexity is 27.29305685909637
At time: 301.46936106681824 and batch: 600, loss is 3.3378956604003904 and perplexity is 28.159806516417756
At time: 302.4005365371704 and batch: 650, loss is 3.1575198364257813 and perplexity is 23.512209429585813
At time: 303.3270101547241 and batch: 700, loss is 3.151366057395935 and perplexity is 23.36796476833821
At time: 304.2514498233795 and batch: 750, loss is 3.2449363231658936 and perplexity is 25.66007605579854
At time: 305.1775059700012 and batch: 800, loss is 3.168285708427429 and perplexity is 23.76670634931582
At time: 306.10308504104614 and batch: 850, loss is 3.227785577774048 and perplexity is 25.2237390724708
At time: 307.0303678512573 and batch: 900, loss is 3.1872330331802368 and perplexity is 24.221315061939496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366197977980522 and perplexity of 78.7436766505239
finished 17 epochs...
Completing Train Step...
At time: 309.2060356140137 and batch: 50, loss is 3.609716100692749 and perplexity is 36.95555966760566
At time: 310.1318280696869 and batch: 100, loss is 3.487700629234314 and perplexity is 32.71064726154822
At time: 311.05409145355225 and batch: 150, loss is 3.4910246753692626 and perplexity is 32.81955987714527
At time: 311.97521924972534 and batch: 200, loss is 3.3650429105758666 and perplexity is 28.934738878719354
At time: 312.89701199531555 and batch: 250, loss is 3.5076036834716797 and perplexity is 33.36821110589031
At time: 313.8194136619568 and batch: 300, loss is 3.4854278421401976 and perplexity is 32.63638734506805
At time: 314.7409243583679 and batch: 350, loss is 3.4608372974395754 and perplexity is 31.843627943668046
At time: 315.6646132469177 and batch: 400, loss is 3.3860607862472536 and perplexity is 29.549321610563705
At time: 316.58700942993164 and batch: 450, loss is 3.3959702253341675 and perplexity is 29.843594444429513
At time: 317.5089497566223 and batch: 500, loss is 3.2560606813430786 and perplexity is 25.947121571076664
At time: 318.4311113357544 and batch: 550, loss is 3.3060675621032716 and perplexity is 27.277646632127126
At time: 319.3506917953491 and batch: 600, loss is 3.337474613189697 and perplexity is 28.14795240417593
At time: 320.2710385322571 and batch: 650, loss is 3.15713849067688 and perplexity is 23.503244857881853
At time: 321.19281816482544 and batch: 700, loss is 3.151111979484558 and perplexity is 23.362028238859185
At time: 322.116956949234 and batch: 750, loss is 3.2448617792129517 and perplexity is 25.65816332358875
At time: 323.04188299179077 and batch: 800, loss is 3.1683252239227295 and perplexity is 23.767645521044667
At time: 323.96738862991333 and batch: 850, loss is 3.227972803115845 and perplexity is 25.22846203775563
At time: 324.8927221298218 and batch: 900, loss is 3.1875458908081056 and perplexity is 24.228894070627156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366057513511344 and perplexity of 78.7326167385626
finished 18 epochs...
Completing Train Step...
At time: 327.0913932323456 and batch: 50, loss is 3.6090296649932863 and perplexity is 36.93020075678279
At time: 328.02008080482483 and batch: 100, loss is 3.4869381904602053 and perplexity is 32.68571690089433
At time: 328.9506435394287 and batch: 150, loss is 3.4902111959457396 and perplexity is 32.79287269670389
At time: 329.87597942352295 and batch: 200, loss is 3.3642674255371094 and perplexity is 28.912309119713893
At time: 330.7979838848114 and batch: 250, loss is 3.506794352531433 and perplexity is 33.34121610563962
At time: 331.72208762168884 and batch: 300, loss is 3.484673094749451 and perplexity is 32.61176441009278
At time: 332.6475899219513 and batch: 350, loss is 3.4600837659835815 and perplexity is 31.819641806628873
At time: 333.5746445655823 and batch: 400, loss is 3.3853564786911012 and perplexity is 29.52851712731159
At time: 334.50054240226746 and batch: 450, loss is 3.395307631492615 and perplexity is 29.82382681221974
At time: 335.426531791687 and batch: 500, loss is 3.2554625701904296 and perplexity is 25.93160694848278
At time: 336.3509418964386 and batch: 550, loss is 3.305527648925781 and perplexity is 27.262923046346177
At time: 337.2721371650696 and batch: 600, loss is 3.3370547819137575 and perplexity is 28.136137493711004
At time: 338.19132256507874 and batch: 650, loss is 3.1567778539657594 and perplexity is 23.494770253174284
At time: 339.10909819602966 and batch: 700, loss is 3.1508586502075193 and perplexity is 23.356110702709497
At time: 340.02614665031433 and batch: 750, loss is 3.2447572898864747 and perplexity is 25.65548245944771
At time: 340.94369101524353 and batch: 800, loss is 3.1683207035064695 and perplexity is 23.76753808163623
At time: 341.8623445034027 and batch: 850, loss is 3.2281134271621705 and perplexity is 25.232010015629612
At time: 342.78818440437317 and batch: 900, loss is 3.187791953086853 and perplexity is 24.234856621062992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365999822747217 and perplexity of 78.72807472475863
finished 19 epochs...
Completing Train Step...
At time: 344.9862115383148 and batch: 50, loss is 3.6083888673782347 and perplexity is 36.906543552763935
At time: 345.9139597415924 and batch: 100, loss is 3.486226644515991 and perplexity is 32.6624677839881
At time: 346.83961606025696 and batch: 150, loss is 3.489452714920044 and perplexity is 32.76800935536369
At time: 347.7656786441803 and batch: 200, loss is 3.3635412883758544 and perplexity is 28.8913224381712
At time: 348.69772458076477 and batch: 250, loss is 3.506034197807312 and perplexity is 33.31588125313283
At time: 349.62440395355225 and batch: 300, loss is 3.4839493894577025 and perplexity is 32.58817164175132
At time: 350.55137300491333 and batch: 350, loss is 3.459365978240967 and perplexity is 31.796810252850683
At time: 351.4767725467682 and batch: 400, loss is 3.384676971435547 and perplexity is 29.508459101236728
At time: 352.4011287689209 and batch: 450, loss is 3.3946710062026977 and perplexity is 29.80484625222228
At time: 353.32909297943115 and batch: 500, loss is 3.254885268211365 and perplexity is 25.916640900851206
At time: 354.2577304840088 and batch: 550, loss is 3.3050043535232545 and perplexity is 27.248660216218777
At time: 355.18359446525574 and batch: 600, loss is 3.3366368198394776 and perplexity is 28.124380112562772
At time: 356.11057019233704 and batch: 650, loss is 3.1564268493652343 and perplexity is 23.486524927885352
At time: 357.0385117530823 and batch: 700, loss is 3.15060209274292 and perplexity is 23.35011928676882
At time: 357.96471786499023 and batch: 750, loss is 3.2446309995651244 and perplexity is 25.65224262490769
At time: 358.88816928863525 and batch: 800, loss is 3.16828800201416 and perplexity is 23.76676086038065
At time: 359.8126599788666 and batch: 850, loss is 3.228217477798462 and perplexity is 25.234635558919006
At time: 360.7387192249298 and batch: 900, loss is 3.187988805770874 and perplexity is 24.239627787228738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365991879815924 and perplexity of 78.72744939555372
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f825458fb70>
ELAPSED
596.3744053840637


RESULTS SO FAR:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.72744939555372, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.6405754777197848, 'batch_size': 32, 'dropout': 0.6765087688232242, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3546874523162842 and batch: 50, loss is 6.942959241867065 and perplexity is 1035.8309583259174
At time: 2.5077788829803467 and batch: 100, loss is 6.111232051849365 and perplexity is 450.89389775111977
At time: 3.65661883354187 and batch: 150, loss is 5.99304386138916 and perplexity is 400.6322248219066
At time: 4.809894800186157 and batch: 200, loss is 5.834548664093018 and perplexity is 341.9103827880385
At time: 5.964848518371582 and batch: 250, loss is 5.874216175079345 and perplexity is 355.745709008292
At time: 7.117366075515747 and batch: 300, loss is 5.780901899337769 and perplexity is 324.0513203327632
At time: 8.270651578903198 and batch: 350, loss is 5.767197265625 and perplexity is 319.64060831345216
At time: 9.423293828964233 and batch: 400, loss is 5.635339469909668 and perplexity is 280.1540050245572
At time: 10.575297832489014 and batch: 450, loss is 5.639179410934449 and perplexity is 281.2318479839943
At time: 11.727643013000488 and batch: 500, loss is 5.591981811523437 and perplexity is 268.2667473736518
At time: 12.879151105880737 and batch: 550, loss is 5.645709857940674 and perplexity is 283.07442754949096
At time: 14.036609411239624 and batch: 600, loss is 5.572766799926757 and perplexity is 263.16120730425934
At time: 15.191521883010864 and batch: 650, loss is 5.471824893951416 and perplexity is 237.89392807074
At time: 16.346998929977417 and batch: 700, loss is 5.575751333236695 and perplexity is 263.94779390642725
At time: 17.5001277923584 and batch: 750, loss is 5.5383571338653566 and perplexity is 254.25994109899466
At time: 18.656371355056763 and batch: 800, loss is 5.520202617645264 and perplexity is 249.6856227776979
At time: 19.812934398651123 and batch: 850, loss is 5.567218017578125 and perplexity is 261.70502679287523
At time: 20.967912197113037 and batch: 900, loss is 5.459275131225586 and perplexity is 234.92707133262712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.362975656169734 and perplexity of 213.3588854734203
finished 1 epochs...
Completing Train Step...
At time: 23.310619354248047 and batch: 50, loss is 5.278602705001831 and perplexity is 196.09568031746556
At time: 24.232654809951782 and batch: 100, loss is 5.111861362457275 and perplexity is 165.97901462729675
At time: 25.15466547012329 and batch: 150, loss is 5.059741315841674 and perplexity is 157.54975542561832
At time: 26.079670667648315 and batch: 200, loss is 4.909638957977295 and perplexity is 135.59045171925467
At time: 27.00218939781189 and batch: 250, loss is 4.978771324157715 and perplexity is 145.29575055752952
At time: 27.92522954940796 and batch: 300, loss is 4.912832593917846 and perplexity is 136.0241704603047
At time: 28.849772930145264 and batch: 350, loss is 4.878052368164062 and perplexity is 131.37454530756952
At time: 29.773671627044678 and batch: 400, loss is 4.739448614120484 and perplexity is 114.37112166953025
At time: 30.694321155548096 and batch: 450, loss is 4.740263786315918 and perplexity is 114.46439183833513
At time: 31.616015672683716 and batch: 500, loss is 4.644202346801758 and perplexity is 103.98039240526708
At time: 32.53852844238281 and batch: 550, loss is 4.70906735420227 and perplexity is 110.94863585857209
At time: 33.46198391914368 and batch: 600, loss is 4.653774137496948 and perplexity is 104.98044949097226
At time: 34.386268854141235 and batch: 650, loss is 4.511018733978272 and perplexity is 91.0144908526296
At time: 35.31072998046875 and batch: 700, loss is 4.55939507484436 and perplexity is 95.52567646377136
At time: 36.233734369277954 and batch: 750, loss is 4.595484018325806 and perplexity is 99.03605921634164
At time: 37.16024565696716 and batch: 800, loss is 4.522862825393677 and perplexity is 92.09888395115757
At time: 38.084559202194214 and batch: 850, loss is 4.5936970615386965 and perplexity is 98.85924408570605
At time: 39.014453649520874 and batch: 900, loss is 4.521381158828735 and perplexity is 91.96252515821695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6188070218857025 and perplexity of 101.37302416476159
finished 2 epochs...
Completing Train Step...
At time: 41.199490547180176 and batch: 50, loss is 4.557691383361816 and perplexity is 95.3630687384457
At time: 42.134435415267944 and batch: 100, loss is 4.411684803962707 and perplexity is 82.40818825151042
At time: 43.064046144485474 and batch: 150, loss is 4.413748197555542 and perplexity is 82.57840433013614
At time: 43.992032527923584 and batch: 200, loss is 4.305157179832459 and perplexity is 74.08085855473023
At time: 44.92411994934082 and batch: 250, loss is 4.4432730293273925 and perplexity is 85.05286712511945
At time: 45.85584616661072 and batch: 300, loss is 4.406030941009521 and perplexity is 81.94357830704163
At time: 46.78838801383972 and batch: 350, loss is 4.39981125831604 and perplexity is 81.43549694154443
At time: 47.71456456184387 and batch: 400, loss is 4.299632959365844 and perplexity is 73.67274784434673
At time: 48.64008045196533 and batch: 450, loss is 4.318135347366333 and perplexity is 75.04855824982755
At time: 49.562904834747314 and batch: 500, loss is 4.199434785842896 and perplexity is 66.64864963257136
At time: 50.48637366294861 and batch: 550, loss is 4.2782904195785525 and perplexity is 72.11704467262983
At time: 51.41101002693176 and batch: 600, loss is 4.2767372846603395 and perplexity is 72.00512410871654
At time: 52.335575580596924 and batch: 650, loss is 4.12336754322052 and perplexity is 61.766895117116675
At time: 53.25970721244812 and batch: 700, loss is 4.149911088943481 and perplexity is 63.42836056485865
At time: 54.184897899627686 and batch: 750, loss is 4.233978447914123 and perplexity is 68.99116465458819
At time: 55.10903525352478 and batch: 800, loss is 4.176016759872437 and perplexity is 65.10600319060497
At time: 56.042773962020874 and batch: 850, loss is 4.254534392356873 and perplexity is 70.42401959183753
At time: 56.96908736228943 and batch: 900, loss is 4.195812854766846 and perplexity is 66.40768945129203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434644725224743 and perplexity of 84.32216202891773
finished 3 epochs...
Completing Train Step...
At time: 59.1433162689209 and batch: 50, loss is 4.268819680213928 and perplexity is 71.43726701832502
At time: 60.075722455978394 and batch: 100, loss is 4.12073429107666 and perplexity is 61.60446126663238
At time: 61.00957536697388 and batch: 150, loss is 4.130018405914306 and perplexity is 62.179067386814395
At time: 61.94857907295227 and batch: 200, loss is 4.017534613609314 and perplexity is 55.56395021879955
At time: 62.87399506568909 and batch: 250, loss is 4.173227248191833 and perplexity is 64.92464230598347
At time: 63.81681680679321 and batch: 300, loss is 4.143799939155579 and perplexity is 63.04192234622889
At time: 64.74321746826172 and batch: 350, loss is 4.135083656311036 and perplexity is 62.4948189378016
At time: 65.66863942146301 and batch: 400, loss is 4.055455193519593 and perplexity is 57.71142688565823
At time: 66.59398794174194 and batch: 450, loss is 4.07722728729248 and perplexity is 58.9817035973407
At time: 67.51768326759338 and batch: 500, loss is 3.948923497200012 and perplexity is 51.87948834908479
At time: 68.44912457466125 and batch: 550, loss is 4.028738312721252 and perplexity is 56.18997233189041
At time: 69.38207340240479 and batch: 600, loss is 4.047651448249817 and perplexity is 57.262814316841364
At time: 70.30850410461426 and batch: 650, loss is 3.8872221183776854 and perplexity is 48.77520640982335
At time: 71.23525261878967 and batch: 700, loss is 3.9038930463790895 and perplexity is 49.59514998449006
At time: 72.1608395576477 and batch: 750, loss is 4.00876097202301 and perplexity is 55.07858436098593
At time: 73.0908637046814 and batch: 800, loss is 3.9539816331863404 and perplexity is 52.142566638036996
At time: 74.0200674533844 and batch: 850, loss is 4.033328170776367 and perplexity is 56.448469106959166
At time: 74.95038318634033 and batch: 900, loss is 3.9856024599075317 and perplexity is 53.81770272127458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373194864351455 and perplexity of 79.29656921607052
finished 4 epochs...
Completing Train Step...
At time: 77.1432831287384 and batch: 50, loss is 4.063831267356872 and perplexity is 58.19685219958264
At time: 78.07036924362183 and batch: 100, loss is 3.920015549659729 and perplexity is 50.401228493925224
At time: 78.99560832977295 and batch: 150, loss is 3.929888334274292 and perplexity is 50.901293421970095
At time: 79.92075085639954 and batch: 200, loss is 3.815736107826233 and perplexity is 45.41017086527197
At time: 80.84471774101257 and batch: 250, loss is 3.975788893699646 and perplexity is 53.292142162018365
At time: 81.77501034736633 and batch: 300, loss is 3.947743935585022 and perplexity is 51.81832937350409
At time: 82.69927072525024 and batch: 350, loss is 3.9443596267700194 and perplexity is 51.643256561956676
At time: 83.6221399307251 and batch: 400, loss is 3.868951530456543 and perplexity is 47.89214629121983
At time: 84.54699802398682 and batch: 450, loss is 3.890541787147522 and perplexity is 48.93739299320822
At time: 85.47108459472656 and batch: 500, loss is 3.7670920848846436 and perplexity is 43.254102522229296
At time: 86.40624499320984 and batch: 550, loss is 3.8454373836517335 and perplexity is 46.77914030824839
At time: 87.33150553703308 and batch: 600, loss is 3.8711740493774416 and perplexity is 47.998705863984455
At time: 88.25728392601013 and batch: 650, loss is 3.708419055938721 and perplexity is 40.78926997196428
At time: 89.1835868358612 and batch: 700, loss is 3.7225307035446167 and perplexity is 41.368954306675555
At time: 90.10927224159241 and batch: 750, loss is 3.8324705600738525 and perplexity is 46.176479193113344
At time: 91.03494906425476 and batch: 800, loss is 3.7799932241439818 and perplexity is 43.81574484538972
At time: 91.96074962615967 and batch: 850, loss is 3.8582944297790527 and perplexity is 47.384464877435555
At time: 92.89031863212585 and batch: 900, loss is 3.816219835281372 and perplexity is 45.43214232533285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357490853087543 and perplexity of 78.06102191630501
finished 5 epochs...
Completing Train Step...
At time: 95.05339455604553 and batch: 50, loss is 3.901105980873108 and perplexity is 49.45711749484955
At time: 95.98358821868896 and batch: 100, loss is 3.758906850814819 and perplexity is 42.901502593702304
At time: 96.90719699859619 and batch: 150, loss is 3.7677924728393553 and perplexity is 43.28440778610979
At time: 97.8312873840332 and batch: 200, loss is 3.659573879241943 and perplexity is 38.84478677414789
At time: 98.75578570365906 and batch: 250, loss is 3.81722234249115 and perplexity is 45.47771121332255
At time: 99.6792459487915 and batch: 300, loss is 3.791220660209656 and perplexity is 44.310455287511836
At time: 100.60191130638123 and batch: 350, loss is 3.7874474334716797 and perplexity is 44.14357692578788
At time: 101.52838683128357 and batch: 400, loss is 3.7121146011352537 and perplexity is 40.94028743674142
At time: 102.45370173454285 and batch: 450, loss is 3.7381941509246825 and perplexity is 42.022036149769875
At time: 103.38580918312073 and batch: 500, loss is 3.6189953660964966 and perplexity is 37.30007607144048
At time: 104.31435060501099 and batch: 550, loss is 3.693742299079895 and perplexity is 40.19498751141427
At time: 105.24034881591797 and batch: 600, loss is 3.726968836784363 and perplexity is 41.55296326397678
At time: 106.16522717475891 and batch: 650, loss is 3.5615985441207885 and perplexity is 35.21945201814012
At time: 107.09145474433899 and batch: 700, loss is 3.5761929178237915 and perplexity is 35.737226972809914
At time: 108.01699256896973 and batch: 750, loss is 3.6894973373413085 and perplexity is 40.024722966261415
At time: 108.93902015686035 and batch: 800, loss is 3.636039719581604 and perplexity is 37.94128068915023
At time: 109.86742448806763 and batch: 850, loss is 3.715354976654053 and perplexity is 41.07316451142616
At time: 110.79252743721008 and batch: 900, loss is 3.6751448106765747 and perplexity is 39.45436985753172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367781965699915 and perplexity of 78.86850450411211
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 112.9601821899414 and batch: 50, loss is 3.782558946609497 and perplexity is 43.92830822773251
At time: 113.88681817054749 and batch: 100, loss is 3.651771402359009 and perplexity is 38.54288056307167
At time: 114.80829787254333 and batch: 150, loss is 3.6605498886108396 and perplexity is 38.882718157653976
At time: 115.73155879974365 and batch: 200, loss is 3.5396159267425538 and perplexity is 34.45368391075089
At time: 116.65408396720886 and batch: 250, loss is 3.6942209911346438 and perplexity is 40.214233138573604
At time: 117.58524918556213 and batch: 300, loss is 3.6497481060028076 and perplexity is 38.46497573212796
At time: 118.51028394699097 and batch: 350, loss is 3.6326573085784912 and perplexity is 37.81316447686307
At time: 119.4336371421814 and batch: 400, loss is 3.5495932292938233 and perplexity is 34.799159329620416
At time: 120.3577139377594 and batch: 450, loss is 3.558360333442688 and perplexity is 35.10558846911611
At time: 121.28404998779297 and batch: 500, loss is 3.433495969772339 and perplexity is 30.98477546361951
At time: 122.20628952980042 and batch: 550, loss is 3.486000657081604 and perplexity is 32.65508731067137
At time: 123.12958598136902 and batch: 600, loss is 3.5110605144500733 and perplexity is 33.48375897139189
At time: 124.05138921737671 and batch: 650, loss is 3.3288518953323365 and perplexity is 27.90628396905876
At time: 124.97593760490417 and batch: 700, loss is 3.3222379636764527 and perplexity is 27.722322738222687
At time: 125.90033721923828 and batch: 750, loss is 3.4227700424194336 and perplexity is 30.654210987386765
At time: 126.82588124275208 and batch: 800, loss is 3.3456765317916872 and perplexity is 28.379768989437064
At time: 127.75038981437683 and batch: 850, loss is 3.401493272781372 and perplexity is 30.008878046979355
At time: 128.67570805549622 and batch: 900, loss is 3.354150152206421 and perplexity is 28.621270128778413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345994087114726 and perplexity of 77.16871178456356
finished 7 epochs...
Completing Train Step...
At time: 130.86955571174622 and batch: 50, loss is 3.664720206260681 and perplexity is 39.04521002932212
At time: 131.79100012779236 and batch: 100, loss is 3.5261934661865233 and perplexity is 33.994320487810704
At time: 132.72006177902222 and batch: 150, loss is 3.533343052864075 and perplexity is 34.2382367398628
At time: 133.64459657669067 and batch: 200, loss is 3.4179645109176637 and perplexity is 30.507254595417326
At time: 134.5692069530487 and batch: 250, loss is 3.570179133415222 and perplexity is 35.52295593013288
At time: 135.4931206703186 and batch: 300, loss is 3.5324611949920652 and perplexity is 34.20805679043886
At time: 136.4174325466156 and batch: 350, loss is 3.5213331604003906 and perplexity is 33.82949856218724
At time: 137.34256792068481 and batch: 400, loss is 3.440527458190918 and perplexity is 31.203412323903247
At time: 138.26793265342712 and batch: 450, loss is 3.455141377449036 and perplexity is 31.662764766770835
At time: 139.19392371177673 and batch: 500, loss is 3.3359720659255983 and perplexity is 28.10569053348118
At time: 140.11951851844788 and batch: 550, loss is 3.3913499689102173 and perplexity is 29.706027427647907
At time: 141.05701398849487 and batch: 600, loss is 3.4250558614730835 and perplexity is 30.7243611116373
At time: 141.98271417617798 and batch: 650, loss is 3.247733817100525 and perplexity is 25.73196046414454
At time: 142.90770149230957 and batch: 700, loss is 3.247469983100891 and perplexity is 25.725172393598147
At time: 143.83344292640686 and batch: 750, loss is 3.355063271522522 and perplexity is 28.647416699036803
At time: 144.75914335250854 and batch: 800, loss is 3.2841883420944216 and perplexity is 26.68731455202445
At time: 145.68463683128357 and batch: 850, loss is 3.346710653305054 and perplexity is 28.40913229909175
At time: 146.6100890636444 and batch: 900, loss is 3.30896755695343 and perplexity is 27.35686647994038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366961335482663 and perplexity of 78.80380917522815
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 148.7817897796631 and batch: 50, loss is 3.607651391029358 and perplexity is 36.87933588351439
At time: 149.70663332939148 and batch: 100, loss is 3.4820871543884278 and perplexity is 32.52754127719385
At time: 150.63265562057495 and batch: 150, loss is 3.496844882965088 and perplexity is 33.01113348713077
At time: 151.5561125278473 and batch: 200, loss is 3.378379807472229 and perplexity is 29.32322333807304
At time: 152.48162174224854 and batch: 250, loss is 3.528454089164734 and perplexity is 34.071255757889524
At time: 153.40699696540833 and batch: 300, loss is 3.485692148208618 and perplexity is 32.64501448034655
At time: 154.33234429359436 and batch: 350, loss is 3.4736528253555297 and perplexity is 32.25434701611181
At time: 155.26294207572937 and batch: 400, loss is 3.391158366203308 and perplexity is 29.70033621762428
At time: 156.19457983970642 and batch: 450, loss is 3.3997861909866334 and perplexity is 29.957694137574492
At time: 157.12180161476135 and batch: 500, loss is 3.271887364387512 and perplexity is 26.36104532002032
At time: 158.04857420921326 and batch: 550, loss is 3.3219955158233643 and perplexity is 27.71560233529873
At time: 158.97074913978577 and batch: 600, loss is 3.351056680679321 and perplexity is 28.532867850029334
At time: 159.88958072662354 and batch: 650, loss is 3.1674371004104613 and perplexity is 23.746546286975963
At time: 160.80973935127258 and batch: 700, loss is 3.1610926485061643 and perplexity is 23.5963643808659
At time: 161.72744297981262 and batch: 750, loss is 3.2589573287963867 and perplexity is 26.022390195401822
At time: 162.64850616455078 and batch: 800, loss is 3.1837557649612425 and perplexity is 24.1372373180039
At time: 163.57191276550293 and batch: 850, loss is 3.2356952667236327 and perplexity is 25.424042125804725
At time: 164.49532461166382 and batch: 900, loss is 3.203507866859436 and perplexity is 24.618738161396372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365090148089683 and perplexity of 78.65649035452518
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 166.6741828918457 and batch: 50, loss is 3.5863584518432616 and perplexity is 36.10236775045518
At time: 167.60401344299316 and batch: 100, loss is 3.458023495674133 and perplexity is 31.754152229638297
At time: 168.52850437164307 and batch: 150, loss is 3.4723416662216184 and perplexity is 32.21208414714127
At time: 169.45249247550964 and batch: 200, loss is 3.354926052093506 and perplexity is 28.643485986565796
At time: 170.37795853614807 and batch: 250, loss is 3.5045098304748534 and perplexity is 33.265134300423654
At time: 171.30929470062256 and batch: 300, loss is 3.4606308698654176 and perplexity is 31.837055219218374
At time: 172.23575353622437 and batch: 350, loss is 3.4495693826675415 and perplexity is 31.486830614000514
At time: 173.15872383117676 and batch: 400, loss is 3.3667941331863402 and perplexity is 28.98545444183121
At time: 174.08021664619446 and batch: 450, loss is 3.3762672567367553 and perplexity is 29.261341927867115
At time: 175.005432844162 and batch: 500, loss is 3.2466395711898803 and perplexity is 25.703818771407896
At time: 175.92998027801514 and batch: 550, loss is 3.2953548192977906 and perplexity is 26.986987875401333
At time: 176.86395716667175 and batch: 600, loss is 3.325391960144043 and perplexity is 27.809896877879293
At time: 177.78827548027039 and batch: 650, loss is 3.1421951818466187 and perplexity is 23.1546397553188
At time: 178.71721529960632 and batch: 700, loss is 3.1332131814956665 and perplexity is 22.947595998417935
At time: 179.64121079444885 and batch: 750, loss is 3.2278120517730713 and perplexity is 25.224406854553756
At time: 180.5664083957672 and batch: 800, loss is 3.152400631904602 and perplexity is 23.39215317920611
At time: 181.49762177467346 and batch: 850, loss is 3.199890947341919 and perplexity is 24.52985500535294
At time: 182.42151999473572 and batch: 900, loss is 3.1696740055084227 and perplexity is 23.79972451257206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363162524079623 and perplexity of 78.5050162546298
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.59752941131592 and batch: 50, loss is 3.5775508785247805 and perplexity is 35.785789688268174
At time: 185.52175760269165 and batch: 100, loss is 3.4480122900009156 and perplexity is 31.437840851654133
At time: 186.44620943069458 and batch: 150, loss is 3.462565460205078 and perplexity is 31.898706494441242
At time: 187.36985278129578 and batch: 200, loss is 3.3459429597854613 and perplexity is 28.38733116169515
At time: 188.29304885864258 and batch: 250, loss is 3.4952035045623777 and perplexity is 32.956994169284364
At time: 189.21673250198364 and batch: 300, loss is 3.451119890213013 and perplexity is 31.53568905034934
At time: 190.13854384422302 and batch: 350, loss is 3.440210680961609 and perplexity is 31.19352935883408
At time: 191.0619523525238 and batch: 400, loss is 3.3574200963973997 and perplexity is 28.715013268662997
At time: 191.98498129844666 and batch: 450, loss is 3.3676119327545164 and perplexity is 29.009168429279825
At time: 192.90969562530518 and batch: 500, loss is 3.2369754695892334 and perplexity is 25.456610900260394
At time: 193.83628153800964 and batch: 550, loss is 3.285482349395752 and perplexity is 26.721870484900528
At time: 194.76165771484375 and batch: 600, loss is 3.3158267068862917 and perplexity is 27.5451563448063
At time: 195.68419861793518 and batch: 650, loss is 3.1337607097625733 and perplexity is 22.960163896210297
At time: 196.60866856575012 and batch: 700, loss is 3.1248533153533935 and perplexity is 22.7565568112015
At time: 197.53350734710693 and batch: 750, loss is 3.217883048057556 and perplexity is 24.975192895730316
At time: 198.4673135280609 and batch: 800, loss is 3.1427468204498292 and perplexity is 23.167416272137217
At time: 199.39145255088806 and batch: 850, loss is 3.189798893928528 and perplexity is 24.28354338399267
At time: 200.31568717956543 and batch: 900, loss is 3.158992738723755 and perplexity is 23.546866133589102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3623724114404965 and perplexity of 78.44301294707626
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.51037311553955 and batch: 50, loss is 3.57433452129364 and perplexity is 35.670874707712414
At time: 203.44113397598267 and batch: 100, loss is 3.444976496696472 and perplexity is 31.342546784622
At time: 204.36839938163757 and batch: 150, loss is 3.4597813844680787 and perplexity is 31.810021589677824
At time: 205.29368329048157 and batch: 200, loss is 3.3432283782958985 and perplexity is 28.310375935857927
At time: 206.21854043006897 and batch: 250, loss is 3.4923512887954713 and perplexity is 32.86312763831942
At time: 207.14326214790344 and batch: 300, loss is 3.4488553380966187 and perplexity is 31.46435563856704
At time: 208.0687963962555 and batch: 350, loss is 3.437090668678284 and perplexity is 31.096356832638886
At time: 208.99299573898315 and batch: 400, loss is 3.354923338890076 and perplexity is 28.6434082710668
At time: 209.91800904273987 and batch: 450, loss is 3.3653447818756104 and perplexity is 28.943474764442673
At time: 210.8426856994629 and batch: 500, loss is 3.2341573810577393 and perplexity is 25.384972905547478
At time: 211.76788330078125 and batch: 550, loss is 3.282611856460571 and perplexity is 26.645275529691176
At time: 212.69315195083618 and batch: 600, loss is 3.313334002494812 and perplexity is 27.47657991850983
At time: 213.61880826950073 and batch: 650, loss is 3.1311025381088258 and perplexity is 22.899212884391293
At time: 214.54468989372253 and batch: 700, loss is 3.122233247756958 and perplexity is 22.69701113500625
At time: 215.47029161453247 and batch: 750, loss is 3.2152065229415894 and perplexity is 24.908435543386837
At time: 216.39603543281555 and batch: 800, loss is 3.140090980529785 and perplexity is 23.105968956392605
At time: 217.32181406021118 and batch: 850, loss is 3.187190599441528 and perplexity is 24.22028728279139
At time: 218.24584102630615 and batch: 900, loss is 3.1559482192993165 and perplexity is 23.475286260707495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36164793249679 and perplexity of 78.38620321712655
Annealing...
Model not improving. Stopping early with 77.16871178456356 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f825458fb70>
ELAPSED
821.3813600540161


RESULTS SO FAR:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.72744939555372, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.16871178456356, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.6405754777197848, 'batch_size': 32, 'dropout': 0.6765087688232242, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.22667666633898154, 'batch_size': 32, 'dropout': 0.16013127858268816, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.371267318725586 and batch: 50, loss is 6.914615592956543 and perplexity is 1006.883899832936
At time: 2.5258872509002686 and batch: 100, loss is 6.016905851364136 and perplexity is 410.3070785221919
At time: 3.6775667667388916 and batch: 150, loss is 5.745875263214112 and perplexity is 312.8973755745454
At time: 4.830047845840454 and batch: 200, loss is 5.489986219406128 and perplexity is 242.25386843277658
At time: 5.983806848526001 and batch: 250, loss is 5.485443334579468 and perplexity is 241.15583302229558
At time: 7.1385338306427 and batch: 300, loss is 5.354813461303711 and perplexity is 211.62449651306727
At time: 8.294714212417603 and batch: 350, loss is 5.301558284759522 and perplexity is 200.6492351330073
At time: 9.453821897506714 and batch: 400, loss is 5.127942705154419 and perplexity is 168.66975743323238
At time: 10.607676982879639 and batch: 450, loss is 5.117415428161621 and perplexity is 166.90343775712498
At time: 11.76324462890625 and batch: 500, loss is 5.040006875991821 and perplexity is 154.4710771641024
At time: 12.913224697113037 and batch: 550, loss is 5.090180349349976 and perplexity is 162.41915160064917
At time: 14.068039894104004 and batch: 600, loss is 5.003112545013428 and perplexity is 148.8758213959553
At time: 15.223571300506592 and batch: 650, loss is 4.882698345184326 and perplexity is 131.986328490972
At time: 16.377942323684692 and batch: 700, loss is 4.953380727767945 and perplexity is 141.65304571900316
At time: 17.533429384231567 and batch: 750, loss is 4.945356321334839 and perplexity is 140.5209125310201
At time: 18.68952775001526 and batch: 800, loss is 4.887041416168213 and perplexity is 132.56080106911588
At time: 19.847496032714844 and batch: 850, loss is 4.932462711334228 and perplexity is 138.72072111032722
At time: 21.003079414367676 and batch: 900, loss is 4.849659194946289 and perplexity is 127.69686269316449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.8587098840164815 and perplexity of 128.8578532497114
finished 1 epochs...
Completing Train Step...
At time: 23.333065032958984 and batch: 50, loss is 4.840000476837158 and perplexity is 126.46941203541542
At time: 24.273388147354126 and batch: 100, loss is 4.706254367828369 and perplexity is 110.63697740869536
At time: 25.199328422546387 and batch: 150, loss is 4.692716464996338 and perplexity is 109.14927764773232
At time: 26.124011754989624 and batch: 200, loss is 4.578989019393921 and perplexity is 97.4158588627508
At time: 27.04889464378357 and batch: 250, loss is 4.694861764907837 and perplexity is 109.38368693261718
At time: 27.973262548446655 and batch: 300, loss is 4.6493939018249515 and perplexity is 104.52161601424656
At time: 28.899067640304565 and batch: 350, loss is 4.632552547454834 and perplexity is 102.7760703733747
At time: 29.82389235496521 and batch: 400, loss is 4.516220941543579 and perplexity is 91.48920082481229
At time: 30.74644923210144 and batch: 450, loss is 4.533345317840576 and perplexity is 93.069387568185
At time: 31.674333095550537 and batch: 500, loss is 4.431534585952758 and perplexity is 84.06031576154177
At time: 32.598278760910034 and batch: 550, loss is 4.50139045715332 and perplexity is 90.1423833232656
At time: 33.52240562438965 and batch: 600, loss is 4.473670835494995 and perplexity is 87.6779844612174
At time: 34.446683406829834 and batch: 650, loss is 4.330679559707642 and perplexity is 75.99591278592698
At time: 35.371012449264526 and batch: 700, loss is 4.367058038711548 and perplexity is 78.8114301265032
At time: 36.30336594581604 and batch: 750, loss is 4.43126148223877 and perplexity is 84.03736171166906
At time: 37.224868297576904 and batch: 800, loss is 4.370019636154175 and perplexity is 79.0451838277419
At time: 38.149604082107544 and batch: 850, loss is 4.440093088150024 and perplexity is 84.78283358363278
At time: 39.07426929473877 and batch: 900, loss is 4.379976797103882 and perplexity is 79.83618095298034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.537859772982663 and perplexity of 93.49049496349495
finished 2 epochs...
Completing Train Step...
At time: 41.25815010070801 and batch: 50, loss is 4.435721492767334 and perplexity is 84.41300629641358
At time: 42.18710708618164 and batch: 100, loss is 4.297041034698486 and perplexity is 73.48204088769474
At time: 43.1109619140625 and batch: 150, loss is 4.295151419639588 and perplexity is 73.34331922322501
At time: 44.034358978271484 and batch: 200, loss is 4.186983366012573 and perplexity is 65.82392446976966
At time: 44.958513021469116 and batch: 250, loss is 4.335719861984253 and perplexity is 76.37992210671382
At time: 45.88339018821716 and batch: 300, loss is 4.298286809921264 and perplexity is 73.57364003768107
At time: 46.806960582733154 and batch: 350, loss is 4.290534515380859 and perplexity is 73.00548062469443
At time: 47.73281955718994 and batch: 400, loss is 4.198504338264465 and perplexity is 66.58666539892693
At time: 48.65805101394653 and batch: 450, loss is 4.223692183494568 and perplexity is 68.28514069211296
At time: 49.58128833770752 and batch: 500, loss is 4.103194575309754 and perplexity is 60.53335741414405
At time: 50.50668406486511 and batch: 550, loss is 4.179996747970581 and perplexity is 65.36564064267789
At time: 51.43310332298279 and batch: 600, loss is 4.186200637817382 and perplexity is 65.77242238686475
At time: 52.35789656639099 and batch: 650, loss is 4.028045053482056 and perplexity is 56.15103161398697
At time: 53.28814911842346 and batch: 700, loss is 4.051465277671814 and perplexity is 57.48162190505979
At time: 54.214064598083496 and batch: 750, loss is 4.144975280761718 and perplexity is 63.11606170149304
At time: 55.13796067237854 and batch: 800, loss is 4.09205060005188 and perplexity is 59.86252001701478
At time: 56.06368064880371 and batch: 850, loss is 4.167053370475769 and perplexity is 64.5250403198198
At time: 56.98824214935303 and batch: 900, loss is 4.1182558059692385 and perplexity is 61.451964585273295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4171121675674225 and perplexity of 82.85666337106125
finished 3 epochs...
Completing Train Step...
At time: 59.16197848320007 and batch: 50, loss is 4.1895311737060545 and perplexity is 65.99184499471605
At time: 60.08736610412598 and batch: 100, loss is 4.053359408378601 and perplexity is 57.59060278962318
At time: 61.01215624809265 and batch: 150, loss is 4.057407474517822 and perplexity is 57.82420586005166
At time: 61.93572282791138 and batch: 200, loss is 3.947858600616455 and perplexity is 51.824271464539024
At time: 62.85983872413635 and batch: 250, loss is 4.103500075340271 and perplexity is 60.55185318176633
At time: 63.785794734954834 and batch: 300, loss is 4.0706961059570315 and perplexity is 58.59773863295425
At time: 64.7124674320221 and batch: 350, loss is 4.066631827354431 and perplexity is 58.36006441195858
At time: 65.63719367980957 and batch: 400, loss is 3.9830310344696045 and perplexity is 53.67949228655667
At time: 66.56281661987305 and batch: 450, loss is 4.01156331539154 and perplexity is 55.23314993837643
At time: 67.486652135849 and batch: 500, loss is 3.886941075325012 and perplexity is 48.76150040299838
At time: 68.41296529769897 and batch: 550, loss is 3.9652079820632933 and perplexity is 52.73123540037959
At time: 69.34005904197693 and batch: 600, loss is 3.9878606271743773 and perplexity is 53.93936941610902
At time: 70.26675367355347 and batch: 650, loss is 3.8220547533035276 and perplexity is 45.69801005525993
At time: 71.19441604614258 and batch: 700, loss is 3.8400730895996094 and perplexity is 46.528875092362306
At time: 72.1218535900116 and batch: 750, loss is 3.941455612182617 and perplexity is 51.493501342474964
At time: 73.04832148551941 and batch: 800, loss is 3.8936335754394533 and perplexity is 49.088931193203784
At time: 73.97439360618591 and batch: 850, loss is 3.9718343210220337 and perplexity is 53.081810672382325
At time: 74.8998155593872 and batch: 900, loss is 3.930675344467163 and perplexity is 50.941369026609664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37351968843643 and perplexity of 79.32233083537844
finished 4 epochs...
Completing Train Step...
At time: 77.09456825256348 and batch: 50, loss is 4.002137131690979 and perplexity is 54.71495824265358
At time: 78.02745580673218 and batch: 100, loss is 3.871119136810303 and perplexity is 47.99607020419221
At time: 78.95329594612122 and batch: 150, loss is 3.8756734704971314 and perplexity is 48.21515884625626
At time: 79.87618732452393 and batch: 200, loss is 3.7705920934677124 and perplexity is 43.4057574943469
At time: 80.80232763290405 and batch: 250, loss is 3.9254811573028565 and perplexity is 50.67745602160589
At time: 81.73014688491821 and batch: 300, loss is 3.8949220895767214 and perplexity is 49.15222374294528
At time: 82.65572261810303 and batch: 350, loss is 3.893174910545349 and perplexity is 49.06642098649178
At time: 83.57921576499939 and batch: 400, loss is 3.81207763671875 and perplexity is 45.24434259110986
At time: 84.50443005561829 and batch: 450, loss is 3.8434157466888426 and perplexity is 46.68466539827072
At time: 85.42963528633118 and batch: 500, loss is 3.722935981750488 and perplexity is 41.385723640148896
At time: 86.35496664047241 and batch: 550, loss is 3.7978089904785155 and perplexity is 44.603350988662115
At time: 87.28018856048584 and batch: 600, loss is 3.8279860162734987 and perplexity is 45.969862386958965
At time: 88.20407891273499 and batch: 650, loss is 3.6603091669082644 and perplexity is 38.87335937001515
At time: 89.12987232208252 and batch: 700, loss is 3.677922611236572 and perplexity is 39.56411858768833
At time: 90.05657911300659 and batch: 750, loss is 3.7787750482559206 and perplexity is 43.76240205854651
At time: 90.9797956943512 and batch: 800, loss is 3.7340941429138184 and perplexity is 41.85009817930844
At time: 91.90268611907959 and batch: 850, loss is 3.816518197059631 and perplexity is 45.445699562487285
At time: 92.83421325683594 and batch: 900, loss is 3.775893220901489 and perplexity is 43.63646791857943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364143267069777 and perplexity of 78.58204726665159
finished 5 epochs...
Completing Train Step...
At time: 95.00389790534973 and batch: 50, loss is 3.849308032989502 and perplexity is 46.96055683007657
At time: 95.93898844718933 and batch: 100, loss is 3.7249504947662353 and perplexity is 41.46917975257656
At time: 96.86348795890808 and batch: 150, loss is 3.7310680866241457 and perplexity is 41.7236488443837
At time: 97.78904867172241 and batch: 200, loss is 3.621595411300659 and perplexity is 37.39718414332711
At time: 98.71308016777039 and batch: 250, loss is 3.780331501960754 and perplexity is 43.830569247138044
At time: 99.64279437065125 and batch: 300, loss is 3.749773759841919 and perplexity is 42.511463111879735
At time: 100.56687307357788 and batch: 350, loss is 3.750245089530945 and perplexity is 42.531504749306634
At time: 101.4903609752655 and batch: 400, loss is 3.6710307931900026 and perplexity is 39.29238731798096
At time: 102.4159996509552 and batch: 450, loss is 3.699388928413391 and perplexity is 40.42259571176336
At time: 103.34014201164246 and batch: 500, loss is 3.587205572128296 and perplexity is 36.132963755933126
At time: 104.26150321960449 and batch: 550, loss is 3.658671884536743 and perplexity is 38.80976477935439
At time: 105.18683552742004 and batch: 600, loss is 3.6936258363723753 and perplexity is 40.19030656692299
At time: 106.11196446418762 and batch: 650, loss is 3.5268624877929686 and perplexity is 34.01707103216777
At time: 107.03675627708435 and batch: 700, loss is 3.5382756900787355 and perplexity is 34.40753874999554
At time: 107.96086072921753 and batch: 750, loss is 3.6431069660186766 and perplexity is 38.210370813031844
At time: 108.88494062423706 and batch: 800, loss is 3.6043879652023314 and perplexity is 36.75917907436431
At time: 109.8091242313385 and batch: 850, loss is 3.6807131242752074 and perplexity is 39.674676962019106
At time: 110.7343897819519 and batch: 900, loss is 3.645388240814209 and perplexity is 38.29763867203244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378133277370505 and perplexity of 79.68913695885341
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 112.93030166625977 and batch: 50, loss is 3.745056347846985 and perplexity is 42.31139130747116
At time: 113.8538613319397 and batch: 100, loss is 3.6285638093948362 and perplexity is 37.65869269969599
At time: 114.77716064453125 and batch: 150, loss is 3.635596899986267 and perplexity is 37.92448326597744
At time: 115.70131778717041 and batch: 200, loss is 3.5125544452667237 and perplexity is 33.53381877440734
At time: 116.63379979133606 and batch: 250, loss is 3.656908106803894 and perplexity is 38.741373311823224
At time: 117.55817031860352 and batch: 300, loss is 3.617451114654541 and perplexity is 37.242519827276546
At time: 118.48137664794922 and batch: 350, loss is 3.6023316478729246 and perplexity is 36.683668201185235
At time: 119.4060640335083 and batch: 400, loss is 3.521477823257446 and perplexity is 33.83439278809982
At time: 120.33116555213928 and batch: 450, loss is 3.5325394916534423 and perplexity is 34.21073527193446
At time: 121.25613403320312 and batch: 500, loss is 3.3977314043045044 and perplexity is 29.89620066625248
At time: 122.18263459205627 and batch: 550, loss is 3.4631397914886475 and perplexity is 31.91703218149034
At time: 123.10344123840332 and batch: 600, loss is 3.4874295949935914 and perplexity is 32.70178275745048
At time: 124.01988744735718 and batch: 650, loss is 3.301837306022644 and perplexity is 27.16249892545413
At time: 124.9372787475586 and batch: 700, loss is 3.291034708023071 and perplexity is 26.87065255650545
At time: 125.85629153251648 and batch: 750, loss is 3.3798006200790405 and perplexity is 29.36491575500618
At time: 126.77635765075684 and batch: 800, loss is 3.319443984031677 and perplexity is 27.644975236775124
At time: 127.70196199417114 and batch: 850, loss is 3.37365834236145 and perplexity is 29.1851010884258
At time: 128.62493777275085 and batch: 900, loss is 3.327673225402832 and perplexity is 27.873411048267585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361447268969392 and perplexity of 78.37047554312772
finished 7 epochs...
Completing Train Step...
At time: 130.80515503883362 and batch: 50, loss is 3.628534684181213 and perplexity is 37.65759589819871
At time: 131.73873209953308 and batch: 100, loss is 3.502676362991333 and perplexity is 33.2041996362814
At time: 132.666419506073 and batch: 150, loss is 3.505245885848999 and perplexity is 33.289628294659344
At time: 133.59249472618103 and batch: 200, loss is 3.3882321548461913 and perplexity is 29.613553790260767
At time: 134.51866173744202 and batch: 250, loss is 3.533357119560242 and perplexity is 34.23871836212372
At time: 135.4459354877472 and batch: 300, loss is 3.5008460807800295 and perplexity is 33.14348216235272
At time: 136.38029432296753 and batch: 350, loss is 3.4883636093139647 and perplexity is 32.73234095952531
At time: 137.30720376968384 and batch: 400, loss is 3.4137507677078247 and perplexity is 30.378975316292323
At time: 138.23176455497742 and batch: 450, loss is 3.4274526596069337 and perplexity is 30.798089523778458
At time: 139.1581165790558 and batch: 500, loss is 3.2994329357147216 and perplexity is 27.097268669818362
At time: 140.08442568778992 and batch: 550, loss is 3.3682047367095946 and perplexity is 29.026370277216223
At time: 141.0120599269867 and batch: 600, loss is 3.3999294090270995 and perplexity is 29.96198492707766
At time: 141.93920016288757 and batch: 650, loss is 3.2222011566162108 and perplexity is 25.083271669930458
At time: 142.86571216583252 and batch: 700, loss is 3.2143884897232056 and perplexity is 24.88806794751517
At time: 143.79289054870605 and batch: 750, loss is 3.3118583583831787 and perplexity is 27.436064165913134
At time: 144.71922087669373 and batch: 800, loss is 3.256578884124756 and perplexity is 25.96057092609694
At time: 145.6509988307953 and batch: 850, loss is 3.3194547128677367 and perplexity is 27.645271836773393
At time: 146.57589530944824 and batch: 900, loss is 3.2816979265213013 and perplexity is 26.62093473922733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.383672844873716 and perplexity of 80.13180527581534
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 148.74831080436707 and batch: 50, loss is 3.5730026531219483 and perplexity is 35.62339742878937
At time: 149.6790988445282 and batch: 100, loss is 3.461612648963928 and perplexity is 31.868327523326073
At time: 150.6111443042755 and batch: 150, loss is 3.471603012084961 and perplexity is 32.188299343388906
At time: 151.54744505882263 and batch: 200, loss is 3.349795017242432 and perplexity is 28.49689167359878
At time: 152.47364020347595 and batch: 250, loss is 3.497114725112915 and perplexity is 33.02004248425054
At time: 153.40642762184143 and batch: 300, loss is 3.460634846687317 and perplexity is 31.837181829768536
At time: 154.33414506912231 and batch: 350, loss is 3.4395039796829225 and perplexity is 31.171492639355513
At time: 155.2599902153015 and batch: 400, loss is 3.366598596572876 and perplexity is 28.979787278316973
At time: 156.18720173835754 and batch: 450, loss is 3.3727741336822508 and perplexity is 29.159306774198914
At time: 157.1100561618805 and batch: 500, loss is 3.240394868850708 and perplexity is 25.543806209601176
At time: 158.0425045490265 and batch: 550, loss is 3.2995955324172974 and perplexity is 27.10167495456686
At time: 158.97082090377808 and batch: 600, loss is 3.330875539779663 and perplexity is 27.962813544247368
At time: 159.8970205783844 and batch: 650, loss is 3.143582525253296 and perplexity is 23.186785485544803
At time: 160.82342529296875 and batch: 700, loss is 3.130554223060608 and perplexity is 22.88666034306258
At time: 161.74991011619568 and batch: 750, loss is 3.220455641746521 and perplexity is 25.03952663616053
At time: 162.68315076828003 and batch: 800, loss is 3.1534379720687866 and perplexity is 23.416431389429853
At time: 163.61288857460022 and batch: 850, loss is 3.2120702934265135 and perplexity is 24.830439343583702
At time: 164.53917241096497 and batch: 900, loss is 3.175408024787903 and perplexity is 23.936584595967098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380509886023116 and perplexity of 79.87875208249092
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 166.72077417373657 and batch: 50, loss is 3.5548037767410277 and perplexity is 34.98095521716907
At time: 167.66135835647583 and batch: 100, loss is 3.433783073425293 and perplexity is 30.99367258297769
At time: 168.59322929382324 and batch: 150, loss is 3.4462997245788576 and perplexity is 31.384047567873736
At time: 169.51859593391418 and batch: 200, loss is 3.3256389904022217 and perplexity is 27.816767612489684
At time: 170.4429225921631 and batch: 250, loss is 3.4740660762786866 and perplexity is 32.267678909310895
At time: 171.36814832687378 and batch: 300, loss is 3.438732933998108 and perplexity is 31.147467257990307
At time: 172.29321146011353 and batch: 350, loss is 3.4159009981155397 and perplexity is 30.444367391592035
At time: 173.2189483642578 and batch: 400, loss is 3.3424739933013914 and perplexity is 28.289027066708794
At time: 174.1421926021576 and batch: 450, loss is 3.349525055885315 and perplexity is 28.489199652369923
At time: 175.06527256965637 and batch: 500, loss is 3.2186340570449827 and perplexity is 24.993956535006674
At time: 175.989839553833 and batch: 550, loss is 3.274490065574646 and perplexity is 26.42974460704572
At time: 176.91657328605652 and batch: 600, loss is 3.3058443737030028 and perplexity is 27.271559257153218
At time: 177.8419542312622 and batch: 650, loss is 3.116240029335022 and perplexity is 22.56138979983577
At time: 178.76669669151306 and batch: 700, loss is 3.102414379119873 and perplexity is 22.25161030206311
At time: 179.6921169757843 and batch: 750, loss is 3.1912685203552247 and perplexity is 24.31925735775552
At time: 180.61629486083984 and batch: 800, loss is 3.1213935232162475 and perplexity is 22.6779598977706
At time: 181.54126119613647 and batch: 850, loss is 3.178187246322632 and perplexity is 24.00320219699984
At time: 182.46422600746155 and batch: 900, loss is 3.1418282699584963 and perplexity is 23.146145601125752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379501551797945 and perplexity of 79.79824819713458
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.6884503364563 and batch: 50, loss is 3.5460553550720215 and perplexity is 34.67626180706403
At time: 185.6273193359375 and batch: 100, loss is 3.4230414772033693 and perplexity is 30.66253273587775
At time: 186.54884505271912 and batch: 150, loss is 3.434702506065369 and perplexity is 31.022182281551615
At time: 187.48213267326355 and batch: 200, loss is 3.315085768699646 and perplexity is 27.52475464574562
At time: 188.4058849811554 and batch: 250, loss is 3.4637484884262086 and perplexity is 31.936465895256248
At time: 189.33049845695496 and batch: 300, loss is 3.428043875694275 and perplexity is 30.816303233352823
At time: 190.26469779014587 and batch: 350, loss is 3.405707235336304 and perplexity is 30.135601151069046
At time: 191.19430899620056 and batch: 400, loss is 3.3323640394210816 and perplexity is 28.004467175197647
At time: 192.11987042427063 and batch: 450, loss is 3.339959673881531 and perplexity is 28.2179887605066
At time: 193.04274320602417 and batch: 500, loss is 3.2095430421829225 and perplexity is 24.76776581410935
At time: 193.96679425239563 and batch: 550, loss is 3.265053949356079 and perplexity is 26.181523429052714
At time: 194.8920774459839 and batch: 600, loss is 3.2971811580657957 and perplexity is 27.036320292730597
At time: 195.81813049316406 and batch: 650, loss is 3.108586459159851 and perplexity is 22.389373727975215
At time: 196.75257635116577 and batch: 700, loss is 3.0933759784698487 and perplexity is 22.05139749780656
At time: 197.67447471618652 and batch: 750, loss is 3.1820695304870608 and perplexity is 24.096570572796576
At time: 198.5998649597168 and batch: 800, loss is 3.111116633415222 and perplexity is 22.446094471398517
At time: 199.52647709846497 and batch: 850, loss is 3.1673414087295533 and perplexity is 23.744274048764883
At time: 200.46130442619324 and batch: 900, loss is 3.131999201774597 and perplexity is 22.91975498486071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379362759524828 and perplexity of 79.78717358542958
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.63251304626465 and batch: 50, loss is 3.542516551017761 and perplexity is 34.553766183035805
At time: 203.56341075897217 and batch: 100, loss is 3.4201675033569336 and perplexity is 30.574535929558444
At time: 204.49461889266968 and batch: 150, loss is 3.4318119859695435 and perplexity is 30.932641512290452
At time: 205.42243099212646 and batch: 200, loss is 3.312037696838379 and perplexity is 27.440984948507015
At time: 206.3483772277832 and batch: 250, loss is 3.4608659839630125 and perplexity is 31.844541439749825
At time: 207.27652788162231 and batch: 300, loss is 3.425258016586304 and perplexity is 30.73057282618022
At time: 208.2030086517334 and batch: 350, loss is 3.4024532794952393 and perplexity is 30.037700604090645
At time: 209.12927794456482 and batch: 400, loss is 3.3292997074127197 and perplexity is 27.918783538658957
At time: 210.05510020256042 and batch: 450, loss is 3.3374385738372805 and perplexity is 28.146937988478978
At time: 210.97811198234558 and batch: 500, loss is 3.2063568019866944 and perplexity is 24.688975352427263
At time: 211.9051113128662 and batch: 550, loss is 3.2621388101577757 and perplexity is 26.10531178158461
At time: 212.83345818519592 and batch: 600, loss is 3.2946266603469847 and perplexity is 26.96734421134742
At time: 213.76090621948242 and batch: 650, loss is 3.1062153244018553 and perplexity is 22.33634839566691
At time: 214.6911699771881 and batch: 700, loss is 3.0908087587356565 and perplexity is 21.994859318999644
At time: 215.6177327632904 and batch: 750, loss is 3.179761362075806 and perplexity is 24.04101576936673
At time: 216.5442144870758 and batch: 800, loss is 3.1084305477142333 and perplexity is 22.385883240461364
At time: 217.471764087677 and batch: 850, loss is 3.1642969751358034 and perplexity is 23.672096109412152
At time: 218.40459442138672 and batch: 900, loss is 3.1289522361755373 and perplexity is 22.850025565401445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378867789490582 and perplexity of 79.74769109751786
Annealing...
Model not improving. Stopping early with 78.37047554312772 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f825458fb70>
ELAPSED
1046.7160685062408


RESULTS SO FAR:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.72744939555372, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.16871178456356, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.6405754777197848, 'batch_size': 32, 'dropout': 0.6765087688232242, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.37047554312772, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.22667666633898154, 'batch_size': 32, 'dropout': 0.16013127858268816, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.33548708067407973, 'batch_size': 32, 'dropout': 0.5084330490668031, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3519659042358398 and batch: 50, loss is 6.971732692718506 and perplexity is 1066.0683199894574
At time: 2.492828369140625 and batch: 100, loss is 6.117358655929565 and perplexity is 453.6648256666655
At time: 3.6490862369537354 and batch: 150, loss is 5.901285581588745 and perplexity is 365.5070550948921
At time: 4.792644023895264 and batch: 200, loss is 5.67030011177063 and perplexity is 290.1215902319827
At time: 5.937873363494873 and batch: 250, loss is 5.696606321334839 and perplexity is 297.85486013558534
At time: 7.082892656326294 and batch: 300, loss is 5.592862939834594 and perplexity is 268.5032289697193
At time: 8.226128339767456 and batch: 350, loss is 5.561923065185547 and perplexity is 260.32297331778193
At time: 9.368047714233398 and batch: 400, loss is 5.400338773727417 and perplexity is 221.4814355876592
At time: 10.509877443313599 and batch: 450, loss is 5.398400907516479 and perplexity is 221.05264979621828
At time: 11.65019416809082 and batch: 500, loss is 5.335802183151245 and perplexity is 207.63924672335835
At time: 12.790258884429932 and batch: 550, loss is 5.387924699783325 and perplexity is 218.74894443439715
At time: 13.936909914016724 and batch: 600, loss is 5.302359371185303 and perplexity is 200.81003691107716
At time: 15.081382989883423 and batch: 650, loss is 5.1940927314758305 and perplexity is 180.20457466961378
At time: 16.225038766860962 and batch: 700, loss is 5.292023677825927 and perplexity is 198.74521501053408
At time: 17.370729446411133 and batch: 750, loss is 5.260307960510254 and perplexity is 192.5407771238981
At time: 18.516444444656372 and batch: 800, loss is 5.228271636962891 and perplexity is 186.470236579653
At time: 19.662250757217407 and batch: 850, loss is 5.26635895729065 and perplexity is 193.70937276496798
At time: 20.81003761291504 and batch: 900, loss is 5.175007390975952 and perplexity is 176.79792098078576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.107694861007063 and perplexity of 165.28890149917007
finished 1 epochs...
Completing Train Step...
At time: 23.142027139663696 and batch: 50, loss is 5.049500465393066 and perplexity is 155.94454532920304
At time: 24.06220054626465 and batch: 100, loss is 4.907975425720215 and perplexity is 135.36508013748903
At time: 24.987868309020996 and batch: 150, loss is 4.879447908401489 and perplexity is 131.55801175935215
At time: 25.909254789352417 and batch: 200, loss is 4.753151245117188 and perplexity is 115.94909343868831
At time: 26.833131551742554 and batch: 250, loss is 4.852228736877441 and perplexity is 128.0254070598252
At time: 27.75840449333191 and batch: 300, loss is 4.7986283874511715 and perplexity is 121.34386655306137
At time: 28.682065725326538 and batch: 350, loss is 4.773945007324219 and perplexity is 118.38535303516296
At time: 29.60609269142151 and batch: 400, loss is 4.645924739837646 and perplexity is 104.15964183368443
At time: 30.53099226951599 and batch: 450, loss is 4.658030271530151 and perplexity is 105.42821254874285
At time: 31.452489137649536 and batch: 500, loss is 4.558524074554444 and perplexity is 95.44250979623051
At time: 32.37769079208374 and batch: 550, loss is 4.628523445129394 and perplexity is 102.36280816615705
At time: 33.30758857727051 and batch: 600, loss is 4.583423900604248 and perplexity is 97.8488460384821
At time: 34.2331919670105 and batch: 650, loss is 4.437034177780151 and perplexity is 84.52388674429494
At time: 35.156538009643555 and batch: 700, loss is 4.48597599029541 and perplexity is 88.76354090505188
At time: 36.08181953430176 and batch: 750, loss is 4.534538717269897 and perplexity is 93.1805228233813
At time: 37.00585126876831 and batch: 800, loss is 4.466523017883301 and perplexity is 87.05351268366623
At time: 37.92901825904846 and batch: 850, loss is 4.532882642745972 and perplexity is 93.02633664055115
At time: 38.853201389312744 and batch: 900, loss is 4.473265676498413 and perplexity is 87.64246813237669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.588270631555009 and perplexity of 98.32424420194619
finished 2 epochs...
Completing Train Step...
At time: 41.037381172180176 and batch: 50, loss is 4.507394065856934 and perplexity is 90.68519069148921
At time: 41.965641021728516 and batch: 100, loss is 4.364213061332703 and perplexity is 78.58753203412017
At time: 42.890745639801025 and batch: 150, loss is 4.3647749853134155 and perplexity is 78.63170466262247
At time: 43.814239263534546 and batch: 200, loss is 4.25459725856781 and perplexity is 70.42844702227468
At time: 44.736417293548584 and batch: 250, loss is 4.40182354927063 and perplexity is 81.59953384505792
At time: 45.67372798919678 and batch: 300, loss is 4.365526251792907 and perplexity is 78.69080022203778
At time: 46.59748435020447 and batch: 350, loss is 4.352700319290161 and perplexity is 77.6879622434942
At time: 47.5253427028656 and batch: 400, loss is 4.260853433609009 and perplexity is 70.87044086853494
At time: 48.44704484939575 and batch: 450, loss is 4.279966688156128 and perplexity is 72.23803358514796
At time: 49.3684606552124 and batch: 500, loss is 4.160500144958496 and perplexity is 64.10357566386914
At time: 50.296024799346924 and batch: 550, loss is 4.243975934982299 and perplexity is 69.68436227442655
At time: 51.22413492202759 and batch: 600, loss is 4.241999297142029 and perplexity is 69.54675756921638
At time: 52.15716862678528 and batch: 650, loss is 4.087024745941162 and perplexity is 59.56241449968537
At time: 53.08048343658447 and batch: 700, loss is 4.113664045333862 and perplexity is 61.17043871766945
At time: 54.00164270401001 and batch: 750, loss is 4.202308354377746 and perplexity is 66.8404445309612
At time: 54.9320125579834 and batch: 800, loss is 4.142521152496338 and perplexity is 62.961356701074784
At time: 55.85715651512146 and batch: 850, loss is 4.221893849372864 and perplexity is 68.16245154470586
At time: 56.7849440574646 and batch: 900, loss is 4.171840748786926 and perplexity is 64.83468670416737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4236822258936215 and perplexity of 83.4028286864813
finished 3 epochs...
Completing Train Step...
At time: 58.97186064720154 and batch: 50, loss is 4.2289756536483765 and perplexity is 68.64687796746463
At time: 59.89967751502991 and batch: 100, loss is 4.089975395202637 and perplexity is 59.73842183429832
At time: 60.829885959625244 and batch: 150, loss is 4.097717208862305 and perplexity is 60.20270042472667
At time: 61.7566339969635 and batch: 200, loss is 3.986812973022461 and perplexity is 53.882889202819406
At time: 62.68336367607117 and batch: 250, loss is 4.140301237106323 and perplexity is 62.82174283917325
At time: 63.60969519615173 and batch: 300, loss is 4.112213778495788 and perplexity is 61.08178955693361
At time: 64.5363438129425 and batch: 350, loss is 4.104101519584656 and perplexity is 60.58828269941305
At time: 65.46494674682617 and batch: 400, loss is 4.024277801513672 and perplexity is 55.93989448270719
At time: 66.39320540428162 and batch: 450, loss is 4.04405026435852 and perplexity is 57.05697125417066
At time: 67.32223868370056 and batch: 500, loss is 3.9200958156585695 and perplexity is 50.40527416123566
At time: 68.2488386631012 and batch: 550, loss is 4.008074374198913 and perplexity is 55.04078050431732
At time: 69.17611026763916 and batch: 600, loss is 4.026677036285401 and perplexity is 56.074268555692676
At time: 70.10335755348206 and batch: 650, loss is 3.8617008113861084 and perplexity is 47.54614967076574
At time: 71.03669118881226 and batch: 700, loss is 3.8797973823547363 and perplexity is 48.41440446484823
At time: 71.96474170684814 and batch: 750, loss is 3.9840077543258667 and perplexity is 53.7319477255113
At time: 72.8904767036438 and batch: 800, loss is 3.9257860708236696 and perplexity is 50.69291061918543
At time: 73.8151113986969 and batch: 850, loss is 4.010232639312744 and perplexity is 55.15970138595069
At time: 74.74192905426025 and batch: 900, loss is 3.965407361984253 and perplexity is 52.74174999809083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367815827670163 and perplexity of 78.87117519228228
finished 4 epochs...
Completing Train Step...
At time: 76.92387342453003 and batch: 50, loss is 4.028908610343933 and perplexity is 56.19954216543386
At time: 77.84732580184937 and batch: 100, loss is 3.897338342666626 and perplexity is 49.27113155327051
At time: 78.77421355247498 and batch: 150, loss is 3.9019114208221435 and perplexity is 49.496968279596686
At time: 79.7005774974823 and batch: 200, loss is 3.7972499990463255 and perplexity is 44.578425064950444
At time: 80.62724876403809 and batch: 250, loss is 3.9505421686172486 and perplexity is 51.96353219535193
At time: 81.55406379699707 and batch: 300, loss is 3.92283625125885 and perplexity is 50.54359601348684
At time: 82.48107695579529 and batch: 350, loss is 3.9181121349334718 and perplexity is 50.3053852970012
At time: 83.40772485733032 and batch: 400, loss is 3.843376898765564 and perplexity is 46.682851831197915
At time: 84.33441352844238 and batch: 450, loss is 3.861835451126099 and perplexity is 47.55255170296926
At time: 85.26209902763367 and batch: 500, loss is 3.7416684675216674 and perplexity is 42.1682879227697
At time: 86.18779110908508 and batch: 550, loss is 3.829377670288086 and perplexity is 46.033881066093635
At time: 87.10867977142334 and batch: 600, loss is 3.8559096097946166 and perplexity is 47.27159609799829
At time: 88.0459496974945 and batch: 650, loss is 3.689753131866455 and perplexity is 40.03496238080396
At time: 88.96627044677734 and batch: 700, loss is 3.707795081138611 and perplexity is 40.763826434275586
At time: 89.88737511634827 and batch: 750, loss is 3.8131525230407717 and perplexity is 45.29300126269523
At time: 90.80514693260193 and batch: 800, loss is 3.7607440423965453 and perplexity is 42.9803933196098
At time: 91.74067330360413 and batch: 850, loss is 3.8423829221725465 and perplexity is 46.63647322262654
At time: 92.65973615646362 and batch: 900, loss is 3.8020098781585694 and perplexity is 44.791118775876264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354211676610659 and perplexity of 77.80546528612061
finished 5 epochs...
Completing Train Step...
At time: 94.81771206855774 and batch: 50, loss is 3.869882574081421 and perplexity is 47.936756732617006
At time: 95.74286699295044 and batch: 100, loss is 3.7391102170944213 and perplexity is 42.06054875282336
At time: 96.6612823009491 and batch: 150, loss is 3.746545186042786 and perplexity is 42.37443304079239
At time: 97.57888770103455 and batch: 200, loss is 3.643583822250366 and perplexity is 38.228596011523585
At time: 98.51903033256531 and batch: 250, loss is 3.795451703071594 and perplexity is 44.49833189978877
At time: 99.44212603569031 and batch: 300, loss is 3.7642965269088746 and perplexity is 43.13335203207666
At time: 100.36473846435547 and batch: 350, loss is 3.7622017669677734 and perplexity is 43.04309258305852
At time: 101.28729748725891 and batch: 400, loss is 3.6917809581756593 and perplexity is 40.116228699949964
At time: 102.20908284187317 and batch: 450, loss is 3.7132436990737916 and perplexity is 40.98653913732558
At time: 103.12966418266296 and batch: 500, loss is 3.595286979675293 and perplexity is 36.42615205333058
At time: 104.04966711997986 and batch: 550, loss is 3.683623614311218 and perplexity is 39.7903179182921
At time: 104.97098660469055 and batch: 600, loss is 3.709147973060608 and perplexity is 40.8190128079504
At time: 105.89072322845459 and batch: 650, loss is 3.5481818962097167 and perplexity is 34.75008076600617
At time: 106.81216955184937 and batch: 700, loss is 3.56457115650177 and perplexity is 35.32430155857374
At time: 107.73301649093628 and batch: 750, loss is 3.6695783233642576 and perplexity is 39.23535773791834
At time: 108.65340948104858 and batch: 800, loss is 3.6218570041656495 and perplexity is 37.406968259541436
At time: 109.5758376121521 and batch: 850, loss is 3.703277015686035 and perplexity is 40.580068226739265
At time: 110.50183272361755 and batch: 900, loss is 3.660580191612244 and perplexity is 38.88389643856954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365766551396618 and perplexity of 78.70971186228562
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 112.67259001731873 and batch: 50, loss is 3.756343331336975 and perplexity is 42.791664602241056
At time: 113.60041666030884 and batch: 100, loss is 3.634348349571228 and perplexity is 37.87716218418105
At time: 114.52367973327637 and batch: 150, loss is 3.6440534591674805 and perplexity is 38.246553787987374
At time: 115.44663333892822 and batch: 200, loss is 3.5250531911849974 and perplexity is 33.95557970573492
At time: 116.36793899536133 and batch: 250, loss is 3.6728859329223633 and perplexity is 39.36534784193256
At time: 117.29338693618774 and batch: 300, loss is 3.633915190696716 and perplexity is 37.86075890810842
At time: 118.21627354621887 and batch: 350, loss is 3.613623046875 and perplexity is 37.10022546709086
At time: 119.13837552070618 and batch: 400, loss is 3.5374426651000976 and perplexity is 34.37888834567135
At time: 120.05948948860168 and batch: 450, loss is 3.540782856941223 and perplexity is 34.49391242232923
At time: 120.98603010177612 and batch: 500, loss is 3.415823793411255 and perplexity is 30.44201703394093
At time: 121.90707778930664 and batch: 550, loss is 3.4774892997741698 and perplexity is 32.37832766555629
At time: 122.8281180858612 and batch: 600, loss is 3.4934915924072265 and perplexity is 32.90062295541882
At time: 123.7500946521759 and batch: 650, loss is 3.315411939620972 and perplexity is 27.533733884631395
At time: 124.67270827293396 and batch: 700, loss is 3.315588173866272 and perplexity is 27.538586699046334
At time: 125.59470915794373 and batch: 750, loss is 3.4065929555892946 and perplexity is 30.162304687530394
At time: 126.51556468009949 and batch: 800, loss is 3.3347248029708862 and perplexity is 28.0706571992938
At time: 127.43636560440063 and batch: 850, loss is 3.3942118644714356 and perplexity is 29.791164744629928
At time: 128.35857343673706 and batch: 900, loss is 3.3398241233825683 and perplexity is 28.214164057276463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345489083904109 and perplexity of 77.12975117579742
finished 7 epochs...
Completing Train Step...
At time: 130.53087186813354 and batch: 50, loss is 3.6372857332229613 and perplexity is 37.98858550756375
At time: 131.45367550849915 and batch: 100, loss is 3.5087850284576416 and perplexity is 33.40765376787916
At time: 132.37692618370056 and batch: 150, loss is 3.515629920959473 and perplexity is 33.63710997230187
At time: 133.30053734779358 and batch: 200, loss is 3.4013998317718506 and perplexity is 30.00607411812305
At time: 134.22502446174622 and batch: 250, loss is 3.550297713279724 and perplexity is 34.823683417490265
At time: 135.14770030975342 and batch: 300, loss is 3.5151180124282835 and perplexity is 33.61989525529867
At time: 136.07196259498596 and batch: 350, loss is 3.4999809408187867 and perplexity is 33.11482081130707
At time: 136.9976670742035 and batch: 400, loss is 3.4289398097991945 and perplexity is 30.843924982192878
At time: 137.9222559928894 and batch: 450, loss is 3.4359751749038696 and perplexity is 31.061688380001563
At time: 138.8471336364746 and batch: 500, loss is 3.31765025138855 and perplexity is 27.59543198922835
At time: 139.774977684021 and batch: 550, loss is 3.3840169668197633 and perplexity is 29.488989807643815
At time: 140.69881629943848 and batch: 600, loss is 3.4063056087493897 and perplexity is 30.15363888969862
At time: 141.62122797966003 and batch: 650, loss is 3.233685154914856 and perplexity is 25.372988287652745
At time: 142.5446469783783 and batch: 700, loss is 3.2406100749969484 and perplexity is 25.54930398525219
At time: 143.46896600723267 and batch: 750, loss is 3.338107843399048 and perplexity is 28.165782182550984
At time: 144.39201402664185 and batch: 800, loss is 3.2726248502731323 and perplexity is 26.380493389330876
At time: 145.31609749794006 and batch: 850, loss is 3.3410579681396486 and perplexity is 28.248997440748607
At time: 146.24081826210022 and batch: 900, loss is 3.29396644115448 and perplexity is 26.949545729224464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3681268561376285 and perplexity of 78.89571018837287
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 148.4068832397461 and batch: 50, loss is 3.5807307291030885 and perplexity is 35.89976426733186
At time: 149.33400917053223 and batch: 100, loss is 3.465221939086914 and perplexity is 31.98355738700517
At time: 150.25397443771362 and batch: 150, loss is 3.479074950218201 and perplexity is 32.429709100928434
At time: 151.17867255210876 and batch: 200, loss is 3.3648254823684693 and perplexity is 28.92844833420914
At time: 152.10120463371277 and batch: 250, loss is 3.5086115026474 and perplexity is 33.4018571806341
At time: 153.02246117591858 and batch: 300, loss is 3.4712872743606566 and perplexity is 32.17813788726693
At time: 153.94424057006836 and batch: 350, loss is 3.452407855987549 and perplexity is 31.576332106336725
At time: 154.87116646766663 and batch: 400, loss is 3.3808774518966676 and perplexity is 29.39655386201604
At time: 155.7981562614441 and batch: 450, loss is 3.3801949644088745 and perplexity is 29.376497926561967
At time: 156.7188766002655 and batch: 500, loss is 3.2553621339797973 and perplexity is 25.929002606932556
At time: 157.64152884483337 and batch: 550, loss is 3.313653435707092 and perplexity is 27.485358252666764
At time: 158.56432843208313 and batch: 600, loss is 3.3373873901367186 and perplexity is 28.14549736090188
At time: 159.48806858062744 and batch: 650, loss is 3.1551856327056886 and perplexity is 23.457391146277907
At time: 160.41001272201538 and batch: 700, loss is 3.1505444478988647 and perplexity is 23.348773311578494
At time: 161.33237838745117 and batch: 750, loss is 3.2443969869613647 and perplexity is 25.64624037914673
At time: 162.25526642799377 and batch: 800, loss is 3.1729416370391847 and perplexity is 23.877620441098287
At time: 163.1846959590912 and batch: 850, loss is 3.2306645154953 and perplexity is 25.296461277478635
At time: 164.10779118537903 and batch: 900, loss is 3.186425976753235 and perplexity is 24.201774979983288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36910049229452 and perplexity of 78.97256331185464
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 166.26796340942383 and batch: 50, loss is 3.559498109817505 and perplexity is 35.14555350962842
At time: 167.19183254241943 and batch: 100, loss is 3.4376890754699705 and perplexity is 31.1149706725329
At time: 168.11243987083435 and batch: 150, loss is 3.4531444358825683 and perplexity is 31.599599165696816
At time: 169.0331847667694 and batch: 200, loss is 3.340360140800476 and perplexity is 28.229291394538812
At time: 169.95415830612183 and batch: 250, loss is 3.4833739042282104 and perplexity is 32.56942302560861
At time: 170.88156056404114 and batch: 300, loss is 3.4477716636657716 and perplexity is 31.430276989293894
At time: 171.80293822288513 and batch: 350, loss is 3.428461904525757 and perplexity is 30.82918802950456
At time: 172.72538423538208 and batch: 400, loss is 3.3575589084625244 and perplexity is 28.718999535619474
At time: 173.64781427383423 and batch: 450, loss is 3.3559515285491943 and perplexity is 28.672874272976358
At time: 174.5700707435608 and batch: 500, loss is 3.2307423877716066 and perplexity is 25.298431247202824
At time: 175.49311113357544 and batch: 550, loss is 3.287919268608093 and perplexity is 26.787068933885863
At time: 176.41680812835693 and batch: 600, loss is 3.31363564491272 and perplexity is 27.48486927065954
At time: 177.33980989456177 and batch: 650, loss is 3.1285198497772218 and perplexity is 22.840147660835488
At time: 178.26296710968018 and batch: 700, loss is 3.1212043237686156 and perplexity is 22.67366964615394
At time: 179.18632626533508 and batch: 750, loss is 3.2128036880493163 and perplexity is 24.848656533658204
At time: 180.10795903205872 and batch: 800, loss is 3.1405325746536255 and perplexity is 23.116174669735056
At time: 181.0320987701416 and batch: 850, loss is 3.196608247756958 and perplexity is 24.449462884291826
At time: 181.95587539672852 and batch: 900, loss is 3.151950159072876 and perplexity is 23.38161802280233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367994334599743 and perplexity of 78.88525550027711
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.1488070487976 and batch: 50, loss is 3.551663455963135 and perplexity is 34.87127610060879
At time: 185.07323026657104 and batch: 100, loss is 3.4272113752365114 and perplexity is 30.790659322569265
At time: 186.00203919410706 and batch: 150, loss is 3.4440660524368285 and perplexity is 31.314024128933628
At time: 186.9270215034485 and batch: 200, loss is 3.3293216848373413 and perplexity is 27.919397128362238
At time: 187.85413789749146 and batch: 250, loss is 3.4730957698822023 and perplexity is 32.23638455907488
At time: 188.77851033210754 and batch: 300, loss is 3.4368615198135375 and perplexity is 31.089231954133027
At time: 189.70369386672974 and batch: 350, loss is 3.4186413717269897 and perplexity is 30.52791075033534
At time: 190.62807059288025 and batch: 400, loss is 3.3469742488861085 and perplexity is 28.41662180788473
At time: 191.55064415931702 and batch: 450, loss is 3.3464028453826904 and perplexity is 28.400389088786348
At time: 192.47232389450073 and batch: 500, loss is 3.2206777715682984 and perplexity is 25.04508927954123
At time: 193.39605259895325 and batch: 550, loss is 3.2785355710983275 and perplexity is 26.536882852906032
At time: 194.32057404518127 and batch: 600, loss is 3.304852857589722 and perplexity is 27.24453246767826
At time: 195.2451422214508 and batch: 650, loss is 3.1197046661376953 and perplexity is 22.63969238794322
At time: 196.16872668266296 and batch: 700, loss is 3.1122438097000122 and perplexity is 22.471409441309426
At time: 197.09371066093445 and batch: 750, loss is 3.2033009147644043 and perplexity is 24.613643789120527
At time: 198.01853609085083 and batch: 800, loss is 3.130526089668274 and perplexity is 22.886016472725103
At time: 198.94344663619995 and batch: 850, loss is 3.1862613773345947 and perplexity is 24.197791709722498
At time: 199.86926913261414 and batch: 900, loss is 3.141195869445801 and perplexity is 23.13151259422889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367793671072346 and perplexity of 78.86942769473357
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.0584671497345 and batch: 50, loss is 3.5490450954437254 and perplexity is 34.780089959198065
At time: 202.99268078804016 and batch: 100, loss is 3.4242385816574097 and perplexity is 30.699260969768076
At time: 203.91940927505493 and batch: 150, loss is 3.441897683143616 and perplexity is 31.246197323921965
At time: 204.84627509117126 and batch: 200, loss is 3.326482710838318 and perplexity is 27.84024709145575
At time: 205.7726435661316 and batch: 250, loss is 3.4701461458206175 and perplexity is 32.14143943856436
At time: 206.69873309135437 and batch: 300, loss is 3.4338401889801027 and perplexity is 30.995442854337394
At time: 207.62481451034546 and batch: 350, loss is 3.4154886388778687 and perplexity is 30.43181596348958
At time: 208.56312441825867 and batch: 400, loss is 3.3438832473754885 and perplexity is 28.3289215975173
At time: 209.48771595954895 and batch: 450, loss is 3.3437369060516358 and perplexity is 28.324776208955623
At time: 210.42420744895935 and batch: 500, loss is 3.217615089416504 and perplexity is 24.968501473534197
At time: 211.351975440979 and batch: 550, loss is 3.2754837799072267 and perplexity is 26.456021276657516
At time: 212.27931714057922 and batch: 600, loss is 3.3023556995391847 and perplexity is 27.176583439135
At time: 213.20654845237732 and batch: 650, loss is 3.117269825935364 and perplexity is 22.5846354094212
At time: 214.1335690021515 and batch: 700, loss is 3.109738574028015 and perplexity is 22.415183723520958
At time: 215.06037664413452 and batch: 750, loss is 3.2006337785720826 and perplexity is 24.548083317163748
At time: 215.98706316947937 and batch: 800, loss is 3.1279202270507813 and perplexity is 22.82645629446024
At time: 216.9123661518097 and batch: 850, loss is 3.183511800765991 and perplexity is 24.131349414574036
At time: 217.83776926994324 and batch: 900, loss is 3.1381470441818236 and perplexity is 23.061096052648523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367365170831549 and perplexity of 78.83563936564545
Annealing...
Model not improving. Stopping early with 77.12975117579742 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f825458fb70>
ELAPSED
1271.301952123642


RESULTS SO FAR:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.72744939555372, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.16871178456356, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.6405754777197848, 'batch_size': 32, 'dropout': 0.6765087688232242, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.37047554312772, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.22667666633898154, 'batch_size': 32, 'dropout': 0.16013127858268816, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.12975117579742, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.33548708067407973, 'batch_size': 32, 'dropout': 0.5084330490668031, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5385178728716176, 'batch_size': 32, 'dropout': 0.15824842137858375, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3599269390106201 and batch: 50, loss is 6.931380529403686 and perplexity is 1023.9065374410529
At time: 2.5047433376312256 and batch: 100, loss is 6.02886079788208 and perplexity is 415.2417155912875
At time: 3.651049852371216 and batch: 150, loss is 5.761671190261841 and perplexity is 317.8791217591266
At time: 4.796719551086426 and batch: 200, loss is 5.5341349220275875 and perplexity is 253.18866493474317
At time: 5.945207357406616 and batch: 250, loss is 5.546010932922363 and perplexity is 256.2134619880493
At time: 7.09102201461792 and batch: 300, loss is 5.4380797290802 and perplexity is 230.00009663613133
At time: 8.238276481628418 and batch: 350, loss is 5.387496833801269 and perplexity is 218.6553692227155
At time: 9.383477210998535 and batch: 400, loss is 5.223592958450317 and perplexity is 185.59984003109804
At time: 10.531396389007568 and batch: 450, loss is 5.223257513046264 and perplexity is 185.537591858782
At time: 11.678100824356079 and batch: 500, loss is 5.148664722442627 and perplexity is 172.20140006900874
At time: 12.824801206588745 and batch: 550, loss is 5.198491907119751 and perplexity is 180.99907253193663
At time: 13.97632646560669 and batch: 600, loss is 5.108456087112427 and perplexity is 165.41477162765383
At time: 15.1210355758667 and batch: 650, loss is 4.991518459320068 and perplexity is 147.1597099602129
At time: 16.266839504241943 and batch: 700, loss is 5.073446102142334 and perplexity is 159.72380456729707
At time: 17.409538984298706 and batch: 750, loss is 5.062844686508178 and perplexity is 158.0394501742632
At time: 18.551374912261963 and batch: 800, loss is 5.011571607589722 and perplexity is 150.1405127957678
At time: 19.697659969329834 and batch: 850, loss is 5.05749773979187 and perplexity is 157.19667679517804
At time: 20.84984540939331 and batch: 900, loss is 4.963752698898316 and perplexity is 143.12991279123105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.953544721211473 and perplexity of 141.67627779465883
finished 1 epochs...
Completing Train Step...
At time: 23.176418781280518 and batch: 50, loss is 4.920834217071533 and perplexity is 137.11695078979332
At time: 24.0988826751709 and batch: 100, loss is 4.790483341217041 and perplexity is 120.3595293277478
At time: 25.023836612701416 and batch: 150, loss is 4.77710560798645 and perplexity is 118.76011378292874
At time: 25.946327686309814 and batch: 200, loss is 4.660884056091309 and perplexity is 105.72951167093265
At time: 26.869746208190918 and batch: 250, loss is 4.765398044586181 and perplexity is 117.37782960001043
At time: 27.79325580596924 and batch: 300, loss is 4.718661546707153 and perplexity is 112.01822112817517
At time: 28.71611189842224 and batch: 350, loss is 4.697320861816406 and perplexity is 109.65300302046421
At time: 29.63727378845215 and batch: 400, loss is 4.5790336799621585 and perplexity is 97.42020960751557
At time: 30.560407876968384 and batch: 450, loss is 4.5929960346221925 and perplexity is 98.78996538061243
At time: 31.48438549041748 and batch: 500, loss is 4.492885408401489 and perplexity is 89.37896899836724
At time: 32.40862202644348 and batch: 550, loss is 4.56421160697937 and perplexity is 95.98688878452197
At time: 33.33199954032898 and batch: 600, loss is 4.524391937255859 and perplexity is 92.23982117402927
At time: 34.25486350059509 and batch: 650, loss is 4.384063663482666 and perplexity is 80.16312839691656
At time: 35.17611074447632 and batch: 700, loss is 4.429345045089722 and perplexity is 83.8764636145667
At time: 36.09763431549072 and batch: 750, loss is 4.485017871856689 and perplexity is 88.6785356488791
At time: 37.02045822143555 and batch: 800, loss is 4.421845188140869 and perplexity is 83.24975518544629
At time: 37.95946478843689 and batch: 850, loss is 4.494393253326416 and perplexity is 89.51384028010196
At time: 38.883498191833496 and batch: 900, loss is 4.421777715682984 and perplexity is 83.24413830933989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.572052001953125 and perplexity of 96.7424218734839
finished 2 epochs...
Completing Train Step...
At time: 41.05731153488159 and batch: 50, loss is 4.474308977127075 and perplexity is 87.73395328943931
At time: 41.98749351501465 and batch: 100, loss is 4.333126783370972 and perplexity is 76.18211953386177
At time: 42.910757064819336 and batch: 150, loss is 4.335359110832214 and perplexity is 76.35237293131834
At time: 43.835564374923706 and batch: 200, loss is 4.2282036018371585 and perplexity is 68.5938994746976
At time: 44.758474826812744 and batch: 250, loss is 4.372662029266357 and perplexity is 79.25432847653772
At time: 45.68197011947632 and batch: 300, loss is 4.335697093009949 and perplexity is 76.37818303402855
At time: 46.60990118980408 and batch: 350, loss is 4.32591851234436 and perplexity is 75.63495260184436
At time: 47.533929109573364 and batch: 400, loss is 4.234082288742066 and perplexity is 68.99832912622254
At time: 48.458374977111816 and batch: 450, loss is 4.258323864936829 and perplexity is 70.69139577043738
At time: 49.38290357589722 and batch: 500, loss is 4.138677263259888 and perplexity is 62.71980476660024
At time: 50.307475090026855 and batch: 550, loss is 4.219835739135743 and perplexity is 68.0223099682688
At time: 51.23147106170654 and batch: 600, loss is 4.216204586029053 and perplexity is 67.77575845016281
At time: 52.15671706199646 and batch: 650, loss is 4.066872591972351 and perplexity is 58.37411714220074
At time: 53.07929801940918 and batch: 700, loss is 4.0897333478927616 and perplexity is 59.72396405980044
At time: 54.00259184837341 and batch: 750, loss is 4.180893692970276 and perplexity is 65.42429632873396
At time: 54.92754817008972 and batch: 800, loss is 4.12468578338623 and perplexity is 61.84837241069276
At time: 55.851194620132446 and batch: 850, loss is 4.20633065700531 and perplexity is 67.10983845536217
At time: 56.77555203437805 and batch: 900, loss is 4.145710678100586 and perplexity is 63.16249415636739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.420557309503424 and perplexity of 83.1426086148698
finished 3 epochs...
Completing Train Step...
At time: 58.94708275794983 and batch: 50, loss is 4.222142724990845 and perplexity is 68.17941762809181
At time: 59.87656784057617 and batch: 100, loss is 4.078390126228332 and perplexity is 59.05032971161244
At time: 60.80230736732483 and batch: 150, loss is 4.087466688156128 and perplexity is 59.58874346258964
At time: 61.73383593559265 and batch: 200, loss is 3.9795753240585325 and perplexity is 53.49431165580232
At time: 62.659717321395874 and batch: 250, loss is 4.130881137847901 and perplexity is 62.232734400644695
At time: 63.585795879364014 and batch: 300, loss is 4.100952382087708 and perplexity is 60.397781980401334
At time: 64.51156258583069 and batch: 350, loss is 4.089723386764526 and perplexity is 59.723369144698744
At time: 65.4394428730011 and batch: 400, loss is 4.012320494651794 and perplexity is 55.27498717113189
At time: 66.3664186000824 and batch: 450, loss is 4.042447876930237 and perplexity is 56.965617092664104
At time: 67.29231643676758 and batch: 500, loss is 3.9150835514068603 and perplexity is 50.15326171156044
At time: 68.21841526031494 and batch: 550, loss is 3.9958827352523802 and perplexity is 54.373817130608266
At time: 69.14508771896362 and batch: 600, loss is 4.006963944435119 and perplexity is 54.979695505000315
At time: 70.07031917572021 and batch: 650, loss is 3.8544570779800416 and perplexity is 47.20298244456117
At time: 70.99416899681091 and batch: 700, loss is 3.871790175437927 and perplexity is 48.02828822983428
At time: 71.91947150230408 and batch: 750, loss is 3.970874786376953 and perplexity is 53.03090126459488
At time: 72.84593868255615 and batch: 800, loss is 3.920528783798218 and perplexity is 50.42710276422153
At time: 73.77689170837402 and batch: 850, loss is 4.0015666913986205 and perplexity is 54.68375552636667
At time: 74.70717716217041 and batch: 900, loss is 3.951939249038696 and perplexity is 52.03618016452262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369200824058219 and perplexity of 78.98048716591602
finished 4 epochs...
Completing Train Step...
At time: 76.89536237716675 and batch: 50, loss is 4.033096551895142 and perplexity is 56.43539608973497
At time: 77.82032251358032 and batch: 100, loss is 3.8919041299819948 and perplexity is 49.00410793388252
At time: 78.74640607833862 and batch: 150, loss is 3.903725504875183 and perplexity is 49.5868414345081
At time: 79.67135381698608 and batch: 200, loss is 3.793257784843445 and perplexity is 44.40081321144093
At time: 80.59661316871643 and batch: 250, loss is 3.946076831817627 and perplexity is 51.73201480904391
At time: 81.52221059799194 and batch: 300, loss is 3.9203504085540772 and perplexity is 50.418108619644876
At time: 82.44647789001465 and batch: 350, loss is 3.910847520828247 and perplexity is 49.9412603005748
At time: 83.37059807777405 and batch: 400, loss is 3.8415605306625364 and perplexity is 46.59813554944065
At time: 84.31226062774658 and batch: 450, loss is 3.8699361753463744 and perplexity is 47.939326272280304
At time: 85.23776459693909 and batch: 500, loss is 3.742359843254089 and perplexity is 42.19745213426962
At time: 86.16278719902039 and batch: 550, loss is 3.821921639442444 and perplexity is 45.69192742154805
At time: 87.0884644985199 and batch: 600, loss is 3.841532907485962 and perplexity is 46.59684837869228
At time: 88.01132535934448 and batch: 650, loss is 3.6876453876495363 and perplexity is 39.95066778729941
At time: 88.92832446098328 and batch: 700, loss is 3.7044208908081053 and perplexity is 40.62651331586248
At time: 89.84548783302307 and batch: 750, loss is 3.807058753967285 and perplexity is 45.01783542239972
At time: 90.76428151130676 and batch: 800, loss is 3.7608133935928345 and perplexity is 42.98337416466489
At time: 91.68222951889038 and batch: 850, loss is 3.8420181560516355 and perplexity is 46.619464919411136
At time: 92.60341787338257 and batch: 900, loss is 3.7938177013397216 and perplexity is 44.425680920476694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363037945473031 and perplexity of 78.4952368182612
finished 5 epochs...
Completing Train Step...
At time: 94.76675343513489 and batch: 50, loss is 3.8788154983520506 and perplexity is 48.36689046603965
At time: 95.70230650901794 and batch: 100, loss is 3.7401067495346068 and perplexity is 42.102484345726474
At time: 96.62351775169373 and batch: 150, loss is 3.7529567956924437 and perplexity is 42.64699420876301
At time: 97.54619598388672 and batch: 200, loss is 3.6429001665115357 and perplexity is 38.20246974417677
At time: 98.46911859512329 and batch: 250, loss is 3.7952152633666993 and perplexity is 44.4878119710395
At time: 99.39325451850891 and batch: 300, loss is 3.771425499916077 and perplexity is 43.441947210815684
At time: 100.31656336784363 and batch: 350, loss is 3.7616768407821657 and perplexity is 43.02050406582291
At time: 101.24016332626343 and batch: 400, loss is 3.6979045629501344 and perplexity is 40.36263831710546
At time: 102.16377305984497 and batch: 450, loss is 3.726574058532715 and perplexity is 41.536562295374495
At time: 103.08741569519043 and batch: 500, loss is 3.602574415206909 and perplexity is 36.69257487859663
At time: 104.01002860069275 and batch: 550, loss is 3.6763035821914674 and perplexity is 39.50011495640073
At time: 104.9330358505249 and batch: 600, loss is 3.701271629333496 and perplexity is 40.49877105509673
At time: 105.85849928855896 and batch: 650, loss is 3.5514664459228515 and perplexity is 34.864406785783686
At time: 106.7873854637146 and batch: 700, loss is 3.56350124835968 and perplexity is 35.28652801143868
At time: 107.72639203071594 and batch: 750, loss is 3.6677397966384886 and perplexity is 39.163288754798366
At time: 108.64923000335693 and batch: 800, loss is 3.623516526222229 and perplexity is 37.469097486590506
At time: 109.57164120674133 and batch: 850, loss is 3.7050973320007325 and perplexity is 40.65400405986973
At time: 110.49477052688599 and batch: 900, loss is 3.6548246717453003 and perplexity is 38.660742200455715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378120735900043 and perplexity of 79.68813754616315
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 112.66566371917725 and batch: 50, loss is 3.7740237808227537 and perplexity is 43.55496835956443
At time: 113.59570384025574 and batch: 100, loss is 3.6382400703430178 and perplexity is 38.02485672958598
At time: 114.5199830532074 and batch: 150, loss is 3.65734974861145 and perplexity is 38.75848690071986
At time: 115.44246768951416 and batch: 200, loss is 3.5262326526641847 and perplexity is 33.99565263159196
At time: 116.36783456802368 and batch: 250, loss is 3.666111054420471 and perplexity is 39.09955377102612
At time: 117.29242539405823 and batch: 300, loss is 3.6367053270339964 and perplexity is 37.96654309482031
At time: 118.21690583229065 and batch: 350, loss is 3.6113475704193116 and perplexity is 37.01590075337477
At time: 119.13968253135681 and batch: 400, loss is 3.5431001806259155 and perplexity is 34.573938670111225
At time: 120.064377784729 and batch: 450, loss is 3.5547336292266847 and perplexity is 34.97850147617416
At time: 120.98924946784973 and batch: 500, loss is 3.420822014808655 and perplexity is 30.594553863724364
At time: 121.91267776489258 and batch: 550, loss is 3.4736133575439454 and perplexity is 32.2530740327421
At time: 122.83592939376831 and batch: 600, loss is 3.4890699863433836 and perplexity is 32.75547050142503
At time: 123.7604570388794 and batch: 650, loss is 3.319908170700073 and perplexity is 27.657810644510274
At time: 124.68363642692566 and batch: 700, loss is 3.319047589302063 and perplexity is 27.63401908591464
At time: 125.60975050926208 and batch: 750, loss is 3.4066220235824587 and perplexity is 30.16318145793978
At time: 126.53380179405212 and batch: 800, loss is 3.341563763618469 and perplexity is 28.263289270001973
At time: 127.45778846740723 and batch: 850, loss is 3.401081657409668 and perplexity is 29.996528473299172
At time: 128.38253140449524 and batch: 900, loss is 3.3350426959991455 and perplexity is 28.07958208402027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356711609722817 and perplexity of 78.00021707689363
finished 7 epochs...
Completing Train Step...
At time: 130.57022142410278 and batch: 50, loss is 3.6573869943618775 and perplexity is 38.75993051653401
At time: 131.50407338142395 and batch: 100, loss is 3.5140639114379884 and perplexity is 33.58447516187391
At time: 132.42699909210205 and batch: 150, loss is 3.531620602607727 and perplexity is 34.17931384066262
At time: 133.350745677948 and batch: 200, loss is 3.404850540161133 and perplexity is 30.109795182464755
At time: 134.2733075618744 and batch: 250, loss is 3.546079158782959 and perplexity is 34.67708724060063
At time: 135.19470715522766 and batch: 300, loss is 3.5207925415039063 and perplexity is 33.811214638768504
At time: 136.12362003326416 and batch: 350, loss is 3.499447774887085 and perplexity is 33.097169822887054
At time: 137.04964137077332 and batch: 400, loss is 3.4365586328506468 and perplexity is 31.079816857014897
At time: 137.97170782089233 and batch: 450, loss is 3.4531465911865236 and perplexity is 31.59966727251128
At time: 138.89441561698914 and batch: 500, loss is 3.3247508573532105 and perplexity is 27.792073589268618
At time: 139.81884455680847 and batch: 550, loss is 3.380173420906067 and perplexity is 29.37586506071351
At time: 140.74532270431519 and batch: 600, loss is 3.402614879608154 and perplexity is 30.04255509213231
At time: 141.67184948921204 and batch: 650, loss is 3.2405960941314698 and perplexity is 25.548946786367082
At time: 142.59790420532227 and batch: 700, loss is 3.245235090255737 and perplexity is 25.667743587392913
At time: 143.52287316322327 and batch: 750, loss is 3.3393810844421385 and perplexity is 28.201666852506015
At time: 144.44899535179138 and batch: 800, loss is 3.2816453790664672 and perplexity is 26.619535913614165
At time: 145.37340211868286 and batch: 850, loss is 3.3472067403793333 and perplexity is 28.423229198772574
At time: 146.30065441131592 and batch: 900, loss is 3.2896487283706666 and perplexity is 26.83343617534635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3780275109696065 and perplexity of 79.68070897135335
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 148.46881103515625 and batch: 50, loss is 3.603477325439453 and perplexity is 36.725719941169196
At time: 149.39797115325928 and batch: 100, loss is 3.4730838251113894 and perplexity is 32.23599950514918
At time: 150.3220796585083 and batch: 150, loss is 3.498064785003662 and perplexity is 33.05142840909897
At time: 151.24653840065002 and batch: 200, loss is 3.365767216682434 and perplexity is 28.955704078475694
At time: 152.17254090309143 and batch: 250, loss is 3.5097564935684202 and perplexity is 33.440123907197744
At time: 153.11868405342102 and batch: 300, loss is 3.481629433631897 and perplexity is 32.51265615326733
At time: 154.04542589187622 and batch: 350, loss is 3.453657655715942 and perplexity is 31.615820869009195
At time: 154.97089648246765 and batch: 400, loss is 3.3897031021118162 and perplexity is 29.65714581916189
At time: 155.89630889892578 and batch: 450, loss is 3.3977149868011476 and perplexity is 29.89570984930669
At time: 156.8227207660675 and batch: 500, loss is 3.265682382583618 and perplexity is 26.197981939318826
At time: 157.74802088737488 and batch: 550, loss is 3.31278263092041 and perplexity is 27.461434289199417
At time: 158.6747281551361 and batch: 600, loss is 3.3326532077789306 and perplexity is 28.012566351939515
At time: 159.60036444664001 and batch: 650, loss is 3.162012815475464 and perplexity is 23.618086968635883
At time: 160.52600860595703 and batch: 700, loss is 3.1586109590530396 and perplexity is 23.537878134617053
At time: 161.4518814086914 and batch: 750, loss is 3.247755823135376 and perplexity is 25.732526728793903
At time: 162.3778624534607 and batch: 800, loss is 3.182775640487671 and perplexity is 24.113591410843213
At time: 163.30271577835083 and batch: 850, loss is 3.2417803812026977 and perplexity is 25.5792219974617
At time: 164.22913432121277 and batch: 900, loss is 3.188093075752258 and perplexity is 24.242155384540844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371156457352312 and perplexity of 79.13509516520192
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 166.4141869544983 and batch: 50, loss is 3.58023540019989 and perplexity is 35.881986479761736
At time: 167.34208846092224 and batch: 100, loss is 3.445570287704468 and perplexity is 31.361163233663422
At time: 168.26477599143982 and batch: 150, loss is 3.470776801109314 and perplexity is 32.16171600042105
At time: 169.1904640197754 and batch: 200, loss is 3.3396169471740724 and perplexity is 28.208319359203298
At time: 170.11271023750305 and batch: 250, loss is 3.4850871467590334 and perplexity is 32.6252701725339
At time: 171.0371289253235 and batch: 300, loss is 3.4578167247772216 and perplexity is 31.74758707386607
At time: 171.95963621139526 and batch: 350, loss is 3.431590585708618 and perplexity is 30.925793775461848
At time: 172.88123393058777 and batch: 400, loss is 3.368343167304993 and perplexity is 29.03038869306538
At time: 173.8036744594574 and batch: 450, loss is 3.375084409713745 and perplexity is 29.22675069877775
At time: 174.7261564731598 and batch: 500, loss is 3.2432253122329713 and perplexity is 25.616208924400127
At time: 175.64879155158997 and batch: 550, loss is 3.28738618850708 and perplexity is 26.77279308589605
At time: 176.57600665092468 and batch: 600, loss is 3.3089510345458986 and perplexity is 27.356414482377666
At time: 177.49835681915283 and batch: 650, loss is 3.1363204288482667 and perplexity is 23.01901074952168
At time: 178.43316221237183 and batch: 700, loss is 3.1284671068191527 and perplexity is 22.838943035653152
At time: 179.3569815158844 and batch: 750, loss is 3.217157034873962 and perplexity is 24.957067156984024
At time: 180.2815523147583 and batch: 800, loss is 3.1497984504699708 and perplexity is 23.331361682043145
At time: 181.2048625946045 and batch: 850, loss is 3.207072238922119 and perplexity is 24.706645077325412
At time: 182.12813639640808 and batch: 900, loss is 3.156537108421326 and perplexity is 23.489114672723556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366811673935145 and perplexity of 78.79201615769934
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.31016850471497 and batch: 50, loss is 3.5698859977722166 and perplexity is 35.512544411672984
At time: 185.23314809799194 and batch: 100, loss is 3.4349946546554566 and perplexity is 31.03124669237957
At time: 186.16385865211487 and batch: 150, loss is 3.459560956954956 and perplexity is 31.8030105584669
At time: 187.0878348350525 and batch: 200, loss is 3.329088225364685 and perplexity is 27.912879841422562
At time: 188.01256012916565 and batch: 250, loss is 3.4737579679489134 and perplexity is 32.25773850009657
At time: 188.9368507862091 and batch: 300, loss is 3.4453818798065186 and perplexity is 31.35525509940838
At time: 189.86661791801453 and batch: 350, loss is 3.420998091697693 and perplexity is 30.599941331880594
At time: 190.79420399665833 and batch: 400, loss is 3.358013892173767 and perplexity is 28.732069185624837
At time: 191.71899008750916 and batch: 450, loss is 3.365736846923828 and perplexity is 28.954824714085667
At time: 192.6454963684082 and batch: 500, loss is 3.2338328742980957 and perplexity is 25.376736646679372
At time: 193.5702166557312 and batch: 550, loss is 3.278067331314087 and perplexity is 26.524460137236396
At time: 194.4956090450287 and batch: 600, loss is 3.3006854391098024 and perplexity is 27.131229354321352
At time: 195.41883373260498 and batch: 650, loss is 3.12866822719574 and perplexity is 22.843536874419136
At time: 196.34349298477173 and batch: 700, loss is 3.119428963661194 and perplexity is 22.633451429048083
At time: 197.26785349845886 and batch: 750, loss is 3.2075658369064333 and perplexity is 24.71884323778079
At time: 198.19161415100098 and batch: 800, loss is 3.139965920448303 and perplexity is 23.103079502713616
At time: 199.11594557762146 and batch: 850, loss is 3.196064519882202 and perplexity is 24.436172643263507
At time: 200.044780254364 and batch: 900, loss is 3.1457282590866087 and perplexity is 23.236591571839483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366075907668022 and perplexity of 78.73406497197001
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.22090601921082 and batch: 50, loss is 3.5670319175720215 and perplexity is 35.41133326288057
At time: 203.15215921401978 and batch: 100, loss is 3.431709494590759 and perplexity is 30.929471345672564
At time: 204.0777027606964 and batch: 150, loss is 3.4565945291519165 and perplexity is 31.708809013845155
At time: 205.00392317771912 and batch: 200, loss is 3.3260374879837036 and perplexity is 27.82785473605715
At time: 205.92949104309082 and batch: 250, loss is 3.4705107259750365 and perplexity is 32.15315970587674
At time: 206.85541582107544 and batch: 300, loss is 3.4421399354934694 and perplexity is 31.25376770558202
At time: 207.78126311302185 and batch: 350, loss is 3.4176530027389527 and perplexity is 30.49775281611865
At time: 208.70737767219543 and batch: 400, loss is 3.3545469522476195 and perplexity is 28.632629303456113
At time: 209.63942408561707 and batch: 450, loss is 3.3631274461746217 and perplexity is 28.87936846339709
At time: 210.56727242469788 and batch: 500, loss is 3.2310341930389406 and perplexity is 25.305814539885503
At time: 211.4932096004486 and batch: 550, loss is 3.275133113861084 and perplexity is 26.446745674694142
At time: 212.41897439956665 and batch: 600, loss is 3.298390064239502 and perplexity is 27.069024431381376
At time: 213.34375929832458 and batch: 650, loss is 3.126520342826843 and perplexity is 22.79452425419961
At time: 214.27002835273743 and batch: 700, loss is 3.1169310092926024 and perplexity is 22.5769846552484
At time: 215.19382047653198 and batch: 750, loss is 3.204855508804321 and perplexity is 24.651937771141547
At time: 216.120055437088 and batch: 800, loss is 3.137468543052673 and perplexity is 23.04545437998371
At time: 217.04519963264465 and batch: 850, loss is 3.193215179443359 and perplexity is 24.366644769711275
At time: 217.97227573394775 and batch: 900, loss is 3.142599630355835 and perplexity is 23.164006508906354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365647407427226 and perplexity of 78.70033463341589
Annealing...
Model not improving. Stopping early with 78.00021707689363 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f825458fb70>
ELAPSED
1496.4209084510803


RESULTS SO FAR:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.72744939555372, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.16871178456356, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.6405754777197848, 'batch_size': 32, 'dropout': 0.6765087688232242, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.37047554312772, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.22667666633898154, 'batch_size': 32, 'dropout': 0.16013127858268816, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.12975117579742, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.33548708067407973, 'batch_size': 32, 'dropout': 0.5084330490668031, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.00021707689363, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5385178728716176, 'batch_size': 32, 'dropout': 0.15824842137858375, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -77.0843649592813, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5310680659871366, 'batch_size': 32, 'dropout': 0.1345551805853541, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.72744939555372, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.9724468455256646, 'batch_size': 32, 'dropout': 0.7992944235583754, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.16871178456356, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.6405754777197848, 'batch_size': 32, 'dropout': 0.6765087688232242, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.37047554312772, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.22667666633898154, 'batch_size': 32, 'dropout': 0.16013127858268816, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -77.12975117579742, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.33548708067407973, 'batch_size': 32, 'dropout': 0.5084330490668031, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -78.00021707689363, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'TRUE', 'wordvec_dim': 300, 'data': 'ptb', 'rnn_dropout': 0.5385178728716176, 'batch_size': 32, 'dropout': 0.15824842137858375, 'num_layers': 3, 'seq_len': 35, 'wordvec_source': 'None'}}]
