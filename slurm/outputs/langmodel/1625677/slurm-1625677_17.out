FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.181049108505249 and batch: 50, loss is 6.834578666687012 and perplexity is 929.4366655868075
At time: 1.803483009338379 and batch: 100, loss is 5.9795040512084965 and perplexity is 395.24429860687223
At time: 2.4396276473999023 and batch: 150, loss is 5.795740852355957 and perplexity is 328.89575694140456
At time: 3.0657668113708496 and batch: 200, loss is 5.583918666839599 and perplexity is 266.11237097212376
At time: 3.693258762359619 and batch: 250, loss is 5.6031724643707275 and perplexity is 271.2856878662185
At time: 4.321444749832153 and batch: 300, loss is 5.488437700271606 and perplexity is 241.87902398390096
At time: 4.9470741748809814 and batch: 350, loss is 5.4483743667602536 and perplexity is 232.38009388282583
At time: 5.575289011001587 and batch: 400, loss is 5.2820455265045165 and perplexity is 196.77196624010202
At time: 6.204848289489746 and batch: 450, loss is 5.2720426654815675 and perplexity is 194.81349509583234
At time: 6.831843852996826 and batch: 500, loss is 5.201188945770264 and perplexity is 181.48789291368644
At time: 7.458071947097778 and batch: 550, loss is 5.24902684211731 and perplexity is 190.3809075996238
At time: 8.085732460021973 and batch: 600, loss is 5.161060733795166 and perplexity is 174.3492957448481
At time: 8.713473558425903 and batch: 650, loss is 5.042567377090454 and perplexity is 154.86710732835348
At time: 9.340082168579102 and batch: 700, loss is 5.1279078960418705 and perplexity is 168.66388629084753
At time: 9.96656584739685 and batch: 750, loss is 5.117556562423706 and perplexity is 166.92699521299528
At time: 10.592762470245361 and batch: 800, loss is 5.081989221572876 and perplexity is 161.0941894517103
At time: 11.216298818588257 and batch: 850, loss is 5.113209781646728 and perplexity is 166.20297487788213
At time: 11.840938091278076 and batch: 900, loss is 5.026009016036987 and perplexity is 152.32387583078574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.928169982074058 and perplexity of 138.126506924627
finished 1 epochs...
Completing Train Step...
At time: 13.331922769546509 and batch: 50, loss is 4.8772959136962895 and perplexity is 131.27520402413595
At time: 13.895345687866211 and batch: 100, loss is 4.739269704818725 and perplexity is 114.35066144232809
At time: 14.447397470474243 and batch: 150, loss is 4.71090238571167 and perplexity is 111.15241701653702
At time: 14.99940013885498 and batch: 200, loss is 4.593045244216919 and perplexity is 98.7948269143879
At time: 15.553541898727417 and batch: 250, loss is 4.70053542137146 and perplexity is 110.00605628040928
At time: 16.104401111602783 and batch: 300, loss is 4.646714239120484 and perplexity is 104.2419082665845
At time: 16.654574871063232 and batch: 350, loss is 4.6254988956451415 and perplexity is 102.05367451817948
At time: 17.20465922355652 and batch: 400, loss is 4.503218660354614 and perplexity is 90.30733265153157
At time: 17.756175756454468 and batch: 450, loss is 4.526731729507446 and perplexity is 92.45589587928323
At time: 18.308362007141113 and batch: 500, loss is 4.422744302749634 and perplexity is 83.32463991644264
At time: 18.8606538772583 and batch: 550, loss is 4.4924161338806154 and perplexity is 89.33703556542974
At time: 19.413408517837524 and batch: 600, loss is 4.463250904083252 and perplexity is 86.76912920475891
At time: 19.96507453918457 and batch: 650, loss is 4.315207166671753 and perplexity is 74.82912393885113
At time: 20.51775884628296 and batch: 700, loss is 4.355706653594971 and perplexity is 77.92186965522345
At time: 21.098147869110107 and batch: 750, loss is 4.414651374816895 and perplexity is 82.65302095815458
At time: 21.715925216674805 and batch: 800, loss is 4.363869857788086 and perplexity is 78.5605651423945
At time: 22.313862562179565 and batch: 850, loss is 4.4288858222961425 and perplexity is 83.8379544734445
At time: 22.904533863067627 and batch: 900, loss is 4.36635687828064 and perplexity is 78.75619003854028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5078058112157535 and perplexity of 90.72253758604774
finished 2 epochs...
Completing Train Step...
At time: 24.437233448028564 and batch: 50, loss is 4.4118583869934085 and perplexity is 82.42249415617694
At time: 25.022401094436646 and batch: 100, loss is 4.2776119947433475 and perplexity is 72.06813527103462
At time: 25.589311599731445 and batch: 150, loss is 4.27651246547699 and perplexity is 71.9889377950825
At time: 26.141507625579834 and batch: 200, loss is 4.17917573928833 and perplexity is 65.31199690818849
At time: 26.69461679458618 and batch: 250, loss is 4.317356233596802 and perplexity is 74.99010965672011
At time: 27.253644466400146 and batch: 300, loss is 4.283948259353638 and perplexity is 72.52622781122398
At time: 27.81454873085022 and batch: 350, loss is 4.271630191802979 and perplexity is 71.63832469028287
At time: 28.377501010894775 and batch: 400, loss is 4.188742346763611 and perplexity is 65.93980937565006
At time: 28.945302486419678 and batch: 450, loss is 4.218621063232422 and perplexity is 67.93973506848545
At time: 29.526952266693115 and batch: 500, loss is 4.0995962429046635 and perplexity is 60.315929695778586
At time: 30.121260166168213 and batch: 550, loss is 4.179284954071045 and perplexity is 65.31913033327017
At time: 30.731157302856445 and batch: 600, loss is 4.177551927566529 and perplexity is 65.2060285816246
At time: 31.34997010231018 and batch: 650, loss is 4.027267537117004 and perplexity is 56.10739023613387
At time: 31.969428300857544 and batch: 700, loss is 4.046738429069519 and perplexity is 57.21055612904636
At time: 32.58783173561096 and batch: 750, loss is 4.1376799297332765 and perplexity is 62.657283385041325
At time: 33.2066445350647 and batch: 800, loss is 4.095957179069519 and perplexity is 60.09683506879152
At time: 33.825669288635254 and batch: 850, loss is 4.171681489944458 and perplexity is 64.82436202918078
At time: 34.44559407234192 and batch: 900, loss is 4.11951135635376 and perplexity is 61.529169079962294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375917199539812 and perplexity of 79.51273516134061
finished 3 epochs...
Completing Train Step...
At time: 36.019381046295166 and batch: 50, loss is 4.191151947975158 and perplexity is 66.09888960319466
At time: 36.647895097732544 and batch: 100, loss is 4.058387179374694 and perplexity is 57.88088427490327
At time: 37.264028787612915 and batch: 150, loss is 4.061093487739563 and perplexity is 58.03773995036011
At time: 37.87912845611572 and batch: 200, loss is 3.9638524580001833 and perplexity is 52.659805365419864
At time: 38.494895458221436 and batch: 250, loss is 4.1147752952575685 and perplexity is 61.238452145987054
At time: 39.111492395401 and batch: 300, loss is 4.087017025947571 and perplexity is 59.56195468000207
At time: 39.72706842422485 and batch: 350, loss is 4.074998307228088 and perplexity is 58.85038096799118
At time: 40.34342169761658 and batch: 400, loss is 4.00913845539093 and perplexity is 55.099379535180056
At time: 40.957921504974365 and batch: 450, loss is 4.04068660736084 and perplexity is 56.865373588607014
At time: 41.57206463813782 and batch: 500, loss is 3.9177898597717284 and perplexity is 50.289175732928285
At time: 42.18558859825134 and batch: 550, loss is 3.996345462799072 and perplexity is 54.39898321568475
At time: 42.8008291721344 and batch: 600, loss is 4.008957161903381 and perplexity is 55.08939128193231
At time: 43.4158890247345 and batch: 650, loss is 3.855917477607727 and perplexity is 47.271968023544936
At time: 44.03178668022156 and batch: 700, loss is 3.8663323593139647 and perplexity is 47.7668726918113
At time: 44.64742732048035 and batch: 750, loss is 3.9731024503707886 and perplexity is 53.14916797423996
At time: 45.262890577316284 and batch: 800, loss is 3.9345491218566893 and perplexity is 51.13908726110051
At time: 45.879040002822876 and batch: 850, loss is 4.011500649452209 and perplexity is 55.22968880960191
At time: 46.49517631530762 and batch: 900, loss is 3.965994577407837 and perplexity is 52.7727298621933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333276618016909 and perplexity of 76.19353510997195
finished 4 epochs...
Completing Train Step...
At time: 48.06986951828003 and batch: 50, loss is 4.042676687240601 and perplexity is 56.97865290449833
At time: 48.67709970474243 and batch: 100, loss is 3.9121686601638794 and perplexity is 50.00728326719497
At time: 49.28785848617554 and batch: 150, loss is 3.916931915283203 and perplexity is 50.24604891462912
At time: 49.8993558883667 and batch: 200, loss is 3.8214457607269288 and perplexity is 45.670188778706226
At time: 50.51004195213318 and batch: 250, loss is 3.9711503744125367 and perplexity is 53.04551796050036
At time: 51.12185287475586 and batch: 300, loss is 3.9510230445861816 and perplexity is 51.988526218276405
At time: 51.744099617004395 and batch: 350, loss is 3.9400058698654177 and perplexity is 51.41890312168234
At time: 52.35435366630554 and batch: 400, loss is 3.8805504751205446 and perplexity is 48.45087873514232
At time: 52.96614980697632 and batch: 450, loss is 3.9149184465408324 and perplexity is 50.14498184754642
At time: 53.577796936035156 and batch: 500, loss is 3.7906165075302125 and perplexity is 44.28369309226253
At time: 54.18816423416138 and batch: 550, loss is 3.864985818862915 and perplexity is 47.702595950839594
At time: 54.79674220085144 and batch: 600, loss is 3.8875223064422606 and perplexity is 48.789850342491704
At time: 55.40649151802063 and batch: 650, loss is 3.7354770946502684 and perplexity is 41.908014884029065
At time: 56.016743183135986 and batch: 700, loss is 3.7427665996551513 and perplexity is 42.21461970930756
At time: 56.626544713974 and batch: 750, loss is 3.8494047260284425 and perplexity is 46.965097808563776
At time: 57.23782444000244 and batch: 800, loss is 3.817543377876282 and perplexity is 45.49231351165867
At time: 57.84855842590332 and batch: 850, loss is 3.8931986474990845 and perplexity is 49.06758568767987
At time: 58.45941781997681 and batch: 900, loss is 3.84960750579834 and perplexity is 46.97462234594972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322457091449058 and perplexity of 75.37360078176609
finished 5 epochs...
Completing Train Step...
At time: 60.00943565368652 and batch: 50, loss is 3.932975215911865 and perplexity is 51.05866245478999
At time: 60.62130951881409 and batch: 100, loss is 3.8056536722183227 and perplexity is 44.954626101001125
At time: 61.22011590003967 and batch: 150, loss is 3.811610517501831 and perplexity is 45.22321302462617
At time: 61.82042670249939 and batch: 200, loss is 3.71123076915741 and perplexity is 40.90411908724908
At time: 62.42076015472412 and batch: 250, loss is 3.8617602825164794 and perplexity is 47.548977378114095
At time: 63.02057886123657 and batch: 300, loss is 3.849348130226135 and perplexity is 46.96243985638804
At time: 63.62045907974243 and batch: 350, loss is 3.839475965499878 and perplexity is 46.50109987316293
At time: 64.22055077552795 and batch: 400, loss is 3.779999680519104 and perplexity is 43.81602773718792
At time: 64.82054305076599 and batch: 450, loss is 3.818679347038269 and perplexity is 45.54402074026358
At time: 65.42094087600708 and batch: 500, loss is 3.694707999229431 and perplexity is 40.23382256535528
At time: 66.02256727218628 and batch: 550, loss is 3.766445827484131 and perplexity is 43.2261582689346
At time: 66.62133765220642 and batch: 600, loss is 3.792755045890808 and perplexity is 44.378496803244666
At time: 67.23359775543213 and batch: 650, loss is 3.6438569164276124 and perplexity is 38.23903744418097
At time: 67.83432269096375 and batch: 700, loss is 3.6439091396331786 and perplexity is 38.24103446143894
At time: 68.43313646316528 and batch: 750, loss is 3.7574675846099854 and perplexity is 42.83980032452928
At time: 69.03354549407959 and batch: 800, loss is 3.724452576637268 and perplexity is 41.44853663590185
At time: 69.63455319404602 and batch: 850, loss is 3.8007942485809325 and perplexity is 44.736702448826364
At time: 70.23450255393982 and batch: 900, loss is 3.759806365966797 and perplexity is 42.940110506926196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326858311483305 and perplexity of 75.7060676768826
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 71.75463461875916 and batch: 50, loss is 3.865227346420288 and perplexity is 47.714118833810815
At time: 72.36631608009338 and batch: 100, loss is 3.732297534942627 and perplexity is 41.774977460771446
At time: 72.9618980884552 and batch: 150, loss is 3.7425080871582033 and perplexity is 42.203708113011565
At time: 73.5591356754303 and batch: 200, loss is 3.619113540649414 and perplexity is 37.30448425171625
At time: 74.15555357933044 and batch: 250, loss is 3.7645707368850707 and perplexity is 43.1451812492812
At time: 74.7799859046936 and batch: 300, loss is 3.7407951021194457 and perplexity is 42.13147567663804
At time: 75.40239238739014 and batch: 350, loss is 3.7204886198043825 and perplexity is 41.284561635513384
At time: 76.00739741325378 and batch: 400, loss is 3.653846220970154 and perplexity is 38.6229330674861
At time: 76.60412287712097 and batch: 450, loss is 3.67833957195282 and perplexity is 39.58061871062485
At time: 77.20049047470093 and batch: 500, loss is 3.5480552768707274 and perplexity is 34.74568101230259
At time: 77.79609632492065 and batch: 550, loss is 3.6003442907333376 and perplexity is 36.61083704600858
At time: 78.39215755462646 and batch: 600, loss is 3.6248812294006347 and perplexity is 37.520266590400254
At time: 78.98885607719421 and batch: 650, loss is 3.46386004447937 and perplexity is 31.94002880007155
At time: 79.58574271202087 and batch: 700, loss is 3.448436779975891 and perplexity is 31.451188732750957
At time: 80.1835720539093 and batch: 750, loss is 3.5477671909332273 and perplexity is 34.7356727119086
At time: 80.78175616264343 and batch: 800, loss is 3.5006192827224734 and perplexity is 33.135966137319805
At time: 81.37954354286194 and batch: 850, loss is 3.5559746646881103 and perplexity is 35.0219379844422
At time: 82.00474190711975 and batch: 900, loss is 3.511873745918274 and perplexity is 33.51100009302842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266655490822988 and perplexity of 71.28283041822999
finished 7 epochs...
Completing Train Step...
At time: 83.54112648963928 and batch: 50, loss is 3.7698924922943116 and perplexity is 43.375401395290886
At time: 84.15042638778687 and batch: 100, loss is 3.63745569229126 and perplexity is 37.995042560864434
At time: 84.75573801994324 and batch: 150, loss is 3.6493851852416994 and perplexity is 38.45101852669225
At time: 85.36227178573608 and batch: 200, loss is 3.5324382734298707 and perplexity is 34.20727269732394
At time: 85.9840099811554 and batch: 250, loss is 3.6802051401138307 and perplexity is 39.65452797263169
At time: 86.5956826210022 and batch: 300, loss is 3.662158770561218 and perplexity is 38.94532621205874
At time: 87.18593621253967 and batch: 350, loss is 3.6434442520141603 and perplexity is 38.22326080967479
At time: 87.77733993530273 and batch: 400, loss is 3.582843427658081 and perplexity is 35.97568982308888
At time: 88.37282609939575 and batch: 450, loss is 3.6128722286224364 and perplexity is 37.07238039523813
At time: 88.96572589874268 and batch: 500, loss is 3.485217180252075 and perplexity is 32.62951282621343
At time: 89.55977773666382 and batch: 550, loss is 3.54116171836853 and perplexity is 34.50698331109441
At time: 90.15307760238647 and batch: 600, loss is 3.572792296409607 and perplexity is 35.615904596135266
At time: 90.74836134910583 and batch: 650, loss is 3.4171254682540892 and perplexity is 30.481668442701142
At time: 91.34244561195374 and batch: 700, loss is 3.4060504722595213 and perplexity is 30.145946577452044
At time: 91.93935585021973 and batch: 750, loss is 3.5110686302185057 and perplexity is 33.484030718928665
At time: 92.53822350502014 and batch: 800, loss is 3.46941782951355 and perplexity is 32.11803882664665
At time: 93.1373541355133 and batch: 850, loss is 3.5320289373397826 and perplexity is 34.193273291491934
At time: 93.73491358757019 and batch: 900, loss is 3.4950727605819703 and perplexity is 32.95268552235537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270999856191139 and perplexity of 71.5931827318447
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 95.25384426116943 and batch: 50, loss is 3.741507091522217 and perplexity is 42.16148352220929
At time: 95.8638858795166 and batch: 100, loss is 3.616081085205078 and perplexity is 37.191531414130125
At time: 96.45930933952332 and batch: 150, loss is 3.629610638618469 and perplexity is 37.69813556110588
At time: 97.06236934661865 and batch: 200, loss is 3.5102160024642943 and perplexity is 33.4554934725678
At time: 97.69028162956238 and batch: 250, loss is 3.655332899093628 and perplexity is 38.680395640733515
At time: 98.29315853118896 and batch: 300, loss is 3.6335136556625365 and perplexity is 37.84555953873005
At time: 98.8956298828125 and batch: 350, loss is 3.612629566192627 and perplexity is 37.0633854127489
At time: 99.49742865562439 and batch: 400, loss is 3.5501524114608767 and perplexity is 34.8186238405425
At time: 100.09945344924927 and batch: 450, loss is 3.572122654914856 and perplexity is 35.592062692199626
At time: 100.70371770858765 and batch: 500, loss is 3.443644347190857 and perplexity is 31.300821624664284
At time: 101.31081628799438 and batch: 550, loss is 3.493977355957031 and perplexity is 32.916608761162955
At time: 101.91657567024231 and batch: 600, loss is 3.5228589820861815 and perplexity is 33.88115574451531
At time: 102.52064204216003 and batch: 650, loss is 3.3622032833099365 and perplexity is 28.85269155231162
At time: 103.1265275478363 and batch: 700, loss is 3.345572004318237 and perplexity is 28.376802678920672
At time: 103.72995090484619 and batch: 750, loss is 3.446895942687988 and perplexity is 31.40276488461865
At time: 104.33136105537415 and batch: 800, loss is 3.4015460777282716 and perplexity is 30.01046270602969
At time: 104.94216060638428 and batch: 850, loss is 3.462680516242981 and perplexity is 31.902376844368632
At time: 105.5552864074707 and batch: 900, loss is 3.4254246234893797 and perplexity is 30.735693178279696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.255792905206549 and perplexity of 70.51270491952658
finished 9 epochs...
Completing Train Step...
At time: 107.0725109577179 and batch: 50, loss is 3.713402895927429 and perplexity is 40.9930645847993
At time: 107.70213460922241 and batch: 100, loss is 3.584770374298096 and perplexity is 36.04507989172435
At time: 108.30335974693298 and batch: 150, loss is 3.597307114601135 and perplexity is 36.49981217194644
At time: 108.90175604820251 and batch: 200, loss is 3.480111951828003 and perplexity is 32.46335620450654
At time: 109.4962043762207 and batch: 250, loss is 3.6245612621307375 and perplexity is 37.50826325357337
At time: 110.09306144714355 and batch: 300, loss is 3.606232099533081 and perplexity is 36.8270304827921
At time: 110.69022107124329 and batch: 350, loss is 3.5859795379638673 and perplexity is 36.088690653620084
At time: 111.29026794433594 and batch: 400, loss is 3.526176495552063 and perplexity is 33.99374358751918
At time: 111.90343451499939 and batch: 450, loss is 3.5499570751190186 and perplexity is 34.8118231621644
At time: 112.5385091304779 and batch: 500, loss is 3.4241371870040895 and perplexity is 30.69614838664717
At time: 113.13830161094666 and batch: 550, loss is 3.4762528944015503 and perplexity is 32.338319665424
At time: 113.7378659248352 and batch: 600, loss is 3.507819209098816 and perplexity is 33.37540358556933
At time: 114.33657598495483 and batch: 650, loss is 3.3498454093933105 and perplexity is 28.49832772944624
At time: 114.93633103370667 and batch: 700, loss is 3.3354901313781737 and perplexity is 28.092148693636204
At time: 115.53708219528198 and batch: 750, loss is 3.439723711013794 and perplexity is 31.178342745482315
At time: 116.13614702224731 and batch: 800, loss is 3.3969696140289307 and perplexity is 29.873434703853682
At time: 116.73845553398132 and batch: 850, loss is 3.461199789047241 and perplexity is 31.855173083937064
At time: 117.3383584022522 and batch: 900, loss is 3.426811542510986 and perplexity is 30.778350670196595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.25649522755244 and perplexity of 70.56224496236399
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 118.87685823440552 and batch: 50, loss is 3.7051229667663574 and perplexity is 40.655046229093344
At time: 119.47364592552185 and batch: 100, loss is 3.5809711265563964 and perplexity is 35.908395516659695
At time: 120.07203698158264 and batch: 150, loss is 3.5930752086639406 and perplexity is 36.34567477762409
At time: 120.66703963279724 and batch: 200, loss is 3.476242671012878 and perplexity is 32.337989059903016
At time: 121.26168060302734 and batch: 250, loss is 3.6207761001586913 and perplexity is 37.36655676207089
At time: 121.85553503036499 and batch: 300, loss is 3.5995545864105223 and perplexity is 36.58193672258855
At time: 122.44839191436768 and batch: 350, loss is 3.5794497108459473 and perplexity is 35.85380545720067
At time: 123.04031229019165 and batch: 400, loss is 3.519366912841797 and perplexity is 33.76304674500182
At time: 123.63412523269653 and batch: 450, loss is 3.5411647605895995 and perplexity is 34.507088289125754
At time: 124.23221230506897 and batch: 500, loss is 3.413775143623352 and perplexity is 30.379715840653887
At time: 124.8308916091919 and batch: 550, loss is 3.465633759498596 and perplexity is 31.996731581290202
At time: 125.43057370185852 and batch: 600, loss is 3.4948561143875123 and perplexity is 32.945547221710534
At time: 126.02995157241821 and batch: 650, loss is 3.3323101139068605 and perplexity is 28.002957060621956
At time: 126.63157439231873 and batch: 700, loss is 3.317465558052063 and perplexity is 27.59033576745605
At time: 127.24850940704346 and batch: 750, loss is 3.421720700263977 and perplexity is 30.6220611026202
At time: 127.85146641731262 and batch: 800, loss is 3.3764668035507204 and perplexity is 29.267181518037592
At time: 128.45480394363403 and batch: 850, loss is 3.4373727941513064 and perplexity is 31.10513114468984
At time: 129.0589632987976 and batch: 900, loss is 3.4034463214874267 and perplexity is 30.06754411761785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252636635140197 and perplexity of 70.29049863568166
finished 11 epochs...
Completing Train Step...
At time: 130.58742213249207 and batch: 50, loss is 3.695502290725708 and perplexity is 40.26579264358146
At time: 131.19832921028137 and batch: 100, loss is 3.5688835191726684 and perplexity is 35.476961684326064
At time: 131.79555892944336 and batch: 150, loss is 3.581413760185242 and perplexity is 35.92429329826007
At time: 132.39265894889832 and batch: 200, loss is 3.4657149076461793 and perplexity is 31.999328162139182
At time: 132.9904043674469 and batch: 250, loss is 3.6106641817092897 and perplexity is 36.99061314632535
At time: 133.58740329742432 and batch: 300, loss is 3.5909145736694335 and perplexity is 36.26722981678168
At time: 134.18247771263123 and batch: 350, loss is 3.5704943561553955 and perplexity is 35.53415533870163
At time: 134.79846286773682 and batch: 400, loss is 3.5112742137908937 and perplexity is 33.49091519322498
At time: 135.39918279647827 and batch: 450, loss is 3.5336816120147705 and perplexity is 34.249830370668924
At time: 135.9964942932129 and batch: 500, loss is 3.407509961128235 and perplexity is 30.18997637360167
At time: 136.59386372566223 and batch: 550, loss is 3.4600660848617553 and perplexity is 31.819079204639355
At time: 137.190753698349 and batch: 600, loss is 3.490604124069214 and perplexity is 32.8057604704545
At time: 137.7864167690277 and batch: 650, loss is 3.329003710746765 and perplexity is 27.91052089473186
At time: 138.38425660133362 and batch: 700, loss is 3.3154733610153198 and perplexity is 27.53542509689597
At time: 138.98208212852478 and batch: 750, loss is 3.420900754928589 and perplexity is 30.59696297741063
At time: 139.5803256034851 and batch: 800, loss is 3.3765316009521484 and perplexity is 29.269078016790516
At time: 140.1781554222107 and batch: 850, loss is 3.4387972068786623 and perplexity is 31.149469259769468
At time: 140.77630925178528 and batch: 900, loss is 3.4063116836547853 and perplexity is 30.15382207075861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252331459358947 and perplexity of 70.26905095066954
finished 12 epochs...
Completing Train Step...
At time: 142.29501581192017 and batch: 50, loss is 3.69020911693573 and perplexity is 40.05322188861366
At time: 142.90533065795898 and batch: 100, loss is 3.5630393505096434 and perplexity is 35.27023300361919
At time: 143.49976992607117 and batch: 150, loss is 3.575288805961609 and perplexity is 35.70493112371236
At time: 144.09785795211792 and batch: 200, loss is 3.459890718460083 and perplexity is 31.813499696458152
At time: 144.6953465938568 and batch: 250, loss is 3.6047141695022584 and perplexity is 36.771172032611084
At time: 145.2931888103485 and batch: 300, loss is 3.5854900741577147 and perplexity is 36.07103086799922
At time: 145.92196655273438 and batch: 350, loss is 3.5651157569885252 and perplexity is 35.343544429759724
At time: 146.53013825416565 and batch: 400, loss is 3.5063600015640257 and perplexity is 33.32673746080392
At time: 147.12428998947144 and batch: 450, loss is 3.5290496158599853 and perplexity is 34.09155214315386
At time: 147.7451090812683 and batch: 500, loss is 3.40349365234375 and perplexity is 30.06896727390781
At time: 148.3436679840088 and batch: 550, loss is 3.456478462219238 and perplexity is 31.705128883218904
At time: 148.9413936138153 and batch: 600, loss is 3.487589511871338 and perplexity is 32.7070127426161
At time: 149.53914856910706 and batch: 650, loss is 3.3265310955047607 and perplexity is 27.841594165113545
At time: 150.13616108894348 and batch: 700, loss is 3.3136638259887694 and perplexity is 27.485643834764645
At time: 150.7324833869934 and batch: 750, loss is 3.419724688529968 and perplexity is 30.56100006888233
At time: 151.33165502548218 and batch: 800, loss is 3.3758542490005494 and perplexity is 29.249259262568014
At time: 151.92917156219482 and batch: 850, loss is 3.4387325191497804 and perplexity is 31.147454336518283
At time: 152.5282256603241 and batch: 900, loss is 3.406874809265137 and perplexity is 30.170807242160297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252524598004067 and perplexity of 70.28262393065529
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 154.05462312698364 and batch: 50, loss is 3.687740068435669 and perplexity is 39.95445052700559
At time: 154.64565753936768 and batch: 100, loss is 3.561928777694702 and perplexity is 35.23108458427757
At time: 155.23619604110718 and batch: 150, loss is 3.57452317237854 and perplexity is 35.677604691714784
At time: 155.82439756393433 and batch: 200, loss is 3.4587747955322268 and perplexity is 31.77801808378613
At time: 156.41679000854492 and batch: 250, loss is 3.6044647312164306 and perplexity is 36.76200103833749
At time: 157.01272535324097 and batch: 300, loss is 3.584820852279663 and perplexity is 36.04689942052539
At time: 157.63136649131775 and batch: 350, loss is 3.564028272628784 and perplexity is 35.305129769431524
At time: 158.22952675819397 and batch: 400, loss is 3.5057592582702637 and perplexity is 33.30672265925221
At time: 158.82826328277588 and batch: 450, loss is 3.5276230239868163 and perplexity is 34.042952086388155
At time: 159.42738890647888 and batch: 500, loss is 3.4002210474014283 and perplexity is 29.970724265954804
At time: 160.0262246131897 and batch: 550, loss is 3.4529896545410157 and perplexity is 31.594708515845863
At time: 160.6236162185669 and batch: 600, loss is 3.483539342880249 and perplexity is 32.57481171278792
At time: 161.22359371185303 and batch: 650, loss is 3.320935688018799 and perplexity is 27.686244129392968
At time: 161.82274222373962 and batch: 700, loss is 3.306667513847351 and perplexity is 27.294016813966998
At time: 162.421550989151 and batch: 750, loss is 3.4128108167648317 and perplexity is 30.350433985623326
At time: 163.02087450027466 and batch: 800, loss is 3.3686550664901733 and perplexity is 29.039444659845135
At time: 163.61914491653442 and batch: 850, loss is 3.4290622520446776 and perplexity is 30.847701812844303
At time: 164.2177996635437 and batch: 900, loss is 3.3973109579086302 and perplexity is 29.88363355851473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.251770437580266 and perplexity of 70.22963953908277
finished 14 epochs...
Completing Train Step...
At time: 165.7422161102295 and batch: 50, loss is 3.6851188373565673 and perplexity is 39.84985782024932
At time: 166.35161757469177 and batch: 100, loss is 3.559076337814331 and perplexity is 35.130733224732325
At time: 166.94883036613464 and batch: 150, loss is 3.5718975687026977 and perplexity is 35.58405231117232
At time: 167.54326510429382 and batch: 200, loss is 3.456010174751282 and perplexity is 31.69028524450921
At time: 168.1373414993286 and batch: 250, loss is 3.601705718040466 and perplexity is 36.66071398350835
At time: 168.7342085838318 and batch: 300, loss is 3.5818341970443726 and perplexity is 35.93940037086342
At time: 169.33084201812744 and batch: 350, loss is 3.5613088274002074 and perplexity is 35.20924983194408
At time: 169.9277172088623 and batch: 400, loss is 3.5031224489212036 and perplexity is 33.21901486673901
At time: 170.52445340156555 and batch: 450, loss is 3.5253089094161987 and perplexity is 33.96426387681979
At time: 171.1206555366516 and batch: 500, loss is 3.3985288381576537 and perplexity is 29.920050416779684
At time: 171.71666049957275 and batch: 550, loss is 3.4514745044708253 and perplexity is 31.54687403837743
At time: 172.31178498268127 and batch: 600, loss is 3.482523775100708 and perplexity is 32.54174657637327
At time: 172.91845297813416 and batch: 650, loss is 3.320182681083679 and perplexity is 27.665404042905674
At time: 173.51125359535217 and batch: 700, loss is 3.306420669555664 and perplexity is 27.287280273191787
At time: 174.10719060897827 and batch: 750, loss is 3.412982339859009 and perplexity is 30.35564023245318
At time: 174.70506739616394 and batch: 800, loss is 3.3691032695770264 and perplexity is 29.052463145827044
At time: 175.3042197227478 and batch: 850, loss is 3.429997797012329 and perplexity is 30.876574728888674
At time: 175.90451335906982 and batch: 900, loss is 3.3988344526290892 and perplexity is 29.929195814584908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2515367481806505 and perplexity of 70.21322953428044
finished 15 epochs...
Completing Train Step...
At time: 177.4251263141632 and batch: 50, loss is 3.6835232973098755 and perplexity is 39.78632647312431
At time: 178.03586196899414 and batch: 100, loss is 3.5572322463989257 and perplexity is 35.066008638544844
At time: 178.63164472579956 and batch: 150, loss is 3.570071702003479 and perplexity is 35.51913985381493
At time: 179.22747135162354 and batch: 200, loss is 3.454184412956238 and perplexity is 31.63247911860144
At time: 179.82179522514343 and batch: 250, loss is 3.5998800563812257 and perplexity is 36.59384498224727
At time: 180.4179196357727 and batch: 300, loss is 3.580042853355408 and perplexity is 35.875078181598155
At time: 181.03212714195251 and batch: 350, loss is 3.5596416902542116 and perplexity is 35.15060008583577
At time: 181.65777778625488 and batch: 400, loss is 3.5014926958084107 and perplexity is 33.1649201663326
At time: 182.2878222465515 and batch: 450, loss is 3.523776636123657 and perplexity is 33.91226119371633
At time: 182.89437174797058 and batch: 500, loss is 3.3973395109176634 and perplexity is 29.884486838355464
At time: 183.50816249847412 and batch: 550, loss is 3.4504526710510253 and perplexity is 31.514654852318788
At time: 184.11347460746765 and batch: 600, loss is 3.4818037271499636 and perplexity is 32.518323392357146
At time: 184.70949602127075 and batch: 650, loss is 3.319608087539673 and perplexity is 27.649512246449355
At time: 185.3051633834839 and batch: 700, loss is 3.3061345767974855 and perplexity is 27.27947469652796
At time: 185.90015602111816 and batch: 750, loss is 3.41296257019043 and perplexity is 30.355040117438335
At time: 186.4940619468689 and batch: 800, loss is 3.369249048233032 and perplexity is 29.056698683576098
At time: 187.0893750190735 and batch: 850, loss is 3.430422234535217 and perplexity is 30.889682687339793
At time: 187.69711303710938 and batch: 900, loss is 3.399547562599182 and perplexity is 29.95054623421093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.251471950583262 and perplexity of 70.20868003310173
finished 16 epochs...
Completing Train Step...
At time: 189.2216341495514 and batch: 50, loss is 3.682150025367737 and perplexity is 39.731726526166334
At time: 189.80864214897156 and batch: 100, loss is 3.555705900192261 and perplexity is 35.012526595716366
At time: 190.3952808380127 and batch: 150, loss is 3.568524179458618 and perplexity is 35.46421569326659
At time: 190.98195123672485 and batch: 200, loss is 3.4526687479019165 and perplexity is 31.58457119077718
At time: 191.56934475898743 and batch: 250, loss is 3.598371329307556 and perplexity is 36.53867648516027
At time: 192.1616678237915 and batch: 300, loss is 3.5786101961135866 and perplexity is 35.82371829038513
At time: 192.75637555122375 and batch: 350, loss is 3.55827365398407 and perplexity is 35.102545667589226
At time: 193.35171389579773 and batch: 400, loss is 3.500185923576355 and perplexity is 33.121609474349015
At time: 193.94859099388123 and batch: 450, loss is 3.522537932395935 and perplexity is 33.87027995588604
At time: 194.54487371444702 and batch: 500, loss is 3.3963253498077393 and perplexity is 29.85419451725732
At time: 195.14336323738098 and batch: 550, loss is 3.449579510688782 and perplexity is 31.48714951490468
At time: 195.74214935302734 and batch: 600, loss is 3.4811413526535033 and perplexity is 32.496791216243814
At time: 196.34143161773682 and batch: 650, loss is 3.3190605640411377 and perplexity is 27.634377632427885
At time: 196.94165182113647 and batch: 700, loss is 3.305782313346863 and perplexity is 27.26986682699018
At time: 197.54156589508057 and batch: 750, loss is 3.412808475494385 and perplexity is 30.35036292713237
At time: 198.1411349773407 and batch: 800, loss is 3.369223370552063 and perplexity is 29.055952584516376
At time: 198.74054956436157 and batch: 850, loss is 3.4305850982666017 and perplexity is 30.894713906013944
At time: 199.3383207321167 and batch: 900, loss is 3.3998966121673586 and perplexity is 29.961002284174334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.251482819857663 and perplexity of 70.20944315465758
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 200.85629558563232 and batch: 50, loss is 3.6814131259918215 and perplexity is 39.70245902661064
At time: 201.45362281799316 and batch: 100, loss is 3.5552923107147216 and perplexity is 34.998048777277376
At time: 202.03940773010254 and batch: 150, loss is 3.5683897352218628 and perplexity is 35.459448054353544
At time: 202.6425039768219 and batch: 200, loss is 3.4522585439682008 and perplexity is 31.571617732391427
At time: 203.23786211013794 and batch: 250, loss is 3.598233437538147 and perplexity is 36.533638449767665
At time: 203.83354353904724 and batch: 300, loss is 3.578277111053467 and perplexity is 35.81178793204703
At time: 204.4329493045807 and batch: 350, loss is 3.557616767883301 and perplexity is 35.07949486494338
At time: 205.03124690055847 and batch: 400, loss is 3.499891004562378 and perplexity is 33.11184272221275
At time: 205.63138890266418 and batch: 450, loss is 3.5220925283432005 and perplexity is 33.855197355101545
At time: 206.23137784004211 and batch: 500, loss is 3.395013132095337 and perplexity is 29.81504500638075
At time: 206.82918500900269 and batch: 550, loss is 3.448232855796814 and perplexity is 31.444775728813134
At time: 207.4283607006073 and batch: 600, loss is 3.479708685874939 and perplexity is 32.45026747752343
At time: 208.02953124046326 and batch: 650, loss is 3.3171373128890993 and perplexity is 27.581280859392024
At time: 208.6291024684906 and batch: 700, loss is 3.303529210090637 and perplexity is 27.208494166678896
At time: 209.22837567329407 and batch: 750, loss is 3.410264377593994 and perplexity is 30.273246769654616
At time: 209.82733297348022 and batch: 800, loss is 3.3666430568695067 and perplexity is 28.981075756898512
At time: 210.4271183013916 and batch: 850, loss is 3.427554473876953 and perplexity is 30.80122536841547
At time: 211.0273892879486 and batch: 900, loss is 3.3967858839035032 and perplexity is 29.867946558132882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.251133748929795 and perplexity of 70.18493935622114
finished 18 epochs...
Completing Train Step...
At time: 212.53528928756714 and batch: 50, loss is 3.680833730697632 and perplexity is 39.67946227143207
At time: 213.13439917564392 and batch: 100, loss is 3.5547201824188233 and perplexity is 34.97803113014786
At time: 213.72044372558594 and batch: 150, loss is 3.5677781105041504 and perplexity is 35.43776681051527
At time: 214.30611991882324 and batch: 200, loss is 3.4517319011688232 and perplexity is 31.55499514471521
At time: 214.89509749412537 and batch: 250, loss is 3.59772011756897 and perplexity is 36.51488981604043
At time: 215.4866328239441 and batch: 300, loss is 3.577684440612793 and perplexity is 35.79056963226266
At time: 216.07989287376404 and batch: 350, loss is 3.5571269035339355 and perplexity is 35.0623148792901
At time: 216.67284560203552 and batch: 400, loss is 3.499375410079956 and perplexity is 33.09477483923361
At time: 217.26659870147705 and batch: 450, loss is 3.521663031578064 and perplexity is 33.840659779498786
At time: 217.8729088306427 and batch: 500, loss is 3.394715442657471 and perplexity is 29.806170703351526
At time: 218.4685399532318 and batch: 550, loss is 3.4479585218429567 and perplexity is 31.43615054230415
At time: 219.0648193359375 and batch: 600, loss is 3.479523220062256 and perplexity is 32.44424962036456
At time: 219.65923070907593 and batch: 650, loss is 3.3170631408691404 and perplexity is 27.579235175944806
At time: 220.25553226470947 and batch: 700, loss is 3.3034837818145752 and perplexity is 27.20725815976971
At time: 220.85185360908508 and batch: 750, loss is 3.4103443479537963 and perplexity is 30.275667828896363
At time: 221.4467580318451 and batch: 800, loss is 3.3668106460571288 and perplexity is 28.985933078846976
At time: 222.04200434684753 and batch: 850, loss is 3.4277392768859865 and perplexity is 30.80691805354194
At time: 222.63774752616882 and batch: 900, loss is 3.397070050239563 and perplexity is 29.87643522911208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.250959004441353 and perplexity of 70.17267599640549
finished 19 epochs...
Completing Train Step...
At time: 224.17112612724304 and batch: 50, loss is 3.6803611755371093 and perplexity is 39.660715966449374
At time: 224.76283740997314 and batch: 100, loss is 3.5542375326156614 and perplexity is 34.96115306373327
At time: 225.35402846336365 and batch: 150, loss is 3.5672720146179198 and perplexity is 35.419836440141324
At time: 225.94710898399353 and batch: 200, loss is 3.451273741722107 and perplexity is 31.5405412369486
At time: 226.5437376499176 and batch: 250, loss is 3.597260217666626 and perplexity is 36.49810048278221
At time: 227.13967514038086 and batch: 300, loss is 3.5771989488601683 and perplexity is 35.7731978231607
At time: 227.7344946861267 and batch: 350, loss is 3.5567036294937133 and perplexity is 35.04747705206667
At time: 228.33006596565247 and batch: 400, loss is 3.498947114944458 and perplexity is 33.08060354312479
At time: 228.96829843521118 and batch: 450, loss is 3.5212853288650514 and perplexity is 33.827880484029144
At time: 229.57386541366577 and batch: 500, loss is 3.394446306228638 and perplexity is 29.798149856410706
At time: 230.1870632171631 and batch: 550, loss is 3.447717900276184 and perplexity is 31.428587236491104
At time: 230.7851905822754 and batch: 600, loss is 3.479357705116272 and perplexity is 32.438880056524944
At time: 231.38141870498657 and batch: 650, loss is 3.316970887184143 and perplexity is 27.576691007226685
At time: 231.97712683677673 and batch: 700, loss is 3.3034371519088745 and perplexity is 27.205989517465913
At time: 232.5859305858612 and batch: 750, loss is 3.410393056869507 and perplexity is 30.277142559764705
At time: 233.1831042766571 and batch: 800, loss is 3.3669166374206543 and perplexity is 28.98900550023925
At time: 233.78115391731262 and batch: 850, loss is 3.427876143455505 and perplexity is 30.811134779291184
At time: 234.3795473575592 and batch: 900, loss is 3.3972924661636354 and perplexity is 29.88308096309269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.250870796099101 and perplexity of 70.16648645397312
Finished Training.
Improved accuracyfrom -10000000 to -70.16648645397312
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f52e60f0b70>
ELAPSED
246.230801820755


RESULTS SO FAR:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8516628742218018 and batch: 50, loss is 6.88984094619751 and perplexity is 982.2451749630707
At time: 1.494300365447998 and batch: 100, loss is 6.041595287322998 and perplexity is 420.5634195081147
At time: 2.1270272731781006 and batch: 150, loss is 5.7643867874145505 and perplexity is 318.7435265534703
At time: 2.7578060626983643 and batch: 200, loss is 5.497529993057251 and perplexity is 244.08828730321517
At time: 3.3891096115112305 and batch: 250, loss is 5.476437034606934 and perplexity is 238.99366244345407
At time: 4.023932456970215 and batch: 300, loss is 5.3574613571167 and perplexity is 212.18559867350032
At time: 4.660735368728638 and batch: 350, loss is 5.306513195037842 and perplexity is 201.64590124698472
At time: 5.295150995254517 and batch: 400, loss is 5.1329857444763185 and perplexity is 169.5225140877615
At time: 5.923118352890015 and batch: 450, loss is 5.114662780761718 and perplexity is 166.44464318268732
At time: 6.552775621414185 and batch: 500, loss is 5.033770923614502 and perplexity is 153.51080011606808
At time: 7.193152666091919 and batch: 550, loss is 5.075472745895386 and perplexity is 160.04783605532504
At time: 7.822887659072876 and batch: 600, loss is 4.9917340660095215 and perplexity is 147.19144199879528
At time: 8.453125953674316 and batch: 650, loss is 4.863718900680542 and perplexity is 129.50492362414937
At time: 9.081555128097534 and batch: 700, loss is 4.939593229293823 and perplexity is 139.7134066786802
At time: 9.718641757965088 and batch: 750, loss is 4.934508590698242 and perplexity is 139.0048174853702
At time: 10.352323532104492 and batch: 800, loss is 4.89025800704956 and perplexity is 132.9878814362355
At time: 10.980870723724365 and batch: 850, loss is 4.921017665863037 and perplexity is 137.14210703608128
At time: 11.619087934494019 and batch: 900, loss is 4.836644563674927 and perplexity is 126.04570303390992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.836389829034674 and perplexity of 126.01359891628123
finished 1 epochs...
Completing Train Step...
At time: 13.105297088623047 and batch: 50, loss is 4.807804403305053 and perplexity is 122.46244398986488
At time: 13.68668246269226 and batch: 100, loss is 4.676524209976196 and perplexity is 107.3961366661003
At time: 14.240571975708008 and batch: 150, loss is 4.6583618068695065 and perplexity is 105.46317152171507
At time: 14.79313063621521 and batch: 200, loss is 4.548874769210816 and perplexity is 94.5259849021128
At time: 15.359098434448242 and batch: 250, loss is 4.65819902420044 and perplexity is 105.44600534238266
At time: 15.945950508117676 and batch: 300, loss is 4.605384330749512 and perplexity is 100.0214167692047
At time: 16.498581409454346 and batch: 350, loss is 4.591765851974487 and perplexity is 98.66851040065517
At time: 17.05140471458435 and batch: 400, loss is 4.472638711929322 and perplexity is 87.58753663196158
At time: 17.612263917922974 and batch: 450, loss is 4.4984992980957035 and perplexity is 89.88214373354309
At time: 18.184738636016846 and batch: 500, loss is 4.386118650436401 and perplexity is 80.3280319592455
At time: 18.77095603942871 and batch: 550, loss is 4.4628834056854245 and perplexity is 86.73724754738308
At time: 19.371489763259888 and batch: 600, loss is 4.439351062774659 and perplexity is 84.7199459047306
At time: 19.98225712776184 and batch: 650, loss is 4.291632680892945 and perplexity is 73.08569676294647
At time: 20.594223260879517 and batch: 700, loss is 4.330757460594177 and perplexity is 76.00183316550448
At time: 21.20409917831421 and batch: 750, loss is 4.389732189178467 and perplexity is 80.61882549523466
At time: 21.81345820426941 and batch: 800, loss is 4.339804630279541 and perplexity is 76.69255447162028
At time: 22.42062020301819 and batch: 850, loss is 4.408229780197144 and perplexity is 82.12395729771772
At time: 23.0276997089386 and batch: 900, loss is 4.3451430606842045 and perplexity is 77.10306710786493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.49596164651113 and perplexity of 89.6543433303466
finished 2 epochs...
Completing Train Step...
At time: 24.555583953857422 and batch: 50, loss is 4.405933885574341 and perplexity is 81.93562562332065
At time: 25.17764925956726 and batch: 100, loss is 4.274949979782105 and perplexity is 71.87654393934977
At time: 25.77282166481018 and batch: 150, loss is 4.271366348266602 and perplexity is 71.61942587463128
At time: 26.37069272994995 and batch: 200, loss is 4.171971950531006 and perplexity is 64.84319368619306
At time: 26.97105884552002 and batch: 250, loss is 4.309371109008789 and perplexity is 74.39368870513516
At time: 27.570060968399048 and batch: 300, loss is 4.2783096885681156 and perplexity is 72.11843430859935
At time: 28.167409658432007 and batch: 350, loss is 4.268847737312317 and perplexity is 71.43927136887235
At time: 28.76491403579712 and batch: 400, loss is 4.176924962997436 and perplexity is 65.16515952507663
At time: 29.364888668060303 and batch: 450, loss is 4.212863545417786 and perplexity is 67.54969474302183
At time: 29.977046728134155 and batch: 500, loss is 4.0918948316574095 and perplexity is 59.853196054590974
At time: 30.576642274856567 and batch: 550, loss is 4.174047117233276 and perplexity is 64.97789383689481
At time: 31.17603635787964 and batch: 600, loss is 4.178415732383728 and perplexity is 65.26237819725475
At time: 31.775550842285156 and batch: 650, loss is 4.025929145812988 and perplexity is 56.03234682287869
At time: 32.37641358375549 and batch: 700, loss is 4.048282160758972 and perplexity is 57.298942082059504
At time: 32.977094411849976 and batch: 750, loss is 4.1385363006591795 and perplexity is 62.710964242910656
At time: 33.576988697052 and batch: 800, loss is 4.092477602958679 and perplexity is 59.88808694525825
At time: 34.17688798904419 and batch: 850, loss is 4.169532771110535 and perplexity is 64.68522224126698
At time: 34.77670907974243 and batch: 900, loss is 4.1138881397247316 and perplexity is 61.184148205925446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384522738522047 and perplexity of 80.19993773672032
finished 3 epochs...
Completing Train Step...
At time: 36.29414367675781 and batch: 50, loss is 4.192810831069946 and perplexity is 66.20863093259334
At time: 36.90012335777283 and batch: 100, loss is 4.0667090463638305 and perplexity is 58.3645710923194
At time: 37.49108338356018 and batch: 150, loss is 4.063559823036194 and perplexity is 58.181057138408505
At time: 38.08328342437744 and batch: 200, loss is 3.969026083946228 and perplexity is 52.93295347457858
At time: 38.67602801322937 and batch: 250, loss is 4.114262366294861 and perplexity is 61.20704922467244
At time: 39.26743412017822 and batch: 300, loss is 4.093555359840393 and perplexity is 59.95266653739503
At time: 39.860621213912964 and batch: 350, loss is 4.083179531097412 and perplexity is 59.33382399078595
At time: 40.45362091064453 and batch: 400, loss is 4.005848956108093 and perplexity is 54.91842794894103
At time: 41.04686641693115 and batch: 450, loss is 4.040410857200623 and perplexity is 56.84969511449938
At time: 41.64375591278076 and batch: 500, loss is 3.9175326299667357 and perplexity is 50.2762415216649
At time: 42.241342306137085 and batch: 550, loss is 4.0015260076522825 and perplexity is 54.68153083158278
At time: 42.83837842941284 and batch: 600, loss is 4.015656204223633 and perplexity is 55.45967633838103
At time: 43.43540692329407 and batch: 650, loss is 3.8627590322494507 and perplexity is 47.596490629547596
At time: 44.03314018249512 and batch: 700, loss is 3.8786848974227905 and perplexity is 48.360574117668826
At time: 44.632583141326904 and batch: 750, loss is 3.977531886100769 and perplexity is 53.38511095926314
At time: 45.24401307106018 and batch: 800, loss is 3.9367982387542724 and perplexity is 51.25423448763327
At time: 45.844717264175415 and batch: 850, loss is 4.015284695625305 and perplexity is 55.43907641852148
At time: 46.445125102996826 and batch: 900, loss is 3.96568359375 and perplexity is 52.75632095720962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341734167647688 and perplexity of 76.84067848078043
finished 4 epochs...
Completing Train Step...
At time: 47.97509026527405 and batch: 50, loss is 4.049064927101135 and perplexity is 57.34381132413811
At time: 48.56830883026123 and batch: 100, loss is 3.9259651565551756 and perplexity is 50.70198980911826
At time: 49.160133838653564 and batch: 150, loss is 3.925738468170166 and perplexity is 50.690497559560626
At time: 49.751224517822266 and batch: 200, loss is 3.8356072998046873 and perplexity is 46.32155019607007
At time: 50.34394073486328 and batch: 250, loss is 3.9788501024246217 and perplexity is 53.45553048786908
At time: 50.93539357185364 and batch: 300, loss is 3.9595909214019773 and perplexity is 52.43587116838878
At time: 51.52769064903259 and batch: 350, loss is 3.9532506227493287 and perplexity is 52.104463806092696
At time: 52.120484590530396 and batch: 400, loss is 3.8801352643966673 and perplexity is 48.43076558659665
At time: 52.712852478027344 and batch: 450, loss is 3.9167291831970217 and perplexity is 50.23586346080433
At time: 53.305880308151245 and batch: 500, loss is 3.794583311080933 and perplexity is 44.45970667811306
At time: 53.90159249305725 and batch: 550, loss is 3.878329801559448 and perplexity is 48.343404526456595
At time: 54.499266147613525 and batch: 600, loss is 3.8967798376083373 and perplexity is 49.24362106016028
At time: 55.096757888793945 and batch: 650, loss is 3.747835087776184 and perplexity is 42.42912716286134
At time: 55.6940655708313 and batch: 700, loss is 3.7563006401062013 and perplexity is 42.78983781240657
At time: 56.29071378707886 and batch: 750, loss is 3.8621571350097654 and perplexity is 47.567851053124585
At time: 56.88714051246643 and batch: 800, loss is 3.820721688270569 and perplexity is 45.63713222205014
At time: 57.482651710510254 and batch: 850, loss is 3.9000712251663208 and perplexity is 49.405967928497255
At time: 58.07878589630127 and batch: 900, loss is 3.8534352350234986 and perplexity is 47.154773044835025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333034985686002 and perplexity of 76.17512651262895
finished 5 epochs...
Completing Train Step...
At time: 59.60274147987366 and batch: 50, loss is 3.9408863019943237 and perplexity is 51.46419391083109
At time: 60.21851825714111 and batch: 100, loss is 3.819481792449951 and perplexity is 45.580581997981035
At time: 60.85170102119446 and batch: 150, loss is 3.821482515335083 and perplexity is 45.67186739944746
At time: 61.44355392456055 and batch: 200, loss is 3.7335816335678103 and perplexity is 41.82865510821672
At time: 62.029940605163574 and batch: 250, loss is 3.878482460975647 and perplexity is 48.35078516571638
At time: 62.61861443519592 and batch: 300, loss is 3.861732668876648 and perplexity is 47.54766439590661
At time: 63.21034479141235 and batch: 350, loss is 3.8542501592636107 and perplexity is 47.19321627445857
At time: 63.80358839035034 and batch: 400, loss is 3.781474690437317 and perplexity is 43.88070450038438
At time: 64.39636874198914 and batch: 450, loss is 3.8213880443573 and perplexity is 45.667552937275985
At time: 64.98936915397644 and batch: 500, loss is 3.69984179019928 and perplexity is 40.44090570628314
At time: 65.58209013938904 and batch: 550, loss is 3.7845276403427124 and perplexity is 44.01487479642261
At time: 66.17516994476318 and batch: 600, loss is 3.8055479764938354 and perplexity is 44.94987484032471
At time: 66.76883292198181 and batch: 650, loss is 3.6575911092758178 and perplexity is 38.76784280389618
At time: 67.36596369743347 and batch: 700, loss is 3.6634632205963134 and perplexity is 38.99616159310173
At time: 67.96390438079834 and batch: 750, loss is 3.770510015487671 and perplexity is 43.40219498365343
At time: 68.5619375705719 and batch: 800, loss is 3.731469850540161 and perplexity is 41.74041526878051
At time: 69.16040182113647 and batch: 850, loss is 3.810314440727234 and perplexity is 45.164638235463755
At time: 69.75877165794373 and batch: 900, loss is 3.7651990127563475 and perplexity is 43.1722968427666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33316541697881 and perplexity of 76.18506278084588
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 71.27260065078735 and batch: 50, loss is 3.8721407985687257 and perplexity is 48.04513101118214
At time: 71.88164258003235 and batch: 100, loss is 3.743797330856323 and perplexity is 42.2581540672438
At time: 72.47812366485596 and batch: 150, loss is 3.7427983713150024 and perplexity is 42.21596095915246
At time: 73.07407331466675 and batch: 200, loss is 3.637295265197754 and perplexity is 37.988947615529014
At time: 73.67737412452698 and batch: 250, loss is 3.7816419124603273 and perplexity is 43.88804293411883
At time: 74.27210903167725 and batch: 300, loss is 3.7498471546173096 and perplexity is 42.51458334566938
At time: 74.86880683898926 and batch: 350, loss is 3.7306677865982056 and perplexity is 41.70695020912403
At time: 75.47663354873657 and batch: 400, loss is 3.6546310663223265 and perplexity is 38.65325799562421
At time: 76.07184839248657 and batch: 450, loss is 3.686202688217163 and perplexity is 39.893072537866374
At time: 76.67137503623962 and batch: 500, loss is 3.5532153463363647 and perplexity is 34.92543451138029
At time: 77.29144215583801 and batch: 550, loss is 3.619192543029785 and perplexity is 37.30743151118937
At time: 77.8928632736206 and batch: 600, loss is 3.637488355636597 and perplexity is 37.99628362632926
At time: 78.48644018173218 and batch: 650, loss is 3.4747080278396605 and perplexity is 32.28839984636308
At time: 79.10033655166626 and batch: 700, loss is 3.4681216192245485 and perplexity is 32.07643406434271
At time: 79.71530508995056 and batch: 750, loss is 3.564784622192383 and perplexity is 35.33184288988046
At time: 80.31401920318604 and batch: 800, loss is 3.5087382125854494 and perplexity is 33.40608979603976
At time: 80.91080451011658 and batch: 850, loss is 3.570633807182312 and perplexity is 35.539110958679345
At time: 81.50749111175537 and batch: 900, loss is 3.5186162424087524 and perplexity is 33.73771133454234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2742982629227315 and perplexity of 71.82971604479268
finished 7 epochs...
Completing Train Step...
At time: 83.02569961547852 and batch: 50, loss is 3.7773867654800415 and perplexity is 43.70168962230336
At time: 83.61586284637451 and batch: 100, loss is 3.650417046546936 and perplexity is 38.490715122025016
At time: 84.20659255981445 and batch: 150, loss is 3.6517103910446167 and perplexity is 38.54052908300223
At time: 84.79693508148193 and batch: 200, loss is 3.5505396032333376 and perplexity is 34.83210793551883
At time: 85.38688087463379 and batch: 250, loss is 3.6980610513687133 and perplexity is 40.368955096783914
At time: 85.97706699371338 and batch: 300, loss is 3.6712947416305544 and perplexity is 39.30275985118586
At time: 86.59334230422974 and batch: 350, loss is 3.6566346168518065 and perplexity is 38.73077938422477
At time: 87.18472123146057 and batch: 400, loss is 3.5853191471099852 and perplexity is 36.064865880080994
At time: 87.77637362480164 and batch: 450, loss is 3.6224796199798583 and perplexity is 37.43026568146049
At time: 88.3695182800293 and batch: 500, loss is 3.491996531486511 and perplexity is 32.85147127130738
At time: 88.96243619918823 and batch: 550, loss is 3.560714559555054 and perplexity is 35.1883323228341
At time: 89.55661201477051 and batch: 600, loss is 3.5865366363525393 and perplexity is 36.10880120629062
At time: 90.17627429962158 and batch: 650, loss is 3.4289679288864137 and perplexity is 30.84479229740363
At time: 90.76605558395386 and batch: 700, loss is 3.426843376159668 and perplexity is 30.779330472994104
At time: 91.35733151435852 and batch: 750, loss is 3.529243788719177 and perplexity is 34.09817244002797
At time: 91.95163941383362 and batch: 800, loss is 3.4777720975875854 and perplexity is 32.38748548066618
At time: 92.56174540519714 and batch: 850, loss is 3.546067681312561 and perplexity is 34.67668923764237
At time: 93.18321943283081 and batch: 900, loss is 3.5017618799209593 and perplexity is 33.17384883760963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.276947439533391 and perplexity of 72.02025792659975
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 94.70915532112122 and batch: 50, loss is 3.74964572429657 and perplexity is 42.5060204819491
At time: 95.30863571166992 and batch: 100, loss is 3.628027482032776 and perplexity is 37.63850072761929
At time: 95.89580082893372 and batch: 150, loss is 3.6327961111068725 and perplexity is 37.81841340397231
At time: 96.48483633995056 and batch: 200, loss is 3.5230640888214113 and perplexity is 33.88810571047393
At time: 97.07042241096497 and batch: 250, loss is 3.6721091270446777 and perplexity is 39.3347804823412
At time: 97.65757298469543 and batch: 300, loss is 3.639094614982605 and perplexity is 38.05736455479097
At time: 98.24331164360046 and batch: 350, loss is 3.6242113637924196 and perplexity is 37.4951414703667
At time: 98.8325765132904 and batch: 400, loss is 3.555723605155945 and perplexity is 35.013146496715876
At time: 99.4220142364502 and batch: 450, loss is 3.5848917531967164 and perplexity is 36.04945526935618
At time: 100.01588559150696 and batch: 500, loss is 3.4488381576538085 and perplexity is 31.463815071648035
At time: 100.60930728912354 and batch: 550, loss is 3.510535593032837 and perplexity is 33.466187241470394
At time: 101.20463013648987 and batch: 600, loss is 3.5395835924148558 and perplexity is 34.452569892055536
At time: 101.79941916465759 and batch: 650, loss is 3.3737014007568358 and perplexity is 29.186357779103187
At time: 102.40393543243408 and batch: 700, loss is 3.365927495956421 and perplexity is 28.960345449651033
At time: 103.00056648254395 and batch: 750, loss is 3.4628047561645507 and perplexity is 31.906340639391356
At time: 103.59987378120422 and batch: 800, loss is 3.4108783531188966 and perplexity is 30.29183950939002
At time: 104.20117449760437 and batch: 850, loss is 3.4739369106292726 and perplexity is 32.263511302771086
At time: 104.80328226089478 and batch: 900, loss is 3.4309050321578978 and perplexity is 30.904599753378857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.257983063998288 and perplexity of 70.66730818118171
finished 9 epochs...
Completing Train Step...
At time: 106.3379738330841 and batch: 50, loss is 3.71955153465271 and perplexity is 41.245892606727494
At time: 106.9432430267334 and batch: 100, loss is 3.5942901802062988 and perplexity is 36.389860574974456
At time: 107.53602457046509 and batch: 150, loss is 3.600656099319458 and perplexity is 36.62225439926645
At time: 108.126620054245 and batch: 200, loss is 3.493295178413391 and perplexity is 32.89416144725055
At time: 108.73679685592651 and batch: 250, loss is 3.6428087615966795 and perplexity is 38.198978010265755
At time: 109.32784533500671 and batch: 300, loss is 3.6123665285110476 and perplexity is 37.05363762785201
At time: 109.91753506660461 and batch: 350, loss is 3.5987506246566774 and perplexity is 36.55253806386413
At time: 110.50711369514465 and batch: 400, loss is 3.5311242818832396 and perplexity is 34.16235414792638
At time: 111.09642696380615 and batch: 450, loss is 3.5635619258880613 and perplexity is 35.28866917570319
At time: 111.6858320236206 and batch: 500, loss is 3.428770456314087 and perplexity is 30.838701898288964
At time: 112.27579402923584 and batch: 550, loss is 3.4926282119750978 and perplexity is 32.87222946031207
At time: 112.86539793014526 and batch: 600, loss is 3.5247339725494387 and perplexity is 33.944742181773485
At time: 113.4565441608429 and batch: 650, loss is 3.361375117301941 and perplexity is 28.82880662563614
At time: 114.04708957672119 and batch: 700, loss is 3.356074709892273 and perplexity is 28.676406453684145
At time: 114.63689637184143 and batch: 750, loss is 3.4559727621078493 and perplexity is 31.68909964934534
At time: 115.22634601593018 and batch: 800, loss is 3.4066952228546143 and perplexity is 30.16538946167955
At time: 115.81533288955688 and batch: 850, loss is 3.4724723863601685 and perplexity is 32.21629519047206
At time: 116.40743780136108 and batch: 900, loss is 3.4320792818069457 and perplexity is 30.940910783728224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258303707593108 and perplexity of 70.68997083403627
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 117.97504663467407 and batch: 50, loss is 3.710609073638916 and perplexity is 40.8786970829165
At time: 118.56344938278198 and batch: 100, loss is 3.5896699237823486 and perplexity is 36.2221178933843
At time: 119.16759920120239 and batch: 150, loss is 3.5982044172286987 and perplexity is 36.53257824765835
At time: 119.75302052497864 and batch: 200, loss is 3.487750000953674 and perplexity is 32.71226228231276
At time: 120.3573408126831 and batch: 250, loss is 3.6372119855880736 and perplexity is 37.98578404253181
At time: 120.9392364025116 and batch: 300, loss is 3.606718797683716 and perplexity is 36.84495849283221
At time: 121.53916525840759 and batch: 350, loss is 3.5898859548568725 and perplexity is 36.229943841727916
At time: 122.13226532936096 and batch: 400, loss is 3.5262844944000244 and perplexity is 33.99741507091895
At time: 122.73157119750977 and batch: 450, loss is 3.552934823036194 and perplexity is 34.915638487302
At time: 123.32122015953064 and batch: 500, loss is 3.4168925380706785 and perplexity is 30.474569168929786
At time: 123.91435718536377 and batch: 550, loss is 3.4795383262634276 and perplexity is 32.44473973342806
At time: 124.50646209716797 and batch: 600, loss is 3.5128348112106322 and perplexity is 33.54322183324802
At time: 125.10058903694153 and batch: 650, loss is 3.345768246650696 and perplexity is 28.38237195531213
At time: 125.69539308547974 and batch: 700, loss is 3.3370869398117065 and perplexity is 28.137042307297577
At time: 126.29123139381409 and batch: 750, loss is 3.435736365318298 and perplexity is 31.054271436726207
At time: 126.89842987060547 and batch: 800, loss is 3.3856675958633424 and perplexity is 29.537705385299024
At time: 127.49691104888916 and batch: 850, loss is 3.449976682662964 and perplexity is 31.499657812046987
At time: 128.09555339813232 and batch: 900, loss is 3.4081469058990477 and perplexity is 30.20921184650042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.254170456977739 and perplexity of 70.39839446279565
finished 11 epochs...
Completing Train Step...
At time: 129.6363615989685 and batch: 50, loss is 3.701056327819824 and perplexity is 40.49005254697439
At time: 130.2452540397644 and batch: 100, loss is 3.5774174880981446 and perplexity is 35.781016524868384
At time: 130.834303855896 and batch: 150, loss is 3.5865098524093626 and perplexity is 36.10783408316268
At time: 131.4238703250885 and batch: 200, loss is 3.477905411720276 and perplexity is 32.3918034780217
At time: 132.01237893104553 and batch: 250, loss is 3.6275544929504395 and perplexity is 37.6207023372536
At time: 132.6005654335022 and batch: 300, loss is 3.597970371246338 and perplexity is 36.52402894499819
At time: 133.18879318237305 and batch: 350, loss is 3.581098690032959 and perplexity is 35.91297640860063
At time: 133.78749585151672 and batch: 400, loss is 3.517748637199402 and perplexity is 33.70845301460764
At time: 134.3866481781006 and batch: 450, loss is 3.5460194540023804 and perplexity is 34.67501691452062
At time: 134.97470569610596 and batch: 500, loss is 3.410513792037964 and perplexity is 30.280798296355606
At time: 135.57694387435913 and batch: 550, loss is 3.4739944982528685 and perplexity is 32.2653693352152
At time: 136.16818690299988 and batch: 600, loss is 3.5084231519699096 and perplexity is 33.395566510649275
At time: 136.76716303825378 and batch: 650, loss is 3.3424641561508177 and perplexity is 28.288748784658708
At time: 137.3913152217865 and batch: 700, loss is 3.3349139499664306 and perplexity is 28.075967181933805
At time: 138.0226583480835 and batch: 750, loss is 3.4347487354278563 and perplexity is 31.023616450411563
At time: 138.638201713562 and batch: 800, loss is 3.3858626794815065 and perplexity is 29.54346826984176
At time: 139.23495626449585 and batch: 850, loss is 3.451533164978027 and perplexity is 31.548724648287486
At time: 139.84333395957947 and batch: 900, loss is 3.4108782768249513 and perplexity is 30.29183719830616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.253837271912457 and perplexity of 70.37494267625024
finished 12 epochs...
Completing Train Step...
At time: 141.43481540679932 and batch: 50, loss is 3.695733985900879 and perplexity is 40.27512311433221
At time: 142.028888463974 and batch: 100, loss is 3.5716012477874757 and perplexity is 35.57350957431803
At time: 142.61192297935486 and batch: 150, loss is 3.5805170154571533 and perplexity is 35.89209281759802
At time: 143.192711353302 and batch: 200, loss is 3.472201690673828 and perplexity is 32.207575558570745
At time: 143.77372670173645 and batch: 250, loss is 3.621894154548645 and perplexity is 37.40835796855294
At time: 144.3606219291687 and batch: 300, loss is 3.5926504373550414 and perplexity is 36.33023945624889
At time: 144.94836330413818 and batch: 350, loss is 3.5759100770950316 and perplexity is 35.727120458823336
At time: 145.53206300735474 and batch: 400, loss is 3.5128226613998415 and perplexity is 33.54281429192521
At time: 146.12315964698792 and batch: 450, loss is 3.541599054336548 and perplexity is 34.52207775647549
At time: 146.71787452697754 and batch: 500, loss is 3.406440010070801 and perplexity is 30.15769185096633
At time: 147.31168270111084 and batch: 550, loss is 3.4703204917907713 and perplexity is 32.14704365752772
At time: 147.90687584877014 and batch: 600, loss is 3.5053966808319093 and perplexity is 33.29464858209669
At time: 148.49978494644165 and batch: 650, loss is 3.340014591217041 and perplexity is 28.219538459815073
At time: 149.091233253479 and batch: 700, loss is 3.333088788986206 and perplexity is 28.024770757221813
At time: 149.70422339439392 and batch: 750, loss is 3.4335754346847533 and perplexity is 30.987237763919847
At time: 150.3484172821045 and batch: 800, loss is 3.38523491859436 and perplexity is 29.524927856073475
At time: 150.9654610157013 and batch: 850, loss is 3.4515303134918214 and perplexity is 31.548634687662602
At time: 151.56137919425964 and batch: 900, loss is 3.411366648674011 and perplexity is 30.306634491846815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2540174510380995 and perplexity of 70.38762391430168
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 153.1249544620514 and batch: 50, loss is 3.6932662200927733 and perplexity is 40.1758560768708
At time: 153.72199845314026 and batch: 100, loss is 3.5701051139831543 and perplexity is 35.52032663842011
At time: 154.30664825439453 and batch: 150, loss is 3.5798328590393065 and perplexity is 35.867545410037614
At time: 154.89128708839417 and batch: 200, loss is 3.4706624507904054 and perplexity is 32.15803850820497
At time: 155.4769332408905 and batch: 250, loss is 3.621329345703125 and perplexity is 37.38723536275474
At time: 156.0593981742859 and batch: 300, loss is 3.5922301244735717 and perplexity is 36.31497259727205
At time: 156.64217042922974 and batch: 350, loss is 3.5732110643386843 and perplexity is 35.63082251810084
At time: 157.22554922103882 and batch: 400, loss is 3.5114457273483275 and perplexity is 33.496659831859105
At time: 157.8101625442505 and batch: 450, loss is 3.538724966049194 and perplexity is 34.42300070344052
At time: 158.39432430267334 and batch: 500, loss is 3.4030428314208985 and perplexity is 30.055414609474077
At time: 158.97854685783386 and batch: 550, loss is 3.466615386009216 and perplexity is 32.028155842184034
At time: 159.58626985549927 and batch: 600, loss is 3.501784853935242 and perplexity is 33.174610982841365
At time: 160.1802430152893 and batch: 650, loss is 3.334884428977966 and perplexity is 28.075138363864323
At time: 160.76708602905273 and batch: 700, loss is 3.3267963123321533 and perplexity is 27.84897920366253
At time: 161.34950375556946 and batch: 750, loss is 3.426490077972412 and perplexity is 30.768458112038832
At time: 161.93241381645203 and batch: 800, loss is 3.376764941215515 and perplexity is 29.275908468042108
At time: 162.51478600502014 and batch: 850, loss is 3.443256964683533 and perplexity is 31.288698582180924
At time: 163.09895396232605 and batch: 900, loss is 3.4022982215881346 and perplexity is 30.033043382179606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.253157524213399 and perplexity of 70.32712172582656
finished 14 epochs...
Completing Train Step...
At time: 164.58783674240112 and batch: 50, loss is 3.690946283340454 and perplexity is 40.08275866361247
At time: 165.17713713645935 and batch: 100, loss is 3.567317190170288 and perplexity is 35.42143658696079
At time: 165.80975580215454 and batch: 150, loss is 3.57705445766449 and perplexity is 35.768029284447145
At time: 166.39111042022705 and batch: 200, loss is 3.4681920051574706 and perplexity is 32.07869187353722
At time: 166.9666624069214 and batch: 250, loss is 3.6184892749786375 and perplexity is 37.28120361024056
At time: 167.5462954044342 and batch: 300, loss is 3.5897413015365602 and perplexity is 36.22470343908644
At time: 168.1292679309845 and batch: 350, loss is 3.5710008096694947 and perplexity is 35.55215629448236
At time: 168.71015238761902 and batch: 400, loss is 3.509127631187439 and perplexity is 33.419101282119
At time: 169.29101133346558 and batch: 450, loss is 3.536704430580139 and perplexity is 34.35351802931064
At time: 169.87263107299805 and batch: 500, loss is 3.401317276954651 and perplexity is 30.003597074406702
At time: 170.45388436317444 and batch: 550, loss is 3.4650781106948854 and perplexity is 31.978957574174608
At time: 171.03662753105164 and batch: 600, loss is 3.5005358934402464 and perplexity is 33.133203068094495
At time: 171.61965322494507 and batch: 650, loss is 3.3340986490249636 and perplexity is 28.0530861481894
At time: 172.20138263702393 and batch: 700, loss is 3.326413803100586 and perplexity is 27.83832874910586
At time: 172.78198337554932 and batch: 750, loss is 3.4265827322006226 and perplexity is 30.77130907185314
At time: 173.36521458625793 and batch: 800, loss is 3.3773123455047607 and perplexity is 29.291938612993842
At time: 173.94715809822083 and batch: 850, loss is 3.4443829584121706 and perplexity is 31.323949302882493
At time: 174.52941155433655 and batch: 900, loss is 3.4035539627075195 and perplexity is 30.070780798948906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.25290418651006 and perplexity of 70.30930747093258
finished 15 epochs...
Completing Train Step...
At time: 176.0149965286255 and batch: 50, loss is 3.689386258125305 and perplexity is 40.02027729832879
At time: 176.60637187957764 and batch: 100, loss is 3.5654963397979738 and perplexity is 35.35699813515678
At time: 177.1833906173706 and batch: 150, loss is 3.5752116775512697 and perplexity is 35.70217736533138
At time: 177.76435112953186 and batch: 200, loss is 3.4664672088623045 and perplexity is 32.0234103530257
At time: 178.34709239006042 and batch: 250, loss is 3.616667456626892 and perplexity is 37.21334586034374
At time: 178.93084120750427 and batch: 300, loss is 3.588088188171387 and perplexity is 36.164869367559646
At time: 179.51202964782715 and batch: 350, loss is 3.5694475412368774 and perplexity is 35.49697711752948
At time: 180.10286140441895 and batch: 400, loss is 3.5075897455215452 and perplexity is 33.36774602466899
At time: 180.6853895187378 and batch: 450, loss is 3.5353689336776735 and perplexity is 34.307669634402814
At time: 181.2673420906067 and batch: 500, loss is 3.40014612197876 and perplexity is 29.968478780894507
At time: 181.84824085235596 and batch: 550, loss is 3.464021711349487 and perplexity is 31.94519286197672
At time: 182.43115973472595 and batch: 600, loss is 3.499695448875427 and perplexity is 33.105368146153715
At time: 183.01364183425903 and batch: 650, loss is 3.3335103797912597 and perplexity is 28.036588233781483
At time: 183.5952866077423 and batch: 700, loss is 3.326082916259766 and perplexity is 27.82911893623932
At time: 184.17812395095825 and batch: 750, loss is 3.4264894914627075 and perplexity is 30.768440066044853
At time: 184.76059651374817 and batch: 800, loss is 3.3774850749969483 and perplexity is 29.296998631670302
At time: 185.34210062026978 and batch: 850, loss is 3.444880337715149 and perplexity is 31.33953306215169
At time: 185.92302823066711 and batch: 900, loss is 3.4041697216033935 and perplexity is 30.089302851705877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252833954275471 and perplexity of 70.30436966455513
finished 16 epochs...
Completing Train Step...
At time: 187.41416788101196 and batch: 50, loss is 3.6880273962020875 and perplexity is 39.96593219945665
At time: 187.98681831359863 and batch: 100, loss is 3.5639867067337034 and perplexity is 35.30366231061005
At time: 188.56317782402039 and batch: 150, loss is 3.5736655950546266 and perplexity is 35.647021502558715
At time: 189.13921904563904 and batch: 200, loss is 3.4649991941452027 and perplexity is 31.97643400475741
At time: 189.7206814289093 and batch: 250, loss is 3.615174398422241 and perplexity is 37.15782562676849
At time: 190.30658841133118 and batch: 300, loss is 3.5867126846313475 and perplexity is 36.11515865818563
At time: 190.89228057861328 and batch: 350, loss is 3.568130865097046 and perplexity is 35.45026985064241
At time: 191.47854804992676 and batch: 400, loss is 3.506331844329834 and perplexity is 33.325799085263476
At time: 192.06445336341858 and batch: 450, loss is 3.5342485761642455 and perplexity is 34.26925430243085
At time: 192.6501181125641 and batch: 500, loss is 3.399149775505066 and perplexity is 29.938634662749
At time: 193.23635458946228 and batch: 550, loss is 3.463116331100464 and perplexity is 31.91628340430903
At time: 193.8244309425354 and batch: 600, loss is 3.4989687061309813 and perplexity is 33.08131780031699
At time: 194.44716620445251 and batch: 650, loss is 3.332963995933533 and perplexity is 28.021273678738638
At time: 195.03215646743774 and batch: 700, loss is 3.3257242250442505 and perplexity is 27.819138665766037
At time: 195.62084889411926 and batch: 750, loss is 3.426295852661133 and perplexity is 30.762482678993404
At time: 196.20935702323914 and batch: 800, loss is 3.377471294403076 and perplexity is 29.296594904412288
At time: 196.79804062843323 and batch: 850, loss is 3.445084643363953 and perplexity is 31.345936559900263
At time: 197.38444018363953 and batch: 900, loss is 3.404481821060181 and perplexity is 30.098695172373855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252842315255779 and perplexity of 70.30495748046282
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 198.8801188468933 and batch: 50, loss is 3.6873174619674685 and perplexity is 39.937569085133994
At time: 199.47404289245605 and batch: 100, loss is 3.563450965881348 and perplexity is 35.28475376196573
At time: 200.05503487586975 and batch: 150, loss is 3.573342065811157 and perplexity is 35.63549051406663
At time: 200.6348774433136 and batch: 200, loss is 3.4643637132644653 and perplexity is 31.956120047562557
At time: 201.21531653404236 and batch: 250, loss is 3.614838876724243 and perplexity is 37.14536046130397
At time: 201.79513311386108 and batch: 300, loss is 3.5864837694168092 and perplexity is 36.10689229507755
At time: 202.37600874900818 and batch: 350, loss is 3.5672820138931276 and perplexity is 35.42019061460445
At time: 202.95988655090332 and batch: 400, loss is 3.5054964876174926 and perplexity is 33.29797177978488
At time: 203.54538106918335 and batch: 450, loss is 3.5333922052383424 and perplexity is 34.239919671849016
At time: 204.12999510765076 and batch: 500, loss is 3.3980470657348634 and perplexity is 29.905639233334508
At time: 204.71444630622864 and batch: 550, loss is 3.461804094314575 and perplexity is 31.874429150508753
At time: 205.29873156547546 and batch: 600, loss is 3.4979029941558837 and perplexity is 33.04608142303475
At time: 205.8835690021515 and batch: 650, loss is 3.331440544128418 and perplexity is 27.978617119627287
At time: 206.4642825126648 and batch: 700, loss is 3.3236342763900755 and perplexity is 27.76105880745258
At time: 207.04679346084595 and batch: 750, loss is 3.4238504648208616 and perplexity is 30.68734838160368
At time: 207.62882709503174 and batch: 800, loss is 3.374566087722778 and perplexity is 29.211605756483458
At time: 208.21181988716125 and batch: 850, loss is 3.4421462059020995 and perplexity is 31.253963680091186
At time: 208.79461336135864 and batch: 900, loss is 3.4015977287292483 and perplexity is 30.012012816500263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252628274159889 and perplexity of 70.28991094066357
finished 18 epochs...
Completing Train Step...
At time: 210.28694796562195 and batch: 50, loss is 3.686894497871399 and perplexity is 39.9206804992112
At time: 210.8773696422577 and batch: 100, loss is 3.5629509544372557 and perplexity is 35.26711539134384
At time: 211.45389485359192 and batch: 150, loss is 3.5727993631362915 and perplexity is 35.61615628488798
At time: 212.03165197372437 and batch: 200, loss is 3.463852686882019 and perplexity is 31.93979379906478
At time: 212.6084885597229 and batch: 250, loss is 3.61427903175354 and perplexity is 37.12457063814673
At time: 213.1883180141449 and batch: 300, loss is 3.5859780597686766 and perplexity is 36.088637307530554
At time: 213.7703731060028 and batch: 350, loss is 3.5668599510192873 and perplexity is 35.40524422154236
At time: 214.35174083709717 and batch: 400, loss is 3.5050941276550294 and perplexity is 33.28457670411125
At time: 214.9343729019165 and batch: 450, loss is 3.5329816436767576 and perplexity is 34.22586496231917
At time: 215.51923513412476 and batch: 500, loss is 3.397729959487915 and perplexity is 29.896157471756997
At time: 216.1021854877472 and batch: 550, loss is 3.461537742614746 and perplexity is 31.865940472660505
At time: 216.684419631958 and batch: 600, loss is 3.4976413869857788 and perplexity is 33.037437461901014
At time: 217.2683207988739 and batch: 650, loss is 3.3312866926193236 and perplexity is 27.974312898275024
At time: 217.8504147529602 and batch: 700, loss is 3.323600993156433 and perplexity is 27.760134845022446
At time: 218.43371725082397 and batch: 750, loss is 3.423904581069946 and perplexity is 30.689009110728257
At time: 219.01584553718567 and batch: 800, loss is 3.3747041511535643 and perplexity is 29.215639089414456
At time: 219.59917879104614 and batch: 850, loss is 3.4423966884613035 and perplexity is 31.261793233441388
At time: 220.1806206703186 and batch: 900, loss is 3.4018347311019896 and perplexity is 30.019126577704363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252512056533605 and perplexity of 70.28174248873056
finished 19 epochs...
Completing Train Step...
At time: 221.6789288520813 and batch: 50, loss is 3.686503987312317 and perplexity is 39.90509409547621
At time: 222.25560879707336 and batch: 100, loss is 3.5625002813339233 and perplexity is 35.251225031952586
At time: 222.83275294303894 and batch: 150, loss is 3.5723261547088625 and perplexity is 35.59930640664796
At time: 223.4099931716919 and batch: 200, loss is 3.4634118938446044 and perplexity is 31.92571806281287
At time: 224.00579643249512 and batch: 250, loss is 3.6138019132614136 and perplexity is 37.10686204386785
At time: 224.58437204360962 and batch: 300, loss is 3.585542073249817 and perplexity is 36.072906577622724
At time: 225.1658055782318 and batch: 350, loss is 3.56648428440094 and perplexity is 35.391946151150854
At time: 225.7490472793579 and batch: 400, loss is 3.504734420776367 and perplexity is 33.27260616598894
At time: 226.33160042762756 and batch: 450, loss is 3.5326341342926026 and perplexity is 34.21397321942974
At time: 226.91524481773376 and batch: 500, loss is 3.3974474716186522 and perplexity is 29.887713362669004
At time: 227.49826765060425 and batch: 550, loss is 3.46129367351532 and perplexity is 31.858163930312422
At time: 228.08170580863953 and batch: 600, loss is 3.497421383857727 and perplexity is 33.03016992178668
At time: 228.66377878189087 and batch: 650, loss is 3.331149530410767 and perplexity is 27.970476142869973
At time: 229.24642777442932 and batch: 700, loss is 3.323554210662842 and perplexity is 27.758836187069438
At time: 229.82922315597534 and batch: 750, loss is 3.423931374549866 and perplexity is 30.689831387093392
At time: 230.41107320785522 and batch: 800, loss is 3.3747979545593263 and perplexity is 29.218379744401933
At time: 230.9925765991211 and batch: 850, loss is 3.4425844955444336 and perplexity is 31.26766497100167
At time: 231.57661318778992 and batch: 900, loss is 3.4020168924331666 and perplexity is 30.024595399851368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.252446840887201 and perplexity of 70.27715916891746
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f52e60f0b70>
ELAPSED
484.8540737628937


RESULTS SO FAR:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.27715916891746, 'params': {'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.43751170550744745, 'num_layers': 2, 'rnn_dropout': 0.9329965743746119, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8364787101745605 and batch: 50, loss is 6.891295385360718 and perplexity is 983.6748302346938
At time: 1.4725725650787354 and batch: 100, loss is 6.079233646392822 and perplexity is 436.6944041189539
At time: 2.095789670944214 and batch: 150, loss is 5.960762548446655 and perplexity is 387.9058084583724
At time: 2.7222042083740234 and batch: 200, loss is 5.810778493881226 and perplexity is 333.87894724419425
At time: 3.347852945327759 and batch: 250, loss is 5.8728378391265865 and perplexity is 355.25570967694927
At time: 3.9769601821899414 and batch: 300, loss is 5.780662450790405 and perplexity is 323.97373600393644
At time: 4.605998992919922 and batch: 350, loss is 5.773105535507202 and perplexity is 321.53472124796514
At time: 5.233772039413452 and batch: 400, loss is 5.640481204986572 and perplexity is 281.598192331678
At time: 5.862460136413574 and batch: 450, loss is 5.64359712600708 and perplexity is 282.4769984927718
At time: 6.493243932723999 and batch: 500, loss is 5.60401575088501 and perplexity is 271.51455591542066
At time: 7.121915817260742 and batch: 550, loss is 5.644970102310181 and perplexity is 282.8650990833642
At time: 7.751874208450317 and batch: 600, loss is 5.582571239471435 and perplexity is 265.75404534356176
At time: 8.379894971847534 and batch: 650, loss is 5.486782293319703 and perplexity is 241.47894700254977
At time: 9.026713609695435 and batch: 700, loss is 5.586459894180297 and perplexity is 266.78948298833046
At time: 9.652693510055542 and batch: 750, loss is 5.562415895462036 and perplexity is 260.4512999797333
At time: 10.278330326080322 and batch: 800, loss is 5.548499126434326 and perplexity is 256.8517644425418
At time: 10.903443098068237 and batch: 850, loss is 5.594803085327149 and perplexity is 269.02466997127976
At time: 11.531023740768433 and batch: 900, loss is 5.479883985519409 and perplexity is 239.81888329710696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.382699626765839 and perplexity of 217.60894610810354
finished 1 epochs...
Completing Train Step...
At time: 13.003843545913696 and batch: 50, loss is 5.266181993484497 and perplexity is 193.67509625001674
At time: 13.558166265487671 and batch: 100, loss is 5.091918516159057 and perplexity is 162.7017086736446
At time: 14.113057613372803 and batch: 150, loss is 5.03194712638855 and perplexity is 153.23108269612388
At time: 14.66859769821167 and batch: 200, loss is 4.883522186279297 and perplexity is 132.09510905515418
At time: 15.244193315505981 and batch: 250, loss is 4.950927791595459 and perplexity is 141.30600564679804
At time: 15.800917148590088 and batch: 300, loss is 4.872671365737915 and perplexity is 130.6695171408577
At time: 16.350955963134766 and batch: 350, loss is 4.853542766571045 and perplexity is 128.19374682373942
At time: 16.90662145614624 and batch: 400, loss is 4.702460041046143 and perplexity is 110.21797997149692
At time: 17.457846879959106 and batch: 450, loss is 4.706528282165527 and perplexity is 110.66728661390084
At time: 18.00918960571289 and batch: 500, loss is 4.615251808166504 and perplexity is 101.01326129437712
At time: 18.55930209159851 and batch: 550, loss is 4.682525367736816 and perplexity is 108.04257557599105
At time: 19.11076593399048 and batch: 600, loss is 4.625378856658935 and perplexity is 102.04142483378627
At time: 19.66152286529541 and batch: 650, loss is 4.4854327583312985 and perplexity is 88.71533480711665
At time: 20.22103261947632 and batch: 700, loss is 4.532798652648926 and perplexity is 93.0185236776192
At time: 20.79134178161621 and batch: 750, loss is 4.565914077758789 and perplexity is 96.15044284138243
At time: 21.371822595596313 and batch: 800, loss is 4.511701793670654 and perplexity is 91.07668041992909
At time: 21.958887577056885 and batch: 850, loss is 4.570797643661499 and perplexity is 96.62114829061817
At time: 22.549598932266235 and batch: 900, loss is 4.503659763336182 and perplexity is 90.34717627214546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.602613475224743 and perplexity of 99.74465548379564
finished 2 epochs...
Completing Train Step...
At time: 24.036781549453735 and batch: 50, loss is 4.540497026443481 and perplexity is 93.73737849902857
At time: 24.629379510879517 and batch: 100, loss is 4.413402724266052 and perplexity is 82.5498806245243
At time: 25.20902919769287 and batch: 150, loss is 4.405779418945312 and perplexity is 81.92297028087243
At time: 25.79007863998413 and batch: 200, loss is 4.306991262435913 and perplexity is 74.21685364364544
At time: 26.3715181350708 and batch: 250, loss is 4.439470176696777 and perplexity is 84.73003783080149
At time: 26.952914476394653 and batch: 300, loss is 4.403309493064881 and perplexity is 81.72087629769365
At time: 27.53337550163269 and batch: 350, loss is 4.393245983123779 and perplexity is 80.9026017097341
At time: 28.115577220916748 and batch: 400, loss is 4.293185200691223 and perplexity is 73.19925187961667
At time: 28.696179628372192 and batch: 450, loss is 4.323777475357056 and perplexity is 75.47318860393652
At time: 29.277594566345215 and batch: 500, loss is 4.21097888469696 and perplexity is 67.42250637774114
At time: 29.858031272888184 and batch: 550, loss is 4.292186875343322 and perplexity is 73.12621167602752
At time: 30.4386203289032 and batch: 600, loss is 4.281935696601868 and perplexity is 72.38041100855565
At time: 31.019320487976074 and batch: 650, loss is 4.13492377281189 and perplexity is 62.48482784619796
At time: 31.59747290611267 and batch: 700, loss is 4.15846137046814 and perplexity is 63.973016065078454
At time: 32.17807984352112 and batch: 750, loss is 4.243101453781128 and perplexity is 69.6234512462581
At time: 32.76093149185181 and batch: 800, loss is 4.191395487785339 and perplexity is 66.11498927459768
At time: 33.34697461128235 and batch: 850, loss is 4.269484424591065 and perplexity is 71.48477032692283
At time: 33.9355788230896 and batch: 900, loss is 4.215547556877136 and perplexity is 67.73124242683174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431212124759203 and perplexity of 84.03321394166812
finished 3 epochs...
Completing Train Step...
At time: 35.42518138885498 and batch: 50, loss is 4.287127704620361 and perplexity is 72.75718795095102
At time: 36.01887798309326 and batch: 100, loss is 4.158391976356507 and perplexity is 63.968576868488974
At time: 36.60224676132202 and batch: 150, loss is 4.157251391410828 and perplexity is 63.89565686634995
At time: 37.18515467643738 and batch: 200, loss is 4.06030083656311 and perplexity is 57.99175449513009
At time: 37.76832175254822 and batch: 250, loss is 4.208002638816834 and perplexity is 67.2221387404894
At time: 38.366334438323975 and batch: 300, loss is 4.1794635200500485 and perplexity is 65.3307951491644
At time: 38.9514582157135 and batch: 350, loss is 4.167336525917054 and perplexity is 64.5433135230418
At time: 39.53790259361267 and batch: 400, loss is 4.0847043466568 and perplexity is 59.424366141302016
At time: 40.12310791015625 and batch: 450, loss is 4.124653096199036 and perplexity is 61.84635079440677
At time: 40.70882725715637 and batch: 500, loss is 4.001484885215759 and perplexity is 54.67928224003625
At time: 41.29517650604248 and batch: 550, loss is 4.082725157737732 and perplexity is 59.306870405795834
At time: 41.87913799285889 and batch: 600, loss is 4.092136301994324 and perplexity is 59.86765057110592
At time: 42.4625928401947 and batch: 650, loss is 3.9449253463745118 and perplexity is 51.672480430111776
At time: 43.04911828041077 and batch: 700, loss is 3.9521660137176515 and perplexity is 52.04798147022076
At time: 43.636171102523804 and batch: 750, loss is 4.055398015975952 and perplexity is 57.708127182364294
At time: 44.22283935546875 and batch: 800, loss is 4.005456037521363 and perplexity is 54.89685371658186
At time: 44.81108903884888 and batch: 850, loss is 4.089378824234009 and perplexity is 59.70279425436692
At time: 45.398825883865356 and batch: 900, loss is 4.043868532180786 and perplexity is 57.0466031086711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.359750408015839 and perplexity of 78.23760450698548
finished 4 epochs...
Completing Train Step...
At time: 46.903178453445435 and batch: 50, loss is 4.122868309020996 and perplexity is 61.73606666662448
At time: 47.48635816574097 and batch: 100, loss is 3.9973876380920412 and perplexity is 54.45570604438606
At time: 48.06987428665161 and batch: 150, loss is 4.001086277961731 and perplexity is 54.65749102484708
At time: 48.65367794036865 and batch: 200, loss is 3.902660036087036 and perplexity is 49.534036338743
At time: 49.23698163032532 and batch: 250, loss is 4.054768528938293 and perplexity is 57.671812095472816
At time: 49.821213483810425 and batch: 300, loss is 4.028256039619446 and perplexity is 56.1628799531314
At time: 50.40665936470032 and batch: 350, loss is 4.017083168029785 and perplexity is 55.538871780293135
At time: 50.99121689796448 and batch: 400, loss is 3.943842258453369 and perplexity is 51.61654488772567
At time: 51.57539486885071 and batch: 450, loss is 3.9872679471969605 and perplexity is 53.90741010361943
At time: 52.15981483459473 and batch: 500, loss is 3.8620589542388917 and perplexity is 47.56318103409617
At time: 52.75977969169617 and batch: 550, loss is 3.9425858306884765 and perplexity is 51.55173315175923
At time: 53.34837245941162 and batch: 600, loss is 3.956952109336853 and perplexity is 52.297685162540276
At time: 53.936357498168945 and batch: 650, loss is 3.809011721611023 and perplexity is 45.10583970515785
At time: 54.525848627090454 and batch: 700, loss is 3.8151321697235105 and perplexity is 45.38275421264921
At time: 55.11586284637451 and batch: 750, loss is 3.922817449569702 and perplexity is 50.5426457174398
At time: 55.73136115074158 and batch: 800, loss is 3.8743342971801757 and perplexity is 48.150633606939174
At time: 56.32140064239502 and batch: 850, loss is 3.959355802536011 and perplexity is 52.4235439550607
At time: 56.910614252090454 and batch: 900, loss is 3.919608750343323 and perplexity is 50.380729478403296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332815091903895 and perplexity of 76.1583779174811
finished 5 epochs...
Completing Train Step...
At time: 58.40512824058533 and batch: 50, loss is 4.001617636680603 and perplexity is 54.68654147667672
At time: 59.01495981216431 and batch: 100, loss is 3.8783036613464357 and perplexity is 48.34214083608117
At time: 59.602184772491455 and batch: 150, loss is 3.8853100299835206 and perplexity is 48.68203300999982
At time: 60.18958783149719 and batch: 200, loss is 3.7856869506835937 and perplexity is 44.065931285366176
At time: 60.77740263938904 and batch: 250, loss is 3.9380482244491577 and perplexity is 51.318341605688275
At time: 61.364346981048584 and batch: 300, loss is 3.9165952730178835 and perplexity is 50.22913681772217
At time: 61.95241975784302 and batch: 350, loss is 3.9050099849700928 and perplexity is 49.65057566921523
At time: 62.539936780929565 and batch: 400, loss is 3.8347211933135985 and perplexity is 46.28052254988037
At time: 63.128689765930176 and batch: 450, loss is 3.880464344024658 and perplexity is 48.446705787573045
At time: 63.7164409160614 and batch: 500, loss is 3.7565752506256103 and perplexity is 42.80158996555224
At time: 64.30483841896057 and batch: 550, loss is 3.8364725494384766 and perplexity is 46.3616472448906
At time: 64.89280700683594 and batch: 600, loss is 3.8522251892089843 and perplexity is 47.09774811742316
At time: 65.4808087348938 and batch: 650, loss is 3.704282827377319 and perplexity is 40.62090466723676
At time: 66.0688726902008 and batch: 700, loss is 3.7126802730560304 and perplexity is 40.96345275914141
At time: 66.65759515762329 and batch: 750, loss is 3.825195145606995 and perplexity is 45.841745308735945
At time: 67.24609875679016 and batch: 800, loss is 3.7753243398666383 and perplexity is 43.61165101915367
At time: 67.84636116027832 and batch: 850, loss is 3.8646410942077636 and perplexity is 47.68615452394603
At time: 68.4329092502594 and batch: 900, loss is 3.823426399230957 and perplexity is 45.76073455271038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323595856967038 and perplexity of 75.45948252960801
finished 6 epochs...
Completing Train Step...
At time: 69.91533255577087 and batch: 50, loss is 3.905796937942505 and perplexity is 49.68966371552996
At time: 70.50949692726135 and batch: 100, loss is 3.787371096611023 and perplexity is 44.14020727232323
At time: 71.09132146835327 and batch: 150, loss is 3.7913828086853028 and perplexity is 44.31764074283149
At time: 71.67263579368591 and batch: 200, loss is 3.6935131645202635 and perplexity is 40.18577850574247
At time: 72.2549786567688 and batch: 250, loss is 3.8483959913253782 and perplexity is 46.91774637110576
At time: 72.8361325263977 and batch: 300, loss is 3.8288641786575317 and perplexity is 46.01024912136644
At time: 73.4177017211914 and batch: 350, loss is 3.819167819023132 and perplexity is 45.566273152870835
At time: 74.00014138221741 and batch: 400, loss is 3.749205274581909 and perplexity is 42.487302839749134
At time: 74.58166813850403 and batch: 450, loss is 3.791856985092163 and perplexity is 44.33866010552856
At time: 75.1681866645813 and batch: 500, loss is 3.673561577796936 and perplexity is 39.39195382452348
At time: 75.75472903251648 and batch: 550, loss is 3.7510722637176515 and perplexity is 42.56670026656195
At time: 76.34062027931213 and batch: 600, loss is 3.767795820236206 and perplexity is 43.2845526764426
At time: 76.9264087677002 and batch: 650, loss is 3.625334143638611 and perplexity is 37.537263902223415
At time: 77.51250886917114 and batch: 700, loss is 3.634569354057312 and perplexity is 37.885534132028816
At time: 78.09841299057007 and batch: 750, loss is 3.7426301527023313 and perplexity is 42.20886004603696
At time: 78.68392753601074 and batch: 800, loss is 3.6953177738189695 and perplexity is 40.25836360948778
At time: 79.2700617313385 and batch: 850, loss is 3.784026713371277 and perplexity is 43.99283207984929
At time: 79.85568928718567 and batch: 900, loss is 3.7424500083923338 and perplexity is 42.20125704490744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323707058005137 and perplexity of 75.46787416897061
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 81.35028123855591 and batch: 50, loss is 3.8476774883270264 and perplexity is 46.88404793732479
At time: 81.93812656402588 and batch: 100, loss is 3.7216207122802736 and perplexity is 41.33132604293373
At time: 82.53468465805054 and batch: 150, loss is 3.7259114265441893 and perplexity is 41.509047957451166
At time: 83.11833119392395 and batch: 200, loss is 3.6118407917022703 and perplexity is 37.03416228655243
At time: 83.7021312713623 and batch: 250, loss is 3.7613534498214722 and perplexity is 43.00659387302009
At time: 84.28981733322144 and batch: 300, loss is 3.728085412979126 and perplexity is 41.5993862261433
At time: 84.87869358062744 and batch: 350, loss is 3.705741047859192 and perplexity is 40.68018211170254
At time: 85.4651186466217 and batch: 400, loss is 3.628630585670471 and perplexity is 37.66120749090301
At time: 86.05315589904785 and batch: 450, loss is 3.656518521308899 and perplexity is 38.72628317436493
At time: 86.64380955696106 and batch: 500, loss is 3.532372136116028 and perplexity is 34.20501039500597
At time: 87.23393273353577 and batch: 550, loss is 3.595288095474243 and perplexity is 36.42619269761548
At time: 87.8241491317749 and batch: 600, loss is 3.6085453748703005 and perplexity is 36.912320155365194
At time: 88.41618371009827 and batch: 650, loss is 3.450858154296875 and perplexity is 31.527436107977536
At time: 89.00743842124939 and batch: 700, loss is 3.44534152507782 and perplexity is 31.353989792126303
At time: 89.59777283668518 and batch: 750, loss is 3.53903329372406 and perplexity is 34.43361590360522
At time: 90.18602299690247 and batch: 800, loss is 3.4839533615112304 and perplexity is 32.588301083970535
At time: 90.7739109992981 and batch: 850, loss is 3.549823732376099 and perplexity is 34.80718156764627
At time: 91.36176133155823 and batch: 900, loss is 3.5033259534835817 and perplexity is 33.22577577573667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270987314720676 and perplexity of 71.5922848536885
finished 8 epochs...
Completing Train Step...
At time: 92.83041167259216 and batch: 50, loss is 3.7569776916503907 and perplexity is 42.81881854779183
At time: 93.42112922668457 and batch: 100, loss is 3.6315877628326416 and perplexity is 37.77274318772191
At time: 93.99816632270813 and batch: 150, loss is 3.6370341682434084 and perplexity is 37.97903011177916
At time: 94.57629776000977 and batch: 200, loss is 3.5295540285110474 and perplexity is 34.1087526910715
At time: 95.15356492996216 and batch: 250, loss is 3.678975028991699 and perplexity is 39.60577848651941
At time: 95.73269128799438 and batch: 300, loss is 3.654733452796936 and perplexity is 38.65721576905034
At time: 96.31283378601074 and batch: 350, loss is 3.633293399810791 and perplexity is 37.83722475070567
At time: 96.9089503288269 and batch: 400, loss is 3.560836753845215 and perplexity is 35.192632398841255
At time: 97.49396681785583 and batch: 450, loss is 3.595389280319214 and perplexity is 36.42987866275522
At time: 98.07885074615479 and batch: 500, loss is 3.4757172775268557 and perplexity is 32.32100335358033
At time: 98.65947484970093 and batch: 550, loss is 3.540176033973694 and perplexity is 34.472987073664626
At time: 99.2712938785553 and batch: 600, loss is 3.5588216543197633 and perplexity is 35.12178714608557
At time: 99.86084008216858 and batch: 650, loss is 3.405596694946289 and perplexity is 30.13227013407383
At time: 100.44490385055542 and batch: 700, loss is 3.4053345251083376 and perplexity is 30.124371397146202
At time: 101.027170419693 and batch: 750, loss is 3.505003957748413 and perplexity is 33.281575572246
At time: 101.61165022850037 and batch: 800, loss is 3.455436654090881 and perplexity is 31.672115422069815
At time: 102.19508123397827 and batch: 850, loss is 3.5281277894973755 and perplexity is 34.06014013208037
At time: 102.78017115592957 and batch: 900, loss is 3.487615160942078 and perplexity is 32.70785165785828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2740762788955475 and perplexity of 71.81377276479579
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 104.27147459983826 and batch: 50, loss is 3.7309611558914186 and perplexity is 41.71918754257012
At time: 104.87247371673584 and batch: 100, loss is 3.613537435531616 and perplexity is 37.09704940290428
At time: 105.45864129066467 and batch: 150, loss is 3.6189568424224854 and perplexity is 37.29863916314698
At time: 106.04607892036438 and batch: 200, loss is 3.5094296979904174 and perplexity is 33.42919760800737
At time: 106.63296270370483 and batch: 250, loss is 3.657182550430298 and perplexity is 38.7520070939269
At time: 107.21984553337097 and batch: 300, loss is 3.629681487083435 and perplexity is 37.70080651075768
At time: 107.80666613578796 and batch: 350, loss is 3.6056583642959597 and perplexity is 36.80590757778245
At time: 108.39115929603577 and batch: 400, loss is 3.5312917375564576 and perplexity is 34.16807530694583
At time: 108.97649383544922 and batch: 450, loss is 3.5580893039703367 and perplexity is 35.09607510925542
At time: 109.56284499168396 and batch: 500, loss is 3.431924238204956 and perplexity is 30.93611396533909
At time: 110.14726948738098 and batch: 550, loss is 3.4955476093292237 and perplexity is 32.96833677949013
At time: 110.7301914691925 and batch: 600, loss is 3.5145914125442506 and perplexity is 33.602195683061396
At time: 111.31374502182007 and batch: 650, loss is 3.353046908378601 and perplexity is 28.589711300906483
At time: 111.91155409812927 and batch: 700, loss is 3.3471013736724853 and perplexity is 28.420234494487747
At time: 112.4997570514679 and batch: 750, loss is 3.445386457443237 and perplexity is 31.355398632703967
At time: 113.09070181846619 and batch: 800, loss is 3.3920399236679075 and perplexity is 29.726530314822618
At time: 113.68060731887817 and batch: 850, loss is 3.4591108846664427 and perplexity is 31.788700125329505
At time: 114.27366590499878 and batch: 900, loss is 3.422298345565796 and perplexity is 30.63975490222625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262536871923159 and perplexity of 70.98984736256777
finished 10 epochs...
Completing Train Step...
At time: 115.77688002586365 and batch: 50, loss is 3.705670466423035 and perplexity is 40.67731094735261
At time: 116.3582375049591 and batch: 100, loss is 3.581677803993225 and perplexity is 35.93378013787715
At time: 116.93946075439453 and batch: 150, loss is 3.5876249170303343 and perplexity is 36.14811910751734
At time: 117.51891446113586 and batch: 200, loss is 3.4801193571090696 and perplexity is 32.463596605673715
At time: 118.09764432907104 and batch: 250, loss is 3.627957830429077 and perplexity is 37.63587923697936
At time: 118.67735552787781 and batch: 300, loss is 3.6035302972793577 and perplexity is 36.72766542165369
At time: 119.25742435455322 and batch: 350, loss is 3.579728155136108 and perplexity is 35.86379013463447
At time: 119.83804130554199 and batch: 400, loss is 3.5080220699310303 and perplexity is 33.38217483450356
At time: 120.41891741752625 and batch: 450, loss is 3.5371968221664427 and perplexity is 34.370437577728005
At time: 120.9987211227417 and batch: 500, loss is 3.413324475288391 and perplexity is 30.366027749325728
At time: 121.5780291557312 and batch: 550, loss is 3.478341579437256 and perplexity is 32.40593481858682
At time: 122.15951323509216 and batch: 600, loss is 3.4993931961059572 and perplexity is 33.09536346899408
At time: 122.74073767662048 and batch: 650, loss is 3.3403459548950196 and perplexity is 28.2288909393204
At time: 123.32711577415466 and batch: 700, loss is 3.3380650377273557 and perplexity is 28.164576553129997
At time: 123.9083366394043 and batch: 750, loss is 3.4387921619415285 and perplexity is 31.1493121130517
At time: 124.49075984954834 and batch: 800, loss is 3.3883019256591798 and perplexity is 29.61562002406476
At time: 125.07241177558899 and batch: 850, loss is 3.458493366241455 and perplexity is 31.769076077025016
At time: 125.65731883049011 and batch: 900, loss is 3.4240655946731566 and perplexity is 30.6939508564976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262826579890839 and perplexity of 71.01041666637481
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 127.14103579521179 and batch: 50, loss is 3.6984038829803465 and perplexity is 40.382797223343395
At time: 127.73438882827759 and batch: 100, loss is 3.5786998462677 and perplexity is 35.826930036215394
At time: 128.31351017951965 and batch: 150, loss is 3.5859201622009276 and perplexity is 36.08654792369278
At time: 128.89253854751587 and batch: 200, loss is 3.478895664215088 and perplexity is 32.42389542917217
At time: 129.47789478302002 and batch: 250, loss is 3.6259907722473144 and perplexity is 37.56192003767013
At time: 130.06262850761414 and batch: 300, loss is 3.601859049797058 and perplexity is 36.66633566616159
At time: 130.64742469787598 and batch: 350, loss is 3.573419690132141 and perplexity is 35.63825680218494
At time: 131.23184609413147 and batch: 400, loss is 3.502733783721924 and perplexity is 33.2061063004237
At time: 131.81392407417297 and batch: 450, loss is 3.5292747116088865 and perplexity is 34.09922687035656
At time: 132.40098524093628 and batch: 500, loss is 3.4017338228225706 and perplexity is 30.016097552121195
At time: 132.98790550231934 and batch: 550, loss is 3.4647470903396607 and perplexity is 31.96837364012414
At time: 133.57644367218018 and batch: 600, loss is 3.488825035095215 and perplexity is 32.74744799064349
At time: 134.1638081073761 and batch: 650, loss is 3.324784655570984 and perplexity is 27.793012927698054
At time: 134.75185370445251 and batch: 700, loss is 3.3194275379180906 and perplexity is 27.644520588110893
At time: 135.35357975959778 and batch: 750, loss is 3.419538769721985 and perplexity is 30.555318732328775
At time: 135.963534116745 and batch: 800, loss is 3.3682580757141114 and perplexity is 29.02791855620301
At time: 136.5564467906952 and batch: 850, loss is 3.435421485900879 and perplexity is 31.044494625166976
At time: 137.1507203578949 and batch: 900, loss is 3.404305920600891 and perplexity is 30.093401263683145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259122247565283 and perplexity of 70.74785708867039
finished 12 epochs...
Completing Train Step...
At time: 138.64879250526428 and batch: 50, loss is 3.689506211280823 and perplexity is 40.02507814480801
At time: 139.25167798995972 and batch: 100, loss is 3.5667163038253786 and perplexity is 35.40015872282784
At time: 139.84238958358765 and batch: 150, loss is 3.5735720920562746 and perplexity is 35.643688554988636
At time: 140.43370270729065 and batch: 200, loss is 3.4676024293899537 and perplexity is 32.059784628329425
At time: 141.02434754371643 and batch: 250, loss is 3.614760537147522 and perplexity is 37.1424506234675
At time: 141.62712216377258 and batch: 300, loss is 3.5916026544570925 and perplexity is 36.29219318826346
At time: 142.21661591529846 and batch: 350, loss is 3.563767070770264 and perplexity is 35.29590920818674
At time: 142.80684065818787 and batch: 400, loss is 3.4943631792068484 and perplexity is 32.92931120441955
At time: 143.39508247375488 and batch: 450, loss is 3.522047142982483 and perplexity is 33.85366085962488
At time: 143.98379802703857 and batch: 500, loss is 3.395970301628113 and perplexity is 29.843596721315162
At time: 144.57397747039795 and batch: 550, loss is 3.459350972175598 and perplexity is 31.79633311141752
At time: 145.1636610031128 and batch: 600, loss is 3.4843043088912964 and perplexity is 32.59973986994534
At time: 145.75334811210632 and batch: 650, loss is 3.321625714302063 and perplexity is 27.705354958255942
At time: 146.3424437046051 and batch: 700, loss is 3.3176637935638427 and perplexity is 27.595805693936008
At time: 146.9321804046631 and batch: 750, loss is 3.418728084564209 and perplexity is 30.530558026865656
At time: 147.5221927165985 and batch: 800, loss is 3.368792200088501 and perplexity is 29.043427216449693
At time: 148.11136054992676 and batch: 850, loss is 3.4375462245941164 and perplexity is 31.110526189176884
At time: 148.69951963424683 and batch: 900, loss is 3.407644090652466 and perplexity is 30.194026012351195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258886049871576 and perplexity of 70.73114858132446
finished 13 epochs...
Completing Train Step...
At time: 150.18209052085876 and batch: 50, loss is 3.684571876525879 and perplexity is 39.82806746869078
At time: 150.75949001312256 and batch: 100, loss is 3.56111364364624 and perplexity is 35.20237822902195
At time: 151.33907675743103 and batch: 150, loss is 3.567603087425232 and perplexity is 35.43156492621038
At time: 151.91820549964905 and batch: 200, loss is 3.4618509149551393 and perplexity is 31.875921566636865
At time: 152.4957356452942 and batch: 250, loss is 3.608870620727539 and perplexity is 36.92432768717093
At time: 153.07227492332458 and batch: 300, loss is 3.5861304664611815 and perplexity is 36.094137876530716
At time: 153.64867734909058 and batch: 350, loss is 3.558375005722046 and perplexity is 35.10610355189585
At time: 154.22506189346313 and batch: 400, loss is 3.4893888330459593 and perplexity is 32.765916140376
At time: 154.80296230316162 and batch: 450, loss is 3.5175787448883056 and perplexity is 33.702726694064204
At time: 155.38077044487 and batch: 500, loss is 3.392126269340515 and perplexity is 29.729097182894318
At time: 155.9787564277649 and batch: 550, loss is 3.4558056020736694 and perplexity is 31.683802941076127
At time: 156.5556333065033 and batch: 600, loss is 3.4813379764556887 and perplexity is 32.503181487110666
At time: 157.13202142715454 and batch: 650, loss is 3.31920440196991 and perplexity is 27.6383527899518
At time: 157.70902848243713 and batch: 700, loss is 3.316080708503723 and perplexity is 27.55215374770849
At time: 158.28845381736755 and batch: 750, loss is 3.417682576179504 and perplexity is 30.498654752935135
At time: 158.86770343780518 and batch: 800, loss is 3.368412528038025 and perplexity is 29.03240233193827
At time: 159.4477686882019 and batch: 850, loss is 3.4378418207168577 and perplexity is 31.119723699402055
At time: 160.02608275413513 and batch: 900, loss is 3.408480000495911 and perplexity is 30.219276047814468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259053687526755 and perplexity of 70.74300677913327
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 161.4892120361328 and batch: 50, loss is 3.682536106109619 and perplexity is 39.74706914217352
At time: 162.0757441520691 and batch: 100, loss is 3.5602920293807983 and perplexity is 35.17346733132086
At time: 162.65199851989746 and batch: 150, loss is 3.5675531339645388 and perplexity is 35.429795041130845
At time: 163.2331726551056 and batch: 200, loss is 3.461875514984131 and perplexity is 31.876705724876658
At time: 163.815096616745 and batch: 250, loss is 3.6091202211380007 and perplexity is 36.93354516481291
At time: 164.3964138031006 and batch: 300, loss is 3.5862576866149904 and perplexity is 36.09873007040659
At time: 164.97766280174255 and batch: 350, loss is 3.5565626430511474 and perplexity is 35.04253618126227
At time: 165.5600609779358 and batch: 400, loss is 3.4878498935699462 and perplexity is 32.715530158991974
At time: 166.14523720741272 and batch: 450, loss is 3.5152800607681276 and perplexity is 33.62534374495796
At time: 166.73119473457336 and batch: 500, loss is 3.3889202737808226 and perplexity is 29.633938450076407
At time: 167.317809343338 and batch: 550, loss is 3.451582307815552 and perplexity is 31.55027508023299
At time: 167.90473079681396 and batch: 600, loss is 3.4772361803054808 and perplexity is 32.37013311760411
At time: 168.48850011825562 and batch: 650, loss is 3.3137769651412965 and perplexity is 27.488753713135957
At time: 169.07412600517273 and batch: 700, loss is 3.310034852027893 and perplexity is 27.386079915692072
At time: 169.66008949279785 and batch: 750, loss is 3.4109837436676025 and perplexity is 30.295032151211405
At time: 170.24502539634705 and batch: 800, loss is 3.3606708955764772 and perplexity is 28.808511900523076
At time: 170.8410439491272 and batch: 850, loss is 3.4283568239212037 and perplexity is 30.825948649989492
At time: 171.4258782863617 and batch: 900, loss is 3.3990800619125365 and perplexity is 29.9365476057202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2583028714950775 and perplexity of 70.68991173031557
finished 15 epochs...
Completing Train Step...
At time: 172.90130734443665 and batch: 50, loss is 3.6800040292739866 and perplexity is 39.646553819078726
At time: 173.492693901062 and batch: 100, loss is 3.5573363733291625 and perplexity is 35.06966014448679
At time: 174.07481718063354 and batch: 150, loss is 3.564850130081177 and perplexity is 35.33415748012651
At time: 174.656188249588 and batch: 200, loss is 3.458863768577576 and perplexity is 31.78084559661455
At time: 175.2379801273346 and batch: 250, loss is 3.6060151767730715 and perplexity is 36.81904272809103
At time: 175.8201825618744 and batch: 300, loss is 3.5833176612854003 and perplexity is 35.99275475103052
At time: 176.402352809906 and batch: 350, loss is 3.5540287113189697 and perplexity is 34.95385319262747
At time: 176.98337244987488 and batch: 400, loss is 3.485497918128967 and perplexity is 32.63867445231584
At time: 177.56492495536804 and batch: 450, loss is 3.513377208709717 and perplexity is 33.56142052789868
At time: 178.14686012268066 and batch: 500, loss is 3.3873626518249513 and perplexity is 29.587815906975685
At time: 178.72789573669434 and batch: 550, loss is 3.4500224924087526 and perplexity is 31.501100836415823
At time: 179.30914330482483 and batch: 600, loss is 3.4762210035324097 and perplexity is 32.33728838474763
At time: 179.89088344573975 and batch: 650, loss is 3.3131194257736207 and perplexity is 27.470684716590533
At time: 180.47397541999817 and batch: 700, loss is 3.3095807647705078 and perplexity is 27.373647068784933
At time: 181.0576651096344 and batch: 750, loss is 3.4110441541671754 and perplexity is 30.296862344519127
At time: 181.64017033576965 and batch: 800, loss is 3.361044020652771 and perplexity is 28.819263084367286
At time: 182.22159361839294 and batch: 850, loss is 3.4294727563858034 and perplexity is 30.86036752783986
At time: 182.80448627471924 and batch: 900, loss is 3.4008460092544555 and perplexity is 29.989460679490648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.25798013765518 and perplexity of 70.667101384694
finished 16 epochs...
Completing Train Step...
At time: 184.28817915916443 and batch: 50, loss is 3.6784748315811155 and perplexity is 39.585972732482695
At time: 184.8656907081604 and batch: 100, loss is 3.5555174016952513 and perplexity is 35.00592740906449
At time: 185.45507311820984 and batch: 150, loss is 3.5629291009902953 and perplexity is 35.266344691729444
At time: 186.0342309474945 and batch: 200, loss is 3.456972827911377 and perplexity is 31.720806686168267
At time: 186.6170654296875 and batch: 250, loss is 3.604040460586548 and perplexity is 36.746407309215755
At time: 187.19952630996704 and batch: 300, loss is 3.5814665269851687 and perplexity is 35.926188958270565
At time: 187.78133463859558 and batch: 350, loss is 3.552380919456482 and perplexity is 34.89630394538627
At time: 188.3631784915924 and batch: 400, loss is 3.4840014791488647 and perplexity is 32.589869193759775
At time: 188.94794511795044 and batch: 450, loss is 3.5120657539367675 and perplexity is 33.517435091519914
At time: 189.53183794021606 and batch: 500, loss is 3.386259388923645 and perplexity is 29.555190767716986
At time: 190.116637468338 and batch: 550, loss is 3.4490198516845703 and perplexity is 31.469532378416073
At time: 190.7002203464508 and batch: 600, loss is 3.475510845184326 and perplexity is 32.31433194176642
At time: 191.28419947624207 and batch: 650, loss is 3.3126259660720825 and perplexity is 27.45713238474905
At time: 191.8691942691803 and batch: 700, loss is 3.3092934036254884 and perplexity is 27.36578207632063
At time: 192.45408034324646 and batch: 750, loss is 3.411019597053528 and perplexity is 30.296118350162587
At time: 193.03851509094238 and batch: 800, loss is 3.361224889755249 and perplexity is 28.824476070035235
At time: 193.62193274497986 and batch: 850, loss is 3.4300419378280638 and perplexity is 30.877937676164876
At time: 194.20684385299683 and batch: 900, loss is 3.4017057752609254 and perplexity is 30.01525568558096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.257869354666096 and perplexity of 70.65927310560082
finished 17 epochs...
Completing Train Step...
At time: 195.6817045211792 and batch: 50, loss is 3.677156505584717 and perplexity is 39.53381990030735
At time: 196.2696487903595 and batch: 100, loss is 3.554031352996826 and perplexity is 34.95394552956941
At time: 196.84573101997375 and batch: 150, loss is 3.5613525390625 and perplexity is 35.21078892042012
At time: 197.42250871658325 and batch: 200, loss is 3.455450735092163 and perplexity is 31.672561400307575
At time: 197.99972558021545 and batch: 250, loss is 3.6024649286270143 and perplexity is 36.68855775398027
At time: 198.58003950119019 and batch: 300, loss is 3.5799893140792847 and perplexity is 35.87315750729766
At time: 199.1608190536499 and batch: 350, loss is 3.5510220384597777 and perplexity is 34.84891622552768
At time: 199.75324726104736 and batch: 400, loss is 3.482759051322937 and perplexity is 32.549403776315714
At time: 200.33412051200867 and batch: 450, loss is 3.510960216522217 and perplexity is 33.48040078816244
At time: 200.9467134475708 and batch: 500, loss is 3.3853115463256835 and perplexity is 29.527190370997154
At time: 201.53768181800842 and batch: 550, loss is 3.44816349029541 and perplexity is 31.442594621825844
At time: 202.12188243865967 and batch: 600, loss is 3.474869341850281 and perplexity is 32.29360883776916
At time: 202.7075695991516 and batch: 650, loss is 3.312134265899658 and perplexity is 27.443635026619784
At time: 203.28988981246948 and batch: 700, loss is 3.3089929151535036 and perplexity is 27.357560209629792
At time: 203.8779284954071 and batch: 750, loss is 3.4108910512924195 and perplexity is 30.29222416286663
At time: 204.47031354904175 and batch: 800, loss is 3.3612582445144654 and perplexity is 28.82543751952846
At time: 205.06058835983276 and batch: 850, loss is 3.4303271198272705 and perplexity is 30.8867447639145
At time: 205.65133595466614 and batch: 900, loss is 3.402164258956909 and perplexity is 30.02902034613869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.257856395146618 and perplexity of 70.65835740130827
finished 18 epochs...
Completing Train Step...
At time: 207.13702630996704 and batch: 50, loss is 3.675940980911255 and perplexity is 39.48579476056658
At time: 207.7306830883026 and batch: 100, loss is 3.5527041435241697 and perplexity is 34.90758509376589
At time: 208.3121678829193 and batch: 150, loss is 3.5599624061584474 and perplexity is 35.16187525029236
At time: 208.89453291893005 and batch: 200, loss is 3.454115295410156 and perplexity is 31.63029283482444
At time: 209.47795629501343 and batch: 250, loss is 3.6010892724990846 and perplexity is 36.63812161402751
At time: 210.06138730049133 and batch: 300, loss is 3.5787010192871094 and perplexity is 35.826972061924344
At time: 210.64278173446655 and batch: 350, loss is 3.549806227684021 and perplexity is 34.80657228398351
At time: 211.22592520713806 and batch: 400, loss is 3.4816428279876708 and perplexity is 32.51309164226754
At time: 211.80916380882263 and batch: 450, loss is 3.5099579906463623 and perplexity is 33.44686267334911
At time: 212.39141416549683 and batch: 500, loss is 3.3844381761550903 and perplexity is 29.501413461736018
At time: 212.97334361076355 and batch: 550, loss is 3.447365989685059 and perplexity is 31.41752912962506
At time: 213.5544250011444 and batch: 600, loss is 3.4742489671707153 and perplexity is 32.27358091358528
At time: 214.13597559928894 and batch: 650, loss is 3.311629490852356 and perplexity is 27.429785660152454
At time: 214.7287917137146 and batch: 700, loss is 3.3086605644226075 and perplexity is 27.348469415248346
At time: 215.31090760231018 and batch: 750, loss is 3.4106900262832642 and perplexity is 30.286135280256303
At time: 216.03466296195984 and batch: 800, loss is 3.3611929273605345 and perplexity is 28.823554785476954
At time: 216.61172366142273 and batch: 850, loss is 3.430449004173279 and perplexity is 30.89050960403326
At time: 217.1873800754547 and batch: 900, loss is 3.402414116859436 and perplexity is 30.036524271595813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.257894019558005 and perplexity of 70.66101593042757
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 218.65740776062012 and batch: 50, loss is 3.675349087715149 and perplexity is 39.46243030261893
At time: 219.2262454032898 and batch: 100, loss is 3.5524126148223876 and perplexity is 34.89741001403711
At time: 219.7944905757904 and batch: 150, loss is 3.559983730316162 and perplexity is 35.1626250556602
At time: 220.36297345161438 and batch: 200, loss is 3.454059906005859 and perplexity is 31.628540900266334
At time: 220.9487748146057 and batch: 250, loss is 3.60113835811615 and perplexity is 36.63992006297367
At time: 221.5202066898346 and batch: 300, loss is 3.578556470870972 and perplexity is 35.82179370412852
At time: 222.09679794311523 and batch: 350, loss is 3.549152855873108 and perplexity is 34.783838078571875
At time: 222.67675685882568 and batch: 400, loss is 3.4807906436920164 and perplexity is 32.48539629861136
At time: 223.2567286491394 and batch: 450, loss is 3.5092459392547606 and perplexity is 33.42305526529275
At time: 223.8354353904724 and batch: 500, loss is 3.3834121561050416 and perplexity is 29.47115994303192
At time: 224.41841626167297 and batch: 550, loss is 3.4458703565597535 and perplexity is 31.370575154056844
At time: 225.00341844558716 and batch: 600, loss is 3.4726103734970093 and perplexity is 32.2207409315255
At time: 225.589102268219 and batch: 650, loss is 3.3097264432907103 and perplexity is 27.377635111661498
At time: 226.1738395690918 and batch: 700, loss is 3.3066784381866454 and perplexity is 27.29431498469604
At time: 226.75889682769775 and batch: 750, loss is 3.408464260101318 and perplexity is 30.21880038822872
At time: 227.34418725967407 and batch: 800, loss is 3.358713321685791 and perplexity is 28.75217227228129
At time: 227.9290931224823 and batch: 850, loss is 3.4274492359161375 and perplexity is 30.79798408082332
At time: 228.51210951805115 and batch: 900, loss is 3.3991064548492433 and perplexity is 29.937337729553178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.257687921393408 and perplexity of 70.64645432534728
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f52e60f0b70>
ELAPSED
719.8251242637634


RESULTS SO FAR:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.27715916891746, 'params': {'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.64645432534728, 'params': {'batch_size': 32, 'dropout': 0.43751170550744745, 'num_layers': 2, 'rnn_dropout': 0.9329965743746119, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.7029347552002905, 'num_layers': 2, 'rnn_dropout': 0.44946109581267146, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8474667072296143 and batch: 50, loss is 6.943435955047607 and perplexity is 1036.3248703143888
At time: 1.4710729122161865 and batch: 100, loss is 6.072253742218018 and perplexity is 433.65693198862135
At time: 2.0954792499542236 and batch: 150, loss is 5.9521861743927005 and perplexity is 384.59320850010783
At time: 2.721615791320801 and batch: 200, loss is 5.792203168869019 and perplexity is 327.7342835271476
At time: 3.346677780151367 and batch: 250, loss is 5.839220876693726 and perplexity is 343.51159848406945
At time: 3.9739747047424316 and batch: 300, loss is 5.738157539367676 and perplexity is 310.49181470507216
At time: 4.600881099700928 and batch: 350, loss is 5.722422304153443 and perplexity is 305.64439066446954
At time: 5.227758884429932 and batch: 400, loss is 5.5851822853088375 and perplexity is 266.4488480235133
At time: 5.853320837020874 and batch: 450, loss is 5.581127672195435 and perplexity is 265.3706882676766
At time: 6.478267669677734 and batch: 500, loss is 5.532764272689819 and perplexity is 252.84186978038738
At time: 7.102600336074829 and batch: 550, loss is 5.5755476379394535 and perplexity is 263.89403445755073
At time: 7.7284650802612305 and batch: 600, loss is 5.500738201141357 and perplexity is 244.8726308156313
At time: 8.354426383972168 and batch: 650, loss is 5.402350053787232 and perplexity is 221.92734505676984
At time: 8.981252908706665 and batch: 700, loss is 5.499840002059937 and perplexity is 244.65278519091711
At time: 9.607747316360474 and batch: 750, loss is 5.467911252975464 and perplexity is 236.96471613289899
At time: 10.235216617584229 and batch: 800, loss is 5.45214262008667 and perplexity is 233.25741288657886
At time: 10.861311197280884 and batch: 850, loss is 5.484525051116943 and perplexity is 240.9344852544913
At time: 11.488177061080933 and batch: 900, loss is 5.379642648696899 and perplexity is 216.94473608723933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.219699807363014 and perplexity of 184.87867652219393
finished 1 epochs...
Completing Train Step...
At time: 12.954287767410278 and batch: 50, loss is 5.12107138633728 and perplexity is 167.51474652368273
At time: 13.521117210388184 and batch: 100, loss is 4.95099422454834 and perplexity is 141.3153933338354
At time: 14.0758376121521 and batch: 150, loss is 4.909059677124024 and perplexity is 135.51192951208867
At time: 14.632094144821167 and batch: 200, loss is 4.775708208084106 and perplexity is 118.59427431054397
At time: 15.189343452453613 and batch: 250, loss is 4.858029432296753 and perplexity is 128.77020152663198
At time: 15.74595856666565 and batch: 300, loss is 4.785448598861694 and perplexity is 119.75507302529012
At time: 16.302613258361816 and batch: 350, loss is 4.762756834030151 and perplexity is 117.06821909068644
At time: 16.859631538391113 and batch: 400, loss is 4.6237178230285645 and perplexity is 101.87207128535373
At time: 17.417144298553467 and batch: 450, loss is 4.638366374969483 and perplexity is 103.37533303608566
At time: 17.973586320877075 and batch: 500, loss is 4.53656572341919 and perplexity is 93.36959187346567
At time: 18.53077507019043 and batch: 550, loss is 4.601779251098633 and perplexity is 99.66148078373493
At time: 19.088740825653076 and batch: 600, loss is 4.557622804641723 and perplexity is 95.35652908549056
At time: 19.657345294952393 and batch: 650, loss is 4.414776945114136 and perplexity is 82.66340037422371
At time: 20.23326301574707 and batch: 700, loss is 4.45884654045105 and perplexity is 86.38780676545433
At time: 20.817513704299927 and batch: 750, loss is 4.501990041732788 and perplexity is 90.19644751267016
At time: 21.39900493621826 and batch: 800, loss is 4.440040540695191 and perplexity is 84.77837857856498
At time: 21.981992959976196 and batch: 850, loss is 4.51101019859314 and perplexity is 91.01371401221292
At time: 22.56432819366455 and batch: 900, loss is 4.442613773345947 and perplexity is 84.99681399243975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.547333181720891 and perplexity of 94.38037708874296
finished 2 epochs...
Completing Train Step...
At time: 24.034168004989624 and batch: 50, loss is 4.477736759185791 and perplexity is 88.03520217325963
At time: 24.618549585342407 and batch: 100, loss is 4.345231213569641 and perplexity is 77.10986426529662
At time: 25.196969509124756 and batch: 150, loss is 4.343136434555054 and perplexity is 76.94850520460801
At time: 25.779134035110474 and batch: 200, loss is 4.243163194656372 and perplexity is 69.62774999177836
At time: 26.359473705291748 and batch: 250, loss is 4.374467058181763 and perplexity is 79.39751401925493
At time: 26.942858934402466 and batch: 300, loss is 4.341688218116761 and perplexity is 76.83714776876597
At time: 27.52594542503357 and batch: 350, loss is 4.331730442047119 and perplexity is 76.0758175264329
At time: 28.122889518737793 and batch: 400, loss is 4.233256831169128 and perplexity is 68.94139743350638
At time: 28.707733631134033 and batch: 450, loss is 4.268186993598938 and perplexity is 71.392083910552
At time: 29.291758060455322 and batch: 500, loss is 4.148251900672912 and perplexity is 63.32320823087048
At time: 29.876817226409912 and batch: 550, loss is 4.229660458564759 and perplexity is 68.69390378690944
At time: 30.462345361709595 and batch: 600, loss is 4.225698494911194 and perplexity is 68.4222794750205
At time: 31.04815149307251 and batch: 650, loss is 4.073730430603027 and perplexity is 58.775813226922914
At time: 31.633626699447632 and batch: 700, loss is 4.094378609657287 and perplexity is 60.00204288088842
At time: 32.21774506568909 and batch: 750, loss is 4.184325079917908 and perplexity is 65.6491780125976
At time: 32.80209422111511 and batch: 800, loss is 4.133830389976501 and perplexity is 62.41654534421911
At time: 33.38658928871155 and batch: 850, loss is 4.217138118743897 and perplexity is 67.83905887986327
At time: 33.974292278289795 and batch: 900, loss is 4.159574708938599 and perplexity is 64.04427934765097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.399585044547303 and perplexity of 81.41707719435104
finished 3 epochs...
Completing Train Step...
At time: 35.468111753463745 and batch: 50, loss is 4.225030832290649 and perplexity is 68.3766117236243
At time: 36.051055669784546 and batch: 100, loss is 4.09613865852356 and perplexity is 60.107742399307675
At time: 36.63184380531311 and batch: 150, loss is 4.103980751037597 and perplexity is 60.58096598236619
At time: 37.21256422996521 and batch: 200, loss is 4.000166521072388 and perplexity is 54.607242532663534
At time: 37.7958025932312 and batch: 250, loss is 4.146316213607788 and perplexity is 63.20075287163877
At time: 38.38167405128479 and batch: 300, loss is 4.119046354293824 and perplexity is 61.50056454069421
At time: 38.96730089187622 and batch: 350, loss is 4.111791296005249 and perplexity is 61.05598902087612
At time: 39.55273699760437 and batch: 400, loss is 4.027929496765137 and perplexity is 56.14454336000993
At time: 40.1369104385376 and batch: 450, loss is 4.068364071846008 and perplexity is 58.46124592232022
At time: 40.72108745574951 and batch: 500, loss is 3.9447335815429687 and perplexity is 51.66257241564141
At time: 41.30699968338013 and batch: 550, loss is 4.027682838439941 and perplexity is 56.130696548761705
At time: 41.892651081085205 and batch: 600, loss is 4.0394973993301395 and perplexity is 56.797789023685276
At time: 42.496914863586426 and batch: 650, loss is 3.89082124710083 and perplexity is 48.9510709459001
At time: 43.08177351951599 and batch: 700, loss is 3.8960770082473757 and perplexity is 49.209023357004114
At time: 43.666144371032715 and batch: 750, loss is 4.000985336303711 and perplexity is 54.65197408552902
At time: 44.251519203186035 and batch: 800, loss is 3.952563943862915 and perplexity is 52.068697052451846
At time: 44.835511207580566 and batch: 850, loss is 4.0396728467941285 and perplexity is 56.807754925950235
At time: 45.42370843887329 and batch: 900, loss is 3.989619326591492 and perplexity is 54.03431602045684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347569295804795 and perplexity of 77.29036439894114
finished 4 epochs...
Completing Train Step...
At time: 46.91509222984314 and batch: 50, loss is 4.061755657196045 and perplexity is 58.076183495744566
At time: 47.514965534210205 and batch: 100, loss is 3.9388948011398317 and perplexity is 51.36180491241106
At time: 48.101988077163696 and batch: 150, loss is 3.945419149398804 and perplexity is 51.698002758203614
At time: 48.6884491443634 and batch: 200, loss is 3.8436546182632445 and perplexity is 46.69581836980573
At time: 49.27479100227356 and batch: 250, loss is 3.9928862047195435 and perplexity is 54.21112820029765
At time: 49.86004400253296 and batch: 300, loss is 3.9713779973983763 and perplexity is 53.05759371399133
At time: 50.44515538215637 and batch: 350, loss is 3.9657642364501955 and perplexity is 52.76057554093224
At time: 51.03194522857666 and batch: 400, loss is 3.8880283880233764 and perplexity is 48.814548236142834
At time: 51.61920499801636 and batch: 450, loss is 3.934348611831665 and perplexity is 51.12883438937019
At time: 52.20723271369934 and batch: 500, loss is 3.8080886793136597 and perplexity is 45.06422431658856
At time: 52.79304337501526 and batch: 550, loss is 3.8878386878967284 and perplexity is 48.805288988428174
At time: 53.3789541721344 and batch: 600, loss is 3.910419101715088 and perplexity is 49.91986909265512
At time: 53.96473670005798 and batch: 650, loss is 3.76042640209198 and perplexity is 42.96674318241706
At time: 54.551114082336426 and batch: 700, loss is 3.7640883493423463 and perplexity is 43.12437357040399
At time: 55.136765241622925 and batch: 750, loss is 3.8744145679473876 and perplexity is 48.15449885037149
At time: 55.723763942718506 and batch: 800, loss is 3.8284972524642944 and perplexity is 45.99336985271871
At time: 56.30995988845825 and batch: 850, loss is 3.9131025886535644 and perplexity is 50.05400830925772
At time: 56.89743494987488 and batch: 900, loss is 3.8683278512954713 and perplexity is 47.862286270103354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332740261130137 and perplexity of 76.152679140358
finished 5 epochs...
Completing Train Step...
At time: 58.38214683532715 and batch: 50, loss is 3.9478785514831545 and perplexity is 51.82530541398488
At time: 58.972247838974 and batch: 100, loss is 3.8218884801864625 and perplexity is 45.69041233635008
At time: 59.549468755722046 and batch: 150, loss is 3.8338100576400755 and perplexity is 46.23837391927223
At time: 60.130436182022095 and batch: 200, loss is 3.730944766998291 and perplexity is 41.718503816866885
At time: 60.71246933937073 and batch: 250, loss is 3.8773195695877076 and perplexity is 48.294591134157415
At time: 61.29518961906433 and batch: 300, loss is 3.8628436279296876 and perplexity is 47.600517257364544
At time: 61.880475997924805 and batch: 350, loss is 3.8568134260177613 and perplexity is 47.314340246963496
At time: 62.46696186065674 and batch: 400, loss is 3.783098030090332 and perplexity is 43.95199563721103
At time: 63.053284883499146 and batch: 450, loss is 3.8306774377822874 and perplexity is 46.09375330990444
At time: 63.64209055900574 and batch: 500, loss is 3.708422303199768 and perplexity is 40.78940242558686
At time: 64.23303413391113 and batch: 550, loss is 3.7825194072723387 and perplexity is 43.92657136588011
At time: 64.82388091087341 and batch: 600, loss is 3.8085358238220213 and perplexity is 45.08437904271704
At time: 65.41350817680359 and batch: 650, loss is 3.660965027809143 and perplexity is 38.89886324909609
At time: 66.02708768844604 and batch: 700, loss is 3.665011920928955 and perplexity is 39.056601751298935
At time: 66.63088417053223 and batch: 750, loss is 3.773438777923584 and perplexity is 43.52949602822112
At time: 67.25147819519043 and batch: 800, loss is 3.7314881467819214 and perplexity is 41.74117896849584
At time: 67.86400270462036 and batch: 850, loss is 3.817151961326599 and perplexity is 45.47451055168073
At time: 68.4679319858551 and batch: 900, loss is 3.770795602798462 and perplexity is 43.414591869913686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325221649587971 and perplexity of 75.58226378086559
finished 6 epochs...
Completing Train Step...
At time: 69.9689404964447 and batch: 50, loss is 3.854259524345398 and perplexity is 47.193658244858334
At time: 70.55149555206299 and batch: 100, loss is 3.732284274101257 and perplexity is 41.77442349309515
At time: 71.13299012184143 and batch: 150, loss is 3.746426191329956 and perplexity is 42.36939100729498
At time: 71.71531462669373 and batch: 200, loss is 3.6431215810775757 and perplexity is 38.21092926393272
At time: 72.30813097953796 and batch: 250, loss is 3.7895697212219237 and perplexity is 44.23736178237418
At time: 72.88919878005981 and batch: 300, loss is 3.7785186100006105 and perplexity is 43.75118114331189
At time: 73.4703140258789 and batch: 350, loss is 3.7693559312820435 and perplexity is 43.35213408873369
At time: 74.05172181129456 and batch: 400, loss is 3.699128522872925 and perplexity is 40.412070814310276
At time: 74.63386487960815 and batch: 450, loss is 3.7472551918029784 and perplexity is 42.40452981551361
At time: 75.21663427352905 and batch: 500, loss is 3.6299311876297 and perplexity is 37.710221598165354
At time: 75.79804944992065 and batch: 550, loss is 3.7012164783477783 and perplexity is 40.49653756954272
At time: 76.38145518302917 and batch: 600, loss is 3.7291315317153932 and perplexity is 41.64292689387506
At time: 76.96714878082275 and batch: 650, loss is 3.5822487831115724 and perplexity is 35.954303434608526
At time: 77.5523316860199 and batch: 700, loss is 3.585883355140686 and perplexity is 36.085219708393446
At time: 78.13898277282715 and batch: 750, loss is 3.6944412183761597 and perplexity is 40.223090383474904
At time: 78.72367572784424 and batch: 800, loss is 3.652499141693115 and perplexity is 38.57093994202569
At time: 79.31029510498047 and batch: 850, loss is 3.738179965019226 and perplexity is 42.021440033366204
At time: 79.89476037025452 and batch: 900, loss is 3.6928696393966676 and perplexity is 40.15992626683755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332370705800514 and perplexity of 76.12454171140524
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 81.40398907661438 and batch: 50, loss is 3.802509989738464 and perplexity is 44.81352493537543
At time: 81.99608302116394 and batch: 100, loss is 3.6814062929153444 and perplexity is 39.702187737598656
At time: 82.57562637329102 and batch: 150, loss is 3.6888155555725097 and perplexity is 39.99744414000167
At time: 83.15522122383118 and batch: 200, loss is 3.5715148353576662 and perplexity is 35.57043571373065
At time: 83.73449492454529 and batch: 250, loss is 3.707123441696167 and perplexity is 40.73645703283101
At time: 84.3125410079956 and batch: 300, loss is 3.6897416353225707 and perplexity is 40.03450211974775
At time: 84.89063787460327 and batch: 350, loss is 3.6671095180511473 and perplexity is 39.13861274968205
At time: 85.46893358230591 and batch: 400, loss is 3.592548928260803 and perplexity is 36.32655179371704
At time: 86.04738521575928 and batch: 450, loss is 3.6223785495758056 and perplexity is 37.42648278055715
At time: 86.62765169143677 and batch: 500, loss is 3.497073221206665 and perplexity is 33.01867205194224
At time: 87.22054648399353 and batch: 550, loss is 3.5529829120635985 and perplexity is 34.91731758677087
At time: 87.80271363258362 and batch: 600, loss is 3.5751763868331907 and perplexity is 35.70091743208728
At time: 88.38439917564392 and batch: 650, loss is 3.410348310470581 and perplexity is 30.275787796976
At time: 88.96527910232544 and batch: 700, loss is 3.3983125400543215 and perplexity is 29.913579466475298
At time: 89.54663372039795 and batch: 750, loss is 3.4974546241760254 and perplexity is 33.03126787339635
At time: 90.13123774528503 and batch: 800, loss is 3.4402798318862917 and perplexity is 31.19568649481648
At time: 90.71635580062866 and batch: 850, loss is 3.5025639295578004 and perplexity is 33.200466583972464
At time: 91.30221390724182 and batch: 900, loss is 3.452785406112671 and perplexity is 31.5882560052675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.278407266695205 and perplexity of 72.1254718336965
finished 8 epochs...
Completing Train Step...
At time: 92.79224729537964 and batch: 50, loss is 3.7139129781723024 and perplexity is 41.01397975298144
At time: 93.38945508003235 and batch: 100, loss is 3.5894522285461425 and perplexity is 36.21423336911644
At time: 93.9752881526947 and batch: 150, loss is 3.5984391164779663 and perplexity is 36.541153422601084
At time: 94.56029081344604 and batch: 200, loss is 3.4841925287246704 and perplexity is 32.596096069246826
At time: 95.14367151260376 and batch: 250, loss is 3.6223176765441893 and perplexity is 37.42420458642856
At time: 95.72719979286194 and batch: 300, loss is 3.6089031505584717 and perplexity is 36.92552884884458
At time: 96.31034207344055 and batch: 350, loss is 3.5914032649993897 and perplexity is 36.284957628915905
At time: 96.89458394050598 and batch: 400, loss is 3.521688070297241 and perplexity is 33.841507116883854
At time: 97.47875237464905 and batch: 450, loss is 3.55666570186615 and perplexity is 35.046147809617665
At time: 98.06296038627625 and batch: 500, loss is 3.4345049476623535 and perplexity is 31.016054194109536
At time: 98.64815306663513 and batch: 550, loss is 3.4937970733642576 and perplexity is 32.91067500448277
At time: 99.24918961524963 and batch: 600, loss is 3.5236907958984376 and perplexity is 33.90935028251618
At time: 99.86483144760132 and batch: 650, loss is 3.3639800643920896 and perplexity is 28.90400203908335
At time: 100.4501473903656 and batch: 700, loss is 3.3570218801498415 and perplexity is 28.703580760286936
At time: 101.03491806983948 and batch: 750, loss is 3.4630848073959353 and perplexity is 31.915277300679485
At time: 101.64019989967346 and batch: 800, loss is 3.411900134086609 and perplexity is 30.322806952755577
At time: 102.2232666015625 and batch: 850, loss is 3.479307370185852 and perplexity is 32.437247288847374
At time: 102.80830264091492 and batch: 900, loss is 3.4386391925811766 and perplexity is 31.144547587124894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.282654644691781 and perplexity of 72.43246747750457
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 104.30427050590515 and batch: 50, loss is 3.6891607570648195 and perplexity is 40.01125370081074
At time: 104.88018870353699 and batch: 100, loss is 3.5760464811325074 and perplexity is 35.73199411468692
At time: 105.46003675460815 and batch: 150, loss is 3.5826023626327514 and perplexity is 35.967018387742314
At time: 106.04134202003479 and batch: 200, loss is 3.4647049140930175 and perplexity is 31.967025362545556
At time: 106.62345480918884 and batch: 250, loss is 3.605772123336792 and perplexity is 36.81009482068931
At time: 107.20574688911438 and batch: 300, loss is 3.584195070266724 and perplexity is 36.02434897581305
At time: 107.78812336921692 and batch: 350, loss is 3.5587481307983397 and perplexity is 35.11920496354264
At time: 108.36976933479309 and batch: 400, loss is 3.4907080793380736 and perplexity is 32.809170979371466
At time: 108.95280528068542 and batch: 450, loss is 3.52129412651062 and perplexity is 33.828178091041096
At time: 109.53412818908691 and batch: 500, loss is 3.3982239294052126 and perplexity is 29.91092892221656
At time: 110.1152172088623 and batch: 550, loss is 3.453325810432434 and perplexity is 31.605331048564558
At time: 110.6974868774414 and batch: 600, loss is 3.4793537712097167 and perplexity is 32.43875244525305
At time: 111.28049325942993 and batch: 650, loss is 3.3145049476623534 and perplexity is 27.508772331082653
At time: 111.86309432983398 and batch: 700, loss is 3.303938479423523 and perplexity is 27.219632047976486
At time: 112.44665002822876 and batch: 750, loss is 3.404855465888977 and perplexity is 30.109943495486544
At time: 113.03192472457886 and batch: 800, loss is 3.348738570213318 and perplexity is 28.46680211386275
At time: 113.61729645729065 and batch: 850, loss is 3.414033918380737 and perplexity is 30.387578361517065
At time: 114.20319151878357 and batch: 900, loss is 3.3704745292663576 and perplexity is 29.09232894434913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270093525925728 and perplexity of 71.52832505921324
finished 10 epochs...
Completing Train Step...
At time: 115.70368599891663 and batch: 50, loss is 3.66345685005188 and perplexity is 38.99591316711288
At time: 116.30017971992493 and batch: 100, loss is 3.543399019241333 and perplexity is 34.58427224203119
At time: 116.88394689559937 and batch: 150, loss is 3.550195269584656 and perplexity is 34.82011613341109
At time: 117.4675703048706 and batch: 200, loss is 3.4335992527008057 and perplexity is 30.987975827235893
At time: 118.05155110359192 and batch: 250, loss is 3.5746663713455202 and perplexity is 35.68271405366991
At time: 118.63313889503479 and batch: 300, loss is 3.5566121196746825 and perplexity is 35.04427001052428
At time: 119.21596431732178 and batch: 350, loss is 3.5323219299316406 and perplexity is 34.20329313505601
At time: 119.79854130744934 and batch: 400, loss is 3.4664569759368895 and perplexity is 32.023082661532655
At time: 120.40486860275269 and batch: 450, loss is 3.4994420862197875 and perplexity is 33.096981544634964
At time: 120.99402952194214 and batch: 500, loss is 3.377376537322998 and perplexity is 29.293818976144426
At time: 121.57581067085266 and batch: 550, loss is 3.435044860839844 and perplexity is 31.03280469198305
At time: 122.15672731399536 and batch: 600, loss is 3.4641571283340453 and perplexity is 31.94951907658008
At time: 122.74957251548767 and batch: 650, loss is 3.302104268074036 and perplexity is 27.169751249892826
At time: 123.3795952796936 and batch: 700, loss is 3.2942431926727296 and perplexity is 26.95700508906814
At time: 123.9843373298645 and batch: 750, loss is 3.3978694248199464 and perplexity is 29.900327240047563
At time: 124.56652855873108 and batch: 800, loss is 3.344931373596191 and perplexity is 28.358629449117323
At time: 125.14928817749023 and batch: 850, loss is 3.4130814981460573 and perplexity is 30.3586503949797
At time: 125.74631524085999 and batch: 900, loss is 3.371701259613037 and perplexity is 29.12803928612244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270971428858091 and perplexity of 71.59114755752262
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 127.27245688438416 and batch: 50, loss is 3.65648542881012 and perplexity is 38.725001646090874
At time: 127.86354565620422 and batch: 100, loss is 3.542931389808655 and perplexity is 34.568103399231106
At time: 128.44094681739807 and batch: 150, loss is 3.549552059173584 and perplexity is 34.79772667353811
At time: 129.01835370063782 and batch: 200, loss is 3.4319593715667724 and perplexity is 30.93720087411749
At time: 129.59800553321838 and batch: 250, loss is 3.57434561252594 and perplexity is 35.67127034386417
At time: 130.20114064216614 and batch: 300, loss is 3.5560662508010865 and perplexity is 35.02514565449786
At time: 130.79103827476501 and batch: 350, loss is 3.5265472650527956 and perplexity is 34.006349767706304
At time: 131.38464498519897 and batch: 400, loss is 3.460209822654724 and perplexity is 31.823653137574173
At time: 131.96494889259338 and batch: 450, loss is 3.490974202156067 and perplexity is 32.817903410306556
At time: 132.54678297042847 and batch: 500, loss is 3.367246513366699 and perplexity is 28.99856985329182
At time: 133.13185834884644 and batch: 550, loss is 3.4242755222320556 and perplexity is 30.70039503905596
At time: 133.7174367904663 and batch: 600, loss is 3.452182912826538 and perplexity is 31.569230025191835
At time: 134.30345225334167 and batch: 650, loss is 3.2857093048095702 and perplexity is 26.7279358463317
At time: 134.89000177383423 and batch: 700, loss is 3.2762778902053835 and perplexity is 26.477038619541293
At time: 135.4744737148285 and batch: 750, loss is 3.378486771583557 and perplexity is 29.326360038352863
At time: 136.05988383293152 and batch: 800, loss is 3.323879499435425 and perplexity is 27.767867293600204
At time: 136.64520502090454 and batch: 850, loss is 3.391591877937317 and perplexity is 29.713214453110048
At time: 137.23000764846802 and batch: 900, loss is 3.3506343936920167 and perplexity is 28.520821334948085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265630852686216 and perplexity of 71.2098287182363
finished 12 epochs...
Completing Train Step...
At time: 138.7460367679596 and batch: 50, loss is 3.6460506391525267 and perplexity is 38.32301536806305
At time: 139.32341408729553 and batch: 100, loss is 3.5285445737838743 and perplexity is 34.07433882197325
At time: 139.90047955513 and batch: 150, loss is 3.5367137670516966 and perplexity is 34.353838771451926
At time: 140.49698734283447 and batch: 200, loss is 3.4202991771697997 and perplexity is 30.578562060343078
At time: 141.07523584365845 and batch: 250, loss is 3.562415189743042 and perplexity is 35.248225576750116
At time: 141.65354919433594 and batch: 300, loss is 3.545327625274658 and perplexity is 34.651036037971046
At time: 142.23716759681702 and batch: 350, loss is 3.516698760986328 and perplexity is 33.67308188251216
At time: 142.82082104682922 and batch: 400, loss is 3.4512677812576293 and perplexity is 31.540353241233223
At time: 143.4040002822876 and batch: 450, loss is 3.4833328485488892 and perplexity is 32.56808589326982
At time: 143.98745441436768 and batch: 500, loss is 3.3609484100341795 and perplexity is 28.816507788516304
At time: 144.57121109962463 and batch: 550, loss is 3.4185154914855955 and perplexity is 30.5240681314208
At time: 145.1554491519928 and batch: 600, loss is 3.4477382135391235 and perplexity is 31.429225660131664
At time: 145.7536027431488 and batch: 650, loss is 3.2829626035690307 and perplexity is 26.654622922226945
At time: 146.33746075630188 and batch: 700, loss is 3.2747662925720213 and perplexity is 26.437046224445425
At time: 146.9244122505188 and batch: 750, loss is 3.3777914810180665 and perplexity is 29.30597678386144
At time: 147.5114827156067 and batch: 800, loss is 3.3243710088729856 and perplexity is 27.78151881708743
At time: 148.0961253643036 and batch: 850, loss is 3.3938850688934328 and perplexity is 29.781430714333485
At time: 148.68148231506348 and batch: 900, loss is 3.3536744356155395 and perplexity is 28.607657753799074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265362465218322 and perplexity of 71.19071945707529
finished 13 epochs...
Completing Train Step...
At time: 150.16004824638367 and batch: 50, loss is 3.6406642961502076 and perplexity is 38.11714939251284
At time: 150.74963974952698 and batch: 100, loss is 3.5224426126480104 and perplexity is 33.867051603203834
At time: 151.32693576812744 and batch: 150, loss is 3.5304928827285766 and perplexity is 34.14079087462872
At time: 151.90778183937073 and batch: 200, loss is 3.414218101501465 and perplexity is 30.39317575598802
At time: 152.49093866348267 and batch: 250, loss is 3.556134328842163 and perplexity is 35.02753017896839
At time: 153.07324934005737 and batch: 300, loss is 3.539453873634338 and perplexity is 34.44810103655695
At time: 153.65567421913147 and batch: 350, loss is 3.5111420345306397 and perplexity is 33.48648868138248
At time: 154.23384642601013 and batch: 400, loss is 3.4461682271957397 and perplexity is 31.379920919074618
At time: 154.81114554405212 and batch: 450, loss is 3.4787765026092528 and perplexity is 32.42003197591762
At time: 155.38901209831238 and batch: 500, loss is 3.3567627668380737 and perplexity is 28.696144243908346
At time: 155.9681088924408 and batch: 550, loss is 3.414815821647644 and perplexity is 30.41134779980105
At time: 156.54685187339783 and batch: 600, loss is 3.4447400331497193 and perplexity is 31.335136291035376
At time: 157.12460088729858 and batch: 650, loss is 3.2807231664657595 and perplexity is 26.594998358330024
At time: 157.70286560058594 and batch: 700, loss is 3.2731098890304566 and perplexity is 26.3932920547355
At time: 158.2831482887268 and batch: 750, loss is 3.3766576051712036 and perplexity is 29.27276627647175
At time: 158.86744475364685 and batch: 800, loss is 3.3239262247085573 and perplexity is 27.76916478509639
At time: 159.45298600196838 and batch: 850, loss is 3.394099097251892 and perplexity is 29.787805467226455
At time: 160.03794693946838 and batch: 900, loss is 3.354212927818298 and perplexity is 28.623066902919657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.265654263431078 and perplexity of 71.21149581288202
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 161.54751348495483 and batch: 50, loss is 3.638564658164978 and perplexity is 38.03720113832618
At time: 162.14740085601807 and batch: 100, loss is 3.5219471216201783 and perplexity is 33.85027493968169
At time: 162.73384475708008 and batch: 150, loss is 3.5307345485687254 and perplexity is 34.14904253457079
At time: 163.32202315330505 and batch: 200, loss is 3.41465350151062 and perplexity is 30.40641182627389
At time: 163.9074845314026 and batch: 250, loss is 3.5566653919219973 and perplexity is 35.04613694727076
At time: 164.49323630332947 and batch: 300, loss is 3.5404873752593993 and perplexity is 34.483721608747516
At time: 165.07852387428284 and batch: 350, loss is 3.5110784149169922 and perplexity is 33.484358351676256
At time: 165.66545057296753 and batch: 400, loss is 3.445436429977417 and perplexity is 31.35696558058572
At time: 166.25241255760193 and batch: 450, loss is 3.4769501638412477 and perplexity is 32.36087605048297
At time: 166.83966422080994 and batch: 500, loss is 3.35307909488678 and perplexity is 28.590631518692316
At time: 167.42496371269226 and batch: 550, loss is 3.4110511207580565 and perplexity is 30.29707341109927
At time: 168.00908660888672 and batch: 600, loss is 3.440938701629639 and perplexity is 31.216247161427628
At time: 168.59213304519653 and batch: 650, loss is 3.275100769996643 and perplexity is 26.445890298570294
At time: 169.17543053627014 and batch: 700, loss is 3.265702362060547 and perplexity is 26.19850536652345
At time: 169.7575614452362 and batch: 750, loss is 3.369126167297363 and perplexity is 29.05312838861949
At time: 170.33983755111694 and batch: 800, loss is 3.315774526596069 and perplexity is 27.54371906805309
At time: 170.9236991405487 and batch: 850, loss is 3.3849512434005735 and perplexity is 29.516553554289363
At time: 171.50827741622925 and batch: 900, loss is 3.3446393966674806 and perplexity is 28.350350592264743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264888815683861 and perplexity of 71.157007990287
finished 15 epochs...
Completing Train Step...
At time: 173.00851130485535 and batch: 50, loss is 3.6357574033737183 and perplexity is 37.93057076252782
At time: 173.58893823623657 and batch: 100, loss is 3.518198232650757 and perplexity is 33.723611589112956
At time: 174.16875958442688 and batch: 150, loss is 3.5269954872131346 and perplexity is 34.021595583765496
At time: 174.74780201911926 and batch: 200, loss is 3.4111431884765624 and perplexity is 30.299862921935613
At time: 175.34028601646423 and batch: 250, loss is 3.553498764038086 and perplexity is 34.93533440059677
At time: 175.9198248386383 and batch: 300, loss is 3.5372383546829225 and perplexity is 34.37186509813717
At time: 176.50003671646118 and batch: 350, loss is 3.5079768800735476 and perplexity is 33.38066633286507
At time: 177.081401348114 and batch: 400, loss is 3.4428507471084595 and perplexity is 31.275991144082752
At time: 177.66050815582275 and batch: 450, loss is 3.474826283454895 and perplexity is 32.29221835672754
At time: 178.23910546302795 and batch: 500, loss is 3.351453776359558 and perplexity is 28.544200378497493
At time: 178.81731748580933 and batch: 550, loss is 3.4091991233825683 and perplexity is 30.241015236475878
At time: 179.39587593078613 and batch: 600, loss is 3.4397686672210694 and perplexity is 31.17974443702842
At time: 179.97464442253113 and batch: 650, loss is 3.274416937828064 and perplexity is 26.427811930046893
At time: 180.553795337677 and batch: 700, loss is 3.265462336540222 and perplexity is 26.192217811258168
At time: 181.13165283203125 and batch: 750, loss is 3.369307451248169 and perplexity is 29.058395731945033
At time: 181.70910263061523 and batch: 800, loss is 3.3163218307495117 and perplexity is 27.558797985901077
At time: 182.29051876068115 and batch: 850, loss is 3.386499533653259 and perplexity is 29.562289143297214
At time: 182.8760359287262 and batch: 900, loss is 3.3464176082611083 and perplexity is 28.40080836337233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264599107716181 and perplexity of 71.13639622395661
finished 16 epochs...
Completing Train Step...
At time: 184.35357093811035 and batch: 50, loss is 3.634099397659302 and perplexity is 37.867733765897036
At time: 184.94170999526978 and batch: 100, loss is 3.516130971908569 and perplexity is 33.653968101216414
At time: 185.51927185058594 and batch: 150, loss is 3.5247974252700804 and perplexity is 33.94689613635282
At time: 186.09706735610962 and batch: 200, loss is 3.409100275039673 and perplexity is 30.238026109969812
At time: 186.67565608024597 and batch: 250, loss is 3.5514288902282716 and perplexity is 34.86309745335733
At time: 187.25551438331604 and batch: 300, loss is 3.535258388519287 and perplexity is 34.303877297245336
At time: 187.83800792694092 and batch: 350, loss is 3.506130576133728 and perplexity is 33.31909233674803
At time: 188.42104482650757 and batch: 400, loss is 3.4411822938919068 and perplexity is 31.223852123910596
At time: 189.0030906200409 and batch: 450, loss is 3.473370699882507 and perplexity is 32.245248526721014
At time: 189.5851972103119 and batch: 500, loss is 3.35029176235199 and perplexity is 28.511050881642767
At time: 190.1788773536682 and batch: 550, loss is 3.408054904937744 and perplexity is 30.206432697814456
At time: 190.76097130775452 and batch: 600, loss is 3.438952431678772 and perplexity is 31.154304805197413
At time: 191.34425330162048 and batch: 650, loss is 3.273921775817871 and perplexity is 26.414729120888953
At time: 191.92609286308289 and batch: 700, loss is 3.2652859783172605 and perplexity is 26.1875990055637
At time: 192.5056927204132 and batch: 750, loss is 3.3693224477767942 and perplexity is 29.058831510276
At time: 193.0868775844574 and batch: 800, loss is 3.3165687942504882 and perplexity is 27.565604843622488
At time: 193.66972160339355 and batch: 850, loss is 3.3872676515579223 and perplexity is 29.585005190075258
At time: 194.25230431556702 and batch: 900, loss is 3.347270278930664 and perplexity is 28.4250352269556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264500030099529 and perplexity of 71.12934854850072
finished 17 epochs...
Completing Train Step...
At time: 195.73482537269592 and batch: 50, loss is 3.6326835632324217 and perplexity is 37.81415726144298
At time: 196.32444095611572 and batch: 100, loss is 3.514495167732239 and perplexity is 33.59896180167935
At time: 196.90167593955994 and batch: 150, loss is 3.5230937099456785 and perplexity is 33.8891095291314
At time: 197.47767186164856 and batch: 200, loss is 3.407474994659424 and perplexity is 30.18892075519013
At time: 198.059166431427 and batch: 250, loss is 3.549762420654297 and perplexity is 34.80504754483426
At time: 198.64193415641785 and batch: 300, loss is 3.5336789178848265 and perplexity is 34.24973809729965
At time: 199.22437381744385 and batch: 350, loss is 3.504659080505371 and perplexity is 33.270099493251536
At time: 199.80776476860046 and batch: 400, loss is 3.439831953048706 and perplexity is 31.181717735400866
At time: 200.40539002418518 and batch: 450, loss is 3.47217303276062 and perplexity is 32.206652569891276
At time: 200.9990096092224 and batch: 500, loss is 3.3492718362808227 and perplexity is 28.481986541790697
At time: 201.58409547805786 and batch: 550, loss is 3.4071145725250243 and perplexity is 30.178041960533285
At time: 202.17013502120972 and batch: 600, loss is 3.438229422569275 and perplexity is 31.1317880998956
At time: 202.75640487670898 and batch: 650, loss is 3.2734502840042112 and perplexity is 26.402277727944004
At time: 203.342600107193 and batch: 700, loss is 3.2650351238250734 and perplexity is 26.18103055261097
At time: 203.9286322593689 and batch: 750, loss is 3.369198603630066 and perplexity is 29.055232966916552
At time: 204.53650856018066 and batch: 800, loss is 3.3166288328170777 and perplexity is 27.567259892707362
At time: 205.12224912643433 and batch: 850, loss is 3.3876480579376222 and perplexity is 29.59626165567294
At time: 205.7089991569519 and batch: 900, loss is 3.3476993322372435 and perplexity is 28.437233699019625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264502956442637 and perplexity of 71.1295566976842
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 207.19961428642273 and batch: 50, loss is 3.6320066928863524 and perplexity is 37.788570640120604
At time: 207.77991843223572 and batch: 100, loss is 3.514103317260742 and perplexity is 33.58579861182506
At time: 208.36005330085754 and batch: 150, loss is 3.5228234815597532 and perplexity is 33.87995296700012
At time: 208.94027471542358 and batch: 200, loss is 3.407367887496948 and perplexity is 30.185687478706463
At time: 209.5229365825653 and batch: 250, loss is 3.549739637374878 and perplexity is 34.80425458074405
At time: 210.1039617061615 and batch: 300, loss is 3.533770589828491 and perplexity is 34.25287798127846
At time: 210.68494844436646 and batch: 350, loss is 3.504417967796326 and perplexity is 33.26207861643905
At time: 211.2671582698822 and batch: 400, loss is 3.4395950174331666 and perplexity is 31.174330551093743
At time: 211.84918665885925 and batch: 450, loss is 3.471716365814209 and perplexity is 32.19194821396026
At time: 212.43217253684998 and batch: 500, loss is 3.3478478670120237 and perplexity is 28.441457930837377
At time: 213.01534485816956 and batch: 550, loss is 3.405690531730652 and perplexity is 30.135097782075384
At time: 213.60632300376892 and batch: 600, loss is 3.436582989692688 and perplexity is 31.080573872423965
At time: 214.20816373825073 and batch: 650, loss is 3.2714772462844848 and perplexity is 26.350236394739323
At time: 214.79059410095215 and batch: 700, loss is 3.262727198600769 and perplexity is 26.12067636506604
At time: 215.37336134910583 and batch: 750, loss is 3.3667744970321656 and perplexity is 28.984885284567014
At time: 215.9566638469696 and batch: 800, loss is 3.3138041210174563 and perplexity is 27.489500204463344
At time: 216.53759741783142 and batch: 850, loss is 3.384359402656555 and perplexity is 29.49908962371553
At time: 217.11901569366455 and batch: 900, loss is 3.34441837310791 and perplexity is 28.344085189288105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2641710255244005 and perplexity of 71.10595051664318
finished 19 epochs...
Completing Train Step...
At time: 218.6007273197174 and batch: 50, loss is 3.6314281892776488 and perplexity is 37.7667161377013
At time: 219.18645811080933 and batch: 100, loss is 3.5134425449371336 and perplexity is 33.56361337613816
At time: 219.75914096832275 and batch: 150, loss is 3.5221838331222535 and perplexity is 33.858288637538124
At time: 220.33235096931458 and batch: 200, loss is 3.406686930656433 and perplexity is 30.165139325329
At time: 220.90776371955872 and batch: 250, loss is 3.549112162590027 and perplexity is 34.78242263880195
At time: 221.4846692085266 and batch: 300, loss is 3.533102149963379 and perplexity is 34.22998964273216
At time: 222.06218647956848 and batch: 350, loss is 3.50385910987854 and perplexity is 33.243495033717075
At time: 222.63729405403137 and batch: 400, loss is 3.439082064628601 and perplexity is 31.1583436914099
At time: 223.21094822883606 and batch: 450, loss is 3.4712682247161863 and perplexity is 32.17752491101896
At time: 223.7903401851654 and batch: 500, loss is 3.347573447227478 and perplexity is 28.43365410289119
At time: 224.3732340335846 and batch: 550, loss is 3.405370845794678 and perplexity is 30.125465554861076
At time: 224.95839977264404 and batch: 600, loss is 3.4364246559143066 and perplexity is 31.075653157296475
At time: 225.54156804084778 and batch: 650, loss is 3.2713953399658204 and perplexity is 26.34807823226507
At time: 226.12766647338867 and batch: 700, loss is 3.2626651239395144 and perplexity is 26.119054983252816
At time: 226.7190248966217 and batch: 750, loss is 3.3668203449249265 and perplexity is 28.986214210943235
At time: 227.31217527389526 and batch: 800, loss is 3.3140011262893676 and perplexity is 27.494916314409423
At time: 227.90304040908813 and batch: 850, loss is 3.384733409881592 and perplexity is 29.510124559811093
At time: 228.49583172798157 and batch: 900, loss is 3.3447487497329713 and perplexity is 28.353450959523986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264010494702483 and perplexity of 71.09453673611982
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f52e60f0b70>
ELAPSED
954.7799134254456


RESULTS SO FAR:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.27715916891746, 'params': {'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.64645432534728, 'params': {'batch_size': 32, 'dropout': 0.43751170550744745, 'num_layers': 2, 'rnn_dropout': 0.9329965743746119, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -71.09453673611982, 'params': {'batch_size': 32, 'dropout': 0.7029347552002905, 'num_layers': 2, 'rnn_dropout': 0.44946109581267146, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.8672620102056839, 'num_layers': 2, 'rnn_dropout': 0.43740970994803685, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.840705156326294 and batch: 50, loss is 7.0124169921875 and perplexity is 1110.3349351701186
At time: 1.4815189838409424 and batch: 100, loss is 6.227224588394165 and perplexity is 506.34820682332145
At time: 2.1085429191589355 and batch: 150, loss is 6.150819005966187 and perplexity is 469.10142636253397
At time: 2.7366366386413574 and batch: 200, loss is 6.02160964012146 and perplexity is 412.241622621672
At time: 3.363863706588745 and batch: 250, loss is 6.0794932079315185 and perplexity is 436.8077679022296
At time: 3.991956949234009 and batch: 300, loss is 5.990764513015747 and perplexity is 399.72008434978596
At time: 4.6294190883636475 and batch: 350, loss is 5.990259151458741 and perplexity is 399.51813221931195
At time: 5.265570878982544 and batch: 400, loss is 5.870791301727295 and perplexity is 354.52940903499507
At time: 5.892564296722412 and batch: 450, loss is 5.877420787811279 and perplexity is 356.8875648624203
At time: 6.533561706542969 and batch: 500, loss is 5.842180500030517 and perplexity is 344.5297693853705
At time: 7.173167705535889 and batch: 550, loss is 5.8841478061676025 and perplexity is 359.2964472764575
At time: 7.824182033538818 and batch: 600, loss is 5.82406361579895 and perplexity is 338.3441645687158
At time: 8.456203699111938 and batch: 650, loss is 5.742318658828736 and perplexity is 311.7865000397989
At time: 9.08555793762207 and batch: 700, loss is 5.837121543884277 and perplexity is 342.7912097473756
At time: 9.712411880493164 and batch: 750, loss is 5.801179151535035 and perplexity is 330.68926285699814
At time: 10.339970350265503 and batch: 800, loss is 5.803566951751709 and perplexity is 331.4798262285725
At time: 10.968425750732422 and batch: 850, loss is 5.842311372756958 and perplexity is 344.5748618862549
At time: 11.59600019454956 and batch: 900, loss is 5.721223134994506 and perplexity is 305.27809100910275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.624685209091395 and perplexity of 277.1850154652317
finished 1 epochs...
Completing Train Step...
At time: 13.09966516494751 and batch: 50, loss is 5.443641834259033 and perplexity is 231.2829457282611
At time: 13.652180433273315 and batch: 100, loss is 5.21793532371521 and perplexity is 184.55274875231504
At time: 14.206737995147705 and batch: 150, loss is 5.118766851425171 and perplexity is 167.12914742597312
At time: 14.763375043869019 and batch: 200, loss is 4.95534574508667 and perplexity is 141.93167006827287
At time: 15.321404695510864 and batch: 250, loss is 5.010250225067138 and perplexity is 149.9422507650669
At time: 15.878263235092163 and batch: 300, loss is 4.922658443450928 and perplexity is 137.36731143630485
At time: 16.432815074920654 and batch: 350, loss is 4.891138763427734 and perplexity is 133.1050629576414
At time: 16.987101078033447 and batch: 400, loss is 4.735719795227051 and perplexity is 113.94544659603268
At time: 17.542853593826294 and batch: 450, loss is 4.746440763473511 and perplexity is 115.17362397235995
At time: 18.098414182662964 and batch: 500, loss is 4.644697132110596 and perplexity is 104.03185310578588
At time: 18.653804779052734 and batch: 550, loss is 4.702486267089844 and perplexity is 110.2208705909609
At time: 19.209017038345337 and batch: 600, loss is 4.645447759628296 and perplexity is 104.10997159271946
At time: 19.764984846115112 and batch: 650, loss is 4.505733547210693 and perplexity is 90.53473119637876
At time: 20.321146488189697 and batch: 700, loss is 4.547242803573608 and perplexity is 94.3718475505696
At time: 20.877256631851196 and batch: 750, loss is 4.578536529541015 and perplexity is 97.37178914640495
At time: 21.436709880828857 and batch: 800, loss is 4.523289394378662 and perplexity is 92.13817885900153
At time: 22.008576154708862 and batch: 850, loss is 4.577838239669799 and perplexity is 97.30381914644501
At time: 22.60798478126526 and batch: 900, loss is 4.506692628860474 and perplexity is 90.6216030476773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.595060583663313 and perplexity of 98.99413279320966
finished 2 epochs...
Completing Train Step...
At time: 24.103875637054443 and batch: 50, loss is 4.553488731384277 and perplexity is 94.96313193527585
At time: 24.69784665107727 and batch: 100, loss is 4.414986028671264 and perplexity is 82.68068573899808
At time: 25.29569101333618 and batch: 150, loss is 4.412041816711426 and perplexity is 82.43761427773573
At time: 25.874033451080322 and batch: 200, loss is 4.313612208366394 and perplexity is 74.70986973416508
At time: 26.450379133224487 and batch: 250, loss is 4.442756967544556 and perplexity is 85.00898591455721
At time: 27.028734922409058 and batch: 300, loss is 4.400831322669983 and perplexity is 81.51860877162106
At time: 27.615971565246582 and batch: 350, loss is 4.388239336013794 and perplexity is 80.49856321575895
At time: 28.20333242416382 and batch: 400, loss is 4.29355619430542 and perplexity is 73.22641337268674
At time: 28.79052209854126 and batch: 450, loss is 4.326518983840942 and perplexity is 75.68038287345362
At time: 29.37849187850952 and batch: 500, loss is 4.201980633735657 and perplexity is 66.81854312652914
At time: 29.96544623374939 and batch: 550, loss is 4.283211660385132 and perplexity is 72.47282473736779
At time: 30.553813457489014 and batch: 600, loss is 4.274849734306335 and perplexity is 71.86933900214315
At time: 31.13982343673706 and batch: 650, loss is 4.1235070180892945 and perplexity is 61.77551064751876
At time: 31.724258422851562 and batch: 700, loss is 4.149265260696411 and perplexity is 63.38740996289159
At time: 32.31071662902832 and batch: 750, loss is 4.230826315879821 and perplexity is 68.77403778046425
At time: 32.89588761329651 and batch: 800, loss is 4.187227082252503 and perplexity is 65.83996878418944
At time: 33.48123335838318 and batch: 850, loss is 4.259641213417053 and perplexity is 70.7845823393366
At time: 34.066484212875366 and batch: 900, loss is 4.201320672035218 and perplexity is 66.77445999537562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.417112585616438 and perplexity of 82.85669800921507
finished 3 epochs...
Completing Train Step...
At time: 35.56641721725464 and batch: 50, loss is 4.281920223236084 and perplexity is 72.37929104864537
At time: 36.161184310913086 and batch: 100, loss is 4.151080212593079 and perplexity is 63.50255952667916
At time: 36.74475049972534 and batch: 150, loss is 4.150049796104431 and perplexity is 63.437159142875096
At time: 37.33889555931091 and batch: 200, loss is 4.054836893081665 and perplexity is 57.67575491427562
At time: 37.92621874809265 and batch: 250, loss is 4.196683692932129 and perplexity is 66.46554498948356
At time: 38.50870966911316 and batch: 300, loss is 4.168743782043457 and perplexity is 64.6342064362198
At time: 39.08891463279724 and batch: 350, loss is 4.155067806243896 and perplexity is 63.756287475629016
At time: 39.67056655883789 and batch: 400, loss is 4.077801103591919 and perplexity is 59.01555797240071
At time: 40.25373387336731 and batch: 450, loss is 4.117666153907776 and perplexity is 61.41573998867828
At time: 40.83839678764343 and batch: 500, loss is 3.9868598127365114 and perplexity is 53.88541312105123
At time: 41.424118757247925 and batch: 550, loss is 4.068963851928711 and perplexity is 58.496320330648715
At time: 42.010106325149536 and batch: 600, loss is 4.0800964593887326 and perplexity is 59.15117526091609
At time: 42.600032567977905 and batch: 650, loss is 3.9258883666992186 and perplexity is 50.69809656010701
At time: 43.191823959350586 and batch: 700, loss is 3.938948564529419 and perplexity is 51.364566371370515
At time: 43.783114433288574 and batch: 750, loss is 4.038065047264099 and perplexity is 56.71649282951566
At time: 44.37366056442261 and batch: 800, loss is 3.9951348304748535 and perplexity is 54.3331658965249
At time: 44.96473741531372 and batch: 850, loss is 4.073675694465638 and perplexity is 58.77259615398081
At time: 45.55647826194763 and batch: 900, loss is 4.02741129398346 and perplexity is 56.11545663852567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355907701466181 and perplexity of 77.93753725615453
finished 4 epochs...
Completing Train Step...
At time: 47.063626527786255 and batch: 50, loss is 4.112398386001587 and perplexity is 61.09306675464907
At time: 47.65022897720337 and batch: 100, loss is 3.9848439645767213 and perplexity is 53.77689772220249
At time: 48.23726725578308 and batch: 150, loss is 3.9877447843551637 and perplexity is 53.93312128939654
At time: 48.82513737678528 and batch: 200, loss is 3.894791831970215 and perplexity is 49.14582170889193
At time: 49.41354990005493 and batch: 250, loss is 4.037228145599365 and perplexity is 56.66904655894949
At time: 50.0014750957489 and batch: 300, loss is 4.0176961755752565 and perplexity is 55.572927965044066
At time: 50.58987212181091 and batch: 350, loss is 4.0057220554351805 and perplexity is 54.911459205657444
At time: 51.177672147750854 and batch: 400, loss is 3.934742431640625 and perplexity is 51.14897390257109
At time: 51.7764790058136 and batch: 450, loss is 3.9806620693206787 and perplexity is 53.55247794579145
At time: 52.36352753639221 and batch: 500, loss is 3.8475743198394774 and perplexity is 46.879211230511075
At time: 52.95026636123657 and batch: 550, loss is 3.9264654159545898 and perplexity is 50.72736030147381
At time: 53.5391001701355 and batch: 600, loss is 3.941777300834656 and perplexity is 51.51006888216248
At time: 54.126774311065674 and batch: 650, loss is 3.7943317222595216 and perplexity is 44.448522519873215
At time: 54.714946031570435 and batch: 700, loss is 3.7996804904937744 and perplexity is 44.6869043213625
At time: 55.30221343040466 and batch: 750, loss is 3.9054827213287355 and perplexity is 49.67405285038295
At time: 55.889909982681274 and batch: 800, loss is 3.866844472885132 and perplexity is 47.791341020315784
At time: 56.48097229003906 and batch: 850, loss is 3.9417974758148193 and perplexity is 51.511108107263524
At time: 57.072306871414185 and batch: 900, loss is 3.903437099456787 and perplexity is 49.572542382818355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331062630431293 and perplexity of 76.0250301718854
finished 5 epochs...
Completing Train Step...
At time: 58.589341163635254 and batch: 50, loss is 3.9922179555892945 and perplexity is 54.174913762508915
At time: 59.19003510475159 and batch: 100, loss is 3.8648586797714235 and perplexity is 47.69653147165297
At time: 59.777578353881836 and batch: 150, loss is 3.8707485389709473 and perplexity is 47.9782862598262
At time: 60.36384177207947 and batch: 200, loss is 3.780651817321777 and perplexity is 43.844611100540895
At time: 60.94910526275635 and batch: 250, loss is 3.9197577667236327 and perplexity is 50.388237591749586
At time: 61.53342866897583 and batch: 300, loss is 3.9044235801696776 and perplexity is 49.621468868317386
At time: 62.11993885040283 and batch: 350, loss is 3.893386845588684 and perplexity is 49.07682098257272
At time: 62.70633244514465 and batch: 400, loss is 3.8304828071594237 and perplexity is 46.084782926971535
At time: 63.29376244544983 and batch: 450, loss is 3.876115708351135 and perplexity is 48.23648613015341
At time: 63.88082671165466 and batch: 500, loss is 3.743867778778076 and perplexity is 42.26113117123912
At time: 64.4664158821106 and batch: 550, loss is 3.820034427642822 and perplexity is 45.60577839327079
At time: 65.05326795578003 and batch: 600, loss is 3.8399156141281128 and perplexity is 46.52154851271236
At time: 65.64052510261536 and batch: 650, loss is 3.693211627006531 and perplexity is 40.173662812764206
At time: 66.22878742218018 and batch: 700, loss is 3.6987174797058104 and perplexity is 40.39546312220612
At time: 66.82751274108887 and batch: 750, loss is 3.8046000814437866 and perplexity is 44.907287263928445
At time: 67.41387701034546 and batch: 800, loss is 3.7695129442214967 and perplexity is 43.35894146914796
At time: 68.00090527534485 and batch: 850, loss is 3.8404470920562743 and perplexity is 46.54628026053765
At time: 68.5870053768158 and batch: 900, loss is 3.806484375 and perplexity is 44.99198554910284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322514782213185 and perplexity of 75.37794926782271
finished 6 epochs...
Completing Train Step...
At time: 70.104323387146 and batch: 50, loss is 3.896292905807495 and perplexity is 49.2196486120248
At time: 70.73227572441101 and batch: 100, loss is 3.7716259145736695 and perplexity is 43.450654486294766
At time: 71.32542061805725 and batch: 150, loss is 3.777388038635254 and perplexity is 43.701745261372714
At time: 71.92979383468628 and batch: 200, loss is 3.6917041015625 and perplexity is 40.11314562095838
At time: 72.53116273880005 and batch: 250, loss is 3.8256670808792115 and perplexity is 45.863384751093534
At time: 73.11420845985413 and batch: 300, loss is 3.8160187435150146 and perplexity is 45.42300721411186
At time: 73.69675874710083 and batch: 350, loss is 3.8053288364410403 and perplexity is 44.940025601599835
At time: 74.27926993370056 and batch: 400, loss is 3.7452759075164797 and perplexity is 42.32068220247808
At time: 74.86120057106018 and batch: 450, loss is 3.786856269836426 and perplexity is 44.11748856038556
At time: 75.44363355636597 and batch: 500, loss is 3.6632032442092894 and perplexity is 38.98602482961962
At time: 76.02359247207642 and batch: 550, loss is 3.73210928440094 and perplexity is 41.76711403880543
At time: 76.60367035865784 and batch: 600, loss is 3.7551864767074585 and perplexity is 42.742189490215196
At time: 77.18779110908508 and batch: 650, loss is 3.6123307228088377 and perplexity is 37.05231092008931
At time: 77.77179193496704 and batch: 700, loss is 3.6175690507888794 and perplexity is 37.24691232511002
At time: 78.35594916343689 and batch: 750, loss is 3.7227525758743285 and perplexity is 41.37813395126222
At time: 78.94172143936157 and batch: 800, loss is 3.6876421117782594 and perplexity is 39.95053691426867
At time: 79.52807831764221 and batch: 850, loss is 3.762623014450073 and perplexity is 43.0612281969622
At time: 80.11474180221558 and batch: 900, loss is 3.730219464302063 and perplexity is 41.68825624421492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3261171105789815 and perplexity of 75.64997506157435
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 81.62067031860352 and batch: 50, loss is 3.838061227798462 and perplexity is 46.43535952765255
At time: 82.20274496078491 and batch: 100, loss is 3.711000208854675 and perplexity is 40.89468930827735
At time: 82.78736639022827 and batch: 150, loss is 3.713877949714661 and perplexity is 41.012543121690584
At time: 83.39502096176147 and batch: 200, loss is 3.6083825492858885 and perplexity is 36.90631037455021
At time: 83.97808289527893 and batch: 250, loss is 3.7362566661834715 and perplexity is 41.94069791717162
At time: 84.56183409690857 and batch: 300, loss is 3.7220355796813966 and perplexity is 41.34847662011583
At time: 85.15205669403076 and batch: 350, loss is 3.69679949760437 and perplexity is 40.31805959997185
At time: 85.74341034889221 and batch: 400, loss is 3.624478635787964 and perplexity is 37.50516421098998
At time: 86.32654094696045 and batch: 450, loss is 3.662027702331543 and perplexity is 38.94022205160194
At time: 86.90906500816345 and batch: 500, loss is 3.5231433057785035 and perplexity is 33.8907903294222
At time: 87.49312138557434 and batch: 550, loss is 3.5769801712036133 and perplexity is 35.765372302829185
At time: 88.07570600509644 and batch: 600, loss is 3.594428277015686 and perplexity is 36.39488624562047
At time: 88.65992021560669 and batch: 650, loss is 3.4393941974639892 and perplexity is 31.168070751560755
At time: 89.24280714988708 and batch: 700, loss is 3.4293465566635133 and perplexity is 30.85647320376455
At time: 89.8253903388977 and batch: 750, loss is 3.517729949951172 and perplexity is 33.7078231022644
At time: 90.40724158287048 and batch: 800, loss is 3.473525061607361 and perplexity is 32.250226343084215
At time: 90.98869037628174 and batch: 850, loss is 3.5296008586883545 and perplexity is 34.110350047409696
At time: 91.57049751281738 and batch: 900, loss is 3.489075927734375 and perplexity is 32.755665115060516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2746690723994005 and perplexity of 71.85635612310001
finished 8 epochs...
Completing Train Step...
At time: 93.0657856464386 and batch: 50, loss is 3.746240425109863 and perplexity is 42.361520936699186
At time: 93.65719413757324 and batch: 100, loss is 3.619651608467102 and perplexity is 37.32456199525697
At time: 94.23810958862305 and batch: 150, loss is 3.621394147872925 and perplexity is 37.38965821523127
At time: 94.8204996585846 and batch: 200, loss is 3.5228802585601806 and perplexity is 33.881876623713424
At time: 95.40303945541382 and batch: 250, loss is 3.6517062902450563 and perplexity is 38.54037103634157
At time: 95.99654340744019 and batch: 300, loss is 3.6454966497421264 and perplexity is 38.30179070303719
At time: 96.57795453071594 and batch: 350, loss is 3.6223337697982787 and perplexity is 37.424806868508384
At time: 97.15970277786255 and batch: 400, loss is 3.5556010150909425 and perplexity is 35.00885449589461
At time: 97.74118256568909 and batch: 450, loss is 3.5991189527511596 and perplexity is 36.56600387032286
At time: 98.32545065879822 and batch: 500, loss is 3.4638211297988892 and perplexity is 31.938785888240165
At time: 98.90851736068726 and batch: 550, loss is 3.521493000984192 and perplexity is 33.834906321165285
At time: 99.49225163459778 and batch: 600, loss is 3.543710446357727 and perplexity is 34.595044399492025
At time: 100.08245635032654 and batch: 650, loss is 3.3939081478118895 and perplexity is 29.782118045475865
At time: 100.67348837852478 and batch: 700, loss is 3.3883142566680906 and perplexity is 29.61598521679077
At time: 101.26372122764587 and batch: 750, loss is 3.4833095502853393 and perplexity is 32.567327122260416
At time: 101.85421538352966 and batch: 800, loss is 3.443970756530762 and perplexity is 31.311040172813325
At time: 102.4444227218628 and batch: 850, loss is 3.507424259185791 and perplexity is 33.36222457552178
At time: 103.03301119804382 and batch: 900, loss is 3.4755650043487547 and perplexity is 32.316082106376754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.278356264715326 and perplexity of 72.1217933856381
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 104.51772856712341 and batch: 50, loss is 3.7188850831985474 and perplexity is 41.218413379424085
At time: 105.10801792144775 and batch: 100, loss is 3.599310908317566 and perplexity is 36.573023592022786
At time: 105.68523454666138 and batch: 150, loss is 3.6059446907043458 and perplexity is 36.8164475899765
At time: 106.26349806785583 and batch: 200, loss is 3.4998433446884154 and perplexity is 33.11026465356752
At time: 106.84296369552612 and batch: 250, loss is 3.6294810438156127 and perplexity is 37.693250395212665
At time: 107.42155480384827 and batch: 300, loss is 3.620066909790039 and perplexity is 37.3400661544577
At time: 107.99920749664307 and batch: 350, loss is 3.591711220741272 and perplexity is 36.29613351071185
At time: 108.57894897460938 and batch: 400, loss is 3.52413788318634 and perplexity is 33.92451411149353
At time: 109.15820550918579 and batch: 450, loss is 3.562828426361084 and perplexity is 35.26279444426689
At time: 109.73808598518372 and batch: 500, loss is 3.4213558530807493 and perplexity is 30.6108907677388
At time: 110.3175139427185 and batch: 550, loss is 3.4748979711532595 and perplexity is 32.2945333945155
At time: 110.90831208229065 and batch: 600, loss is 3.499511761665344 and perplexity is 33.099287671909956
At time: 111.48816180229187 and batch: 650, loss is 3.344231615066528 and perplexity is 28.338792197723148
At time: 112.07097554206848 and batch: 700, loss is 3.332209243774414 and perplexity is 28.000132541091585
At time: 112.65487718582153 and batch: 750, loss is 3.4244872331619263 and perplexity is 30.706895336304786
At time: 113.23978066444397 and batch: 800, loss is 3.381319074630737 and perplexity is 29.409538915541187
At time: 113.82402300834656 and batch: 850, loss is 3.4385285902023317 and perplexity is 31.141103116560544
At time: 114.40996766090393 and batch: 900, loss is 3.4072641134262085 and perplexity is 30.182555149568806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262437376257491 and perplexity of 70.98278453181511
finished 10 epochs...
Completing Train Step...
At time: 115.90274858474731 and batch: 50, loss is 3.6942068004608153 and perplexity is 40.21366247555693
At time: 116.48277044296265 and batch: 100, loss is 3.568837080001831 and perplexity is 35.47531420189577
At time: 117.06390476226807 and batch: 150, loss is 3.57281813621521 and perplexity is 35.6168249160768
At time: 117.6433584690094 and batch: 200, loss is 3.471298084259033 and perplexity is 32.17848573154751
At time: 118.2236659526825 and batch: 250, loss is 3.60018394947052 and perplexity is 36.60496728875852
At time: 118.80420923233032 and batch: 300, loss is 3.5936758422851565 and perplexity is 36.36751176924049
At time: 119.38440608978271 and batch: 350, loss is 3.5657218742370604 and perplexity is 35.36497325519729
At time: 119.96816420555115 and batch: 400, loss is 3.499998240470886 and perplexity is 33.11539369114173
At time: 120.55151772499084 and batch: 450, loss is 3.54189248085022 and perplexity is 34.532208935701895
At time: 121.13497471809387 and batch: 500, loss is 3.4019232988357544 and perplexity is 30.021785421457093
At time: 121.72268009185791 and batch: 550, loss is 3.4577148866653444 and perplexity is 31.744354124163355
At time: 122.31073594093323 and batch: 600, loss is 3.484841842651367 and perplexity is 32.61726804126482
At time: 122.90051341056824 and batch: 650, loss is 3.331861038208008 and perplexity is 27.99038443635132
At time: 123.49179220199585 and batch: 700, loss is 3.322512845993042 and perplexity is 27.72994416196752
At time: 124.0833146572113 and batch: 750, loss is 3.417441487312317 and perplexity is 30.49130275308828
At time: 124.67561364173889 and batch: 800, loss is 3.3772297954559325 and perplexity is 29.289520661833432
At time: 125.27929377555847 and batch: 850, loss is 3.437592000961304 and perplexity is 31.11195034864329
At time: 125.8704948425293 and batch: 900, loss is 3.408970384597778 and perplexity is 30.234098734466155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263105836633134 and perplexity of 71.03024957310963
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 127.37348794937134 and batch: 50, loss is 3.6873628377914427 and perplexity is 39.939381326354415
At time: 127.9742956161499 and batch: 100, loss is 3.5657565450668334 and perplexity is 35.366199409420716
At time: 128.5593774318695 and batch: 150, loss is 3.572404479980469 and perplexity is 35.602094841192276
At time: 129.14752912521362 and batch: 200, loss is 3.4706019258499143 and perplexity is 32.156092203738524
At time: 129.73581862449646 and batch: 250, loss is 3.5969831800460814 and perplexity is 36.487990536352484
At time: 130.3543701171875 and batch: 300, loss is 3.590191283226013 and perplexity is 36.2410075603428
At time: 130.95148158073425 and batch: 350, loss is 3.5582790088653566 and perplexity is 35.10273363805742
At time: 131.53951621055603 and batch: 400, loss is 3.4945264863967895 and perplexity is 32.93468923682276
At time: 132.12636494636536 and batch: 450, loss is 3.532993474006653 and perplexity is 34.2262698679877
At time: 132.715008020401 and batch: 500, loss is 3.3909991073608396 and perplexity is 29.69560655308779
At time: 133.30064725875854 and batch: 550, loss is 3.443372440338135 and perplexity is 31.292311873750936
At time: 133.88652229309082 and batch: 600, loss is 3.4737869834899904 and perplexity is 32.25867448941212
At time: 134.4747200012207 and batch: 650, loss is 3.318219237327576 and perplexity is 27.61113786980044
At time: 135.0626790523529 and batch: 700, loss is 3.3048271131515503 and perplexity is 27.24383108152507
At time: 135.65019488334656 and batch: 750, loss is 3.3977617168426515 and perplexity is 29.89710690971083
At time: 136.2374722957611 and batch: 800, loss is 3.357558584213257 and perplexity is 28.718990223506424
At time: 136.8251657485962 and batch: 850, loss is 3.4168142557144163 and perplexity is 30.472183641222752
At time: 137.41335940361023 and batch: 900, loss is 3.387663893699646 and perplexity is 29.596730338740283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259830840646404 and perplexity of 70.79800629630002
finished 12 epochs...
Completing Train Step...
At time: 138.90151476860046 and batch: 50, loss is 3.6774200201034546 and perplexity is 39.5442390085651
At time: 139.49039769172668 and batch: 100, loss is 3.5544261074066164 and perplexity is 34.96774647752008
At time: 140.06658840179443 and batch: 150, loss is 3.559892511367798 and perplexity is 35.15941770426873
At time: 140.6570806503296 and batch: 200, loss is 3.4588045406341554 and perplexity is 31.7789633382314
At time: 141.23250794410706 and batch: 250, loss is 3.586975336074829 and perplexity is 36.12464560256405
At time: 141.80870127677917 and batch: 300, loss is 3.5802196073532104 and perplexity is 35.88141980552541
At time: 142.3864140510559 and batch: 350, loss is 3.5495073556900025 and perplexity is 34.79617112870448
At time: 142.96709513664246 and batch: 400, loss is 3.4861573362350464 and perplexity is 32.6602040829421
At time: 143.54572939872742 and batch: 450, loss is 3.5259143590927122 and perplexity is 33.98483375578196
At time: 144.1270740032196 and batch: 500, loss is 3.3848090028762816 and perplexity is 29.51235540281722
At time: 144.70871710777283 and batch: 550, loss is 3.4381084203720094 and perplexity is 31.12802131303005
At time: 145.30513763427734 and batch: 600, loss is 3.4692291736602785 and perplexity is 32.11198014214752
At time: 145.88497805595398 and batch: 650, loss is 3.315182309150696 and perplexity is 27.52741202624445
At time: 146.4657769203186 and batch: 700, loss is 3.302652049064636 and perplexity is 27.184638400223577
At time: 147.0469946861267 and batch: 750, loss is 3.397056999206543 and perplexity is 29.87604531331379
At time: 147.62836003303528 and batch: 800, loss is 3.358256850242615 and perplexity is 28.739050721744103
At time: 148.21022081375122 and batch: 850, loss is 3.4186759853363036 and perplexity is 30.528967449799207
At time: 148.79319262504578 and batch: 900, loss is 3.390760841369629 and perplexity is 29.688531942810794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2595457312178935 and perplexity of 70.77782399440419
finished 13 epochs...
Completing Train Step...
At time: 150.30600714683533 and batch: 50, loss is 3.672212829589844 and perplexity is 39.338859810705486
At time: 150.8925199508667 and batch: 100, loss is 3.5487327909469606 and perplexity is 34.76922967665173
At time: 151.48100447654724 and batch: 150, loss is 3.553776969909668 and perplexity is 34.94505496784945
At time: 152.0698914527893 and batch: 200, loss is 3.4530061864852906 and perplexity is 31.59523084212395
At time: 152.65770316123962 and batch: 250, loss is 3.581149878501892 and perplexity is 35.914814785929266
At time: 153.24555778503418 and batch: 300, loss is 3.574647011756897 and perplexity is 35.682023257691654
At time: 153.83451175689697 and batch: 350, loss is 3.544160957336426 and perplexity is 34.61063335803739
At time: 154.42330169677734 and batch: 400, loss is 3.481200084686279 and perplexity is 32.498699874899785
At time: 155.02228212356567 and batch: 450, loss is 3.521485252380371 and perplexity is 33.83464414889662
At time: 155.61028170585632 and batch: 500, loss is 3.3809279584884644 and perplexity is 29.398038619256884
At time: 156.19898509979248 and batch: 550, loss is 3.434636425971985 and perplexity is 31.02013240057839
At time: 156.7869884967804 and batch: 600, loss is 3.4663381814956664 and perplexity is 32.019278723269466
At time: 157.37597274780273 and batch: 650, loss is 3.3128115463256838 and perplexity is 27.462228359181662
At time: 157.96364188194275 and batch: 700, loss is 3.300827770233154 and perplexity is 27.13509124750054
At time: 158.5511531829834 and batch: 750, loss is 3.39589608669281 and perplexity is 29.841381962900375
At time: 159.13956379890442 and batch: 800, loss is 3.357842454910278 and perplexity is 28.727143860513472
At time: 159.72841787338257 and batch: 850, loss is 3.4188320684432982 and perplexity is 30.53373287778384
At time: 160.31600403785706 and batch: 900, loss is 3.3914691686630247 and perplexity is 29.70956858982281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259773567931293 and perplexity of 70.7939516183673
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 161.8117082118988 and batch: 50, loss is 3.6702973651885986 and perplexity is 39.26357974631114
At time: 162.4074625968933 and batch: 100, loss is 3.547849974632263 and perplexity is 34.7385483784117
At time: 162.9887375831604 and batch: 150, loss is 3.5538266134262084 and perplexity is 34.946789806325135
At time: 163.57050323486328 and batch: 200, loss is 3.4535810852050783 and perplexity is 31.61340012213397
At time: 164.15371704101562 and batch: 250, loss is 3.5809349822998047 and perplexity is 35.907097657853555
At time: 164.73694896697998 and batch: 300, loss is 3.5735847568511963 and perplexity is 35.64413997785302
At time: 165.32003617286682 and batch: 350, loss is 3.5431153631210326 and perplexity is 34.57446359275107
At time: 165.90283036231995 and batch: 400, loss is 3.480428695678711 and perplexity is 32.473640401599766
At time: 166.4841537475586 and batch: 450, loss is 3.5195031547546387 and perplexity is 33.76764700044041
At time: 167.0652413368225 and batch: 500, loss is 3.3772223663330077 and perplexity is 29.289303067192296
At time: 167.6479434967041 and batch: 550, loss is 3.4304180002212523 and perplexity is 30.88955189100194
At time: 168.23084139823914 and batch: 600, loss is 3.462884769439697 and perplexity is 31.908893672340753
At time: 168.8128011226654 and batch: 650, loss is 3.3080493450164794 and perplexity is 27.33175860751908
At time: 169.39471077919006 and batch: 700, loss is 3.294777808189392 and perplexity is 26.971420575299472
At time: 169.98943281173706 and batch: 750, loss is 3.388231177330017 and perplexity is 29.61352484254711
At time: 170.57085299491882 and batch: 800, loss is 3.349392056465149 and perplexity is 28.485410857294525
At time: 171.1566767692566 and batch: 850, loss is 3.4102513694763186 and perplexity is 30.272852974259493
At time: 171.74318623542786 and batch: 900, loss is 3.382355833053589 and perplexity is 29.44004531387424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2588609669306505 and perplexity of 70.72937445835323
finished 15 epochs...
Completing Train Step...
At time: 173.22726726531982 and batch: 50, loss is 3.6677442836761474 and perplexity is 39.1634644823441
At time: 173.8227686882019 and batch: 100, loss is 3.544891586303711 and perplexity is 34.635930129496536
At time: 174.4044532775879 and batch: 150, loss is 3.55033034324646 and perplexity is 34.82481973166064
At time: 174.98694372177124 and batch: 200, loss is 3.4503227949142454 and perplexity is 31.510562116474734
At time: 175.56940007209778 and batch: 250, loss is 3.5784246492385865 and perplexity is 35.81707192803042
At time: 176.1509861946106 and batch: 300, loss is 3.571021580696106 and perplexity is 35.55289475693612
At time: 176.73242282867432 and batch: 350, loss is 3.5404487323760985 and perplexity is 34.482389084064074
At time: 177.31505465507507 and batch: 400, loss is 3.477991404533386 and perplexity is 32.3945890600928
At time: 177.89774799346924 and batch: 450, loss is 3.5173948001861572 and perplexity is 33.69652782618281
At time: 178.47995448112488 and batch: 500, loss is 3.3754838132858276 and perplexity is 29.238426298890197
At time: 179.06249165534973 and batch: 550, loss is 3.428799991607666 and perplexity is 30.83961274185407
At time: 179.6459813117981 and batch: 600, loss is 3.461634883880615 and perplexity is 31.86903612081128
At time: 180.23107051849365 and batch: 650, loss is 3.3073340320587157 and perplexity is 27.312214837218058
At time: 180.81553721427917 and batch: 700, loss is 3.2943321323394774 and perplexity is 26.95940274273896
At time: 181.4000780582428 and batch: 750, loss is 3.388295168876648 and perplexity is 29.61541991843675
At time: 181.98562264442444 and batch: 800, loss is 3.3501702690124513 and perplexity is 28.50758718926944
At time: 182.56974864006042 and batch: 850, loss is 3.4115098762512206 and perplexity is 30.31097554855055
At time: 183.15308833122253 and batch: 900, loss is 3.3841263675689697 and perplexity is 29.492216101698386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258682041952055 and perplexity of 70.71672033864498
finished 16 epochs...
Completing Train Step...
At time: 184.65151023864746 and batch: 50, loss is 3.6660994052886964 and perplexity is 39.099098297824845
At time: 185.23057508468628 and batch: 100, loss is 3.5430096626281737 and perplexity is 34.57080924804541
At time: 185.80967497825623 and batch: 150, loss is 3.548287992477417 and perplexity is 34.75376781546536
At time: 186.38794994354248 and batch: 200, loss is 3.448369512557983 and perplexity is 31.449073163650006
At time: 186.96648597717285 and batch: 250, loss is 3.5766402435302735 and perplexity is 35.75321672916114
At time: 187.54634857177734 and batch: 300, loss is 3.569230041503906 and perplexity is 35.48925737403664
At time: 188.12594747543335 and batch: 350, loss is 3.5387259006500242 and perplexity is 34.423032875220585
At time: 188.71016311645508 and batch: 400, loss is 3.476417236328125 and perplexity is 32.34363464390495
At time: 189.29189729690552 and batch: 450, loss is 3.516030344963074 and perplexity is 33.65058177558325
At time: 189.87518501281738 and batch: 500, loss is 3.374403839111328 and perplexity is 29.20686659848254
At time: 190.46310758590698 and batch: 550, loss is 3.4277463006973266 and perplexity is 30.807134436282233
At time: 191.04927778244019 and batch: 600, loss is 3.4608242654800416 and perplexity is 31.843212961501298
At time: 191.63698649406433 and batch: 650, loss is 3.306840147972107 and perplexity is 27.298729099409684
At time: 192.22230744361877 and batch: 700, loss is 3.2940218544006346 and perplexity is 26.951039132412493
At time: 192.80856323242188 and batch: 750, loss is 3.388269138336182 and perplexity is 29.614649023083594
At time: 193.4195568561554 and batch: 800, loss is 3.3504968738555907 and perplexity is 28.51689942593969
At time: 194.00782823562622 and batch: 850, loss is 3.4121113061904906 and perplexity is 30.329210959845412
At time: 194.59417390823364 and batch: 900, loss is 3.3849544095993043 and perplexity is 29.51664700971171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258639400952483 and perplexity of 70.71370497129293
finished 17 epochs...
Completing Train Step...
At time: 196.0905478000641 and batch: 50, loss is 3.6647008800506593 and perplexity is 39.044455440684445
At time: 196.68006563186646 and batch: 100, loss is 3.541462984085083 and perplexity is 34.51738064824967
At time: 197.25717663764954 and batch: 150, loss is 3.5466739082336427 and perplexity is 34.6977175535138
At time: 197.8390176296234 and batch: 200, loss is 3.4468169164657594 and perplexity is 31.400283340797092
At time: 198.4218225479126 and batch: 250, loss is 3.5751294183731077 and perplexity is 35.699240654350085
At time: 199.0164496898651 and batch: 300, loss is 3.567758460044861 and perplexity is 35.43707044896318
At time: 199.59984374046326 and batch: 350, loss is 3.5373189640045166 and perplexity is 34.37463590253949
At time: 200.1814670562744 and batch: 400, loss is 3.475139708518982 and perplexity is 32.30234113361865
At time: 200.76278495788574 and batch: 450, loss is 3.514904203414917 and perplexity is 33.61270778706478
At time: 201.3451063632965 and batch: 500, loss is 3.37348934173584 and perplexity is 29.1801692048408
At time: 201.92717027664185 and batch: 550, loss is 3.426876606941223 and perplexity is 30.780353311196222
At time: 202.5075397491455 and batch: 600, loss is 3.4601376104354857 and perplexity is 31.82135516392872
At time: 203.086110830307 and batch: 650, loss is 3.306360812187195 and perplexity is 27.285646977284845
At time: 203.69078469276428 and batch: 700, loss is 3.293682508468628 and perplexity is 26.94189495852642
At time: 204.29255485534668 and batch: 750, loss is 3.3881242370605467 and perplexity is 29.61035813354785
At time: 204.87956547737122 and batch: 800, loss is 3.350582966804504 and perplexity is 28.519354635591743
At time: 205.46719574928284 and batch: 850, loss is 3.4123851680755615 and perplexity is 30.337518112185865
At time: 206.0498607158661 and batch: 900, loss is 3.3853731870651247 and perplexity is 29.52901050494187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258664483893408 and perplexity of 70.71547870122235
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 207.54152607917786 and batch: 50, loss is 3.6641002464294434 and perplexity is 39.021011069468315
At time: 208.13355159759521 and batch: 100, loss is 3.54108051776886 and perplexity is 34.50418143711519
At time: 208.7165286540985 and batch: 150, loss is 3.5463987970352173 and perplexity is 34.688173135803325
At time: 209.29979181289673 and batch: 200, loss is 3.4467070055007936 and perplexity is 31.396832295012263
At time: 209.8838288784027 and batch: 250, loss is 3.5750544548034666 and perplexity is 35.69656461214125
At time: 210.4659402370453 and batch: 300, loss is 3.56739324092865 and perplexity is 35.42413051651189
At time: 211.04547548294067 and batch: 350, loss is 3.5368549966812135 and perplexity is 34.358690893997775
At time: 211.62749981880188 and batch: 400, loss is 3.4746168994903566 and perplexity is 32.28545759184685
At time: 212.21118831634521 and batch: 450, loss is 3.51425724029541 and perplexity is 33.59096863775197
At time: 212.79604506492615 and batch: 500, loss is 3.3720329427719116 and perplexity is 29.13770216862379
At time: 213.38158655166626 and batch: 550, loss is 3.4254806423187256 and perplexity is 30.737415004057564
At time: 213.97647619247437 and batch: 600, loss is 3.4585908460617065 and perplexity is 31.772173071795347
At time: 214.55941534042358 and batch: 650, loss is 3.3046616744995116 and perplexity is 27.239324271645128
At time: 215.1423852443695 and batch: 700, loss is 3.2918152618408203 and perplexity is 26.891634734738197
At time: 215.72506499290466 and batch: 750, loss is 3.38571093082428 and perplexity is 29.53898542834318
At time: 216.30925631523132 and batch: 800, loss is 3.3476858949661255 and perplexity is 28.43685158276787
At time: 216.88988280296326 and batch: 850, loss is 3.4094503211975096 and perplexity is 30.248612667613987
At time: 217.46964073181152 and batch: 900, loss is 3.382154130935669 and perplexity is 29.43410779320815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258227622672303 and perplexity of 70.68459259780757
finished 19 epochs...
Completing Train Step...
At time: 218.95566177368164 and batch: 50, loss is 3.6635435724258425 and perplexity is 38.99929513192146
At time: 219.5350260734558 and batch: 100, loss is 3.540527791976929 and perplexity is 34.4851153557483
At time: 220.11583971977234 and batch: 150, loss is 3.545721163749695 and perplexity is 34.66467523745074
At time: 220.69647026062012 and batch: 200, loss is 3.4460969877243044 and perplexity is 31.377685509720298
At time: 221.27818274497986 and batch: 250, loss is 3.5745338201522827 and perplexity is 35.67798458079971
At time: 221.85678935050964 and batch: 300, loss is 3.5668806505203245 and perplexity is 35.405977100016926
At time: 222.43785953521729 and batch: 350, loss is 3.536371054649353 and perplexity is 34.342067302066326
At time: 223.01919794082642 and batch: 400, loss is 3.4741912126541137 and perplexity is 32.27171702234519
At time: 223.60320782661438 and batch: 450, loss is 3.5138125610351563 and perplexity is 33.57603475130781
At time: 224.21558332443237 and batch: 500, loss is 3.371747179031372 and perplexity is 29.129376859453753
At time: 224.79843139648438 and batch: 550, loss is 3.425215902328491 and perplexity is 30.729278658165143
At time: 225.38057208061218 and batch: 600, loss is 3.4584360218048094 and perplexity is 31.767254349488088
At time: 225.96291875839233 and batch: 650, loss is 3.304555883407593 and perplexity is 27.23644274620985
At time: 226.54405784606934 and batch: 700, loss is 3.2917413187026976 and perplexity is 26.889646356391037
At time: 227.12566089630127 and batch: 750, loss is 3.3857551050186157 and perplexity is 29.54029031804699
At time: 227.70848059654236 and batch: 800, loss is 3.3478914976119993 and perplexity is 28.44269887578247
At time: 228.3010823726654 and batch: 850, loss is 3.409740381240845 and perplexity is 30.257387854119152
At time: 228.88341212272644 and batch: 900, loss is 3.382490801811218 and perplexity is 29.444019068375127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258024450850813 and perplexity of 70.67023293916662
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f52e60f0b70>
ELAPSED
1190.6685581207275


RESULTS SO FAR:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.27715916891746, 'params': {'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.64645432534728, 'params': {'batch_size': 32, 'dropout': 0.43751170550744745, 'num_layers': 2, 'rnn_dropout': 0.9329965743746119, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -71.09453673611982, 'params': {'batch_size': 32, 'dropout': 0.7029347552002905, 'num_layers': 2, 'rnn_dropout': 0.44946109581267146, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.67023293916662, 'params': {'batch_size': 32, 'dropout': 0.8672620102056839, 'num_layers': 2, 'rnn_dropout': 0.43740970994803685, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'dropout': 0.3922418214619182, 'num_layers': 2, 'rnn_dropout': 0.30099019980318864, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8469221591949463 and batch: 50, loss is 6.931897745132447 and perplexity is 1024.436254984312
At time: 1.4757256507873535 and batch: 100, loss is 5.999510889053345 and perplexity is 403.23152030181325
At time: 2.1051809787750244 and batch: 150, loss is 5.780596151351928 and perplexity is 323.95225743917433
At time: 2.735629081726074 and batch: 200, loss is 5.562312326431274 and perplexity is 260.4243266878568
At time: 3.3638224601745605 and batch: 250, loss is 5.569266357421875 and perplexity is 262.2416370189969
At time: 3.9946019649505615 and batch: 300, loss is 5.4502001476287845 and perplexity is 232.80475656499794
At time: 4.623760461807251 and batch: 350, loss is 5.414146223068237 and perplexity is 224.56073903145696
At time: 5.277746677398682 and batch: 400, loss is 5.241619100570679 and perplexity is 188.97582570774
At time: 5.919469833374023 and batch: 450, loss is 5.229564037322998 and perplexity is 186.71138657816425
At time: 6.558332920074463 and batch: 500, loss is 5.157162313461304 and perplexity is 173.67093203768607
At time: 7.196662425994873 and batch: 550, loss is 5.198704481124878 and perplexity is 181.03755231946545
At time: 7.840259075164795 and batch: 600, loss is 5.117899742126465 and perplexity is 166.98429100040815
At time: 8.483436584472656 and batch: 650, loss is 4.99900411605835 and perplexity is 148.26543039323104
At time: 9.113218069076538 and batch: 700, loss is 5.07497838973999 and perplexity is 159.968734976077
At time: 9.743518352508545 and batch: 750, loss is 5.066354465484619 and perplexity is 158.59510826206213
At time: 10.37167763710022 and batch: 800, loss is 5.02493540763855 and perplexity is 152.16042739391824
At time: 11.001449346542358 and batch: 850, loss is 5.059567146301269 and perplexity is 157.52231744662433
At time: 11.62935209274292 and batch: 900, loss is 4.973798322677612 and perplexity is 144.57498823641419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.887185292701199 and perplexity of 132.57987482968394
finished 1 epochs...
Completing Train Step...
At time: 13.201883316040039 and batch: 50, loss is 4.849047203063964 and perplexity is 127.61873715833286
At time: 13.76072096824646 and batch: 100, loss is 4.710960111618042 and perplexity is 111.15883357575383
At time: 14.337728023529053 and batch: 150, loss is 4.689417953491211 and perplexity is 108.78984062910047
At time: 14.895462274551392 and batch: 200, loss is 4.575465993881226 and perplexity is 97.07326414594883
At time: 15.454497814178467 and batch: 250, loss is 4.679597673416137 and perplexity is 107.72672252728252
At time: 16.011788845062256 and batch: 300, loss is 4.627127285003662 and perplexity is 102.21999301464615
At time: 16.569448947906494 and batch: 350, loss is 4.612908687591553 and perplexity is 100.77685211925788
At time: 17.12611198425293 and batch: 400, loss is 4.491262550354004 and perplexity is 89.2340372528856
At time: 17.68331003189087 and batch: 450, loss is 4.512423324584961 and perplexity is 91.14241877369977
At time: 18.237779140472412 and batch: 500, loss is 4.403769674301148 and perplexity is 81.75849136578763
At time: 18.793184518814087 and batch: 550, loss is 4.481536989212036 and perplexity is 88.3703926890617
At time: 19.34851312637329 and batch: 600, loss is 4.451385631561279 and perplexity is 85.74567363707406
At time: 19.90409255027771 and batch: 650, loss is 4.301910572052002 and perplexity is 73.84073706402941
At time: 20.46015954017639 and batch: 700, loss is 4.342671914100647 and perplexity is 76.9127693506646
At time: 21.018061876296997 and batch: 750, loss is 4.400428524017334 and perplexity is 81.48577979801905
At time: 21.582327842712402 and batch: 800, loss is 4.35284574508667 and perplexity is 77.69926089882068
At time: 22.159282445907593 and batch: 850, loss is 4.420126304626465 and perplexity is 83.10678146646615
At time: 22.742668390274048 and batch: 900, loss is 4.355075139999389 and perplexity is 77.87267646986062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499109555597174 and perplexity of 89.93701172610398
finished 2 epochs...
Completing Train Step...
At time: 24.231061220169067 and batch: 50, loss is 4.405319166183472 and perplexity is 81.88527368318955
At time: 24.818968534469604 and batch: 100, loss is 4.272417950630188 and perplexity is 71.69478064684363
At time: 25.39361310005188 and batch: 150, loss is 4.27322247505188 and perplexity is 71.75248405757104
At time: 25.973856687545776 and batch: 200, loss is 4.1736878490448 and perplexity is 64.95455353964923
At time: 26.556761741638184 and batch: 250, loss is 4.310221481323242 and perplexity is 74.45697794427454
At time: 27.13706946372986 and batch: 300, loss is 4.2781750583648686 and perplexity is 72.10872564268503
At time: 27.719607830047607 and batch: 350, loss is 4.26996636390686 and perplexity is 71.5192299512809
At time: 28.322179555892944 and batch: 400, loss is 4.180903830528259 and perplexity is 65.42495957469335
At time: 28.92726969718933 and batch: 450, loss is 4.214168710708618 and perplexity is 67.63791581902649
At time: 29.516221523284912 and batch: 500, loss is 4.0902313709259035 and perplexity is 59.75371537733853
At time: 30.103159427642822 and batch: 550, loss is 4.175009803771973 and perplexity is 65.04047729989978
At time: 30.68915057182312 and batch: 600, loss is 4.17312493801117 and perplexity is 64.91800019388327
At time: 31.279271364212036 and batch: 650, loss is 4.021677942276001 and perplexity is 55.79464752391933
At time: 31.871185064315796 and batch: 700, loss is 4.042875356674195 and perplexity is 56.989973945733006
At time: 32.46108388900757 and batch: 750, loss is 4.132476778030395 and perplexity is 62.33211471884967
At time: 33.05263328552246 and batch: 800, loss is 4.086590514183045 and perplexity is 59.536556222368404
At time: 33.643102407455444 and batch: 850, loss is 4.163819904327393 and perplexity is 64.31673773738633
At time: 34.23536467552185 and batch: 900, loss is 4.11084258556366 and perplexity is 60.99809203465066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379999866224315 and perplexity of 79.83802272469764
finished 3 epochs...
Completing Train Step...
At time: 35.740142822265625 and batch: 50, loss is 4.187153120040893 and perplexity is 65.83509929456719
At time: 36.34445309638977 and batch: 100, loss is 4.0558721494674685 and perplexity is 57.73549502568699
At time: 36.933085441589355 and batch: 150, loss is 4.0632433366775516 and perplexity is 58.16264654099797
At time: 37.52244210243225 and batch: 200, loss is 3.9660500574111937 and perplexity is 52.77565777464275
At time: 38.111323595047 and batch: 250, loss is 4.111403970718384 and perplexity is 61.03234507165909
At time: 38.70086717605591 and batch: 300, loss is 4.083749322891236 and perplexity is 59.36764155036112
At time: 39.29042673110962 and batch: 350, loss is 4.075570182800293 and perplexity is 58.88404568837986
At time: 39.87932062149048 and batch: 400, loss is 4.000463900566101 and perplexity is 54.62348402162413
At time: 40.46828269958496 and batch: 450, loss is 4.040128364562988 and perplexity is 56.83363776232641
At time: 41.056072473526 and batch: 500, loss is 3.910658431053162 and perplexity is 49.93181781166427
At time: 41.64309620857239 and batch: 550, loss is 3.9921333026885986 and perplexity is 54.170327893020286
At time: 42.23385286331177 and batch: 600, loss is 4.003168478012085 and perplexity is 54.771417423026044
At time: 42.82255458831787 and batch: 650, loss is 3.8546328592300414 and perplexity is 47.211280573125336
At time: 43.431384325027466 and batch: 700, loss is 3.865297164916992 and perplexity is 47.717450278156214
At time: 44.01982927322388 and batch: 750, loss is 3.967592601776123 and perplexity is 52.857129388783356
At time: 44.60840463638306 and batch: 800, loss is 3.927905311584473 and perplexity is 50.800455017639656
At time: 45.197760581970215 and batch: 850, loss is 4.002218189239502 and perplexity is 54.71939348278869
At time: 45.787128925323486 and batch: 900, loss is 3.9547721672058107 and perplexity is 52.183803408214146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3381063382919525 and perplexity of 76.5624186564376
finished 4 epochs...
Completing Train Step...
At time: 47.307464361190796 and batch: 50, loss is 4.040700998306274 and perplexity is 56.86619194098385
At time: 47.90867304801941 and batch: 100, loss is 3.913599581718445 and perplexity is 50.07889098700386
At time: 48.49130415916443 and batch: 150, loss is 3.9220592212677 and perplexity is 50.50433737806765
At time: 49.07477593421936 and batch: 200, loss is 3.826384811401367 and perplexity is 45.896314117971
At time: 49.65702772140503 and batch: 250, loss is 3.9759709453582763 and perplexity is 53.3018449680701
At time: 50.24051356315613 and batch: 300, loss is 3.9507953214645384 and perplexity is 51.97668857669986
At time: 50.82334327697754 and batch: 350, loss is 3.9429911518096925 and perplexity is 51.572632393206796
At time: 51.40709829330444 and batch: 400, loss is 3.8730949544906617 and perplexity is 48.09099543488276
At time: 51.98963928222656 and batch: 450, loss is 3.9142953824996947 and perplexity is 50.11374804385412
At time: 52.57253384590149 and batch: 500, loss is 3.7848185396194456 and perplexity is 44.027680554168924
At time: 53.1553680896759 and batch: 550, loss is 3.863817982673645 and perplexity is 47.646919649693565
At time: 53.73866248130798 and batch: 600, loss is 3.885093846321106 and perplexity is 48.671509887314514
At time: 54.32105779647827 and batch: 650, loss is 3.7327750301361085 and perplexity is 41.79492957485695
At time: 54.90428924560547 and batch: 700, loss is 3.7418029260635377 and perplexity is 42.17395819047639
At time: 55.486896991729736 and batch: 750, loss is 3.846107940673828 and perplexity is 46.810518908664285
At time: 56.071110010147095 and batch: 800, loss is 3.815136203765869 and perplexity is 45.38293728897131
At time: 56.65851831436157 and batch: 850, loss is 3.8852218103408815 and perplexity is 48.67773848787812
At time: 57.246159076690674 and batch: 900, loss is 3.8425606536865233 and perplexity is 46.644762730250484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324806526915668 and perplexity of 75.55089438107119
finished 5 epochs...
Completing Train Step...
At time: 58.742323875427246 and batch: 50, loss is 3.929237518310547 and perplexity is 50.868176825209645
At time: 59.33908987045288 and batch: 100, loss is 3.807339115142822 and perplexity is 45.030458445078935
At time: 59.9227876663208 and batch: 150, loss is 3.8170940494537353 and perplexity is 45.47187711386151
At time: 60.50600290298462 and batch: 200, loss is 3.7226335525512697 and perplexity is 41.373209281338475
At time: 61.09021329879761 and batch: 250, loss is 3.871506576538086 and perplexity is 48.01466939136613
At time: 61.675644397735596 and batch: 300, loss is 3.848239803314209 and perplexity is 46.91041895385377
At time: 62.26013898849487 and batch: 350, loss is 3.842866129875183 and perplexity is 46.65901377115627
At time: 62.84428548812866 and batch: 400, loss is 3.7739565849304197 and perplexity is 43.55204174292934
At time: 63.43049097061157 and batch: 450, loss is 3.817418179512024 and perplexity is 45.48661830494342
At time: 64.01863312721252 and batch: 500, loss is 3.689892930984497 and perplexity is 40.040559624471385
At time: 64.60736751556396 and batch: 550, loss is 3.7658802318572997 and perplexity is 43.201716655539364
At time: 65.19541144371033 and batch: 600, loss is 3.7904984092712404 and perplexity is 44.27846357401206
At time: 65.78355932235718 and batch: 650, loss is 3.6417615604400635 and perplexity is 38.15899693408017
At time: 66.37231969833374 and batch: 700, loss is 3.649350390434265 and perplexity is 38.4496806541826
At time: 66.9612786769867 and batch: 750, loss is 3.7517608070373534 and perplexity is 42.59601937625247
At time: 67.54977583885193 and batch: 800, loss is 3.7249310970306397 and perplexity is 41.46837535219415
At time: 68.1408839225769 and batch: 850, loss is 3.7958688306808472 and perplexity is 44.516897254381675
At time: 68.73317503929138 and batch: 900, loss is 3.754660978317261 and perplexity is 42.719734439007944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326338676557149 and perplexity of 75.66673837931907
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 70.23188495635986 and batch: 50, loss is 3.858930730819702 and perplexity is 47.4146252562703
At time: 70.82603740692139 and batch: 100, loss is 3.7350598573684692 and perplexity is 41.8905329451252
At time: 71.40737628936768 and batch: 150, loss is 3.7422152280807497 and perplexity is 42.19135018364269
At time: 71.98958492279053 and batch: 200, loss is 3.6337694120407105 and perplexity is 37.85524001983727
At time: 72.57205104827881 and batch: 250, loss is 3.7743970346450806 and perplexity is 43.571228452368075
At time: 73.16575312614441 and batch: 300, loss is 3.7388360786437986 and perplexity is 42.04901991947638
At time: 73.74767279624939 and batch: 350, loss is 3.721836724281311 and perplexity is 41.34025506973155
At time: 74.32903599739075 and batch: 400, loss is 3.6460912322998045 and perplexity is 38.324571051444835
At time: 74.90937399864197 and batch: 450, loss is 3.6786198139190676 and perplexity is 39.591712415425825
At time: 75.49218082427979 and batch: 500, loss is 3.5412004518508913 and perplexity is 34.50831991260927
At time: 76.07446432113647 and batch: 550, loss is 3.5996405363082884 and perplexity is 36.58508107143624
At time: 76.65655469894409 and batch: 600, loss is 3.621872024536133 and perplexity is 37.4075301302831
At time: 77.24679064750671 and batch: 650, loss is 3.463641710281372 and perplexity is 31.933055960731792
At time: 77.83744883537292 and batch: 700, loss is 3.456517367362976 and perplexity is 31.706362399810246
At time: 78.4286060333252 and batch: 750, loss is 3.5440577554702757 and perplexity is 34.6070616603923
At time: 79.01697778701782 and batch: 800, loss is 3.505602159500122 and perplexity is 33.30149062506882
At time: 79.60423445701599 and batch: 850, loss is 3.5567322874069216 and perplexity is 35.04848145401416
At time: 80.193439245224 and batch: 900, loss is 3.50919086933136 and perplexity is 33.42121471087954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267871177359803 and perplexity of 71.3695406910521
finished 7 epochs...
Completing Train Step...
At time: 81.71890449523926 and batch: 50, loss is 3.761947207450867 and perplexity is 43.03213694869407
At time: 82.30646395683289 and batch: 100, loss is 3.6420944261550905 and perplexity is 38.17170087011443
At time: 82.89441108703613 and batch: 150, loss is 3.649868359565735 and perplexity is 38.469601560638274
At time: 83.48286533355713 and batch: 200, loss is 3.547154417037964 and perplexity is 34.714394118585425
At time: 84.07159638404846 and batch: 250, loss is 3.689851789474487 and perplexity is 40.03891232927313
At time: 84.66034984588623 and batch: 300, loss is 3.6579030036926268 and perplexity is 38.779936163446074
At time: 85.24945878982544 and batch: 350, loss is 3.6448220205307007 and perplexity is 38.27595991026237
At time: 85.83773851394653 and batch: 400, loss is 3.5757369661331175 and perplexity is 35.720936237927944
At time: 86.42511796951294 and batch: 450, loss is 3.613667483329773 and perplexity is 37.10187410621147
At time: 87.01339626312256 and batch: 500, loss is 3.4799018049240114 and perplexity is 32.456534847475616
At time: 87.61471009254456 and batch: 550, loss is 3.540698184967041 and perplexity is 34.49099187831304
At time: 88.20282673835754 and batch: 600, loss is 3.569142670631409 and perplexity is 35.486156782108274
At time: 88.80360245704651 and batch: 650, loss is 3.4145564460754394 and perplexity is 30.40346086194769
At time: 89.40515613555908 and batch: 700, loss is 3.414426589012146 and perplexity is 30.399513014139707
At time: 89.99413752555847 and batch: 750, loss is 3.5069480991363524 and perplexity is 33.34634259850008
At time: 90.58299589157104 and batch: 800, loss is 3.474160509109497 and perplexity is 32.27072618145299
At time: 91.17127513885498 and batch: 850, loss is 3.5332109832763674 and perplexity is 34.23371520863769
At time: 91.75958919525146 and batch: 900, loss is 3.493282675743103 and perplexity is 32.89375018496652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2707582238602315 and perplexity of 71.57588559408214
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 93.2508316040039 and batch: 50, loss is 3.7332463026046754 and perplexity is 41.81463101650016
At time: 93.83874797821045 and batch: 100, loss is 3.6209251260757447 and perplexity is 37.37212576241187
At time: 94.41320872306824 and batch: 150, loss is 3.6311814165115357 and perplexity is 37.75739749053445
At time: 94.99036478996277 and batch: 200, loss is 3.5231352043151856 and perplexity is 33.89051576553972
At time: 95.57090759277344 and batch: 250, loss is 3.6678588247299193 and perplexity is 39.16795056375061
At time: 96.15589547157288 and batch: 300, loss is 3.6274800205230715 and perplexity is 37.61790073655355
At time: 96.74098896980286 and batch: 350, loss is 3.6108817195892335 and perplexity is 36.99866088119888
At time: 97.32679843902588 and batch: 400, loss is 3.53985182762146 and perplexity is 34.46181252380277
At time: 97.91236567497253 and batch: 450, loss is 3.5736263036727904 and perplexity is 35.64562090934134
At time: 98.4986960887909 and batch: 500, loss is 3.43568208694458 and perplexity is 31.052585907119933
At time: 99.08345055580139 and batch: 550, loss is 3.4923017406463623 and perplexity is 32.86149937151015
At time: 99.66978144645691 and batch: 600, loss is 3.5232580423355104 and perplexity is 33.89467906510465
At time: 100.25617146492004 and batch: 650, loss is 3.358569173812866 and perplexity is 28.748028006506985
At time: 100.8410632610321 and batch: 700, loss is 3.356657247543335 and perplexity is 28.693116406756435
At time: 101.42529058456421 and batch: 750, loss is 3.442900528907776 and perplexity is 31.27754815795247
At time: 102.03199172019958 and batch: 800, loss is 3.405779490470886 and perplexity is 30.13777868165387
At time: 102.67236590385437 and batch: 850, loss is 3.463130922317505 and perplexity is 31.916749105124897
At time: 103.2741231918335 and batch: 900, loss is 3.425400371551514 and perplexity is 30.734947787197097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2571126859482025 and perplexity of 70.60582766680855
finished 9 epochs...
Completing Train Step...
At time: 104.7546980381012 and batch: 50, loss is 3.7063056564331056 and perplexity is 40.7031569766037
At time: 105.34958720207214 and batch: 100, loss is 3.5881550693511963 and perplexity is 36.1672881975768
At time: 105.93178176879883 and batch: 150, loss is 3.597181043624878 and perplexity is 36.49521089504255
At time: 106.51487827301025 and batch: 200, loss is 3.492165846824646 and perplexity is 32.85703400018828
At time: 107.09763479232788 and batch: 250, loss is 3.636989965438843 and perplexity is 37.97735136923638
At time: 107.67928743362427 and batch: 300, loss is 3.599125623703003 and perplexity is 36.56624780118741
At time: 108.26136636734009 and batch: 350, loss is 3.5840068197250368 and perplexity is 36.01756801088461
At time: 108.84339046478271 and batch: 400, loss is 3.514809765815735 and perplexity is 33.60953363352144
At time: 109.42497444152832 and batch: 450, loss is 3.5518444061279295 and perplexity is 34.877586634694666
At time: 110.00931525230408 and batch: 500, loss is 3.4153827810287476 and perplexity is 30.42859468740852
At time: 110.59283924102783 and batch: 550, loss is 3.47449845790863 and perplexity is 32.28163387763076
At time: 111.1755621433258 and batch: 600, loss is 3.507435278892517 and perplexity is 33.362592219477996
At time: 111.75949883460999 and batch: 650, loss is 3.345667014122009 and perplexity is 28.37949888145568
At time: 112.34500169754028 and batch: 700, loss is 3.346751890182495 and perplexity is 28.41030382715349
At time: 112.9306423664093 and batch: 750, loss is 3.4357543277740477 and perplexity is 31.05482925271258
At time: 113.51643252372742 and batch: 800, loss is 3.4015943813323974 and perplexity is 30.011912354551217
At time: 114.1033706665039 and batch: 850, loss is 3.462064070701599 and perplexity is 31.88271682669113
At time: 114.68960571289062 and batch: 900, loss is 3.4268015575408937 and perplexity is 30.778043350819946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2574441988174225 and perplexity of 70.62923828756865
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 116.2191891670227 and batch: 50, loss is 3.698566212654114 and perplexity is 40.389353081733255
At time: 116.80191469192505 and batch: 100, loss is 3.5847621393203735 and perplexity is 36.04478306251663
At time: 117.3922438621521 and batch: 150, loss is 3.5955519485473633 and perplexity is 36.435805128579766
At time: 117.97431635856628 and batch: 200, loss is 3.4884245347976686 and perplexity is 32.73433525398209
At time: 118.55778813362122 and batch: 250, loss is 3.6335101079940797 and perplexity is 37.845425275470404
At time: 119.14183926582336 and batch: 300, loss is 3.593935737609863 and perplexity is 36.37696474385927
At time: 119.72482204437256 and batch: 350, loss is 3.577315802574158 and perplexity is 35.77737829843489
At time: 120.30613231658936 and batch: 400, loss is 3.50572904586792 and perplexity is 33.30571639834733
At time: 120.88735342025757 and batch: 450, loss is 3.5403258752822877 and perplexity is 34.47815293817917
At time: 121.4715485572815 and batch: 500, loss is 3.4053279972076416 and perplexity is 30.124174748883043
At time: 122.05478763580322 and batch: 550, loss is 3.4630293607711793 and perplexity is 31.91350775533312
At time: 122.6411783695221 and batch: 600, loss is 3.495496349334717 and perplexity is 32.96664686604076
At time: 123.22916674613953 and batch: 650, loss is 3.3299455499649047 and perplexity is 27.936820500965393
At time: 123.84489965438843 and batch: 700, loss is 3.3280642938613894 and perplexity is 27.884313591842478
At time: 124.43410348892212 and batch: 750, loss is 3.413457350730896 and perplexity is 30.37006291678143
At time: 125.02102446556091 and batch: 800, loss is 3.3799879693984987 and perplexity is 29.370417767371872
At time: 125.61040616035461 and batch: 850, loss is 3.4386211681365966 and perplexity is 31.14398622901204
At time: 126.20076394081116 and batch: 900, loss is 3.4052374076843264 and perplexity is 30.12144593785499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2517679292861725 and perplexity of 70.22946338271366
finished 11 epochs...
Completing Train Step...
At time: 127.70263743400574 and batch: 50, loss is 3.688941283226013 and perplexity is 40.00247324094143
At time: 128.30207920074463 and batch: 100, loss is 3.5726936721801756 and perplexity is 35.61239217819658
At time: 128.89002513885498 and batch: 150, loss is 3.5823443126678467 and perplexity is 35.95773829732463
At time: 129.47702407836914 and batch: 200, loss is 3.4769565153121946 and perplexity is 32.36108159029976
At time: 130.06244564056396 and batch: 250, loss is 3.622681760787964 and perplexity is 37.43783263038176
At time: 130.64978313446045 and batch: 300, loss is 3.5841329860687257 and perplexity is 36.02211250242402
At time: 131.238374710083 and batch: 350, loss is 3.5681247329711914 and perplexity is 35.450052465792616
At time: 131.8368091583252 and batch: 400, loss is 3.4972917079925536 and perplexity is 33.025886983628105
At time: 132.4234869480133 and batch: 450, loss is 3.532802019119263 and perplexity is 34.21971770858583
At time: 133.01134824752808 and batch: 500, loss is 3.3985298681259155 and perplexity is 29.920081233497875
At time: 133.59639525413513 and batch: 550, loss is 3.45739230632782 and perplexity is 31.734115671146085
At time: 134.1849126815796 and batch: 600, loss is 3.4910006093978883 and perplexity is 32.818770052060735
At time: 134.7731056213379 and batch: 650, loss is 3.326536784172058 and perplexity is 27.841752547130266
At time: 135.36233353614807 and batch: 700, loss is 3.325991506576538 and perplexity is 27.826575201555617
At time: 135.94930338859558 and batch: 750, loss is 3.412566623687744 and perplexity is 30.3430235245859
At time: 136.53603506088257 and batch: 800, loss is 3.3803392362594606 and perplexity is 29.38073643402288
At time: 137.13310837745667 and batch: 850, loss is 3.440511360168457 and perplexity is 31.202910014713904
At time: 137.74299597740173 and batch: 900, loss is 3.408511176109314 and perplexity is 30.220218166967356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.251407152985873 and perplexity of 70.20413082671013
finished 12 epochs...
Completing Train Step...
At time: 139.22237873077393 and batch: 50, loss is 3.6837305307388304 and perplexity is 39.79457238436958
At time: 139.8161277770996 and batch: 100, loss is 3.566872262954712 and perplexity is 35.40568013130634
At time: 140.3970592021942 and batch: 150, loss is 3.575935192108154 and perplexity is 35.72801775719027
At time: 140.97710037231445 and batch: 200, loss is 3.47094144821167 and perplexity is 32.16701176972189
At time: 141.55983138084412 and batch: 250, loss is 3.61649986743927 and perplexity is 37.20710982850266
At time: 142.1426591873169 and batch: 300, loss is 3.5784219217300417 and perplexity is 35.81697423679392
At time: 142.72482991218567 and batch: 350, loss is 3.5627199029922485 and perplexity is 35.25896781466237
At time: 143.30478715896606 and batch: 400, loss is 3.492242622375488 and perplexity is 32.85955671391281
At time: 143.88410592079163 and batch: 450, loss is 3.5282324314117433 and perplexity is 34.06370443683201
At time: 144.47742986679077 and batch: 500, loss is 3.3943734312057496 and perplexity is 29.795978394681516
At time: 145.07430124282837 and batch: 550, loss is 3.4538436222076414 and perplexity is 31.621700899025726
At time: 145.65576243400574 and batch: 600, loss is 3.4878694725036623 and perplexity is 32.71617070045898
At time: 146.2367148399353 and batch: 650, loss is 3.3240156030654906 and perplexity is 27.771646858338453
At time: 146.82977962493896 and batch: 700, loss is 3.324228048324585 and perplexity is 27.777547439803836
At time: 147.41003012657166 and batch: 750, loss is 3.411400194168091 and perplexity is 30.307651159926962
At time: 147.99403762817383 and batch: 800, loss is 3.379835386276245 and perplexity is 29.365936679204943
At time: 148.57728147506714 and batch: 850, loss is 3.4406747388839722 and perplexity is 31.208008322538593
At time: 149.1767406463623 and batch: 900, loss is 3.409214429855347 and perplexity is 30.241478123294964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.251590258454623 and perplexity of 70.21698676395363
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 150.71718883514404 and batch: 50, loss is 3.6812889432907103 and perplexity is 39.69752897412792
At time: 151.29738569259644 and batch: 100, loss is 3.566299991607666 and perplexity is 35.38542427152142
At time: 151.87625288963318 and batch: 150, loss is 3.5757320499420167 and perplexity is 35.72076062741077
At time: 152.45509719848633 and batch: 200, loss is 3.4705112838745116 and perplexity is 32.15317764411266
At time: 153.03505635261536 and batch: 250, loss is 3.615774402618408 and perplexity is 37.18012716790507
At time: 153.61288690567017 and batch: 300, loss is 3.577119665145874 and perplexity is 35.770361703595626
At time: 154.19351959228516 and batch: 350, loss is 3.5613876485824587 and perplexity is 35.21202517601853
At time: 154.77516388893127 and batch: 400, loss is 3.4907888984680175 and perplexity is 32.81182269517744
At time: 155.35867476463318 and batch: 450, loss is 3.524737210273743 and perplexity is 33.94485208566817
At time: 155.9406714439392 and batch: 500, loss is 3.3913322496414184 and perplexity is 29.70550106322638
At time: 156.52374911308289 and batch: 550, loss is 3.450543203353882 and perplexity is 31.517508075748797
At time: 157.10711288452148 and batch: 600, loss is 3.4841994428634644 and perplexity is 32.59632144395833
At time: 157.68993282318115 and batch: 650, loss is 3.3188826608657838 and perplexity is 27.62946182617988
At time: 158.27406358718872 and batch: 700, loss is 3.318057894706726 and perplexity is 27.606683375811414
At time: 158.8583333492279 and batch: 750, loss is 3.403822784423828 and perplexity is 30.0788655644856
At time: 159.44357776641846 and batch: 800, loss is 3.3719372653961184 and perplexity is 29.134914483104996
At time: 160.031005859375 and batch: 850, loss is 3.431659989356995 and perplexity is 30.927940212863245
At time: 160.61696982383728 and batch: 900, loss is 3.3996484756469725 and perplexity is 29.953568787619385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.250427664142766 and perplexity of 70.13540032969782
finished 14 epochs...
Completing Train Step...
At time: 162.1329207420349 and batch: 50, loss is 3.678339133262634 and perplexity is 39.58060134699967
At time: 162.7643485069275 and batch: 100, loss is 3.563146958351135 and perplexity is 35.27402856147385
At time: 163.3482208251953 and batch: 150, loss is 3.5723925828933716 and perplexity is 35.601671282488674
At time: 163.9321267604828 and batch: 200, loss is 3.4673371648788454 and perplexity is 32.05128143308158
At time: 164.51685738563538 and batch: 250, loss is 3.613225302696228 and perplexity is 37.085472002627
At time: 165.1004524230957 and batch: 300, loss is 3.5745378637313845 and perplexity is 35.67812884784423
At time: 165.6851840019226 and batch: 350, loss is 3.5587298107147216 and perplexity is 35.118561582664526
At time: 166.2693862915039 and batch: 400, loss is 3.4881661796569823 and perplexity is 32.72587926256374
At time: 166.85329914093018 and batch: 450, loss is 3.522815856933594 and perplexity is 33.879694646009256
At time: 167.43776202201843 and batch: 500, loss is 3.38948823928833 and perplexity is 29.650774285602004
At time: 168.02053427696228 and batch: 550, loss is 3.448861813545227 and perplexity is 31.464559385044648
At time: 168.6017780303955 and batch: 600, loss is 3.4830646753311156 and perplexity is 32.55935317587189
At time: 169.18391871452332 and batch: 650, loss is 3.3181640625 and perplexity is 27.609614472056297
At time: 169.7670042514801 and batch: 700, loss is 3.317703170776367 and perplexity is 27.596892361236424
At time: 170.35136580467224 and batch: 750, loss is 3.403690667152405 and perplexity is 30.07489188934106
At time: 170.93612027168274 and batch: 800, loss is 3.372362699508667 and perplexity is 29.147312106591272
At time: 171.51866817474365 and batch: 850, loss is 3.4327747058868407 and perplexity is 30.962435321614624
At time: 172.10180187225342 and batch: 900, loss is 3.4011925411224366 and perplexity is 29.9998547841599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.250200245478382 and perplexity of 70.11945204416645
finished 15 epochs...
Completing Train Step...
At time: 173.59759044647217 and batch: 50, loss is 3.6766649293899536 and perplexity is 39.514390791391264
At time: 174.1883316040039 and batch: 100, loss is 3.561293444633484 and perplexity is 35.20870822043316
At time: 174.77056312561035 and batch: 150, loss is 3.570330710411072 and perplexity is 35.528340801177464
At time: 175.3547866344452 and batch: 200, loss is 3.4654229831695558 and perplexity is 31.98998813837025
At time: 175.93853998184204 and batch: 250, loss is 3.611410222053528 and perplexity is 37.01821993269838
At time: 176.54156684875488 and batch: 300, loss is 3.572815728187561 and perplexity is 35.61673914988091
At time: 177.1247103214264 and batch: 350, loss is 3.557081952095032 and perplexity is 35.06073881320847
At time: 177.70709657669067 and batch: 400, loss is 3.4865756034851074 and perplexity is 32.673867633999066
At time: 178.2900242805481 and batch: 450, loss is 3.521462655067444 and perplexity is 33.833879585493584
At time: 178.87316942214966 and batch: 500, loss is 3.388281044960022 and perplexity is 29.61500163566888
At time: 179.45719861984253 and batch: 550, loss is 3.4478213167190552 and perplexity is 31.43183763725711
At time: 180.03945899009705 and batch: 600, loss is 3.482293462753296 and perplexity is 32.5342526733327
At time: 180.62239813804626 and batch: 650, loss is 3.317619547843933 and perplexity is 27.594584724658123
At time: 181.2051649093628 and batch: 700, loss is 3.317396197319031 and perplexity is 27.58842214790841
At time: 181.78572726249695 and batch: 750, loss is 3.403542356491089 and perplexity is 30.07043179298404
At time: 182.36719274520874 and batch: 800, loss is 3.3725172758102415 and perplexity is 29.151817938536
At time: 182.9497573375702 and batch: 850, loss is 3.4332843017578125 and perplexity is 30.97821767177845
At time: 183.53453302383423 and batch: 900, loss is 3.4019179439544676 and perplexity is 30.021624658790575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.250158440576841 and perplexity of 70.11652076864887
finished 16 epochs...
Completing Train Step...
At time: 185.042062997818 and batch: 50, loss is 3.675261664390564 and perplexity is 39.458980516563734
At time: 185.62245678901672 and batch: 100, loss is 3.5597605228424074 and perplexity is 35.15477737081448
At time: 186.20373606681824 and batch: 150, loss is 3.5686705017089846 and perplexity is 35.46940527678082
At time: 186.78241729736328 and batch: 200, loss is 3.4638626289367678 and perplexity is 31.940111347821937
At time: 187.36394691467285 and batch: 250, loss is 3.609883999824524 and perplexity is 36.96176499490807
At time: 187.94666385650635 and batch: 300, loss is 3.5713680267333983 and perplexity is 35.5652140503009
At time: 188.5316641330719 and batch: 350, loss is 3.5557035970687867 and perplexity is 35.01244595763733
At time: 189.11289238929749 and batch: 400, loss is 3.4852838706970215 and perplexity is 32.63168897550557
At time: 189.69605469703674 and batch: 450, loss is 3.520315451622009 and perplexity is 33.7950874977235
At time: 190.27814483642578 and batch: 500, loss is 3.3872673225402834 and perplexity is 29.584995456088304
At time: 190.8725836277008 and batch: 550, loss is 3.4469509363174438 and perplexity is 31.40449188412115
At time: 191.45689415931702 and batch: 600, loss is 3.481592044830322 and perplexity is 32.51144056674255
At time: 192.0423879623413 and batch: 650, loss is 3.317078776359558 and perplexity is 27.579666394183267
At time: 192.62875413894653 and batch: 700, loss is 3.3170474910736085 and perplexity is 27.578803569930656
At time: 193.2146189212799 and batch: 750, loss is 3.403331370353699 and perplexity is 30.064088017978214
At time: 193.8003330230713 and batch: 800, loss is 3.3725095176696778 and perplexity is 29.151591775512046
At time: 194.3847324848175 and batch: 850, loss is 3.433508882522583 and perplexity is 30.98517556486963
At time: 194.96999955177307 and batch: 900, loss is 3.4022942972183228 and perplexity is 30.032925521642067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.250186031811858 and perplexity of 70.11845539674128
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 196.4627275466919 and batch: 50, loss is 3.674496030807495 and perplexity is 39.42878095829984
At time: 197.05503869056702 and batch: 100, loss is 3.5594375133514404 and perplexity is 35.14342387781259
At time: 197.63331151008606 and batch: 150, loss is 3.5684800338745117 and perplexity is 35.462650139306334
At time: 198.2115499973297 and batch: 200, loss is 3.463577904701233 and perplexity is 31.93101851857139
At time: 198.788076877594 and batch: 250, loss is 3.6095670080184936 and perplexity is 36.950050275101454
At time: 199.37018013000488 and batch: 300, loss is 3.570921106338501 and perplexity is 35.54932278212336
At time: 199.95373225212097 and batch: 350, loss is 3.5550637531280516 and perplexity is 34.99005062176506
At time: 200.53751063346863 and batch: 400, loss is 3.4846964263916016 and perplexity is 32.612525304986335
At time: 201.12508749961853 and batch: 450, loss is 3.5192849588394166 and perplexity is 33.76027984156947
At time: 201.71360969543457 and batch: 500, loss is 3.3860726737976075 and perplexity is 29.549672881700143
At time: 202.30231094360352 and batch: 550, loss is 3.445354552268982 and perplexity is 31.354398249205545
At time: 202.8901550769806 and batch: 600, loss is 3.480035834312439 and perplexity is 32.46088526852733
At time: 203.4774625301361 and batch: 650, loss is 3.315438551902771 and perplexity is 27.534466629866483
At time: 204.06554698944092 and batch: 700, loss is 3.3150142526626585 and perplexity is 27.522786254760902
At time: 204.65239810943604 and batch: 750, loss is 3.4010798263549806 and perplexity is 29.99647354806539
At time: 205.2400095462799 and batch: 800, loss is 3.3699101305007932 and perplexity is 29.075913902555953
At time: 205.84645652770996 and batch: 850, loss is 3.4304368257522584 and perplexity is 30.8901334086925
At time: 206.43283009529114 and batch: 900, loss is 3.3990048599243163 and perplexity is 29.934296402468345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2500066887842465 and perplexity of 70.10588126823382
finished 18 epochs...
Completing Train Step...
At time: 207.92247366905212 and batch: 50, loss is 3.6739589977264404 and perplexity is 39.40761208328143
At time: 208.51507997512817 and batch: 100, loss is 3.558849883079529 and perplexity is 35.12277860457122
At time: 209.09854793548584 and batch: 150, loss is 3.567918314933777 and perplexity is 35.44273569071955
At time: 209.68570756912231 and batch: 200, loss is 3.463006267547607 and perplexity is 31.91277077807318
At time: 210.2641351222992 and batch: 250, loss is 3.609066853523254 and perplexity is 36.931574162197656
At time: 210.84222292900085 and batch: 300, loss is 3.570412583351135 and perplexity is 35.53124972997407
At time: 211.4236719608307 and batch: 350, loss is 3.554602303504944 and perplexity is 34.97390820083659
At time: 212.00616645812988 and batch: 400, loss is 3.4842154455184935 and perplexity is 32.59684307581935
At time: 212.58842158317566 and batch: 450, loss is 3.5189120054244993 and perplexity is 33.747691177552525
At time: 213.1687626838684 and batch: 500, loss is 3.385759768486023 and perplexity is 29.54042807854931
At time: 213.7516942024231 and batch: 550, loss is 3.445098252296448 and perplexity is 31.3463631475376
At time: 214.33940172195435 and batch: 600, loss is 3.4798597288131714 and perplexity is 32.455169231448
At time: 214.9260642528534 and batch: 650, loss is 3.3152990245819094 and perplexity is 27.530625087512554
At time: 215.51210522651672 and batch: 700, loss is 3.314984121322632 and perplexity is 27.52195696882359
At time: 216.0990514755249 and batch: 750, loss is 3.4010768127441406 and perplexity is 29.996383150503753
At time: 216.68575072288513 and batch: 800, loss is 3.3700158834457397 and perplexity is 29.078988928671826
At time: 217.27261638641357 and batch: 850, loss is 3.430676636695862 and perplexity is 30.89754208903908
At time: 217.8581817150116 and batch: 900, loss is 3.3993230390548708 and perplexity is 29.943822386275695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.249915554098887 and perplexity of 70.09949248192704
finished 19 epochs...
Completing Train Step...
At time: 219.36071109771729 and batch: 50, loss is 3.673499870300293 and perplexity is 39.38952312066219
At time: 219.93737721443176 and batch: 100, loss is 3.5583565521240232 and perplexity is 35.105455723950165
At time: 220.52482151985168 and batch: 150, loss is 3.5674109935760496 and perplexity is 35.4247593941925
At time: 221.10121512413025 and batch: 200, loss is 3.4625163888931274 and perplexity is 31.897141221469344
At time: 221.67800855636597 and batch: 250, loss is 3.6086111640930176 and perplexity is 36.91474866810101
At time: 222.25598120689392 and batch: 300, loss is 3.5699723625183104 and perplexity is 35.51561157599977
At time: 222.83417391777039 and batch: 350, loss is 3.5541971921920776 and perplexity is 34.959742744456335
At time: 223.41138982772827 and batch: 400, loss is 3.4838072872161865 and perplexity is 32.58354111852577
At time: 223.99287176132202 and batch: 450, loss is 3.5185821723937987 and perplexity is 33.73656190979324
At time: 224.57508087158203 and batch: 500, loss is 3.3854835414886475 and perplexity is 29.53226934168356
At time: 225.1576690673828 and batch: 550, loss is 3.444868049621582 and perplexity is 31.339147961403153
At time: 225.74057936668396 and batch: 600, loss is 3.4797023916244507 and perplexity is 32.45006322805431
At time: 226.32455849647522 and batch: 650, loss is 3.315178756713867 and perplexity is 27.527314237025863
At time: 226.91059708595276 and batch: 700, loss is 3.3149471855163575 and perplexity is 27.520940441925937
At time: 227.49648547172546 and batch: 750, loss is 3.4010658645629883 and perplexity is 29.996054746464825
At time: 228.0802583694458 and batch: 800, loss is 3.370093846321106 and perplexity is 29.081256098637844
At time: 228.66259002685547 and batch: 850, loss is 3.43085648059845 and perplexity is 30.90309932329112
At time: 229.2453908920288 and batch: 900, loss is 3.3995594024658202 and perplexity is 29.950900846783362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.249865806266053 and perplexity of 70.09600527083458
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f52e60f0b70>
ELAPSED
1427.0305392742157


RESULTS SO FAR:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.27715916891746, 'params': {'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.64645432534728, 'params': {'batch_size': 32, 'dropout': 0.43751170550744745, 'num_layers': 2, 'rnn_dropout': 0.9329965743746119, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -71.09453673611982, 'params': {'batch_size': 32, 'dropout': 0.7029347552002905, 'num_layers': 2, 'rnn_dropout': 0.44946109581267146, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.67023293916662, 'params': {'batch_size': 32, 'dropout': 0.8672620102056839, 'num_layers': 2, 'rnn_dropout': 0.43740970994803685, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.09600527083458, 'params': {'batch_size': 32, 'dropout': 0.3922418214619182, 'num_layers': 2, 'rnn_dropout': 0.30099019980318864, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -70.16648645397312, 'params': {'batch_size': 32, 'dropout': 0.4394442725160501, 'num_layers': 2, 'rnn_dropout': 0.3227724377678134, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.27715916891746, 'params': {'batch_size': 32, 'dropout': 0.028910965276651268, 'num_layers': 2, 'rnn_dropout': 0.6471428949710202, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.64645432534728, 'params': {'batch_size': 32, 'dropout': 0.43751170550744745, 'num_layers': 2, 'rnn_dropout': 0.9329965743746119, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -71.09453673611982, 'params': {'batch_size': 32, 'dropout': 0.7029347552002905, 'num_layers': 2, 'rnn_dropout': 0.44946109581267146, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.67023293916662, 'params': {'batch_size': 32, 'dropout': 0.8672620102056839, 'num_layers': 2, 'rnn_dropout': 0.43740970994803685, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}, {'best_accuracy': -70.09600527083458, 'params': {'batch_size': 32, 'dropout': 0.3922418214619182, 'num_layers': 2, 'rnn_dropout': 0.30099019980318864, 'seq_len': 35, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'data': 'ptb'}}]
