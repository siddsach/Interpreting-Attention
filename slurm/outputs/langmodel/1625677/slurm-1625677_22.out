FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0273823738098145 and batch: 50, loss is 6.8857119750976565 and perplexity is 978.197874366275
At time: 1.5031893253326416 and batch: 100, loss is 6.008101263046265 and perplexity is 406.71035070083883
At time: 1.9916973114013672 and batch: 150, loss is 5.7161486721038814 and perplexity is 303.73289250694455
At time: 2.467237710952759 and batch: 200, loss is 5.476775369644165 and perplexity is 239.07453605355133
At time: 2.939157009124756 and batch: 250, loss is 5.4712371826171875 and perplexity is 237.75415618965124
At time: 3.412017822265625 and batch: 300, loss is 5.365300102233887 and perplexity is 213.85540353618003
At time: 3.886259078979492 and batch: 350, loss is 5.320627593994141 and perplexity is 204.51219234899892
At time: 4.358745813369751 and batch: 400, loss is 5.161004657745361 and perplexity is 174.3395191991741
At time: 4.8345866203308105 and batch: 450, loss is 5.148857583999634 and perplexity is 172.23461430191628
At time: 5.309510231018066 and batch: 500, loss is 5.074871072769165 and perplexity is 159.95156853715417
At time: 5.783768653869629 and batch: 550, loss is 5.127720460891724 and perplexity is 168.63227571255558
At time: 6.256533861160278 and batch: 600, loss is 5.033693447113037 and perplexity is 153.4989070970587
At time: 6.726633310317993 and batch: 650, loss is 4.917229146957397 and perplexity is 136.62352452090965
At time: 7.199694871902466 and batch: 700, loss is 4.984858980178833 and perplexity is 146.18295887782998
At time: 7.671924114227295 and batch: 750, loss is 4.9848659801483155 and perplexity is 146.18398215766243
At time: 8.144933938980103 and batch: 800, loss is 4.945374813079834 and perplexity is 140.52351103192638
At time: 8.615787506103516 and batch: 850, loss is 4.970841484069824 and perplexity is 144.14813470896578
At time: 9.087619304656982 and batch: 900, loss is 4.89643012046814 and perplexity is 133.81123602891645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.851697111782962 and perplexity of 127.95736362910154
finished 1 epochs...
Completing Train Step...
At time: 10.40277099609375 and batch: 50, loss is 4.816393575668335 and perplexity is 123.51882525526992
At time: 10.873579502105713 and batch: 100, loss is 4.689018955230713 and perplexity is 108.7464423304282
At time: 11.342319250106812 and batch: 150, loss is 4.678391628265381 and perplexity is 107.59687755116227
At time: 11.813169717788696 and batch: 200, loss is 4.567947731018067 and perplexity is 96.34617846458245
At time: 12.283143281936646 and batch: 250, loss is 4.674447164535523 and perplexity is 107.17330150957642
At time: 12.754090309143066 and batch: 300, loss is 4.617434377670288 and perplexity is 101.23397052696174
At time: 13.223009586334229 and batch: 350, loss is 4.609709358215332 and perplexity is 100.45494898747808
At time: 13.69103741645813 and batch: 400, loss is 4.502539730072021 and perplexity is 90.24604107736148
At time: 14.157786846160889 and batch: 450, loss is 4.528905334472657 and perplexity is 92.65707703860514
At time: 14.623976230621338 and batch: 500, loss is 4.4249285697937015 and perplexity is 83.50684209804032
At time: 15.091423988342285 and batch: 550, loss is 4.498441324234009 and perplexity is 89.8769330696161
At time: 15.557636499404907 and batch: 600, loss is 4.461509342193604 and perplexity is 86.61814690680723
At time: 16.02994990348816 and batch: 650, loss is 4.317746591567993 and perplexity is 75.01938835800058
At time: 16.49857187271118 and batch: 700, loss is 4.3566052532196045 and perplexity is 77.99192168769763
At time: 16.96760845184326 and batch: 750, loss is 4.415056819915772 and perplexity is 82.68653901481615
At time: 17.43390202522278 and batch: 800, loss is 4.376986532211304 and perplexity is 79.5978062033643
At time: 17.919516563415527 and batch: 850, loss is 4.4365751171112064 and perplexity is 84.48509405708401
At time: 18.389282941818237 and batch: 900, loss is 4.372308006286621 and perplexity is 79.22627558898989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.534424246174016 and perplexity of 93.16985695729724
finished 2 epochs...
Completing Train Step...
At time: 19.686336517333984 and batch: 50, loss is 4.428321914672852 and perplexity is 83.7906909391819
At time: 20.180460453033447 and batch: 100, loss is 4.305872797966003 and perplexity is 74.13389113373495
At time: 20.650562524795532 and batch: 150, loss is 4.311140623092651 and perplexity is 74.52544592379675
At time: 21.120718955993652 and batch: 200, loss is 4.199099912643432 and perplexity is 66.62633452260366
At time: 21.587555408477783 and batch: 250, loss is 4.338356142044067 and perplexity is 76.58154662490864
At time: 22.055727243423462 and batch: 300, loss is 4.2982040119171145 and perplexity is 73.56754853931335
At time: 22.523164749145508 and batch: 350, loss is 4.296075162887573 and perplexity is 73.41110092076515
At time: 22.990101099014282 and batch: 400, loss is 4.211199750900269 and perplexity is 67.43739937536174
At time: 23.45880913734436 and batch: 450, loss is 4.249748930931092 and perplexity is 70.0878132554707
At time: 23.929691553115845 and batch: 500, loss is 4.1337449169158935 and perplexity is 62.411210639046004
At time: 24.397047519683838 and batch: 550, loss is 4.211833724975586 and perplexity is 67.48016649346216
At time: 24.8655207157135 and batch: 600, loss is 4.197377624511719 and perplexity is 66.51168353675247
At time: 25.326621055603027 and batch: 650, loss is 4.052538204193115 and perplexity is 57.5433285591224
At time: 25.788081645965576 and batch: 700, loss is 4.068132100105285 and perplexity is 58.44768613814296
At time: 26.254267930984497 and batch: 750, loss is 4.1567906427383425 and perplexity is 63.86622380841309
At time: 26.71495008468628 and batch: 800, loss is 4.127283248901367 and perplexity is 62.00923024609707
At time: 27.175666570663452 and batch: 850, loss is 4.191514563560486 and perplexity is 66.12286243693651
At time: 27.644753456115723 and batch: 900, loss is 4.138667750358581 and perplexity is 62.71920812212544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430124361221105 and perplexity of 83.94185537282137
finished 3 epochs...
Completing Train Step...
At time: 29.020214319229126 and batch: 50, loss is 4.2127896547317505 and perplexity is 67.54470363413628
At time: 29.491238355636597 and batch: 100, loss is 4.0933903789520265 and perplexity is 59.942776309081495
At time: 29.969875812530518 and batch: 150, loss is 4.101187047958374 and perplexity is 60.411956941620815
At time: 30.430795431137085 and batch: 200, loss is 3.988320550918579 and perplexity is 53.964183118618834
At time: 30.91933536529541 and batch: 250, loss is 4.136882352828979 and perplexity is 62.6073293066851
At time: 31.400904178619385 and batch: 300, loss is 4.105540900230408 and perplexity is 60.675555094920725
At time: 31.869676113128662 and batch: 350, loss is 4.103182983398438 and perplexity is 60.53265572090023
At time: 32.3316535949707 and batch: 400, loss is 4.029786386489868 and perplexity is 56.248894439915006
At time: 32.80003643035889 and batch: 450, loss is 4.070825810432434 and perplexity is 58.605339514826966
At time: 33.27081251144409 and batch: 500, loss is 3.9516831731796263 and perplexity is 52.02285666097088
At time: 33.74205923080444 and batch: 550, loss is 4.030242519378662 and perplexity is 56.274557263011225
At time: 34.21306562423706 and batch: 600, loss is 4.027565450668335 and perplexity is 56.12410787809739
At time: 34.68555045127869 and batch: 650, loss is 3.879030022621155 and perplexity is 48.37726745088347
At time: 35.15570592880249 and batch: 700, loss is 3.8897064685821534 and perplexity is 48.896531748754235
At time: 35.625006914138794 and batch: 750, loss is 3.9886884784698484 and perplexity is 53.98404168140208
At time: 36.09275245666504 and batch: 800, loss is 3.9640264081954957 and perplexity is 52.668966345602335
At time: 36.560832262039185 and batch: 850, loss is 4.0272390651702885 and perplexity is 56.105792772250254
At time: 37.02848267555237 and batch: 900, loss is 3.97799973487854 and perplexity is 53.41009296162054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386813229077483 and perplexity of 80.38384547571886
finished 4 epochs...
Completing Train Step...
At time: 38.341050148010254 and batch: 50, loss is 4.062538132667542 and perplexity is 58.12164446853667
At time: 38.810038566589355 and batch: 100, loss is 3.943668885231018 and perplexity is 51.60759673671916
At time: 39.27692127227783 and batch: 150, loss is 3.9529729509353637 and perplexity is 52.08999787359579
At time: 39.743717193603516 and batch: 200, loss is 3.841768183708191 and perplexity is 46.6078127989297
At time: 40.21312928199768 and batch: 250, loss is 3.993237028121948 and perplexity is 54.230150069204356
At time: 40.68274402618408 and batch: 300, loss is 3.964966068267822 and perplexity is 52.71848052993576
At time: 41.153343200683594 and batch: 350, loss is 3.965531129837036 and perplexity is 52.748278135218534
At time: 41.62327432632446 and batch: 400, loss is 3.8963293647766113 and perplexity is 49.22144314238662
At time: 42.107051610946655 and batch: 450, loss is 3.9354084587097167 and perplexity is 51.18305185090477
At time: 42.57742762565613 and batch: 500, loss is 3.8209203767776487 and perplexity is 45.646200696589744
At time: 43.05133557319641 and batch: 550, loss is 3.8969495248794557 and perplexity is 49.25197778483282
At time: 43.51797413825989 and batch: 600, loss is 3.898586559295654 and perplexity is 49.33267099829554
At time: 43.99201726913452 and batch: 650, loss is 3.7523142671585084 and perplexity is 42.619601099466905
At time: 44.4597852230072 and batch: 700, loss is 3.7597661066055297 and perplexity is 42.938381800302984
At time: 44.92727565765381 and batch: 750, loss is 3.860782346725464 and perplexity is 47.502500260832136
At time: 45.39339733123779 and batch: 800, loss is 3.840356483459473 and perplexity is 46.542062958461706
At time: 45.86242914199829 and batch: 850, loss is 3.9003154468536376 and perplexity is 49.41803541085875
At time: 46.330050230026245 and batch: 900, loss is 3.8549823474884035 and perplexity is 47.22778324492492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369549894986087 and perplexity of 79.00806177032086
finished 5 epochs...
Completing Train Step...
At time: 47.64701747894287 and batch: 50, loss is 3.9442538166046144 and perplexity is 51.637792469521166
At time: 48.143654108047485 and batch: 100, loss is 3.8290731048583986 and perplexity is 46.01986287216355
At time: 48.61502647399902 and batch: 150, loss is 3.842340660095215 and perplexity is 46.63450231003646
At time: 49.08745241165161 and batch: 200, loss is 3.7311570978164674 and perplexity is 41.72736288140834
At time: 49.558539390563965 and batch: 250, loss is 3.8801080560684205 and perplexity is 48.42944788435564
At time: 50.02766275405884 and batch: 300, loss is 3.8548151540756224 and perplexity is 47.21988773072349
At time: 50.494741678237915 and batch: 350, loss is 3.85737229347229 and perplexity is 47.34079008214761
At time: 50.96256995201111 and batch: 400, loss is 3.791199688911438 and perplexity is 44.30952604948352
At time: 51.43086099624634 and batch: 450, loss is 3.830599517822266 and perplexity is 46.09016182641523
At time: 51.899545669555664 and batch: 500, loss is 3.7194961881637574 and perplexity is 41.24360985455973
At time: 52.36804413795471 and batch: 550, loss is 3.7924194955825805 and perplexity is 44.363608083060676
At time: 52.8362717628479 and batch: 600, loss is 3.793919086456299 and perplexity is 44.43018525164805
At time: 53.30674719810486 and batch: 650, loss is 3.6532333278656006 and perplexity is 38.59926859076949
At time: 53.775317430496216 and batch: 700, loss is 3.659228973388672 and perplexity is 38.83139129004365
At time: 54.26193618774414 and batch: 750, loss is 3.7589203834533693 and perplexity is 42.902083168158505
At time: 54.73237919807434 and batch: 800, loss is 3.742047724723816 and perplexity is 42.184283582709575
At time: 55.20467972755432 and batch: 850, loss is 3.8009213638305663 and perplexity is 44.74238952737549
At time: 55.675726890563965 and batch: 900, loss is 3.7574536275863646 and perplexity is 42.839202412596784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370877618659033 and perplexity of 79.11303231480468
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 56.982484340667725 and batch: 50, loss is 3.8738778257369995 and perplexity is 48.12865923344531
At time: 57.465736389160156 and batch: 100, loss is 3.75469762802124 and perplexity is 42.72130013332014
At time: 57.93487811088562 and batch: 150, loss is 3.775061001777649 and perplexity is 43.60016792235137
At time: 58.404133319854736 and batch: 200, loss is 3.6433257818222047 and perplexity is 38.2187327608542
At time: 58.87249684333801 and batch: 250, loss is 3.789578747749329 and perplexity is 44.23776109393483
At time: 59.341793060302734 and batch: 300, loss is 3.750271120071411 and perplexity is 42.53261188177167
At time: 59.809247970581055 and batch: 350, loss is 3.7379924201965333 and perplexity is 42.01355986881098
At time: 60.278894901275635 and batch: 400, loss is 3.6681790256500246 and perplexity is 39.18049418569223
At time: 60.7480890750885 and batch: 450, loss is 3.6982836389541625 and perplexity is 40.377941725144865
At time: 61.21841621398926 and batch: 500, loss is 3.5777296209335328 and perplexity is 35.79218669820761
At time: 61.688791036605835 and batch: 550, loss is 3.6254759645462036 and perplexity is 37.542587848573106
At time: 62.15902900695801 and batch: 600, loss is 3.628193612098694 and perplexity is 37.644754133651226
At time: 62.630452156066895 and batch: 650, loss is 3.473225784301758 and perplexity is 32.24057602637159
At time: 63.10123896598816 and batch: 700, loss is 3.4651235246658327 and perplexity is 31.980409898602467
At time: 63.572094202041626 and batch: 750, loss is 3.552219295501709 and perplexity is 34.89066432249701
At time: 64.04260420799255 and batch: 800, loss is 3.5218116235733032 and perplexity is 33.84568860426794
At time: 64.5107638835907 and batch: 850, loss is 3.562934956550598 and perplexity is 35.266551196542046
At time: 64.97964549064636 and batch: 900, loss is 3.5114574813842774 and perplexity is 33.49705355511688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330691402905608 and perplexity of 75.9968128258964
finished 7 epochs...
Completing Train Step...
At time: 66.29481101036072 and batch: 50, loss is 3.7800520515441893 and perplexity is 43.81832248756439
At time: 66.76276206970215 and batch: 100, loss is 3.657929573059082 and perplexity is 38.780966535469226
At time: 67.23598003387451 and batch: 150, loss is 3.6801927995681765 and perplexity is 39.654038617138305
At time: 67.70476961135864 and batch: 200, loss is 3.5538446044921876 and perplexity is 34.947418541982096
At time: 68.1724693775177 and batch: 250, loss is 3.7020210218429566 and perplexity is 40.52913190544383
At time: 68.6423499584198 and batch: 300, loss is 3.669555077552795 and perplexity is 39.23444569079038
At time: 69.11361408233643 and batch: 350, loss is 3.6608757400512695 and perplexity is 38.895390211864914
At time: 69.5849461555481 and batch: 400, loss is 3.5944203758239746 and perplexity is 36.39459868378296
At time: 70.0577039718628 and batch: 450, loss is 3.630503282546997 and perplexity is 37.73180159658694
At time: 70.53006672859192 and batch: 500, loss is 3.5149364995956422 and perplexity is 33.61379336668008
At time: 71.00109028816223 and batch: 550, loss is 3.5644759702682496 and perplexity is 35.32093933137804
At time: 71.4700117111206 and batch: 600, loss is 3.5740436506271362 and perplexity is 35.66050060544557
At time: 71.93884658813477 and batch: 650, loss is 3.424567527770996 and perplexity is 30.709361033451327
At time: 72.4085590839386 and batch: 700, loss is 3.420819878578186 and perplexity is 30.594488506776027
At time: 72.87721395492554 and batch: 750, loss is 3.514961853027344 and perplexity is 33.61464560249794
At time: 73.34738039970398 and batch: 800, loss is 3.4904901361465455 and perplexity is 32.80202122308548
At time: 73.81537556648254 and batch: 850, loss is 3.5380010318756105 and perplexity is 34.398089734912666
At time: 74.28550839424133 and batch: 900, loss is 3.494374842643738 and perplexity is 32.92969527560239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334830088158176 and perplexity of 76.31199147711528
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.59964609146118 and batch: 50, loss is 3.75755934715271 and perplexity is 42.84373159390574
At time: 76.07291388511658 and batch: 100, loss is 3.6418878984451295 and perplexity is 38.16381817017441
At time: 76.5463354587555 and batch: 150, loss is 3.66314642906189 and perplexity is 38.9838098957939
At time: 77.01763033866882 and batch: 200, loss is 3.5379183721542358 and perplexity is 34.39524651591081
At time: 77.48842883110046 and batch: 250, loss is 3.6841353511810304 and perplexity is 39.81068530195854
At time: 77.95915102958679 and batch: 300, loss is 3.6478875923156737 and perplexity is 38.39347765050707
At time: 78.45380806922913 and batch: 350, loss is 3.6302670097351073 and perplexity is 37.72288765082903
At time: 78.9229519367218 and batch: 400, loss is 3.5670917940139772 and perplexity is 35.41345363100067
At time: 79.39068150520325 and batch: 450, loss is 3.5980751085281373 and perplexity is 36.52785457284969
At time: 79.85995364189148 and batch: 500, loss is 3.477693867683411 and perplexity is 32.38495190988237
At time: 80.32992696762085 and batch: 550, loss is 3.5214357471466062 and perplexity is 33.83296919838843
At time: 80.79868626594543 and batch: 600, loss is 3.5361353158950806 and perplexity is 34.33397250006816
At time: 81.26764845848083 and batch: 650, loss is 3.3801990032196043 and perplexity is 29.376616572916593
At time: 81.73631572723389 and batch: 700, loss is 3.3656946086883544 and perplexity is 28.953601739209628
At time: 82.20710229873657 and batch: 750, loss is 3.4534960651397704 and perplexity is 31.610712463045076
At time: 82.67825651168823 and batch: 800, loss is 3.425086145401001 and perplexity is 30.725291580063562
At time: 83.15155553817749 and batch: 850, loss is 3.471830687522888 and perplexity is 32.19562866285939
At time: 83.62459444999695 and batch: 900, loss is 3.430050563812256 and perplexity is 30.878204029915942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3208074700342465 and perplexity of 75.24936537467201
finished 9 epochs...
Completing Train Step...
At time: 84.9306755065918 and batch: 50, loss is 3.7276360273361204 and perplexity is 41.580696259031406
At time: 85.4173641204834 and batch: 100, loss is 3.606971163749695 and perplexity is 36.8542580834594
At time: 85.88624787330627 and batch: 150, loss is 3.6298498010635374 and perplexity is 37.70715261760884
At time: 86.35672378540039 and batch: 200, loss is 3.505353169441223 and perplexity is 33.293199917151426
At time: 86.82556653022766 and batch: 250, loss is 3.652503671646118 and perplexity is 38.57111466696665
At time: 87.2960557937622 and batch: 300, loss is 3.6188823270797728 and perplexity is 37.29585994581548
At time: 87.76466608047485 and batch: 350, loss is 3.6030562782287596 and perplexity is 36.71025993415207
At time: 88.2325336933136 and batch: 400, loss is 3.5417736625671385 and perplexity is 34.52810612167464
At time: 88.70103931427002 and batch: 450, loss is 3.575465979576111 and perplexity is 35.71125765584582
At time: 89.16884350776672 and batch: 500, loss is 3.4569209241867065 and perplexity is 31.71916030087883
At time: 89.63941717147827 and batch: 550, loss is 3.5020528745651247 and perplexity is 33.18350365462962
At time: 90.11253952980042 and batch: 600, loss is 3.519752960205078 and perplexity is 33.77608339639461
At time: 90.60057306289673 and batch: 650, loss is 3.3668142652511595 and perplexity is 28.986037984752787
At time: 91.0697238445282 and batch: 700, loss is 3.3548066091537474 and perplexity is 28.640064928709304
At time: 91.5408685207367 and batch: 750, loss is 3.445764422416687 and perplexity is 31.367252115075146
At time: 92.01197123527527 and batch: 800, loss is 3.4206120491027834 and perplexity is 30.588130730968942
At time: 92.4807801246643 and batch: 850, loss is 3.4701135301589967 and perplexity is 32.14039114134717
At time: 92.9520812034607 and batch: 900, loss is 3.431366410255432 and perplexity is 30.918861748651253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321800336445848 and perplexity of 75.32411504412428
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.25644254684448 and batch: 50, loss is 3.722478885650635 and perplexity is 41.366810710125954
At time: 94.74270296096802 and batch: 100, loss is 3.6087656450271606 and perplexity is 36.920451733455
At time: 95.21133732795715 and batch: 150, loss is 3.628526558876038 and perplexity is 37.65728991998295
At time: 95.67572927474976 and batch: 200, loss is 3.503739447593689 and perplexity is 33.239517279143264
At time: 96.13481330871582 and batch: 250, loss is 3.652362985610962 and perplexity is 38.565688631465264
At time: 96.609778881073 and batch: 300, loss is 3.61741156578064 and perplexity is 37.241046956681515
At time: 97.08116483688354 and batch: 350, loss is 3.596600122451782 and perplexity is 36.47401621113339
At time: 97.5507493019104 and batch: 400, loss is 3.535934987068176 and perplexity is 34.327095104527544
At time: 98.02203297615051 and batch: 450, loss is 3.5664654111862184 and perplexity is 35.391278197654955
At time: 98.49264860153198 and batch: 500, loss is 3.4478259754180907 and perplexity is 31.431984069069884
At time: 98.96620655059814 and batch: 550, loss is 3.489798135757446 and perplexity is 32.77933006368693
At time: 99.43858623504639 and batch: 600, loss is 3.5093720722198487 and perplexity is 33.42727128023931
At time: 99.90957140922546 and batch: 650, loss is 3.3529446601867674 and perplexity is 28.586788204064145
At time: 100.37687492370605 and batch: 700, loss is 3.3370727252960206 and perplexity is 28.13664235571091
At time: 100.85547113418579 and batch: 750, loss is 3.4267761802673338 and perplexity is 30.77726229790473
At time: 101.33093166351318 and batch: 800, loss is 3.400522222518921 and perplexity is 29.97975206176244
At time: 101.7950267791748 and batch: 850, loss is 3.447828826904297 and perplexity is 31.43207369706668
At time: 102.27978658676147 and batch: 900, loss is 3.406795778274536 and perplexity is 30.16842290759613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315443065068493 and perplexity of 74.84677809142904
finished 11 epochs...
Completing Train Step...
At time: 103.63414978981018 and batch: 50, loss is 3.709018325805664 and perplexity is 40.81372107802688
At time: 104.11788725852966 and batch: 100, loss is 3.59493314743042 and perplexity is 36.41326558613572
At time: 104.58725833892822 and batch: 150, loss is 3.6162428140640257 and perplexity is 37.19754684448948
At time: 105.05635333061218 and batch: 200, loss is 3.4929181003570555 and perplexity is 32.881760119068545
At time: 105.5254008769989 and batch: 250, loss is 3.642350640296936 and perplexity is 38.1814822527064
At time: 105.99417066574097 and batch: 300, loss is 3.6073860216140745 and perplexity is 36.86955053413859
At time: 106.46223592758179 and batch: 350, loss is 3.5873016786575316 and perplexity is 36.13643653654648
At time: 106.93027305603027 and batch: 400, loss is 3.526879587173462 and perplexity is 34.01765270798175
At time: 107.3987352848053 and batch: 450, loss is 3.5590488386154173 and perplexity is 35.12976717099431
At time: 107.86720538139343 and batch: 500, loss is 3.441456899642944 and perplexity is 31.232427550650225
At time: 108.33535575866699 and batch: 550, loss is 3.4839130067825317 and perplexity is 32.58698601845627
At time: 108.80371046066284 and batch: 600, loss is 3.504896492958069 and perplexity is 33.27799916687708
At time: 109.27289915084839 and batch: 650, loss is 3.3496225547790526 and perplexity is 28.491977453233556
At time: 109.74438977241516 and batch: 700, loss is 3.334853992462158 and perplexity is 28.07428386747571
At time: 110.21163558959961 and batch: 750, loss is 3.425597357749939 and perplexity is 30.74100274407238
At time: 110.6803560256958 and batch: 800, loss is 3.4004979515075684 and perplexity is 29.979024431689993
At time: 111.15004348754883 and batch: 850, loss is 3.44940890789032 and perplexity is 31.481778177277487
At time: 111.62010288238525 and batch: 900, loss is 3.4100458097457884 and perplexity is 30.266630734302616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315243655688142 and perplexity of 74.83185442979706
finished 12 epochs...
Completing Train Step...
At time: 112.95670461654663 and batch: 50, loss is 3.7031857204437255 and perplexity is 40.576363628686224
At time: 113.42464661598206 and batch: 100, loss is 3.5888221311569213 and perplexity is 36.19142206264495
At time: 113.89659357070923 and batch: 150, loss is 3.609763641357422 and perplexity is 36.95731660123819
At time: 114.36439967155457 and batch: 200, loss is 3.4865891456604006 and perplexity is 32.674310112238125
At time: 114.84997487068176 and batch: 250, loss is 3.6361149549484253 and perplexity is 37.94413532270391
At time: 115.31771540641785 and batch: 300, loss is 3.601475305557251 and perplexity is 36.65226787044528
At time: 115.7891035079956 and batch: 350, loss is 3.581641035079956 and perplexity is 35.93245891612194
At time: 116.25542831420898 and batch: 400, loss is 3.5215337324142455 and perplexity is 33.83628449335275
At time: 116.72421026229858 and batch: 450, loss is 3.5543412923812867 and perplexity is 34.964780812985104
At time: 117.19328927993774 and batch: 500, loss is 3.4372761487960815 and perplexity is 31.102125123502375
At time: 117.66228890419006 and batch: 550, loss is 3.4799665212631226 and perplexity is 32.45863538355995
At time: 118.12982559204102 and batch: 600, loss is 3.5017306709289553 and perplexity is 33.17281353138203
At time: 118.59954190254211 and batch: 650, loss is 3.3470792150497437 and perplexity is 28.41960474821054
At time: 119.06959176063538 and batch: 700, loss is 3.3328668069839478 and perplexity is 28.018550452919637
At time: 119.53790640830994 and batch: 750, loss is 3.4242629623413086 and perplexity is 30.700009447869878
At time: 120.00782465934753 and batch: 800, loss is 3.399764437675476 and perplexity is 29.9570424656204
At time: 120.47767090797424 and batch: 850, loss is 3.449423899650574 and perplexity is 31.482250148086123
At time: 120.94500041007996 and batch: 900, loss is 3.410698051452637 and perplexity is 30.286378332596836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3155889641748715 and perplexity of 74.85769896612146
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 122.25688457489014 and batch: 50, loss is 3.7010586977005007 and perplexity is 40.49014850368122
At time: 122.74109673500061 and batch: 100, loss is 3.589521384239197 and perplexity is 36.21673787612297
At time: 123.21098470687866 and batch: 150, loss is 3.611093964576721 and perplexity is 37.00651449493026
At time: 123.67824196815491 and batch: 200, loss is 3.4855446290969847 and perplexity is 32.64019907200227
At time: 124.14599537849426 and batch: 250, loss is 3.6355707597732545 and perplexity is 37.92349192486348
At time: 124.61434555053711 and batch: 300, loss is 3.6016141748428345 and perplexity is 36.65735809812943
At time: 125.08303141593933 and batch: 350, loss is 3.5808300399780273 and perplexity is 35.903329681370614
At time: 125.55162715911865 and batch: 400, loss is 3.5196993780136108 and perplexity is 33.774273648312615
At time: 126.02202820777893 and batch: 450, loss is 3.5506118726730347 and perplexity is 34.834625323406804
At time: 126.49078464508057 and batch: 500, loss is 3.4344347286224366 and perplexity is 31.013876353025864
At time: 126.97494864463806 and batch: 550, loss is 3.477041177749634 and perplexity is 32.363821474326365
At time: 127.44267439842224 and batch: 600, loss is 3.4980204582214354 and perplexity is 33.049963378099896
At time: 127.91421937942505 and batch: 650, loss is 3.340854887962341 and perplexity is 28.243261211816492
At time: 128.3815655708313 and batch: 700, loss is 3.3260000562667846 and perplexity is 27.826813111171234
At time: 128.8510057926178 and batch: 750, loss is 3.4171403503417968 and perplexity is 30.48212207693988
At time: 129.32002925872803 and batch: 800, loss is 3.3922491312026977 and perplexity is 29.732749979525323
At time: 129.79011178016663 and batch: 850, loss is 3.441456551551819 and perplexity is 31.23241667892127
At time: 130.2592258453369 and batch: 900, loss is 3.400790591239929 and perplexity is 29.987798769173388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314849017417594 and perplexity of 74.80232874254929
finished 14 epochs...
Completing Train Step...
At time: 131.55998706817627 and batch: 50, loss is 3.697474684715271 and perplexity is 40.34529102626979
At time: 132.04390478134155 and batch: 100, loss is 3.5857403230667115 and perplexity is 36.08005873367998
At time: 132.5133500099182 and batch: 150, loss is 3.6079291820526125 and perplexity is 36.88958205504777
At time: 132.9826159477234 and batch: 200, loss is 3.4833256292343138 and perplexity is 32.567850774861334
At time: 133.4526948928833 and batch: 250, loss is 3.633453917503357 and perplexity is 37.843298782197486
At time: 133.92315816879272 and batch: 300, loss is 3.5988991117477416 and perplexity is 36.557966046894975
At time: 134.39166069030762 and batch: 350, loss is 3.578290219306946 and perplexity is 35.8122573651175
At time: 134.86325812339783 and batch: 400, loss is 3.5175194215774535 and perplexity is 33.70072739603504
At time: 135.33396649360657 and batch: 450, loss is 3.54873486995697 and perplexity is 34.769301962303395
At time: 135.80479407310486 and batch: 500, loss is 3.4328747510910036 and perplexity is 30.965533119735095
At time: 136.27414798736572 and batch: 550, loss is 3.4752209043502806 and perplexity is 32.3049640555436
At time: 136.7447965145111 and batch: 600, loss is 3.496770248413086 and perplexity is 33.00866980791081
At time: 137.2184977531433 and batch: 650, loss is 3.340323219299316 and perplexity is 28.228249145964654
At time: 137.68752551078796 and batch: 700, loss is 3.3258016395568846 and perplexity is 27.821292354190042
At time: 138.15741562843323 and batch: 750, loss is 3.4171580696105956 and perplexity is 30.482662202639826
At time: 138.64511346817017 and batch: 800, loss is 3.3925975704193116 and perplexity is 29.74311184077054
At time: 139.11442828178406 and batch: 850, loss is 3.4422353649139406 and perplexity is 31.256750376836738
At time: 139.5853898525238 and batch: 900, loss is 3.4022356176376345 and perplexity is 30.031163253870687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314609893380779 and perplexity of 74.78444384617673
finished 15 epochs...
Completing Train Step...
At time: 140.90472531318665 and batch: 50, loss is 3.695410509109497 and perplexity is 40.262097153646245
At time: 141.3752839565277 and batch: 100, loss is 3.5836483240127563 and perplexity is 36.00465818138347
At time: 141.84579348564148 and batch: 150, loss is 3.605851354598999 and perplexity is 36.813011446506366
At time: 142.3161780834198 and batch: 200, loss is 3.4815100479125975 and perplexity is 32.50877483811755
At time: 142.78849172592163 and batch: 250, loss is 3.631705093383789 and perplexity is 37.77717534450708
At time: 143.25911235809326 and batch: 300, loss is 3.5970496463775636 and perplexity is 36.49041583982621
At time: 143.72982358932495 and batch: 350, loss is 3.576606230735779 and perplexity is 35.75200068302868
At time: 144.20260095596313 and batch: 400, loss is 3.515986232757568 and perplexity is 33.64909740694421
At time: 144.67251563072205 and batch: 450, loss is 3.5473640871047976 and perplexity is 34.72167345102274
At time: 145.14285492897034 and batch: 500, loss is 3.4317504930496217 and perplexity is 30.93073943232579
At time: 145.61258912086487 and batch: 550, loss is 3.4740663242340086 and perplexity is 32.267686910254604
At time: 146.08321142196655 and batch: 600, loss is 3.495965895652771 and perplexity is 32.98212986840421
At time: 146.55337381362915 and batch: 650, loss is 3.3398395109176637 and perplexity is 28.214598207056323
At time: 147.0247495174408 and batch: 700, loss is 3.3255255222320557 and perplexity is 27.81361147383279
At time: 147.4949984550476 and batch: 750, loss is 3.4170453453063967 and perplexity is 30.479226259413693
At time: 147.9644012451172 and batch: 800, loss is 3.392697224617004 and perplexity is 29.746076014411628
At time: 148.43470692634583 and batch: 850, loss is 3.4426547574996946 and perplexity is 31.26986197546099
At time: 148.90531206130981 and batch: 900, loss is 3.40292341709137 and perplexity is 30.051825776573082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314573523116438 and perplexity of 74.78172396564716
finished 16 epochs...
Completing Train Step...
At time: 150.22609043121338 and batch: 50, loss is 3.6937918567657473 and perplexity is 40.19697953133774
At time: 150.69704914093018 and batch: 100, loss is 3.5819965839385985 and perplexity is 35.94523693234771
At time: 151.18257403373718 and batch: 150, loss is 3.6041573905944824 and perplexity is 36.750704318133735
At time: 151.65223121643066 and batch: 200, loss is 3.4799147605895997 and perplexity is 32.45695534621117
At time: 152.12293314933777 and batch: 250, loss is 3.630157437324524 and perplexity is 37.718754489539315
At time: 152.5932457447052 and batch: 300, loss is 3.5955192947387697 and perplexity is 36.43461538019816
At time: 153.0633053779602 and batch: 350, loss is 3.5751929569244383 and perplexity is 35.70150900444794
At time: 153.53339743614197 and batch: 400, loss is 3.5146612453460695 and perplexity is 33.60454230046769
At time: 154.00418424606323 and batch: 450, loss is 3.5461808538436888 and perplexity is 34.6806139084128
At time: 154.47651505470276 and batch: 500, loss is 3.4307584142684937 and perplexity is 30.90006891834899
At time: 154.94533896446228 and batch: 550, loss is 3.4730941820144654 and perplexity is 32.23633337200052
At time: 155.4162290096283 and batch: 600, loss is 3.4952593517303465 and perplexity is 32.95883477546933
At time: 155.88727688789368 and batch: 650, loss is 3.3393265914916994 and perplexity is 28.200130102343326
At time: 156.35596466064453 and batch: 700, loss is 3.3251639127731325 and perplexity is 27.803555627091264
At time: 156.8268370628357 and batch: 750, loss is 3.416829719543457 and perplexity is 30.472654861504505
At time: 157.29753494262695 and batch: 800, loss is 3.3926539325714113 and perplexity is 29.74478827380727
At time: 157.76653265953064 and batch: 850, loss is 3.4428430509567263 and perplexity is 31.275750440235548
At time: 158.23985600471497 and batch: 900, loss is 3.4032710218429565 and perplexity is 30.06227374978424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314624943145334 and perplexity of 74.78556934291817
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 159.55562281608582 and batch: 50, loss is 3.6930159282684327 and perplexity is 40.16580164688218
At time: 160.0425956249237 and batch: 100, loss is 3.5818508005142213 and perplexity is 35.93999709456769
At time: 160.51518964767456 and batch: 150, loss is 3.6046759271621704 and perplexity is 36.76976584383293
At time: 160.98474144935608 and batch: 200, loss is 3.4797340297698973 and perplexity is 32.45108990411545
At time: 161.45517349243164 and batch: 250, loss is 3.629889826774597 and perplexity is 37.70866190340932
At time: 161.92731928825378 and batch: 300, loss is 3.595190110206604 and perplexity is 36.4226236422352
At time: 162.39556741714478 and batch: 350, loss is 3.5749706506729124 and perplexity is 35.69357321792751
At time: 162.86693811416626 and batch: 400, loss is 3.5141669797897337 and perplexity is 33.58793683676458
At time: 163.35164260864258 and batch: 450, loss is 3.545102820396423 and perplexity is 34.64324719155196
At time: 163.82239437103271 and batch: 500, loss is 3.4298326539993287 and perplexity is 30.87147609931992
At time: 164.29208421707153 and batch: 550, loss is 3.4721156263351443 and perplexity is 32.204803754158156
At time: 164.7616376876831 and batch: 600, loss is 3.4937787866592407 and perplexity is 32.91007318217975
At time: 165.2314896583557 and batch: 650, loss is 3.3370953178405762 and perplexity is 28.137278041237828
At time: 165.7020387649536 and batch: 700, loss is 3.322972159385681 and perplexity is 27.742683822222563
At time: 166.1750328540802 and batch: 750, loss is 3.4143943929672242 and perplexity is 30.398534285808783
At time: 166.64400696754456 and batch: 800, loss is 3.3899782705307007 and perplexity is 29.66530765197339
At time: 167.11439657211304 and batch: 850, loss is 3.440327205657959 and perplexity is 31.19716438715188
At time: 167.58240628242493 and batch: 900, loss is 3.399914889335632 and perplexity is 29.961549891459068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314160908738228 and perplexity of 74.75087431605375
finished 18 epochs...
Completing Train Step...
At time: 168.88612532615662 and batch: 50, loss is 3.6923313522338868 and perplexity is 40.13831451125702
At time: 169.37133121490479 and batch: 100, loss is 3.5810957527160645 and perplexity is 35.912870920963215
At time: 169.84133291244507 and batch: 150, loss is 3.6038276100158693 and perplexity is 36.73858664779525
At time: 170.31046605110168 and batch: 200, loss is 3.47918466091156 and perplexity is 32.43326718197554
At time: 170.7801651954651 and batch: 250, loss is 3.629417772293091 and perplexity is 37.690865561318205
At time: 171.24954056739807 and batch: 300, loss is 3.5946171712875366 and perplexity is 36.4017616805016
At time: 171.7183301448822 and batch: 350, loss is 3.5744535446166994 and perplexity is 35.67512062643323
At time: 172.18770813941956 and batch: 400, loss is 3.513749713897705 and perplexity is 33.573924659944026
At time: 172.65620303153992 and batch: 450, loss is 3.544663624763489 and perplexity is 34.62803536940221
At time: 173.12608098983765 and batch: 500, loss is 3.4294481754302977 and perplexity is 30.859608959841967
At time: 173.59689235687256 and batch: 550, loss is 3.471721968650818 and perplexity is 32.19212858069151
At time: 174.06577801704407 and batch: 600, loss is 3.4935511684417726 and perplexity is 32.902583102456916
At time: 174.5349588394165 and batch: 650, loss is 3.3370261430740356 and perplexity is 28.135331718917207
At time: 175.02240085601807 and batch: 700, loss is 3.322947688102722 and perplexity is 27.742004931463402
At time: 175.4923861026764 and batch: 750, loss is 3.414439630508423 and perplexity is 30.399909471860703
At time: 175.96180415153503 and batch: 800, loss is 3.390136103630066 and perplexity is 29.669990188943665
At time: 176.43375396728516 and batch: 850, loss is 3.4405362606048584 and perplexity is 31.203686990463737
At time: 176.90480279922485 and batch: 900, loss is 3.400194001197815 and perplexity is 29.969913682605522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313966515946062 and perplexity of 74.73634469715093
finished 19 epochs...
Completing Train Step...
At time: 178.21947526931763 and batch: 50, loss is 3.6917835807800294 and perplexity is 40.11633390908463
At time: 178.68884301185608 and batch: 100, loss is 3.580516700744629 and perplexity is 35.89208152190866
At time: 179.15987920761108 and batch: 150, loss is 3.6032084083557128 and perplexity is 36.715845095481214
At time: 179.6306493282318 and batch: 200, loss is 3.478716721534729 and perplexity is 32.41809392949984
At time: 180.10219287872314 and batch: 250, loss is 3.6289854717254637 and perplexity is 37.674575300139956
At time: 180.57080101966858 and batch: 300, loss is 3.5941409730911253 and perplexity is 36.38443135390578
At time: 181.04108500480652 and batch: 350, loss is 3.5740032243728637 and perplexity is 35.65905901411987
At time: 181.5116147994995 and batch: 400, loss is 3.5133811378479005 and perplexity is 33.561552395616644
At time: 181.98265147209167 and batch: 450, loss is 3.5443133449554445 and perplexity is 34.615907991931785
At time: 182.452210187912 and batch: 500, loss is 3.429142184257507 and perplexity is 30.850167636458963
At time: 182.92188811302185 and batch: 550, loss is 3.471408338546753 and perplexity is 32.182033743159906
At time: 183.39362692832947 and batch: 600, loss is 3.493357291221619 and perplexity is 32.89620465944651
At time: 183.86598825454712 and batch: 650, loss is 3.3369309949874877 and perplexity is 28.132654823292533
At time: 184.33656692504883 and batch: 700, loss is 3.3229182672500612 and perplexity is 27.741188750030233
At time: 184.8078362941742 and batch: 750, loss is 3.414455060958862 and perplexity is 30.40037855977627
At time: 185.2777886390686 and batch: 800, loss is 3.390227880477905 and perplexity is 29.672713332077464
At time: 185.74948859214783 and batch: 850, loss is 3.440685715675354 and perplexity is 31.208350888215517
At time: 186.21888518333435 and batch: 900, loss is 3.400412287712097 and perplexity is 29.97645642466683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31387830760381 and perplexity of 74.72975261882165
Finished Training.
Improved accuracyfrom -10000000 to -74.72975261882165
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f189bae2b70>
ELAPSED
201.34034204483032


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7528445720672607 and batch: 50, loss is 6.875343971252441 and perplexity is 968.1083097475861
At time: 1.2413053512573242 and batch: 100, loss is 5.9691293430328365 and perplexity is 391.16495195608496
At time: 1.716233491897583 and batch: 150, loss is 5.740755615234375 and perplexity is 311.2995448133674
At time: 2.2043399810791016 and batch: 200, loss is 5.526973361968994 and perplexity is 251.38191639577227
At time: 2.683208703994751 and batch: 250, loss is 5.5320950126647945 and perplexity is 252.6727094367328
At time: 3.1582062244415283 and batch: 300, loss is 5.425513896942139 and perplexity is 227.12803673283648
At time: 3.6311862468719482 and batch: 350, loss is 5.387687768936157 and perplexity is 218.69712220106038
At time: 4.10547137260437 and batch: 400, loss is 5.2318795299530025 and perplexity is 187.14421633139082
At time: 4.578540086746216 and batch: 450, loss is 5.225032758712769 and perplexity is 185.86725919838216
At time: 5.052701950073242 and batch: 500, loss is 5.15372145652771 and perplexity is 173.0743821170566
At time: 5.526948928833008 and batch: 550, loss is 5.205496711730957 and perplexity is 182.27138662322668
At time: 6.000488758087158 and batch: 600, loss is 5.116425971984864 and perplexity is 166.73837579397028
At time: 6.472554445266724 and batch: 650, loss is 5.002318038940429 and perplexity is 148.75758562747123
At time: 6.946741819381714 and batch: 700, loss is 5.074388751983642 and perplexity is 159.8744391730148
At time: 7.421780824661255 and batch: 750, loss is 5.066051731109619 and perplexity is 158.54710333782128
At time: 7.894415616989136 and batch: 800, loss is 5.028952674865723 and perplexity is 152.77292595362528
At time: 8.366517782211304 and batch: 850, loss is 5.064547119140625 and perplexity is 158.3087308425147
At time: 8.841496706008911 and batch: 900, loss is 4.981964473724365 and perplexity is 145.76044314207925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.896139484562286 and perplexity of 133.7723513300383
finished 1 epochs...
Completing Train Step...
At time: 10.167695760726929 and batch: 50, loss is 4.8512700748443605 and perplexity is 127.90273277379264
At time: 10.635616064071655 and batch: 100, loss is 4.730136404037475 and perplexity is 113.31101737483229
At time: 11.103997468948364 and batch: 150, loss is 4.714041080474853 and perplexity is 111.50183860273566
At time: 11.572104692459106 and batch: 200, loss is 4.596080322265625 and perplexity is 99.095132419678
At time: 12.041571140289307 and batch: 250, loss is 4.703725280761719 and perplexity is 110.35752039455498
At time: 12.51047945022583 and batch: 300, loss is 4.644111156463623 and perplexity is 103.97091083044484
At time: 12.979214906692505 and batch: 350, loss is 4.6400856590271 and perplexity is 103.55321747124425
At time: 13.446945190429688 and batch: 400, loss is 4.526341791152954 and perplexity is 92.41985080751569
At time: 13.930826425552368 and batch: 450, loss is 4.557633275985718 and perplexity is 95.35752760173663
At time: 14.404251337051392 and batch: 500, loss is 4.448444271087647 and perplexity is 85.49383525714833
At time: 14.87076735496521 and batch: 550, loss is 4.516284646987915 and perplexity is 91.49502937065591
At time: 15.339771270751953 and batch: 600, loss is 4.4843457984924315 and perplexity is 88.61895718987836
At time: 15.809266567230225 and batch: 650, loss is 4.3402870178222654 and perplexity is 76.72955892904415
At time: 16.277469158172607 and batch: 700, loss is 4.372845306396484 and perplexity is 79.26885531358886
At time: 16.74616837501526 and batch: 750, loss is 4.429869041442871 and perplexity is 83.92042609269902
At time: 17.215962648391724 and batch: 800, loss is 4.3928799533844 and perplexity is 80.87299437042907
At time: 17.682512044906616 and batch: 850, loss is 4.455523443222046 and perplexity is 86.10120814545408
At time: 18.15161919593811 and batch: 900, loss is 4.386561393737793 and perplexity is 80.36360453148737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.538420376712328 and perplexity of 93.54292077734013
finished 2 epochs...
Completing Train Step...
At time: 19.46401882171631 and batch: 50, loss is 4.4381911277771 and perplexity is 84.62173324564633
At time: 19.932854652404785 and batch: 100, loss is 4.314626870155334 and perplexity is 74.78571345559703
At time: 20.402400255203247 and batch: 150, loss is 4.320783934593201 and perplexity is 75.24759436851028
At time: 20.87170100212097 and batch: 200, loss is 4.203482055664063 and perplexity is 66.91894130355537
At time: 21.34059166908264 and batch: 250, loss is 4.346744108200073 and perplexity is 77.22661165587898
At time: 21.80901527404785 and batch: 300, loss is 4.30403579711914 and perplexity is 73.99783212146315
At time: 22.277776956558228 and batch: 350, loss is 4.30381911277771 and perplexity is 73.98179968699411
At time: 22.74731183052063 and batch: 400, loss is 4.220581455230713 and perplexity is 68.0730542176794
At time: 23.217341661453247 and batch: 450, loss is 4.262040867805481 and perplexity is 70.95464483698309
At time: 23.685476064682007 and batch: 500, loss is 4.1394246435165405 and perplexity is 62.766697831676616
At time: 24.154649019241333 and batch: 550, loss is 4.211426105499267 and perplexity is 67.45266586861015
At time: 24.621997833251953 and batch: 600, loss is 4.209514155387878 and perplexity is 67.32382294645818
At time: 25.09203267097473 and batch: 650, loss is 4.058103446960449 and perplexity is 57.86446392147272
At time: 25.5619056224823 and batch: 700, loss is 4.070222525596619 and perplexity is 58.569994464838196
At time: 26.05562710762024 and batch: 750, loss is 4.159159016609192 and perplexity is 64.01766216464647
At time: 26.5247700214386 and batch: 800, loss is 4.132759017944336 and perplexity is 62.3497098124457
At time: 26.99323058128357 and batch: 850, loss is 4.200770077705383 and perplexity is 66.73770447594323
At time: 27.46183466911316 and batch: 900, loss is 4.143351502418518 and perplexity is 63.01365837005021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.419049824753853 and perplexity of 83.01736682410588
finished 3 epochs...
Completing Train Step...
At time: 28.773541927337646 and batch: 50, loss is 4.218132643699646 and perplexity is 67.9065600771428
At time: 29.258979320526123 and batch: 100, loss is 4.093631873130798 and perplexity is 59.957253888674785
At time: 29.72637963294983 and batch: 150, loss is 4.105619249343872 and perplexity is 60.680309157107175
At time: 30.195359230041504 and batch: 200, loss is 3.9890609979629517 and perplexity is 54.00415553541365
At time: 30.665837049484253 and batch: 250, loss is 4.14046660900116 and perplexity is 62.83213264889109
At time: 31.13282299041748 and batch: 300, loss is 4.102803297042847 and perplexity is 60.5096766601493
At time: 31.600287675857544 and batch: 350, loss is 4.105299119949341 and perplexity is 60.660886715489866
At time: 32.06794571876526 and batch: 400, loss is 4.03341037273407 and perplexity is 56.45310947235004
At time: 32.53460192680359 and batch: 450, loss is 4.075778245925903 and perplexity is 58.89629856161571
At time: 33.00175666809082 and batch: 500, loss is 3.954285011291504 and perplexity is 52.15838795090061
At time: 33.47006964683533 and batch: 550, loss is 4.025199661254883 and perplexity is 55.99148699623523
At time: 33.93712496757507 and batch: 600, loss is 4.031303653717041 and perplexity is 56.33430382205651
At time: 34.40542769432068 and batch: 650, loss is 3.884906949996948 and perplexity is 48.662414211026075
At time: 34.872288942337036 and batch: 700, loss is 3.8936928129196167 and perplexity is 49.09183918392175
At time: 35.34632658958435 and batch: 750, loss is 3.987071671485901 and perplexity is 53.89683042667073
At time: 35.81380796432495 and batch: 800, loss is 3.9672641706466676 and perplexity is 52.839772312536525
At time: 36.280969858169556 and batch: 850, loss is 4.032725296020508 and perplexity is 56.41444800618952
At time: 36.74998998641968 and batch: 900, loss is 3.9804720878601074 and perplexity is 53.542304934186504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37858644250321 and perplexity of 79.72525748084095
finished 4 epochs...
Completing Train Step...
At time: 38.043304204940796 and batch: 50, loss is 4.062750949859619 and perplexity is 58.13401507000301
At time: 38.52772855758667 and batch: 100, loss is 3.9423168230056764 and perplexity is 51.537867204586526
At time: 38.998958349227905 and batch: 150, loss is 3.9579288291931154 and perplexity is 52.348790303715376
At time: 39.470184087753296 and batch: 200, loss is 3.8454021549224855 and perplexity is 46.777492367607614
At time: 39.94164299964905 and batch: 250, loss is 3.9935358428955077 and perplexity is 54.2463572605709
At time: 40.41000699996948 and batch: 300, loss is 3.959824814796448 and perplexity is 52.44813700668173
At time: 40.88019895553589 and batch: 350, loss is 3.965535192489624 and perplexity is 52.748492433582506
At time: 41.35274863243103 and batch: 400, loss is 3.898830270767212 and perplexity is 49.34469540132338
At time: 41.82674598693848 and batch: 450, loss is 3.939553623199463 and perplexity is 51.39565435166411
At time: 42.293967485427856 and batch: 500, loss is 3.821929907798767 and perplexity is 45.69230522024694
At time: 42.762847661972046 and batch: 550, loss is 3.8873459672927857 and perplexity is 48.78124754030701
At time: 43.23289132118225 and batch: 600, loss is 3.8995284414291382 and perplexity is 49.37915844911882
At time: 43.70214796066284 and batch: 650, loss is 3.7569363260269166 and perplexity is 42.81704735729963
At time: 44.18357968330383 and batch: 700, loss is 3.7657496404647826 and perplexity is 43.1960752515696
At time: 44.654016971588135 and batch: 750, loss is 3.8591493225097655 and perplexity is 47.4249908322119
At time: 45.12315821647644 and batch: 800, loss is 3.8445275974273683 and perplexity is 46.73660064474243
At time: 45.59355020523071 and batch: 850, loss is 3.9076889848709104 and perplexity is 49.78376788781832
At time: 46.06379008293152 and batch: 900, loss is 3.857413206100464 and perplexity is 47.342726957910784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371999662216395 and perplexity of 79.20185040258015
finished 5 epochs...
Completing Train Step...
At time: 47.374170541763306 and batch: 50, loss is 3.943014883995056 and perplexity is 51.573856339001836
At time: 47.844762563705444 and batch: 100, loss is 3.828797378540039 and perplexity is 46.00717573397173
At time: 48.31355953216553 and batch: 150, loss is 3.8453562450408936 and perplexity is 46.775344867767956
At time: 48.78199768066406 and batch: 200, loss is 3.7349293994903565 and perplexity is 41.885068351541555
At time: 49.24942469596863 and batch: 250, loss is 3.879245777130127 and perplexity is 48.387706190529826
At time: 49.71859788894653 and batch: 300, loss is 3.848417434692383 and perplexity is 46.91875245634712
At time: 50.18574118614197 and batch: 350, loss is 3.854980115890503 and perplexity is 47.22767785162058
At time: 50.67101860046387 and batch: 400, loss is 3.7902041625976564 and perplexity is 44.26543670004515
At time: 51.14047193527222 and batch: 450, loss is 3.8324850368499757 and perplexity is 46.177147684503566
At time: 51.610899686813354 and batch: 500, loss is 3.720418939590454 and perplexity is 41.281685018649476
At time: 52.081888914108276 and batch: 550, loss is 3.7813444423675535 and perplexity is 43.87498949551561
At time: 52.55136752128601 and batch: 600, loss is 3.7968249320983887 and perplexity is 44.55948027655119
At time: 53.02169179916382 and batch: 650, loss is 3.659307613372803 and perplexity is 38.83444511011308
At time: 53.48954463005066 and batch: 700, loss is 3.666188859939575 and perplexity is 39.10259605045554
At time: 53.95944833755493 and batch: 750, loss is 3.758492822647095 and perplexity is 42.883743839756676
At time: 54.42879915237427 and batch: 800, loss is 3.7445261669158936 and perplexity is 42.28896456027418
At time: 54.898773193359375 and batch: 850, loss is 3.8088384580612185 and perplexity is 45.098025184258994
At time: 55.368247509002686 and batch: 900, loss is 3.761720733642578 and perplexity is 43.02239240024465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3769205171767975 and perplexity of 79.59255172488035
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 56.67942786216736 and batch: 50, loss is 3.8746715068817137 and perplexity is 48.16687320564786
At time: 57.148935079574585 and batch: 100, loss is 3.7627726936340333 and perplexity is 43.067674048851934
At time: 57.6207218170166 and batch: 150, loss is 3.778179798126221 and perplexity is 43.73636023451369
At time: 58.0955593585968 and batch: 200, loss is 3.6478015851974486 and perplexity is 38.39017568013423
At time: 58.56606435775757 and batch: 250, loss is 3.792246689796448 and perplexity is 44.35594245724153
At time: 59.037144899368286 and batch: 300, loss is 3.7466213512420654 and perplexity is 42.37766062084238
At time: 59.50684905052185 and batch: 350, loss is 3.741323485374451 and perplexity is 42.15374312524965
At time: 59.976381063461304 and batch: 400, loss is 3.671230745315552 and perplexity is 39.30024469986702
At time: 60.447959423065186 and batch: 450, loss is 3.6968010234832764 and perplexity is 40.318121120495476
At time: 60.91749382019043 and batch: 500, loss is 3.5790498447418213 and perplexity is 35.83947160169688
At time: 61.386969327926636 and batch: 550, loss is 3.623688235282898 and perplexity is 37.475531822525134
At time: 61.85603928565979 and batch: 600, loss is 3.6313798522949217 and perplexity is 37.764890652715266
At time: 62.342873096466064 and batch: 650, loss is 3.48353892326355 and perplexity is 32.57479804385582
At time: 62.812615156173706 and batch: 700, loss is 3.465338349342346 and perplexity is 31.987280817808777
At time: 63.28081035614014 and batch: 750, loss is 3.5574827909469606 and perplexity is 35.074795336514256
At time: 63.7474422454834 and batch: 800, loss is 3.520540351867676 and perplexity is 33.80268887594794
At time: 64.21676063537598 and batch: 850, loss is 3.5666165208816527 and perplexity is 35.39662656700949
At time: 64.68541741371155 and batch: 900, loss is 3.5194045400619505 and perplexity is 33.764317178496036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329254150390625 and perplexity of 75.8876646710506
finished 7 epochs...
Completing Train Step...
At time: 65.98228096961975 and batch: 50, loss is 3.7814310789108276 and perplexity is 43.87879083760688
At time: 66.46880888938904 and batch: 100, loss is 3.663764271736145 and perplexity is 39.00790319932169
At time: 66.93974781036377 and batch: 150, loss is 3.6847639322280883 and perplexity is 39.8357174107401
At time: 67.41010808944702 and batch: 200, loss is 3.558886251449585 and perplexity is 35.1240559860089
At time: 67.87945747375488 and batch: 250, loss is 3.706289782524109 and perplexity is 40.702510863522164
At time: 68.34995651245117 and batch: 300, loss is 3.66611385345459 and perplexity is 39.099663212164316
At time: 68.81901693344116 and batch: 350, loss is 3.6650223684310914 and perplexity is 39.05700979736069
At time: 69.28842568397522 and batch: 400, loss is 3.597909379005432 and perplexity is 36.521801330560216
At time: 69.75840711593628 and batch: 450, loss is 3.6266090726852416 and perplexity is 37.58515177063403
At time: 70.22858905792236 and batch: 500, loss is 3.5146119832992553 and perplexity is 33.60288691270595
At time: 70.69803071022034 and batch: 550, loss is 3.5630166578292846 and perplexity is 35.26943263657673
At time: 71.16649508476257 and batch: 600, loss is 3.577677779197693 and perplexity is 35.79033121721577
At time: 71.63862538337708 and batch: 650, loss is 3.435798625946045 and perplexity is 31.05620495535049
At time: 72.10643529891968 and batch: 700, loss is 3.4212326335906984 and perplexity is 30.60711914176208
At time: 72.57625675201416 and batch: 750, loss is 3.5201779413223266 and perplexity is 33.7904406446163
At time: 73.04501104354858 and batch: 800, loss is 3.4881968259811402 and perplexity is 32.726882205836155
At time: 73.51402497291565 and batch: 850, loss is 3.5408808517456056 and perplexity is 34.49729281215706
At time: 73.98320245742798 and batch: 900, loss is 3.502077579498291 and perplexity is 33.184323460996225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333334726830051 and perplexity of 76.1979627545076
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.29839658737183 and batch: 50, loss is 3.7581855869293213 and perplexity is 42.87057044570946
At time: 75.78361964225769 and batch: 100, loss is 3.648572335243225 and perplexity is 38.41977631567728
At time: 76.25315594673157 and batch: 150, loss is 3.672958965301514 and perplexity is 39.36822289192613
At time: 76.72501254081726 and batch: 200, loss is 3.5449355936050413 and perplexity is 34.6374543968498
At time: 77.19460129737854 and batch: 250, loss is 3.690540704727173 and perplexity is 40.066505250178444
At time: 77.66603541374207 and batch: 300, loss is 3.643109998703003 and perplexity is 38.210486693200224
At time: 78.13353943824768 and batch: 350, loss is 3.63836190700531 and perplexity is 38.029489833449276
At time: 78.60387897491455 and batch: 400, loss is 3.571240472793579 and perplexity is 35.56067785643912
At time: 79.07434582710266 and batch: 450, loss is 3.5934974813461302 and perplexity is 36.36102580412989
At time: 79.54262852668762 and batch: 500, loss is 3.4793107652664186 and perplexity is 32.437357416102216
At time: 80.01593112945557 and batch: 550, loss is 3.52003680229187 and perplexity is 33.78567183112578
At time: 80.48581910133362 and batch: 600, loss is 3.538677463531494 and perplexity is 34.42136556307726
At time: 80.95717287063599 and batch: 650, loss is 3.387135581970215 and perplexity is 29.581098168642516
At time: 81.42533135414124 and batch: 700, loss is 3.3680707931518556 and perplexity is 29.022482642280686
At time: 81.89507102966309 and batch: 750, loss is 3.4641749715805052 and perplexity is 31.950089164809338
At time: 82.364755153656 and batch: 800, loss is 3.4244223976135255 and perplexity is 30.70490450244507
At time: 82.83459997177124 and batch: 850, loss is 3.476408681869507 and perplexity is 32.343357962804255
At time: 83.30276370048523 and batch: 900, loss is 3.438314561843872 and perplexity is 31.134438750586973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319754404564426 and perplexity of 75.17016457550255
finished 9 epochs...
Completing Train Step...
At time: 84.6214919090271 and batch: 50, loss is 3.729581747055054 and perplexity is 41.66167939936711
At time: 85.09162402153015 and batch: 100, loss is 3.6137657260894773 and perplexity is 37.10551927576669
At time: 85.56210136413574 and batch: 150, loss is 3.638500065803528 and perplexity is 38.03474430502889
At time: 86.03198504447937 and batch: 200, loss is 3.5119006633758545 and perplexity is 33.51190213609221
At time: 86.51827907562256 and batch: 250, loss is 3.659123020172119 and perplexity is 38.82727719718778
At time: 86.98942804336548 and batch: 300, loss is 3.614226083755493 and perplexity is 37.12260501849136
At time: 87.45836758613586 and batch: 350, loss is 3.611342396736145 and perplexity is 37.01570924532754
At time: 87.9269917011261 and batch: 400, loss is 3.5453487634658813 and perplexity is 34.6517685059384
At time: 88.39794325828552 and batch: 450, loss is 3.5697981262207032 and perplexity is 35.50942400639678
At time: 88.86978554725647 and batch: 500, loss is 3.457858500480652 and perplexity is 31.748913379351777
At time: 89.3418664932251 and batch: 550, loss is 3.50049515247345 and perplexity is 33.13185321686574
At time: 89.81329250335693 and batch: 600, loss is 3.5223148679733276 and perplexity is 33.86272554403571
At time: 90.28490209579468 and batch: 650, loss is 3.373955821990967 and perplexity is 29.19378435296812
At time: 90.75758743286133 and batch: 700, loss is 3.357118315696716 and perplexity is 28.706348939268096
At time: 91.22591066360474 and batch: 750, loss is 3.4567943525314333 and perplexity is 31.715145808321516
At time: 91.69494986534119 and batch: 800, loss is 3.4198271942138674 and perplexity is 30.56413290565908
At time: 92.16779041290283 and batch: 850, loss is 3.4748966217041017 and perplexity is 32.29448981471401
At time: 92.63739109039307 and batch: 900, loss is 3.4391025638580324 and perplexity is 31.158982419992626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320588412350172 and perplexity of 75.23288322830125
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 93.96201705932617 and batch: 50, loss is 3.723637790679932 and perplexity is 41.41477870491044
At time: 94.4313530921936 and batch: 100, loss is 3.6143397235870363 and perplexity is 37.126823864782054
At time: 94.9030294418335 and batch: 150, loss is 3.638653140068054 and perplexity is 38.040566891172496
At time: 95.37361931800842 and batch: 200, loss is 3.5132223176956177 and perplexity is 33.55622256800754
At time: 95.84665966033936 and batch: 250, loss is 3.659298133850098 and perplexity is 38.834076979853776
At time: 96.31570386886597 and batch: 300, loss is 3.611278228759766 and perplexity is 37.013334098376035
At time: 96.7853410243988 and batch: 350, loss is 3.607576355934143 and perplexity is 36.87656874285254
At time: 97.25728249549866 and batch: 400, loss is 3.539224042892456 and perplexity is 34.44018471368132
At time: 97.7277135848999 and batch: 450, loss is 3.561012654304504 and perplexity is 35.19882334352286
At time: 98.19976735115051 and batch: 500, loss is 3.447726993560791 and perplexity is 31.428873026879018
At time: 98.68551921844482 and batch: 550, loss is 3.4886799240112305 and perplexity is 32.74269631773638
At time: 99.1558768749237 and batch: 600, loss is 3.513695821762085 and perplexity is 33.572115338197484
At time: 99.62605595588684 and batch: 650, loss is 3.359235954284668 and perplexity is 28.767203022237137
At time: 100.0940613746643 and batch: 700, loss is 3.3392963933944704 and perplexity is 28.199278524930698
At time: 100.56430792808533 and batch: 750, loss is 3.4368201732635497 and perplexity is 31.087946548223684
At time: 101.03425431251526 and batch: 800, loss is 3.3993173599243165 and perplexity is 29.943652331881953
At time: 101.5060350894928 and batch: 850, loss is 3.4523363161087035 and perplexity is 31.574073220164642
At time: 101.9744484424591 and batch: 900, loss is 3.4173014068603518 and perplexity is 30.48703181676195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31601913661173 and perplexity of 74.88990761204249
finished 11 epochs...
Completing Train Step...
At time: 103.27700591087341 and batch: 50, loss is 3.7122785806655885 and perplexity is 40.94700135630472
At time: 103.76218223571777 and batch: 100, loss is 3.599744052886963 and perplexity is 36.58886842988311
At time: 104.23247909545898 and batch: 150, loss is 3.626145987510681 and perplexity is 37.56775067347146
At time: 104.70305562019348 and batch: 200, loss is 3.5002032136917114 and perplexity is 33.12218215574846
At time: 105.17649912834167 and batch: 250, loss is 3.6469637870788576 and perplexity is 38.35802593255656
At time: 105.6474015712738 and batch: 300, loss is 3.6009745693206785 and perplexity is 36.63391934603891
At time: 106.11825299263 and batch: 350, loss is 3.597869415283203 and perplexity is 36.52034181260063
At time: 106.58818912506104 and batch: 400, loss is 3.5302522659301756 and perplexity is 34.132577015069515
At time: 107.05745506286621 and batch: 450, loss is 3.553285174369812 and perplexity is 34.927873370938904
At time: 107.53078198432922 and batch: 500, loss is 3.440509114265442 and perplexity is 31.202839936082917
At time: 108.0009994506836 and batch: 550, loss is 3.482719259262085 and perplexity is 32.5481085942291
At time: 108.47084641456604 and batch: 600, loss is 3.5088832330703736 and perplexity is 33.41093471467914
At time: 108.94272184371948 and batch: 650, loss is 3.3556617069244385 and perplexity is 28.664565458063425
At time: 109.41167378425598 and batch: 700, loss is 3.3369717359542848 and perplexity is 28.13380099819658
At time: 109.88230895996094 and batch: 750, loss is 3.436039752960205 and perplexity is 31.063694348233703
At time: 110.35099601745605 and batch: 800, loss is 3.399707627296448 and perplexity is 29.955340643024417
At time: 110.83701705932617 and batch: 850, loss is 3.4546875381469726 and perplexity is 31.64839822000606
At time: 111.30585265159607 and batch: 900, loss is 3.4208950328826906 and perplexity is 30.59678790068502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315760782320205 and perplexity of 74.87056198213989
finished 12 epochs...
Completing Train Step...
At time: 112.60429620742798 and batch: 50, loss is 3.7066376972198487 and perplexity is 40.71667432890109
At time: 113.08949947357178 and batch: 100, loss is 3.5936184549331665 and perplexity is 36.36542479392515
At time: 113.55900311470032 and batch: 150, loss is 3.619712805747986 and perplexity is 37.326846226854926
At time: 114.02928280830383 and batch: 200, loss is 3.4937209272384644 and perplexity is 32.90816907949339
At time: 114.49874186515808 and batch: 250, loss is 3.6405348634719847 and perplexity is 38.11221610705181
At time: 114.96647810935974 and batch: 300, loss is 3.5949949645996093 and perplexity is 36.41551662071077
At time: 115.43684005737305 and batch: 350, loss is 3.5921772623062136 and perplexity is 36.313052959851554
At time: 115.90628504753113 and batch: 400, loss is 3.524778470993042 and perplexity is 33.94625270357677
At time: 116.37640738487244 and batch: 450, loss is 3.5483383417129515 and perplexity is 34.75551768515875
At time: 116.84726285934448 and batch: 500, loss is 3.436017408370972 and perplexity is 31.06300025049812
At time: 117.31638169288635 and batch: 550, loss is 3.4786908340454104 and perplexity is 32.41725471730209
At time: 117.78615689277649 and batch: 600, loss is 3.505620713233948 and perplexity is 33.30210849779378
At time: 118.25796318054199 and batch: 650, loss is 3.3530515909194945 and perplexity is 28.58984517371221
At time: 118.72865581512451 and batch: 700, loss is 3.335017237663269 and perplexity is 28.078867233687795
At time: 119.19965672492981 and batch: 750, loss is 3.434890642166138 and perplexity is 31.0280192230159
At time: 119.67016935348511 and batch: 800, loss is 3.3992237997055055 and perplexity is 29.940850928269793
At time: 120.14112186431885 and batch: 850, loss is 3.4549876737594603 and perplexity is 31.657898456998485
At time: 120.61026191711426 and batch: 900, loss is 3.4216148328781126 and perplexity is 30.61881939665998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316110271297089 and perplexity of 74.89673299121895
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 121.92512202262878 and batch: 50, loss is 3.704674105644226 and perplexity is 40.63680185432423
At time: 122.39498162269592 and batch: 100, loss is 3.593400893211365 and perplexity is 36.357513930074724
At time: 122.87922191619873 and batch: 150, loss is 3.6207256269454957 and perplexity is 37.36467079948075
At time: 123.34807825088501 and batch: 200, loss is 3.493972730636597 and perplexity is 32.91645651165193
At time: 123.81731581687927 and batch: 250, loss is 3.640932631492615 and perplexity is 38.12737894326029
At time: 124.28784441947937 and batch: 300, loss is 3.594444375038147 and perplexity is 36.39547213603254
At time: 124.75943279266357 and batch: 350, loss is 3.5915339279174803 and perplexity is 36.289699037118865
At time: 125.22998857498169 and batch: 400, loss is 3.5238589000701905 and perplexity is 33.915051064909335
At time: 125.70107460021973 and batch: 450, loss is 3.5468270206451415 and perplexity is 34.7030306114591
At time: 126.17276573181152 and batch: 500, loss is 3.4334163999557497 and perplexity is 30.982310108804022
At time: 126.64540529251099 and batch: 550, loss is 3.47518705368042 and perplexity is 32.303870529378905
At time: 127.11391878128052 and batch: 600, loss is 3.5025227308273315 and perplexity is 33.199098795074
At time: 127.58296990394592 and batch: 650, loss is 3.3477292013168336 and perplexity is 28.43808310570174
At time: 128.0508496761322 and batch: 700, loss is 3.3287930059432984 and perplexity is 27.904640633433488
At time: 128.51859068870544 and batch: 750, loss is 3.426901879310608 and perplexity is 30.78113121348457
At time: 128.98742985725403 and batch: 800, loss is 3.3917113399505614 and perplexity is 29.716764265558226
At time: 129.45778703689575 and batch: 850, loss is 3.445819444656372 and perplexity is 31.368978059021494
At time: 129.9268102645874 and batch: 900, loss is 3.412615194320679 and perplexity is 30.34449734023543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31633518166738 and perplexity of 74.91357993762509
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 131.24254894256592 and batch: 50, loss is 3.7025638961791993 and perplexity is 40.55114010432817
At time: 131.7121603488922 and batch: 100, loss is 3.5910843229293823 and perplexity is 36.27338667474973
At time: 132.17996406555176 and batch: 150, loss is 3.619473948478699 and perplexity is 37.31793150300976
At time: 132.64902257919312 and batch: 200, loss is 3.4923533535003664 and perplexity is 32.86319549104997
At time: 133.11895728111267 and batch: 250, loss is 3.639767622947693 and perplexity is 38.08298608504649
At time: 133.58748722076416 and batch: 300, loss is 3.593207335472107 and perplexity is 36.35047733288928
At time: 134.05701184272766 and batch: 350, loss is 3.5897695446014404 and perplexity is 36.225726550183765
At time: 134.52591848373413 and batch: 400, loss is 3.522989649772644 and perplexity is 33.8855832060079
At time: 135.00808000564575 and batch: 450, loss is 3.545753002166748 and perplexity is 34.665778923407665
At time: 135.4771454334259 and batch: 500, loss is 3.4319759130477907 and perplexity is 30.937712625471057
At time: 135.9482843875885 and batch: 550, loss is 3.4737824821472167 and perplexity is 32.258529282387634
At time: 136.41697931289673 and batch: 600, loss is 3.500663342475891 and perplexity is 33.13742613198034
At time: 136.88558959960938 and batch: 650, loss is 3.3457742023468016 and perplexity is 28.38254099259762
At time: 137.35377645492554 and batch: 700, loss is 3.326961512565613 and perplexity is 27.853580241564387
At time: 137.8246624469757 and batch: 750, loss is 3.4249062395095824 and perplexity is 30.71976441629233
At time: 138.29672622680664 and batch: 800, loss is 3.3894084787368772 and perplexity is 29.648409417806814
At time: 138.76663756370544 and batch: 850, loss is 3.4434540700912475 and perplexity is 31.294866361703203
At time: 139.23576712608337 and batch: 900, loss is 3.4098015308380125 and perplexity is 30.259238137769252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31591337021083 and perplexity of 74.88198719491504
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 140.53459310531616 and batch: 50, loss is 3.7019594383239744 and perplexity is 40.5266360557322
At time: 141.01841282844543 and batch: 100, loss is 3.5903741025924685 and perplexity is 36.24763372406176
At time: 141.4865963459015 and batch: 150, loss is 3.6189015436172487 and perplexity is 37.29657664999208
At time: 141.9549582004547 and batch: 200, loss is 3.4918274545669554 and perplexity is 32.84591731527807
At time: 142.42375302314758 and batch: 250, loss is 3.639404411315918 and perplexity is 38.069156413228555
At time: 142.89395952224731 and batch: 300, loss is 3.592828965187073 and perplexity is 36.33672599413274
At time: 143.36388182640076 and batch: 350, loss is 3.589228587150574 and perplexity is 36.20613527299483
At time: 143.8364601135254 and batch: 400, loss is 3.5226381397247315 and perplexity is 33.87367417622605
At time: 144.30612587928772 and batch: 450, loss is 3.5453872537612914 and perplexity is 34.65310228841335
At time: 144.77466297149658 and batch: 500, loss is 3.4315034532546997 and perplexity is 30.923099252552724
At time: 145.2441165447235 and batch: 550, loss is 3.4733578824996947 and perplexity is 32.244835229675466
At time: 145.7130720615387 and batch: 600, loss is 3.500124244689941 and perplexity is 33.11956663336113
At time: 146.1830494403839 and batch: 650, loss is 3.3452540588378907 and perplexity is 28.367781836906197
At time: 146.65077829360962 and batch: 700, loss is 3.3264701890945436 and perplexity is 27.83989848519764
At time: 147.13268780708313 and batch: 750, loss is 3.4243949127197264 and perplexity is 30.704060593003142
At time: 147.6028015613556 and batch: 800, loss is 3.388815550804138 and perplexity is 29.630835258321603
At time: 148.0715627670288 and batch: 850, loss is 3.4428466749191284 and perplexity is 31.27586378258462
At time: 148.53957557678223 and batch: 900, loss is 3.40907630443573 and perplexity is 30.237301294909084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315788791604238 and perplexity of 74.87265908234512
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 149.8301179409027 and batch: 50, loss is 3.7018097925186155 and perplexity is 40.520571868392565
At time: 150.31497502326965 and batch: 100, loss is 3.5901942920684813 and perplexity is 36.24111660398948
At time: 150.7870388031006 and batch: 150, loss is 3.618750033378601 and perplexity is 37.29092626481952
At time: 151.25749897956848 and batch: 200, loss is 3.491689429283142 and perplexity is 32.84138406107756
At time: 151.7289080619812 and batch: 250, loss is 3.639303674697876 and perplexity is 38.06532164831364
At time: 152.19933199882507 and batch: 300, loss is 3.5927285957336426 and perplexity is 36.333079079827726
At time: 152.6678488254547 and batch: 350, loss is 3.5890931558609007 and perplexity is 36.20123216142559
At time: 153.14093565940857 and batch: 400, loss is 3.522546148300171 and perplexity is 33.87055823200575
At time: 153.60929131507874 and batch: 450, loss is 3.545291533470154 and perplexity is 34.649785442120766
At time: 154.0785038471222 and batch: 500, loss is 3.431381177902222 and perplexity is 30.919318350852162
At time: 154.54820775985718 and batch: 550, loss is 3.47324818611145 and perplexity is 32.24129828170999
At time: 155.0191991329193 and batch: 600, loss is 3.499985375404358 and perplexity is 33.11496766213924
At time: 155.48971915245056 and batch: 650, loss is 3.3451225566864013 and perplexity is 28.364051657830323
At time: 155.95861172676086 and batch: 700, loss is 3.3263451290130615 and perplexity is 27.83641704292389
At time: 156.42845845222473 and batch: 750, loss is 3.4242649507522582 and perplexity is 30.70007049216551
At time: 156.89843273162842 and batch: 800, loss is 3.388665738105774 and perplexity is 29.626396515435097
At time: 157.36818647384644 and batch: 850, loss is 3.442692966461182 and perplexity is 31.271056787239033
At time: 157.83478784561157 and batch: 900, loss is 3.408893518447876 and perplexity is 30.23177484501604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315757019879067 and perplexity of 74.87028028658739
finished 17 epochs...
Completing Train Step...
At time: 159.14672183990479 and batch: 50, loss is 3.701760745048523 and perplexity is 40.51858448559417
At time: 159.61476302146912 and batch: 100, loss is 3.5901295948028564 and perplexity is 36.238771978688256
At time: 160.08272337913513 and batch: 150, loss is 3.6186792850494385 and perplexity is 37.288288087417776
At time: 160.55234122276306 and batch: 200, loss is 3.4916268491744997 and perplexity is 32.83932890800158
At time: 161.02228689193726 and batch: 250, loss is 3.6392413139343263 and perplexity is 38.062947939804786
At time: 161.4910876750946 and batch: 300, loss is 3.5926697349548338 and perplexity is 36.33094054943496
At time: 161.95877194404602 and batch: 350, loss is 3.58903329372406 and perplexity is 36.199065143173954
At time: 162.4291272163391 and batch: 400, loss is 3.5224890184402464 and perplexity is 33.868623267031055
At time: 162.89713716506958 and batch: 450, loss is 3.5452369832992554 and perplexity is 34.64789534195641
At time: 163.36584997177124 and batch: 500, loss is 3.4313326549530028 and perplexity is 30.917818090736752
At time: 163.8361940383911 and batch: 550, loss is 3.473214416503906 and perplexity is 32.24020952410386
At time: 164.30401396751404 and batch: 600, loss is 3.4999652528762817 and perplexity is 33.114301311977066
At time: 164.77228140830994 and batch: 650, loss is 3.345108971595764 and perplexity is 28.36366633223506
At time: 165.2428367137909 and batch: 700, loss is 3.326347522735596 and perplexity is 27.836483675662393
At time: 165.7118685245514 and batch: 750, loss is 3.42425190448761 and perplexity is 30.69966997353379
At time: 166.18054604530334 and batch: 800, loss is 3.388685975074768 and perplexity is 29.626996069969355
At time: 166.64969873428345 and batch: 850, loss is 3.4427264022827146 and perplexity is 31.272102378192912
At time: 167.1213300228119 and batch: 900, loss is 3.4089484119415285 and perplexity is 30.233434418306068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315727756447988 and perplexity of 74.86808935735755
finished 18 epochs...
Completing Train Step...
At time: 168.43630647659302 and batch: 50, loss is 3.701712703704834 and perplexity is 40.51663796510821
At time: 168.90409064292908 and batch: 100, loss is 3.5900661754608154 and perplexity is 36.23647381248785
At time: 169.3705530166626 and batch: 150, loss is 3.6186100339889524 and perplexity is 37.28570592333385
At time: 169.83672451972961 and batch: 200, loss is 3.491565079689026 and perplexity is 32.83730050219907
At time: 170.3046751022339 and batch: 250, loss is 3.639179759025574 and perplexity is 38.060605050626414
At time: 170.77269887924194 and batch: 300, loss is 3.5926120615005495 and perplexity is 36.3288452790174
At time: 171.2556710243225 and batch: 350, loss is 3.58897451877594 and perplexity is 36.196937607521676
At time: 171.7247440814972 and batch: 400, loss is 3.522433137893677 and perplexity is 33.866730722730026
At time: 172.19365620613098 and batch: 450, loss is 3.545183458328247 and perplexity is 34.64604086399362
At time: 172.6612515449524 and batch: 500, loss is 3.431285104751587 and perplexity is 30.916347977211572
At time: 173.1282868385315 and batch: 550, loss is 3.473181195259094 and perplexity is 32.239138482001245
At time: 173.59631395339966 and batch: 600, loss is 3.499945592880249 and perplexity is 33.11365029134421
At time: 174.07100820541382 and batch: 650, loss is 3.3450955724716187 and perplexity is 28.3632862864948
At time: 174.54060983657837 and batch: 700, loss is 3.3263499975204467 and perplexity is 27.836552565055737
At time: 175.0098431110382 and batch: 750, loss is 3.4242397928237915 and perplexity is 30.699298151703427
At time: 175.47914218902588 and batch: 800, loss is 3.3887058019638063 and perplexity is 29.627583486956276
At time: 175.95035529136658 and batch: 850, loss is 3.4427592897415162 and perplexity is 31.27313085508342
At time: 176.41947054862976 and batch: 900, loss is 3.409002351760864 and perplexity is 30.235065248279426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31569932911494 and perplexity of 74.86596108749741
finished 19 epochs...
Completing Train Step...
At time: 177.7160563468933 and batch: 50, loss is 3.701665382385254 and perplexity is 40.51472070969863
At time: 178.19961094856262 and batch: 100, loss is 3.590003709793091 and perplexity is 36.234210347650325
At time: 178.6676173210144 and batch: 150, loss is 3.6185421323776246 and perplexity is 37.283174249775485
At time: 179.13374638557434 and batch: 200, loss is 3.491504487991333 and perplexity is 32.835310894691446
At time: 179.60398173332214 and batch: 250, loss is 3.6391196346282957 and perplexity is 38.05831674847978
At time: 180.0747833251953 and batch: 300, loss is 3.592555408477783 and perplexity is 36.32678719841753
At time: 180.5435802936554 and batch: 350, loss is 3.5889168548583985 and perplexity is 36.194850410474785
At time: 181.0216348171234 and batch: 400, loss is 3.5223785543441775 and perplexity is 33.86488220680698
At time: 181.50419855117798 and batch: 450, loss is 3.5451307916641235 and perplexity is 34.64421622064558
At time: 181.99157977104187 and batch: 500, loss is 3.431238889694214 and perplexity is 30.914919209431588
At time: 182.47083497047424 and batch: 550, loss is 3.4731486511230467 and perplexity is 32.23808930416482
At time: 182.93896293640137 and batch: 600, loss is 3.499926323890686 and perplexity is 33.11301223090975
At time: 183.42377471923828 and batch: 650, loss is 3.3450825119018557 and perplexity is 28.36291584823462
At time: 183.89402770996094 and batch: 700, loss is 3.3263524055480955 and perplexity is 27.83661959632467
At time: 184.36429286003113 and batch: 750, loss is 3.4242286348342894 and perplexity is 30.698955611167964
At time: 184.8352267742157 and batch: 800, loss is 3.3887254190444946 and perplexity is 29.628164699352965
At time: 185.3053925037384 and batch: 850, loss is 3.4427916479110716 and perplexity is 31.274142812726613
At time: 185.77527618408203 and batch: 900, loss is 3.4090555763244628 and perplexity is 30.23667453925917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315672992026969 and perplexity of 74.86398936205909
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f189bae2b70>
ELAPSED
395.32755398750305


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.86398936205909}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5386602506977849, 'wordvec_dim': 300, 'rnn_dropout': 0.729059618910752, 'num_layers': 1, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7647705078125 and batch: 50, loss is 6.8486534690856935 and perplexity is 942.6107971640874
At time: 1.2568440437316895 and batch: 100, loss is 6.0096386051177975 and perplexity is 407.3360844941027
At time: 1.730327844619751 and batch: 150, loss is 5.804794626235962 and perplexity is 331.88702545618526
At time: 2.2064216136932373 and batch: 200, loss is 5.606757564544678 and perplexity is 272.2600197280558
At time: 2.68432879447937 and batch: 250, loss is 5.624935779571533 and perplexity is 277.2544785499797
At time: 3.1612706184387207 and batch: 300, loss is 5.525131311416626 and perplexity is 250.9192844243742
At time: 3.6400959491729736 and batch: 350, loss is 5.489817047119141 and perplexity is 242.21288925819044
At time: 4.116271734237671 and batch: 400, loss is 5.34863772392273 and perplexity is 210.3215865551363
At time: 4.59403657913208 and batch: 450, loss is 5.340190925598145 and perplexity is 208.55252450368442
At time: 5.071466684341431 and batch: 500, loss is 5.275719909667969 and perplexity is 195.5311906502781
At time: 5.545713901519775 and batch: 550, loss is 5.329701728820801 and perplexity is 206.3764088403613
At time: 6.01901650428772 and batch: 600, loss is 5.2381244659423825 and perplexity is 188.31657683120847
At time: 6.4935243129730225 and batch: 650, loss is 5.1307329750061035 and perplexity is 169.14104878153546
At time: 6.970267295837402 and batch: 700, loss is 5.213324403762817 and perplexity is 183.7037496384849
At time: 7.444277286529541 and batch: 750, loss is 5.1964883041381835 and perplexity is 180.63678531162608
At time: 7.918292999267578 and batch: 800, loss is 5.158595314025879 and perplexity is 173.9199809823673
At time: 8.392134428024292 and batch: 850, loss is 5.194172706604004 and perplexity is 180.2189871298819
At time: 8.865940809249878 and batch: 900, loss is 5.113402585983277 and perplexity is 166.23502262155858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.983838643113228 and perplexity of 146.0338790552952
finished 1 epochs...
Completing Train Step...
At time: 10.199144840240479 and batch: 50, loss is 4.923614835739135 and perplexity is 137.49875131763991
At time: 10.67420482635498 and batch: 100, loss is 4.792097759246826 and perplexity is 120.55399685565511
At time: 11.165202856063843 and batch: 150, loss is 4.7738984680175784 and perplexity is 118.37984359112016
At time: 11.634684085845947 and batch: 200, loss is 4.6612941265106205 and perplexity is 105.77287710695076
At time: 12.103878736495972 and batch: 250, loss is 4.7651048755645755 and perplexity is 117.34342310025477
At time: 12.572941780090332 and batch: 300, loss is 4.699363584518433 and perplexity is 109.87722263035613
At time: 13.041859149932861 and batch: 350, loss is 4.691951379776001 and perplexity is 109.06580108600735
At time: 13.510380029678345 and batch: 400, loss is 4.570439538955688 and perplexity is 96.58655399729379
At time: 13.97856616973877 and batch: 450, loss is 4.599211626052856 and perplexity is 99.4059157076258
At time: 14.446840524673462 and batch: 500, loss is 4.491708536148071 and perplexity is 89.27384324164058
At time: 14.914024591445923 and batch: 550, loss is 4.563189735412598 and perplexity is 95.8888526108103
At time: 15.383275985717773 and batch: 600, loss is 4.5240482902526855 and perplexity is 92.20812868173643
At time: 15.85236144065857 and batch: 650, loss is 4.379807958602905 and perplexity is 79.82270266972313
At time: 16.321816205978394 and batch: 700, loss is 4.425841960906983 and perplexity is 83.58315135030199
At time: 16.7923424243927 and batch: 750, loss is 4.477474184036255 and perplexity is 88.01208935144366
At time: 17.26351261138916 and batch: 800, loss is 4.43680853843689 and perplexity is 84.50481698152534
At time: 17.734456539154053 and batch: 850, loss is 4.488114261627198 and perplexity is 88.95354450711012
At time: 18.205322742462158 and batch: 900, loss is 4.428476581573486 and perplexity is 83.80365158791733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.565712288634418 and perplexity of 96.1310426851497
finished 2 epochs...
Completing Train Step...
At time: 19.52057194709778 and batch: 50, loss is 4.462783250808716 and perplexity is 86.72856082406504
At time: 19.98911762237549 and batch: 100, loss is 4.335637440681458 and perplexity is 76.37362703345403
At time: 20.4585177898407 and batch: 150, loss is 4.336357126235962 and perplexity is 76.42861181307761
At time: 20.9270498752594 and batch: 200, loss is 4.230049829483033 and perplexity is 68.72065640331421
At time: 21.396280765533447 and batch: 250, loss is 4.373814573287964 and perplexity is 79.3457252382882
At time: 21.863717079162598 and batch: 300, loss is 4.326034574508667 and perplexity is 75.64373146757781
At time: 22.332775831222534 and batch: 350, loss is 4.329559106826782 and perplexity is 75.91081063189243
At time: 22.801318407058716 and batch: 400, loss is 4.2332833385467525 and perplexity is 68.94322491338285
At time: 23.288074254989624 and batch: 450, loss is 4.276636972427368 and perplexity is 71.99790147619706
At time: 23.758031129837036 and batch: 500, loss is 4.158082904815674 and perplexity is 63.94880905686273
At time: 24.231281280517578 and batch: 550, loss is 4.2324884271621706 and perplexity is 68.88844293530308
At time: 24.702651977539062 and batch: 600, loss is 4.224787216186524 and perplexity is 68.35995610873327
At time: 25.17312788963318 and batch: 650, loss is 4.077050342559814 and perplexity is 58.971268018853024
At time: 25.644840240478516 and batch: 700, loss is 4.1022771453857425 and perplexity is 60.47784776765026
At time: 26.114010095596313 and batch: 750, loss is 4.184073834419251 and perplexity is 65.63268602398708
At time: 26.582929372787476 and batch: 800, loss is 4.156418008804321 and perplexity is 63.8424295197393
At time: 27.054097414016724 and batch: 850, loss is 4.212703666687012 and perplexity is 67.53889584684204
At time: 27.523217916488647 and batch: 900, loss is 4.167009849548339 and perplexity is 64.52223219132924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436181891454409 and perplexity of 84.45187888144147
finished 3 epochs...
Completing Train Step...
At time: 28.765936374664307 and batch: 50, loss is 4.226082096099853 and perplexity is 68.44853137756648
At time: 29.23961615562439 and batch: 100, loss is 4.102344727516174 and perplexity is 60.481935127560995
At time: 29.717734813690186 and batch: 150, loss is 4.108213047981263 and perplexity is 60.83790595914401
At time: 30.191147089004517 and batch: 200, loss is 3.9989870166778565 and perplexity is 54.542871020826524
At time: 30.66151189804077 and batch: 250, loss is 4.1580198383331295 and perplexity is 63.94477615758431
At time: 31.132364511489868 and batch: 300, loss is 4.1146650218963625 and perplexity is 61.231699548356524
At time: 31.602396488189697 and batch: 350, loss is 4.116162662506103 and perplexity is 61.32347133162521
At time: 32.075180530548096 and batch: 400, loss is 4.037489948272705 and perplexity is 56.68388460907006
At time: 32.54826307296753 and batch: 450, loss is 4.083167548179627 and perplexity is 59.33311300271106
At time: 33.02062010765076 and batch: 500, loss is 3.9633934783935545 and perplexity is 52.63564113453689
At time: 33.49257564544678 and batch: 550, loss is 4.037421259880066 and perplexity is 56.679991217864675
At time: 33.96644830703735 and batch: 600, loss is 4.039641923904419 and perplexity is 56.80599829317025
At time: 34.437981367111206 and batch: 650, loss is 3.894711318016052 and perplexity is 49.141864943745105
At time: 34.90709209442139 and batch: 700, loss is 3.9110827255249023 and perplexity is 49.95300810106914
At time: 35.39268970489502 and batch: 750, loss is 4.002744402885437 and perplexity is 54.748195151587566
At time: 35.860493898391724 and batch: 800, loss is 3.9782306432723997 and perplexity is 53.422427224389814
At time: 36.334233045578 and batch: 850, loss is 4.034410052299499 and perplexity is 56.50957271016474
At time: 36.80459117889404 and batch: 900, loss is 3.9940202474594115 and perplexity is 54.27264080902455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393555105549016 and perplexity of 80.92761438397265
finished 4 epochs...
Completing Train Step...
At time: 38.107505083084106 and batch: 50, loss is 4.064158005714416 and perplexity is 58.21587045031139
At time: 38.59223198890686 and batch: 100, loss is 3.948134016990662 and perplexity is 51.838546683205955
At time: 39.06561279296875 and batch: 150, loss is 3.952942957878113 and perplexity is 52.0884355587368
At time: 39.5370979309082 and batch: 200, loss is 3.84521222114563 and perplexity is 46.76860858550225
At time: 40.0107262134552 and batch: 250, loss is 4.005787420272827 and perplexity is 54.91504860158222
At time: 40.483662605285645 and batch: 300, loss is 3.9639999866485596 and perplexity is 52.66757476841985
At time: 40.95554184913635 and batch: 350, loss is 3.966750211715698 and perplexity is 52.8126218173635
At time: 41.42533779144287 and batch: 400, loss is 3.898419451713562 and perplexity is 49.32442782369455
At time: 41.89603877067566 and batch: 450, loss is 3.939987072944641 and perplexity is 51.41793661371765
At time: 42.366029500961304 and batch: 500, loss is 3.8237162256240844 and perplexity is 45.77399914347404
At time: 42.83499717712402 and batch: 550, loss is 3.89705020904541 and perplexity is 49.25693692878713
At time: 43.30481481552124 and batch: 600, loss is 3.90498281955719 and perplexity is 49.64922690914621
At time: 43.77379107475281 and batch: 650, loss is 3.7627832794189455 and perplexity is 43.06812995639915
At time: 44.24396800994873 and batch: 700, loss is 3.772994389533997 and perplexity is 43.510156323070085
At time: 44.71289920806885 and batch: 750, loss is 3.865960369110107 and perplexity is 47.74910718760267
At time: 45.18302822113037 and batch: 800, loss is 3.8462247037887574 and perplexity is 46.81598496977447
At time: 45.6531035900116 and batch: 850, loss is 3.901833634376526 and perplexity is 49.49311823610791
At time: 46.12656211853027 and batch: 900, loss is 3.8654262399673462 and perplexity is 47.7236098079659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38116914932042 and perplexity of 79.93143057456355
finished 5 epochs...
Completing Train Step...
At time: 47.45567798614502 and batch: 50, loss is 3.941326766014099 and perplexity is 51.48686702953549
At time: 47.92860174179077 and batch: 100, loss is 3.829195885658264 and perplexity is 46.02551357462848
At time: 48.40218782424927 and batch: 150, loss is 3.836999740600586 and perplexity is 46.38609513936965
At time: 48.87246012687683 and batch: 200, loss is 3.7289750623703 and perplexity is 41.636411562114354
At time: 49.34234118461609 and batch: 250, loss is 3.887208547592163 and perplexity is 48.7745444964497
At time: 49.811870098114014 and batch: 300, loss is 3.8523500680923464 and perplexity is 47.10362999887068
At time: 50.2810583114624 and batch: 350, loss is 3.8534221982955934 and perplexity is 47.15415830489651
At time: 50.75092124938965 and batch: 400, loss is 3.7865910387039183 and perplexity is 44.105788780572816
At time: 51.21868920326233 and batch: 450, loss is 3.828514938354492 and perplexity is 45.99418329360094
At time: 51.68803811073303 and batch: 500, loss is 3.713817558288574 and perplexity is 41.01006639051144
At time: 52.15735912322998 and batch: 550, loss is 3.786218662261963 and perplexity is 44.08936788144481
At time: 52.62731647491455 and batch: 600, loss is 3.8003528356552123 and perplexity is 44.71695944783984
At time: 53.0989887714386 and batch: 650, loss is 3.6633176469802855 and perplexity is 38.99048519402441
At time: 53.5722770690918 and batch: 700, loss is 3.6718167209625245 and perplexity is 39.32328043471188
At time: 54.04746222496033 and batch: 750, loss is 3.761263732910156 and perplexity is 43.00273562732911
At time: 54.52072525024414 and batch: 800, loss is 3.7460464572906496 and perplexity is 42.353304961707664
At time: 54.99349331855774 and batch: 850, loss is 3.797960352897644 and perplexity is 44.610102770739154
At time: 55.464210748672485 and batch: 900, loss is 3.7642037868499756 and perplexity is 43.12935202795187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387339552787886 and perplexity of 80.42616453531134
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 56.782119035720825 and batch: 50, loss is 3.871089806556702 and perplexity is 47.99466248792571
At time: 57.252421379089355 and batch: 100, loss is 3.7585379838943482 and perplexity is 42.88568056684755
At time: 57.72346234321594 and batch: 150, loss is 3.7687431001663207 and perplexity is 43.325574691075005
At time: 58.19464945793152 and batch: 200, loss is 3.6412185716629026 and perplexity is 38.13828265131773
At time: 58.6655752658844 and batch: 250, loss is 3.7886436319351198 and perplexity is 44.196412999592184
At time: 59.13539719581604 and batch: 300, loss is 3.750807876586914 and perplexity is 42.555447666400404
At time: 59.617788553237915 and batch: 350, loss is 3.7321633434295656 and perplexity is 41.769371989449624
At time: 60.08865761756897 and batch: 400, loss is 3.6604305362701415 and perplexity is 38.87807769115993
At time: 60.56185603141785 and batch: 450, loss is 3.6936623859405517 and perplexity is 40.19177553211775
At time: 61.03407096862793 and batch: 500, loss is 3.56699990272522 and perplexity is 35.410199592618355
At time: 61.5064218044281 and batch: 550, loss is 3.6200648593902587 and perplexity is 37.33998959247276
At time: 61.98033165931702 and batch: 600, loss is 3.6339547348022463 and perplexity is 37.86225610755665
At time: 62.45870041847229 and batch: 650, loss is 3.4853484630584717 and perplexity is 32.633796801428595
At time: 62.928362131118774 and batch: 700, loss is 3.473992986679077 and perplexity is 32.2653205637654
At time: 63.39908409118652 and batch: 750, loss is 3.5549095392227175 and perplexity is 34.98465508545489
At time: 63.868653535842896 and batch: 800, loss is 3.521467351913452 and perplexity is 33.83403849838905
At time: 64.33634305000305 and batch: 850, loss is 3.5560344743728636 and perplexity is 35.02403269815395
At time: 64.8038101196289 and batch: 900, loss is 3.5117310428619386 and perplexity is 33.50621831208979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335337599662886 and perplexity of 76.35073052016074
finished 7 epochs...
Completing Train Step...
At time: 66.0961377620697 and batch: 50, loss is 3.7750509071350096 and perplexity is 43.59972779645863
At time: 66.58137202262878 and batch: 100, loss is 3.6593979692459104 and perplexity is 38.837954188838204
At time: 67.0497555732727 and batch: 150, loss is 3.6745754718780517 and perplexity is 39.43191334728843
At time: 67.52305150032043 and batch: 200, loss is 3.552183384895325 and perplexity is 34.88941140008078
At time: 67.99553942680359 and batch: 250, loss is 3.701367049217224 and perplexity is 40.50263562749752
At time: 68.46861410140991 and batch: 300, loss is 3.6681760644912718 and perplexity is 39.180378166200704
At time: 68.94080352783203 and batch: 350, loss is 3.6524313783645628 and perplexity is 38.56832633530417
At time: 69.41262459754944 and batch: 400, loss is 3.587278714179993 and perplexity is 36.13560669168982
At time: 69.8852698802948 and batch: 450, loss is 3.6264362382888793 and perplexity is 37.57865632494995
At time: 70.35467863082886 and batch: 500, loss is 3.5023606967926026 and perplexity is 33.19371984694498
At time: 70.82416653633118 and batch: 550, loss is 3.557753119468689 and perplexity is 35.08427833579234
At time: 71.29376530647278 and batch: 600, loss is 3.580082964897156 and perplexity is 35.87651721515508
At time: 71.77893376350403 and batch: 650, loss is 3.4354204607009886 and perplexity is 31.044462798370805
At time: 72.24764609336853 and batch: 700, loss is 3.4298290967941285 and perplexity is 30.871366283339924
At time: 72.71509623527527 and batch: 750, loss is 3.517228136062622 and perplexity is 33.69091229187352
At time: 73.18447279930115 and batch: 800, loss is 3.4885236740112306 and perplexity is 32.73758067110704
At time: 73.65245461463928 and batch: 850, loss is 3.531429262161255 and perplexity is 34.172774581124614
At time: 74.12107801437378 and batch: 900, loss is 3.494164271354675 and perplexity is 32.922761957224274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3412977244755995 and perplexity of 76.80714920866198
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.41796660423279 and batch: 50, loss is 3.748548974990845 and perplexity is 42.459427588528534
At time: 75.90374040603638 and batch: 100, loss is 3.639646153450012 and perplexity is 38.078360444799344
At time: 76.37561774253845 and batch: 150, loss is 3.6640032720565796 and perplexity is 39.01722721486277
At time: 76.84778308868408 and batch: 200, loss is 3.5347458505630494 and perplexity is 34.28629976304589
At time: 77.31532692909241 and batch: 250, loss is 3.68468367099762 and perplexity is 39.83252027534885
At time: 77.78638768196106 and batch: 300, loss is 3.6427140045166015 and perplexity is 38.19535855813461
At time: 78.25696325302124 and batch: 350, loss is 3.6266154289245605 and perplexity is 37.58539067161277
At time: 78.72720956802368 and batch: 400, loss is 3.5599452543258665 and perplexity is 35.161272164866865
At time: 79.19620156288147 and batch: 450, loss is 3.5926709890365602 and perplexity is 36.33098611143218
At time: 79.66705131530762 and batch: 500, loss is 3.464294409751892 and perplexity is 31.953905452935533
At time: 80.13545989990234 and batch: 550, loss is 3.511129961013794 and perplexity is 33.486084384137925
At time: 80.60343027114868 and batch: 600, loss is 3.5370231676101684 and perplexity is 34.36446951284732
At time: 81.07275795936584 and batch: 650, loss is 3.3858777046203614 and perplexity is 29.543912167889573
At time: 81.54264283180237 and batch: 700, loss is 3.370142822265625 and perplexity is 29.08268041550142
At time: 82.01416158676147 and batch: 750, loss is 3.4591343641281127 and perplexity is 31.789446515658025
At time: 82.486013174057 and batch: 800, loss is 3.4260504770278932 and perplexity is 30.754935241342537
At time: 82.95674467086792 and batch: 850, loss is 3.467865376472473 and perplexity is 32.06821576358627
At time: 83.44593000411987 and batch: 900, loss is 3.428049907684326 and perplexity is 30.81648911754796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328295563998288 and perplexity of 75.81495464333996
finished 9 epochs...
Completing Train Step...
At time: 84.76980209350586 and batch: 50, loss is 3.7213589906692506 and perplexity is 41.32051015713333
At time: 85.23897695541382 and batch: 100, loss is 3.604667654037476 and perplexity is 36.76946164423345
At time: 85.70952677726746 and batch: 150, loss is 3.6296465587615967 and perplexity is 37.6994897078513
At time: 86.17772912979126 and batch: 200, loss is 3.5011785888671874 and perplexity is 33.154504470610036
At time: 86.64590907096863 and batch: 250, loss is 3.652167935371399 and perplexity is 38.55816711821911
At time: 87.12038588523865 and batch: 300, loss is 3.6141098737716675 and perplexity is 37.118291251818825
At time: 87.5901255607605 and batch: 350, loss is 3.5980993938446044 and perplexity is 36.528741674129584
At time: 88.06006407737732 and batch: 400, loss is 3.5341564989089966 and perplexity is 34.266099028821955
At time: 88.53093791007996 and batch: 450, loss is 3.569375081062317 and perplexity is 35.49440509355695
At time: 89.0033175945282 and batch: 500, loss is 3.4429814434051513 and perplexity is 31.280079067433054
At time: 89.47658443450928 and batch: 550, loss is 3.491315245628357 and perplexity is 32.8290976507917
At time: 89.94952487945557 and batch: 600, loss is 3.521236524581909 and perplexity is 33.82622957885785
At time: 90.42217540740967 and batch: 650, loss is 3.372045488357544 and perplexity is 29.138067720454508
At time: 90.89505553245544 and batch: 700, loss is 3.3595719003677367 and perplexity is 28.77686887492506
At time: 91.36692214012146 and batch: 750, loss is 3.451570887565613 and perplexity is 31.549914770263346
At time: 91.83727502822876 and batch: 800, loss is 3.4216958236694337 and perplexity is 30.621299339497142
At time: 92.30701994895935 and batch: 850, loss is 3.466777811050415 and perplexity is 32.0333584392208
At time: 92.77645516395569 and batch: 900, loss is 3.429603247642517 and perplexity is 30.864394798737315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329403811938142 and perplexity of 75.89902298631301
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.09008884429932 and batch: 50, loss is 3.716159462928772 and perplexity is 41.10622060333818
At time: 94.56009006500244 and batch: 100, loss is 3.6025377702713013 and perplexity is 36.691230306188956
At time: 95.03055620193481 and batch: 150, loss is 3.6291380643844606 and perplexity is 37.68032460240109
At time: 95.49903273582458 and batch: 200, loss is 3.498607177734375 and perplexity is 33.06936012618464
At time: 95.98623180389404 and batch: 250, loss is 3.6524855852127076 and perplexity is 38.57041705937829
At time: 96.45811295509338 and batch: 300, loss is 3.6117402696609497 and perplexity is 37.03043972406371
At time: 96.93083715438843 and batch: 350, loss is 3.595729651451111 and perplexity is 36.442280452276364
At time: 97.40216422080994 and batch: 400, loss is 3.5303272294998167 and perplexity is 34.13513581079064
At time: 97.8766143321991 and batch: 450, loss is 3.5588919019699095 and perplexity is 35.12425445576186
At time: 98.34863257408142 and batch: 500, loss is 3.433060436248779 and perplexity is 30.97128349350104
At time: 98.82031011581421 and batch: 550, loss is 3.478597002029419 and perplexity is 32.41421308364261
At time: 99.29102087020874 and batch: 600, loss is 3.5103639364242554 and perplexity is 33.460443042295346
At time: 99.76110649108887 and batch: 650, loss is 3.3567746353149412 and perplexity is 28.696484825453577
At time: 100.23412775993347 and batch: 700, loss is 3.3417589807510377 and perplexity is 28.26880728687841
At time: 100.70533180236816 and batch: 750, loss is 3.431377592086792 and perplexity is 30.919207480082118
At time: 101.1756899356842 and batch: 800, loss is 3.401421933174133 and perplexity is 30.00673730176667
At time: 101.6462230682373 and batch: 850, loss is 3.4437346410751344 and perplexity is 31.30364802503127
At time: 102.11611485481262 and batch: 900, loss is 3.410658621788025 and perplexity is 30.28518417439965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323358823175299 and perplexity of 75.44159820202012
finished 11 epochs...
Completing Train Step...
At time: 103.42325758934021 and batch: 50, loss is 3.7048815155029295 and perplexity is 40.64523120178966
At time: 103.91138958930969 and batch: 100, loss is 3.589330439567566 and perplexity is 36.20982314318838
At time: 104.38497614860535 and batch: 150, loss is 3.615719599723816 and perplexity is 37.17808964514655
At time: 104.8551926612854 and batch: 200, loss is 3.486222491264343 and perplexity is 32.66233212882165
At time: 105.32534337043762 and batch: 250, loss is 3.6406319999694823 and perplexity is 38.11591837404603
At time: 105.7939805984497 and batch: 300, loss is 3.600714211463928 and perplexity is 36.624382658843196
At time: 106.26300501823425 and batch: 350, loss is 3.5850627851486205 and perplexity is 36.05562140534409
At time: 106.73337960243225 and batch: 400, loss is 3.521309051513672 and perplexity is 33.828682980470084
At time: 107.20275640487671 and batch: 450, loss is 3.551233730316162 and perplexity is 34.85629423820149
At time: 107.67513751983643 and batch: 500, loss is 3.4263504266738893 and perplexity is 30.764161556926684
At time: 108.16067457199097 and batch: 550, loss is 3.472926335334778 and perplexity is 32.23092306454251
At time: 108.6299660205841 and batch: 600, loss is 3.506273617744446 and perplexity is 33.32385869426914
At time: 109.09939122200012 and batch: 650, loss is 3.3530282878875735 and perplexity is 28.58917895140004
At time: 109.5679714679718 and batch: 700, loss is 3.3391905212402344 and perplexity is 28.196293164601414
At time: 110.03895950317383 and batch: 750, loss is 3.4305719757080078 and perplexity is 30.894308490980514
At time: 110.51070523262024 and batch: 800, loss is 3.401785144805908 and perplexity is 30.017638077310686
At time: 110.98408627510071 and batch: 850, loss is 3.4459375238418577 and perplexity is 31.372682301092865
At time: 111.45477986335754 and batch: 900, loss is 3.4141504335403443 and perplexity is 30.39111918133552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322836679955051 and perplexity of 75.40221716515966
finished 12 epochs...
Completing Train Step...
At time: 112.76493525505066 and batch: 50, loss is 3.699217286109924 and perplexity is 40.415658079735884
At time: 113.24966549873352 and batch: 100, loss is 3.583086814880371 and perplexity is 35.98444691194343
At time: 113.7240343093872 and batch: 150, loss is 3.6091327095031738 and perplexity is 36.934006407292145
At time: 114.19349265098572 and batch: 200, loss is 3.4798597717285156 and perplexity is 32.45517062427279
At time: 114.66177916526794 and batch: 250, loss is 3.634186224937439 and perplexity is 37.871021860895254
At time: 115.13172483444214 and batch: 300, loss is 3.5946100997924804 and perplexity is 36.40150426653399
At time: 115.60080409049988 and batch: 350, loss is 3.579161219596863 and perplexity is 35.84346343994236
At time: 116.07102465629578 and batch: 400, loss is 3.5159074306488036 and perplexity is 33.64644589158439
At time: 116.54121398925781 and batch: 450, loss is 3.5462862062454223 and perplexity is 34.684267786850704
At time: 117.01131844520569 and batch: 500, loss is 3.4218727350234985 and perplexity is 30.626717074241782
At time: 117.48307800292969 and batch: 550, loss is 3.4689365911483763 and perplexity is 32.10258611266676
At time: 117.95756697654724 and batch: 600, loss is 3.5031943321228027 and perplexity is 33.22140284170821
At time: 118.42942380905151 and batch: 650, loss is 3.3503297710418702 and perplexity is 28.512134569928545
At time: 118.90348672866821 and batch: 700, loss is 3.3371305084228515 and perplexity is 28.138268225858226
At time: 119.37542915344238 and batch: 750, loss is 3.429326114654541 and perplexity is 30.855842441909513
At time: 119.86418843269348 and batch: 800, loss is 3.4013709831237793 and perplexity is 30.00520849593689
At time: 120.33201289176941 and batch: 850, loss is 3.446198205947876 and perplexity is 31.380861664047018
At time: 120.80413126945496 and batch: 900, loss is 3.4148949003219604 and perplexity is 30.41375278395946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323130568412886 and perplexity of 75.42438026305943
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 122.12028980255127 and batch: 50, loss is 3.697638916969299 and perplexity is 40.35191756848555
At time: 122.59056186676025 and batch: 100, loss is 3.5830111646652223 and perplexity is 35.98172478375862
At time: 123.0621190071106 and batch: 150, loss is 3.6091065263748168 and perplexity is 36.933039372121705
At time: 123.5315465927124 and batch: 200, loss is 3.478960075378418 and perplexity is 32.425983957258
At time: 124.00208592414856 and batch: 250, loss is 3.634606351852417 and perplexity is 37.88693583918797
At time: 124.47539901733398 and batch: 300, loss is 3.594331078529358 and perplexity is 36.39134888968292
At time: 124.94688105583191 and batch: 350, loss is 3.579705834388733 and perplexity is 35.86298963696884
At time: 125.42003202438354 and batch: 400, loss is 3.5160348510742185 and perplexity is 33.65073340918645
At time: 125.89201664924622 and batch: 450, loss is 3.543545365333557 and perplexity is 34.58933388549306
At time: 126.36487603187561 and batch: 500, loss is 3.418882040977478 and perplexity is 30.535258763919586
At time: 126.83663249015808 and batch: 550, loss is 3.4647613906860353 and perplexity is 31.968830802209006
At time: 127.30602812767029 and batch: 600, loss is 3.499619040489197 and perplexity is 33.102838715034224
At time: 127.77524876594543 and batch: 650, loss is 3.345096549987793 and perplexity is 28.363314012079453
At time: 128.24408388137817 and batch: 700, loss is 3.331223125457764 and perplexity is 27.972534707125373
At time: 128.71457958221436 and batch: 750, loss is 3.4221861124038697 and perplexity is 30.636316298619214
At time: 129.18392968177795 and batch: 800, loss is 3.3935402822494507 and perplexity is 29.771164244757006
At time: 129.65240716934204 and batch: 850, loss is 3.4361858129501344 and perplexity is 31.068231842482493
At time: 130.12113285064697 and batch: 900, loss is 3.4058173418045046 and perplexity is 30.138919458359094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322283183058647 and perplexity of 75.36049381991299
finished 14 epochs...
Completing Train Step...
At time: 131.43581175804138 and batch: 50, loss is 3.6940711307525635 and perplexity is 40.208207069776165
At time: 131.90825629234314 and batch: 100, loss is 3.5795989656448364 and perplexity is 35.859157209100985
At time: 132.39348316192627 and batch: 150, loss is 3.606108374595642 and perplexity is 36.82247434260943
At time: 132.8660089969635 and batch: 200, loss is 3.4758233976364137 and perplexity is 32.324433443994806
At time: 133.33778619766235 and batch: 250, loss is 3.631466999053955 and perplexity is 37.76818188394889
At time: 133.81159090995789 and batch: 300, loss is 3.5912976360321043 and perplexity is 36.28112508873076
At time: 134.28079390525818 and batch: 350, loss is 3.576158061027527 and perplexity is 35.73598130928031
At time: 134.7504312992096 and batch: 400, loss is 3.5135229206085206 and perplexity is 33.56631118251499
At time: 135.2203471660614 and batch: 450, loss is 3.5412657833099366 and perplexity is 34.51057446514417
At time: 135.69149708747864 and batch: 500, loss is 3.4171206331253052 and perplexity is 30.481521060264978
At time: 136.1614112854004 and batch: 550, loss is 3.463247380256653 and perplexity is 31.920466280393057
At time: 136.63052082061768 and batch: 600, loss is 3.498591270446777 and perplexity is 33.06883408654638
At time: 137.10212755203247 and batch: 650, loss is 3.344315686225891 and perplexity is 28.341174772989685
At time: 137.57406616210938 and batch: 700, loss is 3.330677280426025 and perplexity is 27.957270204435833
At time: 138.04424619674683 and batch: 750, loss is 3.4219213056564333 and perplexity is 30.628204669401217
At time: 138.5149576663971 and batch: 800, loss is 3.39367769241333 and perplexity is 29.77525538639054
At time: 138.9875328540802 and batch: 850, loss is 3.437393403053284 and perplexity is 31.105772193894232
At time: 139.45667123794556 and batch: 900, loss is 3.4078839683532713 and perplexity is 30.20126975466049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321929513591609 and perplexity of 75.33384582679652
finished 15 epochs...
Completing Train Step...
At time: 140.75033569335938 and batch: 50, loss is 3.692146053314209 and perplexity is 40.13087761398621
At time: 141.23852133750916 and batch: 100, loss is 3.577578277587891 and perplexity is 35.78677019881077
At time: 141.70663261413574 and batch: 150, loss is 3.604148745536804 and perplexity is 36.750386607548506
At time: 142.17663502693176 and batch: 200, loss is 3.4738430976867676 and perplexity is 32.26048470980925
At time: 142.64726281166077 and batch: 250, loss is 3.6294480419158934 and perplexity is 37.692006466869145
At time: 143.117431640625 and batch: 300, loss is 3.5893534278869628 and perplexity is 36.210655555735954
At time: 143.5866734981537 and batch: 350, loss is 3.5741916847229005 and perplexity is 35.66577996616025
At time: 144.0559859275818 and batch: 400, loss is 3.5119517993927003 and perplexity is 33.513615845100134
At time: 144.5421540737152 and batch: 450, loss is 3.5397814226150515 and perplexity is 34.45938632508046
At time: 145.01095747947693 and batch: 500, loss is 3.415924925804138 and perplexity is 30.44509586364985
At time: 145.4818992614746 and batch: 550, loss is 3.462180495262146 and perplexity is 31.88642897407509
At time: 145.95319938659668 and batch: 600, loss is 3.497786226272583 and perplexity is 33.04222292733417
At time: 146.42598605155945 and batch: 650, loss is 3.343700032234192 and perplexity is 28.32373178558461
At time: 146.8974165916443 and batch: 700, loss is 3.3302681159973146 and perplexity is 27.945833423865782
At time: 147.36892175674438 and batch: 750, loss is 3.4217766332626343 and perplexity is 30.62377393422419
At time: 147.84106278419495 and batch: 800, loss is 3.3937628936767577 and perplexity is 29.7777923838445
At time: 148.31369185447693 and batch: 850, loss is 3.4379704856872557 and perplexity is 31.12372797533013
At time: 148.784099817276 and batch: 900, loss is 3.4088066244125366 and perplexity is 30.22914799823459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321826673533819 and perplexity of 75.32609888809276
finished 16 epochs...
Completing Train Step...
At time: 150.08472514152527 and batch: 50, loss is 3.6906249475479127 and perplexity is 40.06988070777494
At time: 150.57054138183594 and batch: 100, loss is 3.5759450006484985 and perplexity is 35.72836819861252
At time: 151.04043221473694 and batch: 150, loss is 3.6025171995162966 and perplexity is 36.690475547642514
At time: 151.5114939212799 and batch: 200, loss is 3.472223358154297 and perplexity is 32.20827342314556
At time: 151.98319482803345 and batch: 250, loss is 3.6277983713150026 and perplexity is 37.62987833148108
At time: 152.45271921157837 and batch: 300, loss is 3.587766752243042 and perplexity is 36.153246547297094
At time: 152.92134594917297 and batch: 350, loss is 3.5726428747177126 and perplexity is 35.6105832049877
At time: 153.3927640914917 and batch: 400, loss is 3.5106323099136354 and perplexity is 33.4694241432416
At time: 153.8635094165802 and batch: 450, loss is 3.538538875579834 and perplexity is 34.41659550707431
At time: 154.33430767059326 and batch: 500, loss is 3.4148723316192626 and perplexity is 30.413066392760463
At time: 154.80794405937195 and batch: 550, loss is 3.4612292575836183 and perplexity is 31.85611182309546
At time: 155.2810664176941 and batch: 600, loss is 3.4970583152770995 and perplexity is 33.01817988161043
At time: 155.75098371505737 and batch: 650, loss is 3.343105683326721 and perplexity is 28.306902608239344
At time: 156.23568630218506 and batch: 700, loss is 3.3298381757736206 and perplexity is 27.93382096849612
At time: 156.7049858570099 and batch: 750, loss is 3.421571011543274 and perplexity is 30.61747766852121
At time: 157.17607021331787 and batch: 800, loss is 3.3937454414367676 and perplexity is 29.777272699200285
At time: 157.6456069946289 and batch: 850, loss is 3.438237400054932 and perplexity is 31.132036454279266
At time: 158.11848187446594 and batch: 900, loss is 3.4092612838745118 and perplexity is 30.242895091285593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321834616465111 and perplexity of 75.32669720049691
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 159.433495759964 and batch: 50, loss is 3.6900657033920288 and perplexity is 40.047478126002495
At time: 159.90491366386414 and batch: 100, loss is 3.575798735618591 and perplexity is 35.72314276992747
At time: 160.37568593025208 and batch: 150, loss is 3.6026110887527465 and perplexity is 36.693920550098454
At time: 160.84854459762573 and batch: 200, loss is 3.471989870071411 and perplexity is 32.200754053006754
At time: 161.3232080936432 and batch: 250, loss is 3.6278637981414796 and perplexity is 37.63234041554332
At time: 161.7942533493042 and batch: 300, loss is 3.587522087097168 and perplexity is 36.144402189953894
At time: 162.26671838760376 and batch: 350, loss is 3.5725076580047608 and perplexity is 35.60576838450986
At time: 162.74151277542114 and batch: 400, loss is 3.5107356786727903 and perplexity is 33.47288401490316
At time: 163.21042227745056 and batch: 450, loss is 3.5381949281692506 and perplexity is 34.40476004367374
At time: 163.67942571640015 and batch: 500, loss is 3.414009790420532 and perplexity is 30.386845180080755
At time: 164.1497254371643 and batch: 550, loss is 3.4593416452407837 and perplexity is 31.796036550474263
At time: 164.62002873420715 and batch: 600, loss is 3.4955769681930544 and perplexity is 32.96930470660891
At time: 165.0910198688507 and batch: 650, loss is 3.3409130668640135 and perplexity is 28.244904421533043
At time: 165.56143236160278 and batch: 700, loss is 3.328097453117371 and perplexity is 27.885238230264815
At time: 166.03133273124695 and batch: 750, loss is 3.419602484703064 and perplexity is 30.557265625906158
At time: 166.50809812545776 and batch: 800, loss is 3.391069860458374 and perplexity is 29.697707683564452
At time: 166.9790117740631 and batch: 850, loss is 3.4349117612838747 and perplexity is 31.02867451432657
At time: 167.4531307220459 and batch: 900, loss is 3.4056881618499757 and perplexity is 30.135026365574102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321294915186216 and perplexity of 75.28605425419443
finished 18 epochs...
Completing Train Step...
At time: 168.78173971176147 and batch: 50, loss is 3.6893497610092165 and perplexity is 40.01881670027632
At time: 169.25325918197632 and batch: 100, loss is 3.5751037073135374 and perplexity is 35.698322800846555
At time: 169.8978021144867 and batch: 150, loss is 3.6019584465026857 and perplexity is 36.66998036026653
At time: 170.37046003341675 and batch: 200, loss is 3.4714047574996947 and perplexity is 32.18191849798898
At time: 170.84127569198608 and batch: 250, loss is 3.6272362422943116 and perplexity is 37.60873142902651
At time: 171.31274247169495 and batch: 300, loss is 3.5868027591705323 and perplexity is 36.11841186097252
At time: 171.78327298164368 and batch: 350, loss is 3.57182966709137 and perplexity is 35.581636178713396
At time: 172.25483298301697 and batch: 400, loss is 3.5101580095291136 and perplexity is 33.45355334656075
At time: 172.726633310318 and batch: 450, loss is 3.5376295042037964 and perplexity is 34.38531226645713
At time: 173.19766497612 and batch: 500, loss is 3.4136012029647826 and perplexity is 30.37443203242127
At time: 173.6693046092987 and batch: 550, loss is 3.4591435861587523 and perplexity is 31.78973968025959
At time: 174.1405279636383 and batch: 600, loss is 3.4954005098342895 and perplexity is 32.9634875104723
At time: 174.61223602294922 and batch: 650, loss is 3.3408998918533324 and perplexity is 28.24453229706698
At time: 175.08823823928833 and batch: 700, loss is 3.328050441741943 and perplexity is 27.883927337675157
At time: 175.56403470039368 and batch: 750, loss is 3.4194999074935915 and perplexity is 30.554131307626694
At time: 176.03986144065857 and batch: 800, loss is 3.391151533126831 and perplexity is 29.700133273648877
At time: 176.51382613182068 and batch: 850, loss is 3.4351893711090087 and perplexity is 31.037289574990048
At time: 176.9869773387909 and batch: 900, loss is 3.406103129386902 and perplexity is 30.14753401819565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321029036012415 and perplexity of 75.26603992110566
finished 19 epochs...
Completing Train Step...
At time: 178.30857014656067 and batch: 50, loss is 3.688783597946167 and perplexity is 39.99616593705146
At time: 178.7794895172119 and batch: 100, loss is 3.5745512533187864 and perplexity is 35.678606566466996
At time: 179.25041484832764 and batch: 150, loss is 3.601424765586853 and perplexity is 36.650415512721544
At time: 179.72257947921753 and batch: 200, loss is 3.4709150695800783 and perplexity is 32.166163259160335
At time: 180.19419622421265 and batch: 250, loss is 3.626717848777771 and perplexity is 37.58924035894705
At time: 180.68004155158997 and batch: 300, loss is 3.586256546974182 and perplexity is 36.098688930844105
At time: 181.1515724658966 and batch: 350, loss is 3.571290469169617 and perplexity is 35.56245580590653
At time: 181.6243314743042 and batch: 400, loss is 3.509705662727356 and perplexity is 33.43842416077438
At time: 182.1028573513031 and batch: 450, loss is 3.537202000617981 and perplexity is 34.3706155638342
At time: 182.57840609550476 and batch: 500, loss is 3.413282594680786 and perplexity is 30.364756028263447
At time: 183.05318522453308 and batch: 550, loss is 3.4589383888244627 and perplexity is 31.7832171796422
At time: 183.5272388458252 and batch: 600, loss is 3.495228662490845 and perplexity is 32.95782330941584
At time: 184.00419425964355 and batch: 650, loss is 3.340830674171448 and perplexity is 28.24257734367492
At time: 184.47249102592468 and batch: 700, loss is 3.3279928159713745 and perplexity is 27.88232055117244
At time: 184.93492794036865 and batch: 750, loss is 3.41944721698761 and perplexity is 30.552521437401083
At time: 185.3982150554657 and batch: 800, loss is 3.3911962032318117 and perplexity is 29.70146001135269
At time: 185.87233114242554 and batch: 850, loss is 3.435394959449768 and perplexity is 31.043671135820215
At time: 186.34635376930237 and batch: 900, loss is 3.406417474746704 and perplexity is 30.157012245264045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320891915935359 and perplexity of 75.25572014345254
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f189bae2b70>
ELAPSED
589.4530107975006


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.86398936205909}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5386602506977849, 'wordvec_dim': 300, 'rnn_dropout': 0.729059618910752, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.25572014345254}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.94588761585722, 'wordvec_dim': 300, 'rnn_dropout': 0.2935830384905056, 'num_layers': 1, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7801847457885742 and batch: 50, loss is 7.620665063858032 and perplexity is 2039.9183547541854
At time: 1.2567863464355469 and batch: 100, loss is 6.843039331436157 and perplexity is 937.333677497555
At time: 1.7309658527374268 and batch: 150, loss is 6.655434093475342 and perplexity is 776.9951384098878
At time: 2.204860210418701 and batch: 200, loss is 6.461131677627564 and perplexity is 639.7846770452434
At time: 2.6806480884552 and batch: 250, loss is 6.486223754882812 and perplexity is 656.0413069562543
At time: 3.1522693634033203 and batch: 300, loss is 6.365318489074707 and perplexity is 581.3299460730908
At time: 3.6253929138183594 and batch: 350, loss is 6.3703125476837155 and perplexity is 584.2404033429954
At time: 4.100097179412842 and batch: 400, loss is 6.246991586685181 and perplexity is 516.4567697910644
At time: 4.575369119644165 and batch: 450, loss is 6.244138660430909 and perplexity is 514.9854564854509
At time: 5.053036451339722 and batch: 500, loss is 6.213735675811767 and perplexity is 499.56397891976764
At time: 5.545871257781982 and batch: 550, loss is 6.239036998748779 and perplexity is 512.3648652839348
At time: 6.0211570262908936 and batch: 600, loss is 6.180566825866699 and perplexity is 483.2658062923366
At time: 6.4971396923065186 and batch: 650, loss is 6.108653774261475 and perplexity is 449.732865494919
At time: 6.970290660858154 and batch: 700, loss is 6.2136054039001465 and perplexity is 499.49890400406576
At time: 7.4461541175842285 and batch: 750, loss is 6.1512352466583256 and perplexity is 469.29672610795717
At time: 7.921684265136719 and batch: 800, loss is 6.165939664840698 and perplexity is 476.2484466654318
At time: 8.395529747009277 and batch: 850, loss is 6.203168144226074 and perplexity is 494.3126166312876
At time: 8.869827032089233 and batch: 900, loss is 6.069071226119995 and perplexity is 432.27900562098847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.9842897180008565 and perplexity of 397.14033941265706
finished 1 epochs...
Completing Train Step...
At time: 10.176576614379883 and batch: 50, loss is 5.738711585998535 and perplexity is 310.66388931319557
At time: 10.65275502204895 and batch: 100, loss is 5.429816370010376 and perplexity is 228.10735422679178
At time: 11.115503311157227 and batch: 150, loss is 5.29575478553772 and perplexity is 199.48813991950553
At time: 11.577728748321533 and batch: 200, loss is 5.1109022045135495 and perplexity is 165.8198908615365
At time: 12.039316654205322 and batch: 250, loss is 5.14319540977478 and perplexity is 171.26214764413334
At time: 12.506564378738403 and batch: 300, loss is 5.0474091815948485 and perplexity is 155.61876179977767
At time: 12.97690486907959 and batch: 350, loss is 5.015473966598511 and perplexity is 150.72755966730227
At time: 13.440045595169067 and batch: 400, loss is 4.86214322090149 and perplexity is 129.30102601556231
At time: 13.9015052318573 and batch: 450, loss is 4.8609562015533445 and perplexity is 129.1476342534525
At time: 14.363774299621582 and batch: 500, loss is 4.767326974868775 and perplexity is 117.60446175857729
At time: 14.82560110092163 and batch: 550, loss is 4.824298171997071 and perplexity is 124.49906078703505
At time: 15.287879228591919 and batch: 600, loss is 4.7517921924591064 and perplexity is 115.79161954694209
At time: 15.757656574249268 and batch: 650, loss is 4.611223411560059 and perplexity is 100.60715833644738
At time: 16.219703435897827 and batch: 700, loss is 4.663112926483154 and perplexity is 105.96543186914433
At time: 16.698631048202515 and batch: 750, loss is 4.682380847930908 and perplexity is 108.02696241217187
At time: 17.180283784866333 and batch: 800, loss is 4.630246906280518 and perplexity is 102.5393786017886
At time: 17.643821001052856 and batch: 850, loss is 4.6727344417572025 and perplexity is 106.98990045726579
At time: 18.10772442817688 and batch: 900, loss is 4.60380690574646 and perplexity is 99.86376486027372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.678279928965111 and perplexity of 107.5848597264324
finished 2 epochs...
Completing Train Step...
At time: 19.422462940216064 and batch: 50, loss is 4.6560095596313475 and perplexity is 105.21538760654111
At time: 19.895871877670288 and batch: 100, loss is 4.5234682083129885 and perplexity is 92.15465592238532
At time: 20.369612455368042 and batch: 150, loss is 4.521859931945801 and perplexity is 92.00656488471749
At time: 20.841927528381348 and batch: 200, loss is 4.40835934638977 and perplexity is 82.13459847554176
At time: 21.31399416923523 and batch: 250, loss is 4.5337074089050295 and perplexity is 93.10309326369493
At time: 21.783687114715576 and batch: 300, loss is 4.483622350692749 and perplexity is 88.55486918524734
At time: 22.25430679321289 and batch: 350, loss is 4.48147479057312 and perplexity is 88.36489634185041
At time: 22.724618434906006 and batch: 400, loss is 4.385318603515625 and perplexity is 80.26379146575553
At time: 23.198654651641846 and batch: 450, loss is 4.414924306869507 and perplexity is 82.67558269558985
At time: 23.668396472930908 and batch: 500, loss is 4.303027091026306 and perplexity is 73.9232276906434
At time: 24.138612747192383 and batch: 550, loss is 4.379894866943359 and perplexity is 79.82964022980426
At time: 24.60969853401184 and batch: 600, loss is 4.359890956878662 and perplexity is 78.24860148611727
At time: 25.078837156295776 and batch: 650, loss is 4.2054525566101075 and perplexity is 67.05093514498742
At time: 25.54911780357361 and batch: 700, loss is 4.231127190589905 and perplexity is 68.79473326236014
At time: 26.019819736480713 and batch: 750, loss is 4.311779952049255 and perplexity is 74.57310743344783
At time: 26.492591619491577 and batch: 800, loss is 4.270742378234863 and perplexity is 71.57475143839935
At time: 26.965660572052002 and batch: 850, loss is 4.333359169960022 and perplexity is 76.19982529397777
At time: 27.438698291778564 and batch: 900, loss is 4.2800909042358395 and perplexity is 72.24700726781231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.487882013190283 and perplexity of 88.93288758430216
finished 3 epochs...
Completing Train Step...
At time: 28.748733282089233 and batch: 50, loss is 4.3599270057678225 and perplexity is 78.25142231212274
At time: 29.214644193649292 and batch: 100, loss is 4.241207180023193 and perplexity is 69.49169020476663
At time: 29.696008682250977 and batch: 150, loss is 4.237844295501709 and perplexity is 69.25839017579906
At time: 30.172080516815186 and batch: 200, loss is 4.124794015884399 and perplexity is 61.85506677681392
At time: 30.652048587799072 and batch: 250, loss is 4.268819646835327 and perplexity is 71.43726463384901
At time: 31.13097858428955 and batch: 300, loss is 4.232913703918457 and perplexity is 68.9177458193363
At time: 31.616756439208984 and batch: 350, loss is 4.229927768707276 and perplexity is 68.71226881859104
At time: 32.09310340881348 and batch: 400, loss is 4.150217528343201 and perplexity is 63.44780049202301
At time: 32.59078288078308 and batch: 450, loss is 4.186256422996521 and perplexity is 65.77609161557329
At time: 33.089239835739136 and batch: 500, loss is 4.072954978942871 and perplexity is 58.73025309210318
At time: 33.59176015853882 and batch: 550, loss is 4.148915853500366 and perplexity is 63.36526581459958
At time: 34.080763816833496 and batch: 600, loss is 4.148205804824829 and perplexity is 63.32028936115811
At time: 34.573448181152344 and batch: 650, loss is 3.991501202583313 and perplexity is 54.13609764266791
At time: 35.0626015663147 and batch: 700, loss is 4.007114691734314 and perplexity is 54.987984170339494
At time: 35.545966148376465 and batch: 750, loss is 4.102525224685669 and perplexity is 60.492852930943776
At time: 36.021514892578125 and batch: 800, loss is 4.066410803794861 and perplexity is 58.34716688816633
At time: 36.51276731491089 and batch: 850, loss is 4.1356022787094116 and perplexity is 62.5272385567288
At time: 36.98532032966614 and batch: 900, loss is 4.0888543748855595 and perplexity is 59.671491371925235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404163099315069 and perplexity of 81.79066352964338
finished 4 epochs...
Completing Train Step...
At time: 38.30091381072998 and batch: 50, loss is 4.17245731830597 and perplexity is 64.87467412201059
At time: 38.79992628097534 and batch: 100, loss is 4.0577618646621705 and perplexity is 57.844701820281884
At time: 39.27243947982788 and batch: 150, loss is 4.05722668170929 and perplexity is 57.813752604438285
At time: 39.752309799194336 and batch: 200, loss is 3.94679527759552 and perplexity is 51.769194810973836
At time: 40.2345929145813 and batch: 250, loss is 4.093054780960083 and perplexity is 59.92266300890099
At time: 40.70707321166992 and batch: 300, loss is 4.063938455581665 and perplexity is 58.2030905511914
At time: 41.19092321395874 and batch: 350, loss is 4.059599494934082 and perplexity is 57.95109672272299
At time: 41.67895007133484 and batch: 400, loss is 3.990942153930664 and perplexity is 54.10584138836822
At time: 42.16725301742554 and batch: 450, loss is 4.028617181777954 and perplexity is 56.18316639975077
At time: 42.64983630180359 and batch: 500, loss is 3.91648136138916 and perplexity is 50.223415460808624
At time: 43.13781237602234 and batch: 550, loss is 3.9899336862564088 and perplexity is 54.0513049001044
At time: 43.611265659332275 and batch: 600, loss is 3.9929035902023315 and perplexity is 54.21207069512674
At time: 44.09557867050171 and batch: 650, loss is 3.8432562589645385 and perplexity is 46.67722036093826
At time: 44.574846029281616 and batch: 700, loss is 3.851855125427246 and perplexity is 47.08032217119812
At time: 45.05507469177246 and batch: 750, loss is 3.952321720123291 and perplexity is 52.05608630530729
At time: 45.52556085586548 and batch: 800, loss is 3.920151152610779 and perplexity is 50.40806351265941
At time: 45.990126848220825 and batch: 850, loss is 3.990170946121216 and perplexity is 54.06413062685258
At time: 46.455148696899414 and batch: 900, loss is 3.9483873510360716 and perplexity is 51.85168081553657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379766594873716 and perplexity of 79.81940097335244
finished 5 epochs...
Completing Train Step...
At time: 47.690051555633545 and batch: 50, loss is 4.033560066223145 and perplexity is 56.46156076781224
At time: 48.192524671554565 and batch: 100, loss is 3.9230159854888917 and perplexity is 50.55268124423889
At time: 48.675591707229614 and batch: 150, loss is 3.9233744955062866 and perplexity is 50.570808136013106
At time: 49.14241290092468 and batch: 200, loss is 3.8147991466522218 and perplexity is 45.36764322475123
At time: 49.608245849609375 and batch: 250, loss is 3.9635880422592162 and perplexity is 52.6458831246759
At time: 50.09660220146179 and batch: 300, loss is 3.934712839126587 and perplexity is 51.14746029823865
At time: 50.594560384750366 and batch: 350, loss is 3.9295370101928713 and perplexity is 50.88341371278561
At time: 51.09166622161865 and batch: 400, loss is 3.8663390922546386 and perplexity is 47.76719430441401
At time: 51.57419538497925 and batch: 450, loss is 3.9058851766586304 and perplexity is 49.694048461110235
At time: 52.05805063247681 and batch: 500, loss is 3.80085786819458 and perplexity is 44.73954867108884
At time: 52.56050252914429 and batch: 550, loss is 3.8647783184051514 and perplexity is 47.69269866722423
At time: 53.04474711418152 and batch: 600, loss is 3.8754022455215456 and perplexity is 48.20208346424055
At time: 53.52003765106201 and batch: 650, loss is 3.7275363874435423 and perplexity is 41.5765533693248
At time: 54.00403094291687 and batch: 700, loss is 3.7362668991088865 and perplexity is 41.94112709540113
At time: 54.47180461883545 and batch: 750, loss is 3.838906283378601 and perplexity is 46.474616572193256
At time: 54.94029974937439 and batch: 800, loss is 3.8062085628509523 and perplexity is 44.97957792404414
At time: 55.411662101745605 and batch: 850, loss is 3.8746195888519286 and perplexity is 48.164372541405456
At time: 55.885701417922974 and batch: 900, loss is 3.8350508117675783 and perplexity is 46.295779978599526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37379476468857 and perplexity of 79.3441535261701
finished 6 epochs...
Completing Train Step...
At time: 57.2017457485199 and batch: 50, loss is 3.9244895792007446 and perplexity is 50.627230271436424
At time: 57.67128562927246 and batch: 100, loss is 3.812694067955017 and perplexity is 45.27224121505559
At time: 58.14123249053955 and batch: 150, loss is 3.8191412782669065 and perplexity is 45.56506380557154
At time: 58.61018204689026 and batch: 200, loss is 3.709295558929443 and perplexity is 40.82503756199401
At time: 59.08023524284363 and batch: 250, loss is 3.856757879257202 and perplexity is 47.31171216162627
At time: 59.54970860481262 and batch: 300, loss is 3.829935793876648 and perplexity is 46.05958083214537
At time: 60.01899695396423 and batch: 350, loss is 3.8228078413009645 and perplexity is 45.73243764001262
At time: 60.48902606964111 and batch: 400, loss is 3.769569821357727 and perplexity is 43.3614076717033
At time: 60.96253180503845 and batch: 450, loss is 3.807017011642456 and perplexity is 45.0159563125099
At time: 61.43696713447571 and batch: 500, loss is 3.702920651435852 and perplexity is 40.565609517589806
At time: 61.91000819206238 and batch: 550, loss is 3.766259341239929 and perplexity is 43.21809793662165
At time: 62.38617658615112 and batch: 600, loss is 3.7770357370376586 and perplexity is 43.686351778432964
At time: 62.85948085784912 and batch: 650, loss is 3.635560040473938 and perplexity is 37.923085413781166
At time: 63.33057260513306 and batch: 700, loss is 3.642168192863464 and perplexity is 38.17451677469937
At time: 63.80055284500122 and batch: 750, loss is 3.7451875972747803 and perplexity is 42.31694501782217
At time: 64.26990795135498 and batch: 800, loss is 3.7133726692199707 and perplexity is 40.99182551815482
At time: 64.74111104011536 and batch: 850, loss is 3.780038695335388 and perplexity is 43.81773724480825
At time: 65.21060514450073 and batch: 900, loss is 3.7436126899719238 and perplexity is 42.25035220459732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.383634802413313 and perplexity of 80.12875692276995
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 66.55109119415283 and batch: 50, loss is 3.858954014778137 and perplexity is 47.41572926928682
At time: 67.02142143249512 and batch: 100, loss is 3.746354131698608 and perplexity is 42.36633799459937
At time: 67.49307537078857 and batch: 150, loss is 3.7536223125457764 and perplexity is 42.67538594869572
At time: 67.96787524223328 and batch: 200, loss is 3.6241050338745118 and perplexity is 37.4911548270057
At time: 68.44120955467224 and batch: 250, loss is 3.771378922462463 and perplexity is 43.43992384265662
At time: 68.91399717330933 and batch: 300, loss is 3.7322804069519044 and perplexity is 41.774261945472745
At time: 69.38737058639526 and batch: 350, loss is 3.7142244720458986 and perplexity is 41.026757346374005
At time: 69.86041188240051 and batch: 400, loss is 3.651960024833679 and perplexity is 38.550151302275246
At time: 70.33331370353699 and batch: 450, loss is 3.67817617893219 and perplexity is 39.57415204209396
At time: 70.80456471443176 and batch: 500, loss is 3.564649453163147 and perplexity is 35.32706744172955
At time: 71.27399945259094 and batch: 550, loss is 3.612405524253845 and perplexity is 37.05508259014817
At time: 71.74256014823914 and batch: 600, loss is 3.6141416120529173 and perplexity is 37.11946934128126
At time: 72.21437358856201 and batch: 650, loss is 3.465596623420715 and perplexity is 31.995543370237154
At time: 72.68665051460266 and batch: 700, loss is 3.452474584579468 and perplexity is 31.57843922081776
At time: 73.15888667106628 and batch: 750, loss is 3.546216449737549 and perplexity is 34.68184841783609
At time: 73.62700605392456 and batch: 800, loss is 3.489579133987427 and perplexity is 32.772152118402985
At time: 74.09958219528198 and batch: 850, loss is 3.550944991111755 and perplexity is 34.84623131238509
At time: 74.56829881668091 and batch: 900, loss is 3.4976444339752195 and perplexity is 33.03753812677748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334086378959761 and perplexity of 76.25525864608343
finished 8 epochs...
Completing Train Step...
At time: 75.885981798172 and batch: 50, loss is 3.770300054550171 and perplexity is 43.39308317470055
At time: 76.37521934509277 and batch: 100, loss is 3.655310716629028 and perplexity is 38.67953762374301
At time: 76.8512213230133 and batch: 150, loss is 3.6647806119918824 and perplexity is 39.04756865502039
At time: 77.32362723350525 and batch: 200, loss is 3.538789014816284 and perplexity is 34.42520552480241
At time: 77.79658126831055 and batch: 250, loss is 3.689151334762573 and perplexity is 40.010876704461204
At time: 78.28259229660034 and batch: 300, loss is 3.653670783042908 and perplexity is 38.61615773450714
At time: 78.75067830085754 and batch: 350, loss is 3.6397671699523926 and perplexity is 38.08296883363668
At time: 79.22155618667603 and batch: 400, loss is 3.584418087005615 and perplexity is 36.032383904570814
At time: 79.69119548797607 and batch: 450, loss is 3.613769884109497 and perplexity is 37.105673561579444
At time: 80.16173267364502 and batch: 500, loss is 3.501998600959778 and perplexity is 33.18170271512043
At time: 80.63113069534302 and batch: 550, loss is 3.55575430393219 and perplexity is 35.01422137396444
At time: 81.10013222694397 and batch: 600, loss is 3.5650720739364625 and perplexity is 35.342000549589265
At time: 81.56803393363953 and batch: 650, loss is 3.4195507526397706 and perplexity is 30.55568487639479
At time: 82.03708100318909 and batch: 700, loss is 3.4112868022918703 and perplexity is 30.304214713334343
At time: 82.51058626174927 and batch: 750, loss is 3.5126318979263305 and perplexity is 33.53641615844404
At time: 82.98256468772888 and batch: 800, loss is 3.4612051248550415 and perplexity is 31.85534305747156
At time: 83.45718550682068 and batch: 850, loss is 3.529623894691467 and perplexity is 34.111135822590086
At time: 83.92990398406982 and batch: 900, loss is 3.483395023345947 and perplexity is 32.570110870351414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.339114672517123 and perplexity of 76.63965809857291
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 85.2290735244751 and batch: 50, loss is 3.7517095804214478 and perplexity is 42.59383738221734
At time: 85.71497082710266 and batch: 100, loss is 3.6438912391662597 and perplexity is 38.24034993519331
At time: 86.18562889099121 and batch: 150, loss is 3.651760630607605 and perplexity is 38.54246539097993
At time: 86.6562614440918 and batch: 200, loss is 3.5245256423950195 and perplexity is 33.93767120496746
At time: 87.12664222717285 and batch: 250, loss is 3.6703402280807493 and perplexity is 39.26526273296383
At time: 87.5975615978241 and batch: 300, loss is 3.6296959495544434 and perplexity is 37.701351761521664
At time: 88.06643033027649 and batch: 350, loss is 3.6158855295181276 and perplexity is 37.18425910974911
At time: 88.53659772872925 and batch: 400, loss is 3.5560560274124144 and perplexity is 35.02478758065091
At time: 89.00444316864014 and batch: 450, loss is 3.5801955461502075 and perplexity is 35.88055646578598
At time: 89.47307538986206 and batch: 500, loss is 3.464421248435974 and perplexity is 31.957958701303376
At time: 89.94476890563965 and batch: 550, loss is 3.510048942565918 and perplexity is 33.449904868057
At time: 90.44601583480835 and batch: 600, loss is 3.5246165418624877 and perplexity is 33.94075626141997
At time: 90.91906952857971 and batch: 650, loss is 3.3723852920532225 and perplexity is 29.147970625977504
At time: 91.39075660705566 and batch: 700, loss is 3.360563416481018 and perplexity is 28.80541575411097
At time: 91.86632800102234 and batch: 750, loss is 3.4563752222061157 and perplexity is 31.7018558142557
At time: 92.33785128593445 and batch: 800, loss is 3.402496485710144 and perplexity is 30.038998447475432
At time: 92.8070228099823 and batch: 850, loss is 3.465616478919983 and perplexity is 31.996178664032133
At time: 93.27654600143433 and batch: 900, loss is 3.4231942319869995 and perplexity is 30.667216942189725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328960679981806 and perplexity of 75.86539715466367
finished 10 epochs...
Completing Train Step...
At time: 94.59435415267944 and batch: 50, loss is 3.7294726896286012 and perplexity is 41.65713613157317
At time: 95.06661987304688 and batch: 100, loss is 3.611501417160034 and perplexity is 37.02159596714439
At time: 95.536696434021 and batch: 150, loss is 3.620662670135498 and perplexity is 37.362318513047605
At time: 96.00482845306396 and batch: 200, loss is 3.494521279335022 and perplexity is 32.9345177443081
At time: 96.47464942932129 and batch: 250, loss is 3.6405701303482054 and perplexity is 38.113560229561116
At time: 96.94830799102783 and batch: 300, loss is 3.601410140991211 and perplexity is 36.64987951913391
At time: 97.42628812789917 and batch: 350, loss is 3.5889413213729857 and perplexity is 36.19573598314373
At time: 97.89917421340942 and batch: 400, loss is 3.532080264091492 and perplexity is 34.19502836618105
At time: 98.37502861022949 and batch: 450, loss is 3.559713044166565 and perplexity is 35.15310830815813
At time: 98.84847950935364 and batch: 500, loss is 3.4448001050949095 and perplexity is 31.3370187101649
At time: 99.32063412666321 and batch: 550, loss is 3.4931836032867434 and perplexity is 32.89049148176335
At time: 99.79036903381348 and batch: 600, loss is 3.5100199127197267 and perplexity is 33.448933836558076
At time: 100.26153993606567 and batch: 650, loss is 3.3598290491104126 and perplexity is 28.78426976209803
At time: 100.73159670829773 and batch: 700, loss is 3.3510818481445312 and perplexity is 28.533585959024748
At time: 101.19970774650574 and batch: 750, loss is 3.44916983127594 and perplexity is 31.47425251997639
At time: 101.66958785057068 and batch: 800, loss is 3.398219442367554 and perplexity is 29.91079471105318
At time: 102.13713693618774 and batch: 850, loss is 3.4646008682250975 and perplexity is 31.963699498671083
At time: 102.62031555175781 and batch: 900, loss is 3.4248866510391234 and perplexity is 30.719162668988226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329558908122859 and perplexity of 75.91079554811809
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 103.93373656272888 and batch: 50, loss is 3.726812491416931 and perplexity is 41.546467158498615
At time: 104.40901160240173 and batch: 100, loss is 3.615015230178833 and perplexity is 37.151911751597666
At time: 104.88352346420288 and batch: 150, loss is 3.6250705480575562 and perplexity is 37.52737054931325
At time: 105.35884642601013 and batch: 200, loss is 3.4973462343215944 and perplexity is 33.0276878131045
At time: 105.83321142196655 and batch: 250, loss is 3.642294921875 and perplexity is 38.17935490003503
At time: 106.30932831764221 and batch: 300, loss is 3.597615113258362 and perplexity is 36.511055796506184
At time: 106.7801992893219 and batch: 350, loss is 3.584087791442871 and perplexity is 36.020484533314985
At time: 107.24918818473816 and batch: 400, loss is 3.5268123865127565 and perplexity is 34.01536677605306
At time: 107.72185897827148 and batch: 450, loss is 3.5497148275375365 and perplexity is 34.80339110356052
At time: 108.19439339637756 and batch: 500, loss is 3.4372914743423464 and perplexity is 31.10260178421243
At time: 108.66432476043701 and batch: 550, loss is 3.4781040287017824 and perplexity is 32.39823767920372
At time: 109.13450813293457 and batch: 600, loss is 3.4986669301986693 and perplexity is 33.0713361609807
At time: 109.60700106620789 and batch: 650, loss is 3.345003619194031 and perplexity is 28.360678309265463
At time: 110.07731771469116 and batch: 700, loss is 3.3333205366134644 and perplexity is 28.03126618397005
At time: 110.54612302780151 and batch: 750, loss is 3.430685210227966 and perplexity is 30.897806991243698
At time: 111.01803708076477 and batch: 800, loss is 3.381939058303833 and perplexity is 29.427778002886054
At time: 111.49187922477722 and batch: 850, loss is 3.4435836458206177 and perplexity is 31.298921679567275
At time: 111.96408319473267 and batch: 900, loss is 3.4053368902206422 and perplexity is 30.12444264475192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322166547383348 and perplexity of 75.35170461040308
finished 12 epochs...
Completing Train Step...
At time: 113.28234648704529 and batch: 50, loss is 3.7144414806365966 and perplexity is 41.03566147126751
At time: 113.76934814453125 and batch: 100, loss is 3.600300626754761 and perplexity is 36.60923850610368
At time: 114.23985290527344 and batch: 150, loss is 3.612227816581726 and perplexity is 37.0484982027463
At time: 114.72299766540527 and batch: 200, loss is 3.4853266429901124 and perplexity is 32.633084737520235
At time: 115.19228672981262 and batch: 250, loss is 3.630400218963623 and perplexity is 37.72791302229589
At time: 115.66106796264648 and batch: 300, loss is 3.587303681373596 and perplexity is 36.136508907640916
At time: 116.13146471977234 and batch: 350, loss is 3.574984188079834 and perplexity is 35.69405641962329
At time: 116.60171365737915 and batch: 400, loss is 3.5184109687805174 and perplexity is 33.73078658288712
At time: 117.07378005981445 and batch: 450, loss is 3.542554020881653 and perplexity is 34.55506093220851
At time: 117.54339981079102 and batch: 500, loss is 3.430466742515564 and perplexity is 30.89105755532584
At time: 118.01389813423157 and batch: 550, loss is 3.4730950212478637 and perplexity is 32.236360425819484
At time: 118.48292946815491 and batch: 600, loss is 3.4948506021499632 and perplexity is 32.945365618528584
At time: 118.95324039459229 and batch: 650, loss is 3.3420915937423707 and perplexity is 28.278211423313792
At time: 119.42767572402954 and batch: 700, loss is 3.3315773391723633 and perplexity is 27.98244471757794
At time: 119.9037663936615 and batch: 750, loss is 3.4295509576797487 and perplexity is 30.862780942877023
At time: 120.37879061698914 and batch: 800, loss is 3.38185338973999 and perplexity is 29.42525707539112
At time: 120.85289263725281 and batch: 850, loss is 3.4453264713287353 and perplexity is 31.353517800583802
At time: 121.32558703422546 and batch: 900, loss is 3.408748106956482 and perplexity is 30.227379117150754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321611378290882 and perplexity of 75.30988328296714
finished 13 epochs...
Completing Train Step...
At time: 122.63906455039978 and batch: 50, loss is 3.709138035774231 and perplexity is 40.81860717974593
At time: 123.12435674667358 and batch: 100, loss is 3.5942723321914674 and perplexity is 36.389211093999194
At time: 123.59380221366882 and batch: 150, loss is 3.606367530822754 and perplexity is 36.832018352774405
At time: 124.06362414360046 and batch: 200, loss is 3.4793587827682497 and perplexity is 32.43891501436703
At time: 124.53568053245544 and batch: 250, loss is 3.624335994720459 and perplexity is 37.49981481586077
At time: 125.0072135925293 and batch: 300, loss is 3.5815529108047484 and perplexity is 35.929292533743144
At time: 125.47697138786316 and batch: 350, loss is 3.569609909057617 and perplexity is 35.50274115228112
At time: 125.94769716262817 and batch: 400, loss is 3.5133867502212524 and perplexity is 33.561740756107525
At time: 126.42094302177429 and batch: 450, loss is 3.53824405670166 and perplexity is 34.40645034056314
At time: 126.91028428077698 and batch: 500, loss is 3.4262780284881593 and perplexity is 30.761934368067646
At time: 127.38191556930542 and batch: 550, loss is 3.469655818939209 and perplexity is 32.12568348989872
At time: 127.855459690094 and batch: 600, loss is 3.492044711112976 and perplexity is 32.85305408105134
At time: 128.32914853096008 and batch: 650, loss is 3.3397508096694946 and perplexity is 28.212095647970465
At time: 128.7982759475708 and batch: 700, loss is 3.329859981536865 and perplexity is 27.934430093423867
At time: 129.267972946167 and batch: 750, loss is 3.428370990753174 and perplexity is 30.826385359117722
At time: 129.736891746521 and batch: 800, loss is 3.3812427520751953 and perplexity is 29.4072943900289
At time: 130.20735263824463 and batch: 850, loss is 3.445402674674988 and perplexity is 31.355907134593465
At time: 130.6781451702118 and batch: 900, loss is 3.4094367742538454 and perplexity is 30.248202894137854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321844231592466 and perplexity of 75.32742147976572
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 131.9961428642273 and batch: 50, loss is 3.7078781938552856 and perplexity is 40.76721456742912
At time: 132.46422052383423 and batch: 100, loss is 3.5956750822067263 and perplexity is 36.44029187882641
At time: 132.9353301525116 and batch: 150, loss is 3.6099746227264404 and perplexity is 36.965114729090885
At time: 133.4080913066864 and batch: 200, loss is 3.48224675655365 and perplexity is 32.53273315751769
At time: 133.87992596626282 and batch: 250, loss is 3.6263341188430784 and perplexity is 37.57481900932771
At time: 134.35208129882812 and batch: 300, loss is 3.5817713069915773 and perplexity is 35.9371402111482
At time: 134.8259792327881 and batch: 350, loss is 3.5683776569366454 and perplexity is 35.45901976761279
At time: 135.301029920578 and batch: 400, loss is 3.5137165355682374 and perplexity is 33.57281075168904
At time: 135.77376914024353 and batch: 450, loss is 3.535116276741028 and perplexity is 34.29900265862141
At time: 136.24509167671204 and batch: 500, loss is 3.4240834665298463 and perplexity is 30.69449941929045
At time: 136.7136263847351 and batch: 550, loss is 3.4652769899368288 and perplexity is 31.985318157488127
At time: 137.183185338974 and batch: 600, loss is 3.488325023651123 and perplexity is 32.731077984819535
At time: 137.65185976028442 and batch: 650, loss is 3.3350950717926025 and perplexity is 28.08105281292682
At time: 138.12126350402832 and batch: 700, loss is 3.3224140977859498 and perplexity is 27.72720601489454
At time: 138.590989112854 and batch: 750, loss is 3.4216268491744994 and perplexity is 30.61918732367942
At time: 139.07600378990173 and batch: 800, loss is 3.37541380405426 and perplexity is 29.236379410784135
At time: 139.54677891731262 and batch: 850, loss is 3.436729612350464 and perplexity is 31.085131322874926
At time: 140.01420140266418 and batch: 900, loss is 3.399339880943298 and perplexity is 29.94432670103821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31892959385702 and perplexity of 75.10818898146357
finished 15 epochs...
Completing Train Step...
At time: 141.32925033569336 and batch: 50, loss is 3.7039386081695556 and perplexity is 40.606924577859246
At time: 141.80263805389404 and batch: 100, loss is 3.59168261051178 and perplexity is 36.2950950848573
At time: 142.2780442237854 and batch: 150, loss is 3.605880308151245 and perplexity is 36.81407732938708
At time: 142.75126147270203 and batch: 200, loss is 3.4787987232208253 and perplexity is 32.42075237685917
At time: 143.2244918346405 and batch: 250, loss is 3.622826490402222 and perplexity is 37.44325138557473
At time: 143.7065773010254 and batch: 300, loss is 3.5786148166656493 and perplexity is 35.823883816122986
At time: 144.175954580307 and batch: 350, loss is 3.5654166603088377 and perplexity is 35.354181019842635
At time: 144.64435744285583 and batch: 400, loss is 3.5110337018966673 and perplexity is 33.48286119835208
At time: 145.1052107810974 and batch: 450, loss is 3.5331747484207154 and perplexity is 34.23247477738224
At time: 145.57546591758728 and batch: 500, loss is 3.422162137031555 and perplexity is 30.635581790334697
At time: 146.04766368865967 and batch: 550, loss is 3.4638314294815062 and perplexity is 31.93911484929208
At time: 146.5164279937744 and batch: 600, loss is 3.4873449563980103 and perplexity is 32.69901504161434
At time: 146.98509311676025 and batch: 650, loss is 3.334294958114624 and perplexity is 28.058593764573207
At time: 147.4534397125244 and batch: 700, loss is 3.322169818878174 and perplexity is 27.72043367049767
At time: 147.92139983177185 and batch: 750, loss is 3.421316595077515 and perplexity is 30.609689068878332
At time: 148.38981366157532 and batch: 800, loss is 3.375849003791809 and perplexity is 29.24910584450004
At time: 148.86376881599426 and batch: 850, loss is 3.437959146499634 and perplexity is 31.123375059540024
At time: 149.3374011516571 and batch: 900, loss is 3.4012087965011597 and perplexity is 30.000342447124616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318101020708476 and perplexity of 75.04598212786097
finished 16 epochs...
Completing Train Step...
At time: 150.6371669769287 and batch: 50, loss is 3.7018615865707396 and perplexity is 40.522670647355675
At time: 151.12623620033264 and batch: 100, loss is 3.5894608211517336 and perplexity is 36.214544545077466
At time: 151.60139226913452 and batch: 150, loss is 3.6036015224456786 and perplexity is 36.7302814488943
At time: 152.07302260398865 and batch: 200, loss is 3.4767687845230104 and perplexity is 32.35500698912664
At time: 152.55028367042542 and batch: 250, loss is 3.6207136964797972 and perplexity is 37.3642250242166
At time: 153.02143573760986 and batch: 300, loss is 3.5766996574401855 and perplexity is 35.75534103066485
At time: 153.4922535419464 and batch: 350, loss is 3.563677735328674 and perplexity is 35.29275617339229
At time: 153.9611349105835 and batch: 400, loss is 3.5094402599334718 and perplexity is 33.42955068715346
At time: 154.43255829811096 and batch: 450, loss is 3.531866178512573 and perplexity is 34.18770848730534
At time: 154.90335297584534 and batch: 500, loss is 3.4209089851379395 and perplexity is 30.59721479785769
At time: 155.37409710884094 and batch: 550, loss is 3.462864804267883 and perplexity is 31.908256612155718
At time: 155.8417670726776 and batch: 600, loss is 3.486655898094177 and perplexity is 32.67649127475823
At time: 156.3108196258545 and batch: 650, loss is 3.333768620491028 and perplexity is 28.043829356882195
At time: 156.7851927280426 and batch: 700, loss is 3.32194299697876 and perplexity is 27.714146782108887
At time: 157.26179790496826 and batch: 750, loss is 3.421165871620178 and perplexity is 30.60507581838513
At time: 157.7464301586151 and batch: 800, loss is 3.376039505004883 and perplexity is 29.25467836541388
At time: 158.22046852111816 and batch: 850, loss is 3.438526225090027 and perplexity is 31.141029464441477
At time: 158.69648432731628 and batch: 900, loss is 3.4021133518218996 and perplexity is 30.02749169365571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31784559276006 and perplexity of 75.02681573452962
finished 17 epochs...
Completing Train Step...
At time: 160.0123188495636 and batch: 50, loss is 3.7002966594696045 and perplexity is 40.45930521592153
At time: 160.4949414730072 and batch: 100, loss is 3.58776629447937 and perplexity is 36.15322999765799
At time: 160.96499586105347 and batch: 150, loss is 3.601920132637024 and perplexity is 36.66857541847973
At time: 161.43504357337952 and batch: 200, loss is 3.4751837778091432 and perplexity is 32.30376470623064
At time: 161.90738558769226 and batch: 250, loss is 3.6190817546844483 and perplexity is 37.3032985115318
At time: 162.38117837905884 and batch: 300, loss is 3.5751804447174074 and perplexity is 35.701062302570584
At time: 162.85154700279236 and batch: 350, loss is 3.5622854232788086 and perplexity is 35.24365183591203
At time: 163.3366997241974 and batch: 400, loss is 3.5081575441360475 and perplexity is 33.38669756445072
At time: 163.81102657318115 and batch: 450, loss is 3.5307637548446653 and perplexity is 34.15003991549496
At time: 164.28574991226196 and batch: 500, loss is 3.4198570013046266 and perplexity is 30.565043947120255
At time: 164.7591700553894 and batch: 550, loss is 3.4620206499099733 and perplexity is 31.88133248394218
At time: 165.2335741519928 and batch: 600, loss is 3.4860226249694826 and perplexity is 32.65580468184761
At time: 165.70753049850464 and batch: 650, loss is 3.333274745941162 and perplexity is 28.029982642833332
At time: 166.17878890037537 and batch: 700, loss is 3.3216469192504885 and perplexity is 27.70594245510784
At time: 166.64847040176392 and batch: 750, loss is 3.4209697961807253 and perplexity is 30.599075502971022
At time: 167.1177361011505 and batch: 800, loss is 3.376056046485901 and perplexity is 29.25516228512312
At time: 167.58703994750977 and batch: 850, loss is 3.4387694025039672 and perplexity is 31.148603180295073
At time: 168.05601358413696 and batch: 900, loss is 3.402578926086426 and perplexity is 30.04147497589214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317796262976241 and perplexity of 75.02311476921349
finished 18 epochs...
Completing Train Step...
At time: 169.37260031700134 and batch: 50, loss is 3.698947019577026 and perplexity is 40.40473655587956
At time: 169.84227561950684 and batch: 100, loss is 3.586316089630127 and perplexity is 36.10083840665129
At time: 170.3147246837616 and batch: 150, loss is 3.6005001211166383 and perplexity is 36.616542571313964
At time: 170.7927761077881 and batch: 200, loss is 3.4738059043884277 and perplexity is 32.2592848582902
At time: 171.26717972755432 and batch: 250, loss is 3.617678065299988 and perplexity is 37.25097300037968
At time: 171.7429277896881 and batch: 300, loss is 3.5738493394851685 and perplexity is 35.65357204601973
At time: 172.21805882453918 and batch: 350, loss is 3.5610474491119386 and perplexity is 35.2000481011105
At time: 172.692284822464 and batch: 400, loss is 3.5070195055007933 and perplexity is 33.348723824608776
At time: 173.16541624069214 and batch: 450, loss is 3.5297655200958253 and perplexity is 34.115967168106636
At time: 173.64002561569214 and batch: 500, loss is 3.4188977336883544 and perplexity is 30.535737948666753
At time: 174.1107542514801 and batch: 550, loss is 3.461226058006287 and perplexity is 31.856009897165258
At time: 174.5809977054596 and batch: 600, loss is 3.485404915809631 and perplexity is 32.635639121040334
At time: 175.05085945129395 and batch: 650, loss is 3.332775707244873 and perplexity is 28.015998086545846
At time: 175.5382993221283 and batch: 700, loss is 3.321293902397156 and perplexity is 27.696163516650323
At time: 176.00750517845154 and batch: 750, loss is 3.4207199144363405 and perplexity is 30.591430307844906
At time: 176.47792077064514 and batch: 800, loss is 3.3759655141830445 and perplexity is 29.252513867796473
At time: 176.94894647598267 and batch: 850, loss is 3.4388374280929566 and perplexity is 31.150722154444022
At time: 177.41991901397705 and batch: 900, loss is 3.402812705039978 and perplexity is 30.048498861461486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3178384859267975 and perplexity of 75.02628253335467
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 178.74183011054993 and batch: 50, loss is 3.6984532356262205 and perplexity is 40.38479027041483
At time: 179.21600103378296 and batch: 100, loss is 3.586461796760559 and perplexity is 36.106098939460985
At time: 179.6937141418457 and batch: 150, loss is 3.601474084854126 and perplexity is 36.652223128934665
At time: 180.16855549812317 and batch: 200, loss is 3.4750212383270265 and perplexity is 32.29851449573967
At time: 180.6353394985199 and batch: 250, loss is 3.618471884727478 and perplexity is 37.280555286383525
At time: 181.10261869430542 and batch: 300, loss is 3.574481964111328 and perplexity is 35.67613450973921
At time: 181.57237195968628 and batch: 350, loss is 3.560228590965271 and perplexity is 35.17123605305988
At time: 182.04314184188843 and batch: 400, loss is 3.5066994714736937 and perplexity is 33.33805280586018
At time: 182.51343727111816 and batch: 450, loss is 3.529156608581543 and perplexity is 34.09519988623728
At time: 182.9833791255951 and batch: 500, loss is 3.417954797744751 and perplexity is 30.50695827461746
At time: 183.4547152519226 and batch: 550, loss is 3.4598561906814576 and perplexity is 31.812401265946622
At time: 183.9257984161377 and batch: 600, loss is 3.4835859298706056 and perplexity is 32.57632931057693
At time: 184.39790654182434 and batch: 650, loss is 3.3312664079666137 and perplexity is 27.9737454548083
At time: 184.86851692199707 and batch: 700, loss is 3.319010543823242 and perplexity is 27.632995389407636
At time: 185.34454727172852 and batch: 750, loss is 3.4186027956008913 and perplexity is 30.526733124514983
At time: 185.81827044487 and batch: 800, loss is 3.373624873161316 and perplexity is 29.18412430278275
At time: 186.29311084747314 and batch: 850, loss is 3.435937066078186 and perplexity is 31.060504678088538
At time: 186.76731753349304 and batch: 900, loss is 3.3992730522155763 and perplexity is 29.942325626647666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316889514661815 and perplexity of 74.9551185187551
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f189bae2b70>
ELAPSED
784.1553599834442


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.86398936205909}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5386602506977849, 'wordvec_dim': 300, 'rnn_dropout': 0.729059618910752, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.25572014345254}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.94588761585722, 'wordvec_dim': 300, 'rnn_dropout': 0.2935830384905056, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.9551185187551}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5759509205725062, 'wordvec_dim': 300, 'rnn_dropout': 0.2773236962555252, 'num_layers': 1, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7358903884887695 and batch: 50, loss is 6.861594581604004 and perplexity is 954.8885017820567
At time: 1.2252223491668701 and batch: 100, loss is 5.98054533958435 and perplexity is 395.65607625306535
At time: 1.6988985538482666 and batch: 150, loss is 5.806976957321167 and perplexity is 332.61210372126345
At time: 2.189100980758667 and batch: 200, loss is 5.626512804031372 and perplexity is 277.6920605923558
At time: 2.6643807888031006 and batch: 250, loss is 5.654387998580932 and perplexity is 285.5416773296648
At time: 3.1396026611328125 and batch: 300, loss is 5.553993263244629 and perplexity is 258.2668268901646
At time: 3.6173453330993652 and batch: 350, loss is 5.526278734207153 and perplexity is 251.2073601706377
At time: 4.09267520904541 and batch: 400, loss is 5.382126359939575 and perplexity is 217.48423386831197
At time: 4.5680012702941895 and batch: 450, loss is 5.378824644088745 and perplexity is 216.76734685590083
At time: 5.04816460609436 and batch: 500, loss is 5.319901151657104 and perplexity is 204.36367998340248
At time: 5.527742385864258 and batch: 550, loss is 5.374755115509033 and perplexity is 215.88699845916116
At time: 6.00872540473938 and batch: 600, loss is 5.285959405899048 and perplexity is 197.54361707329736
At time: 6.49149489402771 and batch: 650, loss is 5.178835420608521 and perplexity is 177.47600569742752
At time: 6.9701337814331055 and batch: 700, loss is 5.265943698883056 and perplexity is 193.62895001856418
At time: 7.44591212272644 and batch: 750, loss is 5.251238012313843 and perplexity is 190.8023379438983
At time: 7.923032522201538 and batch: 800, loss is 5.222886714935303 and perplexity is 185.46880762352876
At time: 8.397509098052979 and batch: 850, loss is 5.247117891311645 and perplexity is 190.0178264749395
At time: 8.872117280960083 and batch: 900, loss is 5.164510879516602 and perplexity is 174.95186509991478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.023102224689641 and perplexity of 151.8817450088633
finished 1 epochs...
Completing Train Step...
At time: 10.192899227142334 and batch: 50, loss is 4.9550735378265385 and perplexity is 141.89304049510136
At time: 10.657684564590454 and batch: 100, loss is 4.82110595703125 and perplexity is 124.10226668494768
At time: 11.132093667984009 and batch: 150, loss is 4.801010618209839 and perplexity is 121.63328023255522
At time: 11.602195739746094 and batch: 200, loss is 4.68718409538269 and perplexity is 108.54709079677306
At time: 12.073580503463745 and batch: 250, loss is 4.78479585647583 and perplexity is 119.67692931983316
At time: 12.546391725540161 and batch: 300, loss is 4.716280746459961 and perplexity is 111.75184533921805
At time: 13.020071983337402 and batch: 350, loss is 4.715425958633423 and perplexity is 111.65636203702498
At time: 13.495086908340454 and batch: 400, loss is 4.589982509613037 and perplexity is 98.49270747131442
At time: 13.984762191772461 and batch: 450, loss is 4.611413593292236 and perplexity is 100.62629379963937
At time: 14.457781076431274 and batch: 500, loss is 4.509597215652466 and perplexity is 90.88520399955515
At time: 14.925064086914062 and batch: 550, loss is 4.580599536895752 and perplexity is 97.5728752132568
At time: 15.39756464958191 and batch: 600, loss is 4.537670297622681 and perplexity is 93.47278249639785
At time: 15.867429971694946 and batch: 650, loss is 4.399877367019653 and perplexity is 81.44088071463045
At time: 16.337498664855957 and batch: 700, loss is 4.437581129074097 and perplexity is 84.5701298387264
At time: 16.808278799057007 and batch: 750, loss is 4.488697605133057 and perplexity is 89.00545011754997
At time: 17.278088569641113 and batch: 800, loss is 4.445089349746704 and perplexity is 85.20749076492805
At time: 17.748393535614014 and batch: 850, loss is 4.5026465511322025 and perplexity is 90.25568177005186
At time: 18.220113515853882 and batch: 900, loss is 4.434771728515625 and perplexity is 84.33287190106981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.56366970114512 and perplexity of 95.93488702078025
finished 2 epochs...
Completing Train Step...
At time: 19.54668092727661 and batch: 50, loss is 4.468293056488037 and perplexity is 87.20773721321389
At time: 20.02068781852722 and batch: 100, loss is 4.340550479888916 and perplexity is 76.7497769204327
At time: 20.495965480804443 and batch: 150, loss is 4.3485025119781495 and perplexity is 77.362526683311
At time: 20.969485759735107 and batch: 200, loss is 4.233329019546509 and perplexity is 68.94637438075817
At time: 21.443973302841187 and batch: 250, loss is 4.3734661483764645 and perplexity is 79.31808402671716
At time: 21.914244890213013 and batch: 300, loss is 4.332872204780578 and perplexity is 76.16272766573962
At time: 22.384737491607666 and batch: 350, loss is 4.334291286468506 and perplexity is 76.2708855221388
At time: 22.85270857810974 and batch: 400, loss is 4.241491117477417 and perplexity is 69.51142429986487
At time: 23.320180892944336 and batch: 450, loss is 4.282722344398499 and perplexity is 72.43737130030148
At time: 23.788494348526 and batch: 500, loss is 4.1600301599502565 and perplexity is 64.07345502301185
At time: 24.258156299591064 and batch: 550, loss is 4.237774209976196 and perplexity is 69.25353633522138
At time: 24.727376461029053 and batch: 600, loss is 4.229016304016113 and perplexity is 68.64966854501661
At time: 25.196227550506592 and batch: 650, loss is 4.085050487518311 and perplexity is 59.44493890292424
At time: 25.664360523223877 and batch: 700, loss is 4.101863851547241 and perplexity is 60.45285781025551
At time: 26.15226411819458 and batch: 750, loss is 4.190045199394226 and perplexity is 66.02577521800926
At time: 26.625791549682617 and batch: 800, loss is 4.159943466186523 and perplexity is 64.06790049481548
At time: 27.103419065475464 and batch: 850, loss is 4.224028921127319 and perplexity is 68.30813874067861
At time: 27.57621479034424 and batch: 900, loss is 4.163678560256958 and perplexity is 64.30764759031179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431156524240154 and perplexity of 84.02854178124407
finished 3 epochs...
Completing Train Step...
At time: 28.88136625289917 and batch: 50, loss is 4.224725308418274 and perplexity is 68.35572422740746
At time: 29.36590266227722 and batch: 100, loss is 4.1042085552215575 and perplexity is 60.59476815192161
At time: 29.83601212501526 and batch: 150, loss is 4.115812654495239 and perplexity is 61.3020113812015
At time: 30.307132244110107 and batch: 200, loss is 4.001973934173584 and perplexity is 54.70602962588787
At time: 30.779934406280518 and batch: 250, loss is 4.151292552947998 and perplexity is 63.51604511442393
At time: 31.252113580703735 and batch: 300, loss is 4.116645503044128 and perplexity is 61.35308793899045
At time: 31.721704959869385 and batch: 350, loss is 4.115866293907166 and perplexity is 61.305299673232135
At time: 32.19162702560425 and batch: 400, loss is 4.037414736747742 and perplexity is 56.67962148798773
At time: 32.66278576850891 and batch: 450, loss is 4.0855979108810425 and perplexity is 59.47748935990329
At time: 33.131298780441284 and batch: 500, loss is 3.966697402000427 and perplexity is 52.80983287148498
At time: 33.60338282585144 and batch: 550, loss is 4.03498637676239 and perplexity is 56.542149945932366
At time: 34.07624840736389 and batch: 600, loss is 4.040430507659912 and perplexity is 56.85081224809488
At time: 34.549198389053345 and batch: 650, loss is 3.896822843551636 and perplexity is 49.24573887407438
At time: 35.024723291397095 and batch: 700, loss is 3.90885534286499 and perplexity is 49.84186745933302
At time: 35.49793887138367 and batch: 750, loss is 4.002804450988769 and perplexity is 54.7514827755741
At time: 35.971943616867065 and batch: 800, loss is 3.982664532661438 and perplexity is 53.659822260342445
At time: 36.4444625377655 and batch: 850, loss is 4.045170683860778 and perplexity is 57.12093482382772
At time: 36.91441035270691 and batch: 900, loss is 3.989605164527893 and perplexity is 54.03355078845549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384917794841609 and perplexity of 80.23162748815786
finished 4 epochs...
Completing Train Step...
At time: 38.20318603515625 and batch: 50, loss is 4.062611780166626 and perplexity is 58.12592513992288
At time: 38.68953275680542 and batch: 100, loss is 3.947775373458862 and perplexity is 51.81995845721288
At time: 39.1600387096405 and batch: 150, loss is 3.9593444871902466 and perplexity is 52.422950767890725
At time: 39.629077434539795 and batch: 200, loss is 3.8468528890609743 and perplexity is 46.84540332115693
At time: 40.10028600692749 and batch: 250, loss is 4.001731753349304 and perplexity is 54.692782478707244
At time: 40.57011437416077 and batch: 300, loss is 3.9638320779800416 and perplexity is 52.65873216846178
At time: 41.042604207992554 and batch: 350, loss is 3.966742081642151 and perplexity is 52.8121924486093
At time: 41.51761555671692 and batch: 400, loss is 3.893207855224609 and perplexity is 49.06803749062108
At time: 41.99130368232727 and batch: 450, loss is 3.941654462814331 and perplexity is 51.503741875880614
At time: 42.46442365646362 and batch: 500, loss is 3.829146180152893 and perplexity is 46.02322591007153
At time: 42.94123387336731 and batch: 550, loss is 3.8913323497772216 and perplexity is 48.976096364007105
At time: 43.41742944717407 and batch: 600, loss is 3.9011580467224123 and perplexity is 49.459692588712706
At time: 43.888697385787964 and batch: 650, loss is 3.7608412408828737 and perplexity is 42.984571151818464
At time: 44.356542348861694 and batch: 700, loss is 3.7707056331634523 and perplexity is 43.41068605063426
At time: 44.82881140708923 and batch: 750, loss is 3.8678479766845704 and perplexity is 47.839323884076336
At time: 45.29798698425293 and batch: 800, loss is 3.8535245656967163 and perplexity is 47.158985600608965
At time: 45.76756954193115 and batch: 850, loss is 3.912191939353943 and perplexity is 50.00844740979681
At time: 46.23846936225891 and batch: 900, loss is 3.860522074699402 and perplexity is 47.49013829765263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384807847950556 and perplexity of 80.2228067550664
finished 5 epochs...
Completing Train Step...
At time: 47.55392813682556 and batch: 50, loss is 3.941380681991577 and perplexity is 51.489643069134445
At time: 48.023634910583496 and batch: 100, loss is 3.827306714057922 and perplexity is 45.938645561619495
At time: 48.49817872047424 and batch: 150, loss is 3.8379312324523926 and perplexity is 46.42932353935459
At time: 48.97349286079407 and batch: 200, loss is 3.7313704347610472 and perplexity is 41.73626581913983
At time: 49.44924306869507 and batch: 250, loss is 3.884433345794678 and perplexity is 48.63937294381428
At time: 49.92497682571411 and batch: 300, loss is 3.846329426765442 and perplexity is 46.82088793579904
At time: 50.39952301979065 and batch: 350, loss is 3.8522301864624025 and perplexity is 47.097983477394
At time: 50.88776898384094 and batch: 400, loss is 3.781926794052124 and perplexity is 43.90054761074226
At time: 51.35983729362488 and batch: 450, loss is 3.8315649747848513 and perplexity is 46.13468138145634
At time: 51.83090543746948 and batch: 500, loss is 3.723838415145874 and perplexity is 41.42308835630203
At time: 52.30030798912048 and batch: 550, loss is 3.7794435167312623 and perplexity is 43.791665624523844
At time: 52.771950006484985 and batch: 600, loss is 3.7916948080062864 and perplexity is 44.33146997389402
At time: 53.24181628227234 and batch: 650, loss is 3.658628487586975 and perplexity is 38.80808059048634
At time: 53.713982820510864 and batch: 700, loss is 3.6598729419708254 and perplexity is 38.856405539366634
At time: 54.183300495147705 and batch: 750, loss is 3.764182505607605 and perplexity is 43.12843419152446
At time: 54.654518127441406 and batch: 800, loss is 3.7515630102157593 and perplexity is 42.5875948522068
At time: 55.12448716163635 and batch: 850, loss is 3.8072028160095215 and perplexity is 45.024321250877364
At time: 55.59531235694885 and batch: 900, loss is 3.759545931816101 and perplexity is 42.92892889181594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393618230950342 and perplexity of 80.93272313335328
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 56.922165632247925 and batch: 50, loss is 3.8717087936401366 and perplexity is 48.02437976043463
At time: 57.39671802520752 and batch: 100, loss is 3.760122995376587 and perplexity is 42.95370876146236
At time: 57.87136745452881 and batch: 150, loss is 3.772876501083374 and perplexity is 43.505027280488186
At time: 58.3437225818634 and batch: 200, loss is 3.6484732532501223 and perplexity is 38.415969796247246
At time: 58.814767599105835 and batch: 250, loss is 3.798991641998291 and perplexity is 44.65613241436064
At time: 59.28585410118103 and batch: 300, loss is 3.7410123491287233 and perplexity is 42.14062960802134
At time: 59.75729036331177 and batch: 350, loss is 3.7376704692840574 and perplexity is 42.000035742044155
At time: 60.22789764404297 and batch: 400, loss is 3.658246750831604 and perplexity is 38.7932689469738
At time: 60.69785189628601 and batch: 450, loss is 3.691918468475342 and perplexity is 40.121745473878555
At time: 61.16607642173767 and batch: 500, loss is 3.5825323486328124 and perplexity is 35.9645002810715
At time: 61.63607096672058 and batch: 550, loss is 3.617658772468567 and perplexity is 37.25025433056993
At time: 62.10637164115906 and batch: 600, loss is 3.62440634727478 and perplexity is 37.50245311642413
At time: 62.59144926071167 and batch: 650, loss is 3.4752381229400635 and perplexity is 32.30552030625654
At time: 63.065335750579834 and batch: 700, loss is 3.462746558189392 and perplexity is 31.904483809003832
At time: 63.540568590164185 and batch: 750, loss is 3.5609627199172973 and perplexity is 35.197065755731195
At time: 64.01492142677307 and batch: 800, loss is 3.5334129190444945 and perplexity is 34.24062891825333
At time: 64.48837280273438 and batch: 850, loss is 3.5697885608673094 and perplexity is 35.50908434783182
At time: 64.96119379997253 and batch: 900, loss is 3.508620171546936 and perplexity is 33.402146739233395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324553607261344 and perplexity of 75.53178849120984
finished 7 epochs...
Completing Train Step...
At time: 66.25947618484497 and batch: 50, loss is 3.7750760173797606 and perplexity is 43.60082261004013
At time: 66.7476761341095 and batch: 100, loss is 3.66299852848053 and perplexity is 38.97804459400294
At time: 67.22163653373718 and batch: 150, loss is 3.6776724004745485 and perplexity is 39.55422045778865
At time: 67.69435095787048 and batch: 200, loss is 3.5583135414123537 and perplexity is 35.10394584578669
At time: 68.16363453865051 and batch: 250, loss is 3.7103557348251344 and perplexity is 40.868342233986525
At time: 68.63360500335693 and batch: 300, loss is 3.657795352935791 and perplexity is 38.77576169866421
At time: 69.10643482208252 and batch: 350, loss is 3.6566536474227904 and perplexity is 38.73151646008456
At time: 69.57527804374695 and batch: 400, loss is 3.5810126638412476 and perplexity is 35.90988708489045
At time: 70.04789614677429 and batch: 450, loss is 3.622124752998352 and perplexity is 37.416985272589805
At time: 70.52051091194153 and batch: 500, loss is 3.5195079708099364 and perplexity is 33.76780962768725
At time: 70.99716114997864 and batch: 550, loss is 3.55448281288147 and perplexity is 34.96972939640929
At time: 71.4708423614502 and batch: 600, loss is 3.5693591737747195 and perplexity is 35.493840478337795
At time: 71.94414901733398 and batch: 650, loss is 3.424897985458374 and perplexity is 30.719510854830187
At time: 72.41769027709961 and batch: 700, loss is 3.4180174589157106 and perplexity is 30.508869936238213
At time: 72.88700747489929 and batch: 750, loss is 3.522849454879761 and perplexity is 33.88083295328841
At time: 73.35721850395203 and batch: 800, loss is 3.501754169464111 and perplexity is 33.173593053065794
At time: 73.82865142822266 and batch: 850, loss is 3.543339238166809 and perplexity is 34.582204818871936
At time: 74.29881024360657 and batch: 900, loss is 3.4895635414123536 and perplexity is 32.771641120144665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329802212649828 and perplexity of 75.92926723535221
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.6099464893341 and batch: 50, loss is 3.750978045463562 and perplexity is 42.562689895308544
At time: 76.09477090835571 and batch: 100, loss is 3.6468595123291014 and perplexity is 38.354026367531745
At time: 76.5639181137085 and batch: 150, loss is 3.6685560750961304 and perplexity is 39.19526995474581
At time: 77.035640001297 and batch: 200, loss is 3.5425360107421877 and perplexity is 34.55443859634609
At time: 77.51098775863647 and batch: 250, loss is 3.6916340827941894 and perplexity is 40.11033704623656
At time: 77.98487734794617 and batch: 300, loss is 3.6334404945373535 and perplexity is 37.84279081629368
At time: 78.4589250087738 and batch: 350, loss is 3.6311131000518797 and perplexity is 37.754818126919574
At time: 78.93295288085938 and batch: 400, loss is 3.553019223213196 and perplexity is 34.91858549773281
At time: 79.40725421905518 and batch: 450, loss is 3.586188941001892 and perplexity is 36.096248526374424
At time: 79.87926840782166 and batch: 500, loss is 3.482238836288452 and perplexity is 32.53247549066386
At time: 80.3541271686554 and batch: 550, loss is 3.5100534200668334 and perplexity is 33.45005464037196
At time: 80.82506847381592 and batch: 600, loss is 3.5279541635513305 and perplexity is 34.05422692138577
At time: 81.29455423355103 and batch: 650, loss is 3.3769886302948 and perplexity is 29.282457901543616
At time: 81.76593780517578 and batch: 700, loss is 3.3641571521759035 and perplexity is 28.90912103799051
At time: 82.23566269874573 and batch: 750, loss is 3.466843638420105 and perplexity is 32.03546718035488
At time: 82.70692896842957 and batch: 800, loss is 3.4364596223831176 and perplexity is 31.076739782150987
At time: 83.17628908157349 and batch: 850, loss is 3.4776588773727415 and perplexity is 32.3838187701786
At time: 83.64761662483215 and batch: 900, loss is 3.421703815460205 and perplexity is 30.621544059492486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319334265303938 and perplexity of 75.13858927162394
finished 9 epochs...
Completing Train Step...
At time: 84.96570062637329 and batch: 50, loss is 3.7225459146499635 and perplexity is 41.36958357898355
At time: 85.43908023834229 and batch: 100, loss is 3.6109969806671143 and perplexity is 37.00292563250739
At time: 85.91305828094482 and batch: 150, loss is 3.633935070037842 and perplexity is 37.86151156253114
At time: 86.38502788543701 and batch: 200, loss is 3.5104986476898192 and perplexity is 33.464950844542955
At time: 86.87082314491272 and batch: 250, loss is 3.661724190711975 and perplexity is 38.92840503513235
At time: 87.34224772453308 and batch: 300, loss is 3.6046466207504273 and perplexity is 36.7686882697254
At time: 87.81363916397095 and batch: 350, loss is 3.6037039947509766 and perplexity is 36.73404547835971
At time: 88.28402543067932 and batch: 400, loss is 3.5271930408477785 and perplexity is 34.028317337561035
At time: 88.75286078453064 and batch: 450, loss is 3.563632345199585 and perplexity is 35.29115426698932
At time: 89.22618460655212 and batch: 500, loss is 3.4613574504852296 and perplexity is 31.860195812267733
At time: 89.69800209999084 and batch: 550, loss is 3.490871376991272 and perplexity is 32.8145290774663
At time: 90.1680793762207 and batch: 600, loss is 3.5115637969970703 and perplexity is 33.500615004207816
At time: 90.63842225074768 and batch: 650, loss is 3.3632135725021364 and perplexity is 28.881855844456744
At time: 91.10749578475952 and batch: 700, loss is 3.353318796157837 and perplexity is 28.597485550834957
At time: 91.58225750923157 and batch: 750, loss is 3.459221906661987 and perplexity is 31.79222956617234
At time: 92.0563325881958 and batch: 800, loss is 3.4315693759918213 and perplexity is 30.925137855090124
At time: 92.53096795082092 and batch: 850, loss is 3.476071629524231 and perplexity is 32.33245839511441
At time: 93.00614762306213 and batch: 900, loss is 3.4227854776382447 and perplexity is 30.654684145492475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32035890344071 and perplexity of 75.21561859258553
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.33528780937195 and batch: 50, loss is 3.7160738039016725 and perplexity is 41.10269963527707
At time: 94.80430722236633 and batch: 100, loss is 3.6091043615341185 and perplexity is 36.9329594180615
At time: 95.27485752105713 and batch: 150, loss is 3.634212827682495 and perplexity is 37.87202934743573
At time: 95.74483156204224 and batch: 200, loss is 3.510101718902588 and perplexity is 33.45167027808341
At time: 96.21777033805847 and batch: 250, loss is 3.6626022386550905 and perplexity is 38.96260105177625
At time: 96.68600130081177 and batch: 300, loss is 3.60163875579834 and perplexity is 36.658259182092486
At time: 97.15641069412231 and batch: 350, loss is 3.6028851747512816 and perplexity is 36.70397921835969
At time: 97.61775922775269 and batch: 400, loss is 3.5232436656951904 and perplexity is 33.89419177699776
At time: 98.07721018791199 and batch: 450, loss is 3.554177165031433 and perplexity is 34.959042607083205
At time: 98.5542049407959 and batch: 500, loss is 3.4514232730865477 and perplexity is 31.545257889749937
At time: 99.0409848690033 and batch: 550, loss is 3.480148205757141 and perplexity is 32.464533150056276
At time: 99.51144790649414 and batch: 600, loss is 3.501438913345337 and perplexity is 33.1631365232032
At time: 99.98562455177307 and batch: 650, loss is 3.347220902442932 and perplexity is 28.423631733202523
At time: 100.45870804786682 and batch: 700, loss is 3.335487174987793 and perplexity is 28.092065642400794
At time: 100.93129706382751 and batch: 750, loss is 3.438688611984253 and perplexity is 31.146086770108177
At time: 101.40711569786072 and batch: 800, loss is 3.411581130027771 and perplexity is 30.313135396977085
At time: 101.88015556335449 and batch: 850, loss is 3.4521462059020998 and perplexity is 31.56807123711863
At time: 102.35038709640503 and batch: 900, loss is 3.4029572296142576 and perplexity is 30.052841921799097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31510904390518 and perplexity of 74.82178185840851
finished 11 epochs...
Completing Train Step...
At time: 103.64648795127869 and batch: 50, loss is 3.704821715354919 and perplexity is 40.64280068362128
At time: 104.13326930999756 and batch: 100, loss is 3.5946989679336547 and perplexity is 36.40473934429966
At time: 104.60119843482971 and batch: 150, loss is 3.620523371696472 and perplexity is 37.357114362873766
At time: 105.0726592540741 and batch: 200, loss is 3.4981771898269653 and perplexity is 33.05514375787687
At time: 105.54172539710999 and batch: 250, loss is 3.6501935386657713 and perplexity is 38.48211310518866
At time: 106.01103115081787 and batch: 300, loss is 3.5916876745224 and perplexity is 36.29527888406965
At time: 106.48012852668762 and batch: 350, loss is 3.5925761699676513 and perplexity is 36.327541404471084
At time: 106.95396733283997 and batch: 400, loss is 3.514199161529541 and perplexity is 33.58901777240162
At time: 107.43017363548279 and batch: 450, loss is 3.546160593032837 and perplexity is 34.67991125817234
At time: 107.90642166137695 and batch: 500, loss is 3.444207816123962 and perplexity is 31.318463635126086
At time: 108.38281989097595 and batch: 550, loss is 3.4739345264434816 and perplexity is 32.26343438065757
At time: 108.85709857940674 and batch: 600, loss is 3.4969643688201906 and perplexity is 33.01507808630059
At time: 109.33098220825195 and batch: 650, loss is 3.344267625808716 and perplexity is 28.33981271703761
At time: 109.80503153800964 and batch: 700, loss is 3.3334269094467164 and perplexity is 28.03424810776871
At time: 110.2759120464325 and batch: 750, loss is 3.4377270555496215 and perplexity is 31.11615244403913
At time: 110.74635815620422 and batch: 800, loss is 3.4116698265075684 and perplexity is 30.31582418461965
At time: 111.23453879356384 and batch: 850, loss is 3.4542244720458983 and perplexity is 31.63374631229978
At time: 111.70505237579346 and batch: 900, loss is 3.406223187446594 and perplexity is 30.15115368991541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314691412938784 and perplexity of 74.7905404894792
finished 12 epochs...
Completing Train Step...
At time: 113.00215315818787 and batch: 50, loss is 3.6992062187194823 and perplexity is 40.41521078634314
At time: 113.48738074302673 and batch: 100, loss is 3.5882675790786744 and perplexity is 36.17135759823482
At time: 113.95899152755737 and batch: 150, loss is 3.6137526130676267 and perplexity is 37.10503271347181
At time: 114.43346428871155 and batch: 200, loss is 3.4917859649658203 and perplexity is 32.844554579539626
At time: 114.90959692001343 and batch: 250, loss is 3.6437067127227785 and perplexity is 38.23329423042436
At time: 115.38468313217163 and batch: 300, loss is 3.5857895183563233 and perplexity is 36.081833746279365
At time: 115.86134934425354 and batch: 350, loss is 3.5867601251602172 and perplexity is 36.11687202105369
At time: 116.33861827850342 and batch: 400, loss is 3.508716564178467 and perplexity is 33.40536661524004
At time: 116.8122456073761 and batch: 450, loss is 3.5411943197250366 and perplexity is 34.50810830389734
At time: 117.28495407104492 and batch: 500, loss is 3.439760389328003 and perplexity is 31.179486335506397
At time: 117.75519728660583 and batch: 550, loss is 3.4699662160873412 and perplexity is 32.13565675819274
At time: 118.2258768081665 and batch: 600, loss is 3.493833909034729 and perplexity is 32.91188731359019
At time: 118.69700336456299 and batch: 650, loss is 3.3417871522903444 and perplexity is 28.269603673911718
At time: 119.16809749603271 and batch: 700, loss is 3.33153254032135 and perplexity is 27.981191164285136
At time: 119.63712072372437 and batch: 750, loss is 3.4364602088928224 and perplexity is 31.07675800896581
At time: 120.10881042480469 and batch: 800, loss is 3.410941104888916 and perplexity is 30.293740435578993
At time: 120.57991075515747 and batch: 850, loss is 3.454336223602295 and perplexity is 31.6372816302198
At time: 121.0500898361206 and batch: 900, loss is 3.4068352508544923 and perplexity is 30.169613756584287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315035885327483 and perplexity of 74.8163082034919
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 122.37126421928406 and batch: 50, loss is 3.697271218299866 and perplexity is 40.33708294958849
At time: 122.84626817703247 and batch: 100, loss is 3.5879917192459105 and perplexity is 36.16138074974608
At time: 123.3361337184906 and batch: 150, loss is 3.613476710319519 and perplexity is 37.09479674510837
At time: 123.80874490737915 and batch: 200, loss is 3.491980471611023 and perplexity is 32.85094368500566
At time: 124.27890872955322 and batch: 250, loss is 3.644055347442627 and perplexity is 38.24662600807252
At time: 124.74927306175232 and batch: 300, loss is 3.5867622900009155 and perplexity is 36.11695020841277
At time: 125.22021102905273 and batch: 350, loss is 3.586545634269714 and perplexity is 36.1091261117549
At time: 125.68981766700745 and batch: 400, loss is 3.506921801567078 and perplexity is 33.34546568227597
At time: 126.1600911617279 and batch: 450, loss is 3.5375321292877198 and perplexity is 34.38196416257422
At time: 126.63033556938171 and batch: 500, loss is 3.4368125915527346 and perplexity is 31.08771084929662
At time: 127.09891366958618 and batch: 550, loss is 3.466936616897583 and perplexity is 32.038445927796154
At time: 127.5716381072998 and batch: 600, loss is 3.490496859550476 and perplexity is 32.8022417650653
At time: 128.04225850105286 and batch: 650, loss is 3.33583553314209 and perplexity is 28.10185344727021
At time: 128.513512134552 and batch: 700, loss is 3.325382695198059 and perplexity is 27.80963922188048
At time: 128.98648619651794 and batch: 750, loss is 3.4281734466552733 and perplexity is 30.820296390070173
At time: 129.46077299118042 and batch: 800, loss is 3.4032795429229736 and perplexity is 30.062529913915757
At time: 129.93668460845947 and batch: 850, loss is 3.4447129726409913 and perplexity is 31.334288357779084
At time: 130.41167449951172 and batch: 900, loss is 3.3976336479187013 and perplexity is 29.893278264570142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315227351776541 and perplexity of 74.8306343878033
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 131.72339344024658 and batch: 50, loss is 3.695288772583008 and perplexity is 40.257196084115236
At time: 132.19186091423035 and batch: 100, loss is 3.586234407424927 and perplexity is 36.09788973098947
At time: 132.66315603256226 and batch: 150, loss is 3.6123925495147704 and perplexity is 37.05460181323915
At time: 133.13564252853394 and batch: 200, loss is 3.490712633132935 and perplexity is 32.80932038594585
At time: 133.60570096969604 and batch: 250, loss is 3.642500810623169 and perplexity is 38.187216408891636
At time: 134.0764811038971 and batch: 300, loss is 3.586495304107666 and perplexity is 36.10730877931997
At time: 134.5474238395691 and batch: 350, loss is 3.5850653743743894 and perplexity is 36.055714761609
At time: 135.01663064956665 and batch: 400, loss is 3.5053696203231812 and perplexity is 33.293747624158385
At time: 135.50493836402893 and batch: 450, loss is 3.535896587371826 and perplexity is 34.325776979806896
At time: 135.97880387306213 and batch: 500, loss is 3.43521803855896 and perplexity is 31.03817934768926
At time: 136.45404410362244 and batch: 550, loss is 3.4649732875823975 and perplexity is 31.97560561599198
At time: 136.92728805541992 and batch: 600, loss is 3.4885575675964358 and perplexity is 32.738690283891195
At time: 137.40199494361877 and batch: 650, loss is 3.3342764282226565 and perplexity is 28.058073846679015
At time: 137.87700533866882 and batch: 700, loss is 3.323845314979553 and perplexity is 27.76691808039031
At time: 138.34924840927124 and batch: 750, loss is 3.4258793354034425 and perplexity is 30.749672242137418
At time: 138.81978273391724 and batch: 800, loss is 3.400941700935364 and perplexity is 29.9923305587022
At time: 139.28888726234436 and batch: 850, loss is 3.4422121381759645 and perplexity is 31.256024392916903
At time: 139.75932216644287 and batch: 900, loss is 3.394874401092529 and perplexity is 29.810909022189506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3148389842412245 and perplexity of 74.80157824135712
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 141.0536811351776 and batch: 50, loss is 3.6946565246582033 and perplexity is 40.23175159989137
At time: 141.5391902923584 and batch: 100, loss is 3.585663013458252 and perplexity is 36.077269506284516
At time: 142.01052975654602 and batch: 150, loss is 3.6120693778991697 and perplexity is 37.04262875248663
At time: 142.48121213912964 and batch: 200, loss is 3.4902057313919066 and perplexity is 32.79269349877532
At time: 142.94641637802124 and batch: 250, loss is 3.642033772468567 and perplexity is 38.16938568594872
At time: 143.41308307647705 and batch: 300, loss is 3.586259846687317 and perplexity is 36.09880804635864
At time: 143.8860890865326 and batch: 350, loss is 3.5846377992630005 and perplexity is 36.040301530745374
At time: 144.3591616153717 and batch: 400, loss is 3.5049316453933717 and perplexity is 33.279168990150744
At time: 144.83464241027832 and batch: 450, loss is 3.5354766416549683 and perplexity is 34.31136504311376
At time: 145.30898475646973 and batch: 500, loss is 3.434796915054321 and perplexity is 31.025111192671503
At time: 145.78561758995056 and batch: 550, loss is 3.4643911647796632 and perplexity is 31.95699730351866
At time: 146.25545144081116 and batch: 600, loss is 3.488025703430176 and perplexity is 32.721282377410105
At time: 146.72513246536255 and batch: 650, loss is 3.333851685523987 and perplexity is 28.046158915243105
At time: 147.19385600090027 and batch: 700, loss is 3.32344096660614 and perplexity is 27.755692841836236
At time: 147.6786403656006 and batch: 750, loss is 3.425313410758972 and perplexity is 30.732275167987027
At time: 148.15034580230713 and batch: 800, loss is 3.4003312969207764 and perplexity is 29.97402870605276
At time: 148.61989617347717 and batch: 850, loss is 3.4415715503692628 and perplexity is 31.23600857643328
At time: 149.0894854068756 and batch: 900, loss is 3.3941646146774294 and perplexity is 29.78975715148704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314722348565924 and perplexity of 74.7928542175415
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 150.38806986808777 and batch: 50, loss is 3.694495701789856 and perplexity is 40.22528193444945
At time: 150.8793704509735 and batch: 100, loss is 3.5855117893218993 and perplexity is 36.0718141648615
At time: 151.35536646842957 and batch: 150, loss is 3.611977553367615 and perplexity is 37.03922748661608
At time: 151.83090710639954 and batch: 200, loss is 3.490068545341492 and perplexity is 32.7881951072371
At time: 152.3034474849701 and batch: 250, loss is 3.6419090843200683 and perplexity is 38.16462671261816
At time: 152.7787959575653 and batch: 300, loss is 3.5861885166168213 and perplexity is 36.096233207668696
At time: 153.25418329238892 and batch: 350, loss is 3.584529986381531 and perplexity is 36.03641613144013
At time: 153.724187374115 and batch: 400, loss is 3.5048198890686035 and perplexity is 33.275450040345014
At time: 154.19262886047363 and batch: 450, loss is 3.5353715085983275 and perplexity is 34.30775797404368
At time: 154.66126322746277 and batch: 500, loss is 3.4346919870376587 and perplexity is 31.02185596007286
At time: 155.12986373901367 and batch: 550, loss is 3.4642432928085327 and perplexity is 31.952272108706516
At time: 155.60070037841797 and batch: 600, loss is 3.4878901195526124 and perplexity is 32.716846199810405
At time: 156.07003736495972 and batch: 650, loss is 3.3337427759170533 and perplexity is 28.043104585425592
At time: 156.54095339775085 and batch: 700, loss is 3.3233369255065917 and perplexity is 27.75280525925048
At time: 157.01110100746155 and batch: 750, loss is 3.425171594619751 and perplexity is 30.727917144399562
At time: 157.4815013408661 and batch: 800, loss is 3.4001769733428957 and perplexity is 29.96940336360822
At time: 157.95481204986572 and batch: 850, loss is 3.441409721374512 and perplexity is 31.230954093557298
At time: 158.4299192428589 and batch: 900, loss is 3.3939854097366333 and perplexity is 29.784419158132057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314693085134846 and perplexity of 74.79066555403102
Annealing...
Model not improving. Stopping early with 74.7905404894792 lossat 16 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f189bae2b70>
ELAPSED
950.5730803012848


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.86398936205909}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5386602506977849, 'wordvec_dim': 300, 'rnn_dropout': 0.729059618910752, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.25572014345254}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.94588761585722, 'wordvec_dim': 300, 'rnn_dropout': 0.2935830384905056, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.9551185187551}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5759509205725062, 'wordvec_dim': 300, 'rnn_dropout': 0.2773236962555252, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.7905404894792}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.24459635961389775, 'wordvec_dim': 300, 'rnn_dropout': 0.2836115397488401, 'num_layers': 1, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.734480619430542 and batch: 50, loss is 6.842169847488403 and perplexity is 936.519035121839
At time: 1.231426477432251 and batch: 100, loss is 5.94614987373352 and perplexity is 382.2786808822667
At time: 1.706052541732788 and batch: 150, loss is 5.695663757324219 and perplexity is 297.5742451336097
At time: 2.179776668548584 and batch: 200, loss is 5.44744797706604 and perplexity is 232.16491904197846
At time: 2.6540427207946777 and batch: 250, loss is 5.43130277633667 and perplexity is 228.44666655671247
At time: 3.128521203994751 and batch: 300, loss is 5.320650329589844 and perplexity is 204.51684210837783
At time: 3.601151943206787 and batch: 350, loss is 5.273324584960937 and perplexity is 195.06339044867863
At time: 4.075352191925049 and batch: 400, loss is 5.109085617065429 and perplexity is 165.51893796553722
At time: 4.550696611404419 and batch: 450, loss is 5.100036458969116 and perplexity is 164.02788747856928
At time: 5.024147033691406 and batch: 500, loss is 5.024829359054565 and perplexity is 152.14429185164548
At time: 5.498173952102661 and batch: 550, loss is 5.074524154663086 and perplexity is 159.89608806607936
At time: 5.974544048309326 and batch: 600, loss is 4.984705820083618 and perplexity is 146.16057119642286
At time: 6.4532928466796875 and batch: 650, loss is 4.867670202255249 and perplexity is 130.01764893186674
At time: 6.931121587753296 and batch: 700, loss is 4.9304554271698 and perplexity is 138.44254848272047
At time: 7.4101128578186035 and batch: 750, loss is 4.925887622833252 and perplexity is 137.81161210339474
At time: 7.8881261348724365 and batch: 800, loss is 4.8870509910583495 and perplexity is 132.56207033029906
At time: 8.364251375198364 and batch: 850, loss is 4.926662187576294 and perplexity is 137.9183974700631
At time: 8.837282180786133 and batch: 900, loss is 4.841306581497192 and perplexity is 126.63470224348464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.817919169386772 and perplexity of 123.70740861340506
finished 1 epochs...
Completing Train Step...
At time: 10.165789604187012 and batch: 50, loss is 4.786542367935181 and perplexity is 119.88612908002257
At time: 10.633216857910156 and batch: 100, loss is 4.665376844406128 and perplexity is 106.20560066821437
At time: 11.102113485336304 and batch: 150, loss is 4.6551242923736575 and perplexity is 105.12228508527507
At time: 11.569669961929321 and batch: 200, loss is 4.538869867324829 and perplexity is 93.58497689330198
At time: 12.040703296661377 and batch: 250, loss is 4.654878253936768 and perplexity is 105.09642414409363
At time: 12.510579586029053 and batch: 300, loss is 4.596552143096924 and perplexity is 99.14189859919522
At time: 12.998196840286255 and batch: 350, loss is 4.591157112121582 and perplexity is 98.60846522394776
At time: 13.471710920333862 and batch: 400, loss is 4.486195507049561 and perplexity is 88.78302812824604
At time: 13.944046974182129 and batch: 450, loss is 4.51005334854126 and perplexity is 90.92666918630287
At time: 14.418632745742798 and batch: 500, loss is 4.401248226165771 and perplexity is 81.55260124988888
At time: 14.893678426742554 and batch: 550, loss is 4.476496181488037 and perplexity is 87.92605538136253
At time: 15.36762022972107 and batch: 600, loss is 4.4449404430389405 and perplexity is 85.19480374261627
At time: 15.838542222976685 and batch: 650, loss is 4.29738486289978 and perplexity is 73.5073104295822
At time: 16.307597637176514 and batch: 700, loss is 4.334208145141601 and perplexity is 76.26454452311546
At time: 16.776812314987183 and batch: 750, loss is 4.399682664871216 and perplexity is 81.42502554375274
At time: 17.25014853477478 and batch: 800, loss is 4.360521745681763 and perplexity is 78.29797539841142
At time: 17.720199584960938 and batch: 850, loss is 4.421219139099121 and perplexity is 83.19765306692909
At time: 18.19137144088745 and batch: 900, loss is 4.353942861557007 and perplexity is 77.78455281671451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.526634320820848 and perplexity of 92.44689031051477
finished 2 epochs...
Completing Train Step...
At time: 19.50702142715454 and batch: 50, loss is 4.418889465332032 and perplexity is 83.00405527450555
At time: 19.978763341903687 and batch: 100, loss is 4.301989822387696 and perplexity is 73.84658919911838
At time: 20.453848361968994 and batch: 150, loss is 4.300997633934021 and perplexity is 73.77335580264476
At time: 20.929820775985718 and batch: 200, loss is 4.194133353233338 and perplexity is 66.29625124153864
At time: 21.405349254608154 and batch: 250, loss is 4.329640216827393 and perplexity is 75.91696800749816
At time: 21.87937045097351 and batch: 300, loss is 4.290347766876221 and perplexity is 72.99184823331134
At time: 22.35273790359497 and batch: 350, loss is 4.290622882843017 and perplexity is 73.01193221879218
At time: 22.83173894882202 and batch: 400, loss is 4.208969902992249 and perplexity is 67.28719176374533
At time: 23.30233383178711 and batch: 450, loss is 4.241169857978821 and perplexity is 69.4890966812194
At time: 23.771727323532104 and batch: 500, loss is 4.1278342390060425 and perplexity is 62.04340613278426
At time: 24.243505001068115 and batch: 550, loss is 4.200582227706909 and perplexity is 66.725168975693
At time: 24.714314222335815 and batch: 600, loss is 4.194573407173157 and perplexity is 66.32543158808505
At time: 25.202083826065063 and batch: 650, loss is 4.042716374397278 and perplexity is 56.98091427009671
At time: 25.672762870788574 and batch: 700, loss is 4.064716892242432 and perplexity is 58.24841560970118
At time: 26.143158435821533 and batch: 750, loss is 4.153454704284668 and perplexity is 63.65352498885649
At time: 26.61404061317444 and batch: 800, loss is 4.121232333183289 and perplexity is 61.63515052393616
At time: 27.08404040336609 and batch: 850, loss is 4.188079552650452 and perplexity is 65.89611933852214
At time: 27.556065797805786 and batch: 900, loss is 4.132054095268249 and perplexity is 62.30577357579564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428014885889341 and perplexity of 83.764968734204
finished 3 epochs...
Completing Train Step...
At time: 28.860394716262817 and batch: 50, loss is 4.209713883399964 and perplexity is 67.33727074268738
At time: 29.348798513412476 and batch: 100, loss is 4.096667919158936 and perplexity is 60.139563481321176
At time: 29.821680545806885 and batch: 150, loss is 4.100337781906128 and perplexity is 60.36067289742185
At time: 30.291556358337402 and batch: 200, loss is 3.9937654399871825 and perplexity is 54.258813496334916
At time: 30.761279106140137 and batch: 250, loss is 4.138822727203369 and perplexity is 62.728928900336086
At time: 31.2323157787323 and batch: 300, loss is 4.104362325668335 and perplexity is 60.6040865529217
At time: 31.700382471084595 and batch: 350, loss is 4.107990484237671 and perplexity is 60.82436715372098
At time: 32.17112922668457 and batch: 400, loss is 4.035246968269348 and perplexity is 56.55688626999048
At time: 32.640488386154175 and batch: 450, loss is 4.068092675209045 and perplexity is 58.44538188960417
At time: 33.11098909378052 and batch: 500, loss is 3.953815474510193 and perplexity is 52.13390341794801
At time: 33.58087992668152 and batch: 550, loss is 4.0243110799789426 and perplexity is 55.941756107518806
At time: 34.05116581916809 and batch: 600, loss is 4.029844932556152 and perplexity is 56.25218768781969
At time: 34.52378296852112 and batch: 650, loss is 3.8791287660598757 and perplexity is 48.38204462448088
At time: 34.99915647506714 and batch: 700, loss is 3.8954864883422853 and perplexity is 49.17997302744486
At time: 35.47449231147766 and batch: 750, loss is 3.989580998420715 and perplexity is 54.032245023653616
At time: 35.948566913604736 and batch: 800, loss is 3.9639609479904174 and perplexity is 52.665518737105906
At time: 36.42286157608032 and batch: 850, loss is 4.0290862607955935 and perplexity is 56.20952692635504
At time: 36.89653420448303 and batch: 900, loss is 3.97827260017395 and perplexity is 53.42466871093204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386564907962328 and perplexity of 80.3638869477344
finished 4 epochs...
Completing Train Step...
At time: 38.210514307022095 and batch: 50, loss is 4.055520100593567 and perplexity is 57.715172887081934
At time: 38.69614553451538 and batch: 100, loss is 3.950360813140869 and perplexity is 51.95410917869995
At time: 39.1677782535553 and batch: 150, loss is 3.957245192527771 and perplexity is 52.313014981331705
At time: 39.63921022415161 and batch: 200, loss is 3.8534413146972657 and perplexity is 47.15505973134317
At time: 40.11037302017212 and batch: 250, loss is 3.999943218231201 and perplexity is 54.59504994162739
At time: 40.5817174911499 and batch: 300, loss is 3.9677165079116823 and perplexity is 52.86367911719007
At time: 41.051798820495605 and batch: 350, loss is 3.9727156496047975 and perplexity is 53.12861381079508
At time: 41.52443480491638 and batch: 400, loss is 3.9054903888702395 and perplexity is 49.67443372970505
At time: 41.99908518791199 and batch: 450, loss is 3.9385208654403687 and perplexity is 51.34260249042476
At time: 42.47133755683899 and batch: 500, loss is 3.8283104848861695 and perplexity is 45.98478058454489
At time: 42.94538331031799 and batch: 550, loss is 3.8968622207641603 and perplexity is 49.2476780721798
At time: 43.419761180877686 and batch: 600, loss is 3.9033452796936037 and perplexity is 49.56799085267977
At time: 43.89381957054138 and batch: 650, loss is 3.7592059707641603 and perplexity is 42.91433720843375
At time: 44.36475133895874 and batch: 700, loss is 3.7720365047454836 and perplexity is 43.4684985610359
At time: 44.83381485939026 and batch: 750, loss is 3.8695285272598268 and perplexity is 47.919787880319646
At time: 45.304020166397095 and batch: 800, loss is 3.8442208194732665 and perplexity is 46.722265085044235
At time: 45.77341055870056 and batch: 850, loss is 3.909285535812378 and perplexity is 49.86331369187692
At time: 46.243780851364136 and batch: 900, loss is 3.861114354133606 and perplexity is 47.51827406118841
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377339402290239 and perplexity of 79.62589884375699
finished 5 epochs...
Completing Train Step...
At time: 47.5575385093689 and batch: 50, loss is 3.940524082183838 and perplexity is 51.44555593599154
At time: 48.02755045890808 and batch: 100, loss is 3.837577986717224 and perplexity is 46.412925475271166
At time: 48.496201276779175 and batch: 150, loss is 3.844394254684448 and perplexity is 46.73036907369432
At time: 48.97125744819641 and batch: 200, loss is 3.745468258857727 and perplexity is 42.32882342542471
At time: 49.46194648742676 and batch: 250, loss is 3.890726251602173 and perplexity is 48.946421035369575
At time: 49.9353141784668 and batch: 300, loss is 3.8597662448883057 and perplexity is 47.454257397029515
At time: 50.41015100479126 and batch: 350, loss is 3.8659197664260865 and perplexity is 47.747168485049784
At time: 50.88422155380249 and batch: 400, loss is 3.8019277334213255 and perplexity is 44.78743957230923
At time: 51.35789442062378 and batch: 450, loss is 3.8367820501327516 and perplexity is 46.37599842763658
At time: 51.82893967628479 and batch: 500, loss is 3.728305139541626 and perplexity is 41.60852772055867
At time: 52.303579807281494 and batch: 550, loss is 3.7937301874160765 and perplexity is 44.421793224944956
At time: 52.77426528930664 and batch: 600, loss is 3.80435884475708 and perplexity is 44.89645528532776
At time: 53.244478702545166 and batch: 650, loss is 3.6647157287597656 and perplexity is 39.045035204749865
At time: 53.71470808982849 and batch: 700, loss is 3.6758701276779173 and perplexity is 39.48299716344745
At time: 54.18608021736145 and batch: 750, loss is 3.7726403427124025 and perplexity is 43.49475441717563
At time: 54.65663695335388 and batch: 800, loss is 3.7495841693878176 and perplexity is 42.503404108263055
At time: 55.12887907028198 and batch: 850, loss is 3.8123902797698976 and perplexity is 45.25849013187462
At time: 55.6002459526062 and batch: 900, loss is 3.7669695663452147 and perplexity is 43.24880341738989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380786216422303 and perplexity of 79.90082805993175
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 56.90896654129028 and batch: 50, loss is 3.873707342147827 and perplexity is 48.120454786258804
At time: 57.38180446624756 and batch: 100, loss is 3.773962149620056 and perplexity is 43.55228409719897
At time: 57.853837966918945 and batch: 150, loss is 3.7799854373931883 and perplexity is 43.815403664432125
At time: 58.329429149627686 and batch: 200, loss is 3.6632158374786377 and perplexity is 38.986515794222534
At time: 58.79815888404846 and batch: 250, loss is 3.7947852563858033 and perplexity is 44.46868601376944
At time: 59.266185998916626 and batch: 300, loss is 3.7557817983627317 and perplexity is 42.76764241679998
At time: 59.73067355155945 and batch: 350, loss is 3.7454345512390135 and perplexity is 42.327396645630905
At time: 60.19574594497681 and batch: 400, loss is 3.6791269636154174 and perplexity is 39.61179643272644
At time: 60.66967177391052 and batch: 450, loss is 3.7020547676086424 and perplexity is 40.53049961510964
At time: 61.13972353935242 and batch: 500, loss is 3.585482482910156 and perplexity is 36.070757044913535
At time: 61.625187158584595 and batch: 550, loss is 3.634447469711304 and perplexity is 37.88091675987638
At time: 62.09754180908203 and batch: 600, loss is 3.642858667373657 and perplexity is 38.20088440751283
At time: 62.56797122955322 and batch: 650, loss is 3.4881539392471312 and perplexity is 32.72547868684043
At time: 63.04013681411743 and batch: 700, loss is 3.4812298345565797 and perplexity is 32.49966672138771
At time: 63.513214349746704 and batch: 750, loss is 3.574095287322998 and perplexity is 35.66234204341209
At time: 63.98277950286865 and batch: 800, loss is 3.5314094400405884 and perplexity is 34.17209721097683
At time: 64.45419430732727 and batch: 850, loss is 3.5755010938644407 and perplexity is 35.71251165326025
At time: 64.92613554000854 and batch: 900, loss is 3.5166297340393067 and perplexity is 33.670757612692555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336378541711259 and perplexity of 76.43024858565197
finished 7 epochs...
Completing Train Step...
At time: 66.23758053779602 and batch: 50, loss is 3.77915735244751 and perplexity is 43.77913580677474
At time: 66.7238392829895 and batch: 100, loss is 3.678685550689697 and perplexity is 39.59431513228988
At time: 67.19528794288635 and batch: 150, loss is 3.6849144315719604 and perplexity is 39.841713111236274
At time: 67.667560338974 and batch: 200, loss is 3.5744503784179686 and perplexity is 35.675007672090395
At time: 68.13775658607483 and batch: 250, loss is 3.7089297008514404 and perplexity is 40.810104124143194
At time: 68.61033415794373 and batch: 300, loss is 3.6730424642562864 and perplexity is 39.37151023463179
At time: 69.08150458335876 and batch: 350, loss is 3.667148776054382 and perplexity is 39.14014928362842
At time: 69.55194783210754 and batch: 400, loss is 3.6050398111343385 and perplexity is 36.783148206949924
At time: 70.02370643615723 and batch: 450, loss is 3.634205470085144 and perplexity is 37.87175070131799
At time: 70.49194717407227 and batch: 500, loss is 3.521428804397583 and perplexity is 33.832734305389984
At time: 70.97164821624756 and batch: 550, loss is 3.574337749481201 and perplexity is 35.6709898601723
At time: 71.43574142456055 and batch: 600, loss is 3.5897623777389525 and perplexity is 36.2254669263134
At time: 71.89669895172119 and batch: 650, loss is 3.4391446495056153 and perplexity is 31.160293793540607
At time: 72.37388682365417 and batch: 700, loss is 3.4368097257614134 and perplexity is 31.087621758532332
At time: 72.84491348266602 and batch: 750, loss is 3.53725332736969 and perplexity is 34.37237974115968
At time: 73.31445837020874 and batch: 800, loss is 3.5012036848068235 and perplexity is 33.15533652449344
At time: 73.7986204624176 and batch: 850, loss is 3.549991054534912 and perplexity is 34.81300606767874
At time: 74.26765608787537 and batch: 900, loss is 3.499663691520691 and perplexity is 33.104316823927526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341235853221319 and perplexity of 76.80239720101056
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.56997895240784 and batch: 50, loss is 3.752981448173523 and perplexity is 42.64804557594018
At time: 76.05603361129761 and batch: 100, loss is 3.662986788749695 and perplexity is 38.97758700493692
At time: 76.52505040168762 and batch: 150, loss is 3.6738080978393555 and perplexity is 39.40166592771615
At time: 76.99410128593445 and batch: 200, loss is 3.5587625885009766 and perplexity is 35.11971271023527
At time: 77.46338295936584 and batch: 250, loss is 3.6937233638763427 and perplexity is 40.19422641834971
At time: 77.93393135070801 and batch: 300, loss is 3.6502102756500245 and perplexity is 38.48275718509969
At time: 78.40473198890686 and batch: 350, loss is 3.6412003326416014 and perplexity is 38.137587052711595
At time: 78.87621545791626 and batch: 400, loss is 3.5785787153244017 and perplexity is 35.822590549213004
At time: 79.34693455696106 and batch: 450, loss is 3.6024925565719603 and perplexity is 36.689571397436424
At time: 79.81834173202515 and batch: 500, loss is 3.4845051622390746 and perplexity is 32.60628829444939
At time: 80.28668332099915 and batch: 550, loss is 3.5313323497772218 and perplexity is 34.16946297654126
At time: 80.75446462631226 and batch: 600, loss is 3.549320774078369 and perplexity is 34.78967940865324
At time: 81.22567176818848 and batch: 650, loss is 3.3880615711212156 and perplexity is 29.608502630780507
At time: 81.69552659988403 and batch: 700, loss is 3.382627568244934 and perplexity is 29.44804629724596
At time: 82.16563701629639 and batch: 750, loss is 3.4792409467697145 and perplexity is 32.43509276762847
At time: 82.63496255874634 and batch: 800, loss is 3.4369230461120606 and perplexity is 31.091144818344212
At time: 83.10630536079407 and batch: 850, loss is 3.4832403564453127 and perplexity is 32.56507374179835
At time: 83.57527542114258 and batch: 900, loss is 3.4358322048187255 and perplexity is 31.057247805211396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3256334278681505 and perplexity of 75.6133933242524
finished 9 epochs...
Completing Train Step...
At time: 84.8857204914093 and batch: 50, loss is 3.7270055055618285 and perplexity is 41.554486988276125
At time: 85.3558759689331 and batch: 100, loss is 3.628938579559326 and perplexity is 37.67280869911602
At time: 85.842618227005 and batch: 150, loss is 3.637989158630371 and perplexity is 38.01531704452022
At time: 86.31549549102783 and batch: 200, loss is 3.5264820432662964 and perplexity is 34.00413188515006
At time: 86.78450846672058 and batch: 250, loss is 3.6620213985443115 and perplexity is 38.939976581501085
At time: 87.25446343421936 and batch: 300, loss is 3.6199883222579956 and perplexity is 37.3371318061154
At time: 87.72549176216125 and batch: 350, loss is 3.6131932592391967 and perplexity is 37.084283674937566
At time: 88.19606041908264 and batch: 400, loss is 3.5530680274963378 and perplexity is 34.92028971585259
At time: 88.66691064834595 and batch: 450, loss is 3.5798451137542724 and perplexity is 35.867984959276406
At time: 89.1387243270874 and batch: 500, loss is 3.4632742595672608 and perplexity is 31.921324292052287
At time: 89.60932779312134 and batch: 550, loss is 3.512259268760681 and perplexity is 33.5239218396904
At time: 90.07990217208862 and batch: 600, loss is 3.533179817199707 and perplexity is 34.23264829467098
At time: 90.55112886428833 and batch: 650, loss is 3.375047149658203 and perplexity is 29.225661728711074
At time: 91.02311253547668 and batch: 700, loss is 3.3722761058807373 and perplexity is 29.14478824436834
At time: 91.49402976036072 and batch: 750, loss is 3.47159414768219 and perplexity is 32.18801401460365
At time: 91.96654081344604 and batch: 800, loss is 3.4330523729324343 and perplexity is 30.971033763251455
At time: 92.43631529808044 and batch: 850, loss is 3.48174898147583 and perplexity is 32.516543203550604
At time: 92.90827703475952 and batch: 900, loss is 3.437290811538696 and perplexity is 31.102581169301267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326329061429795 and perplexity of 75.66601083749082
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.21890330314636 and batch: 50, loss is 3.720663461685181 and perplexity is 41.291780536982436
At time: 94.68895983695984 and batch: 100, loss is 3.631157546043396 and perplexity is 37.75649621453759
At time: 95.1585144996643 and batch: 150, loss is 3.638804898262024 and perplexity is 38.046340296971216
At time: 95.62695646286011 and batch: 200, loss is 3.5261140966415407 and perplexity is 33.991622481132794
At time: 96.09794497489929 and batch: 250, loss is 3.661413516998291 and perplexity is 38.91631288142669
At time: 96.56880354881287 and batch: 300, loss is 3.619581069946289 and perplexity is 37.32192926871923
At time: 97.03929662704468 and batch: 350, loss is 3.6099011182785032 and perplexity is 36.96239772859683
At time: 97.50936245918274 and batch: 400, loss is 3.549876356124878 and perplexity is 34.809013300220684
At time: 97.99532890319824 and batch: 450, loss is 3.5738021659851076 and perplexity is 35.651890181906666
At time: 98.46534895896912 and batch: 500, loss is 3.4558465385437014 and perplexity is 31.68509999067386
At time: 98.93534183502197 and batch: 550, loss is 3.500643672943115 and perplexity is 33.136774340701166
At time: 99.40443229675293 and batch: 600, loss is 3.5206871938705446 and perplexity is 33.80765289493916
At time: 99.87444519996643 and batch: 650, loss is 3.3606114387512207 and perplexity is 28.806799088784782
At time: 100.34403920173645 and batch: 700, loss is 3.3548949098587038 and perplexity is 28.64259397828929
At time: 100.81495785713196 and batch: 750, loss is 3.4513437271118166 and perplexity is 31.542748691262595
At time: 101.28813767433167 and batch: 800, loss is 3.4090143251419067 and perplexity is 30.23542726640378
At time: 101.75924634933472 and batch: 850, loss is 3.457495265007019 and perplexity is 31.737383141985497
At time: 102.2293906211853 and batch: 900, loss is 3.4187244415283202 and perplexity is 30.53044680314965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318445493097174 and perplexity of 75.07183784962218
finished 11 epochs...
Completing Train Step...
At time: 103.52054953575134 and batch: 50, loss is 3.710988802909851 and perplexity is 40.8942228683676
At time: 104.00236487388611 and batch: 100, loss is 3.618333306312561 and perplexity is 37.275389364075636
At time: 104.48517489433289 and batch: 150, loss is 3.625863676071167 and perplexity is 37.55714636462881
At time: 104.95765256881714 and batch: 200, loss is 3.5136382961273194 and perplexity is 33.57018413649957
At time: 105.42802596092224 and batch: 250, loss is 3.6493083572387697 and perplexity is 38.4480645252047
At time: 105.8979594707489 and batch: 300, loss is 3.60891574382782 and perplexity is 36.925993864903226
At time: 106.37015843391418 and batch: 350, loss is 3.5994010305404665 and perplexity is 36.57631978273487
At time: 106.84073090553284 and batch: 400, loss is 3.5404839754104613 and perplexity is 34.48360436950253
At time: 107.31154251098633 and batch: 450, loss is 3.565912103652954 and perplexity is 35.37170135332352
At time: 107.7813892364502 and batch: 500, loss is 3.4485096073150636 and perplexity is 31.453479322547945
At time: 108.25207471847534 and batch: 550, loss is 3.4943904781341555 and perplexity is 32.93021015156248
At time: 108.72184610366821 and batch: 600, loss is 3.5159854602813723 and perplexity is 33.649071413827485
At time: 109.19264960289001 and batch: 650, loss is 3.3571291160583496 and perplexity is 28.70665897989209
At time: 109.66142320632935 and batch: 700, loss is 3.353090090751648 and perplexity is 28.590945899141424
At time: 110.14664363861084 and batch: 750, loss is 3.4503665447235106 and perplexity is 31.511940727713935
At time: 110.61648607254028 and batch: 800, loss is 3.40931688785553 and perplexity is 30.244576763403582
At time: 111.08583664894104 and batch: 850, loss is 3.459622883796692 and perplexity is 31.80498007945096
At time: 111.55646824836731 and batch: 900, loss is 3.422645273208618 and perplexity is 30.65038652426628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317914988896618 and perplexity of 75.03202248634199
finished 12 epochs...
Completing Train Step...
At time: 112.8500623703003 and batch: 50, loss is 3.7055291986465453 and perplexity is 40.67156495995249
At time: 113.33475375175476 and batch: 100, loss is 3.612344207763672 and perplexity is 37.05281057219745
At time: 113.80468583106995 and batch: 150, loss is 3.619465665817261 and perplexity is 37.3176224124976
At time: 114.27701783180237 and batch: 200, loss is 3.507434129714966 and perplexity is 33.36255387995799
At time: 114.74544620513916 and batch: 250, loss is 3.642728018760681 and perplexity is 38.195893840962924
At time: 115.21611380577087 and batch: 300, loss is 3.6027870082855227 and perplexity is 36.700376295286546
At time: 115.68643927574158 and batch: 350, loss is 3.5936009073257447 and perplexity is 36.3647866733259
At time: 116.15732336044312 and batch: 400, loss is 3.535118913650513 and perplexity is 34.2990931021061
At time: 116.62966418266296 and batch: 450, loss is 3.561105422973633 and perplexity is 35.20208884298501
At time: 117.10200667381287 and batch: 500, loss is 3.4440075874328615 and perplexity is 31.312193407905784
At time: 117.57201480865479 and batch: 550, loss is 3.490413112640381 and perplexity is 32.79949479370007
At time: 118.04295945167542 and batch: 600, loss is 3.5127423667907713 and perplexity is 33.54012109289066
At time: 118.5121099948883 and batch: 650, loss is 3.3546757459640504 and perplexity is 28.636317243681958
At time: 118.98142004013062 and batch: 700, loss is 3.3512936544418337 and perplexity is 28.539630192296226
At time: 119.45097136497498 and batch: 750, loss is 3.4490669631958006 and perplexity is 31.47101499056791
At time: 119.92079854011536 and batch: 800, loss is 3.408764967918396 and perplexity is 30.227888784135526
At time: 120.39077877998352 and batch: 850, loss is 3.459833331108093 and perplexity is 31.811674056337864
At time: 120.86174130439758 and batch: 900, loss is 3.423519825935364 and perplexity is 30.677203628153315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318114398276969 and perplexity of 75.04698606734212
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 122.17003655433655 and batch: 50, loss is 3.7039510583877564 and perplexity is 40.60743014607792
At time: 122.64112210273743 and batch: 100, loss is 3.61353355884552 and perplexity is 37.09690558956741
At time: 123.1115312576294 and batch: 150, loss is 3.6224223136901856 and perplexity is 37.428120753272346
At time: 123.57956123352051 and batch: 200, loss is 3.50823100566864 and perplexity is 33.38915029251149
At time: 124.05005311965942 and batch: 250, loss is 3.641830267906189 and perplexity is 38.161618832140356
At time: 124.52077722549438 and batch: 300, loss is 3.603411431312561 and perplexity is 36.72330001164964
At time: 124.99082541465759 and batch: 350, loss is 3.5948017263412475 and perplexity is 36.408480429554196
At time: 125.46096062660217 and batch: 400, loss is 3.5344346141815186 and perplexity is 34.27563027962339
At time: 125.9289379119873 and batch: 450, loss is 3.5576992225646973 and perplexity is 35.08238745276808
At time: 126.39808201789856 and batch: 500, loss is 3.441284623146057 and perplexity is 31.2270474008925
At time: 126.86979866027832 and batch: 550, loss is 3.4867138051986695 and perplexity is 32.67838353053992
At time: 127.34123945236206 and batch: 600, loss is 3.508625864982605 and perplexity is 33.40233691274844
At time: 127.81031465530396 and batch: 650, loss is 3.3498017597198486 and perplexity is 28.497083813895088
At time: 128.27920198440552 and batch: 700, loss is 3.345496873855591 and perplexity is 28.374670796692637
At time: 128.74795174598694 and batch: 750, loss is 3.4412356328964235 and perplexity is 31.225517617517554
At time: 129.21942448616028 and batch: 800, loss is 3.4011951875686646 and perplexity is 29.999934177267487
At time: 129.68864035606384 and batch: 850, loss is 3.450212550163269 and perplexity is 31.507088433882238
At time: 130.15820002555847 and batch: 900, loss is 3.415730204582214 and perplexity is 30.439168134527748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31641628317637 and perplexity of 74.91965578837883
finished 14 epochs...
Completing Train Step...
At time: 131.4720697402954 and batch: 50, loss is 3.7010201263427733 and perplexity is 40.48858677379806
At time: 131.94104528427124 and batch: 100, loss is 3.608754277229309 and perplexity is 36.920032031608656
At time: 132.41052103042603 and batch: 150, loss is 3.618135404586792 and perplexity is 37.26801323009068
At time: 132.87955808639526 and batch: 200, loss is 3.5050147008895873 and perplexity is 33.28193312283148
At time: 133.35076594352722 and batch: 250, loss is 3.638920226097107 and perplexity is 38.05072835205809
At time: 133.8213973045349 and batch: 300, loss is 3.6001934719085695 and perplexity is 36.60531585895146
At time: 134.3069462776184 and batch: 350, loss is 3.591234316825867 and perplexity is 36.27882786941856
At time: 134.77741837501526 and batch: 400, loss is 3.5316725301742555 and perplexity is 34.18108873533856
At time: 135.24784135818481 and batch: 450, loss is 3.5557987213134767 and perplexity is 35.01577664852602
At time: 135.71715760231018 and batch: 500, loss is 3.4395009231567384 and perplexity is 31.171397363017668
At time: 136.1865017414093 and batch: 550, loss is 3.485197081565857 and perplexity is 32.6288570224641
At time: 136.656555891037 and batch: 600, loss is 3.507516121864319 and perplexity is 33.365289459605016
At time: 137.12623381614685 and batch: 650, loss is 3.3490052318572996 and perplexity is 28.474394130318142
At time: 137.5952537059784 and batch: 700, loss is 3.345037703514099 and perplexity is 28.361644980176372
At time: 138.06644749641418 and batch: 750, loss is 3.4413540506362916 and perplexity is 31.229215491682663
At time: 138.5352921485901 and batch: 800, loss is 3.4012651824951172 and perplexity is 30.002034093944708
At time: 139.00588989257812 and batch: 850, loss is 3.4513106155395508 and perplexity is 31.54170427855101
At time: 139.47632384300232 and batch: 900, loss is 3.4178878259658814 and perplexity is 30.504915237767573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315965208288741 and perplexity of 74.88586903381406
finished 15 epochs...
Completing Train Step...
At time: 140.7866415977478 and batch: 50, loss is 3.6992909908294678 and perplexity is 40.41863701425924
At time: 141.27295517921448 and batch: 100, loss is 3.6064769554138185 and perplexity is 36.836048901837316
At time: 141.741455078125 and batch: 150, loss is 3.616047978401184 and perplexity is 37.19030014177494
At time: 142.2126648426056 and batch: 200, loss is 3.50304603099823 and perplexity is 33.21647643561172
At time: 142.6835117340088 and batch: 250, loss is 3.6369666719436644 and perplexity is 37.976466754288296
At time: 143.15301775932312 and batch: 300, loss is 3.5981996059417725 and perplexity is 36.53240247936508
At time: 143.62262797355652 and batch: 350, loss is 3.58929940700531 and perplexity is 36.208699477032624
At time: 144.0917661190033 and batch: 400, loss is 3.529955940246582 and perplexity is 34.12246415427691
At time: 144.56045150756836 and batch: 450, loss is 3.5544865846633913 and perplexity is 34.96986129485118
At time: 145.03113746643066 and batch: 500, loss is 3.4383492946624754 and perplexity is 31.13552015618044
At time: 145.501309633255 and batch: 550, loss is 3.4841425800323487 and perplexity is 32.59446797753424
At time: 145.9716284275055 and batch: 600, loss is 3.5066894102096557 and perplexity is 33.33771738459578
At time: 146.45846819877625 and batch: 650, loss is 3.3484282064437867 and perplexity is 28.45796842075225
At time: 146.92793989181519 and batch: 700, loss is 3.344774394035339 and perplexity is 28.354178073316685
At time: 147.396169424057 and batch: 750, loss is 3.441293911933899 and perplexity is 31.227337463657904
At time: 147.86580610275269 and batch: 800, loss is 3.4012476301193235 and perplexity is 30.00150749158929
At time: 148.33776259422302 and batch: 850, loss is 3.4518180656433106 and perplexity is 31.55771418142981
At time: 148.80711030960083 and batch: 900, loss is 3.418876876831055 and perplexity is 30.53510107577943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31584690041738 and perplexity of 74.87700997011247
finished 16 epochs...
Completing Train Step...
At time: 150.10255479812622 and batch: 50, loss is 3.697805542945862 and perplexity is 40.35864180635737
At time: 150.59040784835815 and batch: 100, loss is 3.6047781038284303 and perplexity is 36.77352304787203
At time: 151.059916973114 and batch: 150, loss is 3.614385724067688 and perplexity is 37.128531755806506
At time: 151.53250908851624 and batch: 200, loss is 3.5014484119415283 and perplexity is 33.163451527941525
At time: 152.0022611618042 and batch: 250, loss is 3.6353282833099367 and perplexity is 37.914297485427504
At time: 152.47516202926636 and batch: 300, loss is 3.5965944433212282 and perplexity is 36.47380907102169
At time: 152.9457802772522 and batch: 350, loss is 3.5877649974823 and perplexity is 36.153183107055014
At time: 153.41696286201477 and batch: 400, loss is 3.5285604286193846 and perplexity is 34.07487906929315
At time: 153.88636875152588 and batch: 450, loss is 3.553335356712341 and perplexity is 34.92962617742381
At time: 154.35557675361633 and batch: 500, loss is 3.437326340675354 and perplexity is 31.103686236788967
At time: 154.8263807296753 and batch: 550, loss is 3.4832124662399293 and perplexity is 32.564165507868836
At time: 155.29533052444458 and batch: 600, loss is 3.5059423732757566 and perplexity is 33.312822178394605
At time: 155.76512122154236 and batch: 650, loss is 3.3478849267959596 and perplexity is 28.442511984654498
At time: 156.2356719970703 and batch: 700, loss is 3.3444635915756225 and perplexity is 28.345366894367228
At time: 156.7059063911438 and batch: 750, loss is 3.441096472740173 and perplexity is 31.221172571942525
At time: 157.1776397228241 and batch: 800, loss is 3.401150732040405 and perplexity is 29.99860054398979
At time: 157.64985418319702 and batch: 850, loss is 3.45204852104187 and perplexity is 31.564987665103715
At time: 158.13590574264526 and batch: 900, loss is 3.4193866443634033 and perplexity is 30.550670847049613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3158632043289815 and perplexity of 74.87823076821586
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 159.44866299629211 and batch: 50, loss is 3.6972311782836913 and perplexity is 40.335467884468585
At time: 159.91834115982056 and batch: 100, loss is 3.6046538877487184 and perplexity is 36.76895546869109
At time: 160.38797068595886 and batch: 150, loss is 3.615083246231079 and perplexity is 37.15443876390612
At time: 160.8583014011383 and batch: 200, loss is 3.5016688013076784 and perplexity is 33.170761205461126
At time: 161.33055710792542 and batch: 250, loss is 3.634927053451538 and perplexity is 37.89908818863218
At time: 161.8008852005005 and batch: 300, loss is 3.596367292404175 and perplexity is 36.46552495275093
At time: 162.27117013931274 and batch: 350, loss is 3.588036575317383 and perplexity is 36.16300284360563
At time: 162.74176716804504 and batch: 400, loss is 3.528046488761902 and perplexity is 34.05737113019977
At time: 163.21060132980347 and batch: 450, loss is 3.552229242324829 and perplexity is 34.89101137548961
At time: 163.6811761856079 and batch: 500, loss is 3.436424651145935 and perplexity is 31.075653009116216
At time: 164.15209555625916 and batch: 550, loss is 3.481819887161255 and perplexity is 32.51884889307638
At time: 164.62203884124756 and batch: 600, loss is 3.504417953491211 and perplexity is 33.26207814062119
At time: 165.09177231788635 and batch: 650, loss is 3.3465055418014527 and perplexity is 28.40330585680547
At time: 165.56327772140503 and batch: 700, loss is 3.342983822822571 and perplexity is 28.303453324986542
At time: 166.03285765647888 and batch: 750, loss is 3.438758683204651 and perplexity is 31.148269290883835
At time: 166.50516486167908 and batch: 800, loss is 3.398473105430603 and perplexity is 29.918382937243486
At time: 166.97377610206604 and batch: 850, loss is 3.4485234928131105 and perplexity is 31.45391607280589
At time: 167.44352793693542 and batch: 900, loss is 3.415980749130249 and perplexity is 30.44679545760278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315499083636558 and perplexity of 74.8509710182018
finished 18 epochs...
Completing Train Step...
At time: 168.74978399276733 and batch: 50, loss is 3.6966951656341553 and perplexity is 40.31385335680519
At time: 169.21909999847412 and batch: 100, loss is 3.603813247680664 and perplexity is 36.738058999687986
At time: 169.68942523002625 and batch: 150, loss is 3.6140911293029787 and perplexity is 37.11759549569149
At time: 170.16088223457336 and batch: 200, loss is 3.501046814918518 and perplexity is 33.15013585848073
At time: 170.64904236793518 and batch: 250, loss is 3.6343681240081787 and perplexity is 37.8779111911421
At time: 171.12040376663208 and batch: 300, loss is 3.5958043146133423 and perplexity is 36.445001449751736
At time: 171.59077072143555 and batch: 350, loss is 3.587266354560852 and perplexity is 36.13516007211371
At time: 172.06181001663208 and batch: 400, loss is 3.5275507402420043 and perplexity is 34.04049142326094
At time: 172.53083157539368 and batch: 450, loss is 3.5518115758895874 and perplexity is 34.87644161400841
At time: 173.0014259815216 and batch: 500, loss is 3.4360038661956787 and perplexity is 31.062579592751916
At time: 173.47222208976746 and batch: 550, loss is 3.4814682149887086 and perplexity is 32.507414929458704
At time: 173.94455814361572 and batch: 600, loss is 3.504224429130554 and perplexity is 33.25564174103505
At time: 174.4155089855194 and batch: 650, loss is 3.346291489601135 and perplexity is 28.39722671734033
At time: 174.886150598526 and batch: 700, loss is 3.342816877365112 and perplexity is 28.298728586421333
At time: 175.35611701011658 and batch: 750, loss is 3.4386843490600585 and perplexity is 31.14595399698432
At time: 175.825270652771 and batch: 800, loss is 3.398567752838135 and perplexity is 29.921214768636677
At time: 176.29587602615356 and batch: 850, loss is 3.448849234580994 and perplexity is 31.46416359596684
At time: 176.76796865463257 and batch: 900, loss is 3.4165675115585326 and perplexity is 30.46466573553005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315298002060145 and perplexity of 74.83592138010661
finished 19 epochs...
Completing Train Step...
At time: 178.06453680992126 and batch: 50, loss is 3.6962493753433225 and perplexity is 40.295885837563034
At time: 178.5540807247162 and batch: 100, loss is 3.6031664180755616 and perplexity is 36.714303419227605
At time: 179.02509713172913 and batch: 150, loss is 3.613401479721069 and perplexity is 37.092006186318876
At time: 179.49480509757996 and batch: 200, loss is 3.5005295419692994 and perplexity is 33.132992624186144
At time: 179.96473717689514 and batch: 250, loss is 3.633885555267334 and perplexity is 37.859636904887054
At time: 180.43691444396973 and batch: 300, loss is 3.5953117036819457 and perplexity is 36.4270526648896
At time: 180.90692710876465 and batch: 350, loss is 3.5866954851150514 and perplexity is 36.11453750026757
At time: 181.37730312347412 and batch: 400, loss is 3.527125835418701 and perplexity is 34.0260305267375
At time: 181.84566450119019 and batch: 450, loss is 3.551474919319153 and perplexity is 34.86470220697081
At time: 182.31398272514343 and batch: 500, loss is 3.4356851625442504 and perplexity is 31.052681412589784
At time: 182.79963517189026 and batch: 550, loss is 3.481196975708008 and perplexity is 32.498598837305096
At time: 183.26998353004456 and batch: 600, loss is 3.5040435361862183 and perplexity is 33.24962657415177
At time: 183.9336919784546 and batch: 650, loss is 3.346135902404785 and perplexity is 28.392808816144605
At time: 184.40515804290771 and batch: 700, loss is 3.342720694541931 and perplexity is 28.296006865706946
At time: 184.87717390060425 and batch: 750, loss is 3.4386588621139524 and perplexity is 31.145160191849254
At time: 185.34879851341248 and batch: 800, loss is 3.3986229848861695 and perplexity is 29.92286742424739
At time: 185.8205280303955 and batch: 850, loss is 3.4490654277801513 and perplexity is 31.470966669516088
At time: 186.29294419288635 and batch: 900, loss is 3.416971011161804 and perplexity is 30.476960696407243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315179694188784 and perplexity of 74.82706822525608
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f189bae2b70>
ELAPSED
1145.1559383869171


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.86398936205909}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5386602506977849, 'wordvec_dim': 300, 'rnn_dropout': 0.729059618910752, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.25572014345254}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.94588761585722, 'wordvec_dim': 300, 'rnn_dropout': 0.2935830384905056, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.9551185187551}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5759509205725062, 'wordvec_dim': 300, 'rnn_dropout': 0.2773236962555252, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.7905404894792}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.24459635961389775, 'wordvec_dim': 300, 'rnn_dropout': 0.2836115397488401, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.82706822525608}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.30892150814954267, 'wordvec_dim': 300, 'rnn_dropout': 0.209604656630069, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.72975261882165}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4136984347733138, 'wordvec_dim': 300, 'rnn_dropout': 0.16690666398185616, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.86398936205909}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5386602506977849, 'wordvec_dim': 300, 'rnn_dropout': 0.729059618910752, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.25572014345254}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.94588761585722, 'wordvec_dim': 300, 'rnn_dropout': 0.2935830384905056, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.9551185187551}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.5759509205725062, 'wordvec_dim': 300, 'rnn_dropout': 0.2773236962555252, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.7905404894792}, {'params': {'seq_len': 35, 'tune_wordvecs': 'FALSE', 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.24459635961389775, 'wordvec_dim': 300, 'rnn_dropout': 0.2836115397488401, 'num_layers': 1, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.82706822525608}]
Exception ignored in: <bound method DropoutDescriptor.__del__ of <torch.backends.cudnn.DropoutDescriptor object at 0x7f188d121e10>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 215, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroyDropoutDescriptor'
Exception ignored in: <bound method CuDNNHandle.__del__ of <torch.backends.cudnn.CuDNNHandle object at 0x7f188c5ab588>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 91, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroy'
