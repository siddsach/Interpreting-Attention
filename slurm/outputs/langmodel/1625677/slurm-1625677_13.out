FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.121455430984497 and batch: 50, loss is 10.190164546966553 and perplexity is 26639.87805067687
At time: 1.7750380039215088 and batch: 100, loss is 9.474447116851806 and perplexity is 13022.671992320473
At time: 2.450319528579712 and batch: 150, loss is 8.948537693023681 and perplexity is 7696.628794105058
At time: 3.1082940101623535 and batch: 200, loss is 8.468453578948974 and perplexity is 4762.145490380608
At time: 3.753229856491089 and batch: 250, loss is 8.225262298583985 and perplexity is 3734.1007240754557
At time: 4.399036407470703 and batch: 300, loss is 7.935952339172363 and perplexity is 2796.020231422176
At time: 5.043199777603149 and batch: 350, loss is 7.777247915267944 and perplexity is 2385.7001287330704
At time: 5.687625408172607 and batch: 400, loss is 7.567651081085205 and perplexity is 1934.590743612637
At time: 6.331047058105469 and batch: 450, loss is 7.474775857925415 and perplexity is 1763.0064799669706
At time: 6.975186109542847 and batch: 500, loss is 7.392171936035156 and perplexity is 1623.2278340988707
At time: 7.620710849761963 and batch: 550, loss is 7.3364826011657716 and perplexity is 1535.3023344268345
At time: 8.271058082580566 and batch: 600, loss is 7.236879072189331 and perplexity is 1389.7498865349112
At time: 8.925137996673584 and batch: 650, loss is 7.132069206237793 and perplexity is 1251.4638263732988
At time: 9.574490785598755 and batch: 700, loss is 7.1968326759338375 and perplexity is 1335.1950645574323
At time: 10.233384609222412 and batch: 750, loss is 7.104948787689209 and perplexity is 1217.979707581831
At time: 10.877665281295776 and batch: 800, loss is 7.091827173233032 and perplexity is 1202.1022442300207
At time: 11.522255182266235 and batch: 850, loss is 7.11822325706482 and perplexity is 1234.2555293608627
At time: 12.180377960205078 and batch: 900, loss is 6.960248250961303 and perplexity is 1053.895155255402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.418186971586045 and perplexity of 612.8909175600286
finished 1 epochs...
Completing Train Step...
At time: 13.808706283569336 and batch: 50, loss is 5.799612426757813 and perplexity is 330.17156944276377
At time: 14.443777322769165 and batch: 100, loss is 5.400814990997315 and perplexity is 221.58693399033473
At time: 15.08195686340332 and batch: 150, loss is 5.260844068527222 and perplexity is 192.64402745229896
At time: 15.729314804077148 and batch: 200, loss is 5.081491270065308 and perplexity is 161.0139923260093
At time: 16.367656469345093 and batch: 250, loss is 5.1263931846618656 and perplexity is 168.40860257227294
At time: 17.00304627418518 and batch: 300, loss is 5.04658203125 and perplexity is 155.49009490805793
At time: 17.63772940635681 and batch: 350, loss is 5.0030289649963375 and perplexity is 148.8633788722391
At time: 18.274869203567505 and batch: 400, loss is 4.862348842620849 and perplexity is 129.3276158484765
At time: 18.909306287765503 and batch: 450, loss is 4.8519541358947755 and perplexity is 127.99025598372134
At time: 19.551411390304565 and batch: 500, loss is 4.763532581329346 and perplexity is 117.159069679461
At time: 20.199926376342773 and batch: 550, loss is 4.830675592422486 and perplexity is 125.29558081214569
At time: 20.84708261489868 and batch: 600, loss is 4.77061653137207 and perplexity is 117.99196528851314
At time: 21.4901442527771 and batch: 650, loss is 4.635005321502685 and perplexity is 103.02846626003037
At time: 22.127933502197266 and batch: 700, loss is 4.681270418167114 and perplexity is 107.90707263472497
At time: 22.763261556625366 and batch: 750, loss is 4.720921878814697 and perplexity is 112.27170588196174
At time: 23.400174856185913 and batch: 800, loss is 4.649698390960693 and perplexity is 104.55344655655428
At time: 24.05942940711975 and batch: 850, loss is 4.704207706451416 and perplexity is 110.41077254151361
At time: 24.695523738861084 and batch: 900, loss is 4.650804920196533 and perplexity is 104.66920203347568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.733314827696918 and perplexity of 113.67174075551557
finished 2 epochs...
Completing Train Step...
At time: 26.285328149795532 and batch: 50, loss is 4.680736446380616 and perplexity is 107.84946868318556
At time: 26.935879230499268 and batch: 100, loss is 4.513520126342773 and perplexity is 91.24243877985455
At time: 27.574727296829224 and batch: 150, loss is 4.502217683792114 and perplexity is 90.21698235493407
At time: 28.21310043334961 and batch: 200, loss is 4.394959106445312 and perplexity is 81.04131662745108
At time: 28.855884790420532 and batch: 250, loss is 4.530919342041016 and perplexity is 92.84387713828825
At time: 29.51336359977722 and batch: 300, loss is 4.5008736515045165 and perplexity is 90.09580926627198
At time: 30.15405511856079 and batch: 350, loss is 4.476515779495239 and perplexity is 87.92777857371465
At time: 30.79312252998352 and batch: 400, loss is 4.392928805351257 and perplexity is 80.87694527177358
At time: 31.44427490234375 and batch: 450, loss is 4.405524282455445 and perplexity is 81.90207140794136
At time: 32.08128070831299 and batch: 500, loss is 4.300330152511597 and perplexity is 73.72412988869581
At time: 32.71680784225464 and batch: 550, loss is 4.3866684532165525 and perplexity is 80.37220867766924
At time: 33.36728835105896 and batch: 600, loss is 4.376000919342041 and perplexity is 79.51939223046341
At time: 34.00280475616455 and batch: 650, loss is 4.230471992492676 and perplexity is 68.74967384706083
At time: 34.64818072319031 and batch: 700, loss is 4.254635910987854 and perplexity is 70.43116930480312
At time: 35.28498148918152 and batch: 750, loss is 4.344014163017273 and perplexity is 77.01607474726873
At time: 35.92047071456909 and batch: 800, loss is 4.287454519271851 and perplexity is 72.78096995191608
At time: 36.55601763725281 and batch: 850, loss is 4.353385143280029 and perplexity is 77.74118304512338
At time: 37.19229030609131 and batch: 900, loss is 4.315577359199524 and perplexity is 74.85683024941109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.579526300299658 and perplexity of 97.46821260670774
finished 3 epochs...
Completing Train Step...
At time: 38.79277181625366 and batch: 50, loss is 4.373346500396728 and perplexity is 79.3085943459287
At time: 39.43356919288635 and batch: 100, loss is 4.218670926094055 and perplexity is 67.94312282255544
At time: 40.089412212371826 and batch: 150, loss is 4.214519839286805 and perplexity is 67.66166959431014
At time: 40.730745792388916 and batch: 200, loss is 4.112391438484192 and perplexity is 61.092642310979485
At time: 41.396347522735596 and batch: 250, loss is 4.260778613090515 and perplexity is 70.86513850376859
At time: 42.03943610191345 and batch: 300, loss is 4.240408425331116 and perplexity is 69.43620555341796
At time: 42.682945013046265 and batch: 350, loss is 4.2179893159866335 and perplexity is 67.89682788265164
At time: 43.32668685913086 and batch: 400, loss is 4.148248310089111 and perplexity is 63.32298086399297
At time: 43.97029948234558 and batch: 450, loss is 4.165133166313171 and perplexity is 64.40125795055026
At time: 44.61405372619629 and batch: 500, loss is 4.056997585296631 and perplexity is 57.80050919818059
At time: 45.2564492225647 and batch: 550, loss is 4.143738307952881 and perplexity is 63.03803711646123
At time: 45.90039801597595 and batch: 600, loss is 4.150119686126709 and perplexity is 63.44159292227751
At time: 46.542990922927856 and batch: 650, loss is 4.0019376039505 and perplexity is 54.70404217972992
At time: 47.18572998046875 and batch: 700, loss is 4.016945171356201 and perplexity is 55.53120812952415
At time: 47.82919716835022 and batch: 750, loss is 4.123246726989746 and perplexity is 61.75943312443598
At time: 48.47277474403381 and batch: 800, loss is 4.068658356666565 and perplexity is 58.47845271131021
At time: 49.11536407470703 and batch: 850, loss is 4.140749750137329 and perplexity is 62.84992552914651
At time: 49.75802540779114 and batch: 900, loss is 4.1067531251907345 and perplexity is 60.749152116349144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.524875170563998 and perplexity of 92.28440529935695
finished 4 epochs...
Completing Train Step...
At time: 51.365824460983276 and batch: 50, loss is 4.171314158439636 and perplexity is 64.80055437164654
At time: 52.00928235054016 and batch: 100, loss is 4.025760135650635 and perplexity is 56.0228775870662
At time: 52.65118741989136 and batch: 150, loss is 4.022690153121948 and perplexity is 55.851152063718445
At time: 53.29451656341553 and batch: 200, loss is 3.9221078681945802 and perplexity is 50.50679431863603
At time: 53.938830852508545 and batch: 250, loss is 4.069605884552002 and perplexity is 58.533888935487916
At time: 54.58242869377136 and batch: 300, loss is 4.06037299156189 and perplexity is 57.99593904107104
At time: 55.22648572921753 and batch: 350, loss is 4.03604811668396 and perplexity is 56.60221488481056
At time: 55.8772509098053 and batch: 400, loss is 3.9713312196731567 and perplexity is 53.0551118585
At time: 56.54302263259888 and batch: 450, loss is 3.9893020963668824 and perplexity is 54.01717742083368
At time: 57.18668174743652 and batch: 500, loss is 3.882105002403259 and perplexity is 48.52625552046287
At time: 57.831568479537964 and batch: 550, loss is 3.9707355546951293 and perplexity is 53.023518197013566
At time: 58.475871086120605 and batch: 600, loss is 3.983626546859741 and perplexity is 53.71146860951283
At time: 59.12264609336853 and batch: 650, loss is 3.8340015268325804 and perplexity is 46.24722799100322
At time: 59.777852058410645 and batch: 700, loss is 3.8472779512405397 and perplexity is 46.86531976295845
At time: 60.420268058776855 and batch: 750, loss is 3.9575939893722536 and perplexity is 52.331264778433365
At time: 61.07216000556946 and batch: 800, loss is 3.903551321029663 and perplexity is 49.57820495996886
At time: 61.71977519989014 and batch: 850, loss is 3.9786586236953734 and perplexity is 53.445295870706985
At time: 62.378483295440674 and batch: 900, loss is 3.9506667613983155 and perplexity is 51.9700068796833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.50645425874893 and perplexity of 90.60000414041973
finished 5 epochs...
Completing Train Step...
At time: 64.00283527374268 and batch: 50, loss is 4.013899025917053 and perplexity is 55.36230936884575
At time: 64.65204739570618 and batch: 100, loss is 3.8771556568145753 and perplexity is 48.28667568253686
At time: 65.28859162330627 and batch: 150, loss is 3.87315363407135 and perplexity is 48.093817477127075
At time: 65.9258623123169 and batch: 200, loss is 3.777103214263916 and perplexity is 43.68929971173433
At time: 66.56224703788757 and batch: 250, loss is 3.9231302690505983 and perplexity is 50.558458914845396
At time: 67.19819402694702 and batch: 300, loss is 3.915333890914917 and perplexity is 50.16581862610507
At time: 67.83499360084534 and batch: 350, loss is 3.892857885360718 and perplexity is 49.050868160768324
At time: 68.47166562080383 and batch: 400, loss is 3.8320015239715577 and perplexity is 46.15482583579504
At time: 69.10801339149475 and batch: 450, loss is 3.8529330110549926 and perplexity is 47.13109673348563
At time: 69.74427151679993 and batch: 500, loss is 3.747070589065552 and perplexity is 42.39670254572032
At time: 70.38020706176758 and batch: 550, loss is 3.833373990058899 and perplexity is 46.21821525898825
At time: 71.01582741737366 and batch: 600, loss is 3.8507616329193115 and perplexity is 47.028868328955326
At time: 71.65337777137756 and batch: 650, loss is 3.7034585952758787 and perplexity is 40.5874374079099
At time: 72.30740308761597 and batch: 700, loss is 3.7096506118774415 and perplexity is 40.83953518549222
At time: 72.97003149986267 and batch: 750, loss is 3.8248958206176757 and perplexity is 45.82802578221204
At time: 73.61372089385986 and batch: 800, loss is 3.7723877143859865 and perplexity is 43.48376779798338
At time: 74.2531886100769 and batch: 850, loss is 3.8493176031112672 and perplexity is 46.9610062504741
At time: 74.9060423374176 and batch: 900, loss is 3.820284266471863 and perplexity is 45.61717391100417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.506051677547089 and perplexity of 90.56353762272589
finished 6 epochs...
Completing Train Step...
At time: 76.517254114151 and batch: 50, loss is 3.8826882600784303 and perplexity is 48.55456708710916
At time: 77.18199014663696 and batch: 100, loss is 3.753977828025818 and perplexity is 42.69056040623468
At time: 77.81758689880371 and batch: 150, loss is 3.7489879322052 and perplexity is 42.47806955179871
At time: 78.45162510871887 and batch: 200, loss is 3.6576436281204225 and perplexity is 38.769878899674296
At time: 79.08607411384583 and batch: 250, loss is 3.7999229621887207 and perplexity is 44.6977409445293
At time: 79.72100448608398 and batch: 300, loss is 3.796754207611084 and perplexity is 44.556328941593655
At time: 80.3556432723999 and batch: 350, loss is 3.769228777885437 and perplexity is 43.34662206807767
At time: 80.99193120002747 and batch: 400, loss is 3.7164085245132448 and perplexity is 41.116459858823006
At time: 81.62604284286499 and batch: 450, loss is 3.7374510049819945 and perplexity is 41.99081924489645
At time: 82.28213953971863 and batch: 500, loss is 3.631350517272949 and perplexity is 37.76378283506719
At time: 82.91726517677307 and batch: 550, loss is 3.719173059463501 and perplexity is 41.23028501344877
At time: 83.55197811126709 and batch: 600, loss is 3.736517767906189 and perplexity is 41.95165013540907
At time: 84.18803524971008 and batch: 650, loss is 3.5908339691162108 and perplexity is 36.26430663073831
At time: 84.82461190223694 and batch: 700, loss is 3.5962663507461547 and perplexity is 36.46184424797297
At time: 85.46200323104858 and batch: 750, loss is 3.7126483488082886 and perplexity is 40.96214505260106
At time: 86.098637342453 and batch: 800, loss is 3.6598981714248655 and perplexity is 38.857385877630996
At time: 86.73600745201111 and batch: 850, loss is 3.739703531265259 and perplexity is 42.08551127700901
At time: 87.37509727478027 and batch: 900, loss is 3.708675675392151 and perplexity is 40.79973863530418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.517686817744007 and perplexity of 91.6234110106237
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.97002220153809 and batch: 50, loss is 3.7839139556884764 and perplexity is 43.98787182970254
At time: 89.60788178443909 and batch: 100, loss is 3.659363718032837 and perplexity is 38.836623964574976
At time: 90.24461960792542 and batch: 150, loss is 3.654357352256775 and perplexity is 38.642679503038565
At time: 90.88303017616272 and batch: 200, loss is 3.5463511991500853 and perplexity is 34.686522091416386
At time: 91.52100729942322 and batch: 250, loss is 3.6768905544281005 and perplexity is 39.523307233169355
At time: 92.15730667114258 and batch: 300, loss is 3.6560745429992676 and perplexity is 38.709093360857416
At time: 92.79397869110107 and batch: 350, loss is 3.616831407546997 and perplexity is 37.219447522810654
At time: 93.43110489845276 and batch: 400, loss is 3.5604993057250978 and perplexity is 35.18075871468517
At time: 94.06814098358154 and batch: 450, loss is 3.560414319038391 and perplexity is 35.17776894561326
At time: 94.7087950706482 and batch: 500, loss is 3.44070508480072 and perplexity is 31.208955372530493
At time: 95.3457624912262 and batch: 550, loss is 3.50870174407959 and perplexity is 33.40487154807226
At time: 95.98216009140015 and batch: 600, loss is 3.512900695800781 and perplexity is 33.54543188747424
At time: 96.61858749389648 and batch: 650, loss is 3.351857261657715 and perplexity is 28.5557198675138
At time: 97.25607895851135 and batch: 700, loss is 3.3403917932510376 and perplexity is 28.23018493493036
At time: 97.89331030845642 and batch: 750, loss is 3.4359214448928834 and perplexity is 31.060019479979065
At time: 98.5312147140503 and batch: 800, loss is 3.3604790115356447 and perplexity is 28.802984537172595
At time: 99.16878390312195 and batch: 850, loss is 3.4201260948181154 and perplexity is 30.573269908912767
At time: 99.8066737651825 and batch: 900, loss is 3.376024422645569 and perplexity is 29.254237139170534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.46654813583583 and perplexity of 87.05569931712711
finished 8 epochs...
Completing Train Step...
At time: 101.4013123512268 and batch: 50, loss is 3.672934527397156 and perplexity is 39.36726082681584
At time: 102.03648447990417 and batch: 100, loss is 3.5497183656692504 and perplexity is 34.80351424276017
At time: 102.67282223701477 and batch: 150, loss is 3.5437580299377442 and perplexity is 34.59669059472102
At time: 103.30762481689453 and batch: 200, loss is 3.4425512838363646 and perplexity is 31.266626535684853
At time: 103.94310069084167 and batch: 250, loss is 3.574746317863464 and perplexity is 35.6855668764444
At time: 104.57919669151306 and batch: 300, loss is 3.562503652572632 and perplexity is 35.25134387244726
At time: 105.23825120925903 and batch: 350, loss is 3.5263211059570314 and perplexity is 33.998659792004354
At time: 105.87838506698608 and batch: 400, loss is 3.47647385597229 and perplexity is 32.345465980833815
At time: 106.51511406898499 and batch: 450, loss is 3.4835240840911865 and perplexity is 32.57431466439943
At time: 107.16404390335083 and batch: 500, loss is 3.3686538124084473 and perplexity is 29.03940824203109
At time: 107.80108571052551 and batch: 550, loss is 3.441457209587097 and perplexity is 31.23243723096003
At time: 108.43715834617615 and batch: 600, loss is 3.4537938165664674 and perplexity is 31.620125999157214
At time: 109.07377052307129 and batch: 650, loss is 3.2995114803314207 and perplexity is 27.099397097986817
At time: 109.71135377883911 and batch: 700, loss is 3.2944617319107055 and perplexity is 26.962896896190486
At time: 110.35842418670654 and batch: 750, loss is 3.3979343223571776 and perplexity is 29.90226776061467
At time: 110.99427723884583 and batch: 800, loss is 3.3291224098205565 and perplexity is 27.913834044341105
At time: 111.62979102134705 and batch: 850, loss is 3.397722115516663 and perplexity is 29.895922968076963
At time: 112.26590394973755 and batch: 900, loss is 3.3640724754333498 and perplexity is 28.906673211429375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.474855187821062 and perplexity of 87.78188760287729
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 113.88557696342468 and batch: 50, loss is 3.6284611511230467 and perplexity is 37.65482692181602
At time: 114.53712892532349 and batch: 100, loss is 3.5127039194107055 and perplexity is 33.53883158789675
At time: 115.17568635940552 and batch: 150, loss is 3.5105784225463865 and perplexity is 33.46762061268538
At time: 115.81476163864136 and batch: 200, loss is 3.4059584665298464 and perplexity is 30.143173105230066
At time: 116.45335721969604 and batch: 250, loss is 3.5354571866989137 and perplexity is 34.310697523507976
At time: 117.09203934669495 and batch: 300, loss is 3.5209678745269777 and perplexity is 33.81714338098179
At time: 117.73064017295837 and batch: 350, loss is 3.47928062915802 and perplexity is 32.43637989511238
At time: 118.36939311027527 and batch: 400, loss is 3.4300478219985964 and perplexity is 30.87811936775041
At time: 119.00848627090454 and batch: 450, loss is 3.42870623588562 and perplexity is 30.836721487231788
At time: 119.65214610099792 and batch: 500, loss is 3.307722873687744 and perplexity is 27.322837028368948
At time: 120.2919852733612 and batch: 550, loss is 3.3737007761001587 and perplexity is 29.186339547655614
At time: 120.93342542648315 and batch: 600, loss is 3.3808065462112427 and perplexity is 29.394469553110874
At time: 121.58861136436462 and batch: 650, loss is 3.2208395338058473 and perplexity is 25.049140956918063
At time: 122.23049259185791 and batch: 700, loss is 3.2089784479141237 and perplexity is 24.753786022306276
At time: 122.86972856521606 and batch: 750, loss is 3.3040242099761965 and perplexity is 27.221965702106825
At time: 123.50640082359314 and batch: 800, loss is 3.2289549922943115 and perplexity is 25.25325333303644
At time: 124.14479398727417 and batch: 850, loss is 3.291563730239868 and perplexity is 26.88487148942177
At time: 124.78413486480713 and batch: 900, loss is 3.256474242210388 and perplexity is 25.957854504385416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.466092044360017 and perplexity of 87.01600300800983
finished 10 epochs...
Completing Train Step...
At time: 126.37665843963623 and batch: 50, loss is 3.59864399433136 and perplexity is 36.548640662633474
At time: 127.03229522705078 and batch: 100, loss is 3.4796172904968263 and perplexity is 32.447301808583745
At time: 127.67653799057007 and batch: 150, loss is 3.475139088630676 and perplexity is 32.30232110978133
At time: 128.3222532272339 and batch: 200, loss is 3.3714133501052856 and perplexity is 29.119654253792994
At time: 128.9677927494049 and batch: 250, loss is 3.5006268405914307 and perplexity is 33.136216575556034
At time: 129.61257100105286 and batch: 300, loss is 3.4890463733673096 and perplexity is 32.754697056415495
At time: 130.25702214241028 and batch: 350, loss is 3.4491594409942627 and perplexity is 31.473925495326064
At time: 130.9019742012024 and batch: 400, loss is 3.401444425582886 and perplexity is 30.00741223315779
At time: 131.5450839996338 and batch: 450, loss is 3.403438768386841 and perplexity is 30.067317015290424
At time: 132.1891851425171 and batch: 500, loss is 3.2852337741851807 and perplexity is 26.715228915816805
At time: 132.83211207389832 and batch: 550, loss is 3.3535212326049804 and perplexity is 28.603275310216496
At time: 133.47808980941772 and batch: 600, loss is 3.3643052530288697 and perplexity is 28.91340282053474
At time: 134.13126134872437 and batch: 650, loss is 3.20770432472229 and perplexity is 24.72226673344204
At time: 134.77540826797485 and batch: 700, loss is 3.1990898275375366 and perplexity is 24.510211522151817
At time: 135.43129682540894 and batch: 750, loss is 3.297995381355286 and perplexity is 27.058342858801474
At time: 136.07889366149902 and batch: 800, loss is 3.22657066822052 and perplexity is 25.193113118544964
At time: 136.722918510437 and batch: 850, loss is 3.2931057691574095 and perplexity is 26.926360988541276
At time: 137.39375686645508 and batch: 900, loss is 3.261806640625 and perplexity is 26.096641832393363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.468870398116438 and perplexity of 87.258100407211
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 139.0208911895752 and batch: 50, loss is 3.584650731086731 and perplexity is 36.040767600585525
At time: 139.66848230361938 and batch: 100, loss is 3.46849711894989 and perplexity is 32.08848101819619
At time: 140.31837701797485 and batch: 150, loss is 3.4649166107177733 and perplexity is 31.973793390277326
At time: 140.96991443634033 and batch: 200, loss is 3.359359230995178 and perplexity is 28.770749566994983
At time: 141.61409735679626 and batch: 250, loss is 3.487861361503601 and perplexity is 32.71590534067259
At time: 142.2605881690979 and batch: 300, loss is 3.475480737686157 and perplexity is 32.313359052722355
At time: 142.90531086921692 and batch: 350, loss is 3.434659733772278 and perplexity is 31.020855420055405
At time: 143.55043363571167 and batch: 400, loss is 3.3867327117919923 and perplexity is 29.56918322659986
At time: 144.1956329345703 and batch: 450, loss is 3.3863434553146363 and perplexity is 29.557675470378413
At time: 144.84071683883667 and batch: 500, loss is 3.2657418203353883 and perplexity is 26.199539134743855
At time: 145.4848916530609 and batch: 550, loss is 3.333211803436279 and perplexity is 28.028218421036822
At time: 146.13037371635437 and batch: 600, loss is 3.342778458595276 and perplexity is 28.29764140496534
At time: 146.77541041374207 and batch: 650, loss is 3.183067145347595 and perplexity is 24.12062166455648
At time: 147.4204626083374 and batch: 700, loss is 3.1713503932952882 and perplexity is 23.839655540668176
At time: 148.0660400390625 and batch: 750, loss is 3.2677709674835205 and perplexity is 26.25275582885334
At time: 148.71128582954407 and batch: 800, loss is 3.195609211921692 and perplexity is 24.425049191824787
At time: 149.35722947120667 and batch: 850, loss is 3.259205183982849 and perplexity is 26.028840779148197
At time: 150.00811982154846 and batch: 900, loss is 3.2275172424316407 and perplexity is 25.21697155983201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.468344492455051 and perplexity of 87.21222294287145
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.64413166046143 and batch: 50, loss is 3.5775303745269778 and perplexity is 35.78505594403741
At time: 152.28568077087402 and batch: 100, loss is 3.461059684753418 and perplexity is 31.85071035003892
At time: 152.92880654335022 and batch: 150, loss is 3.4578765296936034 and perplexity is 31.749485792432136
At time: 153.58406734466553 and batch: 200, loss is 3.3528081130981446 and perplexity is 28.582885027851543
At time: 154.23965215682983 and batch: 250, loss is 3.4811478662490845 and perplexity is 32.49700288788885
At time: 154.88595008850098 and batch: 300, loss is 3.4687261724472047 and perplexity is 32.09583183883037
At time: 155.52928471565247 and batch: 350, loss is 3.428120884895325 and perplexity is 30.81867646362324
At time: 156.16638851165771 and batch: 400, loss is 3.3799170303344725 and perplexity is 29.368334331324775
At time: 156.8037462234497 and batch: 450, loss is 3.3799269580841065 and perplexity is 29.36862589424246
At time: 157.44126152992249 and batch: 500, loss is 3.259277400970459 and perplexity is 26.030720571495902
At time: 158.07703495025635 and batch: 550, loss is 3.326441922187805 and perplexity is 27.839111548505738
At time: 158.71286916732788 and batch: 600, loss is 3.3363171434402465 and perplexity is 28.115390848902056
At time: 159.34942483901978 and batch: 650, loss is 3.17600061416626 and perplexity is 23.950773365394625
At time: 159.9842689037323 and batch: 700, loss is 3.1636908864974975 and perplexity is 23.65775306793166
At time: 160.62073230743408 and batch: 750, loss is 3.259717116355896 and perplexity is 26.04216919670905
At time: 161.257009267807 and batch: 800, loss is 3.1876511144638062 and perplexity is 24.231443657570928
At time: 161.89314556121826 and batch: 850, loss is 3.2503138637542723 and perplexity is 25.79843584054542
At time: 162.53087425231934 and batch: 900, loss is 3.218128490447998 and perplexity is 24.98132361911531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4679761912724745 and perplexity of 87.18010849228453
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.10283708572388 and batch: 50, loss is 3.575538444519043 and perplexity is 35.71384556385905
At time: 164.75301051139832 and batch: 100, loss is 3.4589726638793947 and perplexity is 31.784306569826295
At time: 165.39028143882751 and batch: 150, loss is 3.4557264280319213 and perplexity is 31.68129450564213
At time: 166.02775883674622 and batch: 200, loss is 3.3509212636947634 and perplexity is 28.529004276706925
At time: 166.6643762588501 and batch: 250, loss is 3.4793021297454834 and perplexity is 32.43707730383263
At time: 167.30178475379944 and batch: 300, loss is 3.46688024520874 and perplexity is 32.03663991739572
At time: 167.9383773803711 and batch: 350, loss is 3.4263046741485597 and perplexity is 30.762754051044524
At time: 168.57539534568787 and batch: 400, loss is 3.3779808044433595 and perplexity is 29.311525617013192
At time: 169.21240234375 and batch: 450, loss is 3.3781710338592528 and perplexity is 29.31710206179536
At time: 169.84928488731384 and batch: 500, loss is 3.257561917304993 and perplexity is 25.98610357636025
At time: 170.51237273216248 and batch: 550, loss is 3.324595127105713 and perplexity is 27.787745859758033
At time: 171.1559898853302 and batch: 600, loss is 3.3345939111709595 and perplexity is 28.066983220899928
At time: 171.79214358329773 and batch: 650, loss is 3.1741209745407106 and perplexity is 23.905796825799513
At time: 172.42901492118835 and batch: 700, loss is 3.161685185432434 and perplexity is 23.61035024124744
At time: 173.06542491912842 and batch: 750, loss is 3.257679891586304 and perplexity is 25.989169449097258
At time: 173.70212960243225 and batch: 800, loss is 3.1855830764770507 and perplexity is 24.18138389220256
At time: 174.33929824829102 and batch: 850, loss is 3.2480545902252196 and perplexity is 25.74021590949886
At time: 174.97662663459778 and batch: 900, loss is 3.2157038164138796 and perplexity is 24.92082542623612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.467892999518408 and perplexity of 87.17285612781197
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 176.55223536491394 and batch: 50, loss is 3.575040717124939 and perplexity is 35.69607422758045
At time: 177.1987590789795 and batch: 100, loss is 3.4584598636627195 and perplexity is 31.768011748881353
At time: 177.83433389663696 and batch: 150, loss is 3.455191078186035 and perplexity is 31.664338468621928
At time: 178.47069478034973 and batch: 200, loss is 3.350446548461914 and perplexity is 28.51546433786063
At time: 179.10625767707825 and batch: 250, loss is 3.478809175491333 and perplexity is 32.421091249104066
At time: 179.7438051700592 and batch: 300, loss is 3.4664074563980103 and perplexity is 32.02149693250837
At time: 180.37986755371094 and batch: 350, loss is 3.4258248138427736 and perplexity is 30.747995767719136
At time: 181.01665830612183 and batch: 400, loss is 3.377498369216919 and perplexity is 29.297388115003525
At time: 181.6525444984436 and batch: 450, loss is 3.377719225883484 and perplexity is 29.30385935306339
At time: 182.28900027275085 and batch: 500, loss is 3.2571241950988767 and perplexity is 25.974731370889454
At time: 182.92539358139038 and batch: 550, loss is 3.3241240549087525 and perplexity is 27.774658907958873
At time: 183.56194496154785 and batch: 600, loss is 3.3341545057296753 and perplexity is 28.054653144901863
At time: 184.20088005065918 and batch: 650, loss is 3.173644309043884 and perplexity is 23.894404472665048
At time: 184.83959031105042 and batch: 700, loss is 3.1611772108078005 and perplexity is 23.59835982811681
At time: 185.47515630722046 and batch: 750, loss is 3.257157950401306 and perplexity is 25.975608170600633
At time: 186.121741771698 and batch: 800, loss is 3.185061993598938 and perplexity is 24.168786669470844
At time: 186.75698399543762 and batch: 850, loss is 3.247488522529602 and perplexity is 25.725649328018843
At time: 187.39177203178406 and batch: 900, loss is 3.215090389251709 and perplexity is 24.905543002821997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.467872515116652 and perplexity of 87.1710704622941
Annealing...
Model not improving. Stopping early with 87.01600300800983 lossat 14 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -87.01600300800983
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f61b3402b70>
ELAPSED
198.74154806137085


RESULTS SO FAR:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8444693088531494 and batch: 50, loss is 6.531675109863281 and perplexity is 686.5472909446243
At time: 1.5011906623840332 and batch: 100, loss is 5.6250706386566165 and perplexity is 277.29187135661533
At time: 2.144787549972534 and batch: 150, loss is 5.384414405822754 and perplexity is 217.98241749042435
At time: 2.7874398231506348 and batch: 200, loss is 5.169534959793091 and perplexity is 175.83304903057697
At time: 3.4294090270996094 and batch: 250, loss is 5.187548818588257 and perplexity is 179.0291816602296
At time: 4.071830987930298 and batch: 300, loss is 5.1108544731140135 and perplexity is 165.81197623496436
At time: 4.714901685714722 and batch: 350, loss is 5.062728605270386 and perplexity is 158.02110582400493
At time: 5.356974124908447 and batch: 400, loss is 4.910753707885743 and perplexity is 135.7416854411228
At time: 5.999702215194702 and batch: 450, loss is 4.901556367874146 and perplexity is 134.49894671206212
At time: 6.643558740615845 and batch: 500, loss is 4.809678783416748 and perplexity is 122.69220041748132
At time: 7.286375522613525 and batch: 550, loss is 4.873667440414429 and perplexity is 130.79973858224724
At time: 7.929259538650513 and batch: 600, loss is 4.812757835388184 and perplexity is 123.07055827188495
At time: 8.573534727096558 and batch: 650, loss is 4.678130512237549 and perplexity is 107.5687859496323
At time: 9.234864473342896 and batch: 700, loss is 4.728677682876587 and perplexity is 113.14584869283965
At time: 9.88381314277649 and batch: 750, loss is 4.763400087356567 and perplexity is 117.14354783717008
At time: 10.532852411270142 and batch: 800, loss is 4.681805934906006 and perplexity is 107.96487415382484
At time: 11.194519519805908 and batch: 850, loss is 4.746303815841674 and perplexity is 115.15785229727832
At time: 11.843464851379395 and batch: 900, loss is 4.686654148101806 and perplexity is 108.48958180087135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.751228646056293 and perplexity of 115.7263839796316
finished 1 epochs...
Completing Train Step...
At time: 13.502338886260986 and batch: 50, loss is 4.717667465209961 and perplexity is 111.90692121695994
At time: 14.138583898544312 and batch: 100, loss is 4.553136482238769 and perplexity is 94.92968714399228
At time: 14.774813175201416 and batch: 150, loss is 4.52853982925415 and perplexity is 92.62321658187976
At time: 15.410274982452393 and batch: 200, loss is 4.418762722015381 and perplexity is 82.99353573189954
At time: 16.059741258621216 and batch: 250, loss is 4.54975438117981 and perplexity is 94.60916766872887
At time: 16.69577121734619 and batch: 300, loss is 4.517770767211914 and perplexity is 91.6311030700935
At time: 17.331782341003418 and batch: 350, loss is 4.498459854125977 and perplexity is 89.87859849490627
At time: 17.970067024230957 and batch: 400, loss is 4.415216536521911 and perplexity is 82.69974648289896
At time: 18.607173204421997 and batch: 450, loss is 4.425953722000122 and perplexity is 83.59249321668379
At time: 19.244030237197876 and batch: 500, loss is 4.315737323760986 and perplexity is 74.86880564722853
At time: 19.880934476852417 and batch: 550, loss is 4.399110555648804 and perplexity is 81.37845485873343
At time: 20.51782727241516 and batch: 600, loss is 4.392604751586914 and perplexity is 80.85074103922909
At time: 21.154786109924316 and batch: 650, loss is 4.248235034942627 and perplexity is 69.98178787209406
At time: 21.799021005630493 and batch: 700, loss is 4.269710235595703 and perplexity is 71.50091419738987
At time: 22.435725688934326 and batch: 750, loss is 4.360642304420471 and perplexity is 78.30741547259917
At time: 23.07298731803894 and batch: 800, loss is 4.3002875757217405 and perplexity is 73.72099101873218
At time: 23.70983123779297 and batch: 850, loss is 4.368307795524597 and perplexity is 78.90998682137247
At time: 24.35639762878418 and batch: 900, loss is 4.327178087234497 and perplexity is 75.73028051267634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.576689001632063 and perplexity of 97.19205812857204
finished 2 epochs...
Completing Train Step...
At time: 25.9593243598938 and batch: 50, loss is 4.393540840148926 and perplexity is 80.92645992740958
At time: 26.60202383995056 and batch: 100, loss is 4.241923537254333 and perplexity is 69.54148891425221
At time: 27.244372367858887 and batch: 150, loss is 4.223908610343933 and perplexity is 68.29992102934466
At time: 27.886821746826172 and batch: 200, loss is 4.122422723770142 and perplexity is 61.70856411369311
At time: 28.529895544052124 and batch: 250, loss is 4.268549203872681 and perplexity is 71.41794754056177
At time: 29.172007083892822 and batch: 300, loss is 4.249497804641724 and perplexity is 70.07021457283638
At time: 29.815001249313354 and batch: 350, loss is 4.231030912399292 and perplexity is 68.78811014875376
At time: 30.456971883773804 and batch: 400, loss is 4.163464312553406 and perplexity is 64.29387130031665
At time: 31.099514722824097 and batch: 450, loss is 4.178394389152527 and perplexity is 65.26098530209265
At time: 31.742039918899536 and batch: 500, loss is 4.066471099853516 and perplexity is 58.35068509842937
At time: 32.39793920516968 and batch: 550, loss is 4.148591628074646 and perplexity is 63.344724514500825
At time: 33.04127764701843 and batch: 600, loss is 4.158714442253113 and perplexity is 63.98920787920341
At time: 33.689274072647095 and batch: 650, loss is 4.017294225692749 and perplexity is 55.55059492186062
At time: 34.33691430091858 and batch: 700, loss is 4.024634881019592 and perplexity is 55.95987303934459
At time: 34.99303984642029 and batch: 750, loss is 4.128327078819275 and perplexity is 62.07399112960805
At time: 35.65263557434082 and batch: 800, loss is 4.0767601251602175 and perplexity is 58.95415601401856
At time: 36.30066251754761 and batch: 850, loss is 4.146507802009583 and perplexity is 63.21286256287483
At time: 36.94810438156128 and batch: 900, loss is 4.114894676208496 and perplexity is 61.24576328703437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.517299704355736 and perplexity of 91.5879492258511
finished 3 epochs...
Completing Train Step...
At time: 38.52386784553528 and batch: 50, loss is 4.185783953666687 and perplexity is 65.74502177000112
At time: 39.18120765686035 and batch: 100, loss is 4.0423654317855835 and perplexity is 56.96092074772071
At time: 39.82491755485535 and batch: 150, loss is 4.029880957603455 and perplexity is 56.254214212044616
At time: 40.46827220916748 and batch: 200, loss is 3.9294762086868285 and perplexity is 50.880320018650856
At time: 41.11206364631653 and batch: 250, loss is 4.078637161254883 and perplexity is 59.064919013340635
At time: 41.75536775588989 and batch: 300, loss is 4.06673228263855 and perplexity is 58.36592728328354
At time: 42.39808392524719 and batch: 350, loss is 4.047508339881897 and perplexity is 57.25462011528539
At time: 43.0421998500824 and batch: 400, loss is 3.9872338819503783 and perplexity is 53.90557376567947
At time: 43.685733795166016 and batch: 450, loss is 4.005153403282166 and perplexity is 54.88024256270179
At time: 44.329636335372925 and batch: 500, loss is 3.898013224601746 and perplexity is 49.304394973056745
At time: 44.97395896911621 and batch: 550, loss is 3.9730523109436033 and perplexity is 53.14650317220874
At time: 45.617703676223755 and batch: 600, loss is 3.9914179277420043 and perplexity is 54.131589655431206
At time: 46.26200866699219 and batch: 650, loss is 3.849879221916199 and perplexity is 46.98738784218845
At time: 46.90586543083191 and batch: 700, loss is 3.8541454887390136 and perplexity is 47.1882767942672
At time: 47.550318002700806 and batch: 750, loss is 3.965811023712158 and perplexity is 52.763044121549974
At time: 48.19435787200928 and batch: 800, loss is 3.9152776479721068 and perplexity is 50.162997232179556
At time: 48.86007356643677 and batch: 850, loss is 3.9870236682891846 and perplexity is 53.894243268613806
At time: 49.504793882369995 and batch: 900, loss is 3.9528771448135376 and perplexity is 52.08500757196813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.501290517310574 and perplexity of 90.13337495780631
finished 4 epochs...
Completing Train Step...
At time: 51.08133506774902 and batch: 50, loss is 4.0287782144546505 and perplexity is 56.192214453918034
At time: 51.72815799713135 and batch: 100, loss is 3.891578321456909 and perplexity is 48.98814457839331
At time: 52.36432909965515 and batch: 150, loss is 3.884114065170288 and perplexity is 48.62384581333872
At time: 53.00175213813782 and batch: 200, loss is 3.7836857271194457 and perplexity is 43.977833686199276
At time: 53.639183044433594 and batch: 250, loss is 3.932021827697754 and perplexity is 51.010006925266964
At time: 54.275519371032715 and batch: 300, loss is 3.927259840965271 and perplexity is 50.76767539676286
At time: 54.91198396682739 and batch: 350, loss is 3.902062726020813 and perplexity is 49.50445799481791
At time: 55.547967195510864 and batch: 400, loss is 3.84924156665802 and perplexity is 46.95743563786798
At time: 56.1849524974823 and batch: 450, loss is 3.864733624458313 and perplexity is 47.69056713991895
At time: 56.82189655303955 and batch: 500, loss is 3.7601731967926026 and perplexity is 42.955865152591805
At time: 57.458553314208984 and batch: 550, loss is 3.834998941421509 and perplexity is 46.29337866274956
At time: 58.095465898513794 and batch: 600, loss is 3.8568155431747435 and perplexity is 47.31444041895534
At time: 58.73192000389099 and batch: 650, loss is 3.7158557176589966 and perplexity is 41.093736679333986
At time: 59.36747598648071 and batch: 700, loss is 3.7173761892318726 and perplexity is 41.15626606280673
At time: 60.00444531440735 and batch: 750, loss is 3.832587146759033 and perplexity is 46.18186306959384
At time: 60.64092135429382 and batch: 800, loss is 3.7839156103134157 and perplexity is 43.98794461319251
At time: 61.277745723724365 and batch: 850, loss is 3.857802586555481 and perplexity is 47.361164879925674
At time: 61.92843317985535 and batch: 900, loss is 3.8237972927093504 and perplexity is 45.77771005858008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.502118254361087 and perplexity of 90.20801257764685
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 63.51486134529114 and batch: 50, loss is 3.9123762893676757 and perplexity is 50.01766731758251
At time: 64.15183067321777 and batch: 100, loss is 3.77347767829895 and perplexity is 43.53118937489182
At time: 64.80207777023315 and batch: 150, loss is 3.763704581260681 and perplexity is 43.10782698751413
At time: 65.43969488143921 and batch: 200, loss is 3.6450833177566526 and perplexity is 38.285962619190705
At time: 66.07872033119202 and batch: 250, loss is 3.7844207525253295 and perplexity is 44.01017039394937
At time: 66.71611452102661 and batch: 300, loss is 3.76900776386261 and perplexity is 43.33704291536097
At time: 67.3536057472229 and batch: 350, loss is 3.7259983253479003 and perplexity is 41.51265520079215
At time: 67.99056649208069 and batch: 400, loss is 3.6686860942840576 and perplexity is 39.20036642322808
At time: 68.6281533241272 and batch: 450, loss is 3.6691598987579344 and perplexity is 39.21894413297047
At time: 69.26616883277893 and batch: 500, loss is 3.5554355144500733 and perplexity is 35.00306098746734
At time: 69.90413975715637 and batch: 550, loss is 3.608541502952576 and perplexity is 36.91217723417522
At time: 70.54131245613098 and batch: 600, loss is 3.6160276556015014 and perplexity is 37.18954433843506
At time: 71.17864322662354 and batch: 650, loss is 3.462047758102417 and perplexity is 31.882196740952686
At time: 71.81576037406921 and batch: 700, loss is 3.449459867477417 and perplexity is 31.483382516572377
At time: 72.45305609703064 and batch: 750, loss is 3.5442148447036743 and perplexity is 34.61249848420077
At time: 73.09033632278442 and batch: 800, loss is 3.4724615478515624 and perplexity is 32.21594601577165
At time: 73.72647714614868 and batch: 850, loss is 3.5295870876312256 and perplexity is 34.10988031506487
At time: 74.36579298973083 and batch: 900, loss is 3.4837208652496336 and perplexity is 32.58072530650082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431404009257277 and perplexity of 84.04934015988303
finished 6 epochs...
Completing Train Step...
At time: 75.95201921463013 and batch: 50, loss is 3.792599248886108 and perplexity is 44.37158330493478
At time: 76.58867144584656 and batch: 100, loss is 3.6587700271606445 and perplexity is 38.813573858416284
At time: 77.22612619400024 and batch: 150, loss is 3.652147889137268 and perplexity is 38.55739417992068
At time: 77.8629777431488 and batch: 200, loss is 3.539343571662903 and perplexity is 34.44430155264955
At time: 78.50011801719666 and batch: 250, loss is 3.6801768064498903 and perplexity is 39.653404430479505
At time: 79.1368498802185 and batch: 300, loss is 3.6725412034988403 and perplexity is 39.35177978705246
At time: 79.77405309677124 and batch: 350, loss is 3.6346895790100096 and perplexity is 37.89008919238824
At time: 80.4104790687561 and batch: 400, loss is 3.582677307128906 and perplexity is 35.969714018823666
At time: 81.0768313407898 and batch: 450, loss is 3.5905607032775877 and perplexity is 36.25439818845583
At time: 81.71465492248535 and batch: 500, loss is 3.4801862382888795 and perplexity is 32.465767881923526
At time: 82.35107588768005 and batch: 550, loss is 3.539168701171875 and perplexity is 34.43827878734123
At time: 82.99002408981323 and batch: 600, loss is 3.5539737367630004 and perplexity is 34.951931672886616
At time: 83.64253306388855 and batch: 650, loss is 3.406130299568176 and perplexity is 30.148353143287732
At time: 84.28019833564758 and batch: 700, loss is 3.3999350929260252 and perplexity is 29.962155228455586
At time: 84.91826319694519 and batch: 750, loss is 3.501980481147766 and perplexity is 33.181101474352204
At time: 85.55487012863159 and batch: 800, loss is 3.436428632736206 and perplexity is 31.07577673988022
At time: 86.19017434120178 and batch: 850, loss is 3.503957872390747 and perplexity is 33.24677840693557
At time: 86.82602500915527 and batch: 900, loss is 3.4668061685562135 and perplexity is 32.0342668382484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4372579496200775 and perplexity of 84.54280292632492
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.39442539215088 and batch: 50, loss is 3.743648691177368 and perplexity is 42.25187329558752
At time: 89.05454063415527 and batch: 100, loss is 3.616833972930908 and perplexity is 37.21954300510498
At time: 89.69161987304688 and batch: 150, loss is 3.6149467420578003 and perplexity is 37.14936737409984
At time: 90.3259949684143 and batch: 200, loss is 3.4954896116256715 and perplexity is 32.96642474711426
At time: 90.9602153301239 and batch: 250, loss is 3.634938540458679 and perplexity is 37.89952353822927
At time: 91.59443879127502 and batch: 300, loss is 3.6258231830596923 and perplexity is 37.55562559346062
At time: 92.22948384284973 and batch: 350, loss is 3.5807584619522093 and perplexity is 35.90075988388334
At time: 92.86437845230103 and batch: 400, loss is 3.5278125524520876 and perplexity is 34.04940480631813
At time: 93.49921011924744 and batch: 450, loss is 3.5307305669784546 and perplexity is 34.14890656734596
At time: 94.13313388824463 and batch: 500, loss is 3.4155635023117066 and perplexity is 30.434094279010726
At time: 94.7673921585083 and batch: 550, loss is 3.468672695159912 and perplexity is 32.09411548670357
At time: 95.40223860740662 and batch: 600, loss is 3.4808076143264772 and perplexity is 32.485947601075225
At time: 96.03718113899231 and batch: 650, loss is 3.327562913894653 and perplexity is 27.87033645984113
At time: 96.67235040664673 and batch: 700, loss is 3.3138156652450563 and perplexity is 27.489817551342075
At time: 97.32037019729614 and batch: 750, loss is 3.407933497428894 and perplexity is 30.20276563267849
At time: 97.95850229263306 and batch: 800, loss is 3.334800944328308 and perplexity is 28.072794618608647
At time: 98.59601402282715 and batch: 850, loss is 3.3974860954284667 and perplexity is 29.888867762319226
At time: 99.23294281959534 and batch: 900, loss is 3.3560807609558108 and perplexity is 28.676579976966632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423452716984161 and perplexity of 83.38368919064997
finished 8 epochs...
Completing Train Step...
At time: 100.81321024894714 and batch: 50, loss is 3.711739501953125 and perplexity is 40.92493364818458
At time: 101.46350193023682 and batch: 100, loss is 3.582277946472168 and perplexity is 35.955351998214304
At time: 102.10235381126404 and batch: 150, loss is 3.5798602628707887 and perplexity is 35.868528331675556
At time: 102.74119353294373 and batch: 200, loss is 3.461692237854004 and perplexity is 31.87086398907813
At time: 103.37936997413635 and batch: 250, loss is 3.600514144897461 and perplexity is 36.617056077282115
At time: 104.0177891254425 and batch: 300, loss is 3.593400993347168 and perplexity is 36.35751757076376
At time: 104.65360569953918 and batch: 350, loss is 3.5506534147262574 and perplexity is 34.83607245532419
At time: 105.29067945480347 and batch: 400, loss is 3.4995907831192015 and perplexity is 33.101903329088586
At time: 105.92805576324463 and batch: 450, loss is 3.5053826475143435 and perplexity is 33.29418135099831
At time: 106.5653350353241 and batch: 500, loss is 3.392346167564392 and perplexity is 29.73563527739364
At time: 107.20171427726746 and batch: 550, loss is 3.4481550645828247 and perplexity is 31.442329696676715
At time: 107.83980059623718 and batch: 600, loss is 3.4635651540756225 and perplexity is 31.930611380704534
At time: 108.4777307510376 and batch: 650, loss is 3.31311092376709 and perplexity is 27.47045116164251
At time: 109.11498403549194 and batch: 700, loss is 3.3029719305038454 and perplexity is 27.193335652450045
At time: 109.75246500968933 and batch: 750, loss is 3.400535888671875 and perplexity is 29.98016177243921
At time: 110.39180374145508 and batch: 800, loss is 3.330749707221985 and perplexity is 27.95929513326919
At time: 111.03038334846497 and batch: 850, loss is 3.3978669023513794 and perplexity is 29.900251817507083
At time: 111.68091487884521 and batch: 900, loss is 3.3597732067108153 and perplexity is 28.782662424283075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425484017150043 and perplexity of 83.5532386369447
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 113.27540683746338 and batch: 50, loss is 3.695830283164978 and perplexity is 40.27900168524426
At time: 113.91809344291687 and batch: 100, loss is 3.568552575111389 and perplexity is 35.46522273711897
At time: 114.57082724571228 and batch: 150, loss is 3.568841586112976 and perplexity is 35.475474057964625
At time: 115.2153434753418 and batch: 200, loss is 3.449176592826843 and perplexity is 31.474465335456422
At time: 115.85936498641968 and batch: 250, loss is 3.5875736618041993 and perplexity is 36.146266374979646
At time: 116.50620222091675 and batch: 300, loss is 3.5792069911956785 and perplexity is 35.8451040901184
At time: 117.1697883605957 and batch: 350, loss is 3.5352111625671387 and perplexity is 34.30225730223079
At time: 117.821706533432 and batch: 400, loss is 3.483343930244446 and perplexity is 32.568446804882306
At time: 118.46363258361816 and batch: 450, loss is 3.48819881439209 and perplexity is 32.72694728039178
At time: 119.10621356964111 and batch: 500, loss is 3.3730305576324464 and perplexity is 29.16678487756835
At time: 119.74894547462463 and batch: 550, loss is 3.42610631942749 and perplexity is 30.75665271867944
At time: 120.3920385837555 and batch: 600, loss is 3.441076226234436 and perplexity is 31.220540458691985
At time: 121.03537082672119 and batch: 650, loss is 3.28958815574646 and perplexity is 26.831810852926246
At time: 121.68038964271545 and batch: 700, loss is 3.274655146598816 and perplexity is 26.434108016502144
At time: 122.32601118087769 and batch: 750, loss is 3.3702661752700807 and perplexity is 29.086268072777905
At time: 122.9691870212555 and batch: 800, loss is 3.299206509590149 and perplexity is 27.091133834856617
At time: 123.61159563064575 and batch: 850, loss is 3.363286814689636 and perplexity is 28.883971292226914
At time: 124.25482320785522 and batch: 900, loss is 3.3250409936904908 and perplexity is 27.80013824957416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423009585027826 and perplexity of 83.34674739898118
finished 10 epochs...
Completing Train Step...
At time: 125.85691523551941 and batch: 50, loss is 3.6869378662109376 and perplexity is 39.922411830379914
At time: 126.50108480453491 and batch: 100, loss is 3.5588414239883424 and perplexity is 35.122481499040894
At time: 127.150062084198 and batch: 150, loss is 3.559127469062805 and perplexity is 35.13252954890568
At time: 127.80242705345154 and batch: 200, loss is 3.4395471715927126 and perplexity is 31.172839024729882
At time: 128.446275472641 and batch: 250, loss is 3.5776427030563354 and perplexity is 35.78907585251556
At time: 129.0917465686798 and batch: 300, loss is 3.5702037382125855 and perplexity is 35.52382997601811
At time: 129.74938368797302 and batch: 350, loss is 3.526568112373352 and perplexity is 34.007058716370686
At time: 130.3929567337036 and batch: 400, loss is 3.475342450141907 and perplexity is 32.308890826609606
At time: 131.035973072052 and batch: 450, loss is 3.4809062576293943 and perplexity is 32.48915228030246
At time: 131.6794183254242 and batch: 500, loss is 3.3665384006500245 and perplexity is 28.978042865781497
At time: 132.3232228755951 and batch: 550, loss is 3.420773272514343 and perplexity is 30.59306265131845
At time: 132.96735882759094 and batch: 600, loss is 3.436922426223755 and perplexity is 31.091125545313105
At time: 133.61166381835938 and batch: 650, loss is 3.286425538063049 and perplexity is 26.747086140001606
At time: 134.25527048110962 and batch: 700, loss is 3.272851519584656 and perplexity is 26.386473715357674
At time: 134.90014576911926 and batch: 750, loss is 3.3696673250198366 and perplexity is 29.06885496830523
At time: 135.54419112205505 and batch: 800, loss is 3.2995718240737917 and perplexity is 27.101032426364128
At time: 136.1893289089203 and batch: 850, loss is 3.3652610778808594 and perplexity is 28.941052181374268
At time: 136.8327920436859 and batch: 900, loss is 3.3279965496063233 and perplexity is 27.882424653773246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423361582298801 and perplexity of 83.37609039063382
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.40366625785828 and batch: 50, loss is 3.6817946672439574 and perplexity is 39.71761004272556
At time: 139.05444526672363 and batch: 100, loss is 3.553876495361328 and perplexity is 34.94853306330506
At time: 139.6905312538147 and batch: 150, loss is 3.5553157997131346 and perplexity is 34.99887085604443
At time: 140.33306455612183 and batch: 200, loss is 3.4350372457504275 and perplexity is 31.03256837530027
At time: 140.9793725013733 and batch: 250, loss is 3.57326192855835 and perplexity is 35.632634898176526
At time: 141.61684203147888 and batch: 300, loss is 3.565329532623291 and perplexity is 35.351100826062996
At time: 142.25591397285461 and batch: 350, loss is 3.5216795778274537 and perplexity is 33.841219720127455
At time: 142.8960144519806 and batch: 400, loss is 3.470065746307373 and perplexity is 32.138855386358195
At time: 143.54425168037415 and batch: 450, loss is 3.4756860017776487 and perplexity is 32.319992505792925
At time: 144.1823570728302 and batch: 500, loss is 3.3606840705871583 and perplexity is 28.808891455475386
At time: 144.82059812545776 and batch: 550, loss is 3.414192810058594 and perplexity is 30.39240707844024
At time: 145.45847344398499 and batch: 600, loss is 3.430237112045288 and perplexity is 30.883964841634842
At time: 146.11071634292603 and batch: 650, loss is 3.279641180038452 and perplexity is 26.56623849276989
At time: 146.74845480918884 and batch: 700, loss is 3.2646429109573365 and perplexity is 26.170764029000566
At time: 147.38529539108276 and batch: 750, loss is 3.3608061027526857 and perplexity is 28.8124072814037
At time: 148.02343654632568 and batch: 800, loss is 3.2905963850021362 and perplexity is 26.85887711181517
At time: 148.66236901283264 and batch: 850, loss is 3.3552162647247314 and perplexity is 28.651799894343608
At time: 149.29805040359497 and batch: 900, loss is 3.3179103565216064 and perplexity is 27.60261063629866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423714833716824 and perplexity of 83.40554831551405
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 150.86193227767944 and batch: 50, loss is 3.680013561248779 and perplexity is 39.64693173083146
At time: 151.517338514328 and batch: 100, loss is 3.552124972343445 and perplexity is 34.887373480047955
At time: 152.15255045890808 and batch: 150, loss is 3.553687791824341 and perplexity is 34.94193877370594
At time: 152.7963001728058 and batch: 200, loss is 3.4332285642623903 and perplexity is 30.97649107163141
At time: 153.44963479042053 and batch: 250, loss is 3.571607732772827 and perplexity is 35.57374026875454
At time: 154.09020924568176 and batch: 300, loss is 3.5636794996261596 and perplexity is 35.292818440368194
At time: 154.72629380226135 and batch: 350, loss is 3.520140824317932 and perplexity is 33.78918646795818
At time: 155.36509895324707 and batch: 400, loss is 3.468237614631653 and perplexity is 32.08015499917197
At time: 156.00392508506775 and batch: 450, loss is 3.4740584325790405 and perplexity is 32.26743226580767
At time: 156.64152812957764 and batch: 500, loss is 3.358979468345642 and perplexity is 28.759825585301684
At time: 157.28074026107788 and batch: 550, loss is 3.412357759475708 and perplexity is 30.33668661468653
At time: 157.92077350616455 and batch: 600, loss is 3.4284200525283812 and perplexity is 30.827897793407722
At time: 158.5594744682312 and batch: 650, loss is 3.2778480100631713 and perplexity is 26.518643397349884
At time: 159.1982295513153 and batch: 700, loss is 3.2625672245025634 and perplexity is 26.116498067638606
At time: 159.8412458896637 and batch: 750, loss is 3.3585948657989504 and perplexity is 28.748766609930538
At time: 160.48528242111206 and batch: 800, loss is 3.2883144187927247 and perplexity is 26.79765594071192
At time: 161.1339192390442 and batch: 850, loss is 3.352694058418274 and perplexity is 28.57962520195264
At time: 161.78694128990173 and batch: 900, loss is 3.3152856826782227 and perplexity is 27.530257779014505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423752040079195 and perplexity of 83.40865159029889
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 163.37980246543884 and batch: 50, loss is 3.679554233551025 and perplexity is 39.62872497870941
At time: 164.01841068267822 and batch: 100, loss is 3.551694293022156 and perplexity is 34.87235144478818
At time: 164.6567633152008 and batch: 150, loss is 3.5532377338409424 and perplexity is 34.926216413457674
At time: 165.294659614563 and batch: 200, loss is 3.432767992019653 and perplexity is 30.962227444633903
At time: 165.9385006427765 and batch: 250, loss is 3.5711428499221802 and perplexity is 35.557206490403004
At time: 166.59041786193848 and batch: 300, loss is 3.5632460832595827 and perplexity is 35.277525269628164
At time: 167.23771715164185 and batch: 350, loss is 3.519709630012512 and perplexity is 33.774619903903854
At time: 167.88172698020935 and batch: 400, loss is 3.4677676725387574 and perplexity is 32.065082725816474
At time: 168.52239179611206 and batch: 450, loss is 3.4736260843276976 and perplexity is 32.2534845132527
At time: 169.1596119403839 and batch: 500, loss is 3.3585357999801637 and perplexity is 28.747068590639557
At time: 169.79563999176025 and batch: 550, loss is 3.411879940032959 and perplexity is 30.322194618547925
At time: 170.43140697479248 and batch: 600, loss is 3.4279563665390014 and perplexity is 30.81360664267812
At time: 171.06567358970642 and batch: 650, loss is 3.2773583698272706 and perplexity is 26.5056619809123
At time: 171.70133185386658 and batch: 700, loss is 3.262036876678467 and perplexity is 26.102650911944423
At time: 172.3375895023346 and batch: 750, loss is 3.358043189048767 and perplexity is 28.73291095779485
At time: 172.97461652755737 and batch: 800, loss is 3.287745223045349 and perplexity is 26.782407169090504
At time: 173.60930919647217 and batch: 850, loss is 3.352065978050232 and perplexity is 28.56168053637329
At time: 174.24707770347595 and batch: 900, loss is 3.3146130895614623 and perplexity is 27.5117473428235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423755802520334 and perplexity of 83.40896541103129
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 175.82338452339172 and batch: 50, loss is 3.679439754486084 and perplexity is 39.624188578995394
At time: 176.45832419395447 and batch: 100, loss is 3.5515844678878783 and perplexity is 34.86852179440796
At time: 177.09342408180237 and batch: 150, loss is 3.553126292228699 and perplexity is 34.92232439646134
At time: 177.72799801826477 and batch: 200, loss is 3.4326544332504274 and perplexity is 30.958711611823404
At time: 178.38851118087769 and batch: 250, loss is 3.5710264015197755 and perplexity is 35.55306615158582
At time: 179.02373147010803 and batch: 300, loss is 3.5631370878219606 and perplexity is 35.273680389864154
At time: 179.65841150283813 and batch: 350, loss is 3.519601335525513 and perplexity is 33.77096249680985
At time: 180.29336643218994 and batch: 400, loss is 3.4676485204696657 and perplexity is 32.0612623324725
At time: 180.92835688591003 and batch: 450, loss is 3.4735169410705566 and perplexity is 32.24996445499759
At time: 181.56440138816833 and batch: 500, loss is 3.358424334526062 and perplexity is 28.74386446416303
At time: 182.20046496391296 and batch: 550, loss is 3.411759033203125 and perplexity is 30.31852867974598
At time: 182.8348319530487 and batch: 600, loss is 3.4278393268585203 and perplexity is 30.81000043904077
At time: 183.46934533119202 and batch: 650, loss is 3.277233152389526 and perplexity is 26.502343217621185
At time: 184.1037096977234 and batch: 700, loss is 3.2619027519226074 and perplexity is 26.099150135039245
At time: 184.73853015899658 and batch: 750, loss is 3.3579033422470093 and perplexity is 28.72889303304572
At time: 185.37286353111267 and batch: 800, loss is 3.2876019716262816 and perplexity is 26.778570826044845
At time: 186.00714540481567 and batch: 850, loss is 3.3519081115722655 and perplexity is 28.557171960348082
At time: 186.64195895195007 and batch: 900, loss is 3.3144433975219725 and perplexity is 27.507079214390288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423759146912457 and perplexity of 83.40924436378468
Annealing...
Model not improving. Stopping early with 83.34674739898118 lossat 14 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f61b3402b70>
ELAPSED
391.52296781539917


RESULTS SO FAR:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.34674739898118, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.10738997417746055, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.6689429055087728, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8412692546844482 and batch: 50, loss is 6.528353137969971 and perplexity is 684.2703841447737
At time: 1.4973790645599365 and batch: 100, loss is 5.610079641342163 and perplexity is 273.1659924447405
At time: 2.1410181522369385 and batch: 150, loss is 5.4063566875457765 and perplexity is 222.81831034331893
At time: 2.7846391201019287 and batch: 200, loss is 5.204447631835937 and perplexity is 182.08026964210228
At time: 3.4284474849700928 and batch: 250, loss is 5.236118326187134 and perplexity is 187.9391661557122
At time: 4.07161808013916 and batch: 300, loss is 5.154507732391357 and perplexity is 173.21051984022012
At time: 4.7155067920684814 and batch: 350, loss is 5.105864772796631 and perplexity is 164.9866848551289
At time: 5.359374284744263 and batch: 400, loss is 4.962797985076905 and perplexity is 142.99332989439597
At time: 6.002293586730957 and batch: 450, loss is 4.956929416656494 and perplexity is 142.15662129643482
At time: 6.644482612609863 and batch: 500, loss is 4.871357450485229 and perplexity is 130.49794121202385
At time: 7.2876574993133545 and batch: 550, loss is 4.932907571792603 and perplexity is 138.78244620241483
At time: 7.930271863937378 and batch: 600, loss is 4.8699889087677 and perplexity is 130.31947148491176
At time: 8.584984302520752 and batch: 650, loss is 4.738590393066406 and perplexity is 114.27300807253442
At time: 9.24135446548462 and batch: 700, loss is 4.792432813644409 and perplexity is 120.59439576998683
At time: 9.885014772415161 and batch: 750, loss is 4.82456416130066 and perplexity is 124.53218061007534
At time: 10.528851747512817 and batch: 800, loss is 4.755224056243897 and perplexity is 116.18968327219967
At time: 11.172451734542847 and batch: 850, loss is 4.810859956741333 and perplexity is 122.83720679372175
At time: 11.816394329071045 and batch: 900, loss is 4.744863471984863 and perplexity is 114.99210478751696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.779362926744435 and perplexity of 119.02849601363165
finished 1 epochs...
Completing Train Step...
At time: 13.418882131576538 and batch: 50, loss is 4.74915846824646 and perplexity is 115.48705759706012
At time: 14.061116456985474 and batch: 100, loss is 4.5819864845275875 and perplexity is 97.70829757161633
At time: 14.70299243927002 and batch: 150, loss is 4.5602062320709225 and perplexity is 95.60319424185973
At time: 15.344908237457275 and batch: 200, loss is 4.454564933776855 and perplexity is 86.0187188638976
At time: 15.986389636993408 and batch: 250, loss is 4.582808361053467 and perplexity is 97.78863473686705
At time: 16.62933588027954 and batch: 300, loss is 4.55435619354248 and perplexity is 95.04554459841955
At time: 17.271507263183594 and batch: 350, loss is 4.531201019287109 and perplexity is 92.87003282947578
At time: 17.912952423095703 and batch: 400, loss is 4.4437398338317875 and perplexity is 85.09257945483122
At time: 18.55468988418579 and batch: 450, loss is 4.457789459228516 and perplexity is 86.29653608582646
At time: 19.19736075401306 and batch: 500, loss is 4.349840965270996 and perplexity is 77.46614213864834
At time: 19.838731288909912 and batch: 550, loss is 4.4351779747009275 and perplexity is 84.36713876849602
At time: 20.49133014678955 and batch: 600, loss is 4.425656661987305 and perplexity is 83.56766491750747
At time: 21.13773250579834 and batch: 650, loss is 4.275910849571228 and perplexity is 71.9456411303531
At time: 21.78001880645752 and batch: 700, loss is 4.2967780447006225 and perplexity is 73.46271838684228
At time: 22.42105197906494 and batch: 750, loss is 4.3878234767913815 and perplexity is 80.46509410555612
At time: 23.063058614730835 and batch: 800, loss is 4.331908569335938 and perplexity is 76.0893699125426
At time: 23.705485582351685 and batch: 850, loss is 4.398520736694336 and perplexity is 81.33047045601869
At time: 24.34631323814392 and batch: 900, loss is 4.350332794189453 and perplexity is 77.5042515984524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.590252183888056 and perplexity of 98.51927200252078
finished 2 epochs...
Completing Train Step...
At time: 25.934507846832275 and batch: 50, loss is 4.40771333694458 and perplexity is 82.08155588400192
At time: 26.578239917755127 and batch: 100, loss is 4.249200758934021 and perplexity is 70.04940360741668
At time: 27.223531246185303 and batch: 150, loss is 4.237562947273254 and perplexity is 69.23890719129754
At time: 27.868192434310913 and batch: 200, loss is 4.139657363891602 and perplexity is 62.7813066209529
At time: 28.512845516204834 and batch: 250, loss is 4.286479549407959 and perplexity is 72.71004527987529
At time: 29.156503200531006 and batch: 300, loss is 4.270025963783264 and perplexity is 71.52349261557237
At time: 29.802655935287476 and batch: 350, loss is 4.241255121231079 and perplexity is 69.49502180019282
At time: 30.44616413116455 and batch: 400, loss is 4.178817734718323 and perplexity is 65.28861909974991
At time: 31.090932607650757 and batch: 450, loss is 4.198542809486389 and perplexity is 66.58922711858457
At time: 31.734686136245728 and batch: 500, loss is 4.082185139656067 and perplexity is 59.274852269374136
At time: 32.378801107406616 and batch: 550, loss is 4.17267689704895 and perplexity is 64.88892078548054
At time: 33.02235412597656 and batch: 600, loss is 4.177792344093323 and perplexity is 65.22170707315095
At time: 33.66690278053284 and batch: 650, loss is 4.025227446556091 and perplexity is 55.99304275818008
At time: 34.30980920791626 and batch: 700, loss is 4.03869131565094 and perplexity is 56.752023700756695
At time: 34.95376968383789 and batch: 750, loss is 4.145615916252137 and perplexity is 63.15650904525305
At time: 35.59837508201599 and batch: 800, loss is 4.093027248382568 and perplexity is 59.92101320624857
At time: 36.242067098617554 and batch: 850, loss is 4.163114223480225 and perplexity is 64.27136665804643
At time: 36.886505365371704 and batch: 900, loss is 4.125534844398499 and perplexity is 61.90090775207794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.523137758855951 and perplexity of 92.12420849731687
finished 3 epochs...
Completing Train Step...
At time: 38.469698905944824 and batch: 50, loss is 4.195001811981201 and perplexity is 66.35385180911801
At time: 39.11850714683533 and batch: 100, loss is 4.043965988159179 and perplexity is 57.0521629121047
At time: 39.75541925430298 and batch: 150, loss is 4.0362693881988525 and perplexity is 56.61474072839946
At time: 40.39242959022522 and batch: 200, loss is 3.942082891464233 and perplexity is 51.52581228193695
At time: 41.02888512611389 and batch: 250, loss is 4.093611364364624 and perplexity is 59.95602425198354
At time: 41.6764349937439 and batch: 300, loss is 4.077830715179443 and perplexity is 59.017305542634936
At time: 42.31989026069641 and batch: 350, loss is 4.048369750976563 and perplexity is 57.30396112867069
At time: 42.962411403656006 and batch: 400, loss is 3.997032947540283 and perplexity is 54.436394544968444
At time: 43.612690448760986 and batch: 450, loss is 4.017534880638123 and perplexity is 55.563965055976965
At time: 44.25013613700867 and batch: 500, loss is 3.9027658224105837 and perplexity is 49.53927663950894
At time: 44.888062477111816 and batch: 550, loss is 3.991376757621765 and perplexity is 54.12936109725167
At time: 45.52540731430054 and batch: 600, loss is 4.00540937423706 and perplexity is 54.894292108856554
At time: 46.16385006904602 and batch: 650, loss is 3.85428599357605 and perplexity is 47.19490744121627
At time: 46.803874254226685 and batch: 700, loss is 3.8616315126419067 and perplexity is 47.542854896465116
At time: 47.44269561767578 and batch: 750, loss is 3.9720798921585083 and perplexity is 53.09484763364045
At time: 48.080666303634644 and batch: 800, loss is 3.9238052225112914 and perplexity is 50.59259504050943
At time: 48.71867108345032 and batch: 850, loss is 3.996271905899048 and perplexity is 54.39498194227737
At time: 49.354127168655396 and batch: 900, loss is 3.9626775979995728 and perplexity is 52.59797379528751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5021909948897685 and perplexity of 90.21457459483244
finished 4 epochs...
Completing Train Step...
At time: 50.91954159736633 and batch: 50, loss is 4.03655963897705 and perplexity is 56.631175585953066
At time: 51.565956592559814 and batch: 100, loss is 3.8893722438812257 and perplexity is 48.88019205077155
At time: 52.2015655040741 and batch: 150, loss is 3.8841685390472414 and perplexity is 48.62649461487713
At time: 52.838135957717896 and batch: 200, loss is 3.7928872156143187 and perplexity is 44.38436268453438
At time: 53.47531604766846 and batch: 250, loss is 3.946682920455933 and perplexity is 51.76337849908433
At time: 54.111772298812866 and batch: 300, loss is 3.9309942865371705 and perplexity is 50.957618963552534
At time: 54.768346548080444 and batch: 350, loss is 3.9033622217178343 and perplexity is 49.56883064189571
At time: 55.40456223487854 and batch: 400, loss is 3.8546339416503907 and perplexity is 47.2113316756038
At time: 56.04151916503906 and batch: 450, loss is 3.8788254690170287 and perplexity is 48.367372718504704
At time: 56.67812705039978 and batch: 500, loss is 3.764322247505188 and perplexity is 43.134461461879454
At time: 57.31389379501343 and batch: 550, loss is 3.8488389110565184 and perplexity is 46.938531769505886
At time: 57.978368043899536 and batch: 600, loss is 3.8693842935562133 and perplexity is 47.91287673025966
At time: 58.622512102127075 and batch: 650, loss is 3.718299446105957 and perplexity is 41.19428141462794
At time: 59.26019287109375 and batch: 700, loss is 3.7224327993392943 and perplexity is 41.36490431033821
At time: 59.899314641952515 and batch: 750, loss is 3.8378286790847778 and perplexity is 46.42456230001429
At time: 60.53835105895996 and batch: 800, loss is 3.7898654985427855 and perplexity is 44.250448125950584
At time: 61.17568564414978 and batch: 850, loss is 3.8627553367614746 and perplexity is 47.596314737613774
At time: 61.814488887786865 and batch: 900, loss is 3.831383967399597 and perplexity is 46.12633141913517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.502358214495933 and perplexity of 90.22966150184467
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 63.4045467376709 and batch: 50, loss is 3.9202423238754274 and perplexity is 50.41265948906565
At time: 64.04882764816284 and batch: 100, loss is 3.7682892894744873 and perplexity is 43.305917542699774
At time: 64.68835806846619 and batch: 150, loss is 3.7620142841339113 and perplexity is 43.03502349851391
At time: 65.32775616645813 and batch: 200, loss is 3.6501758527755737 and perplexity is 38.481432520780096
At time: 65.96627473831177 and batch: 250, loss is 3.797125735282898 and perplexity is 44.57288592624844
At time: 66.6048583984375 and batch: 300, loss is 3.769195222854614 and perplexity is 43.34516759524069
At time: 67.24381875991821 and batch: 350, loss is 3.724059853553772 and perplexity is 41.43226203470277
At time: 67.88219547271729 and batch: 400, loss is 3.671970157623291 and perplexity is 39.32931453046607
At time: 68.52519726753235 and batch: 450, loss is 3.6781630992889403 and perplexity is 39.57363442968843
At time: 69.16876125335693 and batch: 500, loss is 3.5530063486099244 and perplexity is 34.91813593769169
At time: 69.8128890991211 and batch: 550, loss is 3.618629741668701 and perplexity is 37.28644074532618
At time: 70.46286702156067 and batch: 600, loss is 3.6285674667358396 and perplexity is 37.6588304306288
At time: 71.10649347305298 and batch: 650, loss is 3.4654488229751585 and perplexity is 31.99081476412486
At time: 71.75424408912659 and batch: 700, loss is 3.453946795463562 and perplexity is 31.6249635811731
At time: 72.39277815818787 and batch: 750, loss is 3.547386813163757 and perplexity is 34.72246254678725
At time: 73.03204941749573 and batch: 800, loss is 3.4804886198043823 and perplexity is 32.475586414414174
At time: 73.66997456550598 and batch: 850, loss is 3.533501262664795 and perplexity is 34.243653992994396
At time: 74.32192182540894 and batch: 900, loss is 3.490111036300659 and perplexity is 32.78958833869623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4299905855361725 and perplexity of 83.93062674469984
finished 6 epochs...
Completing Train Step...
At time: 75.9105327129364 and batch: 50, loss is 3.801306920051575 and perplexity is 44.759643559986614
At time: 76.54595112800598 and batch: 100, loss is 3.652462139129639 and perplexity is 38.569512744777285
At time: 77.18164157867432 and batch: 150, loss is 3.6472830295562746 and perplexity is 38.37027339863578
At time: 77.81879806518555 and batch: 200, loss is 3.544197883605957 and perplexity is 34.61191142321035
At time: 78.46354651451111 and batch: 250, loss is 3.6937094926834106 and perplexity is 40.19366888034717
At time: 79.11052823066711 and batch: 300, loss is 3.673830018043518 and perplexity is 39.40252962974386
At time: 79.75394821166992 and batch: 350, loss is 3.6322824478149416 and perplexity is 37.79899246158724
At time: 80.3946750164032 and batch: 400, loss is 3.5855253458023073 and perplexity is 36.07230317501813
At time: 81.03480815887451 and batch: 450, loss is 3.5986914443969726 and perplexity is 36.550374939176415
At time: 81.67103409767151 and batch: 500, loss is 3.4774758863449096 and perplexity is 32.37789336406133
At time: 82.30677461624146 and batch: 550, loss is 3.5489456033706666 and perplexity is 34.776629788079504
At time: 82.94176626205444 and batch: 600, loss is 3.5658279180526735 and perplexity is 35.368723690752056
At time: 83.58247423171997 and batch: 650, loss is 3.4088855361938477 and perplexity is 30.231533528272625
At time: 84.23288583755493 and batch: 700, loss is 3.4041686725616453 and perplexity is 30.08927128678757
At time: 84.87009787559509 and batch: 750, loss is 3.5048214530944826 and perplexity is 33.275502084050714
At time: 85.50744938850403 and batch: 800, loss is 3.4450636672973634 and perplexity is 31.34527905234366
At time: 86.14337253570557 and batch: 850, loss is 3.5070557451248168 and perplexity is 33.349932391720714
At time: 86.79017853736877 and batch: 900, loss is 3.473230309486389 and perplexity is 32.24072192126082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43503309276006 and perplexity of 84.35491637910891
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.37153267860413 and batch: 50, loss is 3.7523510456085205 and perplexity is 42.621168611160634
At time: 89.02214646339417 and batch: 100, loss is 3.6093939685821534 and perplexity is 36.94365701238835
At time: 89.66101861000061 and batch: 150, loss is 3.6070587968826295 and perplexity is 36.85748787907372
At time: 90.32028102874756 and batch: 200, loss is 3.4978253984451295 and perplexity is 33.0435172883433
At time: 90.95696663856506 and batch: 250, loss is 3.646491165161133 and perplexity is 38.339901372149804
At time: 91.59412717819214 and batch: 300, loss is 3.6244284534454345 and perplexity is 37.50328216121615
At time: 92.23213315010071 and batch: 350, loss is 3.577721605300903 and perplexity is 35.79189980233785
At time: 92.86874675750732 and batch: 400, loss is 3.530865683555603 and perplexity is 34.15352096244832
At time: 93.50632667541504 and batch: 450, loss is 3.537936110496521 and perplexity is 34.395856635977744
At time: 94.14314365386963 and batch: 500, loss is 3.41093346118927 and perplexity is 30.29350888021092
At time: 94.78018736839294 and batch: 550, loss is 3.474882278442383 and perplexity is 32.29402660971647
At time: 95.41843819618225 and batch: 600, loss is 3.4912099027633667 and perplexity is 32.82563952173793
At time: 96.05501747131348 and batch: 650, loss is 3.327367935180664 and perplexity is 27.864902867214415
At time: 96.6923623085022 and batch: 700, loss is 3.315823864936829 and perplexity is 27.545078062975257
At time: 97.32987427711487 and batch: 750, loss is 3.4110684776306153 and perplexity is 30.29759927810506
At time: 97.96689796447754 and batch: 800, loss is 3.3449399948120115 and perplexity is 28.35887393603606
At time: 98.60485816001892 and batch: 850, loss is 3.3988409852981567 and perplexity is 29.929391332755248
At time: 99.24228024482727 and batch: 900, loss is 3.3627637577056886 and perplexity is 28.868867279792322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.420123374625428 and perplexity of 83.10653796386697
finished 8 epochs...
Completing Train Step...
At time: 100.82800102233887 and batch: 50, loss is 3.720569610595703 and perplexity is 41.287905440236436
At time: 101.49174690246582 and batch: 100, loss is 3.5748169469833373 and perplexity is 35.688087405635386
At time: 102.13505172729492 and batch: 150, loss is 3.5711924695968627 and perplexity is 35.558970871195314
At time: 102.77783274650574 and batch: 200, loss is 3.46535870552063 and perplexity is 31.987931963227314
At time: 103.42140316963196 and batch: 250, loss is 3.6132213497161865 and perplexity is 37.0853254047861
At time: 104.06475472450256 and batch: 300, loss is 3.5935469245910645 and perplexity is 36.362823655680224
At time: 104.70762419700623 and batch: 350, loss is 3.548329153060913 and perplexity is 34.75519833026755
At time: 105.35095500946045 and batch: 400, loss is 3.503114171028137 and perplexity is 33.2187398844243
At time: 105.99393391609192 and batch: 450, loss is 3.5128801107406615 and perplexity is 33.5447413598494
At time: 106.64917349815369 and batch: 500, loss is 3.388215641975403 and perplexity is 29.613064789510855
At time: 107.29218649864197 and batch: 550, loss is 3.454614214897156 and perplexity is 31.646077741670624
At time: 107.93528509140015 and batch: 600, loss is 3.4741392993927 and perplexity is 32.27004173574832
At time: 108.57779812812805 and batch: 650, loss is 3.313821759223938 and perplexity is 27.48998507422013
At time: 109.22658205032349 and batch: 700, loss is 3.305456519126892 and perplexity is 27.2609839090788
At time: 109.8694715499878 and batch: 750, loss is 3.403749318122864 and perplexity is 30.07665586266569
At time: 110.51237487792969 and batch: 800, loss is 3.3410725593566895 and perplexity is 28.24940963100862
At time: 111.15918564796448 and batch: 850, loss is 3.3987948274612427 and perplexity is 29.928009888673657
At time: 111.81125283241272 and batch: 900, loss is 3.366163830757141 and perplexity is 28.967190595962947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.421927256126926 and perplexity of 83.25658760555172
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 113.40568542480469 and batch: 50, loss is 3.7049665737152098 and perplexity is 40.64868855952965
At time: 114.06247138977051 and batch: 100, loss is 3.5611609172821046 and perplexity is 35.20404241276761
At time: 114.70515394210815 and batch: 150, loss is 3.5584763193130495 and perplexity is 35.10966045749124
At time: 115.34918260574341 and batch: 200, loss is 3.451510853767395 and perplexity is 31.54802076589894
At time: 116.00090289115906 and batch: 250, loss is 3.5986411952972412 and perplexity is 36.54853836188444
At time: 116.64981842041016 and batch: 300, loss is 3.5786046981811523 and perplexity is 35.823521334543855
At time: 117.29322218894958 and batch: 350, loss is 3.5325449562072753 and perplexity is 34.21092221884981
At time: 117.93816041946411 and batch: 400, loss is 3.4873109006881715 and perplexity is 32.697901472407864
At time: 118.5824146270752 and batch: 450, loss is 3.495335168838501 and perplexity is 32.961333713740075
At time: 119.22687864303589 and batch: 500, loss is 3.3675938510894774 and perplexity is 29.00864389995542
At time: 119.87103414535522 and batch: 550, loss is 3.4316032123565674 and perplexity is 30.9261842670377
At time: 120.51511454582214 and batch: 600, loss is 3.4510970735549926 and perplexity is 31.534969519525518
At time: 121.15902614593506 and batch: 650, loss is 3.289200701713562 and perplexity is 26.821416773345383
At time: 121.80296778678894 and batch: 700, loss is 3.2777546882629394 and perplexity is 26.516168745279625
At time: 122.44666409492493 and batch: 750, loss is 3.373856520652771 and perplexity is 29.190885515046496
At time: 123.1193037033081 and batch: 800, loss is 3.309321322441101 and perplexity is 27.36654610720989
At time: 123.76849412918091 and batch: 850, loss is 3.364060053825378 and perplexity is 28.906314146297067
At time: 124.41318607330322 and batch: 900, loss is 3.331412968635559 and perplexity is 27.977845606108186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42061876270869 and perplexity of 83.14771815166044
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.99697875976562 and batch: 50, loss is 3.6973620367050173 and perplexity is 40.34074646548525
At time: 126.63350319862366 and batch: 100, loss is 3.553562126159668 and perplexity is 34.93754804763252
At time: 127.26945471763611 and batch: 150, loss is 3.551235032081604 and perplexity is 34.85633961295031
At time: 127.90518260002136 and batch: 200, loss is 3.4445615720748903 and perplexity is 31.32954468788956
At time: 128.54736375808716 and batch: 250, loss is 3.591666660308838 and perplexity is 36.29451617534177
At time: 129.18359851837158 and batch: 300, loss is 3.571632056236267 and perplexity is 35.574605555848734
At time: 129.81978821754456 and batch: 350, loss is 3.525605812072754 and perplexity is 33.97434945413603
At time: 130.4562749862671 and batch: 400, loss is 3.4807777309417727 and perplexity is 32.48497682551067
At time: 131.09292221069336 and batch: 450, loss is 3.4890316200256346 and perplexity is 32.754213818743054
At time: 131.72953867912292 and batch: 500, loss is 3.360962777137756 and perplexity is 28.81692180124249
At time: 132.36502194404602 and batch: 550, loss is 3.4246994781494142 and perplexity is 30.713413412611278
At time: 133.00116872787476 and batch: 600, loss is 3.4447683811187746 and perplexity is 31.33602459109999
At time: 133.6466224193573 and batch: 650, loss is 3.281727056503296 and perplexity is 26.621710217871748
At time: 134.28380250930786 and batch: 700, loss is 3.2700269508361814 and perplexity is 26.31204846541792
At time: 134.92112398147583 and batch: 750, loss is 3.3659681224823 and perplexity is 28.961522031774965
At time: 135.5574185848236 and batch: 800, loss is 3.3008945322036745 and perplexity is 27.13690290013655
At time: 136.195570230484 and batch: 850, loss is 3.3550584602355955 and perplexity is 28.64727886842693
At time: 136.83276963233948 and batch: 900, loss is 3.3217909383773803 and perplexity is 27.70993292809526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42061876270869 and perplexity of 83.14771815166044
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.40987300872803 and batch: 50, loss is 3.695307970046997 and perplexity is 40.25796892760566
At time: 139.06169199943542 and batch: 100, loss is 3.5514738035202025 and perplexity is 34.86466330499438
At time: 139.6989130973816 and batch: 150, loss is 3.5491544580459595 and perplexity is 34.78389380833757
At time: 140.33585047721863 and batch: 200, loss is 3.4426040124893187 and perplexity is 31.268275226250733
At time: 140.97316002845764 and batch: 250, loss is 3.5898457670211794 and perplexity is 36.22848786795405
At time: 141.61101460456848 and batch: 300, loss is 3.5696477127075195 and perplexity is 35.5040833108473
At time: 142.24833989143372 and batch: 350, loss is 3.52355535030365 and perplexity is 33.904757721426414
At time: 142.8839294910431 and batch: 400, loss is 3.47887610912323 and perplexity is 32.4232613831181
At time: 143.5214455127716 and batch: 450, loss is 3.4872397661209105 and perplexity is 32.69557560406208
At time: 144.15688037872314 and batch: 500, loss is 3.3591698884963987 and perplexity is 28.76530255707172
At time: 144.7924723625183 and batch: 550, loss is 3.4227886056900023 and perplexity is 30.65478003508107
At time: 145.42730069160461 and batch: 600, loss is 3.4431671285629273 and perplexity is 31.28588785313589
At time: 146.0629494190216 and batch: 650, loss is 3.2797142457962036 and perplexity is 26.56817964603103
At time: 146.69911289215088 and batch: 700, loss is 3.267929639816284 and perplexity is 26.25692174536128
At time: 147.3347873687744 and batch: 750, loss is 3.3639325714111328 and perplexity is 28.902629334462105
At time: 147.96913719177246 and batch: 800, loss is 3.2986609029769896 and perplexity is 27.07635676467851
At time: 148.60524249076843 and batch: 850, loss is 3.352754373550415 and perplexity is 28.581349037809414
At time: 149.2414848804474 and batch: 900, loss is 3.319324564933777 and perplexity is 27.641674095884433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.420571105120933 and perplexity of 83.14375562640878
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 150.80725598335266 and batch: 50, loss is 3.6947893381118773 and perplexity is 40.23709527261666
At time: 151.45597958564758 and batch: 100, loss is 3.5509670400619506 and perplexity is 34.846999643674216
At time: 152.09272289276123 and batch: 150, loss is 3.5486407136917113 and perplexity is 34.766028368802054
At time: 152.72940945625305 and batch: 200, loss is 3.4420964336395263 and perplexity is 31.252408138316184
At time: 153.37095403671265 and batch: 250, loss is 3.5893666124343873 and perplexity is 36.211132979988705
At time: 154.01688432693481 and batch: 300, loss is 3.5691406202316283 and perplexity is 35.486084021374786
At time: 154.6640908718109 and batch: 350, loss is 3.5230129528045655 and perplexity is 33.886372852035564
At time: 155.3160719871521 and batch: 400, loss is 3.4783982133865354 and perplexity is 32.40777014662614
At time: 155.95541286468506 and batch: 450, loss is 3.4867637395858764 and perplexity is 32.68001534633795
At time: 156.5916392803192 and batch: 500, loss is 3.358708815574646 and perplexity is 28.752042712089274
At time: 157.22919130325317 and batch: 550, loss is 3.4222915410995483 and perplexity is 30.6395464157575
At time: 157.86629176139832 and batch: 600, loss is 3.4427463102340696 and perplexity is 31.27272494788285
At time: 158.5031657218933 and batch: 650, loss is 3.2792021226882935 and perplexity is 26.554576950723785
At time: 159.13934087753296 and batch: 700, loss is 3.267397651672363 and perplexity is 26.242957089142003
At time: 159.7894926071167 and batch: 750, loss is 3.363414225578308 and perplexity is 28.88765165913206
At time: 160.4291603565216 and batch: 800, loss is 3.298099646568298 and perplexity is 27.061164249767316
At time: 161.06591033935547 and batch: 850, loss is 3.3521747398376465 and perplexity is 28.564787124735975
At time: 161.70261573791504 and batch: 900, loss is 3.3186990642547607 and perplexity is 27.62438961625873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.420554801209332 and perplexity of 83.14240006901733
Annealing...
Model not improving. Stopping early with 83.10653796386697 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f61b3402b70>
ELAPSED
559.4702134132385


RESULTS SO FAR:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.34674739898118, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.10653796386697, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.10738997417746055, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.6689429055087728, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.010038557844579299, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.03467218496325619, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8785221576690674 and batch: 50, loss is 6.443124561309815 and perplexity is 628.3671072178693
At time: 1.5286309719085693 and batch: 100, loss is 5.585287389755249 and perplexity is 266.4768544539561
At time: 2.1724932193756104 and batch: 150, loss is 5.354631071090698 and perplexity is 211.58590179582586
At time: 2.816525459289551 and batch: 200, loss is 5.147655467987061 and perplexity is 172.02769271094178
At time: 3.4611520767211914 and batch: 250, loss is 5.163261528015137 and perplexity is 174.73342520708536
At time: 4.119095325469971 and batch: 300, loss is 5.088659896850586 and perplexity is 162.17238863886854
At time: 4.763517618179321 and batch: 350, loss is 5.032950572967529 and perplexity is 153.38491907223298
At time: 5.407445669174194 and batch: 400, loss is 4.888900966644287 and perplexity is 132.8075339048351
At time: 6.054363965988159 and batch: 450, loss is 4.876084442138672 and perplexity is 131.116264143252
At time: 6.698851585388184 and batch: 500, loss is 4.781090478897095 and perplexity is 119.23430166696315
At time: 7.343866586685181 and batch: 550, loss is 4.846731090545655 and perplexity is 127.32349983703779
At time: 7.988912105560303 and batch: 600, loss is 4.784951019287109 and perplexity is 119.69550016934707
At time: 8.63121771812439 and batch: 650, loss is 4.641006174087525 and perplexity is 103.64858365375161
At time: 9.27423644065857 and batch: 700, loss is 4.689360303878784 and perplexity is 108.78356911772904
At time: 9.917259454727173 and batch: 750, loss is 4.724731693267822 and perplexity is 112.70025607972065
At time: 10.559934139251709 and batch: 800, loss is 4.649432182312012 and perplexity is 104.52561722919965
At time: 11.202670097351074 and batch: 850, loss is 4.7104072093963625 and perplexity is 111.09739059725543
At time: 11.845713138580322 and batch: 900, loss is 4.644447450637817 and perplexity is 104.0058815219331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.733791403574486 and perplexity of 113.72592687599418
finished 1 epochs...
Completing Train Step...
At time: 13.422728538513184 and batch: 50, loss is 4.6962177944183345 and perplexity is 109.53211505377674
At time: 14.072141647338867 and batch: 100, loss is 4.531764850616455 and perplexity is 92.92241062827682
At time: 14.70937728881836 and batch: 150, loss is 4.507997035980225 and perplexity is 90.73988764075703
At time: 15.347681045532227 and batch: 200, loss is 4.400068950653076 and perplexity is 81.45648494917721
At time: 15.984579801559448 and batch: 250, loss is 4.533484144210815 and perplexity is 93.08230895033503
At time: 16.62113857269287 and batch: 300, loss is 4.50383584022522 and perplexity is 90.36308572247898
At time: 17.25905966758728 and batch: 350, loss is 4.479720592498779 and perplexity is 88.21002269009395
At time: 17.910653591156006 and batch: 400, loss is 4.392853970527649 and perplexity is 80.87089308630014
At time: 18.5478355884552 and batch: 450, loss is 4.4057369041442875 and perplexity is 81.91948741612877
At time: 19.185235023498535 and batch: 500, loss is 4.300305738449096 and perplexity is 73.72233000515224
At time: 19.832058429718018 and batch: 550, loss is 4.384183950424195 and perplexity is 80.17277155441609
At time: 20.471707105636597 and batch: 600, loss is 4.374721269607544 and perplexity is 79.41770034018529
At time: 21.109450817108154 and batch: 650, loss is 4.221189823150635 and perplexity is 68.11448027994251
At time: 21.746384620666504 and batch: 700, loss is 4.250095295906067 and perplexity is 70.11209342381221
At time: 22.38356924057007 and batch: 750, loss is 4.336848073005676 and perplexity is 76.46614340541416
At time: 23.02099871635437 and batch: 800, loss is 4.2777753257751465 and perplexity is 72.07990719526236
At time: 23.685955286026 and batch: 850, loss is 4.351418247222901 and perplexity is 77.58842449805522
At time: 24.325648069381714 and batch: 900, loss is 4.30671802520752 and perplexity is 74.19657760646602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.576357906811858 and perplexity of 97.1598836682531
finished 2 epochs...
Completing Train Step...
At time: 25.915110111236572 and batch: 50, loss is 4.382591571807861 and perplexity is 80.04520773928135
At time: 26.557844400405884 and batch: 100, loss is 4.22562759399414 and perplexity is 68.4174284446321
At time: 27.200674057006836 and batch: 150, loss is 4.2151752281188966 and perplexity is 67.70602883160949
At time: 27.843270540237427 and batch: 200, loss is 4.110865960121155 and perplexity is 60.9995178547239
At time: 28.486727952957153 and batch: 250, loss is 4.258008375167846 and perplexity is 70.66909687603821
At time: 29.128634214401245 and batch: 300, loss is 4.237777009010315 and perplexity is 69.25373017850369
At time: 29.77162456512451 and batch: 350, loss is 4.213315448760986 and perplexity is 67.58022757433302
At time: 30.41435694694519 and batch: 400, loss is 4.146021909713745 and perplexity is 63.18215538075345
At time: 31.072879791259766 and batch: 450, loss is 4.163481407165527 and perplexity is 64.29497038850252
At time: 31.722093105316162 and batch: 500, loss is 4.05610689163208 and perplexity is 57.74904957161371
At time: 32.365397691726685 and batch: 550, loss is 4.142885890007019 and perplexity is 62.984325258079565
At time: 33.00926399230957 and batch: 600, loss is 4.1470310020446775 and perplexity is 63.24594418818527
At time: 33.65246629714966 and batch: 650, loss is 3.9913943576812745 and perplexity is 54.13031378561185
At time: 34.29595232009888 and batch: 700, loss is 4.012333235740662 and perplexity is 55.27569143914215
At time: 34.93870568275452 and batch: 750, loss is 4.115716743469238 and perplexity is 61.29613212434128
At time: 35.58176803588867 and batch: 800, loss is 4.059814686775208 and perplexity is 57.9635686678043
At time: 36.225014209747314 and batch: 850, loss is 4.138043022155761 and perplexity is 62.680037900598016
At time: 36.869088649749756 and batch: 900, loss is 4.097081341743469 and perplexity is 60.16443167527065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.517596519156678 and perplexity of 91.615137919573
finished 3 epochs...
Completing Train Step...
At time: 38.4605348110199 and batch: 50, loss is 4.178507628440857 and perplexity is 65.26837582806503
At time: 39.115479469299316 and batch: 100, loss is 4.0300013017654415 and perplexity is 56.260984485685476
At time: 39.760356426239014 and batch: 150, loss is 4.024762811660767 and perplexity is 55.96703247972871
At time: 40.404977560043335 and batch: 200, loss is 3.9223807144165037 and perplexity is 50.52057678680902
At time: 41.06188988685608 and batch: 250, loss is 4.070873770713806 and perplexity is 58.60815031080274
At time: 41.70663356781006 and batch: 300, loss is 4.0566280698776245 and perplexity is 57.77915496440775
At time: 42.350178718566895 and batch: 350, loss is 4.031649656295777 and perplexity is 56.35379900894821
At time: 42.994887590408325 and batch: 400, loss is 3.9732908964157105 and perplexity is 53.15918466850916
At time: 43.63949775695801 and batch: 450, loss is 3.9931716299057007 and perplexity is 54.22660363008967
At time: 44.28510046005249 and batch: 500, loss is 3.8824632501602174 and perplexity is 48.54364305699369
At time: 44.92968511581421 and batch: 550, loss is 3.971275095939636 and perplexity is 53.052134291097026
At time: 45.57517194747925 and batch: 600, loss is 3.9821529674530027 and perplexity is 53.63237878234601
At time: 46.219449043273926 and batch: 650, loss is 3.824500765800476 and perplexity is 45.809924775544374
At time: 46.86444330215454 and batch: 700, loss is 3.8439273262023925 and perplexity is 46.70855442673318
At time: 47.50903582572937 and batch: 750, loss is 3.951260919570923 and perplexity is 52.000894459146544
At time: 48.153555393218994 and batch: 800, loss is 3.8991621017456053 and perplexity is 49.36107221689401
At time: 48.79724740982056 and batch: 850, loss is 3.9738438940048217 and perplexity is 53.18858969917615
At time: 49.44176697731018 and batch: 900, loss is 3.9423407983779906 and perplexity is 51.539102858953605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499729104238014 and perplexity of 89.99274934377985
finished 4 epochs...
Completing Train Step...
At time: 51.02618980407715 and batch: 50, loss is 4.024157724380493 and perplexity is 55.933177783816426
At time: 51.6631224155426 and batch: 100, loss is 3.8836640882492066 and perplexity is 48.601971126829625
At time: 52.2995126247406 and batch: 150, loss is 3.879852523803711 and perplexity is 48.417074178866955
At time: 52.93668580055237 and batch: 200, loss is 3.779186267852783 and perplexity is 43.780401716531166
At time: 53.57435321807861 and batch: 250, loss is 3.92905029296875 and perplexity is 50.85865390491077
At time: 54.21233105659485 and batch: 300, loss is 3.9153045177459718 and perplexity is 50.164345118680195
At time: 54.85163474082947 and batch: 350, loss is 3.8920695972442627 and perplexity is 49.01221718035113
At time: 55.50312876701355 and batch: 400, loss is 3.838857479095459 and perplexity is 46.4723484671942
At time: 56.140472650527954 and batch: 450, loss is 3.855476026535034 and perplexity is 47.25110436803301
At time: 56.77776265144348 and batch: 500, loss is 3.7465303325653077 and perplexity is 42.373803637779986
At time: 57.41449689865112 and batch: 550, loss is 3.831662130355835 and perplexity is 46.13916384051252
At time: 58.0520236492157 and batch: 600, loss is 3.8505467224121093 and perplexity is 47.018762416981836
At time: 58.68915033340454 and batch: 650, loss is 3.6939532279968263 and perplexity is 40.20346669081675
At time: 59.32634162902832 and batch: 700, loss is 3.7108271169662475 and perplexity is 40.88761138186181
At time: 59.96395015716553 and batch: 750, loss is 3.8214750862121583 and perplexity is 45.671528098790695
At time: 60.60805630683899 and batch: 800, loss is 3.767919020652771 and perplexity is 43.289885679870544
At time: 61.2539381980896 and batch: 850, loss is 3.8455086088180543 and perplexity is 46.782472278955865
At time: 61.89012002944946 and batch: 900, loss is 3.8174167108535766 and perplexity is 45.48655150068626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.500491625642123 and perplexity of 90.06139691066247
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 63.465832233428955 and batch: 50, loss is 3.9102058696746824 and perplexity is 49.909225711906934
At time: 64.10166072845459 and batch: 100, loss is 3.7668702363967896 and perplexity is 43.24450772932576
At time: 64.73865056037903 and batch: 150, loss is 3.7615529537200927 and perplexity is 43.0151747120911
At time: 65.374196767807 and batch: 200, loss is 3.637908492088318 and perplexity is 38.01225060403042
At time: 66.00976538658142 and batch: 250, loss is 3.783487629890442 and perplexity is 43.96912266205182
At time: 66.66106629371643 and batch: 300, loss is 3.762158923149109 and perplexity is 43.04124849210937
At time: 67.30661725997925 and batch: 350, loss is 3.719379258155823 and perplexity is 41.23878752087544
At time: 67.94697523117065 and batch: 400, loss is 3.6578265237808227 and perplexity is 38.776970390760994
At time: 68.5993447303772 and batch: 450, loss is 3.6602712774276736 and perplexity is 38.871886506523
At time: 69.23428773880005 and batch: 500, loss is 3.542393627166748 and perplexity is 34.5495189620773
At time: 69.86947345733643 and batch: 550, loss is 3.604146890640259 and perplexity is 36.75031843944657
At time: 70.5040602684021 and batch: 600, loss is 3.613827357292175 and perplexity is 37.10780620401872
At time: 71.1522569656372 and batch: 650, loss is 3.440554995536804 and perplexity is 31.20427159489221
At time: 71.78674650192261 and batch: 700, loss is 3.4401152753829956 and perplexity is 31.190553464077347
At time: 72.42319059371948 and batch: 750, loss is 3.532167744636536 and perplexity is 34.19801989674877
At time: 73.05861949920654 and batch: 800, loss is 3.461026611328125 and perplexity is 31.849656955369408
At time: 73.69450092315674 and batch: 850, loss is 3.5179055547714233 and perplexity is 33.71374287823672
At time: 74.3306155204773 and batch: 900, loss is 3.473800663948059 and perplexity is 32.25911580587452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431796557282748 and perplexity of 84.08234003899986
finished 6 epochs...
Completing Train Step...
At time: 75.89359307289124 and batch: 50, loss is 3.7888160228729246 and perplexity is 44.20403271744308
At time: 76.54201340675354 and batch: 100, loss is 3.651578311920166 and perplexity is 38.53543901981802
At time: 77.17876672744751 and batch: 150, loss is 3.6479058361053465 and perplexity is 38.394178099427535
At time: 77.81535720825195 and batch: 200, loss is 3.53069438457489 and perplexity is 34.14767100018029
At time: 78.45316004753113 and batch: 250, loss is 3.6788378953933716 and perplexity is 39.60034757598965
At time: 79.09066987037659 and batch: 300, loss is 3.66345899105072 and perplexity is 38.99599665740711
At time: 79.73070049285889 and batch: 350, loss is 3.6262716341018675 and perplexity is 37.57247122983682
At time: 80.37350368499756 and batch: 400, loss is 3.571112399101257 and perplexity is 35.55612376076073
At time: 81.01112985610962 and batch: 450, loss is 3.5793869209289553 and perplexity is 35.85155427040882
At time: 81.6483564376831 and batch: 500, loss is 3.466025776863098 and perplexity is 32.00927731458936
At time: 82.28573179244995 and batch: 550, loss is 3.532730345726013 and perplexity is 34.21726515319414
At time: 82.9222719669342 and batch: 600, loss is 3.550501518249512 and perplexity is 34.83078138051256
At time: 83.55935049057007 and batch: 650, loss is 3.3835768699645996 and perplexity is 29.476014651339685
At time: 84.1952965259552 and batch: 700, loss is 3.389640045166016 and perplexity is 29.655275789085188
At time: 84.83295607566833 and batch: 750, loss is 3.4899316930770876 and perplexity is 32.78370827551449
At time: 85.47075748443604 and batch: 800, loss is 3.425869655609131 and perplexity is 30.749374593075565
At time: 86.10863208770752 and batch: 850, loss is 3.4924243021011354 and perplexity is 32.86552717150062
At time: 86.74482893943787 and batch: 900, loss is 3.4570808887481688 and perplexity is 31.724234648293372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.437086131474743 and perplexity of 84.5282781865674
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.32357358932495 and batch: 50, loss is 3.7397005128860474 and perplexity is 42.08538424716838
At time: 88.97012066841125 and batch: 100, loss is 3.6104988193511964 and perplexity is 36.984496797029074
At time: 89.60509824752808 and batch: 150, loss is 3.6087849855422975 and perplexity is 36.92116580091581
At time: 90.24151945114136 and batch: 200, loss is 3.4883485555648805 and perplexity is 32.731848218786375
At time: 90.876389503479 and batch: 250, loss is 3.6338211059570313 and perplexity is 37.85719695602757
At time: 91.51190233230591 and batch: 300, loss is 3.6149252891540526 and perplexity is 37.148570420845786
At time: 92.14720964431763 and batch: 350, loss is 3.5738438034057616 and perplexity is 35.6533746655601
At time: 92.78293085098267 and batch: 400, loss is 3.516380648612976 and perplexity is 33.66237176211766
At time: 93.41843676567078 and batch: 450, loss is 3.519637885093689 and perplexity is 33.7721968334631
At time: 94.05433297157288 and batch: 500, loss is 3.4019621658325194 and perplexity is 30.02295230077035
At time: 94.69028210639954 and batch: 550, loss is 3.46001042842865 and perplexity is 31.81730831746722
At time: 95.32727932929993 and batch: 600, loss is 3.4759063005447386 and perplexity is 32.32711334462151
At time: 95.96325421333313 and batch: 650, loss is 3.3040512895584104 and perplexity is 27.222702871546154
At time: 96.5994930267334 and batch: 700, loss is 3.301299157142639 and perplexity is 27.147885389560926
At time: 97.23504900932312 and batch: 750, loss is 3.3954811477661133 and perplexity is 29.82900218050198
At time: 97.87088680267334 and batch: 800, loss is 3.3258042478561403 and perplexity is 27.82136492054082
At time: 98.50615286827087 and batch: 850, loss is 3.3832384157180786 and perplexity is 29.46604005707753
At time: 99.14263367652893 and batch: 900, loss is 3.3456875896453857 and perplexity is 28.380082810505638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424970652959118 and perplexity of 83.5103564042358
finished 8 epochs...
Completing Train Step...
At time: 100.74658179283142 and batch: 50, loss is 3.707149248123169 and perplexity is 40.73750830880053
At time: 101.38501405715942 and batch: 100, loss is 3.574961051940918 and perplexity is 35.69323060652858
At time: 102.0213360786438 and batch: 150, loss is 3.5728377962112425 and perplexity is 35.61752514959662
At time: 102.65780735015869 and batch: 200, loss is 3.4543987131118774 and perplexity is 31.639258690205505
At time: 103.30791473388672 and batch: 250, loss is 3.599478645324707 and perplexity is 36.57915875607485
At time: 103.94653177261353 and batch: 300, loss is 3.582851824760437 and perplexity is 35.97599191590701
At time: 104.58493161201477 and batch: 350, loss is 3.54358594417572 and perplexity is 34.590737509091845
At time: 105.22337532043457 and batch: 400, loss is 3.4883954334259033 and perplexity is 32.73338265378343
At time: 105.8620057106018 and batch: 450, loss is 3.494308819770813 and perplexity is 32.92752123428477
At time: 106.50067973136902 and batch: 500, loss is 3.3786426258087157 and perplexity is 29.330931031668417
At time: 107.13977026939392 and batch: 550, loss is 3.439569129943848 and perplexity is 31.173523536390416
At time: 107.77822160720825 and batch: 600, loss is 3.4589111948013307 and perplexity is 31.78235287785098
At time: 108.41683888435364 and batch: 650, loss is 3.2896554183959963 and perplexity is 26.83361569231453
At time: 109.05633997917175 and batch: 700, loss is 3.290580167770386 and perplexity is 26.858441538712395
At time: 109.69832682609558 and batch: 750, loss is 3.3883717346191404 and perplexity is 29.617687531861673
At time: 110.34551095962524 and batch: 800, loss is 3.321853313446045 and perplexity is 27.71166139097028
At time: 110.98538613319397 and batch: 850, loss is 3.3840073204040526 and perplexity is 29.48870534596126
At time: 111.62432479858398 and batch: 900, loss is 3.3493959426879885 and perplexity is 28.485521558163896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426884063302654 and perplexity of 83.67029895303801
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 113.22568655014038 and batch: 50, loss is 3.6912882280349733 and perplexity is 40.096467093908124
At time: 113.86988186836243 and batch: 100, loss is 3.5620338487625123 and perplexity is 35.234786546436105
At time: 114.51523351669312 and batch: 150, loss is 3.560271725654602 and perplexity is 35.172753186120715
At time: 115.15978336334229 and batch: 200, loss is 3.4422416734695434 and perplexity is 31.256947562406427
At time: 115.80462646484375 and batch: 250, loss is 3.586384515762329 and perplexity is 36.10330873190915
At time: 116.4513988494873 and batch: 300, loss is 3.5676735496520995 and perplexity is 35.43406160113609
At time: 117.09661674499512 and batch: 350, loss is 3.527495174407959 and perplexity is 34.03859998751378
At time: 117.74127626419067 and batch: 400, loss is 3.4727078580856325 and perplexity is 32.223882110306114
At time: 118.3927595615387 and batch: 450, loss is 3.4761624908447266 and perplexity is 32.335396298447925
At time: 119.03992509841919 and batch: 500, loss is 3.3583672571182253 and perplexity is 28.742223885708636
At time: 119.69851779937744 and batch: 550, loss is 3.417543816566467 and perplexity is 30.494423065003943
At time: 120.34338140487671 and batch: 600, loss is 3.436158437728882 and perplexity is 31.067381354403082
At time: 120.98842120170593 and batch: 650, loss is 3.2651974821090697 and perplexity is 26.185281604892186
At time: 121.63217496871948 and batch: 700, loss is 3.263088665008545 and perplexity is 26.130119818763212
At time: 122.27555561065674 and batch: 750, loss is 3.3584685850143434 and perplexity is 28.745136422342846
At time: 122.91817259788513 and batch: 800, loss is 3.2904229402542113 and perplexity is 26.85421898462049
At time: 123.5614504814148 and batch: 850, loss is 3.348129143714905 and perplexity is 28.449458975550286
At time: 124.20414638519287 and batch: 900, loss is 3.313298993110657 and perplexity is 27.475617997206424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425489451787243 and perplexity of 83.55369271971745
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.78146767616272 and batch: 50, loss is 3.683861966133118 and perplexity is 39.799803143427326
At time: 126.43884444236755 and batch: 100, loss is 3.5542687702178957 and perplexity is 34.962245183383835
At time: 127.08353018760681 and batch: 150, loss is 3.5528448104858397 and perplexity is 34.91249578307804
At time: 127.72775316238403 and batch: 200, loss is 3.4353649377822877 and perplexity is 31.042739167037702
At time: 128.38539266586304 and batch: 250, loss is 3.5794482803344727 and perplexity is 35.85375416795724
At time: 129.04040837287903 and batch: 300, loss is 3.5606675291061403 and perplexity is 35.186677438683716
At time: 129.69763588905334 and batch: 350, loss is 3.520904278755188 and perplexity is 33.814992822032714
At time: 130.34389305114746 and batch: 400, loss is 3.4665116119384765 and perplexity is 32.024832322524574
At time: 130.99016046524048 and batch: 450, loss is 3.4695597505569458 and perplexity is 32.12259737569813
At time: 131.63606905937195 and batch: 500, loss is 3.351643500328064 and perplexity is 28.54961641123049
At time: 132.28246450424194 and batch: 550, loss is 3.41012047290802 and perplexity is 30.268890621027428
At time: 132.92850041389465 and batch: 600, loss is 3.4292635345458984 and perplexity is 30.85391154035588
At time: 133.57497024536133 and batch: 650, loss is 3.2579599809646607 and perplexity is 25.996449758933398
At time: 134.22138953208923 and batch: 700, loss is 3.255539364814758 and perplexity is 25.933598432963297
At time: 134.8673539161682 and batch: 750, loss is 3.350631628036499 and perplexity is 28.52074245629027
At time: 135.52189421653748 and batch: 800, loss is 3.2820513439178467 and perplexity is 26.630344703401047
At time: 136.18147563934326 and batch: 850, loss is 3.33884819984436 and perplexity is 28.18664262205467
At time: 136.8273787498474 and batch: 900, loss is 3.303485617637634 and perplexity is 27.20730810752746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4252340238388275 and perplexity of 83.53235349683649
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.41397976875305 and batch: 50, loss is 3.6819844245910645 and perplexity is 39.72514746615882
At time: 139.06402826309204 and batch: 100, loss is 3.5520942783355713 and perplexity is 34.88630266316558
At time: 139.70257711410522 and batch: 150, loss is 3.550646963119507 and perplexity is 34.835847707408966
At time: 140.35168623924255 and batch: 200, loss is 3.433185477256775 and perplexity is 30.975156416140024
At time: 140.99231553077698 and batch: 250, loss is 3.577449998855591 and perplexity is 35.78217981172737
At time: 141.63166570663452 and batch: 300, loss is 3.5586683845520017 and perplexity is 35.11640445043911
At time: 142.27107858657837 and batch: 350, loss is 3.519011654853821 and perplexity is 33.751054283282286
At time: 142.90897226333618 and batch: 400, loss is 3.4646475172042845 and perplexity is 31.965190607402818
At time: 143.54904222488403 and batch: 450, loss is 3.4677236890792846 and perplexity is 32.06367242356513
At time: 144.18688869476318 and batch: 500, loss is 3.349718437194824 and perplexity is 28.494709463835676
At time: 144.82456016540527 and batch: 550, loss is 3.4080544996261595 and perplexity is 30.206420454799833
At time: 145.46290040016174 and batch: 600, loss is 3.4273733854293824 and perplexity is 30.795648127332438
At time: 146.10114669799805 and batch: 650, loss is 3.2560138082504273 and perplexity is 25.945905377746826
At time: 146.73844575881958 and batch: 700, loss is 3.2535197734832764 and perplexity is 25.88127601510889
At time: 147.37567687034607 and batch: 750, loss is 3.3486530780792236 and perplexity is 28.46436853022681
At time: 148.01175713539124 and batch: 800, loss is 3.2798468255996704 and perplexity is 26.571702283577107
At time: 148.6484055519104 and batch: 850, loss is 3.336487627029419 and perplexity is 28.12018447025024
At time: 149.3034770488739 and batch: 900, loss is 3.300995583534241 and perplexity is 27.139645258839224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425144979398545 and perplexity of 83.5249157363244
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 150.9347813129425 and batch: 50, loss is 3.681520981788635 and perplexity is 39.706741397899556
At time: 151.58617854118347 and batch: 100, loss is 3.551568102836609 and perplexity is 34.867951173930244
At time: 152.263573884964 and batch: 150, loss is 3.550101089477539 and perplexity is 34.81683692556432
At time: 152.91318011283875 and batch: 200, loss is 3.432638373374939 and perplexity is 30.958214422762037
At time: 153.55309081077576 and batch: 250, loss is 3.5769348669052126 and perplexity is 35.76375201443325
At time: 154.1912281513214 and batch: 300, loss is 3.558155965805054 and perplexity is 35.098414755995286
At time: 154.82822608947754 and batch: 350, loss is 3.5185049200057983 and perplexity is 33.733955780488934
At time: 155.48177814483643 and batch: 400, loss is 3.4641903829574585 and perplexity is 31.950581563471406
At time: 156.12861371040344 and batch: 450, loss is 3.4672457075119016 and perplexity is 32.048350241316115
At time: 156.76981735229492 and batch: 500, loss is 3.349217357635498 and perplexity is 28.480434924013146
At time: 157.40689182281494 and batch: 550, loss is 3.407527542114258 and perplexity is 30.190507147820085
At time: 158.04566717147827 and batch: 600, loss is 3.4268797969818117 and perplexity is 30.78045150192923
At time: 158.68402695655823 and batch: 650, loss is 3.2555084466934203 and perplexity is 25.93279662721545
At time: 159.3218674659729 and batch: 700, loss is 3.253002109527588 and perplexity is 25.867881678570374
At time: 159.95877742767334 and batch: 750, loss is 3.34814697265625 and perplexity is 28.449966203807318
At time: 160.59696292877197 and batch: 800, loss is 3.279292497634888 and perplexity is 26.5569769276448
At time: 161.23577547073364 and batch: 850, loss is 3.3358973693847656 and perplexity is 28.103591214027542
At time: 161.87356114387512 and batch: 900, loss is 3.3003625583648684 and perplexity is 27.12247061686638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425118224261558 and perplexity of 83.52268104565694
Annealing...
Model not improving. Stopping early with 83.5103564042358 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f61b3402b70>
ELAPSED
727.8177118301392


RESULTS SO FAR:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.34674739898118, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.10653796386697, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.10738997417746055, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.6689429055087728, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.5103564042358, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.010038557844579299, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.03467218496325619, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.7667358092686467, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.2410374528905116, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8556323051452637 and batch: 50, loss is 7.0631730842590335 and perplexity is 1168.145916690996
At time: 1.512739658355713 and batch: 100, loss is 6.187004537582397 and perplexity is 486.38696802555006
At time: 2.1566696166992188 and batch: 150, loss is 6.091104373931885 and perplexity is 441.9091747325364
At time: 2.8143410682678223 and batch: 200, loss is 5.947169704437256 and perplexity is 382.6687392813729
At time: 3.457322120666504 and batch: 250, loss is 6.00335862159729 and perplexity is 404.7860361078621
At time: 4.101454019546509 and batch: 300, loss is 5.915647029876709 and perplexity is 370.7941399917067
At time: 4.746044874191284 and batch: 350, loss is 5.915771913528443 and perplexity is 370.84044900950965
At time: 5.389652252197266 and batch: 400, loss is 5.792724266052246 and perplexity is 327.9051094437269
At time: 6.032512903213501 and batch: 450, loss is 5.799163808822632 and perplexity is 330.02348177490046
At time: 6.676893472671509 and batch: 500, loss is 5.757159004211426 and perplexity is 316.44802313989544
At time: 7.321040630340576 and batch: 550, loss is 5.808192863464355 and perplexity is 333.0167747926413
At time: 7.977096319198608 and batch: 600, loss is 5.749789552688599 and perplexity is 314.12454666293075
At time: 8.61984133720398 and batch: 650, loss is 5.654535970687866 and perplexity is 285.5839326595047
At time: 9.26202130317688 and batch: 700, loss is 5.7747164058685305 and perplexity is 322.053089400415
At time: 9.905184507369995 and batch: 750, loss is 5.725002193450928 and perplexity is 306.43393739043137
At time: 10.549378395080566 and batch: 800, loss is 5.723611125946045 and perplexity is 306.00796344568107
At time: 11.19429612159729 and batch: 850, loss is 5.754298429489136 and perplexity is 315.54409341963344
At time: 11.838136434555054 and batch: 900, loss is 5.655317926406861 and perplexity is 285.8073339824885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.442072202081549 and perplexity of 230.92020133665326
finished 1 epochs...
Completing Train Step...
At time: 13.428738355636597 and batch: 50, loss is 5.324041662216186 and perplexity is 205.21160416639455
At time: 14.064544677734375 and batch: 100, loss is 5.120033626556396 and perplexity is 167.34099662795566
At time: 14.700706005096436 and batch: 150, loss is 5.053544635772705 and perplexity is 156.5764886221608
At time: 15.336002349853516 and batch: 200, loss is 4.916396169662476 and perplexity is 136.50976761207025
At time: 15.97074580192566 and batch: 250, loss is 4.985856513977051 and perplexity is 146.32885407563148
At time: 16.605701446533203 and batch: 300, loss is 4.933306283950806 and perplexity is 138.837791483746
At time: 17.240882635116577 and batch: 350, loss is 4.893454694747925 and perplexity is 133.41368237456683
At time: 17.875234842300415 and batch: 400, loss is 4.7767563152313235 and perplexity is 118.71863897943221
At time: 18.51087236404419 and batch: 450, loss is 4.768765563964844 and perplexity is 117.77376800677472
At time: 19.146408557891846 and batch: 500, loss is 4.677230062484742 and perplexity is 107.47196925873854
At time: 19.789544582366943 and batch: 550, loss is 4.751978902816773 and perplexity is 115.81324106005985
At time: 20.425156593322754 and batch: 600, loss is 4.701929426193237 and perplexity is 110.15951218757586
At time: 21.0606529712677 and batch: 650, loss is 4.570014219284058 and perplexity is 96.54548257072572
At time: 21.695249795913696 and batch: 700, loss is 4.606775579452514 and perplexity is 100.16066827983812
At time: 22.33080792427063 and batch: 750, loss is 4.65517300605774 and perplexity is 105.12740610379153
At time: 22.96584701538086 and batch: 800, loss is 4.591235618591309 and perplexity is 98.61620693032071
At time: 23.600438117980957 and batch: 850, loss is 4.646622142791748 and perplexity is 104.23230841159524
At time: 24.25890278816223 and batch: 900, loss is 4.59720856666565 and perplexity is 99.20699904247903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.692648691673801 and perplexity of 109.14188048920144
finished 2 epochs...
Completing Train Step...
At time: 25.838067054748535 and batch: 50, loss is 4.595390462875367 and perplexity is 99.02679428661104
At time: 26.47401523590088 and batch: 100, loss is 4.444344382286072 and perplexity is 85.14403759511409
At time: 27.110127210617065 and batch: 150, loss is 4.445542650222778 and perplexity is 85.24612411665373
At time: 27.745155811309814 and batch: 200, loss is 4.339461154937744 and perplexity is 76.66621699365591
At time: 28.38080358505249 and batch: 250, loss is 4.476792545318603 and perplexity is 87.95211734566296
At time: 29.01661705970764 and batch: 300, loss is 4.453077278137207 and perplexity is 85.89084776932472
At time: 29.651257038116455 and batch: 350, loss is 4.427176780700684 and perplexity is 83.69479429017069
At time: 30.286601305007935 and batch: 400, loss is 4.35964605808258 and perplexity is 78.2294408441231
At time: 30.922184944152832 and batch: 450, loss is 4.363888731002808 and perplexity is 78.56204784680074
At time: 31.558053731918335 and batch: 500, loss is 4.256253385543824 and perplexity is 70.54518211065013
At time: 32.20483207702637 and batch: 550, loss is 4.35322642326355 and perplexity is 77.72884494244718
At time: 32.842804193496704 and batch: 600, loss is 4.344091267585754 and perplexity is 77.0220132674188
At time: 33.47990369796753 and batch: 650, loss is 4.201841487884521 and perplexity is 66.80924625030003
At time: 34.11651945114136 and batch: 700, loss is 4.2149905014038085 and perplexity is 67.69352287443952
At time: 34.75336408615112 and batch: 750, loss is 4.311080408096314 and perplexity is 74.52095850944957
At time: 35.389315128326416 and batch: 800, loss is 4.2546496057510375 and perplexity is 70.43213384959205
At time: 36.02668786048889 and batch: 850, loss is 4.321947164535523 and perplexity is 75.33517555201942
At time: 36.663443088531494 and batch: 900, loss is 4.288945722579956 and perplexity is 72.88958213637324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561310232502141 and perplexity of 95.70879849237161
finished 3 epochs...
Completing Train Step...
At time: 38.23403239250183 and batch: 50, loss is 4.324366712570191 and perplexity is 75.51767331999389
At time: 38.88918375968933 and batch: 100, loss is 4.179220862388611 and perplexity is 65.31494405446622
At time: 39.53174138069153 and batch: 150, loss is 4.186001958847046 and perplexity is 65.75935608775082
At time: 40.18648171424866 and batch: 200, loss is 4.079516034126282 and perplexity is 59.11685238638635
At time: 40.8287091255188 and batch: 250, loss is 4.226289730072022 and perplexity is 68.46274509359961
At time: 41.47130084037781 and batch: 300, loss is 4.213835940361023 and perplexity is 67.61541167083308
At time: 42.11355710029602 and batch: 350, loss is 4.184626026153564 and perplexity is 65.66893785877095
At time: 42.75625133514404 and batch: 400, loss is 4.133462948799133 and perplexity is 62.39361514831766
At time: 43.399147748947144 and batch: 450, loss is 4.139925150871277 and perplexity is 62.798120888653486
At time: 44.04148602485657 and batch: 500, loss is 4.035690007209777 and perplexity is 56.581948724369184
At time: 44.68457007408142 and batch: 550, loss is 4.129490618705749 and perplexity is 62.14625872916415
At time: 45.327824115753174 and batch: 600, loss is 4.134161734580994 and perplexity is 62.437230156461204
At time: 45.970332622528076 and batch: 650, loss is 3.989831886291504 and perplexity is 54.04580275922613
At time: 46.61162567138672 and batch: 700, loss is 3.992481393814087 and perplexity is 54.18918738564521
At time: 47.25396108627319 and batch: 750, loss is 4.098703503608704 and perplexity is 60.262107323414845
At time: 47.895830392837524 and batch: 800, loss is 4.050435175895691 and perplexity is 57.42244047092435
At time: 48.53867268562317 and batch: 850, loss is 4.122110590934754 and perplexity is 61.6893058503331
At time: 49.18041443824768 and batch: 900, loss is 4.091127901077271 and perplexity is 59.8073104060203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.51010633494756 and perplexity of 90.93148719138314
finished 4 epochs...
Completing Train Step...
At time: 50.757542848587036 and batch: 50, loss is 4.141022319793701 and perplexity is 62.86705884665619
At time: 51.41419267654419 and batch: 100, loss is 3.999226565361023 and perplexity is 54.55593825881303
At time: 52.05833029747009 and batch: 150, loss is 4.006769533157349 and perplexity is 54.96900787107769
At time: 52.71772646903992 and batch: 200, loss is 3.900709023475647 and perplexity is 49.43748902229432
At time: 53.36664128303528 and batch: 250, loss is 4.052337923049927 and perplexity is 57.53180486952311
At time: 54.0220422744751 and batch: 300, loss is 4.043592367172241 and perplexity is 57.030851008225724
At time: 54.670037269592285 and batch: 350, loss is 4.014159321784973 and perplexity is 55.37672182488299
At time: 55.31332230567932 and batch: 400, loss is 3.9703289461135864 and perplexity is 53.001962762100455
At time: 55.95748281478882 and batch: 450, loss is 3.9761676168441773 and perplexity is 53.312328952037795
At time: 56.623136043548584 and batch: 500, loss is 3.8753597927093506 and perplexity is 48.200037193679115
At time: 57.26719069480896 and batch: 550, loss is 3.964255204200745 and perplexity is 52.68101817335526
At time: 57.91641139984131 and batch: 600, loss is 3.9767862033843993 and perplexity is 53.3453174432218
At time: 58.56875729560852 and batch: 650, loss is 3.8323893308639527 and perplexity is 46.17272846652887
At time: 59.21319317817688 and batch: 700, loss is 3.8347230339050293 and perplexity is 46.280607733491976
At time: 59.85713028907776 and batch: 750, loss is 3.943667492866516 and perplexity is 51.607524880183455
At time: 60.501917600631714 and batch: 800, loss is 3.8952445459365843 and perplexity is 49.1680757457448
At time: 61.14604949951172 and batch: 850, loss is 3.9703480005264282 and perplexity is 53.002972693002135
At time: 61.79040884971619 and batch: 900, loss is 3.940642681121826 and perplexity is 51.45165768611315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.49663846786708 and perplexity of 89.71504384395102
finished 5 epochs...
Completing Train Step...
At time: 63.37617826461792 and batch: 50, loss is 3.9948228216171264 and perplexity is 54.31621611187581
At time: 64.0115258693695 and batch: 100, loss is 3.859923830032349 and perplexity is 47.461736072265474
At time: 64.65967535972595 and batch: 150, loss is 3.8676274728775026 and perplexity is 47.828776293966975
At time: 65.30073571205139 and batch: 200, loss is 3.763767833709717 and perplexity is 43.110553749379974
At time: 65.93714308738708 and batch: 250, loss is 3.913223104476929 and perplexity is 50.0600409727902
At time: 66.57289862632751 and batch: 300, loss is 3.9094627904891968 and perplexity is 49.872152980809986
At time: 67.2092924118042 and batch: 350, loss is 3.876857328414917 and perplexity is 48.27227254439506
At time: 67.84647011756897 and batch: 400, loss is 3.8376691722869873 and perplexity is 46.41715785728817
At time: 68.48389482498169 and batch: 450, loss is 3.846311535835266 and perplexity is 46.820050274055504
At time: 69.11983823776245 and batch: 500, loss is 3.746014289855957 and perplexity is 42.35194258644847
At time: 69.75682473182678 and batch: 550, loss is 3.8302136135101317 and perplexity is 46.072378865700756
At time: 70.39468574523926 and batch: 600, loss is 3.847587475776672 and perplexity is 46.87982797452668
At time: 71.03117370605469 and batch: 650, loss is 3.703940958976746 and perplexity is 40.60702003702172
At time: 71.66901278495789 and batch: 700, loss is 3.7056662034988403 and perplexity is 40.67713754342921
At time: 72.31410527229309 and batch: 750, loss is 3.8167836809158326 and perplexity is 45.45776626374123
At time: 72.9721884727478 and batch: 800, loss is 3.7688590621948244 and perplexity is 43.330599103917194
At time: 73.6090247631073 and batch: 850, loss is 3.8449305868148804 and perplexity is 46.7554387943429
At time: 74.24682545661926 and batch: 900, loss is 3.8181642436981202 and perplexity is 45.52056690414971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.503948472950556 and perplexity of 90.37326413629422
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.82411289215088 and batch: 50, loss is 3.887833318710327 and perplexity is 48.805026944437714
At time: 76.46186947822571 and batch: 100, loss is 3.7469680881500245 and perplexity is 42.392357067605595
At time: 77.09844183921814 and batch: 150, loss is 3.7498475980758665 and perplexity is 42.51460219912934
At time: 77.73540043830872 and batch: 200, loss is 3.6270789909362793 and perplexity is 37.602817869905216
At time: 78.3718810081482 and batch: 250, loss is 3.7694620990753176 and perplexity is 43.35673693347623
At time: 79.00821924209595 and batch: 300, loss is 3.756307291984558 and perplexity is 42.79012244614926
At time: 79.64491558074951 and batch: 350, loss is 3.7096176195144652 and perplexity is 40.83818781495019
At time: 80.29294276237488 and batch: 400, loss is 3.6629444551467896 and perplexity is 38.97593697817248
At time: 80.943190574646 and batch: 450, loss is 3.655807542800903 and perplexity is 38.698759404897096
At time: 81.5844235420227 and batch: 500, loss is 3.5469800806045533 and perplexity is 34.708342662436735
At time: 82.23338747024536 and batch: 550, loss is 3.6124338817596438 and perplexity is 37.05613339476662
At time: 82.8754723072052 and batch: 600, loss is 3.6185571718215943 and perplexity is 37.2837349722021
At time: 83.51698160171509 and batch: 650, loss is 3.4565662479400636 and perplexity is 31.707912262980496
At time: 84.16723203659058 and batch: 700, loss is 3.4444568586349487 and perplexity is 31.32626423525022
At time: 84.80518817901611 and batch: 750, loss is 3.537119073867798 and perplexity is 34.36776543856114
At time: 85.44151616096497 and batch: 800, loss is 3.4708341264724734 and perplexity is 32.16355973531649
At time: 86.07945895195007 and batch: 850, loss is 3.525445747375488 and perplexity is 33.96891179537606
At time: 86.71595311164856 and batch: 900, loss is 3.485362658500671 and perplexity is 32.63426005589289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.438084014474529 and perplexity of 84.61266961775534
finished 7 epochs...
Completing Train Step...
At time: 88.28188347816467 and batch: 50, loss is 3.7756857585906984 and perplexity is 43.62741593511547
At time: 88.93319487571716 and batch: 100, loss is 3.636232075691223 and perplexity is 37.94857962827283
At time: 89.57095098495483 and batch: 150, loss is 3.6444703006744383 and perplexity is 38.26249986236625
At time: 90.20788788795471 and batch: 200, loss is 3.5262230253219604 and perplexity is 33.99532534538539
At time: 90.84370136260986 and batch: 250, loss is 3.6715432262420653 and perplexity is 39.31252719566593
At time: 91.4809627532959 and batch: 300, loss is 3.663810987472534 and perplexity is 39.009725524809944
At time: 92.12073588371277 and batch: 350, loss is 3.6218548679351805 and perplexity is 37.40688834972144
At time: 92.75694370269775 and batch: 400, loss is 3.582654204368591 and perplexity is 35.96888302874121
At time: 93.39353322982788 and batch: 450, loss is 3.5798467063903807 and perplexity is 35.868042083969875
At time: 94.03004670143127 and batch: 500, loss is 3.4754814863204957 and perplexity is 32.313383243621594
At time: 94.6663122177124 and batch: 550, loss is 3.5451649951934816 and perplexity is 34.64540119537722
At time: 95.30346465110779 and batch: 600, loss is 3.5586382055282595 and perplexity is 35.115344687626845
At time: 95.9399254322052 and batch: 650, loss is 3.4018813943862916 and perplexity is 30.020527401425618
At time: 96.57535886764526 and batch: 700, loss is 3.396474142074585 and perplexity is 29.858636921023862
At time: 97.21137547492981 and batch: 750, loss is 3.4970943450927736 and perplexity is 33.01936954197695
At time: 97.84780359268188 and batch: 800, loss is 3.4383896255493163 and perplexity is 31.136775904643148
At time: 98.48601365089417 and batch: 850, loss is 3.502113924026489 and perplexity is 33.18552955149325
At time: 99.12193846702576 and batch: 900, loss is 3.4709149408340454 and perplexity is 32.166159117894686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.443382367695848 and perplexity of 85.06216717526023
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.69965982437134 and batch: 50, loss is 3.730929980278015 and perplexity is 41.71788694158139
At time: 101.34553217887878 and batch: 100, loss is 3.596378359794617 and perplexity is 36.46592853318654
At time: 101.98044300079346 and batch: 150, loss is 3.606905460357666 and perplexity is 36.8518367132396
At time: 102.61580467224121 and batch: 200, loss is 3.4852347373962402 and perplexity is 32.63008571230326
At time: 103.25110578536987 and batch: 250, loss is 3.6277208614349363 and perplexity is 37.626961757157815
At time: 103.88686895370483 and batch: 300, loss is 3.6176486110687254 and perplexity is 37.24987581776459
At time: 104.52322936058044 and batch: 350, loss is 3.568857579231262 and perplexity is 35.47604142595447
At time: 105.1709725856781 and batch: 400, loss is 3.529855794906616 and perplexity is 34.119047119606606
At time: 105.80505990982056 and batch: 450, loss is 3.519604368209839 and perplexity is 33.771064913633786
At time: 106.44026494026184 and batch: 500, loss is 3.410926413536072 and perplexity is 30.29329538281851
At time: 107.07606315612793 and batch: 550, loss is 3.4732117652893066 and perplexity is 32.24012404850299
At time: 107.71104097366333 and batch: 600, loss is 3.4866360092163085 and perplexity is 32.675841382476925
At time: 108.3461241722107 and batch: 650, loss is 3.3249456882476807 and perplexity is 27.79748887134019
At time: 108.98180747032166 and batch: 700, loss is 3.310986738204956 and perplexity is 27.412160757634936
At time: 109.6167221069336 and batch: 750, loss is 3.406324782371521 and perplexity is 30.154217049719268
At time: 110.2516348361969 and batch: 800, loss is 3.341369462013245 and perplexity is 28.257798201006928
At time: 110.88666105270386 and batch: 850, loss is 3.397954611778259 and perplexity is 29.902874466471392
At time: 111.52207207679749 and batch: 900, loss is 3.3627753496170043 and perplexity is 28.869201927081214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430354706228596 and perplexity of 83.96119318722504
finished 9 epochs...
Completing Train Step...
At time: 113.0981092453003 and batch: 50, loss is 3.699354395866394 and perplexity is 40.421199840678824
At time: 113.7354302406311 and batch: 100, loss is 3.5632544803619384 and perplexity is 35.27782149986244
At time: 114.37310814857483 and batch: 150, loss is 3.5737597846984865 and perplexity is 35.650379240948325
At time: 115.00967383384705 and batch: 200, loss is 3.4529874420166013 and perplexity is 31.59463861185924
At time: 115.64776730537415 and batch: 250, loss is 3.5969864416122435 and perplexity is 36.488109544541814
At time: 116.28408789634705 and batch: 300, loss is 3.588036217689514 and perplexity is 36.162989910710316
At time: 116.92517685890198 and batch: 350, loss is 3.540823431015015 and perplexity is 34.495312009270506
At time: 117.5720329284668 and batch: 400, loss is 3.5035879039764404 and perplexity is 33.234480424120434
At time: 118.2094476222992 and batch: 450, loss is 3.4966146755218506 and perplexity is 33.003534953145426
At time: 118.84697031974792 and batch: 500, loss is 3.3898185110092163 and perplexity is 29.66056871517344
At time: 119.48353910446167 and batch: 550, loss is 3.454218726158142 and perplexity is 31.633564548866357
At time: 120.12158727645874 and batch: 600, loss is 3.4704383754730226 and perplexity is 32.150833492783036
At time: 120.75882172584534 and batch: 650, loss is 3.3115560483932494 and perplexity is 27.427771223214307
At time: 121.40867352485657 and batch: 700, loss is 3.300900421142578 and perplexity is 27.137062708170312
At time: 122.04602551460266 and batch: 750, loss is 3.399859838485718 and perplexity is 29.95990052807264
At time: 122.68222904205322 and batch: 800, loss is 3.3380732631683347 and perplexity is 28.164808220144916
At time: 123.31843614578247 and batch: 850, loss is 3.3985904932022093 and perplexity is 29.921895195690613
At time: 123.9551682472229 and batch: 900, loss is 3.3664601182937623 and perplexity is 28.975774485094348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432389768835616 and perplexity of 84.13223345171447
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.54188919067383 and batch: 50, loss is 3.6848722314834594 and perplexity is 39.840031822892456
At time: 126.18473291397095 and batch: 100, loss is 3.550521078109741 and perplexity is 34.831462672391005
At time: 126.82743668556213 and batch: 150, loss is 3.562145709991455 and perplexity is 35.23872817341424
At time: 127.47005462646484 and batch: 200, loss is 3.4405461406707762 and perplexity is 31.203995286471073
At time: 128.11316514015198 and batch: 250, loss is 3.584432339668274 and perplexity is 36.0328974656432
At time: 128.7554361820221 and batch: 300, loss is 3.5741599655151366 and perplexity is 35.66464869381707
At time: 129.39893698692322 and batch: 350, loss is 3.524968204498291 and perplexity is 33.95269405614317
At time: 130.0418198108673 and batch: 400, loss is 3.4882380294799806 and perplexity is 32.72823069567023
At time: 130.68428254127502 and batch: 450, loss is 3.4795397090911866 and perplexity is 32.44478459894581
At time: 131.34079790115356 and batch: 500, loss is 3.3700018310546875 and perplexity is 29.078580302219095
At time: 131.98990106582642 and batch: 550, loss is 3.4318112087249757 and perplexity is 30.932617470072213
At time: 132.6350257396698 and batch: 600, loss is 3.4483240842819214 and perplexity is 31.44764451892312
At time: 133.2766077518463 and batch: 650, loss is 3.287483353614807 and perplexity is 26.775394593606244
At time: 133.91834378242493 and batch: 700, loss is 3.2739175939559937 and perplexity is 26.414618658371214
At time: 134.560711145401 and batch: 750, loss is 3.3717321586608886 and perplexity is 29.128939328707318
At time: 135.20335030555725 and batch: 800, loss is 3.3075836515426635 and perplexity is 27.31903334917251
At time: 135.8456289768219 and batch: 850, loss is 3.365772495269775 and perplexity is 28.955856924092096
At time: 136.48824501037598 and batch: 900, loss is 3.332757496833801 and perplexity is 28.01548790834939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430804945018194 and perplexity of 83.99900428459145
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.06278371810913 and batch: 50, loss is 3.67686607837677 and perplexity is 39.52233987051142
At time: 138.7186450958252 and batch: 100, loss is 3.543274931907654 and perplexity is 34.57998103814898
At time: 139.36448097229004 and batch: 150, loss is 3.556030297279358 and perplexity is 35.02388639979998
At time: 140.00751399993896 and batch: 200, loss is 3.4341084480285646 and perplexity is 31.00375877770513
At time: 140.64995765686035 and batch: 250, loss is 3.5783156871795656 and perplexity is 35.81316943874054
At time: 141.2937252521515 and batch: 300, loss is 3.567601647377014 and perplexity is 35.43151390308518
At time: 141.93735146522522 and batch: 350, loss is 3.5184546422958376 and perplexity is 33.73225975708082
At time: 142.58317279815674 and batch: 400, loss is 3.4818059635162353 and perplexity is 32.518396115320115
At time: 143.22635126113892 and batch: 450, loss is 3.473720383644104 and perplexity is 32.256526138203355
At time: 143.87044310569763 and batch: 500, loss is 3.363631567955017 and perplexity is 28.893930852343868
At time: 144.51427340507507 and batch: 550, loss is 3.424978380203247 and perplexity is 30.721980641345578
At time: 145.15847826004028 and batch: 600, loss is 3.4420802640914916 and perplexity is 31.25190280508711
At time: 145.80212593078613 and batch: 650, loss is 3.280668635368347 and perplexity is 26.59354814342513
At time: 146.44612669944763 and batch: 700, loss is 3.266696605682373 and perplexity is 26.224566016556242
At time: 147.09029006958008 and batch: 750, loss is 3.364123682975769 and perplexity is 28.908153489024414
At time: 147.73429989814758 and batch: 800, loss is 3.2994638156890868 and perplexity is 27.098105445699993
At time: 148.37872099876404 and batch: 850, loss is 3.3571517419815065 and perplexity is 28.707308501900247
At time: 149.03583693504333 and batch: 900, loss is 3.323927946090698 and perplexity is 27.769212586481856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430399855522261 and perplexity of 83.96498406136969
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 150.69155859947205 and batch: 50, loss is 3.674655623435974 and perplexity is 39.43507400323912
At time: 151.34461760520935 and batch: 100, loss is 3.541260099411011 and perplexity is 34.510378311084494
At time: 151.98632526397705 and batch: 150, loss is 3.5542137908935545 and perplexity is 34.96032303560588
At time: 152.62372517585754 and batch: 200, loss is 3.4322939109802246 and perplexity is 30.9475523185383
At time: 153.26112842559814 and batch: 250, loss is 3.5767099237442017 and perplexity is 35.75570810775033
At time: 153.92118334770203 and batch: 300, loss is 3.5656744337081907 and perplexity is 35.36329556195824
At time: 154.55823469161987 and batch: 350, loss is 3.516669821739197 and perplexity is 33.67210742297402
At time: 155.19438195228577 and batch: 400, loss is 3.479939842224121 and perplexity is 32.45776942991207
At time: 155.83146405220032 and batch: 450, loss is 3.4720524406433104 and perplexity is 32.20276893563896
At time: 156.46917486190796 and batch: 500, loss is 3.3619461631774903 and perplexity is 28.845273898093243
At time: 157.10803985595703 and batch: 550, loss is 3.423102970123291 and perplexity is 30.6644183225224
At time: 157.74453949928284 and batch: 600, loss is 3.44036940574646 and perplexity is 31.198480938029086
At time: 158.3819785118103 and batch: 650, loss is 3.2788390016555784 and perplexity is 26.544936175808132
At time: 159.01901507377625 and batch: 700, loss is 3.2647924041748047 and perplexity is 26.174676673168456
At time: 159.65523505210876 and batch: 750, loss is 3.3622053337097166 and perplexity is 28.852750711924685
At time: 160.2915232181549 and batch: 800, loss is 3.2973833179473875 and perplexity is 27.041786504545403
At time: 160.92801117897034 and batch: 850, loss is 3.354941892623901 and perplexity is 28.64393971816986
At time: 161.5652675628662 and batch: 900, loss is 3.3216743898391723 and perplexity is 27.70670356411113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430299523758562 and perplexity of 83.95656012903092
finished 13 epochs...
Completing Train Step...
At time: 163.15824222564697 and batch: 50, loss is 3.674036965370178 and perplexity is 39.41068472172252
At time: 163.79397344589233 and batch: 100, loss is 3.5406406354904174 and perplexity is 34.489006996897196
At time: 164.42997002601624 and batch: 150, loss is 3.5535566997528076 and perplexity is 34.93735846279649
At time: 165.06631469726562 and batch: 200, loss is 3.431691346168518 and perplexity is 30.928910029660408
At time: 165.70245623588562 and batch: 250, loss is 3.576056900024414 and perplexity is 35.73236640441062
At time: 166.339781999588 and batch: 300, loss is 3.5649970626831053 and perplexity is 35.33934960125861
At time: 166.9779236316681 and batch: 350, loss is 3.5160672998428346 and perplexity is 33.65182535176461
At time: 167.61362075805664 and batch: 400, loss is 3.479416027069092 and perplexity is 32.44077201052783
At time: 168.25006413459778 and batch: 450, loss is 3.471551718711853 and perplexity is 32.18664833928412
At time: 168.8881652355194 and batch: 500, loss is 3.361543951034546 and perplexity is 28.83367431156853
At time: 169.52532744407654 and batch: 550, loss is 3.422716236114502 and perplexity is 30.65256164193593
At time: 170.18605947494507 and batch: 600, loss is 3.4400798797607424 and perplexity is 31.18944947456738
At time: 170.82297110557556 and batch: 650, loss is 3.278657088279724 and perplexity is 26.540107736048753
At time: 171.46008253097534 and batch: 700, loss is 3.2646701335906982 and perplexity is 26.17147647581182
At time: 172.09669828414917 and batch: 750, loss is 3.362180995941162 and perplexity is 28.852048508900754
At time: 172.73314833641052 and batch: 800, loss is 3.297490396499634 and perplexity is 27.044682254928077
At time: 173.37022066116333 and batch: 850, loss is 3.3552185678482056 and perplexity is 28.65186588305251
At time: 174.00719904899597 and batch: 900, loss is 3.3220140600204466 and perplexity is 27.716116303656243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430227619327911 and perplexity of 83.95052349740834
finished 14 epochs...
Completing Train Step...
At time: 175.58988785743713 and batch: 50, loss is 3.6734724283218383 and perplexity is 39.38844220904862
At time: 176.22685194015503 and batch: 100, loss is 3.5400526571273803 and perplexity is 34.468734167596935
At time: 176.86423659324646 and batch: 150, loss is 3.552947201728821 and perplexity is 34.91607069993381
At time: 177.50074982643127 and batch: 200, loss is 3.4311266851425173 and perplexity is 30.911450609380953
At time: 178.13856172561646 and batch: 250, loss is 3.57546902179718 and perplexity is 35.71136629755153
At time: 178.77519178390503 and batch: 300, loss is 3.564378809928894 and perplexity is 35.317507703638015
At time: 179.41301441192627 and batch: 350, loss is 3.5155198097229006 and perplexity is 33.63340635243693
At time: 180.05092120170593 and batch: 400, loss is 3.4789143705368044 and perplexity is 32.424501966664394
At time: 180.68784427642822 and batch: 450, loss is 3.471081404685974 and perplexity is 32.17151406633351
At time: 181.3250253200531 and batch: 500, loss is 3.361162180900574 and perplexity is 28.822668576827777
At time: 181.96158599853516 and batch: 550, loss is 3.4223560523986816 and perplexity is 30.641523076459425
At time: 182.59809637069702 and batch: 600, loss is 3.4398053026199342 and perplexity is 31.18088674032661
At time: 183.23623967170715 and batch: 650, loss is 3.278480486869812 and perplexity is 26.535421129446203
At time: 183.88499116897583 and batch: 700, loss is 3.26456139087677 and perplexity is 26.16863067316529
At time: 184.52377939224243 and batch: 750, loss is 3.3621634006500245 and perplexity is 28.851540853173518
At time: 185.16030526161194 and batch: 800, loss is 3.2975942516326904 and perplexity is 27.047491129857647
At time: 185.81931138038635 and batch: 850, loss is 3.355473403930664 and perplexity is 28.65916834273486
At time: 186.45533323287964 and batch: 900, loss is 3.3223213148117066 and perplexity is 27.72463352159666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430174527102953 and perplexity of 83.94606649564662
finished 15 epochs...
Completing Train Step...
At time: 188.024071931839 and batch: 50, loss is 3.6729386949539187 and perplexity is 39.3674248924518
At time: 188.67115426063538 and batch: 100, loss is 3.539491310119629 and perplexity is 34.449390676524345
At time: 189.30553936958313 and batch: 150, loss is 3.5523703956604002 and perplexity is 34.89593670573193
At time: 189.9389145374298 and batch: 200, loss is 3.4305855083465575 and perplexity is 30.894726575319453
At time: 190.57270121574402 and batch: 250, loss is 3.5749132680892943 and perplexity is 35.69152508724178
At time: 191.20697236061096 and batch: 300, loss is 3.563799567222595 and perplexity is 35.297056218654596
At time: 191.84094262123108 and batch: 350, loss is 3.5150007390975953 and perplexity is 33.61595276938344
At time: 192.47667264938354 and batch: 400, loss is 3.478432264328003 and perplexity is 32.408873680498566
At time: 193.11128497123718 and batch: 450, loss is 3.4706357097625733 and perplexity is 32.15717858069992
At time: 193.74497961997986 and batch: 500, loss is 3.3607960081100465 and perplexity is 28.812116431916635
At time: 194.3783736228943 and batch: 550, loss is 3.422015080451965 and perplexity is 30.63107695770362
At time: 195.0126347541809 and batch: 600, loss is 3.4395427656173707 and perplexity is 31.172701678272375
At time: 195.6456708908081 and batch: 650, loss is 3.278309254646301 and perplexity is 26.53087779927777
At time: 196.27929186820984 and batch: 700, loss is 3.2644599533081053 and perplexity is 26.165976325522077
At time: 196.91352438926697 and batch: 750, loss is 3.36214732170105 and perplexity is 28.851076954449816
At time: 197.55009508132935 and batch: 800, loss is 3.297691335678101 and perplexity is 27.050117137184383
At time: 198.1978976726532 and batch: 850, loss is 3.3557074451446534 and perplexity is 28.665876554254083
At time: 198.8342273235321 and batch: 900, loss is 3.3226060104370116 and perplexity is 27.73252772714302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430133976348459 and perplexity of 83.94266248833135
finished 16 epochs...
Completing Train Step...
At time: 200.47115302085876 and batch: 50, loss is 3.672427854537964 and perplexity is 39.34731955649099
At time: 201.13172483444214 and batch: 100, loss is 3.5389525365829466 and perplexity is 34.43083525550675
At time: 201.77074146270752 and batch: 150, loss is 3.5518183088302613 and perplexity is 34.87667643581123
At time: 202.4233386516571 and batch: 200, loss is 3.430062699317932 and perplexity is 30.87857875480994
At time: 203.06130933761597 and batch: 250, loss is 3.574379415512085 and perplexity is 35.67247615970137
At time: 203.70023488998413 and batch: 300, loss is 3.5632498931884764 and perplexity is 35.27765967474702
At time: 204.3389630317688 and batch: 350, loss is 3.514502639770508 and perplexity is 33.59921285534567
At time: 204.97732496261597 and batch: 400, loss is 3.4779672479629515 and perplexity is 32.393806527372156
At time: 205.61668634414673 and batch: 450, loss is 3.4702103757858276 and perplexity is 32.14350394840229
At time: 206.25632071495056 and batch: 500, loss is 3.3604427480697634 and perplexity is 28.80194006006385
At time: 206.89522075653076 and batch: 550, loss is 3.421689705848694 and perplexity is 30.621112004450005
At time: 207.53417420387268 and batch: 600, loss is 3.439290318489075 and perplexity is 31.164833212480058
At time: 208.1731231212616 and batch: 650, loss is 3.2781425189971922 and perplexity is 26.52645452491549
At time: 208.8120973110199 and batch: 700, loss is 3.2643629360198974 and perplexity is 26.163437896593397
At time: 209.45052886009216 and batch: 750, loss is 3.3621309995651245 and perplexity is 28.850606047093287
At time: 210.0889275074005 and batch: 800, loss is 3.297781238555908 and perplexity is 27.05254912987997
At time: 210.7281515598297 and batch: 850, loss is 3.3559230089187624 and perplexity is 28.67205654485938
At time: 211.36868500709534 and batch: 900, loss is 3.3228720378875733 and perplexity is 27.739906322202803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430103040721319 and perplexity of 83.94006570959017
finished 17 epochs...
Completing Train Step...
At time: 212.977046251297 and batch: 50, loss is 3.6719355964660645 and perplexity is 39.327955287331626
At time: 213.62235569953918 and batch: 100, loss is 3.538432788848877 and perplexity is 34.41294455662843
At time: 214.2682056427002 and batch: 150, loss is 3.551285991668701 and perplexity is 34.858115922884316
At time: 214.9241440296173 and batch: 200, loss is 3.429554991722107 and perplexity is 30.86290544489377
At time: 215.5685727596283 and batch: 250, loss is 3.573863034248352 and perplexity is 35.65406031658891
At time: 216.21343183517456 and batch: 300, loss is 3.5627243089675904 and perplexity is 35.25912316514737
At time: 216.8577036857605 and batch: 350, loss is 3.514021759033203 and perplexity is 33.58305952531884
At time: 217.50339078903198 and batch: 400, loss is 3.4775173091888427 and perplexity is 32.37923457626326
At time: 218.14797019958496 and batch: 450, loss is 3.4698015069961547 and perplexity is 32.13036415925496
At time: 218.80631756782532 and batch: 500, loss is 3.3601001024246218 and perplexity is 28.79207289129838
At time: 219.450177192688 and batch: 550, loss is 3.421376953125 and perplexity is 30.611536665702758
At time: 220.09436511993408 and batch: 600, loss is 3.439046583175659 and perplexity is 31.157238167717374
At time: 220.73905038833618 and batch: 650, loss is 3.2779797649383546 and perplexity is 26.52213758808401
At time: 221.3833830356598 and batch: 700, loss is 3.264268617630005 and perplexity is 26.16097031962771
At time: 222.02780890464783 and batch: 750, loss is 3.3621136140823364 and perplexity is 28.850104469738522
At time: 222.67263269424438 and batch: 800, loss is 3.2978638935089113 and perplexity is 27.054785249468825
At time: 223.3171670436859 and batch: 850, loss is 3.3561225652694704 and perplexity is 28.67777880676863
At time: 223.9627182483673 and batch: 900, loss is 3.3231219816207886 and perplexity is 27.74684060450212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430083810466609 and perplexity of 83.93845153626677
finished 18 epochs...
Completing Train Step...
At time: 225.5646677017212 and batch: 50, loss is 3.6714589262008666 and perplexity is 39.30921328768696
At time: 226.21048760414124 and batch: 100, loss is 3.537929582595825 and perplexity is 34.39563210397413
At time: 226.85623502731323 and batch: 150, loss is 3.5507701635360718 and perplexity is 34.84013976274399
At time: 227.5020558834076 and batch: 200, loss is 3.429060354232788 and perplexity is 30.847643269764593
At time: 228.1463644504547 and batch: 250, loss is 3.573361344337463 and perplexity is 35.636177520430145
At time: 228.7928159236908 and batch: 300, loss is 3.5622188806533814 and perplexity is 35.24130670881555
At time: 229.43889117240906 and batch: 350, loss is 3.513555998802185 and perplexity is 33.56742151382066
At time: 230.08878326416016 and batch: 400, loss is 3.477080612182617 and perplexity is 32.36509774843978
At time: 230.73535561561584 and batch: 450, loss is 3.469406933784485 and perplexity is 32.11768887910397
At time: 231.3837821483612 and batch: 500, loss is 3.359766411781311 and perplexity is 28.782466848785415
At time: 232.04099106788635 and batch: 550, loss is 3.421074914932251 and perplexity is 30.60229220865081
At time: 232.69025444984436 and batch: 600, loss is 3.4388097953796386 and perplexity is 31.1498613873618
At time: 233.33578372001648 and batch: 650, loss is 3.27782030582428 and perplexity is 26.517908728694895
At time: 233.98169589042664 and batch: 700, loss is 3.264175925254822 and perplexity is 26.158545509534136
At time: 234.65034294128418 and batch: 750, loss is 3.362094831466675 and perplexity is 28.84956259440341
At time: 235.2957422733307 and batch: 800, loss is 3.297939643859863 and perplexity is 27.056834736570103
At time: 235.94170260429382 and batch: 850, loss is 3.3563076877593994 and perplexity is 28.683088200015856
At time: 236.58774828910828 and batch: 900, loss is 3.3233573389053346 and perplexity is 27.7533717941129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430070850947132 and perplexity of 83.93736374131788
finished 19 epochs...
Completing Train Step...
At time: 238.1723175048828 and batch: 50, loss is 3.6709952402114867 and perplexity is 39.29099038141158
At time: 238.825049161911 and batch: 100, loss is 3.537440299987793 and perplexity is 34.378807035835656
At time: 239.46231865882874 and batch: 150, loss is 3.550268054008484 and perplexity is 34.82265058773476
At time: 240.1009111404419 and batch: 200, loss is 3.428577494621277 and perplexity is 30.832751784257105
At time: 240.7397701740265 and batch: 250, loss is 3.572872533798218 and perplexity is 35.61876243796443
At time: 241.37999963760376 and batch: 300, loss is 3.5617302942276 and perplexity is 35.22409249038999
At time: 242.0189380645752 and batch: 350, loss is 3.5131034660339355 and perplexity is 33.55223459218903
At time: 242.65553998947144 and batch: 400, loss is 3.476655993461609 and perplexity is 32.35135783934629
At time: 243.29245853424072 and batch: 450, loss is 3.4690245485305784 and perplexity is 32.10540989628764
At time: 243.93080401420593 and batch: 500, loss is 3.3594409036636352 and perplexity is 28.773099446844803
At time: 244.5687837600708 and batch: 550, loss is 3.420782060623169 and perplexity is 30.59333150766372
At time: 245.20607495307922 and batch: 600, loss is 3.4385794496536253 and perplexity is 31.142686976254502
At time: 245.84428930282593 and batch: 650, loss is 3.2776636695861816 and perplexity is 26.51375538851968
At time: 246.48180985450745 and batch: 700, loss is 3.2640844202041626 and perplexity is 26.1561519800138
At time: 247.11835765838623 and batch: 750, loss is 3.3620744132995606 and perplexity is 28.84897354522685
At time: 247.75552582740784 and batch: 800, loss is 3.2980092477798464 and perplexity is 27.058718063872828
At time: 248.39328265190125 and batch: 850, loss is 3.356480178833008 and perplexity is 28.688036203424826
At time: 249.03023552894592 and batch: 900, loss is 3.323579578399658 and perplexity is 27.759540374851937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430064162162886 and perplexity of 83.93680230427925
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f61b3402b70>
ELAPSED
983.3086476325989


RESULTS SO FAR:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.34674739898118, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.10653796386697, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.10738997417746055, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.6689429055087728, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.5103564042358, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.010038557844579299, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.03467218496325619, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.93680230427925, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.7667358092686467, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.2410374528905116, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.02721183700724882, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.5451628532116214, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8555753231048584 and batch: 50, loss is 6.508430662155152 and perplexity is 670.7729218303972
At time: 1.5172348022460938 and batch: 100, loss is 5.605464601516724 and perplexity is 271.90822506631247
At time: 2.1620962619781494 and batch: 150, loss is 5.365197305679321 and perplexity is 213.83342106740167
At time: 2.808065414428711 and batch: 200, loss is 5.155561780929565 and perplexity is 173.39318838930618
At time: 3.455108642578125 and batch: 250, loss is 5.174028224945069 and perplexity is 176.62489118850263
At time: 4.1014180183410645 and batch: 300, loss is 5.097825536727905 and perplexity is 163.66563517736165
At time: 4.7477123737335205 and batch: 350, loss is 5.04705319404602 and perplexity is 155.5633733176028
At time: 5.393491744995117 and batch: 400, loss is 4.897684116363525 and perplexity is 133.97914002311038
At time: 6.040061950683594 and batch: 450, loss is 4.889937133789062 and perplexity is 132.94521602657298
At time: 6.68381404876709 and batch: 500, loss is 4.792350854873657 and perplexity is 120.58451240656959
At time: 7.328556060791016 and batch: 550, loss is 4.858159112930298 and perplexity is 128.78690161076295
At time: 7.9734416007995605 and batch: 600, loss is 4.80264235496521 and perplexity is 121.83191574294531
At time: 8.617755889892578 and batch: 650, loss is 4.656023950576782 and perplexity is 105.21690176633808
At time: 9.26206636428833 and batch: 700, loss is 4.706492033004761 and perplexity is 110.66327509034441
At time: 9.906976461410522 and batch: 750, loss is 4.746294517517089 and perplexity is 115.15678152716735
At time: 10.5505690574646 and batch: 800, loss is 4.67161081314087 and perplexity is 106.86975105774629
At time: 11.194361448287964 and batch: 850, loss is 4.726951999664307 and perplexity is 112.95076317752404
At time: 11.83892035484314 and batch: 900, loss is 4.666952238082886 and perplexity is 106.37304816317388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.728305346345248 and perplexity of 113.10372820198737
finished 1 epochs...
Completing Train Step...
At time: 13.431872129440308 and batch: 50, loss is 4.7112697315216066 and perplexity is 111.193255891728
At time: 14.06929636001587 and batch: 100, loss is 4.53655701637268 and perplexity is 93.36877890362592
At time: 14.707987785339355 and batch: 150, loss is 4.521547574996948 and perplexity is 91.97783048276438
At time: 15.34457778930664 and batch: 200, loss is 4.4061596488952635 and perplexity is 81.95412577051198
At time: 15.986854553222656 and batch: 250, loss is 4.54214807510376 and perplexity is 93.89227130481115
At time: 16.62635588645935 and batch: 300, loss is 4.5150518894195555 and perplexity is 91.38230767426927
At time: 17.2782564163208 and batch: 350, loss is 4.487384357452393 and perplexity is 88.88864063329642
At time: 17.915804862976074 and batch: 400, loss is 4.4016719865798954 and perplexity is 81.58716733731993
At time: 18.551894664764404 and batch: 450, loss is 4.4134727954864506 and perplexity is 82.5556651980671
At time: 19.18905472755432 and batch: 500, loss is 4.305908813476562 and perplexity is 74.13656115175459
At time: 19.826513528823853 and batch: 550, loss is 4.392550354003906 and perplexity is 80.8463430739526
At time: 20.463964700698853 and batch: 600, loss is 4.384491949081421 and perplexity is 80.19746846351316
At time: 21.101664781570435 and batch: 650, loss is 4.234699220657348 and perplexity is 69.04090953081644
At time: 21.738584756851196 and batch: 700, loss is 4.254712433815002 and perplexity is 70.43655910321633
At time: 22.375487089157104 and batch: 750, loss is 4.347463326454163 and perplexity is 77.28217442316652
At time: 23.012204885482788 and batch: 800, loss is 4.294552993774414 and perplexity is 73.29944181396189
At time: 23.649203062057495 and batch: 850, loss is 4.35764139175415 and perplexity is 78.0727740030078
At time: 24.28514552116394 and batch: 900, loss is 4.316862483024597 and perplexity is 74.9530923865686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.560944857662672 and perplexity of 95.67383529321228
finished 2 epochs...
Completing Train Step...
At time: 25.865753412246704 and batch: 50, loss is 4.390316848754883 and perplexity is 80.66597384512391
At time: 26.500057697296143 and batch: 100, loss is 4.23069275856018 and perplexity is 68.76485311767054
At time: 27.135287284851074 and batch: 150, loss is 4.219941582679748 and perplexity is 68.02951007166281
At time: 27.77094006538391 and batch: 200, loss is 4.115672488212585 and perplexity is 61.29341950830648
At time: 28.406656980514526 and batch: 250, loss is 4.264753293991089 and perplexity is 71.14736532552146
At time: 29.042540311813354 and batch: 300, loss is 4.245439567565918 and perplexity is 69.78642925367753
At time: 29.677796363830566 and batch: 350, loss is 4.216943130493164 and perplexity is 67.82583234999183
At time: 30.312252044677734 and batch: 400, loss is 4.152979803085327 and perplexity is 63.62330303028721
At time: 30.947790384292603 and batch: 450, loss is 4.1721515417098995 and perplexity is 64.85483999754648
At time: 31.59357762336731 and batch: 500, loss is 4.05756115436554 and perplexity is 57.833092958069074
At time: 32.233147382736206 and batch: 550, loss is 4.149972558021545 and perplexity is 63.43225956753851
At time: 32.86556839942932 and batch: 600, loss is 4.154543843269348 and perplexity is 63.72289029180507
At time: 33.512720823287964 and batch: 650, loss is 4.003318123817444 and perplexity is 54.77961434919948
At time: 34.14631128311157 and batch: 700, loss is 4.01633282661438 and perplexity is 55.49721429525411
At time: 34.793731689453125 and batch: 750, loss is 4.1239974355697635 and perplexity is 61.80581386781232
At time: 35.429439067840576 and batch: 800, loss is 4.07284451007843 and perplexity is 58.723765586076084
At time: 36.063323974609375 and batch: 850, loss is 4.143189020156861 and perplexity is 63.00342060007345
At time: 36.69713234901428 and batch: 900, loss is 4.103570919036866 and perplexity is 60.55614305083466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.504791677814641 and perplexity of 90.44949944867334
finished 3 epochs...
Completing Train Step...
At time: 38.263243436813354 and batch: 50, loss is 4.185033106803894 and perplexity is 65.6956758545921
At time: 38.91169738769531 and batch: 100, loss is 4.036060280799866 and perplexity is 56.602903404900545
At time: 39.54765725135803 and batch: 150, loss is 4.030307216644287 and perplexity is 56.27819819076815
At time: 40.18446111679077 and batch: 200, loss is 3.926757855415344 and perplexity is 50.74219715270051
At time: 40.821234941482544 and batch: 250, loss is 4.074165725708008 and perplexity is 58.801403619994204
At time: 41.45870566368103 and batch: 300, loss is 4.060127091407776 and perplexity is 57.98167958399608
At time: 42.0954806804657 and batch: 350, loss is 4.032033338546753 and perplexity is 56.37542510991325
At time: 42.73274803161621 and batch: 400, loss is 3.978005485534668 and perplexity is 53.41040010558208
At time: 43.368804693222046 and batch: 450, loss is 4.000832571983337 and perplexity is 54.643625851523126
At time: 44.005661725997925 and batch: 500, loss is 3.8844508647918703 and perplexity is 48.64022506431645
At time: 44.642590284347534 and batch: 550, loss is 3.9751237297058104 and perplexity is 53.256705934655635
At time: 45.279563903808594 and batch: 600, loss is 3.9929466009140016 and perplexity is 54.21440244501319
At time: 45.917397022247314 and batch: 650, loss is 3.8384508752822875 and perplexity is 46.453456474138896
At time: 46.55468726158142 and batch: 700, loss is 3.850042281150818 and perplexity is 46.995050194378415
At time: 47.19194483757019 and batch: 750, loss is 3.95714834690094 and perplexity is 52.30794893992036
At time: 47.829020738601685 and batch: 800, loss is 3.915006742477417 and perplexity is 50.149409641158876
At time: 48.46606469154358 and batch: 850, loss is 3.9863690948486328 and perplexity is 53.85897707178862
At time: 49.10355091094971 and batch: 900, loss is 3.944592890739441 and perplexity is 51.65530447809392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.485175563864512 and perplexity of 88.6925206478487
finished 4 epochs...
Completing Train Step...
At time: 50.704652070999146 and batch: 50, loss is 4.030711069107055 and perplexity is 56.30093086972288
At time: 51.36017560958862 and batch: 100, loss is 3.888150520324707 and perplexity is 48.82051043333835
At time: 52.002033948898315 and batch: 150, loss is 3.885214047431946 and perplexity is 48.67736060849377
At time: 52.644598960876465 and batch: 200, loss is 3.7810278701782227 and perplexity is 43.86110209233294
At time: 53.28706073760986 and batch: 250, loss is 3.9274046754837038 and perplexity is 50.775028841084335
At time: 53.929423809051514 and batch: 300, loss is 3.9200249242782594 and perplexity is 50.40170098843054
At time: 54.571486473083496 and batch: 350, loss is 3.8920023679733275 and perplexity is 49.00892223548279
At time: 55.21406078338623 and batch: 400, loss is 3.842060966491699 and perplexity is 46.62146076194101
At time: 55.8558509349823 and batch: 450, loss is 3.865075249671936 and perplexity is 47.706862223353134
At time: 56.50114059448242 and batch: 500, loss is 3.748193035125732 and perplexity is 42.44431727494093
At time: 57.142786264419556 and batch: 550, loss is 3.837055549621582 and perplexity is 46.3886839741667
At time: 57.78531813621521 and batch: 600, loss is 3.8599832963943483 and perplexity is 47.46455853296377
At time: 58.42777466773987 and batch: 650, loss is 3.7115922737121583 and perplexity is 40.91890878571771
At time: 59.07004141807556 and batch: 700, loss is 3.716538667678833 and perplexity is 41.12181123328167
At time: 59.712138175964355 and batch: 750, loss is 3.82333598613739 and perplexity is 45.756597370166126
At time: 60.354002237319946 and batch: 800, loss is 3.7872658348083497 and perplexity is 44.13556123906458
At time: 60.99593448638916 and batch: 850, loss is 3.8553692436218263 and perplexity is 47.24605902683922
At time: 61.638712882995605 and batch: 900, loss is 3.814727420806885 and perplexity is 45.36438930888632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.48893716890518 and perplexity of 89.02677515317062
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 63.23138928413391 and batch: 50, loss is 3.919468741416931 and perplexity is 50.373676220329266
At time: 63.87704300880432 and batch: 100, loss is 3.772368674278259 and perplexity is 43.48293987024206
At time: 64.52306747436523 and batch: 150, loss is 3.7658809089660643 and perplexity is 43.20174590781026
At time: 65.16785192489624 and batch: 200, loss is 3.6377405071258546 and perplexity is 38.00586565384238
At time: 65.8238480091095 and batch: 250, loss is 3.7799041366577146 and perplexity is 43.81184158469095
At time: 66.46789312362671 and batch: 300, loss is 3.7626267719268798 and perplexity is 43.0613899988324
At time: 67.11142492294312 and batch: 350, loss is 3.717153911590576 and perplexity is 41.1471189616974
At time: 67.75448727607727 and batch: 400, loss is 3.6639263677597045 and perplexity is 39.01422673781406
At time: 68.39798927307129 and batch: 450, loss is 3.667405924797058 and perplexity is 39.150215417996286
At time: 69.04203963279724 and batch: 500, loss is 3.5401999235153196 and perplexity is 34.47381062736141
At time: 69.68542242050171 and batch: 550, loss is 3.6063666105270387 and perplexity is 36.83198445644133
At time: 70.32883071899414 and batch: 600, loss is 3.6228776741027833 and perplexity is 37.44516791878891
At time: 70.97274017333984 and batch: 650, loss is 3.4555160999298096 and perplexity is 31.674631739804322
At time: 71.61696910858154 and batch: 700, loss is 3.4470670795440674 and perplexity is 31.408139514958755
At time: 72.26117253303528 and batch: 750, loss is 3.535438084602356 and perplexity is 34.31004212351069
At time: 72.90578031539917 and batch: 800, loss is 3.477912883758545 and perplexity is 32.392045511721136
At time: 73.55016994476318 and batch: 850, loss is 3.5301831436157225 and perplexity is 34.13021777388703
At time: 74.20514249801636 and batch: 900, loss is 3.4743430185317994 and perplexity is 32.27661643054227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423479054072132 and perplexity of 83.38588530312701
finished 6 epochs...
Completing Train Step...
At time: 75.80934596061707 and batch: 50, loss is 3.7984801149368286 and perplexity is 44.63329543553474
At time: 76.45143342018127 and batch: 100, loss is 3.6563468408584594 and perplexity is 38.71963519930571
At time: 77.08656311035156 and batch: 150, loss is 3.6520734357833864 and perplexity is 38.55452355947203
At time: 77.72082281112671 and batch: 200, loss is 3.531826820373535 and perplexity is 34.186362949200415
At time: 78.35624051094055 and batch: 250, loss is 3.6771253395080565 and perplexity is 39.532587805445544
At time: 78.99141335487366 and batch: 300, loss is 3.668005709648132 and perplexity is 39.173704167514195
At time: 79.62720370292664 and batch: 350, loss is 3.6281577730178833 and perplexity is 37.643405004441654
At time: 80.26159310340881 and batch: 400, loss is 3.5771544408798217 and perplexity is 35.771605665807165
At time: 80.89948153495789 and batch: 450, loss is 3.5879142093658447 and perplexity is 36.15857799408318
At time: 81.53561210632324 and batch: 500, loss is 3.465453824996948 and perplexity is 31.990974783277586
At time: 82.18525314331055 and batch: 550, loss is 3.536302909851074 and perplexity is 34.339727148553095
At time: 82.82320809364319 and batch: 600, loss is 3.5598019647598265 and perplexity is 35.156234282383565
At time: 83.4618079662323 and batch: 650, loss is 3.3990659284591676 and perplexity is 29.936124501910555
At time: 84.09856748580933 and batch: 700, loss is 3.396472911834717 and perplexity is 29.858600187760906
At time: 84.73492193222046 and batch: 750, loss is 3.4924107456207274 and perplexity is 32.86508163364538
At time: 85.37185215950012 and batch: 800, loss is 3.4429587745666503 and perplexity is 31.279369992409357
At time: 86.00875306129456 and batch: 850, loss is 3.504657964706421 and perplexity is 33.27006237053016
At time: 86.64500045776367 and batch: 900, loss is 3.4570513105392457 and perplexity is 31.72329631613018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429564175540453 and perplexity of 83.89484551578792
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 88.21261382102966 and batch: 50, loss is 3.7502522897720336 and perplexity is 42.53181098749719
At time: 88.86302423477173 and batch: 100, loss is 3.6163075447082518 and perplexity is 37.199954743591945
At time: 89.50066709518433 and batch: 150, loss is 3.614725637435913 and perplexity is 37.141154385271584
At time: 90.14032077789307 and batch: 200, loss is 3.488527798652649 and perplexity is 32.73771570216668
At time: 90.78380942344666 and batch: 250, loss is 3.63252402305603 and perplexity is 37.80812486534095
At time: 91.42837452888489 and batch: 300, loss is 3.621537890434265 and perplexity is 37.3950330867604
At time: 92.0661985874176 and batch: 350, loss is 3.5744493293762205 and perplexity is 35.67497024753761
At time: 92.70456981658936 and batch: 400, loss is 3.524269366264343 and perplexity is 33.928974904281624
At time: 93.34259080886841 and batch: 450, loss is 3.530624313354492 and perplexity is 34.145278315029714
At time: 93.98005700111389 and batch: 500, loss is 3.4011800575256346 and perplexity is 29.999480280406235
At time: 94.61765718460083 and batch: 550, loss is 3.4628084802627566 and perplexity is 31.906459461958537
At time: 95.25482892990112 and batch: 600, loss is 3.48588369846344 and perplexity is 32.651268240124445
At time: 95.89174795150757 and batch: 650, loss is 3.319851403236389 and perplexity is 27.65624062531236
At time: 96.52888488769531 and batch: 700, loss is 3.308313331604004 and perplexity is 27.338974777649078
At time: 97.16550588607788 and batch: 750, loss is 3.3987371301651 and perplexity is 29.926283173238037
At time: 97.80211734771729 and batch: 800, loss is 3.344853377342224 and perplexity is 28.356417668509078
At time: 98.45178818702698 and batch: 850, loss is 3.3965136575698853 and perplexity is 29.859816823162845
At time: 99.08905816078186 and batch: 900, loss is 3.34711030960083 and perplexity is 28.42048845680143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.41694662015732 and perplexity of 82.84294780035039
finished 8 epochs...
Completing Train Step...
At time: 100.66717314720154 and batch: 50, loss is 3.7174347114562987 and perplexity is 41.158674689524204
At time: 101.32072257995605 and batch: 100, loss is 3.5804720544815063 and perplexity is 35.890479110364126
At time: 101.96740365028381 and batch: 150, loss is 3.578365430831909 and perplexity is 35.81495096089976
At time: 102.61105108261108 and batch: 200, loss is 3.4549797677993777 and perplexity is 31.657648171906356
At time: 103.26102375984192 and batch: 250, loss is 3.5981405687332155 and perplexity is 36.53024577196444
At time: 103.90654754638672 and batch: 300, loss is 3.5890809297561646 and perplexity is 36.20078956407523
At time: 104.55609798431396 and batch: 350, loss is 3.543927731513977 and perplexity is 34.60256220584412
At time: 105.19220447540283 and batch: 400, loss is 3.495152096748352 and perplexity is 32.95529996580528
At time: 105.82763338088989 and batch: 450, loss is 3.504887790679932 and perplexity is 33.277709573732544
At time: 106.4660964012146 and batch: 500, loss is 3.3780069875717165 and perplexity is 29.312293094498198
At time: 107.10206294059753 and batch: 550, loss is 3.4422582817077636 and perplexity is 31.257466689548465
At time: 107.73908424377441 and batch: 600, loss is 3.4684658336639402 and perplexity is 32.08747713659529
At time: 108.37724089622498 and batch: 650, loss is 3.3056047582626342 and perplexity is 27.26502535331566
At time: 109.01310300827026 and batch: 700, loss is 3.2970609521865843 and perplexity is 27.033070563401367
At time: 109.6508150100708 and batch: 750, loss is 3.391349196434021 and perplexity is 29.706004480457693
At time: 110.28731274604797 and batch: 800, loss is 3.34102530002594 and perplexity is 28.248074614361634
At time: 110.92522096633911 and batch: 850, loss is 3.3967673206329345 and perplexity is 29.867392116505734
At time: 111.56230640411377 and batch: 900, loss is 3.3506905460357665 and perplexity is 28.522422890876854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.419015544734589 and perplexity of 83.014521035949
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 113.14071321487427 and batch: 50, loss is 3.7016839218139648 and perplexity is 40.515471836437676
At time: 113.77553677558899 and batch: 100, loss is 3.5668990850448608 and perplexity is 35.40662979838658
At time: 114.42296409606934 and batch: 150, loss is 3.5672477531433104 and perplexity is 35.41897711310318
At time: 115.05741357803345 and batch: 200, loss is 3.4429298973083498 and perplexity is 31.278466743004355
At time: 115.69229984283447 and batch: 250, loss is 3.5848995304107665 and perplexity is 36.04973563477643
At time: 116.32858872413635 and batch: 300, loss is 3.57590042591095 and perplexity is 35.72677565147098
At time: 116.96505403518677 and batch: 350, loss is 3.5281283807754518 and perplexity is 34.06016027110046
At time: 117.60114574432373 and batch: 400, loss is 3.480200672149658 and perplexity is 32.46623649167912
At time: 118.23579907417297 and batch: 450, loss is 3.488270559310913 and perplexity is 32.72929535679801
At time: 118.8722562789917 and batch: 500, loss is 3.358824095726013 and perplexity is 28.755357442982906
At time: 119.50556826591492 and batch: 550, loss is 3.4200526809692384 and perplexity is 30.57102548988273
At time: 120.14035391807556 and batch: 600, loss is 3.4456145095825197 and perplexity is 31.362550113864504
At time: 120.77565360069275 and batch: 650, loss is 3.2814063262939452 and perplexity is 26.613173200293197
At time: 121.41125297546387 and batch: 700, loss is 3.269193181991577 and perplexity is 26.29011944230823
At time: 122.04589104652405 and batch: 750, loss is 3.361610531806946 and perplexity is 28.835594143786565
At time: 122.68111252784729 and batch: 800, loss is 3.309602222442627 and perplexity is 27.374234449830332
At time: 123.31589078903198 and batch: 850, loss is 3.361676297187805 and perplexity is 28.837490589977307
At time: 123.95092272758484 and batch: 900, loss is 3.315438652038574 and perplexity is 27.534469387052553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4182981726241435 and perplexity of 82.95499008926926
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.53350687026978 and batch: 50, loss is 3.6939003896713256 and perplexity is 40.20134246307829
At time: 126.17090892791748 and batch: 100, loss is 3.559584903717041 and perplexity is 35.148604061651604
At time: 126.8078863620758 and batch: 150, loss is 3.560073666572571 and perplexity is 35.16578759273445
At time: 127.44482636451721 and batch: 200, loss is 3.435852217674255 and perplexity is 31.05786935564435
At time: 128.0825638771057 and batch: 250, loss is 3.5780666351318358 and perplexity is 35.80425120615427
At time: 128.72048044204712 and batch: 300, loss is 3.5690512084960937 and perplexity is 35.4829112908569
At time: 129.35773849487305 and batch: 350, loss is 3.521173987388611 and perplexity is 33.82411424754426
At time: 129.9936740398407 and batch: 400, loss is 3.4732823848724363 and perplexity is 32.24240091301801
At time: 130.6436755657196 and batch: 450, loss is 3.4818020057678223 and perplexity is 32.51826741594417
At time: 131.2806887626648 and batch: 500, loss is 3.352009792327881 and perplexity is 28.56007582280221
At time: 131.91764163970947 and batch: 550, loss is 3.4127973651885988 and perplexity is 30.350025727192733
At time: 132.5546624660492 and batch: 600, loss is 3.4389177179336547 and perplexity is 31.15322334137204
At time: 133.19244170188904 and batch: 650, loss is 3.274271354675293 and perplexity is 26.423964765913208
At time: 133.8295702934265 and batch: 700, loss is 3.2614583921432496 and perplexity is 26.087555298774046
At time: 134.4667844772339 and batch: 750, loss is 3.3534633493423462 and perplexity is 28.60161970723583
At time: 135.10354161262512 and batch: 800, loss is 3.3010506010055543 and perplexity is 27.14113845456926
At time: 135.7405242919922 and batch: 850, loss is 3.3525200939178466 and perplexity is 28.574653794168757
At time: 136.38839745521545 and batch: 900, loss is 3.3056794452667235 and perplexity is 27.267061772421794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.418362970221533 and perplexity of 82.96036554747498
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.98756527900696 and batch: 50, loss is 3.691809368133545 and perplexity is 40.11736841650744
At time: 138.65969514846802 and batch: 100, loss is 3.5576504468917847 and perplexity is 35.080676327443676
At time: 139.3089816570282 and batch: 150, loss is 3.5580525159835816 and perplexity is 35.0947840190576
At time: 139.97221279144287 and batch: 200, loss is 3.4338904571533204 and perplexity is 30.997000977789437
At time: 140.62009716033936 and batch: 250, loss is 3.5761709642410278 and perplexity is 35.73644242125171
At time: 141.2631871700287 and batch: 300, loss is 3.5670122289657593 and perplexity is 35.410636069946136
At time: 141.90558004379272 and batch: 350, loss is 3.5191906976699827 and perplexity is 33.757097708089354
At time: 142.54881501197815 and batch: 400, loss is 3.4712919902801516 and perplexity is 32.17828963713252
At time: 143.1913595199585 and batch: 450, loss is 3.4800487089157106 and perplexity is 32.4613031922373
At time: 143.8335473537445 and batch: 500, loss is 3.3501230382919314 and perplexity is 28.506240787182225
At time: 144.4766616821289 and batch: 550, loss is 3.410718460083008 and perplexity is 30.286996442404867
At time: 145.13068199157715 and batch: 600, loss is 3.4370956945419313 and perplexity is 31.096513119080996
At time: 145.77660012245178 and batch: 650, loss is 3.272361078262329 and perplexity is 26.373535871183208
At time: 146.4194519519806 and batch: 700, loss is 3.2593018627166748 and perplexity is 26.03135733616449
At time: 147.0743567943573 and batch: 750, loss is 3.351328339576721 and perplexity is 28.540620110386705
At time: 147.71665263175964 and batch: 800, loss is 3.29881178855896 and perplexity is 27.080442504758476
At time: 148.36001467704773 and batch: 850, loss is 3.3501990270614623 and perplexity is 28.508407023647376
At time: 149.002379655838 and batch: 900, loss is 3.3031836223602293 and perplexity is 27.199092869512064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.418323673614084 and perplexity of 82.95710555061007
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 150.58178043365479 and batch: 50, loss is 3.691273307800293 and perplexity is 40.09586884967222
At time: 151.23653364181519 and batch: 100, loss is 3.5571551513671875 and perplexity is 35.06330532770318
At time: 151.88036680221558 and batch: 150, loss is 3.5575183153152468 and perplexity is 35.076041368593465
At time: 152.52407670021057 and batch: 200, loss is 3.433394503593445 and perplexity is 30.981631716344037
At time: 153.1717507839203 and batch: 250, loss is 3.575689082145691 and perplexity is 35.71922581801745
At time: 153.82638430595398 and batch: 300, loss is 3.566485481262207 and perplexity is 35.39198851042571
At time: 154.46892142295837 and batch: 350, loss is 3.5186724138259886 and perplexity is 33.73960648282839
At time: 155.11191082000732 and batch: 400, loss is 3.4707920789718627 and perplexity is 32.16220736645094
At time: 155.75509190559387 and batch: 450, loss is 3.479584035873413 and perplexity is 32.44622280372238
At time: 156.39863300323486 and batch: 500, loss is 3.3496334457397463 and perplexity is 28.49228775992985
At time: 157.04288721084595 and batch: 550, loss is 3.410189166069031 and perplexity is 30.270969958221787
At time: 157.68667721748352 and batch: 600, loss is 3.4366216611862184 and perplexity is 31.081775827875955
At time: 158.3306782245636 and batch: 650, loss is 3.2718680906295776 and perplexity is 26.36053724851016
At time: 158.97495317459106 and batch: 700, loss is 3.25875590801239 and perplexity is 26.017149273000268
At time: 159.61834073066711 and batch: 750, loss is 3.3507823181152343 and perplexity is 28.525040573050237
At time: 160.26273250579834 and batch: 800, loss is 3.298246364593506 and perplexity is 27.06513490162184
At time: 160.90703773498535 and batch: 850, loss is 3.349616298675537 and perplexity is 28.491799205030812
At time: 161.5530343055725 and batch: 900, loss is 3.302554235458374 and perplexity is 27.181979502747012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.41830945994756 and perplexity of 82.9559264343558
Annealing...
Model not improving. Stopping early with 82.84294780035039 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f61b3402b70>
ELAPSED
1151.5930778980255


RESULTS SO FAR:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.34674739898118, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.10653796386697, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.10738997417746055, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.6689429055087728, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.5103564042358, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.010038557844579299, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.03467218496325619, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.93680230427925, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.7667358092686467, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.2410374528905116, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -82.84294780035039, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.02721183700724882, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.5451628532116214, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -87.01600300800983, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.966149116756947, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.10632658490051683, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.34674739898118, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.04153731542816619, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.7295571153454115, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.10653796386697, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.10738997417746055, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.6689429055087728, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.5103564042358, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.010038557844579299, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.03467218496325619, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -83.93680230427925, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.7667358092686467, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.2410374528905116, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}, {'best_accuracy': -82.84294780035039, 'params': {'tune_wordvecs': 'FALSE', 'batch_size': 32, 'seq_len': 35, 'dropout': 0.02721183700724882, 'tie_weights': 'TRUE', 'wordvec_dim': 300, 'rnn_dropout': 0.5451628532116214, 'data': 'ptb', 'wordvec_source': 'None', 'num_layers': 1}}]
