TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'rnn_dropout', 'domain': [0, 1], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3745226860046387 and batch: 50, loss is 6.705862369537353 and perplexity is 817.1824357772821
At time: 2.191781759262085 and batch: 100, loss is 5.905253887176514 and perplexity is 366.9603804964699
At time: 3.010178565979004 and batch: 150, loss is 5.726815423965454 and perplexity is 306.99007680850247
At time: 3.829064130783081 and batch: 200, loss is 5.5322558879852295 and perplexity is 252.71336150969853
At time: 4.64938759803772 and batch: 250, loss is 5.566865530014038 and perplexity is 261.61279528165693
At time: 5.468218803405762 and batch: 300, loss is 5.477333440780639 and perplexity is 239.20799388760764
At time: 6.286909341812134 and batch: 350, loss is 5.442308311462402 and perplexity is 230.974730199552
At time: 7.10858678817749 and batch: 400, loss is 5.291971950531006 and perplexity is 198.73493472407105
At time: 7.9275054931640625 and batch: 450, loss is 5.290309925079345 and perplexity is 198.4049065380185
At time: 8.746267318725586 and batch: 500, loss is 5.2318599414825435 and perplexity is 187.14055049834178
At time: 9.56715178489685 and batch: 550, loss is 5.28464828491211 and perplexity is 197.28478320945118
At time: 10.386685848236084 and batch: 600, loss is 5.209066801071167 and perplexity is 182.92327471461
At time: 11.20577883720398 and batch: 650, loss is 5.0985077857971195 and perplexity is 163.77733400350124
At time: 12.02676796913147 and batch: 700, loss is 5.191669187545776 and perplexity is 179.76836976089112
At time: 12.850488662719727 and batch: 750, loss is 5.172228231430053 and perplexity is 176.30725348843472
At time: 13.669824838638306 and batch: 800, loss is 5.1333160400390625 and perplexity is 169.57851587001085
At time: 14.488686084747314 and batch: 850, loss is 5.176951971054077 and perplexity is 177.14205318365032
At time: 15.30787205696106 and batch: 900, loss is 5.0952416610717775 and perplexity is 163.24328940633987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0066114451787245 and perplexity of 149.39763538722616
finished 1 epochs...
Completing Train Step...
At time: 17.216060876846313 and batch: 50, loss is 4.960942993164062 and perplexity is 142.72832429146686
At time: 17.97973394393921 and batch: 100, loss is 4.803018417358398 and perplexity is 121.87774076073923
At time: 18.72950291633606 and batch: 150, loss is 4.776207675933838 and perplexity is 118.6535231329336
At time: 19.479198217391968 and batch: 200, loss is 4.656436519622803 and perplexity is 105.26031995901039
At time: 20.22796607017517 and batch: 250, loss is 4.757351007461548 and perplexity is 116.43707606344779
At time: 20.977079391479492 and batch: 300, loss is 4.715528020858764 and perplexity is 111.66775851537307
At time: 21.72544026374817 and batch: 350, loss is 4.690074901580811 and perplexity is 108.86133338801409
At time: 22.47687339782715 and batch: 400, loss is 4.572394695281982 and perplexity is 96.7755805373794
At time: 23.227688789367676 and batch: 450, loss is 4.580781574249268 and perplexity is 97.59063873799897
At time: 23.978772163391113 and batch: 500, loss is 4.473170185089112 and perplexity is 87.63409942915605
At time: 24.730319023132324 and batch: 550, loss is 4.558338603973389 and perplexity is 95.42480965995927
At time: 25.50625729560852 and batch: 600, loss is 4.521512775421143 and perplexity is 91.97462974897252
At time: 26.256314754486084 and batch: 650, loss is 4.370343580245971 and perplexity is 79.07079419596765
At time: 27.007747411727905 and batch: 700, loss is 4.408865079879761 and perplexity is 82.17614719808246
At time: 27.757938385009766 and batch: 750, loss is 4.475185327529907 and perplexity is 87.8108726739906
At time: 28.50589609146118 and batch: 800, loss is 4.407486324310303 and perplexity is 82.06292444864071
At time: 29.252877473831177 and batch: 850, loss is 4.4762069129943844 and perplexity is 87.90062482207667
At time: 30.00029420852661 and batch: 900, loss is 4.4162672328948975 and perplexity is 82.78668447129192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.542033992401541 and perplexity of 93.88156043175756
finished 2 epochs...
Completing Train Step...
At time: 31.829626321792603 and batch: 50, loss is 4.444166421890259 and perplexity is 85.12888667665422
At time: 32.60196089744568 and batch: 100, loss is 4.28919584274292 and perplexity is 72.90781557071531
At time: 33.36894130706787 and batch: 150, loss is 4.289976983070374 and perplexity is 72.96478905489018
At time: 34.14487957954407 and batch: 200, loss is 4.185003142356873 and perplexity is 65.69370734948615
At time: 34.915842056274414 and batch: 250, loss is 4.3274663543701175 and perplexity is 75.75211421053706
At time: 35.66664719581604 and batch: 300, loss is 4.298727746009827 and perplexity is 73.60608846405596
At time: 36.41507172584534 and batch: 350, loss is 4.283313159942627 and perplexity is 72.48018107033498
At time: 37.16674447059631 and batch: 400, loss is 4.199580183029175 and perplexity is 66.65834086321901
At time: 37.92842984199524 and batch: 450, loss is 4.217660279273987 and perplexity is 67.8744910086333
At time: 38.694011211395264 and batch: 500, loss is 4.092039136886597 and perplexity is 59.86183380698666
At time: 39.472633838653564 and batch: 550, loss is 4.185574407577515 and perplexity is 65.73124660112347
At time: 40.23676156997681 and batch: 600, loss is 4.18806688785553 and perplexity is 65.8952847829693
At time: 41.003588914871216 and batch: 650, loss is 4.027647266387939 and perplexity is 56.12869990021783
At time: 41.785269021987915 and batch: 700, loss is 4.048950514793396 and perplexity is 57.337250861655946
At time: 42.554977893829346 and batch: 750, loss is 4.15114953994751 and perplexity is 63.5069621437399
At time: 43.337390661239624 and batch: 800, loss is 4.089193930625916 and perplexity is 59.69175660975039
At time: 44.13195204734802 and batch: 850, loss is 4.166560955047608 and perplexity is 64.49327501596827
At time: 44.904622077941895 and batch: 900, loss is 4.120182199478149 and perplexity is 61.57045934808673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.420565670483733 and perplexity of 83.14330377148936
finished 3 epochs...
Completing Train Step...
At time: 46.90300917625427 and batch: 50, loss is 4.176008505821228 and perplexity is 65.10546580453844
At time: 47.69577431678772 and batch: 100, loss is 4.028222675323486 and perplexity is 56.16100614944193
At time: 48.486889123916626 and batch: 150, loss is 4.027489256858826 and perplexity is 56.11983173142289
At time: 49.256171226501465 and batch: 200, loss is 3.926572208404541 and perplexity is 50.73277788983357
At time: 50.020479917526245 and batch: 250, loss is 4.081955342292786 and perplexity is 59.261232629552346
At time: 50.79280948638916 and batch: 300, loss is 4.0584754419326785 and perplexity is 57.88599321526851
At time: 51.555853605270386 and batch: 350, loss is 4.041077818870544 and perplexity is 56.88762432934767
At time: 52.32178473472595 and batch: 400, loss is 3.9699404287338256 and perplexity is 52.98137457809857
At time: 53.07967448234558 and batch: 450, loss is 3.9914980792999266 and perplexity is 54.13592856055758
At time: 53.83612298965454 and batch: 500, loss is 3.8646965551376344 and perplexity is 47.68879931575852
At time: 54.60681653022766 and batch: 550, loss is 3.955239758491516 and perplexity is 52.20820980560746
At time: 55.39021682739258 and batch: 600, loss is 3.971774392127991 and perplexity is 53.07862963349396
At time: 56.157461166381836 and batch: 650, loss is 3.8093259525299072 and perplexity is 45.12001558174817
At time: 56.94414043426514 and batch: 700, loss is 3.824547653198242 and perplexity is 45.812072734064664
At time: 57.712650299072266 and batch: 750, loss is 3.9400440502166747 and perplexity is 51.42086635094293
At time: 58.49079895019531 and batch: 800, loss is 3.8786450242996215 and perplexity is 48.358645868983416
At time: 59.25533723831177 and batch: 850, loss is 3.956312184333801 and perplexity is 52.26422927197516
At time: 60.013036251068115 and batch: 900, loss is 3.9166440629959105 and perplexity is 50.23158755598907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376603636023116 and perplexity of 79.5673343409332
finished 4 epochs...
Completing Train Step...
At time: 61.9183349609375 and batch: 50, loss is 3.982710027694702 and perplexity is 53.66226357127447
At time: 62.67460823059082 and batch: 100, loss is 3.8384336709976195 and perplexity is 46.45265728252468
At time: 63.497822761535645 and batch: 150, loss is 3.840532341003418 and perplexity is 46.55024845106294
At time: 64.25457715988159 and batch: 200, loss is 3.743612537384033 and perplexity is 42.25034575770569
At time: 65.01089835166931 and batch: 250, loss is 3.8957506036758422 and perplexity is 49.192963927897814
At time: 65.76723074913025 and batch: 300, loss is 3.877707347869873 and perplexity is 48.313322359291725
At time: 66.52362012863159 and batch: 350, loss is 3.8608279275894164 and perplexity is 47.50466551518064
At time: 67.28210806846619 and batch: 400, loss is 3.796765155792236 and perplexity is 44.55681675502472
At time: 68.03671193122864 and batch: 450, loss is 3.8198047828674317 and perplexity is 45.59530646699276
At time: 68.79680895805359 and batch: 500, loss is 3.6984044027328493 and perplexity is 40.38281821240878
At time: 69.55568766593933 and batch: 550, loss is 3.7815323448181153 and perplexity is 43.88323448816309
At time: 70.3114583492279 and batch: 600, loss is 3.806928863525391 and perplexity is 45.01198841560695
At time: 71.0669093132019 and batch: 650, loss is 3.643745069503784 and perplexity is 38.234760764643866
At time: 71.82746148109436 and batch: 700, loss is 3.6597368574142455 and perplexity is 38.85111814242321
At time: 72.58334684371948 and batch: 750, loss is 3.775114893913269 and perplexity is 43.60251769183057
At time: 73.33838295936584 and batch: 800, loss is 3.715724506378174 and perplexity is 41.08834507123814
At time: 74.09288454055786 and batch: 850, loss is 3.793122797012329 and perplexity is 44.394820046477
At time: 74.84851813316345 and batch: 900, loss is 3.7582140159606934 and perplexity is 42.87178923182597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372862933433219 and perplexity of 79.27025260092842
finished 5 epochs...
Completing Train Step...
At time: 76.73342895507812 and batch: 50, loss is 3.8257433128356935 and perplexity is 45.86688113991058
At time: 77.56230878829956 and batch: 100, loss is 3.689036445617676 and perplexity is 40.006280153102026
At time: 78.31995844841003 and batch: 150, loss is 3.6907877635955813 and perplexity is 40.076405258518704
At time: 79.0763521194458 and batch: 200, loss is 3.5958323097229004 and perplexity is 36.44602174584175
At time: 79.84258961677551 and batch: 250, loss is 3.746185750961304 and perplexity is 42.359204919924
At time: 80.59803628921509 and batch: 300, loss is 3.7338400173187254 and perplexity is 41.83946434942533
At time: 81.3532886505127 and batch: 350, loss is 3.714772310256958 and perplexity is 41.049239529461396
At time: 82.11978507041931 and batch: 400, loss is 3.6546823740005494 and perplexity is 38.65524125542551
At time: 82.8738648891449 and batch: 450, loss is 3.677628493309021 and perplexity is 39.55248378221024
At time: 83.63805532455444 and batch: 500, loss is 3.558729872703552 and perplexity is 35.11856375962315
At time: 84.3977837562561 and batch: 550, loss is 3.641483578681946 and perplexity is 38.14839090323429
At time: 85.15956950187683 and batch: 600, loss is 3.670890202522278 and perplexity is 39.28686356331471
At time: 85.91720223426819 and batch: 650, loss is 3.5078339910507204 and perplexity is 33.3758969428263
At time: 86.67367959022522 and batch: 700, loss is 3.5219355392456055 and perplexity is 33.84988287538847
At time: 87.43075656890869 and batch: 750, loss is 3.6382271957397463 and perplexity is 38.02436717779253
At time: 88.18575239181519 and batch: 800, loss is 3.581353950500488 and perplexity is 35.92214474185578
At time: 88.94796204566956 and batch: 850, loss is 3.6586125326156616 and perplexity is 38.80746141361328
At time: 89.71360230445862 and batch: 900, loss is 3.62339599609375 and perplexity is 37.46458160361191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.390627508294092 and perplexity of 80.69103739230198
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.61206436157227 and batch: 50, loss is 3.7228782176971436 and perplexity is 41.3833331020451
At time: 92.38531827926636 and batch: 100, loss is 3.587789692878723 and perplexity is 36.15407593526814
At time: 93.14077353477478 and batch: 150, loss is 3.594711518287659 and perplexity is 36.40519623953295
At time: 93.89602160453796 and batch: 200, loss is 3.478377552032471 and perplexity is 32.40710056512995
At time: 94.65244388580322 and batch: 250, loss is 3.6218092393875123 and perplexity is 37.405181566672574
At time: 95.4119040966034 and batch: 300, loss is 3.594611258506775 and perplexity is 36.40154644550177
At time: 96.16805481910706 and batch: 350, loss is 3.560881848335266 and perplexity is 35.19421942843573
At time: 96.92819786071777 and batch: 400, loss is 3.4973228979110718 and perplexity is 33.026917074416254
At time: 97.68432092666626 and batch: 450, loss is 3.498063154220581 and perplexity is 33.05137450943267
At time: 98.44049096107483 and batch: 500, loss is 3.37195143699646 and perplexity is 29.1353273743947
At time: 99.19802522659302 and batch: 550, loss is 3.4288613033294677 and perplexity is 30.841503629577172
At time: 99.96193718910217 and batch: 600, loss is 3.4466301870346068 and perplexity is 31.394420531147425
At time: 100.71959328651428 and batch: 650, loss is 3.26301299571991 and perplexity is 26.128142645991154
At time: 101.49421405792236 and batch: 700, loss is 3.2543495416641237 and perplexity is 25.902760386717237
At time: 102.25322222709656 and batch: 750, loss is 3.350012001991272 and perplexity is 28.503075735380587
At time: 103.01428079605103 and batch: 800, loss is 3.273035078048706 and perplexity is 26.391317620500853
At time: 103.77758002281189 and batch: 850, loss is 3.325674090385437 and perplexity is 27.817743997701445
At time: 104.54132914543152 and batch: 900, loss is 3.276128578186035 and perplexity is 26.47308557456455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3626771691727315 and perplexity of 78.46692270496843
finished 7 epochs...
Completing Train Step...
At time: 106.4373083114624 and batch: 50, loss is 3.600912985801697 and perplexity is 36.631663369837675
At time: 107.20314908027649 and batch: 100, loss is 3.4604396533966066 and perplexity is 31.83096803194579
At time: 107.95991325378418 and batch: 150, loss is 3.4675842094421387 and perplexity is 32.05920050604789
At time: 108.71668648719788 and batch: 200, loss is 3.356678614616394 and perplexity is 28.69372950122098
At time: 109.47285413742065 and batch: 250, loss is 3.5007025051116942 and perplexity is 33.138723906343365
At time: 110.22837543487549 and batch: 300, loss is 3.4788957691192626 and perplexity is 32.42389883057434
At time: 110.98409366607666 and batch: 350, loss is 3.4496183443069457 and perplexity is 31.488372298588413
At time: 111.74047636985779 and batch: 400, loss is 3.3912253046035765 and perplexity is 29.702324377159492
At time: 112.50011944770813 and batch: 450, loss is 3.3973610496520994 and perplexity is 29.885130519313243
At time: 113.25557923316956 and batch: 500, loss is 3.2780408239364625 and perplexity is 26.52375705267376
At time: 114.00994682312012 and batch: 550, loss is 3.3396420812606813 and perplexity is 28.20902835845515
At time: 114.7667145729065 and batch: 600, loss is 3.366039295196533 and perplexity is 28.96358337526113
At time: 115.52333331108093 and batch: 650, loss is 3.1896204471588137 and perplexity is 24.27921045072905
At time: 116.27998471260071 and batch: 700, loss is 3.1871912479400635 and perplexity is 24.22030298961731
At time: 117.03679203987122 and batch: 750, loss is 3.290659155845642 and perplexity is 26.86056311910259
At time: 117.79285025596619 and batch: 800, loss is 3.221810221672058 and perplexity is 25.0734676590111
At time: 118.55022287368774 and batch: 850, loss is 3.282716460227966 and perplexity is 26.648062871675556
At time: 119.30415964126587 and batch: 900, loss is 3.2427283239364626 and perplexity is 25.603481131407833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3811841990849745 and perplexity of 79.9326335328263
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.16505718231201 and batch: 50, loss is 3.546943688392639 and perplexity is 34.70707957205884
At time: 121.93269991874695 and batch: 100, loss is 3.418430209159851 and perplexity is 30.521465078898135
At time: 122.68542432785034 and batch: 150, loss is 3.430154314041138 and perplexity is 30.881407816845446
At time: 123.43905520439148 and batch: 200, loss is 3.3160317373275756 and perplexity is 27.55080451937099
At time: 124.19167923927307 and batch: 250, loss is 3.46197970867157 and perplexity is 31.880027249427474
At time: 124.95811939239502 and batch: 300, loss is 3.433995456695557 and perplexity is 31.000255819578264
At time: 125.72387361526489 and batch: 350, loss is 3.3993852376937865 and perplexity is 29.945684909194654
At time: 126.47870516777039 and batch: 400, loss is 3.3396819686889647 and perplexity is 28.21015356649142
At time: 127.23202228546143 and batch: 450, loss is 3.3379870080947875 and perplexity is 28.16237896730964
At time: 127.98624205589294 and batch: 500, loss is 3.2183942890167234 and perplexity is 24.987964501707587
At time: 128.73709535598755 and batch: 550, loss is 3.265052914619446 and perplexity is 26.181496338085324
At time: 129.4965476989746 and batch: 600, loss is 3.2886926174163817 and perplexity is 26.807792694038184
At time: 130.24967980384827 and batch: 650, loss is 3.1073369693756105 and perplexity is 22.361415904371434
At time: 131.0031988620758 and batch: 700, loss is 3.097463822364807 and perplexity is 22.141724664337328
At time: 131.76021361351013 and batch: 750, loss is 3.1901152896881104 and perplexity is 24.29122780974276
At time: 132.51398491859436 and batch: 800, loss is 3.114950032234192 and perplexity is 22.532304436453906
At time: 133.26669263839722 and batch: 850, loss is 3.1702494049072265 and perplexity is 23.81342280036363
At time: 134.02044320106506 and batch: 900, loss is 3.1275858163833616 and perplexity is 22.818824160180668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375099913714683 and perplexity of 79.44777707825207
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.89135670661926 and batch: 50, loss is 3.5204782104492187 and perplexity is 33.80058839417766
At time: 136.66218757629395 and batch: 100, loss is 3.391014018058777 and perplexity is 29.696049338608276
At time: 137.4144787788391 and batch: 150, loss is 3.402971038818359 and perplexity is 30.053256930492488
At time: 138.1683406829834 and batch: 200, loss is 3.290980453491211 and perplexity is 26.869194741377118
At time: 138.9208164215088 and batch: 250, loss is 3.437762475013733 and perplexity is 31.117254581002342
At time: 139.70047068595886 and batch: 300, loss is 3.408792552947998 and perplexity is 30.228722632843272
At time: 140.46886777877808 and batch: 350, loss is 3.3746838569641113 and perplexity is 29.215046187716037
At time: 141.23802161216736 and batch: 400, loss is 3.3149271631240844 and perplexity is 27.520389412377167
At time: 142.0029809474945 and batch: 450, loss is 3.3120248460769655 and perplexity is 27.44063231322231
At time: 142.76466512680054 and batch: 500, loss is 3.1942229318618773 and perplexity is 24.391212692021494
At time: 143.5176956653595 and batch: 550, loss is 3.237469344139099 and perplexity is 25.46918637760836
At time: 144.271657705307 and batch: 600, loss is 3.2616377210617067 and perplexity is 26.09223397134907
At time: 145.02598524093628 and batch: 650, loss is 3.079681496620178 and perplexity is 21.751473375011276
At time: 145.78452682495117 and batch: 700, loss is 3.067413454055786 and perplexity is 21.48625555195224
At time: 146.53839468955994 and batch: 750, loss is 3.1587132596969605 and perplexity is 23.54028619787787
At time: 147.29198336601257 and batch: 800, loss is 3.080609827041626 and perplexity is 21.771675305038883
At time: 148.05037450790405 and batch: 850, loss is 3.1342730808258055 and perplexity is 22.97193103411083
At time: 148.80464434623718 and batch: 900, loss is 3.093555364608765 and perplexity is 22.055353567682833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374595746602098 and perplexity of 79.40773221738039
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.67887234687805 and batch: 50, loss is 3.511402716636658 and perplexity is 33.49521914766383
At time: 151.43600869178772 and batch: 100, loss is 3.3794828939437864 and perplexity is 29.35558723584693
At time: 152.19273471832275 and batch: 150, loss is 3.3921711444854736 and perplexity is 29.730431310374247
At time: 152.94763565063477 and batch: 200, loss is 3.2803488540649415 and perplexity is 26.585045383521923
At time: 153.7039020061493 and batch: 250, loss is 3.4277979803085326 and perplexity is 30.80872657815255
At time: 154.47602701187134 and batch: 300, loss is 3.398667311668396 and perplexity is 29.92419383807291
At time: 155.24178767204285 and batch: 350, loss is 3.365440173149109 and perplexity is 28.94623585104961
At time: 156.00498008728027 and batch: 400, loss is 3.3059573650360106 and perplexity is 27.274640881082004
At time: 156.76258540153503 and batch: 450, loss is 3.3032191467285155 and perplexity is 27.20005911726678
At time: 157.51838874816895 and batch: 500, loss is 3.1850755500793455 and perplexity is 24.16911431537466
At time: 158.3037941455841 and batch: 550, loss is 3.2288132762908934 and perplexity is 25.249674796474718
At time: 159.06195712089539 and batch: 600, loss is 3.2532861328125 and perplexity is 25.875229802768224
At time: 159.81822752952576 and batch: 650, loss is 3.0709941387176514 and perplexity is 21.563328963090143
At time: 160.5750322341919 and batch: 700, loss is 3.0580402326583864 and perplexity is 21.285801041529346
At time: 161.33261823654175 and batch: 750, loss is 3.1500164079666138 and perplexity is 23.336447481452556
At time: 162.0888810157776 and batch: 800, loss is 3.0709683513641357 and perplexity is 21.56277290907281
At time: 162.8449547290802 and batch: 850, loss is 3.1235790395736696 and perplexity is 22.727577149988647
At time: 163.60137677192688 and batch: 900, loss is 3.0833938789367674 and perplexity is 21.832373232817197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375279256742295 and perplexity of 79.46202676088274
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 165.48895645141602 and batch: 50, loss is 3.5090705585479736 and perplexity is 33.41719402022702
At time: 166.2652130126953 and batch: 100, loss is 3.376549253463745 and perplexity is 29.26959469408994
At time: 167.02116799354553 and batch: 150, loss is 3.389488682746887 and perplexity is 29.650787434494497
At time: 167.7775068283081 and batch: 200, loss is 3.277579927444458 and perplexity is 26.51153516282283
At time: 168.5329144001007 and batch: 250, loss is 3.4245770931243897 and perplexity is 30.709654780747005
At time: 169.28891348838806 and batch: 300, loss is 3.3958202981948853 and perplexity is 29.83912041508615
At time: 170.04606461524963 and batch: 350, loss is 3.362161536216736 and perplexity is 28.851487061450467
At time: 170.8024709224701 and batch: 400, loss is 3.303082904815674 and perplexity is 27.1963535816136
At time: 171.55772304534912 and batch: 450, loss is 3.300726079940796 and perplexity is 27.132332012434503
At time: 172.32075476646423 and batch: 500, loss is 3.1823802471160887 and perplexity is 24.104058941296152
At time: 173.08725428581238 and batch: 550, loss is 3.2262723350524904 and perplexity is 25.185598298311582
At time: 173.84996342658997 and batch: 600, loss is 3.251063575744629 and perplexity is 25.81778448927823
At time: 174.61869382858276 and batch: 650, loss is 3.0685839796066285 and perplexity is 21.511420488294952
At time: 175.3810269832611 and batch: 700, loss is 3.0555845212936403 and perplexity is 21.23359338770495
At time: 176.13465094566345 and batch: 750, loss is 3.147746443748474 and perplexity is 23.28353465851902
At time: 176.8870506286621 and batch: 800, loss is 3.0684556055068968 and perplexity is 21.50865915630136
At time: 177.65422797203064 and batch: 850, loss is 3.1207983255386353 and perplexity is 22.664466044859783
At time: 178.41210889816284 and batch: 900, loss is 3.080561571121216 and perplexity is 21.7706247181569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3752332713505995 and perplexity of 79.45837275247338
Annealing...
Model not improving. Stopping early with 78.46692270496843 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -78.46692270496843
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc03c314b70>
ELAPSED
190.52072167396545


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.046699047088623 and batch: 50, loss is 6.74489239692688 and perplexity is 849.7076905993049
At time: 1.8852458000183105 and batch: 100, loss is 5.826321487426758 and perplexity is 339.10896534440406
At time: 2.7091753482818604 and batch: 150, loss is 5.5878563976287845 and perplexity is 267.1623156918296
At time: 3.5354843139648438 and batch: 200, loss is 5.377043447494507 and perplexity is 216.38158525655348
At time: 4.363933563232422 and batch: 250, loss is 5.394100923538208 and perplexity is 220.1041676342317
At time: 5.1993818283081055 and batch: 300, loss is 5.310322408676147 and perplexity is 202.4154783754161
At time: 6.029860734939575 and batch: 350, loss is 5.260424938201904 and perplexity is 192.56330141694886
At time: 6.863222122192383 and batch: 400, loss is 5.0973539066314695 and perplexity is 163.588463737677
At time: 7.697906970977783 and batch: 450, loss is 5.101734886169433 and perplexity is 164.30671362019856
At time: 8.526904344558716 and batch: 500, loss is 5.021989431381225 and perplexity is 151.71282602273686
At time: 9.354890584945679 and batch: 550, loss is 5.080204601287842 and perplexity is 160.80695387286798
At time: 10.183343410491943 and batch: 600, loss is 5.006590147018432 and perplexity is 149.3944535263244
At time: 11.012219429016113 and batch: 650, loss is 4.887955541610718 and perplexity is 132.68203367251357
At time: 11.849043607711792 and batch: 700, loss is 4.9622181797027585 and perplexity is 142.91044562395084
At time: 12.690390586853027 and batch: 750, loss is 4.9690929126739505 and perplexity is 143.89630164205877
At time: 13.523639917373657 and batch: 800, loss is 4.90912802696228 and perplexity is 135.52119204709547
At time: 14.353561639785767 and batch: 850, loss is 4.956871881484985 and perplexity is 142.14844252613315
At time: 15.185032844543457 and batch: 900, loss is 4.879876909255981 and perplexity is 131.6144623666258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.845614916657748 and perplexity of 127.18146395432076
finished 1 epochs...
Completing Train Step...
At time: 17.189617395401 and batch: 50, loss is 4.828080930709839 and perplexity is 124.97090256311121
At time: 17.946850061416626 and batch: 100, loss is 4.674331836700439 and perplexity is 107.1609421574369
At time: 18.708317518234253 and batch: 150, loss is 4.653575229644775 and perplexity is 104.95957013184693
At time: 19.473534107208252 and batch: 200, loss is 4.541701726913452 and perplexity is 93.85037201096296
At time: 20.26552438735962 and batch: 250, loss is 4.657396373748779 and perplexity is 105.36140301615853
At time: 21.03245258331299 and batch: 300, loss is 4.616946830749511 and perplexity is 101.18462624615682
At time: 21.801711320877075 and batch: 350, loss is 4.5938409805297855 and perplexity is 98.87347283224355
At time: 22.561216831207275 and batch: 400, loss is 4.486037797927857 and perplexity is 88.7690273389128
At time: 23.320784330368042 and batch: 450, loss is 4.504265298843384 and perplexity is 90.40190126264397
At time: 24.08701801300049 and batch: 500, loss is 4.395010099411011 and perplexity is 81.04544926989702
At time: 24.850839614868164 and batch: 550, loss is 4.474307126998902 and perplexity is 87.73379097053073
At time: 25.617947816848755 and batch: 600, loss is 4.4516877174377445 and perplexity is 85.77158010684015
At time: 26.378510236740112 and batch: 650, loss is 4.307727069854736 and perplexity is 74.27148305105501
At time: 27.148915767669678 and batch: 700, loss is 4.334206776618958 and perplexity is 76.26444015343078
At time: 27.9076144695282 and batch: 750, loss is 4.412437324523926 and perplexity is 82.47022544678738
At time: 28.671647787094116 and batch: 800, loss is 4.348407135009766 and perplexity is 77.35514843191268
At time: 29.430410146713257 and batch: 850, loss is 4.419817295074463 and perplexity is 83.08110464455277
At time: 30.19285297393799 and batch: 900, loss is 4.36215262889862 and perplexity is 78.42577443672953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.511869665694563 and perplexity of 91.09197092999067
finished 2 epochs...
Completing Train Step...
At time: 32.0558967590332 and batch: 50, loss is 4.401379737854004 and perplexity is 81.56332707542927
At time: 32.84083032608032 and batch: 100, loss is 4.2506253528594975 and perplexity is 70.14926667755155
At time: 33.605398654937744 and batch: 150, loss is 4.2452045345306395 and perplexity is 69.77002906475737
At time: 34.371845722198486 and batch: 200, loss is 4.138001260757446 and perplexity is 62.67742034922551
At time: 35.13534617424011 and batch: 250, loss is 4.2903583002090455 and perplexity is 72.99261708479155
At time: 35.89018678665161 and batch: 300, loss is 4.259183554649353 and perplexity is 70.75219456645044
At time: 36.645297050476074 and batch: 350, loss is 4.246274828910828 and perplexity is 69.84474351086453
At time: 37.40261387825012 and batch: 400, loss is 4.16397572517395 and perplexity is 64.32676040675851
At time: 38.15958642959595 and batch: 450, loss is 4.185817399024963 and perplexity is 65.74722067258045
At time: 38.93525815010071 and batch: 500, loss is 4.062204165458679 and perplexity is 58.102236986070764
At time: 39.69425177574158 and batch: 550, loss is 4.148439378738403 and perplexity is 63.33508105636267
At time: 40.45315337181091 and batch: 600, loss is 4.1535216808319095 and perplexity is 63.65778842495352
At time: 41.21134161949158 and batch: 650, loss is 3.9971879291534425 and perplexity is 54.44483183900544
At time: 41.96992349624634 and batch: 700, loss is 4.015747780799866 and perplexity is 55.4647553782161
At time: 42.72774147987366 and batch: 750, loss is 4.120403900146484 and perplexity is 61.58411107331626
At time: 43.48592162132263 and batch: 800, loss is 4.061874542236328 and perplexity is 58.08308829558953
At time: 44.24426460266113 and batch: 850, loss is 4.141644239425659 and perplexity is 62.906169265264346
At time: 45.00351285934448 and batch: 900, loss is 4.095303840637207 and perplexity is 60.05758432018827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.40722196396083 and perplexity of 82.04123313254301
finished 3 epochs...
Completing Train Step...
At time: 46.86263704299927 and batch: 50, loss is 4.15194950580597 and perplexity is 63.55778587114237
At time: 47.63784837722778 and batch: 100, loss is 4.010766372680664 and perplexity is 55.18914981725101
At time: 48.3949031829834 and batch: 150, loss is 4.0102299785614015 and perplexity is 55.15955461989642
At time: 49.1494996547699 and batch: 200, loss is 3.903648142814636 and perplexity is 49.58300544266074
At time: 49.90217685699463 and batch: 250, loss is 4.061575455665588 and perplexity is 58.06571902148101
At time: 50.66506314277649 and batch: 300, loss is 4.033925929069519 and perplexity is 56.48222173445505
At time: 51.42104697227478 and batch: 350, loss is 4.019271249771118 and perplexity is 55.660528420368294
At time: 52.17688179016113 and batch: 400, loss is 3.9508304834365844 and perplexity is 51.97851621170207
At time: 52.93133497238159 and batch: 450, loss is 3.976748433113098 and perplexity is 53.34330261415988
At time: 53.68614912033081 and batch: 500, loss is 3.8496364068984987 and perplexity is 46.97597998383358
At time: 54.44073438644409 and batch: 550, loss is 3.935706739425659 and perplexity is 51.1983210453949
At time: 55.199947118759155 and batch: 600, loss is 3.9530303859710694 and perplexity is 52.09298975040202
At time: 55.963590145111084 and batch: 650, loss is 3.793767123222351 and perplexity is 44.423434009995546
At time: 56.72201061248779 and batch: 700, loss is 3.8064044380187987 and perplexity is 44.98838916934358
At time: 57.48474431037903 and batch: 750, loss is 3.9216153621673584 and perplexity is 50.481925542532196
At time: 58.269047260284424 and batch: 800, loss is 3.8687136793136596 and perplexity is 47.88075644408709
At time: 59.03702235221863 and batch: 850, loss is 3.946298017501831 and perplexity is 51.74345845567346
At time: 59.79901075363159 and batch: 900, loss is 3.9054264545440676 and perplexity is 49.67125792977897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379498207405822 and perplexity of 79.79798132094814
finished 4 epochs...
Completing Train Step...
At time: 61.71524167060852 and batch: 50, loss is 3.969873285293579 and perplexity is 52.97781734576415
At time: 62.47605514526367 and batch: 100, loss is 3.8351827096939086 and perplexity is 46.301886698699555
At time: 63.233609199523926 and batch: 150, loss is 3.834636187553406 and perplexity is 46.276588606085205
At time: 63.99069857597351 and batch: 200, loss is 3.7297070026397705 and perplexity is 41.66689808420844
At time: 64.75033402442932 and batch: 250, loss is 3.8906621170043945 and perplexity is 48.94328197700598
At time: 65.5056254863739 and batch: 300, loss is 3.865577621459961 and perplexity is 47.73083482610481
At time: 66.26015090942383 and batch: 350, loss is 3.8465559387207033 and perplexity is 46.83149462789839
At time: 67.02378487586975 and batch: 400, loss is 3.7853122615814208 and perplexity is 44.04942335400057
At time: 67.78432130813599 and batch: 450, loss is 3.812912592887878 and perplexity is 45.282135409552296
At time: 68.53950500488281 and batch: 500, loss is 3.6894647884368896 and perplexity is 40.02342022658069
At time: 69.30089378356934 and batch: 550, loss is 3.772247133255005 and perplexity is 43.477655230392905
At time: 70.06061887741089 and batch: 600, loss is 3.7955182123184206 and perplexity is 44.50129154874937
At time: 70.8194932937622 and batch: 650, loss is 3.6379913330078124 and perplexity is 38.01539970425789
At time: 71.57762241363525 and batch: 700, loss is 3.6491269874572754 and perplexity is 38.44109184047915
At time: 72.3412127494812 and batch: 750, loss is 3.7669556760787963 and perplexity is 43.248202684160326
At time: 73.1019172668457 and batch: 800, loss is 3.7142542934417726 and perplexity is 41.02798083978931
At time: 73.87168645858765 and batch: 850, loss is 3.7934467124938966 and perplexity is 44.40920254522272
At time: 74.64054012298584 and batch: 900, loss is 3.7542445182800295 and perplexity is 42.701947080931944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385681570392766 and perplexity of 80.29292985130584
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.5240831375122 and batch: 50, loss is 3.838947949409485 and perplexity is 46.476553025344536
At time: 77.29591369628906 and batch: 100, loss is 3.707666721343994 and perplexity is 40.75859433368955
At time: 78.05367159843445 and batch: 150, loss is 3.711049060821533 and perplexity is 40.896687143082794
At time: 78.81161332130432 and batch: 200, loss is 3.5887546300888062 and perplexity is 36.18897918544846
At time: 79.56942129135132 and batch: 250, loss is 3.7452886962890624 and perplexity is 42.321223435519165
At time: 80.32732105255127 and batch: 300, loss is 3.7026539611816407 and perplexity is 40.55479250733504
At time: 81.09444665908813 and batch: 350, loss is 3.6707707500457762 and perplexity is 39.28217093044697
At time: 81.86514472961426 and batch: 400, loss is 3.6021070003509523 and perplexity is 36.67542823160601
At time: 82.63556742668152 and batch: 450, loss is 3.6144912910461424 and perplexity is 37.13245150961306
At time: 83.41242551803589 and batch: 500, loss is 3.480428304672241 and perplexity is 32.473627704198755
At time: 84.1811625957489 and batch: 550, loss is 3.546929273605347 and perplexity is 34.706579280495085
At time: 84.94528341293335 and batch: 600, loss is 3.5584692192077636 and perplexity is 35.1094111760904
At time: 85.72311496734619 and batch: 650, loss is 3.382571539878845 and perplexity is 29.44639641754788
At time: 86.50180625915527 and batch: 700, loss is 3.3701035261154173 and perplexity is 29.081537600577636
At time: 87.27282929420471 and batch: 750, loss is 3.4684559869766236 and perplexity is 32.08716118279669
At time: 88.04066252708435 and batch: 800, loss is 3.395699119567871 and perplexity is 29.83550477051679
At time: 88.81252884864807 and batch: 850, loss is 3.452224488258362 and perplexity is 31.570542556846828
At time: 89.58514904975891 and batch: 900, loss is 3.4024616289138794 and perplexity is 30.03795140247498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353074165239726 and perplexity of 77.71701100305113
finished 6 epochs...
Completing Train Step...
At time: 91.4895339012146 and batch: 50, loss is 3.7143532848358154 and perplexity is 41.0320424578367
At time: 92.25924849510193 and batch: 100, loss is 3.5804149389266966 and perplexity is 35.88842926427696
At time: 93.01572561264038 and batch: 150, loss is 3.5855758953094483 and perplexity is 36.074126658252766
At time: 93.77411270141602 and batch: 200, loss is 3.465439486503601 and perplexity is 31.990516084187018
At time: 94.54216885566711 and batch: 250, loss is 3.6244936656951903 and perplexity is 37.505727914364776
At time: 95.30179119110107 and batch: 300, loss is 3.5873397827148437 and perplexity is 36.13781350762925
At time: 96.05756998062134 and batch: 350, loss is 3.560360426902771 and perplexity is 35.175873191602015
At time: 96.829993724823 and batch: 400, loss is 3.497332649230957 and perplexity is 33.02723913201971
At time: 97.58881449699402 and batch: 450, loss is 3.516932239532471 and perplexity is 33.68094474258309
At time: 98.35000538825989 and batch: 500, loss is 3.3873668479919434 and perplexity is 29.58794006265265
At time: 99.11438393592834 and batch: 550, loss is 3.4563105297088623 and perplexity is 31.699805008372103
At time: 99.88054776191711 and batch: 600, loss is 3.475750527381897 and perplexity is 32.32207804012311
At time: 100.64510130882263 and batch: 650, loss is 3.305598087310791 and perplexity is 27.26484347025119
At time: 101.4021680355072 and batch: 700, loss is 3.3002589416503905 and perplexity is 27.119660421166675
At time: 102.15989589691162 and batch: 750, loss is 3.4066316747665404 and perplexity is 30.163472569761304
At time: 102.9229485988617 and batch: 800, loss is 3.3408861684799196 and perplexity is 28.244144689463045
At time: 103.68662643432617 and batch: 850, loss is 3.4069900703430176 and perplexity is 30.1742849623426
At time: 104.44682383537292 and batch: 900, loss is 3.3657917833328246 and perplexity is 28.956415431872347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37003065135381 and perplexity of 79.04605453101743
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 106.31976914405823 and batch: 50, loss is 3.656343731880188 and perplexity is 38.71951482098834
At time: 107.08479166030884 and batch: 100, loss is 3.5337841367721556 and perplexity is 34.25334200622988
At time: 107.84177279472351 and batch: 150, loss is 3.5435880184173585 and perplexity is 34.5908092587143
At time: 108.59891843795776 and batch: 200, loss is 3.4184268856048585 and perplexity is 30.521363639299064
At time: 109.36288905143738 and batch: 250, loss is 3.574939332008362 and perplexity is 35.692455360386276
At time: 110.11928462982178 and batch: 300, loss is 3.5372142791748047 and perplexity is 34.371037587981384
At time: 110.87695932388306 and batch: 350, loss is 3.5032894086837767 and perplexity is 33.2245615685992
At time: 111.63440275192261 and batch: 400, loss is 3.43978524684906 and perplexity is 31.180261389877455
At time: 112.39227342605591 and batch: 450, loss is 3.45389762878418 and perplexity is 31.623408724952107
At time: 113.14862871170044 and batch: 500, loss is 3.3177675342559816 and perplexity is 27.5986686504188
At time: 113.90457272529602 and batch: 550, loss is 3.3808928298950196 and perplexity is 29.397005925648795
At time: 114.66189742088318 and batch: 600, loss is 3.3990281009674073 and perplexity is 29.93499211482544
At time: 115.4374749660492 and batch: 650, loss is 3.219926567077637 and perplexity is 25.026282360810733
At time: 116.19762420654297 and batch: 700, loss is 3.2039475297927855 and perplexity is 24.6295644878246
At time: 116.95719981193542 and batch: 750, loss is 3.303897957801819 and perplexity is 27.218529086690747
At time: 117.7267906665802 and batch: 800, loss is 3.23324236869812 and perplexity is 25.361755965103896
At time: 118.49008703231812 and batch: 850, loss is 3.2948819971084595 and perplexity is 26.974230844851856
At time: 119.24681949615479 and batch: 900, loss is 3.252613248825073 and perplexity is 25.857824631450214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363356080773759 and perplexity of 78.52021289670756
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.09758424758911 and batch: 50, loss is 3.628363461494446 and perplexity is 37.65114861542812
At time: 121.86833310127258 and batch: 100, loss is 3.5062091827392576 and perplexity is 33.32171154043795
At time: 122.62352228164673 and batch: 150, loss is 3.5180702304840086 and perplexity is 33.71929517002037
At time: 123.38184452056885 and batch: 200, loss is 3.3934052753448487 and perplexity is 29.7671452033312
At time: 124.14071440696716 and batch: 250, loss is 3.5466636323928835 and perplexity is 34.69736100712542
At time: 124.89916276931763 and batch: 300, loss is 3.5139652776718138 and perplexity is 33.581162761963625
At time: 125.66102457046509 and batch: 350, loss is 3.477522087097168 and perplexity is 32.37938928164729
At time: 126.42276906967163 and batch: 400, loss is 3.4153387689590455 and perplexity is 30.42725549144881
At time: 127.1899082660675 and batch: 450, loss is 3.429300990104675 and perplexity is 30.85506721249611
At time: 127.94711780548096 and batch: 500, loss is 3.291504240036011 and perplexity is 26.883272150509153
At time: 128.71278142929077 and batch: 550, loss is 3.352626671791077 and perplexity is 28.577699382291705
At time: 129.47268533706665 and batch: 600, loss is 3.3716702270507812 and perplexity is 29.127135382455197
At time: 130.24423933029175 and batch: 650, loss is 3.1923621845245362 and perplexity is 24.345869007603408
At time: 131.01098370552063 and batch: 700, loss is 3.173891296386719 and perplexity is 23.90030681700654
At time: 131.76721167564392 and batch: 750, loss is 3.2726591491699217 and perplexity is 26.381398226668264
At time: 132.52409434318542 and batch: 800, loss is 3.200770263671875 and perplexity is 24.551433993418488
At time: 133.28133392333984 and batch: 850, loss is 3.259370594024658 and perplexity is 26.033146566890164
At time: 134.03822326660156 and batch: 900, loss is 3.219351668357849 and perplexity is 25.011898918028027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360930560386344 and perplexity of 78.32999130593423
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.89849638938904 and batch: 50, loss is 3.618541131019592 and perplexity is 37.283136915988166
At time: 136.6697633266449 and batch: 100, loss is 3.495235996246338 and perplexity is 32.95806501491989
At time: 137.42828512191772 and batch: 150, loss is 3.508539662361145 and perplexity is 33.39945766784467
At time: 138.18590760231018 and batch: 200, loss is 3.383799977302551 and perplexity is 29.48259170016861
At time: 138.9437711238861 and batch: 250, loss is 3.5363669872283934 and perplexity is 34.341927618706066
At time: 139.70790219306946 and batch: 300, loss is 3.5037396717071534 and perplexity is 33.23952472856747
At time: 140.47674012184143 and batch: 350, loss is 3.467478184700012 and perplexity is 32.055801617767756
At time: 141.24112010002136 and batch: 400, loss is 3.406135892868042 and perplexity is 30.148521772538924
At time: 142.0001699924469 and batch: 450, loss is 3.4212202072143554 and perplexity is 30.606738808543938
At time: 142.75696420669556 and batch: 500, loss is 3.2828756046295164 and perplexity is 26.652304099168596
At time: 143.5153350830078 and batch: 550, loss is 3.343076219558716 and perplexity is 28.306068592514638
At time: 144.27560019493103 and batch: 600, loss is 3.3627293157577514 and perplexity is 28.867872996891084
At time: 145.0335991382599 and batch: 650, loss is 3.183459858894348 and perplexity is 24.130096019672294
At time: 145.7924063205719 and batch: 700, loss is 3.165426983833313 and perplexity is 23.698860903280963
At time: 146.5511598587036 and batch: 750, loss is 3.2643184995651247 and perplexity is 26.16227531199937
At time: 147.30969858169556 and batch: 800, loss is 3.1925528049468994 and perplexity is 24.35051026978206
At time: 148.06870436668396 and batch: 850, loss is 3.2486877250671387 and perplexity is 25.756518097226635
At time: 148.8278887271881 and batch: 900, loss is 3.2089216041564943 and perplexity is 24.75237896408483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3608155969071065 and perplexity of 78.32098673521334
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.72316980361938 and batch: 50, loss is 3.6162461853027343 and perplexity is 37.19767224651065
At time: 151.47866225242615 and batch: 100, loss is 3.4923003816604616 and perplexity is 32.861454713226166
At time: 152.2335662841797 and batch: 150, loss is 3.505539240837097 and perplexity is 33.299395405716844
At time: 152.9894347190857 and batch: 200, loss is 3.3810516929626466 and perplexity is 29.401676395161928
At time: 153.76174783706665 and batch: 250, loss is 3.533515200614929 and perplexity is 34.24413128266237
At time: 154.51502871513367 and batch: 300, loss is 3.501085720062256 and perplexity is 33.15142559436975
At time: 155.26889944076538 and batch: 350, loss is 3.464596037864685 and perplexity is 31.96354510285528
At time: 156.02316856384277 and batch: 400, loss is 3.4034050130844116 and perplexity is 30.0663021010408
At time: 156.77791047096252 and batch: 450, loss is 3.4191058349609373 and perplexity is 30.542093135831465
At time: 157.5327970981598 and batch: 500, loss is 3.280572476387024 and perplexity is 26.59099105787123
At time: 158.2870111465454 and batch: 550, loss is 3.340450963973999 and perplexity is 28.231855384802667
At time: 159.04169511795044 and batch: 600, loss is 3.360371527671814 and perplexity is 28.799888847476012
At time: 159.79482293128967 and batch: 650, loss is 3.180863881111145 and perplexity is 24.067536063759647
At time: 160.5491647720337 and batch: 700, loss is 3.1629824209213258 and perplexity is 23.640998300062396
At time: 161.30389785766602 and batch: 750, loss is 3.262142605781555 and perplexity is 26.10541086771482
At time: 162.06074452400208 and batch: 800, loss is 3.1903988313674927 and perplexity is 24.298116361819854
At time: 162.8169014453888 and batch: 850, loss is 3.2459773349761964 and perplexity is 25.68680240683581
At time: 163.57383346557617 and batch: 900, loss is 3.205944690704346 and perplexity is 24.67880284338175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360577727017337 and perplexity of 78.30235874633867
Annealing...
Model not improving. Stopping early with 77.71701100305113 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc03c314b70>
ELAPSED
360.95639657974243


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -77.71701100305113}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'dropout': 0.636192611183658, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.9359653836036915, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0358881950378418 and batch: 50, loss is 6.921063041687011 and perplexity is 1013.3967050824042
At time: 1.8730947971343994 and batch: 100, loss is 6.15620192527771 and perplexity is 471.63336999995556
At time: 2.6957850456237793 and batch: 150, loss is 6.0697740459442135 and perplexity is 432.5829266640892
At time: 3.517068386077881 and batch: 200, loss is 5.935130681991577 and perplexity is 378.0894024194222
At time: 4.338405609130859 and batch: 250, loss is 5.996760158538819 and perplexity is 402.1238631860399
At time: 5.159905910491943 and batch: 300, loss is 5.904402751922607 and perplexity is 366.6481804609179
At time: 5.9818713665008545 and batch: 350, loss is 5.9088567447662355 and perplexity is 368.2848710311352
At time: 6.804211378097534 and batch: 400, loss is 5.788296709060669 and perplexity is 326.45650015039257
At time: 7.624235391616821 and batch: 450, loss is 5.792578096389771 and perplexity is 327.85718316732476
At time: 8.4554603099823 and batch: 500, loss is 5.755070610046387 and perplexity is 315.7878445313503
At time: 9.279536008834839 and batch: 550, loss is 5.8099019241333005 and perplexity is 333.58640729413776
At time: 10.115595579147339 and batch: 600, loss is 5.743160028457641 and perplexity is 312.04893811986847
At time: 10.948704957962036 and batch: 650, loss is 5.653525381088257 and perplexity is 285.2954702904289
At time: 11.796362400054932 and batch: 700, loss is 5.762107477188111 and perplexity is 318.01783852203897
At time: 12.620041847229004 and batch: 750, loss is 5.719876747131348 and perplexity is 304.86734486633935
At time: 13.444538116455078 and batch: 800, loss is 5.7221004676818845 and perplexity is 305.54603897965507
At time: 14.2747163772583 and batch: 850, loss is 5.7630782794952395 and perplexity is 318.3267208809786
At time: 15.098783254623413 and batch: 900, loss is 5.645048198699951 and perplexity is 282.8871906890207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.54258790734696 and perplexity of 255.33793608356868
finished 1 epochs...
Completing Train Step...
At time: 17.001778841018677 and batch: 50, loss is 5.444245080947876 and perplexity is 231.42250849061247
At time: 17.756911993026733 and batch: 100, loss is 5.238733558654785 and perplexity is 188.4313140250281
At time: 18.52247428894043 and batch: 150, loss is 5.151224365234375 and perplexity is 172.64273873512965
At time: 19.288551092147827 and batch: 200, loss is 4.982898750305176 and perplexity is 145.89668734548943
At time: 20.043457984924316 and batch: 250, loss is 5.027940664291382 and perplexity is 152.61839634307583
At time: 20.797508239746094 and batch: 300, loss is 4.9495993328094485 and perplexity is 141.11841107549773
At time: 21.553054571151733 and batch: 350, loss is 4.909957447052002 and perplexity is 135.6336426743204
At time: 22.311060190200806 and batch: 400, loss is 4.762799367904663 and perplexity is 117.07319856152397
At time: 23.068098545074463 and batch: 450, loss is 4.766224222183228 and perplexity is 117.47484460350883
At time: 23.82274031639099 and batch: 500, loss is 4.667988014221192 and perplexity is 106.48328390814186
At time: 24.590431451797485 and batch: 550, loss is 4.734695749282837 and perplexity is 113.82882094878671
At time: 25.35810422897339 and batch: 600, loss is 4.671768102645874 and perplexity is 106.88656187003764
At time: 26.125203371047974 and batch: 650, loss is 4.53155821800232 and perplexity is 92.9032118112761
At time: 26.882189750671387 and batch: 700, loss is 4.576109571456909 and perplexity is 97.13575842975435
At time: 27.640073537826538 and batch: 750, loss is 4.6129105758666995 and perplexity is 100.77704241386279
At time: 28.397876739501953 and batch: 800, loss is 4.542150583267212 and perplexity is 93.89250680226979
At time: 29.164600372314453 and batch: 850, loss is 4.601221342086792 and perplexity is 99.60589425302726
At time: 29.9228572845459 and batch: 900, loss is 4.535833721160889 and perplexity is 93.30127013024351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.625877902932363 and perplexity of 102.09236093526347
finished 2 epochs...
Completing Train Step...
At time: 31.80848002433777 and batch: 50, loss is 4.572682342529297 and perplexity is 96.80342177076335
At time: 32.57566952705383 and batch: 100, loss is 4.424113602638244 and perplexity is 83.43881448837342
At time: 33.330190896987915 and batch: 150, loss is 4.418088541030884 and perplexity is 82.93760192514598
At time: 34.08539366722107 and batch: 200, loss is 4.309798917770386 and perplexity is 74.42552178572059
At time: 34.84114646911621 and batch: 250, loss is 4.444803943634033 and perplexity is 85.18317549624663
At time: 35.59718441963196 and batch: 300, loss is 4.407987790107727 and perplexity is 82.10408651831081
At time: 36.352375984191895 and batch: 350, loss is 4.395169515609741 and perplexity is 81.05837025722394
At time: 37.12054491043091 and batch: 400, loss is 4.302400941848755 and perplexity is 73.87695521067246
At time: 37.884159088134766 and batch: 450, loss is 4.319792728424073 and perplexity is 75.17304544154196
At time: 38.65160918235779 and batch: 500, loss is 4.205030560493469 and perplexity is 67.02264588013904
At time: 39.407190561294556 and batch: 550, loss is 4.286415672302246 and perplexity is 72.7054009209622
At time: 40.163193464279175 and batch: 600, loss is 4.27866099357605 and perplexity is 72.14377432651335
At time: 40.91739368438721 and batch: 650, loss is 4.131424870491028 and perplexity is 62.26658157088115
At time: 41.67970657348633 and batch: 700, loss is 4.1519341564178465 and perplexity is 63.55681030550597
At time: 42.44490098953247 and batch: 750, loss is 4.240607419013977 and perplexity is 69.45002429456046
At time: 43.20260190963745 and batch: 800, loss is 4.181571192741394 and perplexity is 65.46863629298404
At time: 43.95800304412842 and batch: 850, loss is 4.24904682636261 and perplexity is 70.03862155246964
At time: 44.72415518760681 and batch: 900, loss is 4.201989274024964 and perplexity is 66.81912046056699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.443306700824058 and perplexity of 85.05573103066696
finished 3 epochs...
Completing Train Step...
At time: 46.56289458274841 and batch: 50, loss is 4.269157490730286 and perplexity is 71.46140335490826
At time: 47.34023118019104 and batch: 100, loss is 4.122551960945129 and perplexity is 61.716539669549746
At time: 48.096805572509766 and batch: 150, loss is 4.12786304473877 and perplexity is 62.045193364299934
At time: 48.85349369049072 and batch: 200, loss is 4.019462480545044 and perplexity is 55.67117344409084
At time: 49.60969614982605 and batch: 250, loss is 4.172653174400329 and perplexity is 64.8873814666718
At time: 50.37650680541992 and batch: 300, loss is 4.145917792320251 and perplexity is 63.17557736186888
At time: 51.138829469680786 and batch: 350, loss is 4.128669986724853 and perplexity is 62.0952804418253
At time: 51.89959788322449 and batch: 400, loss is 4.0546248292922975 and perplexity is 57.66352527191183
At time: 52.655367612838745 and batch: 450, loss is 4.0727293491363525 and perplexity is 58.71700329129244
At time: 53.41174554824829 and batch: 500, loss is 3.9548079347610474 and perplexity is 52.18566992866525
At time: 54.16848373413086 and batch: 550, loss is 4.039549202919006 and perplexity is 56.800731429209314
At time: 54.92872977256775 and batch: 600, loss is 4.04895866394043 and perplexity is 57.33771811324757
At time: 55.69840717315674 and batch: 650, loss is 3.8971012258529663 and perplexity is 49.2594499245612
At time: 56.46453380584717 and batch: 700, loss is 3.906802625656128 and perplexity is 49.73966113650474
At time: 57.22784733772278 and batch: 750, loss is 4.011722273826599 and perplexity is 55.24193041130066
At time: 57.985431432724 and batch: 800, loss is 3.959798364639282 and perplexity is 52.44674976356133
At time: 58.74093508720398 and batch: 850, loss is 4.029104170799255 and perplexity is 56.21053364820327
At time: 59.49736762046814 and batch: 900, loss is 3.985614342689514 and perplexity is 53.81834222910236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384212964201627 and perplexity of 80.1750977031111
finished 4 epochs...
Completing Train Step...
At time: 61.40435075759888 and batch: 50, loss is 4.063625020980835 and perplexity is 58.18485054741088
At time: 62.16123819351196 and batch: 100, loss is 3.91957115650177 and perplexity is 50.378835508843
At time: 62.91916275024414 and batch: 150, loss is 3.9287649631500243 and perplexity is 50.84414448449504
At time: 63.681012868881226 and batch: 200, loss is 3.821711220741272 and perplexity is 45.68231399698359
At time: 64.44346189498901 and batch: 250, loss is 3.9760954904556276 and perplexity is 53.3084838649531
At time: 65.2008707523346 and batch: 300, loss is 3.9526291275024414 and perplexity is 52.07209119025105
At time: 65.96489810943604 and batch: 350, loss is 3.937433223724365 and perplexity is 51.286790491379165
At time: 66.72648549079895 and batch: 400, loss is 3.873791551589966 and perplexity is 48.12450715353323
At time: 67.4831976890564 and batch: 450, loss is 3.8891240978240966 and perplexity is 48.86806412865269
At time: 68.24851298332214 and batch: 500, loss is 3.774998264312744 and perplexity is 43.5974326441496
At time: 69.0260705947876 and batch: 550, loss is 3.858274164199829 and perplexity is 47.3835046135388
At time: 69.78808069229126 and batch: 600, loss is 3.8757168436050415 and perplexity is 48.217250132896275
At time: 70.55916094779968 and batch: 650, loss is 3.717274193763733 and perplexity is 41.152068524251355
At time: 71.31545686721802 and batch: 700, loss is 3.7268807554244994 and perplexity is 41.54930338365211
At time: 72.0725884437561 and batch: 750, loss is 3.838584566116333 and perplexity is 46.45966729063459
At time: 72.82948780059814 and batch: 800, loss is 3.7902052116394045 and perplexity is 44.26548313636061
At time: 73.58631992340088 and batch: 850, loss is 3.862483034133911 and perplexity is 47.58335590049114
At time: 74.34319853782654 and batch: 900, loss is 3.8207706785202027 and perplexity is 45.639368051316744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368573332486087 and perplexity of 78.93094312171726
finished 5 epochs...
Completing Train Step...
At time: 76.19847965240479 and batch: 50, loss is 3.902928628921509 and perplexity is 49.54734261287105
At time: 76.96666717529297 and batch: 100, loss is 3.762228217124939 and perplexity is 43.04423109467906
At time: 77.7237708568573 and batch: 150, loss is 3.7708350133895876 and perplexity is 43.41630289835886
At time: 78.48161935806274 and batch: 200, loss is 3.6635997533798217 and perplexity is 39.001486211074365
At time: 79.23817706108093 and batch: 250, loss is 3.817781891822815 and perplexity is 45.50316535699803
At time: 79.99567914009094 and batch: 300, loss is 3.7990648412704466 and perplexity is 44.65940133039033
At time: 80.75295424461365 and batch: 350, loss is 3.783838291168213 and perplexity is 43.98454363439787
At time: 81.51101088523865 and batch: 400, loss is 3.724816846847534 and perplexity is 41.463637853352274
At time: 82.26760601997375 and batch: 450, loss is 3.740417823791504 and perplexity is 42.11558338203871
At time: 83.02513694763184 and batch: 500, loss is 3.630280704498291 and perplexity is 37.72340426037944
At time: 83.79132914543152 and batch: 550, loss is 3.7073722648620606 and perplexity is 40.74659446819929
At time: 84.54941606521606 and batch: 600, loss is 3.7317603969573976 and perplexity is 41.75254455886638
At time: 85.30691647529602 and batch: 650, loss is 3.574966912269592 and perplexity is 35.6934397812043
At time: 86.06313705444336 and batch: 700, loss is 3.5818162965774536 and perplexity is 35.93875704457393
At time: 86.81974840164185 and batch: 750, loss is 3.6949245071411134 and perplexity is 40.24253444931978
At time: 87.57749676704407 and batch: 800, loss is 3.650184407234192 and perplexity is 38.48176171001019
At time: 88.35210824012756 and batch: 850, loss is 3.7188991641998292 and perplexity is 41.21899378004202
At time: 89.11337971687317 and batch: 900, loss is 3.68209171295166 and perplexity is 39.72940974074703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37876871187393 and perplexity of 79.73979027775405
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 90.95104575157166 and batch: 50, loss is 3.784445676803589 and perplexity is 44.01126732935263
At time: 91.72392892837524 and batch: 100, loss is 3.6469796085357666 and perplexity is 38.358632817211856
At time: 92.4903552532196 and batch: 150, loss is 3.656982431411743 and perplexity is 38.744252856211325
At time: 93.2480537891388 and batch: 200, loss is 3.5349183893203735 and perplexity is 34.292215988974704
At time: 94.00672388076782 and batch: 250, loss is 3.678495235443115 and perplexity is 39.586780447447666
At time: 94.76475858688354 and batch: 300, loss is 3.644467663764954 and perplexity is 38.262398967750485
At time: 95.52092790603638 and batch: 350, loss is 3.618329210281372 and perplexity is 37.275236683230915
At time: 96.27462077140808 and batch: 400, loss is 3.5524322509765627 and perplexity is 34.89809527168835
At time: 97.0296425819397 and batch: 450, loss is 3.551354947090149 and perplexity is 34.86051966183312
At time: 97.78388690948486 and batch: 500, loss is 3.429788842201233 and perplexity is 30.870123594070204
At time: 98.53854846954346 and batch: 550, loss is 3.4864850521087645 and perplexity is 32.67090910426585
At time: 99.29291486740112 and batch: 600, loss is 3.500983943939209 and perplexity is 33.14805174249098
At time: 100.04744529724121 and batch: 650, loss is 3.325828471183777 and perplexity is 27.82203885474102
At time: 100.80137968063354 and batch: 700, loss is 3.3150477504730222 and perplexity is 27.523708223277424
At time: 101.55634427070618 and batch: 750, loss is 3.412867603302002 and perplexity is 30.3521575306076
At time: 102.31034207344055 and batch: 800, loss is 3.34247661113739 and perplexity is 28.28910112283915
At time: 103.06392240524292 and batch: 850, loss is 3.388009343147278 and perplexity is 29.606956279058533
At time: 103.81865692138672 and batch: 900, loss is 3.339259729385376 and perplexity is 28.1982446452796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3609293062392975 and perplexity of 78.32989306866857
finished 7 epochs...
Completing Train Step...
At time: 105.66239070892334 and batch: 50, loss is 3.665906352996826 and perplexity is 39.09155085583732
At time: 106.41711330413818 and batch: 100, loss is 3.5267668104171754 and perplexity is 34.01381652377314
At time: 107.1832869052887 and batch: 150, loss is 3.533689031600952 and perplexity is 34.25008449117979
At time: 107.93782043457031 and batch: 200, loss is 3.417712616920471 and perplexity is 30.499570968884235
At time: 108.69191026687622 and batch: 250, loss is 3.559919652938843 and perplexity is 35.16037199905272
At time: 109.44676947593689 and batch: 300, loss is 3.5326516675949096 and perplexity is 34.21457310862432
At time: 110.20150685310364 and batch: 350, loss is 3.5110936117172242 and perplexity is 33.48486721064753
At time: 110.95648407936096 and batch: 400, loss is 3.4506192636489867 and perplexity is 31.519905397883367
At time: 111.71179533004761 and batch: 450, loss is 3.455206427574158 and perplexity is 31.66482450057287
At time: 112.46755123138428 and batch: 500, loss is 3.3392419481277464 and perplexity is 28.197743249484603
At time: 113.22216176986694 and batch: 550, loss is 3.3985108947753906 and perplexity is 29.919513554694298
At time: 113.9762909412384 and batch: 600, loss is 3.420563817024231 and perplexity is 30.586655437423275
At time: 114.73151898384094 and batch: 650, loss is 3.25341157913208 and perplexity is 25.878475958720173
At time: 115.48654770851135 and batch: 700, loss is 3.247759690284729 and perplexity is 25.73262624051041
At time: 116.24260187149048 and batch: 750, loss is 3.35223801612854 and perplexity is 28.56659465570172
At time: 117.00988960266113 and batch: 800, loss is 3.289577856063843 and perplexity is 26.83153449521362
At time: 117.77015089988708 and batch: 850, loss is 3.3459251308441162 and perplexity is 28.386825050144658
At time: 118.53332614898682 and batch: 900, loss is 3.3052827882766724 and perplexity is 27.256248246543702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38013155166417 and perplexity of 79.84853692210542
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.39137625694275 and batch: 50, loss is 3.612247052192688 and perplexity is 37.049210860098626
At time: 121.17114925384521 and batch: 100, loss is 3.4871425151824953 and perplexity is 32.69239608326076
At time: 121.93352580070496 and batch: 150, loss is 3.496979846954346 and perplexity is 33.01558910206313
At time: 122.70074701309204 and batch: 200, loss is 3.3765007781982423 and perplexity is 29.268175877105033
At time: 123.45876979827881 and batch: 250, loss is 3.513834285736084 and perplexity is 33.57676418854449
At time: 124.21751070022583 and batch: 300, loss is 3.486352925300598 and perplexity is 32.666592686489544
At time: 124.97523760795593 and batch: 350, loss is 3.4565254259109497 and perplexity is 31.70661790808223
At time: 125.743901014328 and batch: 400, loss is 3.394449009895325 and perplexity is 29.798230420784716
At time: 126.5030837059021 and batch: 450, loss is 3.395287733078003 and perplexity is 29.8232333712528
At time: 127.26106405258179 and batch: 500, loss is 3.2747178077697754 and perplexity is 26.43576446056055
At time: 128.0193576812744 and batch: 550, loss is 3.3259336042404173 and perplexity is 27.82496402449111
At time: 128.77693343162537 and batch: 600, loss is 3.344237642288208 and perplexity is 28.338963002420602
At time: 129.53478169441223 and batch: 650, loss is 3.1709474325180054 and perplexity is 23.8300510297919
At time: 130.29167556762695 and batch: 700, loss is 3.156399278640747 and perplexity is 23.485877396303895
At time: 131.0505940914154 and batch: 750, loss is 3.253804688453674 and perplexity is 25.88865102867459
At time: 131.80820870399475 and batch: 800, loss is 3.1836575984954836 and perplexity is 24.134867967020515
At time: 132.56644129753113 and batch: 850, loss is 3.237042226791382 and perplexity is 25.45831036910554
At time: 133.32404327392578 and batch: 900, loss is 3.1927202415466307 and perplexity is 24.354587777775865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376903377167166 and perplexity of 79.59118751946849
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.16056442260742 and batch: 50, loss is 3.5873302936553957 and perplexity is 36.13747059539551
At time: 135.9293110370636 and batch: 100, loss is 3.4617917203903197 and perplexity is 31.87403474117696
At time: 136.6856734752655 and batch: 150, loss is 3.4731005907058714 and perplexity is 32.236539965375165
At time: 137.44141626358032 and batch: 200, loss is 3.3519844388961793 and perplexity is 28.559351736049525
At time: 138.19611406326294 and batch: 250, loss is 3.4882128047943115 and perplexity is 32.72740514675058
At time: 138.95223236083984 and batch: 300, loss is 3.4636327028274536 and perplexity is 31.932768326497182
At time: 139.70729207992554 and batch: 350, loss is 3.4319049072265626 and perplexity is 30.935515945768614
At time: 140.46247339248657 and batch: 400, loss is 3.3706979656219485 and perplexity is 29.098829954558074
At time: 141.22508025169373 and batch: 450, loss is 3.370812087059021 and perplexity is 29.10215094434407
At time: 141.98086643218994 and batch: 500, loss is 3.250748553276062 and perplexity is 25.809652588008543
At time: 142.74328780174255 and batch: 550, loss is 3.2993598556518555 and perplexity is 27.09528847207784
At time: 143.49851608276367 and batch: 600, loss is 3.3198519849777224 and perplexity is 27.656256714095335
At time: 144.2622447013855 and batch: 650, loss is 3.144567861557007 and perplexity is 23.209643526629545
At time: 145.0320258140564 and batch: 700, loss is 3.127752113342285 and perplexity is 22.822619176785825
At time: 145.78772687911987 and batch: 750, loss is 3.2234508991241455 and perplexity is 25.114638897170472
At time: 146.55876564979553 and batch: 800, loss is 3.150811152458191 and perplexity is 23.355001366363755
At time: 147.32262182235718 and batch: 850, loss is 3.2015458822250364 and perplexity is 24.57048392791681
At time: 148.08651494979858 and batch: 900, loss is 3.1608304452896117 and perplexity is 23.59017814928664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375352833369007 and perplexity of 79.46787352385343
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.93618655204773 and batch: 50, loss is 3.577759017944336 and perplexity is 35.793238896972326
At time: 150.70168089866638 and batch: 100, loss is 3.4512465143203737 and perplexity is 31.539682481652346
At time: 151.45837211608887 and batch: 150, loss is 3.4627690362930297 and perplexity is 31.905200969357566
At time: 152.21509075164795 and batch: 200, loss is 3.3428422164916993 and perplexity is 28.29944566057222
At time: 152.97329664230347 and batch: 250, loss is 3.4790808153152466 and perplexity is 32.42989930487726
At time: 153.74345993995667 and batch: 300, loss is 3.454460401535034 and perplexity is 31.641210526386182
At time: 154.50118565559387 and batch: 350, loss is 3.4231572008132933 and perplexity is 30.666081320178893
At time: 155.26523733139038 and batch: 400, loss is 3.362275629043579 and perplexity is 28.854778996957478
At time: 156.02407789230347 and batch: 450, loss is 3.3625588846206664 and perplexity is 28.862953431706455
At time: 156.78100872039795 and batch: 500, loss is 3.2421420335769655 and perplexity is 25.588474456815106
At time: 157.53718972206116 and batch: 550, loss is 3.2902731132507324 and perplexity is 26.850195798857715
At time: 158.29204154014587 and batch: 600, loss is 3.311641812324524 and perplexity is 27.43012363757522
At time: 159.04669499397278 and batch: 650, loss is 3.1363478755950926 and perplexity is 23.019642555152373
At time: 159.80275225639343 and batch: 700, loss is 3.1195413875579834 and perplexity is 22.635996112894485
At time: 160.56154584884644 and batch: 750, loss is 3.2153331518173216 and perplexity is 24.911589870286164
At time: 161.31907558441162 and batch: 800, loss is 3.1415202951431276 and perplexity is 23.139018268783513
At time: 162.07586789131165 and batch: 850, loss is 3.191088662147522 and perplexity is 24.314883733042432
At time: 162.83306646347046 and batch: 900, loss is 3.1509808254241944 and perplexity is 23.358964414918194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375514200288955 and perplexity of 79.48069804453766
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 164.6762354373932 and batch: 50, loss is 3.575262231826782 and perplexity is 35.70398230866572
At time: 165.44646072387695 and batch: 100, loss is 3.4486865425109863 and perplexity is 31.45904504244568
At time: 166.2039496898651 and batch: 150, loss is 3.4599316358566283 and perplexity is 31.8148014486727
At time: 166.9611451625824 and batch: 200, loss is 3.340106420516968 and perplexity is 28.222129959262066
At time: 167.7183575630188 and batch: 250, loss is 3.47653124332428 and perplexity is 32.34732225473815
At time: 168.4754674434662 and batch: 300, loss is 3.451893439292908 and perplexity is 31.560092891165503
At time: 169.23316955566406 and batch: 350, loss is 3.4204619312286377 and perplexity is 30.58353925044979
At time: 169.99150276184082 and batch: 400, loss is 3.359633708000183 and perplexity is 28.77864756002703
At time: 170.74915409088135 and batch: 450, loss is 3.360275511741638 and perplexity is 28.797123732109043
At time: 171.50610613822937 and batch: 500, loss is 3.239693512916565 and perplexity is 25.525897190567985
At time: 172.26251101493835 and batch: 550, loss is 3.2875525760650635 and perplexity is 26.77724811617854
At time: 173.02020812034607 and batch: 600, loss is 3.30932731628418 and perplexity is 27.36671013848445
At time: 173.7784914970398 and batch: 650, loss is 3.13398202419281 and perplexity is 22.96524587413791
At time: 174.53595185279846 and batch: 700, loss is 3.1173093795776365 and perplexity is 22.585528731678
At time: 175.2938196659088 and batch: 750, loss is 3.213209562301636 and perplexity is 24.858744010535037
At time: 176.0501036643982 and batch: 800, loss is 3.139019503593445 and perplexity is 23.08122470238011
At time: 176.81989550590515 and batch: 850, loss is 3.18850209236145 and perplexity is 24.252072856802915
At time: 177.5860140323639 and batch: 900, loss is 3.148180966377258 and perplexity is 23.293654079605734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375353251418022 and perplexity of 79.46790674532662
Annealing...
Model not improving. Stopping early with 78.32989306866857 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc03c314b70>
ELAPSED
545.355288028717


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -77.71701100305113}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.636192611183658, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.9359653836036915, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.32989306866857}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'dropout': 0.17658611098779187, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.18394514580407484, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0452189445495605 and batch: 50, loss is 6.749844217300415 and perplexity is 853.9257253092388
At time: 1.8710885047912598 and batch: 100, loss is 5.8301067066192624 and perplexity is 340.3949995335115
At time: 2.704387903213501 and batch: 150, loss is 5.563799343109131 and perplexity is 260.8118700752895
At time: 3.527665376663208 and batch: 200, loss is 5.327299737930298 and perplexity is 205.88128946048388
At time: 4.352461099624634 and batch: 250, loss is 5.342655944824219 and perplexity is 209.0672446230938
At time: 5.176379442214966 and batch: 300, loss is 5.2496474075317385 and perplexity is 190.49908807201194
At time: 6.001283168792725 and batch: 350, loss is 5.196602849960327 and perplexity is 180.6574776857989
At time: 6.82692813873291 and batch: 400, loss is 5.03037049293518 and perplexity is 152.98968379384678
At time: 7.653397560119629 and batch: 450, loss is 5.031196031570435 and perplexity is 153.1160348352713
At time: 8.47755742073059 and batch: 500, loss is 4.950140810012817 and perplexity is 141.19484416959222
At time: 9.30203366279602 and batch: 550, loss is 5.008131313323974 and perplexity is 149.62487273588752
At time: 10.127447843551636 and batch: 600, loss is 4.930212621688843 and perplexity is 138.4089379537266
At time: 10.951688528060913 and batch: 650, loss is 4.801214771270752 and perplexity is 121.65811457394078
At time: 11.781921625137329 and batch: 700, loss is 4.874056701660156 and perplexity is 130.8506637623851
At time: 12.614732027053833 and batch: 750, loss is 4.88044361114502 and perplexity is 131.6890696691267
At time: 13.444199800491333 and batch: 800, loss is 4.814133205413818 and perplexity is 123.2399422849162
At time: 14.276924133300781 and batch: 850, loss is 4.867237129211426 and perplexity is 129.96135398368517
At time: 15.10204291343689 and batch: 900, loss is 4.794144096374512 and perplexity is 120.80094355723975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.786580124946489 and perplexity of 119.89065570740941
finished 1 epochs...
Completing Train Step...
At time: 17.014612674713135 and batch: 50, loss is 4.7784995841979985 and perplexity is 118.92577799558254
At time: 17.770954608917236 and batch: 100, loss is 4.628107118606567 and perplexity is 102.32020068409427
At time: 18.522706747055054 and batch: 150, loss is 4.605930423736572 and perplexity is 100.07605268024429
At time: 19.277093410491943 and batch: 200, loss is 4.488332939147949 and perplexity is 88.972998774713
At time: 20.032033443450928 and batch: 250, loss is 4.6179664421081545 and perplexity is 101.2878478544171
At time: 20.78417658805847 and batch: 300, loss is 4.569488563537598 and perplexity is 96.49474621911328
At time: 21.536123514175415 and batch: 350, loss is 4.551153869628906 and perplexity is 94.74166479853585
At time: 22.29651427268982 and batch: 400, loss is 4.446380758285523 and perplexity is 85.31759952846923
At time: 23.052862644195557 and batch: 450, loss is 4.46559549331665 and perplexity is 86.97280584661196
At time: 23.81146788597107 and batch: 500, loss is 4.352622833251953 and perplexity is 77.68194274430007
At time: 24.581916332244873 and batch: 550, loss is 4.435171394348145 and perplexity is 84.36658360478624
At time: 25.34302306175232 and batch: 600, loss is 4.416810035705566 and perplexity is 82.83163351443875
At time: 26.09628915786743 and batch: 650, loss is 4.265951552391052 and perplexity is 71.23266935158412
At time: 26.86358666419983 and batch: 700, loss is 4.294973564147949 and perplexity is 73.33027587107647
At time: 27.62210178375244 and batch: 750, loss is 4.375591230392456 and perplexity is 79.48682068674523
At time: 28.38063621520996 and batch: 800, loss is 4.308675427436828 and perplexity is 74.34195238492167
At time: 29.139671564102173 and batch: 850, loss is 4.378854846954345 and perplexity is 79.74665896681526
At time: 29.897757291793823 and batch: 900, loss is 4.328452229499817 and perplexity is 75.82683316168527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.496209967626284 and perplexity of 89.67660916128436
finished 2 epochs...
Completing Train Step...
At time: 31.79410743713379 and batch: 50, loss is 4.379284934997559 and perplexity is 79.78096442797863
At time: 32.57334661483765 and batch: 100, loss is 4.231522765159607 and perplexity is 68.82195209255177
At time: 33.33255696296692 and batch: 150, loss is 4.223662691116333 and perplexity is 68.28312683061279
At time: 34.09137225151062 and batch: 200, loss is 4.112792601585388 and perplexity is 61.11715534136216
At time: 34.85072374343872 and batch: 250, loss is 4.27007640838623 and perplexity is 71.52710068076304
At time: 35.616174936294556 and batch: 300, loss is 4.227496356964111 and perplexity is 68.54540394210161
At time: 36.37768578529358 and batch: 350, loss is 4.220007605552674 and perplexity is 68.03400172363568
At time: 37.13642907142639 and batch: 400, loss is 4.136115450859069 and perplexity is 62.55933402869795
At time: 37.89492607116699 and batch: 450, loss is 4.161023178100586 and perplexity is 64.13711272820136
At time: 38.653172969818115 and batch: 500, loss is 4.041937479972839 and perplexity is 56.936549433677094
At time: 39.41168403625488 and batch: 550, loss is 4.124735889434814 and perplexity is 61.85147146588567
At time: 40.17067909240723 and batch: 600, loss is 4.132023782730102 and perplexity is 62.30388495828188
At time: 40.9321231842041 and batch: 650, loss is 3.9761688995361326 and perplexity is 53.31239733537711
At time: 41.6914279460907 and batch: 700, loss is 3.9917033910751343 and perplexity is 54.14704444522482
At time: 42.45976734161377 and batch: 750, loss is 4.097246432304383 and perplexity is 60.17436507497566
At time: 43.21408200263977 and batch: 800, loss is 4.037479643821716 and perplexity is 56.68330051576866
At time: 43.969257831573486 and batch: 850, loss is 4.112283744812012 and perplexity is 61.086063374247125
At time: 44.72346830368042 and batch: 900, loss is 4.0730063915252686 and perplexity is 58.733272643700325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.407345288420377 and perplexity of 82.05135144718466
finished 3 epochs...
Completing Train Step...
At time: 46.5646858215332 and batch: 50, loss is 4.1372949981689455 and perplexity is 62.63316926037281
At time: 47.3326461315155 and batch: 100, loss is 3.995416440963745 and perplexity is 54.3484688405697
At time: 48.088024616241455 and batch: 150, loss is 3.9969352436065675 and perplexity is 54.43107615490216
At time: 48.84347629547119 and batch: 200, loss is 3.8831073570251466 and perplexity is 48.574920422636986
At time: 49.599029302597046 and batch: 250, loss is 4.044199028015137 and perplexity is 57.06545988923443
At time: 50.35527515411377 and batch: 300, loss is 4.005910153388977 and perplexity is 54.92178891024171
At time: 51.11166477203369 and batch: 350, loss is 4.004014463424682 and perplexity is 54.81777284843539
At time: 51.86946702003479 and batch: 400, loss is 3.9297772741317747 and perplexity is 50.895640630974036
At time: 52.63371181488037 and batch: 450, loss is 3.956423268318176 and perplexity is 52.270035313276104
At time: 53.39045310020447 and batch: 500, loss is 3.837460741996765 and perplexity is 46.40748412378924
At time: 54.14869427680969 and batch: 550, loss is 3.921457076072693 and perplexity is 50.473935588052896
At time: 54.90596151351929 and batch: 600, loss is 3.9362158203125 and perplexity is 51.22439176756758
At time: 55.6625599861145 and batch: 650, loss is 3.781768593788147 and perplexity is 43.89360308184914
At time: 56.421095848083496 and batch: 700, loss is 3.7882556533813476 and perplexity is 44.179269065149185
At time: 57.17894673347473 and batch: 750, loss is 3.9030616331100463 and perplexity is 49.55393305523796
At time: 57.94638633728027 and batch: 800, loss is 3.8461228656768798 and perplexity is 46.81121756101553
At time: 58.70494341850281 and batch: 850, loss is 3.9239662504196167 and perplexity is 50.60074251623343
At time: 59.47212553024292 and batch: 900, loss is 3.890079851150513 and perplexity is 48.91479227023051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378349408711473 and perplexity of 79.7063621402686
finished 4 epochs...
Completing Train Step...
At time: 61.34549307823181 and batch: 50, loss is 3.95985463142395 and perplexity is 52.44970085656029
At time: 62.11269998550415 and batch: 100, loss is 3.818899312019348 and perplexity is 45.55403993181922
At time: 62.8688690662384 and batch: 150, loss is 3.824938735961914 and perplexity is 45.8299925499162
At time: 63.62394952774048 and batch: 200, loss is 3.7136274099349977 and perplexity is 41.00226913524835
At time: 64.37907433509827 and batch: 250, loss is 3.8729844856262208 and perplexity is 48.08568317065253
At time: 65.14433455467224 and batch: 300, loss is 3.837473850250244 and perplexity is 46.40809244884149
At time: 65.90114116668701 and batch: 350, loss is 3.839505505561829 and perplexity is 46.50247353882295
At time: 66.65726399421692 and batch: 400, loss is 3.769735727310181 and perplexity is 43.36860218413253
At time: 67.41367721557617 and batch: 450, loss is 3.796208453178406 and perplexity is 44.53201876186749
At time: 68.1695806980133 and batch: 500, loss is 3.683982729911804 and perplexity is 39.804609808275565
At time: 68.92609405517578 and batch: 550, loss is 3.7661683702468873 and perplexity is 43.214166522160035
At time: 69.68122386932373 and batch: 600, loss is 3.783033561706543 and perplexity is 43.94916221442208
At time: 70.4375410079956 and batch: 650, loss is 3.6283176803588866 and perplexity is 37.64942494254554
At time: 71.19407606124878 and batch: 700, loss is 3.631888771057129 and perplexity is 37.78411480547223
At time: 71.95784044265747 and batch: 750, loss is 3.7526691579818725 and perplexity is 42.63472908902611
At time: 72.71437954902649 and batch: 800, loss is 3.697246260643005 and perplexity is 40.33607624307604
At time: 73.4773817062378 and batch: 850, loss is 3.770223422050476 and perplexity is 43.38975798167796
At time: 74.24112391471863 and batch: 900, loss is 3.7405762338638304 and perplexity is 42.12225544309522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379651631394478 and perplexity of 79.81022518475628
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.09224915504456 and batch: 50, loss is 3.834538712501526 and perplexity is 46.27207801304873
At time: 76.86230850219727 and batch: 100, loss is 3.6946689605712892 and perplexity is 40.232251921568526
At time: 77.61961126327515 and batch: 150, loss is 3.7047076416015625 and perplexity is 40.63816467122906
At time: 78.37687921524048 and batch: 200, loss is 3.5750055503845215 and perplexity is 35.69481893507657
At time: 79.13375210762024 and batch: 250, loss is 3.7306833600997926 and perplexity is 41.707599737437
At time: 79.90213131904602 and batch: 300, loss is 3.678172631263733 and perplexity is 39.57401164637207
At time: 80.6679630279541 and batch: 350, loss is 3.665282120704651 and perplexity is 39.0671562621806
At time: 81.42418813705444 and batch: 400, loss is 3.5891474962234495 and perplexity is 36.203199402955775
At time: 82.19243478775024 and batch: 450, loss is 3.602159662246704 and perplexity is 36.67735968004062
At time: 82.95696091651917 and batch: 500, loss is 3.477905774116516 and perplexity is 32.39181521669162
At time: 83.72377824783325 and batch: 550, loss is 3.537845711708069 and perplexity is 34.3927474327463
At time: 84.4856367111206 and batch: 600, loss is 3.545684604644775 and perplexity is 34.663407951117286
At time: 85.24297738075256 and batch: 650, loss is 3.3707658195495607 and perplexity is 29.100804491448695
At time: 86.00035810470581 and batch: 700, loss is 3.35597017288208 and perplexity is 28.673408864572625
At time: 86.75671887397766 and batch: 750, loss is 3.455728120803833 and perplexity is 31.68134813489298
At time: 87.51382756233215 and batch: 800, loss is 3.381641192436218 and perplexity is 29.419013777605834
At time: 88.27101564407349 and batch: 850, loss is 3.4326910734176637 and perplexity is 30.959845964975596
At time: 89.02841544151306 and batch: 900, loss is 3.3872746419906616 and perplexity is 29.585212002786985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3426751959813785 and perplexity of 76.91302176961456
finished 6 epochs...
Completing Train Step...
At time: 90.86242389678955 and batch: 50, loss is 3.707741184234619 and perplexity is 40.76162944944179
At time: 91.63197827339172 and batch: 100, loss is 3.567089738845825 and perplexity is 35.4133808504734
At time: 92.39207172393799 and batch: 150, loss is 3.576282649040222 and perplexity is 35.740433861534896
At time: 93.15509915351868 and batch: 200, loss is 3.451526575088501 and perplexity is 31.54851674636239
At time: 93.92469215393066 and batch: 250, loss is 3.607629098892212 and perplexity is 36.878513773464356
At time: 94.68570065498352 and batch: 300, loss is 3.5634554624557495 and perplexity is 35.284912422842986
At time: 95.44451093673706 and batch: 350, loss is 3.5532618045806883 and perplexity is 34.927057123441486
At time: 96.20483565330505 and batch: 400, loss is 3.4844082260131835 and perplexity is 32.603127717111505
At time: 96.96550369262695 and batch: 450, loss is 3.503175950050354 and perplexity is 33.22079216908706
At time: 97.72748708724976 and batch: 500, loss is 3.3842948055267335 and perplexity is 29.4971841287389
At time: 98.48878121376038 and batch: 550, loss is 3.448304467201233 and perplexity is 31.44702761399409
At time: 99.27722787857056 and batch: 600, loss is 3.4625410270690917 and perplexity is 31.89792711852901
At time: 100.04437279701233 and batch: 650, loss is 3.2956980991363527 and perplexity is 26.996253554511902
At time: 100.80787777900696 and batch: 700, loss is 3.285624966621399 and perplexity is 26.725681755703157
At time: 101.57096290588379 and batch: 750, loss is 3.3928375482559203 and perplexity is 29.75025038491603
At time: 102.32626295089722 and batch: 800, loss is 3.326597819328308 and perplexity is 27.843451924707615
At time: 103.08395147323608 and batch: 850, loss is 3.3882331609725953 and perplexity is 29.61358358525414
At time: 103.84782314300537 and batch: 900, loss is 3.3519052362442014 and perplexity is 28.557089849228163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358626274213399 and perplexity of 78.14970438614222
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 105.71336388587952 and batch: 50, loss is 3.6493022108078 and perplexity is 38.44782820755644
At time: 106.47046208381653 and batch: 100, loss is 3.520063028335571 and perplexity is 33.78655790725079
At time: 107.22845768928528 and batch: 150, loss is 3.5339763498306276 and perplexity is 34.25992657865991
At time: 107.9930534362793 and batch: 200, loss is 3.4053748798370362 and perplexity is 30.125587082510314
At time: 108.75048446655273 and batch: 250, loss is 3.558969340324402 and perplexity is 35.12697452554719
At time: 109.50704431533813 and batch: 300, loss is 3.5140577077865602 and perplexity is 33.584266816142865
At time: 110.2650420665741 and batch: 350, loss is 3.4989842653274534 and perplexity is 33.081832523044525
At time: 111.02280330657959 and batch: 400, loss is 3.4288931274414063 and perplexity is 30.842485148658938
At time: 111.78057503700256 and batch: 450, loss is 3.4427711582183838 and perplexity is 31.273502021716133
At time: 112.55331897735596 and batch: 500, loss is 3.3139308309555053 and perplexity is 27.49298361801815
At time: 113.3103711605072 and batch: 550, loss is 3.370581979751587 and perplexity is 29.095455097160965
At time: 114.06716823577881 and batch: 600, loss is 3.3868419218063353 and perplexity is 29.572412653863836
At time: 114.8247938156128 and batch: 650, loss is 3.2113212823867796 and perplexity is 24.811848033811216
At time: 115.5834698677063 and batch: 700, loss is 3.191200685501099 and perplexity is 24.317607720432292
At time: 116.34084463119507 and batch: 750, loss is 3.293198275566101 and perplexity is 26.928851964709306
At time: 117.09825277328491 and batch: 800, loss is 3.218753457069397 and perplexity is 24.99694099219575
At time: 117.8657066822052 and batch: 850, loss is 3.27380241394043 and perplexity is 26.411576397390228
At time: 118.62194800376892 and batch: 900, loss is 3.2393753862380983 and perplexity is 25.5177780132118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350370642257063 and perplexity of 77.50718504011921
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.46894001960754 and batch: 50, loss is 3.6220298910140993 and perplexity is 37.41343599147068
At time: 121.23750925064087 and batch: 100, loss is 3.4918880605697633 and perplexity is 32.847908035359296
At time: 121.99202919006348 and batch: 150, loss is 3.5070547437667847 and perplexity is 33.34989899651477
At time: 122.74611735343933 and batch: 200, loss is 3.379023690223694 and perplexity is 29.34211013558684
At time: 123.50142478942871 and batch: 250, loss is 3.531167631149292 and perplexity is 34.16383509300402
At time: 124.25534105300903 and batch: 300, loss is 3.4892154932022095 and perplexity is 32.760236993817315
At time: 125.00904273986816 and batch: 350, loss is 3.472948441505432 and perplexity is 32.23163557470352
At time: 125.76220107078552 and batch: 400, loss is 3.4051565599441527 and perplexity is 30.11901078545976
At time: 126.51646947860718 and batch: 450, loss is 3.4200427532196045 and perplexity is 30.57072198990215
At time: 127.26799201965332 and batch: 500, loss is 3.2884978342056272 and perplexity is 26.8025714946215
At time: 128.02133631706238 and batch: 550, loss is 3.341691298484802 and perplexity is 28.266894054684133
At time: 128.77632546424866 and batch: 600, loss is 3.358921933174133 and perplexity is 28.758170931404923
At time: 129.53114867210388 and batch: 650, loss is 3.184377307891846 and perplexity is 24.152244310485123
At time: 130.28596568107605 and batch: 700, loss is 3.1608490562438964 and perplexity is 23.59061718909921
At time: 131.03927373886108 and batch: 750, loss is 3.2625609922409056 and perplexity is 26.116335303296257
At time: 131.79355335235596 and batch: 800, loss is 3.187472252845764 and perplexity is 24.227109969927582
At time: 132.5504434108734 and batch: 850, loss is 3.2371295833587648 and perplexity is 25.460534416851928
At time: 133.3054051399231 and batch: 900, loss is 3.206296544075012 and perplexity is 24.687487691153084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348152892230308 and perplexity of 77.33548394385295
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.16761922836304 and batch: 50, loss is 3.612667317390442 and perplexity is 37.06478462635413
At time: 135.93695306777954 and batch: 100, loss is 3.481354537010193 and perplexity is 32.5037197622763
At time: 136.69089913368225 and batch: 150, loss is 3.4972191715240477 and perplexity is 33.023491489298436
At time: 137.45659136772156 and batch: 200, loss is 3.369696702957153 and perplexity is 29.06970896384861
At time: 138.21178936958313 and batch: 250, loss is 3.520723195075989 and perplexity is 33.80887003310571
At time: 138.96646118164062 and batch: 300, loss is 3.478964715003967 and perplexity is 32.42613440203063
At time: 139.72162008285522 and batch: 350, loss is 3.4629612827301024 and perplexity is 31.911335220193052
At time: 140.47694063186646 and batch: 400, loss is 3.3956817483901975 and perplexity is 29.83498649716396
At time: 141.23207426071167 and batch: 450, loss is 3.411217999458313 and perplexity is 30.302129769219196
At time: 141.9875636100769 and batch: 500, loss is 3.2802054929733275 and perplexity is 26.581234395575358
At time: 142.74175190925598 and batch: 550, loss is 3.3327826166152956 and perplexity is 28.016191660123116
At time: 143.49690556526184 and batch: 600, loss is 3.3500392770767213 and perplexity is 28.503853169809084
At time: 144.251526594162 and batch: 650, loss is 3.1759674739837647 and perplexity is 23.949979645546478
At time: 145.0052194595337 and batch: 700, loss is 3.1524303913116456 and perplexity is 23.392849326172605
At time: 145.75930738449097 and batch: 750, loss is 3.2535065174102784 and perplexity is 25.88093293329872
At time: 146.51376271247864 and batch: 800, loss is 3.1787298107147217 and perplexity is 24.016229013431865
At time: 147.2683970928192 and batch: 850, loss is 3.227008037567139 and perplexity is 25.204134223940063
At time: 148.02342343330383 and batch: 900, loss is 3.1957007217407227 and perplexity is 24.427284425927525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348422533845248 and perplexity of 77.35633962028852
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.88470768928528 and batch: 50, loss is 3.609870057106018 and perplexity is 36.96124965101592
At time: 150.64307570457458 and batch: 100, loss is 3.4787347412109373 and perplexity is 32.41867809831898
At time: 151.40946435928345 and batch: 150, loss is 3.494665575027466 and perplexity is 32.93927039623668
At time: 152.16815519332886 and batch: 200, loss is 3.367358407974243 and perplexity is 29.001814818428553
At time: 152.926522731781 and batch: 250, loss is 3.5182583141326904 and perplexity is 33.72563781454209
At time: 153.68433809280396 and batch: 300, loss is 3.476619300842285 and perplexity is 32.35017080506632
At time: 154.441748380661 and batch: 350, loss is 3.459883189201355 and perplexity is 31.81326016528964
At time: 155.20525741577148 and batch: 400, loss is 3.3926897621154786 and perplexity is 29.74585403510225
At time: 155.97711157798767 and batch: 450, loss is 3.4088950729370118 and perplexity is 30.231821840018117
At time: 156.73446774482727 and batch: 500, loss is 3.277977647781372 and perplexity is 26.522081436614666
At time: 157.49302673339844 and batch: 550, loss is 3.330282292366028 and perplexity is 27.946229597112538
At time: 158.2503502368927 and batch: 600, loss is 3.3476254320144654 and perplexity is 28.43513225876348
At time: 159.01403403282166 and batch: 650, loss is 3.1734371900558473 and perplexity is 23.889456000269917
At time: 159.7878167629242 and batch: 700, loss is 3.150044960975647 and perplexity is 23.33711381676119
At time: 160.54828548431396 and batch: 750, loss is 3.251106128692627 and perplexity is 25.818883135494172
At time: 161.30542135238647 and batch: 800, loss is 3.1763530111312868 and perplexity is 23.959215032560223
At time: 162.06272888183594 and batch: 850, loss is 3.224428720474243 and perplexity is 25.139208537685988
At time: 162.82013845443726 and batch: 900, loss is 3.1927515268325806 and perplexity is 24.35534972993757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34821559958262 and perplexity of 77.34033359934374
Annealing...
Model not improving. Stopping early with 76.91302176961456 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc03c314b70>
ELAPSED
714.9931483268738


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -77.71701100305113}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.636192611183658, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.9359653836036915, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.32989306866857}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.17658611098779187, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.18394514580407484, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.91302176961456}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'dropout': 0.040248218725259965, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.466024297238987, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0431206226348877 and batch: 50, loss is 6.710178546905517 and perplexity is 820.7171628793237
At time: 1.8868954181671143 and batch: 100, loss is 5.848228950500488 and perplexity is 346.61995545214353
At time: 2.715174674987793 and batch: 150, loss is 5.5591318225860595 and perplexity is 259.5973618948945
At time: 3.551560878753662 and batch: 200, loss is 5.325392475128174 and perplexity is 205.48899395967015
At time: 4.382471799850464 and batch: 250, loss is 5.330316762924195 and perplexity is 206.50337641062464
At time: 5.215348958969116 and batch: 300, loss is 5.233078994750977 and perplexity is 187.36882390853697
At time: 6.045256614685059 and batch: 350, loss is 5.175006542205811 and perplexity is 176.79777092005307
At time: 6.876716136932373 and batch: 400, loss is 5.019221019744873 and perplexity is 151.29340330503263
At time: 7.703642129898071 and batch: 450, loss is 5.012482242584229 and perplexity is 150.2772982721811
At time: 8.529256105422974 and batch: 500, loss is 4.933013334274292 and perplexity is 138.797124954557
At time: 9.354457378387451 and batch: 550, loss is 4.986789531707764 and perplexity is 146.46544520206155
At time: 10.192116737365723 and batch: 600, loss is 4.907489137649536 and perplexity is 135.29926971653987
At time: 11.023607015609741 and batch: 650, loss is 4.7833443450927735 and perplexity is 119.50334290644092
At time: 11.848217964172363 and batch: 700, loss is 4.852587661743164 and perplexity is 128.07136680941835
At time: 12.670493602752686 and batch: 750, loss is 4.8636412525177 and perplexity is 129.49486819514883
At time: 13.49494194984436 and batch: 800, loss is 4.79780083656311 and perplexity is 121.24348986767065
At time: 14.318790912628174 and batch: 850, loss is 4.8472629642486575 and perplexity is 127.39123787081796
At time: 15.152671098709106 and batch: 900, loss is 4.772585563659668 and perplexity is 118.22452416066972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.78138210348887 and perplexity of 119.26907839213997
finished 1 epochs...
Completing Train Step...
At time: 17.04236388206482 and batch: 50, loss is 4.771483383178711 and perplexity is 118.0942911810774
At time: 17.796694040298462 and batch: 100, loss is 4.625500040054321 and perplexity is 102.05379130940827
At time: 18.55334234237671 and batch: 150, loss is 4.599351625442505 and perplexity is 99.41983344936732
At time: 19.309959173202515 and batch: 200, loss is 4.487485694885254 and perplexity is 88.8976488363751
At time: 20.065943479537964 and batch: 250, loss is 4.613597898483277 and perplexity is 100.84633256396143
At time: 20.822272300720215 and batch: 300, loss is 4.570252733230591 and perplexity is 96.56851276119521
At time: 21.578835010528564 and batch: 350, loss is 4.551964187622071 and perplexity is 94.81846678703863
At time: 22.34427523612976 and batch: 400, loss is 4.446570682525635 and perplexity is 85.33380494758009
At time: 23.101805686950684 and batch: 450, loss is 4.466356859207154 and perplexity is 87.03904918889562
At time: 23.859308004379272 and batch: 500, loss is 4.351651315689087 and perplexity is 77.60651002064742
At time: 24.616665363311768 and batch: 550, loss is 4.434468927383423 and perplexity is 84.30733967776447
At time: 25.373944997787476 and batch: 600, loss is 4.412473430633545 and perplexity is 82.47320317954454
At time: 26.13189387321472 and batch: 650, loss is 4.26428596496582 and perplexity is 71.1141238645882
At time: 26.888362169265747 and batch: 700, loss is 4.295418791770935 and perplexity is 73.3629318046178
At time: 27.64528203010559 and batch: 750, loss is 4.372086858749389 and perplexity is 79.20875683044588
At time: 28.40248394012451 and batch: 800, loss is 4.3111699676513675 and perplexity is 74.52763287220789
At time: 29.15913677215576 and batch: 850, loss is 4.38411955833435 and perplexity is 80.16760922831537
At time: 29.915644884109497 and batch: 900, loss is 4.328184776306152 and perplexity is 75.80655574474153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493304526969178 and perplexity of 89.41643723510373
finished 2 epochs...
Completing Train Step...
At time: 31.767256259918213 and batch: 50, loss is 4.381963186264038 and perplexity is 79.99492428823712
At time: 32.537957191467285 and batch: 100, loss is 4.235379190444946 and perplexity is 69.0878712278638
At time: 33.29497838020325 and batch: 150, loss is 4.223106637001037 and perplexity is 68.24516827141794
At time: 34.063291788101196 and batch: 200, loss is 4.119366974830627 and perplexity is 61.520286046101845
At time: 34.83411192893982 and batch: 250, loss is 4.269656801223755 and perplexity is 71.49709369302009
At time: 35.60099220275879 and batch: 300, loss is 4.240072031021118 and perplexity is 69.41285153726126
At time: 36.361544132232666 and batch: 350, loss is 4.224930768013 and perplexity is 68.36977000967538
At time: 37.13015031814575 and batch: 400, loss is 4.142461824417114 and perplexity is 62.95762143552056
At time: 37.88752722740173 and batch: 450, loss is 4.166391220092773 and perplexity is 64.48232918181792
At time: 38.64539957046509 and batch: 500, loss is 4.044028558731079 and perplexity is 57.05573281025035
At time: 39.40309000015259 and batch: 550, loss is 4.128785281181336 and perplexity is 62.102440096160265
At time: 40.15962624549866 and batch: 600, loss is 4.131944174766541 and perplexity is 62.298925270296316
At time: 40.91586351394653 and batch: 650, loss is 3.978056802749634 and perplexity is 53.4131410488939
At time: 41.67164158821106 and batch: 700, loss is 3.996304259300232 and perplexity is 54.39674183341962
At time: 42.434314489364624 and batch: 750, loss is 4.097937154769897 and perplexity is 60.215943218606895
At time: 43.19148921966553 and batch: 800, loss is 4.046423292160034 and perplexity is 57.192529811726075
At time: 43.947813510894775 and batch: 850, loss is 4.120838017463684 and perplexity is 61.610851606241795
At time: 44.703909158706665 and batch: 900, loss is 4.075896215438843 and perplexity is 58.90324693911126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4004930470087755 and perplexity of 81.49103767391667
finished 3 epochs...
Completing Train Step...
At time: 46.54788637161255 and batch: 50, loss is 4.147002248764038 and perplexity is 63.24412568594682
At time: 47.317110776901245 and batch: 100, loss is 3.999485020637512 and perplexity is 54.57004035122142
At time: 48.07427740097046 and batch: 150, loss is 3.994396834373474 and perplexity is 54.29308302423862
At time: 48.83014273643494 and batch: 200, loss is 3.8973289489746095 and perplexity is 49.270668717609254
At time: 49.586310625076294 and batch: 250, loss is 4.047748894691467 and perplexity is 57.26839464622262
At time: 50.34317922592163 and batch: 300, loss is 4.024368839263916 and perplexity is 55.944987356668165
At time: 51.11124563217163 and batch: 350, loss is 4.009813370704651 and perplexity is 55.136579502206494
At time: 51.869011640548706 and batch: 400, loss is 3.935783796310425 and perplexity is 51.202266380525565
At time: 52.62581419944763 and batch: 450, loss is 3.9632140827178954 and perplexity is 52.62619937506255
At time: 53.395493030548096 and batch: 500, loss is 3.8414680337905884 and perplexity is 46.593825566996735
At time: 54.15232801437378 and batch: 550, loss is 3.9232588624954223 and perplexity is 50.56496081928444
At time: 54.90941023826599 and batch: 600, loss is 3.9414136743545534 and perplexity is 51.49134186215154
At time: 55.666014671325684 and batch: 650, loss is 3.7858730506896974 and perplexity is 44.07413271856706
At time: 56.42460250854492 and batch: 700, loss is 3.7981013250350952 and perplexity is 44.61639199557097
At time: 57.18955445289612 and batch: 750, loss is 3.904829807281494 and perplexity is 49.641630549133275
At time: 57.95087170600891 and batch: 800, loss is 3.857294774055481 and perplexity is 47.33712039394706
At time: 58.7070894241333 and batch: 850, loss is 3.9296005535125733 and perplexity is 50.88664711654021
At time: 59.46402883529663 and batch: 900, loss is 3.893131022453308 and perplexity is 49.064267602145705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372982077402611 and perplexity of 79.27969773613225
finished 4 epochs...
Completing Train Step...
At time: 61.31756067276001 and batch: 50, loss is 3.9714984607696535 and perplexity is 53.06398559558805
At time: 62.07060790061951 and batch: 100, loss is 3.825761160850525 and perplexity is 45.86769977999097
At time: 62.8268723487854 and batch: 150, loss is 3.823376851081848 and perplexity is 45.75846724918224
At time: 63.58152413368225 and batch: 200, loss is 3.726358199119568 and perplexity is 41.52759720504764
At time: 64.33416962623596 and batch: 250, loss is 3.8770223474502563 and perplexity is 48.28023904553807
At time: 65.08813309669495 and batch: 300, loss is 3.858561897277832 and perplexity is 47.39714037680381
At time: 65.84859895706177 and batch: 350, loss is 3.8457910346984865 and perplexity is 46.7956867258411
At time: 66.60709166526794 and batch: 400, loss is 3.7764012908935545 and perplexity is 43.6586439314949
At time: 67.35958504676819 and batch: 450, loss is 3.805081000328064 and perplexity is 44.9288892203934
At time: 68.1263792514801 and batch: 500, loss is 3.686290435791016 and perplexity is 39.89657321178068
At time: 68.88710474967957 and batch: 550, loss is 3.7662587118148805 and perplexity is 43.21807073407682
At time: 69.64176201820374 and batch: 600, loss is 3.7920006036758425 and perplexity is 44.345028418388246
At time: 70.39661931991577 and batch: 650, loss is 3.6333469915390015 and perplexity is 37.83925256730737
At time: 71.15630412101746 and batch: 700, loss is 3.6416579818725587 and perplexity is 38.15504468452803
At time: 71.92440605163574 and batch: 750, loss is 3.7507047986984254 and perplexity is 42.55106136678039
At time: 72.6784176826477 and batch: 800, loss is 3.706799569129944 and perplexity is 40.72326574821586
At time: 73.4317033290863 and batch: 850, loss is 3.779715394973755 and perplexity is 43.80357324424767
At time: 74.18618702888489 and batch: 900, loss is 3.7439494276046754 and perplexity is 42.26458188388142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375580252033391 and perplexity of 79.4859480566768
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.027658700943 and batch: 50, loss is 3.8417724847793577 and perplexity is 46.60801326288058
At time: 76.80539584159851 and batch: 100, loss is 3.7017374229431153 and perplexity is 40.51763951791518
At time: 77.5644109249115 and batch: 150, loss is 3.7061574554443357 and perplexity is 40.6971251754642
At time: 78.3201310634613 and batch: 200, loss is 3.58649911403656 and perplexity is 36.107446345861035
At time: 79.07533812522888 and batch: 250, loss is 3.729744176864624 and perplexity is 41.66844704763735
At time: 79.83068037033081 and batch: 300, loss is 3.7019629955291746 and perplexity is 40.526780217549124
At time: 80.58486366271973 and batch: 350, loss is 3.6713057708740235 and perplexity is 39.30319333328375
At time: 81.33935379981995 and batch: 400, loss is 3.596537957191467 and perplexity is 36.4717488648967
At time: 82.102534532547 and batch: 450, loss is 3.6111127948760986 and perplexity is 37.00721134523805
At time: 82.8573215007782 and batch: 500, loss is 3.4808654403686523 and perplexity is 32.48782618916644
At time: 83.61117219924927 and batch: 550, loss is 3.539875898361206 and perplexity is 34.4626420551069
At time: 84.36767864227295 and batch: 600, loss is 3.551028890609741 and perplexity is 34.84915501634584
At time: 85.12303948402405 and batch: 650, loss is 3.3773142766952513 and perplexity is 29.291995181361763
At time: 85.87840390205383 and batch: 700, loss is 3.366922903060913 and perplexity is 28.9891871354882
At time: 86.63292193412781 and batch: 750, loss is 3.458070993423462 and perplexity is 31.75566051622087
At time: 87.39609217643738 and batch: 800, loss is 3.39246169090271 and perplexity is 29.739070635676104
At time: 88.15692210197449 and batch: 850, loss is 3.44376202583313 and perplexity is 31.304505279594608
At time: 88.91634464263916 and batch: 900, loss is 3.3940213775634764 and perplexity is 29.785490458229013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342100796634203 and perplexity of 76.86885566582573
finished 6 epochs...
Completing Train Step...
At time: 90.76832342147827 and batch: 50, loss is 3.7178102207183836 and perplexity is 41.17413305528338
At time: 91.53862357139587 and batch: 100, loss is 3.5745882892608645 and perplexity is 35.67992798174299
At time: 92.29709100723267 and batch: 150, loss is 3.581435341835022 and perplexity is 35.92506861214287
At time: 93.0567638874054 and batch: 200, loss is 3.464339323043823 and perplexity is 31.955340640248725
At time: 93.82332801818848 and batch: 250, loss is 3.6102508211135866 and perplexity is 36.97532584424159
At time: 94.58098912239075 and batch: 300, loss is 3.588051047325134 and perplexity is 36.16352619865008
At time: 95.33840823173523 and batch: 350, loss is 3.561524658203125 and perplexity is 35.21684989273954
At time: 96.09601616859436 and batch: 400, loss is 3.4905404567718508 and perplexity is 32.803671882835474
At time: 96.8543529510498 and batch: 450, loss is 3.5117738342285154 and perplexity is 33.50765211963726
At time: 97.61195015907288 and batch: 500, loss is 3.38708149433136 and perplexity is 29.579498240156408
At time: 98.37053275108337 and batch: 550, loss is 3.4505608892440796 and perplexity is 31.518065495865148
At time: 99.13334441184998 and batch: 600, loss is 3.4689981651306154 and perplexity is 32.1045628575913
At time: 99.89429569244385 and batch: 650, loss is 3.3029415225982666 and perplexity is 27.192508772639055
At time: 100.65080261230469 and batch: 700, loss is 3.2973498249053956 and perplexity is 27.04088080802182
At time: 101.4076361656189 and batch: 750, loss is 3.3957089614868163 and perplexity is 29.83579841058142
At time: 102.16558742523193 and batch: 800, loss is 3.3376346158981325 and perplexity is 28.15245651312271
At time: 102.92353892326355 and batch: 850, loss is 3.397444319725037 and perplexity is 29.887619159924537
At time: 103.69870138168335 and batch: 900, loss is 3.3574694108963015 and perplexity is 28.716429370070177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357530985793022 and perplexity of 78.06415477917166
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 105.61340427398682 and batch: 50, loss is 3.6613382863998414 and perplexity is 38.91338529404263
At time: 106.36643600463867 and batch: 100, loss is 3.5267012786865233 and perplexity is 34.01158761254325
At time: 107.12039947509766 and batch: 150, loss is 3.5405787658691406 and perplexity is 34.4868732411041
At time: 107.8771800994873 and batch: 200, loss is 3.418587155342102 and perplexity is 30.52625568224259
At time: 108.63275694847107 and batch: 250, loss is 3.566112213134766 and perplexity is 35.37878027440812
At time: 109.38923573493958 and batch: 300, loss is 3.537423324584961 and perplexity is 34.37822344669069
At time: 110.15567708015442 and batch: 350, loss is 3.5040443086624147 and perplexity is 33.24965225870676
At time: 110.91162395477295 and batch: 400, loss is 3.4352870750427247 and perplexity is 31.040322188420127
At time: 111.66668891906738 and batch: 450, loss is 3.450141959190369 and perplexity is 31.50486439635584
At time: 112.42289733886719 and batch: 500, loss is 3.3220086526870727 and perplexity is 27.715966433780753
At time: 113.17839288711548 and batch: 550, loss is 3.3765736293792723 and perplexity is 29.270308175953573
At time: 113.93360090255737 and batch: 600, loss is 3.390562343597412 and perplexity is 29.68263942020541
At time: 114.68990325927734 and batch: 650, loss is 3.217486410140991 and perplexity is 24.96528875156374
At time: 115.44503903388977 and batch: 700, loss is 3.203486919403076 and perplexity is 24.618222466854366
At time: 116.2004234790802 and batch: 750, loss is 3.2958793115615843 and perplexity is 27.001146054368178
At time: 116.95587873458862 and batch: 800, loss is 3.2314301443099978 and perplexity is 25.315836393269567
At time: 117.71616411209106 and batch: 850, loss is 3.284273862838745 and perplexity is 26.689596968624528
At time: 118.47066330909729 and batch: 900, loss is 3.242937459945679 and perplexity is 25.608836301233605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348550456843964 and perplexity of 77.36623590819033
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.31166911125183 and batch: 50, loss is 3.636301555633545 and perplexity is 37.951216384996386
At time: 121.08225417137146 and batch: 100, loss is 3.50040931224823 and perplexity is 33.129009293186925
At time: 121.85043358802795 and batch: 150, loss is 3.5134540224075317 and perplexity is 33.56399860372785
At time: 122.62082195281982 and batch: 200, loss is 3.391353807449341 and perplexity is 29.70614145561524
At time: 123.38014197349548 and batch: 250, loss is 3.5411436939239502 and perplexity is 34.506361347491385
At time: 124.13701248168945 and batch: 300, loss is 3.512789807319641 and perplexity is 33.54171229171702
At time: 124.89341378211975 and batch: 350, loss is 3.480098261833191 and perplexity is 32.46291178437058
At time: 125.65000867843628 and batch: 400, loss is 3.411074833869934 and perplexity is 30.297791857508898
At time: 126.40726828575134 and batch: 450, loss is 3.4253206777572633 and perplexity is 30.73249850018963
At time: 127.16540932655334 and batch: 500, loss is 3.2980490589141844 and perplexity is 27.059795323576008
At time: 127.92313861846924 and batch: 550, loss is 3.3489134454727174 and perplexity is 28.47178068856877
At time: 128.68016386032104 and batch: 600, loss is 3.3653037261962893 and perplexity is 28.94228649481708
At time: 129.44767832756042 and batch: 650, loss is 3.191183581352234 and perplexity is 24.31719179200687
At time: 130.21219563484192 and batch: 700, loss is 3.17332088470459 and perplexity is 23.88667769026775
At time: 130.96711468696594 and batch: 750, loss is 3.2650534296035767 and perplexity is 26.181509821143933
At time: 131.72328567504883 and batch: 800, loss is 3.199085192680359 and perplexity is 24.510097921085283
At time: 132.4803557395935 and batch: 850, loss is 3.249396634101868 and perplexity is 25.77478359913364
At time: 133.23818159103394 and batch: 900, loss is 3.2099724864959716 and perplexity is 24.778404474477746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345441426316353 and perplexity of 77.12607544550107
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.0927381515503 and batch: 50, loss is 3.626727695465088 and perplexity is 37.589610490265635
At time: 135.863361120224 and batch: 100, loss is 3.4898568058013915 and perplexity is 32.781253284839444
At time: 136.62072777748108 and batch: 150, loss is 3.5028587055206297 and perplexity is 33.210254726059766
At time: 137.37751412391663 and batch: 200, loss is 3.380645809173584 and perplexity is 29.389745152854584
At time: 138.13545942306519 and batch: 250, loss is 3.5304297256469725 and perplexity is 34.13863471000268
At time: 138.893545627594 and batch: 300, loss is 3.502772207260132 and perplexity is 33.20738222103041
At time: 139.65067553520203 and batch: 350, loss is 3.471395397186279 and perplexity is 32.18161726655534
At time: 140.40807557106018 and batch: 400, loss is 3.4019344234466553 and perplexity is 30.022119403996168
At time: 141.17280793190002 and batch: 450, loss is 3.4163439893722534 and perplexity is 30.457856967824252
At time: 141.930969953537 and batch: 500, loss is 3.288536787033081 and perplexity is 26.803615550898588
At time: 142.69062757492065 and batch: 550, loss is 3.3396916151046754 and perplexity is 28.210425694672512
At time: 143.454922914505 and batch: 600, loss is 3.3575137567520144 and perplexity is 28.717702852940246
At time: 144.22687935829163 and batch: 650, loss is 3.1829622316360475 and perplexity is 24.118091213354653
At time: 144.98910069465637 and batch: 700, loss is 3.1648620843887327 and perplexity is 23.685477210495726
At time: 145.74793219566345 and batch: 750, loss is 3.2561150979995728 and perplexity is 25.94853356509586
At time: 146.5043330192566 and batch: 800, loss is 3.19014609336853 and perplexity is 24.291976080485902
At time: 147.25998616218567 and batch: 850, loss is 3.2390150928497317 and perplexity is 25.508585782557233
At time: 148.02861523628235 and batch: 900, loss is 3.1998006248474122 and perplexity is 24.527639507715108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34568222254923 and perplexity of 77.1446493500983
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.88571071624756 and batch: 50, loss is 3.6237253284454347 and perplexity is 37.47692193429984
At time: 150.64232969284058 and batch: 100, loss is 3.487352604866028 and perplexity is 32.699265139941524
At time: 151.40688753128052 and batch: 150, loss is 3.500483808517456 and perplexity is 33.131477372712645
At time: 152.1687068939209 and batch: 200, loss is 3.3780586385726927 and perplexity is 29.313807142878158
At time: 152.93373847007751 and batch: 250, loss is 3.52770489692688 and perplexity is 34.04573939706501
At time: 153.6929907798767 and batch: 300, loss is 3.499836812019348 and perplexity is 33.1100483558723
At time: 154.45112586021423 and batch: 350, loss is 3.4681114625930785 and perplexity is 32.0761082774775
At time: 155.20842337608337 and batch: 400, loss is 3.3990442514419557 and perplexity is 29.935475583057805
At time: 155.9640839099884 and batch: 450, loss is 3.4139390420913696 and perplexity is 30.384695437601984
At time: 156.72638463974 and batch: 500, loss is 3.285825505256653 and perplexity is 26.731041824881196
At time: 157.4886336326599 and batch: 550, loss is 3.3370048141479494 and perplexity is 28.134731628906227
At time: 158.25132632255554 and batch: 600, loss is 3.3553239822387697 and perplexity is 28.65488636123123
At time: 159.00956273078918 and batch: 650, loss is 3.180667314529419 and perplexity is 24.06280565540036
At time: 159.76756405830383 and batch: 700, loss is 3.162571473121643 and perplexity is 23.63128507977863
At time: 160.52616357803345 and batch: 750, loss is 3.2538181495666505 and perplexity is 25.888999521076435
At time: 161.28369426727295 and batch: 800, loss is 3.187682228088379 and perplexity is 24.232197597340583
At time: 162.04192280769348 and batch: 850, loss is 3.2362352418899536 and perplexity is 25.43777418433179
At time: 162.80070781707764 and batch: 900, loss is 3.196920852661133 and perplexity is 24.45710710104147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345554717599529 and perplexity of 77.13481365252662
Annealing...
Model not improving. Stopping early with 76.86885566582573 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc03c314b70>
ELAPSED
884.425820350647


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -77.71701100305113}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.636192611183658, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.9359653836036915, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.32989306866857}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.17658611098779187, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.18394514580407484, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.91302176961456}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.040248218725259965, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.466024297238987, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.86885566582573}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'dropout': 0.04326650961669323, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.47480192822778794, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0517072677612305 and batch: 50, loss is 6.728498458862305 and perplexity is 835.8911981507398
At time: 1.8780150413513184 and batch: 100, loss is 5.832093000411987 and perplexity is 341.07179594389663
At time: 2.7229814529418945 and batch: 150, loss is 5.569455976486206 and perplexity is 262.29136774763526
At time: 3.566574811935425 and batch: 200, loss is 5.320037412643432 and perplexity is 204.3915286773088
At time: 4.392151117324829 and batch: 250, loss is 5.331601486206055 and perplexity is 206.7688465974334
At time: 5.2205283641815186 and batch: 300, loss is 5.2413403606414795 and perplexity is 188.92315794010773
At time: 6.047241449356079 and batch: 350, loss is 5.185389652252197 and perplexity is 178.64304489478303
At time: 6.875012159347534 and batch: 400, loss is 5.020607748031616 and perplexity is 151.5033516840268
At time: 7.703423738479614 and batch: 450, loss is 5.020662298202515 and perplexity is 151.51161644317284
At time: 8.529743432998657 and batch: 500, loss is 4.939291877746582 and perplexity is 139.67131017064972
At time: 9.358515739440918 and batch: 550, loss is 4.996716051101685 and perplexity is 147.9265772644796
At time: 10.193708896636963 and batch: 600, loss is 4.91715440750122 and perplexity is 136.61331373456505
At time: 11.02091670036316 and batch: 650, loss is 4.792197256088257 and perplexity is 120.56599219430336
At time: 11.84765362739563 and batch: 700, loss is 4.858855571746826 and perplexity is 128.8766276254533
At time: 12.67418622970581 and batch: 750, loss is 4.867534484863281 and perplexity is 130.0000044730009
At time: 13.50589370727539 and batch: 800, loss is 4.799683866500854 and perplexity is 121.47201007652143
At time: 14.332998991012573 and batch: 850, loss is 4.856523962020874 and perplexity is 128.57648766762978
At time: 15.160388469696045 and batch: 900, loss is 4.777839660644531 and perplexity is 118.84732196390775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.782616184182363 and perplexity of 119.41635691721505
finished 1 epochs...
Completing Train Step...
At time: 17.057958602905273 and batch: 50, loss is 4.7741051769256595 and perplexity is 118.40431628860284
At time: 17.8124418258667 and batch: 100, loss is 4.6300702571868895 and perplexity is 102.52126671326882
At time: 18.56764841079712 and batch: 150, loss is 4.609971971511841 and perplexity is 100.48133325706067
At time: 19.322571277618408 and batch: 200, loss is 4.485226373672486 and perplexity is 88.69702721227937
At time: 20.07732391357422 and batch: 250, loss is 4.610563497543335 and perplexity is 100.54078816419054
At time: 20.83350419998169 and batch: 300, loss is 4.567962303161621 and perplexity is 96.3475824451554
At time: 21.588874101638794 and batch: 350, loss is 4.550209560394287 and perplexity is 94.6522415977856
At time: 22.343178272247314 and batch: 400, loss is 4.446421570777893 and perplexity is 85.32108162340506
At time: 23.110098361968994 and batch: 450, loss is 4.460992050170899 and perplexity is 86.57335161751266
At time: 23.864936113357544 and batch: 500, loss is 4.355408525466919 and perplexity is 77.89864241660744
At time: 24.61646819114685 and batch: 550, loss is 4.43350061416626 and perplexity is 84.22574327826224
At time: 25.367931365966797 and batch: 600, loss is 4.416667842864991 and perplexity is 82.81985628651839
At time: 26.12106227874756 and batch: 650, loss is 4.2611323165893555 and perplexity is 70.8902081845587
At time: 26.87332010269165 and batch: 700, loss is 4.290519080162048 and perplexity is 73.0043537778232
At time: 27.626583099365234 and batch: 750, loss is 4.375122389793396 and perplexity is 79.44956277281088
At time: 28.38017749786377 and batch: 800, loss is 4.309394087791443 and perplexity is 74.39539820117972
At time: 29.13273310661316 and batch: 850, loss is 4.378936438560486 and perplexity is 79.75316589025628
At time: 29.886043787002563 and batch: 900, loss is 4.322207007408142 and perplexity is 75.35475340391665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493200014715326 and perplexity of 89.40709261003929
finished 2 epochs...
Completing Train Step...
At time: 31.73159623146057 and batch: 50, loss is 4.3794407081604 and perplexity is 79.79339312914604
At time: 32.503485918045044 and batch: 100, loss is 4.233519711494446 and perplexity is 68.95952315283468
At time: 33.26692295074463 and batch: 150, loss is 4.22758990764618 and perplexity is 68.55181671134785
At time: 34.02413868904114 and batch: 200, loss is 4.1136856508255 and perplexity is 61.17176034934889
At time: 34.781421422958374 and batch: 250, loss is 4.26286057472229 and perplexity is 71.01283069455266
At time: 35.5389084815979 and batch: 300, loss is 4.234963865280151 and perplexity is 69.05918325419184
At time: 36.2975287437439 and batch: 350, loss is 4.220550189018249 and perplexity is 68.07092586437614
At time: 37.05699133872986 and batch: 400, loss is 4.138636660575867 and perplexity is 62.717258225883974
At time: 37.82742142677307 and batch: 450, loss is 4.1577142810821535 and perplexity is 63.92524035237278
At time: 38.59037899971008 and batch: 500, loss is 4.041463899612427 and perplexity is 56.90959178588576
At time: 39.350154876708984 and batch: 550, loss is 4.121345381736756 and perplexity is 61.642118682408295
At time: 40.107662200927734 and batch: 600, loss is 4.136763215065002 and perplexity is 62.59987085376346
At time: 40.86419224739075 and batch: 650, loss is 3.971504578590393 and perplexity is 53.06431023253268
At time: 41.62257361412048 and batch: 700, loss is 3.988136715888977 and perplexity is 53.95426352323026
At time: 42.391040325164795 and batch: 750, loss is 4.09891037940979 and perplexity is 60.27457538476815
At time: 43.149251222610474 and batch: 800, loss is 4.039960255622864 and perplexity is 56.82408432275059
At time: 43.911529302597046 and batch: 850, loss is 4.114191708564758 and perplexity is 61.202724626292856
At time: 44.67753720283508 and batch: 900, loss is 4.070267434120178 and perplexity is 58.572624815876644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.400309105441995 and perplexity of 81.47604946328806
finished 3 epochs...
Completing Train Step...
At time: 46.52995824813843 and batch: 50, loss is 4.141715121269226 and perplexity is 62.91062832854506
At time: 47.29979634284973 and batch: 100, loss is 3.9960515880584717 and perplexity is 54.3829990773856
At time: 48.05584979057312 and batch: 150, loss is 3.999006567001343 and perplexity is 54.543937362022476
At time: 48.811065435409546 and batch: 200, loss is 3.8876313829421996 and perplexity is 48.795172458853244
At time: 49.565672874450684 and batch: 250, loss is 4.040871052742005 and perplexity is 56.87586311145592
At time: 50.33365345001221 and batch: 300, loss is 4.01814875125885 and perplexity is 55.59808461312134
At time: 51.090842962265015 and batch: 350, loss is 4.005145134925843 and perplexity is 54.879788795177134
At time: 51.8468873500824 and batch: 400, loss is 3.9315178298950197 and perplexity is 50.984304471393365
At time: 52.6025013923645 and batch: 450, loss is 3.953791608810425 and perplexity is 52.132659220708184
At time: 53.357778787612915 and batch: 500, loss is 3.8404172086715698 and perplexity is 46.54488932092115
At time: 54.11405920982361 and batch: 550, loss is 3.9146689558029175 and perplexity is 50.132472699545715
At time: 54.87036466598511 and batch: 600, loss is 3.9445654296875 and perplexity is 51.65388598857132
At time: 55.62709832191467 and batch: 650, loss is 3.7735364770889284 and perplexity is 43.53374903140499
At time: 56.38305902481079 and batch: 700, loss is 3.789832854270935 and perplexity is 44.2490036258699
At time: 57.13936138153076 and batch: 750, loss is 3.9065760707855226 and perplexity is 49.72839365041212
At time: 57.89529371261597 and batch: 800, loss is 3.849476957321167 and perplexity is 46.96849028081127
At time: 58.65068507194519 and batch: 850, loss is 3.9257903242111207 and perplexity is 50.69312623623387
At time: 59.40508699417114 and batch: 900, loss is 3.8898934745788574 and perplexity is 48.90567654844882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367561653868793 and perplexity of 78.85113075335869
finished 4 epochs...
Completing Train Step...
At time: 61.250535011291504 and batch: 50, loss is 3.9609919738769532 and perplexity is 52.50938806395519
At time: 62.015265464782715 and batch: 100, loss is 3.8221533346176146 and perplexity is 45.70251524720256
At time: 62.77141880989075 and batch: 150, loss is 3.826164860725403 and perplexity is 45.886220302767924
At time: 63.53658103942871 and batch: 200, loss is 3.7141819286346434 and perplexity is 41.02501196529125
At time: 64.29519462585449 and batch: 250, loss is 3.8751536798477173 and perplexity is 48.190103569841284
At time: 65.04966855049133 and batch: 300, loss is 3.852690143585205 and perplexity is 47.119651513165756
At time: 65.8076798915863 and batch: 350, loss is 3.83966694355011 and perplexity is 46.5099814106127
At time: 66.56421566009521 and batch: 400, loss is 3.7722149229049684 and perplexity is 43.4762548224531
At time: 67.32028818130493 and batch: 450, loss is 3.7930418920516966 and perplexity is 44.39122843060059
At time: 68.07628893852234 and batch: 500, loss is 3.6856613969802856 and perplexity is 39.87148461049456
At time: 68.83404994010925 and batch: 550, loss is 3.754403576850891 and perplexity is 42.708739731807995
At time: 69.60247898101807 and batch: 600, loss is 3.792780885696411 and perplexity is 44.37964354979079
At time: 70.36052775382996 and batch: 650, loss is 3.6200777864456177 and perplexity is 37.34047229170526
At time: 71.1169810295105 and batch: 700, loss is 3.6339542293548583 and perplexity is 37.86223697018303
At time: 71.87373685836792 and batch: 750, loss is 3.755720853805542 and perplexity is 42.76503604119385
At time: 72.63141345977783 and batch: 800, loss is 3.698077926635742 and perplexity is 40.3696363394289
At time: 73.39869332313538 and batch: 850, loss is 3.776307716369629 and perplexity is 43.65455878580954
At time: 74.15514254570007 and batch: 900, loss is 3.7432903051376343 and perplexity is 42.23673352714866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375485772955908 and perplexity of 79.4784386523779
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 75.9949882030487 and batch: 50, loss is 3.8357854747772215 and perplexity is 46.32980427231707
At time: 76.76495742797852 and batch: 100, loss is 3.6992778062820433 and perplexity is 40.418104116335705
At time: 77.52228546142578 and batch: 150, loss is 3.7030024194717406 and perplexity is 40.56892662341944
At time: 78.27958917617798 and batch: 200, loss is 3.5743036556243895 and perplexity is 35.66977371928326
At time: 79.03696823120117 and batch: 250, loss is 3.7262074184417724 and perplexity is 41.521336117831204
At time: 79.80421614646912 and batch: 300, loss is 3.692595477104187 and perplexity is 40.14891743855814
At time: 80.56121754646301 and batch: 350, loss is 3.6664274168014526 and perplexity is 39.111925355801255
At time: 81.3234031200409 and batch: 400, loss is 3.594776062965393 and perplexity is 36.407546077026005
At time: 82.08406567573547 and batch: 450, loss is 3.5994441890716553 and perplexity is 36.577898397038076
At time: 82.84275984764099 and batch: 500, loss is 3.477027645111084 and perplexity is 32.36338350939168
At time: 83.59960293769836 and batch: 550, loss is 3.528937177658081 and perplexity is 34.08771916582134
At time: 84.36453747749329 and batch: 600, loss is 3.5564128017425536 and perplexity is 35.03728575515961
At time: 85.12175846099854 and batch: 650, loss is 3.364287395477295 and perplexity is 28.912886502562767
At time: 85.87890219688416 and batch: 700, loss is 3.360029168128967 and perplexity is 28.79003061832185
At time: 86.63685989379883 and batch: 750, loss is 3.4636805295944213 and perplexity is 31.934295604088653
At time: 87.39281439781189 and batch: 800, loss is 3.3822361183166505 and perplexity is 29.436521117547365
At time: 88.15011620521545 and batch: 850, loss is 3.4394327497482298 and perplexity is 31.169272375046116
At time: 88.90686416625977 and batch: 900, loss is 3.390112910270691 and perplexity is 29.66930205017872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3415769812178935 and perplexity of 76.82860111809065
finished 6 epochs...
Completing Train Step...
At time: 90.74693942070007 and batch: 50, loss is 3.7129542303085326 and perplexity is 40.974676531459025
At time: 91.51779866218567 and batch: 100, loss is 3.5712116289138796 and perplexity is 35.55965216331755
At time: 92.27458834648132 and batch: 150, loss is 3.5772365856170656 and perplexity is 35.774544235647745
At time: 93.03408741950989 and batch: 200, loss is 3.4534967947006225 and perplexity is 31.61073552499181
At time: 93.79932284355164 and batch: 250, loss is 3.606306834220886 and perplexity is 36.8297828422651
At time: 94.5602400302887 and batch: 300, loss is 3.580207929611206 and perplexity is 35.88100079400873
At time: 95.31739830970764 and batch: 350, loss is 3.556453013420105 and perplexity is 35.03869469152432
At time: 96.075275182724 and batch: 400, loss is 3.491290793418884 and perplexity is 32.828294916633496
At time: 96.83368349075317 and batch: 450, loss is 3.5027016973495484 and perplexity is 33.20504085402505
At time: 97.59166646003723 and batch: 500, loss is 3.385111050605774 and perplexity is 29.521270889142034
At time: 98.3489100933075 and batch: 550, loss is 3.438817849159241 and perplexity is 31.150112262490303
At time: 99.11808466911316 and batch: 600, loss is 3.4747647619247437 and perplexity is 32.29023175115239
At time: 99.87668466567993 and batch: 650, loss is 3.28994505405426 and perplexity is 26.841388789887947
At time: 100.63507628440857 and batch: 700, loss is 3.291940712928772 and perplexity is 26.89500853119126
At time: 101.39241814613342 and batch: 750, loss is 3.40236704826355 and perplexity is 30.035110527844775
At time: 102.14837121963501 and batch: 800, loss is 3.3284561157226564 and perplexity is 27.89524141623463
At time: 102.90507459640503 and batch: 850, loss is 3.395084509849548 and perplexity is 29.817173213287422
At time: 103.66351509094238 and batch: 900, loss is 3.3540981435775756 and perplexity is 28.619781614471332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357163520708476 and perplexity of 78.0354741978146
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 105.50796031951904 and batch: 50, loss is 3.6578752040863036 and perplexity is 38.778858111472275
At time: 106.2631003856659 and batch: 100, loss is 3.5233279514312743 and perplexity is 33.897048694297645
At time: 107.01753544807434 and batch: 150, loss is 3.5346900272369384 and perplexity is 34.28438584117432
At time: 107.77178382873535 and batch: 200, loss is 3.406619691848755 and perplexity is 30.163111125514956
At time: 108.52575421333313 and batch: 250, loss is 3.5583868932724 and perplexity is 35.10652087995007
At time: 109.280198097229 and batch: 300, loss is 3.530576105117798 and perplexity is 34.14363227104747
At time: 110.03577089309692 and batch: 350, loss is 3.5009486293792724 and perplexity is 33.14688115430045
At time: 110.7902159690857 and batch: 400, loss is 3.433524627685547 and perplexity is 30.98566343534916
At time: 111.54497623443604 and batch: 450, loss is 3.4410944890975954 and perplexity is 31.22111064035671
At time: 112.29869318008423 and batch: 500, loss is 3.315581998825073 and perplexity is 27.538416647663947
At time: 113.05164289474487 and batch: 550, loss is 3.361991901397705 and perplexity is 28.84659325975535
At time: 113.80664801597595 and batch: 600, loss is 3.3955832815170286 and perplexity is 29.832048883963715
At time: 114.56130480766296 and batch: 650, loss is 3.205157685279846 and perplexity is 24.659388132417355
At time: 115.31601166725159 and batch: 700, loss is 3.2009020233154297 and perplexity is 24.55466909473344
At time: 116.0700011253357 and batch: 750, loss is 3.30327654838562 and perplexity is 27.20162049054573
At time: 116.83531856536865 and batch: 800, loss is 3.2231125831604004 and perplexity is 25.106143651027587
At time: 117.61018872261047 and batch: 850, loss is 3.2836382007598877 and perplexity is 26.67263679497229
At time: 118.36520528793335 and batch: 900, loss is 3.2408253574371337 and perplexity is 25.554804893862443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350462613040453 and perplexity of 77.51431376445785
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.20482873916626 and batch: 50, loss is 3.631864833831787 and perplexity is 37.783210369426676
At time: 120.97853350639343 and batch: 100, loss is 3.4965501832962036 and perplexity is 33.00140655035553
At time: 121.73982238769531 and batch: 150, loss is 3.509943027496338 and perplexity is 33.44636220666937
At time: 122.49501872062683 and batch: 200, loss is 3.3823596334457395 and perplexity is 29.44015719780397
At time: 123.25909972190857 and batch: 250, loss is 3.5335258769989015 and perplexity is 34.244496888108415
At time: 124.01375031471252 and batch: 300, loss is 3.5075515794754026 and perplexity is 33.36647253403674
At time: 124.76931309700012 and batch: 350, loss is 3.475202589035034 and perplexity is 32.304372385361226
At time: 125.52478051185608 and batch: 400, loss is 3.409039306640625 and perplexity is 30.236182602125947
At time: 126.28061413764954 and batch: 450, loss is 3.4151194524765014 and perplexity is 30.420583024518688
At time: 127.04135727882385 and batch: 500, loss is 3.2885538148880005 and perplexity is 26.804071962861354
At time: 127.80717539787292 and batch: 550, loss is 3.3341377639770506 and perplexity is 28.054183464770585
At time: 128.56415104866028 and batch: 600, loss is 3.3701779413223267 and perplexity is 29.08370178973872
At time: 129.33744072914124 and batch: 650, loss is 3.178000054359436 and perplexity is 23.99870941097743
At time: 130.09820938110352 and batch: 700, loss is 3.173086304664612 and perplexity is 23.881075009624436
At time: 130.8560049533844 and batch: 750, loss is 3.2716176557540892 and perplexity is 26.353936477212688
At time: 131.6104371547699 and batch: 800, loss is 3.191652226448059 and perplexity is 24.328590595472473
At time: 132.3666467666626 and batch: 850, loss is 3.249301066398621 and perplexity is 25.772320479962577
At time: 133.1278576850891 and batch: 900, loss is 3.207935485839844 and perplexity is 24.72798222082238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349072600064212 and perplexity of 77.4066427118972
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 134.99207758903503 and batch: 50, loss is 3.621549744606018 and perplexity is 37.395476376532734
At time: 135.76423478126526 and batch: 100, loss is 3.48599244594574 and perplexity is 32.65481917641365
At time: 136.52308630943298 and batch: 150, loss is 3.49985134601593 and perplexity is 33.110529580698994
At time: 137.2928967475891 and batch: 200, loss is 3.372429299354553 and perplexity is 29.14925337772903
At time: 138.0491442680359 and batch: 250, loss is 3.5242017936706542 and perplexity is 33.92668231290518
At time: 138.80530333518982 and batch: 300, loss is 3.4986985540390014 and perplexity is 33.07238202017198
At time: 139.56186151504517 and batch: 350, loss is 3.465823369026184 and perplexity is 32.00279904165566
At time: 140.33008861541748 and batch: 400, loss is 3.4003251886367796 and perplexity is 29.973845616732074
At time: 141.09887647628784 and batch: 450, loss is 3.406135983467102 and perplexity is 30.148524503966776
At time: 141.85553908348083 and batch: 500, loss is 3.2789908123016356 and perplexity is 26.548966285617574
At time: 142.61350798606873 and batch: 550, loss is 3.324654793739319 and perplexity is 27.78940391047364
At time: 143.37167859077454 and batch: 600, loss is 3.3617639255523684 and perplexity is 28.840017682836702
At time: 144.13067770004272 and batch: 650, loss is 3.1694519233703615 and perplexity is 23.794439605730467
At time: 144.88795232772827 and batch: 700, loss is 3.165133543014526 and perplexity is 23.691907710358393
At time: 145.64514422416687 and batch: 750, loss is 3.262675986289978 and perplexity is 26.11933869912273
At time: 146.40308904647827 and batch: 800, loss is 3.1827397298812867 and perplexity is 24.11272549270147
At time: 147.16073989868164 and batch: 850, loss is 3.239791531562805 and perplexity is 25.52839932708068
At time: 147.92841386795044 and batch: 900, loss is 3.197497673034668 and perplexity is 24.471218528183147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349235221131207 and perplexity of 77.41923168631597
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.78074717521667 and batch: 50, loss is 3.618287568092346 and perplexity is 37.27368449309749
At time: 150.537278175354 and batch: 100, loss is 3.482862515449524 and perplexity is 32.5527716461722
At time: 151.30269289016724 and batch: 150, loss is 3.4973066234588623 and perplexity is 33.026379583806396
At time: 152.0765151977539 and batch: 200, loss is 3.3696372175216673 and perplexity is 29.067979790982257
At time: 152.83286213874817 and batch: 250, loss is 3.521584930419922 and perplexity is 33.8380168879855
At time: 153.58840823173523 and batch: 300, loss is 3.496255326271057 and perplexity is 32.99167728823557
At time: 154.3443832397461 and batch: 350, loss is 3.4631632471084597 and perplexity is 31.91778082404263
At time: 155.09921050071716 and batch: 400, loss is 3.397620530128479 and perplexity is 29.89288613338877
At time: 155.8663046360016 and batch: 450, loss is 3.4038292217254638 and perplexity is 30.07905919183932
At time: 156.619943857193 and batch: 500, loss is 3.2764259958267212 and perplexity is 26.480960308202125
At time: 157.37425446510315 and batch: 550, loss is 3.3220084953308104 and perplexity is 27.715962072500215
At time: 158.12781476974487 and batch: 600, loss is 3.3595269823074343 and perplexity is 28.775576302823758
At time: 158.8832266330719 and batch: 650, loss is 3.16694938659668 and perplexity is 23.734967592096435
At time: 159.63778018951416 and batch: 700, loss is 3.162673788070679 and perplexity is 23.633703037201702
At time: 160.39285278320312 and batch: 750, loss is 3.260240230560303 and perplexity is 26.055795789150714
At time: 161.15644359588623 and batch: 800, loss is 3.1804175758361817 and perplexity is 24.05679699208945
At time: 161.91142392158508 and batch: 850, loss is 3.237253818511963 and perplexity is 25.46369770673755
At time: 162.66734766960144 and batch: 900, loss is 3.194599537849426 and perplexity is 24.400400298710135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348951365849743 and perplexity of 77.3972589472015
Annealing...
Model not improving. Stopping early with 76.82860111809065 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc03c314b70>
ELAPSED
1054.032966375351


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -77.71701100305113}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.636192611183658, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.9359653836036915, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.32989306866857}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.17658611098779187, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.18394514580407484, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.91302176961456}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.040248218725259965, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.466024297238987, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.86885566582573}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.04326650961669323, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.47480192822778794, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.82860111809065}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'tie_weights': 'FALSE', 'dropout': 0.5327361632215586, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.00395424850675874, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.46692270496843}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.2524344661014074, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.29004854721358686, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -77.71701100305113}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.636192611183658, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.9359653836036915, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -78.32989306866857}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.17658611098779187, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.18394514580407484, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.91302176961456}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.040248218725259965, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.466024297238987, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.86885566582573}, {'params': {'tie_weights': 'FALSE', 'dropout': 0.04326650961669323, 'seq_len': 35, 'num_layers': 2, 'rnn_dropout': 0.47480192822778794, 'batch_size': 32, 'wordvec_source': 'None', 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': 'TRUE'}, 'best_accuracy': -76.82860111809065}]
Exception ignored in: <bound method DropoutDescriptor.__del__ of <torch.backends.cudnn.DropoutDescriptor object at 0x7fc0374e97f0>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 215, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroyDropoutDescriptor'
Exception ignored in: <bound method CuDNNHandle.__del__ of <torch.backends.cudnn.CuDNNHandle object at 0x7fc03359b5f8>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 91, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroy'
