FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.5236952304840088 and batch: 50, loss is 7.958402051925659 and perplexity is 2859.4999673167385
At time: 2.63222074508667 and batch: 100, loss is 7.128201360702515 and perplexity is 1246.6327066352403
At time: 3.7459874153137207 and batch: 150, loss is 6.899170503616333 and perplexity is 991.4519685968369
At time: 4.853257179260254 and batch: 200, loss is 6.692738904953003 and perplexity is 806.5282339524072
At time: 5.956280946731567 and batch: 250, loss is 6.668194255828857 and perplexity is 786.973248287497
At time: 7.057587146759033 and batch: 300, loss is 6.502095651626587 and perplexity is 666.5369997801711
At time: 8.159874439239502 and batch: 350, loss is 6.457057619094849 and perplexity is 637.1834591774585
At time: 9.261404037475586 and batch: 400, loss is 6.3992167091369625 and perplexity is 601.373802734135
At time: 10.360361576080322 and batch: 450, loss is 6.418152952194214 and perplexity is 612.8700677384059
At time: 11.461965322494507 and batch: 500, loss is 6.424363613128662 and perplexity is 616.6882403407424
At time: 12.564090728759766 and batch: 550, loss is 6.467552585601807 and perplexity is 643.9058923929937
At time: 13.6651930809021 and batch: 600, loss is 6.43805908203125 and perplexity is 625.1921747231249
At time: 14.764997959136963 and batch: 650, loss is 6.391835193634034 and perplexity is 596.9510959072677
At time: 15.865304708480835 and batch: 700, loss is 6.470004987716675 and perplexity is 645.4869464630391
At time: 16.96604871749878 and batch: 750, loss is 6.453546686172485 and perplexity is 634.9502733698082
At time: 18.063356399536133 and batch: 800, loss is 6.456474323272705 and perplexity is 636.811901102452
At time: 19.15989923477173 and batch: 850, loss is 6.489848566055298 and perplexity is 658.4236479749771
At time: 20.259291172027588 and batch: 900, loss is 6.417762689590454 and perplexity is 612.6309341354897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.495506391133348 and perplexity of 662.159452100045
finished 1 epochs...
Completing Train Step...
At time: 22.495176553726196 and batch: 50, loss is 6.458383388519287 and perplexity is 638.0287777503372
At time: 23.376424074172974 and batch: 100, loss is 6.442966814041138 and perplexity is 628.2679918407748
At time: 24.2513906955719 and batch: 150, loss is 6.482039222717285 and perplexity is 653.3018167452761
At time: 25.125976085662842 and batch: 200, loss is 6.408015546798706 and perplexity is 606.6885406678932
At time: 26.00257635116577 and batch: 250, loss is 6.503357734680176 and perplexity is 667.3787559035197
At time: 26.880592823028564 and batch: 300, loss is 6.467284164428711 and perplexity is 643.733077612605
At time: 27.75732398033142 and batch: 350, loss is 6.491647672653198 and perplexity is 659.6092885313311
At time: 28.63269829750061 and batch: 400, loss is 6.429289684295655 and perplexity is 619.733585134863
At time: 29.506944179534912 and batch: 450, loss is 6.434422330856323 and perplexity is 622.9226357228603
At time: 30.38344955444336 and batch: 500, loss is 6.435599489212036 and perplexity is 623.6563460703719
At time: 31.259007453918457 and batch: 550, loss is 6.474210405349732 and perplexity is 648.2072045511669
At time: 32.14974904060364 and batch: 600, loss is 6.44270619392395 and perplexity is 628.1042738981373
At time: 33.02558159828186 and batch: 650, loss is 6.394699802398682 and perplexity is 598.6635788744783
At time: 33.8997757434845 and batch: 700, loss is 6.4714587306976314 and perplexity is 646.4260009873399
At time: 34.77599596977234 and batch: 750, loss is 6.454684610366821 and perplexity is 635.6732098935476
At time: 35.65265345573425 and batch: 800, loss is 6.456585998535156 and perplexity is 636.8830212097444
At time: 36.527406454086304 and batch: 850, loss is 6.491572732925415 and perplexity is 659.5598594429297
At time: 37.40250062942505 and batch: 900, loss is 6.41757495880127 and perplexity is 612.5159352415002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.4990167487157535 and perplexity of 664.4879530979524
Annealing...
finished 2 epochs...
Completing Train Step...
At time: 39.48199439048767 and batch: 50, loss is 6.417372179031372 and perplexity is 612.3917419934581
At time: 40.364896297454834 and batch: 100, loss is 6.364349641799927 and perplexity is 580.7669988880476
At time: 41.24229311943054 and batch: 150, loss is 6.380396718978882 and perplexity is 590.1617896055706
At time: 42.12052035331726 and batch: 200, loss is 6.32626296043396 and perplexity is 559.0634429888011
At time: 42.99722242355347 and batch: 250, loss is 6.4330556774139405 and perplexity is 622.0718978227645
At time: 43.877525091171265 and batch: 300, loss is 6.357850551605225 and perplexity is 577.0047805208446
At time: 44.7571542263031 and batch: 350, loss is 6.3809654426574705 and perplexity is 590.4975240505323
At time: 45.63601899147034 and batch: 400, loss is 6.329358959197998 and perplexity is 560.7969848543537
At time: 46.51408243179321 and batch: 450, loss is 6.325546617507935 and perplexity is 558.6631052528887
At time: 47.39404296875 and batch: 500, loss is 6.32262544631958 and perplexity is 557.0335359700385
At time: 48.271947145462036 and batch: 550, loss is 6.356103706359863 and perplexity is 575.9977223069512
At time: 49.1485538482666 and batch: 600, loss is 6.3299750709533695 and perplexity is 561.1426049284615
At time: 50.02440428733826 and batch: 650, loss is 6.272350435256958 and perplexity is 529.7209906823139
At time: 50.91531825065613 and batch: 700, loss is 6.348973331451416 and perplexity is 571.9052503692906
At time: 51.794121980667114 and batch: 750, loss is 6.323213920593262 and perplexity is 557.3614323453631
At time: 52.68517565727234 and batch: 800, loss is 6.338429403305054 and perplexity is 565.9068016815813
At time: 53.56142973899841 and batch: 850, loss is 6.369154739379883 and perplexity is 583.5643563944768
At time: 54.43649744987488 and batch: 900, loss is 6.291096057891846 and perplexity is 539.7445963182669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.383340077857449 and perplexity of 591.9014064512305
finished 3 epochs...
Completing Train Step...
At time: 56.526379108428955 and batch: 50, loss is 6.358190422058105 and perplexity is 577.2009207261568
At time: 57.40448307991028 and batch: 100, loss is 6.328072214126587 and perplexity is 560.0758461583615
At time: 58.28251576423645 and batch: 150, loss is 6.361444091796875 and perplexity is 579.0820004417036
At time: 59.162776947021484 and batch: 200, loss is 6.310203971862793 and perplexity is 550.1571539478247
At time: 60.04158425331116 and batch: 250, loss is 6.409878072738647 and perplexity is 607.8195667681166
At time: 60.92356777191162 and batch: 300, loss is 6.348848791122436 and perplexity is 571.8340295362896
At time: 61.8021879196167 and batch: 350, loss is 6.370056762695312 and perplexity is 584.0909825288169
At time: 62.68329668045044 and batch: 400, loss is 6.3181165599822995 and perplexity is 554.5275888312067
At time: 63.57306122779846 and batch: 450, loss is 6.319140071868897 and perplexity is 555.0954449639996
At time: 64.45466089248657 and batch: 500, loss is 6.316531410217285 and perplexity is 553.6492758667831
At time: 65.33344721794128 and batch: 550, loss is 6.351508255004883 and perplexity is 573.3568254974956
At time: 66.21373867988586 and batch: 600, loss is 6.326983318328858 and perplexity is 559.4663138418854
At time: 67.09153771400452 and batch: 650, loss is 6.269033155441284 and perplexity is 527.9666693281567
At time: 67.97045111656189 and batch: 700, loss is 6.350552892684936 and perplexity is 572.8093235635328
At time: 68.84960126876831 and batch: 750, loss is 6.32805890083313 and perplexity is 560.0683897538981
At time: 69.72903847694397 and batch: 800, loss is 6.343295164108277 and perplexity is 568.6670787936306
At time: 70.60658001899719 and batch: 850, loss is 6.373608922958374 and perplexity is 586.1694566580853
At time: 71.48237419128418 and batch: 900, loss is 6.294172172546387 and perplexity is 541.4074688627708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.3814634558272685 and perplexity of 590.791672833147
finished 4 epochs...
Completing Train Step...
At time: 73.56692457199097 and batch: 50, loss is 6.353979396820068 and perplexity is 574.7754235806315
At time: 74.44354128837585 and batch: 100, loss is 6.324953451156616 and perplexity is 558.3318233594788
At time: 75.33517622947693 and batch: 150, loss is 6.358423643112182 and perplexity is 577.3355518320967
At time: 76.21314001083374 and batch: 200, loss is 6.307279491424561 and perplexity is 548.5505804545571
At time: 77.09030055999756 and batch: 250, loss is 6.403502807617188 and perplexity is 603.956881787283
At time: 77.98492312431335 and batch: 300, loss is 6.347236757278442 and perplexity is 570.9129563275847
At time: 78.86960244178772 and batch: 350, loss is 6.368901014328003 and perplexity is 583.4163102801738
At time: 79.74901103973389 and batch: 400, loss is 6.316927976608277 and perplexity is 553.8688781025436
At time: 80.6270022392273 and batch: 450, loss is 6.318321962356567 and perplexity is 554.6415018131474
At time: 81.50330257415771 and batch: 500, loss is 6.315784826278686 and perplexity is 553.2360844700264
At time: 82.38326334953308 and batch: 550, loss is 6.351001043319702 and perplexity is 573.0660859553045
At time: 83.26285123825073 and batch: 600, loss is 6.327343816757202 and perplexity is 559.66803692698
At time: 84.13918900489807 and batch: 650, loss is 6.269200229644776 and perplexity is 528.0548863080905
At time: 85.01914811134338 and batch: 700, loss is 6.3517196750640865 and perplexity is 573.4780574464684
At time: 85.8982572555542 and batch: 750, loss is 6.329975843429565 and perplexity is 561.1430383979338
At time: 86.77600836753845 and batch: 800, loss is 6.344956760406494 and perplexity is 569.6127593587578
At time: 87.6533453464508 and batch: 850, loss is 6.374814262390137 and perplexity is 586.8764157951612
At time: 88.52742385864258 and batch: 900, loss is 6.2949415302276615 and perplexity is 541.8241651313234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381115220997431 and perplexity of 590.5859744132446
finished 5 epochs...
Completing Train Step...
At time: 90.60131168365479 and batch: 50, loss is 6.352655706405639 and perplexity is 574.0151021880748
At time: 91.48315167427063 and batch: 100, loss is 6.3237849044799805 and perplexity is 557.679767615817
At time: 92.36120676994324 and batch: 150, loss is 6.357483797073364 and perplexity is 576.793200204075
At time: 93.23776960372925 and batch: 200, loss is 6.306686639785767 and perplexity is 548.2254677253026
At time: 94.11345410346985 and batch: 250, loss is 6.402915201187134 and perplexity is 603.6020970871633
At time: 94.99465203285217 and batch: 300, loss is 6.346586399078369 and perplexity is 570.5417791170631
At time: 95.87371826171875 and batch: 350, loss is 6.368450679779053 and perplexity is 583.1536369091198
At time: 96.75713324546814 and batch: 400, loss is 6.316569833755493 and perplexity is 553.6705494395885
At time: 97.64549255371094 and batch: 450, loss is 6.31808970451355 and perplexity is 554.5126969328396
At time: 98.53009629249573 and batch: 500, loss is 6.315685882568359 and perplexity is 553.1813479471041
At time: 99.4088203907013 and batch: 550, loss is 6.351045055389404 and perplexity is 573.0913083348642
At time: 100.28733515739441 and batch: 600, loss is 6.327657308578491 and perplexity is 559.8435157833494
At time: 101.16216254234314 and batch: 650, loss is 6.269403257369995 and perplexity is 528.1621069744643
At time: 102.04099917411804 and batch: 700, loss is 6.35222825050354 and perplexity is 573.7697884788821
At time: 102.91915678977966 and batch: 750, loss is 6.330744571685791 and perplexity is 561.5745707516406
At time: 103.79773044586182 and batch: 800, loss is 6.345591592788696 and perplexity is 569.9744827884392
At time: 104.67400240898132 and batch: 850, loss is 6.375272932052613 and perplexity is 587.1456599450503
At time: 105.5479793548584 and batch: 900, loss is 6.295238151550293 and perplexity is 541.9849055701604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381097662938784 and perplexity of 590.5756049611037
finished 6 epochs...
Completing Train Step...
At time: 107.63401341438293 and batch: 50, loss is 6.352055358886719 and perplexity is 573.6705970673976
At time: 108.51102328300476 and batch: 100, loss is 6.323235664367676 and perplexity is 557.373551618374
At time: 109.38925075531006 and batch: 150, loss is 6.357058200836182 and perplexity is 576.5477714189242
At time: 110.2795786857605 and batch: 200, loss is 6.306423234939575 and perplexity is 548.0810814971491
At time: 111.15692734718323 and batch: 250, loss is 6.402577419281005 and perplexity is 603.3982456508677
At time: 112.03588914871216 and batch: 300, loss is 6.346323661804199 and perplexity is 570.3918962159925
At time: 112.91519260406494 and batch: 350, loss is 6.368286762237549 and perplexity is 583.0580556325766
At time: 113.79411292076111 and batch: 400, loss is 6.316452522277832 and perplexity is 553.6056013389493
At time: 114.67045140266418 and batch: 450, loss is 6.318016414642334 and perplexity is 554.4720582579134
At time: 115.54845476150513 and batch: 500, loss is 6.315672025680542 and perplexity is 553.1736826283321
At time: 116.4272153377533 and batch: 550, loss is 6.3510988903045655 and perplexity is 573.1221614873091
At time: 117.30336904525757 and batch: 600, loss is 6.327840919494629 and perplexity is 559.9463186017473
At time: 118.17768788337708 and batch: 650, loss is 6.269513444900513 and perplexity is 528.2203070591478
At time: 119.0631971359253 and batch: 700, loss is 6.352467622756958 and perplexity is 573.9071494856445
At time: 119.94075012207031 and batch: 750, loss is 6.331100625991821 and perplexity is 561.7745573966539
At time: 120.81921815872192 and batch: 800, loss is 6.345880088806152 and perplexity is 570.1389418785307
At time: 121.69502782821655 and batch: 850, loss is 6.375482559204102 and perplexity is 587.2687545187853
At time: 122.57060813903809 and batch: 900, loss is 6.295373554229736 and perplexity is 542.0582967471616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381098499036815 and perplexity of 590.5760987404105
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 124.6573760509491 and batch: 50, loss is 6.344526863098144 and perplexity is 569.3679369947158
At time: 125.53692197799683 and batch: 100, loss is 6.3102997875213624 and perplexity is 550.2098701433249
At time: 126.41887736320496 and batch: 150, loss is 6.341345949172974 and perplexity is 567.5597040367553
At time: 127.29860711097717 and batch: 200, loss is 6.288149251937866 and perplexity is 538.1564149086942
At time: 128.18095517158508 and batch: 250, loss is 6.3821572494506835 and perplexity is 591.2017025500795
At time: 129.06377863883972 and batch: 300, loss is 6.32141372680664 and perplexity is 556.3589763362135
At time: 129.94102716445923 and batch: 350, loss is 6.343824214935303 and perplexity is 568.9680121794783
At time: 130.81928038597107 and batch: 400, loss is 6.2900128841400145 and perplexity is 539.1602756563743
At time: 131.7334644794464 and batch: 450, loss is 6.2913844108581545 and perplexity is 539.9002557150103
At time: 132.62186408042908 and batch: 500, loss is 6.284223651885986 and perplexity is 536.047969223309
At time: 133.5046317577362 and batch: 550, loss is 6.320077266693115 and perplexity is 555.6159213977811
At time: 134.38393020629883 and batch: 600, loss is 6.299043207168579 and perplexity is 544.0511168115125
At time: 135.2595660686493 and batch: 650, loss is 6.232559156417847 and perplexity is 509.05657333618626
At time: 136.15952444076538 and batch: 700, loss is 6.320512762069702 and perplexity is 555.8579422583659
At time: 137.04109144210815 and batch: 750, loss is 6.296136646270752 and perplexity is 542.4720949821659
At time: 137.91517615318298 and batch: 800, loss is 6.3065851783752445 and perplexity is 548.1698468177987
At time: 138.79108786582947 and batch: 850, loss is 6.339263963699341 and perplexity is 566.379282214469
At time: 139.66992378234863 and batch: 900, loss is 6.265221853256225 and perplexity is 525.9582585690501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.357180660718108 and perplexity of 576.6183797141907
finished 8 epochs...
Completing Train Step...
At time: 141.769291639328 and batch: 50, loss is 6.324751529693604 and perplexity is 558.2190955623237
At time: 142.64293003082275 and batch: 100, loss is 6.298356075286865 and perplexity is 543.6774103513843
At time: 143.51293015480042 and batch: 150, loss is 6.3341531085968015 and perplexity is 563.4919843354239
At time: 144.3823323249817 and batch: 200, loss is 6.281964445114136 and perplexity is 534.8382929900894
At time: 145.25254821777344 and batch: 250, loss is 6.375638723373413 and perplexity is 587.3604720173045
At time: 146.12442016601562 and batch: 300, loss is 6.317163553237915 and perplexity is 553.9993720361655
At time: 146.99544167518616 and batch: 350, loss is 6.340333890914917 and perplexity is 566.9855911182085
At time: 147.8755202293396 and batch: 400, loss is 6.286751232147217 and perplexity is 537.4045872476131
At time: 148.75052857398987 and batch: 450, loss is 6.288241920471191 and perplexity is 538.2062873851322
At time: 149.6335575580597 and batch: 500, loss is 6.28172399520874 and perplexity is 534.7097066330458
At time: 150.51538825035095 and batch: 550, loss is 6.318087692260742 and perplexity is 554.5115811142308
At time: 151.3919312953949 and batch: 600, loss is 6.298505916595459 and perplexity is 543.7588817897454
At time: 152.26490592956543 and batch: 650, loss is 6.233732528686524 and perplexity is 509.65423677480703
At time: 153.13956761360168 and batch: 700, loss is 6.322564744949341 and perplexity is 556.9997242973541
At time: 154.01469254493713 and batch: 750, loss is 6.298299932479859 and perplexity is 543.646887632285
At time: 154.887793302536 and batch: 800, loss is 6.308807773590088 and perplexity is 549.3895614601123
At time: 155.76186180114746 and batch: 850, loss is 6.3425044918060305 and perplexity is 568.2176271931102
At time: 156.6379017829895 and batch: 900, loss is 6.266807184219361 and perplexity is 526.7927377697611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.356295650952483 and perplexity of 576.1082925664517
finished 9 epochs...
Completing Train Step...
At time: 158.7047438621521 and batch: 50, loss is 6.3225663375854495 and perplexity is 557.0006113959337
At time: 159.58278584480286 and batch: 100, loss is 6.296346988677978 and perplexity is 542.5862118698672
At time: 160.48281121253967 and batch: 150, loss is 6.3322311210632325 and perplexity is 562.4099998796571
At time: 161.35744500160217 and batch: 200, loss is 6.280159664154053 and perplexity is 533.8738975452178
At time: 162.23273038864136 and batch: 250, loss is 6.373165016174316 and perplexity is 585.9093098044161
At time: 163.11482524871826 and batch: 300, loss is 6.316234579086304 and perplexity is 553.484959914312
At time: 163.99482774734497 and batch: 350, loss is 6.339746189117432 and perplexity is 566.6524705645222
At time: 164.8761522769928 and batch: 400, loss is 6.285783567428589 and perplexity is 536.8848113138799
At time: 165.75509405136108 and batch: 450, loss is 6.2873288345336915 and perplexity is 537.715083082645
At time: 166.6439914703369 and batch: 500, loss is 6.281072797775269 and perplexity is 534.3616183938193
At time: 167.52444529533386 and batch: 550, loss is 6.317597742080689 and perplexity is 554.2399646099332
At time: 168.40414237976074 and batch: 600, loss is 6.298724746704101 and perplexity is 543.877885625259
At time: 169.29080486297607 and batch: 650, loss is 6.234901008605957 and perplexity is 510.2501055789012
At time: 170.1675136089325 and batch: 700, loss is 6.324029340744018 and perplexity is 557.8161014365285
At time: 171.04744052886963 and batch: 750, loss is 6.299497518539429 and perplexity is 544.2983415744617
At time: 171.923095703125 and batch: 800, loss is 6.309872398376465 and perplexity is 549.9747666613267
At time: 172.80115294456482 and batch: 850, loss is 6.343956136703492 and perplexity is 569.043076396881
At time: 173.68226528167725 and batch: 900, loss is 6.267460079193115 and perplexity is 527.1367904033572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355978351749786 and perplexity of 575.9255228624273
finished 10 epochs...
Completing Train Step...
At time: 175.77185726165771 and batch: 50, loss is 6.321505537033081 and perplexity is 556.4100581246927
At time: 176.64868593215942 and batch: 100, loss is 6.295288677215576 and perplexity is 542.0122904099009
At time: 177.5271623134613 and batch: 150, loss is 6.331193561553955 and perplexity is 561.826768657042
At time: 178.41413593292236 and batch: 200, loss is 6.279185571670532 and perplexity is 533.3541081970201
At time: 179.29816007614136 and batch: 250, loss is 6.371844415664673 and perplexity is 585.1360683551862
At time: 180.1793613433838 and batch: 300, loss is 6.31587067604065 and perplexity is 553.2835816949712
At time: 181.05698156356812 and batch: 350, loss is 6.3395677661895755 and perplexity is 566.5513757907281
At time: 181.93587017059326 and batch: 400, loss is 6.285334739685059 and perplexity is 536.6438965841338
At time: 182.827819108963 and batch: 450, loss is 6.286928415298462 and perplexity is 537.499814721978
At time: 183.70606803894043 and batch: 500, loss is 6.280804796218872 and perplexity is 534.2184278369141
At time: 184.5892083644867 and batch: 550, loss is 6.317378244400024 and perplexity is 554.118323573621
At time: 185.4687740802765 and batch: 600, loss is 6.298962202072143 and perplexity is 544.0070476832652
At time: 186.34641027450562 and batch: 650, loss is 6.2357361793518065 and perplexity is 510.6764295420181
At time: 187.22334933280945 and batch: 700, loss is 6.32500846862793 and perplexity is 558.3625422095831
At time: 188.1147689819336 and batch: 750, loss is 6.30023678779602 and perplexity is 544.7008733761617
At time: 188.9927577972412 and batch: 800, loss is 6.310481424331665 and perplexity is 550.3098175859274
At time: 189.87097215652466 and batch: 850, loss is 6.344750938415527 and perplexity is 569.4955325908835
At time: 190.75010228157043 and batch: 900, loss is 6.2678008556365965 and perplexity is 527.3164568153207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355831198496361 and perplexity of 575.8407797832704
finished 11 epochs...
Completing Train Step...
At time: 192.83921194076538 and batch: 50, loss is 6.320859937667847 and perplexity is 556.0509560748578
At time: 193.7309606075287 and batch: 100, loss is 6.294629135131836 and perplexity is 541.6549283550825
At time: 194.61014604568481 and batch: 150, loss is 6.33053409576416 and perplexity is 561.4563852643331
At time: 195.48821234703064 and batch: 200, loss is 6.2785766983032225 and perplexity is 533.0294619294564
At time: 196.3681788444519 and batch: 250, loss is 6.3710694503784175 and perplexity is 584.6827838770287
At time: 197.2511875629425 and batch: 300, loss is 6.315692310333252 and perplexity is 553.1849036781795
At time: 198.14443635940552 and batch: 350, loss is 6.339513626098633 and perplexity is 566.5207034780274
At time: 199.0250267982483 and batch: 400, loss is 6.285084714889527 and perplexity is 536.5097390756675
At time: 199.90521693229675 and batch: 450, loss is 6.286705722808838 and perplexity is 537.3801308769067
At time: 200.78392148017883 and batch: 500, loss is 6.280664596557617 and perplexity is 534.143535844334
At time: 201.6813359260559 and batch: 550, loss is 6.317246713638306 and perplexity is 554.0454447614469
At time: 202.56575870513916 and batch: 600, loss is 6.299146699905395 and perplexity is 544.1074250642565
At time: 203.44501566886902 and batch: 650, loss is 6.236333532333374 and perplexity is 510.98157476045054
At time: 204.3234829902649 and batch: 700, loss is 6.3256988620758055 and perplexity is 558.7481651507227
At time: 205.2029812335968 and batch: 750, loss is 6.300738725662232 and perplexity is 544.9743479981599
At time: 206.08158087730408 and batch: 800, loss is 6.310868873596191 and perplexity is 550.5230760307584
At time: 206.96741032600403 and batch: 850, loss is 6.345234079360962 and perplexity is 569.7707456789227
At time: 207.84921145439148 and batch: 900, loss is 6.2680012893676755 and perplexity is 527.4221594130488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355755949673587 and perplexity of 575.7974500727618
finished 12 epochs...
Completing Train Step...
At time: 209.93416261672974 and batch: 50, loss is 6.320423784255982 and perplexity is 555.8084854342374
At time: 210.81920051574707 and batch: 100, loss is 6.294181146621704 and perplexity is 541.4123275159747
At time: 211.69854617118835 and batch: 150, loss is 6.330081844329834 and perplexity is 561.2025232178501
At time: 212.57509660720825 and batch: 200, loss is 6.278164377212525 and perplexity is 532.8097279439309
At time: 213.46142435073853 and batch: 250, loss is 6.370570802688599 and perplexity is 584.3913058360372
At time: 214.3455889225006 and batch: 300, loss is 6.315593090057373 and perplexity is 553.1300192422939
At time: 215.2230408191681 and batch: 350, loss is 6.339507656097412 and perplexity is 566.5173213588319
At time: 216.10340309143066 and batch: 400, loss is 6.2849302577972415 and perplexity is 536.4268777408124
At time: 216.98278522491455 and batch: 450, loss is 6.286563348770142 and perplexity is 537.3036273435463
At time: 217.8621804714203 and batch: 500, loss is 6.280580177307129 and perplexity is 534.0984457506473
At time: 218.740296125412 and batch: 550, loss is 6.317155485153198 and perplexity is 553.9949023403299
At time: 219.62115502357483 and batch: 600, loss is 6.299284610748291 and perplexity is 544.1824685524094
At time: 220.50147604942322 and batch: 650, loss is 6.236772928237915 and perplexity is 511.2061473062116
At time: 221.38070106506348 and batch: 700, loss is 6.326209897994995 and perplexity is 559.033778506022
At time: 222.2584192752838 and batch: 750, loss is 6.301101922988892 and perplexity is 545.1723171732126
At time: 223.15032601356506 and batch: 800, loss is 6.31113335609436 and perplexity is 550.6686990057219
At time: 224.03058075904846 and batch: 850, loss is 6.3455494785308835 and perplexity is 569.9504792415737
At time: 224.91098642349243 and batch: 900, loss is 6.268127927780151 and perplexity is 527.4889555474102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.35571748916417 and perplexity of 575.7753050353683
finished 13 epochs...
Completing Train Step...
At time: 227.00204420089722 and batch: 50, loss is 6.320111236572266 and perplexity is 555.634795924066
At time: 227.87779116630554 and batch: 100, loss is 6.293860197067261 and perplexity is 541.2385893527727
At time: 228.76170468330383 and batch: 150, loss is 6.329756069183349 and perplexity is 561.019727160464
At time: 229.63974022865295 and batch: 200, loss is 6.277869920730591 and perplexity is 532.652861762166
At time: 230.5202658176422 and batch: 250, loss is 6.370225391387939 and perplexity is 584.1894853325422
At time: 231.41262006759644 and batch: 300, loss is 6.315533971786499 and perplexity is 553.097320118555
At time: 232.28965973854065 and batch: 350, loss is 6.339522523880005 and perplexity is 566.5257442778156
At time: 233.16911125183105 and batch: 400, loss is 6.284827613830567 and perplexity is 536.3718195839923
At time: 234.04813265800476 and batch: 450, loss is 6.286464443206787 and perplexity is 537.2504876535403
At time: 234.92593383789062 and batch: 500, loss is 6.280524206161499 and perplexity is 534.0685524853477
At time: 235.803391456604 and batch: 550, loss is 6.317086668014526 and perplexity is 553.956779308086
At time: 236.69489336013794 and batch: 600, loss is 6.299388980865478 and perplexity is 544.2392679044493
At time: 237.57613492012024 and batch: 650, loss is 6.237106533050537 and perplexity is 511.3767165869793
At time: 238.45483922958374 and batch: 700, loss is 6.326601963043213 and perplexity is 559.2529990829024
At time: 239.33387088775635 and batch: 750, loss is 6.301376991271972 and perplexity is 545.3222974129384
At time: 240.20854902267456 and batch: 800, loss is 6.311322984695434 and perplexity is 550.7731314421451
At time: 241.08684086799622 and batch: 850, loss is 6.345766792297363 and perplexity is 570.074350785933
At time: 241.96661925315857 and batch: 900, loss is 6.2682127761840825 and perplexity is 527.5337140421964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.35569825890946 and perplexity of 575.764232836058
finished 14 epochs...
Completing Train Step...
At time: 244.05266046524048 and batch: 50, loss is 6.319877796173095 and perplexity is 555.5051034538326
At time: 244.93511128425598 and batch: 100, loss is 6.293621063232422 and perplexity is 541.1091763674664
At time: 245.81276035308838 and batch: 150, loss is 6.32951325416565 and perplexity is 560.8835196827634
At time: 246.69000697135925 and batch: 200, loss is 6.27765157699585 and perplexity is 532.5365730429268
At time: 247.5683159828186 and batch: 250, loss is 6.369971981048584 and perplexity is 584.0414644326216
At time: 248.44716691970825 and batch: 300, loss is 6.3154979419708255 and perplexity is 553.0773924830585
At time: 249.32324147224426 and batch: 350, loss is 6.339546146392823 and perplexity is 566.53912719754
At time: 250.2074592113495 and batch: 400, loss is 6.284755840301513 and perplexity is 536.3333236671264
At time: 251.08585023880005 and batch: 450, loss is 6.286392526626587 and perplexity is 537.2118518250522
At time: 251.96382188796997 and batch: 500, loss is 6.280484409332275 and perplexity is 534.0472986732905
At time: 252.84123873710632 and batch: 550, loss is 6.317032270431518 and perplexity is 553.9266462177918
At time: 253.7189643383026 and batch: 600, loss is 6.299469547271729 and perplexity is 544.283117072766
At time: 254.59684467315674 and batch: 650, loss is 6.237365884780884 and perplexity is 511.50936022322
At time: 255.4746789932251 and batch: 700, loss is 6.326911611557007 and perplexity is 559.4261977568751
At time: 256.3536114692688 and batch: 750, loss is 6.301591329574585 and perplexity is 545.4391933957382
At time: 257.22976875305176 and batch: 800, loss is 6.311464023590088 and perplexity is 550.8508173540475
At time: 258.10749316215515 and batch: 850, loss is 6.345922927856446 and perplexity is 570.1633666124987
At time: 258.9871962070465 and batch: 900, loss is 6.268272371292114 and perplexity is 527.5651534076819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355685717438998 and perplexity of 575.757011951219
finished 15 epochs...
Completing Train Step...
At time: 261.05933380126953 and batch: 50, loss is 6.319698238372803 and perplexity is 555.4053671338903
At time: 261.9445495605469 and batch: 100, loss is 6.293437414169311 and perplexity is 541.0098112986161
At time: 262.8229923248291 and batch: 150, loss is 6.329327287673951 and perplexity is 560.7792238404227
At time: 263.70143127441406 and batch: 200, loss is 6.277484312057495 and perplexity is 532.4475057949865
At time: 264.5801508426666 and batch: 250, loss is 6.36977861404419 and perplexity is 583.9285410023867
At time: 265.46020674705505 and batch: 300, loss is 6.315475635528564 and perplexity is 553.0650554317353
At time: 266.33986949920654 and batch: 350, loss is 6.339573278427124 and perplexity is 566.5544987651019
At time: 267.22074151039124 and batch: 400, loss is 6.284704313278199 and perplexity is 536.3056887194327
At time: 268.10099482536316 and batch: 450, loss is 6.2863379573822025 and perplexity is 537.1825373800646
At time: 268.9799430370331 and batch: 500, loss is 6.280454912185669 and perplexity is 534.031546034157
At time: 269.8587076663971 and batch: 550, loss is 6.316988134384156 and perplexity is 553.902198624613
At time: 270.73896956443787 and batch: 600, loss is 6.299533157348633 and perplexity is 544.3177400648747
At time: 271.6176552772522 and batch: 650, loss is 6.237571907043457 and perplexity is 511.6147533952378
At time: 272.5115282535553 and batch: 700, loss is 6.32716157913208 and perplexity is 559.5660536459515
At time: 273.39225697517395 and batch: 750, loss is 6.301762790679931 and perplexity is 545.5327230208546
At time: 274.26906061172485 and batch: 800, loss is 6.311571321487427 and perplexity is 550.9099256595387
At time: 275.14694714546204 and batch: 850, loss is 6.346038599014282 and perplexity is 570.2293218837583
At time: 276.02809739112854 and batch: 900, loss is 6.268315162658691 and perplexity is 527.5877291245743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355677774507705 and perplexity of 575.7524387709938
finished 16 epochs...
Completing Train Step...
At time: 278.11720848083496 and batch: 50, loss is 6.319557065963745 and perplexity is 555.3269647544649
At time: 278.995721578598 and batch: 100, loss is 6.293293495178222 and perplexity is 540.9319553150165
At time: 279.87269115448 and batch: 150, loss is 6.3291819190979 and perplexity is 560.6977100880869
At time: 280.7496452331543 and batch: 200, loss is 6.277353763580322 and perplexity is 532.3780001209593
At time: 281.64097261428833 and batch: 250, loss is 6.369627265930176 and perplexity is 583.8401712064568
At time: 282.523455619812 and batch: 300, loss is 6.315461568832397 and perplexity is 553.0572756883578
At time: 283.40177154541016 and batch: 350, loss is 6.3396006774902345 and perplexity is 566.57002204023
At time: 284.2809708118439 and batch: 400, loss is 6.2846657371521 and perplexity is 536.2850005225949
At time: 285.15873289108276 and batch: 450, loss is 6.286295671463012 and perplexity is 537.1598226029596
At time: 286.03634095191956 and batch: 500, loss is 6.280432386398315 and perplexity is 534.0195166885967
At time: 286.92683005332947 and batch: 550, loss is 6.316951284408569 and perplexity is 553.8817877181893
At time: 287.8161356449127 and batch: 600, loss is 6.299583892822266 and perplexity is 544.3453569837965
At time: 288.6934697628021 and batch: 650, loss is 6.237738409042358 and perplexity is 511.69994536646544
At time: 289.57101917266846 and batch: 700, loss is 6.327366876602173 and perplexity is 559.6809429339511
At time: 290.45013642311096 and batch: 750, loss is 6.301902132034302 and perplexity is 545.608743585615
At time: 291.3274927139282 and batch: 800, loss is 6.311655197143555 and perplexity is 550.9561355289353
At time: 292.2189223766327 and batch: 850, loss is 6.346127223968506 and perplexity is 570.2798606707734
At time: 293.09698724746704 and batch: 900, loss is 6.268347082138061 and perplexity is 527.6045697189799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355674430115582 and perplexity of 575.7505132322925
finished 17 epochs...
Completing Train Step...
At time: 295.18076395988464 and batch: 50, loss is 6.319444007873535 and perplexity is 555.2641840973853
At time: 296.06404185295105 and batch: 100, loss is 6.293177900314331 and perplexity is 540.8694299731411
At time: 296.947536945343 and batch: 150, loss is 6.329066200256348 and perplexity is 560.6328305525791
At time: 297.8277814388275 and batch: 200, loss is 6.277248649597168 and perplexity is 532.3220426898288
At time: 298.71106338500977 and batch: 250, loss is 6.369506015777588 and perplexity is 583.7693847881295
At time: 299.5966737270355 and batch: 300, loss is 6.31545334815979 and perplexity is 553.0527292042487
At time: 300.474680185318 and batch: 350, loss is 6.339627485275269 and perplexity is 566.5852107311745
At time: 301.3539295196533 and batch: 400, loss is 6.28463659286499 and perplexity is 536.2693711063222
At time: 302.231796503067 and batch: 450, loss is 6.286262149810791 and perplexity is 537.1418164199993
At time: 303.12026143074036 and batch: 500, loss is 6.280414571762085 and perplexity is 534.010003409905
At time: 304.0110445022583 and batch: 550, loss is 6.3169205856323245 and perplexity is 553.8647844861129
At time: 304.8920969963074 and batch: 600, loss is 6.299625310897827 and perplexity is 544.3679031878305
At time: 305.76994132995605 and batch: 650, loss is 6.237874603271484 and perplexity is 511.7696406920111
At time: 306.64848232269287 and batch: 700, loss is 6.32753776550293 and perplexity is 559.7765943677153
At time: 307.52877593040466 and batch: 750, loss is 6.3020164680480955 and perplexity is 545.6711298808797
At time: 308.4102487564087 and batch: 800, loss is 6.311721487045288 and perplexity is 550.9926595675929
At time: 309.29243516921997 and batch: 850, loss is 6.346196393966675 and perplexity is 570.3193082919721
At time: 310.1795711517334 and batch: 900, loss is 6.268371372222901 and perplexity is 527.6173854343868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355672339870505 and perplexity of 575.7493097738746
finished 18 epochs...
Completing Train Step...
At time: 312.26138257980347 and batch: 50, loss is 6.319351930618286 and perplexity is 555.2130592491291
At time: 313.1447479724884 and batch: 100, loss is 6.293084020614624 and perplexity is 540.818655696849
At time: 314.02288722991943 and batch: 150, loss is 6.32897292137146 and perplexity is 560.5805377862573
At time: 314.89973616600037 and batch: 200, loss is 6.2771639633178715 and perplexity is 532.2769642254364
At time: 315.7773723602295 and batch: 250, loss is 6.369407243728638 and perplexity is 583.7117275373893
At time: 316.6641173362732 and batch: 300, loss is 6.315448684692383 and perplexity is 553.0501500668856
At time: 317.5418417453766 and batch: 350, loss is 6.33965238571167 and perplexity is 566.5993191258318
At time: 318.4192044734955 and batch: 400, loss is 6.284613857269287 and perplexity is 536.2571788413126
At time: 319.2968068122864 and batch: 450, loss is 6.286235074996949 and perplexity is 537.1272736021856
At time: 320.17437720298767 and batch: 500, loss is 6.2803999614715575 and perplexity is 534.0022014256053
At time: 321.05083990097046 and batch: 550, loss is 6.316894283294678 and perplexity is 553.8502167391244
At time: 321.9400134086609 and batch: 600, loss is 6.299659614562988 and perplexity is 544.3865773223995
At time: 322.81758546829224 and batch: 650, loss is 6.237987279891968 and perplexity is 511.82730841443146
At time: 323.6960084438324 and batch: 700, loss is 6.327681198120117 and perplexity is 559.8568903480791
At time: 324.5746216773987 and batch: 750, loss is 6.3021120357513425 and perplexity is 545.723280909428
At time: 325.4501004219055 and batch: 800, loss is 6.311774806976318 and perplexity is 551.0220392414535
At time: 326.32796239852905 and batch: 850, loss is 6.346251468658448 and perplexity is 570.3507193170565
At time: 327.20809507369995 and batch: 900, loss is 6.268389902114868 and perplexity is 527.6271622181202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355671921821489 and perplexity of 575.7490690824926
finished 19 epochs...
Completing Train Step...
At time: 329.2946274280548 and batch: 50, loss is 6.319276247024536 and perplexity is 555.1710403196004
At time: 330.17340302467346 and batch: 100, loss is 6.293006820678711 and perplexity is 540.7769061428406
At time: 331.05311012268066 and batch: 150, loss is 6.328896522521973 and perplexity is 560.5377117140774
At time: 331.93129873275757 and batch: 200, loss is 6.2770940208435055 and perplexity is 532.2397367594161
At time: 332.81102681159973 and batch: 250, loss is 6.369326105117798 and perplexity is 583.6643679000595
At time: 333.69295716285706 and batch: 300, loss is 6.315446643829346 and perplexity is 553.0490213684286
At time: 334.57319378852844 and batch: 350, loss is 6.3396752834320065 and perplexity is 566.6122931071209
At time: 335.46135997772217 and batch: 400, loss is 6.284596405029297 and perplexity is 536.2478200339974
At time: 336.3465232849121 and batch: 450, loss is 6.286212644577026 and perplexity is 537.1152257470068
At time: 337.2312524318695 and batch: 500, loss is 6.280388240814209 and perplexity is 533.9959426054578
At time: 338.1258752346039 and batch: 550, loss is 6.31687162399292 and perplexity is 553.8376670221193
At time: 339.0062050819397 and batch: 600, loss is 6.299687805175782 and perplexity is 544.4019241299277
At time: 339.88457798957825 and batch: 650, loss is 6.238081483840943 and perplexity is 511.8755268392246
At time: 340.76550364494324 and batch: 700, loss is 6.327802982330322 and perplexity is 559.9250762291957
At time: 341.64634108543396 and batch: 750, loss is 6.302192077636719 and perplexity is 545.7669633779161
At time: 342.5229842662811 and batch: 800, loss is 6.311818141937255 and perplexity is 551.0459182773942
At time: 343.40525555610657 and batch: 850, loss is 6.3462958335876465 and perplexity is 570.3760234476413
At time: 344.29494166374207 and batch: 900, loss is 6.268404579162597 and perplexity is 527.6349062839931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355668159380351 and perplexity of 575.746902864585
Finished Training.
Improved accuracyfrom -10000000 to -575.746902864585
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f05edd19b70>
ELAPSED
356.01671171188354


RESULTS SO FAR:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3143906593322754 and batch: 50, loss is 7.027668523788452 and perplexity is 1127.3990396145468
At time: 2.4209113121032715 and batch: 100, loss is 6.433479928970337 and perplexity is 622.3358687846929
At time: 3.5271308422088623 and batch: 150, loss is 6.062496442794799 and perplexity is 429.4461876100049
At time: 4.633688926696777 and batch: 200, loss is 5.872824468612671 and perplexity is 355.25095975729397
At time: 5.738358974456787 and batch: 250, loss is 5.939305801391601 and perplexity is 379.6712707651394
At time: 6.845236778259277 and batch: 300, loss is 5.850648756027222 and perplexity is 347.4597239644974
At time: 7.953522443771362 and batch: 350, loss is 5.798781986236572 and perplexity is 329.8974954093793
At time: 9.061075448989868 and batch: 400, loss is 5.6387636184692385 and perplexity is 281.11493820741185
At time: 10.169082880020142 and batch: 450, loss is 5.629893207550049 and perplexity is 278.6323602117517
At time: 11.274745225906372 and batch: 500, loss is 5.570998067855835 and perplexity is 262.6961570329815
At time: 12.376481771469116 and batch: 550, loss is 5.609685106277466 and perplexity is 273.0582401396672
At time: 13.478190422058105 and batch: 600, loss is 5.52549144744873 and perplexity is 251.00966577365867
At time: 14.58045244216919 and batch: 650, loss is 5.42000973701477 and perplexity is 225.8813219010932
At time: 15.681886196136475 and batch: 700, loss is 5.506786222457886 and perplexity is 246.35811329341928
At time: 16.791401147842407 and batch: 750, loss is 5.47004825592041 and perplexity is 237.47165189781268
At time: 17.896415948867798 and batch: 800, loss is 5.457977161407471 and perplexity is 234.62234089281827
At time: 19.004610776901245 and batch: 850, loss is 5.480039091110229 and perplexity is 239.8560834315904
At time: 20.112231731414795 and batch: 900, loss is 5.365813703536987 and perplexity is 213.96526816100345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.31343099515732 and perplexity of 203.0456834110123
finished 1 epochs...
Completing Train Step...
At time: 22.349069118499756 and batch: 50, loss is 5.223224563598633 and perplexity is 185.53147859833018
At time: 23.227832794189453 and batch: 100, loss is 5.086994190216064 and perplexity is 161.90248187033055
At time: 24.106176376342773 and batch: 150, loss is 5.057139806747436 and perplexity is 157.1404209785863
At time: 24.9853355884552 and batch: 200, loss is 4.922975988388061 and perplexity is 137.41093865701401
At time: 25.864352703094482 and batch: 250, loss is 4.993109254837036 and perplexity is 147.39399726928852
At time: 26.743391275405884 and batch: 300, loss is 4.920754585266113 and perplexity is 137.1060323541814
At time: 27.622578859329224 and batch: 350, loss is 4.901871929168701 and perplexity is 134.541396071138
At time: 28.501041889190674 and batch: 400, loss is 4.757479906082153 and perplexity is 116.45208560927381
At time: 29.38093066215515 and batch: 450, loss is 4.769762010574341 and perplexity is 117.89118176714936
At time: 30.267256498336792 and batch: 500, loss is 4.675710783004761 and perplexity is 107.30881327232494
At time: 31.14764142036438 and batch: 550, loss is 4.740151376724243 and perplexity is 114.4515256659414
At time: 32.028618812561035 and batch: 600, loss is 4.666976337432861 and perplexity is 106.37561171537934
At time: 32.907227993011475 and batch: 650, loss is 4.54187084197998 and perplexity is 93.86624486500087
At time: 33.78707146644592 and batch: 700, loss is 4.593558597564697 and perplexity is 98.8455565895378
At time: 34.66670107841492 and batch: 750, loss is 4.616568508148194 and perplexity is 101.14635305540507
At time: 35.54494380950928 and batch: 800, loss is 4.570471086502075 and perplexity is 96.58960111415064
At time: 36.422521352767944 and batch: 850, loss is 4.616401596069336 and perplexity is 101.12947191621991
At time: 37.30102849006653 and batch: 900, loss is 4.5362362289428715 and perplexity is 93.33883217654066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.645927533711473 and perplexity of 104.15993284298806
finished 2 epochs...
Completing Train Step...
At time: 39.4049608707428 and batch: 50, loss is 4.588871822357178 and perplexity is 98.38337360543646
At time: 40.29372239112854 and batch: 100, loss is 4.462046918869018 and perplexity is 86.66472332032706
At time: 41.174697399139404 and batch: 150, loss is 4.468342609405518 and perplexity is 87.21205871809038
At time: 42.05687189102173 and batch: 200, loss is 4.3523179721832275 and perplexity is 77.65826415373797
At time: 42.937721729278564 and batch: 250, loss is 4.488137855529785 and perplexity is 88.95564329313319
At time: 43.82725429534912 and batch: 300, loss is 4.438008141517639 and perplexity is 84.60625004786007
At time: 44.7095000743866 and batch: 350, loss is 4.44539662361145 and perplexity is 85.2336768228603
At time: 45.59091401100159 and batch: 400, loss is 4.345727310180664 and perplexity is 77.14812769803
At time: 46.472784757614136 and batch: 450, loss is 4.374624042510987 and perplexity is 79.40997916312581
At time: 47.35497522354126 and batch: 500, loss is 4.25368369102478 and perplexity is 70.36413526001135
At time: 48.2349750995636 and batch: 550, loss is 4.333088808059692 and perplexity is 76.17922654908988
At time: 49.11346387863159 and batch: 600, loss is 4.310269460678101 and perplexity is 74.46055042774309
At time: 49.992799043655396 and batch: 650, loss is 4.168441214561462 and perplexity is 64.6146531853658
At time: 50.88482308387756 and batch: 700, loss is 4.19372344493866 and perplexity is 66.26908142719525
At time: 51.76648545265198 and batch: 750, loss is 4.2735566139221195 and perplexity is 71.7764633575159
At time: 52.64511489868164 and batch: 800, loss is 4.235104007720947 and perplexity is 69.068862054882
At time: 53.52258014678955 and batch: 850, loss is 4.300533561706543 and perplexity is 73.73912757988703
At time: 54.40136480331421 and batch: 900, loss is 4.234583911895752 and perplexity is 69.03294896800905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.453017979452055 and perplexity of 85.88575470599284
finished 3 epochs...
Completing Train Step...
At time: 56.498578786849976 and batch: 50, loss is 4.310960807800293 and perplexity is 74.51204631371336
At time: 57.39106845855713 and batch: 100, loss is 4.188829822540283 and perplexity is 65.94557776398224
At time: 58.27253174781799 and batch: 150, loss is 4.190535364151001 and perplexity is 66.05814665908147
At time: 59.15636134147644 and batch: 200, loss is 4.070231599807739 and perplexity is 58.570525943744656
At time: 60.037288665771484 and batch: 250, loss is 4.221653261184692 and perplexity is 68.14605443654175
At time: 60.91978716850281 and batch: 300, loss is 4.1813726806640625 and perplexity is 65.45564126787083
At time: 61.801084756851196 and batch: 350, loss is 4.190870957374573 and perplexity is 66.08031904570527
At time: 62.68272352218628 and batch: 400, loss is 4.1084441614151 and perplexity is 60.85196804140319
At time: 63.56276607513428 and batch: 450, loss is 4.140296335220337 and perplexity is 62.82143489490716
At time: 64.44432425498962 and batch: 500, loss is 4.013283400535584 and perplexity is 55.328237414877464
At time: 65.32568836212158 and batch: 550, loss is 4.090922584533692 and perplexity is 59.79503223626579
At time: 66.21657991409302 and batch: 600, loss is 4.090128045082093 and perplexity is 59.74754159323756
At time: 67.09899401664734 and batch: 650, loss is 3.9416434717178346 and perplexity is 51.50317579639464
At time: 67.9825439453125 and batch: 700, loss is 3.957963352203369 and perplexity is 52.3505975727358
At time: 68.86520624160767 and batch: 750, loss is 4.057063593864441 and perplexity is 57.804324652936415
At time: 69.74785566329956 and batch: 800, loss is 4.0206231594085695 and perplexity is 55.7358273123459
At time: 70.6290237903595 and batch: 850, loss is 4.093260750770569 and perplexity is 59.93500653959847
At time: 71.51103806495667 and batch: 900, loss is 4.035160789489746 and perplexity is 56.55201247656542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368030704864084 and perplexity of 78.88812463004743
finished 4 epochs...
Completing Train Step...
At time: 73.6057300567627 and batch: 50, loss is 4.119235286712646 and perplexity is 61.512185088826406
At time: 74.48509383201599 and batch: 100, loss is 3.99811550617218 and perplexity is 54.495357043194815
At time: 75.3650586605072 and batch: 150, loss is 3.997907991409302 and perplexity is 54.48404962536868
At time: 76.24666929244995 and batch: 200, loss is 3.8798744821548463 and perplexity is 48.41813734965541
At time: 77.12808442115784 and batch: 250, loss is 4.034070019721985 and perplexity is 56.490360881010865
At time: 78.00853824615479 and batch: 300, loss is 3.997135992050171 and perplexity is 54.442004205581796
At time: 78.88982558250427 and batch: 350, loss is 4.002474241256714 and perplexity is 54.73340628779747
At time: 79.77197360992432 and batch: 400, loss is 3.9338276624679565 and perplexity is 51.10220579230547
At time: 80.65441966056824 and batch: 450, loss is 3.96459144115448 and perplexity is 52.698734456693195
At time: 81.53585696220398 and batch: 500, loss is 3.8390062284469604 and perplexity is 46.47926171304913
At time: 82.41762351989746 and batch: 550, loss is 3.911069164276123 and perplexity is 49.95233068049234
At time: 83.29956126213074 and batch: 600, loss is 3.9265294027328492 and perplexity is 50.730606285678036
At time: 84.18003749847412 and batch: 650, loss is 3.7735969495773314 and perplexity is 43.536381705139796
At time: 85.05973029136658 and batch: 700, loss is 3.7903804206848144 and perplexity is 44.27323952888076
At time: 85.93855857849121 and batch: 750, loss is 3.8917287683486936 and perplexity is 48.9955152469126
At time: 86.81751418113708 and batch: 800, loss is 3.8579455375671388 and perplexity is 47.36793569029411
At time: 87.7027223110199 and batch: 850, loss is 3.931905026435852 and perplexity is 51.00404924002782
At time: 88.58213329315186 and batch: 900, loss is 3.8803494644165037 and perplexity is 48.44114056866701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.343677259471319 and perplexity of 76.99013212891631
finished 5 epochs...
Completing Train Step...
At time: 90.68871450424194 and batch: 50, loss is 3.9654184341430665 and perplexity is 52.74233396635579
At time: 91.57340836524963 and batch: 100, loss is 3.8481387758255003 and perplexity is 46.905679951421384
At time: 92.44998836517334 and batch: 150, loss is 3.8482346820831297 and perplexity is 46.91017871537344
At time: 93.33097791671753 and batch: 200, loss is 3.732246870994568 and perplexity is 41.772861029097044
At time: 94.21229887008667 and batch: 250, loss is 3.8864185571670533 and perplexity is 48.73602828902907
At time: 95.09364748001099 and batch: 300, loss is 3.8501002645492552 and perplexity is 46.99777520610037
At time: 95.97446131706238 and batch: 350, loss is 3.8543695497512815 and perplexity is 47.19885103192586
At time: 96.85989451408386 and batch: 400, loss is 3.7914463233947755 and perplexity is 44.32045565430099
At time: 97.74280405044556 and batch: 450, loss is 3.8241279220581053 and perplexity is 45.79284801543274
At time: 98.62398028373718 and batch: 500, loss is 3.7013143920898437 and perplexity is 40.500502931205375
At time: 99.50309824943542 and batch: 550, loss is 3.7677078247070312 and perplexity is 43.28074399690058
At time: 100.39025807380676 and batch: 600, loss is 3.7901338148117065 and perplexity is 44.26232283410727
At time: 101.27108383178711 and batch: 650, loss is 3.640816249847412 and perplexity is 38.1229418743733
At time: 102.14757323265076 and batch: 700, loss is 3.651410474777222 and perplexity is 38.52897188455877
At time: 103.02785181999207 and batch: 750, loss is 3.7566856002807616 and perplexity is 42.806313366852926
At time: 103.90821290016174 and batch: 800, loss is 3.7228827619552614 and perplexity is 41.38352115901978
At time: 104.78748846054077 and batch: 850, loss is 3.7973715686798095 and perplexity is 44.583844777176424
At time: 105.6806206703186 and batch: 900, loss is 3.751589159965515 and perplexity is 42.58870852171591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350680834626498 and perplexity of 77.53123090672439
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 107.76504397392273 and batch: 50, loss is 3.8597409868240358 and perplexity is 47.453058809483366
At time: 108.66384625434875 and batch: 100, loss is 3.7467757987976076 and perplexity is 42.38420625240025
At time: 109.54307198524475 and batch: 150, loss is 3.7485922050476073 and perplexity is 42.461263151668746
At time: 110.42736053466797 and batch: 200, loss is 3.621093988418579 and perplexity is 37.37843703997837
At time: 111.30756783485413 and batch: 250, loss is 3.76801167011261 and perplexity is 43.293896650199514
At time: 112.18726205825806 and batch: 300, loss is 3.7258522796630857 and perplexity is 41.50659289933214
At time: 113.06712460517883 and batch: 350, loss is 3.7178168869018555 and perplexity is 41.17440753052347
At time: 113.94726419448853 and batch: 400, loss is 3.6439651584625246 and perplexity is 38.24317673942584
At time: 114.82883787155151 and batch: 450, loss is 3.6713012075424194 and perplexity is 39.303013980188695
At time: 115.71003246307373 and batch: 500, loss is 3.5452881336212156 and perplexity is 34.649667638284775
At time: 116.59048461914062 and batch: 550, loss is 3.588234691619873 and perplexity is 36.170168033763
At time: 117.4706163406372 and batch: 600, loss is 3.606483244895935 and perplexity is 36.8362805822367
At time: 118.35106611251831 and batch: 650, loss is 3.443790988922119 and perplexity is 31.305411967896973
At time: 119.23048496246338 and batch: 700, loss is 3.438830518722534 and perplexity is 31.150506923309287
At time: 120.11001992225647 and batch: 750, loss is 3.536209697723389 and perplexity is 34.33652641869718
At time: 120.99114108085632 and batch: 800, loss is 3.481754808425903 and perplexity is 32.51673267637643
At time: 121.87095189094543 and batch: 850, loss is 3.5389595890045165 and perplexity is 34.43107807712821
At time: 122.75075721740723 and batch: 900, loss is 3.4895282220840453 and perplexity is 32.77048366823308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3113950964522685 and perplexity of 74.54441307761329
finished 7 epochs...
Completing Train Step...
At time: 124.88025951385498 and batch: 50, loss is 3.766098966598511 and perplexity is 43.211167405417875
At time: 125.76158952713013 and batch: 100, loss is 3.6443625497817993 and perplexity is 38.25837726596219
At time: 126.64306807518005 and batch: 150, loss is 3.6473006916046145 and perplexity is 38.37095110224416
At time: 127.522634267807 and batch: 200, loss is 3.5239654541015626 and perplexity is 33.918665042863026
At time: 128.4050543308258 and batch: 250, loss is 3.6715411138534546 and perplexity is 39.312444152418934
At time: 129.28669500350952 and batch: 300, loss is 3.633978090286255 and perplexity is 37.8631404092003
At time: 130.16978001594543 and batch: 350, loss is 3.6273785161972047 and perplexity is 37.614082550683236
At time: 131.05056357383728 and batch: 400, loss is 3.5596748638153075 and perplexity is 35.15176617575684
At time: 131.93831181526184 and batch: 450, loss is 3.592732367515564 and perplexity is 36.33321612053699
At time: 132.81951379776 and batch: 500, loss is 3.4710442495346068 and perplexity is 32.170318751064855
At time: 133.70209741592407 and batch: 550, loss is 3.5145881366729737 and perplexity is 33.60208560677402
At time: 134.58378505706787 and batch: 600, loss is 3.53971941947937 and perplexity is 34.45724980131092
At time: 135.46692061424255 and batch: 650, loss is 3.382986536026001 and perplexity is 29.45861909461532
At time: 136.34913563728333 and batch: 700, loss is 3.3817370653152468 and perplexity is 29.42183439836323
At time: 137.2319757938385 and batch: 750, loss is 3.485463328361511 and perplexity is 32.63754550768154
At time: 138.113511800766 and batch: 800, loss is 3.435568776130676 and perplexity is 31.049067512676814
At time: 138.99535512924194 and batch: 850, loss is 3.4997967433929444 and perplexity is 33.10872170829318
At time: 139.8789026737213 and batch: 900, loss is 3.458819966316223 and perplexity is 31.779453554197243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320157403815283 and perplexity of 75.20046420046626
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 141.97567105293274 and batch: 50, loss is 3.7262738513946534 and perplexity is 41.524094594422934
At time: 142.86227297782898 and batch: 100, loss is 3.6133870697021484 and perplexity is 37.09147169365895
At time: 143.7441029548645 and batch: 150, loss is 3.6213171529769896 and perplexity is 37.38677951321203
At time: 144.624520778656 and batch: 200, loss is 3.4970941877365114 and perplexity is 33.019364346172786
At time: 145.504390001297 and batch: 250, loss is 3.639272699356079 and perplexity is 38.064142580226076
At time: 146.4059398174286 and batch: 300, loss is 3.604963140487671 and perplexity is 36.78032812730048
At time: 147.29177594184875 and batch: 350, loss is 3.5904579162597656 and perplexity is 36.2506718984935
At time: 148.17136096954346 and batch: 400, loss is 3.5217139101028443 and perplexity is 33.842381586147084
At time: 149.05063009262085 and batch: 450, loss is 3.550656976699829 and perplexity is 34.83619654071461
At time: 149.9411165714264 and batch: 500, loss is 3.4277957248687745 and perplexity is 30.808657091004093
At time: 150.8269784450531 and batch: 550, loss is 3.464462928771973 and perplexity is 31.95929074751975
At time: 151.70971298217773 and batch: 600, loss is 3.485734143257141 and perplexity is 32.64638543810021
At time: 152.59943199157715 and batch: 650, loss is 3.3223952150344847 and perplexity is 27.72668245389769
At time: 153.48602056503296 and batch: 700, loss is 3.3161981296539307 and perplexity is 27.555389143240458
At time: 154.37520480155945 and batch: 750, loss is 3.417452130317688 and perplexity is 30.49162727391419
At time: 155.26777935028076 and batch: 800, loss is 3.3613316297531126 and perplexity is 28.82755295876
At time: 156.14953923225403 and batch: 850, loss is 3.4194494342803954 and perplexity is 30.552589181361547
At time: 157.03161907196045 and batch: 900, loss is 3.3784812259674073 and perplexity is 29.326197406067966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31221572666952 and perplexity of 74.60561158274892
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.12668681144714 and batch: 50, loss is 3.710236043930054 and perplexity is 40.86345095825051
At time: 160.01425766944885 and batch: 100, loss is 3.595025939941406 and perplexity is 36.41664462125482
At time: 160.89501476287842 and batch: 150, loss is 3.605181555747986 and perplexity is 36.78836238961368
At time: 161.7758560180664 and batch: 200, loss is 3.480511713027954 and perplexity is 32.4763363890515
At time: 162.65231823921204 and batch: 250, loss is 3.6221491479873658 and perplexity is 37.41789807066826
At time: 163.5323622226715 and batch: 300, loss is 3.5881019973754884 and perplexity is 36.16536877907028
At time: 164.41418361663818 and batch: 350, loss is 3.5744846296310424 and perplexity is 35.67622960530581
At time: 165.29337000846863 and batch: 400, loss is 3.506694836616516 and perplexity is 33.33789828910493
At time: 166.17333221435547 and batch: 450, loss is 3.5337393140792845 and perplexity is 34.251806713609554
At time: 167.05447912216187 and batch: 500, loss is 3.4106647634506224 and perplexity is 30.285370176353737
At time: 167.9350061416626 and batch: 550, loss is 3.445130944252014 and perplexity is 31.347387938199894
At time: 168.816011428833 and batch: 600, loss is 3.4672391605377197 and perplexity is 32.04814042228136
At time: 169.6969714164734 and batch: 650, loss is 3.3023917293548584 and perplexity is 27.177562624066773
At time: 170.5777463912964 and batch: 700, loss is 3.2939747762680054 and perplexity is 26.949770357683732
At time: 171.45771765708923 and batch: 750, loss is 3.392303829193115 and perplexity is 29.734376345677862
At time: 172.33733010292053 and batch: 800, loss is 3.3362718534469606 and perplexity is 28.114117531873756
At time: 173.21631860733032 and batch: 850, loss is 3.3927129697799683 and perplexity is 29.746544374913213
At time: 174.09677171707153 and batch: 900, loss is 3.3512642669677732 and perplexity is 28.538791496977886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306716709920805 and perplexity of 74.19648001675736
finished 10 epochs...
Completing Train Step...
At time: 176.18618297576904 and batch: 50, loss is 3.6983885955810547 and perplexity is 40.382179880116524
At time: 177.06697702407837 and batch: 100, loss is 3.5808923625946045 and perplexity is 35.905567340547854
At time: 177.94704270362854 and batch: 150, loss is 3.5900696897506714 and perplexity is 36.23660115818396
At time: 178.828551530838 and batch: 200, loss is 3.4656821584701536 and perplexity is 31.998280227668086
At time: 179.7086443901062 and batch: 250, loss is 3.6077135610580444 and perplexity is 36.88162874415708
At time: 180.58659529685974 and batch: 300, loss is 3.57376699924469 and perplexity is 35.650636443184325
At time: 181.46808004379272 and batch: 350, loss is 3.5600239038467407 and perplexity is 35.16403769082818
At time: 182.34847378730774 and batch: 400, loss is 3.4937597513198853 and perplexity is 32.90944673373086
At time: 183.2285008430481 and batch: 450, loss is 3.521752481460571 and perplexity is 33.84368695792839
At time: 184.1092665195465 and batch: 500, loss is 3.3999302053451537 and perplexity is 29.962008786356694
At time: 184.98982763290405 and batch: 550, loss is 3.436343445777893 and perplexity is 31.07312960173497
At time: 185.86961245536804 and batch: 600, loss is 3.4606245708465577 and perplexity is 31.836854677638716
At time: 186.74925756454468 and batch: 650, loss is 3.2969172859191893 and perplexity is 27.029187102025176
At time: 187.62898445129395 and batch: 700, loss is 3.2902872943878174 and perplexity is 26.850576567864962
At time: 188.50823545455933 and batch: 750, loss is 3.3906668758392335 and perplexity is 29.685742375223782
At time: 189.38887882232666 and batch: 800, loss is 3.3367679834365847 and perplexity is 28.12806924935968
At time: 190.26836824417114 and batch: 850, loss is 3.3955629682540893 and perplexity is 29.831442903865454
At time: 191.14910626411438 and batch: 900, loss is 3.355542039871216 and perplexity is 28.661135459216172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306523989324701 and perplexity of 74.18218220468542
finished 11 epochs...
Completing Train Step...
At time: 193.245539188385 and batch: 50, loss is 3.6913706350326536 and perplexity is 40.099771459528476
At time: 194.13696193695068 and batch: 100, loss is 3.5730100297927856 and perplexity is 35.623660211835535
At time: 195.022198677063 and batch: 150, loss is 3.581548767089844 and perplexity is 35.929143653306625
At time: 195.905091047287 and batch: 200, loss is 3.457042922973633 and perplexity is 31.72303023601676
At time: 196.78856658935547 and batch: 250, loss is 3.598955373764038 and perplexity is 36.560022929638116
At time: 197.67444109916687 and batch: 300, loss is 3.5651511764526367 and perplexity is 35.3447963013334
At time: 198.55870294570923 and batch: 350, loss is 3.551485357284546 and perplexity is 34.86506612542516
At time: 199.44185590744019 and batch: 400, loss is 3.485858154296875 and perplexity is 32.65043420134243
At time: 200.32573080062866 and batch: 450, loss is 3.5142622661590575 and perplexity is 33.59113746180438
At time: 201.209969997406 and batch: 500, loss is 3.3932917070388795 and perplexity is 29.7637647910344
At time: 202.09435653686523 and batch: 550, loss is 3.4303680419921876 and perplexity is 30.888008742239673
At time: 202.97691202163696 and batch: 600, loss is 3.455725383758545 and perplexity is 31.681261421727026
At time: 203.85909390449524 and batch: 650, loss is 3.2928200244903563 and perplexity is 26.91866802364984
At time: 204.74197149276733 and batch: 700, loss is 3.2871934366226196 and perplexity is 26.767633076893187
At time: 205.62325358390808 and batch: 750, loss is 3.388658242225647 and perplexity is 29.626174440350553
At time: 206.50620913505554 and batch: 800, loss is 3.335897831916809 and perplexity is 28.103604212842015
At time: 207.38873386383057 and batch: 850, loss is 3.395839648246765 and perplexity is 29.83969780920052
At time: 208.27148938179016 and batch: 900, loss is 3.3564755153656005 and perplexity is 28.68790241801496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307059510113442 and perplexity of 74.22191894438565
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 210.35640215873718 and batch: 50, loss is 3.6874964809417725 and perplexity is 39.944719307781476
At time: 211.24315786361694 and batch: 100, loss is 3.570803375244141 and perplexity is 35.54513776780646
At time: 212.12464809417725 and batch: 150, loss is 3.58114070892334 and perplexity is 35.91448546372378
At time: 213.0059597492218 and batch: 200, loss is 3.4561651611328124 and perplexity is 31.695197187781247
At time: 213.88630723953247 and batch: 250, loss is 3.597424192428589 and perplexity is 36.5040857408232
At time: 214.76744961738586 and batch: 300, loss is 3.5622948265075682 and perplexity is 35.24398324159071
At time: 215.6475169658661 and batch: 350, loss is 3.550084099769592 and perplexity is 34.816245402698236
At time: 216.52967596054077 and batch: 400, loss is 3.4847037744522096 and perplexity is 32.6127649446793
At time: 217.41177678108215 and batch: 450, loss is 3.511541042327881 and perplexity is 33.499852717468585
At time: 218.29342579841614 and batch: 500, loss is 3.3894179487228393 and perplexity is 29.648690189157247
At time: 219.17642784118652 and batch: 550, loss is 3.4256365633010866 and perplexity is 30.74220798565372
At time: 220.07170724868774 and batch: 600, loss is 3.4500912952423097 and perplexity is 31.503268275975675
At time: 220.95294713974 and batch: 650, loss is 3.286548776626587 and perplexity is 26.750382615598607
At time: 221.83391523361206 and batch: 700, loss is 3.2799283409118654 and perplexity is 26.57386837246781
At time: 222.71546602249146 and batch: 750, loss is 3.3794597673416136 and perplexity is 29.354908348709582
At time: 223.59691214561462 and batch: 800, loss is 3.3262704515457155 and perplexity is 27.834338367415217
At time: 224.4781813621521 and batch: 850, loss is 3.3854900121688845 and perplexity is 29.532460436173395
At time: 225.35953617095947 and batch: 900, loss is 3.3453713178634645 and perplexity is 28.371108410393653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305787805008562 and perplexity of 74.12759054283755
finished 13 epochs...
Completing Train Step...
At time: 227.45513129234314 and batch: 50, loss is 3.6842612409591675 and perplexity is 39.81569737577651
At time: 228.33534026145935 and batch: 100, loss is 3.5678108167648315 and perplexity is 35.43892586630862
At time: 229.21589350700378 and batch: 150, loss is 3.5778789806365965 and perplexity is 35.79753300783653
At time: 230.09593510627747 and batch: 200, loss is 3.4531608200073243 and perplexity is 31.6001169017131
At time: 230.97672176361084 and batch: 250, loss is 3.5942139720916746 and perplexity is 36.387087477976195
At time: 231.85809922218323 and batch: 300, loss is 3.5589228963851927 and perplexity is 35.12534312836227
At time: 232.73620748519897 and batch: 350, loss is 3.5464965963363646 and perplexity is 34.69156578079053
At time: 233.61457777023315 and batch: 400, loss is 3.481492838859558 and perplexity is 32.508215397700845
At time: 234.52002668380737 and batch: 450, loss is 3.508728919029236 and perplexity is 33.405779336109006
At time: 235.4030418395996 and batch: 500, loss is 3.3868943500518798 and perplexity is 29.573963124219663
At time: 236.28376460075378 and batch: 550, loss is 3.4236645174026488 and perplexity is 30.681642678897106
At time: 237.1616508960724 and batch: 600, loss is 3.4486928462982176 and perplexity is 31.459243354197188
At time: 238.04051280021667 and batch: 650, loss is 3.285399751663208 and perplexity is 26.719663410140086
At time: 238.91729283332825 and batch: 700, loss is 3.2792317390441896 and perplexity is 26.555363412171435
At time: 239.79286932945251 and batch: 750, loss is 3.37922945022583 and perplexity is 29.348148189404696
At time: 240.68304443359375 and batch: 800, loss is 3.3266734790802004 and perplexity is 27.84555863306758
At time: 241.56411790847778 and batch: 850, loss is 3.3864495611190795 and perplexity is 29.560811877704236
At time: 242.44003796577454 and batch: 900, loss is 3.3466669702529908 and perplexity is 28.407891328591347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305423684316139 and perplexity of 74.10060406670684
finished 14 epochs...
Completing Train Step...
At time: 244.50524020195007 and batch: 50, loss is 3.682114267349243 and perplexity is 39.73030582375532
At time: 245.38279724121094 and batch: 100, loss is 3.5656008291244508 and perplexity is 35.36069275709909
At time: 246.25567197799683 and batch: 150, loss is 3.575431752204895 and perplexity is 35.7100353742913
At time: 247.13101744651794 and batch: 200, loss is 3.4508062553405763 and perplexity is 31.525799909407624
At time: 248.00741362571716 and batch: 250, loss is 3.5918125343322753 and perplexity is 36.299810988623456
At time: 248.88232517242432 and batch: 300, loss is 3.5565261793136598 and perplexity is 35.04125842271812
At time: 249.75754308700562 and batch: 350, loss is 3.5440493202209473 and perplexity is 34.606769742429876
At time: 250.63352990150452 and batch: 400, loss is 3.4793271827697754 and perplexity is 32.437889960898
At time: 251.511075258255 and batch: 450, loss is 3.506682677268982 and perplexity is 33.33749292447806
At time: 252.40002918243408 and batch: 500, loss is 3.3850821161270144 and perplexity is 29.52041671891408
At time: 253.2764675617218 and batch: 550, loss is 3.4221428871154784 and perplexity is 30.634992063632385
At time: 254.15224146842957 and batch: 600, loss is 3.4475460767745973 and perplexity is 31.423187530493685
At time: 255.02792477607727 and batch: 650, loss is 3.2844744110107422 and perplexity is 26.694950055265853
At time: 255.90290141105652 and batch: 700, loss is 3.27864239692688 and perplexity is 26.539717828825623
At time: 256.77733540534973 and batch: 750, loss is 3.378988208770752 and perplexity is 29.34106905335651
At time: 257.6528604030609 and batch: 800, loss is 3.326767659187317 and perplexity is 27.848181254259796
At time: 258.5309810638428 and batch: 850, loss is 3.3869500827789305 and perplexity is 29.575611407765514
At time: 259.40691804885864 and batch: 900, loss is 3.3473343372344972 and perplexity is 28.426856144820476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305392330639983 and perplexity of 74.098280776786
finished 15 epochs...
Completing Train Step...
At time: 261.4722807407379 and batch: 50, loss is 3.680253119468689 and perplexity is 39.65643061694449
At time: 262.369108915329 and batch: 100, loss is 3.5636511182785036 and perplexity is 35.29181679683234
At time: 263.2550640106201 and batch: 150, loss is 3.5733268451690674 and perplexity is 35.6349481231476
At time: 264.1414074897766 and batch: 200, loss is 3.4487491416931153 and perplexity is 31.461014414575796
At time: 265.0342786312103 and batch: 250, loss is 3.5897397661209105 and perplexity is 36.22464781915257
At time: 265.9128887653351 and batch: 300, loss is 3.554485869407654 and perplexity is 34.9698362824662
At time: 266.790892124176 and batch: 350, loss is 3.5419990110397337 and perplexity is 34.535887854418625
At time: 267.6682918071747 and batch: 400, loss is 3.4774852705001833 and perplexity is 32.37819720466574
At time: 268.5454988479614 and batch: 450, loss is 3.5049194288253784 and perplexity is 33.27876243540337
At time: 269.4245140552521 and batch: 500, loss is 3.3835251092910767 and perplexity is 29.474488992453466
At time: 270.30520009994507 and batch: 550, loss is 3.420782217979431 and perplexity is 30.593336321716382
At time: 271.1827926635742 and batch: 600, loss is 3.4464777946472167 and perplexity is 31.389636624976653
At time: 272.0598738193512 and batch: 650, loss is 3.283589844703674 and perplexity is 26.671347042631993
At time: 272.93688225746155 and batch: 700, loss is 3.278024582862854 and perplexity is 26.523326281881186
At time: 273.81349182128906 and batch: 750, loss is 3.3786471033096315 and perplexity is 29.331062361232988
At time: 274.69100761413574 and batch: 800, loss is 3.32667760848999 and perplexity is 27.84567361902741
At time: 275.56825256347656 and batch: 850, loss is 3.387168040275574 and perplexity is 29.582058336542357
At time: 276.4554133415222 and batch: 900, loss is 3.347668786048889 and perplexity is 28.4363650631896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305493498501712 and perplexity of 74.10577752061835
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 278.53345227241516 and batch: 50, loss is 3.679100570678711 and perplexity is 39.610750974886784
At time: 279.4101424217224 and batch: 100, loss is 3.5630057096481322 and perplexity is 35.26904650255282
At time: 280.28711223602295 and batch: 150, loss is 3.573351125717163 and perplexity is 35.63581336972368
At time: 281.1633462905884 and batch: 200, loss is 3.4488508796691892 and perplexity is 31.46421535733353
At time: 282.0389416217804 and batch: 250, loss is 3.589735536575317 and perplexity is 36.22449460567704
At time: 282.91596579551697 and batch: 300, loss is 3.5537810468673707 and perplexity is 34.94519743765089
At time: 283.7939827442169 and batch: 350, loss is 3.541394896507263 and perplexity is 34.515030523416705
At time: 284.6705129146576 and batch: 400, loss is 3.4770479726791383 and perplexity is 32.36404138495892
At time: 285.55711221694946 and batch: 450, loss is 3.5041572141647337 and perplexity is 33.25340653933244
At time: 286.43375182151794 and batch: 500, loss is 3.3822412872314453 and perplexity is 29.43667327281012
At time: 287.3121883869171 and batch: 550, loss is 3.418993363380432 and perplexity is 30.538658211513837
At time: 288.1883764266968 and batch: 600, loss is 3.4447197246551515 and perplexity is 31.334499928052036
At time: 289.0659930706024 and batch: 650, loss is 3.281811065673828 and perplexity is 26.623946779609827
At time: 289.9478826522827 and batch: 700, loss is 3.2755795240402223 and perplexity is 26.4585544067414
At time: 290.8497998714447 and batch: 750, loss is 3.376095690727234 and perplexity is 29.256322106826957
At time: 291.73053669929504 and batch: 800, loss is 3.323655800819397 and perplexity is 27.761656354831686
At time: 292.60833954811096 and batch: 850, loss is 3.384165964126587 and perplexity is 29.49338391505306
At time: 293.4838345050812 and batch: 900, loss is 3.344169340133667 and perplexity is 28.337027456291814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304793684449915 and perplexity of 74.05393539822866
finished 17 epochs...
Completing Train Step...
At time: 295.5717704296112 and batch: 50, loss is 3.678369278907776 and perplexity is 39.581794547747194
At time: 296.4572947025299 and batch: 100, loss is 3.562245807647705 and perplexity is 35.24225566405745
At time: 297.33438301086426 and batch: 150, loss is 3.572531599998474 and perplexity is 35.606620867797716
At time: 298.2123670578003 and batch: 200, loss is 3.448113660812378 and perplexity is 31.441027892625623
At time: 299.0881462097168 and batch: 250, loss is 3.5889227533340455 and perplexity is 36.195063905548125
At time: 299.9654812812805 and batch: 300, loss is 3.55307457447052 and perplexity is 34.920518338836196
At time: 300.84289145469666 and batch: 350, loss is 3.5406519412994384 and perplexity is 34.48939692522785
At time: 301.7214210033417 and batch: 400, loss is 3.476305432319641 and perplexity is 32.3400186980452
At time: 302.5989372730255 and batch: 450, loss is 3.5035657358169554 and perplexity is 33.233743685024095
At time: 303.47692584991455 and batch: 500, loss is 3.38174156665802 and perplexity is 29.421966836422946
At time: 304.3551592826843 and batch: 550, loss is 3.4186829566955566 and perplexity is 30.529180278940775
At time: 305.23319697380066 and batch: 600, loss is 3.4444502782821655 and perplexity is 31.326058098058404
At time: 306.1099696159363 and batch: 650, loss is 3.2815487003326416 and perplexity is 26.616962494986442
At time: 306.9999489784241 and batch: 700, loss is 3.2754798555374145 and perplexity is 26.455917453649985
At time: 307.8854248523712 and batch: 750, loss is 3.376063141822815 and perplexity is 29.255369861092415
At time: 308.77600145339966 and batch: 800, loss is 3.3237911224365235 and perplexity is 27.76541336126009
At time: 309.6625053882599 and batch: 850, loss is 3.38441463470459 and perplexity is 29.50071896384609
At time: 310.5395302772522 and batch: 900, loss is 3.3443818521499633 and perplexity is 28.343050055047033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30447889354131 and perplexity of 74.03062756135894
finished 18 epochs...
Completing Train Step...
At time: 312.62468218803406 and batch: 50, loss is 3.6777826499938966 and perplexity is 39.55858153198106
At time: 313.5074737071991 and batch: 100, loss is 3.5616321420669554 and perplexity is 35.22063533927152
At time: 314.38447737693787 and batch: 150, loss is 3.5718637561798094 and perplexity is 35.58284914493025
At time: 315.2627239227295 and batch: 200, loss is 3.447507481575012 and perplexity is 31.421974769702867
At time: 316.1390116214752 and batch: 250, loss is 3.588280129432678 and perplexity is 36.171811564426186
At time: 317.01728439331055 and batch: 300, loss is 3.5524764394760133 and perplexity is 34.89963740022401
At time: 317.89421796798706 and batch: 350, loss is 3.540027837753296 and perplexity is 34.46787868580572
At time: 318.7706596851349 and batch: 400, loss is 3.4757254552841186 and perplexity is 32.321267667981
At time: 319.6479833126068 and batch: 450, loss is 3.5030635452270507 and perplexity is 33.2170582016752
At time: 320.52653646469116 and batch: 500, loss is 3.3813205575942993 and perplexity is 29.40958252884812
At time: 321.404842376709 and batch: 550, loss is 3.4183563137054445 and perplexity is 30.519209764696736
At time: 322.2815570831299 and batch: 600, loss is 3.4442023468017577 and perplexity is 31.318292344825938
At time: 323.15810537338257 and batch: 650, loss is 3.281329941749573 and perplexity is 26.611140442820606
At time: 324.03594303131104 and batch: 700, loss is 3.2753662633895875 and perplexity is 26.452912439839906
At time: 324.91332817077637 and batch: 750, loss is 3.3760177659988404 and perplexity is 29.254042404696726
At time: 325.7935335636139 and batch: 800, loss is 3.3238734149932863 and perplexity is 27.76769834213233
At time: 326.6729738712311 and batch: 850, loss is 3.384584321975708 and perplexity is 29.505725285085575
At time: 327.5512125492096 and batch: 900, loss is 3.344541835784912 and perplexity is 28.347584841956543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304329650042808 and perplexity of 74.01957979592949
finished 19 epochs...
Completing Train Step...
At time: 329.6551158428192 and batch: 50, loss is 3.6772577381134033 and perplexity is 39.537822211443114
At time: 330.53901410102844 and batch: 100, loss is 3.5610854768753053 and perplexity is 35.20138670566209
At time: 331.4190835952759 and batch: 150, loss is 3.5712715911865236 and perplexity is 35.56178446480388
At time: 332.30049204826355 and batch: 200, loss is 3.4469571495056153 and perplexity is 31.404687006744822
At time: 333.1791172027588 and batch: 250, loss is 3.5877108573913574 and perplexity is 36.151225823417974
At time: 334.0539917945862 and batch: 300, loss is 3.551929597854614 and perplexity is 34.88055804309057
At time: 334.93064856529236 and batch: 350, loss is 3.539465856552124 and perplexity is 34.44851382779277
At time: 335.81150126457214 and batch: 400, loss is 3.475216646194458 and perplexity is 32.30482649626568
At time: 336.6938455104828 and batch: 450, loss is 3.502602186203003 and perplexity is 33.20173674673905
At time: 337.5746397972107 and batch: 500, loss is 3.380930619239807 and perplexity is 29.398116840231676
At time: 338.4561746120453 and batch: 550, loss is 3.4180262517929076 and perplexity is 30.509138198164376
At time: 339.3382821083069 and batch: 600, loss is 3.4439548444747925 and perplexity is 31.310541953753493
At time: 340.22810196876526 and batch: 650, loss is 3.2811212062835695 and perplexity is 26.605586333709333
At time: 341.1102488040924 and batch: 700, loss is 3.2752388191223143 and perplexity is 26.449541382612292
At time: 341.99765706062317 and batch: 750, loss is 3.3759595108032228 and perplexity is 29.25233825437212
At time: 342.891729593277 and batch: 800, loss is 3.323915057182312 and perplexity is 27.768854673951438
At time: 343.77552556991577 and batch: 850, loss is 3.3847026872634887 and perplexity is 29.509217945450914
At time: 344.669353723526 and batch: 900, loss is 3.3446657419204713 and perplexity is 28.351097499262153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304261090004281 and perplexity of 74.01450518464667
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f05edd19b70>
ELAPSED
707.7218639850616


RESULTS SO FAR:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.01450518464667, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8745980334815576, 'dropout': 0.43719893342278404, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3116672039031982 and batch: 50, loss is 7.10804407119751 and perplexity is 1221.7555407065358
At time: 2.4176747798919678 and batch: 100, loss is 6.485425701141358 and perplexity is 655.5179595941256
At time: 3.523498773574829 and batch: 150, loss is 6.069530925750732 and perplexity is 432.47776980265814
At time: 4.630849361419678 and batch: 200, loss is 5.876037139892578 and perplexity is 356.39409959590995
At time: 5.737871408462524 and batch: 250, loss is 5.943935174942016 and perplexity is 381.432985575556
At time: 6.84412956237793 and batch: 300, loss is 5.864522981643677 and perplexity is 352.31405574563576
At time: 7.951966762542725 and batch: 350, loss is 5.830704126358032 and perplexity is 340.5984189825582
At time: 9.058632850646973 and batch: 400, loss is 5.682843990325928 and perplexity is 293.7837611198045
At time: 10.166471481323242 and batch: 450, loss is 5.681777896881104 and perplexity is 293.4707270693019
At time: 11.275721788406372 and batch: 500, loss is 5.6212246799468994 and perplexity is 276.2274664095788
At time: 12.385873317718506 and batch: 550, loss is 5.65799241065979 and perplexity is 286.5727442792495
At time: 13.493489742279053 and batch: 600, loss is 5.578302278518676 and perplexity is 264.6219698133169
At time: 14.600996255874634 and batch: 650, loss is 5.479363880157471 and perplexity is 239.69418464104086
At time: 15.708431243896484 and batch: 700, loss is 5.5668767547607425 and perplexity is 261.6157318354997
At time: 16.815058708190918 and batch: 750, loss is 5.529860000610352 and perplexity is 252.10861350073947
At time: 17.922413110733032 and batch: 800, loss is 5.518726615905762 and perplexity is 249.31735821057802
At time: 19.029000520706177 and batch: 850, loss is 5.5451334190368655 and perplexity is 255.9887297347606
At time: 20.134422540664673 and batch: 900, loss is 5.431554718017578 and perplexity is 228.5042290447731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.3698404390517975 and perplexity of 214.8285867164572
finished 1 epochs...
Completing Train Step...
At time: 22.379709243774414 and batch: 50, loss is 5.274318761825562 and perplexity is 195.2574143896737
At time: 23.25486135482788 and batch: 100, loss is 5.135931930541992 and perplexity is 170.02269540840513
At time: 24.14324140548706 and batch: 150, loss is 5.101204977035523 and perplexity is 164.21966905677988
At time: 25.022152185440063 and batch: 200, loss is 4.963654327392578 and perplexity is 143.1158335787017
At time: 25.91093420982361 and batch: 250, loss is 5.033808603286743 and perplexity is 153.51658446167735
At time: 26.796788454055786 and batch: 300, loss is 4.957431850433349 and perplexity is 142.22806353057067
At time: 27.676076650619507 and batch: 350, loss is 4.9403575420379635 and perplexity is 139.82023223479854
At time: 28.557892322540283 and batch: 400, loss is 4.791606712341308 and perplexity is 120.49481372058821
At time: 29.44749355316162 and batch: 450, loss is 4.804498128890991 and perplexity is 122.05821815361874
At time: 30.326444149017334 and batch: 500, loss is 4.711757698059082 and perplexity is 111.24752772013731
At time: 31.205193996429443 and batch: 550, loss is 4.768464527130127 and perplexity is 117.73831910042226
At time: 32.084912061691284 and batch: 600, loss is 4.702499828338623 and perplexity is 110.22236533374289
At time: 32.96426033973694 and batch: 650, loss is 4.564278564453125 and perplexity is 95.99331603928253
At time: 33.843082666397095 and batch: 700, loss is 4.617873601913452 and perplexity is 101.27844470740304
At time: 34.727684020996094 and batch: 750, loss is 4.647331171035766 and perplexity is 104.30623826827934
At time: 35.607595920562744 and batch: 800, loss is 4.594289226531982 and perplexity is 98.91780240569872
At time: 36.485448360443115 and batch: 850, loss is 4.648822002410888 and perplexity is 104.46185725292015
At time: 37.36241936683655 and batch: 900, loss is 4.56866413116455 and perplexity is 96.41522561068733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.666983826519692 and perplexity of 106.37640837455525
finished 2 epochs...
Completing Train Step...
At time: 39.44114542007446 and batch: 50, loss is 4.613395299911499 and perplexity is 100.8259033105536
At time: 40.32549738883972 and batch: 100, loss is 4.487965536117554 and perplexity is 88.94031582961397
At time: 41.204838514328 and batch: 150, loss is 4.488571062088012 and perplexity is 88.99418780946496
At time: 42.09264421463013 and batch: 200, loss is 4.378064870834351 and perplexity is 79.68368588747148
At time: 42.978052377700806 and batch: 250, loss is 4.514376430511475 and perplexity is 91.32060352216534
At time: 43.85630917549133 and batch: 300, loss is 4.472745361328125 and perplexity is 87.59687828821825
At time: 44.735411167144775 and batch: 350, loss is 4.475967884063721 and perplexity is 87.87961654061432
At time: 45.61495494842529 and batch: 400, loss is 4.367159404754639 and perplexity is 78.81941933423579
At time: 46.49513649940491 and batch: 450, loss is 4.404270915985108 and perplexity is 81.79948240202762
At time: 47.37533640861511 and batch: 500, loss is 4.286498827934265 and perplexity is 72.71144703590778
At time: 48.255260705947876 and batch: 550, loss is 4.355036635398864 and perplexity is 77.8696780712878
At time: 49.13916611671448 and batch: 600, loss is 4.337415704727173 and perplexity is 76.50956033527913
At time: 50.02508854866028 and batch: 650, loss is 4.191512961387634 and perplexity is 66.12275649676629
At time: 50.90408229827881 and batch: 700, loss is 4.219958043098449 and perplexity is 68.0306298750988
At time: 51.78239130973816 and batch: 750, loss is 4.295890998840332 and perplexity is 73.39758248015077
At time: 52.66125988960266 and batch: 800, loss is 4.252854351997375 and perplexity is 70.30580372816198
At time: 53.54107642173767 and batch: 850, loss is 4.329930305480957 and perplexity is 75.93899385309962
At time: 54.420814037323 and batch: 900, loss is 4.252796235084534 and perplexity is 70.30171789062375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.470772521136558 and perplexity of 87.4242340018646
finished 3 epochs...
Completing Train Step...
At time: 56.49784564971924 and batch: 50, loss is 4.331366157531738 and perplexity is 76.04810933125268
At time: 57.38461494445801 and batch: 100, loss is 4.203135161399842 and perplexity is 66.89573153255037
At time: 58.26506686210632 and batch: 150, loss is 4.210665621757507 and perplexity is 67.40138871307154
At time: 59.16264796257019 and batch: 200, loss is 4.092570123672485 and perplexity is 59.89362809014228
At time: 60.04504609107971 and batch: 250, loss is 4.245219969749451 and perplexity is 69.77110598873374
At time: 60.92602252960205 and batch: 300, loss is 4.217734861373901 and perplexity is 67.87955341948363
At time: 61.80667996406555 and batch: 350, loss is 4.217116146087647 and perplexity is 67.83756829191833
At time: 62.68807053565979 and batch: 400, loss is 4.13157091140747 and perplexity is 62.27567570355909
At time: 63.581451177597046 and batch: 450, loss is 4.166872425079346 and perplexity is 64.51336586707096
At time: 64.4610059261322 and batch: 500, loss is 4.0439101457595825 and perplexity is 57.048977071378815
At time: 65.34169745445251 and batch: 550, loss is 4.112301936149597 and perplexity is 61.08717462155525
At time: 66.22370457649231 and batch: 600, loss is 4.117691707611084 and perplexity is 61.417309408328556
At time: 67.1040723323822 and batch: 650, loss is 3.963327326774597 and perplexity is 52.6321593168262
At time: 67.98504400253296 and batch: 700, loss is 3.9822257232666014 and perplexity is 53.636280991652036
At time: 68.86604809761047 and batch: 750, loss is 4.076792726516723 and perplexity is 58.95607803080628
At time: 69.74671745300293 and batch: 800, loss is 4.038472337722778 and perplexity is 56.73959762075599
At time: 70.62900185585022 and batch: 850, loss is 4.118312249183655 and perplexity is 61.45543322957647
At time: 71.51149988174438 and batch: 900, loss is 4.045937223434448 and perplexity is 57.16473706678673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.389310235846533 and perplexity of 80.58481528906482
finished 4 epochs...
Completing Train Step...
At time: 73.60030102729797 and batch: 50, loss is 4.138509430885315 and perplexity is 62.709279236120594
At time: 74.48101282119751 and batch: 100, loss is 4.009724125862122 and perplexity is 55.13165906641623
At time: 75.36301040649414 and batch: 150, loss is 4.016922178268433 and perplexity is 55.529931310260835
At time: 76.25603437423706 and batch: 200, loss is 3.9046261978149412 and perplexity is 49.63152407214047
At time: 77.13692665100098 and batch: 250, loss is 4.058962931632996 and perplexity is 57.914218920064805
At time: 78.01747298240662 and batch: 300, loss is 4.0335702276229854 and perplexity is 56.462134499221776
At time: 78.9086983203888 and batch: 350, loss is 4.031032361984253 and perplexity is 56.319022864049735
At time: 79.8108823299408 and batch: 400, loss is 3.9560104179382325 and perplexity is 52.24846006331887
At time: 80.70216417312622 and batch: 450, loss is 3.992896013259888 and perplexity is 54.211659934943484
At time: 81.58247017860413 and batch: 500, loss is 3.871140551567078 and perplexity is 47.99709803936717
At time: 82.46339917182922 and batch: 550, loss is 3.9356413173675535 and perplexity is 51.19497165542374
At time: 83.34433054924011 and batch: 600, loss is 3.94970513343811 and perplexity is 51.920055089344174
At time: 84.22203969955444 and batch: 650, loss is 3.795152397155762 and perplexity is 44.48501527877763
At time: 85.10071659088135 and batch: 700, loss is 3.814336061477661 and perplexity is 45.34663900551571
At time: 85.98035621643066 and batch: 750, loss is 3.9125394296646117 and perplexity is 50.02582788032095
At time: 86.86022543907166 and batch: 800, loss is 3.879473099708557 and perplexity is 48.39870705899089
At time: 87.74069380760193 and batch: 850, loss is 3.9551175117492674 and perplexity is 52.20182791213089
At time: 88.6318097114563 and batch: 900, loss is 3.8875727891921996 and perplexity is 48.79231345047781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360908403788527 and perplexity of 78.32825579904639
finished 5 epochs...
Completing Train Step...
At time: 90.72283363342285 and batch: 50, loss is 3.9850944900512695 and perplexity is 53.79037189276509
At time: 91.6076877117157 and batch: 100, loss is 3.8606027269363405 and perplexity is 47.493968637999544
At time: 92.48633122444153 and batch: 150, loss is 3.866437301635742 and perplexity is 47.7718857213705
At time: 93.36532044410706 and batch: 200, loss is 3.756452732086182 and perplexity is 42.79634629849422
At time: 94.2450761795044 and batch: 250, loss is 3.9092381381988526 and perplexity is 49.86095034581437
At time: 95.1375379562378 and batch: 300, loss is 3.886665463447571 and perplexity is 48.7480630061636
At time: 96.01743078231812 and batch: 350, loss is 3.8834029912948607 and perplexity is 48.589282956686574
At time: 96.89683961868286 and batch: 400, loss is 3.811393351554871 and perplexity is 45.213393149054966
At time: 97.77618741989136 and batch: 450, loss is 3.850043797492981 and perplexity is 46.9951214550085
At time: 98.66281604766846 and batch: 500, loss is 3.7330971574783325 and perplexity is 41.80839503311904
At time: 99.54173135757446 and batch: 550, loss is 3.7945880842208863 and perplexity is 44.459918891021786
At time: 100.42125034332275 and batch: 600, loss is 3.8145318841934204 and perplexity is 45.35551977701634
At time: 101.31012797355652 and batch: 650, loss is 3.6621523189544676 and perplexity is 38.94507495293976
At time: 102.19023418426514 and batch: 700, loss is 3.6756799030303955 and perplexity is 39.47548723853799
At time: 103.07032585144043 and batch: 750, loss is 3.778740153312683 and perplexity is 43.760874998654224
At time: 103.95012140274048 and batch: 800, loss is 3.7453905630111692 and perplexity is 42.3255347794136
At time: 104.83584380149841 and batch: 850, loss is 3.8214945888519285 and perplexity is 45.67241882283667
At time: 105.71385335922241 and batch: 900, loss is 3.7570582008361817 and perplexity is 42.82226599478336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356916035691353 and perplexity of 78.01616397674029
finished 6 epochs...
Completing Train Step...
At time: 107.8067638874054 and batch: 50, loss is 3.8561844301223753 and perplexity is 47.28458907881747
At time: 108.69468474388123 and batch: 100, loss is 3.7358688163757323 and perplexity is 41.92443437965682
At time: 109.57422423362732 and batch: 150, loss is 3.7407453441619873 and perplexity is 42.12937935261849
At time: 110.4546320438385 and batch: 200, loss is 3.634090552330017 and perplexity is 37.86739881480398
At time: 111.33511352539062 and batch: 250, loss is 3.78499813079834 and perplexity is 44.03558824727834
At time: 112.21679639816284 and batch: 300, loss is 3.761010808944702 and perplexity is 42.99186058024686
At time: 113.09836220741272 and batch: 350, loss is 3.762718925476074 and perplexity is 43.06535844160426
At time: 113.98033475875854 and batch: 400, loss is 3.6894310760498046 and perplexity is 40.0220709642891
At time: 114.86114621162415 and batch: 450, loss is 3.730308184623718 and perplexity is 41.69195500379329
At time: 115.74263596534729 and batch: 500, loss is 3.6146929121017455 and perplexity is 37.139938948470856
At time: 116.62438941001892 and batch: 550, loss is 3.6758653020858763 and perplexity is 39.48280663507029
At time: 117.50637006759644 and batch: 600, loss is 3.6993542098999024 and perplexity is 40.42119232369081
At time: 118.387042760849 and batch: 650, loss is 3.5470332050323488 and perplexity is 34.7101865722583
At time: 119.273921251297 and batch: 700, loss is 3.5598971176147463 and perplexity is 35.15957965760224
At time: 120.15689516067505 and batch: 750, loss is 3.665747981071472 and perplexity is 39.08536034187795
At time: 121.03829717636108 and batch: 800, loss is 3.6304658174514772 and perplexity is 37.730387997516466
At time: 121.9201169013977 and batch: 850, loss is 3.7093661880493163 and perplexity is 40.827921100295484
At time: 122.80711317062378 and batch: 900, loss is 3.6447897386550903 and perplexity is 38.27472431042953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370029815255779 and perplexity of 79.04598844079453
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 124.9155580997467 and batch: 50, loss is 3.7764747285842897 and perplexity is 43.661850239216335
At time: 125.80369925498962 and batch: 100, loss is 3.6677931213378905 and perplexity is 39.165377181080665
At time: 126.6864082813263 and batch: 150, loss is 3.6656884002685546 and perplexity is 39.0830316740991
At time: 127.5690610408783 and batch: 200, loss is 3.55479691028595 and perplexity is 34.98071502283613
At time: 128.45319628715515 and batch: 250, loss is 3.69829448223114 and perplexity is 40.37837955672454
At time: 129.3487045764923 and batch: 300, loss is 3.6606800413131713 and perplexity is 38.88777917784178
At time: 130.233971118927 and batch: 350, loss is 3.6525203943252564 and perplexity is 38.57175968473444
At time: 131.11698698997498 and batch: 400, loss is 3.57496723651886 and perplexity is 35.69345135477789
At time: 131.99578428268433 and batch: 450, loss is 3.606189670562744 and perplexity is 36.825467982956006
At time: 132.87576961517334 and batch: 500, loss is 3.481161003112793 and perplexity is 32.49742979939085
At time: 133.75514578819275 and batch: 550, loss is 3.5214128065109254 and perplexity is 33.83219305747067
At time: 134.63596773147583 and batch: 600, loss is 3.5381316900253297 and perplexity is 34.40258441929848
At time: 135.51537704467773 and batch: 650, loss is 3.3713829469680787 and perplexity is 29.11876893840755
At time: 136.39778661727905 and batch: 700, loss is 3.366811156272888 and perplexity is 28.98594786793061
At time: 137.278981924057 and batch: 750, loss is 3.4606489181518554 and perplexity is 31.83762982869565
At time: 138.16081643104553 and batch: 800, loss is 3.4057236337661743 and perplexity is 30.13609533166301
At time: 139.04298377037048 and batch: 850, loss is 3.4730352926254273 and perplexity is 32.23443504991948
At time: 139.92594838142395 and batch: 900, loss is 3.4064057540893553 and perplexity is 30.156658787328244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335401561162243 and perplexity of 76.3556141835434
finished 8 epochs...
Completing Train Step...
At time: 142.01265382766724 and batch: 50, loss is 3.6861452770233156 and perplexity is 39.89078229468916
At time: 142.89696097373962 and batch: 100, loss is 3.568231043815613 and perplexity is 35.45382139114031
At time: 143.77686715126038 and batch: 150, loss is 3.5657946157455442 and perplexity is 35.36754585026545
At time: 144.66318082809448 and batch: 200, loss is 3.457050518989563 and perplexity is 31.723271205574985
At time: 145.5421371459961 and batch: 250, loss is 3.602326583862305 and perplexity is 36.683482435170255
At time: 146.41969513893127 and batch: 300, loss is 3.566923460960388 and perplexity is 35.40749287792279
At time: 147.30013513565063 and batch: 350, loss is 3.5625345516204834 and perplexity is 35.25243312223671
At time: 148.17983531951904 and batch: 400, loss is 3.4894790410995484 and perplexity is 32.7688720232153
At time: 149.05925989151 and batch: 450, loss is 3.5272429037094115 and perplexity is 34.03001412914313
At time: 149.9400293827057 and batch: 500, loss is 3.404967164993286 and perplexity is 30.113306937048996
At time: 150.82131958007812 and batch: 550, loss is 3.4481563806533813 and perplexity is 31.442371077028287
At time: 151.70649337768555 and batch: 600, loss is 3.4728366231918333 and perplexity is 32.22803168906299
At time: 152.5867702960968 and batch: 650, loss is 3.3104769372940064 and perplexity is 27.398189574673083
At time: 153.4659662246704 and batch: 700, loss is 3.310747814178467 and perplexity is 27.405612116157002
At time: 154.36244988441467 and batch: 750, loss is 3.4117719411849974 and perplexity is 30.318920033290357
At time: 155.2452404499054 and batch: 800, loss is 3.361003541946411 and perplexity is 28.81809654148961
At time: 156.12766027450562 and batch: 850, loss is 3.435698637962341 and perplexity is 31.05309986327405
At time: 157.0088496208191 and batch: 900, loss is 3.3764471864700316 and perplexity is 29.266607387007628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34646104786494 and perplexity of 77.20475495883042
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.1223669052124 and batch: 50, loss is 3.6550294065475466 and perplexity is 38.668658210178506
At time: 160.01027154922485 and batch: 100, loss is 3.550281047821045 and perplexity is 34.8231030696691
At time: 160.8903784751892 and batch: 150, loss is 3.554838557243347 and perplexity is 34.98217189352132
At time: 161.77044034004211 and batch: 200, loss is 3.4398692655563354 and perplexity is 31.18288122518799
At time: 162.64954495429993 and batch: 250, loss is 3.5893614196777346 and perplexity is 36.21094494487523
At time: 163.5294394493103 and batch: 300, loss is 3.550895586013794 and perplexity is 34.84450977344028
At time: 164.41859102249146 and batch: 350, loss is 3.538909149169922 and perplexity is 34.42934142304374
At time: 165.30104684829712 and batch: 400, loss is 3.4652616119384767 and perplexity is 31.984826291100184
At time: 166.18019700050354 and batch: 450, loss is 3.499031138420105 and perplexity is 33.083383207187865
At time: 167.06925630569458 and batch: 500, loss is 3.3735038900375365 and perplexity is 29.180593729834005
At time: 167.94772624969482 and batch: 550, loss is 3.409350619316101 and perplexity is 30.24559697435867
At time: 168.8271996974945 and batch: 600, loss is 3.4348591947555542 and perplexity is 31.027043487498055
At time: 169.70717549324036 and batch: 650, loss is 3.260989856719971 and perplexity is 26.075335218003737
At time: 170.60011529922485 and batch: 700, loss is 3.252158923149109 and perplexity is 25.84607942607178
At time: 171.47975254058838 and batch: 750, loss is 3.351054353713989 and perplexity is 28.532801455112274
At time: 172.36091446876526 and batch: 800, loss is 3.29356294631958 and perplexity is 26.93867392022672
At time: 173.24007654190063 and batch: 850, loss is 3.363189902305603 and perplexity is 28.88117221334353
At time: 174.11943292617798 and batch: 900, loss is 3.315926651954651 and perplexity is 27.547909484919245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.339683219178082 and perplexity of 76.68324370933452
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 176.2175431251526 and batch: 50, loss is 3.6457903146743775 and perplexity is 38.31304024752661
At time: 177.0958957672119 and batch: 100, loss is 3.5375194358825683 and perplexity is 34.38152774114305
At time: 177.9875602722168 and batch: 150, loss is 3.5438561964035036 and perplexity is 34.600086996267315
At time: 178.87126278877258 and batch: 200, loss is 3.424808979034424 and perplexity is 30.716776742702073
At time: 179.76065039634705 and batch: 250, loss is 3.5772363471984865 and perplexity is 35.774535706332756
At time: 180.64209723472595 and batch: 300, loss is 3.5396560716629026 and perplexity is 34.45506707891061
At time: 181.52113151550293 and batch: 350, loss is 3.5242180919647215 and perplexity is 33.92723526445631
At time: 182.39970016479492 and batch: 400, loss is 3.452038369178772 and perplexity is 31.564667223296787
At time: 183.2900960445404 and batch: 450, loss is 3.4821668243408204 and perplexity is 32.53013284809264
At time: 184.1715760231018 and batch: 500, loss is 3.3569466066360474 and perplexity is 28.7014202222213
At time: 185.05701279640198 and batch: 550, loss is 3.3921200942993166 and perplexity is 29.728913605061223
At time: 185.94130611419678 and batch: 600, loss is 3.419453501701355 and perplexity is 30.552713451855883
At time: 186.81967949867249 and batch: 650, loss is 3.2443022966384887 and perplexity is 25.64381204333644
At time: 187.69576787948608 and batch: 700, loss is 3.231960344314575 and perplexity is 25.329262408763377
At time: 188.5797564983368 and batch: 750, loss is 3.328635048866272 and perplexity is 27.900233246061738
At time: 189.45926427841187 and batch: 800, loss is 3.270576276779175 and perplexity is 26.326506326928914
At time: 190.3389105796814 and batch: 850, loss is 3.339105472564697 and perplexity is 28.193895209185616
At time: 191.21781826019287 and batch: 900, loss is 3.295045404434204 and perplexity is 26.97863899193018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3336419928563785 and perplexity of 76.22137939712256
finished 11 epochs...
Completing Train Step...
At time: 193.2965087890625 and batch: 50, loss is 3.63143292427063 and perplexity is 37.766894963260505
At time: 194.19044542312622 and batch: 100, loss is 3.5206874895095823 and perplexity is 33.80766288980261
At time: 195.0739505290985 and batch: 150, loss is 3.525071692466736 and perplexity is 33.95620793329314
At time: 195.9563765525818 and batch: 200, loss is 3.4072413635253906 and perplexity is 30.181868507243276
At time: 196.83803272247314 and batch: 250, loss is 3.5581891345977783 and perplexity is 35.099578947346586
At time: 197.7197871208191 and batch: 300, loss is 3.523001446723938 and perplexity is 33.88598295494045
At time: 198.59933352470398 and batch: 350, loss is 3.5091082525253294 and perplexity is 33.41845367092219
At time: 199.48207187652588 and batch: 400, loss is 3.4382747125625612 and perplexity is 31.133198090298624
At time: 200.3667185306549 and batch: 450, loss is 3.4696700096130373 and perplexity is 32.12613937822932
At time: 201.2499520778656 and batch: 500, loss is 3.3464275121688845 and perplexity is 28.401089643752012
At time: 202.13189554214478 and batch: 550, loss is 3.3826339292526244 and perplexity is 29.448233617090693
At time: 203.0134515762329 and batch: 600, loss is 3.410987014770508 and perplexity is 30.295131249541175
At time: 203.8950629234314 and batch: 650, loss is 3.2375326585769653 and perplexity is 25.470798995877242
At time: 204.77582001686096 and batch: 700, loss is 3.2277785253524782 and perplexity is 25.223561184656564
At time: 205.6700119972229 and batch: 750, loss is 3.327121000289917 and perplexity is 27.858022899956158
At time: 206.55119371414185 and batch: 800, loss is 3.2717530870437623 and perplexity is 26.357505866515805
At time: 207.43255043029785 and batch: 850, loss is 3.3431635189056395 and perplexity is 28.308539801682638
At time: 208.31533694267273 and batch: 900, loss is 3.3009545612335205 and perplexity is 27.138531950985353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33322645213506 and perplexity of 76.18971288996525
finished 12 epochs...
Completing Train Step...
At time: 210.42005586624146 and batch: 50, loss is 3.624194836616516 and perplexity is 37.49452178668892
At time: 211.3063280582428 and batch: 100, loss is 3.5122400951385497 and perplexity is 33.52327907084281
At time: 212.18647694587708 and batch: 150, loss is 3.515793924331665 and perplexity is 33.64262702416332
At time: 213.0676600933075 and batch: 200, loss is 3.397770962715149 and perplexity is 29.897383335827254
At time: 213.9490909576416 and batch: 250, loss is 3.548185248374939 and perplexity is 34.75019725421363
At time: 214.83004522323608 and batch: 300, loss is 3.5136375617980957 and perplexity is 33.57015948494137
At time: 215.70984864234924 and batch: 350, loss is 3.5001470613479615 and perplexity is 33.12032231980787
At time: 216.59089994430542 and batch: 400, loss is 3.429862742424011 and perplexity is 30.87240498737769
At time: 217.47213339805603 and batch: 450, loss is 3.4616956758499144 and perplexity is 31.87097356116655
At time: 218.35571360588074 and batch: 500, loss is 3.3393313598632814 and perplexity is 28.200264571362915
At time: 219.24069571495056 and batch: 550, loss is 3.3762565517425536 and perplexity is 29.261028687048068
At time: 220.13577723503113 and batch: 600, loss is 3.405461049079895 and perplexity is 30.128183093386433
At time: 221.01739645004272 and batch: 650, loss is 3.2329968214035034 and perplexity is 25.355529219051544
At time: 221.89737725257874 and batch: 700, loss is 3.224572768211365 and perplexity is 25.142830044617554
At time: 222.77776765823364 and batch: 750, loss is 3.3252354001998903 and perplexity is 27.805543302783825
At time: 223.65882968902588 and batch: 800, loss is 3.2712085008621217 and perplexity is 26.343155840805895
At time: 224.548677444458 and batch: 850, loss is 3.3439375734329224 and perplexity is 28.33046063794367
At time: 225.42921590805054 and batch: 900, loss is 3.302490315437317 and perplexity is 27.180242085573287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333695503130351 and perplexity of 76.22545813314285
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 227.50879287719727 and batch: 50, loss is 3.6212565422058107 and perplexity is 37.384513540345715
At time: 228.37990522384644 and batch: 100, loss is 3.511692123413086 and perplexity is 33.50491429391595
At time: 229.2532296180725 and batch: 150, loss is 3.5171699571609496 and perplexity is 33.68895224861718
At time: 230.13270449638367 and batch: 200, loss is 3.398463182449341 and perplexity is 29.91808605916317
At time: 231.01262664794922 and batch: 250, loss is 3.5497002935409547 and perplexity is 34.80288527486905
At time: 231.89172625541687 and batch: 300, loss is 3.513525199890137 and perplexity is 33.56638768967818
At time: 232.77745366096497 and batch: 350, loss is 3.5010194540023805 and perplexity is 33.14922885280186
At time: 233.65764355659485 and batch: 400, loss is 3.4300745058059694 and perplexity is 30.878943324532763
At time: 234.53589987754822 and batch: 450, loss is 3.4592024850845338 and perplexity is 31.791612116919346
At time: 235.418212890625 and batch: 500, loss is 3.335557012557983 and perplexity is 28.094027592515005
At time: 236.30355429649353 and batch: 550, loss is 3.3708513879776003 and perplexity is 29.10329470808416
At time: 237.1830599308014 and batch: 600, loss is 3.399462537765503 and perplexity is 29.94799980225528
At time: 238.0621702671051 and batch: 650, loss is 3.2275985288619995 and perplexity is 25.21902144074719
At time: 238.94234418869019 and batch: 700, loss is 3.2169612216949464 and perplexity is 24.95218071275581
At time: 239.82216691970825 and batch: 750, loss is 3.3151287317276 and perplexity is 27.52593721795215
At time: 240.70332765579224 and batch: 800, loss is 3.261002149581909 and perplexity is 26.07565576046975
At time: 241.583074092865 and batch: 850, loss is 3.333396105766296 and perplexity is 28.033384563049424
At time: 242.46176409721375 and batch: 900, loss is 3.2924572658538818 and perplexity is 26.90890481529301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333121103783176 and perplexity of 76.18168685205416
finished 14 epochs...
Completing Train Step...
At time: 244.55355310440063 and batch: 50, loss is 3.617455830574036 and perplexity is 37.24269546041597
At time: 245.4381012916565 and batch: 100, loss is 3.507924656867981 and perplexity is 33.37892313298336
At time: 246.31614542007446 and batch: 150, loss is 3.512137198448181 and perplexity is 33.519829813837724
At time: 247.1954803466797 and batch: 200, loss is 3.394024524688721 and perplexity is 29.785584197045456
At time: 248.074809551239 and batch: 250, loss is 3.544704065322876 and perplexity is 34.62943577483942
At time: 248.95519518852234 and batch: 300, loss is 3.5097218418121336 and perplexity is 33.438965168250206
At time: 249.83477067947388 and batch: 350, loss is 3.496874141693115 and perplexity is 33.01209936503744
At time: 250.71519684791565 and batch: 400, loss is 3.426615071296692 and perplexity is 30.772304204264152
At time: 251.6079933643341 and batch: 450, loss is 3.4563190841674807 and perplexity is 31.700076184202135
At time: 252.48721432685852 and batch: 500, loss is 3.332872395515442 and perplexity is 28.018707035908793
At time: 253.36587071418762 and batch: 550, loss is 3.3688738107681275 and perplexity is 29.04579756700516
At time: 254.2436122894287 and batch: 600, loss is 3.398026256561279 and perplexity is 29.905016928171403
At time: 255.1434042453766 and batch: 650, loss is 3.226411318778992 and perplexity is 25.189098929877023
At time: 256.0241115093231 and batch: 700, loss is 3.2165101766586304 and perplexity is 24.94092869327459
At time: 256.9040336608887 and batch: 750, loss is 3.3151925897598264 and perplexity is 27.527695026262574
At time: 257.783762216568 and batch: 800, loss is 3.2616196298599243 and perplexity is 26.0917619357492
At time: 258.66232991218567 and batch: 850, loss is 3.3350000953674317 and perplexity is 28.078385901564474
At time: 259.5420985221863 and batch: 900, loss is 3.2946614742279055 and perplexity is 26.96828306560003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332658323523116 and perplexity of 76.14643962768984
finished 15 epochs...
Completing Train Step...
At time: 261.621541261673 and batch: 50, loss is 3.6150818777084353 and perplexity is 37.154387917250155
At time: 262.50813031196594 and batch: 100, loss is 3.5053260278701783 and perplexity is 33.29229629966341
At time: 263.38864040374756 and batch: 150, loss is 3.509153289794922 and perplexity is 33.41995878072229
At time: 264.26834511756897 and batch: 200, loss is 3.391131000518799 and perplexity is 29.699523458714438
At time: 265.148574590683 and batch: 250, loss is 3.541559252738953 and perplexity is 34.52070374997249
At time: 266.0290575027466 and batch: 300, loss is 3.5069567155838013 and perplexity is 33.34662992674656
At time: 266.9090676307678 and batch: 350, loss is 3.4942096662521362 and perplexity is 32.92425651654988
At time: 267.7901475429535 and batch: 400, loss is 3.4241916370391845 and perplexity is 30.697819838508998
At time: 268.67094826698303 and batch: 450, loss is 3.4541397047042848 and perplexity is 31.631064917368576
At time: 269.54969692230225 and batch: 500, loss is 3.331049180030823 and perplexity is 27.96766943579124
At time: 270.4302055835724 and batch: 550, loss is 3.3673406887054442 and perplexity is 29.00130093202899
At time: 271.3112485408783 and batch: 600, loss is 3.3967767429351805 and perplexity is 29.86767353742737
At time: 272.1933913230896 and batch: 650, loss is 3.225432734489441 and perplexity is 25.164461330346356
At time: 273.0745086669922 and batch: 700, loss is 3.216031594276428 and perplexity is 24.928995259999457
At time: 273.95530438423157 and batch: 750, loss is 3.3151101875305176 and perplexity is 27.525426776280383
At time: 274.85443449020386 and batch: 800, loss is 3.2619399404525757 and perplexity is 26.100120742113806
At time: 275.7410707473755 and batch: 850, loss is 3.3358656930923463 and perplexity is 28.10270101055346
At time: 276.63186955451965 and batch: 900, loss is 3.295765128135681 and perplexity is 26.998063147020186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332576803965111 and perplexity of 76.14023245659408
finished 16 epochs...
Completing Train Step...
At time: 278.75047159194946 and batch: 50, loss is 3.6131148290634156 and perplexity is 37.08137526210538
At time: 279.6462457180023 and batch: 100, loss is 3.503154215812683 and perplexity is 33.22007014834075
At time: 280.52669978141785 and batch: 150, loss is 3.5067671871185304 and perplexity is 33.340310390039555
At time: 281.40692949295044 and batch: 200, loss is 3.388785734176636 and perplexity is 29.629951779915366
At time: 282.2873306274414 and batch: 250, loss is 3.5390542793273925 and perplexity is 34.434338521392135
At time: 283.16811990737915 and batch: 300, loss is 3.5046748113632202 and perplexity is 33.270622864573824
At time: 284.0491569042206 and batch: 350, loss is 3.492018222808838 and perplexity is 32.85218387088822
At time: 284.9302079677582 and batch: 400, loss is 3.422165775299072 and perplexity is 30.635693250979553
At time: 285.8102869987488 and batch: 450, loss is 3.4522647571563723 and perplexity is 31.57181389340267
At time: 286.69072437286377 and batch: 500, loss is 3.329453639984131 and perplexity is 27.9230814795878
At time: 287.57203364372253 and batch: 550, loss is 3.3659678888320923 and perplexity is 28.961515264910123
At time: 288.45408487319946 and batch: 600, loss is 3.3956132745742797 and perplexity is 29.83294365173215
At time: 289.33609676361084 and batch: 650, loss is 3.2245000982284546 and perplexity is 25.141002981975003
At time: 290.2291615009308 and batch: 700, loss is 3.2154791927337647 and perplexity is 24.915228247370372
At time: 291.1102063655853 and batch: 750, loss is 3.3148684549331664 and perplexity is 27.518773787527078
At time: 291.9907648563385 and batch: 800, loss is 3.262013669013977 and perplexity is 26.1020451374091
At time: 292.87135553359985 and batch: 850, loss is 3.3363095235824587 and perplexity is 28.115176614438358
At time: 293.7512586116791 and batch: 900, loss is 3.2963555812835694 and perplexity is 27.01400894555243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332648708395762 and perplexity of 76.14570747349516
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 295.83045721054077 and batch: 50, loss is 3.612139120101929 and perplexity is 37.045212277101115
At time: 296.7147397994995 and batch: 100, loss is 3.502790536880493 and perplexity is 33.20799090531816
At time: 297.5945951938629 and batch: 150, loss is 3.506845326423645 and perplexity is 33.34291568051221
At time: 298.4802839756012 and batch: 200, loss is 3.389056239128113 and perplexity is 29.63796791273679
At time: 299.36024928092957 and batch: 250, loss is 3.5392752265930176 and perplexity is 34.44194753489646
At time: 300.24053859710693 and batch: 300, loss is 3.504289526939392 and perplexity is 33.25780668090955
At time: 301.1207318305969 and batch: 350, loss is 3.4920702457427977 and perplexity is 32.85389298233628
At time: 301.999559879303 and batch: 400, loss is 3.422051033973694 and perplexity is 30.632178272592547
At time: 302.8786895275116 and batch: 450, loss is 3.451555805206299 and perplexity is 31.549438926700887
At time: 303.76008582115173 and batch: 500, loss is 3.3284476613998413 and perplexity is 27.8950055818556
At time: 304.63982033729553 and batch: 550, loss is 3.36400607585907 and perplexity is 28.904753884356246
At time: 305.52027797698975 and batch: 600, loss is 3.393339948654175 and perplexity is 29.76520067775966
At time: 306.39950156211853 and batch: 650, loss is 3.222474174499512 and perplexity is 25.090120786591168
At time: 307.2783749103546 and batch: 700, loss is 3.2127915143966677 and perplexity is 24.848354036586027
At time: 308.15861225128174 and batch: 750, loss is 3.3117720460891724 and perplexity is 27.433696198470294
At time: 309.03758454322815 and batch: 800, loss is 3.2591127920150758 and perplexity is 26.026436034421014
At time: 309.9160313606262 and batch: 850, loss is 3.3328311967849733 and perplexity is 28.01755272452779
At time: 310.79117727279663 and batch: 900, loss is 3.292478814125061 and perplexity is 26.909484661918434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332183419841609 and perplexity of 76.11028598860464
finished 18 epochs...
Completing Train Step...
At time: 312.8745279312134 and batch: 50, loss is 3.611345901489258 and perplexity is 37.01583897647709
At time: 313.75949907302856 and batch: 100, loss is 3.502003688812256 and perplexity is 33.18187153915829
At time: 314.6373345851898 and batch: 150, loss is 3.5058498668670652 and perplexity is 33.30974067138328
At time: 315.51669120788574 and batch: 200, loss is 3.3881183862686157 and perplexity is 29.61018489001022
At time: 316.3964784145355 and batch: 250, loss is 3.5383773183822633 and perplexity is 34.411035707479186
At time: 317.2760293483734 and batch: 300, loss is 3.5034982061386106 and perplexity is 33.23149949677836
At time: 318.16837096214294 and batch: 350, loss is 3.4912303781509397 and perplexity is 32.82631164631045
At time: 319.0477011203766 and batch: 400, loss is 3.421371188163757 and perplexity is 30.611360191888988
At time: 319.9389235973358 and batch: 450, loss is 3.4510006427764894 and perplexity is 31.53192872448015
At time: 320.8370566368103 and batch: 500, loss is 3.327871479988098 and perplexity is 27.878937627631668
At time: 321.71732473373413 and batch: 550, loss is 3.3636505460739134 and perplexity is 28.89447921000235
At time: 322.59499406814575 and batch: 600, loss is 3.3931231451034547 and perplexity is 29.758748176052737
At time: 323.47216868400574 and batch: 650, loss is 3.2222713232040405 and perplexity is 25.08503173926345
At time: 324.3477690219879 and batch: 700, loss is 3.2127666234970094 and perplexity is 24.84773554639645
At time: 325.2270441055298 and batch: 750, loss is 3.311800298690796 and perplexity is 27.434471282709072
At time: 326.10770630836487 and batch: 800, loss is 3.2592156648635866 and perplexity is 26.029113585753766
At time: 326.9866976737976 and batch: 850, loss is 3.3331749725341795 and perplexity is 28.027186135478246
At time: 327.87864446640015 and batch: 900, loss is 3.292985939979553 and perplexity is 26.92313461815187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331950984589041 and perplexity of 76.09259733087133
finished 19 epochs...
Completing Train Step...
At time: 329.9822633266449 and batch: 50, loss is 3.6107137203216553 and perplexity is 36.992445655360775
At time: 330.8791539669037 and batch: 100, loss is 3.5013294410705567 and perplexity is 33.15950627791839
At time: 331.76095724105835 and batch: 150, loss is 3.5050528860092163 and perplexity is 33.28320402169382
At time: 332.6431007385254 and batch: 200, loss is 3.3873612689971924 and perplexity is 29.58777499215081
At time: 333.52532839775085 and batch: 250, loss is 3.5375982999801634 and perplexity is 34.38423931622385
At time: 334.41542768478394 and batch: 300, loss is 3.5028273391723634 and perplexity is 33.20921305798076
At time: 335.3011746406555 and batch: 350, loss is 3.490542950630188 and perplexity is 32.803753690648094
At time: 336.18271684646606 and batch: 400, loss is 3.4207763385772707 and perplexity is 30.593156451717487
At time: 337.0655891895294 and batch: 450, loss is 3.4504849910736084 and perplexity is 31.515673423135343
At time: 337.95820927619934 and batch: 500, loss is 3.3273858499526976 and perplexity is 27.86540206506851
At time: 338.8588581085205 and batch: 550, loss is 3.3633181381225588 and perplexity is 28.88487605153423
At time: 339.74192547798157 and batch: 600, loss is 3.392887806892395 and perplexity is 29.75174562950932
At time: 340.63518834114075 and batch: 650, loss is 3.222073450088501 and perplexity is 25.08006857693419
At time: 341.51989459991455 and batch: 700, loss is 3.212700662612915 and perplexity is 24.84609662184512
At time: 342.4078676700592 and batch: 750, loss is 3.311806073188782 and perplexity is 27.434629703465642
At time: 343.2891266345978 and batch: 800, loss is 3.2592950296401977 and perplexity is 26.03117946251672
At time: 344.17054533958435 and batch: 850, loss is 3.3334422159194945 and perplexity is 28.03467721650829
At time: 345.05301427841187 and batch: 900, loss is 3.2933603620529173 and perplexity is 26.93321712147517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33182724208048 and perplexity of 76.08318202454346
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f05edd19b70>
ELAPSED
1059.7551982402802


RESULTS SO FAR:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.01450518464667, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -76.08318202454346, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8745980334815576, 'dropout': 0.43719893342278404, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9350301414536679, 'dropout': 0.2917336399028122, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3207132816314697 and batch: 50, loss is 7.064891166687012 and perplexity is 1170.154612722348
At time: 2.42569637298584 and batch: 100, loss is 6.466463527679443 and perplexity is 643.205023292422
At time: 3.5342648029327393 and batch: 150, loss is 6.0918528270721435 and perplexity is 442.24004684776514
At time: 4.641963958740234 and batch: 200, loss is 5.912359199523926 and perplexity is 369.577033679538
At time: 5.748952865600586 and batch: 250, loss is 5.98037709236145 and perplexity is 395.58951381664167
At time: 6.859176158905029 and batch: 300, loss is 5.903625144958496 and perplexity is 366.3631831047381
At time: 7.968210220336914 and batch: 350, loss is 5.90688380241394 and perplexity is 367.5589825149268
At time: 9.08795166015625 and batch: 400, loss is 5.793069953918457 and perplexity is 328.0184818559349
At time: 10.199198246002197 and batch: 450, loss is 5.807400093078614 and perplexity is 332.7528735759867
At time: 11.307649374008179 and batch: 500, loss is 5.765478582382202 and perplexity is 319.091719174621
At time: 12.41992974281311 and batch: 550, loss is 5.8013842487335205 and perplexity is 330.7570932540539
At time: 13.531372547149658 and batch: 600, loss is 5.7186587619781495 and perplexity is 304.4962470083144
At time: 14.64347219467163 and batch: 650, loss is 5.614768590927124 and perplexity is 274.44986165835974
At time: 15.749478101730347 and batch: 700, loss is 5.701456871032715 and perplexity is 299.30312954925813
At time: 16.856929779052734 and batch: 750, loss is 5.660238609313965 and perplexity is 287.2171670716293
At time: 17.968518495559692 and batch: 800, loss is 5.651381807327271 and perplexity is 284.6845733913857
At time: 19.07609987258911 and batch: 850, loss is 5.688622093200683 and perplexity is 295.4861875755878
At time: 20.185651779174805 and batch: 900, loss is 5.565485229492188 and perplexity is 261.25194010539366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.545009247244221 and perplexity of 255.95694512870924
finished 1 epochs...
Completing Train Step...
At time: 22.45490336418152 and batch: 50, loss is 5.45151050567627 and perplexity is 233.11001410591513
At time: 23.346348762512207 and batch: 100, loss is 5.31377721786499 and perplexity is 203.1159946082677
At time: 24.23462700843811 and batch: 150, loss is 5.276488828659057 and perplexity is 195.68159611351732
At time: 25.11449694633484 and batch: 200, loss is 5.132716693878174 and perplexity is 169.47691008911383
At time: 25.99365544319153 and batch: 250, loss is 5.179238958358765 and perplexity is 177.54763841777026
At time: 26.873079776763916 and batch: 300, loss is 5.099785804748535 and perplexity is 163.9867783485835
At time: 27.753920793533325 and batch: 350, loss is 5.062481241226196 and perplexity is 157.9820219183768
At time: 28.63453722000122 and batch: 400, loss is 4.902575435638428 and perplexity is 134.63608011523388
At time: 29.513967990875244 and batch: 450, loss is 4.904564771652222 and perplexity is 134.90418310321064
At time: 30.40266442298889 and batch: 500, loss is 4.814824647903443 and perplexity is 123.32518508428137
At time: 31.29596781730652 and batch: 550, loss is 4.873331794738769 and perplexity is 130.75584358261023
At time: 32.18800354003906 and batch: 600, loss is 4.7885949993133545 and perplexity is 120.13246384111999
At time: 33.07552909851074 and batch: 650, loss is 4.667656059265137 and perplexity is 106.44794212057687
At time: 33.964768409729004 and batch: 700, loss is 4.7178850173950195 and perplexity is 111.93126946060467
At time: 34.84620141983032 and batch: 750, loss is 4.727217769622802 and perplexity is 112.98078608658273
At time: 35.72590136528015 and batch: 800, loss is 4.678131456375122 and perplexity is 107.56888750941279
At time: 36.607476472854614 and batch: 850, loss is 4.7191543292999265 and perplexity is 112.07343536080958
At time: 37.48735952377319 and batch: 900, loss is 4.635126533508301 and perplexity is 103.040955303957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.738550473565924 and perplexity of 114.26844644218319
finished 2 epochs...
Completing Train Step...
At time: 39.58736491203308 and batch: 50, loss is 4.684777860641479 and perplexity is 108.28621500590783
At time: 40.46546459197998 and batch: 100, loss is 4.564028539657593 and perplexity is 95.96931833020352
At time: 41.34506011009216 and batch: 150, loss is 4.571523380279541 and perplexity is 96.69129524703858
At time: 42.225568532943726 and batch: 200, loss is 4.467534732818604 and perplexity is 87.14163059020373
At time: 43.1063973903656 and batch: 250, loss is 4.592111253738404 and perplexity is 98.70259656456227
At time: 43.98632049560547 and batch: 300, loss is 4.546732683181762 and perplexity is 94.3237188234841
At time: 44.86625671386719 and batch: 350, loss is 4.552249145507813 and perplexity is 94.84548990690627
At time: 45.7483913898468 and batch: 400, loss is 4.434734168052674 and perplexity is 84.32970437884636
At time: 46.62869095802307 and batch: 450, loss is 4.465901737213135 and perplexity is 86.99944481636537
At time: 47.52019286155701 and batch: 500, loss is 4.356528110504151 and perplexity is 77.98590541113413
At time: 48.40251088142395 and batch: 550, loss is 4.428470907211303 and perplexity is 83.80317605699514
At time: 49.284268617630005 and batch: 600, loss is 4.405581579208374 and perplexity is 81.90676426513272
At time: 50.16534066200256 and batch: 650, loss is 4.269311490058899 and perplexity is 71.47240921047214
At time: 51.057591915130615 and batch: 700, loss is 4.295880393981934 and perplexity is 73.39680411330902
At time: 51.94021391868591 and batch: 750, loss is 4.361225924491882 and perplexity is 78.35313059084199
At time: 52.832040548324585 and batch: 800, loss is 4.3154997062683105 and perplexity is 74.85101762280756
At time: 53.72485041618347 and batch: 850, loss is 4.382742147445679 and perplexity is 80.05726150496992
At time: 54.606980323791504 and batch: 900, loss is 4.312386732101441 and perplexity is 74.61837063848417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5155267584813785 and perplexity of 91.42571260999668
finished 3 epochs...
Completing Train Step...
At time: 56.73465132713318 and batch: 50, loss is 4.398435535430909 and perplexity is 81.32354129237162
At time: 57.62388253211975 and batch: 100, loss is 4.2724324274063115 and perplexity is 71.69581856364512
At time: 58.51699161529541 and batch: 150, loss is 4.277252950668335 and perplexity is 72.04226427875952
At time: 59.40933704376221 and batch: 200, loss is 4.167151069641113 and perplexity is 64.53134467036269
At time: 60.29841160774231 and batch: 250, loss is 4.310427417755127 and perplexity is 74.47231292760301
At time: 61.18252968788147 and batch: 300, loss is 4.278742184638977 and perplexity is 72.1496319940264
At time: 62.07243037223816 and batch: 350, loss is 4.286363134384155 and perplexity is 72.70158123090404
At time: 62.955559968948364 and batch: 400, loss is 4.188188233375549 and perplexity is 65.90328136573308
At time: 63.85005569458008 and batch: 450, loss is 4.227377271652221 and perplexity is 68.53724167730712
At time: 64.74134635925293 and batch: 500, loss is 4.096443419456482 and perplexity is 60.12606368262101
At time: 65.63038110733032 and batch: 550, loss is 4.174564237594605 and perplexity is 65.01150391831384
At time: 66.51657509803772 and batch: 600, loss is 4.175721855163574 and perplexity is 65.08680595450562
At time: 67.39655542373657 and batch: 650, loss is 4.023802223205567 and perplexity is 55.913297007425975
At time: 68.27694869041443 and batch: 700, loss is 4.0436987829208375 and perplexity is 57.036920311857855
At time: 69.15391373634338 and batch: 750, loss is 4.134409852027893 and perplexity is 62.45272384464678
At time: 70.028480052948 and batch: 800, loss is 4.0910789585113525 and perplexity is 59.80438335441765
At time: 70.90556478500366 and batch: 850, loss is 4.165311994552613 and perplexity is 64.41277574395004
At time: 71.78569531440735 and batch: 900, loss is 4.104374785423278 and perplexity is 60.60484166969299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.415121836205051 and perplexity of 82.69191516152291
finished 4 epochs...
Completing Train Step...
At time: 73.86796426773071 and batch: 50, loss is 4.197386035919189 and perplexity is 66.51224299597715
At time: 74.7514123916626 and batch: 100, loss is 4.073501214981079 and perplexity is 58.76234243625542
At time: 75.63803339004517 and batch: 150, loss is 4.078174161911011 and perplexity is 59.03757832444052
At time: 76.5154800415039 and batch: 200, loss is 3.96709370136261 and perplexity is 52.83076552209301
At time: 77.3915548324585 and batch: 250, loss is 4.116026310920716 and perplexity is 61.31511034911722
At time: 78.26651763916016 and batch: 300, loss is 4.086706047058105 and perplexity is 59.54343504923765
At time: 79.14199686050415 and batch: 350, loss is 4.093411674499512 and perplexity is 59.944052836912924
At time: 80.02208662033081 and batch: 400, loss is 4.0073026847839355 and perplexity is 54.99832250091292
At time: 80.89858889579773 and batch: 450, loss is 4.048668432235718 and perplexity is 57.321079304247014
At time: 81.77058529853821 and batch: 500, loss is 3.9144248723983766 and perplexity is 50.12023768817351
At time: 82.64420175552368 and batch: 550, loss is 3.9924014520645144 and perplexity is 54.184855580346046
At time: 83.52688026428223 and batch: 600, loss is 4.005159969329834 and perplexity is 54.88060291017352
At time: 84.40399527549744 and batch: 650, loss is 3.8486847400665285 and perplexity is 46.93129576739945
At time: 85.28075432777405 and batch: 700, loss is 3.8657449960708616 and perplexity is 47.738824424621114
At time: 86.15851163864136 and batch: 750, loss is 3.967744288444519 and perplexity is 52.86514771876283
At time: 87.03731274604797 and batch: 800, loss is 3.9238095808029176 and perplexity is 50.59281553827324
At time: 87.91606783866882 and batch: 850, loss is 4.000883140563965 and perplexity is 54.6463891719908
At time: 88.79324007034302 and batch: 900, loss is 3.9423353147506712 and perplexity is 51.53882023849605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376474458877355 and perplexity of 79.55705672361812
finished 5 epochs...
Completing Train Step...
At time: 90.87002682685852 and batch: 50, loss is 4.03745973110199 and perplexity is 56.682171808330175
At time: 91.74661827087402 and batch: 100, loss is 3.917660002708435 and perplexity is 50.28264575224339
At time: 92.62273621559143 and batch: 150, loss is 3.9255838871002195 and perplexity is 50.682662373813045
At time: 93.4993155002594 and batch: 200, loss is 3.8152214241027833 and perplexity is 45.386805002978846
At time: 94.37857151031494 and batch: 250, loss is 3.9621990060806276 and perplexity is 52.57280685290419
At time: 95.25726985931396 and batch: 300, loss is 3.9362173318862914 and perplexity is 51.224469197074185
At time: 96.15091729164124 and batch: 350, loss is 3.943129291534424 and perplexity is 51.579757114541465
At time: 97.0332670211792 and batch: 400, loss is 3.8629798221588136 and perplexity is 47.607000614606314
At time: 97.91176080703735 and batch: 450, loss is 3.9010474634170533 and perplexity is 49.45422347482613
At time: 98.78998756408691 and batch: 500, loss is 3.772181262969971 and perplexity is 43.47479143917067
At time: 99.66977858543396 and batch: 550, loss is 3.848320927619934 and perplexity is 46.914224683389165
At time: 100.55895400047302 and batch: 600, loss is 3.86994658946991 and perplexity is 47.93982552094593
At time: 101.4381091594696 and batch: 650, loss is 3.7109940528869627 and perplexity is 40.894437562665225
At time: 102.31663942337036 and batch: 700, loss is 3.727928595542908 and perplexity is 41.592863228520315
At time: 103.1957859992981 and batch: 750, loss is 3.8312834453582765 and perplexity is 46.12169493918041
At time: 104.07574200630188 and batch: 800, loss is 3.7880947637557982 and perplexity is 44.17216165086219
At time: 104.95692682266235 and batch: 850, loss is 3.863294048309326 and perplexity is 47.62196232970485
At time: 105.84721326828003 and batch: 900, loss is 3.809842085838318 and perplexity is 45.143309535541526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3746337890625 and perplexity of 79.41075314035021
finished 6 epochs...
Completing Train Step...
At time: 107.93921041488647 and batch: 50, loss is 3.9092548370361326 and perplexity is 49.86178297266276
At time: 108.8189582824707 and batch: 100, loss is 3.7909755325317382 and perplexity is 44.299594899644944
At time: 109.69931316375732 and batch: 150, loss is 3.7992534160614015 and perplexity is 44.667823761764545
At time: 110.57970404624939 and batch: 200, loss is 3.6900561475753784 and perplexity is 40.04709544147265
At time: 111.45975637435913 and batch: 250, loss is 3.8362808895111082 and perplexity is 46.352762426405995
At time: 112.34031343460083 and batch: 300, loss is 3.81032621383667 and perplexity is 45.16516996682241
At time: 113.22026419639587 and batch: 350, loss is 3.817323088645935 and perplexity is 45.48229314865804
At time: 114.10133910179138 and batch: 400, loss is 3.743340926170349 and perplexity is 42.2388716483348
At time: 114.98131227493286 and batch: 450, loss is 3.779843635559082 and perplexity is 43.80919100032439
At time: 115.86170744895935 and batch: 500, loss is 3.655772395133972 and perplexity is 38.69739925769402
At time: 116.7568142414093 and batch: 550, loss is 3.7278412437438964 and perplexity is 41.589230175770425
At time: 117.63536405563354 and batch: 600, loss is 3.752864785194397 and perplexity is 42.6430704181034
At time: 118.51360726356506 and batch: 650, loss is 3.596973834037781 and perplexity is 36.48764952088363
At time: 119.39120149612427 and batch: 700, loss is 3.6117989301681517 and perplexity is 37.032612012152974
At time: 120.26895785331726 and batch: 750, loss is 3.718389835357666 and perplexity is 41.19800510318787
At time: 121.1610975265503 and batch: 800, loss is 3.6737747621536254 and perplexity is 39.40035246805621
At time: 122.04528999328613 and batch: 850, loss is 3.7515297317504883 and perplexity is 42.58617762599222
At time: 122.92764687538147 and batch: 900, loss is 3.6961786270141603 and perplexity is 40.29303507181256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386756374411387 and perplexity of 80.37927540894087
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 125.01221561431885 and batch: 50, loss is 3.831560640335083 and perplexity is 46.134481413430684
At time: 125.90289568901062 and batch: 100, loss is 3.71806764125824 and perplexity is 41.184733487168494
At time: 126.78503203392029 and batch: 150, loss is 3.7221274900436403 and perplexity is 41.352277148231494
At time: 127.67246961593628 and batch: 200, loss is 3.6047564363479614 and perplexity is 36.77272626691177
At time: 128.5628011226654 and batch: 250, loss is 3.7440414142608645 and perplexity is 42.26846984026145
At time: 129.44608283042908 and batch: 300, loss is 3.7055644893646242 and perplexity is 40.67300031401251
At time: 130.33550667762756 and batch: 350, loss is 3.700338087081909 and perplexity is 40.460981383051696
At time: 131.21698641777039 and batch: 400, loss is 3.6196352005004884 and perplexity is 37.32394958011413
At time: 132.1089768409729 and batch: 450, loss is 3.6428973054885865 and perplexity is 38.20236044619046
At time: 133.00272035598755 and batch: 500, loss is 3.5120927715301513 and perplexity is 33.51834066418563
At time: 133.8928520679474 and batch: 550, loss is 3.5722852087020875 and perplexity is 35.597848787048704
At time: 134.7783558368683 and batch: 600, loss is 3.5898888397216795 and perplexity is 36.23004836036863
At time: 135.6606056690216 and batch: 650, loss is 3.4168645429611204 and perplexity is 30.473716041968913
At time: 136.54607343673706 and batch: 700, loss is 3.410613660812378 and perplexity is 30.28382255358165
At time: 137.42963695526123 and batch: 750, loss is 3.511038031578064 and perplexity is 33.483006168787156
At time: 138.31135940551758 and batch: 800, loss is 3.4506708574295044 and perplexity is 31.52153167091683
At time: 139.19338536262512 and batch: 850, loss is 3.520057053565979 and perplexity is 33.786356040955035
At time: 140.07735204696655 and batch: 900, loss is 3.451715488433838 and perplexity is 31.554477245192523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336356803162457 and perplexity of 76.42858712102212
finished 8 epochs...
Completing Train Step...
At time: 142.1748948097229 and batch: 50, loss is 3.7404411935806277 and perplexity is 42.11656762584192
At time: 143.06063055992126 and batch: 100, loss is 3.620119638442993 and perplexity is 37.34203509775666
At time: 143.94235515594482 and batch: 150, loss is 3.6222896862030027 and perplexity is 37.423157084833576
At time: 144.82467603683472 and batch: 200, loss is 3.505526165962219 and perplexity is 33.2989600231347
At time: 145.7096769809723 and batch: 250, loss is 3.6509895944595336 and perplexity is 38.5127592106688
At time: 146.59142422676086 and batch: 300, loss is 3.6169830799102782 and perplexity is 37.225093112505604
At time: 147.47379302978516 and batch: 350, loss is 3.6125350522994997 and perplexity is 37.05988257343716
At time: 148.35597133636475 and batch: 400, loss is 3.5366098165512083 and perplexity is 34.35026785832026
At time: 149.23765993118286 and batch: 450, loss is 3.564530863761902 and perplexity is 35.32287827435422
At time: 150.12516450881958 and batch: 500, loss is 3.4367617750167847 and perplexity is 31.086131119659182
At time: 151.0090034008026 and batch: 550, loss is 3.500390877723694 and perplexity is 33.128398581281374
At time: 151.89160776138306 and batch: 600, loss is 3.525898895263672 and perplexity is 33.98430822418617
At time: 152.77400135993958 and batch: 650, loss is 3.3582475757598877 and perplexity is 28.738784183150596
At time: 153.65623807907104 and batch: 700, loss is 3.3561279010772704 and perplexity is 28.677931826292713
At time: 154.5381374359131 and batch: 750, loss is 3.464350709915161 and perplexity is 31.95570451367284
At time: 155.42156100273132 and batch: 800, loss is 3.4082556676864626 and perplexity is 30.21249763305801
At time: 156.305593252182 and batch: 850, loss is 3.485540452003479 and perplexity is 32.64006273112346
At time: 157.1875660419464 and batch: 900, loss is 3.4240548276901244 and perplexity is 30.693620377028672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3446981351669525 and perplexity of 77.06876961633061
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.28924798965454 and batch: 50, loss is 3.7067549085617064 and perplexity is 40.72144706463909
At time: 160.1694917678833 and batch: 100, loss is 3.6082494115829467 and perplexity is 36.9013970802425
At time: 161.0505940914154 and batch: 150, loss is 3.6119745111465456 and perplexity is 37.03911480526923
At time: 161.93192100524902 and batch: 200, loss is 3.4868415927886964 and perplexity is 32.682559689242204
At time: 162.81322836875916 and batch: 250, loss is 3.6346083784103396 and perplexity is 37.88701261933575
At time: 163.69077253341675 and batch: 300, loss is 3.599325666427612 and perplexity is 36.573563344712525
At time: 164.5686275959015 and batch: 350, loss is 3.587831988334656 and perplexity is 36.15560512073222
At time: 165.448406457901 and batch: 400, loss is 3.510219020843506 and perplexity is 33.45559445408621
At time: 166.3439326286316 and batch: 450, loss is 3.530060648918152 and perplexity is 34.12603725923346
At time: 167.23364448547363 and batch: 500, loss is 3.399110155105591 and perplexity is 29.93744850558209
At time: 168.11866164207458 and batch: 550, loss is 3.4599495792388915 and perplexity is 31.815372318938373
At time: 168.9983081817627 and batch: 600, loss is 3.485240240097046 and perplexity is 32.630265266396215
At time: 169.87739086151123 and batch: 650, loss is 3.3103166675567626 and perplexity is 27.39379882589047
At time: 170.75765180587769 and batch: 700, loss is 3.3016524171829222 and perplexity is 27.15747734677509
At time: 171.6429784297943 and batch: 750, loss is 3.4036508655548094 and perplexity is 30.073694884417865
At time: 172.53539848327637 and batch: 800, loss is 3.3421347379684447 and perplexity is 28.279431491179658
At time: 173.42274737358093 and batch: 850, loss is 3.4143909215927124 and perplexity is 30.39842876129482
At time: 174.30696892738342 and batch: 900, loss is 3.357533688545227 and perplexity is 28.71827525395953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337830843990797 and perplexity of 76.54132905158141
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 176.4355788230896 and batch: 50, loss is 3.693322191238403 and perplexity is 40.1781048284941
At time: 177.33766388893127 and batch: 100, loss is 3.593631458282471 and perplexity is 36.36589766932083
At time: 178.22061395645142 and batch: 150, loss is 3.59693576335907 and perplexity is 36.48626043774364
At time: 179.1023302078247 and batch: 200, loss is 3.4722608041763308 and perplexity is 32.20947951744342
At time: 179.98327136039734 and batch: 250, loss is 3.620629138946533 and perplexity is 37.36106573108913
At time: 180.8638653755188 and batch: 300, loss is 3.58502028465271 and perplexity is 36.05408905611703
At time: 181.74389910697937 and batch: 350, loss is 3.572894082069397 and perplexity is 35.61952996898595
At time: 182.62247967720032 and batch: 400, loss is 3.497400164604187 and perplexity is 33.029469053672614
At time: 183.5030357837677 and batch: 450, loss is 3.5169287967681884 and perplexity is 33.680828787229125
At time: 184.38389039039612 and batch: 500, loss is 3.383315391540527 and perplexity is 29.4683083170447
At time: 185.27082753181458 and batch: 550, loss is 3.443076663017273 and perplexity is 31.283057686238205
At time: 186.15341806411743 and batch: 600, loss is 3.4702851438522337 and perplexity is 32.1459073458876
At time: 187.0517828464508 and batch: 650, loss is 3.2910171794891356 and perplexity is 26.87018155748822
At time: 187.93817400932312 and batch: 700, loss is 3.283999147415161 and perplexity is 26.682265931708567
At time: 188.81944251060486 and batch: 750, loss is 3.3834987545013426 and perplexity is 29.47371220872958
At time: 189.70113801956177 and batch: 800, loss is 3.3202189922332765 and perplexity is 27.666408623769183
At time: 190.5801763534546 and batch: 850, loss is 3.3881500864028933 and perplexity is 29.61112355172499
At time: 191.45967197418213 and batch: 900, loss is 3.333253788948059 and perplexity is 28.029395224835685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330467328633348 and perplexity of 75.97978580309842
finished 11 epochs...
Completing Train Step...
At time: 193.55119562149048 and batch: 50, loss is 3.682239670753479 and perplexity is 39.735288451769684
At time: 194.44151496887207 and batch: 100, loss is 3.576447591781616 and perplexity is 35.74632947288042
At time: 195.32920670509338 and batch: 150, loss is 3.579819722175598 and perplexity is 35.86707422607695
At time: 196.2135579586029 and batch: 200, loss is 3.4559679794311524 and perplexity is 31.68894809098933
At time: 197.1122691631317 and batch: 250, loss is 3.606116786003113 and perplexity is 36.82278407274785
At time: 197.99436235427856 and batch: 300, loss is 3.570002069473267 and perplexity is 35.51666665234458
At time: 198.87812304496765 and batch: 350, loss is 3.557688593864441 and perplexity is 35.08201457456918
At time: 199.7610046863556 and batch: 400, loss is 3.4847244215011597 and perplexity is 32.613438308984975
At time: 200.642911195755 and batch: 450, loss is 3.5055288457870484 and perplexity is 33.29904925863412
At time: 201.52651071548462 and batch: 500, loss is 3.3730037689208983 and perplexity is 29.166003547446937
At time: 202.40997624397278 and batch: 550, loss is 3.432639102935791 and perplexity is 30.958237008671574
At time: 203.2927393913269 and batch: 600, loss is 3.4620356512069703 and perplexity is 31.88181074886672
At time: 204.17570805549622 and batch: 650, loss is 3.2854537630081175 and perplexity is 26.721106614070738
At time: 205.05502891540527 and batch: 700, loss is 3.2799437618255616 and perplexity is 26.574278168958266
At time: 205.93667578697205 and batch: 750, loss is 3.382131404876709 and perplexity is 29.43343887953992
At time: 206.8215308189392 and batch: 800, loss is 3.3210176277160643 and perplexity is 27.68851282480219
At time: 207.70425462722778 and batch: 850, loss is 3.3919315910339356 and perplexity is 29.723310135923146
At time: 208.59538793563843 and batch: 900, loss is 3.3388492679595947 and perplexity is 28.186672728653143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329989080559717 and perplexity of 75.94345730461289
finished 12 epochs...
Completing Train Step...
At time: 210.7270700931549 and batch: 50, loss is 3.6753147411346436 and perplexity is 39.461074926356
At time: 211.62302231788635 and batch: 100, loss is 3.5681637144088745 and perplexity is 35.451434386738136
At time: 212.512939453125 and batch: 150, loss is 3.5708495950698853 and perplexity is 35.54678069584778
At time: 213.39898943901062 and batch: 200, loss is 3.447050566673279 and perplexity is 31.40762088069132
At time: 214.28139996528625 and batch: 250, loss is 3.5973505353927613 and perplexity is 36.501397057093385
At time: 215.16655707359314 and batch: 300, loss is 3.5611124658584594 and perplexity is 35.20233676811544
At time: 216.0741879940033 and batch: 350, loss is 3.5487957906723024 and perplexity is 34.77142019757209
At time: 216.96600651741028 and batch: 400, loss is 3.476796507835388 and perplexity is 32.35590398952624
At time: 217.86197352409363 and batch: 450, loss is 3.4982494115829468 and perplexity is 33.057531144612874
At time: 218.7480492591858 and batch: 500, loss is 3.366416277885437 and perplexity is 28.97450420315442
At time: 219.64816975593567 and batch: 550, loss is 3.426206965446472 and perplexity is 30.759748409114636
At time: 220.53279757499695 and batch: 600, loss is 3.45675124168396 and perplexity is 31.713778570979574
At time: 221.42358946800232 and batch: 650, loss is 3.281370940208435 and perplexity is 26.61223148093261
At time: 222.32236051559448 and batch: 700, loss is 3.2769054841995238 and perplexity is 26.49366066536402
At time: 223.20665645599365 and batch: 750, loss is 3.3804426431655883 and perplexity is 29.383774762166624
At time: 224.10455107688904 and batch: 800, loss is 3.3204409074783325 and perplexity is 27.67254890290432
At time: 225.008296251297 and batch: 850, loss is 3.39276602268219 and perplexity is 29.748122557286575
At time: 225.90692687034607 and batch: 900, loss is 3.3403004360198976 and perplexity is 28.227606021203137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330336061242509 and perplexity of 75.96981278943962
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 228.00357341766357 and batch: 50, loss is 3.6723252391815184 and perplexity is 39.343282124424334
At time: 228.91092610359192 and batch: 100, loss is 3.5671048402786254 and perplexity is 35.41391564730264
At time: 229.80547833442688 and batch: 150, loss is 3.5710507440567016 and perplexity is 35.55393161394519
At time: 230.68666338920593 and batch: 200, loss is 3.4481968688964844 and perplexity is 31.443644149164243
At time: 231.58170342445374 and batch: 250, loss is 3.5982187032699584 and perplexity is 36.53310015730652
At time: 232.46736478805542 and batch: 300, loss is 3.5606270170211793 and perplexity is 35.18525198189214
At time: 233.34835028648376 and batch: 350, loss is 3.546352424621582 and perplexity is 34.686564598786575
At time: 234.2298083305359 and batch: 400, loss is 3.4750406837463377 and perplexity is 32.29914256000364
At time: 235.11043214797974 and batch: 450, loss is 3.4972964191436766 and perplexity is 33.02604257393916
At time: 236.00536513328552 and batch: 500, loss is 3.3648645877838135 and perplexity is 28.929579615315973
At time: 236.89326524734497 and batch: 550, loss is 3.4212364673614504 and perplexity is 30.60723648266518
At time: 237.7809784412384 and batch: 600, loss is 3.4509785652160643 and perplexity is 31.53123258410298
At time: 238.66487455368042 and batch: 650, loss is 3.273668270111084 and perplexity is 26.408033685015035
At time: 239.5567343235016 and batch: 700, loss is 3.269790778160095 and perplexity is 26.305835012271338
At time: 240.44055461883545 and batch: 750, loss is 3.371860160827637 and perplexity is 29.132668134699003
At time: 241.3216154575348 and batch: 800, loss is 3.310551815032959 and perplexity is 27.400241165968133
At time: 242.20137310028076 and batch: 850, loss is 3.3827554035186767 and perplexity is 29.45181103693369
At time: 243.08238220214844 and batch: 900, loss is 3.329289355278015 and perplexity is 27.918494521146954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32951417687821 and perplexity of 75.90740003969417
finished 14 epochs...
Completing Train Step...
At time: 245.17474961280823 and batch: 50, loss is 3.6694751501083376 and perplexity is 39.231309907130864
At time: 246.06245589256287 and batch: 100, loss is 3.563219437599182 and perplexity is 35.2765852891933
At time: 246.94118905067444 and batch: 150, loss is 3.5670886754989626 and perplexity is 35.413343193786005
At time: 247.8215353488922 and batch: 200, loss is 3.444523730278015 and perplexity is 31.328359144054982
At time: 248.71120691299438 and batch: 250, loss is 3.594502577781677 and perplexity is 36.397590514010034
At time: 249.5967378616333 and batch: 300, loss is 3.556957902908325 and perplexity is 35.05638982682304
At time: 250.47970271110535 and batch: 350, loss is 3.5431667375564575 and perplexity is 34.5762398819258
At time: 251.3747079372406 and batch: 400, loss is 3.471775007247925 and perplexity is 32.193836051309844
At time: 252.25587606430054 and batch: 450, loss is 3.4943366718292235 and perplexity is 32.928438346301164
At time: 253.136901140213 and batch: 500, loss is 3.362121787071228 and perplexity is 28.850340262285435
At time: 254.01557302474976 and batch: 550, loss is 3.419270782470703 and perplexity is 30.547131393549858
At time: 254.89521384239197 and batch: 600, loss is 3.4495240116119383 and perplexity is 31.485402055665805
At time: 255.77997016906738 and batch: 650, loss is 3.27282027721405 and perplexity is 26.385649352244478
At time: 256.65950751304626 and batch: 700, loss is 3.26920693397522 and perplexity is 26.290480986086738
At time: 257.5394172668457 and batch: 750, loss is 3.371931138038635 and perplexity is 29.134735963615643
At time: 258.42107677459717 and batch: 800, loss is 3.311315016746521 and perplexity is 27.421161059011673
At time: 259.30142092704773 and batch: 850, loss is 3.3839224100112917 and perplexity is 29.486201554708778
At time: 260.1857075691223 and batch: 900, loss is 3.331003098487854 and perplexity is 27.96638067212473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3290948737157535 and perplexity of 75.87557849870625
finished 15 epochs...
Completing Train Step...
At time: 262.27411937713623 and batch: 50, loss is 3.6673628187179563 and perplexity is 39.148527842086274
At time: 263.15578508377075 and batch: 100, loss is 3.5605902671813965 and perplexity is 35.18395895327853
At time: 264.03584933280945 and batch: 150, loss is 3.564353222846985 and perplexity is 35.31660404323664
At time: 264.92901706695557 and batch: 200, loss is 3.441839952468872 and perplexity is 31.24439351193542
At time: 265.81290006637573 and batch: 250, loss is 3.591949028968811 and perplexity is 36.30476605629305
At time: 266.6960828304291 and batch: 300, loss is 3.5544094038009644 and perplexity is 34.96716239495054
At time: 267.5793266296387 and batch: 350, loss is 3.540789551734924 and perplexity is 34.49414335272986
At time: 268.4624466896057 and batch: 400, loss is 3.4695819330215456 and perplexity is 32.12330994198047
At time: 269.36205434799194 and batch: 450, loss is 3.4922113466262816 and perplexity is 32.85852902272898
At time: 270.24693059921265 and batch: 500, loss is 3.3603219985961914 and perplexity is 28.798462450927705
At time: 271.136971950531 and batch: 550, loss is 3.417728452682495 and perplexity is 30.50005395665616
At time: 272.02402448654175 and batch: 600, loss is 3.4483392953872682 and perplexity is 31.44812287599497
At time: 272.9167411327362 and batch: 650, loss is 3.2720274543762207 and perplexity is 26.364738497244083
At time: 273.8120265007019 and batch: 700, loss is 3.268704957962036 and perplexity is 26.277287107039108
At time: 274.6935062408447 and batch: 750, loss is 3.371894845962524 and perplexity is 29.13367862274724
At time: 275.57498812675476 and batch: 800, loss is 3.311663656234741 and perplexity is 27.43072282527748
At time: 276.4566447734833 and batch: 850, loss is 3.384616732597351 and perplexity is 29.50668159948138
At time: 277.3387842178345 and batch: 900, loss is 3.331912188529968 and perplexity is 27.991816190144053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328992451706978 and perplexity of 75.86780756750339
finished 16 epochs...
Completing Train Step...
At time: 279.429532289505 and batch: 50, loss is 3.665491042137146 and perplexity is 39.07531908109463
At time: 280.314825296402 and batch: 100, loss is 3.5584214305877686 and perplexity is 35.10773338587142
At time: 281.19508123397827 and batch: 150, loss is 3.5620832204818726 and perplexity is 35.23652619137348
At time: 282.0810925960541 and batch: 200, loss is 3.4396080064773558 and perplexity is 31.17473547848064
At time: 282.9617519378662 and batch: 250, loss is 3.5897813844680786 and perplexity is 36.226155460494105
At time: 283.8424563407898 and batch: 300, loss is 3.55225649356842 and perplexity is 34.891962211895425
At time: 284.7246832847595 and batch: 350, loss is 3.5387280654907225 and perplexity is 34.42310739568378
At time: 285.6147267818451 and batch: 400, loss is 3.4677144479751587 and perplexity is 32.06337612119869
At time: 286.49404883384705 and batch: 450, loss is 3.490415139198303 and perplexity is 32.799561263843444
At time: 287.3725035190582 and batch: 500, loss is 3.358796920776367 and perplexity is 28.754576028209836
At time: 288.26422238349915 and batch: 550, loss is 3.416345443725586 and perplexity is 30.457901264342244
At time: 289.14395356178284 and batch: 600, loss is 3.44723961353302 and perplexity is 31.413558954060132
At time: 290.02528071403503 and batch: 650, loss is 3.2712240076065062 and perplexity is 26.34356434055704
At time: 290.9043941497803 and batch: 700, loss is 3.268159799575806 and perplexity is 26.26296572767005
At time: 291.78552651405334 and batch: 750, loss is 3.371713318824768 and perplexity is 29.128390549433476
At time: 292.6800971031189 and batch: 800, loss is 3.3117551517486574 and perplexity is 27.433232728180155
At time: 293.56068181991577 and batch: 850, loss is 3.3849938678741456 and perplexity is 29.51781170866017
At time: 294.44047474861145 and batch: 900, loss is 3.332413067817688 and perplexity is 28.00584022297996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329034674657534 and perplexity of 75.8710109978198
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 296.53726506233215 and batch: 50, loss is 3.6645786714553834 and perplexity is 39.039684164183164
At time: 297.4387068748474 and batch: 100, loss is 3.5579783725738525 and perplexity is 35.09218206856665
At time: 298.32125306129456 and batch: 150, loss is 3.5620536470413207 and perplexity is 35.23548414146948
At time: 299.20038533210754 and batch: 200, loss is 3.4400600385665894 and perplexity is 31.18883064478401
At time: 300.079638004303 and batch: 250, loss is 3.5901106500625612 and perplexity is 36.23808545106756
At time: 300.9586615562439 and batch: 300, loss is 3.552431936264038 and perplexity is 34.89808428882241
At time: 301.83846783638 and batch: 350, loss is 3.538444633483887 and perplexity is 34.41335216781031
At time: 302.71835255622864 and batch: 400, loss is 3.466738433837891 and perplexity is 32.03209707969226
At time: 303.603404045105 and batch: 450, loss is 3.489867444038391 and perplexity is 32.78160202143601
At time: 304.49156308174133 and batch: 500, loss is 3.3583844184875487 and perplexity is 28.74271714586041
At time: 305.39164566993713 and batch: 550, loss is 3.4145906019210814 and perplexity is 30.404499335598743
At time: 306.2767939567566 and batch: 600, loss is 3.4449001216888426 and perplexity is 31.340153088782632
At time: 307.155549287796 and batch: 650, loss is 3.2687941074371336 and perplexity is 26.279629817815856
At time: 308.0349876880646 and batch: 700, loss is 3.265859456062317 and perplexity is 26.20262131785904
At time: 308.9146959781647 and batch: 750, loss is 3.3693247747421267 and perplexity is 29.0588991292482
At time: 309.79348158836365 and batch: 800, loss is 3.3085731792449953 and perplexity is 27.3460796688068
At time: 310.6856791973114 and batch: 850, loss is 3.3818581199645994 and perplexity is 29.42539626379547
At time: 311.56826758384705 and batch: 900, loss is 3.32860622882843 and perplexity is 27.899429171870565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32847030848673 and perplexity of 75.82820404640171
finished 18 epochs...
Completing Train Step...
At time: 313.6522901058197 and batch: 50, loss is 3.663853578567505 and perplexity is 39.01138702711691
At time: 314.5318491458893 and batch: 100, loss is 3.5570953607559206 and perplexity is 35.06120893391756
At time: 315.41140246391296 and batch: 150, loss is 3.5611729335784914 and perplexity is 35.20446543751685
At time: 316.3062846660614 and batch: 200, loss is 3.439222464561462 and perplexity is 31.162718627885443
At time: 317.1863989830017 and batch: 250, loss is 3.5892274045944212 and perplexity is 36.20609245723212
At time: 318.0654606819153 and batch: 300, loss is 3.551576657295227 and perplexity is 34.86824945165146
At time: 318.9454040527344 and batch: 350, loss is 3.5376929950714113 and perplexity is 34.38749548907275
At time: 319.82464623451233 and batch: 400, loss is 3.46604962348938 and perplexity is 32.01004063696434
At time: 320.7039804458618 and batch: 450, loss is 3.4892824411392214 and perplexity is 32.76243029751783
At time: 321.5825159549713 and batch: 500, loss is 3.3577746772766113 and perplexity is 28.725196868662596
At time: 322.4827961921692 and batch: 550, loss is 3.41422016620636 and perplexity is 30.393238508991562
At time: 323.36737298965454 and batch: 600, loss is 3.4447005081176756 and perplexity is 31.33389779324537
At time: 324.2487938404083 and batch: 650, loss is 3.268632912635803 and perplexity is 26.27539401951184
At time: 325.13471484184265 and batch: 700, loss is 3.265793552398682 and perplexity is 26.200894526018924
At time: 326.0332326889038 and batch: 750, loss is 3.3693483018875123 and perplexity is 29.05958281023526
At time: 326.9127836227417 and batch: 800, loss is 3.3087117290496826 and perplexity is 27.349868725284335
At time: 327.8068177700043 and batch: 850, loss is 3.3821273469924926 and perplexity is 29.433319442295193
At time: 328.6855721473694 and batch: 900, loss is 3.328927402496338 and perplexity is 27.90839117297257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328186453205266 and perplexity of 75.80668286479425
finished 19 epochs...
Completing Train Step...
At time: 330.7654676437378 and batch: 50, loss is 3.6632692003250122 and perplexity is 38.9885962811854
At time: 331.65423011779785 and batch: 100, loss is 3.5563831090927125 and perplexity is 35.03624542074751
At time: 332.5341217517853 and batch: 150, loss is 3.5604580211639405 and perplexity is 35.179306322481345
At time: 333.41399002075195 and batch: 200, loss is 3.438541865348816 and perplexity is 31.1415165220101
At time: 334.29147386550903 and batch: 250, loss is 3.5885163021087645 and perplexity is 36.18035536682888
At time: 335.17042303085327 and batch: 300, loss is 3.550893015861511 and perplexity is 34.844420217859025
At time: 336.0559129714966 and batch: 350, loss is 3.5370809173583986 and perplexity is 34.36645410961418
At time: 336.951762676239 and batch: 400, loss is 3.4654801988601687 and perplexity is 31.99181851999706
At time: 337.8462381362915 and batch: 450, loss is 3.488764009475708 and perplexity is 32.74544961831916
At time: 338.73959136009216 and batch: 500, loss is 3.3572919750213623 and perplexity is 28.71133449731974
At time: 339.62263655662537 and batch: 550, loss is 3.4138880252838133 and perplexity is 30.38314534698286
At time: 340.50399017333984 and batch: 600, loss is 3.4444839000701903 and perplexity is 31.327111353849503
At time: 341.390665769577 and batch: 650, loss is 3.26847620010376 and perplexity is 26.27127665861406
At time: 342.28564977645874 and batch: 700, loss is 3.265713653564453 and perplexity is 26.198801188719266
At time: 343.1732850074768 and batch: 750, loss is 3.369357662200928 and perplexity is 29.059854818311127
At time: 344.05721402168274 and batch: 800, loss is 3.308818054199219 and perplexity is 27.352776858767474
At time: 344.9415271282196 and batch: 850, loss is 3.3823318386077883 and perplexity is 29.439338924777335
At time: 345.8289632797241 and batch: 900, loss is 3.3291693115234375 and perplexity is 27.915143281394204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328038045804795 and perplexity of 75.79543342682254
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f05edd19b70>
ELAPSED
1412.8891112804413


RESULTS SO FAR:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.01450518464667, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -76.08318202454346, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8745980334815576, 'dropout': 0.43719893342278404, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -75.79543342682254, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9350301414536679, 'dropout': 0.2917336399028122, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.45251056448281146, 'dropout': 0.9861115130718427, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3089852333068848 and batch: 50, loss is 7.9305895328521725 and perplexity is 2781.065851016863
At time: 2.416456937789917 and batch: 100, loss is 7.0952848625183105 and perplexity is 1206.2659345050408
At time: 3.5217297077178955 and batch: 150, loss is 6.876280660629273 and perplexity is 969.0155513524021
At time: 4.626228332519531 and batch: 200, loss is 6.668809633255005 and perplexity is 787.4576828992501
At time: 5.730846405029297 and batch: 250, loss is 6.641184997558594 and perplexity is 766.0021661116527
At time: 6.837388038635254 and batch: 300, loss is 6.471957006454468 and perplexity is 646.7481796524637
At time: 7.940780878067017 and batch: 350, loss is 6.45736174583435 and perplexity is 637.3772731758783
At time: 9.042694330215454 and batch: 400, loss is 6.406273641586304 and perplexity is 605.632666619834
At time: 10.142056703567505 and batch: 450, loss is 6.42147497177124 and perplexity is 614.9094196095342
At time: 11.240682363510132 and batch: 500, loss is 6.426391401290894 and perplexity is 617.9400221995742
At time: 12.348920822143555 and batch: 550, loss is 6.469296445846558 and perplexity is 645.0297539240114
At time: 13.448889970779419 and batch: 600, loss is 6.43804162979126 and perplexity is 625.1812638144619
At time: 14.548114776611328 and batch: 650, loss is 6.390029296875 and perplexity is 595.8740366794663
At time: 15.650335311889648 and batch: 700, loss is 6.470328197479248 and perplexity is 645.6956078646342
At time: 16.745675086975098 and batch: 750, loss is 6.45477988243103 and perplexity is 635.7337746774363
At time: 17.84542679786682 and batch: 800, loss is 6.457156209945679 and perplexity is 637.2462827336966
At time: 18.945098638534546 and batch: 850, loss is 6.489573049545288 and perplexity is 658.2422663773388
At time: 20.042567014694214 and batch: 900, loss is 6.417436752319336 and perplexity is 612.4312874185358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.495645183406464 and perplexity of 662.2513610935488
finished 1 epochs...
Completing Train Step...
At time: 22.263519287109375 and batch: 50, loss is 6.458503971099853 and perplexity is 638.10571754554
At time: 23.134501218795776 and batch: 100, loss is 6.44318790435791 and perplexity is 628.4069111664019
At time: 24.00420331954956 and batch: 150, loss is 6.482074918746949 and perplexity is 653.3251374425317
At time: 24.875216722488403 and batch: 200, loss is 6.4082408618927005 and perplexity is 606.8252521544617
At time: 25.74864673614502 and batch: 250, loss is 6.503389501571656 and perplexity is 667.399956788776
At time: 26.620201587677002 and batch: 300, loss is 6.467075214385987 and perplexity is 643.598583610288
At time: 27.500304460525513 and batch: 350, loss is 6.491635046005249 and perplexity is 659.6009599296419
At time: 28.372387409210205 and batch: 400, loss is 6.429259033203125 and perplexity is 619.7145899145147
At time: 29.240577697753906 and batch: 450, loss is 6.434509801864624 and perplexity is 622.9771257770154
At time: 30.11247730255127 and batch: 500, loss is 6.43539098739624 and perplexity is 623.52632614495
At time: 30.99015736579895 and batch: 550, loss is 6.473814430236817 and perplexity is 647.950581441688
At time: 31.86300539970398 and batch: 600, loss is 6.44231011390686 and perplexity is 627.8555436083979
At time: 32.73998475074768 and batch: 650, loss is 6.394884119033813 and perplexity is 598.7739327006226
At time: 33.61136078834534 and batch: 700, loss is 6.471778478622436 and perplexity is 646.6327274081062
At time: 34.48319602012634 and batch: 750, loss is 6.454810047149659 and perplexity is 635.7529516971055
At time: 35.36522340774536 and batch: 800, loss is 6.45664077758789 and perplexity is 636.9179100139277
At time: 36.23796772956848 and batch: 850, loss is 6.491512575149536 and perplexity is 659.5201829821623
At time: 37.108675956726074 and batch: 900, loss is 6.417568616867065 and perplexity is 612.5120507180574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.49894693453018 and perplexity of 664.4415640320095
Annealing...
finished 2 epochs...
Completing Train Step...
At time: 39.177523136138916 and batch: 50, loss is 6.41729326248169 and perplexity is 612.343416057009
At time: 40.05689239501953 and batch: 100, loss is 6.364459362030029 and perplexity is 580.8307242727195
At time: 40.97872447967529 and batch: 150, loss is 6.380459804534912 and perplexity is 590.1990214645992
At time: 41.860183000564575 and batch: 200, loss is 6.32626787185669 and perplexity is 559.0661887924452
At time: 42.76605463027954 and batch: 250, loss is 6.433120841979981 and perplexity is 622.1124361888501
At time: 43.65403175354004 and batch: 300, loss is 6.357852735519409 and perplexity is 577.0060406511454
At time: 44.53153324127197 and batch: 350, loss is 6.380942649841309 and perplexity is 590.484065102407
At time: 45.40721249580383 and batch: 400, loss is 6.329384288787842 and perplexity is 560.8111897918674
At time: 46.28175973892212 and batch: 450, loss is 6.325605335235596 and perplexity is 558.6959096440477
At time: 47.168044567108154 and batch: 500, loss is 6.322662973403931 and perplexity is 557.0544402067644
At time: 48.04784679412842 and batch: 550, loss is 6.35609148979187 and perplexity is 575.990685634595
At time: 48.92293643951416 and batch: 600, loss is 6.329940299987793 and perplexity is 561.1230937974747
At time: 49.80078864097595 and batch: 650, loss is 6.272337226867676 and perplexity is 529.713993967466
At time: 50.67598795890808 and batch: 700, loss is 6.348976020812988 and perplexity is 571.9067884313622
At time: 51.55453848838806 and batch: 750, loss is 6.323224773406983 and perplexity is 557.3674813179875
At time: 52.42877388000488 and batch: 800, loss is 6.3384238052368165 and perplexity is 565.9036337055568
At time: 53.331974029541016 and batch: 850, loss is 6.369155769348144 and perplexity is 583.5649574475522
At time: 54.20625305175781 and batch: 900, loss is 6.291088991165161 and perplexity is 539.7407821042021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.383345094445634 and perplexity of 591.9043757842807
finished 3 epochs...
Completing Train Step...
At time: 56.28572177886963 and batch: 50, loss is 6.358185443878174 and perplexity is 577.1980473232691
At time: 57.182477951049805 and batch: 100, loss is 6.3280842494964595 and perplexity is 560.0825869188902
At time: 58.05702090263367 and batch: 150, loss is 6.3614538955688475 and perplexity is 579.0876776574186
At time: 58.932262897491455 and batch: 200, loss is 6.310218553543091 and perplexity is 550.1651762220462
At time: 59.806509256362915 and batch: 250, loss is 6.409912090301514 and perplexity is 607.8402436601276
At time: 60.68326163291931 and batch: 300, loss is 6.348841962814331 and perplexity is 571.8301248906818
At time: 61.5659875869751 and batch: 350, loss is 6.3700520706176755 and perplexity is 584.0882419350095
At time: 62.450201988220215 and batch: 400, loss is 6.318125638961792 and perplexity is 554.5326233986682
At time: 63.32782506942749 and batch: 450, loss is 6.319149894714355 and perplexity is 555.1008976075507
At time: 64.20342564582825 and batch: 500, loss is 6.316537218093872 and perplexity is 553.6524914027874
At time: 65.07713723182678 and batch: 550, loss is 6.351499242782593 and perplexity is 573.3516583016166
At time: 65.95121502876282 and batch: 600, loss is 6.3269623756408695 and perplexity is 559.4545972361237
At time: 66.8232159614563 and batch: 650, loss is 6.269024467468261 and perplexity is 527.9620823879023
At time: 67.69845604896545 and batch: 700, loss is 6.350550489425659 and perplexity is 572.8079469558662
At time: 68.57329416275024 and batch: 750, loss is 6.328063669204712 and perplexity is 560.0710603744591
At time: 69.44986057281494 and batch: 800, loss is 6.343295593261718 and perplexity is 568.667322839117
At time: 70.3230369091034 and batch: 850, loss is 6.373608541488648 and perplexity is 586.169233052226
At time: 71.19533133506775 and batch: 900, loss is 6.294169998168945 and perplexity is 541.4062916398635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381466382170377 and perplexity of 590.7934016948168
finished 4 epochs...
Completing Train Step...
At time: 73.26334691047668 and batch: 50, loss is 6.3539825057983395 and perplexity is 574.777210547712
At time: 74.13525676727295 and batch: 100, loss is 6.324960870742798 and perplexity is 558.3359659659285
At time: 75.01295351982117 and batch: 150, loss is 6.35842978477478 and perplexity is 577.3390976431505
At time: 75.88468527793884 and batch: 200, loss is 6.307285175323487 and perplexity is 548.5536983694731
At time: 76.75361037254333 and batch: 250, loss is 6.403506155014038 and perplexity is 603.9589034740305
At time: 77.62874150276184 and batch: 300, loss is 6.347234115600586 and perplexity is 570.9114481614623
At time: 78.50244140625 and batch: 350, loss is 6.368902740478515 and perplexity is 583.4173173454058
At time: 79.37784028053284 and batch: 400, loss is 6.31693395614624 and perplexity is 553.8721899924287
At time: 80.25000929832458 and batch: 450, loss is 6.318323535919189 and perplexity is 554.6423745769703
At time: 81.12270307540894 and batch: 500, loss is 6.315784206390381 and perplexity is 553.235741525554
At time: 81.99685311317444 and batch: 550, loss is 6.35099310874939 and perplexity is 573.0615389401914
At time: 82.87126874923706 and batch: 600, loss is 6.327331199645996 and perplexity is 559.6609755776664
At time: 83.74417853355408 and batch: 650, loss is 6.2691959953308105 and perplexity is 528.0526503626448
At time: 84.62110996246338 and batch: 700, loss is 6.351718902587891 and perplexity is 573.4776144484913
At time: 85.49633026123047 and batch: 750, loss is 6.329978685379029 and perplexity is 561.1446331403566
At time: 86.37257099151611 and batch: 800, loss is 6.344957132339477 and perplexity is 569.6129712165699
At time: 87.24649930000305 and batch: 850, loss is 6.374814405441284 and perplexity is 586.8764997485115
At time: 88.12399506568909 and batch: 900, loss is 6.294941053390503 and perplexity is 541.8239067694899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381115220997431 and perplexity of 590.5859744132446
finished 5 epochs...
Completing Train Step...
At time: 90.21071434020996 and batch: 50, loss is 6.352658605575561 and perplexity is 574.0167663578062
At time: 91.08855271339417 and batch: 100, loss is 6.3237895584106445 and perplexity is 557.6823630248276
At time: 91.96266865730286 and batch: 150, loss is 6.357487907409668 and perplexity is 576.7955710229783
At time: 92.83778882026672 and batch: 200, loss is 6.306690273284912 and perplexity is 548.2274597056899
At time: 93.71081256866455 and batch: 250, loss is 6.402916593551636 and perplexity is 603.6029375218817
At time: 94.587242603302 and batch: 300, loss is 6.346584377288818 and perplexity is 570.5406256028218
At time: 95.46395826339722 and batch: 350, loss is 6.368451070785523 and perplexity is 583.1538649260093
At time: 96.35690760612488 and batch: 400, loss is 6.316571884155273 and perplexity is 553.671684686725
At time: 97.23930430412292 and batch: 450, loss is 6.318090200424194 and perplexity is 554.5129719216565
At time: 98.12402248382568 and batch: 500, loss is 6.315685596466064 and perplexity is 553.1811896806736
At time: 99.00096654891968 and batch: 550, loss is 6.351040639877319 and perplexity is 573.0887778488532
At time: 99.88572382926941 and batch: 600, loss is 6.327649927139282 and perplexity is 559.8393833477227
At time: 100.75846076011658 and batch: 650, loss is 6.269401226043701 and perplexity is 528.1610341059787
At time: 101.64472889900208 and batch: 700, loss is 6.352227964401245 and perplexity is 573.7696243220524
At time: 102.52250814437866 and batch: 750, loss is 6.330746040344239 and perplexity is 561.5753955134834
At time: 103.40972900390625 and batch: 800, loss is 6.345591945648193 and perplexity is 569.9746839093841
At time: 104.28878283500671 and batch: 850, loss is 6.375272932052613 and perplexity is 587.1456599450503
At time: 105.17128920555115 and batch: 900, loss is 6.295237941741943 and perplexity is 541.984791857214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381098499036815 and perplexity of 590.5760987404105
finished 6 epochs...
Completing Train Step...
At time: 107.23836779594421 and batch: 50, loss is 6.3520573806762695 and perplexity is 573.6717569097889
At time: 108.12433648109436 and batch: 100, loss is 6.323238306045532 and perplexity is 557.3750240216876
At time: 109.00130367279053 and batch: 150, loss is 6.357060842514038 and perplexity is 576.5492944744166
At time: 109.88618159294128 and batch: 200, loss is 6.306425466537475 and perplexity is 548.0823045951043
At time: 110.7613513469696 and batch: 250, loss is 6.402578077316284 and perplexity is 603.3986427083311
At time: 111.64443969726562 and batch: 300, loss is 6.3463218402862545 and perplexity is 570.3908572378645
At time: 112.53767657279968 and batch: 350, loss is 6.368286685943604 and perplexity is 583.0580111487789
At time: 113.42848324775696 and batch: 400, loss is 6.316453504562378 and perplexity is 553.6061451374433
At time: 114.31371140480042 and batch: 450, loss is 6.318016490936279 and perplexity is 554.472100560776
At time: 115.19611883163452 and batch: 500, loss is 6.315671854019165 and perplexity is 553.1735876697838
At time: 116.070561170578 and batch: 550, loss is 6.351096172332763 and perplexity is 573.1206037595518
At time: 116.94769883155823 and batch: 600, loss is 6.327836532592773 and perplexity is 559.943862177591
At time: 117.82970213890076 and batch: 650, loss is 6.269512310028076 and perplexity is 528.2197075968209
At time: 118.72201800346375 and batch: 700, loss is 6.352467346191406 and perplexity is 573.9069907627189
At time: 119.6126959323883 and batch: 750, loss is 6.331101484298706 and perplexity is 561.7750395718314
At time: 120.49004125595093 and batch: 800, loss is 6.345880346298218 and perplexity is 570.1390886848036
At time: 121.3646969795227 and batch: 850, loss is 6.375482711791992 and perplexity is 587.2688441288927
At time: 122.25124096870422 and batch: 900, loss is 6.295373630523682 and perplexity is 542.0583381029293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.3810972448897685 and perplexity of 590.5753580716049
finished 7 epochs...
Completing Train Step...
At time: 124.3563916683197 and batch: 50, loss is 6.351755380630493 and perplexity is 573.4985341708957
At time: 125.26387763023376 and batch: 100, loss is 6.322964105606079 and perplexity is 557.2222124966121
At time: 126.20153856277466 and batch: 150, loss is 6.356855564117431 and perplexity is 576.4309535065199
At time: 127.15230298042297 and batch: 200, loss is 6.306301889419555 and perplexity is 548.014578348313
At time: 128.0769009590149 and batch: 250, loss is 6.402422065734863 and perplexity is 603.3045128747181
At time: 128.99783325195312 and batch: 300, loss is 6.346198492050171 and perplexity is 570.3205048707551
At time: 129.9060742855072 and batch: 350, loss is 6.3682120895385745 and perplexity is 583.0145187394326
At time: 130.86203575134277 and batch: 400, loss is 6.316408348083496 and perplexity is 553.5811467976644
At time: 131.78670930862427 and batch: 450, loss is 6.317990131378174 and perplexity is 554.4574851138527
At time: 132.67576575279236 and batch: 500, loss is 6.3156757831573485 and perplexity is 553.1757611695193
At time: 133.57300329208374 and batch: 550, loss is 6.3511326885223385 and perplexity is 573.1415323222815
At time: 134.45542097091675 and batch: 600, loss is 6.327942361831665 and perplexity is 560.0031237460956
At time: 135.3314507007599 and batch: 650, loss is 6.2695712471008305 and perplexity is 528.2508402375822
At time: 136.20823192596436 and batch: 700, loss is 6.352589073181153 and perplexity is 573.9768549851935
At time: 137.08471727371216 and batch: 750, loss is 6.331280708312988 and perplexity is 561.8757321725434
At time: 137.96363043785095 and batch: 800, loss is 6.346024589538574 and perplexity is 570.2213333258835
At time: 138.84114789962769 and batch: 850, loss is 6.375587224960327 and perplexity is 587.3302246639381
At time: 139.71730017662048 and batch: 900, loss is 6.295441970825196 and perplexity is 542.0953837990359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381095990742723 and perplexity of 590.5746174037287
finished 8 epochs...
Completing Train Step...
At time: 141.84897446632385 and batch: 50, loss is 6.351594152450562 and perplexity is 573.4060774995487
At time: 142.74594140052795 and batch: 100, loss is 6.322820014953614 and perplexity is 557.1419277687233
At time: 143.63732147216797 and batch: 150, loss is 6.356748876571655 and perplexity is 576.3694587831997
At time: 144.53442335128784 and batch: 200, loss is 6.306238174438477 and perplexity is 547.9796627221587
At time: 145.4180610179901 and batch: 250, loss is 6.402340879440308 and perplexity is 603.25553480503
At time: 146.29922652244568 and batch: 300, loss is 6.346136159896851 and perplexity is 570.2849566735135
At time: 147.17831826210022 and batch: 350, loss is 6.3681759262084965 and perplexity is 582.9934353741758
At time: 148.0754029750824 and batch: 400, loss is 6.316389436721802 and perplexity is 553.5706779233608
At time: 148.95373511314392 and batch: 450, loss is 6.317978849411011 and perplexity is 554.4512297779986
At time: 149.83149099349976 and batch: 500, loss is 6.315680875778198 and perplexity is 553.1785782911073
At time: 150.71138882637024 and batch: 550, loss is 6.351155099868774 and perplexity is 573.1543773396555
At time: 151.59872174263 and batch: 600, loss is 6.328003282546997 and perplexity is 560.0372405761827
At time: 152.47338819503784 and batch: 650, loss is 6.269602708816528 and perplexity is 528.2674601767793
At time: 153.3614842891693 and batch: 700, loss is 6.352653942108154 and perplexity is 574.0140894555672
At time: 154.24003648757935 and batch: 750, loss is 6.331376113891602 and perplexity is 561.9293408091208
At time: 155.11589932441711 and batch: 800, loss is 6.346100721359253 and perplexity is 570.2647469667384
At time: 155.9904339313507 and batch: 850, loss is 6.375642395019531 and perplexity is 587.3626286010604
At time: 156.86398482322693 and batch: 900, loss is 6.295478639602661 and perplexity is 542.1152621384842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.381099335134846 and perplexity of 590.5765925201301
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 158.93425011634827 and batch: 50, loss is 6.34427430152893 and perplexity is 569.2241546927945
At time: 159.82599472999573 and batch: 100, loss is 6.310066108703613 and perplexity is 550.081312772508
At time: 160.70143818855286 and batch: 150, loss is 6.341160039901734 and perplexity is 567.4541992332572
At time: 161.57933235168457 and batch: 200, loss is 6.288025579452515 and perplexity is 538.0898638827049
At time: 162.46203875541687 and batch: 250, loss is 6.381996364593506 and perplexity is 591.106594799505
At time: 163.36550164222717 and batch: 300, loss is 6.32127965927124 and perplexity is 556.2843916592617
At time: 164.250994682312 and batch: 350, loss is 6.343732643127441 and perplexity is 568.9159131354256
At time: 165.13779973983765 and batch: 400, loss is 6.289950571060181 and perplexity is 539.1266799658105
At time: 166.01201748847961 and batch: 450, loss is 6.291334857940674 and perplexity is 539.8735027450405
At time: 166.90192484855652 and batch: 500, loss is 6.284205913543701 and perplexity is 536.0384607052828
At time: 167.80094599723816 and batch: 550, loss is 6.32009388923645 and perplexity is 555.6251572242733
At time: 168.7176492214203 and batch: 600, loss is 6.299142436981201 and perplexity is 544.105105580494
At time: 169.63778114318848 and batch: 650, loss is 6.23260838508606 and perplexity is 509.0816341301864
At time: 170.5171675682068 and batch: 700, loss is 6.32064112663269 and perplexity is 555.929299299967
At time: 171.39790225028992 and batch: 750, loss is 6.296344900131226 and perplexity is 542.5850786543798
At time: 172.2667007446289 and batch: 800, loss is 6.3067615032196045 and perplexity is 548.2665113026463
At time: 173.1454780101776 and batch: 850, loss is 6.339397659301758 and perplexity is 566.4550096959042
At time: 174.0234134197235 and batch: 900, loss is 6.265315837860108 and perplexity is 526.0076928706358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.35719236609054 and perplexity of 576.6251292865791
finished 10 epochs...
Completing Train Step...
At time: 176.12555170059204 and batch: 50, loss is 6.324526891708374 and perplexity is 558.0937124328141
At time: 176.99864292144775 and batch: 100, loss is 6.298149480819702 and perplexity is 543.5651012081075
At time: 177.87321496009827 and batch: 150, loss is 6.33398964881897 and perplexity is 563.3998835884436
At time: 178.7475824356079 and batch: 200, loss is 6.281857328414917 and perplexity is 534.7810059457827
At time: 179.62359070777893 and batch: 250, loss is 6.375498170852661 and perplexity is 587.277922823757
At time: 180.49910640716553 and batch: 300, loss is 6.317047872543335 and perplexity is 553.9352887106849
At time: 181.37174797058105 and batch: 350, loss is 6.3402567386627195 and perplexity is 566.9418485903291
At time: 182.24757051467896 and batch: 400, loss is 6.286700162887573 and perplexity is 537.3771430939956
At time: 183.13267588615417 and batch: 450, loss is 6.288202314376831 and perplexity is 538.1849715582501
At time: 184.01758646965027 and batch: 500, loss is 6.281712694168091 and perplexity is 534.7036638910605
At time: 184.9126000404358 and batch: 550, loss is 6.318106565475464 and perplexity is 554.5220466291255
At time: 185.8014099597931 and batch: 600, loss is 6.298597812652588 and perplexity is 543.8088533830719
At time: 186.67668437957764 and batch: 650, loss is 6.233778381347657 and perplexity is 509.6776063135945
At time: 187.5510618686676 and batch: 700, loss is 6.32267991065979 and perplexity is 557.0638752602474
At time: 188.42764258384705 and batch: 750, loss is 6.298484544754029 and perplexity is 543.7472607853292
At time: 189.29942989349365 and batch: 800, loss is 6.3089643001556395 and perplexity is 549.4755622518463
At time: 190.17342829704285 and batch: 850, loss is 6.342623910903931 and perplexity is 568.2854872813751
At time: 191.04990410804749 and batch: 900, loss is 6.2668922996521 and perplexity is 526.8375778698654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.356304848030822 and perplexity of 576.1135911039157
finished 11 epochs...
Completing Train Step...
At time: 193.12530732154846 and batch: 50, loss is 6.322365932464599 and perplexity is 556.8889968055346
At time: 194.00633716583252 and batch: 100, loss is 6.296163702011109 and perplexity is 542.4867721648685
At time: 194.88264513015747 and batch: 150, loss is 6.332086877822876 and perplexity is 562.3288818893672
At time: 195.75821924209595 and batch: 200, loss is 6.280065927505493 and perplexity is 533.8238563406925
At time: 196.63491702079773 and batch: 250, loss is 6.3730416584014895 and perplexity is 585.8370377946288
At time: 197.51423954963684 and batch: 300, loss is 6.316134262084961 and perplexity is 553.4294387477498
At time: 198.40911531448364 and batch: 350, loss is 6.339679985046387 and perplexity is 566.6149571058888
At time: 199.29039216041565 and batch: 400, loss is 6.2857410335540775 and perplexity is 536.861976008329
At time: 200.16857600212097 and batch: 450, loss is 6.2872965621948245 and perplexity is 537.697730039283
At time: 201.04675006866455 and batch: 500, loss is 6.281065635681152 and perplexity is 534.3577912593211
At time: 201.9253227710724 and batch: 550, loss is 6.317617273330688 and perplexity is 554.2507897149552
At time: 202.80496096611023 and batch: 600, loss is 6.298809356689453 and perplexity is 543.92390507202
At time: 203.68256068229675 and batch: 650, loss is 6.234943361282348 and perplexity is 510.27171649413845
At time: 204.56079292297363 and batch: 700, loss is 6.324132423400879 and perplexity is 557.8736055660928
At time: 205.44435143470764 and batch: 750, loss is 6.299662103652954 and perplexity is 544.3879323512532
At time: 206.32317900657654 and batch: 800, loss is 6.310011711120605 and perplexity is 550.0513904924926
At time: 207.21399116516113 and batch: 850, loss is 6.3440625 and perplexity is 569.1036049132908
At time: 208.1108477115631 and batch: 900, loss is 6.267536487579346 and perplexity is 527.1770696136497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355988384926156 and perplexity of 575.9313012537617
finished 12 epochs...
Completing Train Step...
At time: 210.23992395401 and batch: 50, loss is 6.321326074600219 and perplexity is 556.3102123815407
At time: 211.13401579856873 and batch: 100, loss is 6.2951257610321045 and perplexity is 541.923995028721
At time: 212.01029634475708 and batch: 150, loss is 6.331065368652344 and perplexity is 561.7547510695354
At time: 212.8953619003296 and batch: 200, loss is 6.279103307723999 and perplexity is 533.3102341878296
At time: 213.77641248703003 and batch: 250, loss is 6.371736106872558 and perplexity is 585.0726964063316
At time: 214.65564465522766 and batch: 300, loss is 6.31578290939331 and perplexity is 553.2350239808832
At time: 215.54470992088318 and batch: 350, loss is 6.339510974884033 and perplexity is 566.5192015120582
At time: 216.42081356048584 and batch: 400, loss is 6.285299062728882 and perplexity is 536.624751104881
At time: 217.29602885246277 and batch: 450, loss is 6.286901769638061 and perplexity is 537.4854928752579
At time: 218.18634796142578 and batch: 500, loss is 6.280800409317017 and perplexity is 534.2160842782424
At time: 219.0604841709137 and batch: 550, loss is 6.317397737503052 and perplexity is 554.1291251644698
At time: 219.93328070640564 and batch: 600, loss is 6.299039554595947 and perplexity is 544.0491296289221
At time: 220.8085696697235 and batch: 650, loss is 6.2357751083374025 and perplexity is 510.6963100443494
At time: 221.68456435203552 and batch: 700, loss is 6.325101280212403 and perplexity is 558.414367126775
At time: 222.5617241859436 and batch: 750, loss is 6.300383367538452 and perplexity is 544.7807213417877
At time: 223.4372022151947 and batch: 800, loss is 6.310605487823486 and perplexity is 550.3780951787712
At time: 224.31913375854492 and batch: 850, loss is 6.344845752716065 and perplexity is 569.5495314713581
At time: 225.2023265361786 and batch: 900, loss is 6.267869882583618 and perplexity is 527.3528571167352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355839141427654 and perplexity of 575.8453536651849
finished 13 epochs...
Completing Train Step...
At time: 227.2806270122528 and batch: 50, loss is 6.320699052810669 and perplexity is 555.9615030922142
At time: 228.15401887893677 and batch: 100, loss is 6.294483547210693 and perplexity is 541.5760756802243
At time: 229.0346121788025 and batch: 150, loss is 6.3304204082489015 and perplexity is 561.3925583111989
At time: 229.9082736968994 and batch: 200, loss is 6.278503894805908 and perplexity is 532.9906569330432
At time: 230.79198837280273 and batch: 250, loss is 6.370973892211914 and perplexity is 584.6269153316058
At time: 231.67411470413208 and batch: 300, loss is 6.31561505317688 and perplexity is 553.1421678364195
At time: 232.5508680343628 and batch: 350, loss is 6.339464139938355 and perplexity is 566.4926692373534
At time: 233.42958045005798 and batch: 400, loss is 6.285054759979248 and perplexity is 536.4936682152718
At time: 234.30769205093384 and batch: 450, loss is 6.286683540344239 and perplexity is 537.3682105933882
At time: 235.18169450759888 and batch: 500, loss is 6.280662307739258 and perplexity is 534.1423132882015
At time: 236.05478811264038 and batch: 550, loss is 6.317265882492065 and perplexity is 554.0560652793448
At time: 236.92971849441528 and batch: 600, loss is 6.299217109680176 and perplexity is 544.1457368942595
At time: 237.80351781845093 and batch: 650, loss is 6.236368713378906 and perplexity is 510.99955194272434
At time: 238.67752718925476 and batch: 700, loss is 6.325782423019409 and perplexity is 558.7948566254041
At time: 239.55865240097046 and batch: 750, loss is 6.300869550704956 and perplexity is 545.0456489543941
At time: 240.43299221992493 and batch: 800, loss is 6.310979385375976 and perplexity is 550.583918677556
At time: 241.3153977394104 and batch: 850, loss is 6.345318565368652 and perplexity is 569.8188853680603
At time: 242.19762229919434 and batch: 900, loss is 6.268063249588013 and perplexity is 527.4548396186829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355765146751926 and perplexity of 575.8027457513698
finished 14 epochs...
Completing Train Step...
At time: 244.29351449012756 and batch: 50, loss is 6.320279579162598 and perplexity is 555.728340798463
At time: 245.18373847007751 and batch: 100, loss is 6.29405143737793 and perplexity is 541.3421058866983
At time: 246.06508874893188 and batch: 150, loss is 6.32998067855835 and perplexity is 561.1457516033503
At time: 246.94408631324768 and batch: 200, loss is 6.2781000995635985 and perplexity is 532.7754812879526
At time: 247.81671905517578 and batch: 250, loss is 6.37048680305481 and perplexity is 584.3422192420143
At time: 248.69096207618713 and batch: 300, loss is 6.315525064468384 and perplexity is 553.0923935267175
At time: 249.56527280807495 and batch: 350, loss is 6.339464349746704 and perplexity is 566.4927880922578
At time: 250.44536638259888 and batch: 400, loss is 6.284904718399048 and perplexity is 536.4131778961237
At time: 251.32025051116943 and batch: 450, loss is 6.286544647216797 and perplexity is 537.2935790250569
At time: 252.21256232261658 and batch: 500, loss is 6.280579261779785 and perplexity is 534.0979567691398
At time: 253.0904839038849 and batch: 550, loss is 6.317173662185669 and perplexity is 554.0049724151803
At time: 253.96590900421143 and batch: 600, loss is 6.299348640441894 and perplexity is 544.2173135046789
At time: 254.8407952785492 and batch: 650, loss is 6.236805219650268 and perplexity is 511.22265514124086
At time: 255.7145311832428 and batch: 700, loss is 6.326285028457642 and perplexity is 559.075780550233
At time: 256.60036039352417 and batch: 750, loss is 6.301218824386597 and perplexity is 545.2360523043707
At time: 257.48410630226135 and batch: 800, loss is 6.311231746673584 and perplexity is 550.7228822834957
At time: 258.36841082572937 and batch: 850, loss is 6.3456250762939455 and perplexity is 569.9935678515412
At time: 259.2523601055145 and batch: 900, loss is 6.268183631896973 and perplexity is 527.5183396722128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355727940389555 and perplexity of 575.7813226242977
finished 15 epochs...
Completing Train Step...
At time: 261.32958984375 and batch: 50, loss is 6.319981555938721 and perplexity is 555.562745523583
At time: 262.21506929397583 and batch: 100, loss is 6.293744468688965 and perplexity is 541.1759563128359
At time: 263.10963702201843 and batch: 150, loss is 6.329665908813476 and perplexity is 560.969147694523
At time: 263.98685336112976 and batch: 200, loss is 6.277813119888306 and perplexity is 532.6226074902142
At time: 264.87932658195496 and batch: 250, loss is 6.370150823593139 and perplexity is 584.1459252349858
At time: 265.7628769874573 and batch: 300, loss is 6.315474061965943 and perplexity is 553.0641851499215
At time: 266.64789295196533 and batch: 350, loss is 6.339484672546387 and perplexity is 566.5043009286983
At time: 267.5206196308136 and batch: 400, loss is 6.284805641174317 and perplexity is 536.360034199857
At time: 268.3960440158844 and batch: 450, loss is 6.286448860168457 and perplexity is 537.2421157238286
At time: 269.2715744972229 and batch: 500, loss is 6.280524339675903 and perplexity is 534.068623791197
At time: 270.14644956588745 and batch: 550, loss is 6.317103881835937 and perplexity is 553.9663151032279
At time: 271.0248508453369 and batch: 600, loss is 6.299447278976441 and perplexity is 544.2709969505426
At time: 271.9012942314148 and batch: 650, loss is 6.237135715484619 and perplexity is 511.3916400220525
At time: 272.78420305252075 and batch: 700, loss is 6.3266695117950436 and perplexity is 559.2907772008658
At time: 273.66212463378906 and batch: 750, loss is 6.301481313705445 and perplexity is 545.3791897295521
At time: 274.53749203681946 and batch: 800, loss is 6.311410655975342 and perplexity is 550.8214205442703
At time: 275.4262478351593 and batch: 850, loss is 6.345834197998047 and perplexity is 570.1127783420934
At time: 276.30483412742615 and batch: 900, loss is 6.268262987136841 and perplexity is 527.5602026775948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355704529644692 and perplexity of 575.767843312438
finished 16 epochs...
Completing Train Step...
At time: 278.38414192199707 and batch: 50, loss is 6.319761323928833 and perplexity is 555.4404062955188
At time: 279.2571818828583 and batch: 100, loss is 6.293517818450928 and perplexity is 541.0533125526613
At time: 280.1461932659149 and batch: 150, loss is 6.3294328022003175 and perplexity is 560.8383973163983
At time: 281.02101016044617 and batch: 200, loss is 6.277601099014282 and perplexity is 532.5096923500557
At time: 281.89566469192505 and batch: 250, loss is 6.369906044006347 and perplexity is 584.0029557355019
At time: 282.77376198768616 and batch: 300, loss is 6.315444831848144 and perplexity is 553.0480192549063
At time: 283.6476240158081 and batch: 350, loss is 6.339513177871704 and perplexity is 566.5204495482494
At time: 284.5314185619354 and batch: 400, loss is 6.284737195968628 and perplexity is 536.3233241833194
At time: 285.4102895259857 and batch: 450, loss is 6.286379337310791 and perplexity is 537.2047664150152
At time: 286.2843406200409 and batch: 500, loss is 6.280485305786133 and perplexity is 534.0477774222662
At time: 287.15939474105835 and batch: 550, loss is 6.317048482894897 and perplexity is 553.9356268060568
At time: 288.05356216430664 and batch: 600, loss is 6.299522476196289 and perplexity is 544.3119261552192
At time: 288.9290316104889 and batch: 650, loss is 6.237392597198486 and perplexity is 511.523024057354
At time: 289.8034346103668 and batch: 700, loss is 6.326972560882568 and perplexity is 559.4602954454349
At time: 290.6807699203491 and batch: 750, loss is 6.301684846878052 and perplexity is 545.4902037834498
At time: 291.5546748638153 and batch: 800, loss is 6.311542081832886 and perplexity is 550.8938174791291
At time: 292.44410014152527 and batch: 850, loss is 6.345982837677002 and perplexity is 570.197526020712
At time: 293.3261215686798 and batch: 900, loss is 6.26831750869751 and perplexity is 527.5889668673187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.3556932423212755 and perplexity of 575.7613444712551
finished 17 epochs...
Completing Train Step...
At time: 295.4403464794159 and batch: 50, loss is 6.319593601226806 and perplexity is 555.347254141844
At time: 296.32554030418396 and batch: 100, loss is 6.293345022201538 and perplexity is 540.9598286465989
At time: 297.20779371261597 and batch: 150, loss is 6.329255304336548 and perplexity is 560.7388585331767
At time: 298.0891749858856 and batch: 200, loss is 6.27743953704834 and perplexity is 532.423665986758
At time: 298.97611236572266 and batch: 250, loss is 6.369720335006714 and perplexity is 583.8945112006842
At time: 299.87173342704773 and batch: 300, loss is 6.315428123474121 and perplexity is 553.0387787989447
At time: 300.75948190689087 and batch: 350, loss is 6.339544124603272 and perplexity is 566.5379817758103
At time: 301.6375939846039 and batch: 400, loss is 6.28468804359436 and perplexity is 536.2969632664169
At time: 302.5152094364166 and batch: 450, loss is 6.286326961517334 and perplexity is 537.1766306259489
At time: 303.40041399002075 and batch: 500, loss is 6.280456333160401 and perplexity is 534.032304880029
At time: 304.282114982605 and batch: 550, loss is 6.317003183364868 and perplexity is 553.9105343508385
At time: 305.15921568870544 and batch: 600, loss is 6.299581289291382 and perplexity is 544.3439397656928
At time: 306.04548931121826 and batch: 650, loss is 6.23759612083435 and perplexity is 511.6271416778775
At time: 306.9297182559967 and batch: 700, loss is 6.327216606140137 and perplexity is 559.5968457388844
At time: 307.8179988861084 and batch: 750, loss is 6.301846446990967 and perplexity is 545.5783621849873
At time: 308.6925811767578 and batch: 800, loss is 6.311641206741333 and perplexity is 550.9484274849123
At time: 309.5687005519867 and batch: 850, loss is 6.3460923767089845 and perplexity is 570.259988326719
At time: 310.4467098712921 and batch: 900, loss is 6.2683557033538815 and perplexity is 527.6091183314505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355686553537029 and perplexity of 575.7574933407242
finished 18 epochs...
Completing Train Step...
At time: 312.52548933029175 and batch: 50, loss is 6.319462738037109 and perplexity is 555.2745843837794
At time: 313.403644323349 and batch: 100, loss is 6.293210554122925 and perplexity is 540.8870917083416
At time: 314.27780771255493 and batch: 150, loss is 6.3291174507141115 and perplexity is 560.6615639780766
At time: 315.1492795944214 and batch: 200, loss is 6.277313461303711 and perplexity is 532.3565445078947
At time: 316.0220160484314 and batch: 250, loss is 6.369575357437133 and perplexity is 583.8098657295544
At time: 316.9038803577423 and batch: 300, loss is 6.3154198169708256 and perplexity is 553.034184999585
At time: 317.7788951396942 and batch: 350, loss is 6.339574871063232 and perplexity is 566.5554010809726
At time: 318.6535441875458 and batch: 400, loss is 6.284651670455933 and perplexity is 536.2774568174913
At time: 319.52800369262695 and batch: 450, loss is 6.286286220550537 and perplexity is 537.1547459764805
At time: 320.4029974937439 and batch: 500, loss is 6.280433988571167 and perplexity is 534.0203722808537
At time: 321.27991223335266 and batch: 550, loss is 6.31696551322937 and perplexity is 553.8896688589612
At time: 322.16563296318054 and batch: 600, loss is 6.299627695083618 and perplexity is 544.3692010635975
At time: 323.0445866584778 and batch: 650, loss is 6.237760028839111 and perplexity is 511.71100833487185
At time: 323.92502546310425 and batch: 700, loss is 6.3274164962768555 and perplexity is 559.7087148092758
At time: 324.800733089447 and batch: 750, loss is 6.301977005004883 and perplexity is 545.6495964623912
At time: 325.67365431785583 and batch: 800, loss is 6.311717472076416 and perplexity is 550.990447353657
At time: 326.5482814311981 and batch: 850, loss is 6.346175155639648 and perplexity is 570.3071957926179
At time: 327.4242799282074 and batch: 900, loss is 6.268383464813232 and perplexity is 527.6237657338579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.3556802828017975 and perplexity of 575.7538829292458
finished 19 epochs...
Completing Train Step...
At time: 329.53111743927 and batch: 50, loss is 6.31935899734497 and perplexity is 555.2169828019339
At time: 330.40461325645447 and batch: 100, loss is 6.293103713989257 and perplexity is 540.8293063461175
At time: 331.28008246421814 and batch: 150, loss is 6.329008340835571 and perplexity is 560.6003936001358
At time: 332.1549472808838 and batch: 200, loss is 6.277213153839111 and perplexity is 532.3031478507381
At time: 333.0313494205475 and batch: 250, loss is 6.369459857940674 and perplexity is 583.7424398779356
At time: 333.91548776626587 and batch: 300, loss is 6.315416040420533 and perplexity is 553.0320964421154
At time: 334.79487347602844 and batch: 350, loss is 6.339604406356812 and perplexity is 566.5721347081877
At time: 335.67267990112305 and batch: 400, loss is 6.284624423980713 and perplexity is 536.2628453461095
At time: 336.54898953437805 and batch: 450, loss is 6.286254072189331 and perplexity is 537.1374776092597
At time: 337.4238455295563 and batch: 500, loss is 6.280416269302368 and perplexity is 534.0109099141666
At time: 338.3103823661804 and batch: 550, loss is 6.316933355331421 and perplexity is 553.871857217909
At time: 339.18751668930054 and batch: 600, loss is 6.299665050506592 and perplexity is 544.3895365851756
At time: 340.06254053115845 and batch: 650, loss is 6.237894353866577 and perplexity is 511.77974854678286
At time: 340.9519941806793 and batch: 700, loss is 6.327582302093506 and perplexity is 559.8015254638817
At time: 341.8317368030548 and batch: 750, loss is 6.3020839023590085 and perplexity is 545.707928078225
At time: 342.71944880485535 and batch: 800, loss is 6.3117770576477055 and perplexity is 551.0232794123864
At time: 343.6009318828583 and batch: 850, loss is 6.346239042282105 and perplexity is 570.3436319684058
At time: 344.49591970443726 and batch: 900, loss is 6.26840389251709 and perplexity is 527.6345439859796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355678610605736 and perplexity of 575.7529201566754
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f05edd19b70>
ELAPSED
1764.534479856491


RESULTS SO FAR:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.01450518464667, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -76.08318202454346, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8745980334815576, 'dropout': 0.43719893342278404, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -75.79543342682254, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9350301414536679, 'dropout': 0.2917336399028122, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -575.7529201566754, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.45251056448281146, 'dropout': 0.9861115130718427, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5930728150108364, 'dropout': 0.18093450056593288, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3219428062438965 and batch: 50, loss is 7.071975297927857 and perplexity is 1178.4735730839498
At time: 2.4320127964019775 and batch: 100, loss is 6.386102228164673 and perplexity is 593.5385871315635
At time: 3.5483763217926025 and batch: 150, loss is 5.970112609863281 and perplexity is 391.5497606323764
At time: 4.6631760597229 and batch: 200, loss is 5.752448606491089 and perplexity is 314.9609322375015
At time: 5.776282787322998 and batch: 250, loss is 5.712124719619751 and perplexity is 302.5131415354747
At time: 6.887738227844238 and batch: 300, loss is 5.566115255355835 and perplexity is 261.4165874451832
At time: 7.995956659317017 and batch: 350, loss is 5.508945016860962 and perplexity is 246.89052428571432
At time: 9.107585191726685 and batch: 400, loss is 5.335449371337891 and perplexity is 207.56600206574828
At time: 10.215078353881836 and batch: 450, loss is 5.308021907806396 and perplexity is 201.95035660301386
At time: 11.323103189468384 and batch: 500, loss is 5.233107261657715 and perplexity is 187.37412032046421
At time: 12.43154764175415 and batch: 550, loss is 5.277031011581421 and perplexity is 195.78772009985775
At time: 13.53908634185791 and batch: 600, loss is 5.166516399383545 and perplexity is 175.3030866142728
At time: 14.645029067993164 and batch: 650, loss is 5.0560322284698485 and perplexity is 156.96647201061094
At time: 15.751651763916016 and batch: 700, loss is 5.127790460586548 and perplexity is 168.644080333548
At time: 16.85788059234619 and batch: 750, loss is 5.100059928894043 and perplexity is 164.03173724595106
At time: 17.96638250350952 and batch: 800, loss is 5.062665252685547 and perplexity is 158.01109509559805
At time: 19.072506189346313 and batch: 850, loss is 5.0872334957122805 and perplexity is 161.94123066030747
At time: 20.178273677825928 and batch: 900, loss is 4.991870899200439 and perplexity is 147.211584051499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.982965338720034 and perplexity of 145.90640269809424
finished 1 epochs...
Completing Train Step...
At time: 22.425345420837402 and batch: 50, loss is 4.935228748321533 and perplexity is 139.10495891883681
At time: 23.313180923461914 and batch: 100, loss is 4.817644777297974 and perplexity is 123.67346893575832
At time: 24.192676544189453 and batch: 150, loss is 4.8047815036773684 and perplexity is 122.09281127629153
At time: 25.068853855133057 and batch: 200, loss is 4.685185337066651 and perplexity is 108.3303480766761
At time: 25.948697090148926 and batch: 250, loss is 4.7900235080719 and perplexity is 120.30419674968094
At time: 26.834912061691284 and batch: 300, loss is 4.728733434677124 and perplexity is 113.15215695347437
At time: 27.728680849075317 and batch: 350, loss is 4.718873443603516 and perplexity is 112.0419599565715
At time: 28.6081805229187 and batch: 400, loss is 4.592777738571167 and perplexity is 98.7684022749449
At time: 29.494741916656494 and batch: 450, loss is 4.615606727600098 and perplexity is 101.04911922682332
At time: 30.389984607696533 and batch: 500, loss is 4.512304935455322 and perplexity is 91.13162914076825
At time: 31.27090620994568 and batch: 550, loss is 4.57801100730896 and perplexity is 97.32063154983803
At time: 32.151514530181885 and batch: 600, loss is 4.535847063064575 and perplexity is 93.30251495510758
At time: 33.03111982345581 and batch: 650, loss is 4.395298862457276 and perplexity is 81.06885557999014
At time: 33.9109103679657 and batch: 700, loss is 4.434713678359985 and perplexity is 84.32797650682097
At time: 34.79012107849121 and batch: 750, loss is 4.48155876159668 and perplexity is 88.372316744188
At time: 35.66873550415039 and batch: 800, loss is 4.436358289718628 and perplexity is 84.46677736028045
At time: 36.54782009124756 and batch: 850, loss is 4.49659836769104 and perplexity is 89.71144632703044
At time: 37.42734909057617 and batch: 900, loss is 4.421970090866089 and perplexity is 83.26015395614668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.563805149026113 and perplexity of 95.94788207799712
finished 2 epochs...
Completing Train Step...
At time: 39.49903440475464 and batch: 50, loss is 4.4783695030212405 and perplexity is 88.09092353155124
At time: 40.38308882713318 and batch: 100, loss is 4.352693824768067 and perplexity is 77.68745769894532
At time: 41.26239538192749 and batch: 150, loss is 4.354210438728333 and perplexity is 77.80536897216984
At time: 42.148898124694824 and batch: 200, loss is 4.244553141593933 and perplexity is 69.72459615957402
At time: 43.04373598098755 and batch: 250, loss is 4.385864973068237 and perplexity is 80.3076571399328
At time: 43.922367334365845 and batch: 300, loss is 4.343638195991516 and perplexity is 76.98712468518588
At time: 44.80114984512329 and batch: 350, loss is 4.340142126083374 and perplexity is 76.7184422552031
At time: 45.6794855594635 and batch: 400, loss is 4.244400706291199 and perplexity is 69.7139684796878
At time: 46.5575909614563 and batch: 450, loss is 4.281815218925476 and perplexity is 72.37169131009612
At time: 47.43618869781494 and batch: 500, loss is 4.157984967231751 and perplexity is 63.942546371690035
At time: 48.31492877006531 and batch: 550, loss is 4.228650617599487 and perplexity is 68.6245688833062
At time: 49.193755865097046 and batch: 600, loss is 4.226408948898316 and perplexity is 68.47090762826875
At time: 50.07327628135681 and batch: 650, loss is 4.0743540143966674 and perplexity is 58.812476301570776
At time: 50.95133185386658 and batch: 700, loss is 4.0886232376098635 and perplexity is 59.65770065980802
At time: 51.83139753341675 and batch: 750, loss is 4.174403171539307 and perplexity is 65.00103361505661
At time: 52.7102210521698 and batch: 800, loss is 4.1394568538665775 and perplexity is 62.76871960154522
At time: 53.58858585357666 and batch: 850, loss is 4.207010521888733 and perplexity is 67.15547959100479
At time: 54.467188358306885 and batch: 900, loss is 4.149659676551819 and perplexity is 63.412415893457776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.422066884498074 and perplexity of 83.26821339889281
finished 3 epochs...
Completing Train Step...
At time: 56.55690383911133 and batch: 50, loss is 4.222699770927429 and perplexity is 68.21740727564725
At time: 57.43956208229065 and batch: 100, loss is 4.098302297592163 and perplexity is 60.23793465282195
At time: 58.31601357460022 and batch: 150, loss is 4.104310555458069 and perplexity is 60.60094914783062
At time: 59.206241607666016 and batch: 200, loss is 3.995638146400452 and perplexity is 54.360519527390366
At time: 60.08457684516907 and batch: 250, loss is 4.143175377845764 and perplexity is 63.0025610936723
At time: 60.966726541519165 and batch: 300, loss is 4.107665305137634 and perplexity is 60.80459155622936
At time: 61.846800088882446 and batch: 350, loss is 4.103256731033325 and perplexity is 60.537120025707054
At time: 62.74541997909546 and batch: 400, loss is 4.024560861587524 and perplexity is 55.95573107461845
At time: 63.624876499176025 and batch: 450, loss is 4.063016757965088 and perplexity is 58.14946961629294
At time: 64.51970362663269 and batch: 500, loss is 3.940782918930054 and perplexity is 51.458873659781084
At time: 65.39980483055115 and batch: 550, loss is 4.007253022193908 and perplexity is 54.99559120959244
At time: 66.28114581108093 and batch: 600, loss is 4.02384717464447 and perplexity is 55.915810447071244
At time: 67.16797423362732 and batch: 650, loss is 3.868372302055359 and perplexity is 47.86441383238354
At time: 68.05157661437988 and batch: 700, loss is 3.877652645111084 and perplexity is 48.31067955955729
At time: 68.93242001533508 and batch: 750, loss is 3.97183650970459 and perplexity is 53.08192685174253
At time: 69.81322503089905 and batch: 800, loss is 3.938805117607117 and perplexity is 51.35719881084872
At time: 70.6930763721466 and batch: 850, loss is 4.008525724411011 and perplexity is 55.065628779487255
At time: 71.5791118144989 and batch: 900, loss is 3.9579747247695924 and perplexity is 52.351192936758935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365473499036815 and perplexity of 78.6866491749375
finished 4 epochs...
Completing Train Step...
At time: 73.66499876976013 and batch: 50, loss is 4.037706165313721 and perplexity is 56.69614195594943
At time: 74.55107140541077 and batch: 100, loss is 3.9145934486389162 and perplexity is 50.12868748161514
At time: 75.4383339881897 and batch: 150, loss is 3.924741291999817 and perplexity is 50.63997539726567
At time: 76.31725144386292 and batch: 200, loss is 3.8164385461807253 and perplexity is 45.442079916730535
At time: 77.19710850715637 and batch: 250, loss is 3.965779242515564 and perplexity is 52.761367275518076
At time: 78.07588338851929 and batch: 300, loss is 3.933836827278137 and perplexity is 51.1026741364675
At time: 78.9547221660614 and batch: 350, loss is 3.925737738609314 and perplexity is 50.69046057777152
At time: 79.83360052108765 and batch: 400, loss is 3.856474452018738 and perplexity is 47.29830463382033
At time: 80.72106003761292 and batch: 450, loss is 3.897886567115784 and perplexity is 49.29815059779936
At time: 81.60321998596191 and batch: 500, loss is 3.7820631504058837 and perplexity is 43.90653413748372
At time: 82.4828987121582 and batch: 550, loss is 3.8405801391601564 and perplexity is 46.552473520311295
At time: 83.37083101272583 and batch: 600, loss is 3.8668043565750123 and perplexity is 47.78942384651361
At time: 84.25699806213379 and batch: 650, loss is 3.7105580949783326 and perplexity is 40.876613194810176
At time: 85.136789560318 and batch: 700, loss is 3.7181407690048216 and perplexity is 41.18774534404579
At time: 86.02880764007568 and batch: 750, loss is 3.8170591592788696 and perplexity is 45.47029061979423
At time: 86.91190385818481 and batch: 800, loss is 3.785759391784668 and perplexity is 44.06912358557382
At time: 87.79387593269348 and batch: 850, loss is 3.8528804063796995 and perplexity is 47.128617482656416
At time: 88.68233919143677 and batch: 900, loss is 3.8081131553649903 and perplexity is 45.06532732435469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355623010086687 and perplexity of 77.91535226924574
finished 5 epochs...
Completing Train Step...
At time: 90.78658652305603 and batch: 50, loss is 3.893037109375 and perplexity is 49.05966004209871
At time: 91.67332291603088 and batch: 100, loss is 3.770702247619629 and perplexity is 43.410539082103014
At time: 92.56158804893494 and batch: 150, loss is 3.7800396871566773 and perplexity is 43.81778070419444
At time: 93.46200037002563 and batch: 200, loss is 3.672599425315857 and perplexity is 39.354070985872745
At time: 94.34934687614441 and batch: 250, loss is 3.8234250736236572 and perplexity is 45.76067389198681
At time: 95.23769211769104 and batch: 300, loss is 3.792373051643372 and perplexity is 44.361547710190074
At time: 96.1293089389801 and batch: 350, loss is 3.7857425022125244 and perplexity is 44.06837928321721
At time: 97.00916934013367 and batch: 400, loss is 3.718956208229065 and perplexity is 41.22134514459329
At time: 97.89999675750732 and batch: 450, loss is 3.7610721588134766 and perplexity is 42.99449820616
At time: 98.78868699073792 and batch: 500, loss is 3.651544089317322 and perplexity is 38.53412025935886
At time: 99.66748213768005 and batch: 550, loss is 3.704180941581726 and perplexity is 40.61676618487689
At time: 100.54629826545715 and batch: 600, loss is 3.734780149459839 and perplexity is 41.87881747029557
At time: 101.42310309410095 and batch: 650, loss is 3.5813225889205933 and perplexity is 35.92101818430887
At time: 102.30124425888062 and batch: 700, loss is 3.5880330562591554 and perplexity is 36.16287558411686
At time: 103.19219422340393 and batch: 750, loss is 3.690933117866516 and perplexity is 40.08223095857167
At time: 104.07111859321594 and batch: 800, loss is 3.6605390214920046 and perplexity is 38.88229561683104
At time: 104.94751453399658 and batch: 850, loss is 3.722538743019104 and perplexity is 41.36928689266518
At time: 105.82326459884644 and batch: 900, loss is 3.683607668876648 and perplexity is 39.789683449439664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362867799523759 and perplexity of 78.48188230781957
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 107.90271019935608 and batch: 50, loss is 3.801908950805664 and perplexity is 44.78659835494544
At time: 108.79616641998291 and batch: 100, loss is 3.6809509992599487 and perplexity is 39.68411569777112
At time: 109.67584300041199 and batch: 150, loss is 3.702550392150879 and perplexity is 40.550592504281184
At time: 110.55451583862305 and batch: 200, loss is 3.575049042701721 and perplexity is 35.6963714192244
At time: 111.43658542633057 and batch: 250, loss is 3.7168834733963014 and perplexity is 41.13599271369482
At time: 112.32617354393005 and batch: 300, loss is 3.67754590511322 and perplexity is 39.5492173488215
At time: 113.20461082458496 and batch: 350, loss is 3.6650783395767212 and perplexity is 39.059195924123365
At time: 114.0968406200409 and batch: 400, loss is 3.591928472518921 and perplexity is 36.304019766859405
At time: 114.97796988487244 and batch: 450, loss is 3.619083671569824 and perplexity is 37.30337001774772
At time: 115.8557665348053 and batch: 500, loss is 3.495418334007263 and perplexity is 32.9640750626118
At time: 116.73306655883789 and batch: 550, loss is 3.5332008171081544 and perplexity is 34.23336718469936
At time: 117.61622428894043 and batch: 600, loss is 3.562174425125122 and perplexity is 35.23974007273233
At time: 118.49570083618164 and batch: 650, loss is 3.3929766845703124 and perplexity is 29.754390013086454
At time: 119.38506412506104 and batch: 700, loss is 3.379771180152893 and perplexity is 29.364051266780322
At time: 120.26594018936157 and batch: 750, loss is 3.4712862586975097 and perplexity is 32.178105205134734
At time: 121.17314863204956 and batch: 800, loss is 3.426634974479675 and perplexity is 30.772916677160598
At time: 122.05340266227722 and batch: 850, loss is 3.4699963569641112 and perplexity is 32.13662536966033
At time: 122.94515037536621 and batch: 900, loss is 3.427038321495056 and perplexity is 30.785331344798152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318340144745291 and perplexity of 75.06392957180124
finished 7 epochs...
Completing Train Step...
At time: 125.03203988075256 and batch: 50, loss is 3.7079724216461183 and perplexity is 40.77105615298543
At time: 125.91365432739258 and batch: 100, loss is 3.578515748977661 and perplexity is 35.820335002567845
At time: 126.79502534866333 and batch: 150, loss is 3.5946735334396362 and perplexity is 36.40381341994982
At time: 127.67620325088501 and batch: 200, loss is 3.47493839263916 and perplexity is 32.29583881392509
At time: 128.5741879940033 and batch: 250, loss is 3.6175442790985106 and perplexity is 37.245989667558646
At time: 129.4706575870514 and batch: 300, loss is 3.580963945388794 and perplexity is 35.908137653379036
At time: 130.35155415534973 and batch: 350, loss is 3.5715631818771363 and perplexity is 35.572155462065034
At time: 131.23183035850525 and batch: 400, loss is 3.5034366416931153 and perplexity is 33.22945368091427
At time: 132.11155343055725 and batch: 450, loss is 3.535254168510437 and perplexity is 34.303732534884986
At time: 132.99095249176025 and batch: 500, loss is 3.4175375413894655 and perplexity is 30.49423170770205
At time: 133.8703875541687 and batch: 550, loss is 3.4578671979904176 and perplexity is 31.7491895170368
At time: 134.7498755455017 and batch: 600, loss is 3.493817868232727 and perplexity is 32.9113593847565
At time: 135.63111090660095 and batch: 650, loss is 3.3290586709976195 and perplexity is 27.91205490611615
At time: 136.51131987571716 and batch: 700, loss is 3.3214325094223023 and perplexity is 27.70000266554369
At time: 137.39194512367249 and batch: 750, loss is 3.4194274616241453 and perplexity is 30.55191786719722
At time: 138.27661871910095 and batch: 800, loss is 3.381534209251404 and perplexity is 29.415866606168027
At time: 139.15714955329895 and batch: 850, loss is 3.430636110305786 and perplexity is 30.896289948568537
At time: 140.03685188293457 and batch: 900, loss is 3.3951224088668823 and perplexity is 29.818303276265894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329892511237158 and perplexity of 75.93612385048718
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 142.1381335258484 and batch: 50, loss is 3.672311863899231 and perplexity is 39.342755900439
At time: 143.0203185081482 and batch: 100, loss is 3.5540192699432374 and perplexity is 34.953523181724066
At time: 143.9032428264618 and batch: 150, loss is 3.57485200881958 and perplexity is 35.68933871744834
At time: 144.7851631641388 and batch: 200, loss is 3.460250163078308 and perplexity is 31.82493694311618
At time: 145.67073488235474 and batch: 250, loss is 3.5964874029159546 and perplexity is 36.469905108661436
At time: 146.55631613731384 and batch: 300, loss is 3.5517372369766234 and perplexity is 34.873849033616764
At time: 147.43713307380676 and batch: 350, loss is 3.544334716796875 and perplexity is 34.61664780553392
At time: 148.31705260276794 and batch: 400, loss is 3.4741390419006346 and perplexity is 32.270033426469695
At time: 149.1963086128235 and batch: 450, loss is 3.4979018354415894 and perplexity is 33.04604313209002
At time: 150.08240056037903 and batch: 500, loss is 3.3792611885070802 and perplexity is 29.34907966396773
At time: 150.9744577407837 and batch: 550, loss is 3.4116570425033568 and perplexity is 30.315436629472853
At time: 151.8548502922058 and batch: 600, loss is 3.4471796464920046 and perplexity is 31.411675232363063
At time: 152.73249506950378 and batch: 650, loss is 3.271157827377319 and perplexity is 26.34182097512018
At time: 153.6083414554596 and batch: 700, loss is 3.2557326316833497 and perplexity is 25.938611022691905
At time: 154.48728728294373 and batch: 750, loss is 3.352159147262573 and perplexity is 28.56434172962072
At time: 155.36701345443726 and batch: 800, loss is 3.3088795518875123 and perplexity is 27.354459043037384
At time: 156.24646878242493 and batch: 850, loss is 3.3534746885299684 and perplexity is 28.601944028206756
At time: 157.12672424316406 and batch: 900, loss is 3.3176341581344606 and perplexity is 27.59498789250313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318595990742723 and perplexity of 75.08313683467954
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.20411086082458 and batch: 50, loss is 3.6554264783859254 and perplexity is 38.684015494152376
At time: 160.10200667381287 and batch: 100, loss is 3.5400822448730467 and perplexity is 34.46975403482464
At time: 160.99315357208252 and batch: 150, loss is 3.55615939617157 and perplexity is 35.028408236610936
At time: 161.8736219406128 and batch: 200, loss is 3.4452227783203124 and perplexity is 31.35026682855287
At time: 162.75410223007202 and batch: 250, loss is 3.5790346908569335 and perplexity is 35.838928498584856
At time: 163.63762187957764 and batch: 300, loss is 3.5348151874542237 and perplexity is 34.288677150900725
At time: 164.51694202423096 and batch: 350, loss is 3.528441672325134 and perplexity is 34.07083270319833
At time: 165.39454746246338 and batch: 400, loss is 3.457463040351868 and perplexity is 31.73636043223666
At time: 166.27356123924255 and batch: 450, loss is 3.476459012031555 and perplexity is 32.34498585021728
At time: 167.1529130935669 and batch: 500, loss is 3.3609517335891725 and perplexity is 28.8166035619238
At time: 168.03157877922058 and batch: 550, loss is 3.394781675338745 and perplexity is 29.808144911333546
At time: 168.9104676246643 and batch: 600, loss is 3.4305867528915406 and perplexity is 30.89476502522034
At time: 169.78870916366577 and batch: 650, loss is 3.252451934814453 and perplexity is 25.85365373847379
At time: 170.6793451309204 and batch: 700, loss is 3.2349459886550904 and perplexity is 25.404999583596688
At time: 171.56730484962463 and batch: 750, loss is 3.3286883306503294 and perplexity is 27.90171985986907
At time: 172.46186876296997 and batch: 800, loss is 3.2829070711135864 and perplexity is 26.653142766665848
At time: 173.33935499191284 and batch: 850, loss is 3.3267700147628783 and perplexity is 27.848246852832247
At time: 174.21756386756897 and batch: 900, loss is 3.291790065765381 and perplexity is 26.890957179616635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312660530821918 and perplexity of 74.63880385005501
finished 10 epochs...
Completing Train Step...
At time: 176.30360913276672 and batch: 50, loss is 3.643125042915344 and perplexity is 38.21106154419978
At time: 177.18316793441772 and batch: 100, loss is 3.524644346237183 and perplexity is 33.941699976044134
At time: 178.07081699371338 and batch: 150, loss is 3.539484634399414 and perplexity is 34.44916070279824
At time: 178.95521640777588 and batch: 200, loss is 3.42772659778595 and perplexity is 30.806527452022365
At time: 179.8334743976593 and batch: 250, loss is 3.5624712705612183 and perplexity is 35.250202381509624
At time: 180.71165561676025 and batch: 300, loss is 3.5194409847259522 and perplexity is 33.76554773011424
At time: 181.60141348838806 and batch: 350, loss is 3.512511582374573 and perplexity is 33.532381448753995
At time: 182.48013138771057 and batch: 400, loss is 3.443140354156494 and perplexity is 31.28505020327272
At time: 183.35773420333862 and batch: 450, loss is 3.464695897102356 and perplexity is 31.966737117475933
At time: 184.235830783844 and batch: 500, loss is 3.349673042297363 and perplexity is 28.493415978780416
At time: 185.1138858795166 and batch: 550, loss is 3.385059871673584 and perplexity is 29.51976006068266
At time: 185.99233627319336 and batch: 600, loss is 3.422815036773682 and perplexity is 30.655590284845196
At time: 186.871591091156 and batch: 650, loss is 3.2463729619979858 and perplexity is 25.696966810495113
At time: 187.7502715587616 and batch: 700, loss is 3.2305654287338257 and perplexity is 25.293954857232468
At time: 188.6286849975586 and batch: 750, loss is 3.3268467664718626 and perplexity is 27.85038433539709
At time: 189.50607323646545 and batch: 800, loss is 3.2836771059036254 and perplexity is 26.673674517926912
At time: 190.38632106781006 and batch: 850, loss is 3.3303320693969725 and perplexity is 27.947620712070478
At time: 191.26413989067078 and batch: 900, loss is 3.296709351539612 and perplexity is 27.02356738906065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312059376337757 and perplexity of 74.59394788246203
finished 11 epochs...
Completing Train Step...
At time: 193.34251070022583 and batch: 50, loss is 3.6359788179397583 and perplexity is 37.9389700732234
At time: 194.22817063331604 and batch: 100, loss is 3.516513047218323 and perplexity is 33.66682890824509
At time: 195.10782074928284 and batch: 150, loss is 3.530501914024353 and perplexity is 34.1410992116015
At time: 195.98859524726868 and batch: 200, loss is 3.4182750844955443 and perplexity is 30.516730814083733
At time: 196.869384765625 and batch: 250, loss is 3.5528066778182983 and perplexity is 34.91116450186612
At time: 197.7501015663147 and batch: 300, loss is 3.5101031732559203 and perplexity is 33.45171892866693
At time: 198.63110446929932 and batch: 350, loss is 3.50319806098938 and perplexity is 33.22152672011788
At time: 199.5121133327484 and batch: 400, loss is 3.4344990158081057 and perplexity is 31.015870211942392
At time: 200.39029383659363 and batch: 450, loss is 3.456884813308716 and perplexity is 31.718014914831816
At time: 201.26752424240112 and batch: 500, loss is 3.342355127334595 and perplexity is 28.28566466399836
At time: 202.14546537399292 and batch: 550, loss is 3.378579087257385 and perplexity is 29.32906744600688
At time: 203.02707028388977 and batch: 600, loss is 3.417328119277954 and perplexity is 30.487846209964452
At time: 203.90800142288208 and batch: 650, loss is 3.241966986656189 and perplexity is 25.58399566516485
At time: 204.7889723777771 and batch: 700, loss is 3.2271983861923217 and perplexity is 25.208932252873133
At time: 205.6785101890564 and batch: 750, loss is 3.3248105669021606 and perplexity is 27.793733090990578
At time: 206.56325435638428 and batch: 800, loss is 3.2829672145843505 and perplexity is 26.654745827384943
At time: 207.44394373893738 and batch: 850, loss is 3.3309297800064086 and perplexity is 27.964330294728526
At time: 208.32380890846252 and batch: 900, loss is 3.297802543640137 and perplexity is 27.053125492857497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312395487746147 and perplexity of 74.6190239732862
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 210.41031551361084 and batch: 50, loss is 3.6323960256576537 and perplexity is 37.803285833419054
At time: 211.29642295837402 and batch: 100, loss is 3.5162783908843993 and perplexity is 33.65892970043446
At time: 212.17708039283752 and batch: 150, loss is 3.5300101041793823 and perplexity is 34.124312411186324
At time: 213.05840015411377 and batch: 200, loss is 3.4178818130493163 and perplexity is 30.504731814808878
At time: 213.93843793869019 and batch: 250, loss is 3.551507692337036 and perplexity is 34.86584484720351
At time: 214.82900142669678 and batch: 300, loss is 3.508994154930115 and perplexity is 33.414640923239354
At time: 215.7180516719818 and batch: 350, loss is 3.5021399736404417 and perplexity is 33.186394032986534
At time: 216.6113440990448 and batch: 400, loss is 3.4321799516677856 and perplexity is 30.944025757700448
At time: 217.4902687072754 and batch: 450, loss is 3.4524318838119505 and perplexity is 31.57709082601498
At time: 218.3702278137207 and batch: 500, loss is 3.3383466529846193 and perplexity is 28.172509244532765
At time: 219.25015139579773 and batch: 550, loss is 3.374266405105591 and perplexity is 29.202852857629438
At time: 220.12979912757874 and batch: 600, loss is 3.412580256462097 and perplexity is 30.34343718699847
At time: 221.01013612747192 and batch: 650, loss is 3.2351952743530275 and perplexity is 25.41133347609058
At time: 221.88933682441711 and batch: 700, loss is 3.220722646713257 and perplexity is 25.04621320677116
At time: 222.76996445655823 and batch: 750, loss is 3.3152623319625856 and perplexity is 27.52961493529915
At time: 223.66238832473755 and batch: 800, loss is 3.272232937812805 and perplexity is 26.370156570958283
At time: 224.5429503917694 and batch: 850, loss is 3.3192763185501097 and perplexity is 27.640340517241228
At time: 225.42099165916443 and batch: 900, loss is 3.287253746986389 and perplexity is 26.769247491263766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311063165534033 and perplexity of 74.51967358825893
finished 13 epochs...
Completing Train Step...
At time: 227.52982091903687 and batch: 50, loss is 3.629107265472412 and perplexity is 37.67916410726878
At time: 228.413724899292 and batch: 100, loss is 3.5123238372802734 and perplexity is 33.52608649957821
At time: 229.29292011260986 and batch: 150, loss is 3.5256667613983153 and perplexity is 33.976420230927225
At time: 230.18375992774963 and batch: 200, loss is 3.4137522697448732 and perplexity is 30.379020946673013
At time: 231.0641062259674 and batch: 250, loss is 3.548045473098755 and perplexity is 34.74534037523868
At time: 231.94253206253052 and batch: 300, loss is 3.5054117107391356 and perplexity is 33.29514900133663
At time: 232.82134747505188 and batch: 350, loss is 3.4979716110229493 and perplexity is 33.04834901940758
At time: 233.6994378566742 and batch: 400, loss is 3.4288147115707397 and perplexity is 30.840066703155976
At time: 234.59130787849426 and batch: 450, loss is 3.4498171615600586 and perplexity is 31.494633352655974
At time: 235.48301339149475 and batch: 500, loss is 3.3357895278930663 and perplexity is 28.10056064424243
At time: 236.36158108711243 and batch: 550, loss is 3.37235463142395 and perplexity is 29.147076944556584
At time: 237.24108338356018 and batch: 600, loss is 3.4112657260894776 and perplexity is 30.30357602230231
At time: 238.12515807151794 and batch: 650, loss is 3.2340668869018554 and perplexity is 25.382675817790332
At time: 239.00404739379883 and batch: 700, loss is 3.2196710205078123 and perplexity is 25.01988779728715
At time: 239.88173389434814 and batch: 750, loss is 3.315334677696228 and perplexity is 27.531606657533974
At time: 240.76430583000183 and batch: 800, loss is 3.272807364463806 and perplexity is 26.385308643144114
At time: 241.64248156547546 and batch: 850, loss is 3.3205770015716554 and perplexity is 27.67631522963879
At time: 242.521625995636 and batch: 900, loss is 3.2888811206817627 and perplexity is 26.812846526815694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310667691165453 and perplexity of 74.49020879406935
finished 14 epochs...
Completing Train Step...
At time: 244.59660124778748 and batch: 50, loss is 3.626971321105957 and perplexity is 37.598769398838535
At time: 245.47994899749756 and batch: 100, loss is 3.509796724319458 and perplexity is 33.441469255559326
At time: 246.35842418670654 and batch: 150, loss is 3.5228972959518434 and perplexity is 33.88245388743325
At time: 247.25112962722778 and batch: 200, loss is 3.4109268045425414 and perplexity is 30.2933072276953
At time: 248.1352195739746 and batch: 250, loss is 3.5453587102890016 and perplexity is 34.65211318266475
At time: 249.00994753837585 and batch: 300, loss is 3.5028528118133546 and perplexity is 33.21005899511667
At time: 249.9025137424469 and batch: 350, loss is 3.4952350187301637 and perplexity is 32.958032797894006
At time: 250.78079509735107 and batch: 400, loss is 3.4263574934005736 and perplexity is 30.764378959616238
At time: 251.67458748817444 and batch: 450, loss is 3.4477922105789185 and perplexity is 31.43092279109997
At time: 252.56575322151184 and batch: 500, loss is 3.333929533958435 and perplexity is 28.048342349794318
At time: 253.44539260864258 and batch: 550, loss is 3.3708107614517213 and perplexity is 29.102112366345917
At time: 254.33750534057617 and batch: 600, loss is 3.4101392078399657 and perplexity is 30.269457711945478
At time: 255.21811079978943 and batch: 650, loss is 3.2331506967544557 and perplexity is 25.35943111020344
At time: 256.09800028800964 and batch: 700, loss is 3.2189318895339967 and perplexity is 25.001401655936217
At time: 256.9767520427704 and batch: 750, loss is 3.3151570081710817 and perplexity is 27.52671556456456
At time: 257.8555450439453 and batch: 800, loss is 3.2730453777313233 and perplexity is 26.39158944409605
At time: 258.7346258163452 and batch: 850, loss is 3.3212582635879517 and perplexity is 27.695176475951648
At time: 259.6137297153473 and batch: 900, loss is 3.2897083520889283 and perplexity is 26.835036132282077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310602475519049 and perplexity of 74.48535102535506
finished 15 epochs...
Completing Train Step...
At time: 261.71708393096924 and batch: 50, loss is 3.6250701761245727 and perplexity is 37.52735659164895
At time: 262.6166830062866 and batch: 100, loss is 3.5076879930496214 and perplexity is 33.37102448428102
At time: 263.4979205131531 and batch: 150, loss is 3.5206210565567018 and perplexity is 33.805417021527504
At time: 264.3791573047638 and batch: 200, loss is 3.4085842180252075 and perplexity is 30.22242559021722
At time: 265.2595896720886 and batch: 250, loss is 3.5430554294586183 and perplexity is 34.572391480617206
At time: 266.1387884616852 and batch: 300, loss is 3.5006395387649536 and perplexity is 33.136637347655515
At time: 267.01680183410645 and batch: 350, loss is 3.4929717111587526 and perplexity is 32.88352298384361
At time: 267.91134095191956 and batch: 400, loss is 3.424274401664734 and perplexity is 30.700360637215795
At time: 268.79231810569763 and batch: 450, loss is 3.446002969741821 and perplexity is 31.37473558170899
At time: 269.67259192466736 and batch: 500, loss is 3.332296829223633 and perplexity is 28.002585052678974
At time: 270.5533003807068 and batch: 550, loss is 3.3694190692901613 and perplexity is 29.061639354200082
At time: 271.4342534542084 and batch: 600, loss is 3.4090533113479613 and perplexity is 30.236606053979415
At time: 272.31767678260803 and batch: 650, loss is 3.232251205444336 and perplexity is 25.336630778177778
At time: 273.1997423171997 and batch: 700, loss is 3.218230185508728 and perplexity is 24.983864225519252
At time: 274.08063435554504 and batch: 750, loss is 3.3148294830322267 and perplexity is 27.517701349498658
At time: 274.9620065689087 and batch: 800, loss is 3.2730431079864504 and perplexity is 26.3915295419892
At time: 275.84193873405457 and batch: 850, loss is 3.321592402458191 and perplexity is 27.70443205716901
At time: 276.7223563194275 and batch: 900, loss is 3.2901343441009523 and perplexity is 26.84647007852878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3106710355575775 and perplexity of 74.49045791895355
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 278.8108687400818 and batch: 50, loss is 3.623970332145691 and perplexity is 37.486105043749916
At time: 279.6908850669861 and batch: 100, loss is 3.507531623840332 and perplexity is 33.365806691530935
At time: 280.5709478855133 and batch: 150, loss is 3.5203633642196657 and perplexity is 33.79670674694445
At time: 281.4517548084259 and batch: 200, loss is 3.408447766304016 and perplexity is 30.21830196957082
At time: 282.33757519721985 and batch: 250, loss is 3.542756190299988 and perplexity is 34.562047615000715
At time: 283.230660200119 and batch: 300, loss is 3.5003388261795045 and perplexity is 33.12667424185634
At time: 284.111834526062 and batch: 350, loss is 3.4922246074676515 and perplexity is 32.8589647573591
At time: 285.0004212856293 and batch: 400, loss is 3.423573389053345 and perplexity is 30.678846838837927
At time: 285.88172125816345 and batch: 450, loss is 3.444710192680359 and perplexity is 31.334201249812082
At time: 286.7618634700775 and batch: 500, loss is 3.330930218696594 and perplexity is 27.96434256240846
At time: 287.64321851730347 and batch: 550, loss is 3.3675455045700073 and perplexity is 29.007241466889962
At time: 288.52304673194885 and batch: 600, loss is 3.4075255823135375 and perplexity is 30.190447980500405
At time: 289.41394448280334 and batch: 650, loss is 3.23031524181366 and perplexity is 25.287627432120427
At time: 290.293288230896 and batch: 700, loss is 3.2159734964370728 and perplexity is 24.92754698130889
At time: 291.1739614009857 and batch: 750, loss is 3.311756114959717 and perplexity is 27.43325915218604
At time: 292.05392575263977 and batch: 800, loss is 3.269788341522217 and perplexity is 26.30577091455542
At time: 292.94798827171326 and batch: 850, loss is 3.3179651260375977 and perplexity is 27.604122459323836
At time: 293.8273038864136 and batch: 900, loss is 3.2866692686080934 and perplexity is 26.753606016399093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310160179660745 and perplexity of 74.45241374763988
finished 17 epochs...
Completing Train Step...
At time: 295.9036486148834 and batch: 50, loss is 3.6232289552688597 and perplexity is 37.45832401164788
At time: 296.7843542098999 and batch: 100, loss is 3.506605076789856 and perplexity is 33.3349060194281
At time: 297.6623935699463 and batch: 150, loss is 3.5195550155639648 and perplexity is 33.76939826335345
At time: 298.54117941856384 and batch: 200, loss is 3.4076102066040037 and perplexity is 30.193002933843644
At time: 299.4200506210327 and batch: 250, loss is 3.5421209001541136 and perplexity is 34.540097659763035
At time: 300.29891657829285 and batch: 300, loss is 3.499515347480774 and perplexity is 33.099406360059206
At time: 301.1821639537811 and batch: 350, loss is 3.491455750465393 and perplexity is 32.83371062187196
At time: 302.0609278678894 and batch: 400, loss is 3.422836675643921 and perplexity is 30.656253644362625
At time: 302.9387729167938 and batch: 450, loss is 3.4441761064529417 and perplexity is 31.31747055269259
At time: 303.8171212673187 and batch: 500, loss is 3.330404405593872 and perplexity is 27.94964240978526
At time: 304.7003438472748 and batch: 550, loss is 3.3672569179534912 and perplexity is 28.998871572998336
At time: 305.5901370048523 and batch: 600, loss is 3.407267813682556 and perplexity is 30.182666832966714
At time: 306.47001576423645 and batch: 650, loss is 3.2300470733642577 and perplexity is 25.280846997472853
At time: 307.3572418689728 and batch: 700, loss is 3.2157950973510743 and perplexity is 24.923100326362622
At time: 308.2345480918884 and batch: 750, loss is 3.311775665283203 and perplexity is 27.433795486519486
At time: 309.1130540370941 and batch: 800, loss is 3.269859638214111 and perplexity is 26.307646495859938
At time: 309.9925844669342 and batch: 850, loss is 3.318275661468506 and perplexity is 27.612695848488244
At time: 310.88258695602417 and batch: 900, loss is 3.287029447555542 and perplexity is 26.763243837620493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309926908310145 and perplexity of 74.4350481580556
finished 18 epochs...
Completing Train Step...
At time: 312.97293424606323 and batch: 50, loss is 3.6226408815383913 and perplexity is 37.43630223115921
At time: 313.857382774353 and batch: 100, loss is 3.5058861446380614 and perplexity is 33.31094909644665
At time: 314.7358944416046 and batch: 150, loss is 3.51885320186615 and perplexity is 33.74570675157144
At time: 315.615850687027 and batch: 200, loss is 3.4069102573394776 and perplexity is 30.171876758134367
At time: 316.4946188926697 and batch: 250, loss is 3.541498589515686 and perplexity is 34.51860967633085
At time: 317.373939037323 and batch: 300, loss is 3.498847093582153 and perplexity is 33.07729494156071
At time: 318.25226759910583 and batch: 350, loss is 3.4907928800582884 and perplexity is 32.811953338671536
At time: 319.1313261985779 and batch: 400, loss is 3.4222300386428834 and perplexity is 30.637662066328488
At time: 320.0111274719238 and batch: 450, loss is 3.4437016296386718 and perplexity is 31.302614663699906
At time: 320.890620470047 and batch: 500, loss is 3.3299550819396972 and perplexity is 27.937086795303347
At time: 321.76988983154297 and batch: 550, loss is 3.3669592428207396 and perplexity is 28.990240614727753
At time: 322.6481363773346 and batch: 600, loss is 3.4070394611358643 and perplexity is 30.175775331005415
At time: 323.5276880264282 and batch: 650, loss is 3.229823184013367 and perplexity is 25.275187518620783
At time: 324.4063606262207 and batch: 700, loss is 3.215625343322754 and perplexity is 24.918869888761062
At time: 325.28423595428467 and batch: 750, loss is 3.311763572692871 and perplexity is 27.433463742875237
At time: 326.17214703559875 and batch: 800, loss is 3.269912724494934 and perplexity is 26.30904310803975
At time: 327.05343413352966 and batch: 850, loss is 3.318498315811157 and perplexity is 27.61884461963111
At time: 327.9325039386749 and batch: 900, loss is 3.287295823097229 and perplexity is 26.77037386078477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309816125321062 and perplexity of 74.42680247767706
finished 19 epochs...
Completing Train Step...
At time: 330.0285265445709 and batch: 50, loss is 3.622111783027649 and perplexity is 37.41649997853415
At time: 330.9092116355896 and batch: 100, loss is 3.50526300907135 and perplexity is 33.29019832524698
At time: 331.7884042263031 and batch: 150, loss is 3.518214464187622 and perplexity is 33.724158979600176
At time: 332.66913318634033 and batch: 200, loss is 3.406275930404663 and perplexity is 30.15274399288848
At time: 333.5497224330902 and batch: 250, loss is 3.5409012269973754 and perplexity is 34.49799571034433
At time: 334.4375808238983 and batch: 300, loss is 3.4982466077804566 and perplexity is 33.057438457954675
At time: 335.3254199028015 and batch: 350, loss is 3.490186529159546 and perplexity is 32.79206381190074
At time: 336.20510387420654 and batch: 400, loss is 3.421680521965027 and perplexity is 30.620830785010952
At time: 337.09934639930725 and batch: 450, loss is 3.44325110912323 and perplexity is 31.288515369855983
At time: 337.993043422699 and batch: 500, loss is 3.329538025856018 and perplexity is 27.925437892586814
At time: 338.87307953834534 and batch: 550, loss is 3.3666502141952517 and perplexity is 28.981283184640457
At time: 339.7521092891693 and batch: 600, loss is 3.406810345649719 and perplexity is 30.168862385532318
At time: 340.6332426071167 and batch: 650, loss is 3.2296126174926756 and perplexity is 25.269865970614752
At time: 341.5127897262573 and batch: 700, loss is 3.2154589033126832 and perplexity is 24.914722736941396
At time: 342.3969495296478 and batch: 750, loss is 3.311726565361023 and perplexity is 27.432448522364183
At time: 343.27700448036194 and batch: 800, loss is 3.2699472856521608 and perplexity is 26.309952394728
At time: 344.1540825366974 and batch: 850, loss is 3.318663091659546 and perplexity is 27.623395913146158
At time: 345.0308554172516 and batch: 900, loss is 3.287497334480286 and perplexity is 26.775768939413027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309763033096105 and perplexity of 74.42285109803178
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f05edd19b70>
ELAPSED
2117.220257282257


RESULTS SO FAR:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.01450518464667, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -76.08318202454346, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8745980334815576, 'dropout': 0.43719893342278404, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -75.79543342682254, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9350301414536679, 'dropout': 0.2917336399028122, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -575.7529201566754, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.45251056448281146, 'dropout': 0.9861115130718427, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.42285109803178, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5930728150108364, 'dropout': 0.18093450056593288, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -575.746902864585, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5182247091811346, 'dropout': 0.9867452719445398, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.01450518464667, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8254338531327815, 'dropout': 0.47543301443135955, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -76.08318202454346, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.8745980334815576, 'dropout': 0.43719893342278404, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -75.79543342682254, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.9350301414536679, 'dropout': 0.2917336399028122, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -575.7529201566754, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.45251056448281146, 'dropout': 0.9861115130718427, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}, {'best_accuracy': -74.42285109803178, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'rnn_dropout': 0.5930728150108364, 'dropout': 0.18093450056593288, 'seq_len': 35, 'num_layers': 3, 'wordvec_source': 'gigavec', 'wordvec_dim': 300}}]
