FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2465386390686035 and batch: 50, loss is 9.93522294998169 and perplexity is 20644.886693562574
At time: 1.7251076698303223 and batch: 100, loss is 9.202929248809815 and perplexity is 9926.162714900927
At time: 2.2151033878326416 and batch: 150, loss is 8.615807399749755 and perplexity is 5518.202210663367
At time: 2.692345142364502 and batch: 200, loss is 8.093738832473754 and perplexity is 3273.9052994223493
At time: 3.16971755027771 and batch: 250, loss is 7.839583883285522 and perplexity is 2539.148032028971
At time: 3.646649122238159 and batch: 300, loss is 7.567946481704712 and perplexity is 1935.1623073327744
At time: 4.1235926151275635 and batch: 350, loss is 7.441402597427368 and perplexity is 1705.1401699065698
At time: 4.601519823074341 and batch: 400, loss is 7.296019163131714 and perplexity is 1474.418808710651
At time: 5.078959703445435 and batch: 450, loss is 7.224706153869629 and perplexity is 1372.93512467633
At time: 5.55684494972229 and batch: 500, loss is 7.155067777633667 and perplexity is 1280.5792295394476
At time: 6.034583330154419 and batch: 550, loss is 7.120945911407471 and perplexity is 1237.6205593658929
At time: 6.512213468551636 and batch: 600, loss is 7.045388784408569 and perplexity is 1147.5549005520434
At time: 6.990447044372559 and batch: 650, loss is 6.93577730178833 and perplexity is 1028.4183328296629
At time: 7.468430995941162 and batch: 700, loss is 7.0023694896698 and perplexity is 1099.2347003150353
At time: 7.9458417892456055 and batch: 750, loss is 6.917326965332031 and perplexity is 1009.6176414452442
At time: 8.424128293991089 and batch: 800, loss is 6.910813589096069 and perplexity is 1003.0629915154843
At time: 8.902585744857788 and batch: 850, loss is 6.933450613021851 and perplexity is 1026.028304951132
At time: 9.38143539428711 and batch: 900, loss is 6.80490852355957 and perplexity is 902.2652301416298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.739481468723245 and perplexity of 845.1223998183943
finished 1 epochs...
Completing Train Step...
At time: 10.653002262115479 and batch: 50, loss is 6.089440031051636 and perplexity is 441.17429805687124
At time: 11.124797582626343 and batch: 100, loss is 5.588548736572266 and perplexity is 267.34734661187855
At time: 11.5959792137146 and batch: 150, loss is 5.429592781066894 and perplexity is 228.0563576458081
At time: 12.069224834442139 and batch: 200, loss is 5.2388920402526855 and perplexity is 188.4611792872539
At time: 12.541693449020386 and batch: 250, loss is 5.261370573043823 and perplexity is 192.74548210867266
At time: 13.014573574066162 and batch: 300, loss is 5.153726053237915 and perplexity is 173.07517769166367
At time: 13.487201690673828 and batch: 350, loss is 5.1214288806915285 and perplexity is 167.57464280547168
At time: 13.961374521255493 and batch: 400, loss is 4.960047416687011 and perplexity is 142.60055738268127
At time: 14.434524774551392 and batch: 450, loss is 4.9489913749694825 and perplexity is 141.0326431054175
At time: 14.908092021942139 and batch: 500, loss is 4.858606958389283 and perplexity is 128.8445911568625
At time: 15.381114721298218 and batch: 550, loss is 4.914552211761475 and perplexity is 136.2582812838773
At time: 15.854195356369019 and batch: 600, loss is 4.841971426010132 and perplexity is 126.71892262404572
At time: 16.327460289001465 and batch: 650, loss is 4.713138685226441 and perplexity is 111.40126525867007
At time: 16.799754858016968 and batch: 700, loss is 4.761812582015991 and perplexity is 116.95772936229969
At time: 17.27253818511963 and batch: 750, loss is 4.782505216598511 and perplexity is 119.40310630782324
At time: 17.745619535446167 and batch: 800, loss is 4.717284345626831 and perplexity is 111.86405569578864
At time: 18.242954969406128 and batch: 850, loss is 4.760735416412354 and perplexity is 116.83181434698766
At time: 18.71657085418701 and batch: 900, loss is 4.689343395233155 and perplexity is 108.78172975045918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.785998200717038 and perplexity of 119.82090872566596
finished 2 epochs...
Completing Train Step...
At time: 19.971868991851807 and batch: 50, loss is 4.7468714427948 and perplexity is 115.22323755356759
At time: 20.458861589431763 and batch: 100, loss is 4.616978540420532 and perplexity is 101.1878348282387
At time: 20.932618141174316 and batch: 150, loss is 4.598904285430908 and perplexity is 99.37536892604079
At time: 21.406973361968994 and batch: 200, loss is 4.501296558380127 and perplexity is 90.13391946143817
At time: 21.88043999671936 and batch: 250, loss is 4.626581029891968 and perplexity is 102.1641700691173
At time: 22.354193925857544 and batch: 300, loss is 4.58110405921936 and perplexity is 97.62211532730437
At time: 22.828289270401 and batch: 350, loss is 4.567607593536377 and perplexity is 96.31341309075086
At time: 23.3021023273468 and batch: 400, loss is 4.46790180683136 and perplexity is 87.17362388981782
At time: 23.77489399909973 and batch: 450, loss is 4.492421426773071 and perplexity is 89.3375084180027
At time: 24.248868227005005 and batch: 500, loss is 4.391666765213013 and perplexity is 80.77493970168297
At time: 24.7221782207489 and batch: 550, loss is 4.472214250564575 and perplexity is 87.55036699572335
At time: 25.203709363937378 and batch: 600, loss is 4.441275491714477 and perplexity is 84.8831403981476
At time: 25.67447829246521 and batch: 650, loss is 4.311244220733642 and perplexity is 74.53316698412318
At time: 26.14410400390625 and batch: 700, loss is 4.339797372817993 and perplexity is 76.6919978803749
At time: 26.614208698272705 and batch: 750, loss is 4.409564971923828 and perplexity is 82.23368176136142
At time: 27.085044622421265 and batch: 800, loss is 4.3618700885772705 and perplexity is 78.40361912325015
At time: 27.555466651916504 and batch: 850, loss is 4.42610728263855 and perplexity is 83.60533071895225
At time: 28.026322841644287 and batch: 900, loss is 4.368750295639038 and perplexity is 78.94491222624941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.571225101000642 and perplexity of 96.66245853812661
finished 3 epochs...
Completing Train Step...
At time: 29.302754163742065 and batch: 50, loss is 4.450413284301757 and perplexity is 85.66233958768025
At time: 29.773158311843872 and batch: 100, loss is 4.325399231910706 and perplexity is 75.59568704665743
At time: 30.257635593414307 and batch: 150, loss is 4.319192385673523 and perplexity is 75.12792939257585
At time: 30.729816436767578 and batch: 200, loss is 4.233313503265381 and perplexity is 68.9453045977301
At time: 31.201698303222656 and batch: 250, loss is 4.368263540267944 and perplexity is 78.90649471692576
At time: 31.672488689422607 and batch: 300, loss is 4.344106230735779 and perplexity is 77.02316576798103
At time: 32.1440966129303 and batch: 350, loss is 4.3271310520172115 and perplexity is 75.72671860624553
At time: 32.61509680747986 and batch: 400, loss is 4.242193689346314 and perplexity is 69.56027823083414
At time: 33.08637595176697 and batch: 450, loss is 4.281385660171509 and perplexity is 72.34061009263692
At time: 33.55762577056885 and batch: 500, loss is 4.172120051383972 and perplexity is 64.85279772965299
At time: 34.028881788253784 and batch: 550, loss is 4.254858016967773 and perplexity is 70.44681422602937
At time: 34.50001263618469 and batch: 600, loss is 4.243978672027588 and perplexity is 69.684553003943
At time: 34.97133708000183 and batch: 650, loss is 4.105327129364014 and perplexity is 60.66258581521559
At time: 35.44273328781128 and batch: 700, loss is 4.122181568145752 and perplexity is 61.693684540602554
At time: 35.91350817680359 and batch: 750, loss is 4.2124527549743656 and perplexity is 67.52195167264965
At time: 36.38437056541443 and batch: 800, loss is 4.174222445487976 and perplexity is 64.98928729638378
At time: 36.855600118637085 and batch: 850, loss is 4.241863379478454 and perplexity is 69.53730557877915
At time: 37.32995128631592 and batch: 900, loss is 4.192878360748291 and perplexity is 66.21310213111148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493691222308433 and perplexity of 89.45102084086399
finished 4 epochs...
Completing Train Step...
At time: 38.60387587547302 and batch: 50, loss is 4.277668056488037 and perplexity is 72.07217564968869
At time: 39.07778286933899 and batch: 100, loss is 4.155075783729553 and perplexity is 63.75679609252663
At time: 39.552242040634155 and batch: 150, loss is 4.157913179397583 and perplexity is 63.93795623953463
At time: 40.02735209465027 and batch: 200, loss is 4.068552284240723 and perplexity is 58.47225008894106
At time: 40.50223994255066 and batch: 250, loss is 4.211201944351196 and perplexity is 67.43754729615017
At time: 40.97436499595642 and batch: 300, loss is 4.194800529479981 and perplexity is 66.34049728392148
At time: 41.44686770439148 and batch: 350, loss is 4.176909437179566 and perplexity is 65.16414779053238
At time: 41.92220067977905 and batch: 400, loss is 4.100550971031189 and perplexity is 60.37354250824673
At time: 42.40933060646057 and batch: 450, loss is 4.14341598033905 and perplexity is 63.01772149069638
At time: 42.882484912872314 and batch: 500, loss is 4.029145193099976 and perplexity is 56.21283958091527
At time: 43.35818886756897 and batch: 550, loss is 4.109849677085877 and perplexity is 60.93755657000223
At time: 43.83249521255493 and batch: 600, loss is 4.113292469978332 and perplexity is 61.14771351247959
At time: 44.30660128593445 and batch: 650, loss is 3.971358714103699 and perplexity is 53.056570598641414
At time: 44.78012990951538 and batch: 700, loss is 3.9806888198852537 and perplexity is 53.55391052397195
At time: 45.25489854812622 and batch: 750, loss is 4.079762787818908 and perplexity is 59.131441487892445
At time: 45.72830939292908 and batch: 800, loss is 4.044150671958923 and perplexity is 57.06270049536542
At time: 46.202629804611206 and batch: 850, loss is 4.116202230453491 and perplexity is 61.325897823517835
At time: 46.67652893066406 and batch: 900, loss is 4.066682772636414 and perplexity is 58.36303765763233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.457794607502141 and perplexity of 86.29698036515077
finished 5 epochs...
Completing Train Step...
At time: 47.946075439453125 and batch: 50, loss is 4.154973015785218 and perplexity is 63.75024427431898
At time: 48.43265891075134 and batch: 100, loss is 4.037355980873108 and perplexity is 56.676291325087554
At time: 48.905516386032104 and batch: 150, loss is 4.043321948051453 and perplexity is 57.01543086068361
At time: 49.378464460372925 and batch: 200, loss is 3.954460916519165 and perplexity is 52.16756369101422
At time: 49.85105752944946 and batch: 250, loss is 4.095343360900879 and perplexity is 60.05995785865719
At time: 50.32502484321594 and batch: 300, loss is 4.088459458351135 and perplexity is 59.647930765891346
At time: 50.79821157455444 and batch: 350, loss is 4.067481002807617 and perplexity is 58.40964339375145
At time: 51.27213478088379 and batch: 400, loss is 3.9943588399887084 and perplexity is 54.29102023113962
At time: 51.74531817436218 and batch: 450, loss is 4.038037295341492 and perplexity is 56.714918859636555
At time: 52.21919298171997 and batch: 500, loss is 3.9235844659805297 and perplexity is 50.58142762743109
At time: 52.692728757858276 and batch: 550, loss is 3.999398045539856 and perplexity is 54.56529432302891
At time: 53.16466283798218 and batch: 600, loss is 4.012656741142273 and perplexity is 55.29357631667177
At time: 53.63787865638733 and batch: 650, loss is 3.870415439605713 and perplexity is 47.962307384552375
At time: 54.11097979545593 and batch: 700, loss is 3.877907853126526 and perplexity is 48.32301040561078
At time: 54.597424030303955 and batch: 750, loss is 3.9787587547302246 and perplexity is 53.450647671426644
At time: 55.070255279541016 and batch: 800, loss is 3.9462080907821657 and perplexity is 51.73880554540401
At time: 55.54219698905945 and batch: 850, loss is 4.019174122810364 and perplexity is 55.65512254494123
At time: 56.012895345687866 and batch: 900, loss is 3.967612648010254 and perplexity is 52.858188985794996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.443300848137842 and perplexity of 85.05523322761914
finished 6 epochs...
Completing Train Step...
At time: 57.27947759628296 and batch: 50, loss is 4.058314819335937 and perplexity is 57.87669616340161
At time: 57.767096281051636 and batch: 100, loss is 3.9437306547164916 and perplexity is 51.61078460987174
At time: 58.240578174591064 and batch: 150, loss is 3.956000609397888 and perplexity is 52.24794758470375
At time: 58.714351654052734 and batch: 200, loss is 3.8653358173370362 and perplexity is 47.71929470873343
At time: 59.18772220611572 and batch: 250, loss is 4.0051864862442015 and perplexity is 54.882058193716055
At time: 59.661279916763306 and batch: 300, loss is 4.002037816047668 and perplexity is 54.70952446121119
At time: 60.13351225852966 and batch: 350, loss is 3.9792461156845094 and perplexity is 53.476703778931906
At time: 60.60708689689636 and batch: 400, loss is 3.909645233154297 and perplexity is 49.88125261937001
At time: 61.080183029174805 and batch: 450, loss is 3.954331846237183 and perplexity is 52.16083084337292
At time: 61.55437898635864 and batch: 500, loss is 3.8414872884750366 and perplexity is 46.59472272504248
At time: 62.02789235115051 and batch: 550, loss is 3.916042675971985 and perplexity is 50.20138801275862
At time: 62.50268054008484 and batch: 600, loss is 3.929687986373901 and perplexity is 50.891096476208276
At time: 62.97569465637207 and batch: 650, loss is 3.7911510181427004 and perplexity is 44.30736952326862
At time: 63.44898319244385 and batch: 700, loss is 3.798273482322693 and perplexity is 44.624073693810516
At time: 63.922260999679565 and batch: 750, loss is 3.8987946557998656 and perplexity is 49.342938022902615
At time: 64.40230679512024 and batch: 800, loss is 3.8675970792770387 and perplexity is 47.827322627340806
At time: 64.88197660446167 and batch: 850, loss is 3.944088673591614 and perplexity is 51.629265552987775
At time: 65.35404348373413 and batch: 900, loss is 3.894531126022339 and perplexity is 49.13301077087559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430797838184931 and perplexity of 83.99840731979506
finished 7 epochs...
Completing Train Step...
At time: 66.66495084762573 and batch: 50, loss is 3.9785705375671387 and perplexity is 53.44058828886075
At time: 67.13691568374634 and batch: 100, loss is 3.8689430809020995 and perplexity is 47.891741625631944
At time: 67.60846972465515 and batch: 150, loss is 3.8804645776748656 and perplexity is 48.446717107157234
At time: 68.08083939552307 and batch: 200, loss is 3.792450199127197 and perplexity is 44.364970223991975
At time: 68.55320811271667 and batch: 250, loss is 3.9289701890945437 and perplexity is 50.854580092862314
At time: 69.03162479400635 and batch: 300, loss is 3.9309784603118896 and perplexity is 50.956812503176664
At time: 69.50375556945801 and batch: 350, loss is 3.9093399620056153 and perplexity is 49.86602763607751
At time: 69.9904670715332 and batch: 400, loss is 3.8403293085098267 and perplexity is 46.54079819742937
At time: 70.47206401824951 and batch: 450, loss is 3.8846748876571655 and perplexity is 48.65112280753022
At time: 70.94653129577637 and batch: 500, loss is 3.7709046936035158 and perplexity is 43.419328261035524
At time: 71.42225503921509 and batch: 550, loss is 3.8494894218444826 and perplexity is 46.96907572430211
At time: 71.90099382400513 and batch: 600, loss is 3.863273720741272 and perplexity is 47.620994300863586
At time: 72.37611889839172 and batch: 650, loss is 3.7262723064422607 and perplexity is 41.52403044172319
At time: 72.85061192512512 and batch: 700, loss is 3.73363534450531 and perplexity is 41.830901824833035
At time: 73.32436895370483 and batch: 750, loss is 3.8344790506362916 and perplexity is 46.26931741691817
At time: 73.79836654663086 and batch: 800, loss is 3.8042724657058717 and perplexity is 44.892577339606646
At time: 74.27210760116577 and batch: 850, loss is 3.8833015060424803 and perplexity is 48.58435211125101
At time: 74.74478459358215 and batch: 900, loss is 3.831669421195984 and perplexity is 46.13950023500699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4321966301904965 and perplexity of 84.11598583520653
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 76.02813291549683 and batch: 50, loss is 3.9372406435012817 and perplexity is 51.2769146208043
At time: 76.50104761123657 and batch: 100, loss is 3.808430733680725 and perplexity is 45.07964136789901
At time: 76.97370505332947 and batch: 150, loss is 3.824350700378418 and perplexity is 45.80305080563398
At time: 77.44616913795471 and batch: 200, loss is 3.7121652698516847 and perplexity is 40.942361881110436
At time: 77.91957688331604 and batch: 250, loss is 3.84082311630249 and perplexity is 46.56378608158708
At time: 78.39138579368591 and batch: 300, loss is 3.8299215269088744 and perplexity is 46.05892370627759
At time: 78.88421726226807 and batch: 350, loss is 3.80441499710083 and perplexity is 44.89897639730061
At time: 79.35503935813904 and batch: 400, loss is 3.7202347469329835 and perplexity is 41.27408193561856
At time: 79.82489538192749 and batch: 450, loss is 3.757350926399231 and perplexity is 42.834803001569384
At time: 80.29592061042786 and batch: 500, loss is 3.633202886581421 and perplexity is 37.833800136291885
At time: 80.76961469650269 and batch: 550, loss is 3.7032067251205443 and perplexity is 40.57721593104186
At time: 81.2408881187439 and batch: 600, loss is 3.709717826843262 and perplexity is 40.84228030570937
At time: 81.71289467811584 and batch: 650, loss is 3.5657195091247558 and perplexity is 35.3648896131628
At time: 82.18479466438293 and batch: 700, loss is 3.555340118408203 and perplexity is 34.999721993261744
At time: 82.65657758712769 and batch: 750, loss is 3.65231840133667 and perplexity is 38.5639692465541
At time: 83.12813210487366 and batch: 800, loss is 3.591099429130554 and perplexity is 36.273934631964785
At time: 83.60168743133545 and batch: 850, loss is 3.666297450065613 and perplexity is 39.106842436842705
At time: 84.07462000846863 and batch: 900, loss is 3.602814130783081 and perplexity is 36.70137171465208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3486089837061215 and perplexity of 77.37076404372247
finished 9 epochs...
Completing Train Step...
At time: 85.33545422554016 and batch: 50, loss is 3.849199595451355 and perplexity is 46.955464818991466
At time: 85.81690430641174 and batch: 100, loss is 3.7273576879501342 and perplexity is 41.56912432410313
At time: 86.28648042678833 and batch: 150, loss is 3.746620922088623 and perplexity is 42.377642434327335
At time: 86.75666904449463 and batch: 200, loss is 3.642260932922363 and perplexity is 38.178057245802705
At time: 87.22622275352478 and batch: 250, loss is 3.772039518356323 and perplexity is 43.46862955837173
At time: 87.69551968574524 and batch: 300, loss is 3.7680661392211916 and perplexity is 43.29625489438223
At time: 88.16444683074951 and batch: 350, loss is 3.7460004186630247 and perplexity is 42.35135511855624
At time: 88.6346697807312 and batch: 400, loss is 3.6652062368392944 and perplexity is 39.06419180783349
At time: 89.10389709472656 and batch: 450, loss is 3.7084011840820312 and perplexity is 40.78854099849094
At time: 89.57378220558167 and batch: 500, loss is 3.5857807397842407 and perplexity is 36.08151700069124
At time: 90.04332661628723 and batch: 550, loss is 3.660098714828491 and perplexity is 38.86517925147963
At time: 90.51252484321594 and batch: 600, loss is 3.6726259422302245 and perplexity is 39.35511454823905
At time: 90.99536299705505 and batch: 650, loss is 3.532941708564758 and perplexity is 34.224498175860184
At time: 91.46718001365662 and batch: 700, loss is 3.52514760017395 and perplexity is 33.95878556901302
At time: 91.93965888023376 and batch: 750, loss is 3.6299742746353147 and perplexity is 37.71184645369992
At time: 92.41135859489441 and batch: 800, loss is 3.57258918762207 and perplexity is 35.60867142752064
At time: 92.88357043266296 and batch: 850, loss is 3.6556272172927855 and perplexity is 38.69178166059544
At time: 93.3549337387085 and batch: 900, loss is 3.5981883335113527 and perplexity is 36.5319906727211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346893310546875 and perplexity of 77.23813490720029
finished 10 epochs...
Completing Train Step...
At time: 94.60484409332275 and batch: 50, loss is 3.8103168964385987 and perplexity is 45.16474914691535
At time: 95.08997559547424 and batch: 100, loss is 3.689461097717285 and perplexity is 40.0232725116316
At time: 95.56266784667969 and batch: 150, loss is 3.709290552139282 and perplexity is 40.824833160109314
At time: 96.0354859828949 and batch: 200, loss is 3.606848964691162 and perplexity is 36.8497548029726
At time: 96.50808572769165 and batch: 250, loss is 3.7361157178878783 and perplexity is 41.934786863870436
At time: 96.98095846176147 and batch: 300, loss is 3.734825863838196 and perplexity is 41.88073197816249
At time: 97.45389914512634 and batch: 350, loss is 3.7137876749038696 and perplexity is 41.00884088923189
At time: 97.92793345451355 and batch: 400, loss is 3.6346491813659667 and perplexity is 37.88855855296961
At time: 98.40150165557861 and batch: 450, loss is 3.679478254318237 and perplexity is 39.62571413297148
At time: 98.87486433982849 and batch: 500, loss is 3.557750163078308 and perplexity is 35.08417461312267
At time: 99.3489089012146 and batch: 550, loss is 3.6333788871765136 and perplexity is 37.840459493638726
At time: 99.82479500770569 and batch: 600, loss is 3.648345766067505 and perplexity is 38.411072564664174
At time: 100.30036473274231 and batch: 650, loss is 3.5104795408248903 and perplexity is 33.46431144035584
At time: 100.77438688278198 and batch: 700, loss is 3.503915433883667 and perplexity is 33.24536749323351
At time: 101.24660515785217 and batch: 750, loss is 3.612264537811279 and perplexity is 37.04985869413271
At time: 101.72007155418396 and batch: 800, loss is 3.5562428712844847 and perplexity is 35.03133235898773
At time: 102.1932053565979 and batch: 850, loss is 3.6425949811935423 and perplexity is 38.1908126901711
At time: 102.67905116081238 and batch: 900, loss is 3.5882866048812865 and perplexity is 36.172045793891414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348855214576199 and perplexity of 77.38981745994488
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 103.9366865158081 and batch: 50, loss is 3.808046369552612 and perplexity is 45.06231770036045
At time: 104.40766048431396 and batch: 100, loss is 3.68528790473938 and perplexity is 39.85659570097833
At time: 104.88018536567688 and batch: 150, loss is 3.7086034488677977 and perplexity is 40.79679191840487
At time: 105.34988856315613 and batch: 200, loss is 3.594704818725586 and perplexity is 36.40495234147798
At time: 105.82052063941956 and batch: 250, loss is 3.7217739629745483 and perplexity is 41.33766058271902
At time: 106.29149627685547 and batch: 300, loss is 3.7158459758758546 and perplexity is 41.0933363550127
At time: 106.76321578025818 and batch: 350, loss is 3.692696142196655 and perplexity is 40.152959236475155
At time: 107.2355797290802 and batch: 400, loss is 3.6111962890625 and perplexity is 37.01030136123769
At time: 107.70914101600647 and batch: 450, loss is 3.6495512390136717 and perplexity is 38.45740399350554
At time: 108.18149161338806 and batch: 500, loss is 3.5353701972961424 and perplexity is 34.30771298623518
At time: 108.6551103591919 and batch: 550, loss is 3.5972243118286134 and perplexity is 36.49679001142544
At time: 109.1300106048584 and batch: 600, loss is 3.6119181632995607 and perplexity is 37.0370277896957
At time: 109.60214424133301 and batch: 650, loss is 3.470852632522583 and perplexity is 32.164154961272274
At time: 110.07533526420593 and batch: 700, loss is 3.45937237739563 and perplexity is 31.797013726208313
At time: 110.54833459854126 and batch: 750, loss is 3.564647979736328 and perplexity is 35.32701538991928
At time: 111.02096748352051 and batch: 800, loss is 3.503504033088684 and perplexity is 33.23169313563096
At time: 111.49291324615479 and batch: 850, loss is 3.5870423221588137 and perplexity is 36.12706553215819
At time: 111.9651391506195 and batch: 900, loss is 3.5421453380584715 and perplexity is 34.54094175768011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334619391454409 and perplexity of 76.2959144857972
finished 12 epochs...
Completing Train Step...
At time: 113.24712419509888 and batch: 50, loss is 3.7878605699539185 and perplexity is 44.16181801564284
At time: 113.72058391571045 and batch: 100, loss is 3.6601665925979616 and perplexity is 38.86781742269287
At time: 114.19502878189087 and batch: 150, loss is 3.681141285896301 and perplexity is 39.69166777317059
At time: 114.66887950897217 and batch: 200, loss is 3.5731271648406984 and perplexity is 35.62783323538016
At time: 115.15508031845093 and batch: 250, loss is 3.702279124259949 and perplexity is 40.539593922425084
At time: 115.63044953346252 and batch: 300, loss is 3.6979840564727784 and perplexity is 40.36584701294207
At time: 116.10511875152588 and batch: 350, loss is 3.6754924058914185 and perplexity is 39.46808639146154
At time: 116.57875084877014 and batch: 400, loss is 3.5962726259231568 and perplexity is 36.462073053217345
At time: 117.05306673049927 and batch: 450, loss is 3.6360292339324953 and perplexity is 37.94088285227999
At time: 117.52691292762756 and batch: 500, loss is 3.522970676422119 and perplexity is 33.88494028905915
At time: 118.00170755386353 and batch: 550, loss is 3.5862349033355714 and perplexity is 36.09790763232167
At time: 118.47519278526306 and batch: 600, loss is 3.6024947786331176 and perplexity is 36.68965292399849
At time: 118.94951510429382 and batch: 650, loss is 3.4636677503585815 and perplexity is 31.933887510801316
At time: 119.42325019836426 and batch: 700, loss is 3.454076399803162 and perplexity is 31.629062579311142
At time: 119.8966155052185 and batch: 750, loss is 3.560542869567871 and perplexity is 35.18229135711011
At time: 120.36907505989075 and batch: 800, loss is 3.501269221305847 and perplexity is 33.15750948037639
At time: 120.84152412414551 and batch: 850, loss is 3.5875907945632934 and perplexity is 36.14688566555866
At time: 121.31440830230713 and batch: 900, loss is 3.543943557739258 and perplexity is 34.60310983812236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333623598699701 and perplexity of 76.21997738202221
finished 13 epochs...
Completing Train Step...
At time: 122.56305408477783 and batch: 50, loss is 3.7760130262374876 and perplexity is 43.64169611345677
At time: 123.0457010269165 and batch: 100, loss is 3.647907829284668 and perplexity is 38.39425462598566
At time: 123.51800489425659 and batch: 150, loss is 3.6686159324645997 and perplexity is 39.19761615067961
At time: 123.9910204410553 and batch: 200, loss is 3.561362371444702 and perplexity is 35.21113512805645
At time: 124.46437740325928 and batch: 250, loss is 3.690707082748413 and perplexity is 40.07317199062415
At time: 124.93714475631714 and batch: 300, loss is 3.6870925283432006 and perplexity is 39.928586793222834
At time: 125.40944838523865 and batch: 350, loss is 3.6651533365249636 and perplexity is 39.06212535446627
At time: 125.88133835792542 and batch: 400, loss is 3.5868477630615234 and perplexity is 36.12003736661945
At time: 126.3542902469635 and batch: 450, loss is 3.6271990013122557 and perplexity is 37.607330869013495
At time: 126.8273196220398 and batch: 500, loss is 3.5149307155609133 and perplexity is 33.61359894389415
At time: 127.31309795379639 and batch: 550, loss is 3.5788112020492555 and perplexity is 35.83091979414749
At time: 127.78719568252563 and batch: 600, loss is 3.5959201192855836 and perplexity is 36.44922219558578
At time: 128.26189351081848 and batch: 650, loss is 3.458077139854431 and perplexity is 31.755855700795955
At time: 128.73615765571594 and batch: 700, loss is 3.4494193410873413 and perplexity is 31.48210663458522
At time: 129.20995473861694 and batch: 750, loss is 3.5564905786514283 and perplexity is 35.04001095291836
At time: 129.68401885032654 and batch: 800, loss is 3.4981700944900513 and perplexity is 33.05490922132723
At time: 130.17188668251038 and batch: 850, loss is 3.5857885837554933 and perplexity is 36.08180002418335
At time: 130.6521499156952 and batch: 900, loss is 3.5429041481018064 and perplexity is 34.56716171791856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3335199225438785 and perplexity of 76.2120755973908
finished 14 epochs...
Completing Train Step...
At time: 131.91790080070496 and batch: 50, loss is 3.7662955045700075 and perplexity is 43.21966087522306
At time: 132.40423369407654 and batch: 100, loss is 3.637896246910095 and perplexity is 38.01178514009697
At time: 132.87825083732605 and batch: 150, loss is 3.658630437850952 and perplexity is 38.80815627656174
At time: 133.35148668289185 and batch: 200, loss is 3.5518278217315675 and perplexity is 34.87700821577014
At time: 133.82547235488892 and batch: 250, loss is 3.6813005781173707 and perplexity is 39.6979908506833
At time: 134.2996997833252 and batch: 300, loss is 3.678239507675171 and perplexity is 39.57665830275564
At time: 134.7737636566162 and batch: 350, loss is 3.656712951660156 and perplexity is 38.73381347124082
At time: 135.24823117256165 and batch: 400, loss is 3.5790896368026734 and perplexity is 35.84089775650639
At time: 135.72197246551514 and batch: 450, loss is 3.6197827529907225 and perplexity is 37.32945722814365
At time: 136.19692492485046 and batch: 500, loss is 3.508161482810974 and perplexity is 33.38682906405827
At time: 136.67104935646057 and batch: 550, loss is 3.572333016395569 and perplexity is 35.59955067877355
At time: 137.14549827575684 and batch: 600, loss is 3.590088310241699 and perplexity is 36.23727590777276
At time: 137.6181218624115 and batch: 650, loss is 3.4527904653549193 and perplexity is 31.588415818311095
At time: 138.08988785743713 and batch: 700, loss is 3.4447629499435424 and perplexity is 31.335854400121523
At time: 138.5634891986847 and batch: 750, loss is 3.552311019897461 and perplexity is 34.89386479437781
At time: 139.05055117607117 and batch: 800, loss is 3.494650430679321 and perplexity is 32.93877155623547
At time: 139.52416563034058 and batch: 850, loss is 3.5831142616271974 and perplexity is 35.9854345815016
At time: 139.99866437911987 and batch: 900, loss is 3.5407939052581785 and perplexity is 34.494293524111974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333792490501926 and perplexity of 76.23285139849516
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 141.26746106147766 and batch: 50, loss is 3.772572708129883 and perplexity is 43.491812767098594
At time: 141.74042296409607 and batch: 100, loss is 3.649705381393433 and perplexity is 38.46333236617153
At time: 142.21309232711792 and batch: 150, loss is 3.6662792921066285 and perplexity is 39.10613234284868
At time: 142.68680572509766 and batch: 200, loss is 3.557451243400574 and perplexity is 35.07368883023524
At time: 143.16122484207153 and batch: 250, loss is 3.6848809194564818 and perplexity is 39.84037795351773
At time: 143.63480186462402 and batch: 300, loss is 3.6773979330062865 and perplexity is 39.543365600761234
At time: 144.10754871368408 and batch: 350, loss is 3.651675443649292 and perplexity is 38.53918221543123
At time: 144.58791208267212 and batch: 400, loss is 3.5790085792541504 and perplexity is 35.8379926989374
At time: 145.07107830047607 and batch: 450, loss is 3.613182239532471 and perplexity is 37.08387501925897
At time: 145.5450336933136 and batch: 500, loss is 3.50554105758667 and perplexity is 33.29945590243418
At time: 146.01925230026245 and batch: 550, loss is 3.5621414518356325 and perplexity is 35.23857812173816
At time: 146.4926209449768 and batch: 600, loss is 3.583181209564209 and perplexity is 35.987843812754924
At time: 146.96608138084412 and batch: 650, loss is 3.442883834838867 and perplexity is 31.27702601276659
At time: 147.43996334075928 and batch: 700, loss is 3.434904794692993 and perplexity is 31.028458350998598
At time: 147.913831949234 and batch: 750, loss is 3.5400819063186644 and perplexity is 34.46974236494033
At time: 148.3876667022705 and batch: 800, loss is 3.485495882034302 and perplexity is 32.63860799695256
At time: 148.86214566230774 and batch: 850, loss is 3.5660411167144774 and perplexity is 35.3762650591889
At time: 149.33668851852417 and batch: 900, loss is 3.529968981742859 and perplexity is 34.122909165167925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328100335108091 and perplexity of 75.80015481860826
finished 16 epochs...
Completing Train Step...
At time: 150.5935606956482 and batch: 50, loss is 3.7601078748703003 and perplexity is 42.95305928454922
At time: 151.0648934841156 and batch: 100, loss is 3.6354732847213747 and perplexity is 37.919795510677645
At time: 151.54898619651794 and batch: 150, loss is 3.654547986984253 and perplexity is 38.650046841927654
At time: 152.03147411346436 and batch: 200, loss is 3.5480676984786985 and perplexity is 34.74611261221139
At time: 152.51130270957947 and batch: 250, loss is 3.6777061557769777 and perplexity is 39.555555644997256
At time: 152.98064851760864 and batch: 300, loss is 3.671770052909851 and perplexity is 39.32144533660983
At time: 153.45250296592712 and batch: 350, loss is 3.6455302953720095 and perplexity is 38.30307941259064
At time: 153.9256558418274 and batch: 400, loss is 3.5744031476974487 and perplexity is 35.67332275556371
At time: 154.39806699752808 and batch: 450, loss is 3.608919939994812 and perplexity is 36.92614881286493
At time: 154.86996269226074 and batch: 500, loss is 3.501986093521118 and perplexity is 33.18128769960448
At time: 155.34284472465515 and batch: 550, loss is 3.5587470006942747 and perplexity is 35.11916527520878
At time: 155.8151080608368 and batch: 600, loss is 3.5805197381973266 and perplexity is 35.89219054257408
At time: 156.28714728355408 and batch: 650, loss is 3.4408439064025877 and perplexity is 31.21328815044255
At time: 156.75901079177856 and batch: 700, loss is 3.4338425064086913 and perplexity is 30.99551468414601
At time: 157.23098230361938 and batch: 750, loss is 3.5392713594436644 and perplexity is 34.44181434299886
At time: 157.7050220966339 and batch: 800, loss is 3.485755877494812 and perplexity is 32.64709499011118
At time: 158.17913055419922 and batch: 850, loss is 3.5671655321121216 and perplexity is 35.416065047999425
At time: 158.65088057518005 and batch: 900, loss is 3.532049469947815 and perplexity is 34.19397537577756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327595749946489 and perplexity of 75.76191683321105
finished 17 epochs...
Completing Train Step...
At time: 159.90528464317322 and batch: 50, loss is 3.7565570783615114 and perplexity is 42.8008121708227
At time: 160.3964238166809 and batch: 100, loss is 3.631631808280945 and perplexity is 37.774406941769385
At time: 160.86708807945251 and batch: 150, loss is 3.6506699180603026 and perplexity is 38.500449558137404
At time: 161.3386936187744 and batch: 200, loss is 3.5444549560546874 and perplexity is 34.620810335818774
At time: 161.80961513519287 and batch: 250, loss is 3.674286518096924 and perplexity is 39.42052099284084
At time: 162.27970814704895 and batch: 300, loss is 3.66860071182251 and perplexity is 39.19701954233379
At time: 162.75186467170715 and batch: 350, loss is 3.6424066543579103 and perplexity is 38.183621012481154
At time: 163.22207117080688 and batch: 400, loss is 3.571849250793457 and perplexity is 35.58233300569929
At time: 163.70601987838745 and batch: 450, loss is 3.606392493247986 and perplexity is 36.832937780754214
At time: 164.17637729644775 and batch: 500, loss is 3.499844608306885 and perplexity is 33.110306492335894
At time: 164.64804530143738 and batch: 550, loss is 3.5567113399505614 and perplexity is 35.04774728516792
At time: 165.12119817733765 and batch: 600, loss is 3.5789210653305052 and perplexity is 35.83485651281273
At time: 165.5941882133484 and batch: 650, loss is 3.4395304250717165 and perplexity is 31.17231699249777
At time: 166.06736183166504 and batch: 700, loss is 3.4329078722000124 and perplexity is 30.966558749517972
At time: 166.54145884513855 and batch: 750, loss is 3.5384177684783937 and perplexity is 34.41242766533372
At time: 167.01515460014343 and batch: 800, loss is 3.4854223346710205 and perplexity is 32.63620760166566
At time: 167.48834443092346 and batch: 850, loss is 3.567111620903015 and perplexity is 35.41415577657691
At time: 167.9591624736786 and batch: 900, loss is 3.5323589515686034 and perplexity is 34.20455942039722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327480786467252 and perplexity of 75.75320748029654
finished 18 epochs...
Completing Train Step...
At time: 169.21719121932983 and batch: 50, loss is 3.753498477935791 and perplexity is 42.67010158612103
At time: 169.70027327537537 and batch: 100, loss is 3.628487048149109 and perplexity is 37.65580208247701
At time: 170.17128014564514 and batch: 150, loss is 3.647579507827759 and perplexity is 38.3816510374974
At time: 170.64389729499817 and batch: 200, loss is 3.5415249824523927 and perplexity is 34.51952073583396
At time: 171.1159291267395 and batch: 250, loss is 3.671455044746399 and perplexity is 39.309060711061534
At time: 171.58804297447205 and batch: 300, loss is 3.6659809970855712 and perplexity is 39.094468917935274
At time: 172.0614743232727 and batch: 350, loss is 3.63982147693634 and perplexity is 38.08503706097289
At time: 172.53585481643677 and batch: 400, loss is 3.569710555076599 and perplexity is 35.50631454166174
At time: 173.00921750068665 and batch: 450, loss is 3.6042569160461424 and perplexity is 36.75436213059952
At time: 173.4823455810547 and batch: 500, loss is 3.4979451847076417 and perplexity is 33.047475684855556
At time: 173.95666360855103 and batch: 550, loss is 3.5549005126953124 and perplexity is 34.98433929693224
At time: 174.43125009536743 and batch: 600, loss is 3.5774706411361694 and perplexity is 35.78291844514627
At time: 174.9038848876953 and batch: 650, loss is 3.4382485818862913 and perplexity is 31.13238456940705
At time: 175.3906021118164 and batch: 700, loss is 3.4318853998184204 and perplexity is 30.934912479919017
At time: 175.86422681808472 and batch: 750, loss is 3.537427296638489 and perplexity is 34.378359999105605
At time: 176.33807611465454 and batch: 800, loss is 3.48482367515564 and perplexity is 32.6166754725698
At time: 176.81132674217224 and batch: 850, loss is 3.5666676807403563 and perplexity is 35.39843749974638
At time: 177.28425574302673 and batch: 900, loss is 3.5321938228607177 and perplexity is 34.198911732006906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327471589388913 and perplexity of 75.75251077531675
finished 19 epochs...
Completing Train Step...
At time: 178.53835582733154 and batch: 50, loss is 3.7507293462753295 and perplexity is 42.55210590505203
At time: 179.01026964187622 and batch: 100, loss is 3.6256702184677123 and perplexity is 37.54988135185915
At time: 179.4810552597046 and batch: 150, loss is 3.6448270082473755 and perplexity is 38.276150820381964
At time: 179.95178127288818 and batch: 200, loss is 3.5389007472991945 and perplexity is 34.42905215338307
At time: 180.42145609855652 and batch: 250, loss is 3.668894553184509 and perplexity is 39.20853894029734
At time: 180.89148330688477 and batch: 300, loss is 3.6636081981658934 and perplexity is 39.00181557167259
At time: 181.36241054534912 and batch: 350, loss is 3.637479863166809 and perplexity is 37.99596094540869
At time: 181.83257818222046 and batch: 400, loss is 3.5677565813064573 and perplexity is 35.43700387204056
At time: 182.30266189575195 and batch: 450, loss is 3.6022857570648195 and perplexity is 36.68198479663392
At time: 182.77146887779236 and batch: 500, loss is 3.4961517572402956 and perplexity is 32.98826054913287
At time: 183.2415828704834 and batch: 550, loss is 3.553176918029785 and perplexity is 34.92409241186293
At time: 183.7134976387024 and batch: 600, loss is 3.5760652303695677 and perplexity is 35.73266406859575
At time: 184.18666768074036 and batch: 650, loss is 3.4369557762145995 and perplexity is 31.092162451355694
At time: 184.66010689735413 and batch: 700, loss is 3.43079927444458 and perplexity is 30.901331526401087
At time: 185.13320016860962 and batch: 750, loss is 3.5363651275634767 and perplexity is 34.34186375428748
At time: 185.60616827011108 and batch: 800, loss is 3.4840945529937746 and perplexity is 32.59290259935392
At time: 186.07850742340088 and batch: 850, loss is 3.566043381690979 and perplexity is 35.37634518568871
At time: 186.5514931678772 and batch: 900, loss is 3.531816210746765 and perplexity is 34.18600024657297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327514230388484 and perplexity of 75.7557410069659
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -75.75251077531675
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f06654d0b70>
ELAPSED
207.72251296043396


RESULTS SO FAR:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7007956504821777 and batch: 50, loss is 6.770518388748169 and perplexity is 871.763689365465
At time: 1.1883354187011719 and batch: 100, loss is 6.111744203567505 and perplexity is 451.1248829802438
At time: 1.6624338626861572 and batch: 150, loss is 6.05258261680603 and perplexity is 425.20976709999496
At time: 2.149540901184082 and batch: 200, loss is 5.914458312988281 and perplexity is 370.3536326065241
At time: 2.6245579719543457 and batch: 250, loss is 5.97941780090332 and perplexity is 395.21021013558993
At time: 3.09930157661438 and batch: 300, loss is 5.8940564727783205 and perplexity is 362.87429255175556
At time: 3.5743911266326904 and batch: 350, loss is 5.899064779281616 and perplexity is 364.69623685042336
At time: 4.049734354019165 and batch: 400, loss is 5.776086091995239 and perplexity is 322.4945032792542
At time: 4.524885654449463 and batch: 450, loss is 5.772822141647339 and perplexity is 321.44361319257206
At time: 4.999370336532593 and batch: 500, loss is 5.741731281280518 and perplexity is 311.6034174244108
At time: 5.474832773208618 and batch: 550, loss is 5.784341955184937 and perplexity is 325.167994581721
At time: 5.949887275695801 and batch: 600, loss is 5.724373044967652 and perplexity is 306.2412055783188
At time: 6.425493478775024 and batch: 650, loss is 5.644615383148193 and perplexity is 282.76477920620516
At time: 6.900737285614014 and batch: 700, loss is 5.743376579284668 and perplexity is 312.1165198926714
At time: 7.375227212905884 and batch: 750, loss is 5.7013899612426755 and perplexity is 299.28310390966504
At time: 7.849367380142212 and batch: 800, loss is 5.709164485931397 and perplexity is 301.618956094642
At time: 8.323180675506592 and batch: 850, loss is 5.739060859680176 and perplexity is 310.77241498504605
At time: 8.798049449920654 and batch: 900, loss is 5.628937101364135 and perplexity is 278.36608540233806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.467480803189212 and perplexity of 236.86273667158372
finished 1 epochs...
Completing Train Step...
At time: 10.059515953063965 and batch: 50, loss is 5.295970630645752 and perplexity is 199.53120310593948
At time: 10.528079271316528 and batch: 100, loss is 5.09083251953125 and perplexity is 162.52511107624056
At time: 10.997079133987427 and batch: 150, loss is 5.026811466217041 and perplexity is 152.44615720816768
At time: 11.464819431304932 and batch: 200, loss is 4.8912644195556645 and perplexity is 133.1217894753333
At time: 11.942726612091064 and batch: 250, loss is 4.9594395637512205 and perplexity is 142.5139035542909
At time: 12.418596029281616 and batch: 300, loss is 4.889715671539307 and perplexity is 132.915776939881
At time: 12.902037620544434 and batch: 350, loss is 4.859169273376465 and perplexity is 128.91706277551643
At time: 13.37590765953064 and batch: 400, loss is 4.72161982536316 and perplexity is 112.35009288335853
At time: 13.857922554016113 and batch: 450, loss is 4.7309912109375 and perplexity is 113.407917824019
At time: 14.327016353607178 and batch: 500, loss is 4.636338529586792 and perplexity is 103.16591624850007
At time: 14.795844793319702 and batch: 550, loss is 4.705549812316894 and perplexity is 110.55905497005433
At time: 15.265873908996582 and batch: 600, loss is 4.654195280075073 and perplexity is 105.0246705391531
At time: 15.735198497772217 and batch: 650, loss is 4.509709310531616 and perplexity is 90.89539233653358
At time: 16.20464062690735 and batch: 700, loss is 4.558564624786377 and perplexity is 95.44638009060917
At time: 16.674754858016968 and batch: 750, loss is 4.60207579612732 and perplexity is 99.69103928290927
At time: 17.14477229118347 and batch: 800, loss is 4.549945135116577 and perplexity is 94.62721646130034
At time: 17.614072561264038 and batch: 850, loss is 4.604590320587159 and perplexity is 99.94203026885175
At time: 18.084912538528442 and batch: 900, loss is 4.54100154876709 and perplexity is 93.7846830311307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.641813931399828 and perplexity of 103.73234037810704
finished 2 epochs...
Completing Train Step...
At time: 19.34140634536743 and batch: 50, loss is 4.570883865356445 and perplexity is 96.62947948895132
At time: 19.80992865562439 and batch: 100, loss is 4.440531449317932 and perplexity is 84.82000723272789
At time: 20.280213356018066 and batch: 150, loss is 4.434825038909912 and perplexity is 84.33736783956124
At time: 20.74983286857605 and batch: 200, loss is 4.33858030796051 and perplexity is 76.59871552175926
At time: 21.21961236000061 and batch: 250, loss is 4.465384407043457 and perplexity is 86.954449018662
At time: 21.688867807388306 and batch: 300, loss is 4.435576314926148 and perplexity is 84.40075228791822
At time: 22.158823251724243 and batch: 350, loss is 4.422146673202515 and perplexity is 83.27485752682067
At time: 22.627036094665527 and batch: 400, loss is 4.329070763587952 and perplexity is 75.87374915086461
At time: 23.096569776535034 and batch: 450, loss is 4.36226866722107 and perplexity is 78.43487536004969
At time: 23.568391799926758 and batch: 500, loss is 4.252032713890076 and perplexity is 70.24806152550006
At time: 24.040504693984985 and batch: 550, loss is 4.33952919960022 and perplexity is 76.67143389800373
At time: 24.511968851089478 and batch: 600, loss is 4.321229581832886 and perplexity is 75.28113572448478
At time: 24.98602533340454 and batch: 650, loss is 4.177453722953796 and perplexity is 65.19962536325785
At time: 25.46036148071289 and batch: 700, loss is 4.206895833015442 and perplexity is 67.14777804636438
At time: 25.947938442230225 and batch: 750, loss is 4.289114146232605 and perplexity is 72.9018594999069
At time: 26.423054695129395 and batch: 800, loss is 4.246750702857971 and perplexity is 69.877988714282
At time: 26.895440578460693 and batch: 850, loss is 4.317488498687744 and perplexity is 75.00002888636311
At time: 27.36758065223694 and batch: 900, loss is 4.266357245445252 and perplexity is 71.26157381354544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.496849164570848 and perplexity of 89.73394849947168
finished 3 epochs...
Completing Train Step...
At time: 28.61955976486206 and batch: 50, loss is 4.33241268157959 and perplexity is 76.12773716541572
At time: 29.10171866416931 and batch: 100, loss is 4.201476573944092 and perplexity is 66.78487107268526
At time: 29.57079029083252 and batch: 150, loss is 4.207979755401611 and perplexity is 67.22060048597679
At time: 30.04016900062561 and batch: 200, loss is 4.110291352272034 and perplexity is 60.96447712127636
At time: 30.508623361587524 and batch: 250, loss is 4.25358660697937 and perplexity is 70.35730435669977
At time: 30.97779393196106 and batch: 300, loss is 4.23178831577301 and perplexity is 68.8402302309236
At time: 31.447676181793213 and batch: 350, loss is 4.218850469589233 and perplexity is 67.95532266346856
At time: 31.91735863685608 and batch: 400, loss is 4.141171326637268 and perplexity is 62.87642716661383
At time: 32.38859272003174 and batch: 450, loss is 4.176912202835083 and perplexity is 65.16432801236644
At time: 32.85853099822998 and batch: 500, loss is 4.0632212400436405 and perplexity is 58.16136135648923
At time: 33.32764267921448 and batch: 550, loss is 4.14892083644867 and perplexity is 63.365581561230066
At time: 33.79706907272339 and batch: 600, loss is 4.1449093866348266 and perplexity is 63.111902860737466
At time: 34.2672700881958 and batch: 650, loss is 4.005402970314026 and perplexity is 54.89394057116047
At time: 34.73789024353027 and batch: 700, loss is 4.021609482765197 and perplexity is 55.790827980387924
At time: 35.20922875404358 and batch: 750, loss is 4.118424272537231 and perplexity is 61.46231805892598
At time: 35.68041920661926 and batch: 800, loss is 4.07651930809021 and perplexity is 58.93996055622535
At time: 36.15127158164978 and batch: 850, loss is 4.153042869567871 and perplexity is 63.62731565474687
At time: 36.62204670906067 and batch: 900, loss is 4.107202053070068 and perplexity is 60.77643022687409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.439070192101884 and perplexity of 84.69615389792177
finished 4 epochs...
Completing Train Step...
At time: 37.87039065361023 and batch: 50, loss is 4.183403954505921 and perplexity is 65.58873472865022
At time: 38.360177993774414 and batch: 100, loss is 4.055882434844971 and perplexity is 57.73608886010253
At time: 38.834052085876465 and batch: 150, loss is 4.065103740692138 and perplexity is 58.270953278027626
At time: 39.307695150375366 and batch: 200, loss is 3.9692341232299806 and perplexity is 52.94396675386398
At time: 39.78214240074158 and batch: 250, loss is 4.116106204986572 and perplexity is 61.32000925827572
At time: 40.255614042282104 and batch: 300, loss is 4.102291326522828 and perplexity is 60.47870541838128
At time: 40.729048013687134 and batch: 350, loss is 4.087090110778808 and perplexity is 59.5663079144838
At time: 41.202176570892334 and batch: 400, loss is 4.017317953109742 and perplexity is 55.55191300962789
At time: 41.67563605308533 and batch: 450, loss is 4.052878127098084 and perplexity is 57.562892179400734
At time: 42.14969873428345 and batch: 500, loss is 3.937925410270691 and perplexity is 51.3120393727334
At time: 42.62383818626404 and batch: 550, loss is 4.0214762878417964 and perplexity is 55.78339742019604
At time: 43.0975558757782 and batch: 600, loss is 4.0263522481918335 and perplexity is 56.05605925815188
At time: 43.572036027908325 and batch: 650, loss is 3.8911636447906495 and perplexity is 48.967834549251926
At time: 44.04636001586914 and batch: 700, loss is 3.899213309288025 and perplexity is 49.36359994081215
At time: 44.519134521484375 and batch: 750, loss is 4.002709336280823 and perplexity is 54.746275351935495
At time: 44.9907968044281 and batch: 800, loss is 3.9610427904129026 and perplexity is 52.5120564769606
At time: 45.46371269226074 and batch: 850, loss is 4.037467093467712 and perplexity is 56.68258912474518
At time: 45.93852186203003 and batch: 900, loss is 3.998079767227173 and perplexity is 54.4934094714286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.416290701252141 and perplexity of 82.78862736157622
finished 5 epochs...
Completing Train Step...
At time: 47.1993567943573 and batch: 50, loss is 4.078114347457886 and perplexity is 59.03404712958852
At time: 47.671507596969604 and batch: 100, loss is 3.95552303314209 and perplexity is 52.223001162906606
At time: 48.1446259021759 and batch: 150, loss is 3.9617476940155028 and perplexity is 52.54908546415111
At time: 48.61921501159668 and batch: 200, loss is 3.8666803073883056 and perplexity is 47.78349597503369
At time: 49.093822717666626 and batch: 250, loss is 4.017401366233826 and perplexity is 55.556546961504395
At time: 49.567368268966675 and batch: 300, loss is 4.005435056686402 and perplexity is 54.895701946836745
At time: 50.03938937187195 and batch: 350, loss is 3.9894395780563356 and perplexity is 54.02460430416353
At time: 50.525590896606445 and batch: 400, loss is 3.923319263458252 and perplexity is 50.568015083842695
At time: 50.99921369552612 and batch: 450, loss is 3.9615430545806887 and perplexity is 52.53833294923354
At time: 51.47354197502136 and batch: 500, loss is 3.845438017845154 and perplexity is 46.779169975280794
At time: 51.94679856300354 and batch: 550, loss is 3.92928231716156 and perplexity is 50.8704557121299
At time: 52.42066502571106 and batch: 600, loss is 3.935089430809021 and perplexity is 51.166725633719444
At time: 52.89386987686157 and batch: 650, loss is 3.803337116241455 and perplexity is 44.85060672309254
At time: 53.36763787269592 and batch: 700, loss is 3.808898663520813 and perplexity is 45.10074041332701
At time: 53.841203451156616 and batch: 750, loss is 3.9133589839935303 and perplexity is 50.066843569114916
At time: 54.31495904922485 and batch: 800, loss is 3.8742105102539064 and perplexity is 48.144673556902795
At time: 54.78883504867554 and batch: 850, loss is 3.948649535179138 and perplexity is 51.865277286349375
At time: 55.262155532836914 and batch: 900, loss is 3.9125521993637085 and perplexity is 50.02646669916881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.407924704355736 and perplexity of 82.09890708364723
finished 6 epochs...
Completing Train Step...
At time: 56.521002531051636 and batch: 50, loss is 3.9950392866134643 and perplexity is 54.327974944039426
At time: 56.99495244026184 and batch: 100, loss is 3.8765562629699706 and perplexity is 48.25774161867248
At time: 57.468132734298706 and batch: 150, loss is 3.882121934890747 and perplexity is 48.527077197633815
At time: 57.94120788574219 and batch: 200, loss is 3.789545302391052 and perplexity is 44.236281570907416
At time: 58.413737535476685 and batch: 250, loss is 3.9372223949432374 and perplexity is 51.27597889958932
At time: 58.885873794555664 and batch: 300, loss is 3.9285046339035032 and perplexity is 50.83090998940919
At time: 59.35955476760864 and batch: 350, loss is 3.9106105852127073 and perplexity is 49.9294288390273
At time: 59.83300018310547 and batch: 400, loss is 3.8500705480575563 and perplexity is 46.996378617854546
At time: 60.30668830871582 and batch: 450, loss is 3.887460174560547 and perplexity is 48.7868190314529
At time: 60.780622482299805 and batch: 500, loss is 3.7749155235290526 and perplexity is 43.59382550763639
At time: 61.25393891334534 and batch: 550, loss is 3.8542749071121216 and perplexity is 47.194384219477676
At time: 61.72810387611389 and batch: 600, loss is 3.86519832611084 and perplexity is 47.712734175408926
At time: 62.21435308456421 and batch: 650, loss is 3.734384689331055 and perplexity is 41.86225934200101
At time: 62.6877601146698 and batch: 700, loss is 3.7361345386505125 and perplexity is 41.93557611596726
At time: 63.16029453277588 and batch: 750, loss is 3.8421409845352175 and perplexity is 46.625191469277105
At time: 63.633394718170166 and batch: 800, loss is 3.804297342300415 and perplexity is 44.89369412794203
At time: 64.10692691802979 and batch: 850, loss is 3.8793442058563232 and perplexity is 48.39246916521664
At time: 64.57882452011108 and batch: 900, loss is 3.84160831451416 and perplexity is 46.6003622410352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4092779290186215 and perplexity of 82.21008055388444
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 65.82775545120239 and batch: 50, loss is 3.9432024574279785 and perplexity is 51.583531131623054
At time: 66.31311130523682 and batch: 100, loss is 3.8102231931686403 and perplexity is 45.16051726050732
At time: 66.78480458259583 and batch: 150, loss is 3.8137748575210573 and perplexity is 45.32119743191075
At time: 67.25711750984192 and batch: 200, loss is 3.700794596672058 and perplexity is 40.479456425776135
At time: 67.72902035713196 and batch: 250, loss is 3.8429488277435304 and perplexity is 46.662872531687775
At time: 68.20195960998535 and batch: 300, loss is 3.8199870872497557 and perplexity is 45.603619448898584
At time: 68.67480874061584 and batch: 350, loss is 3.7951081705093386 and perplexity is 44.48304789924138
At time: 69.14782357215881 and batch: 400, loss is 3.728770637512207 and perplexity is 41.62790091451286
At time: 69.62079811096191 and batch: 450, loss is 3.7525803995132447 and perplexity is 42.63094506369644
At time: 70.09267163276672 and batch: 500, loss is 3.6345032978057863 and perplexity is 37.8830316383106
At time: 70.56553030014038 and batch: 550, loss is 3.69659309387207 and perplexity is 40.309738660757326
At time: 71.03898215293884 and batch: 600, loss is 3.703381938934326 and perplexity is 40.58432624269201
At time: 71.51222491264343 and batch: 650, loss is 3.562326512336731 and perplexity is 35.245099994115236
At time: 71.98754167556763 and batch: 700, loss is 3.551012110710144 and perplexity is 34.84857025592975
At time: 72.46042251586914 and batch: 750, loss is 3.6440423011779783 and perplexity is 38.246127035722566
At time: 72.93452596664429 and batch: 800, loss is 3.593259472846985 and perplexity is 36.352372600759686
At time: 73.40757966041565 and batch: 850, loss is 3.654713134765625 and perplexity is 38.65643033850917
At time: 73.88116359710693 and batch: 900, loss is 3.609475212097168 and perplexity is 36.946658566868294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329813918022261 and perplexity of 75.93015602090549
finished 8 epochs...
Completing Train Step...
At time: 75.14408993721008 and batch: 50, loss is 3.8570394849777223 and perplexity is 47.32503728654623
At time: 75.6295838356018 and batch: 100, loss is 3.727795834541321 and perplexity is 41.58734168487017
At time: 76.1023051738739 and batch: 150, loss is 3.736105351448059 and perplexity is 41.934352151679285
At time: 76.57624793052673 and batch: 200, loss is 3.6314239740371703 and perplexity is 37.76655694224622
At time: 77.04962253570557 and batch: 250, loss is 3.775583505630493 and perplexity is 43.62295513075911
At time: 77.52273941040039 and batch: 300, loss is 3.7600058031082155 and perplexity is 42.94867521384983
At time: 77.99529337882996 and batch: 350, loss is 3.7368271255493166 and perplexity is 41.96463020665888
At time: 78.46816730499268 and batch: 400, loss is 3.674534840583801 and perplexity is 39.430311210163026
At time: 78.94035220146179 and batch: 450, loss is 3.7037303972244264 and perplexity is 40.59847065184461
At time: 79.4117476940155 and batch: 500, loss is 3.5876352405548095 and perplexity is 36.14849228543593
At time: 79.88187003135681 and batch: 550, loss is 3.654578742980957 and perplexity is 38.65123558092127
At time: 80.35349869728088 and batch: 600, loss is 3.6667412900924683 and perplexity is 39.12420347131611
At time: 80.82628846168518 and batch: 650, loss is 3.529968514442444 and perplexity is 34.12289321952205
At time: 81.29883480072021 and batch: 700, loss is 3.5216801404953 and perplexity is 33.84123876149904
At time: 81.77111864089966 and batch: 750, loss is 3.6205958843231203 and perplexity is 37.35982332357595
At time: 82.24408674240112 and batch: 800, loss is 3.574606146812439 and perplexity is 35.68056514358616
At time: 82.71708416938782 and batch: 850, loss is 3.643583903312683 and perplexity is 38.22859911042227
At time: 83.1897177696228 and batch: 900, loss is 3.604743595123291 and perplexity is 36.77225406310388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328288875214041 and perplexity of 75.81444753516166
finished 9 epochs...
Completing Train Step...
At time: 84.45211791992188 and batch: 50, loss is 3.819494910240173 and perplexity is 45.58117991841557
At time: 84.9241681098938 and batch: 100, loss is 3.690411310195923 and perplexity is 40.061321198913944
At time: 85.39833474159241 and batch: 150, loss is 3.698374319076538 and perplexity is 40.38160336785837
At time: 85.87126159667969 and batch: 200, loss is 3.5962271022796632 and perplexity is 36.4604132045841
At time: 86.35654711723328 and batch: 250, loss is 3.7399962091445924 and perplexity is 42.09783057790572
At time: 86.82872104644775 and batch: 300, loss is 3.7279418992996214 and perplexity is 41.593416573534505
At time: 87.30069851875305 and batch: 350, loss is 3.704477519989014 and perplexity is 40.62881402717781
At time: 87.77410864830017 and batch: 400, loss is 3.6442379713058473 and perplexity is 38.25361139249878
At time: 88.24742579460144 and batch: 450, loss is 3.6749342775344847 and perplexity is 39.44606427940683
At time: 88.72106623649597 and batch: 500, loss is 3.559796133041382 and perplexity is 35.156029261721464
At time: 89.1934072971344 and batch: 550, loss is 3.6282673978805544 and perplexity is 37.64753188374589
At time: 89.66612124443054 and batch: 600, loss is 3.6425613355636597 and perplexity is 38.18952775783871
At time: 90.13781380653381 and batch: 650, loss is 3.5080828714370726 and perplexity is 33.38420458271372
At time: 90.61040878295898 and batch: 700, loss is 3.5009553956985475 and perplexity is 33.1471054374401
At time: 91.08352255821228 and batch: 750, loss is 3.6019547843933104 and perplexity is 36.669846071033554
At time: 91.5562789440155 and batch: 800, loss is 3.558353319168091 and perplexity is 35.105342229742284
At time: 92.02921223640442 and batch: 850, loss is 3.630493893623352 and perplexity is 37.731447337245825
At time: 92.50422930717468 and batch: 900, loss is 3.5938293743133545 and perplexity is 36.37309577573389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329765006287457 and perplexity of 75.92644223607515
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 93.76447868347168 and batch: 50, loss is 3.8093230390548705 and perplexity is 45.11988412590061
At time: 94.23772883415222 and batch: 100, loss is 3.6790005588531494 and perplexity is 39.60678962946391
At time: 94.71003937721252 and batch: 150, loss is 3.6870481634140013 and perplexity is 39.9268154035908
At time: 95.18107414245605 and batch: 200, loss is 3.5799462270736693 and perplexity is 35.87161187365731
At time: 95.65232801437378 and batch: 250, loss is 3.7211214971542357 and perplexity is 41.31069796914552
At time: 96.12277245521545 and batch: 300, loss is 3.7060731840133667 and perplexity is 40.693695714994135
At time: 96.59281373023987 and batch: 350, loss is 3.6801354551315306 and perplexity is 39.65176474383069
At time: 97.0635232925415 and batch: 400, loss is 3.6201684713363647 and perplexity is 37.34385866189946
At time: 97.53499817848206 and batch: 450, loss is 3.6402904653549193 and perplexity is 38.102902691335046
At time: 98.0046718120575 and batch: 500, loss is 3.5238308572769164 and perplexity is 33.91410000547868
At time: 98.48810291290283 and batch: 550, loss is 3.58718092918396 and perplexity is 36.13207334428983
At time: 98.9590094089508 and batch: 600, loss is 3.6011947107315065 and perplexity is 36.641984876473956
At time: 99.43045091629028 and batch: 650, loss is 3.4624352788925172 and perplexity is 31.89455414924554
At time: 99.90156126022339 and batch: 700, loss is 3.4506199741363526 and perplexity is 31.51992779238588
At time: 100.37254571914673 and batch: 750, loss is 3.546103639602661 and perplexity is 34.67793617451242
At time: 100.84264588356018 and batch: 800, loss is 3.498866572380066 and perplexity is 33.077939253779576
At time: 101.31520462036133 and batch: 850, loss is 3.570217332839966 and perplexity is 35.52431291253242
At time: 101.78748178482056 and batch: 900, loss is 3.532798728942871 and perplexity is 34.219605119863715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314622016802226 and perplexity of 74.785350495003
finished 11 epochs...
Completing Train Step...
At time: 103.03531670570374 and batch: 50, loss is 3.7845042610168456 and perplexity is 44.01384577035078
At time: 103.52185606956482 and batch: 100, loss is 3.6532015228271484 and perplexity is 38.598040959070275
At time: 103.99573040008545 and batch: 150, loss is 3.6633793020248415 and perplexity is 38.99288922823597
At time: 104.46925520896912 and batch: 200, loss is 3.5589182329177858 and perplexity is 35.12517932285139
At time: 104.94193124771118 and batch: 250, loss is 3.700971665382385 and perplexity is 40.486624705540486
At time: 105.4149079322815 and batch: 300, loss is 3.6876057147979737 and perplexity is 39.94908286182592
At time: 105.88762044906616 and batch: 350, loss is 3.663223090171814 and perplexity is 38.986798552484984
At time: 106.36061573028564 and batch: 400, loss is 3.6046979665756225 and perplexity is 36.770576236835154
At time: 106.83450388908386 and batch: 450, loss is 3.6272434425354003 and perplexity is 37.60900222193473
At time: 107.30753493309021 and batch: 500, loss is 3.511577205657959 and perplexity is 33.50106420560556
At time: 107.78001141548157 and batch: 550, loss is 3.576670570373535 and perplexity is 35.75430102780257
At time: 108.25285983085632 and batch: 600, loss is 3.592819628715515 and perplexity is 36.33638673890773
At time: 108.72606062889099 and batch: 650, loss is 3.45558406829834 and perplexity is 31.676784686012766
At time: 109.19874405860901 and batch: 700, loss is 3.445378928184509 and perplexity is 31.3551625506839
At time: 109.67133498191833 and batch: 750, loss is 3.542436022758484 and perplexity is 34.550983740427746
At time: 110.14510607719421 and batch: 800, loss is 3.4970616483688355 and perplexity is 33.01828993441634
At time: 110.63388776779175 and batch: 850, loss is 3.570976595878601 and perplexity is 35.551295452426196
At time: 111.11236190795898 and batch: 900, loss is 3.535364007949829 and perplexity is 34.30750064457542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313650470890411 and perplexity of 74.71272837703917
finished 12 epochs...
Completing Train Step...
At time: 112.36282706260681 and batch: 50, loss is 3.772529263496399 and perplexity is 43.48992332227679
At time: 112.84817028045654 and batch: 100, loss is 3.641306438446045 and perplexity is 38.14163388675786
At time: 113.32125353813171 and batch: 150, loss is 3.651404848098755 and perplexity is 38.528755095032224
At time: 113.79585409164429 and batch: 200, loss is 3.5474531936645506 and perplexity is 34.724767517741604
At time: 114.26957249641418 and batch: 250, loss is 3.6896517372131346 and perplexity is 40.03090325546294
At time: 114.7433750629425 and batch: 300, loss is 3.67700786113739 and perplexity is 39.52794385422918
At time: 115.21807813644409 and batch: 350, loss is 3.6530982637405396 and perplexity is 38.594055566383496
At time: 115.6926121711731 and batch: 400, loss is 3.595511541366577 and perplexity is 36.434332890159546
At time: 116.16749739646912 and batch: 450, loss is 3.6187478590011595 and perplexity is 37.29084518035883
At time: 116.6417338848114 and batch: 500, loss is 3.50356680393219 and perplexity is 33.23377918251098
At time: 117.11553120613098 and batch: 550, loss is 3.569439630508423 and perplexity is 35.49669631169324
At time: 117.58862090110779 and batch: 600, loss is 3.5867865657806397 and perplexity is 36.11782698618252
At time: 118.06119894981384 and batch: 650, loss is 3.45025915145874 and perplexity is 31.508556739231974
At time: 118.53439044952393 and batch: 700, loss is 3.4407375144958494 and perplexity is 31.209967485849702
At time: 119.00808620452881 and batch: 750, loss is 3.538602204322815 and perplexity is 34.41877513582132
At time: 119.48184204101562 and batch: 800, loss is 3.4942148685455323 and perplexity is 32.92442779863766
At time: 119.95593309402466 and batch: 850, loss is 3.5693950557708742 and perplexity is 35.4951140910351
At time: 120.42952227592468 and batch: 900, loss is 3.534626007080078 and perplexity is 34.28219101967207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313651306988442 and perplexity of 74.71279084423035
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 121.69785785675049 and batch: 50, loss is 3.7704530000686645 and perplexity is 43.39972045986431
At time: 122.17050528526306 and batch: 100, loss is 3.642013568878174 and perplexity is 38.168614535104794
At time: 122.65594720840454 and batch: 150, loss is 3.652351670265198 and perplexity is 38.56525224983268
At time: 123.12776112556458 and batch: 200, loss is 3.5462747859954833 and perplexity is 34.68387168610541
At time: 123.59968447685242 and batch: 250, loss is 3.689211468696594 and perplexity is 40.013282788224
At time: 124.07281804084778 and batch: 300, loss is 3.674643774032593 and perplexity is 39.43460672390839
At time: 124.54599976539612 and batch: 350, loss is 3.647851676940918 and perplexity is 38.39209875913094
At time: 125.01915287971497 and batch: 400, loss is 3.5927995920181273 and perplexity is 36.33565868501639
At time: 125.49138474464417 and batch: 450, loss is 3.6129814338684083 and perplexity is 37.07642911472463
At time: 125.96425771713257 and batch: 500, loss is 3.4982741975784304 and perplexity is 33.05835051858499
At time: 126.43706583976746 and batch: 550, loss is 3.557871413230896 and perplexity is 35.0884288325551
At time: 126.91083097457886 and batch: 600, loss is 3.5787958192825315 and perplexity is 35.83036861970609
At time: 127.3833258152008 and batch: 650, loss is 3.4355910491943358 and perplexity is 31.04975907823571
At time: 127.85611772537231 and batch: 700, loss is 3.4268013191223146 and perplexity is 30.778036012763454
At time: 128.3295283317566 and batch: 750, loss is 3.523606333732605 and perplexity is 33.90648634629818
At time: 128.8023021221161 and batch: 800, loss is 3.477386498451233 and perplexity is 32.37499930172083
At time: 129.27476143836975 and batch: 850, loss is 3.548661675453186 and perplexity is 34.766757133634215
At time: 129.74738192558289 and batch: 900, loss is 3.5176869773864747 and perplexity is 33.70637462177802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309829502889555 and perplexity of 74.42779813398462
finished 14 epochs...
Completing Train Step...
At time: 131.00700163841248 and batch: 50, loss is 3.7614442539215087 and perplexity is 43.010499225380684
At time: 131.47761297225952 and batch: 100, loss is 3.6317227125167846 and perplexity is 37.77784095144737
At time: 131.94877910614014 and batch: 150, loss is 3.6430247735977175 and perplexity is 38.207230339212316
At time: 132.4285750389099 and batch: 200, loss is 3.537715849876404 and perplexity is 34.38828141755742
At time: 132.90204334259033 and batch: 250, loss is 3.6821121311187746 and perplexity is 39.730220950756156
At time: 133.37490582466125 and batch: 300, loss is 3.668718523979187 and perplexity is 39.201637699773585
At time: 133.84824872016907 and batch: 350, loss is 3.6419009923934937 and perplexity is 38.164317888510546
At time: 134.32109689712524 and batch: 400, loss is 3.5870620727539064 and perplexity is 36.12777907024778
At time: 134.8072154521942 and batch: 450, loss is 3.6083220863342285 and perplexity is 36.90407897754919
At time: 135.27952480316162 and batch: 500, loss is 3.494132571220398 and perplexity is 32.921718317791296
At time: 135.7523889541626 and batch: 550, loss is 3.5545747756958006 and perplexity is 34.972945459017794
At time: 136.22532963752747 and batch: 600, loss is 3.5765125703811647 and perplexity is 35.74865229477461
At time: 136.69795536994934 and batch: 650, loss is 3.4340651512145994 and perplexity is 31.002416442788732
At time: 137.17020511627197 and batch: 700, loss is 3.4257542657852174 and perplexity is 30.745826632859004
At time: 137.6418867111206 and batch: 750, loss is 3.5229397916793825 and perplexity is 33.88389377755638
At time: 138.11435651779175 and batch: 800, loss is 3.4777555799484254 and perplexity is 32.386950520285865
At time: 138.58556985855103 and batch: 850, loss is 3.5502631759643553 and perplexity is 34.822480721722826
At time: 139.0570170879364 and batch: 900, loss is 3.5201094818115233 and perplexity is 33.78812744676104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309410617776113 and perplexity of 74.39662796613545
finished 15 epochs...
Completing Train Step...
At time: 140.303302526474 and batch: 50, loss is 3.7576816701889038 and perplexity is 42.84897268978303
At time: 140.7880837917328 and batch: 100, loss is 3.6277854156494143 and perplexity is 37.62939081451935
At time: 141.25998187065125 and batch: 150, loss is 3.6390808916091917 and perplexity is 38.05684228294974
At time: 141.73254466056824 and batch: 200, loss is 3.534000778198242 and perplexity is 34.26076350296312
At time: 142.20569467544556 and batch: 250, loss is 3.6784651136398314 and perplexity is 39.585588040193215
At time: 142.67841720581055 and batch: 300, loss is 3.6653294563293457 and perplexity is 39.06900557419614
At time: 143.15087723731995 and batch: 350, loss is 3.638644037246704 and perplexity is 38.04022061626407
At time: 143.62283635139465 and batch: 400, loss is 3.584087390899658 and perplexity is 36.02047010555727
At time: 144.09422373771667 and batch: 450, loss is 3.6056774377822878 and perplexity is 36.80660960145243
At time: 144.56565713882446 and batch: 500, loss is 3.4917875719070435 and perplexity is 32.84460735885074
At time: 145.04013991355896 and batch: 550, loss is 3.5525283432006836 and perplexity is 34.90144886840529
At time: 145.51293087005615 and batch: 600, loss is 3.5750407934188844 and perplexity is 35.69607695097489
At time: 145.9859356880188 and batch: 650, loss is 3.432819700241089 and perplexity is 30.963828487739942
At time: 146.45776557922363 and batch: 700, loss is 3.424908366203308 and perplexity is 30.719829747892035
At time: 146.9421706199646 and batch: 750, loss is 3.522339587211609 and perplexity is 33.863562615163126
At time: 147.41493105888367 and batch: 800, loss is 3.477549657821655 and perplexity is 32.38028201717486
At time: 147.8873143196106 and batch: 850, loss is 3.5505517435073854 and perplexity is 34.832530809421726
At time: 148.35849046707153 and batch: 900, loss is 3.5207333326339723 and perplexity is 33.8092127742233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309314466502569 and perplexity of 74.38947497949904
finished 16 epochs...
Completing Train Step...
At time: 149.60780453681946 and batch: 50, loss is 3.754517397880554 and perplexity is 42.71360116120108
At time: 150.09338331222534 and batch: 100, loss is 3.624596047401428 and perplexity is 37.509568011356826
At time: 150.56627821922302 and batch: 150, loss is 3.635874333381653 and perplexity is 37.93500624378341
At time: 151.03893899917603 and batch: 200, loss is 3.5310025358200074 and perplexity is 34.158195268966836
At time: 151.51056623458862 and batch: 250, loss is 3.6755165004730226 and perplexity is 39.46903736994653
At time: 151.98205995559692 and batch: 300, loss is 3.6625688219070436 and perplexity is 38.96129907010776
At time: 152.4550540447235 and batch: 350, loss is 3.635983443260193 and perplexity is 37.93914555352278
At time: 152.92810463905334 and batch: 400, loss is 3.581694288253784 and perplexity is 35.934372484554004
At time: 153.4006028175354 and batch: 450, loss is 3.6034579610824586 and perplexity is 36.72500877810301
At time: 153.8747296333313 and batch: 500, loss is 3.4897957324981688 and perplexity is 32.77925128655251
At time: 154.3474133014679 and batch: 550, loss is 3.5507268571853636 and perplexity is 34.83863099610253
At time: 154.81922483444214 and batch: 600, loss is 3.5736719512939454 and perplexity is 35.647248084278495
At time: 155.29193758964539 and batch: 650, loss is 3.431573920249939 and perplexity is 30.925278387218174
At time: 155.76574969291687 and batch: 700, loss is 3.42394633769989 and perplexity is 30.69029060708034
At time: 156.23798298835754 and batch: 750, loss is 3.521566891670227 and perplexity is 33.837406497974044
At time: 156.7104082107544 and batch: 800, loss is 3.477049150466919 and perplexity is 32.364079502953835
At time: 157.18215775489807 and batch: 850, loss is 3.5503826570510864 and perplexity is 34.8266415981302
At time: 157.65561532974243 and batch: 900, loss is 3.520796718597412 and perplexity is 33.81135587166856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309320737237799 and perplexity of 74.38994145766316
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 158.91703724861145 and batch: 50, loss is 3.7537628316879275 and perplexity is 42.68138307866611
At time: 159.38990831375122 and batch: 100, loss is 3.625045671463013 and perplexity is 37.526437007743525
At time: 159.86286973953247 and batch: 150, loss is 3.636406774520874 and perplexity is 37.955209779843806
At time: 160.33432841300964 and batch: 200, loss is 3.530964479446411 and perplexity is 34.15689535666143
At time: 160.8072006702423 and batch: 250, loss is 3.675771565437317 and perplexity is 39.479105822554196
At time: 161.27949047088623 and batch: 300, loss is 3.6630637884140014 and perplexity is 38.98058838160279
At time: 161.7515275478363 and batch: 350, loss is 3.634476647377014 and perplexity is 37.88202205272725
At time: 162.22517561912537 and batch: 400, loss is 3.5803579568862913 and perplexity is 35.88638432661345
At time: 162.69851803779602 and batch: 450, loss is 3.6016899824142454 and perplexity is 36.66013710875488
At time: 163.17183208465576 and batch: 500, loss is 3.4891797208786013 and perplexity is 32.75906510497881
At time: 163.64524292945862 and batch: 550, loss is 3.547303137779236 and perplexity is 34.71955725293461
At time: 164.11820912361145 and batch: 600, loss is 3.570581068992615 and perplexity is 35.537236739725834
At time: 164.59050226211548 and batch: 650, loss is 3.4263187885284423 and perplexity is 30.763188251305664
At time: 165.06234550476074 and batch: 700, loss is 3.4188269186019897 and perplexity is 30.53357563430985
At time: 165.53451776504517 and batch: 750, loss is 3.517127833366394 and perplexity is 33.68753317200252
At time: 166.00693559646606 and batch: 800, loss is 3.4704792642593385 and perplexity is 32.15214812822035
At time: 166.47906231880188 and batch: 850, loss is 3.542023596763611 and perplexity is 34.5367369546591
At time: 166.9520707130432 and batch: 900, loss is 3.5124478340148926 and perplexity is 33.530243882574354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308340412296661 and perplexity of 74.31705087674717
finished 18 epochs...
Completing Train Step...
At time: 168.21210432052612 and batch: 50, loss is 3.7514996194839476 and perplexity is 42.584895278967885
At time: 168.68576955795288 and batch: 100, loss is 3.622390098571777 and perplexity is 37.42691502135198
At time: 169.16066122055054 and batch: 150, loss is 3.63410879611969 and perplexity is 37.86808966596528
At time: 169.63412261009216 and batch: 200, loss is 3.5287096977233885 and perplexity is 34.07996577559547
At time: 170.10799551010132 and batch: 250, loss is 3.674070620536804 and perplexity is 39.41201111720358
At time: 170.58179998397827 and batch: 300, loss is 3.6611198949813843 and perplexity is 38.904887872547036
At time: 171.0686755180359 and batch: 350, loss is 3.6328646659851076 and perplexity is 37.82100612956924
At time: 171.54253435134888 and batch: 400, loss is 3.578715581893921 and perplexity is 35.827493799830656
At time: 172.0182979106903 and batch: 450, loss is 3.6003740501403807 and perplexity is 36.61192657902226
At time: 172.49210572242737 and batch: 500, loss is 3.4878520154953003 and perplexity is 32.71559957897854
At time: 172.96552324295044 and batch: 550, loss is 3.5464465761184694 and perplexity is 34.68983054450986
At time: 173.4391827583313 and batch: 600, loss is 3.5699740982055665 and perplexity is 35.515673220047674
At time: 173.91385102272034 and batch: 650, loss is 3.425991678237915 and perplexity is 30.753126941527928
At time: 174.38743257522583 and batch: 700, loss is 3.4186256551742553 and perplexity is 30.527430960586493
At time: 174.86126565933228 and batch: 750, loss is 3.516959056854248 and perplexity is 33.68184798742725
At time: 175.33455300331116 and batch: 800, loss is 3.4709321594238283 and perplexity is 32.16671297856176
At time: 175.8085114955902 and batch: 850, loss is 3.542915964126587 and perplexity is 34.56757016677112
At time: 176.28165006637573 and batch: 900, loss is 3.513872299194336 and perplexity is 33.57804058172812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308018096505779 and perplexity of 74.29310117760825
finished 19 epochs...
Completing Train Step...
At time: 177.54534316062927 and batch: 50, loss is 3.7502839279174807 and perplexity is 42.53315663640615
At time: 178.0317726135254 and batch: 100, loss is 3.6210165786743165 and perplexity is 37.3755436967141
At time: 178.50571250915527 and batch: 150, loss is 3.6328174686431884 and perplexity is 37.819221120735385
At time: 178.97964239120483 and batch: 200, loss is 3.5274432706832886 and perplexity is 34.03683330324101
At time: 179.45303010940552 and batch: 250, loss is 3.6729272174835206 and perplexity is 39.36697305659092
At time: 179.92580461502075 and batch: 300, loss is 3.659999885559082 and perplexity is 38.86133842400497
At time: 180.3994333744049 and batch: 350, loss is 3.631863236427307 and perplexity is 37.78315001440537
At time: 180.87292098999023 and batch: 400, loss is 3.5777221584320067 and perplexity is 35.791919599956366
At time: 181.34793066978455 and batch: 450, loss is 3.599507508277893 and perplexity is 36.580214553858255
At time: 181.82436203956604 and batch: 500, loss is 3.4870917987823487 and perplexity is 32.690738084663465
At time: 182.29806065559387 and batch: 550, loss is 3.5458635902404785 and perplexity is 34.6696127571074
At time: 182.77208852767944 and batch: 600, loss is 3.569599361419678 and perplexity is 35.50236668419648
At time: 183.26519227027893 and batch: 650, loss is 3.425768299102783 and perplexity is 30.746258101835426
At time: 183.7393012046814 and batch: 700, loss is 3.418505907058716 and perplexity is 30.52377557712371
At time: 184.21391582489014 and batch: 750, loss is 3.5168756675720214 and perplexity is 33.67903939940421
At time: 184.68738746643066 and batch: 800, loss is 3.4711353063583372 and perplexity is 32.173248211480384
At time: 185.16057085990906 and batch: 850, loss is 3.5433546781539915 and perplexity is 34.58273877179319
At time: 185.63395166397095 and batch: 900, loss is 3.5145273351669313 and perplexity is 33.60004261147218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307883902771832 and perplexity of 74.28313217785833
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f06654d0b70>
ELAPSED
399.7095983028412


RESULTS SO FAR:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.28313217785833, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.443992016443709, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8989021853099624, 'seq_len': 35, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.716712474822998 and batch: 50, loss is 7.233355035781861 and perplexity is 1384.8609767450753
At time: 1.2079088687896729 and batch: 100, loss is 6.460816593170166 and perplexity is 639.5831225924302
At time: 1.6864323616027832 and batch: 150, loss is 6.3711714267730715 and perplexity is 584.7424107595602
At time: 2.1651601791381836 and batch: 200, loss is 6.226600599288941 and perplexity is 506.0323496147908
At time: 2.6451797485351562 and batch: 250, loss is 6.284139633178711 and perplexity is 536.0029330578642
At time: 3.124737501144409 and batch: 300, loss is 6.182449378967285 and perplexity is 484.17643672050747
At time: 3.6032989025115967 and batch: 350, loss is 6.199036750793457 and perplexity is 492.2746294953779
At time: 4.08174729347229 and batch: 400, loss is 6.086980895996094 and perplexity is 440.090723748761
At time: 4.560445308685303 and batch: 450, loss is 6.088014745712281 and perplexity is 440.5459466941146
At time: 5.038633584976196 and batch: 500, loss is 6.06933702468872 and perplexity is 432.3939200333392
At time: 5.516779899597168 and batch: 550, loss is 6.094333734512329 and perplexity is 443.338565568532
At time: 5.995622158050537 and batch: 600, loss is 6.041936588287354 and perplexity is 420.7069827065008
At time: 6.474462509155273 and batch: 650, loss is 5.969416809082031 and perplexity is 391.2774147632523
At time: 6.953959941864014 and batch: 700, loss is 6.077651433944702 and perplexity is 436.00400711812495
At time: 7.432534694671631 and batch: 750, loss is 6.027632532119751 and perplexity is 414.73200150519705
At time: 7.910999774932861 and batch: 800, loss is 6.041888475418091 and perplexity is 420.6867417733725
At time: 8.389801979064941 and batch: 850, loss is 6.075858154296875 and perplexity is 435.22283064906634
At time: 8.867695331573486 and batch: 900, loss is 5.952127361297608 and perplexity is 384.5705900483014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.882756899480951 and perplexity of 358.7970468356692
finished 1 epochs...
Completing Train Step...
At time: 10.133711099624634 and batch: 50, loss is 5.596414852142334 and perplexity is 269.4586246297209
At time: 10.605860948562622 and batch: 100, loss is 5.317669153213501 and perplexity is 203.90804924046074
At time: 11.091078758239746 and batch: 150, loss is 5.2229430294036865 and perplexity is 185.47925249492775
At time: 11.564032793045044 and batch: 200, loss is 5.054347581863404 and perplexity is 156.70226158929398
At time: 12.037198066711426 and batch: 250, loss is 5.101768207550049 and perplexity is 164.3121886379579
At time: 12.510197639465332 and batch: 300, loss is 5.011258811950683 and perplexity is 150.0935568423139
At time: 12.984802484512329 and batch: 350, loss is 4.983174724578857 and perplexity is 145.93695663420038
At time: 13.458194732666016 and batch: 400, loss is 4.837264814376831 and perplexity is 126.12390722028229
At time: 13.930795669555664 and batch: 450, loss is 4.83305811882019 and perplexity is 125.59445673946057
At time: 14.402957916259766 and batch: 500, loss is 4.744117164611817 and perplexity is 114.9063173477531
At time: 14.874677658081055 and batch: 550, loss is 4.805933027267456 and perplexity is 122.23348500767189
At time: 15.347205877304077 and batch: 600, loss is 4.742931108474732 and perplexity is 114.77011279396334
At time: 15.81947660446167 and batch: 650, loss is 4.605797681808472 and perplexity is 100.06276927370688
At time: 16.29181432723999 and batch: 700, loss is 4.656670293807983 and perplexity is 105.2849299810225
At time: 16.764402389526367 and batch: 750, loss is 4.687069339752197 and perplexity is 108.53463512162357
At time: 17.237705945968628 and batch: 800, loss is 4.629320383071899 and perplexity is 102.44441748634866
At time: 17.710773229599 and batch: 850, loss is 4.681055355072021 and perplexity is 107.88386830098912
At time: 18.182855129241943 and batch: 900, loss is 4.612618141174316 and perplexity is 100.74757601917321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.699765140063142 and perplexity of 109.92135329820476
finished 2 epochs...
Completing Train Step...
At time: 19.443726301193237 and batch: 50, loss is 4.658120393753052 and perplexity is 105.43771440177179
At time: 19.91604971885681 and batch: 100, loss is 4.52284255027771 and perplexity is 92.09701665453495
At time: 20.388679027557373 and batch: 150, loss is 4.513527965545654 and perplexity is 91.24315405064704
At time: 20.862208366394043 and batch: 200, loss is 4.414854803085327 and perplexity is 82.66983662942225
At time: 21.334616899490356 and batch: 250, loss is 4.5393167495727536 and perplexity is 93.62680770417529
At time: 21.807632207870483 and batch: 300, loss is 4.502648611068725 and perplexity is 90.25586769121863
At time: 22.28081703186035 and batch: 350, loss is 4.491059617996216 and perplexity is 89.21593061658425
At time: 22.754193544387817 and batch: 400, loss is 4.3931922388076785 and perplexity is 80.89825377157398
At time: 23.240299940109253 and batch: 450, loss is 4.421587677001953 and perplexity is 83.22832020616538
At time: 23.71297264099121 and batch: 500, loss is 4.317271060943604 and perplexity is 74.98372282211274
At time: 24.185943126678467 and batch: 550, loss is 4.398522434234619 and perplexity is 81.33060851788566
At time: 24.65855050086975 and batch: 600, loss is 4.377127389907837 and perplexity is 79.60901895667799
At time: 25.13071608543396 and batch: 650, loss is 4.238939981460572 and perplexity is 69.33431720994466
At time: 25.60289978981018 and batch: 700, loss is 4.271275396347046 and perplexity is 71.61291224658846
At time: 26.077019453048706 and batch: 750, loss is 4.351178379058838 and perplexity is 77.56981573703223
At time: 26.550068140029907 and batch: 800, loss is 4.303305706977844 and perplexity is 73.94382675054732
At time: 27.025341749191284 and batch: 850, loss is 4.3718659210205075 and perplexity is 79.19125856068938
At time: 27.500343084335327 and batch: 900, loss is 4.32044476032257 and perplexity is 75.22207664829536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5243162390303935 and perplexity of 92.23283904751975
finished 3 epochs...
Completing Train Step...
At time: 28.753305912017822 and batch: 50, loss is 4.400306568145752 and perplexity is 81.47584273467618
At time: 29.238906383514404 and batch: 100, loss is 4.272042775154114 and perplexity is 71.66788756850963
At time: 29.714073657989502 and batch: 150, loss is 4.264908390045166 and perplexity is 71.158400856903
At time: 30.190163135528564 and batch: 200, loss is 4.168305521011352 and perplexity is 64.60588598852543
At time: 30.66545343399048 and batch: 250, loss is 4.306727237701416 and perplexity is 74.19726114513291
At time: 31.140860080718994 and batch: 300, loss is 4.277182965278626 and perplexity is 72.03722254924419
At time: 31.616459608078003 and batch: 350, loss is 4.271465606689453 and perplexity is 71.6265350587065
At time: 32.091721296310425 and batch: 400, loss is 4.185000810623169 and perplexity is 65.69355416943317
At time: 32.56667876243591 and batch: 450, loss is 4.222624130249024 and perplexity is 68.21224745983032
At time: 33.04146933555603 and batch: 500, loss is 4.1128258752822875 and perplexity is 61.119188968897284
At time: 33.51715874671936 and batch: 550, loss is 4.198378963470459 and perplexity is 66.57831763277964
At time: 33.99187755584717 and batch: 600, loss is 4.1885791015625 and perplexity is 65.92904589677431
At time: 34.46765351295471 and batch: 650, loss is 4.050910749435425 and perplexity is 57.44975555885096
At time: 34.94336414337158 and batch: 700, loss is 4.069161767959595 and perplexity is 58.507898835937986
At time: 35.43304467201233 and batch: 750, loss is 4.165344095230102 and perplexity is 64.41484347087794
At time: 35.90937042236328 and batch: 800, loss is 4.124560904502869 and perplexity is 61.84064933724284
At time: 36.3848295211792 and batch: 850, loss is 4.197839670181274 and perplexity is 66.54242207286234
At time: 36.86074900627136 and batch: 900, loss is 4.153015127182007 and perplexity is 63.62555050568932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455416326653467 and perplexity of 86.09198577343031
finished 4 epochs...
Completing Train Step...
At time: 38.11405849456787 and batch: 50, loss is 4.243442344665527 and perplexity is 69.64718929194021
At time: 38.602696895599365 and batch: 100, loss is 4.115308094024658 and perplexity is 61.271088611351104
At time: 39.07759094238281 and batch: 150, loss is 4.112990546226501 and perplexity is 61.12925435216971
At time: 39.553221702575684 and batch: 200, loss is 4.01844132900238 and perplexity is 55.61435375514043
At time: 40.02750754356384 and batch: 250, loss is 4.159734444618225 and perplexity is 64.05451032124276
At time: 40.50203037261963 and batch: 300, loss is 4.136109762191772 and perplexity is 62.558978150472555
At time: 40.97601294517517 and batch: 350, loss is 4.1298735809326175 and perplexity is 62.170062956567634
At time: 41.45049858093262 and batch: 400, loss is 4.04952908039093 and perplexity is 57.37043382079534
At time: 41.92493653297424 and batch: 450, loss is 4.091474738121033 and perplexity is 59.828057394460956
At time: 42.39809489250183 and batch: 500, loss is 3.9784248542785643 and perplexity is 53.432803455289324
At time: 42.8723783493042 and batch: 550, loss is 4.063559217453003 and perplexity is 58.181021904948956
At time: 43.34820055961609 and batch: 600, loss is 4.058782720565796 and perplexity is 57.90378307722262
At time: 43.822531938552856 and batch: 650, loss is 3.923447003364563 and perplexity is 50.5744750499408
At time: 44.29804849624634 and batch: 700, loss is 3.9363337993621825 and perplexity is 51.2304355291405
At time: 44.77427077293396 and batch: 750, loss is 4.037629165649414 and perplexity is 56.69177654012126
At time: 45.24837779998779 and batch: 800, loss is 4.0015956401824955 and perplexity is 54.68533857750046
At time: 45.723875999450684 and batch: 850, loss is 4.076760988235474 and perplexity is 58.95420689591384
At time: 46.20005011558533 and batch: 900, loss is 4.036652350425721 and perplexity is 56.63642618767322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4265199426102315 and perplexity of 83.63983841184125
finished 5 epochs...
Completing Train Step...
At time: 47.47584080696106 and batch: 50, loss is 4.129575295448303 and perplexity is 62.151521294720546
At time: 47.95294117927551 and batch: 100, loss is 4.003773722648621 and perplexity is 54.804577563643164
At time: 48.4300332069397 and batch: 150, loss is 4.005374774932862 and perplexity is 54.89239283740204
At time: 48.90700602531433 and batch: 200, loss is 3.9129401206970216 and perplexity is 50.045876797383876
At time: 49.38451409339905 and batch: 250, loss is 4.054819903373718 and perplexity is 57.674775028368
At time: 49.86163115501404 and batch: 300, loss is 4.035173106193542 and perplexity is 56.55270901524171
At time: 50.338552474975586 and batch: 350, loss is 4.028789005279541 and perplexity is 56.19282081753599
At time: 50.81446433067322 and batch: 400, loss is 3.952428059577942 and perplexity is 52.06162221547386
At time: 51.290990591049194 and batch: 450, loss is 3.994267897605896 and perplexity is 54.286083100895084
At time: 51.76792931556702 and batch: 500, loss is 3.8794844007492064 and perplexity is 48.39925401783734
At time: 52.24433088302612 and batch: 550, loss is 3.961549301147461 and perplexity is 52.53866113446343
At time: 52.72108173370361 and batch: 600, loss is 3.9648586559295653 and perplexity is 52.71281821877918
At time: 53.19750118255615 and batch: 650, loss is 3.830275149345398 and perplexity is 46.075214055248956
At time: 53.67434048652649 and batch: 700, loss is 3.840658497810364 and perplexity is 46.55612145222182
At time: 54.15103530883789 and batch: 750, loss is 3.9421892881393434 and perplexity is 51.53129474869905
At time: 54.62648892402649 and batch: 800, loss is 3.911049723625183 and perplexity is 49.95135958410736
At time: 55.103845834732056 and batch: 850, loss is 3.985791425704956 and perplexity is 53.82787338730859
At time: 55.5809428691864 and batch: 900, loss is 3.94718505859375 and perplexity is 51.789377392542676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.413605154377141 and perplexity of 82.56659289750606
finished 6 epochs...
Completing Train Step...
At time: 56.85443735122681 and batch: 50, loss is 4.040066604614258 and perplexity is 56.830127828159846
At time: 57.329325914382935 and batch: 100, loss is 3.9175796794891355 and perplexity is 50.278607050464615
At time: 57.804948806762695 and batch: 150, loss is 3.9201421785354613 and perplexity is 50.40761114893062
At time: 58.27908372879028 and batch: 200, loss is 3.8291107606887818 and perplexity is 46.02159582094173
At time: 58.752453565597534 and batch: 250, loss is 3.970737810134888 and perplexity is 53.02363778849951
At time: 59.22829556465149 and batch: 300, loss is 3.954793152809143 and perplexity is 52.184898528303684
At time: 59.73052096366882 and batch: 350, loss is 3.9501488780975342 and perplexity is 51.94309944903471
At time: 60.205281257629395 and batch: 400, loss is 3.8756984758377073 and perplexity is 48.21636449779793
At time: 60.6805202960968 and batch: 450, loss is 3.9175379419326783 and perplexity is 50.27650858805691
At time: 61.15548086166382 and batch: 500, loss is 3.8040448379516603 and perplexity is 44.88235970599868
At time: 61.63076043128967 and batch: 550, loss is 3.8840484189987183 and perplexity is 48.62065394878206
At time: 62.10595226287842 and batch: 600, loss is 3.8891652488708495 and perplexity is 48.87007514202174
At time: 62.591426610946655 and batch: 650, loss is 3.7557104206085206 and perplexity is 42.76458986747472
At time: 63.07253074645996 and batch: 700, loss is 3.764796423912048 and perplexity is 43.15491965584043
At time: 63.543649196624756 and batch: 750, loss is 3.868698854446411 and perplexity is 47.880046623490564
At time: 64.01506400108337 and batch: 800, loss is 3.838241333961487 and perplexity is 46.44372357527141
At time: 64.48664116859436 and batch: 850, loss is 3.9121351528167723 and perplexity is 50.00560768386899
At time: 64.96037077903748 and batch: 900, loss is 3.8752802133560182 and perplexity is 48.19620161850697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4084451753799225 and perplexity of 82.14164830772116
finished 7 epochs...
Completing Train Step...
At time: 66.20331835746765 and batch: 50, loss is 3.966404685974121 and perplexity is 52.79437684928
At time: 66.68817210197449 and batch: 100, loss is 3.8480744552612305 and perplexity is 46.90266304864493
At time: 67.16026020050049 and batch: 150, loss is 3.8519144296646117 and perplexity is 47.08311431659162
At time: 67.63252997398376 and batch: 200, loss is 3.763278775215149 and perplexity is 43.08947532157553
At time: 68.10518741607666 and batch: 250, loss is 3.902758002281189 and perplexity is 49.538889237470265
At time: 68.59611082077026 and batch: 300, loss is 3.8900840616226198 and perplexity is 48.91499822503256
At time: 69.07332015037537 and batch: 350, loss is 3.881219229698181 and perplexity is 48.483291318910844
At time: 69.5466480255127 and batch: 400, loss is 3.8122224044799804 and perplexity is 45.250892987426774
At time: 70.01952695846558 and batch: 450, loss is 3.854246563911438 and perplexity is 47.19304659853091
At time: 70.49206900596619 and batch: 500, loss is 3.742946496009827 and perplexity is 42.222214648637745
At time: 70.96448588371277 and batch: 550, loss is 3.8210039567947387 and perplexity is 45.65001596626199
At time: 71.43724012374878 and batch: 600, loss is 3.8261284828186035 and perplexity is 45.884551088483825
At time: 71.92261385917664 and batch: 650, loss is 3.6927770376205444 and perplexity is 40.156207558518425
At time: 72.3950719833374 and batch: 700, loss is 3.7021981143951415 and perplexity is 40.53630994842102
At time: 72.87032198905945 and batch: 750, loss is 3.8063629388809206 and perplexity is 44.98652222871701
At time: 73.33954524993896 and batch: 800, loss is 3.777811245918274 and perplexity is 43.72024407238769
At time: 73.80944800376892 and batch: 850, loss is 3.8513264560699465 and perplexity is 47.055438825645126
At time: 74.27913188934326 and batch: 900, loss is 3.8155677413940428 and perplexity is 45.40252596040897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.409923814747431 and perplexity of 82.26319602309377
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.5211763381958 and batch: 50, loss is 3.9205172491073608 and perplexity is 50.426521106534956
At time: 76.003901720047 and batch: 100, loss is 3.788469605445862 and perplexity is 44.18872232221028
At time: 76.47288966178894 and batch: 150, loss is 3.7918634605407715 and perplexity is 44.33894721917303
At time: 76.94265842437744 and batch: 200, loss is 3.679539928436279 and perplexity is 39.62815808930606
At time: 77.41220164299011 and batch: 250, loss is 3.8190751552581785 and perplexity is 45.562051006068614
At time: 77.88422107696533 and batch: 300, loss is 3.789251198768616 and perplexity is 44.223273433218274
At time: 78.35736036300659 and batch: 350, loss is 3.7750190687179566 and perplexity is 43.598339672239604
At time: 78.8293468952179 and batch: 400, loss is 3.7024859809875488 and perplexity is 40.54798067756056
At time: 79.30224823951721 and batch: 450, loss is 3.727313551902771 and perplexity is 41.56728966775065
At time: 79.77687549591064 and batch: 500, loss is 3.6121155405044556 and perplexity is 37.04433877620575
At time: 80.24942088127136 and batch: 550, loss is 3.6751873302459717 and perplexity is 39.45604747601444
At time: 80.72095894813538 and batch: 600, loss is 3.672883462905884 and perplexity is 39.36525060899476
At time: 81.19384980201721 and batch: 650, loss is 3.5295667171478273 and perplexity is 34.10918548739121
At time: 81.66604542732239 and batch: 700, loss is 3.520504899024963 and perplexity is 33.80149049577908
At time: 82.13793396949768 and batch: 750, loss is 3.621843476295471 and perplexity is 37.406462226353845
At time: 82.61166834831238 and batch: 800, loss is 3.5812627410888673 and perplexity is 35.91886845358615
At time: 83.0838975906372 and batch: 850, loss is 3.6372326898574827 and perplexity is 37.986570518580024
At time: 83.57491827011108 and batch: 900, loss is 3.5920306968688966 and perplexity is 36.30773111137323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325593713211687 and perplexity of 75.61039042394243
finished 9 epochs...
Completing Train Step...
At time: 84.82948994636536 and batch: 50, loss is 3.8383536767959594 and perplexity is 46.44894148791337
At time: 85.30210757255554 and batch: 100, loss is 3.7145372152328493 and perplexity is 41.039590191804656
At time: 85.77479195594788 and batch: 150, loss is 3.720924162864685 and perplexity is 41.302546756194296
At time: 86.24724698066711 and batch: 200, loss is 3.615354838371277 and perplexity is 37.16453098787018
At time: 86.71929240226746 and batch: 250, loss is 3.7553655195236204 and perplexity is 42.749842857310206
At time: 87.18881869316101 and batch: 300, loss is 3.7340847778320314 and perplexity is 41.849706251551424
At time: 87.6580822467804 and batch: 350, loss is 3.7224247550964353 and perplexity is 41.36457156234045
At time: 88.12790441513062 and batch: 400, loss is 3.6543574047088625 and perplexity is 38.64268152992783
At time: 88.59775853157043 and batch: 450, loss is 3.683388338088989 and perplexity is 39.78095730381917
At time: 89.06750559806824 and batch: 500, loss is 3.5696145629882814 and perplexity is 35.50290637996132
At time: 89.53546071052551 and batch: 550, loss is 3.63636372089386 and perplexity is 37.95357570557536
At time: 90.00532793998718 and batch: 600, loss is 3.6395853853225706 and perplexity is 38.076046564444866
At time: 90.4746642112732 and batch: 650, loss is 3.5006249713897706 and perplexity is 33.13615463734289
At time: 90.9451379776001 and batch: 700, loss is 3.4966267061233522 and perplexity is 33.00393200791099
At time: 91.41594576835632 and batch: 750, loss is 3.602577419281006 and perplexity is 36.69268510597594
At time: 91.88589191436768 and batch: 800, loss is 3.5656239223480224 and perplexity is 35.361509358911526
At time: 92.35624504089355 and batch: 850, loss is 3.629576115608215 and perplexity is 37.69683413045012
At time: 92.82734036445618 and batch: 900, loss is 3.5904464769363402 and perplexity is 36.250257217705105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32294704489512 and perplexity of 75.41053938557036
finished 10 epochs...
Completing Train Step...
At time: 94.0819764137268 and batch: 50, loss is 3.802531542778015 and perplexity is 44.81449081345955
At time: 94.55345225334167 and batch: 100, loss is 3.681014986038208 and perplexity is 39.686655037723924
At time: 95.02430725097656 and batch: 150, loss is 3.686672248840332 and perplexity is 39.911809152511346
At time: 95.49539470672607 and batch: 200, loss is 3.5833278942108153 and perplexity is 35.99312306408982
At time: 95.9791750907898 and batch: 250, loss is 3.723203730583191 and perplexity is 41.39680610293615
At time: 96.44954824447632 and batch: 300, loss is 3.7048232984542846 and perplexity is 40.64286502526418
At time: 96.9190719127655 and batch: 350, loss is 3.6937023305892946 and perplexity is 40.19338101053866
At time: 97.3890450000763 and batch: 400, loss is 3.6273025178909304 and perplexity is 37.611224052739196
At time: 97.85789489746094 and batch: 450, loss is 3.6575771284103396 and perplexity is 38.76730079968991
At time: 98.32760977745056 and batch: 500, loss is 3.5446750211715696 and perplexity is 34.62843000687304
At time: 98.79675507545471 and batch: 550, loss is 3.611913323402405 and perplexity is 37.03684853472402
At time: 99.26649785041809 and batch: 600, loss is 3.618080720901489 and perplexity is 37.2659753335037
At time: 99.73752570152283 and batch: 650, loss is 3.4811239099502562 and perplexity is 32.49622438930166
At time: 100.20715928077698 and batch: 700, loss is 3.479498152732849 and perplexity is 32.44343633986539
At time: 100.67721128463745 and batch: 750, loss is 3.5875212430953978 and perplexity is 36.144371684027334
At time: 101.1471438407898 and batch: 800, loss is 3.551557264328003 and perplexity is 34.86757325938939
At time: 101.61804151535034 and batch: 850, loss is 3.6192234134674073 and perplexity is 37.30858322570355
At time: 102.09049606323242 and batch: 900, loss is 3.5820418405532837 and perplexity is 35.946863728896695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322909420483733 and perplexity of 75.40770216178842
finished 11 epochs...
Completing Train Step...
At time: 103.34103274345398 and batch: 50, loss is 3.7751793098449706 and perplexity is 43.605326479096576
At time: 103.82514810562134 and batch: 100, loss is 3.65545352935791 and perplexity is 38.6850619485255
At time: 104.2979187965393 and batch: 150, loss is 3.660468702316284 and perplexity is 38.87956154198321
At time: 104.77305579185486 and batch: 200, loss is 3.5588455390930176 and perplexity is 35.122626032026105
At time: 105.24708533287048 and batch: 250, loss is 3.6987439584732056 and perplexity is 40.39653275843921
At time: 105.72110557556152 and batch: 300, loss is 3.6821763467788697 and perplexity is 39.73277233503877
At time: 106.19590258598328 and batch: 350, loss is 3.6712601709365846 and perplexity is 39.30140115098862
At time: 106.67027020454407 and batch: 400, loss is 3.6059471654891966 and perplexity is 36.816538702875995
At time: 107.14514017105103 and batch: 450, loss is 3.636957540512085 and perplexity is 37.9761199763638
At time: 107.61979866027832 and batch: 500, loss is 3.524791746139526 and perplexity is 33.94670334804518
At time: 108.10642409324646 and batch: 550, loss is 3.5916223430633547 and perplexity is 36.292907737999755
At time: 108.58077549934387 and batch: 600, loss is 3.6001211404800415 and perplexity is 36.60266823991785
At time: 109.05537867546082 and batch: 650, loss is 3.4644911766052244 and perplexity is 31.960193540986545
At time: 109.52928376197815 and batch: 700, loss is 3.464252004623413 and perplexity is 31.952550472198652
At time: 110.00397753715515 and batch: 750, loss is 3.5734716129302977 and perplexity is 35.64010728824033
At time: 110.47844791412354 and batch: 800, loss is 3.537924847602844 and perplexity is 34.39546924128312
At time: 110.95251989364624 and batch: 850, loss is 3.6076665449142458 and perplexity is 36.87989475295962
At time: 111.42434883117676 and batch: 900, loss is 3.571524209976196 and perplexity is 35.570769174559445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324656447319136 and perplexity of 75.53955658413209
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 112.67227554321289 and batch: 50, loss is 3.77260648727417 and perplexity is 43.49328190813038
At time: 113.16091752052307 and batch: 100, loss is 3.652569308280945 and perplexity is 38.57364642822215
At time: 113.63574504852295 and batch: 150, loss is 3.6557718086242676 and perplexity is 38.69737656130048
At time: 114.11114192008972 and batch: 200, loss is 3.544669852256775 and perplexity is 34.62825101593145
At time: 114.58624410629272 and batch: 250, loss is 3.686172995567322 and perplexity is 39.89188802441818
At time: 115.06086850166321 and batch: 300, loss is 3.661781096458435 and perplexity is 38.930620348110786
At time: 115.53556704521179 and batch: 350, loss is 3.6505995702743532 and perplexity is 38.49774123201642
At time: 116.01078653335571 and batch: 400, loss is 3.5860044813156127 and perplexity is 36.08959083775185
At time: 116.48493218421936 and batch: 450, loss is 3.6089651775360108 and perplexity is 36.92781929882722
At time: 116.96007585525513 and batch: 500, loss is 3.494055051803589 and perplexity is 32.91916634430197
At time: 117.43563199043274 and batch: 550, loss is 3.5540468168258665 and perplexity is 34.95448605558665
At time: 117.91106271743774 and batch: 600, loss is 3.5623701715469362 and perplexity is 35.246638800935884
At time: 118.38579630851746 and batch: 650, loss is 3.4216990995407106 and perplexity is 30.621399651096414
At time: 118.86115527153015 and batch: 700, loss is 3.415812201499939 and perplexity is 30.441664154824466
At time: 119.33717703819275 and batch: 750, loss is 3.522500581741333 and perplexity is 33.86901490238357
At time: 119.82652974128723 and batch: 800, loss is 3.483699679374695 and perplexity is 32.580035062640796
At time: 120.3020236492157 and batch: 850, loss is 3.55167809009552 and perplexity is 34.87178641521367
At time: 120.77718162536621 and batch: 900, loss is 3.512630596160889 and perplexity is 33.53637250192486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305768992802868 and perplexity of 74.12619605247345
finished 13 epochs...
Completing Train Step...
At time: 122.0432276725769 and batch: 50, loss is 3.748302335739136 and perplexity is 42.44895671839697
At time: 122.51751399040222 and batch: 100, loss is 3.6297723054885864 and perplexity is 37.704230593360414
At time: 122.99239897727966 and batch: 150, loss is 3.6341011095047 and perplexity is 37.86779858965829
At time: 123.4679605960846 and batch: 200, loss is 3.5262395620346068 and perplexity is 33.995887520960196
At time: 123.94321870803833 and batch: 250, loss is 3.6688590383529665 and perplexity is 39.20714648036847
At time: 124.41793656349182 and batch: 300, loss is 3.6464016437530518 and perplexity is 38.33646928381826
At time: 124.89234638214111 and batch: 350, loss is 3.636247572898865 and perplexity is 37.94916772984799
At time: 125.3661458492279 and batch: 400, loss is 3.572789421081543 and perplexity is 35.61580218887249
At time: 125.83896160125732 and batch: 450, loss is 3.5974864625930785 and perplexity is 36.50635892702193
At time: 126.31125617027283 and batch: 500, loss is 3.4835966444015503 and perplexity is 32.5766783525353
At time: 126.7826759815216 and batch: 550, loss is 3.5445302152633666 and perplexity is 34.62341596865619
At time: 127.25616669654846 and batch: 600, loss is 3.5545753908157347 and perplexity is 34.97296697158031
At time: 127.72966074943542 and batch: 650, loss is 3.4159708261489867 and perplexity is 30.446493336120927
At time: 128.20227456092834 and batch: 700, loss is 3.4119630527496336 and perplexity is 30.324714883249786
At time: 128.67680501937866 and batch: 750, loss is 3.519432849884033 and perplexity is 33.76527305383837
At time: 129.15031003952026 and batch: 800, loss is 3.482911901473999 and perplexity is 32.55437933784788
At time: 129.62349557876587 and batch: 850, loss is 3.5525466442108153 and perplexity is 34.9020876060194
At time: 130.09767484664917 and batch: 900, loss is 3.5150649690628053 and perplexity is 33.61811199020292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304853047410103 and perplexity of 74.05833158953168
finished 14 epochs...
Completing Train Step...
At time: 131.37343859672546 and batch: 50, loss is 3.7381393671035767 and perplexity is 42.01973408511747
At time: 131.84700083732605 and batch: 100, loss is 3.619635810852051 and perplexity is 37.32397236085203
At time: 132.3360025882721 and batch: 150, loss is 3.623726239204407 and perplexity is 37.476956066758284
At time: 132.81189393997192 and batch: 200, loss is 3.5165246868133546 and perplexity is 33.667220778780184
At time: 133.2857928276062 and batch: 250, loss is 3.659287052154541 and perplexity is 38.833646634819935
At time: 133.75968647003174 and batch: 300, loss is 3.637495880126953 and perplexity is 37.99656953007461
At time: 134.23387169837952 and batch: 350, loss is 3.6278423738479613 and perplexity is 37.631534177873036
At time: 134.7086420059204 and batch: 400, loss is 3.5652279806137086 and perplexity is 35.3475110330116
At time: 135.18324184417725 and batch: 450, loss is 3.5903128576278687 and perplexity is 36.24541380699756
At time: 135.656480550766 and batch: 500, loss is 3.4768735933303834 and perplexity is 32.358398256535956
At time: 136.1305742263794 and batch: 550, loss is 3.538136396408081 and perplexity is 34.4027463314094
At time: 136.60321950912476 and batch: 600, loss is 3.5492172241210938 and perplexity is 34.78607712534821
At time: 137.07810950279236 and batch: 650, loss is 3.41162223815918 and perplexity is 30.3143815389435
At time: 137.55126905441284 and batch: 700, loss is 3.408559551239014 and perplexity is 30.221680109301282
At time: 138.02537536621094 and batch: 750, loss is 3.5162169408798216 and perplexity is 33.65686142259879
At time: 138.49924492835999 and batch: 800, loss is 3.48093864440918 and perplexity is 32.49020451636196
At time: 138.97330117225647 and batch: 850, loss is 3.5514210176467897 and perplexity is 34.86282299186228
At time: 139.44718527793884 and batch: 900, loss is 3.51454514503479 and perplexity is 33.60064102911999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304797446891053 and perplexity of 74.05421402232581
finished 15 epochs...
Completing Train Step...
At time: 140.70616936683655 and batch: 50, loss is 3.7297744798660277 and perplexity is 41.6697097457784
At time: 141.19190311431885 and batch: 100, loss is 3.611361575126648 and perplexity is 37.016419153861634
At time: 141.66424679756165 and batch: 150, loss is 3.6153577327728272 and perplexity is 37.164638557101966
At time: 142.1368591785431 and batch: 200, loss is 3.5086927318573 and perplexity is 33.40457049730088
At time: 142.60871291160583 and batch: 250, loss is 3.651616139411926 and perplexity is 38.536896746390916
At time: 143.0818009376526 and batch: 300, loss is 3.630316948890686 and perplexity is 37.72477154702401
At time: 143.55467462539673 and batch: 350, loss is 3.6209847211837767 and perplexity is 37.374353024650375
At time: 144.0279643535614 and batch: 400, loss is 3.5590961503982546 and perplexity is 35.13142926222777
At time: 144.5142228603363 and batch: 450, loss is 3.584304256439209 and perplexity is 36.028282551336105
At time: 144.9874243736267 and batch: 500, loss is 3.4711489295959472 and perplexity is 32.173686518271026
At time: 145.46054220199585 and batch: 550, loss is 3.5325270318984985 and perplexity is 34.21030901721204
At time: 145.93354105949402 and batch: 600, loss is 3.5444382762908937 and perplexity is 34.62023287369601
At time: 146.40765118598938 and batch: 650, loss is 3.407506537437439 and perplexity is 30.189873012634376
At time: 146.8807532787323 and batch: 700, loss is 3.405049538612366 and perplexity is 30.115787581292274
At time: 147.35450887680054 and batch: 750, loss is 3.512805209159851 and perplexity is 33.54222889978845
At time: 147.82894945144653 and batch: 800, loss is 3.4783917808532716 and perplexity is 32.40756168323714
At time: 148.30304980278015 and batch: 850, loss is 3.5495008707046507 and perplexity is 34.79594547677608
At time: 148.77645254135132 and batch: 900, loss is 3.512957844734192 and perplexity is 33.54734902790816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305087572907748 and perplexity of 74.07570219344764
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 150.02305722236633 and batch: 50, loss is 3.7310448598861696 and perplexity is 41.72267975137907
At time: 150.50885462760925 and batch: 100, loss is 3.6157823371887208 and perplexity is 37.18042217740871
At time: 150.98236727714539 and batch: 150, loss is 3.619727916717529 and perplexity is 37.327410275953056
At time: 151.45725560188293 and batch: 200, loss is 3.509227542877197 and perplexity is 33.42244040780436
At time: 151.9312207698822 and batch: 250, loss is 3.6525969171524046 and perplexity is 38.57471141776963
At time: 152.4058632850647 and batch: 300, loss is 3.630440716743469 and perplexity is 37.72944094995014
At time: 152.8797595500946 and batch: 350, loss is 3.617260022163391 and perplexity is 37.235403741323005
At time: 153.35437560081482 and batch: 400, loss is 3.556970839500427 and perplexity is 35.056843339972254
At time: 153.82772159576416 and batch: 450, loss is 3.580871596336365 and perplexity is 35.90482172400608
At time: 154.30861711502075 and batch: 500, loss is 3.466249942779541 and perplexity is 32.016453507872185
At time: 154.78347516059875 and batch: 550, loss is 3.522027359008789 and perplexity is 33.8529911063142
At time: 155.25572967529297 and batch: 600, loss is 3.53770797252655 and perplexity is 34.388010530100765
At time: 155.72993397712708 and batch: 650, loss is 3.395531644821167 and perplexity is 29.830508495299203
At time: 156.21437072753906 and batch: 700, loss is 3.391948046684265 and perplexity is 29.723799256345764
At time: 156.68640875816345 and batch: 750, loss is 3.4979797410964966 and perplexity is 33.048617706007946
At time: 157.1594820022583 and batch: 800, loss is 3.4629968786239624 and perplexity is 31.912471152911635
At time: 157.63280582427979 and batch: 850, loss is 3.531379990577698 and perplexity is 34.171090875886215
At time: 158.1058931350708 and batch: 900, loss is 3.495118236541748 and perplexity is 32.95418411143139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.301570526541096 and perplexity of 73.81563212152223
finished 17 epochs...
Completing Train Step...
At time: 159.37169814109802 and batch: 50, loss is 3.7230150508880615 and perplexity is 41.38899610299868
At time: 159.8446924686432 and batch: 100, loss is 3.607381892204285 and perplexity is 36.869398284970025
At time: 160.31748390197754 and batch: 150, loss is 3.611136584281921 and perplexity is 37.00809173527904
At time: 160.79262900352478 and batch: 200, loss is 3.5021530294418337 and perplexity is 33.18682731078433
At time: 161.26594424247742 and batch: 250, loss is 3.6461548089981077 and perplexity is 38.32700767859171
At time: 161.7394106388092 and batch: 300, loss is 3.625021562576294 and perplexity is 37.525532298030555
At time: 162.21399760246277 and batch: 350, loss is 3.6122555351257324 and perplexity is 37.04952514740674
At time: 162.68804478645325 and batch: 400, loss is 3.5525254297256468 and perplexity is 34.90134718405339
At time: 163.16173100471497 and batch: 450, loss is 3.576849913597107 and perplexity is 35.76071389444036
At time: 163.63507890701294 and batch: 500, loss is 3.462508587837219 and perplexity is 31.89689239105793
At time: 164.1083207130432 and batch: 550, loss is 3.5194761037826536 and perplexity is 33.76673356512211
At time: 164.582350730896 and batch: 600, loss is 3.5354804039001464 and perplexity is 34.31149413112428
At time: 165.05577039718628 and batch: 650, loss is 3.3940459442138673 and perplexity is 29.786222196947968
At time: 165.52942085266113 and batch: 700, loss is 3.3911985683441164 and perplexity is 29.7015302587243
At time: 166.00329327583313 and batch: 750, loss is 3.4977681112289427 and perplexity is 33.04162437144532
At time: 166.47554540634155 and batch: 800, loss is 3.4635086822509766 and perplexity is 31.92880825173128
At time: 166.94921851158142 and batch: 850, loss is 3.53284942150116 and perplexity is 34.221339843159285
At time: 167.42368841171265 and batch: 900, loss is 3.4974054956436156 and perplexity is 33.02964513554379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30133516494542 and perplexity of 73.79826080091132
finished 18 epochs...
Completing Train Step...
At time: 168.68637585639954 and batch: 50, loss is 3.7200141620635985 and perplexity is 41.26497850172261
At time: 169.16003894805908 and batch: 100, loss is 3.6042221641540526 and perplexity is 36.753084869166685
At time: 169.78233695030212 and batch: 150, loss is 3.6078607702255248 and perplexity is 36.88705845766182
At time: 170.25702714920044 and batch: 200, loss is 3.4991231775283813 and perplexity is 33.08642831240925
At time: 170.73166155815125 and batch: 250, loss is 3.643277521133423 and perplexity is 38.2168883429936
At time: 171.20327258110046 and batch: 300, loss is 3.622348680496216 and perplexity is 37.42536490265929
At time: 171.67577862739563 and batch: 350, loss is 3.6096025943756103 and perplexity is 36.951365216182865
At time: 172.14943528175354 and batch: 400, loss is 3.550055742263794 and perplexity is 34.81525811481593
At time: 172.6240894794464 and batch: 450, loss is 3.5746548414230346 and perplexity is 35.6823026371146
At time: 173.09800457954407 and batch: 500, loss is 3.460526638031006 and perplexity is 31.833736957487872
At time: 173.57169580459595 and batch: 550, loss is 3.5178027391433715 and perplexity is 33.710276756777496
At time: 174.04549956321716 and batch: 600, loss is 3.534151544570923 and perplexity is 34.26592926340344
At time: 174.51919984817505 and batch: 650, loss is 3.3930340671539305 and perplexity is 29.756097445847367
At time: 174.993745803833 and batch: 700, loss is 3.390560383796692 and perplexity is 29.682581248204304
At time: 175.46796536445618 and batch: 750, loss is 3.4973019695281984 and perplexity is 33.02622588168339
At time: 175.94178199768066 and batch: 800, loss is 3.463448209762573 and perplexity is 31.92687749562394
At time: 176.41506266593933 and batch: 850, loss is 3.5330922603607178 and perplexity is 34.229651123409404
At time: 176.88985228538513 and batch: 900, loss is 3.497950325012207 and perplexity is 33.04764555938229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.301323041523973 and perplexity of 73.79736611891688
finished 19 epochs...
Completing Train Step...
At time: 178.1708722114563 and batch: 50, loss is 3.717483639717102 and perplexity is 41.16068856116095
At time: 178.64524054527283 and batch: 100, loss is 3.601670365333557 and perplexity is 36.659417950941084
At time: 179.11947679519653 and batch: 150, loss is 3.605275897979736 and perplexity is 36.79183324954572
At time: 179.60778522491455 and batch: 200, loss is 3.4966830587387085 and perplexity is 33.00579191820159
At time: 180.090665102005 and batch: 250, loss is 3.6409521675109864 and perplexity is 38.1281238077116
At time: 180.5780782699585 and batch: 300, loss is 3.620202159881592 and perplexity is 37.3451167433623
At time: 181.05125308036804 and batch: 350, loss is 3.607471647262573 and perplexity is 36.87270764847598
At time: 181.52609419822693 and batch: 400, loss is 3.5481234550476075 and perplexity is 34.74804999024385
At time: 182.0009641647339 and batch: 450, loss is 3.5728941488265993 and perplexity is 35.619532346846206
At time: 182.47597002983093 and batch: 500, loss is 3.4588975715637207 and perplexity is 31.781919902255193
At time: 182.950843334198 and batch: 550, loss is 3.5163099193572998 and perplexity is 33.6599909318168
At time: 183.42567682266235 and batch: 600, loss is 3.5329695653915407 and perplexity is 34.22545157505687
At time: 183.90080571174622 and batch: 650, loss is 3.3920373582839964 and perplexity is 29.726454054957834
At time: 184.37605690956116 and batch: 700, loss is 3.389813599586487 and perplexity is 29.660423039939822
At time: 184.85234379768372 and batch: 750, loss is 3.496656575202942 and perplexity is 33.00491781970548
At time: 185.32802152633667 and batch: 800, loss is 3.4630984115600585 and perplexity is 31.915711484303266
At time: 185.80323672294617 and batch: 850, loss is 3.5329179763793945 and perplexity is 34.22368596336333
At time: 186.27615237236023 and batch: 900, loss is 3.497973051071167 and perplexity is 33.048396610657946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3013807322880995 and perplexity of 73.80162366816828
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f06654d0b70>
ELAPSED
592.5755846500397


RESULTS SO FAR:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.28313217785833, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -73.79736611891688, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.443992016443709, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8989021853099624, 'seq_len': 35, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.7650501504885647, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.39050037147704286, 'seq_len': 35, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.7129151821136475 and batch: 50, loss is 6.64445309638977 and perplexity is 768.5096319868062
At time: 1.191598892211914 and batch: 100, loss is 5.859150190353393 and perplexity is 350.4262218655171
At time: 1.67094087600708 and batch: 150, loss is 5.661460762023926 and perplexity is 287.568404900126
At time: 2.1508071422576904 and batch: 200, loss is 5.471589384078979 and perplexity is 237.83790829895017
At time: 2.6307928562164307 and batch: 250, loss is 5.494940261840821 and perplexity is 243.45698205430782
At time: 3.110936164855957 and batch: 300, loss is 5.396775331497192 and perplexity is 220.69360381704126
At time: 3.590982437133789 and batch: 350, loss is 5.367318630218506 and perplexity is 214.28751261829976
At time: 4.071218490600586 and batch: 400, loss is 5.214370880126953 and perplexity is 183.89609189374374
At time: 4.550512075424194 and batch: 450, loss is 5.209236421585083 and perplexity is 182.95430488607676
At time: 5.0309295654296875 and batch: 500, loss is 5.139594049453735 and perplexity is 170.64648022663826
At time: 5.524463415145874 and batch: 550, loss is 5.193861293792724 and perplexity is 180.16287336617978
At time: 6.00356650352478 and batch: 600, loss is 5.116367492675781 and perplexity is 166.7286253340592
At time: 6.483805418014526 and batch: 650, loss is 5.005704021453857 and perplexity is 149.26212991817357
At time: 6.964090347290039 and batch: 700, loss is 5.076881685256958 and perplexity is 160.2734926822003
At time: 7.445409059524536 and batch: 750, loss is 5.084091758728027 and perplexity is 161.43325229171415
At time: 7.925570964813232 and batch: 800, loss is 5.049149503707886 and perplexity is 155.88982437182102
At time: 8.403781414031982 and batch: 850, loss is 5.076628475189209 and perplexity is 160.2329149578209
At time: 8.884159326553345 and batch: 900, loss is 5.002267179489135 and perplexity is 148.75002009068155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.908305128959761 and perplexity of 135.4097178011185
finished 1 epochs...
Completing Train Step...
At time: 10.138025283813477 and batch: 50, loss is 4.857299327850342 and perplexity is 128.67622014222016
At time: 10.623047113418579 and batch: 100, loss is 4.721463623046875 and perplexity is 112.33254490916819
At time: 11.096336126327515 and batch: 150, loss is 4.702540168762207 and perplexity is 110.22681184033524
At time: 11.569311380386353 and batch: 200, loss is 4.599746112823486 and perplexity is 99.45906105597149
At time: 12.0428147315979 and batch: 250, loss is 4.705539836883545 and perplexity is 110.5579521010711
At time: 12.516478776931763 and batch: 300, loss is 4.653173360824585 and perplexity is 104.91739862750428
At time: 12.989943981170654 and batch: 350, loss is 4.639714965820312 and perplexity is 103.51483811091026
At time: 13.462594032287598 and batch: 400, loss is 4.529171056747437 and perplexity is 92.6817013593607
At time: 13.93608045578003 and batch: 450, loss is 4.55455228805542 and perplexity is 95.06418433571002
At time: 14.409854650497437 and batch: 500, loss is 4.4519573593139645 and perplexity is 85.79471083499284
At time: 14.882806062698364 and batch: 550, loss is 4.526221723556518 and perplexity is 92.40875484431241
At time: 15.35533356666565 and batch: 600, loss is 4.495300436019898 and perplexity is 89.59508253204186
At time: 15.828867197036743 and batch: 650, loss is 4.3584347915649415 and perplexity is 78.13474150638184
At time: 16.302169799804688 and batch: 700, loss is 4.396499814987183 and perplexity is 81.16627391288353
At time: 16.775171279907227 and batch: 750, loss is 4.458951578140259 and perplexity is 86.39688121762426
At time: 17.261449813842773 and batch: 800, loss is 4.412707767486572 and perplexity is 82.49253195507048
At time: 17.73589563369751 and batch: 850, loss is 4.478392534255981 and perplexity is 88.09295239765318
At time: 18.211065769195557 and batch: 900, loss is 4.418287229537964 and perplexity is 82.95408231063065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.568236886638484 and perplexity of 96.37404153177833
finished 2 epochs...
Completing Train Step...
At time: 19.47585964202881 and batch: 50, loss is 4.459187688827515 and perplexity is 86.41728285305261
At time: 19.949453353881836 and batch: 100, loss is 4.338620872497558 and perplexity is 76.60182277621465
At time: 20.42343521118164 and batch: 150, loss is 4.327205381393433 and perplexity is 75.73234753519759
At time: 20.89684295654297 and batch: 200, loss is 4.237750177383423 and perplexity is 69.25187201318357
At time: 21.371184825897217 and batch: 250, loss is 4.374821062088013 and perplexity is 79.42562602495063
At time: 21.844703435897827 and batch: 300, loss is 4.342687568664551 and perplexity is 76.9139733959518
At time: 22.317638874053955 and batch: 350, loss is 4.336272201538086 and perplexity is 76.4221214119119
At time: 22.78963875770569 and batch: 400, loss is 4.2503670835494995 and perplexity is 70.13115161423302
At time: 23.261772871017456 and batch: 450, loss is 4.28597930431366 and perplexity is 72.67368153256918
At time: 23.735000133514404 and batch: 500, loss is 4.176563944816589 and perplexity is 65.14163796384139
At time: 24.20767331123352 and batch: 550, loss is 4.253545942306519 and perplexity is 70.3544433581066
At time: 24.681077241897583 and batch: 600, loss is 4.246687340736389 and perplexity is 69.87356123693381
At time: 25.15475368499756 and batch: 650, loss is 4.110393562316895 and perplexity is 60.97070862167338
At time: 25.628671646118164 and batch: 700, loss is 4.127720670700073 and perplexity is 62.03636036834854
At time: 26.101344347000122 and batch: 750, loss is 4.217072567939758 and perplexity is 67.83461212074761
At time: 26.57428002357483 and batch: 800, loss is 4.183529148101806 and perplexity is 65.59694653222239
At time: 27.046897411346436 and batch: 850, loss is 4.256587891578675 and perplexity is 70.56878384703735
At time: 27.520158052444458 and batch: 900, loss is 4.20245964050293 and perplexity is 66.85055732776347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.466911420430223 and perplexity of 87.08733105685707
finished 3 epochs...
Completing Train Step...
At time: 28.782902002334595 and batch: 50, loss is 4.270268640518188 and perplexity is 71.54085180948175
At time: 29.25545883178711 and batch: 100, loss is 4.15028778553009 and perplexity is 63.45225831259505
At time: 29.741446256637573 and batch: 150, loss is 4.145591611862183 and perplexity is 63.154974083482315
At time: 30.214982271194458 and batch: 200, loss is 4.0583087873458865 and perplexity is 57.87634705279911
At time: 30.68831515312195 and batch: 250, loss is 4.199820938110352 and perplexity is 66.67439112949872
At time: 31.16194462776184 and batch: 300, loss is 4.175269718170166 and perplexity is 65.05738445352654
At time: 31.63630723953247 and batch: 350, loss is 4.168480277061462 and perplexity is 64.61717724455356
At time: 32.12259340286255 and batch: 400, loss is 4.091117205619812 and perplexity is 59.806670742896905
At time: 32.606502056121826 and batch: 450, loss is 4.131731042861938 and perplexity is 62.285648796569106
At time: 33.081615924835205 and batch: 500, loss is 4.02250292301178 and perplexity is 55.84069602523772
At time: 33.5556435585022 and batch: 550, loss is 4.096352605819702 and perplexity is 60.12060366403858
At time: 34.02890610694885 and batch: 600, loss is 4.099129486083984 and perplexity is 60.28778339344777
At time: 34.50204086303711 and batch: 650, loss is 3.9630491590499877 and perplexity is 52.6175207849057
At time: 34.97604560852051 and batch: 700, loss is 3.9672372436523435 and perplexity is 52.83834951544329
At time: 35.44908261299133 and batch: 750, loss is 4.070023322105408 and perplexity is 58.5583282794716
At time: 35.922370195388794 and batch: 800, loss is 4.041069045066833 and perplexity is 56.887125210687806
At time: 36.39618229866028 and batch: 850, loss is 4.117649264335633 and perplexity is 61.4147027118666
At time: 36.868834018707275 and batch: 900, loss is 4.068866305351257 and perplexity is 58.490614493103806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.437807684075342 and perplexity of 84.58929179513392
finished 4 epochs...
Completing Train Step...
At time: 38.121437788009644 and batch: 50, loss is 4.145583367347717 and perplexity is 63.15445340353128
At time: 38.60646080970764 and batch: 100, loss is 4.024781436920166 and perplexity is 55.96807488993397
At time: 39.078744649887085 and batch: 150, loss is 4.0272277021408085 and perplexity is 56.105155244095116
At time: 39.551467180252075 and batch: 200, loss is 3.9368377447128298 and perplexity is 51.25625937529413
At time: 40.043639183044434 and batch: 250, loss is 4.076738057136535 and perplexity is 58.95285502666264
At time: 40.54138493537903 and batch: 300, loss is 4.060335698127747 and perplexity is 57.99377621366776
At time: 41.02611684799194 and batch: 350, loss is 4.054337663650513 and perplexity is 57.6469686660275
At time: 41.507280349731445 and batch: 400, loss is 3.9808614826202393 and perplexity is 53.56315808696397
At time: 42.017558574676514 and batch: 450, loss is 4.024622960090637 and perplexity is 55.95920594964993
At time: 42.4990656375885 and batch: 500, loss is 3.9155585098266603 and perplexity is 50.177088083310814
At time: 42.97266364097595 and batch: 550, loss is 3.9839877319335937 and perplexity is 53.73087189414674
At time: 43.447001457214355 and batch: 600, loss is 3.994702863693237 and perplexity is 54.30970084214444
At time: 43.920438289642334 and batch: 650, loss is 3.858483285903931 and perplexity is 47.39341456892716
At time: 44.39393901824951 and batch: 700, loss is 3.8587118577957153 and perplexity is 47.40424860948471
At time: 44.86748909950256 and batch: 750, loss is 3.966082701683044 and perplexity is 52.777380625682696
At time: 45.340173959732056 and batch: 800, loss is 3.9388326263427733 and perplexity is 51.35861160188683
At time: 45.81356120109558 and batch: 850, loss is 4.01794536113739 and perplexity is 55.58677766183904
At time: 46.287495851516724 and batch: 900, loss is 3.9698901891708376 and perplexity is 52.978712883854996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425977314988228 and perplexity of 83.59446543664721
finished 5 epochs...
Completing Train Step...
At time: 47.55251741409302 and batch: 50, loss is 4.052469081878662 and perplexity is 57.53935116853589
At time: 48.03834319114685 and batch: 100, loss is 3.9309186601638793 and perplexity is 50.95376536935728
At time: 48.51151776313782 and batch: 150, loss is 3.9381885862350465 and perplexity is 51.325545245310934
At time: 48.98522639274597 and batch: 200, loss is 3.8470930528640745 and perplexity is 46.85665524247433
At time: 49.45844912528992 and batch: 250, loss is 3.985552439689636 and perplexity is 53.81501081538323
At time: 49.932215452194214 and batch: 300, loss is 3.9738697147369386 and perplexity is 53.18996308523328
At time: 50.40631055831909 and batch: 350, loss is 3.9676152992248537 and perplexity is 52.858329124383125
At time: 50.880664348602295 and batch: 400, loss is 3.895939202308655 and perplexity is 49.20224252857682
At time: 51.35435676574707 and batch: 450, loss is 3.939540090560913 and perplexity is 51.39495883755683
At time: 51.82942199707031 and batch: 500, loss is 3.832467770576477 and perplexity is 46.176350384125485
At time: 52.3026077747345 and batch: 550, loss is 3.902492394447327 and perplexity is 49.52573306767619
At time: 52.79821991920471 and batch: 600, loss is 3.9139202308654784 and perplexity is 50.09495131541091
At time: 53.30041241645813 and batch: 650, loss is 3.781646227836609 and perplexity is 43.88823232794701
At time: 53.811359882354736 and batch: 700, loss is 3.776067271232605 and perplexity is 43.6440635212588
At time: 54.296045541763306 and batch: 750, loss is 3.8836530017852784 and perplexity is 48.60143230581671
At time: 54.76975703239441 and batch: 800, loss is 3.8591772317886353 and perplexity is 47.42631444797692
At time: 55.243248462677 and batch: 850, loss is 3.939100284576416 and perplexity is 51.372359997004025
At time: 55.717472314834595 and batch: 900, loss is 3.8918898153305053 and perplexity is 49.00340646217657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424377859455266 and perplexity of 83.46086667749849
finished 6 epochs...
Completing Train Step...
At time: 56.98307919502258 and batch: 50, loss is 3.975380530357361 and perplexity is 53.27038404763809
At time: 57.45748424530029 and batch: 100, loss is 3.8579969215393066 and perplexity is 47.37036970551742
At time: 57.93239784240723 and batch: 150, loss is 3.867457990646362 and perplexity is 47.82067085313145
At time: 58.40580773353577 and batch: 200, loss is 3.774734392166138 and perplexity is 43.5859300136899
At time: 58.88060688972473 and batch: 250, loss is 3.91277202129364 and perplexity is 50.03746482239633
At time: 59.354753732681274 and batch: 300, loss is 3.9042793321609497 and perplexity is 49.61431158646722
At time: 59.83016347885132 and batch: 350, loss is 3.898688554763794 and perplexity is 49.33770296378307
At time: 60.305986642837524 and batch: 400, loss is 3.826417260169983 and perplexity is 45.89780342101104
At time: 60.78184771537781 and batch: 450, loss is 3.8727762413024904 and perplexity is 48.075670642641796
At time: 61.25758099555969 and batch: 500, loss is 3.7658771657943726 and perplexity is 43.2015841965606
At time: 61.73297953605652 and batch: 550, loss is 3.833193316459656 and perplexity is 46.209865601990835
At time: 62.207223892211914 and batch: 600, loss is 3.845612635612488 and perplexity is 46.787339162721494
At time: 62.68037509918213 and batch: 650, loss is 3.717556552886963 and perplexity is 41.16368982685217
At time: 63.15467882156372 and batch: 700, loss is 3.711031017303467 and perplexity is 40.895949229626765
At time: 63.62878894805908 and batch: 750, loss is 3.8203923320770263 and perplexity is 45.62210382488099
At time: 64.10249090194702 and batch: 800, loss is 3.798719916343689 and perplexity is 44.64399984598805
At time: 64.57634687423706 and batch: 850, loss is 3.8729808044433596 and perplexity is 48.085506158785584
At time: 65.04947209358215 and batch: 900, loss is 3.8274455785751345 and perplexity is 45.94502525240288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43157958984375 and perplexity of 84.06409888795609
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 66.30100083351135 and batch: 50, loss is 3.9293382358551026 and perplexity is 50.87330040108816
At time: 66.77181315422058 and batch: 100, loss is 3.7973480367660524 and perplexity is 44.58279564633024
At time: 67.24177360534668 and batch: 150, loss is 3.8045480060577392 and perplexity is 44.904948760497604
At time: 67.71143960952759 and batch: 200, loss is 3.695369758605957 and perplexity is 40.260456486342896
At time: 68.18334531784058 and batch: 250, loss is 3.824586944580078 and perplexity is 45.81387278907025
At time: 68.65539169311523 and batch: 300, loss is 3.808102021217346 and perplexity is 45.06482556313997
At time: 69.12690949440002 and batch: 350, loss is 3.788043580055237 and perplexity is 44.1699008140266
At time: 69.59779477119446 and batch: 400, loss is 3.709691343307495 and perplexity is 40.84119867204091
At time: 70.06990218162537 and batch: 450, loss is 3.7395053815841677 and perplexity is 42.07717287252429
At time: 70.54160499572754 and batch: 500, loss is 3.631607565879822 and perplexity is 37.773491210543924
At time: 71.01374769210815 and batch: 550, loss is 3.6826918601989744 and perplexity is 39.7532603928761
At time: 71.48790407180786 and batch: 600, loss is 3.688127374649048 and perplexity is 39.96992813103947
At time: 71.96037602424622 and batch: 650, loss is 3.5513185977935793 and perplexity is 34.85925252949519
At time: 72.43188667297363 and batch: 700, loss is 3.5323563241958618 and perplexity is 34.20446955238822
At time: 72.9039397239685 and batch: 750, loss is 3.6269761896133423 and perplexity is 37.59895244917062
At time: 73.37559628486633 and batch: 800, loss is 3.592015714645386 and perplexity is 36.30718714490548
At time: 73.84726548194885 and batch: 850, loss is 3.653187103271484 and perplexity is 38.59748439648284
At time: 74.31939744949341 and batch: 900, loss is 3.594823842048645 and perplexity is 36.409285637758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346912122752569 and perplexity of 77.2395879405489
finished 8 epochs...
Completing Train Step...
At time: 75.55806994438171 and batch: 50, loss is 3.8374028396606445 and perplexity is 46.40479709983823
At time: 76.04293775558472 and batch: 100, loss is 3.716263384819031 and perplexity is 41.110492661461016
At time: 76.5156683921814 and batch: 150, loss is 3.7269686603546144 and perplexity is 41.55295593279857
At time: 76.98924040794373 and batch: 200, loss is 3.6230757236480713 and perplexity is 37.452584651684504
At time: 77.4625084400177 and batch: 250, loss is 3.7548318767547606 and perplexity is 42.727035798751686
At time: 77.94821190834045 and batch: 300, loss is 3.7453397417068484 and perplexity is 42.323383795188406
At time: 78.4195728302002 and batch: 350, loss is 3.7276831674575805 and perplexity is 41.582656424304304
At time: 78.89215636253357 and batch: 400, loss is 3.6541384935379027 and perplexity is 38.634223141116756
At time: 79.36335945129395 and batch: 450, loss is 3.689834642410278 and perplexity is 40.038225785358684
At time: 79.83602404594421 and batch: 500, loss is 3.585731644630432 and perplexity is 36.07974561654799
At time: 80.30835556983948 and batch: 550, loss is 3.639738459587097 and perplexity is 38.081875473385395
At time: 80.78127312660217 and batch: 600, loss is 3.6507867527008058 and perplexity is 38.50494800710295
At time: 81.25343346595764 and batch: 650, loss is 3.518350043296814 and perplexity is 33.72873158100049
At time: 81.72595477104187 and batch: 700, loss is 3.5038308238983156 and perplexity is 33.24255472217284
At time: 82.19847989082336 and batch: 750, loss is 3.6047822713851927 and perplexity is 36.773676303936035
At time: 82.66979622840881 and batch: 800, loss is 3.572947220802307 and perplexity is 35.62142279596613
At time: 83.14228367805481 and batch: 850, loss is 3.6414690446853637 and perplexity is 38.14783645868044
At time: 83.61380386352539 and batch: 900, loss is 3.5904945707321168 and perplexity is 36.25200067209691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346345666336687 and perplexity of 77.19584747010455
finished 9 epochs...
Completing Train Step...
At time: 84.8636224269867 and batch: 50, loss is 3.798952169418335 and perplexity is 44.65436975639194
At time: 85.34936261177063 and batch: 100, loss is 3.678831391334534 and perplexity is 39.60009001383662
At time: 85.82275104522705 and batch: 150, loss is 3.689661808013916 and perplexity is 40.03130640074471
At time: 86.2959496974945 and batch: 200, loss is 3.5878319358825683 and perplexity is 36.1556032242953
At time: 86.78565311431885 and batch: 250, loss is 3.7195782470703125 and perplexity is 41.24699439895088
At time: 87.26460361480713 and batch: 300, loss is 3.712285614013672 and perplexity is 40.94728935183102
At time: 87.7380690574646 and batch: 350, loss is 3.6948342084884644 and perplexity is 40.23890076674061
At time: 88.21117639541626 and batch: 400, loss is 3.623316559791565 and perplexity is 37.4616056739864
At time: 88.68498373031616 and batch: 450, loss is 3.660560812950134 and perplexity is 38.88314292797999
At time: 89.15811944007874 and batch: 500, loss is 3.5583658599853516 and perplexity is 35.105782482184615
At time: 89.63104224205017 and batch: 550, loss is 3.613427176475525 and perplexity is 37.09295934274058
At time: 90.12167048454285 and batch: 600, loss is 3.626799421310425 and perplexity is 37.59230673354794
At time: 90.59297204017639 and batch: 650, loss is 3.49692928314209 and perplexity is 33.01391975021901
At time: 91.0667974948883 and batch: 700, loss is 3.4839156341552733 and perplexity is 32.58707163672754
At time: 91.54029655456543 and batch: 750, loss is 3.587329149246216 and perplexity is 36.13742923936609
At time: 92.02197313308716 and batch: 800, loss is 3.556429133415222 and perplexity is 35.03785797731441
At time: 92.50532174110413 and batch: 850, loss is 3.6277459192276003 and perplexity is 37.62790461757705
At time: 92.97790265083313 and batch: 900, loss is 3.5801392936706544 and perplexity is 35.87853815228513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348660821784033 and perplexity of 77.37477489937363
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.24335670471191 and batch: 50, loss is 3.7899310398101806 and perplexity is 44.253348451448026
At time: 94.71701169013977 and batch: 100, loss is 3.674278564453125 and perplexity is 39.42020745730537
At time: 95.19023513793945 and batch: 150, loss is 3.6835465908050535 and perplexity is 39.78725324652202
At time: 95.6635479927063 and batch: 200, loss is 3.572186532020569 and perplexity is 35.59433628276518
At time: 96.13722395896912 and batch: 250, loss is 3.7049787282943725 and perplexity is 40.64918263023522
At time: 96.60984301567078 and batch: 300, loss is 3.6932492733001707 and perplexity is 40.175175230739335
At time: 97.0834596157074 and batch: 350, loss is 3.6739791440963745 and perplexity is 39.40840601161003
At time: 97.55745530128479 and batch: 400, loss is 3.599474549293518 and perplexity is 36.57900892700657
At time: 98.03045439720154 and batch: 450, loss is 3.6291078329086304 and perplexity is 37.67918548779724
At time: 98.5042154788971 and batch: 500, loss is 3.526024651527405 and perplexity is 33.98858223255003
At time: 98.97810506820679 and batch: 550, loss is 3.574631624221802 and perplexity is 35.681474203530826
At time: 99.45150256156921 and batch: 600, loss is 3.590522441864014 and perplexity is 36.25301107046959
At time: 99.92462706565857 and batch: 650, loss is 3.4510010051727296 and perplexity is 31.531940151534634
At time: 100.39793467521667 and batch: 700, loss is 3.434763512611389 and perplexity is 31.024074895472957
At time: 100.87163758277893 and batch: 750, loss is 3.5357190990447998 and perplexity is 34.319685095712025
At time: 101.34573578834534 and batch: 800, loss is 3.4987903213500977 and perplexity is 33.075417123001
At time: 101.81973624229431 and batch: 850, loss is 3.5666880559921266 and perplexity is 35.39915875917061
At time: 102.30673956871033 and batch: 900, loss is 3.518795957565308 and perplexity is 33.74377505747179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327635046553938 and perplexity of 75.76489407801385
finished 11 epochs...
Completing Train Step...
At time: 103.57188272476196 and batch: 50, loss is 3.765625686645508 and perplexity is 43.190721264894904
At time: 104.04505896568298 and batch: 100, loss is 3.6485133743286133 and perplexity is 38.417511117306276
At time: 104.51927304267883 and batch: 150, loss is 3.658739857673645 and perplexity is 38.812402890468206
At time: 104.99149870872498 and batch: 200, loss is 3.5498248291015626 and perplexity is 34.80721974158955
At time: 105.46308302879333 and batch: 250, loss is 3.6839793491363526 and perplexity is 39.804475238055346
At time: 105.93682956695557 and batch: 300, loss is 3.674043140411377 and perplexity is 39.41092808507574
At time: 106.41069507598877 and batch: 350, loss is 3.656006946563721 and perplexity is 38.70647685255729
At time: 106.88462853431702 and batch: 400, loss is 3.583581781387329 and perplexity is 36.00226241661159
At time: 107.37436246871948 and batch: 450, loss is 3.6153665018081664 and perplexity is 37.16496445655975
At time: 107.8520016670227 and batch: 500, loss is 3.513411660194397 and perplexity is 33.562576788581175
At time: 108.32608032226562 and batch: 550, loss is 3.5628452110290527 and perplexity is 35.26338632353052
At time: 108.79976177215576 and batch: 600, loss is 3.580987687110901 and perplexity is 35.908990184524825
At time: 109.27345943450928 and batch: 650, loss is 3.4434210062026978 and perplexity is 31.29383164883555
At time: 109.74756956100464 and batch: 700, loss is 3.42970983505249 and perplexity is 30.867684729968815
At time: 110.22170305252075 and batch: 750, loss is 3.532146372795105 and perplexity is 34.197289029900304
At time: 110.69516396522522 and batch: 800, loss is 3.4971190214157106 and perplexity is 33.02018434865612
At time: 111.16820669174194 and batch: 850, loss is 3.5674350595474245 and perplexity is 35.425611935696615
At time: 111.64201831817627 and batch: 900, loss is 3.521273036003113 and perplexity is 33.827464645120614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326900534433861 and perplexity of 75.70926427791943
finished 12 epochs...
Completing Train Step...
At time: 112.89417362213135 and batch: 50, loss is 3.753614068031311 and perplexity is 42.67503411230931
At time: 113.38055086135864 and batch: 100, loss is 3.6365468597412107 and perplexity is 37.960527116200154
At time: 113.85471558570862 and batch: 150, loss is 3.646549663543701 and perplexity is 38.34214425996991
At time: 114.34107184410095 and batch: 200, loss is 3.5379402351379396 and perplexity is 34.39599850684524
At time: 114.81448459625244 and batch: 250, loss is 3.672420859336853 and perplexity is 39.3470443150402
At time: 115.28886675834656 and batch: 300, loss is 3.663049054145813 and perplexity is 38.98001403539074
At time: 115.76299691200256 and batch: 350, loss is 3.6453681802749633 and perplexity is 38.29687040845476
At time: 116.23691082000732 and batch: 400, loss is 3.57404438495636 and perplexity is 35.66052679200291
At time: 116.71153140068054 and batch: 450, loss is 3.6066974449157714 and perplexity is 36.84417175938309
At time: 117.18629717826843 and batch: 500, loss is 3.5053235578536985 and perplexity is 33.292214067244466
At time: 117.66056942939758 and batch: 550, loss is 3.5551133489608766 and perplexity is 34.991786025500275
At time: 118.13457417488098 and batch: 600, loss is 3.574491229057312 and perplexity is 35.676465048729554
At time: 118.60935378074646 and batch: 650, loss is 3.4377831745147707 and perplexity is 31.11789869931227
At time: 119.08316898345947 and batch: 700, loss is 3.4253173542022703 and perplexity is 30.73239635921053
At time: 119.55766701698303 and batch: 750, loss is 3.5283690595626833 and perplexity is 34.068358815735735
At time: 120.04814028739929 and batch: 800, loss is 3.494210958480835 and perplexity is 32.924299062246526
At time: 120.52914476394653 and batch: 850, loss is 3.5657622146606447 and perplexity is 35.36639992197443
At time: 121.00324845314026 and batch: 900, loss is 3.520569248199463 and perplexity is 33.803665663773714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327191078499572 and perplexity of 75.73126435121534
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 122.25517630577087 and batch: 50, loss is 3.752449107170105 and perplexity is 42.62534831444197
At time: 122.74328207969666 and batch: 100, loss is 3.640639967918396 and perplexity is 38.116222080946386
At time: 123.2188982963562 and batch: 150, loss is 3.64993492603302 and perplexity is 38.47216243134513
At time: 123.69405937194824 and batch: 200, loss is 3.538019509315491 and perplexity is 34.398725329419804
At time: 124.16979598999023 and batch: 250, loss is 3.6733839893341065 and perplexity is 39.384958889124235
At time: 124.64459490776062 and batch: 300, loss is 3.6635962867736818 and perplexity is 39.00135100851715
At time: 125.11766982078552 and batch: 350, loss is 3.6431479454040527 and perplexity is 38.21193668262672
At time: 125.59273147583008 and batch: 400, loss is 3.57190221786499 and perplexity is 35.58421774759111
At time: 126.06749415397644 and batch: 450, loss is 3.6002117919921877 and perplexity is 36.605986477541734
At time: 126.55538368225098 and batch: 500, loss is 3.4976518297195436 and perplexity is 33.03778246486608
At time: 127.02959728240967 and batch: 550, loss is 3.5462059688568117 and perplexity is 34.68148492342396
At time: 127.50471615791321 and batch: 600, loss is 3.566228413581848 and perplexity is 35.382891543354255
At time: 127.97905659675598 and batch: 650, loss is 3.4242490339279175 and perplexity is 30.699581848425073
At time: 128.45348000526428 and batch: 700, loss is 3.411922960281372 and perplexity is 30.323499114952522
At time: 128.92803716659546 and batch: 750, loss is 3.512273669242859 and perplexity is 33.5244046038054
At time: 129.4026644229889 and batch: 800, loss is 3.476816749572754 and perplexity is 32.35655893586563
At time: 129.87802076339722 and batch: 850, loss is 3.5471784019470216 and perplexity is 34.715226750156596
At time: 130.352876663208 and batch: 900, loss is 3.5023849248886108 and perplexity is 33.194524077318746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321664052466824 and perplexity of 75.3138502734874
finished 14 epochs...
Completing Train Step...
At time: 131.62381410598755 and batch: 50, loss is 3.7458964824676513 and perplexity is 42.34695350858357
At time: 132.09776639938354 and batch: 100, loss is 3.628908243179321 and perplexity is 37.67166585981036
At time: 132.5720272064209 and batch: 150, loss is 3.639806408882141 and perplexity is 38.08446319789379
At time: 133.04723739624023 and batch: 200, loss is 3.5294761514663695 and perplexity is 34.10609650564355
At time: 133.5223343372345 and batch: 250, loss is 3.6650353145599364 and perplexity is 39.05751543771486
At time: 133.9967646598816 and batch: 300, loss is 3.6560657787323 and perplexity is 38.70875410551579
At time: 134.4706552028656 and batch: 350, loss is 3.6364236879348755 and perplexity is 37.95585173744915
At time: 134.94226169586182 and batch: 400, loss is 3.5661635398864746 and perplexity is 35.380596198881385
At time: 135.41531133651733 and batch: 450, loss is 3.595462889671326 and perplexity is 36.4325603412182
At time: 135.88956546783447 and batch: 500, loss is 3.4931929016113283 and perplexity is 32.89079730965074
At time: 136.36369037628174 and batch: 550, loss is 3.542302551269531 and perplexity is 34.546372476926074
At time: 136.83809328079224 and batch: 600, loss is 3.563318090438843 and perplexity is 35.280065596173884
At time: 137.31248211860657 and batch: 650, loss is 3.4220902490615845 and perplexity is 30.633379539709534
At time: 137.78750729560852 and batch: 700, loss is 3.4108428001403808 and perplexity is 30.290762563415168
At time: 138.26158356666565 and batch: 750, loss is 3.5119943523406985 and perplexity is 33.5150419785953
At time: 138.75390696525574 and batch: 800, loss is 3.477466311454773 and perplexity is 32.37758335077368
At time: 139.2288463115692 and batch: 850, loss is 3.5486279726028442 and perplexity is 34.76558541456692
At time: 139.7041883468628 and batch: 900, loss is 3.5049622583389284 and perplexity is 33.28018777913319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321266905902183 and perplexity of 75.28394557524857
finished 15 epochs...
Completing Train Step...
At time: 140.97046875953674 and batch: 50, loss is 3.742440071105957 and perplexity is 42.20083768101439
At time: 141.4441237449646 and batch: 100, loss is 3.6249431133270265 and perplexity is 37.5225885636618
At time: 141.91845536231995 and batch: 150, loss is 3.6357434797286987 and perplexity is 37.93004263440187
At time: 142.39130449295044 and batch: 200, loss is 3.525658087730408 and perplexity is 33.976125532019516
At time: 142.86493158340454 and batch: 250, loss is 3.6612908172607423 and perplexity is 38.91153815298474
At time: 143.33815693855286 and batch: 300, loss is 3.652452211380005 and perplexity is 38.56912983821195
At time: 143.8119502067566 and batch: 350, loss is 3.6329990577697755 and perplexity is 37.826089303641695
At time: 144.28497338294983 and batch: 400, loss is 3.5631301021575927 and perplexity is 35.2734339806326
At time: 144.75863194465637 and batch: 450, loss is 3.5927211666107177 and perplexity is 36.33280915791964
At time: 145.2308828830719 and batch: 500, loss is 3.490800952911377 and perplexity is 32.812218225819585
At time: 145.70412588119507 and batch: 550, loss is 3.5400622844696046 and perplexity is 34.46906601149419
At time: 146.17744398117065 and batch: 600, loss is 3.5615731287002563 and perplexity is 35.21855691233095
At time: 146.65197920799255 and batch: 650, loss is 3.4206551027297976 and perplexity is 30.589447689290203
At time: 147.1260802745819 and batch: 700, loss is 3.4099690914154053 and perplexity is 30.26430881799427
At time: 147.60184574127197 and batch: 750, loss is 3.5114116621017457 and perplexity is 33.49551877931749
At time: 148.07424235343933 and batch: 800, loss is 3.477311553955078 and perplexity is 32.37257306462784
At time: 148.54773879051208 and batch: 850, loss is 3.5488407230377197 and perplexity is 34.772982594831326
At time: 149.02040219306946 and batch: 900, loss is 3.5056192827224733 and perplexity is 33.30206085877953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321283627862799 and perplexity of 75.28520448094716
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 150.26022028923035 and batch: 50, loss is 3.741952109336853 and perplexity is 42.18025030893564
At time: 150.74643683433533 and batch: 100, loss is 3.625688982009888 and perplexity is 37.550585927251724
At time: 151.21888160705566 and batch: 150, loss is 3.6368292474746706 and perplexity is 37.97124821709589
At time: 151.6920940876007 and batch: 200, loss is 3.5265731239318847 and perplexity is 34.00722914516302
At time: 152.1672534942627 and batch: 250, loss is 3.662885584831238 and perplexity is 38.97364252000205
At time: 152.64043593406677 and batch: 300, loss is 3.6526422834396364 and perplexity is 38.57646144890359
At time: 153.1141755580902 and batch: 350, loss is 3.632441267967224 and perplexity is 37.80499618006923
At time: 153.5879421234131 and batch: 400, loss is 3.562283101081848 and perplexity is 35.24356999330589
At time: 154.06064772605896 and batch: 450, loss is 3.591536450386047 and perplexity is 36.28979057685944
At time: 154.53423690795898 and batch: 500, loss is 3.487891573905945 and perplexity is 32.716893781699305
At time: 155.00813007354736 and batch: 550, loss is 3.5374755334854124 and perplexity is 34.38001834279068
At time: 155.48155426979065 and batch: 600, loss is 3.55913375377655 and perplexity is 35.13275034749086
At time: 155.95602083206177 and batch: 650, loss is 3.4163947248458864 and perplexity is 30.45940230082463
At time: 156.42898392677307 and batch: 700, loss is 3.40478253364563 and perplexity is 30.107747589837498
At time: 156.9030191898346 and batch: 750, loss is 3.5050229930877688 and perplexity is 33.282209104361044
At time: 157.37613821029663 and batch: 800, loss is 3.470826840400696 and perplexity is 32.16332539016536
At time: 157.85020065307617 and batch: 850, loss is 3.540557317733765 and perplexity is 34.48613356991045
At time: 158.32360339164734 and batch: 900, loss is 3.49757381439209 and perplexity is 33.03520511198656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321159049256207 and perplexity of 75.2758261392584
finished 17 epochs...
Completing Train Step...
At time: 159.57584953308105 and batch: 50, loss is 3.7401571559906004 and perplexity is 42.104606636239
At time: 160.06173515319824 and batch: 100, loss is 3.6221048831939697 and perplexity is 37.416241811798095
At time: 160.53555870056152 and batch: 150, loss is 3.6340237712860106 and perplexity is 37.86487007481421
At time: 161.00962018966675 and batch: 200, loss is 3.524082360267639 and perplexity is 33.92263057574473
At time: 161.4826798439026 and batch: 250, loss is 3.6600182819366456 and perplexity is 38.86205333843514
At time: 161.95655465126038 and batch: 300, loss is 3.650227870941162 and perplexity is 38.4834343063732
At time: 162.42938470840454 and batch: 350, loss is 3.6302830743789674 and perplexity is 37.72349366045217
At time: 162.91502594947815 and batch: 400, loss is 3.5604099082946776 and perplexity is 35.17761378583222
At time: 163.3880331516266 and batch: 450, loss is 3.5897926664352418 and perplexity is 36.22656416509595
At time: 163.86107420921326 and batch: 500, loss is 3.4863962936401367 and perplexity is 32.66800941309306
At time: 164.33306908607483 and batch: 550, loss is 3.536325521469116 and perplexity is 34.34050363412581
At time: 164.80368757247925 and batch: 600, loss is 3.5583595991134644 and perplexity is 35.105562690066044
At time: 165.27530932426453 and batch: 650, loss is 3.415775580406189 and perplexity is 30.440549368200028
At time: 165.74769353866577 and batch: 700, loss is 3.4046831846237184 and perplexity is 30.10475656314274
At time: 166.2205822467804 and batch: 750, loss is 3.505121374130249 and perplexity is 33.28548360386047
At time: 166.69362473487854 and batch: 800, loss is 3.4714428281784055 and perplexity is 32.183143708790574
At time: 167.1682324409485 and batch: 850, loss is 3.5416451406478884 and perplexity is 34.52366878836121
At time: 167.65642762184143 and batch: 900, loss is 3.4991767692565916 and perplexity is 33.08820151879699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320918671072346 and perplexity of 75.25773364748969
finished 18 epochs...
Completing Train Step...
At time: 168.92329692840576 and batch: 50, loss is 3.7390507459640503 and perplexity is 42.05804743882373
At time: 169.39660215377808 and batch: 100, loss is 3.6205326747894286 and perplexity is 37.35746190119785
At time: 169.87150239944458 and batch: 150, loss is 3.6325473976135254 and perplexity is 37.8090086238581
At time: 170.34775614738464 and batch: 200, loss is 3.522705273628235 and perplexity is 33.875948324534875
At time: 170.82212710380554 and batch: 250, loss is 3.6585738229751588 and perplexity is 38.80595921980801
At time: 171.2973186969757 and batch: 300, loss is 3.648865051269531 and perplexity is 38.431024046047135
At time: 171.77208948135376 and batch: 350, loss is 3.6290397882461547 and perplexity is 37.67662170756513
At time: 172.24557733535767 and batch: 400, loss is 3.5593022966384886 and perplexity is 35.138672220812914
At time: 172.7173240184784 and batch: 450, loss is 3.5888261079788206 and perplexity is 36.191565989770965
At time: 173.1894223690033 and batch: 500, loss is 3.485563859939575 and perplexity is 32.64082677656837
At time: 173.66038942337036 and batch: 550, loss is 3.535656757354736 and perplexity is 34.31754561523082
At time: 174.13233280181885 and batch: 600, loss is 3.5578670644760133 and perplexity is 35.08827624191067
At time: 174.6053593158722 and batch: 650, loss is 3.4154220533370974 and perplexity is 30.42978971202727
At time: 175.09046912193298 and batch: 700, loss is 3.4046303701400755 and perplexity is 30.103166637955567
At time: 175.56303811073303 and batch: 750, loss is 3.5051935958862304 and perplexity is 33.28788762674536
At time: 176.03496932983398 and batch: 800, loss is 3.471729221343994 and perplexity is 32.192362061169476
At time: 176.5063877105713 and batch: 850, loss is 3.542151074409485 and perplexity is 34.541139897214656
At time: 176.97785115242004 and batch: 900, loss is 3.499927182197571 and perplexity is 33.113040652048326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320802035397047 and perplexity of 75.24895642278294
finished 19 epochs...
Completing Train Step...
At time: 178.2289378643036 and batch: 50, loss is 3.7380811977386474 and perplexity is 42.01728989496043
At time: 178.69880151748657 and batch: 100, loss is 3.619363794326782 and perplexity is 37.31382100431216
At time: 179.16853094100952 and batch: 150, loss is 3.6314123821258546 and perplexity is 37.76611915820483
At time: 179.63754725456238 and batch: 200, loss is 3.5216347122192384 and perplexity is 33.83970144728134
At time: 180.1098039150238 and batch: 250, loss is 3.657507371902466 and perplexity is 38.76459662248452
At time: 180.58134818077087 and batch: 300, loss is 3.6478375959396363 and perplexity is 38.39155816374517
At time: 181.05225014686584 and batch: 350, loss is 3.6280773973464964 and perplexity is 37.64037951208075
At time: 181.52479124069214 and batch: 400, loss is 3.5584485483169557 and perplexity is 35.10868544078647
At time: 181.99773240089417 and batch: 450, loss is 3.5880732345581055 and perplexity is 36.16432857613215
At time: 182.4707407951355 and batch: 500, loss is 3.484907388687134 and perplexity is 32.6194060439505
At time: 182.94449925422668 and batch: 550, loss is 3.5350997495651244 and perplexity is 34.29843579765548
At time: 183.41676306724548 and batch: 600, loss is 3.557449908256531 and perplexity is 35.073642001839794
At time: 183.88821411132812 and batch: 650, loss is 3.415107822418213 and perplexity is 30.420229233422255
At time: 184.361323595047 and batch: 700, loss is 3.4045214319229125 and perplexity is 30.099887431269774
At time: 184.8337709903717 and batch: 750, loss is 3.505170478820801 and perplexity is 33.28711811736352
At time: 185.3060598373413 and batch: 800, loss is 3.4718385887145997 and perplexity is 32.1958830476987
At time: 185.7773745059967 and batch: 850, loss is 3.542396125793457 and perplexity is 34.549605288536014
At time: 186.26402378082275 and batch: 900, loss is 3.5003321027755736 and perplexity is 33.12645151859326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32075521390732 and perplexity of 75.24543323702392
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f06654d0b70>
ELAPSED
785.4275169372559


RESULTS SO FAR:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.28313217785833, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -73.79736611891688, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.443992016443709, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8989021853099624, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.24543323702392, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.7650501504885647, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.39050037147704286, 'seq_len': 35, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.3348144614713373, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.11193593374944022, 'seq_len': 35, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6762394905090332 and batch: 50, loss is 6.623186531066895 and perplexity is 752.338631973568
At time: 1.1683502197265625 and batch: 100, loss is 5.861334104537963 and perplexity is 351.1923589464074
At time: 1.6449320316314697 and batch: 150, loss is 5.597579164505005 and perplexity is 269.77254135070234
At time: 2.136207103729248 and batch: 200, loss is 5.340808610916138 and perplexity is 208.6813841293532
At time: 2.6141726970672607 and batch: 250, loss is 5.330454845428466 and perplexity is 206.53189288274757
At time: 3.0954818725585938 and batch: 300, loss is 5.220870552062988 and perplexity is 185.09524900367592
At time: 3.574276924133301 and batch: 350, loss is 5.171902351379394 and perplexity is 176.24980783244138
At time: 4.060287952423096 and batch: 400, loss is 5.006796827316284 and perplexity is 149.42533360751895
At time: 4.541525840759277 and batch: 450, loss is 4.997454977035522 and perplexity is 148.0359244434819
At time: 5.02079439163208 and batch: 500, loss is 4.909921112060547 and perplexity is 135.6287145166057
At time: 5.499661207199097 and batch: 550, loss is 4.961410999298096 and perplexity is 142.7951376560483
At time: 5.997677564620972 and batch: 600, loss is 4.887675914764404 and perplexity is 132.64493740067374
At time: 6.482038974761963 and batch: 650, loss is 4.754062652587891 and perplexity is 116.05481848064758
At time: 6.963443756103516 and batch: 700, loss is 4.809222602844239 and perplexity is 122.6362433834787
At time: 7.457791328430176 and batch: 750, loss is 4.829297075271606 and perplexity is 125.12297770057484
At time: 7.947548151016235 and batch: 800, loss is 4.773064155578613 and perplexity is 118.28111900439133
At time: 8.427775859832764 and batch: 850, loss is 4.817630777359009 and perplexity is 123.6717375268614
At time: 8.908068418502808 and batch: 900, loss is 4.741721878051758 and perplexity is 114.63141315873497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.763367953365797 and perplexity of 117.13978360796523
finished 1 epochs...
Completing Train Step...
At time: 10.185054779052734 and batch: 50, loss is 4.742549324035645 and perplexity is 114.72630371417095
At time: 10.659146785736084 and batch: 100, loss is 4.614647808074952 and perplexity is 100.95226769723071
At time: 11.133474111557007 and batch: 150, loss is 4.594302368164063 and perplexity is 98.91910235560584
At time: 11.60802936553955 and batch: 200, loss is 4.490612802505493 and perplexity is 89.17607646115555
At time: 12.082921028137207 and batch: 250, loss is 4.609162712097168 and perplexity is 100.40005068590256
At time: 12.557588815689087 and batch: 300, loss is 4.565819606781006 and perplexity is 96.14135984407952
At time: 13.03266429901123 and batch: 350, loss is 4.5499765586853025 and perplexity is 94.63019003286001
At time: 13.506870031356812 and batch: 400, loss is 4.4440207862854 and perplexity is 85.11648978248864
At time: 13.994117259979248 and batch: 450, loss is 4.474241876602173 and perplexity is 87.72806649262768
At time: 14.468928813934326 and batch: 500, loss is 4.371188116073609 and perplexity is 79.13760052078253
At time: 14.943415403366089 and batch: 550, loss is 4.450836267471313 and perplexity is 85.69858097979973
At time: 15.418165922164917 and batch: 600, loss is 4.423374080657959 and perplexity is 83.3771324614688
At time: 15.893982887268066 and batch: 650, loss is 4.286843395233154 and perplexity is 72.73650533969935
At time: 16.366551399230957 and batch: 700, loss is 4.313358297348023 and perplexity is 74.69090248315737
At time: 16.838873624801636 and batch: 750, loss is 4.388452672958374 and perplexity is 80.51573836526032
At time: 17.312792778015137 and batch: 800, loss is 4.342077302932739 and perplexity is 76.86704975309269
At time: 17.786624908447266 and batch: 850, loss is 4.415644903182983 and perplexity is 82.73517988587373
At time: 18.26031517982483 and batch: 900, loss is 4.352490825653076 and perplexity is 77.67168881437553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.544619207512842 and perplexity of 94.12457845203984
finished 2 epochs...
Completing Train Step...
At time: 19.534411907196045 and batch: 50, loss is 4.428547143936157 and perplexity is 83.80956518020992
At time: 20.009286880493164 and batch: 100, loss is 4.3042670297622685 and perplexity is 74.01494481420069
At time: 20.483391761779785 and batch: 150, loss is 4.2962044858932495 and perplexity is 73.42059527889249
At time: 20.958005905151367 and batch: 200, loss is 4.206925754547119 and perplexity is 67.14978724079118
At time: 21.432902336120605 and batch: 250, loss is 4.3424115657806395 and perplexity is 76.89274784677258
At time: 21.910844326019287 and batch: 300, loss is 4.313235354423523 and perplexity is 74.68172032962455
At time: 22.39992618560791 and batch: 350, loss is 4.299490003585816 and perplexity is 73.66221665197712
At time: 22.901430368423462 and batch: 400, loss is 4.212947001457215 and perplexity is 67.55533240825503
At time: 23.38822054862976 and batch: 450, loss is 4.252750906944275 and perplexity is 70.29853131671602
At time: 23.87092423439026 and batch: 500, loss is 4.141893758773803 and perplexity is 62.92186753004261
At time: 24.360179662704468 and batch: 550, loss is 4.227069010734558 and perplexity is 68.51611758032763
At time: 24.85062861442566 and batch: 600, loss is 4.219449715614319 and perplexity is 67.99605682413106
At time: 25.32947564125061 and batch: 650, loss is 4.076608462333679 and perplexity is 58.94521553806735
At time: 25.805193185806274 and batch: 700, loss is 4.093323531150818 and perplexity is 59.93876940021493
At time: 26.303751468658447 and batch: 750, loss is 4.1844153881073 and perplexity is 65.65510693871022
At time: 26.780744552612305 and batch: 800, loss is 4.142792649269104 and perplexity is 62.97845282689461
At time: 27.257244348526 and batch: 850, loss is 4.220524711608887 and perplexity is 68.06919161562438
At time: 27.733804941177368 and batch: 900, loss is 4.167055368423462 and perplexity is 64.525169237604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.468008381046661 and perplexity of 87.18291484545817
finished 3 epochs...
Completing Train Step...
At time: 28.992613077163696 and batch: 50, loss is 4.255240812301635 and perplexity is 70.47378609982489
At time: 29.480207204818726 and batch: 100, loss is 4.134215044975281 and perplexity is 62.440558798543826
At time: 29.955564260482788 and batch: 150, loss is 4.130010857582092 and perplexity is 62.178598040328374
At time: 30.428623914718628 and batch: 200, loss is 4.042418022155761 and perplexity is 56.96391642239962
At time: 30.901683807373047 and batch: 250, loss is 4.180450859069825 and perplexity is 65.39533064637281
At time: 31.37228226661682 and batch: 300, loss is 4.162761025428772 and perplexity is 64.24867014499662
At time: 31.845782995224 and batch: 350, loss is 4.150078392028808 and perplexity is 63.43897321301801
At time: 32.31882977485657 and batch: 400, loss is 4.068918080329895 and perplexity is 58.49364292181743
At time: 32.79297971725464 and batch: 450, loss is 4.112673988342285 and perplexity is 61.109906467272616
At time: 33.27484059333801 and batch: 500, loss is 4.000564932823181 and perplexity is 54.6290030342989
At time: 33.74884080886841 and batch: 550, loss is 4.086086683273315 and perplexity is 59.5065674203618
At time: 34.22271513938904 and batch: 600, loss is 4.086782584190368 and perplexity is 59.547992507406576
At time: 34.69688296318054 and batch: 650, loss is 3.943837971687317 and perplexity is 51.61632362014758
At time: 35.16946625709534 and batch: 700, loss is 3.952768349647522 and perplexity is 52.07934128316042
At time: 35.64386773109436 and batch: 750, loss is 4.050019521713256 and perplexity is 57.39857755308137
At time: 36.1166296005249 and batch: 800, loss is 4.0142398452758785 and perplexity is 55.38118113137628
At time: 36.58956694602966 and batch: 850, loss is 4.090918574333191 and perplexity is 59.794792446678386
At time: 37.063652753829956 and batch: 900, loss is 4.044515800476074 and perplexity is 57.08353951881128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.435468699834118 and perplexity of 84.39166998189874
finished 4 epochs...
Completing Train Step...
At time: 38.31930994987488 and batch: 50, loss is 4.1349735975265505 and perplexity is 62.48794121247667
At time: 38.807270526885986 and batch: 100, loss is 4.018424081802368 and perplexity is 55.613394571529305
At time: 39.280683755874634 and batch: 150, loss is 4.014903388023376 and perplexity is 55.41794110702274
At time: 39.75443720817566 and batch: 200, loss is 3.9266327142715456 and perplexity is 50.73584761341256
At time: 40.227983713150024 and batch: 250, loss is 4.070999174118042 and perplexity is 58.61550043322325
At time: 40.7024040222168 and batch: 300, loss is 4.056354689598083 and perplexity is 57.76336144178911
At time: 41.175864696502686 and batch: 350, loss is 4.044123182296753 and perplexity is 57.06113188256668
At time: 41.64989757537842 and batch: 400, loss is 3.967856011390686 and perplexity is 52.87105429875963
At time: 42.123637199401855 and batch: 450, loss is 4.011759524345398 and perplexity is 55.243988240195264
At time: 42.597033977508545 and batch: 500, loss is 3.900606517791748 and perplexity is 49.43242165839307
At time: 43.070432901382446 and batch: 550, loss is 3.9854941606521606 and perplexity is 53.8118746197393
At time: 43.54416275024414 and batch: 600, loss is 3.9909276723861695 and perplexity is 54.10505785789211
At time: 44.017176151275635 and batch: 650, loss is 3.8461518478393555 and perplexity is 46.81257427098867
At time: 44.491076946258545 and batch: 700, loss is 3.8516282176971437 and perplexity is 47.06964049408515
At time: 44.96574878692627 and batch: 750, loss is 3.9547475576400757 and perplexity is 52.182519203275795
At time: 45.43857550621033 and batch: 800, loss is 3.9211278629302977 and perplexity is 50.45732164002398
At time: 45.910818099975586 and batch: 850, loss is 3.9978050804138183 and perplexity is 54.478442906085235
At time: 46.3831901550293 and batch: 900, loss is 3.9526791381835937 and perplexity is 52.07469541611951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430061235819777 and perplexity of 83.9365566767557
finished 5 epochs...
Completing Train Step...
At time: 47.66006016731262 and batch: 50, loss is 4.04560115814209 and perplexity is 57.14552921043951
At time: 48.13265061378479 and batch: 100, loss is 3.9315686559677125 and perplexity is 50.986895869213356
At time: 48.60631561279297 and batch: 150, loss is 3.9287613344192507 and perplexity is 50.84395998511804
At time: 49.078749895095825 and batch: 200, loss is 3.839128556251526 and perplexity is 46.484947766866455
At time: 49.55180478096008 and batch: 250, loss is 3.9866635513305666 and perplexity is 53.874838531837995
At time: 50.02546834945679 and batch: 300, loss is 3.9753592824935913 and perplexity is 53.26925217779983
At time: 50.4987587928772 and batch: 350, loss is 3.9629405832290647 and perplexity is 52.61180810452664
At time: 50.98564887046814 and batch: 400, loss is 3.889869141578674 and perplexity is 48.904486541090684
At time: 51.45919966697693 and batch: 450, loss is 3.934466562271118 and perplexity is 51.134865413529134
At time: 51.93316149711609 and batch: 500, loss is 3.8246859455108644 and perplexity is 45.8184086296418
At time: 52.406529664993286 and batch: 550, loss is 3.905527420043945 and perplexity is 49.67627326634912
At time: 52.88031363487244 and batch: 600, loss is 3.9187112522125243 and perplexity is 50.335533152710894
At time: 53.35344123840332 and batch: 650, loss is 3.771934370994568 and perplexity is 43.46405918694001
At time: 53.82738995552063 and batch: 700, loss is 3.778835988044739 and perplexity is 43.76506901134708
At time: 54.30064916610718 and batch: 750, loss is 3.8811247730255127 and perplexity is 48.478711964811524
At time: 54.77448582649231 and batch: 800, loss is 3.8468232488632204 and perplexity is 46.84401483471623
At time: 55.24815058708191 and batch: 850, loss is 3.92137216091156 and perplexity is 50.46964976764757
At time: 55.72170829772949 and batch: 900, loss is 3.879548764228821 and perplexity is 48.40236926248959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428383605120933 and perplexity of 83.79586018389587
finished 6 epochs...
Completing Train Step...
At time: 56.98458290100098 and batch: 50, loss is 3.974480085372925 and perplexity is 53.22243858686777
At time: 57.457513093948364 and batch: 100, loss is 3.8612041091918945 and perplexity is 47.52253925805517
At time: 57.9310827255249 and batch: 150, loss is 3.85962149143219 and perplexity is 47.44738872640779
At time: 58.4052095413208 and batch: 200, loss is 3.7713259410858155 and perplexity is 43.437622396657396
At time: 58.87861609458923 and batch: 250, loss is 3.918427906036377 and perplexity is 50.321272792272445
At time: 59.35221719741821 and batch: 300, loss is 3.9101683616638185 and perplexity is 49.90735375123371
At time: 59.825892210006714 and batch: 350, loss is 3.89703085899353 and perplexity is 49.25598381372355
At time: 60.300036668777466 and batch: 400, loss is 3.829014678001404 and perplexity is 46.01717415476402
At time: 60.77163815498352 and batch: 450, loss is 3.8713725662231444 and perplexity is 48.00823536152199
At time: 61.24080538749695 and batch: 500, loss is 3.763922247886658 and perplexity is 43.11721114403949
At time: 61.71130609512329 and batch: 550, loss is 3.841552858352661 and perplexity is 46.59777803547658
At time: 62.18244767189026 and batch: 600, loss is 3.856868953704834 and perplexity is 47.316967575786855
At time: 62.66641068458557 and batch: 650, loss is 3.711678566932678 and perplexity is 40.92243996249311
At time: 63.13680410385132 and batch: 700, loss is 3.715570693016052 and perplexity is 41.08202562075913
At time: 63.607335805892944 and batch: 750, loss is 3.8209754991531373 and perplexity is 45.64871689295289
At time: 64.07796597480774 and batch: 800, loss is 3.7877654790878297 and perplexity is 44.15761882977495
At time: 64.5481345653534 and batch: 850, loss is 3.862199454307556 and perplexity is 47.56986413377434
At time: 65.01785016059875 and batch: 900, loss is 3.8223338317871094 and perplexity is 45.7107651663646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.439159654591181 and perplexity of 84.70373136562792
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 66.26002240180969 and batch: 50, loss is 3.930211219787598 and perplexity is 50.91773136586875
At time: 66.74543762207031 and batch: 100, loss is 3.8065190649032594 and perplexity is 44.99354634380074
At time: 67.21626901626587 and batch: 150, loss is 3.7996366930007937 and perplexity is 44.684947189843186
At time: 67.68860459327698 and batch: 200, loss is 3.6959223842620847 and perplexity is 40.28271159633066
At time: 68.16584610939026 and batch: 250, loss is 3.830101447105408 and perplexity is 46.067211382420744
At time: 68.64078950881958 and batch: 300, loss is 3.8133457994461057 and perplexity is 45.30175617719802
At time: 69.11612915992737 and batch: 350, loss is 3.7923539590835573 and perplexity is 44.360700742772366
At time: 69.59030508995056 and batch: 400, loss is 3.7119647598266603 and perplexity is 40.93415335007898
At time: 70.06543898582458 and batch: 450, loss is 3.742523455619812 and perplexity is 42.20435672406351
At time: 70.54058361053467 and batch: 500, loss is 3.6299610424041746 and perplexity is 37.71134744513243
At time: 71.01545357704163 and batch: 550, loss is 3.690301251411438 and perplexity is 40.05691234121915
At time: 71.49008202552795 and batch: 600, loss is 3.694121217727661 and perplexity is 40.2102210276785
At time: 71.96468257904053 and batch: 650, loss is 3.5388767385482787 and perplexity is 34.428225564768375
At time: 72.4409830570221 and batch: 700, loss is 3.5361156272888183 and perplexity is 34.33329651865677
At time: 72.91544270515442 and batch: 750, loss is 3.6271732950210573 and perplexity is 37.60636413644059
At time: 73.39043712615967 and batch: 800, loss is 3.58067729473114 and perplexity is 35.89784603722531
At time: 73.86448431015015 and batch: 850, loss is 3.638389410972595 and perplexity is 38.03053580967771
At time: 74.33940958976746 and batch: 900, loss is 3.5930270099639894 and perplexity is 36.343923005567866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348723947185359 and perplexity of 77.37965936725709
finished 8 epochs...
Completing Train Step...
At time: 75.60908532142639 and batch: 50, loss is 3.842754673957825 and perplexity is 46.653813637771584
At time: 76.09671425819397 and batch: 100, loss is 3.7235530185699464 and perplexity is 41.411268035540566
At time: 76.57181668281555 and batch: 150, loss is 3.7217301607131956 and perplexity is 41.33584993936189
At time: 77.04656839370728 and batch: 200, loss is 3.623723225593567 and perplexity is 37.47684312596741
At time: 77.52185273170471 and batch: 250, loss is 3.760990195274353 and perplexity is 42.990974369339234
At time: 77.99615049362183 and batch: 300, loss is 3.75108286857605 and perplexity is 42.56715168278437
At time: 78.4716386795044 and batch: 350, loss is 3.73276969909668 and perplexity is 41.794706765033375
At time: 78.94623923301697 and batch: 400, loss is 3.6592186069488526 and perplexity is 38.830988748849194
At time: 79.41945838928223 and batch: 450, loss is 3.6942336130142213 and perplexity is 40.2147407209849
At time: 79.89518070220947 and batch: 500, loss is 3.584415159225464 and perplexity is 36.03227840982685
At time: 80.37102937698364 and batch: 550, loss is 3.6478611040115356 and perplexity is 38.39246068586304
At time: 80.84574699401855 and batch: 600, loss is 3.6580060338974 and perplexity is 38.78393187404599
At time: 81.32030844688416 and batch: 650, loss is 3.5060762643814085 and perplexity is 33.3172827675984
At time: 81.7948477268219 and batch: 700, loss is 3.5091459226608275 and perplexity is 33.419712572311454
At time: 82.26957178115845 and batch: 750, loss is 3.6040866231918334 and perplexity is 36.74810365826568
At time: 82.744788646698 and batch: 800, loss is 3.562581720352173 and perplexity is 35.25409597401305
At time: 83.21888470649719 and batch: 850, loss is 3.6276295518875123 and perplexity is 37.623526213160204
At time: 83.69058275222778 and batch: 900, loss is 3.5889456701278686 and perplexity is 36.19589338986959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347232766347389 and perplexity of 77.26435829070344
finished 9 epochs...
Completing Train Step...
At time: 84.95121550559998 and batch: 50, loss is 3.8040324068069458 and perplexity is 44.881801770357946
At time: 85.42414021492004 and batch: 100, loss is 3.686702694892883 and perplexity is 39.91302432804876
At time: 85.89849424362183 and batch: 150, loss is 3.6846985244750976 and perplexity is 39.833111931185705
At time: 86.37207531929016 and batch: 200, loss is 3.588669595718384 and perplexity is 36.18590200922192
At time: 86.85847854614258 and batch: 250, loss is 3.7261580514907835 and perplexity is 41.51928638666099
At time: 87.33172154426575 and batch: 300, loss is 3.7187931632995603 and perplexity is 41.21462476115722
At time: 87.80501174926758 and batch: 350, loss is 3.7007311964035035 and perplexity is 40.47689009872157
At time: 88.27789783477783 and batch: 400, loss is 3.62938916683197 and perplexity is 37.689787412149336
At time: 88.75204181671143 and batch: 450, loss is 3.6667772054672243 and perplexity is 39.125608656979544
At time: 89.22555732727051 and batch: 500, loss is 3.5577418422698974 and perplexity is 35.083882685642
At time: 89.69967913627625 and batch: 550, loss is 3.6224260044097902 and perplexity is 37.42825889022629
At time: 90.17331409454346 and batch: 600, loss is 3.634515280723572 and perplexity is 37.88348559028402
At time: 90.64665532112122 and batch: 650, loss is 3.484717836380005 and perplexity is 32.613223546249564
At time: 91.11955857276917 and batch: 700, loss is 3.490331225395203 and perplexity is 32.79680904339403
At time: 91.59312629699707 and batch: 750, loss is 3.586590838432312 and perplexity is 36.11075843145647
At time: 92.06743240356445 and batch: 800, loss is 3.5471256875991823 and perplexity is 34.713396807850906
At time: 92.53982520103455 and batch: 850, loss is 3.61574182510376 and perplexity is 37.178915951496954
At time: 93.01361298561096 and batch: 900, loss is 3.579275436401367 and perplexity is 35.847557599605295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348869010193707 and perplexity of 77.39088510762997
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 94.31505942344666 and batch: 50, loss is 3.7985019969940184 and perplexity is 44.63427211454386
At time: 94.7883529663086 and batch: 100, loss is 3.6805008125305174 and perplexity is 39.666254456263374
At time: 95.26256322860718 and batch: 150, loss is 3.6784521913528443 and perplexity is 39.5850765071691
At time: 95.73547673225403 and batch: 200, loss is 3.576580090522766 and perplexity is 35.75106613032995
At time: 96.2095046043396 and batch: 250, loss is 3.7109999322891234 and perplexity is 40.8946779982166
At time: 96.68358612060547 and batch: 300, loss is 3.700487060546875 and perplexity is 40.46700944464371
At time: 97.15785121917725 and batch: 350, loss is 3.680310592651367 and perplexity is 39.65870986372285
At time: 97.63162684440613 and batch: 400, loss is 3.606222162246704 and perplexity is 36.826664523862085
At time: 98.10496783256531 and batch: 450, loss is 3.638836398124695 and perplexity is 38.04753877034149
At time: 98.57922434806824 and batch: 500, loss is 3.5285389709472654 and perplexity is 34.0741479095551
At time: 99.09604287147522 and batch: 550, loss is 3.584702048301697 and perplexity is 36.04261715986073
At time: 99.57788753509521 and batch: 600, loss is 3.5946197366714476 and perplexity is 36.401855065115136
At time: 100.05991888046265 and batch: 650, loss is 3.44080705165863 and perplexity is 31.212137813897503
At time: 100.53162455558777 and batch: 700, loss is 3.4397782468795777 and perplexity is 31.180043129763185
At time: 101.00448894500732 and batch: 750, loss is 3.529622640609741 and perplexity is 34.11109304446482
At time: 101.47832012176514 and batch: 800, loss is 3.4878202533721923 and perplexity is 32.714560478579266
At time: 101.9514479637146 and batch: 850, loss is 3.5560134077072143 and perplexity is 35.02329486633927
At time: 102.42560696601868 and batch: 900, loss is 3.5169011831283568 and perplexity is 33.67989874979468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327825676904966 and perplexity of 75.77933854310047
finished 11 epochs...
Completing Train Step...
At time: 103.6807451248169 and batch: 50, loss is 3.773425326347351 and perplexity is 43.528910491825116
At time: 104.16594529151917 and batch: 100, loss is 3.654090247154236 and perplexity is 38.6323592245284
At time: 104.63880324363708 and batch: 150, loss is 3.6536390352249146 and perplexity is 38.61493177522066
At time: 105.11175727844238 and batch: 200, loss is 3.554706654548645 and perplexity is 34.97755795508417
At time: 105.58374452590942 and batch: 250, loss is 3.6907895851135253 and perplexity is 40.0764782584765
At time: 106.05689001083374 and batch: 300, loss is 3.681466794013977 and perplexity is 39.70458983623897
At time: 106.53100490570068 and batch: 350, loss is 3.6619157505035402 and perplexity is 38.935862866574354
At time: 107.02209758758545 and batch: 400, loss is 3.590432953834534 and perplexity is 36.24976700510085
At time: 107.49710941314697 and batch: 450, loss is 3.624716591835022 and perplexity is 37.51408985352309
At time: 107.96978330612183 and batch: 500, loss is 3.51515016078949 and perplexity is 33.62097609720862
At time: 108.44265270233154 and batch: 550, loss is 3.5737173414230345 and perplexity is 35.64886615419264
At time: 108.91530728340149 and batch: 600, loss is 3.5854891204833983 and perplexity is 36.07099646799991
At time: 109.38939380645752 and batch: 650, loss is 3.4332789325714113 and perplexity is 30.978051344399912
At time: 109.86245822906494 and batch: 700, loss is 3.434725832939148 and perplexity is 31.022905940522374
At time: 110.3354926109314 and batch: 750, loss is 3.525874400138855 and perplexity is 33.9834757845098
At time: 110.80961751937866 and batch: 800, loss is 3.486237964630127 and perplexity is 32.662837528944145
At time: 111.30567288398743 and batch: 850, loss is 3.5570910024642943 and perplexity is 35.06105612727725
At time: 111.77822089195251 and batch: 900, loss is 3.519965271949768 and perplexity is 33.78325521689321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326711994327911 and perplexity of 75.69499139075918
finished 12 epochs...
Completing Train Step...
At time: 113.03033518791199 and batch: 50, loss is 3.761588649749756 and perplexity is 43.016710210449034
At time: 113.51559710502625 and batch: 100, loss is 3.641990752220154 and perplexity is 38.16774366481507
At time: 113.98890256881714 and batch: 150, loss is 3.6412087154388426 and perplexity is 38.13790675371112
At time: 114.46258068084717 and batch: 200, loss is 3.543165535926819 and perplexity is 34.576198334116135
At time: 114.936115026474 and batch: 250, loss is 3.6793156671524048 and perplexity is 39.61927202413281
At time: 115.40948915481567 and batch: 300, loss is 3.6707608032226564 and perplexity is 39.28178019958423
At time: 115.8818793296814 and batch: 350, loss is 3.651306948661804 and perplexity is 38.52498333623154
At time: 116.35503816604614 and batch: 400, loss is 3.5811062908172606 and perplexity is 35.91324937642532
At time: 116.82813358306885 and batch: 450, loss is 3.6160207653045653 and perplexity is 37.18928809231446
At time: 117.30158829689026 and batch: 500, loss is 3.506780047416687 and perplexity is 33.340739159128844
At time: 117.77527785301208 and batch: 550, loss is 3.566480622291565 and perplexity is 35.391816542210364
At time: 118.24931979179382 and batch: 600, loss is 3.579312858581543 and perplexity is 35.84889911846578
At time: 118.72572350502014 and batch: 650, loss is 3.4279004526138306 and perplexity is 30.811883781148484
At time: 119.19944620132446 and batch: 700, loss is 3.430300946235657 and perplexity is 30.885936357450273
At time: 119.68855810165405 and batch: 750, loss is 3.522055745124817 and perplexity is 33.85395207488666
At time: 120.16935515403748 and batch: 800, loss is 3.483454966545105 and perplexity is 32.572063285510325
At time: 120.6468517780304 and batch: 850, loss is 3.5558919095993042 and perplexity is 35.01903986077303
At time: 121.12025475502014 and batch: 900, loss is 3.5194413805007936 and perplexity is 33.76556109367118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326689001632063 and perplexity of 75.6932509788534
finished 13 epochs...
Completing Train Step...
At time: 122.39255046844482 and batch: 50, loss is 3.751825003623962 and perplexity is 42.59875398307482
At time: 122.86642217636108 and batch: 100, loss is 3.632202634811401 and perplexity is 37.795975730856846
At time: 123.35289120674133 and batch: 150, loss is 3.6313379859924315 and perplexity is 37.76330960947614
At time: 123.82623600959778 and batch: 200, loss is 3.533952488899231 and perplexity is 34.25910911465493
At time: 124.29946613311768 and batch: 250, loss is 3.67008508682251 and perplexity is 39.25524582234297
At time: 124.79154467582703 and batch: 300, loss is 3.662187352180481 and perplexity is 38.946439348452145
At time: 125.28774189949036 and batch: 350, loss is 3.6427243995666503 and perplexity is 38.19575560286209
At time: 125.77064990997314 and batch: 400, loss is 3.573510274887085 and perplexity is 35.64148523116501
At time: 126.25091481208801 and batch: 450, loss is 3.6088141345977784 and perplexity is 36.92224203371167
At time: 126.74392414093018 and batch: 500, loss is 3.4997661113739014 and perplexity is 33.10770753683246
At time: 127.22391152381897 and batch: 550, loss is 3.5602665328979493 and perplexity is 35.172570543046824
At time: 127.69810366630554 and batch: 600, loss is 3.573823618888855 and perplexity is 35.652655026679184
At time: 128.16902661323547 and batch: 650, loss is 3.4229523944854736 and perplexity is 30.659801355785262
At time: 128.64101481437683 and batch: 700, loss is 3.425900392532349 and perplexity is 30.75031974876694
At time: 129.11408638954163 and batch: 750, loss is 3.5180676794052124 and perplexity is 33.719209149551155
At time: 129.5879602432251 and batch: 800, loss is 3.4801240396499633 and perplexity is 32.46374861814828
At time: 130.05959153175354 and batch: 850, loss is 3.553639907836914 and perplexity is 34.94026565440716
At time: 130.5319492816925 and batch: 900, loss is 3.517722182273865 and perplexity is 33.70756127178873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327113739431721 and perplexity of 75.7254075923043
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 131.80149841308594 and batch: 50, loss is 3.7539607238769532 and perplexity is 42.68983022677894
At time: 132.27631759643555 and batch: 100, loss is 3.636311960220337 and perplexity is 37.951611253775354
At time: 132.75124979019165 and batch: 150, loss is 3.637016954421997 and perplexity is 37.97837635316429
At time: 133.22626566886902 and batch: 200, loss is 3.534809293746948 and perplexity is 34.28847506407025
At time: 133.69925236701965 and batch: 250, loss is 3.6699460983276366 and perplexity is 39.249790173955205
At time: 134.17220401763916 and batch: 300, loss is 3.661632442474365 and perplexity is 38.92483358641698
At time: 134.6450846195221 and batch: 350, loss is 3.641161322593689 and perplexity is 38.13609933263161
At time: 135.11890149116516 and batch: 400, loss is 3.5717171669006347 and perplexity is 35.57763346301398
At time: 135.60652422904968 and batch: 450, loss is 3.603727707862854 and perplexity is 36.734916567217915
At time: 136.0806748867035 and batch: 500, loss is 3.4950699281692503 and perplexity is 32.95259218688192
At time: 136.55502200126648 and batch: 550, loss is 3.5514102935791017 and perplexity is 34.86244912259342
At time: 137.02904891967773 and batch: 600, loss is 3.565742630958557 and perplexity is 35.365707323716286
At time: 137.50429129600525 and batch: 650, loss is 3.4103816747665405 and perplexity is 30.27679794417179
At time: 137.97741317749023 and batch: 700, loss is 3.4120298433303833 and perplexity is 30.32674035620841
At time: 138.45247793197632 and batch: 750, loss is 3.5000290203094484 and perplexity is 33.11641299330037
At time: 138.94265031814575 and batch: 800, loss is 3.4626190423965455 and perplexity is 31.90041574283242
At time: 139.41913652420044 and batch: 850, loss is 3.5347091436386107 and perplexity is 34.285041241529576
At time: 139.894549369812 and batch: 900, loss is 3.4992727184295656 and perplexity is 33.09137645668181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321506029938998 and perplexity of 75.30194992877294
finished 15 epochs...
Completing Train Step...
At time: 141.143887758255 and batch: 50, loss is 3.74370352268219 and perplexity is 42.25419009289803
At time: 141.631507396698 and batch: 100, loss is 3.626001467704773 and perplexity is 37.56232178173638
At time: 142.10460805892944 and batch: 150, loss is 3.6279221534729005 and perplexity is 37.6345365273172
At time: 142.57942485809326 and batch: 200, loss is 3.526734414100647 and perplexity is 34.01271461925561
At time: 143.0535683631897 and batch: 250, loss is 3.6630228900909425 and perplexity is 38.97899417350658
At time: 143.52556204795837 and batch: 300, loss is 3.6550364971160887 and perplexity is 38.668932393922034
At time: 143.99948048591614 and batch: 350, loss is 3.6345348453521726 and perplexity is 37.88422677386016
At time: 144.47339248657227 and batch: 400, loss is 3.566308922767639 and perplexity is 35.38574030581757
At time: 144.94700932502747 and batch: 450, loss is 3.599188585281372 and perplexity is 36.568550142342765
At time: 145.43550610542297 and batch: 500, loss is 3.490941162109375 and perplexity is 32.81681912315809
At time: 145.91583585739136 and batch: 550, loss is 3.5481218194961546 and perplexity is 34.74799315806668
At time: 146.391037940979 and batch: 600, loss is 3.56315185546875 and perplexity is 35.274201302963434
At time: 146.86593103408813 and batch: 650, loss is 3.4085994386672973 and perplexity is 30.222885598441017
At time: 147.34035086631775 and batch: 700, loss is 3.411091475486755 and perplexity is 30.298296065946925
At time: 147.82650184631348 and batch: 750, loss is 3.499707646369934 and perplexity is 33.10577195116253
At time: 148.2997589111328 and batch: 800, loss is 3.4629545974731446 and perplexity is 31.91112188543034
At time: 148.7730622291565 and batch: 850, loss is 3.536366858482361 and perplexity is 34.34192319731942
At time: 149.24575686454773 and batch: 900, loss is 3.5019112396240235 and perplexity is 33.17880404386637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320965910611087 and perplexity of 75.26128887208682
finished 16 epochs...
Completing Train Step...
At time: 150.54121899604797 and batch: 50, loss is 3.7402183389663697 and perplexity is 42.10718280017448
At time: 151.0269603729248 and batch: 100, loss is 3.6221782112121583 and perplexity is 37.41898557125421
At time: 151.49979400634766 and batch: 150, loss is 3.624109377861023 and perplexity is 37.4913176884303
At time: 151.9722650051117 and batch: 200, loss is 3.5231169414520265 and perplexity is 33.889896833339655
At time: 152.44381165504456 and batch: 250, loss is 3.6595952129364013 and perplexity is 38.84561548579993
At time: 152.915913105011 and batch: 300, loss is 3.6517404651641847 and perplexity is 38.5416881729113
At time: 153.39414978027344 and batch: 350, loss is 3.6311731910705567 and perplexity is 37.75708692056716
At time: 153.87998485565186 and batch: 400, loss is 3.563504528999329 and perplexity is 35.28664377401143
At time: 154.35201048851013 and batch: 450, loss is 3.5966625928878786 and perplexity is 36.476294830004704
At time: 154.82220029830933 and batch: 500, loss is 3.4885817766189575 and perplexity is 32.73948286517538
At time: 155.29240560531616 and batch: 550, loss is 3.546234703063965 and perplexity is 34.682481482713726
At time: 155.76561212539673 and batch: 600, loss is 3.561587233543396 and perplexity is 35.21905366805513
At time: 156.23706674575806 and batch: 650, loss is 3.4073371076583863 and perplexity is 30.18475838241778
At time: 156.70844793319702 and batch: 700, loss is 3.410285224914551 and perplexity is 30.273877892312907
At time: 157.17988085746765 and batch: 750, loss is 3.499103136062622 and perplexity is 33.08576521853384
At time: 157.6515293121338 and batch: 800, loss is 3.462766628265381 and perplexity is 31.90512414084399
At time: 158.12161684036255 and batch: 850, loss is 3.536626448631287 and perplexity is 34.35083917947711
At time: 158.5933187007904 and batch: 900, loss is 3.5025624561309816 and perplexity is 33.20041766555064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320872267631636 and perplexity of 75.25424151073258
finished 17 epochs...
Completing Train Step...
At time: 159.85919880867004 and batch: 50, loss is 3.737243609428406 and perplexity is 41.98211143870488
At time: 160.33359503746033 and batch: 100, loss is 3.61908983707428 and perplexity is 37.3036000125508
At time: 160.80747437477112 and batch: 150, loss is 3.621066117286682 and perplexity is 37.37739527514718
At time: 161.28133392333984 and batch: 200, loss is 3.520208797454834 and perplexity is 33.79148330301643
At time: 161.75533747673035 and batch: 250, loss is 3.656802649497986 and perplexity is 38.73728796638512
At time: 162.22884607315063 and batch: 300, loss is 3.649096755981445 and perplexity is 38.43992972710658
At time: 162.701824426651 and batch: 350, loss is 3.628497853279114 and perplexity is 37.65620896051213
At time: 163.17472386360168 and batch: 400, loss is 3.5612583541870118 and perplexity is 35.20747275281868
At time: 163.64813661575317 and batch: 450, loss is 3.59456015586853 and perplexity is 36.39968627797235
At time: 164.12138104438782 and batch: 500, loss is 3.4865947771072388 and perplexity is 32.6744941163966
At time: 164.59447145462036 and batch: 550, loss is 3.5445829820632935 and perplexity is 34.62524298372183
At time: 165.06814908981323 and batch: 600, loss is 3.5602052927017214 and perplexity is 35.17041663387851
At time: 165.54215931892395 and batch: 650, loss is 3.4061246585845946 and perplexity is 30.14818307740231
At time: 166.01532173156738 and batch: 700, loss is 3.409384412765503 and perplexity is 30.246619094680042
At time: 166.50309085845947 and batch: 750, loss is 3.498313455581665 and perplexity is 33.059648348891514
At time: 167.00083255767822 and batch: 800, loss is 3.462280993461609 and perplexity is 31.889633663804357
At time: 167.48453617095947 and batch: 850, loss is 3.536417593955994 and perplexity is 34.34366559525862
At time: 167.96634483337402 and batch: 900, loss is 3.5025968170166015 and perplexity is 33.20155848090418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32091281838613 and perplexity of 75.25729318887828
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 169.31679391860962 and batch: 50, loss is 3.7374925088882445 and perplexity is 41.9925620640884
At time: 169.7907316684723 and batch: 100, loss is 3.620766954421997 and perplexity is 37.366215018944324
At time: 170.2645833492279 and batch: 150, loss is 3.6234062814712527 and perplexity is 37.464966942958355
At time: 170.73793148994446 and batch: 200, loss is 3.5217980766296386 and perplexity is 33.84523010173679
At time: 171.21102333068848 and batch: 250, loss is 3.657387585639954 and perplexity is 38.759953434437946
At time: 171.68386363983154 and batch: 300, loss is 3.6491458892822264 and perplexity is 38.44181845413519
At time: 172.17073774337769 and batch: 350, loss is 3.6284527492523195 and perplexity is 37.654510552157014
At time: 172.64308214187622 and batch: 400, loss is 3.5615759658813477 and perplexity is 35.21865683389644
At time: 173.11432194709778 and batch: 450, loss is 3.593276901245117 and perplexity is 36.35300616990345
At time: 173.5859797000885 and batch: 500, loss is 3.485157618522644 and perplexity is 32.627569413876074
At time: 174.05831336975098 and batch: 550, loss is 3.541695647239685 and perplexity is 34.525412505242265
At time: 174.53142070770264 and batch: 600, loss is 3.5578485345840454 and perplexity is 35.08762606596643
At time: 175.00518250465393 and batch: 650, loss is 3.4014670705795287 and perplexity is 30.008091758600962
At time: 175.47932147979736 and batch: 700, loss is 3.4045239877700806 and perplexity is 30.09996436208014
At time: 175.95305633544922 and batch: 750, loss is 3.4916856575012205 and perplexity is 32.841260190772495
At time: 176.42572021484375 and batch: 800, loss is 3.456254014968872 and perplexity is 31.698013552756606
At time: 176.89942860603333 and batch: 850, loss is 3.528331198692322 and perplexity is 34.067068982436446
At time: 177.37379574775696 and batch: 900, loss is 3.4944398021697998 and perplexity is 32.93183444247973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320376879548373 and perplexity of 75.21697068879597
finished 19 epochs...
Completing Train Step...
At time: 178.6285276412964 and batch: 50, loss is 3.734891495704651 and perplexity is 41.88348077897419
At time: 179.1154589653015 and batch: 100, loss is 3.6181960821151735 and perplexity is 37.27027462962854
At time: 179.58980870246887 and batch: 150, loss is 3.620488986968994 and perplexity is 37.35582987076061
At time: 180.06313633918762 and batch: 200, loss is 3.519234676361084 and perplexity is 33.758582333707636
At time: 180.53652787208557 and batch: 250, loss is 3.6548851490020753 and perplexity is 38.66308036679111
At time: 181.01166558265686 and batch: 300, loss is 3.64688090801239 and perplexity is 38.354846986911575
At time: 181.48501706123352 and batch: 350, loss is 3.626124691963196 and perplexity is 37.56695065617152
At time: 181.95834946632385 and batch: 400, loss is 3.5597860431671142 and perplexity is 35.155674543596
At time: 182.43233275413513 and batch: 450, loss is 3.5916639089584352 and perplexity is 36.294416316547455
At time: 182.9061017036438 and batch: 500, loss is 3.4838103103637694 and perplexity is 32.58363962352825
At time: 183.38023614883423 and batch: 550, loss is 3.5405726146698 and perplexity is 34.4866611061246
At time: 183.8539342880249 and batch: 600, loss is 3.5570568895339965 and perplexity is 35.05986011231331
At time: 184.34614992141724 and batch: 650, loss is 3.400994825363159 and perplexity is 29.993923926424372
At time: 184.820148229599 and batch: 700, loss is 3.404431314468384 and perplexity is 30.09717502825214
At time: 185.29373216629028 and batch: 750, loss is 3.491957082748413 and perplexity is 32.85017534778251
At time: 185.76707863807678 and batch: 800, loss is 3.4566264152526855 and perplexity is 31.709820100244354
At time: 186.23976755142212 and batch: 850, loss is 3.5295741367340088 and perplexity is 34.109438564371374
At time: 186.71317434310913 and batch: 900, loss is 3.496028070449829 and perplexity is 32.984180589386234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320066687178938 and perplexity of 75.19364257672451
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f06654d0b70>
ELAPSED
979.1627860069275


RESULTS SO FAR:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.28313217785833, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -73.79736611891688, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.443992016443709, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8989021853099624, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.24543323702392, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.7650501504885647, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.39050037147704286, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.19364257672451, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.3348144614713373, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.11193593374944022, 'seq_len': 35, 'batch_size': 32}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.44369151064185103, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8942889183256518, 'seq_len': 35, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.6965811252593994 and batch: 50, loss is 7.191024961471558 and perplexity is 1327.463107104538
At time: 1.1881530284881592 and batch: 100, loss is 6.4362654209136965 and perplexity is 624.0717969177535
At time: 1.6678705215454102 and batch: 150, loss is 6.34979552268982 and perplexity is 572.3756592118563
At time: 2.1463308334350586 and batch: 200, loss is 6.212548952102662 and perplexity is 498.97148613392125
At time: 2.625006914138794 and batch: 250, loss is 6.262298631668091 and perplexity is 524.4230110607259
At time: 3.1037118434906006 and batch: 300, loss is 6.17091661453247 and perplexity is 478.6246193664463
At time: 3.583235025405884 and batch: 350, loss is 6.178416175842285 and perplexity is 482.22758749704656
At time: 4.06219744682312 and batch: 400, loss is 6.0674264621734615 and perplexity is 431.5685930883137
At time: 4.541079759597778 and batch: 450, loss is 6.071112575531006 and perplexity is 433.162339404209
At time: 5.026318073272705 and batch: 500, loss is 6.045108261108399 and perplexity is 422.0434459008231
At time: 5.510327577590942 and batch: 550, loss is 6.079508657455444 and perplexity is 436.81451642642145
At time: 5.989018201828003 and batch: 600, loss is 6.02165584564209 and perplexity is 412.260670900535
At time: 6.48699951171875 and batch: 650, loss is 5.954402179718017 and perplexity is 385.44641410315313
At time: 6.969192028045654 and batch: 700, loss is 6.061892023086548 and perplexity is 429.18670029810244
At time: 7.448011159896851 and batch: 750, loss is 6.0062778949737545 and perplexity is 405.9694437109592
At time: 7.928098201751709 and batch: 800, loss is 6.026579685211182 and perplexity is 414.29558198131485
At time: 8.406679630279541 and batch: 850, loss is 6.059790887832642 and perplexity is 428.28586770856816
At time: 8.898458480834961 and batch: 900, loss is 5.937787837982178 and perplexity is 379.09538086898493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.86766574807363 and perplexity of 353.42303825655426
finished 1 epochs...
Completing Train Step...
At time: 10.173653841018677 and batch: 50, loss is 5.58657021522522 and perplexity is 266.8189171064395
At time: 10.647921085357666 and batch: 100, loss is 5.324401979446411 and perplexity is 205.2855587659759
At time: 11.122787475585938 and batch: 150, loss is 5.222332878112793 and perplexity is 185.3661166080976
At time: 11.596903085708618 and batch: 200, loss is 5.055303153991699 and perplexity is 156.8520734695385
At time: 12.071747779846191 and batch: 250, loss is 5.102830686569214 and perplexity is 164.4868596666988
At time: 12.54586410522461 and batch: 300, loss is 5.012396049499512 and perplexity is 150.26434596648775
At time: 13.020147323608398 and batch: 350, loss is 4.98227972984314 and perplexity is 145.80640225772066
At time: 13.494247913360596 and batch: 400, loss is 4.834099941253662 and perplexity is 125.72537204542148
At time: 13.968807458877563 and batch: 450, loss is 4.834009571075439 and perplexity is 125.71401073451223
At time: 14.44255018234253 and batch: 500, loss is 4.74432804107666 and perplexity is 114.93055094080093
At time: 14.916809558868408 and batch: 550, loss is 4.807216596603394 and perplexity is 122.39048089685222
At time: 15.39116907119751 and batch: 600, loss is 4.743687238693237 and perplexity is 114.85692676162408
At time: 15.86534571647644 and batch: 650, loss is 4.608150033950806 and perplexity is 100.29842921228658
At time: 16.34055495262146 and batch: 700, loss is 4.657157745361328 and perplexity is 105.33626379404252
At time: 16.81461453437805 and batch: 750, loss is 4.687141523361206 and perplexity is 108.54246982605436
At time: 17.28856325149536 and batch: 800, loss is 4.636483535766602 and perplexity is 103.18087702857846
At time: 17.762415409088135 and batch: 850, loss is 4.683961715698242 and perplexity is 108.19787381362076
At time: 18.23585796356201 and batch: 900, loss is 4.6122943305969235 and perplexity is 100.71495816969887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.707805894825556 and perplexity of 110.80876689958714
finished 2 epochs...
Completing Train Step...
At time: 19.50424551963806 and batch: 50, loss is 4.663160123825073 and perplexity is 105.97043327388944
At time: 19.981428384780884 and batch: 100, loss is 4.52268835067749 and perplexity is 92.08281642624826
At time: 20.45502543449402 and batch: 150, loss is 4.516163892745972 and perplexity is 91.48398162478718
At time: 20.929543018341064 and batch: 200, loss is 4.420605478286743 and perplexity is 83.1466135896253
At time: 21.41706156730652 and batch: 250, loss is 4.549439821243286 and perplexity is 94.5794120951535
At time: 21.89268398284912 and batch: 300, loss is 4.509681320190429 and perplexity is 90.89284817909582
At time: 22.369229078292847 and batch: 350, loss is 4.495731611251831 and perplexity is 89.63372204213388
At time: 22.84603786468506 and batch: 400, loss is 4.399385671615601 and perplexity is 81.4008464510194
At time: 23.322447299957275 and batch: 450, loss is 4.433629703521729 and perplexity is 84.23661662697764
At time: 23.7991361618042 and batch: 500, loss is 4.326959543228149 and perplexity is 75.71373192213477
At time: 24.27334761619568 and batch: 550, loss is 4.40955493927002 and perplexity is 82.23285674343951
At time: 24.749366283416748 and batch: 600, loss is 4.383562602996826 and perplexity is 80.12297188211708
At time: 25.227556467056274 and batch: 650, loss is 4.250171394348144 and perplexity is 70.11742904790633
At time: 25.704542875289917 and batch: 700, loss is 4.278426251411438 and perplexity is 72.1268411283102
At time: 26.18110179901123 and batch: 750, loss is 4.354201536178589 and perplexity is 77.8046763090855
At time: 26.657463550567627 and batch: 800, loss is 4.316702852249145 and perplexity is 74.94112852123439
At time: 27.13496494293213 and batch: 850, loss is 4.376914167404175 and perplexity is 79.59204633187903
At time: 27.611380100250244 and batch: 900, loss is 4.320636992454529 and perplexity is 75.2365381383968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.530930610552226 and perplexity of 92.84492335645325
finished 3 epochs...
Completing Train Step...
At time: 28.868293285369873 and batch: 50, loss is 4.408148784637451 and perplexity is 82.11730589120302
At time: 29.35488533973694 and batch: 100, loss is 4.271017818450928 and perplexity is 71.59446871874164
At time: 29.829028129577637 and batch: 150, loss is 4.269338893890381 and perplexity is 71.47436785516678
At time: 30.303884029388428 and batch: 200, loss is 4.179847083091736 and perplexity is 65.35585843403528
At time: 30.777331113815308 and batch: 250, loss is 4.3200430583953855 and perplexity is 75.1918658634104
At time: 31.25076150894165 and batch: 300, loss is 4.294857282638549 and perplexity is 73.32174941165785
At time: 31.724438905715942 and batch: 350, loss is 4.281735811233521 and perplexity is 72.36594466929306
At time: 32.19875383377075 and batch: 400, loss is 4.201486029624939 and perplexity is 66.78550257209717
At time: 32.673105001449585 and batch: 450, loss is 4.241013717651367 and perplexity is 69.47824747793028
At time: 33.14958739280701 and batch: 500, loss is 4.127815599441528 and perplexity is 62.04224968149088
At time: 33.63611340522766 and batch: 550, loss is 4.208551440238953 and perplexity is 67.25904047076291
At time: 34.11035203933716 and batch: 600, loss is 4.198822784423828 and perplexity is 66.60787304335524
At time: 34.58416700363159 and batch: 650, loss is 4.066865653991699 and perplexity is 58.373712145110346
At time: 35.05860161781311 and batch: 700, loss is 4.079997639656067 and perplexity is 59.145330246396114
At time: 35.53272342681885 and batch: 750, loss is 4.175544352531433 and perplexity is 65.07525390042137
At time: 36.00717639923096 and batch: 800, loss is 4.139678587913513 and perplexity is 62.782639106920556
At time: 36.481311321258545 and batch: 850, loss is 4.209701294898987 and perplexity is 67.33642307272433
At time: 36.95554804801941 and batch: 900, loss is 4.15882134437561 and perplexity is 63.99604882699202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.465703676824701 and perplexity of 86.98221537882237
finished 4 epochs...
Completing Train Step...
At time: 38.21035885810852 and batch: 50, loss is 4.252844586372375 and perplexity is 70.30511715139986
At time: 38.69724249839783 and batch: 100, loss is 4.11867473602295 and perplexity is 61.47771405333409
At time: 39.16870141029358 and batch: 150, loss is 4.119726271629333 and perplexity is 61.54239405936505
At time: 39.66072392463684 and batch: 200, loss is 4.029666757583618 and perplexity is 56.242165848670396
At time: 40.14307236671448 and batch: 250, loss is 4.172405929565429 and perplexity is 64.87134037987363
At time: 40.633331060409546 and batch: 300, loss is 4.160184164047241 and perplexity is 64.0833233574558
At time: 41.106489181518555 and batch: 350, loss is 4.143733043670654 and perplexity is 63.037705267316284
At time: 41.579134464263916 and batch: 400, loss is 4.06783540725708 and perplexity is 58.43034769989414
At time: 42.059285163879395 and batch: 450, loss is 4.113881649971009 and perplexity is 61.18375113716029
At time: 42.55158281326294 and batch: 500, loss is 3.99824188709259 and perplexity is 54.502244651798
At time: 43.02374625205994 and batch: 550, loss is 4.076170144081115 and perplexity is 58.91938443573109
At time: 43.49739480018616 and batch: 600, loss is 4.0750735569000245 and perplexity is 58.85480960647705
At time: 43.9690625667572 and batch: 650, loss is 3.9443953943252565 and perplexity is 51.645103748022855
At time: 44.452184438705444 and batch: 700, loss is 3.9509662055969237 and perplexity is 51.985571326970465
At time: 44.93467998504639 and batch: 750, loss is 4.053079767227173 and perplexity is 57.57450033870656
At time: 45.4236741065979 and batch: 800, loss is 4.017067461013794 and perplexity is 55.53799943719693
At time: 45.89334201812744 and batch: 850, loss is 4.090679740905761 and perplexity is 59.78051315670966
At time: 46.38659167289734 and batch: 900, loss is 4.041666774749756 and perplexity is 56.92113849836552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432848786654538 and perplexity of 84.17086051059042
finished 5 epochs...
Completing Train Step...
At time: 47.651620864868164 and batch: 50, loss is 4.142478699684143 and perplexity is 62.958683871158215
At time: 48.12043857574463 and batch: 100, loss is 4.007711954116822 and perplexity is 55.020836234448815
At time: 48.58913969993591 and batch: 150, loss is 4.014416251182556 and perplexity is 55.39095156060202
At time: 49.058115005493164 and batch: 200, loss is 3.9233689498901367 and perplexity is 50.57052769050042
At time: 49.527584075927734 and batch: 250, loss is 4.066737194061279 and perplexity is 58.36621394372935
At time: 49.99669146537781 and batch: 300, loss is 4.060171904563904 and perplexity is 57.98427798427668
At time: 50.4656081199646 and batch: 350, loss is 4.0427808809280394 and perplexity is 56.98459002974987
At time: 50.935003995895386 and batch: 400, loss is 3.9682915687561033 and perplexity is 52.89408769169467
At time: 51.40403389930725 and batch: 450, loss is 4.016647863388061 and perplexity is 55.51470071288167
At time: 51.87347054481506 and batch: 500, loss is 3.9002157258987427 and perplexity is 49.413107642883496
At time: 52.34278988838196 and batch: 550, loss is 3.9773511409759523 and perplexity is 53.375462732680674
At time: 52.814714431762695 and batch: 600, loss is 3.980483741760254 and perplexity is 53.54292891449772
At time: 53.29085302352905 and batch: 650, loss is 3.8505723381042483 and perplexity is 47.019966850550794
At time: 53.75655555725098 and batch: 700, loss is 3.854620771408081 and perplexity is 47.210709895020386
At time: 54.221882343292236 and batch: 750, loss is 3.963681526184082 and perplexity is 52.6508048985082
At time: 54.68818664550781 and batch: 800, loss is 3.9243262004852295 and perplexity is 50.61895953523411
At time: 55.15339779853821 and batch: 850, loss is 4.000699253082275 and perplexity is 54.63634130896895
At time: 55.61842489242554 and batch: 900, loss is 3.9505588579177857 and perplexity is 51.96439943759452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.417279805222603 and perplexity of 82.87055443212476
finished 6 epochs...
Completing Train Step...
At time: 56.854243993759155 and batch: 50, loss is 4.053739285469055 and perplexity is 57.61248429612678
At time: 57.3185977935791 and batch: 100, loss is 3.9221413373947143 and perplexity is 50.508484768932064
At time: 57.79716181755066 and batch: 150, loss is 3.932402186393738 and perplexity is 51.02941271532973
At time: 58.26338744163513 and batch: 200, loss is 3.8413871049880983 and perplexity is 46.59005493706851
At time: 58.730034828186035 and batch: 250, loss is 3.9851525592803956 and perplexity is 53.793495548888586
At time: 59.198176860809326 and batch: 300, loss is 3.983290266990662 and perplexity is 53.69340956049826
At time: 59.67285370826721 and batch: 350, loss is 3.96307559967041 and perplexity is 52.61891204319313
At time: 60.14004325866699 and batch: 400, loss is 3.891173663139343 and perplexity is 48.96832512855061
At time: 60.6063346862793 and batch: 450, loss is 3.940619721412659 and perplexity is 51.45047638457773
At time: 61.0719268321991 and batch: 500, loss is 3.8225626945495605 and perplexity is 45.72122785556864
At time: 61.53715419769287 and batch: 550, loss is 3.8992489194869995 and perplexity is 49.36535781972716
At time: 62.00222682952881 and batch: 600, loss is 3.9055512142181397 and perplexity is 49.677455286311094
At time: 62.46768236160278 and batch: 650, loss is 3.777078037261963 and perplexity is 43.68819975999696
At time: 62.933305978775024 and batch: 700, loss is 3.779192080497742 and perplexity is 43.7806561972021
At time: 63.399189710617065 and batch: 750, loss is 3.8920973110198975 and perplexity is 49.01357551276361
At time: 63.86495399475098 and batch: 800, loss is 3.8485874223709104 and perplexity is 46.92672874407269
At time: 64.33120226860046 and batch: 850, loss is 3.9272948026657106 and perplexity is 50.769450352049645
At time: 64.79762935638428 and batch: 900, loss is 3.878606791496277 and perplexity is 48.356797017729505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.412484364966824 and perplexity of 82.47410497396494
finished 7 epochs...
Completing Train Step...
At time: 66.02950167655945 and batch: 50, loss is 3.982755255699158 and perplexity is 53.66469066325624
At time: 66.507408618927 and batch: 100, loss is 3.854253468513489 and perplexity is 47.193372448862185
At time: 66.97350072860718 and batch: 150, loss is 3.862510042190552 and perplexity is 47.58464105181711
At time: 67.43923044204712 and batch: 200, loss is 3.774226965904236 and perplexity is 43.563818978485926
At time: 67.90556716918945 and batch: 250, loss is 3.9158446073532103 and perplexity is 50.191445677839134
At time: 68.3711473941803 and batch: 300, loss is 3.9183415603637695 and perplexity is 50.316927955708344
At time: 68.83735394477844 and batch: 350, loss is 3.8988038158416747 and perplexity is 49.34339000834798
At time: 69.30286288261414 and batch: 400, loss is 3.825899767875671 and perplexity is 45.87405780603114
At time: 69.78060412406921 and batch: 450, loss is 3.8755503129959106 and perplexity is 48.209221153415
At time: 70.24599647521973 and batch: 500, loss is 3.7583022594451903 and perplexity is 42.87557255481875
At time: 70.71194052696228 and batch: 550, loss is 3.8371833896636964 and perplexity is 46.394614684562654
At time: 71.22321105003357 and batch: 600, loss is 3.844803876876831 and perplexity is 46.74951479091277
At time: 71.6994194984436 and batch: 650, loss is 3.715503430366516 and perplexity is 41.07926242779846
At time: 72.19880843162537 and batch: 700, loss is 3.7170496559143067 and perplexity is 41.142829364594824
At time: 72.66756010055542 and batch: 750, loss is 3.828276252746582 and perplexity is 45.9832064540601
At time: 73.13538908958435 and batch: 800, loss is 3.7878005599975584 and perplexity is 44.159167946387
At time: 73.60562014579773 and batch: 850, loss is 3.8659424018859863 and perplexity is 47.74824927639941
At time: 74.07579016685486 and batch: 900, loss is 3.8181525230407716 and perplexity is 45.520033376309364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.412944636932791 and perplexity of 82.5120742298244
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 75.3333568572998 and batch: 50, loss is 3.9393739557266234 and perplexity is 51.38642105381831
At time: 75.81804132461548 and batch: 100, loss is 3.7988778066635134 and perplexity is 44.651049257904276
At time: 76.28878426551819 and batch: 150, loss is 3.7986581134796142 and perplexity is 44.64124080419303
At time: 76.76046681404114 and batch: 200, loss is 3.695567560195923 and perplexity is 40.2684208563052
At time: 77.23176503181458 and batch: 250, loss is 3.8325835180282595 and perplexity is 46.18169548835019
At time: 77.70334839820862 and batch: 300, loss is 3.821376976966858 and perplexity is 45.667047519433936
At time: 78.1899254322052 and batch: 350, loss is 3.790257430076599 and perplexity is 44.26779467106349
At time: 78.6755952835083 and batch: 400, loss is 3.710948791503906 and perplexity is 40.892586665749214
At time: 79.15676403045654 and batch: 450, loss is 3.752753691673279 and perplexity is 42.63833331239512
At time: 79.62852835655212 and batch: 500, loss is 3.6230993938446043 and perplexity is 37.4534711722159
At time: 80.10728812217712 and batch: 550, loss is 3.686009588241577 and perplexity is 39.88536993024387
At time: 80.58990120887756 and batch: 600, loss is 3.6950591945648195 and perplexity is 40.247954977638464
At time: 81.07569861412048 and batch: 650, loss is 3.555763421058655 and perplexity is 35.014540604504
At time: 81.5625057220459 and batch: 700, loss is 3.5402993106842042 and perplexity is 34.47723705206884
At time: 82.03178548812866 and batch: 750, loss is 3.6416913652420044 and perplexity is 38.156318449742116
At time: 82.5016565322876 and batch: 800, loss is 3.5850850534439087 and perplexity is 36.05642431150798
At time: 82.97187113761902 and batch: 850, loss is 3.655814538002014 and perplexity is 38.69903011144869
At time: 83.4419641494751 and batch: 900, loss is 3.6014745664596557 and perplexity is 36.652240780852246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3384942877782535 and perplexity of 76.59212676967637
finished 9 epochs...
Completing Train Step...
At time: 84.69602799415588 and batch: 50, loss is 3.8533908462524415 and perplexity is 47.15267994886541
At time: 85.1707923412323 and batch: 100, loss is 3.7268126964569093 and perplexity is 41.546475677186216
At time: 85.65710377693176 and batch: 150, loss is 3.727775368690491 and perplexity is 41.58649057324822
At time: 86.14358639717102 and batch: 200, loss is 3.6311488199234008 and perplexity is 37.756166748258515
At time: 86.61880683898926 and batch: 250, loss is 3.770470151901245 and perplexity is 43.4004648509875
At time: 87.0936918258667 and batch: 300, loss is 3.7655168104171755 and perplexity is 43.18601907804745
At time: 87.5688967704773 and batch: 350, loss is 3.73840615272522 and perplexity is 42.03094584149785
At time: 88.04379963874817 and batch: 400, loss is 3.6623353815078734 and perplexity is 38.952204990404745
At time: 88.51848196983337 and batch: 450, loss is 3.7081781959533693 and perplexity is 40.77944665206616
At time: 88.99121403694153 and batch: 500, loss is 3.582415027618408 and perplexity is 35.960281136917956
At time: 89.46002411842346 and batch: 550, loss is 3.6487978887557984 and perplexity is 38.42844300854234
At time: 89.93316555023193 and batch: 600, loss is 3.663380813598633 and perplexity is 38.992948168909926
At time: 90.40815687179565 and batch: 650, loss is 3.527457175254822 and perplexity is 34.037306574114744
At time: 90.88307404518127 and batch: 700, loss is 3.5161753606796267 and perplexity is 33.655461992657386
At time: 91.35868835449219 and batch: 750, loss is 3.6222992372512817 and perplexity is 37.42351451692057
At time: 91.8339695930481 and batch: 800, loss is 3.5690938091278075 and perplexity is 35.484422917490825
At time: 92.30935859680176 and batch: 850, loss is 3.6463500308990477 and perplexity is 38.33449068028721
At time: 92.78459978103638 and batch: 900, loss is 3.59974259853363 and perplexity is 36.588815216779054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3364324700342465 and perplexity of 76.43437045192539
finished 10 epochs...
Completing Train Step...
At time: 94.05859112739563 and batch: 50, loss is 3.8184257984161376 and perplexity is 45.53247458037688
At time: 94.53221082687378 and batch: 100, loss is 3.6941055774688722 and perplexity is 40.209592134333704
At time: 95.00517082214355 and batch: 150, loss is 3.6940049219131468 and perplexity is 40.205545019177656
At time: 95.4786970615387 and batch: 200, loss is 3.5992675542831423 and perplexity is 36.571438038269314
At time: 95.95334792137146 and batch: 250, loss is 3.7381356382369995 and perplexity is 42.01957739942758
At time: 96.42804169654846 and batch: 300, loss is 3.736234817504883 and perplexity is 41.93978157835348
At time: 96.90244698524475 and batch: 350, loss is 3.709665660858154 and perplexity is 40.84014978349406
At time: 97.37667441368103 and batch: 400, loss is 3.6349497413635254 and perplexity is 37.89994804956359
At time: 97.8500874042511 and batch: 450, loss is 3.681535630226135 and perplexity is 39.707323043879335
At time: 98.3231737613678 and batch: 500, loss is 3.557742323875427 and perplexity is 35.083899582237976
At time: 98.79763317108154 and batch: 550, loss is 3.625760178565979 and perplexity is 37.55325949482222
At time: 99.27143359184265 and batch: 600, loss is 3.6427825117111206 and perplexity is 38.19797530462503
At time: 99.7455050945282 and batch: 650, loss is 3.5079747104644774 and perplexity is 33.380593909947194
At time: 100.21982049942017 and batch: 700, loss is 3.4982299041748046 and perplexity is 33.056886284150465
At time: 100.69399833679199 and batch: 750, loss is 3.6065976810455322 and perplexity is 36.840496225558404
At time: 101.16824507713318 and batch: 800, loss is 3.5542615985870363 and perplexity is 34.96199444796646
At time: 101.64254832267761 and batch: 850, loss is 3.634382100105286 and perplexity is 37.87844058020651
At time: 102.1165885925293 and batch: 900, loss is 3.59170382976532 and perplexity is 36.29586524785328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337052854773116 and perplexity of 76.48180388084525
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 103.36210298538208 and batch: 50, loss is 3.811275963783264 and perplexity is 45.208085961091996
At time: 103.85836291313171 and batch: 100, loss is 3.687727074623108 and perplexity is 39.95393136973738
At time: 104.3303210735321 and batch: 150, loss is 3.6880883502960207 and perplexity is 39.96836836088832
At time: 104.80276370048523 and batch: 200, loss is 3.5819000482559202 and perplexity is 35.941767101845336
At time: 105.2754282951355 and batch: 250, loss is 3.7194458484649657 and perplexity is 41.241533715919104
At time: 105.76152491569519 and batch: 300, loss is 3.7160103034973146 and perplexity is 41.10008968009751
At time: 106.23434829711914 and batch: 350, loss is 3.687516646385193 and perplexity is 39.94552481888034
At time: 106.7039430141449 and batch: 400, loss is 3.6106683921813967 and perplexity is 36.99076889459811
At time: 107.17334079742432 and batch: 450, loss is 3.649733905792236 and perplexity is 38.46442952525108
At time: 107.6419358253479 and batch: 500, loss is 3.5239727115631103 and perplexity is 33.91891120716359
At time: 108.11300897598267 and batch: 550, loss is 3.5886601257324218 and perplexity is 36.18555933086045
At time: 108.5854721069336 and batch: 600, loss is 3.604004855155945 and perplexity is 36.74509896085269
At time: 109.058664560318 and batch: 650, loss is 3.463490629196167 and perplexity is 31.928231844408874
At time: 109.52929520606995 and batch: 700, loss is 3.452700123786926 and perplexity is 31.585562200197757
At time: 110.00130462646484 and batch: 750, loss is 3.5540851402282714 and perplexity is 34.955825656090475
At time: 110.47361540794373 and batch: 800, loss is 3.4997452688217163 and perplexity is 33.107017494901534
At time: 110.94487476348877 and batch: 850, loss is 3.574521245956421 and perplexity is 35.67753596165415
At time: 111.4168210029602 and batch: 900, loss is 3.535842776298523 and perplexity is 34.32392992260217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3163473050888275 and perplexity of 74.9144881520321
finished 12 epochs...
Completing Train Step...
At time: 112.67120862007141 and batch: 50, loss is 3.7881868743896483 and perplexity is 44.17623056406259
At time: 113.15667366981506 and batch: 100, loss is 3.6631443166732787 and perplexity is 38.983727546924825
At time: 113.6291892528534 and batch: 150, loss is 3.665498876571655 and perplexity is 39.07562521532208
At time: 114.10160160064697 and batch: 200, loss is 3.5627367305755615 and perplexity is 35.259561142872926
At time: 114.57383418083191 and batch: 250, loss is 3.7018085145950317 and perplexity is 40.52052008623123
At time: 115.04620575904846 and batch: 300, loss is 3.698815484046936 and perplexity is 40.39942224695641
At time: 115.51782584190369 and batch: 350, loss is 3.672271828651428 and perplexity is 39.34118083498657
At time: 115.98997902870178 and batch: 400, loss is 3.597222080230713 and perplexity is 36.496708565356364
At time: 116.4625997543335 and batch: 450, loss is 3.6379916524887084 and perplexity is 38.01541184945379
At time: 116.93501734733582 and batch: 500, loss is 3.51350435256958 and perplexity is 33.565687927728185
At time: 117.41075420379639 and batch: 550, loss is 3.5791565942764283 and perplexity is 35.843297652821875
At time: 117.89734625816345 and batch: 600, loss is 3.5963451290130615 and perplexity is 36.4647167620154
At time: 118.37072420120239 and batch: 650, loss is 3.457544040679932 and perplexity is 31.738931191958013
At time: 118.84517502784729 and batch: 700, loss is 3.448365345001221 and perplexity is 31.448942098125585
At time: 119.31745195388794 and batch: 750, loss is 3.551119341850281 and perplexity is 34.85230730821103
At time: 119.79061150550842 and batch: 800, loss is 3.4984922647476195 and perplexity is 33.065560245574446
At time: 120.26156163215637 and batch: 850, loss is 3.575409297943115 and perplexity is 35.70923354081117
At time: 120.73327994346619 and batch: 900, loss is 3.538725485801697 and perplexity is 34.423018594885946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315169242963399 and perplexity of 74.82628619478771
finished 13 epochs...
Completing Train Step...
At time: 121.99325823783875 and batch: 50, loss is 3.7774245882034303 and perplexity is 43.70334257048088
At time: 122.46603465080261 and batch: 100, loss is 3.6525640726089477 and perplexity is 38.57344446979041
At time: 122.9388108253479 and batch: 150, loss is 3.654655728340149 and perplexity is 38.654211274716644
At time: 123.4114294052124 and batch: 200, loss is 3.552367868423462 and perplexity is 34.89584851554313
At time: 123.88449573516846 and batch: 250, loss is 3.6916351747512817 and perplexity is 40.110380845027485
At time: 124.35578465461731 and batch: 300, loss is 3.6891105508804323 and perplexity is 40.00924493885643
At time: 124.82744550704956 and batch: 350, loss is 3.66330108165741 and perplexity is 38.98983930939775
At time: 125.3008770942688 and batch: 400, loss is 3.588993577957153 and perplexity is 36.19762749808926
At time: 125.77314233779907 and batch: 450, loss is 3.630213837623596 and perplexity is 37.72088189856586
At time: 126.24545288085938 and batch: 500, loss is 3.506491947174072 and perplexity is 33.33113506762512
At time: 126.71773481369019 and batch: 550, loss is 3.572746686935425 and perplexity is 35.614280210498094
At time: 127.19115900993347 and batch: 600, loss is 3.59086386680603 and perplexity is 36.265390865937455
At time: 127.66375708580017 and batch: 650, loss is 3.452878952026367 and perplexity is 31.591211095753742
At time: 128.13663983345032 and batch: 700, loss is 3.4445855045318603 and perplexity is 31.330294489841954
At time: 128.61355209350586 and batch: 750, loss is 3.5478904247283936 and perplexity is 34.73995358445321
At time: 129.08564448356628 and batch: 800, loss is 3.4961081171035766 and perplexity is 32.98682096834437
At time: 129.55810546875 and batch: 850, loss is 3.5741608572006225 and perplexity is 35.664680495480845
At time: 130.0448784828186 and batch: 900, loss is 3.5384529876708983 and perplexity is 34.41363966459091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314938897955908 and perplexity of 74.80905231827855
finished 14 epochs...
Completing Train Step...
At time: 131.3068699836731 and batch: 50, loss is 3.7685295581817626 and perplexity is 43.31632384963003
At time: 131.77954578399658 and batch: 100, loss is 3.643812184333801 and perplexity is 38.23732697022759
At time: 132.25284671783447 and batch: 150, loss is 3.6459207010269163 and perplexity is 38.318036070785666
At time: 132.724285364151 and batch: 200, loss is 3.54394784450531 and perplexity is 34.60325817387684
At time: 133.19646430015564 and batch: 250, loss is 3.683357515335083 and perplexity is 39.77973116405865
At time: 133.6692817211151 and batch: 300, loss is 3.6812763929367067 and perplexity is 39.697030759212616
At time: 134.14205312728882 and batch: 350, loss is 3.6559516906738283 and perplexity is 38.70433815082267
At time: 134.6142373085022 and batch: 400, loss is 3.5822167015075683 and perplexity is 35.95314998138591
At time: 135.086895942688 and batch: 450, loss is 3.6236320209503172 and perplexity is 37.473425219726806
At time: 135.56191086769104 and batch: 500, loss is 3.500472912788391 and perplexity is 33.13111638307829
At time: 136.03443336486816 and batch: 550, loss is 3.5671540117263794 and perplexity is 35.415657043618786
At time: 136.50646877288818 and batch: 600, loss is 3.5859450674057007 and perplexity is 36.08744667775015
At time: 136.97860646247864 and batch: 650, loss is 3.448497271537781 and perplexity is 31.453091321825404
At time: 137.4518961906433 and batch: 700, loss is 3.440784983634949 and perplexity is 31.211449031301143
At time: 137.92372465133667 and batch: 750, loss is 3.544424319267273 and perplexity is 34.619749681660004
At time: 138.40116262435913 and batch: 800, loss is 3.493194317817688 and perplexity is 32.890843889840056
At time: 138.8729124069214 and batch: 850, loss is 3.5720597171783446 and perplexity is 35.58982267882744
At time: 139.3449683189392 and batch: 900, loss is 3.5370223236083986 and perplexity is 34.36444050918647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31506807510167 and perplexity of 74.81871656232049
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 140.60942673683167 and batch: 50, loss is 3.768725233078003 and perplexity is 43.324800596121015
At time: 141.0977749824524 and batch: 100, loss is 3.6474143409729005 and perplexity is 38.375312184409886
At time: 141.57343864440918 and batch: 150, loss is 3.649130172729492 and perplexity is 38.44121428601598
At time: 142.0593798160553 and batch: 200, loss is 3.54285840511322 and perplexity is 34.56558054879861
At time: 142.5323679447174 and batch: 250, loss is 3.6811249828338624 and perplexity is 39.69102068270738
At time: 143.00613164901733 and batch: 300, loss is 3.6797568798065186 and perplexity is 39.63675640517808
At time: 143.47843170166016 and batch: 350, loss is 3.65032772064209 and perplexity is 38.48727705762495
At time: 143.9508512020111 and batch: 400, loss is 3.578920283317566 and perplexity is 35.83482848950221
At time: 144.42332696914673 and batch: 450, loss is 3.61665593624115 and perplexity is 37.21291715071414
At time: 144.89633917808533 and batch: 500, loss is 3.4936091470718384 and perplexity is 32.90449080445326
At time: 145.36975717544556 and batch: 550, loss is 3.5578528833389282 and perplexity is 35.087778653783396
At time: 145.84168791770935 and batch: 600, loss is 3.5774611377716066 and perplexity is 35.782578388643
At time: 146.31480813026428 and batch: 650, loss is 3.4356675338745117 and perplexity is 31.052133999949756
At time: 146.78765058517456 and batch: 700, loss is 3.4274133586883546 and perplexity is 30.796879154354166
At time: 147.26065731048584 and batch: 750, loss is 3.528883023262024 and perplexity is 34.08587321596055
At time: 147.73279738426208 and batch: 800, loss is 3.476881866455078 and perplexity is 32.358665962707036
At time: 148.20516633987427 and batch: 850, loss is 3.553181109428406 and perplexity is 34.92423879296246
At time: 148.67910599708557 and batch: 900, loss is 3.5168948698043825 and perplexity is 33.679686118353665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311211154885488 and perplexity of 74.53070252248618
finished 16 epochs...
Completing Train Step...
At time: 149.91665053367615 and batch: 50, loss is 3.7619016695022585 and perplexity is 43.030177398070485
At time: 150.40049982070923 and batch: 100, loss is 3.6374838542938233 and perplexity is 37.996112592417475
At time: 150.8718740940094 and batch: 150, loss is 3.6408658981323243 and perplexity is 38.12483466003956
At time: 151.34424686431885 and batch: 200, loss is 3.535652256011963 and perplexity is 34.31739114054254
At time: 151.8166961669922 and batch: 250, loss is 3.675437250137329 and perplexity is 39.46590955942712
At time: 152.28870701789856 and batch: 300, loss is 3.6737098360061644 and perplexity is 39.39779443800427
At time: 152.76657843589783 and batch: 350, loss is 3.64479416847229 and perplexity is 38.27489386083713
At time: 153.24117255210876 and batch: 400, loss is 3.5741609477996827 and perplexity is 35.66468372666753
At time: 153.71366429328918 and batch: 450, loss is 3.6128526067733766 and perplexity is 37.07165297372243
At time: 154.1992633342743 and batch: 500, loss is 3.4901290464401247 and perplexity is 32.79017888907316
At time: 154.67203330993652 and batch: 550, loss is 3.5548539113998414 and perplexity is 34.98270901938663
At time: 155.14557766914368 and batch: 600, loss is 3.575481028556824 and perplexity is 35.71179507791734
At time: 155.61756014823914 and batch: 650, loss is 3.4341798305511473 and perplexity is 31.00597198320738
At time: 156.0900707244873 and batch: 700, loss is 3.426538028717041 and perplexity is 30.76993351788952
At time: 156.56290674209595 and batch: 750, loss is 3.5282938480377197 and perplexity is 34.06579657887227
At time: 157.03644561767578 and batch: 800, loss is 3.4773479509353638 and perplexity is 32.373751349974356
At time: 157.5097212791443 and batch: 850, loss is 3.554700722694397 and perplexity is 34.97735047392382
At time: 157.98311948776245 and batch: 900, loss is 3.5191146612167357 and perplexity is 33.75453103568928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310839927359803 and perplexity of 74.50303980909922
finished 17 epochs...
Completing Train Step...
At time: 159.24088883399963 and batch: 50, loss is 3.758926982879639 and perplexity is 42.90236629822743
At time: 159.71340775489807 and batch: 100, loss is 3.6341364192962646 and perplexity is 37.86913571734023
At time: 160.18635821342468 and batch: 150, loss is 3.6374933528900146 and perplexity is 37.9964735038619
At time: 160.65868043899536 and batch: 200, loss is 3.53246461391449 and perplexity is 34.20817374533126
At time: 161.13265585899353 and batch: 250, loss is 3.6724558734893797 and perplexity is 39.348422042571165
At time: 161.60513401031494 and batch: 300, loss is 3.670731773376465 and perplexity is 39.28063987209876
At time: 162.07734632492065 and batch: 350, loss is 3.641963448524475 and perplexity is 38.16670155858403
At time: 162.54977416992188 and batch: 400, loss is 3.5717282485961914 and perplexity is 35.57802772570119
At time: 163.0234832763672 and batch: 450, loss is 3.6106892013549805 and perplexity is 36.991538649938
At time: 163.49618101119995 and batch: 500, loss is 3.4882105350494386 and perplexity is 32.72733086397484
At time: 163.96875977516174 and batch: 550, loss is 3.553091468811035 and perplexity is 34.92110830294749
At time: 164.44005918502808 and batch: 600, loss is 3.574225549697876 and perplexity is 35.66698780735792
At time: 164.90982270240784 and batch: 650, loss is 3.433131289482117 and perplexity is 30.973477986819734
At time: 165.37885689735413 and batch: 700, loss is 3.425840487480164 and perplexity is 30.748477704432116
At time: 165.84786462783813 and batch: 750, loss is 3.527735123634338 and perplexity is 34.04676850322314
At time: 166.32941102981567 and batch: 800, loss is 3.4772300338745117 and perplexity is 32.3699341574269
At time: 166.79871821403503 and batch: 850, loss is 3.5549880933761595 and perplexity is 34.987403383362285
At time: 167.4251823425293 and batch: 900, loss is 3.5196868324279786 and perplexity is 33.77384993292828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310747120478382 and perplexity of 74.49612573515995
finished 18 epochs...
Completing Train Step...
At time: 168.6745629310608 and batch: 50, loss is 3.7563464546203615 and perplexity is 42.79179825294491
At time: 169.14340686798096 and batch: 100, loss is 3.6314721584320067 and perplexity is 37.76837674478023
At time: 169.61115670204163 and batch: 150, loss is 3.6347907066345213 and perplexity is 37.89392112085445
At time: 170.0795497894287 and batch: 200, loss is 3.5299058628082274 and perplexity is 34.1207554314663
At time: 170.54766178131104 and batch: 250, loss is 3.6699949741363525 and perplexity is 39.25170858607348
At time: 171.01565408706665 and batch: 300, loss is 3.668369288444519 and perplexity is 39.187949485216244
At time: 171.48318791389465 and batch: 350, loss is 3.639693808555603 and perplexity is 38.08017511632591
At time: 171.96439385414124 and batch: 400, loss is 3.569739465713501 and perplexity is 35.50734106666785
At time: 172.43394994735718 and batch: 450, loss is 3.608841953277588 and perplexity is 36.92326917602746
At time: 172.902907371521 and batch: 500, loss is 3.4865724992752076 and perplexity is 32.67376620761312
At time: 173.3718204498291 and batch: 550, loss is 3.5515721654891967 and perplexity is 34.86809283059006
At time: 173.8401861190796 and batch: 600, loss is 3.5730773258209227 and perplexity is 35.62605762334273
At time: 174.30875444412231 and batch: 650, loss is 3.4321020793914796 and perplexity is 30.94161616979789
At time: 174.77791595458984 and batch: 700, loss is 3.425066142082214 and perplexity is 30.72467697840833
At time: 175.24688243865967 and batch: 750, loss is 3.5270445346832275 and perplexity is 34.02326429788017
At time: 175.71530938148499 and batch: 800, loss is 3.4768177461624146 and perplexity is 32.35659118209379
At time: 176.18429446220398 and batch: 850, loss is 3.5548415279388426 and perplexity is 34.98227581505614
At time: 176.65689873695374 and batch: 900, loss is 3.5197506189346313 and perplexity is 33.776004317541314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310741267792166 and perplexity of 74.49568973398763
finished 19 epochs...
Completing Train Step...
At time: 177.93243193626404 and batch: 50, loss is 3.75398530960083 and perplexity is 42.69087980005945
At time: 178.4175820350647 and batch: 100, loss is 3.62909321308136 and perplexity is 37.67863462864046
At time: 178.88977694511414 and batch: 150, loss is 3.6323908615112304 and perplexity is 37.803090612219805
At time: 179.36221265792847 and batch: 200, loss is 3.527603478431702 and perplexity is 34.04228670449454
At time: 179.83571910858154 and batch: 250, loss is 3.6677780246734617 and perplexity is 39.164785918987185
At time: 180.30832529067993 and batch: 300, loss is 3.666270418167114 and perplexity is 39.10578531893537
At time: 180.78112268447876 and batch: 350, loss is 3.6376636266708373 and perplexity is 38.00294385791352
At time: 181.25428414344788 and batch: 400, loss is 3.5679384994506838 and perplexity is 35.443451092437314
At time: 181.727068901062 and batch: 450, loss is 3.6071321249008177 and perplexity is 36.86019066470904
At time: 182.20035362243652 and batch: 500, loss is 3.485045018196106 and perplexity is 32.623895745737926
At time: 182.67301487922668 and batch: 550, loss is 3.5501480436325075 and perplexity is 34.81847175910165
At time: 183.14618277549744 and batch: 600, loss is 3.5719557714462282 and perplexity is 35.58612346091548
At time: 183.61834454536438 and batch: 650, loss is 3.4310578155517577 and perplexity is 30.909321823732842
At time: 184.09029507637024 and batch: 700, loss is 3.4242319679260254 and perplexity is 30.699057933773737
At time: 184.5628297328949 and batch: 750, loss is 3.526269612312317 and perplexity is 33.99690912217083
At time: 185.0331950187683 and batch: 800, loss is 3.476251368522644 and perplexity is 32.3382703211018
At time: 185.50385642051697 and batch: 850, loss is 3.5544926357269286 and perplexity is 34.97007290034398
At time: 185.97243762016296 and batch: 900, loss is 3.5195870542526246 and perplexity is 33.770480207922596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310778892203553 and perplexity of 74.49849264319327
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f06654d0b70>
ELAPSED
1172.0622954368591


RESULTS SO FAR:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.28313217785833, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -73.79736611891688, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.443992016443709, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8989021853099624, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.24543323702392, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.7650501504885647, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.39050037147704286, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.19364257672451, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.3348144614713373, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.11193593374944022, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.49568973398763, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.44369151064185103, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8942889183256518, 'seq_len': 35, 'batch_size': 32}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -75.75251077531675, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.4381406884947143, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.9869426693327623, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.28313217785833, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.9393984731226613, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.7894813749263431, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -73.79736611891688, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.443992016443709, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8989021853099624, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.24543323702392, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.7650501504885647, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.39050037147704286, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -75.19364257672451, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.3348144614713373, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.11193593374944022, 'seq_len': 35, 'batch_size': 32}}, {'best_accuracy': -74.49568973398763, 'params': {'tune_wordvecs': 'FALSE', 'tie_weights': 'FALSE', 'data': 'ptb', 'rnn_dropout': 0.44369151064185103, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'num_layers': 1, 'dropout': 0.8942889183256518, 'seq_len': 35, 'batch_size': 32}}]
