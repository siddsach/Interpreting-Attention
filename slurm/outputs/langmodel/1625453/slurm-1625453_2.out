Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'rnn_dropout', 'domain': [0, 1], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3053061962127686 and batch: 50, loss is 6.727530603408813 and perplexity is 835.082567677914
At time: 2.12607479095459 and batch: 100, loss is 5.835754489898681 and perplexity is 342.322915822332
At time: 2.957090377807617 and batch: 150, loss is 5.647401657104492 and perplexity is 283.5537379634733
At time: 3.7742607593536377 and batch: 200, loss is 5.435963125228882 and perplexity is 229.51379238406818
At time: 4.594652414321899 and batch: 250, loss is 5.472459421157837 and perplexity is 238.04492614146903
At time: 5.410399675369263 and batch: 300, loss is 5.387317562103272 and perplexity is 218.6161740167992
At time: 6.224557399749756 and batch: 350, loss is 5.351134729385376 and perplexity is 210.84741693312577
At time: 7.041139125823975 and batch: 400, loss is 5.187915201187134 and perplexity is 179.09478685464796
At time: 7.857531785964966 and batch: 450, loss is 5.198357133865357 and perplexity is 180.9746803416336
At time: 8.675992488861084 and batch: 500, loss is 5.128489904403686 and perplexity is 168.76207865462177
At time: 9.496008396148682 and batch: 550, loss is 5.186307353973389 and perplexity is 178.80706117206097
At time: 10.314651489257812 and batch: 600, loss is 5.10756085395813 and perplexity is 165.26675310531238
At time: 11.133031129837036 and batch: 650, loss is 4.993648786544799 and perplexity is 147.4735424609962
At time: 11.950399398803711 and batch: 700, loss is 5.077273740768432 and perplexity is 160.33634110758035
At time: 12.76852536201477 and batch: 750, loss is 5.07512038230896 and perplexity is 159.99145096042338
At time: 13.584702968597412 and batch: 800, loss is 5.020350847244263 and perplexity is 151.46443535272462
At time: 14.406191110610962 and batch: 850, loss is 5.065087242126465 and perplexity is 158.3942601229878
At time: 15.222635269165039 and batch: 900, loss is 4.988935375213623 and perplexity is 146.78007457845354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.929551634069991 and perplexity of 138.31748158849314
finished 1 epochs...
Completing Train Step...
At time: 17.137346267700195 and batch: 50, loss is 4.892478456497193 and perplexity is 133.28350238834162
At time: 17.901957750320435 and batch: 100, loss is 4.744421396255493 and perplexity is 114.9412808037738
At time: 18.64980387687683 and batch: 150, loss is 4.719455585479737 and perplexity is 112.10720326194364
At time: 19.40537452697754 and batch: 200, loss is 4.594847936630249 and perplexity is 98.97308422261773
At time: 20.155452728271484 and batch: 250, loss is 4.712219457626343 and perplexity is 111.29890919242646
At time: 20.90425992012024 and batch: 300, loss is 4.669627571105957 and perplexity is 106.65801250898483
At time: 21.655654191970825 and batch: 350, loss is 4.64584753036499 and perplexity is 104.15160003312198
At time: 22.409186124801636 and batch: 400, loss is 4.530140533447265 and perplexity is 92.77159767848144
At time: 23.165518760681152 and batch: 450, loss is 4.544529685974121 and perplexity is 94.11615265209622
At time: 23.916071891784668 and batch: 500, loss is 4.437524318695068 and perplexity is 84.56532551406481
At time: 24.667793035507202 and batch: 550, loss is 4.517845497131348 and perplexity is 91.63795091090968
At time: 25.4199960231781 and batch: 600, loss is 4.488394441604615 and perplexity is 88.97847100099047
At time: 26.169758081436157 and batch: 650, loss is 4.342687206268311 and perplexity is 76.91394552262207
At time: 26.919053077697754 and batch: 700, loss is 4.378546295166015 and perplexity is 79.72205678829597
At time: 27.668967723846436 and batch: 750, loss is 4.445173778533936 and perplexity is 85.21468503373382
At time: 28.440316677093506 and batch: 800, loss is 4.381017026901245 and perplexity is 79.91927213678535
At time: 29.18826675415039 and batch: 850, loss is 4.454297723770142 and perplexity is 85.99573687209833
At time: 29.937241554260254 and batch: 900, loss is 4.394107971191406 and perplexity is 80.97236885195005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.53019777062821 and perplexity of 92.7769078151715
finished 2 epochs...
Completing Train Step...
At time: 31.786375761032104 and batch: 50, loss is 4.423903017044068 and perplexity is 83.42124532601815
At time: 32.552016496658325 and batch: 100, loss is 4.280371322631836 and perplexity is 72.26726949852421
At time: 33.30255436897278 and batch: 150, loss is 4.270830783843994 and perplexity is 71.58107932760498
At time: 34.056349992752075 and batch: 200, loss is 4.157692499160767 and perplexity is 63.92384795297743
At time: 34.80887317657471 and batch: 250, loss is 4.311864881515503 and perplexity is 74.57944115661468
At time: 35.55970549583435 and batch: 300, loss is 4.285065422058105 and perplexity is 72.60729668315898
At time: 36.3097710609436 and batch: 350, loss is 4.27103211402893 and perplexity is 71.59549221036941
At time: 37.0598258972168 and batch: 400, loss is 4.189230589866638 and perplexity is 65.97201189348156
At time: 37.81146454811096 and batch: 450, loss is 4.207457957267761 and perplexity is 67.18553405168252
At time: 38.56516218185425 and batch: 500, loss is 4.082543044090271 and perplexity is 59.29607079871451
At time: 39.31923460960388 and batch: 550, loss is 4.1754391527175905 and perplexity is 65.06840835590661
At time: 40.08345055580139 and batch: 600, loss is 4.178498473167419 and perplexity is 65.26777828097283
At time: 40.837401390075684 and batch: 650, loss is 4.02160080909729 and perplexity is 55.79034407137238
At time: 41.58669447898865 and batch: 700, loss is 4.04212254524231 and perplexity is 56.94708738661525
At time: 42.3348753452301 and batch: 750, loss is 4.138561415672302 and perplexity is 62.712539249378615
At time: 43.08384346961975 and batch: 800, loss is 4.082322058677673 and perplexity is 59.2829686797855
At time: 43.83262348175049 and batch: 850, loss is 4.163032388687133 and perplexity is 64.26610723925894
At time: 44.58484888076782 and batch: 900, loss is 4.112948803901673 and perplexity is 61.12670272823374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.416795704462757 and perplexity of 82.83044644269269
finished 3 epochs...
Completing Train Step...
At time: 46.46048641204834 and batch: 50, loss is 4.1705679607391355 and perplexity is 64.75221838332774
At time: 47.228190183639526 and batch: 100, loss is 4.0291402673721315 and perplexity is 56.21256269244808
At time: 47.98601412773132 and batch: 150, loss is 4.022459130287171 and perplexity is 55.83825066255964
At time: 48.7469801902771 and batch: 200, loss is 3.9147471141815187 and perplexity is 50.13639112545409
At time: 49.500314712524414 and batch: 250, loss is 4.0731488227844235 and perplexity is 58.741638693456615
At time: 50.253926515579224 and batch: 300, loss is 4.04967188835144 and perplexity is 57.37862736048031
At time: 51.008912563323975 and batch: 350, loss is 4.0412442588806154 and perplexity is 56.897093494116056
At time: 51.761027574539185 and batch: 400, loss is 3.970655469894409 and perplexity is 53.01927198915589
At time: 52.51214623451233 and batch: 450, loss is 3.993254795074463 and perplexity is 54.23111358226484
At time: 53.26545858383179 and batch: 500, loss is 3.864175319671631 and perplexity is 47.66394869929798
At time: 54.02105236053467 and batch: 550, loss is 3.955930848121643 and perplexity is 52.24430282833196
At time: 54.77532410621643 and batch: 600, loss is 3.9727600765228273 and perplexity is 53.130974203798
At time: 55.529054164886475 and batch: 650, loss is 3.813593277931213 and perplexity is 45.31296877457054
At time: 56.288084506988525 and batch: 700, loss is 3.8279771280288695 and perplexity is 45.969453797392326
At time: 57.0436053276062 and batch: 750, loss is 3.9351918077468873 and perplexity is 51.171964194559806
At time: 57.79575777053833 and batch: 800, loss is 3.87927264213562 and perplexity is 48.389006143983984
At time: 58.55490303039551 and batch: 850, loss is 3.9643002128601075 and perplexity is 52.683389328717965
At time: 59.306753635406494 and batch: 900, loss is 3.9202040576934816 and perplexity is 50.41073042597445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38232881728917 and perplexity of 80.02417826216433
finished 4 epochs...
Completing Train Step...
At time: 61.1904296875 and batch: 50, loss is 3.985800070762634 and perplexity is 53.828338734390194
At time: 61.94139385223389 and batch: 100, loss is 3.8482613515853883 and perplexity is 46.91142980317351
At time: 62.69168257713318 and batch: 150, loss is 3.840252685546875 and perplexity is 46.53723224019219
At time: 63.44233155250549 and batch: 200, loss is 3.739540972709656 and perplexity is 42.07867047311466
At time: 64.19449877738953 and batch: 250, loss is 3.8979409313201905 and perplexity is 49.30083072538618
At time: 64.94644403457642 and batch: 300, loss is 3.8758202600479126 and perplexity is 48.22223684723973
At time: 65.70933890342712 and batch: 350, loss is 3.8668764352798464 and perplexity is 47.792868570433356
At time: 66.46029758453369 and batch: 400, loss is 3.8007208585739134 and perplexity is 44.73341934239471
At time: 67.22304582595825 and batch: 450, loss is 3.8277861642837525 and perplexity is 45.96067613646837
At time: 67.97667503356934 and batch: 500, loss is 3.702141442298889 and perplexity is 40.53401273585645
At time: 68.72996091842651 and batch: 550, loss is 3.7893836641311647 and perplexity is 44.22913187317832
At time: 69.48060512542725 and batch: 600, loss is 3.8112807512283324 and perplexity is 45.20830239283826
At time: 70.23253440856934 and batch: 650, loss is 3.6516843366622926 and perplexity is 38.53952494640367
At time: 70.98348379135132 and batch: 700, loss is 3.6658492946624754 and perplexity is 39.089320420691365
At time: 71.73699998855591 and batch: 750, loss is 3.776406989097595 and perplexity is 43.65889270806495
At time: 72.49156355857849 and batch: 800, loss is 3.7221993684768675 and perplexity is 41.35524959194931
At time: 73.24411273002625 and batch: 850, loss is 3.8086727714538573 and perplexity is 45.09055366445
At time: 73.99693512916565 and batch: 900, loss is 3.7643887567520142 and perplexity is 43.13733039782787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380341412269906 and perplexity of 79.86529574287766
finished 5 epochs...
Completing Train Step...
At time: 75.84408330917358 and batch: 50, loss is 3.833184871673584 and perplexity is 46.20947537120913
At time: 76.61793446540833 and batch: 100, loss is 3.7009771823883058 and perplexity is 40.486848071104845
At time: 77.36841368675232 and batch: 150, loss is 3.6961810207366943 and perplexity is 40.29313152227402
At time: 78.12193727493286 and batch: 200, loss is 3.59553439617157 and perplexity is 36.435165599248464
At time: 78.87482404708862 and batch: 250, loss is 3.751668963432312 and perplexity is 42.59210738392104
At time: 79.62829852104187 and batch: 300, loss is 3.735987639427185 and perplexity is 41.92941626485578
At time: 80.3806779384613 and batch: 350, loss is 3.7239979314804077 and perplexity is 41.429696542564514
At time: 81.13482761383057 and batch: 400, loss is 3.66200795173645 and perplexity is 38.93945296663835
At time: 81.88787317276001 and batch: 450, loss is 3.690908637046814 and perplexity is 40.08124972471307
At time: 82.64075493812561 and batch: 500, loss is 3.5681541347503662 and perplexity is 35.451094775729764
At time: 83.39468812942505 and batch: 550, loss is 3.6503056478500366 and perplexity is 38.48642754533736
At time: 84.14886569976807 and batch: 600, loss is 3.6756698942184447 and perplexity is 39.475092137786795
At time: 84.91983675956726 and batch: 650, loss is 3.5177990674972532 and perplexity is 33.71015298479792
At time: 85.67507410049438 and batch: 700, loss is 3.530232005119324 and perplexity is 34.131885468388404
At time: 86.42888283729553 and batch: 750, loss is 3.6449054431915284 and perplexity is 38.27915312587524
At time: 87.18298935890198 and batch: 800, loss is 3.5903927278518677 and perplexity is 36.24830885192967
At time: 87.93763279914856 and batch: 850, loss is 3.674106092453003 and perplexity is 39.413409161554675
At time: 88.69197463989258 and batch: 900, loss is 3.6355651426315307 and perplexity is 37.92327890383296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.402287731431935 and perplexity of 81.63741968532493
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 90.59292221069336 and batch: 50, loss is 3.728977394104004 and perplexity is 41.63650864725169
At time: 91.36185693740845 and batch: 100, loss is 3.605173397064209 and perplexity is 36.78806224622266
At time: 92.11174607276917 and batch: 150, loss is 3.602490530014038 and perplexity is 36.689497043970185
At time: 92.86234784126282 and batch: 200, loss is 3.4837076902389525 and perplexity is 32.58029605792459
At time: 93.61211013793945 and batch: 250, loss is 3.629385552406311 and perplexity is 37.68965118546081
At time: 94.36174607276917 and batch: 300, loss is 3.6025667285919187 and perplexity is 36.6922928379845
At time: 95.11263847351074 and batch: 350, loss is 3.5731538677215577 and perplexity is 35.62878461386854
At time: 95.86300444602966 and batch: 400, loss is 3.5067903327941896 and perplexity is 33.34108208298086
At time: 96.6133599281311 and batch: 450, loss is 3.513580837249756 and perplexity is 33.56825528681479
At time: 97.36484479904175 and batch: 500, loss is 3.376428236961365 and perplexity is 29.266052804431855
At time: 98.11524558067322 and batch: 550, loss is 3.4367622423171995 and perplexity is 31.086145646224544
At time: 98.8649353981018 and batch: 600, loss is 3.454017653465271 and perplexity is 31.627204542290663
At time: 99.61343383789062 and batch: 650, loss is 3.275452146530151 and perplexity is 26.455184396597293
At time: 100.36213827133179 and batch: 700, loss is 3.26743013381958 and perplexity is 26.243809530582066
At time: 101.11047315597534 and batch: 750, loss is 3.36276376247406 and perplexity is 28.868867417449806
At time: 101.86284303665161 and batch: 800, loss is 3.285224189758301 and perplexity is 26.714972866885727
At time: 102.61279559135437 and batch: 850, loss is 3.345988435745239 and perplexity is 28.3886221321791
At time: 103.37469696998596 and batch: 900, loss is 3.290509934425354 and perplexity is 26.856555246761864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375076084920805 and perplexity of 79.44588395610347
finished 7 epochs...
Completing Train Step...
At time: 105.25061750411987 and batch: 50, loss is 3.610415806770325 and perplexity is 36.98142674592591
At time: 106.00261163711548 and batch: 100, loss is 3.48062415599823 and perplexity is 32.47998833009247
At time: 106.75394797325134 and batch: 150, loss is 3.474920473098755 and perplexity is 32.295260092521765
At time: 107.50419783592224 and batch: 200, loss is 3.359564952850342 and perplexity is 28.776668947822486
At time: 108.25690007209778 and batch: 250, loss is 3.5070419788360594 and perplexity is 33.34947329008144
At time: 109.00781726837158 and batch: 300, loss is 3.4871666193008424 and perplexity is 32.693184114142355
At time: 109.75978374481201 and batch: 350, loss is 3.4634022235870363 and perplexity is 31.925409334388902
At time: 110.50663924217224 and batch: 400, loss is 3.400680351257324 and perplexity is 29.984493096970677
At time: 111.25619959831238 and batch: 450, loss is 3.4151999235153196 and perplexity is 30.423031098934356
At time: 112.0089201927185 and batch: 500, loss is 3.2834521341323852 and perplexity is 26.667674369082494
At time: 112.760981798172 and batch: 550, loss is 3.347944722175598 and perplexity is 28.444212766305423
At time: 113.5128231048584 and batch: 600, loss is 3.372453694343567 and perplexity is 29.149964482118595
At time: 114.26602077484131 and batch: 650, loss is 3.2014957332611083 and perplexity is 24.56925177450048
At time: 115.01793456077576 and batch: 700, loss is 3.1998056077957155 and perplexity is 24.527761727979286
At time: 115.76929664611816 and batch: 750, loss is 3.3037420129776 and perplexity is 27.21428482890045
At time: 116.5283739566803 and batch: 800, loss is 3.233158106803894 and perplexity is 25.359619025537928
At time: 117.28787875175476 and batch: 850, loss is 3.3038318824768065 and perplexity is 27.216730672951027
At time: 118.03709197044373 and batch: 900, loss is 3.2566781044006348 and perplexity is 25.963146868896967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.395696352605951 and perplexity of 81.10108605667166
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 119.87528252601624 and batch: 50, loss is 3.5577412986755372 and perplexity is 35.083863614246425
At time: 120.63632607460022 and batch: 100, loss is 3.443092737197876 and perplexity is 31.28356053979874
At time: 121.38341093063354 and batch: 150, loss is 3.440605864524841 and perplexity is 31.205858964984284
At time: 122.14263367652893 and batch: 200, loss is 3.3196996021270753 and perplexity is 27.65204269593922
At time: 122.89166903495789 and batch: 250, loss is 3.4678508710861204 and perplexity is 32.06775060510064
At time: 123.63856506347656 and batch: 300, loss is 3.4472757959365845 and perplexity is 31.414695592690638
At time: 124.38515973091125 and batch: 350, loss is 3.4178605461120606 and perplexity is 30.5040830794897
At time: 125.13453435897827 and batch: 400, loss is 3.353471989631653 and perplexity is 28.601866834572366
At time: 125.88138008117676 and batch: 450, loss is 3.3577104663848876 and perplexity is 28.723352457373
At time: 126.62714099884033 and batch: 500, loss is 3.2193122005462644 and perplexity is 25.01091177259452
At time: 127.38236808776855 and batch: 550, loss is 3.275373454093933 and perplexity is 26.45310265559624
At time: 128.12932562828064 and batch: 600, loss is 3.2988137578964234 and perplexity is 27.080495835340937
At time: 128.87762808799744 and batch: 650, loss is 3.1205358028411867 and perplexity is 22.65851688902584
At time: 129.6253936290741 and batch: 700, loss is 3.112350926399231 and perplexity is 22.473816633438506
At time: 130.3734941482544 and batch: 750, loss is 3.20592312335968 and perplexity is 24.678270592874522
At time: 131.11928629875183 and batch: 800, loss is 3.1308478212356565 and perplexity is 22.89338081128259
At time: 131.86636400222778 and batch: 850, loss is 3.1944019842147826 and perplexity is 24.39558038705558
At time: 132.61332845687866 and batch: 900, loss is 3.1442078161239624 and perplexity is 23.201288504659196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.389071529858733 and perplexity of 80.56558150683044
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 134.47260189056396 and batch: 50, loss is 3.5319040298461912 and perplexity is 34.18900256215601
At time: 135.23478174209595 and batch: 100, loss is 3.416223258972168 and perplexity is 30.454180000532123
At time: 135.99298620224 and batch: 150, loss is 3.4151179599761963 and perplexity is 30.420537621823126
At time: 136.74059009552002 and batch: 200, loss is 3.2942503547668456 and perplexity is 26.957198158367063
At time: 137.48871684074402 and batch: 250, loss is 3.4419566869735716 and perplexity is 31.248041023627763
At time: 138.23558521270752 and batch: 300, loss is 3.4245691442489625 and perplexity is 30.709410674496926
At time: 138.98343420028687 and batch: 350, loss is 3.393990788459778 and perplexity is 29.784579360707575
At time: 139.7336347103119 and batch: 400, loss is 3.330032801628113 and perplexity is 27.939258141361396
At time: 140.4874415397644 and batch: 450, loss is 3.3335946226119995 and perplexity is 28.03895021454678
At time: 141.2490749359131 and batch: 500, loss is 3.19421763420105 and perplexity is 24.391083475991753
At time: 141.99752044677734 and batch: 550, loss is 3.2472056484222414 and perplexity is 25.71837323708629
At time: 142.74869012832642 and batch: 600, loss is 3.2735392332077025 and perplexity is 26.40462629396825
At time: 143.4966013431549 and batch: 650, loss is 3.09396550655365 and perplexity is 22.064401248580104
At time: 144.24436902999878 and batch: 700, loss is 3.083781337738037 and perplexity is 21.840834016975656
At time: 144.99275183677673 and batch: 750, loss is 3.1730843448638915 and perplexity is 23.88102820752229
At time: 145.73978519439697 and batch: 800, loss is 3.097506012916565 and perplexity is 22.142658855624667
At time: 146.48770666122437 and batch: 850, loss is 3.160099620819092 and perplexity is 23.572944168105867
At time: 147.23662209510803 and batch: 900, loss is 3.1113191270828247 and perplexity is 22.450640123608746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387761782293451 and perplexity of 80.46013000510474
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.0833523273468 and batch: 50, loss is 3.522223615646362 and perplexity is 33.859635632515385
At time: 149.8326873779297 and batch: 100, loss is 3.4046958017349245 and perplexity is 30.105136400600347
At time: 150.58250284194946 and batch: 150, loss is 3.404972643852234 and perplexity is 30.11347192406213
At time: 151.33338618278503 and batch: 200, loss is 3.2839870262145996 and perplexity is 26.68194251257189
At time: 152.08536291122437 and batch: 250, loss is 3.4319326639175416 and perplexity is 30.936374625241992
At time: 152.84013414382935 and batch: 300, loss is 3.4145507860183715 and perplexity is 30.403288777111154
At time: 153.59116673469543 and batch: 350, loss is 3.383946590423584 and perplexity is 29.486914551839558
At time: 154.34902811050415 and batch: 400, loss is 3.319718918800354 and perplexity is 27.652576846572455
At time: 155.10276985168457 and batch: 450, loss is 3.324760799407959 and perplexity is 27.792349900959362
At time: 155.85223817825317 and batch: 500, loss is 3.184836015701294 and perplexity is 24.16332567492594
At time: 156.6019253730774 and batch: 550, loss is 3.2379219007492064 and perplexity is 25.480715234791102
At time: 157.35180163383484 and batch: 600, loss is 3.265513734817505 and perplexity is 26.193564080729548
At time: 158.10096526145935 and batch: 650, loss is 3.0861767721176148 and perplexity is 21.89321501423209
At time: 158.8499882221222 and batch: 700, loss is 3.0753056049346923 and perplexity is 21.65649923335132
At time: 159.62239956855774 and batch: 750, loss is 3.163714256286621 and perplexity is 23.65830595109235
At time: 160.3736126422882 and batch: 800, loss is 3.0875642681121827 and perplexity is 21.923612845930727
At time: 161.1220986843109 and batch: 850, loss is 3.1496369886398314 and perplexity is 23.327594861793347
At time: 161.86936020851135 and batch: 900, loss is 3.1013565063476562 and perplexity is 22.228083375827996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387561954864084 and perplexity of 80.44405347047908
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 163.69198536872864 and batch: 50, loss is 3.519509582519531 and perplexity is 33.76786405163387
At time: 164.45374083518982 and batch: 100, loss is 3.401841320991516 and perplexity is 30.01932440108401
At time: 165.20297193527222 and batch: 150, loss is 3.402465753555298 and perplexity is 30.038075298508982
At time: 165.952392578125 and batch: 200, loss is 3.2814304637908935 and perplexity is 26.61381558343283
At time: 166.70692420005798 and batch: 250, loss is 3.4293351697921755 and perplexity is 30.85612184707467
At time: 167.4581389427185 and batch: 300, loss is 3.4114450454711913 and perplexity is 30.309010528059883
At time: 168.208487033844 and batch: 350, loss is 3.3809644651412962 and perplexity is 29.39911186283684
At time: 168.95818281173706 and batch: 400, loss is 3.316766366958618 and perplexity is 27.571051592869463
At time: 169.7089569568634 and batch: 450, loss is 3.322106862068176 and perplexity is 27.71868853535667
At time: 170.46052408218384 and batch: 500, loss is 3.1822750473022463 and perplexity is 24.101523332157825
At time: 171.21015906333923 and batch: 550, loss is 3.23536406993866 and perplexity is 25.415623159035967
At time: 171.9603841304779 and batch: 600, loss is 3.2632640981674195 and perplexity is 26.134704310348955
At time: 172.71141171455383 and batch: 650, loss is 3.0838530445098877 and perplexity is 21.842400208830146
At time: 173.46998119354248 and batch: 700, loss is 3.0730339527130126 and perplexity is 21.607359034606837
At time: 174.21918511390686 and batch: 750, loss is 3.1612458753585817 and perplexity is 23.599980254525867
At time: 174.9679844379425 and batch: 800, loss is 3.0849359035491943 and perplexity is 21.86606525998127
At time: 175.7168893814087 and batch: 850, loss is 3.1467907333374026 and perplexity is 23.261292972037925
At time: 176.4663166999817 and batch: 900, loss is 3.0986461734771726 and perplexity is 22.16791943978325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3873224127782535 and perplexity of 80.42478604189027
Annealing...
Model not improving. Stopping early with 79.44588395610347 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -79.44588395610347
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7ebb4a0b70>
ELAPSED
190.84255170822144


RESULTS SO FAR:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0479497909545898 and batch: 50, loss is 6.83746265411377 and perplexity is 932.1210182033595
At time: 1.8828730583190918 and batch: 100, loss is 6.101202783584594 and perplexity is 446.3943631273988
At time: 2.7091307640075684 and batch: 150, loss is 6.0107691192626955 and perplexity is 407.7968440979021
At time: 3.5424325466156006 and batch: 200, loss is 5.85926700592041 and perplexity is 350.4671594943515
At time: 4.36118483543396 and batch: 250, loss is 5.914473695755005 and perplexity is 370.35932971387825
At time: 5.181156873703003 and batch: 300, loss is 5.824630136489868 and perplexity is 338.5358978439011
At time: 6.000052213668823 and batch: 350, loss is 5.824451904296875 and perplexity is 338.4755652251888
At time: 6.82181191444397 and batch: 400, loss is 5.689450788497925 and perplexity is 295.7311570781497
At time: 7.643158674240112 and batch: 450, loss is 5.700981636047363 and perplexity is 299.16092402406775
At time: 8.465508460998535 and batch: 500, loss is 5.65652663230896 and perplexity is 286.15299985604327
At time: 9.286793947219849 and batch: 550, loss is 5.702008419036865 and perplexity is 299.46825512615214
At time: 10.107308626174927 and batch: 600, loss is 5.634209022521973 and perplexity is 279.8374845999756
At time: 10.92702054977417 and batch: 650, loss is 5.540322341918945 and perplexity is 254.76010608610628
At time: 11.747509241104126 and batch: 700, loss is 5.649861946105957 and perplexity is 284.2522209890341
At time: 12.572299003601074 and batch: 750, loss is 5.609375019073486 and perplexity is 272.973581399933
At time: 13.39338493347168 and batch: 800, loss is 5.600445642471313 and perplexity is 270.54694777562105
At time: 14.214674472808838 and batch: 850, loss is 5.639990968704224 and perplexity is 281.4601765136982
At time: 15.03600263595581 and batch: 900, loss is 5.533497877120972 and perplexity is 253.02742374972453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.396565476508989 and perplexity of 220.64729502265152
finished 1 epochs...
Completing Train Step...
At time: 16.95126175880432 and batch: 50, loss is 5.290380191802979 and perplexity is 198.41884829056863
At time: 17.70799970626831 and batch: 100, loss is 5.091029434204102 and perplexity is 162.55711780651234
At time: 18.46402931213379 and batch: 150, loss is 5.0251639938354495 and perplexity is 152.19521314295446
At time: 19.218904972076416 and batch: 200, loss is 4.873769950866699 and perplexity is 130.8131476098772
At time: 19.97429585456848 and batch: 250, loss is 4.937333545684814 and perplexity is 139.39805501532857
At time: 20.73104739189148 and batch: 300, loss is 4.875574913024902 and perplexity is 131.04947360668874
At time: 21.48200297355652 and batch: 350, loss is 4.837712182998657 and perplexity is 126.18034372185342
At time: 22.2329843044281 and batch: 400, loss is 4.703086242675782 and perplexity is 110.2870202644865
At time: 22.99635410308838 and batch: 450, loss is 4.703905410766602 and perplexity is 110.37740088572524
At time: 23.753902435302734 and batch: 500, loss is 4.60570764541626 and perplexity is 100.05376038853656
At time: 24.507915019989014 and batch: 550, loss is 4.672451992034912 and perplexity is 106.95968545690381
At time: 25.2602117061615 and batch: 600, loss is 4.621651983261108 and perplexity is 101.6618371390971
At time: 26.015371799468994 and batch: 650, loss is 4.479334449768066 and perplexity is 88.17596760654075
At time: 26.77152395248413 and batch: 700, loss is 4.518621368408203 and perplexity is 91.70907775396053
At time: 27.527767419815063 and batch: 750, loss is 4.565062294006347 and perplexity is 96.06857832676144
At time: 28.279407739639282 and batch: 800, loss is 4.493671016693115 and perplexity is 89.44921344620693
At time: 29.029146671295166 and batch: 850, loss is 4.556367874145508 and perplexity is 95.23693832386964
At time: 29.780644416809082 and batch: 900, loss is 4.495344333648681 and perplexity is 89.59901563004189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.595976529056078 and perplexity of 99.08484755159272
finished 2 epochs...
Completing Train Step...
At time: 31.62447953224182 and batch: 50, loss is 4.517574253082276 and perplexity is 91.61309803280595
At time: 32.39331769943237 and batch: 100, loss is 4.366921563148498 and perplexity is 78.8006750261228
At time: 33.15061831474304 and batch: 150, loss is 4.361033234596253 and perplexity is 78.33803418879495
At time: 33.90217065811157 and batch: 200, loss is 4.250774955749511 and perplexity is 70.15976199562425
At time: 34.653565406799316 and batch: 250, loss is 4.391253795623779 and perplexity is 80.74158899490182
At time: 35.40482544898987 and batch: 300, loss is 4.357696251869202 and perplexity is 78.07705720185932
At time: 36.15722370147705 and batch: 350, loss is 4.342705116271973 and perplexity is 76.91532306400393
At time: 36.914146184921265 and batch: 400, loss is 4.256139268875122 and perplexity is 70.53713218879861
At time: 37.67050242424011 and batch: 450, loss is 4.27155113697052 and perplexity is 71.63266155837877
At time: 38.42599010467529 and batch: 500, loss is 4.15338906288147 and perplexity is 63.649346819289654
At time: 39.17785668373108 and batch: 550, loss is 4.232773780822754 and perplexity is 68.90810330961304
At time: 39.92919182777405 and batch: 600, loss is 4.234765124320984 and perplexity is 69.04545972963099
At time: 40.68129825592041 and batch: 650, loss is 4.078011364936828 and perplexity is 59.02796796761598
At time: 41.43440651893616 and batch: 700, loss is 4.099048938751221 and perplexity is 60.28292756886167
At time: 42.20624566078186 and batch: 750, loss is 4.196520428657532 and perplexity is 66.45469442627396
At time: 42.96684288978577 and batch: 800, loss is 4.136564602851868 and perplexity is 62.58743898947217
At time: 43.72353935241699 and batch: 850, loss is 4.212485194206238 and perplexity is 67.52414206842361
At time: 44.484556436538696 and batch: 900, loss is 4.1637751007080075 and perplexity is 64.31385617930125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429006916202911 and perplexity of 83.84810735360072
finished 3 epochs...
Completing Train Step...
At time: 46.346776723861694 and batch: 50, loss is 4.219572858810425 and perplexity is 68.00443059146664
At time: 47.11307239532471 and batch: 100, loss is 4.076205458641052 and perplexity is 58.92146518460434
At time: 47.87039828300476 and batch: 150, loss is 4.074705214500427 and perplexity is 58.83313487678478
At time: 48.62093687057495 and batch: 200, loss is 3.96881142616272 and perplexity is 52.92159222354529
At time: 49.37364721298218 and batch: 250, loss is 4.121495008468628 and perplexity is 61.65134268123361
At time: 50.128945112228394 and batch: 300, loss is 4.088810071945191 and perplexity is 59.66884780796053
At time: 50.88326597213745 and batch: 350, loss is 4.076074867248535 and perplexity is 58.91377105082151
At time: 51.63439178466797 and batch: 400, loss is 4.004346432685852 and perplexity is 54.83597368487884
At time: 52.3906147480011 and batch: 450, loss is 4.027950410842895 and perplexity is 56.145717583634344
At time: 53.14683246612549 and batch: 500, loss is 3.9050670194625856 and perplexity is 49.65340754535704
At time: 53.897987842559814 and batch: 550, loss is 3.987943296432495 and perplexity is 53.943828728083446
At time: 54.65741443634033 and batch: 600, loss is 4.003434057235718 and perplexity is 54.78596550529123
At time: 55.414488792419434 and batch: 650, loss is 3.8424842834472654 and perplexity is 46.64120059458296
At time: 56.165318727493286 and batch: 700, loss is 3.8566419458389283 and perplexity is 47.30622747104659
At time: 56.916325092315674 and batch: 750, loss is 3.9684978246688845 and perplexity is 52.90499853520656
At time: 57.66646361351013 and batch: 800, loss is 3.912309865951538 and perplexity is 50.01434508359032
At time: 58.41671419143677 and batch: 850, loss is 3.991689348220825 and perplexity is 54.146284071507324
At time: 59.16747188568115 and batch: 900, loss is 3.950983166694641 and perplexity is 51.98645306680326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382365187553511 and perplexity of 80.02708881560984
finished 4 epochs...
Completing Train Step...
At time: 61.01633143424988 and batch: 50, loss is 4.013954181671142 and perplexity is 55.36536300297909
At time: 61.762614250183105 and batch: 100, loss is 3.8763751554489136 and perplexity is 48.249002570093715
At time: 62.510777711868286 and batch: 150, loss is 3.879112582206726 and perplexity is 48.38126162291161
At time: 63.26515173912048 and batch: 200, loss is 3.7719726610183715 and perplexity is 43.465723458663184
At time: 64.02046918869019 and batch: 250, loss is 3.9244317865371703 and perplexity is 50.624304473495286
At time: 64.77701473236084 and batch: 300, loss is 3.896557502746582 and perplexity is 49.23267370351416
At time: 65.52897191047668 and batch: 350, loss is 3.8852130126953126 and perplexity is 48.6773102402716
At time: 66.28048181533813 and batch: 400, loss is 3.8187395095825196 and perplexity is 45.54676086685238
At time: 67.03134083747864 and batch: 450, loss is 3.8429789543151855 and perplexity is 46.664278345236795
At time: 67.78094720840454 and batch: 500, loss is 3.726643195152283 and perplexity is 41.53943409215209
At time: 68.53418946266174 and batch: 550, loss is 3.8059781074523924 and perplexity is 44.96921333182058
At time: 69.28406620025635 and batch: 600, loss is 3.8299702739715578 and perplexity is 46.06116899824388
At time: 70.03101468086243 and batch: 650, loss is 3.6674265003204347 and perplexity is 39.15102096245604
At time: 70.78742384910583 and batch: 700, loss is 3.680991325378418 and perplexity is 39.68571603638961
At time: 71.54221892356873 and batch: 750, loss is 3.794202947616577 and perplexity is 44.442799045783374
At time: 72.29056930541992 and batch: 800, loss is 3.7396644067764284 and perplexity is 42.08386473510339
At time: 73.03863978385925 and batch: 850, loss is 3.8242517137527465 and perplexity is 45.79851714057909
At time: 73.78665709495544 and batch: 900, loss is 3.7866337966918944 and perplexity is 44.107674695677844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375844877060145 and perplexity of 79.50698481111098
finished 5 epochs...
Completing Train Step...
At time: 75.61839532852173 and batch: 50, loss is 3.851413984298706 and perplexity is 47.05955768511465
At time: 76.38276648521423 and batch: 100, loss is 3.716039528846741 and perplexity is 41.10129086213226
At time: 77.13393115997314 and batch: 150, loss is 3.7274831199645995 and perplexity is 41.5743387501278
At time: 77.88304662704468 and batch: 200, loss is 3.6157974338531496 and perplexity is 37.18098348200254
At time: 78.64229679107666 and batch: 250, loss is 3.7705690908432006 and perplexity is 43.40475905948896
At time: 79.41553711891174 and batch: 300, loss is 3.7443652391433715 and perplexity is 42.28215963897033
At time: 80.16921520233154 and batch: 350, loss is 3.729770345687866 and perplexity is 41.669537476130465
At time: 80.92145562171936 and batch: 400, loss is 3.6710364770889283 and perplexity is 39.29261065257373
At time: 81.67344427108765 and batch: 450, loss is 3.6946043252944945 and perplexity is 40.22965158286721
At time: 82.42344546318054 and batch: 500, loss is 3.5820753526687623 and perplexity is 35.948068404530574
At time: 83.17480087280273 and batch: 550, loss is 3.6647798824310303 and perplexity is 39.04754016745332
At time: 83.92411255836487 and batch: 600, loss is 3.686848578453064 and perplexity is 39.91884740687073
At time: 84.68213748931885 and batch: 650, loss is 3.526463861465454 and perplexity is 34.003513634416784
At time: 85.44012999534607 and batch: 700, loss is 3.5398573160171507 and perplexity is 34.46200166438516
At time: 86.19459104537964 and batch: 750, loss is 3.653867230415344 and perplexity is 38.623744522405545
At time: 86.94868040084839 and batch: 800, loss is 3.6007255601882933 and perplexity is 36.62479830122553
At time: 87.69940733909607 and batch: 850, loss is 3.6842293310165406 and perplexity is 39.81442687942843
At time: 88.44956183433533 and batch: 900, loss is 3.6498598194122316 and perplexity is 38.469273025738595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.394311774266909 and perplexity of 80.98887295148883
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 90.29329323768616 and batch: 50, loss is 3.739777398109436 and perplexity is 42.08862011573127
At time: 91.05515623092651 and batch: 100, loss is 3.609912271499634 and perplexity is 36.962809980691176
At time: 91.80420660972595 and batch: 150, loss is 3.6218284034729002 and perplexity is 37.405898409634865
At time: 92.55292391777039 and batch: 200, loss is 3.4969332122802737 and perplexity is 33.01404946672653
At time: 93.30222296714783 and batch: 250, loss is 3.642776246070862 and perplexity is 38.197735970602956
At time: 94.05278325080872 and batch: 300, loss is 3.5999925565719604 and perplexity is 36.597962028367384
At time: 94.80334782600403 and batch: 350, loss is 3.5710600566864015 and perplexity is 35.55426271608639
At time: 95.55407214164734 and batch: 400, loss is 3.503943290710449 and perplexity is 33.246293616576445
At time: 96.30869102478027 and batch: 450, loss is 3.511361699104309 and perplexity is 33.493845284605335
At time: 97.0650725364685 and batch: 500, loss is 3.3902019309997558 and perplexity is 29.67194335064157
At time: 97.82234859466553 and batch: 550, loss is 3.4482731771469117 and perplexity is 31.446043650186027
At time: 98.58678841590881 and batch: 600, loss is 3.456466460227966 and perplexity is 31.70474836082229
At time: 99.3362717628479 and batch: 650, loss is 3.275259165763855 and perplexity is 26.450079547424618
At time: 100.09342527389526 and batch: 700, loss is 3.2680243015289308 and perplexity is 26.259407388188272
At time: 100.84857821464539 and batch: 750, loss is 3.362329707145691 and perplexity is 28.856339450835303
At time: 101.60346722602844 and batch: 800, loss is 3.2894815158843995 and perplexity is 26.82894966487912
At time: 102.36079168319702 and batch: 850, loss is 3.3466719675064085 and perplexity is 28.40803329037809
At time: 103.12305212020874 and batch: 900, loss is 3.2981382608413696 and perplexity is 27.062209217128487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373603298239512 and perplexity of 79.32896323710268
finished 7 epochs...
Completing Train Step...
At time: 104.99414086341858 and batch: 50, loss is 3.6209132051467896 and perplexity is 37.37168025461119
At time: 105.74776434898376 and batch: 100, loss is 3.485462689399719 and perplexity is 32.637524653543636
At time: 106.5002191066742 and batch: 150, loss is 3.495403232574463 and perplexity is 32.96357726160618
At time: 107.25853538513184 and batch: 200, loss is 3.3755186557769776 and perplexity is 29.23944505624768
At time: 108.01369094848633 and batch: 250, loss is 3.5213738250732423 and perplexity is 33.83087425564988
At time: 108.77207040786743 and batch: 300, loss is 3.4844522619247438 and perplexity is 32.60456345717207
At time: 109.52575635910034 and batch: 350, loss is 3.460493063926697 and perplexity is 31.83266818622433
At time: 110.28555536270142 and batch: 400, loss is 3.397917194366455 and perplexity is 29.901755599236044
At time: 111.03965330123901 and batch: 450, loss is 3.4113109064102174 and perplexity is 30.304945178515784
At time: 111.80023050308228 and batch: 500, loss is 3.2971556425094604 and perplexity is 27.035630454777902
At time: 112.56080770492554 and batch: 550, loss is 3.3581429147720336 and perplexity is 28.735776511003976
At time: 113.31801867485046 and batch: 600, loss is 3.3754112911224365 and perplexity is 29.236305941848222
At time: 114.07649445533752 and batch: 650, loss is 3.2012235832214357 and perplexity is 24.56256616144152
At time: 114.83021688461304 and batch: 700, loss is 3.199374113082886 and perplexity is 24.517180411531346
At time: 115.58599209785461 and batch: 750, loss is 3.30182315826416 and perplexity is 27.16211463969791
At time: 116.33959293365479 and batch: 800, loss is 3.237395610809326 and perplexity is 25.46730851892137
At time: 117.10362386703491 and batch: 850, loss is 3.303852467536926 and perplexity is 27.2172909367547
At time: 117.85532784461975 and batch: 900, loss is 3.2639494943618774 and perplexity is 26.1526230772524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39571641895869 and perplexity of 81.10271347600015
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 119.73246216773987 and batch: 50, loss is 3.565011558532715 and perplexity is 35.33986187886867
At time: 120.50621581077576 and batch: 100, loss is 3.44311683177948 and perplexity is 31.28431431318191
At time: 121.26502251625061 and batch: 150, loss is 3.457071738243103 and perplexity is 31.723944356851675
At time: 122.01685047149658 and batch: 200, loss is 3.333704137802124 and perplexity is 28.04202107366021
At time: 122.76948428153992 and batch: 250, loss is 3.480121669769287 and perplexity is 32.46367168302892
At time: 123.52653336524963 and batch: 300, loss is 3.440330858230591 and perplexity is 31.19727833726882
At time: 124.27739095687866 and batch: 350, loss is 3.411616926193237 and perplexity is 30.31422051040888
At time: 125.02868819236755 and batch: 400, loss is 3.346886739730835 and perplexity is 28.41413520211656
At time: 125.78568649291992 and batch: 450, loss is 3.3499441289901735 and perplexity is 28.50114121174159
At time: 126.53575277328491 and batch: 500, loss is 3.2329366779327393 and perplexity is 25.35400429537881
At time: 127.28506255149841 and batch: 550, loss is 3.284230546951294 and perplexity is 26.68844091008415
At time: 128.034738779068 and batch: 600, loss is 3.2966877174377442 and perplexity is 27.02298276477486
At time: 128.7905192375183 and batch: 650, loss is 3.1184704065322877 and perplexity is 22.61176636767451
At time: 129.54960203170776 and batch: 700, loss is 3.1058695936203002 and perplexity is 22.32862736725449
At time: 130.30310368537903 and batch: 750, loss is 3.201348509788513 and perplexity is 24.565634870188312
At time: 131.05334615707397 and batch: 800, loss is 3.128548583984375 and perplexity is 22.840803963798912
At time: 131.80327343940735 and batch: 850, loss is 3.1886024379730227 and perplexity is 24.254506567989708
At time: 132.55193829536438 and batch: 900, loss is 3.149092073440552 and perplexity is 23.314886763522924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.394130340994221 and perplexity of 80.97418020813447
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 134.39857649803162 and batch: 50, loss is 3.5397352027893065 and perplexity is 34.45779365505599
At time: 135.16200590133667 and batch: 100, loss is 3.417425527572632 and perplexity is 30.490816123716936
At time: 135.9124014377594 and batch: 150, loss is 3.4328371238708497 and perplexity is 30.96436799472356
At time: 136.67336797714233 and batch: 200, loss is 3.308253998756409 and perplexity is 27.337352726546122
At time: 137.42308807373047 and batch: 250, loss is 3.453555550575256 and perplexity is 31.612592895970586
At time: 138.18632626533508 and batch: 300, loss is 3.415798192024231 and perplexity is 30.44123768605729
At time: 138.95226001739502 and batch: 350, loss is 3.385709547996521 and perplexity is 29.538944581042404
At time: 139.70267987251282 and batch: 400, loss is 3.324316687583923 and perplexity is 27.780009730161318
At time: 140.4529755115509 and batch: 450, loss is 3.3231962537765503 and perplexity is 27.74890149870197
At time: 141.20386338233948 and batch: 500, loss is 3.205924243927002 and perplexity is 24.67829824655361
At time: 141.9536998271942 and batch: 550, loss is 3.2545937585830687 and perplexity is 25.909087051558902
At time: 142.7038083076477 and batch: 600, loss is 3.269094772338867 and perplexity is 26.287532368082836
At time: 143.4534456729889 and batch: 650, loss is 3.0922988986968996 and perplexity is 22.027659169927713
At time: 144.20299458503723 and batch: 700, loss is 3.0757259845733644 and perplexity is 21.66560509849999
At time: 144.95636820793152 and batch: 750, loss is 3.1694010400772097 and perplexity is 23.793228897087307
At time: 145.71047139167786 and batch: 800, loss is 3.094551196098328 and perplexity is 22.077327922840432
At time: 146.46009254455566 and batch: 850, loss is 3.1531343126296996 and perplexity is 23.409321848502955
At time: 147.21052813529968 and batch: 900, loss is 3.116545333862305 and perplexity is 22.56827894587368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391280082807149 and perplexity of 80.74371149175438
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.045396566391 and batch: 50, loss is 3.5301955413818358 and perplexity is 34.13064091496738
At time: 149.79347729682922 and batch: 100, loss is 3.4056928539276123 and perplexity is 30.135167761789113
At time: 150.54032802581787 and batch: 150, loss is 3.4228122138977053 and perplexity is 30.655503748037976
At time: 151.2848401069641 and batch: 200, loss is 3.2975990390777588 and perplexity is 27.04762061854563
At time: 152.02942752838135 and batch: 250, loss is 3.4433918046951293 and perplexity is 31.292917835116246
At time: 152.77462458610535 and batch: 300, loss is 3.4058797359466553 and perplexity is 30.140800009051112
At time: 153.52722191810608 and batch: 350, loss is 3.37625048160553 and perplexity is 29.260851069133565
At time: 154.27728390693665 and batch: 400, loss is 3.3158542203903196 and perplexity is 27.545914219002185
At time: 155.0367443561554 and batch: 450, loss is 3.314864912033081 and perplexity is 27.51867629143379
At time: 155.78521847724915 and batch: 500, loss is 3.1963015508651735 and perplexity is 24.441965459796577
At time: 156.53914523124695 and batch: 550, loss is 3.2451156902313234 and perplexity is 25.66467904113902
At time: 157.29363369941711 and batch: 600, loss is 3.260840182304382 and perplexity is 26.071432699504577
At time: 158.05205011367798 and batch: 650, loss is 3.0839037704467773 and perplexity is 21.843508213146695
At time: 158.81722235679626 and batch: 700, loss is 3.067033610343933 and perplexity is 21.478095682725183
At time: 159.5821566581726 and batch: 750, loss is 3.1606884527206422 and perplexity is 23.586828757088806
At time: 160.34183979034424 and batch: 800, loss is 3.084880485534668 and perplexity is 21.864853519635492
At time: 161.09465885162354 and batch: 850, loss is 3.1427838373184205 and perplexity is 23.168273873213717
At time: 161.85157918930054 and batch: 900, loss is 3.1063262891769408 and perplexity is 22.338827081063705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39085367281143 and perplexity of 80.70928890567181
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 163.70033288002014 and batch: 50, loss is 3.527639489173889 and perplexity is 34.04351261457736
At time: 164.4613561630249 and batch: 100, loss is 3.4024392223358153 and perplexity is 30.03727836231229
At time: 165.21023058891296 and batch: 150, loss is 3.4201229333877565 and perplexity is 30.573173253801894
At time: 165.95822954177856 and batch: 200, loss is 3.2947995376586916 and perplexity is 26.972006656322417
At time: 166.7079954147339 and batch: 250, loss is 3.4409533500671388 and perplexity is 31.216704434022308
At time: 167.45669507980347 and batch: 300, loss is 3.4028447341918944 and perplexity is 30.04946130480942
At time: 168.20674753189087 and batch: 350, loss is 3.373367147445679 and perplexity is 29.176603772620204
At time: 168.95386934280396 and batch: 400, loss is 3.313148307800293 and perplexity is 27.471478137096955
At time: 169.70556473731995 and batch: 450, loss is 3.312535972595215 and perplexity is 27.454661533122867
At time: 170.45912837982178 and batch: 500, loss is 3.193803253173828 and perplexity is 24.380978367593112
At time: 171.20868730545044 and batch: 550, loss is 3.2425562143325806 and perplexity is 25.599074905600073
At time: 171.9574489593506 and batch: 600, loss is 3.258764967918396 and perplexity is 26.017384986994994
At time: 172.70776987075806 and batch: 650, loss is 3.0814391231536864 and perplexity is 21.7897379593317
At time: 173.45636892318726 and batch: 700, loss is 3.0645474338531495 and perplexity is 21.424763670040818
At time: 174.21605587005615 and batch: 750, loss is 3.1583189249038695 and perplexity is 23.53100527400659
At time: 174.97021889686584 and batch: 800, loss is 3.0824096250534057 and perplexity is 21.810895206329196
At time: 175.71841621398926 and batch: 850, loss is 3.1401105737686157 and perplexity is 23.10642168159594
At time: 176.4683437347412 and batch: 900, loss is 3.103512668609619 and perplexity is 22.27606243709132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39063963171554 and perplexity of 80.6920156496852
Annealing...
Model not improving. Stopping early with 79.32896323710268 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7ebb4a0b70>
ELAPSED
374.36091351509094


RESULTS SO FAR:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.32896323710268, 'params': {'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'dropout': 0.2674339239639524, 'rnn_dropout': 0.8396743911700816, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0319569110870361 and batch: 50, loss is 6.744161691665649 and perplexity is 849.0870315063238
At time: 1.8716437816619873 and batch: 100, loss is 5.933675031661988 and perplexity is 377.5394368320814
At time: 2.6888372898101807 and batch: 150, loss is 5.747422618865967 and perplexity is 313.3819138769538
At time: 3.5088696479797363 and batch: 200, loss is 5.578891315460205 and perplexity is 264.77788784530924
At time: 4.326603889465332 and batch: 250, loss is 5.622661991119385 and perplexity is 276.6247766940529
At time: 5.142452001571655 and batch: 300, loss is 5.532703237533569 and perplexity is 252.8264380083039
At time: 5.96094274520874 and batch: 350, loss is 5.504749784469604 and perplexity is 245.85693075956632
At time: 6.780432939529419 and batch: 400, loss is 5.358604698181153 and perplexity is 212.42833792218178
At time: 7.603551387786865 and batch: 450, loss is 5.361978673934937 and perplexity is 213.14627645628428
At time: 8.425729274749756 and batch: 500, loss is 5.313306217193603 and perplexity is 203.02034936469298
At time: 9.24268913269043 and batch: 550, loss is 5.3681558322906495 and perplexity is 214.46698968670202
At time: 10.060078859329224 and batch: 600, loss is 5.284398908615112 and perplexity is 197.2355911946767
At time: 10.877197742462158 and batch: 650, loss is 5.184070177078247 and perplexity is 178.40748527375763
At time: 11.695849657058716 and batch: 700, loss is 5.275511512756347 and perplexity is 195.49044679961466
At time: 12.514963150024414 and batch: 750, loss is 5.255621471405029 and perplexity is 191.64054797430373
At time: 13.334249496459961 and batch: 800, loss is 5.222343282699585 and perplexity is 185.36804527597965
At time: 14.151505947113037 and batch: 850, loss is 5.268962097167969 and perplexity is 194.21428224770773
At time: 14.976427793502808 and batch: 900, loss is 5.176628866195679 and perplexity is 177.08482697117154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.112838954141695 and perplexity of 166.14135366950478
finished 1 epochs...
Completing Train Step...
At time: 16.894953966140747 and batch: 50, loss is 5.055847873687744 and perplexity is 156.93753715808694
At time: 17.644347429275513 and batch: 100, loss is 4.902418556213379 and perplexity is 134.61496014108258
At time: 18.394450187683105 and batch: 150, loss is 4.866069307327271 and perplexity is 129.80967085708483
At time: 19.14370608329773 and batch: 200, loss is 4.737257633209229 and perplexity is 114.1208110381936
At time: 19.905943632125854 and batch: 250, loss is 4.827018098831177 and perplexity is 124.83815006325294
At time: 20.657012462615967 and batch: 300, loss is 4.7723439598083495 and perplexity is 118.19596411056021
At time: 21.407602787017822 and batch: 350, loss is 4.74246654510498 and perplexity is 114.716807186492
At time: 22.156821250915527 and batch: 400, loss is 4.618178148269653 and perplexity is 101.30929338588827
At time: 22.906013011932373 and batch: 450, loss is 4.628665103912353 and perplexity is 102.37730978409866
At time: 23.655353307724 and batch: 500, loss is 4.522407846450806 and perplexity is 92.05699042935568
At time: 24.406668424606323 and batch: 550, loss is 4.599707326889038 and perplexity is 99.45520351815868
At time: 25.16389012336731 and batch: 600, loss is 4.554298429489136 and perplexity is 95.04005454107676
At time: 25.92172932624817 and batch: 650, loss is 4.4115185546875 and perplexity is 82.39448908871199
At time: 26.679197788238525 and batch: 700, loss is 4.446357040405274 and perplexity is 85.31557599985747
At time: 27.436800718307495 and batch: 750, loss is 4.506040592193603 and perplexity is 90.56253369946285
At time: 28.198851346969604 and batch: 800, loss is 4.435589942932129 and perplexity is 84.40190250971284
At time: 28.956823348999023 and batch: 850, loss is 4.503146152496338 and perplexity is 90.30078489763908
At time: 29.714648962020874 and batch: 900, loss is 4.439224691390991 and perplexity is 84.70924040439105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.563170968669734 and perplexity of 95.88705310625534
finished 2 epochs...
Completing Train Step...
At time: 31.569438457489014 and batch: 50, loss is 4.474397478103637 and perplexity is 87.74171817357795
At time: 32.33943510055542 and batch: 100, loss is 4.328272137641907 and perplexity is 75.81317859599672
At time: 33.10086178779602 and batch: 150, loss is 4.319543871879578 and perplexity is 75.1543404647387
At time: 33.86004424095154 and batch: 200, loss is 4.2128663873672485 and perplexity is 67.54988671611329
At time: 34.61642360687256 and batch: 250, loss is 4.357264709472656 and perplexity is 78.04337091053345
At time: 35.37381386756897 and batch: 300, loss is 4.324934458732605 and perplexity is 75.56056036254064
At time: 36.13693451881409 and batch: 350, loss is 4.313902969360352 and perplexity is 74.73159560851276
At time: 36.894179344177246 and batch: 400, loss is 4.225122876167298 and perplexity is 68.38290566169421
At time: 37.655497550964355 and batch: 450, loss is 4.243511581420899 and perplexity is 69.65201160428623
At time: 38.435266971588135 and batch: 500, loss is 4.123921999931335 and perplexity is 61.80115168263429
At time: 39.19861102104187 and batch: 550, loss is 4.209284300804138 and perplexity is 67.30835003548694
At time: 39.961273193359375 and batch: 600, loss is 4.209441933631897 and perplexity is 67.31896087732133
At time: 40.71411204338074 and batch: 650, loss is 4.053285293579101 and perplexity is 57.586334631813166
At time: 41.467201709747314 and batch: 700, loss is 4.0707453346252445 and perplexity is 58.6006233925933
At time: 42.22304439544678 and batch: 750, loss is 4.169672675132752 and perplexity is 64.69427259711237
At time: 42.979012966156006 and batch: 800, loss is 4.111203331947326 and perplexity is 61.02010084532347
At time: 43.73759984970093 and batch: 850, loss is 4.185922122001648 and perplexity is 65.75410627777225
At time: 44.495824098587036 and batch: 900, loss is 4.136184978485107 and perplexity is 62.563683781891726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.419299818065069 and perplexity of 83.03812320489672
finished 3 epochs...
Completing Train Step...
At time: 46.35370635986328 and batch: 50, loss is 4.200181193351746 and perplexity is 66.6984152555187
At time: 47.125524044036865 and batch: 100, loss is 4.059468040466308 and perplexity is 57.94347929282999
At time: 47.884516954422 and batch: 150, loss is 4.0564891052246095 and perplexity is 57.77112626205246
At time: 48.642277002334595 and batch: 200, loss is 3.9508227348327636 and perplexity is 51.97811345233317
At time: 49.401869773864746 and batch: 250, loss is 4.104374499320984 and perplexity is 60.60482433051119
At time: 50.16250491142273 and batch: 300, loss is 4.078458299636841 and perplexity is 59.05435551108712
At time: 50.92356848716736 and batch: 350, loss is 4.061661806106567 and perplexity is 58.07073323841108
At time: 51.688405990600586 and batch: 400, loss is 3.990292582511902 and perplexity is 54.07070719253436
At time: 52.44707655906677 and batch: 450, loss is 4.012346253395081 and perplexity is 55.2764110036746
At time: 53.20483160018921 and batch: 500, loss is 3.891830325126648 and perplexity is 49.00049132624831
At time: 53.96470069885254 and batch: 550, loss is 3.9762673234939574 and perplexity is 53.3176448107584
At time: 54.72671413421631 and batch: 600, loss is 3.9946949100494384 and perplexity is 54.30926888384695
At time: 55.49410080909729 and batch: 650, loss is 3.834175672531128 and perplexity is 46.255282448131766
At time: 56.25601649284363 and batch: 700, loss is 3.844305992126465 and perplexity is 46.72624471380033
At time: 57.01640748977661 and batch: 750, loss is 3.953242788314819 and perplexity is 52.10405559868239
At time: 57.7885377407074 and batch: 800, loss is 3.9029783535003664 and perplexity is 49.549806394870735
At time: 58.55313158035278 and batch: 850, loss is 3.975431580543518 and perplexity is 53.2731035800761
At time: 59.3135826587677 and batch: 900, loss is 3.9331556510925294 and perplexity is 51.0678760649792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370501374545163 and perplexity of 79.08327210096957
finished 4 epochs...
Completing Train Step...
At time: 61.19559359550476 and batch: 50, loss is 4.004959907531738 and perplexity is 54.869624496291955
At time: 61.957449436187744 and batch: 100, loss is 3.870151090621948 and perplexity is 47.94963027300084
At time: 62.71792435646057 and batch: 150, loss is 3.868761353492737 and perplexity is 47.88303917425738
At time: 63.477928161621094 and batch: 200, loss is 3.762031526565552 and perplexity is 43.03576553336196
At time: 64.2378613948822 and batch: 250, loss is 3.9165411901474 and perplexity is 50.226420355278876
At time: 64.99697637557983 and batch: 300, loss is 3.8977542686462403 and perplexity is 49.29162895933486
At time: 65.76026558876038 and batch: 350, loss is 3.8792901945114138 and perplexity is 48.389855493458136
At time: 66.51901626586914 and batch: 400, loss is 3.8130789470672606 and perplexity is 45.28966890862619
At time: 67.27839612960815 and batch: 450, loss is 3.8351993417739867 and perplexity is 46.30265680179108
At time: 68.03757882118225 and batch: 500, loss is 3.720795202255249 and perplexity is 41.29722069802659
At time: 68.79574394226074 and batch: 550, loss is 3.803223009109497 and perplexity is 44.84548924096895
At time: 69.55446720123291 and batch: 600, loss is 3.826822476387024 and perplexity is 45.91640572400667
At time: 70.31469368934631 and batch: 650, loss is 3.6681391191482544 and perplexity is 39.178930660429266
At time: 71.07463669776917 and batch: 700, loss is 3.6712069416046145 and perplexity is 39.299309219336415
At time: 71.83397698402405 and batch: 750, loss is 3.7866137266159057 and perplexity is 44.10678946017842
At time: 72.59313297271729 and batch: 800, loss is 3.7408658027648927 and perplexity is 42.13445450446329
At time: 73.35302495956421 and batch: 850, loss is 3.809343738555908 and perplexity is 45.120818094655206
At time: 74.11213541030884 and batch: 900, loss is 3.775605616569519 and perplexity is 43.62391968592371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366672463613014 and perplexity of 78.78104825918872
finished 5 epochs...
Completing Train Step...
At time: 75.98083925247192 and batch: 50, loss is 3.8519525051116945 and perplexity is 47.08490706134884
At time: 76.75000834465027 and batch: 100, loss is 3.721091446876526 and perplexity is 41.30945658985116
At time: 77.50375533103943 and batch: 150, loss is 3.721631255149841 and perplexity is 41.3317617960103
At time: 78.2598466873169 and batch: 200, loss is 3.6147928190231324 and perplexity is 37.14364967079204
At time: 79.01469397544861 and batch: 250, loss is 3.7697213172912596 and perplexity is 43.36797724625716
At time: 79.78056144714355 and batch: 300, loss is 3.752764253616333 and perplexity is 42.638783658421744
At time: 80.54473066329956 and batch: 350, loss is 3.7328242826461793 and perplexity is 41.79698813074085
At time: 81.30330729484558 and batch: 400, loss is 3.6687645006179808 and perplexity is 39.20344010074409
At time: 82.06061911582947 and batch: 450, loss is 3.691381974220276 and perplexity is 40.10022616093863
At time: 82.81745648384094 and batch: 500, loss is 3.5815306901931763 and perplexity is 35.92849417175977
At time: 83.57337927818298 and batch: 550, loss is 3.6612879276275634 and perplexity is 38.9114257130755
At time: 84.33194756507874 and batch: 600, loss is 3.6894917392730715 and perplexity is 40.02449890575824
At time: 85.09389281272888 and batch: 650, loss is 3.531448006629944 and perplexity is 34.17341513763118
At time: 85.8499870300293 and batch: 700, loss is 3.5352855110168457 and perplexity is 34.304807716691165
At time: 86.6135630607605 and batch: 750, loss is 3.6495228719711306 and perplexity is 38.45631308616341
At time: 87.37357521057129 and batch: 800, loss is 3.6066290760040283 and perplexity is 36.84165284956437
At time: 88.1324417591095 and batch: 850, loss is 3.6765331649780273 and perplexity is 39.50918454393261
At time: 88.89568138122559 and batch: 900, loss is 3.6407390022277832 and perplexity is 38.119997081600836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380498598699701 and perplexity of 79.87785047027054
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 90.75860381126404 and batch: 50, loss is 3.7407716703414917 and perplexity is 42.13048847282113
At time: 91.52669167518616 and batch: 100, loss is 3.620971693992615 and perplexity is 37.37386614498032
At time: 92.28189468383789 and batch: 150, loss is 3.624357147216797 and perplexity is 37.50060803894562
At time: 93.04059362411499 and batch: 200, loss is 3.5009974813461304 and perplexity is 33.14850048419345
At time: 93.80071902275085 and batch: 250, loss is 3.641265640258789 and perplexity is 38.140077808979086
At time: 94.56352663040161 and batch: 300, loss is 3.6127477931976317 and perplexity is 37.067767564840835
At time: 95.31634187698364 and batch: 350, loss is 3.5782315349578857 and perplexity is 35.81015580777052
At time: 96.08930945396423 and batch: 400, loss is 3.5046483945846556 and perplexity is 33.26974397350569
At time: 96.8422122001648 and batch: 450, loss is 3.5133406114578247 and perplexity is 33.56019229461288
At time: 97.60213828086853 and batch: 500, loss is 3.391253275871277 and perplexity is 29.7031552004455
At time: 98.35863637924194 and batch: 550, loss is 3.442150502204895 and perplexity is 31.254097956871156
At time: 99.11146473884583 and batch: 600, loss is 3.465220923423767 and perplexity is 31.98352490250112
At time: 99.86947274208069 and batch: 650, loss is 3.2891041612625123 and perplexity is 26.818827546656856
At time: 100.62114810943604 and batch: 700, loss is 3.272560381889343 and perplexity is 26.37879273637827
At time: 101.37197494506836 and batch: 750, loss is 3.367468547821045 and perplexity is 29.005009249783495
At time: 102.1233422756195 and batch: 800, loss is 3.303890571594238 and perplexity is 27.21832804572734
At time: 102.87815237045288 and batch: 850, loss is 3.350376105308533 and perplexity is 28.513455689381143
At time: 103.63302111625671 and batch: 900, loss is 3.2968772935867308 and perplexity is 27.02810616340328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356056108866652 and perplexity of 77.94910462177792
finished 7 epochs...
Completing Train Step...
At time: 105.5408444404602 and batch: 50, loss is 3.6236344003677368 and perplexity is 37.47351438475363
At time: 106.30244016647339 and batch: 100, loss is 3.499028925895691 and perplexity is 33.08331000947579
At time: 107.06458568572998 and batch: 150, loss is 3.4994433689117432 and perplexity is 33.097023997894176
At time: 107.82479214668274 and batch: 200, loss is 3.382012825012207 and perplexity is 29.42994887327209
At time: 108.58390092849731 and batch: 250, loss is 3.5214150667190554 and perplexity is 33.83226952535489
At time: 109.34291744232178 and batch: 300, loss is 3.501543116569519 and perplexity is 33.16659240900701
At time: 110.10131859779358 and batch: 350, loss is 3.4693893146514894 and perplexity is 32.11712299825728
At time: 110.85868716239929 and batch: 400, loss is 3.4020755004882814 and perplexity is 30.026355134560173
At time: 111.6149070262909 and batch: 450, loss is 3.4145020818710328 and perplexity is 30.401808046914113
At time: 112.37415432929993 and batch: 500, loss is 3.3000330781936644 and perplexity is 27.113535772611897
At time: 113.13408184051514 and batch: 550, loss is 3.3547590827941893 and perplexity is 28.638703803030666
At time: 113.89495635032654 and batch: 600, loss is 3.3854839611053467 and perplexity is 29.53228173391954
At time: 114.67342257499695 and batch: 650, loss is 3.2158182144165037 and perplexity is 24.92367648196306
At time: 115.42789316177368 and batch: 700, loss is 3.2059014654159546 and perplexity is 24.677736118066612
At time: 116.18346405029297 and batch: 750, loss is 3.307996768951416 and perplexity is 27.33032164897538
At time: 116.94287061691284 and batch: 800, loss is 3.2522460508346556 and perplexity is 25.848331433257293
At time: 117.70297646522522 and batch: 850, loss is 3.308271131515503 and perplexity is 27.337821094836865
At time: 118.46416234970093 and batch: 900, loss is 3.263850436210632 and perplexity is 26.150032575067453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3759092566085185 and perplexity of 79.51210359965653
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.34094429016113 and batch: 50, loss is 3.5698010206222532 and perplexity is 35.509526785077405
At time: 121.1143479347229 and batch: 100, loss is 3.4580332612991334 and perplexity is 31.75446233029533
At time: 121.88087177276611 and batch: 150, loss is 3.4621532773971557 and perplexity is 31.88556110536707
At time: 122.63743305206299 and batch: 200, loss is 3.344130001068115 and perplexity is 28.335912726037535
At time: 123.39593648910522 and batch: 250, loss is 3.4805637407302856 and perplexity is 32.47802610216954
At time: 124.15473747253418 and batch: 300, loss is 3.457584443092346 and perplexity is 31.74021354725057
At time: 124.914724111557 and batch: 350, loss is 3.4201553106307983 and perplexity is 30.574163144887773
At time: 125.6728253364563 and batch: 400, loss is 3.352136721611023 and perplexity is 28.563701162828927
At time: 126.43072652816772 and batch: 450, loss is 3.3587663698196413 and perplexity is 28.753697561820996
At time: 127.19113254547119 and batch: 500, loss is 3.2371796131134034 and perplexity is 25.461808233005858
At time: 127.95507383346558 and batch: 550, loss is 3.281438989639282 and perplexity is 26.614042489756823
At time: 128.7189757823944 and batch: 600, loss is 3.3098221731185915 and perplexity is 27.380256093409596
At time: 129.47907042503357 and batch: 650, loss is 3.134625015258789 and perplexity is 22.980017070427706
At time: 130.24571251869202 and batch: 700, loss is 3.114879469871521 and perplexity is 22.530714559909825
At time: 131.00665068626404 and batch: 750, loss is 3.2133566999435423 and perplexity is 24.862401936612212
At time: 131.76467108726501 and batch: 800, loss is 3.147758083343506 and perplexity is 23.28380567101059
At time: 132.52646899223328 and batch: 850, loss is 3.194833741188049 and perplexity is 24.406115623171747
At time: 133.2886106967926 and batch: 900, loss is 3.1525295782089233 and perplexity is 23.39516970538946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371921487050514 and perplexity of 79.19565902679591
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.19716262817383 and batch: 50, loss is 3.5457287549972536 and perplexity is 34.664938386580815
At time: 135.97072625160217 and batch: 100, loss is 3.4329065990448 and perplexity is 30.966519324307388
At time: 136.72918844223022 and batch: 150, loss is 3.435114097595215 and perplexity is 31.034953377073123
At time: 137.48668003082275 and batch: 200, loss is 3.317516412734985 and perplexity is 27.59173890091087
At time: 138.25156426429749 and batch: 250, loss is 3.455096096992493 and perplexity is 31.66133109478569
At time: 139.00950241088867 and batch: 300, loss is 3.4326629209518433 and perplexity is 30.95897438123894
At time: 139.76782822608948 and batch: 350, loss is 3.3950884532928467 and perplexity is 29.817290795851154
At time: 140.52786588668823 and batch: 400, loss is 3.3290635204315184 and perplexity is 27.912190264109604
At time: 141.28675055503845 and batch: 450, loss is 3.3342121601104737 and perplexity is 28.05627066518556
At time: 142.04497289657593 and batch: 500, loss is 3.2116758584976197 and perplexity is 24.820647282299266
At time: 142.8029911518097 and batch: 550, loss is 3.255222382545471 and perplexity is 25.92537924481851
At time: 143.56144905090332 and batch: 600, loss is 3.2838007736206056 and perplexity is 26.676973394336073
At time: 144.3190689086914 and batch: 650, loss is 3.1081595993041993 and perplexity is 22.379818642623825
At time: 145.0758810043335 and batch: 700, loss is 3.0865722465515137 and perplexity is 21.901874933321505
At time: 145.8343551158905 and batch: 750, loss is 3.1818960189819334 and perplexity is 24.092389903275702
At time: 146.59833908081055 and batch: 800, loss is 3.1157441186904906 and perplexity is 22.550204140251676
At time: 147.3583037853241 and batch: 850, loss is 3.1588891458511354 and perplexity is 23.54442697242717
At time: 148.11673045158386 and batch: 900, loss is 3.118358364105225 and perplexity is 22.609233032413663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37232992093857 and perplexity of 79.22801182426907
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.0017602443695 and batch: 50, loss is 3.5369034719467165 and perplexity is 34.360356481030756
At time: 150.75988674163818 and batch: 100, loss is 3.4233594369888305 and perplexity is 30.672283738339324
At time: 151.52596759796143 and batch: 150, loss is 3.4243403816223146 and perplexity is 30.7023863125346
At time: 152.28898930549622 and batch: 200, loss is 3.3072088718414308 and perplexity is 27.30879664838959
At time: 153.05973076820374 and batch: 250, loss is 3.445450267791748 and perplexity is 31.35739949545561
At time: 153.8192868232727 and batch: 300, loss is 3.423210425376892 and perplexity is 30.66771355241148
At time: 154.57976412773132 and batch: 350, loss is 3.3859103679656983 and perplexity is 29.5448771866556
At time: 155.3380093574524 and batch: 400, loss is 3.3204218435287474 and perplexity is 27.672021359855684
At time: 156.09575295448303 and batch: 450, loss is 3.3256694412231447 and perplexity is 27.817614668795624
At time: 156.85345005989075 and batch: 500, loss is 3.2026136541366577 and perplexity is 24.596733612353827
At time: 157.61207818984985 and batch: 550, loss is 3.246587452888489 and perplexity is 25.70247916694351
At time: 158.3718934059143 and batch: 600, loss is 3.2757221460342407 and perplexity is 26.462328247639693
At time: 159.13150644302368 and batch: 650, loss is 3.1000808000564577 and perplexity is 22.19974494962169
At time: 159.89473819732666 and batch: 700, loss is 3.0785821866989136 and perplexity is 21.72757490284852
At time: 160.66618418693542 and batch: 750, loss is 3.1723498773574828 and perplexity is 23.863494807928564
At time: 161.4299294948578 and batch: 800, loss is 3.10612841129303 and perplexity is 22.334407158548732
At time: 162.1913616657257 and batch: 850, loss is 3.1486582803726195 and perplexity is 23.30477512060415
At time: 162.95160293579102 and batch: 900, loss is 3.1083721017837522 and perplexity is 22.38457491491926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37264512989619 and perplexity of 79.25298913962045
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 164.83697509765625 and batch: 50, loss is 3.5339669227600097 and perplexity is 34.25960360943502
At time: 165.61937952041626 and batch: 100, loss is 3.4208455657958985 and perplexity is 30.595274404156818
At time: 166.38976526260376 and batch: 150, loss is 3.4217187213897704 and perplexity is 30.622000505473284
At time: 167.15086579322815 and batch: 200, loss is 3.3046849584579467 and perplexity is 27.239958518323125
At time: 167.91659474372864 and batch: 250, loss is 3.442666049003601 and perplexity is 31.270215061223073
At time: 168.67971301078796 and batch: 300, loss is 3.4204987716674804 and perplexity is 30.584665982211654
At time: 169.4403703212738 and batch: 350, loss is 3.3827266311645507 and perplexity is 29.45096365118759
At time: 170.19916009902954 and batch: 400, loss is 3.317639799118042 and perplexity is 27.595143555815813
At time: 170.95758414268494 and batch: 450, loss is 3.323529796600342 and perplexity is 27.75815848938055
At time: 171.71630930900574 and batch: 500, loss is 3.2003057384490967 and perplexity is 24.54003188156248
At time: 172.49133491516113 and batch: 550, loss is 3.2441270971298217 and perplexity is 25.639319653609824
At time: 173.26118516921997 and batch: 600, loss is 3.2735232305526734 and perplexity is 26.404203753223392
At time: 174.02098631858826 and batch: 650, loss is 3.0976775789260866 and perplexity is 22.146458109146796
At time: 174.7794635295868 and batch: 700, loss is 3.076417121887207 and perplexity is 21.680584182316167
At time: 175.5384407043457 and batch: 750, loss is 3.169887170791626 and perplexity is 23.804798328349413
At time: 176.29980492591858 and batch: 800, loss is 3.103480439186096 and perplexity is 22.275344504009958
At time: 177.06453037261963 and batch: 850, loss is 3.1459558534622194 and perplexity is 23.24188069125368
At time: 177.8248748779297 and batch: 900, loss is 3.105641484260559 and perplexity is 22.32353457924024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372475820044949 and perplexity of 79.23957196368089
Annealing...
Model not improving. Stopping early with 77.94910462177792 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7ebb4a0b70>
ELAPSED
559.0222856998444


RESULTS SO FAR:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.32896323710268, 'params': {'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -77.94910462177792, 'params': {'dropout': 0.2674339239639524, 'rnn_dropout': 0.8396743911700816, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'dropout': 0.7587669668369824, 'rnn_dropout': 0.7297519073495046, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1005237102508545 and batch: 50, loss is 6.846928749084473 and perplexity is 940.9864586360804
At time: 1.9309720993041992 and batch: 100, loss is 6.124513301849365 and perplexity is 456.92227591670337
At time: 2.7540042400360107 and batch: 150, loss is 6.0301197910308835 and perplexity is 415.7648312967612
At time: 3.578512668609619 and batch: 200, loss is 5.893171272277832 and perplexity is 362.55321817493945
At time: 4.396850109100342 and batch: 250, loss is 5.9485178565979 and perplexity is 383.1849828783164
At time: 5.222860097885132 and batch: 300, loss is 5.857717761993408 and perplexity is 349.92462074683596
At time: 6.041193723678589 and batch: 350, loss is 5.85571515083313 and perplexity is 349.2245590057637
At time: 6.888587951660156 and batch: 400, loss is 5.72390944480896 and perplexity is 306.09926501119173
At time: 7.713080406188965 and batch: 450, loss is 5.7311048412323 and perplexity is 308.3097135577008
At time: 8.532787561416626 and batch: 500, loss is 5.687366209030151 and perplexity is 295.11532407954303
At time: 9.359418153762817 and batch: 550, loss is 5.745640640258789 and perplexity is 312.8239712790784
At time: 10.179148197174072 and batch: 600, loss is 5.679667177200318 and perplexity is 292.8519458964715
At time: 10.997945308685303 and batch: 650, loss is 5.5922323513031005 and perplexity is 268.3339672857091
At time: 11.823570489883423 and batch: 700, loss is 5.693973636627197 and perplexity is 297.071733515271
At time: 12.642559289932251 and batch: 750, loss is 5.641978073120117 and perplexity is 282.0200233261665
At time: 13.465022325515747 and batch: 800, loss is 5.644964647293091 and perplexity is 282.8635560536233
At time: 14.298522472381592 and batch: 850, loss is 5.685925874710083 and perplexity is 294.69056532063473
At time: 15.129535436630249 and batch: 900, loss is 5.580042066574097 and perplexity is 265.08275667511094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.4522813770869005 and perplexity of 233.28978123108772
finished 1 epochs...
Completing Train Step...
At time: 17.04085111618042 and batch: 50, loss is 5.342994995117188 and perplexity is 209.13814095166487
At time: 17.79023838043213 and batch: 100, loss is 5.135839614868164 and perplexity is 170.00700037317247
At time: 18.536926746368408 and batch: 150, loss is 5.064326734542846 and perplexity is 158.27384588073852
At time: 19.286916255950928 and batch: 200, loss is 4.903023176193237 and perplexity is 134.69637564581183
At time: 20.037018060684204 and batch: 250, loss is 4.964718570709229 and perplexity is 143.2682247243405
At time: 20.79239273071289 and batch: 300, loss is 4.894530754089356 and perplexity is 133.55732068157175
At time: 21.549625396728516 and batch: 350, loss is 4.859842472076416 and perplexity is 129.00387879351487
At time: 22.306683778762817 and batch: 400, loss is 4.718791370391846 and perplexity is 112.03276469042369
At time: 23.06513738632202 and batch: 450, loss is 4.720893383026123 and perplexity is 112.26850665675052
At time: 23.821463584899902 and batch: 500, loss is 4.621525659561157 and perplexity is 101.6489956507951
At time: 24.57834815979004 and batch: 550, loss is 4.690057859420777 and perplexity is 108.85947817155743
At time: 25.333240509033203 and batch: 600, loss is 4.6294691848754885 and perplexity is 102.45966253465548
At time: 26.088650941848755 and batch: 650, loss is 4.490902032852173 and perplexity is 89.20187261900197
At time: 26.844688415527344 and batch: 700, loss is 4.52904182434082 and perplexity is 92.66972465395058
At time: 27.600011348724365 and batch: 750, loss is 4.5754115104675295 and perplexity is 97.0679754072151
At time: 28.356666326522827 and batch: 800, loss is 4.508267936706543 and perplexity is 90.76447247209892
At time: 29.113688468933105 and batch: 850, loss is 4.570064468383789 and perplexity is 96.55033401619735
At time: 29.86937713623047 and batch: 900, loss is 4.5040743541717525 and perplexity is 90.38464114920829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.605596254949701 and perplexity of 100.04261597418818
finished 2 epochs...
Completing Train Step...
At time: 31.731640338897705 and batch: 50, loss is 4.529335861206055 and perplexity is 92.69697697568726
At time: 32.49638390541077 and batch: 100, loss is 4.384555377960205 and perplexity is 80.20255546034866
At time: 33.24858570098877 and batch: 150, loss is 4.375673141479492 and perplexity is 79.49333180529547
At time: 34.00065517425537 and batch: 200, loss is 4.261343450546264 and perplexity is 70.90517709488583
At time: 34.75634717941284 and batch: 250, loss is 4.409159631729126 and perplexity is 82.20035589939717
At time: 35.51356029510498 and batch: 300, loss is 4.37044406414032 and perplexity is 79.07873993650047
At time: 36.267369508743286 and batch: 350, loss is 4.35967188835144 and perplexity is 78.23146155771053
At time: 37.024248123168945 and batch: 400, loss is 4.267444052696228 and perplexity is 71.33906350923631
At time: 37.77596187591553 and batch: 450, loss is 4.282770256996155 and perplexity is 72.44084204607343
At time: 38.532960653305054 and batch: 500, loss is 4.1667641830444335 and perplexity is 64.50638318698705
At time: 39.289039611816406 and batch: 550, loss is 4.250104298591614 and perplexity is 70.1127246237836
At time: 40.043206453323364 and batch: 600, loss is 4.241359348297119 and perplexity is 69.50226543990453
At time: 40.80017042160034 and batch: 650, loss is 4.096472039222717 and perplexity is 60.12778450113284
At time: 41.551878929138184 and batch: 700, loss is 4.110097246170044 and perplexity is 60.95264469267295
At time: 42.302024602890015 and batch: 750, loss is 4.210379528999328 and perplexity is 67.38210842197687
At time: 43.05255126953125 and batch: 800, loss is 4.1528763008117675 and perplexity is 63.61671821454977
At time: 43.80360746383667 and batch: 850, loss is 4.226928501129151 and perplexity is 68.50649108400579
At time: 44.55340576171875 and batch: 900, loss is 4.174542980194092 and perplexity is 65.01012195742561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436849097683005 and perplexity of 84.5082445027036
finished 3 epochs...
Completing Train Step...
At time: 46.41297173500061 and batch: 50, loss is 4.23363842010498 and perplexity is 68.96770972791012
At time: 47.18670153617859 and batch: 100, loss is 4.091153798103332 and perplexity is 59.80885925755178
At time: 47.93636417388916 and batch: 150, loss is 4.091856508255005 and perplexity is 59.85090232042551
At time: 48.68616843223572 and batch: 200, loss is 3.9769478845596313 and perplexity is 53.35394307412143
At time: 49.43754458427429 and batch: 250, loss is 4.13674674987793 and perplexity is 62.59884014366458
At time: 50.19820809364319 and batch: 300, loss is 4.1058219528198245 and perplexity is 60.69261051341694
At time: 50.94824266433716 and batch: 350, loss is 4.098138637542725 and perplexity is 60.22807691613947
At time: 51.69795322418213 and batch: 400, loss is 4.015998845100403 and perplexity is 55.47868234643869
At time: 52.44621396064758 and batch: 450, loss is 4.037191662788391 and perplexity is 56.66697915054847
At time: 53.193604946136475 and batch: 500, loss is 3.9204413318634033 and perplexity is 50.422693009341316
At time: 53.9433708190918 and batch: 550, loss is 4.004148144721984 and perplexity is 54.8251014492625
At time: 54.698578119277954 and batch: 600, loss is 4.013805527687072 and perplexity is 55.357133332891
At time: 55.44928050041199 and batch: 650, loss is 3.8605982875823974 and perplexity is 47.4937577959306
At time: 56.199066400527954 and batch: 700, loss is 3.8684045267105103 and perplexity is 47.865956271465464
At time: 56.94837713241577 and batch: 750, loss is 3.9817445039749146 and perplexity is 53.61047638783907
At time: 57.69697165489197 and batch: 800, loss is 3.9319298791885378 and perplexity is 51.005316846801236
At time: 58.44530010223389 and batch: 850, loss is 4.008015999794006 and perplexity is 55.03756762528562
At time: 59.19155406951904 and batch: 900, loss is 3.960366101264954 and perplexity is 52.47653415834542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38404323630137 and perplexity of 80.1614909068842
finished 4 epochs...
Completing Train Step...
At time: 61.03678035736084 and batch: 50, loss is 4.029992322921753 and perplexity is 56.260479329367904
At time: 61.78411912918091 and batch: 100, loss is 3.888356103897095 and perplexity is 48.83054816003954
At time: 62.53292655944824 and batch: 150, loss is 3.894211368560791 and perplexity is 49.11730263560386
At time: 63.28243088722229 and batch: 200, loss is 3.779895987510681 and perplexity is 43.8114845570068
At time: 64.03296256065369 and batch: 250, loss is 3.9383392667770387 and perplexity is 51.33327958897946
At time: 64.7821249961853 and batch: 300, loss is 3.915532937049866 and perplexity is 50.175804932244006
At time: 65.53141403198242 and batch: 350, loss is 3.9081372880935668 and perplexity is 49.806091114811835
At time: 66.28057646751404 and batch: 400, loss is 3.831662826538086 and perplexity is 46.13919596179065
At time: 67.03184938430786 and batch: 450, loss is 3.854238362312317 and perplexity is 47.192659541668675
At time: 67.78741264343262 and batch: 500, loss is 3.738537049293518 and perplexity is 42.036447908163794
At time: 68.55155158042908 and batch: 550, loss is 3.8234625101089477 and perplexity is 45.76238704284882
At time: 69.30306959152222 and batch: 600, loss is 3.842034869194031 and perplexity is 46.62024408367785
At time: 70.05361008644104 and batch: 650, loss is 3.6838965463638305 and perplexity is 39.80117945359877
At time: 70.80905246734619 and batch: 700, loss is 3.6938933658599855 and perplexity is 40.20106009742486
At time: 71.56265211105347 and batch: 750, loss is 3.8115516233444215 and perplexity is 45.22054972002704
At time: 72.31389546394348 and batch: 800, loss is 3.760830297470093 and perplexity is 42.98410075648701
At time: 73.07774376869202 and batch: 850, loss is 3.839242115020752 and perplexity is 46.49022684005926
At time: 73.83621764183044 and batch: 900, loss is 3.7939212322235107 and perplexity is 44.43028058858506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370803623983305 and perplexity of 79.10717858820766
finished 5 epochs...
Completing Train Step...
At time: 75.66294813156128 and batch: 50, loss is 3.8696137857437134 and perplexity is 47.92387362295213
At time: 76.42436790466309 and batch: 100, loss is 3.732574486732483 and perplexity is 41.7865487178166
At time: 77.1706862449646 and batch: 150, loss is 3.7364687395095824 and perplexity is 41.94959336368845
At time: 77.91836857795715 and batch: 200, loss is 3.6259121656417848 and perplexity is 37.55896753868328
At time: 78.66766548156738 and batch: 250, loss is 3.7805250215530397 and perplexity is 43.83905214180508
At time: 79.41900205612183 and batch: 300, loss is 3.7579160261154176 and perplexity is 42.85901577726048
At time: 80.17334413528442 and batch: 350, loss is 3.7533621120452882 and perplexity is 42.66428323644163
At time: 80.92594456672668 and batch: 400, loss is 3.678779273033142 and perplexity is 39.598026178192406
At time: 81.67760038375854 and batch: 450, loss is 3.7031667852401733 and perplexity is 40.57559531425562
At time: 82.42813205718994 and batch: 500, loss is 3.5910349082946778 and perplexity is 36.271594282883264
At time: 83.17913842201233 and batch: 550, loss is 3.677777900695801 and perplexity is 39.558393656931266
At time: 83.92986345291138 and batch: 600, loss is 3.699256348609924 and perplexity is 40.417236847214724
At time: 84.67973756790161 and batch: 650, loss is 3.5409080743789674 and perplexity is 34.498231932093866
At time: 85.42912268638611 and batch: 700, loss is 3.5494962167739867 and perplexity is 34.795783539235266
At time: 86.17894172668457 and batch: 750, loss is 3.671193518638611 and perplexity is 39.29878170958518
At time: 86.92914032936096 and batch: 800, loss is 3.61830472946167 and perplexity is 37.274324166051954
At time: 87.69035768508911 and batch: 850, loss is 3.698316159248352 and perplexity is 40.37925484904001
At time: 88.44308757781982 and batch: 900, loss is 3.655545129776001 and perplexity is 38.68860567867497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3853258106806505 and perplexity of 80.26436994223079
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 90.279541015625 and batch: 50, loss is 3.752137532234192 and perplexity is 42.61206939306959
At time: 91.04419112205505 and batch: 100, loss is 3.6256256198883055 and perplexity is 37.54820671783739
At time: 91.79527807235718 and batch: 150, loss is 3.6246915578842165 and perplexity is 37.51315073939809
At time: 92.54634642601013 and batch: 200, loss is 3.4968372774124146 and perplexity is 33.01088242017097
At time: 93.29818797111511 and batch: 250, loss is 3.6406769514083863 and perplexity is 38.11763177793179
At time: 94.05394625663757 and batch: 300, loss is 3.6129036283493043 and perplexity is 37.073544476132696
At time: 94.80961489677429 and batch: 350, loss is 3.5922386264801025 and perplexity is 36.31528134871874
At time: 95.56449055671692 and batch: 400, loss is 3.5143314838409423 and perplexity is 33.59346264294226
At time: 96.31506299972534 and batch: 450, loss is 3.5186158084869383 and perplexity is 33.73769669501661
At time: 97.06727647781372 and batch: 500, loss is 3.396881494522095 and perplexity is 29.870802387500998
At time: 97.81756567955017 and batch: 550, loss is 3.4611495971679687 and perplexity is 31.853574253059932
At time: 98.56967997550964 and batch: 600, loss is 3.46853805065155 and perplexity is 32.08979448120891
At time: 99.32406640052795 and batch: 650, loss is 3.2945761680603027 and perplexity is 26.965982602848104
At time: 100.07668852806091 and batch: 700, loss is 3.280280199050903 and perplexity is 26.583220249510926
At time: 100.82798838615417 and batch: 750, loss is 3.3847075080871583 and perplexity is 29.50936020453016
At time: 101.58154082298279 and batch: 800, loss is 3.3099251127243043 and perplexity is 27.383074751249442
At time: 102.34219717979431 and batch: 850, loss is 3.3626216745376585 and perplexity is 28.86476579105493
At time: 103.09356451034546 and batch: 900, loss is 3.306248803138733 and perplexity is 27.282590909087087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369933245933219 and perplexity of 79.03835539180484
finished 7 epochs...
Completing Train Step...
At time: 104.97237610816956 and batch: 50, loss is 3.6366217803955077 and perplexity is 37.96337125027002
At time: 105.72319793701172 and batch: 100, loss is 3.5017335987091065 and perplexity is 33.17291065422923
At time: 106.48421025276184 and batch: 150, loss is 3.4998844385147097 and perplexity is 33.11162530898883
At time: 107.23377871513367 and batch: 200, loss is 3.378506088256836 and perplexity is 29.32692653153954
At time: 107.98232626914978 and batch: 250, loss is 3.5216238069534302 and perplexity is 33.839332418354374
At time: 108.72979092597961 and batch: 300, loss is 3.4978294467926023 and perplexity is 33.04365106025379
At time: 109.47808909416199 and batch: 350, loss is 3.4836808681488036 and perplexity is 32.57942219800607
At time: 110.2247724533081 and batch: 400, loss is 3.40958890914917 and perplexity is 30.252805051384392
At time: 110.97255110740662 and batch: 450, loss is 3.41877583026886 and perplexity is 30.532015764672114
At time: 111.71958947181702 and batch: 500, loss is 3.30436408996582 and perplexity is 27.231219476024403
At time: 112.46752619743347 and batch: 550, loss is 3.372800464630127 and perplexity is 29.160074576494132
At time: 113.21596956253052 and batch: 600, loss is 3.3882707023620604 and perplexity is 29.614695341197272
At time: 113.96513175964355 and batch: 650, loss is 3.2201762533187868 and perplexity is 25.03253185935779
At time: 114.7197995185852 and batch: 700, loss is 3.212950797080994 and perplexity is 24.852312264348267
At time: 115.47403287887573 and batch: 750, loss is 3.3262797164916993 and perplexity is 27.83459625225133
At time: 116.22626638412476 and batch: 800, loss is 3.2579396533966065 and perplexity is 25.995921319702717
At time: 116.97485637664795 and batch: 850, loss is 3.320219120979309 and perplexity is 27.666412185709756
At time: 117.72194242477417 and batch: 900, loss is 3.272798991203308 and perplexity is 26.38508771300648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.389670594097817 and perplexity of 80.61385992510527
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 119.55059862136841 and batch: 50, loss is 3.5817551946640016 and perplexity is 35.93656118483772
At time: 120.31790351867676 and batch: 100, loss is 3.4602738666534423 and perplexity is 31.8256913168408
At time: 121.06620526313782 and batch: 150, loss is 3.4632037019729616 and perplexity is 31.919072079659674
At time: 121.81506729125977 and batch: 200, loss is 3.335398015975952 and perplexity is 28.08956109324078
At time: 122.57026028633118 and batch: 250, loss is 3.477930178642273 and perplexity is 32.392605733226446
At time: 123.32018971443176 and batch: 300, loss is 3.4522815561294555 and perplexity is 31.572344271909348
At time: 124.07535552978516 and batch: 350, loss is 3.433471093177795 and perplexity is 30.984004677510576
At time: 124.84543633460999 and batch: 400, loss is 3.357149724960327 and perplexity is 28.70725059870939
At time: 125.59528398513794 and batch: 450, loss is 3.3600329542160035 and perplexity is 28.790139620089896
At time: 126.34499287605286 and batch: 500, loss is 3.2397546100616457 and perplexity is 25.527456797655233
At time: 127.0936496257782 and batch: 550, loss is 3.297824892997742 and perplexity is 27.053730119589954
At time: 127.84199643135071 and batch: 600, loss is 3.311276526451111 and perplexity is 27.42010563073403
At time: 128.59054136276245 and batch: 650, loss is 3.1370688247680665 and perplexity is 23.036244531286044
At time: 129.33745646476746 and batch: 700, loss is 3.122340726852417 and perplexity is 22.699450720332518
At time: 130.0932776927948 and batch: 750, loss is 3.2260907173156737 and perplexity is 25.18102456229663
At time: 130.84501433372498 and batch: 800, loss is 3.1522436141967773 and perplexity is 23.388480485279366
At time: 131.60036826133728 and batch: 850, loss is 3.2060678005218506 and perplexity is 24.68184123332001
At time: 132.35046339035034 and batch: 900, loss is 3.1561451864242556 and perplexity is 23.479910575753458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382846780019264 and perplexity of 80.0656385405231
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 134.18787670135498 and batch: 50, loss is 3.555887498855591 and perplexity is 35.01888540110376
At time: 134.951514005661 and batch: 100, loss is 3.4316949462890625 and perplexity is 30.92902137766525
At time: 135.70265817642212 and batch: 150, loss is 3.436838355064392 and perplexity is 31.088511788214944
At time: 136.45450282096863 and batch: 200, loss is 3.311049642562866 and perplexity is 27.413885156241847
At time: 137.20638346672058 and batch: 250, loss is 3.452287735939026 and perplexity is 31.572539383587515
At time: 137.9668483734131 and batch: 300, loss is 3.4271647787094115 and perplexity is 30.789224618204095
At time: 138.71819281578064 and batch: 350, loss is 3.4102526998519895 and perplexity is 30.27289324855337
At time: 139.46861028671265 and batch: 400, loss is 3.3334412574768066 and perplexity is 28.03465034688978
At time: 140.21928000450134 and batch: 450, loss is 3.3349113845825196 and perplexity is 28.075895156391695
At time: 140.96965408325195 and batch: 500, loss is 3.215035357475281 and perplexity is 24.90417244426012
At time: 141.71975421905518 and batch: 550, loss is 3.270748705863953 and perplexity is 26.331046173709833
At time: 142.4711151123047 and batch: 600, loss is 3.2833128595352172 and perplexity is 26.66396049810728
At time: 143.2243857383728 and batch: 650, loss is 3.1081017303466796 and perplexity is 22.378523583321723
At time: 143.98824405670166 and batch: 700, loss is 3.093985915184021 and perplexity is 22.06485155738462
At time: 144.73750472068787 and batch: 750, loss is 3.1946498012542723 and perplexity is 24.401626776731984
At time: 145.49292063713074 and batch: 800, loss is 3.1199189615249634 and perplexity is 22.644544489464046
At time: 146.24806308746338 and batch: 850, loss is 3.171453137397766 and perplexity is 23.842105050514274
At time: 147.0056917667389 and batch: 900, loss is 3.1214852857589723 and perplexity is 22.68004098051589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382782400470891 and perplexity of 80.06048411679515
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 148.88783192634583 and batch: 50, loss is 3.5473778295516967 and perplexity is 34.722150615055085
At time: 149.65022206306458 and batch: 100, loss is 3.421652412414551 and perplexity is 30.61997005931974
At time: 150.4179208278656 and batch: 150, loss is 3.427602128982544 and perplexity is 30.80269323903822
At time: 151.18692803382874 and batch: 200, loss is 3.3019983959198 and perplexity is 27.166874882064263
At time: 151.9536054134369 and batch: 250, loss is 3.44282479763031 and perplexity is 31.27517955896411
At time: 152.71340084075928 and batch: 300, loss is 3.417466778755188 and perplexity is 30.492073931881997
At time: 153.4719693660736 and batch: 350, loss is 3.4013616132736204 and perplexity is 30.00492735294643
At time: 154.23524641990662 and batch: 400, loss is 3.324767770767212 and perplexity is 27.792543652090362
At time: 154.99994683265686 and batch: 450, loss is 3.3265138339996336 and perplexity is 27.84111358144093
At time: 155.7639262676239 and batch: 500, loss is 3.2071487951278685 and perplexity is 24.708536596732177
At time: 156.5222065448761 and batch: 550, loss is 3.262890362739563 and perplexity is 26.124938670448095
At time: 157.28050112724304 and batch: 600, loss is 3.2749572706222536 and perplexity is 26.442095602132074
At time: 158.03806400299072 and batch: 650, loss is 3.0990804338455202 and perplexity is 22.17754817918331
At time: 158.80234265327454 and batch: 700, loss is 3.085717325210571 and perplexity is 21.88315855469008
At time: 159.5606472492218 and batch: 750, loss is 3.185295066833496 and perplexity is 24.174420423268348
At time: 160.32498049736023 and batch: 800, loss is 3.1108417320251465 and perplexity is 22.43992485688322
At time: 161.08573627471924 and batch: 850, loss is 3.1616672611236574 and perplexity is 23.60992704583214
At time: 161.84653043746948 and batch: 900, loss is 3.1113008737564085 and perplexity is 22.450230328486395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382843435627141 and perplexity of 80.06537077007998
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 163.70526719093323 and batch: 50, loss is 3.544734516143799 and perplexity is 34.630490285642146
At time: 164.47632026672363 and batch: 100, loss is 3.418953638076782 and perplexity is 30.537445078139534
At time: 165.23251962661743 and batch: 150, loss is 3.4250038719177245 and perplexity is 30.72276380728635
At time: 165.99442076683044 and batch: 200, loss is 3.299690537452698 and perplexity is 27.10424987246848
At time: 166.76076340675354 and batch: 250, loss is 3.4401090240478513 and perplexity is 31.19035848208376
At time: 167.51804614067078 and batch: 300, loss is 3.4147514152526854 and perplexity is 30.40938917759801
At time: 168.2755675315857 and batch: 350, loss is 3.398175001144409 and perplexity is 29.909465468287802
At time: 169.03513884544373 and batch: 400, loss is 3.3218558025360108 and perplexity is 27.711730367874427
At time: 169.79255032539368 and batch: 450, loss is 3.3241713142395017 and perplexity is 27.775971550767718
At time: 170.5493459701538 and batch: 500, loss is 3.20480055809021 and perplexity is 24.650583166775384
At time: 171.30603623390198 and batch: 550, loss is 3.2605404901504516 and perplexity is 26.063620466373763
At time: 172.0624816417694 and batch: 600, loss is 3.2728277921676634 and perplexity is 26.385847639920474
At time: 172.8203272819519 and batch: 650, loss is 3.096485385894775 and perplexity is 22.12007098851689
At time: 173.57916808128357 and batch: 700, loss is 3.0833343172073366 and perplexity is 21.831072897635362
At time: 174.33749413490295 and batch: 750, loss is 3.1827567481994627 and perplexity is 24.113135854227814
At time: 175.10586524009705 and batch: 800, loss is 3.108490090370178 and perplexity is 22.38721619508861
At time: 175.86279797554016 and batch: 850, loss is 3.1589912033081053 and perplexity is 23.546829979390093
At time: 176.6188886165619 and batch: 900, loss is 3.108540334701538 and perplexity is 22.388341054055996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382626050139127 and perplexity of 80.04796761204763
Annealing...
Model not improving. Stopping early with 79.03835539180484 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7ebb4a0b70>
ELAPSED
743.0650250911713


RESULTS SO FAR:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.32896323710268, 'params': {'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -77.94910462177792, 'params': {'dropout': 0.2674339239639524, 'rnn_dropout': 0.8396743911700816, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.03835539180484, 'params': {'dropout': 0.7587669668369824, 'rnn_dropout': 0.7297519073495046, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'dropout': 0.062420125232273804, 'rnn_dropout': 0.9603983682435051, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0391674041748047 and batch: 50, loss is 6.800767240524292 and perplexity is 898.5364208073
At time: 1.8717780113220215 and batch: 100, loss is 6.0471838760375975 and perplexity is 422.92035532622936
At time: 2.698169469833374 and batch: 150, loss is 5.91151312828064 and perplexity is 369.26447742023913
At time: 3.525209426879883 and batch: 200, loss is 5.748706636428833 and perplexity is 313.78456020541466
At time: 4.3507795333862305 and batch: 250, loss is 5.797215576171875 and perplexity is 329.3811451659389
At time: 5.173948526382446 and batch: 300, loss is 5.7157509231567385 and perplexity is 303.61210709146553
At time: 5.993014097213745 and batch: 350, loss is 5.685834417343139 and perplexity is 294.663614929892
At time: 6.814797878265381 and batch: 400, loss is 5.554394035339356 and perplexity is 258.37035377133265
At time: 7.636090993881226 and batch: 450, loss is 5.566877298355102 and perplexity is 261.61587404837456
At time: 8.470310688018799 and batch: 500, loss is 5.5184792709350585 and perplexity is 249.2556984418468
At time: 9.289444208145142 and batch: 550, loss is 5.573064088821411 and perplexity is 263.2394538390312
At time: 10.112571001052856 and batch: 600, loss is 5.495199346542359 and perplexity is 243.52006620555707
At time: 10.941778421401978 and batch: 650, loss is 5.408311910629273 and perplexity is 223.254396030603
At time: 11.764630556106567 and batch: 700, loss is 5.50758095741272 and perplexity is 246.55398051841635
At time: 12.588944673538208 and batch: 750, loss is 5.478941822052002 and perplexity is 239.59304111330374
At time: 13.409820079803467 and batch: 800, loss is 5.461745681762696 and perplexity is 235.50818807845755
At time: 14.227996587753296 and batch: 850, loss is 5.50488920211792 and perplexity is 245.8912099441811
At time: 15.047979593276978 and batch: 900, loss is 5.4078958797454835 and perplexity is 223.1615346248583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.358980779778467 and perplexity of 212.5082433353584
finished 1 epochs...
Completing Train Step...
At time: 16.990886449813843 and batch: 50, loss is 5.27525411605835 and perplexity is 195.44013467948295
At time: 17.746105432510376 and batch: 100, loss is 5.103666486740113 and perplexity is 164.62439528025706
At time: 18.497304439544678 and batch: 150, loss is 5.04870774269104 and perplexity is 155.8209735334184
At time: 19.247029066085815 and batch: 200, loss is 4.903966064453125 and perplexity is 134.82343917099712
At time: 20.002418756484985 and batch: 250, loss is 4.964746170043945 and perplexity is 143.27217888659476
At time: 20.76821231842041 and batch: 300, loss is 4.903185062408447 and perplexity is 134.71818289736672
At time: 21.528191804885864 and batch: 350, loss is 4.8645172119140625 and perplexity is 129.6083501372555
At time: 22.286391735076904 and batch: 400, loss is 4.727615890502929 and perplexity is 113.02577505150587
At time: 23.052494764328003 and batch: 450, loss is 4.731937580108642 and perplexity is 113.51529438212924
At time: 23.814891576766968 and batch: 500, loss is 4.633030738830566 and perplexity is 102.82522875647958
At time: 24.57457184791565 and batch: 550, loss is 4.695264558792115 and perplexity is 109.4277548873089
At time: 25.33544373512268 and batch: 600, loss is 4.640701208114624 and perplexity is 103.61697918198706
At time: 26.094027042388916 and batch: 650, loss is 4.499351186752319 and perplexity is 89.958745935863
At time: 26.852303981781006 and batch: 700, loss is 4.550933218002319 and perplexity is 94.72076220227324
At time: 27.620179176330566 and batch: 750, loss is 4.589046020507812 and perplexity is 98.40051329997986
At time: 28.37737798690796 and batch: 800, loss is 4.51722731590271 and perplexity is 91.58131955584953
At time: 29.135390520095825 and batch: 850, loss is 4.579518260955811 and perplexity is 97.4674290293822
At time: 29.89989423751831 and batch: 900, loss is 4.5105257415771485 and perplexity is 90.96963245857938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.618266902557791 and perplexity of 101.31828541913939
finished 2 epochs...
Completing Train Step...
At time: 31.755311965942383 and batch: 50, loss is 4.559880170822144 and perplexity is 95.5720268264773
At time: 32.52184176445007 and batch: 100, loss is 4.408829188346862 and perplexity is 82.17319782312093
At time: 33.266151666641235 and batch: 150, loss is 4.398915100097656 and perplexity is 81.36255054233158
At time: 34.0103325843811 and batch: 200, loss is 4.2920298624038695 and perplexity is 73.1147308159266
At time: 34.75419282913208 and batch: 250, loss is 4.430022869110108 and perplexity is 83.93333636903171
At time: 35.49940061569214 and batch: 300, loss is 4.394746675491333 and perplexity is 81.02410277169079
At time: 36.248504877090454 and batch: 350, loss is 4.378860502243042 and perplexity is 79.74710995846958
At time: 37.00031542778015 and batch: 400, loss is 4.286256194114685 and perplexity is 72.69380691991718
At time: 37.74786424636841 and batch: 450, loss is 4.298294124603271 and perplexity is 73.5741782074303
At time: 38.50142312049866 and batch: 500, loss is 4.186354780197144 and perplexity is 65.78256148598649
At time: 39.25762176513672 and batch: 550, loss is 4.265742406845093 and perplexity is 71.21777291387855
At time: 40.00683808326721 and batch: 600, loss is 4.260873498916626 and perplexity is 70.87186291999885
At time: 40.754889249801636 and batch: 650, loss is 4.1076040363311765 and perplexity is 60.8008662456014
At time: 41.502227544784546 and batch: 700, loss is 4.1352520847320555 and perplexity is 62.50534572795765
At time: 42.25096130371094 and batch: 750, loss is 4.223641777038575 and perplexity is 68.28169876692203
At time: 43.00431156158447 and batch: 800, loss is 4.163235845565796 and perplexity is 64.27918395107002
At time: 43.75676679611206 and batch: 850, loss is 4.241063385009766 and perplexity is 69.48169836464616
At time: 44.5056791305542 and batch: 900, loss is 4.1847513389587405 and perplexity is 65.677167533218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.438681406517551 and perplexity of 84.66323165449936
finished 3 epochs...
Completing Train Step...
At time: 46.34341859817505 and batch: 50, loss is 4.2606076765060426 and perplexity is 70.85302609429101
At time: 47.10571098327637 and batch: 100, loss is 4.113652629852295 and perplexity is 61.16974043163946
At time: 47.8562867641449 and batch: 150, loss is 4.106200613975525 and perplexity is 60.71559679918853
At time: 48.60509371757507 and batch: 200, loss is 4.0050340747833255 and perplexity is 54.87369417645311
At time: 49.35444474220276 and batch: 250, loss is 4.157867288589477 and perplexity is 63.93502214237874
At time: 50.10417032241821 and batch: 300, loss is 4.127374649047852 and perplexity is 62.01489815784497
At time: 50.86274027824402 and batch: 350, loss is 4.114840288162231 and perplexity is 61.24243234020985
At time: 51.61229062080383 and batch: 400, loss is 4.039250621795654 and perplexity is 56.78377433467211
At time: 52.36069107055664 and batch: 450, loss is 4.052342982292175 and perplexity is 57.53209593759721
At time: 53.10828948020935 and batch: 500, loss is 3.939339671134949 and perplexity is 51.38465932155517
At time: 53.86564254760742 and batch: 550, loss is 4.021734495162963 and perplexity is 55.797802961537585
At time: 54.61656212806702 and batch: 600, loss is 4.033747382164002 and perplexity is 56.47213790879234
At time: 55.36524033546448 and batch: 650, loss is 3.8738937044143675 and perplexity is 48.129423458964865
At time: 56.113964319229126 and batch: 700, loss is 3.8911242628097535 and perplexity is 48.96590613689979
At time: 56.86141228675842 and batch: 750, loss is 3.9960284566879274 and perplexity is 54.381741138631604
At time: 57.60856652259827 and batch: 800, loss is 3.943091778755188 and perplexity is 51.57782225079105
At time: 58.355565309524536 and batch: 850, loss is 4.024048829078675 and perplexity is 55.927087255160984
At time: 59.10420274734497 and batch: 900, loss is 3.9768383836746217 and perplexity is 53.348101089993115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375470723191353 and perplexity of 79.47724252958972
finished 4 epochs...
Completing Train Step...
At time: 60.97132420539856 and batch: 50, loss is 4.054677519798279 and perplexity is 57.666563672281804
At time: 61.72283697128296 and batch: 100, loss is 3.916479034423828 and perplexity is 50.22329859279797
At time: 62.47472667694092 and batch: 150, loss is 3.9140323543548585 and perplexity is 50.10056845105326
At time: 63.22630071640015 and batch: 200, loss is 3.807103395462036 and perplexity is 45.01984513072129
At time: 63.97849440574646 and batch: 250, loss is 3.9640696048736572 and perplexity is 52.671241519130284
At time: 64.72991800308228 and batch: 300, loss is 3.9396737480163573 and perplexity is 51.40182861606605
At time: 65.4920117855072 and batch: 350, loss is 3.926264863014221 and perplexity is 50.71718780030481
At time: 66.24357962608337 and batch: 400, loss is 3.8568497705459595 and perplexity is 47.31605989558649
At time: 66.99432587623596 and batch: 450, loss is 3.873733959197998 and perplexity is 48.12173562786414
At time: 67.74501466751099 and batch: 500, loss is 3.762775979042053 and perplexity is 43.067815543966084
At time: 68.49502754211426 and batch: 550, loss is 3.8421320581436156 and perplexity is 46.62477527641709
At time: 69.24643135070801 and batch: 600, loss is 3.859444251060486 and perplexity is 47.43897987880919
At time: 69.99673414230347 and batch: 650, loss is 3.7042065191268922 and perplexity is 40.617805075334566
At time: 70.74663233757019 and batch: 700, loss is 3.7137398052215578 and perplexity is 41.00687785603181
At time: 71.49691891670227 and batch: 750, loss is 3.8280102968215943 and perplexity is 45.970978573964366
At time: 72.25416612625122 and batch: 800, loss is 3.7745728731155395 and perplexity is 43.5788906241677
At time: 73.0099675655365 and batch: 850, loss is 3.8581850290298463 and perplexity is 47.379281265027934
At time: 73.76192688941956 and batch: 900, loss is 3.811604781150818 and perplexity is 45.22295360914636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.359117481806507 and perplexity of 78.18810154405156
finished 5 epochs...
Completing Train Step...
At time: 75.60798501968384 and batch: 50, loss is 3.8937156867980955 and perplexity is 49.09296211752842
At time: 76.37183403968811 and batch: 100, loss is 3.7598428678512574 and perplexity is 42.94167793048542
At time: 77.12318348884583 and batch: 150, loss is 3.756830892562866 and perplexity is 42.81253324564973
At time: 77.87479567527771 and batch: 200, loss is 3.6544286012649536 and perplexity is 38.64543285371211
At time: 78.62658143043518 and batch: 250, loss is 3.810779480934143 and perplexity is 45.18564649263727
At time: 79.37757849693298 and batch: 300, loss is 3.7889591121673583 and perplexity is 44.21035829384608
At time: 80.12627983093262 and batch: 350, loss is 3.774436674118042 and perplexity is 43.57295562713213
At time: 80.87553787231445 and batch: 400, loss is 3.7090098667144775 and perplexity is 40.81337583249885
At time: 81.63622736930847 and batch: 450, loss is 3.727819004058838 and perplexity is 41.58830525467451
At time: 82.3900818824768 and batch: 500, loss is 3.6240397310256958 and perplexity is 37.488706627728156
At time: 83.14266562461853 and batch: 550, loss is 3.6963229751586915 and perplexity is 40.29885171646354
At time: 83.89281582832336 and batch: 600, loss is 3.7188444900512696 and perplexity is 41.216740228258686
At time: 84.66425776481628 and batch: 650, loss is 3.5602696704864503 and perplexity is 35.17268090027284
At time: 85.41373205184937 and batch: 700, loss is 3.56983766078949 and perplexity is 35.5108278839134
At time: 86.16468048095703 and batch: 750, loss is 3.68904004573822 and perplexity is 40.006424180792365
At time: 86.91364121437073 and batch: 800, loss is 3.6359431743621826 and perplexity is 37.93761781670023
At time: 87.66325402259827 and batch: 850, loss is 3.719354720115662 and perplexity is 41.23777561426621
At time: 88.41514873504639 and batch: 900, loss is 3.676369113922119 and perplexity is 39.50270355211141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368823743846319 and perplexity of 78.95071080147122
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 90.24555253982544 and batch: 50, loss is 3.781369676589966 and perplexity is 43.87609666072805
At time: 91.01110291481018 and batch: 100, loss is 3.647231574058533 and perplexity is 38.36829908791457
At time: 91.76179242134094 and batch: 150, loss is 3.6425896692276 and perplexity is 38.1906098224136
At time: 92.51202464103699 and batch: 200, loss is 3.523736481666565 and perplexity is 33.91089949261869
At time: 93.2619240283966 and batch: 250, loss is 3.6750699949264525 and perplexity is 39.451418159673345
At time: 94.01138401031494 and batch: 300, loss is 3.6400172328948974 and perplexity is 38.0924931636739
At time: 94.7673556804657 and batch: 350, loss is 3.6125066328048705 and perplexity is 37.05882936526929
At time: 95.51885509490967 and batch: 400, loss is 3.5381801652908327 and perplexity is 34.40425213413333
At time: 96.26841735839844 and batch: 450, loss is 3.5396961116790773 and perplexity is 34.45644668797335
At time: 97.01796770095825 and batch: 500, loss is 3.423081569671631 and perplexity is 30.66376209714219
At time: 97.76902079582214 and batch: 550, loss is 3.477683176994324 and perplexity is 32.38460569428105
At time: 98.51887798309326 and batch: 600, loss is 3.4884448289871215 and perplexity is 32.73499957752428
At time: 99.26863479614258 and batch: 650, loss is 3.3204970836639403 and perplexity is 27.67410348481256
At time: 100.01786184310913 and batch: 700, loss is 3.3021057176589967 and perplexity is 27.169790634784178
At time: 100.76938700675964 and batch: 750, loss is 3.4060052967071535 and perplexity is 30.144584748424677
At time: 101.52082896232605 and batch: 800, loss is 3.3317522192001343 and perplexity is 27.987338716206086
At time: 102.27188730239868 and batch: 850, loss is 3.394519610404968 and perplexity is 29.800334265304265
At time: 103.04327964782715 and batch: 900, loss is 3.33610089302063 and perplexity is 28.109311541183413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341753815951413 and perplexity of 76.8421882846021
finished 7 epochs...
Completing Train Step...
At time: 104.89647722244263 and batch: 50, loss is 3.66547203540802 and perplexity is 39.07457639414738
At time: 105.64650702476501 and batch: 100, loss is 3.527566752433777 and perplexity is 34.04103649050149
At time: 106.3958969116211 and batch: 150, loss is 3.5217433977127075 and perplexity is 33.84337953180558
At time: 107.1463303565979 and batch: 200, loss is 3.4087931632995607 and perplexity is 30.228741082996997
At time: 107.89822506904602 and batch: 250, loss is 3.558533034324646 and perplexity is 35.11165175875904
At time: 108.65114450454712 and batch: 300, loss is 3.5296979999542235 and perplexity is 34.11366373093764
At time: 109.40072298049927 and batch: 350, loss is 3.504714879989624 and perplexity is 33.271955999439875
At time: 110.15285634994507 and batch: 400, loss is 3.437512307167053 and perplexity is 31.109471018068405
At time: 110.90832901000977 and batch: 450, loss is 3.445439715385437 and perplexity is 31.357068601181147
At time: 111.65779280662537 and batch: 500, loss is 3.334551758766174 and perplexity is 28.065800154996808
At time: 112.4127607345581 and batch: 550, loss is 3.3912527179718017 and perplexity is 29.70313862907543
At time: 113.16659665107727 and batch: 600, loss is 3.4098676156997683 and perplexity is 30.261237881414083
At time: 113.92044186592102 and batch: 650, loss is 3.249734125137329 and perplexity is 25.783483825580937
At time: 114.67852973937988 and batch: 700, loss is 3.2364568853378297 and perplexity is 25.443412925180198
At time: 115.43173098564148 and batch: 750, loss is 3.3481421422958375 and perplexity is 28.449828780548728
At time: 116.1813383102417 and batch: 800, loss is 3.2812325096130373 and perplexity is 26.608547788857567
At time: 116.93948221206665 and batch: 850, loss is 3.3524627351760863 and perplexity is 28.573014834985656
At time: 117.68973469734192 and batch: 900, loss is 3.302665910720825 and perplexity is 27.18501522694642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357931894798801 and perplexity of 78.09545767623857
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 119.53760743141174 and batch: 50, loss is 3.614741954803467 and perplexity is 37.14176043608363
At time: 120.31243801116943 and batch: 100, loss is 3.488147315979004 and perplexity is 32.72526193793829
At time: 121.06708073616028 and batch: 150, loss is 3.483824782371521 and perplexity is 32.58411117762562
At time: 121.83335304260254 and batch: 200, loss is 3.3643533849716185 and perplexity is 28.914794512276114
At time: 122.58819389343262 and batch: 250, loss is 3.5148103761672975 and perplexity is 33.60955414715907
At time: 123.34151124954224 and batch: 300, loss is 3.483810534477234 and perplexity is 32.58364692596144
At time: 124.09396696090698 and batch: 350, loss is 3.455205993652344 and perplexity is 31.66481076051776
At time: 124.84682440757751 and batch: 400, loss is 3.3870508527755736 and perplexity is 29.578591892196975
At time: 125.59963345527649 and batch: 450, loss is 3.3852432775497436 and perplexity is 29.525174654659608
At time: 126.3518602848053 and batch: 500, loss is 3.2703095197677614 and perplexity is 26.319484483384823
At time: 127.10488319396973 and batch: 550, loss is 3.320610146522522 and perplexity is 27.677232574949844
At time: 127.85965704917908 and batch: 600, loss is 3.334888195991516 and perplexity is 28.075244123490158
At time: 128.61273384094238 and batch: 650, loss is 3.16955087184906 and perplexity is 23.7967941458181
At time: 129.3664710521698 and batch: 700, loss is 3.1494648838043213 and perplexity is 23.323580415379542
At time: 130.12047362327576 and batch: 750, loss is 3.252396512031555 and perplexity is 25.85222089674192
At time: 130.8735682964325 and batch: 800, loss is 3.179934763908386 and perplexity is 24.045184887014102
At time: 131.6282021999359 and batch: 850, loss is 3.243346371650696 and perplexity is 25.61931019545207
At time: 132.38176083564758 and batch: 900, loss is 3.1941871786117555 and perplexity is 24.390340642482737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350358100786601 and perplexity of 77.50621299214289
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 134.2479157447815 and batch: 50, loss is 3.591721706390381 and perplexity is 36.296514101227224
At time: 135.01329350471497 and batch: 100, loss is 3.4634514093399047 and perplexity is 31.926979648300865
At time: 135.7712321281433 and batch: 150, loss is 3.4602396059036256 and perplexity is 31.824600963471116
At time: 136.52611637115479 and batch: 200, loss is 3.3414515352249143 and perplexity is 28.26011750443503
At time: 137.2843141555786 and batch: 250, loss is 3.4898445129394533 and perplexity is 32.780850311895506
At time: 138.03702807426453 and batch: 300, loss is 3.4610300207138063 and perplexity is 31.849765543318895
At time: 138.788343667984 and batch: 350, loss is 3.432347640991211 and perplexity is 30.949215175536633
At time: 139.53840613365173 and batch: 400, loss is 3.3658951234817507 and perplexity is 28.959407946776246
At time: 140.2931387424469 and batch: 450, loss is 3.3636100339889525 and perplexity is 28.8933086581166
At time: 141.06625247001648 and batch: 500, loss is 3.2481758880615232 and perplexity is 25.743338331362352
At time: 141.8171992301941 and batch: 550, loss is 3.29507839679718 and perplexity is 26.979529095663636
At time: 142.57491970062256 and batch: 600, loss is 3.310247654914856 and perplexity is 27.391908372694974
At time: 143.32783007621765 and batch: 650, loss is 3.14254967212677 and perplexity is 23.16284930506929
At time: 144.09444737434387 and batch: 700, loss is 3.122567739486694 and perplexity is 22.704604367386555
At time: 144.8502447605133 and batch: 750, loss is 3.2225933504104614 and perplexity is 25.093111102775033
At time: 145.60119104385376 and batch: 800, loss is 3.148226456642151 and perplexity is 23.294713738202027
At time: 146.35183548927307 and batch: 850, loss is 3.211770009994507 and perplexity is 24.822984293409426
At time: 147.1022675037384 and batch: 900, loss is 3.1633925867080688 and perplexity is 23.65069701763462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348411246521832 and perplexity of 77.35546647919264
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 148.96856927871704 and batch: 50, loss is 3.582401728630066 and perplexity is 35.959802904738346
At time: 149.7216601371765 and batch: 100, loss is 3.4530182838439942 and perplexity is 31.5956130632767
At time: 150.48053979873657 and batch: 150, loss is 3.4507775354385375 and perplexity is 31.524894504524163
At time: 151.24225616455078 and batch: 200, loss is 3.3325949239730837 and perplexity is 28.010933720539434
At time: 151.9948432445526 and batch: 250, loss is 3.481058521270752 and perplexity is 32.49409957357017
At time: 152.74387955665588 and batch: 300, loss is 3.451761064529419 and perplexity is 31.55591540783613
At time: 153.50558638572693 and batch: 350, loss is 3.423678789138794 and perplexity is 30.682080562330597
At time: 154.25486588478088 and batch: 400, loss is 3.3574411630630494 and perplexity is 28.715618204618625
At time: 155.00435256958008 and batch: 450, loss is 3.3557971858978273 and perplexity is 28.66844916703883
At time: 155.75850462913513 and batch: 500, loss is 3.240315203666687 and perplexity is 25.54177133863415
At time: 156.50729656219482 and batch: 550, loss is 3.2868233203887938 and perplexity is 26.757727774520013
At time: 157.25348925590515 and batch: 600, loss is 3.30204674243927 and perplexity is 27.168188337659934
At time: 157.99899578094482 and batch: 650, loss is 3.134432134628296 and perplexity is 22.975585097681005
At time: 158.74382495880127 and batch: 700, loss is 3.114939351081848 and perplexity is 22.532063766762874
At time: 159.50044584274292 and batch: 750, loss is 3.2140620708465577 and perplexity is 24.87994533809245
At time: 160.24592089653015 and batch: 800, loss is 3.1386932039260866 and perplexity is 23.07369453504986
At time: 160.99289393424988 and batch: 850, loss is 3.20206503868103 and perplexity is 24.583243165008458
At time: 161.7407591342926 and batch: 900, loss is 3.1535964441299438 and perplexity is 23.42014253362565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347820961312072 and perplexity of 77.30981816552358
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 163.58061003684998 and batch: 50, loss is 3.579413366317749 and perplexity is 35.852502391237024
At time: 164.34501433372498 and batch: 100, loss is 3.4501873445510864 and perplexity is 31.506294288438667
At time: 165.09599590301514 and batch: 150, loss is 3.4479506492614744 and perplexity is 31.435903059621683
At time: 165.84587144851685 and batch: 200, loss is 3.3299265193939207 and perplexity is 27.93628885237859
At time: 166.5968894958496 and batch: 250, loss is 3.4782404088974 and perplexity is 32.402656458506215
At time: 167.3491325378418 and batch: 300, loss is 3.44928870677948 and perplexity is 31.47799425998945
At time: 168.1007752418518 and batch: 350, loss is 3.4207265424728392 and perplexity is 30.59163306963349
At time: 168.85222148895264 and batch: 400, loss is 3.3549465465545656 and perplexity is 28.644073025389467
At time: 169.60487461090088 and batch: 450, loss is 3.353864188194275 and perplexity is 28.61308664568627
At time: 170.35816860198975 and batch: 500, loss is 3.2380059957504272 and perplexity is 25.48285812567182
At time: 171.11614656448364 and batch: 550, loss is 3.2841106128692625 and perplexity is 26.6852402483608
At time: 171.86794257164001 and batch: 600, loss is 3.299801301956177 and perplexity is 27.10725222752189
At time: 172.61946535110474 and batch: 650, loss is 3.1320437383651734 and perplexity is 22.920775775335677
At time: 173.37229800224304 and batch: 700, loss is 3.1127764081954954 and perplexity is 22.483380867868156
At time: 174.12461066246033 and batch: 750, loss is 3.211866250038147 and perplexity is 24.825373373461844
At time: 174.87458539009094 and batch: 800, loss is 3.1362643814086915 and perplexity is 23.017720629061927
At time: 175.62459468841553 and batch: 850, loss is 3.199374876022339 and perplexity is 24.51719911666269
At time: 176.38033175468445 and batch: 900, loss is 3.1508559465408323 and perplexity is 23.356047555656417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347400822051584 and perplexity of 77.27734409798406
Annealing...
Model not improving. Stopping early with 76.8421882846021 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7ebb4a0b70>
ELAPSED
926.4226973056793


RESULTS SO FAR:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.32896323710268, 'params': {'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -77.94910462177792, 'params': {'dropout': 0.2674339239639524, 'rnn_dropout': 0.8396743911700816, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.03835539180484, 'params': {'dropout': 0.7587669668369824, 'rnn_dropout': 0.7297519073495046, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -76.8421882846021, 'params': {'dropout': 0.062420125232273804, 'rnn_dropout': 0.9603983682435051, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'dropout': 0.0, 'rnn_dropout': 1.0, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0285170078277588 and batch: 50, loss is 7.074819450378418 and perplexity is 1181.8301025628923
At time: 1.8361196517944336 and batch: 100, loss is 6.476803245544434 and perplexity is 649.8900830202841
At time: 2.6380484104156494 and batch: 150, loss is 6.429941339492798 and perplexity is 620.1375693613745
At time: 3.4380576610565186 and batch: 200, loss is 6.3738241863250735 and perplexity is 586.2956510508097
At time: 4.237255573272705 and batch: 250, loss is 6.465473079681397 and perplexity is 642.5682775485955
At time: 5.039473533630371 and batch: 300, loss is 6.419982671737671 and perplexity is 613.9924746107017
At time: 5.842599153518677 and batch: 350, loss is 6.466945362091065 and perplexity is 643.5150162830196
At time: 6.649831771850586 and batch: 400, loss is 6.405092697143555 and perplexity is 604.9178702383225
At time: 7.456470012664795 and batch: 450, loss is 6.4190167045593265 and perplexity is 613.399664396178
At time: 8.266233444213867 and batch: 500, loss is 6.422359218597412 and perplexity is 615.4533917795958
At time: 9.079217433929443 and batch: 550, loss is 6.466812744140625 and perplexity is 643.4296802991495
At time: 9.897988080978394 and batch: 600, loss is 6.435789604187011 and perplexity is 623.7749237523376
At time: 10.70631217956543 and batch: 650, loss is 6.393078088760376 and perplexity is 597.6935047876532
At time: 11.516315698623657 and batch: 700, loss is 6.472394304275513 and perplexity is 647.0310630698336
At time: 12.321694612503052 and batch: 750, loss is 6.456048555374146 and perplexity is 636.5408247494419
At time: 13.133758783340454 and batch: 800, loss is 6.458243932723999 and perplexity is 637.9398071435967
At time: 13.945555448532104 and batch: 850, loss is 6.489337759017944 and perplexity is 658.0874064266171
At time: 14.747881889343262 and batch: 900, loss is 6.417256450653076 and perplexity is 612.3208749910158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.4952108304794525 and perplexity of 661.9637727383839
finished 1 epochs...
Completing Train Step...
At time: 16.62928533554077 and batch: 50, loss is 6.455393018722535 and perplexity is 636.1236856484946
At time: 17.38300848007202 and batch: 100, loss is 5.995083875656128 and perplexity is 401.4503544907175
At time: 18.140151977539062 and batch: 150, loss is 5.795881872177124 and perplexity is 328.9421410326923
At time: 18.896822452545166 and batch: 200, loss is 5.575807161331177 and perplexity is 263.96253002014464
At time: 19.650653839111328 and batch: 250, loss is 5.567490396499633 and perplexity is 261.776319434692
At time: 20.40171980857849 and batch: 300, loss is 5.4282818794250485 and perplexity is 227.75759405975913
At time: 21.16237235069275 and batch: 350, loss is 5.374643793106079 and perplexity is 215.86296673738556
At time: 21.911259412765503 and batch: 400, loss is 5.20254264831543 and perplexity is 181.7337399005433
At time: 22.660126209259033 and batch: 450, loss is 5.176455898284912 and perplexity is 177.05419962747203
At time: 23.408167362213135 and batch: 500, loss is 5.102895193099975 and perplexity is 164.4974704856016
At time: 24.163926601409912 and batch: 550, loss is 5.155510368347168 and perplexity is 173.38427402687824
At time: 24.917007207870483 and batch: 600, loss is 5.061942901611328 and perplexity is 157.89699682578237
At time: 25.67924189567566 and batch: 650, loss is 4.937592325210571 and perplexity is 139.43413304582327
At time: 26.44131898880005 and batch: 700, loss is 4.999089469909668 and perplexity is 148.27808595882544
At time: 27.200803518295288 and batch: 750, loss is 4.983585062026978 and perplexity is 145.99685232045326
At time: 27.965054035186768 and batch: 800, loss is 4.932759132385254 and perplexity is 138.76184694725853
At time: 28.726500988006592 and batch: 850, loss is 4.967550630569458 and perplexity is 143.67454400151055
At time: 29.484435558319092 and batch: 900, loss is 4.891935358047485 and perplexity is 133.2111359776444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.985047640865797 and perplexity of 146.21054045697224
finished 2 epochs...
Completing Train Step...
At time: 31.334731578826904 and batch: 50, loss is 4.969435434341431 and perplexity is 143.945597685241
At time: 32.099936723709106 and batch: 100, loss is 4.8677263355255125 and perplexity is 130.0249474525362
At time: 32.856651306152344 and batch: 150, loss is 4.8817855072021485 and perplexity is 131.86590133080261
At time: 33.6168954372406 and batch: 200, loss is 4.769305124282837 and perplexity is 117.8373312050393
At time: 34.375056743621826 and batch: 250, loss is 4.894959592819214 and perplexity is 133.6146075158648
At time: 35.12724685668945 and batch: 300, loss is 4.8616102600097655 and perplexity is 129.2321319859787
At time: 35.8776068687439 and batch: 350, loss is 4.843170700073242 and perplexity is 126.87098450496347
At time: 36.62874484062195 and batch: 400, loss is 4.734508323669433 and perplexity is 113.80748851138242
At time: 37.37860941886902 and batch: 450, loss is 4.750609006881714 and perplexity is 115.65469759072651
At time: 38.13608956336975 and batch: 500, loss is 4.68028636932373 and perplexity is 107.80093903359415
At time: 38.89310622215271 and batch: 550, loss is 4.763587265014649 and perplexity is 117.1654765443316
At time: 39.65669322013855 and batch: 600, loss is 4.70954888343811 and perplexity is 111.00207373533196
At time: 40.425118923187256 and batch: 650, loss is 4.592921524047852 and perplexity is 98.78260475777834
At time: 41.17576885223389 and batch: 700, loss is 4.654462413787842 and perplexity is 105.05272991696259
At time: 41.92787146568298 and batch: 750, loss is 4.690752305984497 and perplexity is 108.9351015172492
At time: 42.6786162853241 and batch: 800, loss is 4.636698198318482 and perplexity is 103.2030284764047
At time: 43.428072452545166 and batch: 850, loss is 4.691846332550049 and perplexity is 109.0543446279021
At time: 44.17731833457947 and batch: 900, loss is 4.642359504699707 and perplexity is 103.7889494141364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.791853395226884 and perplexity of 120.52454139543723
finished 3 epochs...
Completing Train Step...
At time: 46.015965700149536 and batch: 50, loss is 4.731373624801636 and perplexity is 113.45129487756239
At time: 46.79782700538635 and batch: 100, loss is 4.620506420135498 and perplexity is 101.54544376789454
At time: 47.55301547050476 and batch: 150, loss is 4.642396059036255 and perplexity is 103.7927434196665
At time: 48.31790637969971 and batch: 200, loss is 4.533124341964721 and perplexity is 93.04882375088933
At time: 49.07851243019104 and batch: 250, loss is 4.6718323612213135 and perplexity is 106.89343046891784
At time: 49.83156371116638 and batch: 300, loss is 4.652046251296997 and perplexity is 104.79921184513748
At time: 50.583648443222046 and batch: 350, loss is 4.633510427474976 and perplexity is 102.87456468306871
At time: 51.33589959144592 and batch: 400, loss is 4.540475120544434 and perplexity is 93.73532511996882
At time: 52.09687399864197 and batch: 450, loss is 4.566148185729981 and perplexity is 96.17295506152749
At time: 52.855956077575684 and batch: 500, loss is 4.485735893249512 and perplexity is 88.74223159935015
At time: 53.60760974884033 and batch: 550, loss is 4.574594488143921 and perplexity is 96.98870109325185
At time: 54.36198663711548 and batch: 600, loss is 4.536817045211792 and perplexity is 93.39306063565198
At time: 55.11763048171997 and batch: 650, loss is 4.415284337997437 and perplexity is 82.7053538378274
At time: 55.868387937545776 and batch: 700, loss is 4.46741738319397 and perplexity is 87.13140515255138
At time: 56.61831974983215 and batch: 750, loss is 4.528283290863037 and perplexity is 92.59945821851456
At time: 57.368948459625244 and batch: 800, loss is 4.474155340194702 and perplexity is 87.72047514938755
At time: 58.121296644210815 and batch: 850, loss is 4.541083068847656 and perplexity is 93.79232867767988
At time: 58.893447399139404 and batch: 900, loss is 4.495185289382935 and perplexity is 89.58476655353638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.701092863736087 and perplexity of 110.0673954114723
finished 4 epochs...
Completing Train Step...
At time: 60.7463173866272 and batch: 50, loss is 4.588434391021728 and perplexity is 98.3403470462029
At time: 61.50479316711426 and batch: 100, loss is 4.472050924301147 and perplexity is 87.53606888908011
At time: 62.25531721115112 and batch: 150, loss is 4.49766432762146 and perplexity is 89.80712612046823
At time: 63.00204873085022 and batch: 200, loss is 4.3866813850402835 and perplexity is 80.37324804362515
At time: 63.74887013435364 and batch: 250, loss is 4.535977945327759 and perplexity is 93.31472739860433
At time: 64.49653649330139 and batch: 300, loss is 4.517053937911987 and perplexity is 91.56544274706151
At time: 65.24564456939697 and batch: 350, loss is 4.499475841522217 and perplexity is 89.96996042159294
At time: 65.99462699890137 and batch: 400, loss is 4.418599472045899 and perplexity is 82.9799881455757
At time: 66.74237203598022 and batch: 450, loss is 4.445765552520752 and perplexity is 85.26512779151535
At time: 67.49746966362 and batch: 500, loss is 4.3586701393127445 and perplexity is 78.1531325058758
At time: 68.2454993724823 and batch: 550, loss is 4.443906726837159 and perplexity is 85.10678199627067
At time: 68.99247932434082 and batch: 600, loss is 4.417587394714356 and perplexity is 82.89604846449103
At time: 69.7394335269928 and batch: 650, loss is 4.295136852264404 and perplexity is 73.34225081135868
At time: 70.48848843574524 and batch: 700, loss is 4.339213962554932 and perplexity is 76.64726803091213
At time: 71.23644185066223 and batch: 750, loss is 4.415190544128418 and perplexity is 82.69759694648248
At time: 71.98460459709167 and batch: 800, loss is 4.355692367553711 and perplexity is 77.9207564681301
At time: 72.73514556884766 and batch: 850, loss is 4.429108810424805 and perplexity is 83.85665142654523
At time: 73.4851086139679 and batch: 900, loss is 4.387987051010132 and perplexity is 80.47825719700297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.647611853194563 and perplexity of 104.3355192775487
finished 5 epochs...
Completing Train Step...
At time: 75.33207511901855 and batch: 50, loss is 4.480383434295654 and perplexity is 88.26851136226306
At time: 76.10536813735962 and batch: 100, loss is 4.366093335151672 and perplexity is 78.73543712055651
At time: 76.86248397827148 and batch: 150, loss is 4.391626205444336 and perplexity is 80.7716635552541
At time: 77.62764096260071 and batch: 200, loss is 4.277538452148438 and perplexity is 72.06283538824242
At time: 78.39196062088013 and batch: 250, loss is 4.433614234924317 and perplexity is 84.2353136147456
At time: 79.14082098007202 and batch: 300, loss is 4.413730230331421 and perplexity is 82.57692063876694
At time: 79.88867402076721 and batch: 350, loss is 4.397474546432495 and perplexity is 81.24542780299103
At time: 80.63649487495422 and batch: 400, loss is 4.32436282157898 and perplexity is 75.51737948196238
At time: 81.38895797729492 and batch: 450, loss is 4.353089294433594 and perplexity is 77.71818680767186
At time: 82.14316320419312 and batch: 500, loss is 4.2552370882034305 and perplexity is 70.47352364901329
At time: 82.8943018913269 and batch: 550, loss is 4.346671018600464 and perplexity is 77.2209674000246
At time: 83.64357662200928 and batch: 600, loss is 4.329021034240722 and perplexity is 75.86997609266412
At time: 84.38881969451904 and batch: 650, loss is 4.19730206489563 and perplexity is 66.50665812934031
At time: 85.13426327705383 and batch: 700, loss is 4.232711176872254 and perplexity is 68.90378952515577
At time: 85.88152599334717 and batch: 750, loss is 4.320924010276794 and perplexity is 75.25813546498864
At time: 86.63098311424255 and batch: 800, loss is 4.261712055206299 and perplexity is 70.93131789109728
At time: 87.38126564025879 and batch: 850, loss is 4.337139792442322 and perplexity is 76.48845331965047
At time: 88.13287806510925 and batch: 900, loss is 4.29918267250061 and perplexity is 73.63958144142488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6131963860498715 and perplexity of 100.80584963531714
finished 6 epochs...
Completing Train Step...
At time: 89.96396565437317 and batch: 50, loss is 4.390024185180664 and perplexity is 80.64236930716328
At time: 90.72770714759827 and batch: 100, loss is 4.279782452583313 and perplexity is 72.22472599555475
At time: 91.47770547866821 and batch: 150, loss is 4.296895747184753 and perplexity is 73.47136564017903
At time: 92.22757935523987 and batch: 200, loss is 4.190883297920227 and perplexity is 66.08113451793093
At time: 92.97784519195557 and batch: 250, loss is 4.343088073730469 and perplexity is 76.9447840014267
At time: 93.72896933555603 and batch: 300, loss is 4.325480155944824 and perplexity is 75.60180480214856
At time: 94.48158955574036 and batch: 350, loss is 4.309866847991944 and perplexity is 74.4305776996278
At time: 95.23503684997559 and batch: 400, loss is 4.240957927703858 and perplexity is 69.47437139827456
At time: 95.988529920578 and batch: 450, loss is 4.272723526954651 and perplexity is 71.71669222205567
At time: 96.75127339363098 and batch: 500, loss is 4.169669919013977 and perplexity is 64.69409429225875
At time: 97.50190424919128 and batch: 550, loss is 4.258139762878418 and perplexity is 70.67838253688242
At time: 98.25224208831787 and batch: 600, loss is 4.250819354057312 and perplexity is 70.16287703948339
At time: 99.00232768058777 and batch: 650, loss is 4.112548680305481 and perplexity is 61.1022493846209
At time: 99.75561285018921 and batch: 700, loss is 4.140815811157227 and perplexity is 62.854077596470866
At time: 100.50848245620728 and batch: 750, loss is 4.236946310997009 and perplexity is 69.19622513040183
At time: 101.26557946205139 and batch: 800, loss is 4.177426738739014 and perplexity is 65.19786602630057
At time: 102.02209806442261 and batch: 850, loss is 4.2545063829422 and perplexity is 70.42204708389438
At time: 102.77950119972229 and batch: 900, loss is 4.217759432792664 and perplexity is 67.88122133690759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.580815981512201 and perplexity of 97.59399662253331
finished 7 epochs...
Completing Train Step...
At time: 104.62789249420166 and batch: 50, loss is 4.311241598129272 and perplexity is 74.53297151337006
At time: 105.37693095207214 and batch: 100, loss is 4.200146450996399 and perplexity is 66.69609803572786
At time: 106.12633419036865 and batch: 150, loss is 4.218896584510803 and perplexity is 67.958456490101
At time: 106.88094019889832 and batch: 200, loss is 4.107420644760132 and perplexity is 60.789716901598844
At time: 107.64285016059875 and batch: 250, loss is 4.265220069885254 and perplexity is 71.18058295257946
At time: 108.40431475639343 and batch: 300, loss is 4.244778337478638 and perplexity is 69.74029961980283
At time: 109.15524673461914 and batch: 350, loss is 4.232912955284118 and perplexity is 68.91769422516457
At time: 109.90489029884338 and batch: 400, loss is 4.159932413101196 and perplexity is 64.06719235075818
At time: 110.65368628501892 and batch: 450, loss is 4.194944815635681 and perplexity is 66.35006998982969
At time: 111.40552306175232 and batch: 500, loss is 4.088589501380921 and perplexity is 59.655688067909175
At time: 112.1557891368866 and batch: 550, loss is 4.178473715782165 and perplexity is 65.26616244144333
At time: 112.91520023345947 and batch: 600, loss is 4.17343150138855 and perplexity is 64.93790472611992
At time: 113.6732861995697 and batch: 650, loss is 4.029519500732422 and perplexity is 56.233884414187045
At time: 114.43103170394897 and batch: 700, loss is 4.056419644355774 and perplexity is 57.767113568792894
At time: 115.20988607406616 and batch: 750, loss is 4.153807563781738 and perplexity is 63.675989702881566
At time: 115.96715116500854 and batch: 800, loss is 4.101434941291809 and perplexity is 60.426934519350574
At time: 116.72674512863159 and batch: 850, loss is 4.177642278671264 and perplexity is 65.21192028449929
At time: 117.48627495765686 and batch: 900, loss is 4.14238203048706 and perplexity is 62.95259799990189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.560808573683647 and perplexity of 95.66079737070014
finished 8 epochs...
Completing Train Step...
At time: 119.33563184738159 and batch: 50, loss is 4.235012812614441 and perplexity is 69.06256359984914
At time: 120.10623550415039 and batch: 100, loss is 4.122156057357788 and perplexity is 61.69211070617249
At time: 120.86657619476318 and batch: 150, loss is 4.14412157535553 and perplexity is 63.06220217175811
At time: 121.62358951568604 and batch: 200, loss is 4.026876873970032 and perplexity is 56.08547542742859
At time: 122.38931441307068 and batch: 250, loss is 4.189809160232544 and perplexity is 66.01019238852874
At time: 123.1497073173523 and batch: 300, loss is 4.173046002388 and perplexity is 64.91287605332434
At time: 123.90981245040894 and batch: 350, loss is 4.157440981864929 and perplexity is 63.90777202137293
At time: 124.66981101036072 and batch: 400, loss is 4.087760033607483 and perplexity is 59.60622611353909
At time: 125.42878222465515 and batch: 450, loss is 4.119181718826294 and perplexity is 61.50889009933987
At time: 126.18493509292603 and batch: 500, loss is 4.014528245925903 and perplexity is 55.39715540339819
At time: 126.94112873077393 and batch: 550, loss is 4.104207139015198 and perplexity is 60.59468233728638
At time: 127.6981508731842 and batch: 600, loss is 4.103853693008423 and perplexity is 60.57326917320405
At time: 128.4570333957672 and batch: 650, loss is 3.955975832939148 and perplexity is 52.246653081622824
At time: 129.21437191963196 and batch: 700, loss is 3.980919404029846 and perplexity is 53.56626063043434
At time: 129.97295808792114 and batch: 750, loss is 4.087283024787903 and perplexity is 59.57780019822616
At time: 130.73074007034302 and batch: 800, loss is 4.030728273391723 and perplexity is 56.301899495296865
At time: 131.48802614212036 and batch: 850, loss is 4.106184225082398 and perplexity is 60.714601745915374
At time: 132.24432015419006 and batch: 900, loss is 4.073169245719909 and perplexity is 58.74283838240453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.549241157427226 and perplexity of 94.56062445449994
finished 9 epochs...
Completing Train Step...
At time: 134.11450004577637 and batch: 50, loss is 4.166573467254639 and perplexity is 64.4940819742258
At time: 134.8894865512848 and batch: 100, loss is 4.054640851020813 and perplexity is 57.66444914866001
At time: 135.648827791214 and batch: 150, loss is 4.072572393417358 and perplexity is 58.70778804503539
At time: 136.409033536911 and batch: 200, loss is 3.9607675647735596 and perplexity is 52.49760580133295
At time: 137.16807222366333 and batch: 250, loss is 4.12364019870758 and perplexity is 61.78373849610452
At time: 137.9264838695526 and batch: 300, loss is 4.104044451713562 and perplexity is 60.58482515376444
At time: 138.6851110458374 and batch: 350, loss is 4.090686435699463 and perplexity is 59.780913376252315
At time: 139.4431290626526 and batch: 400, loss is 4.02410126209259 and perplexity is 55.9300197577846
At time: 140.20185899734497 and batch: 450, loss is 4.0549850749969485 and perplexity is 57.684302051354145
At time: 140.9594054222107 and batch: 500, loss is 3.9523678064346313 and perplexity is 52.05848543359096
At time: 141.71832156181335 and batch: 550, loss is 4.040914058685303 and perplexity is 56.87830916419695
At time: 142.47999095916748 and batch: 600, loss is 4.0414153003692626 and perplexity is 56.9068260900021
At time: 143.24098324775696 and batch: 650, loss is 3.8896756839752196 and perplexity is 48.89502651141311
At time: 144.00065159797668 and batch: 700, loss is 3.912102770805359 and perplexity is 50.003988427927766
At time: 144.75959634780884 and batch: 750, loss is 4.022920160293579 and perplexity is 55.86399970670535
At time: 145.5219371318817 and batch: 800, loss is 3.965879216194153 and perplexity is 52.76664228716876
At time: 146.28026962280273 and batch: 850, loss is 4.0454082202911374 and perplexity is 57.134504738395265
At time: 147.03785872459412 and batch: 900, loss is 4.008395504951477 and perplexity is 55.05845862992535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.54886825770548 and perplexity of 94.52536939766166
finished 10 epochs...
Completing Train Step...
At time: 148.90862846374512 and batch: 50, loss is 4.102590336799621 and perplexity is 60.49679187671226
At time: 149.66833186149597 and batch: 100, loss is 3.992192087173462 and perplexity is 54.17351236143811
At time: 150.4288523197174 and batch: 150, loss is 4.01105899810791 and perplexity is 55.205301928939654
At time: 151.18914556503296 and batch: 200, loss is 3.8969000482559206 and perplexity is 49.24954102355146
At time: 151.9450147151947 and batch: 250, loss is 4.0609509372711186 and perplexity is 58.02946723299661
At time: 152.70134782791138 and batch: 300, loss is 4.039165778160095 and perplexity is 56.778956797188386
At time: 153.46752285957336 and batch: 350, loss is 4.029389152526855 and perplexity is 56.2265549059661
At time: 154.2277090549469 and batch: 400, loss is 3.9660407638549806 and perplexity is 52.775167303379654
At time: 155.0010209083557 and batch: 450, loss is 3.9940576267242434 and perplexity is 54.274669518354074
At time: 155.76990842819214 and batch: 500, loss is 3.8888922262191774 and perplexity is 48.85673432577444
At time: 156.53430485725403 and batch: 550, loss is 3.9776623153686526 and perplexity is 53.392074394309425
At time: 157.29312539100647 and batch: 600, loss is 3.984532675743103 and perplexity is 53.76016017967514
At time: 158.05252957344055 and batch: 650, loss is 3.8319519901275636 and perplexity is 46.152539666474354
At time: 158.81097769737244 and batch: 700, loss is 3.8523116445541383 and perplexity is 47.10182014551439
At time: 159.57160449028015 and batch: 750, loss is 3.9646626043319704 and perplexity is 52.70248479952843
At time: 160.33845686912537 and batch: 800, loss is 3.909754166603088 and perplexity is 49.886686652216454
At time: 161.10858368873596 and batch: 850, loss is 3.9889194536209107 and perplexity is 53.99651209370664
At time: 161.8707618713379 and batch: 900, loss is 3.954020314216614 and perplexity is 52.14458360524459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.552398263591609 and perplexity of 94.85963413917212
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 163.74497818946838 and batch: 50, loss is 4.053289079666138 and perplexity is 57.586552659100946
At time: 164.52521896362305 and batch: 100, loss is 3.934373369216919 and perplexity is 51.13010022129005
At time: 165.2862572669983 and batch: 150, loss is 3.958582949638367 and perplexity is 52.383043919511074
At time: 166.05183172225952 and batch: 200, loss is 3.831297574043274 and perplexity is 46.12234658268318
At time: 166.81265139579773 and batch: 250, loss is 3.989098229408264 and perplexity is 54.00616622560734
At time: 167.57889413833618 and batch: 300, loss is 3.9503381776809694 and perplexity is 51.952933186854615
At time: 168.3428430557251 and batch: 350, loss is 3.9257222318649294 and perplexity is 50.68967453985106
At time: 169.10041403770447 and batch: 400, loss is 3.8539790725708007 and perplexity is 47.1804245554464
At time: 169.86834907531738 and batch: 450, loss is 3.872241315841675 and perplexity is 48.04996061945533
At time: 170.63765478134155 and batch: 500, loss is 3.7479819154739378 and perplexity is 42.43535739129423
At time: 171.402277469635 and batch: 550, loss is 3.8252588939666747 and perplexity is 45.84466773795329
At time: 172.169597864151 and batch: 600, loss is 3.819673752784729 and perplexity is 45.58933250160867
At time: 172.95837330818176 and batch: 650, loss is 3.6495965671539308 and perplexity is 38.45914723561641
At time: 173.71681356430054 and batch: 700, loss is 3.6626914978027343 and perplexity is 38.96607897555206
At time: 174.47305035591125 and batch: 750, loss is 3.761313066482544 and perplexity is 43.00485715823096
At time: 175.22818422317505 and batch: 800, loss is 3.6878767013549805 and perplexity is 39.95990999318349
At time: 175.98627376556396 and batch: 850, loss is 3.7494277238845823 and perplexity is 42.496755161930565
At time: 176.75808691978455 and batch: 900, loss is 3.7047691202163695 and perplexity is 40.64066312610134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.535654146377355 and perplexity of 93.28451707911422
finished 12 epochs...
Completing Train Step...
At time: 178.60901546478271 and batch: 50, loss is 3.9474278354644774 and perplexity is 51.8019521818952
At time: 179.38128304481506 and batch: 100, loss is 3.8312757778167725 and perplexity is 46.121341300526
At time: 180.13849139213562 and batch: 150, loss is 3.858269786834717 and perplexity is 47.38329719909278
At time: 180.89505553245544 and batch: 200, loss is 3.7373100090026856 and perplexity is 41.98489912558136
At time: 181.6513969898224 and batch: 250, loss is 3.8967305278778075 and perplexity is 49.2411929303412
At time: 182.406414270401 and batch: 300, loss is 3.861385245323181 and perplexity is 47.5311480866267
At time: 183.1619062423706 and batch: 350, loss is 3.8413577032089234 and perplexity is 46.58868512669903
At time: 183.91912055015564 and batch: 400, loss is 3.7740745878219606 and perplexity is 43.55718131302361
At time: 184.67504143714905 and batch: 450, loss is 3.797721667289734 and perplexity is 44.59945625187633
At time: 185.4302477836609 and batch: 500, loss is 3.677063298225403 and perplexity is 39.530135229072755
At time: 186.18611001968384 and batch: 550, loss is 3.759194784164429 and perplexity is 42.913857145605796
At time: 186.94126057624817 and batch: 600, loss is 3.7583082151412963 and perplexity is 42.875827909459666
At time: 187.6968755722046 and batch: 650, loss is 3.593821573257446 and perplexity is 36.372812028286944
At time: 188.45361614227295 and batch: 700, loss is 3.611451120376587 and perplexity is 37.01973394677638
At time: 189.21180319786072 and batch: 750, loss is 3.7155532836914062 and perplexity is 41.08131041666362
At time: 189.97241377830505 and batch: 800, loss is 3.6486126613616943 and perplexity is 38.421325667368016
At time: 190.7299027442932 and batch: 850, loss is 3.7176731491088866 and perplexity is 41.168489637380866
At time: 191.52146244049072 and batch: 900, loss is 3.6786711549758913 and perplexity is 39.59374514796357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.547354502220676 and perplexity of 94.38238934700351
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 193.39157938957214 and batch: 50, loss is 3.9009665632247925 and perplexity is 49.45022278046956
At time: 194.14957547187805 and batch: 100, loss is 3.7905130052566527 and perplexity is 44.27910986653709
At time: 194.90878772735596 and batch: 150, loss is 3.8216489696502687 and perplexity is 45.6794703116099
At time: 195.66753029823303 and batch: 200, loss is 3.6975235891342164 and perplexity is 40.347264137531155
At time: 196.42889165878296 and batch: 250, loss is 3.8591443252563478 and perplexity is 47.42475383810653
At time: 197.18927097320557 and batch: 300, loss is 3.822659125328064 and perplexity is 45.72563700175018
At time: 197.94852590560913 and batch: 350, loss is 3.7965571641922 and perplexity is 44.54755027512371
At time: 198.70798468589783 and batch: 400, loss is 3.7282808208465577 and perplexity is 41.60751586776431
At time: 199.46874928474426 and batch: 450, loss is 3.744520888328552 and perplexity is 42.28874133487033
At time: 200.22819805145264 and batch: 500, loss is 3.618337645530701 and perplexity is 37.275551110472264
At time: 200.98694825172424 and batch: 550, loss is 3.693471689224243 and perplexity is 40.18411182324645
At time: 201.7503788471222 and batch: 600, loss is 3.691674838066101 and perplexity is 40.11197178724127
At time: 202.5178039073944 and batch: 650, loss is 3.5240465354919435 and perplexity is 33.92141532688149
At time: 203.28202271461487 and batch: 700, loss is 3.5370241451263427 and perplexity is 34.36450310468851
At time: 204.04935956001282 and batch: 750, loss is 3.6347099637985227 and perplexity is 37.89086158171564
At time: 204.80914998054504 and batch: 800, loss is 3.565155816078186 and perplexity is 35.34496028833379
At time: 205.5682818889618 and batch: 850, loss is 3.6268100929260254 and perplexity is 37.59270790633552
At time: 206.3271427154541 and batch: 900, loss is 3.584290385246277 and perplexity is 36.02778279954391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.54914876859482 and perplexity of 94.551888512373
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 208.19422960281372 and batch: 50, loss is 3.8787099599838255 and perplexity is 48.361786172697876
At time: 208.9678132534027 and batch: 100, loss is 3.7694753503799436 and perplexity is 43.3573114706116
At time: 209.72685956954956 and batch: 150, loss is 3.801007013320923 and perplexity is 44.74622185435192
At time: 210.4965479373932 and batch: 200, loss is 3.675328869819641 and perplexity is 39.46163246339192
At time: 211.25432181358337 and batch: 250, loss is 3.8410880613327025 and perplexity is 46.576124559734325
At time: 212.0154149532318 and batch: 300, loss is 3.8048664474487306 and perplexity is 44.919250631876345
At time: 212.77443742752075 and batch: 350, loss is 3.7755563402175905 and perplexity is 43.62177011126679
At time: 213.54146885871887 and batch: 400, loss is 3.709147825241089 and perplexity is 40.81900677410402
At time: 214.30797362327576 and batch: 450, loss is 3.7228373527526855 and perplexity is 41.381642008989836
At time: 215.07122993469238 and batch: 500, loss is 3.596949782371521 and perplexity is 36.48677194266839
At time: 215.84082865715027 and batch: 550, loss is 3.6704650783538817 and perplexity is 39.27016531777882
At time: 216.60460019111633 and batch: 600, loss is 3.670551452636719 and perplexity is 39.27355739663712
At time: 217.36783981323242 and batch: 650, loss is 3.5027814388275145 and perplexity is 33.207688778631976
At time: 218.1311914920807 and batch: 700, loss is 3.512630624771118 and perplexity is 33.53637346140818
At time: 218.89202547073364 and batch: 750, loss is 3.6098402309417725 and perplexity is 36.96014725515331
At time: 219.66965198516846 and batch: 800, loss is 3.540876989364624 and perplexity is 34.4971595707267
At time: 220.43542051315308 and batch: 850, loss is 3.599248013496399 and perplexity is 36.57072341057993
At time: 221.19901585578918 and batch: 900, loss is 3.5577715015411377 and perplexity is 35.08492326346605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.548262504682149 and perplexity of 94.46812770828109
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 223.0619204044342 and batch: 50, loss is 3.8729816389083864 and perplexity is 48.08554628447551
At time: 223.83614134788513 and batch: 100, loss is 3.7627701950073242 and perplexity is 43.06756643894569
At time: 224.59753131866455 and batch: 150, loss is 3.7950239419937133 and perplexity is 44.47930131593313
At time: 225.36278629302979 and batch: 200, loss is 3.66671471118927 and perplexity is 39.12316360671862
At time: 226.12403345108032 and batch: 250, loss is 3.834861454963684 and perplexity is 46.28701438760733
At time: 226.88976216316223 and batch: 300, loss is 3.799743585586548 and perplexity is 44.689723934687365
At time: 227.65930151939392 and batch: 350, loss is 3.769195294380188 and perplexity is 43.345170695528786
At time: 228.4215066432953 and batch: 400, loss is 3.702956166267395 and perplexity is 40.567050223961324
At time: 229.17985653877258 and batch: 450, loss is 3.71653263092041 and perplexity is 41.12156299159063
At time: 229.95994472503662 and batch: 500, loss is 3.589931511878967 and perplexity is 36.23159440767715
At time: 230.71898412704468 and batch: 550, loss is 3.6639374399185183 and perplexity is 39.014658711919935
At time: 231.47881388664246 and batch: 600, loss is 3.6645039033889772 and perplexity is 39.03676535160345
At time: 232.24010634422302 and batch: 650, loss is 3.4973620176315308 and perplexity is 33.028209103451566
At time: 233.00132393836975 and batch: 700, loss is 3.5060020065307618 and perplexity is 33.31480878964797
At time: 233.75758624076843 and batch: 750, loss is 3.6025545024871826 and perplexity is 36.691844236911585
At time: 234.51818823814392 and batch: 800, loss is 3.5339834785461424 and perplexity is 34.26017080880057
At time: 235.2819139957428 and batch: 850, loss is 3.5914356327056884 and perplexity is 36.28613210877501
At time: 236.0516710281372 and batch: 900, loss is 3.5500471925735475 and perplexity is 34.81496045641565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.547131682095462 and perplexity of 94.36136139400378
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 237.93553113937378 and batch: 50, loss is 3.8709738445281983 and perplexity is 47.98909725219051
At time: 238.69737482070923 and batch: 100, loss is 3.7603181838989257 and perplexity is 42.96209365069493
At time: 239.4562952518463 and batch: 150, loss is 3.7929513597488405 and perplexity is 44.387209772376096
At time: 240.21395587921143 and batch: 200, loss is 3.6643874645233154 and perplexity is 39.03222021954695
At time: 240.9723174571991 and batch: 250, loss is 3.8323946142196657 and perplexity is 46.17297241412203
At time: 241.73593163490295 and batch: 300, loss is 3.7978575563430788 and perplexity is 44.605517241567846
At time: 242.5059516429901 and batch: 350, loss is 3.7670389127731325 and perplexity is 43.25180267141118
At time: 243.267972946167 and batch: 400, loss is 3.700927982330322 and perplexity is 40.48485616483352
At time: 244.02724838256836 and batch: 450, loss is 3.7149655628204346 and perplexity is 41.05717316680242
At time: 244.78549766540527 and batch: 500, loss is 3.5880116558074953 and perplexity is 36.162101690526896
At time: 245.543372631073 and batch: 550, loss is 3.6619651985168455 and perplexity is 38.937788215241376
At time: 246.3016393184662 and batch: 600, loss is 3.662727975845337 and perplexity is 38.96750040776636
At time: 247.05994534492493 and batch: 650, loss is 3.4957061338424684 and perplexity is 32.973563483299976
At time: 247.81818079948425 and batch: 700, loss is 3.504126954078674 and perplexity is 33.25240030361326
At time: 248.59660577774048 and batch: 750, loss is 3.6006404876708986 and perplexity is 36.62168266996415
At time: 249.35453009605408 and batch: 800, loss is 3.532114906311035 and perplexity is 34.19621297837968
At time: 250.1126368045807 and batch: 850, loss is 3.589434461593628 and perplexity is 36.21358995826756
At time: 250.870858669281 and batch: 900, loss is 3.5479293155670164 and perplexity is 34.74130467665421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.54657985739512 and perplexity of 94.30930482839898
Annealing...
Model not improving. Stopping early with 93.28451707911422 lossat 16 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7ebb4a0b70>
ELAPSED
1184.2584764957428


RESULTS SO FAR:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.32896323710268, 'params': {'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -77.94910462177792, 'params': {'dropout': 0.2674339239639524, 'rnn_dropout': 0.8396743911700816, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.03835539180484, 'params': {'dropout': 0.7587669668369824, 'rnn_dropout': 0.7297519073495046, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -76.8421882846021, 'params': {'dropout': 0.062420125232273804, 'rnn_dropout': 0.9603983682435051, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -93.28451707911422, 'params': {'dropout': 0.0, 'rnn_dropout': 1.0, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -79.44588395610347, 'params': {'dropout': 0.36317465845966, 'rnn_dropout': 0.33147982863933767, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.32896323710268, 'params': {'dropout': 0.7887606587515079, 'rnn_dropout': 0.36976247660393846, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -77.94910462177792, 'params': {'dropout': 0.2674339239639524, 'rnn_dropout': 0.8396743911700816, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -79.03835539180484, 'params': {'dropout': 0.7587669668369824, 'rnn_dropout': 0.7297519073495046, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -76.8421882846021, 'params': {'dropout': 0.062420125232273804, 'rnn_dropout': 0.9603983682435051, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}, {'best_accuracy': -93.28451707911422, 'params': {'dropout': 0.0, 'rnn_dropout': 1.0, 'tie_weights': True, 'tune_wordvecs': True, 'data': 'ptb', 'batch_size': 32, 'num_layers': 2, 'wordvec_dim': 300, 'wordvec_source': 'None', 'seq_len': 35}}]
