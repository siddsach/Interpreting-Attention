Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.6672861576080322 and batch: 50, loss is 7.671705951690674 and perplexity is 2146.7405589106834
At time: 2.820256471633911 and batch: 100, loss is 6.948051948547363 and perplexity is 1041.1195968791312
At time: 3.9658920764923096 and batch: 150, loss is 6.740924215316772 and perplexity is 846.3425772736279
At time: 5.108392000198364 and batch: 200, loss is 6.498143119812012 and perplexity is 663.9076907198365
At time: 6.252036809921265 and batch: 250, loss is 6.504964065551758 and perplexity is 668.4516484817024
At time: 7.397369861602783 and batch: 300, loss is 6.372409429550171 and perplexity is 585.4667717759272
At time: 8.542589902877808 and batch: 350, loss is 6.363594264984131 and perplexity is 580.3284666111175
At time: 9.68663477897644 and batch: 400, loss is 6.254192981719971 and perplexity is 520.1894029604974
At time: 10.831435918807983 and batch: 450, loss is 6.238766641616821 and perplexity is 512.2263625118902
At time: 11.976672172546387 and batch: 500, loss is 6.215187721252441 and perplexity is 500.289895422011
At time: 13.120803356170654 and batch: 550, loss is 6.229887247085571 and perplexity is 507.6982358126946
At time: 14.267788648605347 and batch: 600, loss is 6.180138130187988 and perplexity is 483.05867673045634
At time: 15.415275573730469 and batch: 650, loss is 6.115250854492188 and perplexity is 452.70959736449134
At time: 16.560837030410767 and batch: 700, loss is 6.217277250289917 and perplexity is 501.33635861251304
At time: 17.706471920013428 and batch: 750, loss is 6.146311407089233 and perplexity is 466.99166385369966
At time: 18.850306510925293 and batch: 800, loss is 6.1601941204071045 and perplexity is 473.51998580606534
At time: 19.994431972503662 and batch: 850, loss is 6.198497867584228 and perplexity is 492.00942242744924
At time: 21.13768744468689 and batch: 900, loss is 6.066237678527832 and perplexity is 431.0558562298626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.069802898250214 and perplexity of 432.5954078591146
finished 1 epochs...
Completing Train Step...
At time: 23.526772022247314 and batch: 50, loss is 5.884611988067627 and perplexity is 359.4632648978914
At time: 24.454665422439575 and batch: 100, loss is 5.558984966278076 and perplexity is 259.55924118396626
At time: 25.371065378189087 and batch: 150, loss is 5.399917497634887 and perplexity is 221.38815040469532
At time: 26.294653177261353 and batch: 200, loss is 5.189802551269532 and perplexity is 179.43312059204737
At time: 27.21579623222351 and batch: 250, loss is 5.205079622268677 and perplexity is 182.19537900068207
At time: 28.13473343849182 and batch: 300, loss is 5.108488435745239 and perplexity is 165.42012265591174
At time: 29.054964065551758 and batch: 350, loss is 5.049379463195801 and perplexity is 155.92567683815412
At time: 29.978697538375854 and batch: 400, loss is 4.8876392269134525 and perplexity is 132.6400710322497
At time: 30.90116548538208 and batch: 450, loss is 4.882989320755005 and perplexity is 132.02473887619735
At time: 31.820002555847168 and batch: 500, loss is 4.790560092926025 and perplexity is 120.36876748183371
At time: 32.73531246185303 and batch: 550, loss is 4.841924200057983 and perplexity is 126.71293834357729
At time: 33.651487827301025 and batch: 600, loss is 4.765883665084839 and perplexity is 117.43484452283934
At time: 34.572022438049316 and batch: 650, loss is 4.626992559432983 and perplexity is 102.20622229540726
At time: 35.491827726364136 and batch: 700, loss is 4.684703254699707 and perplexity is 108.27813651221196
At time: 36.41479754447937 and batch: 750, loss is 4.70213812828064 and perplexity is 110.18250510696839
At time: 37.35819673538208 and batch: 800, loss is 4.626814279556275 and perplexity is 102.18800260684776
At time: 38.27989983558655 and batch: 850, loss is 4.683845548629761 and perplexity is 108.18530551382935
At time: 39.202332735061646 and batch: 900, loss is 4.608587331771851 and perplexity is 100.34229908823683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.705752856110873 and perplexity of 110.58150557918549
finished 2 epochs...
Completing Train Step...
At time: 41.432079792022705 and batch: 50, loss is 4.668429555892945 and perplexity is 106.53031109680013
At time: 42.36261558532715 and batch: 100, loss is 4.521999645233154 and perplexity is 92.01942032237247
At time: 43.27928066253662 and batch: 150, loss is 4.514387693405151 and perplexity is 91.32163206220542
At time: 44.20447778701782 and batch: 200, loss is 4.4062112522125245 and perplexity is 81.95835498438475
At time: 45.12413477897644 and batch: 250, loss is 4.5352967262268065 and perplexity is 93.25118127077567
At time: 46.04782009124756 and batch: 300, loss is 4.491721620559693 and perplexity is 89.27501134499455
At time: 46.971556425094604 and batch: 350, loss is 4.481469202041626 and perplexity is 88.36440251322409
At time: 47.893067359924316 and batch: 400, loss is 4.3791011095047 and perplexity is 79.76630000076098
At time: 48.810954093933105 and batch: 450, loss is 4.396078424453735 and perplexity is 81.13207841875618
At time: 49.73398232460022 and batch: 500, loss is 4.284384322166443 and perplexity is 72.55786069858716
At time: 50.6547429561615 and batch: 550, loss is 4.358520851135254 and perplexity is 78.14146603801338
At time: 51.575610876083374 and batch: 600, loss is 4.345245380401611 and perplexity is 77.11095667552486
At time: 52.49860095977783 and batch: 650, loss is 4.190917630195617 and perplexity is 66.08340327258486
At time: 53.42193126678467 and batch: 700, loss is 4.227947626113892 and perplexity is 68.57634334873475
At time: 54.342082262039185 and batch: 750, loss is 4.308562202453613 and perplexity is 74.33353549512081
At time: 55.26585149765015 and batch: 800, loss is 4.243801350593567 and perplexity is 69.6721975345524
At time: 56.185036182403564 and batch: 850, loss is 4.317595944404602 and perplexity is 75.00808775116867
At time: 57.10075902938843 and batch: 900, loss is 4.257346930503846 and perplexity is 70.6223686347764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.479266441031678 and perplexity of 88.16997107431462
finished 3 epochs...
Completing Train Step...
At time: 59.357062101364136 and batch: 50, loss is 4.344737901687622 and perplexity is 77.07183443408869
At time: 60.28275442123413 and batch: 100, loss is 4.201011452674866 and perplexity is 66.75381523161283
At time: 61.20729446411133 and batch: 150, loss is 4.201403231620788 and perplexity is 66.779973094696
At time: 62.13267469406128 and batch: 200, loss is 4.0917182970047 and perplexity is 59.842630824004026
At time: 63.05359768867493 and batch: 250, loss is 4.240610985755921 and perplexity is 69.45027200531685
At time: 63.98173999786377 and batch: 300, loss is 4.203062968254089 and perplexity is 66.89090229357473
At time: 64.90557956695557 and batch: 350, loss is 4.1992708015441895 and perplexity is 66.6377211965721
At time: 65.83162236213684 and batch: 400, loss is 4.117557415962219 and perplexity is 61.409062130361924
At time: 66.75743079185486 and batch: 450, loss is 4.136115312576294 and perplexity is 62.5593253778202
At time: 67.68579459190369 and batch: 500, loss is 4.014554595947265 and perplexity is 55.39861513885842
At time: 68.60855889320374 and batch: 550, loss is 4.093783087730408 and perplexity is 59.96632098633676
At time: 69.53032112121582 and batch: 600, loss is 4.100659637451172 and perplexity is 60.38010344144291
At time: 70.45269417762756 and batch: 650, loss is 3.9391649675369265 and perplexity is 51.375683020807394
At time: 71.38017129898071 and batch: 700, loss is 3.9645471811294555 and perplexity is 52.69640206100371
At time: 72.30563712120056 and batch: 750, loss is 4.0687536525726316 and perplexity is 58.484025733985625
At time: 73.22871851921082 and batch: 800, loss is 4.009171900749206 and perplexity is 55.10122238448659
At time: 74.15425825119019 and batch: 850, loss is 4.087082028388977 and perplexity is 59.56582647830801
At time: 75.08202648162842 and batch: 900, loss is 4.035673341751099 and perplexity is 56.58100576809817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.399334215138056 and perplexity of 81.39665795795536
finished 4 epochs...
Completing Train Step...
At time: 77.30880045890808 and batch: 50, loss is 4.124659690856934 and perplexity is 61.846758651277334
At time: 78.22883582115173 and batch: 100, loss is 3.9874227142333982 and perplexity is 53.915753839374176
At time: 79.1561553478241 and batch: 150, loss is 3.9848511743545534 and perplexity is 53.777285443085255
At time: 80.07894277572632 and batch: 200, loss is 3.87792019367218 and perplexity is 48.32360674160639
At time: 81.00200629234314 and batch: 250, loss is 4.029744954109192 and perplexity is 56.24656396258688
At time: 81.93241667747498 and batch: 300, loss is 4.000683274269104 and perplexity is 54.63546829205372
At time: 82.86479711532593 and batch: 350, loss is 3.9959129381179808 and perplexity is 54.37545940049975
At time: 83.78784537315369 and batch: 400, loss is 3.9219057178497314 and perplexity is 50.49658538465184
At time: 84.70964026451111 and batch: 450, loss is 3.948342480659485 and perplexity is 51.84935426328875
At time: 85.63147330284119 and batch: 500, loss is 3.824318218231201 and perplexity is 45.801563048357636
At time: 86.55601191520691 and batch: 550, loss is 3.8981907320022584 and perplexity is 49.313147644851284
At time: 87.47986578941345 and batch: 600, loss is 3.917591247558594 and perplexity is 50.27918868024741
At time: 88.4051673412323 and batch: 650, loss is 3.753863658905029 and perplexity is 42.68568674070347
At time: 89.326336145401 and batch: 700, loss is 3.774774069786072 and perplexity is 43.587659433964866
At time: 90.24335861206055 and batch: 750, loss is 3.882171506881714 and perplexity is 48.529482841092076
At time: 91.16191244125366 and batch: 800, loss is 3.826339325904846 and perplexity is 45.89422654881229
At time: 92.08308720588684 and batch: 850, loss is 3.908818688392639 and perplexity is 49.840040565462424
At time: 93.00708293914795 and batch: 900, loss is 3.860328779220581 and perplexity is 47.48095955576434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372525567877783 and perplexity of 79.24351405871528
finished 5 epochs...
Completing Train Step...
At time: 95.25531458854675 and batch: 50, loss is 3.950922212600708 and perplexity is 51.98328437623309
At time: 96.18657946586609 and batch: 100, loss is 3.818887252807617 and perplexity is 45.553490589318805
At time: 97.10695934295654 and batch: 150, loss is 3.820862503051758 and perplexity is 45.64355905732417
At time: 98.02716302871704 and batch: 200, loss is 3.7081531524658202 and perplexity is 40.77842540528952
At time: 98.95098757743835 and batch: 250, loss is 3.8592831230163576 and perplexity is 47.43133674454405
At time: 99.87255597114563 and batch: 300, loss is 3.8400661611557005 and perplexity is 46.52855272077785
At time: 100.79525375366211 and batch: 350, loss is 3.832206983566284 and perplexity is 46.16430976185437
At time: 101.71843194961548 and batch: 400, loss is 3.762939281463623 and perplexity is 43.07484919682677
At time: 102.64074516296387 and batch: 450, loss is 3.78704119682312 and perplexity is 44.12564782901677
At time: 103.56423854827881 and batch: 500, loss is 3.6666274309158324 and perplexity is 39.11974907531407
At time: 104.48328614234924 and batch: 550, loss is 3.738417444229126 and perplexity is 42.03142043676644
At time: 105.41624283790588 and batch: 600, loss is 3.7657772588729856 and perplexity is 43.1972682748833
At time: 106.33626937866211 and batch: 650, loss is 3.602702050209045 and perplexity is 36.69725843435611
At time: 107.25885272026062 and batch: 700, loss is 3.622969217300415 and perplexity is 37.44859592609903
At time: 108.17917585372925 and batch: 750, loss is 3.731603698730469 and perplexity is 41.74600252174044
At time: 109.09920525550842 and batch: 800, loss is 3.6772594118118285 and perplexity is 39.53788838588926
At time: 110.01607251167297 and batch: 850, loss is 3.7615009212493895 and perplexity is 43.01293658450126
At time: 110.9330747127533 and batch: 900, loss is 3.717469482421875 and perplexity is 41.16010584126611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376293025604666 and perplexity of 79.54262373580299
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 113.1601243019104 and batch: 50, loss is 3.827404041290283 and perplexity is 45.943116860436454
At time: 114.09074711799622 and batch: 100, loss is 3.696444935798645 and perplexity is 40.303766889931026
At time: 115.01420331001282 and batch: 150, loss is 3.7030724573135374 and perplexity is 40.571768082987845
At time: 115.93786549568176 and batch: 200, loss is 3.572575092315674 and perplexity is 35.60816951592381
At time: 116.85502457618713 and batch: 250, loss is 3.7179491567611693 and perplexity is 41.17985402381037
At time: 117.77968120574951 and batch: 300, loss is 3.687238221168518 and perplexity is 39.934404525634314
At time: 118.70150470733643 and batch: 350, loss is 3.6679659128189086 and perplexity is 39.172145209320384
At time: 119.62864804267883 and batch: 400, loss is 3.5953940534591675 and perplexity is 36.43005254807956
At time: 120.55229687690735 and batch: 450, loss is 3.6002704381942747 and perplexity is 36.60813334257442
At time: 121.47823357582092 and batch: 500, loss is 3.4694425678253173 and perplexity is 32.11883338253245
At time: 122.39912438392639 and batch: 550, loss is 3.5230707931518555 and perplexity is 33.88833290829434
At time: 123.32018899917603 and batch: 600, loss is 3.546846809387207 and perplexity is 34.70371734757556
At time: 124.24634718894958 and batch: 650, loss is 3.363283076286316 and perplexity is 28.88386331249457
At time: 125.16838359832764 and batch: 700, loss is 3.3641658926010134 and perplexity is 28.909373717102195
At time: 126.09287571907043 and batch: 750, loss is 3.455906004905701 and perplexity is 31.686984244324194
At time: 127.01808166503906 and batch: 800, loss is 3.382607855796814 and perplexity is 29.447465809882523
At time: 127.94049334526062 and batch: 850, loss is 3.4449589824676514 and perplexity is 31.341997848892902
At time: 128.87102842330933 and batch: 900, loss is 3.3908946323394775 and perplexity is 29.692504266017366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366142377461473 and perplexity of 78.73929858294282
finished 7 epochs...
Completing Train Step...
At time: 131.11320066452026 and batch: 50, loss is 3.7070281457901 and perplexity is 40.73257520021244
At time: 132.03397583961487 and batch: 100, loss is 3.5722394037246703 and perplexity is 35.59621826573219
At time: 132.9578824043274 and batch: 150, loss is 3.5758170318603515 and perplexity is 35.72379637516327
At time: 133.88522338867188 and batch: 200, loss is 3.4500747203826903 and perplexity is 31.502746118053807
At time: 134.8123345375061 and batch: 250, loss is 3.593050775527954 and perplexity is 36.344786749658425
At time: 135.7374505996704 and batch: 300, loss is 3.569389615058899 and perplexity is 35.49492097286816
At time: 136.66416001319885 and batch: 350, loss is 3.5537678527832033 and perplexity is 34.94473637081634
At time: 137.5909390449524 and batch: 400, loss is 3.4865211391448976 and perplexity is 32.6720881218167
At time: 138.518572807312 and batch: 450, loss is 3.4973309993743897 and perplexity is 33.027184641857275
At time: 139.44770574569702 and batch: 500, loss is 3.3706261825561525 and perplexity is 29.09674122630142
At time: 140.37195754051208 and batch: 550, loss is 3.4279466485977172 and perplexity is 30.813307199313005
At time: 141.2966866493225 and batch: 600, loss is 3.4590686988830566 and perplexity is 31.78735912239772
At time: 142.22370409965515 and batch: 650, loss is 3.2809489154815674 and perplexity is 26.6010028307612
At time: 143.14919877052307 and batch: 700, loss is 3.2867066764831545 and perplexity is 26.754606830669438
At time: 144.07542538642883 and batch: 750, loss is 3.3852895975112913 and perplexity is 29.526542291288497
At time: 144.9972596168518 and batch: 800, loss is 3.3190724515914916 and perplexity is 27.634706139436055
At time: 145.92220044136047 and batch: 850, loss is 3.388708257675171 and perplexity is 29.62765624383908
At time: 146.85045313835144 and batch: 900, loss is 3.343476529121399 and perplexity is 28.317402050748694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388715770146618 and perplexity of 80.53692461653293
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 149.10396814346313 and batch: 50, loss is 3.650882906913757 and perplexity is 38.50865059808032
At time: 150.0400996208191 and batch: 100, loss is 3.5279317569732664 and perplexity is 34.05346389124032
At time: 150.9638957977295 and batch: 150, loss is 3.535456590652466 and perplexity is 34.310677072744696
At time: 151.89655995368958 and batch: 200, loss is 3.406509337425232 and perplexity is 30.159782676432965
At time: 152.81918907165527 and batch: 250, loss is 3.548742814064026 and perplexity is 34.769578174457564
At time: 153.74172496795654 and batch: 300, loss is 3.5197456645965577 and perplexity is 33.77583698021167
At time: 154.66805338859558 and batch: 350, loss is 3.4981789064407347 and perplexity is 33.0552005008405
At time: 155.59398317337036 and batch: 400, loss is 3.4298368740081786 and perplexity is 30.87160637749716
At time: 156.5156512260437 and batch: 450, loss is 3.435675344467163 and perplexity is 31.05237653646656
At time: 157.43990468978882 and batch: 500, loss is 3.30498619556427 and perplexity is 27.248165440656262
At time: 158.3667767047882 and batch: 550, loss is 3.3523153352737425 and perplexity is 28.56880348577317
At time: 159.29523873329163 and batch: 600, loss is 3.382201690673828 and perplexity is 29.435507704957203
At time: 160.22234082221985 and batch: 650, loss is 3.200713109970093 and perplexity is 24.550030828180137
At time: 161.15238165855408 and batch: 700, loss is 3.1941038846969603 and perplexity is 24.388309160133666
At time: 162.07642436027527 and batch: 750, loss is 3.2890674448013306 and perplexity is 26.81784287229329
At time: 163.006005525589 and batch: 800, loss is 3.2160167121887206 and perplexity is 24.92862426726612
At time: 163.9254026412964 and batch: 850, loss is 3.2791542196273804 and perplexity is 26.553304935673545
At time: 164.85203552246094 and batch: 900, loss is 3.2338290119171145 and perplexity is 25.37663863224367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.390229107582406 and perplexity of 80.65889642849471
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 167.08519291877747 and batch: 50, loss is 3.6299086809158325 and perplexity is 37.709372874549
At time: 168.0156373977661 and batch: 100, loss is 3.5057530069351195 and perplexity is 33.30651444841711
At time: 168.9378616809845 and batch: 150, loss is 3.512959351539612 and perplexity is 33.54739957727358
At time: 169.85515356063843 and batch: 200, loss is 3.383100290298462 and perplexity is 29.461970329003126
At time: 170.78011631965637 and batch: 250, loss is 3.526667218208313 and perplexity is 34.010429181332945
At time: 171.70017790794373 and batch: 300, loss is 3.4945042896270753 and perplexity is 32.93395820122351
At time: 172.61856365203857 and batch: 350, loss is 3.4728974866867066 and perplexity is 32.22999325939788
At time: 173.54095768928528 and batch: 400, loss is 3.405294075012207 and perplexity is 30.12315288807189
At time: 174.46163511276245 and batch: 450, loss is 3.4103182983398437 and perplexity is 30.274879169709443
At time: 175.39283776283264 and batch: 500, loss is 3.279403581619263 and perplexity is 26.55992714631287
At time: 176.30923175811768 and batch: 550, loss is 3.325190749168396 and perplexity is 27.80430178431184
At time: 177.23298263549805 and batch: 600, loss is 3.354601583480835 and perplexity is 28.634193582034005
At time: 178.14977836608887 and batch: 650, loss is 3.1742599296569822 and perplexity is 23.909118889380526
At time: 179.07324242591858 and batch: 700, loss is 3.1657161045074464 and perplexity is 23.70571372451992
At time: 179.9898178577423 and batch: 750, loss is 3.258534064292908 and perplexity is 26.01137817200015
At time: 180.91419672966003 and batch: 800, loss is 3.1835324716567994 and perplexity is 24.131848236218367
At time: 181.8390724658966 and batch: 850, loss is 3.2457414531707762 and perplexity is 25.680744072062176
At time: 182.76083302497864 and batch: 900, loss is 3.2015999603271483 and perplexity is 24.57181268898372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38541610926798 and perplexity of 80.2716180286905
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.9897518157959 and batch: 50, loss is 3.6207138013839724 and perplexity is 37.364228943880015
At time: 185.91015362739563 and batch: 100, loss is 3.4952629232406616 and perplexity is 32.95895248849791
At time: 186.83404970169067 and batch: 150, loss is 3.503983807563782 and perplexity is 33.24764067906795
At time: 187.7603781223297 and batch: 200, loss is 3.3742529773712158 and perplexity is 29.202460732110957
At time: 188.68187808990479 and batch: 250, loss is 3.519448037147522 and perplexity is 33.76578585983107
At time: 189.60826992988586 and batch: 300, loss is 3.485273389816284 and perplexity is 32.63134696845742
At time: 190.5303122997284 and batch: 350, loss is 3.4630845546722413 and perplexity is 31.915269234933728
At time: 191.45199489593506 and batch: 400, loss is 3.3968112468719482 and perplexity is 29.868704107525765
At time: 192.37467527389526 and batch: 450, loss is 3.402842812538147 and perplexity is 30.049403560204983
At time: 193.29863834381104 and batch: 500, loss is 3.271291470527649 and perplexity is 26.345341614310133
At time: 194.2201383113861 and batch: 550, loss is 3.316290020942688 and perplexity is 27.55792135980358
At time: 195.14445567131042 and batch: 600, loss is 3.344494647979736 and perplexity is 28.34624721320595
At time: 196.06799983978271 and batch: 650, loss is 3.165032358169556 and perplexity is 23.68951056962887
At time: 196.99163150787354 and batch: 700, loss is 3.1564281511306764 and perplexity is 23.486555501851758
At time: 197.9233419895172 and batch: 750, loss is 3.2491252279281615 and perplexity is 25.767789112955192
At time: 198.84689450263977 and batch: 800, loss is 3.1740803050994875 and perplexity is 23.90482461017048
At time: 199.76996517181396 and batch: 850, loss is 3.23524621963501 and perplexity is 25.41262809661719
At time: 200.69019556045532 and batch: 900, loss is 3.1919309997558596 and perplexity is 24.335373702577574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382812081950984 and perplexity of 80.06286046572733
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.9197804927826 and batch: 50, loss is 3.6169946241378783 and perplexity is 37.22552284993342
At time: 203.85701608657837 and batch: 100, loss is 3.490865864753723 and perplexity is 32.814348196485504
At time: 204.77919745445251 and batch: 150, loss is 3.499959568977356 and perplexity is 33.114113094170314
At time: 205.7059063911438 and batch: 200, loss is 3.3711614751815797 and perplexity is 29.11232066671159
At time: 206.62788438796997 and batch: 250, loss is 3.516802234649658 and perplexity is 33.676566339921834
At time: 207.54784774780273 and batch: 300, loss is 3.4828246068954467 and perplexity is 32.55153764105771
At time: 208.4702262878418 and batch: 350, loss is 3.4599299383163453 and perplexity is 31.814747441811484
At time: 209.391015291214 and batch: 400, loss is 3.3940097379684446 and perplexity is 29.78514376919992
At time: 210.31594896316528 and batch: 450, loss is 3.400532813072205 and perplexity is 29.980069565605348
At time: 211.24341344833374 and batch: 500, loss is 3.2689481163024903 and perplexity is 26.28367742546182
At time: 212.1690857410431 and batch: 550, loss is 3.313798623085022 and perplexity is 27.489349069464037
At time: 213.0999572277069 and batch: 600, loss is 3.3418736457824707 and perplexity is 28.272048916401758
At time: 214.0271816253662 and batch: 650, loss is 3.1624832057952883 and perplexity is 23.629199301480703
At time: 214.9526925086975 and batch: 700, loss is 3.153657121658325 and perplexity is 23.4215636531035
At time: 215.88063669204712 and batch: 750, loss is 3.2464368200302123 and perplexity is 25.698607820625103
At time: 216.80763459205627 and batch: 800, loss is 3.1713788414001467 and perplexity is 23.84033374333553
At time: 217.73636960983276 and batch: 850, loss is 3.23238224029541 and perplexity is 25.339950977344667
At time: 218.6658501625061 and batch: 900, loss is 3.1890800189971924 and perplexity is 24.266092826545574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381851823362585 and perplexity of 79.98601631738487
Annealing...
Model not improving. Stopping early with 78.73929858294282 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -78.73929858294282
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9e58990b70>
ELAPSED
234.2737135887146


RESULTS SO FAR:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.426487922668457 and batch: 50, loss is 7.585795822143555 and perplexity is 1970.013790858967
At time: 2.5777106285095215 and batch: 100, loss is 6.914925813674927 and perplexity is 1007.1963045343713
At time: 3.7272651195526123 and batch: 150, loss is 6.734120864868164 and perplexity is 840.6041545052484
At time: 4.881242036819458 and batch: 200, loss is 6.498088874816895 and perplexity is 663.8716780271582
At time: 6.026424407958984 and batch: 250, loss is 6.510425872802735 and perplexity is 672.1125911229416
At time: 7.174364805221558 and batch: 300, loss is 6.374430408477783 and perplexity is 586.6511842176668
At time: 8.324288845062256 and batch: 350, loss is 6.360115842819214 and perplexity is 578.3133459619066
At time: 9.47439432144165 and batch: 400, loss is 6.237908411026001 and perplexity is 511.7869427668632
At time: 10.621936559677124 and batch: 450, loss is 6.225432052612304 and perplexity is 505.44137255376666
At time: 11.772669792175293 and batch: 500, loss is 6.212027053833008 and perplexity is 498.71114172126016
At time: 12.922075748443604 and batch: 550, loss is 6.218601713180542 and perplexity is 502.00079993202127
At time: 14.069428443908691 and batch: 600, loss is 6.170125617980957 and perplexity is 478.2461786354463
At time: 15.222052097320557 and batch: 650, loss is 6.098447599411011 and perplexity is 445.16615718666475
At time: 16.37291669845581 and batch: 700, loss is 6.212336120605468 and perplexity is 498.8653005856861
At time: 17.519686222076416 and batch: 750, loss is 6.146947793960571 and perplexity is 467.2889458007314
At time: 18.66975712776184 and batch: 800, loss is 6.162729759216308 and perplexity is 474.72218498633305
At time: 19.817952394485474 and batch: 850, loss is 6.19946515083313 and perplexity is 492.485565145378
At time: 20.97021460533142 and batch: 900, loss is 6.063859186172485 and perplexity is 430.0318114951499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.066477736381636 and perplexity of 431.159346994967
finished 1 epochs...
Completing Train Step...
At time: 23.35310459136963 and batch: 50, loss is 5.915874032974243 and perplexity is 370.8783209643417
At time: 24.275780200958252 and batch: 100, loss is 5.579947786331177 and perplexity is 265.05776578650966
At time: 25.198752403259277 and batch: 150, loss is 5.416196146011353 and perplexity is 225.02154338786633
At time: 26.116340160369873 and batch: 200, loss is 5.196437358856201 and perplexity is 180.62758295407232
At time: 27.044381141662598 and batch: 250, loss is 5.213077878952026 and perplexity is 183.65846768815516
At time: 27.970868825912476 and batch: 300, loss is 5.1074756908416745 and perplexity is 165.25267907287414
At time: 28.88994336128235 and batch: 350, loss is 5.0527169895172115 and perplexity is 156.44695229017873
At time: 29.80497646331787 and batch: 400, loss is 4.890343866348267 and perplexity is 132.9993001726674
At time: 30.731401205062866 and batch: 450, loss is 4.87904800415039 and perplexity is 131.50541166938427
At time: 31.647473096847534 and batch: 500, loss is 4.783502864837646 and perplexity is 119.52228804741968
At time: 32.57114934921265 and batch: 550, loss is 4.83833456993103 and perplexity is 126.25890116293476
At time: 33.49520993232727 and batch: 600, loss is 4.765604648590088 and perplexity is 117.40208283489713
At time: 34.416417598724365 and batch: 650, loss is 4.6292769145965575 and perplexity is 102.43996448049663
At time: 35.34199404716492 and batch: 700, loss is 4.678430795669556 and perplexity is 107.60109192408576
At time: 36.262371301651 and batch: 750, loss is 4.694279470443726 and perplexity is 109.32001195784405
At time: 37.18296146392822 and batch: 800, loss is 4.625415983200074 and perplexity is 102.04521334926997
At time: 38.10179829597473 and batch: 850, loss is 4.678762454986572 and perplexity is 107.63678474734502
At time: 39.02029585838318 and batch: 900, loss is 4.615652360916138 and perplexity is 101.05373053843044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.70539793249679 and perplexity of 110.54226455576938
finished 2 epochs...
Completing Train Step...
At time: 41.258453369140625 and batch: 50, loss is 4.670612926483154 and perplexity is 106.76316035061971
At time: 42.19014859199524 and batch: 100, loss is 4.52150390625 and perplexity is 91.97381401385796
At time: 43.10997772216797 and batch: 150, loss is 4.512386083602905 and perplexity is 91.13902460371924
At time: 44.034337282180786 and batch: 200, loss is 4.400478553771973 and perplexity is 81.48985661356907
At time: 44.95101714134216 and batch: 250, loss is 4.530509519577026 and perplexity is 92.80583542749935
At time: 45.875834465026855 and batch: 300, loss is 4.486581678390503 and perplexity is 88.81732021015469
At time: 46.79517698287964 and batch: 350, loss is 4.474295778274536 and perplexity is 87.73279530956921
At time: 47.72230410575867 and batch: 400, loss is 4.373689670562744 and perplexity is 79.33581535986964
At time: 48.65040946006775 and batch: 450, loss is 4.391008491516113 and perplexity is 80.72178518053971
At time: 49.57266330718994 and batch: 500, loss is 4.277772464752197 and perplexity is 72.0797009732887
At time: 50.49617552757263 and batch: 550, loss is 4.356269354820252 and perplexity is 77.96572872537361
At time: 51.41692924499512 and batch: 600, loss is 4.342026748657227 and perplexity is 76.86316389330588
At time: 52.33847975730896 and batch: 650, loss is 4.196229438781739 and perplexity is 66.43535959625329
At time: 53.256155490875244 and batch: 700, loss is 4.218731060028076 and perplexity is 67.9472086326671
At time: 54.18688154220581 and batch: 750, loss is 4.3017551612854 and perplexity is 73.8292623101491
At time: 55.107632875442505 and batch: 800, loss is 4.241284656524658 and perplexity is 69.49707438637567
At time: 56.030359506607056 and batch: 850, loss is 4.313769989013672 and perplexity is 74.72165843575976
At time: 56.95419359207153 and batch: 900, loss is 4.260983171463013 and perplexity is 70.8796360439138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.481803580506207 and perplexity of 88.3939546069766
finished 3 epochs...
Completing Train Step...
At time: 59.18004369735718 and batch: 50, loss is 4.349232473373413 and perplexity is 77.41901895731098
At time: 60.12558579444885 and batch: 100, loss is 4.198902592658997 and perplexity is 66.6131891122814
At time: 61.04953742027283 and batch: 150, loss is 4.203716936111451 and perplexity is 66.93466110049167
At time: 61.97348189353943 and batch: 200, loss is 4.087648258209229 and perplexity is 59.599563976215265
At time: 62.901209115982056 and batch: 250, loss is 4.241648669242859 and perplexity is 69.52237681025272
At time: 63.82236099243164 and batch: 300, loss is 4.206564178466797 and perplexity is 67.12551187288688
At time: 64.75056457519531 and batch: 350, loss is 4.1979314947128294 and perplexity is 66.54853258014067
At time: 65.67200207710266 and batch: 400, loss is 4.115320639610291 and perplexity is 61.271857297861885
At time: 66.5999116897583 and batch: 450, loss is 4.136452617645264 and perplexity is 62.580430514616204
At time: 67.5318591594696 and batch: 500, loss is 4.0086268615722656 and perplexity is 55.07119824249981
At time: 68.4579963684082 and batch: 550, loss is 4.095657620429993 and perplexity is 60.078835238775405
At time: 69.38669896125793 and batch: 600, loss is 4.102399654388428 and perplexity is 60.48525730232293
At time: 70.31051683425903 and batch: 650, loss is 3.948484115600586 and perplexity is 51.85669846361138
At time: 71.23584270477295 and batch: 700, loss is 3.958469352722168 and perplexity is 52.377093705230095
At time: 72.16257381439209 and batch: 750, loss is 4.063433556556702 and perplexity is 58.173711284927826
At time: 73.08567237854004 and batch: 800, loss is 4.0076420736312866 and perplexity is 55.01699148603622
At time: 74.00587868690491 and batch: 850, loss is 4.082266721725464 and perplexity is 59.2796882317467
At time: 74.93990635871887 and batch: 900, loss is 4.037224392890931 and perplexity is 56.66883389693952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.401872608759632 and perplexity of 81.603537174691
finished 4 epochs...
Completing Train Step...
At time: 77.19054245948792 and batch: 50, loss is 4.130666646957398 and perplexity is 62.21938747747255
At time: 78.11429524421692 and batch: 100, loss is 3.9841278123855592 and perplexity is 53.73839906615869
At time: 79.0387761592865 and batch: 150, loss is 3.9894025230407717 and perplexity is 54.022602458699645
At time: 79.96038699150085 and batch: 200, loss is 3.8746427059173585 and perplexity is 48.165485973226474
At time: 80.88458490371704 and batch: 250, loss is 4.034316296577454 and perplexity is 56.50427486272845
At time: 81.80487203598022 and batch: 300, loss is 4.003423986434936 and perplexity is 54.785413769525206
At time: 82.72862577438354 and batch: 350, loss is 3.9965598440170287 and perplexity is 54.41064658612124
At time: 83.65404224395752 and batch: 400, loss is 3.9215040397644043 and perplexity is 50.47630608606656
At time: 84.57966637611389 and batch: 450, loss is 3.945316505432129 and perplexity is 51.69269654246153
At time: 85.50420212745667 and batch: 500, loss is 3.8147733402252197 and perplexity is 45.36647246308475
At time: 86.42628264427185 and batch: 550, loss is 3.9018602848052977 and perplexity is 49.494437266506445
At time: 87.35338687896729 and batch: 600, loss is 3.9217838859558105 and perplexity is 50.490433664763344
At time: 88.27795505523682 and batch: 650, loss is 3.76124032497406 and perplexity is 43.00172903382277
At time: 89.2034056186676 and batch: 700, loss is 3.7690438270568847 and perplexity is 43.338605815740344
At time: 90.12720322608948 and batch: 750, loss is 3.880601463317871 and perplexity is 48.453349221090185
At time: 91.05170202255249 and batch: 800, loss is 3.8270591115951538 and perplexity is 45.92727244790577
At time: 91.9784369468689 and batch: 850, loss is 3.901375398635864 and perplexity is 49.47044391590405
At time: 92.90450763702393 and batch: 900, loss is 3.860854887962341 and perplexity is 47.505946275943415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37693849328446 and perplexity of 79.59398250201917
finished 5 epochs...
Completing Train Step...
At time: 95.14522910118103 and batch: 50, loss is 3.9550457429885864 and perplexity is 52.19808158607255
At time: 96.07889413833618 and batch: 100, loss is 3.8131157112121583 and perplexity is 45.2913339751835
At time: 96.99762725830078 and batch: 150, loss is 3.8212781238555906 and perplexity is 45.662533412824665
At time: 97.92209458351135 and batch: 200, loss is 3.7070591592788698 and perplexity is 40.733838479065206
At time: 98.84536695480347 and batch: 250, loss is 3.867381730079651 and perplexity is 47.817024160722866
At time: 99.78326630592346 and batch: 300, loss is 3.840171904563904 and perplexity is 46.533473068663916
At time: 100.70455646514893 and batch: 350, loss is 3.833658061027527 and perplexity is 46.231346377161564
At time: 101.62519216537476 and batch: 400, loss is 3.762411036491394 and perplexity is 43.05210113311315
At time: 102.54487204551697 and batch: 450, loss is 3.7839890909194946 and perplexity is 43.991176992780005
At time: 103.46576929092407 and batch: 500, loss is 3.6552234935760497 and perplexity is 38.676164023513664
At time: 104.38911986351013 and batch: 550, loss is 3.7446626472473143 and perplexity is 42.29473656604648
At time: 105.30762076377869 and batch: 600, loss is 3.767190237045288 and perplexity is 43.2583482142071
At time: 106.23157548904419 and batch: 650, loss is 3.606037449836731 and perplexity is 36.819862810106436
At time: 107.1536386013031 and batch: 700, loss is 3.6130042028427125 and perplexity is 37.07727331659721
At time: 108.07961988449097 and batch: 750, loss is 3.7313505172729493 and perplexity is 41.73543454584059
At time: 109.00251007080078 and batch: 800, loss is 3.6788384103775025 and perplexity is 39.60036796954548
At time: 109.92041850090027 and batch: 850, loss is 3.75353093624115 and perplexity is 42.67148660778562
At time: 110.8452079296112 and batch: 900, loss is 3.7153189182281494 and perplexity is 41.07168350446855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38287813369542 and perplexity of 80.06814893197989
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 113.06520247459412 and batch: 50, loss is 3.836778883934021 and perplexity is 46.37585159224168
At time: 113.99775314331055 and batch: 100, loss is 3.695792860984802 and perplexity is 40.27749438538929
At time: 114.92038416862488 and batch: 150, loss is 3.7067517948150637 and perplexity is 40.7213202685674
At time: 115.84512782096863 and batch: 200, loss is 3.574127445220947 and perplexity is 35.6634888878081
At time: 116.76350498199463 and batch: 250, loss is 3.7257708501815796 and perplexity is 41.503213176599665
At time: 117.68980407714844 and batch: 300, loss is 3.6902683925628663 and perplexity is 40.05559613882684
At time: 118.61180567741394 and batch: 350, loss is 3.6697918033599852 and perplexity is 39.24373459603461
At time: 119.53103017807007 and batch: 400, loss is 3.595357422828674 and perplexity is 36.4287181167265
At time: 120.45299506187439 and batch: 450, loss is 3.599608678817749 and perplexity is 36.58391558112702
At time: 121.36799931526184 and batch: 500, loss is 3.460496816635132 and perplexity is 31.832787645170892
At time: 122.28846287727356 and batch: 550, loss is 3.5341107654571533 and perplexity is 34.26453195766622
At time: 123.21714544296265 and batch: 600, loss is 3.541845679283142 and perplexity is 34.530592812027905
At time: 124.14072942733765 and batch: 650, loss is 3.364726119041443 and perplexity is 28.925574050138522
At time: 125.08048915863037 and batch: 700, loss is 3.354979147911072 and perplexity is 28.64500687624828
At time: 126.01820516586304 and batch: 750, loss is 3.4544515371322633 and perplexity is 31.64093004719506
At time: 126.95099592208862 and batch: 800, loss is 3.383411417007446 and perplexity is 29.47113816097652
At time: 127.87910008430481 and batch: 850, loss is 3.4352293014526367 and perplexity is 31.038528929371818
At time: 128.80425834655762 and batch: 900, loss is 3.384202170372009 and perplexity is 29.494451779080975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372937346157962 and perplexity of 79.27615153589069
finished 7 epochs...
Completing Train Step...
At time: 131.03369522094727 and batch: 50, loss is 3.718594722747803 and perplexity is 41.206446919713656
At time: 131.96937251091003 and batch: 100, loss is 3.5694468641281127 and perplexity is 35.49695308222329
At time: 132.90075731277466 and batch: 150, loss is 3.5769492387771606 and perplexity is 35.764266010191115
At time: 133.84776973724365 and batch: 200, loss is 3.4504802942276003 and perplexity is 31.515525399218056
At time: 134.79554653167725 and batch: 250, loss is 3.603297390937805 and perplexity is 36.71911231154203
At time: 135.72674989700317 and batch: 300, loss is 3.5726978015899657 and perplexity is 35.61253923666112
At time: 136.6727159023285 and batch: 350, loss is 3.554290814399719 and perplexity is 34.96301590596856
At time: 137.59636449813843 and batch: 400, loss is 3.4858305501937865 and perplexity is 32.64953292783033
At time: 138.51561832427979 and batch: 450, loss is 3.4935476636886595 and perplexity is 32.90246778722843
At time: 139.44984078407288 and batch: 500, loss is 3.36095338344574 and perplexity is 28.816651105225652
At time: 140.3804018497467 and batch: 550, loss is 3.43691463470459 and perplexity is 31.090883299156292
At time: 141.30884766578674 and batch: 600, loss is 3.4530002069473267 and perplexity is 31.595041917806494
At time: 142.23312091827393 and batch: 650, loss is 3.2805189085006714 and perplexity is 26.58956667283523
At time: 143.15699315071106 and batch: 700, loss is 3.278213529586792 and perplexity is 26.528338250972745
At time: 144.08102655410767 and batch: 750, loss is 3.3846271657943725 and perplexity is 29.506989450109884
At time: 145.00678539276123 and batch: 800, loss is 3.3199302625656126 and perplexity is 27.658421663893407
At time: 145.94861793518066 and batch: 850, loss is 3.379227056503296 and perplexity is 29.348077938165126
At time: 146.87577056884766 and batch: 900, loss is 3.336625237464905 and perplexity is 28.124054367344137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39656631260702 and perplexity of 81.1716714563697
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 149.1077663898468 and batch: 50, loss is 3.664288487434387 and perplexity is 39.02835711519777
At time: 150.0368230342865 and batch: 100, loss is 3.5293122577667235 and perplexity is 34.10050718934613
At time: 150.96294140815735 and batch: 150, loss is 3.5360901021957396 and perplexity is 34.33242016925193
At time: 151.88124179840088 and batch: 200, loss is 3.4092548942565917 and perplexity is 30.242701851358525
At time: 152.80846095085144 and batch: 250, loss is 3.5590563106536863 and perplexity is 35.13002966293967
At time: 153.73050165176392 and batch: 300, loss is 3.521845941543579 and perplexity is 33.84685013953408
At time: 154.65526580810547 and batch: 350, loss is 3.501292052268982 and perplexity is 33.15826650689476
At time: 155.58159375190735 and batch: 400, loss is 3.430600504875183 and perplexity is 30.895189892444986
At time: 156.5026969909668 and batch: 450, loss is 3.431942873001099 and perplexity is 30.93669045888768
At time: 157.4270429611206 and batch: 500, loss is 3.293371834754944 and perplexity is 26.933526120022343
At time: 158.34766149520874 and batch: 550, loss is 3.3628542900085447 and perplexity is 28.871480963137614
At time: 159.27290081977844 and batch: 600, loss is 3.3754311180114747 and perplexity is 29.23688561258853
At time: 160.19716501235962 and batch: 650, loss is 3.1984266090393065 and perplexity is 24.49396128579912
At time: 161.12118577957153 and batch: 700, loss is 3.189172878265381 and perplexity is 24.268346262791844
At time: 162.04351782798767 and batch: 750, loss is 3.2859213876724245 and perplexity is 26.733604984626467
At time: 162.9696843624115 and batch: 800, loss is 3.2146661615371706 and perplexity is 24.894979622032434
At time: 163.89678525924683 and batch: 850, loss is 3.2698514795303346 and perplexity is 26.30743186096684
At time: 164.81909894943237 and batch: 900, loss is 3.227358341217041 and perplexity is 25.21296487076494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.397309603756422 and perplexity of 81.23202806983481
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 167.0594506263733 and batch: 50, loss is 3.643929805755615 and perplexity is 38.24182476350543
At time: 167.99175930023193 and batch: 100, loss is 3.5083182573318483 and perplexity is 33.392063678504655
At time: 168.91546082496643 and batch: 150, loss is 3.5130795574188234 and perplexity is 33.55143241431553
At time: 169.8454875946045 and batch: 200, loss is 3.385530209541321 and perplexity is 29.533647587344532
At time: 170.77096319198608 and batch: 250, loss is 3.5377670335769653 and perplexity is 34.3900415821018
At time: 171.69295454025269 and batch: 300, loss is 3.4997055959701537 and perplexity is 33.10570407116459
At time: 172.6152377128601 and batch: 350, loss is 3.477981276512146 and perplexity is 32.394260968668185
At time: 173.53429317474365 and batch: 400, loss is 3.4091650915145872 and perplexity is 30.23998609574963
At time: 174.45604038238525 and batch: 450, loss is 3.407523236274719 and perplexity is 30.190377152620584
At time: 175.37895894050598 and batch: 500, loss is 3.2680693531036376 and perplexity is 26.260590442491008
At time: 176.29946899414062 and batch: 550, loss is 3.3346236181259155 and perplexity is 28.067817017890942
At time: 177.22350692749023 and batch: 600, loss is 3.3486032772064207 and perplexity is 28.462951015127256
At time: 178.1441249847412 and batch: 650, loss is 3.171606168746948 and perplexity is 23.84575391920602
At time: 179.07118201255798 and batch: 700, loss is 3.1617666578292845 and perplexity is 23.612273911434016
At time: 179.99987483024597 and batch: 750, loss is 3.2555316209793093 and perplexity is 25.933397608222013
At time: 180.92898225784302 and batch: 800, loss is 3.18175154209137 and perplexity is 24.08890936113124
At time: 181.84939575195312 and batch: 850, loss is 3.234436011314392 and perplexity is 25.3920469125337
At time: 182.76993227005005 and batch: 900, loss is 3.195552177429199 and perplexity is 24.42365616126579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.389757548293022 and perplexity of 80.62086994318827
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 185.00818419456482 and batch: 50, loss is 3.6342221784591673 and perplexity is 37.87238348197999
At time: 185.93021535873413 and batch: 100, loss is 3.4966041326522825 and perplexity is 33.00318700301533
At time: 186.85376477241516 and batch: 150, loss is 3.5030521059036257 and perplexity is 33.21667822317656
At time: 187.77819395065308 and batch: 200, loss is 3.375678849220276 and perplexity is 29.244129398821837
At time: 188.70311546325684 and batch: 250, loss is 3.5284357976913454 and perplexity is 34.07063255012123
At time: 189.62904381752014 and batch: 300, loss is 3.490297179222107 and perplexity is 32.795692456564204
At time: 190.54852628707886 and batch: 350, loss is 3.46829617023468 and perplexity is 32.08203352699178
At time: 191.47306776046753 and batch: 400, loss is 3.401335506439209 and perplexity is 30.00414402950159
At time: 192.4000735282898 and batch: 450, loss is 3.3995823860168457 and perplexity is 29.951589232751978
At time: 193.32389163970947 and batch: 500, loss is 3.2589266014099123 and perplexity is 26.021590607645987
At time: 194.2399492263794 and batch: 550, loss is 3.3249600458145143 and perplexity is 27.797887978509564
At time: 195.1655445098877 and batch: 600, loss is 3.339597420692444 and perplexity is 28.207768555351205
At time: 196.0884907245636 and batch: 650, loss is 3.162118549346924 and perplexity is 23.620584332433204
At time: 197.0114142894745 and batch: 700, loss is 3.1529693508148195 and perplexity is 23.40546052278012
At time: 197.93325448036194 and batch: 750, loss is 3.2472398853302002 and perplexity is 25.719253769736934
At time: 198.85312509536743 and batch: 800, loss is 3.1727139854431154 and perplexity is 23.872185281379398
At time: 199.7754967212677 and batch: 850, loss is 3.2233433675765992 and perplexity is 25.111938426379297
At time: 200.6933171749115 and batch: 900, loss is 3.1855912446975707 and perplexity is 24.181581411885364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387887615047089 and perplexity of 80.4702551618462
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.92492389678955 and batch: 50, loss is 3.630214147567749 and perplexity is 37.720893589934455
At time: 203.86205315589905 and batch: 100, loss is 3.4932037925720216 and perplexity is 32.891155523982064
At time: 204.7828814983368 and batch: 150, loss is 3.5001974058151246 and perplexity is 33.12198978676085
At time: 205.708322763443 and batch: 200, loss is 3.373202180862427 and perplexity is 29.17179100496877
At time: 206.6318175792694 and batch: 250, loss is 3.5255158281326295 and perplexity is 33.971292445851866
At time: 207.5551826953888 and batch: 300, loss is 3.487277054786682 and perplexity is 32.69679480118396
At time: 208.4784893989563 and batch: 350, loss is 3.4644103765487673 and perplexity is 31.957611259869687
At time: 209.40016841888428 and batch: 400, loss is 3.3981320095062255 and perplexity is 29.908179639010275
At time: 210.32237672805786 and batch: 450, loss is 3.3970739889144896 and perplexity is 29.876552902910152
At time: 211.24880623817444 and batch: 500, loss is 3.2564189577102662 and perplexity is 25.956419477042655
At time: 212.17816948890686 and batch: 550, loss is 3.322153615951538 and perplexity is 27.71998452198336
At time: 213.1077961921692 and batch: 600, loss is 3.3374666500091554 and perplexity is 28.147728257841514
At time: 214.03483152389526 and batch: 650, loss is 3.1591943073272706 and perplexity is 23.55161292089839
At time: 214.96208453178406 and batch: 700, loss is 3.150359330177307 and perplexity is 23.344451439901754
At time: 215.89726662635803 and batch: 750, loss is 3.24506196975708 and perplexity is 25.663300359441678
At time: 216.81718158721924 and batch: 800, loss is 3.1702915096282958 and perplexity is 23.814425478996945
At time: 217.74270462989807 and batch: 850, loss is 3.2203165817260744 and perplexity is 25.036044881166603
At time: 218.66581392288208 and batch: 900, loss is 3.1825795555114746 and perplexity is 24.108863561390685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386839566165453 and perplexity of 80.38596258000759
Annealing...
Model not improving. Stopping early with 79.27615153589069 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9e58990b70>
ELAPSED
461.1798975467682


RESULTS SO FAR:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}, {'best_accuracy': -79.27615153589069, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.45979727262087977, 'dropout': 0.13450648890583938}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.4067249298095703 and batch: 50, loss is 6.9820792102813725 and perplexity is 1077.15567342532
At time: 2.549076557159424 and batch: 100, loss is 6.05997163772583 and perplexity is 428.36328732999414
At time: 3.6974895000457764 and batch: 150, loss is 5.773225927352906 and perplexity is 321.5734337368011
At time: 4.849956274032593 and batch: 200, loss is 5.522439441680908 and perplexity is 250.24475068232232
At time: 6.0018110275268555 and batch: 250, loss is 5.520451345443726 and perplexity is 249.74773425706442
At time: 7.148447751998901 and batch: 300, loss is 5.406174287796021 and perplexity is 222.77767204559166
At time: 8.298516750335693 and batch: 350, loss is 5.351683492660523 and perplexity is 210.96315400542116
At time: 9.44425368309021 and batch: 400, loss is 5.18184684753418 and perplexity is 178.01126726568253
At time: 10.589776039123535 and batch: 450, loss is 5.178914308547974 and perplexity is 177.4900069660777
At time: 11.738449573516846 and batch: 500, loss is 5.103060979843139 and perplexity is 164.52474424624364
At time: 12.887862205505371 and batch: 550, loss is 5.1537388324737545 and perplexity is 173.07738947430974
At time: 14.039424657821655 and batch: 600, loss is 5.063252248764038 and perplexity is 158.1038742166033
At time: 15.1871497631073 and batch: 650, loss is 4.945485525131225 and perplexity is 140.5390695393427
At time: 16.333048105239868 and batch: 700, loss is 5.024616870880127 and perplexity is 152.11196642332598
At time: 17.48138451576233 and batch: 750, loss is 5.008789567947388 and perplexity is 149.72339642342246
At time: 18.633694648742676 and batch: 800, loss is 4.95549578666687 and perplexity is 141.95296731802694
At time: 19.78718113899231 and batch: 850, loss is 5.000778923034668 and perplexity is 148.52880656523854
At time: 20.937528610229492 and batch: 900, loss is 4.918746013641357 and perplexity is 136.83092145048064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.911679620612158 and perplexity of 135.86742859978975
finished 1 epochs...
Completing Train Step...
At time: 23.323094129562378 and batch: 50, loss is 4.885774250030518 and perplexity is 132.3929308929793
At time: 24.24406147003174 and batch: 100, loss is 4.756421356201172 and perplexity is 116.32888048879947
At time: 25.165573596954346 and batch: 150, loss is 4.744835243225098 and perplexity is 114.98885874883206
At time: 26.084547996520996 and batch: 200, loss is 4.623168315887451 and perplexity is 101.816107232434
At time: 27.01619052886963 and batch: 250, loss is 4.731447639465332 and perplexity is 113.45969224775914
At time: 27.941166639328003 and batch: 300, loss is 4.689713792800903 and perplexity is 108.82202970161545
At time: 28.864951848983765 and batch: 350, loss is 4.667386054992676 and perplexity is 106.41920460121072
At time: 29.787351846694946 and batch: 400, loss is 4.555738153457642 and perplexity is 95.17698453261065
At time: 30.709052324295044 and batch: 450, loss is 4.5728497886657715 and perplexity is 96.8196324869094
At time: 31.6323823928833 and batch: 500, loss is 4.471104030609131 and perplexity is 87.45322076799695
At time: 32.55402970314026 and batch: 550, loss is 4.542500705718994 and perplexity is 93.92538643256788
At time: 33.47381544113159 and batch: 600, loss is 4.512599515914917 and perplexity is 91.1584786924466
At time: 34.396037101745605 and batch: 650, loss is 4.367923345565796 and perplexity is 78.87965571096065
At time: 35.31678009033203 and batch: 700, loss is 4.405930843353271 and perplexity is 81.93537635741318
At time: 36.240742206573486 and batch: 750, loss is 4.470290431976318 and perplexity is 87.38209788380898
At time: 37.16255593299866 and batch: 800, loss is 4.402351484298706 and perplexity is 81.64262447077392
At time: 38.08945870399475 and batch: 850, loss is 4.472055587768555 and perplexity is 87.53647711163624
At time: 39.00935101509094 and batch: 900, loss is 4.4132407283782955 and perplexity is 82.53650896643438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.563436011745505 and perplexity of 95.91247067396361
finished 2 epochs...
Completing Train Step...
At time: 41.24233293533325 and batch: 50, loss is 4.462725458145141 and perplexity is 86.72354869436057
At time: 42.17302346229553 and batch: 100, loss is 4.322280416488647 and perplexity is 75.36028533012002
At time: 43.09532690048218 and batch: 150, loss is 4.325625796318054 and perplexity is 75.61281627905514
At time: 44.019721269607544 and batch: 200, loss is 4.213137230873108 and perplexity is 67.56818464207724
At time: 44.94204378128052 and batch: 250, loss is 4.360429544448852 and perplexity is 78.29075656134323
At time: 45.86396598815918 and batch: 300, loss is 4.329255657196045 and perplexity is 75.88777901908118
At time: 46.78642272949219 and batch: 350, loss is 4.3147454357147215 and perplexity is 74.79458099122886
At time: 47.710588455200195 and batch: 400, loss is 4.2292157173156735 and perplexity is 68.66335956697544
At time: 48.63556504249573 and batch: 450, loss is 4.256776213645935 and perplexity is 70.5820747577421
At time: 49.56018614768982 and batch: 500, loss is 4.134591512680053 and perplexity is 62.46407007774042
At time: 50.49360728263855 and batch: 550, loss is 4.2122193574905396 and perplexity is 67.50619405799158
At time: 51.41640615463257 and batch: 600, loss is 4.215146455764771 and perplexity is 67.70408079779638
At time: 52.339293003082275 and batch: 650, loss is 4.057099871635437 and perplexity is 57.806421703026686
At time: 53.258624792099 and batch: 700, loss is 4.079081463813782 and perplexity is 59.091167538711105
At time: 54.18057203292847 and batch: 750, loss is 4.175909385681153 and perplexity is 65.09901286146183
At time: 55.10085082054138 and batch: 800, loss is 4.118610363006592 and perplexity is 61.47375667481728
At time: 56.02531027793884 and batch: 850, loss is 4.194713673591614 and perplexity is 66.33473547132247
At time: 56.94858908653259 and batch: 900, loss is 4.143220553398132 and perplexity is 63.00540733346007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.437336124785959 and perplexity of 84.54941233231008
finished 3 epochs...
Completing Train Step...
At time: 59.18239951133728 and batch: 50, loss is 4.214153838157654 and perplexity is 67.63690987815684
At time: 60.11537718772888 and batch: 100, loss is 4.073751635551453 and perplexity is 58.777059578225426
At time: 61.03827404975891 and batch: 150, loss is 4.082198128700257 and perplexity is 59.2756221980499
At time: 61.96418309211731 and batch: 200, loss is 3.9713500118255616 and perplexity is 53.056108887616034
At time: 62.88866305351257 and batch: 250, loss is 4.125731840133667 and perplexity is 61.91310316809116
At time: 63.813515186309814 and batch: 300, loss is 4.094622573852539 and perplexity is 60.016683016755195
At time: 64.74068021774292 and batch: 350, loss is 4.085779027938843 and perplexity is 59.488262723372
At time: 65.66534185409546 and batch: 400, loss is 4.012856259346008 and perplexity is 55.30460949231966
At time: 66.58946013450623 and batch: 450, loss is 4.042279653549194 and perplexity is 56.9560349499464
At time: 67.51174139976501 and batch: 500, loss is 3.910156078338623 and perplexity is 49.906740726742925
At time: 68.44139289855957 and batch: 550, loss is 3.991038498878479 and perplexity is 54.11105446395597
At time: 69.36613988876343 and batch: 600, loss is 4.01188473701477 and perplexity is 55.25090592051117
At time: 70.29754447937012 and batch: 650, loss is 3.8429853296279908 and perplexity is 46.664575845556406
At time: 71.22202134132385 and batch: 700, loss is 3.8606812715530396 and perplexity is 47.49769918006682
At time: 72.14772295951843 and batch: 750, loss is 3.9723606729507446 and perplexity is 53.10975774016049
At time: 73.09178829193115 and batch: 800, loss is 3.917991008758545 and perplexity is 50.299292367116045
At time: 74.0203914642334 and batch: 850, loss is 3.997094011306763 and perplexity is 54.43971873774577
At time: 74.94340467453003 and batch: 900, loss is 3.9504674768447874 and perplexity is 51.95965109197397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3828668463720035 and perplexity of 80.06724518198801
finished 4 epochs...
Completing Train Step...
At time: 77.18488669395447 and batch: 50, loss is 4.028854093551636 and perplexity is 56.19647843017969
At time: 78.10791540145874 and batch: 100, loss is 3.8905469465255735 and perplexity is 48.93764548037086
At time: 79.02949714660645 and batch: 150, loss is 3.8992422008514405 and perplexity is 49.365026152992904
At time: 79.95714259147644 and batch: 200, loss is 3.78951856136322 and perplexity is 44.235098663086895
At time: 80.88151907920837 and batch: 250, loss is 3.94222909450531 and perplexity is 51.53334606310396
At time: 81.80385899543762 and batch: 300, loss is 3.9150833654403687 and perplexity is 50.15325238473519
At time: 82.7250337600708 and batch: 350, loss is 3.907743201255798 and perplexity is 49.78646705690822
At time: 83.64709568023682 and batch: 400, loss is 3.841432194709778 and perplexity is 46.592155717040086
At time: 84.57448863983154 and batch: 450, loss is 3.8733245229721067 and perplexity is 48.10203687901121
At time: 85.49929022789001 and batch: 500, loss is 3.7413351345062256 and perplexity is 42.1542341826183
At time: 86.42166662216187 and batch: 550, loss is 3.8215809297561645 and perplexity is 45.67636239101965
At time: 87.34482407569885 and batch: 600, loss is 3.850016655921936 and perplexity is 46.99384595089044
At time: 88.27200245857239 and batch: 650, loss is 3.680735778808594 and perplexity is 39.6755757834941
At time: 89.19853949546814 and batch: 700, loss is 3.6931702518463134 and perplexity is 40.172000655415054
At time: 90.12093710899353 and batch: 750, loss is 3.8093948936462403 and perplexity is 45.12312631321876
At time: 91.04563331604004 and batch: 800, loss is 3.7598243236541746 and perplexity is 42.94088161893031
At time: 91.97072267532349 and batch: 850, loss is 3.836967792510986 and perplexity is 46.3846132159184
At time: 92.8928701877594 and batch: 900, loss is 3.7924067115783693 and perplexity is 44.36304094213329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371712044493793 and perplexity of 79.17907382237257
finished 5 epochs...
Completing Train Step...
At time: 95.12163925170898 and batch: 50, loss is 3.87595513343811 and perplexity is 48.22874118242723
At time: 96.05307197570801 and batch: 100, loss is 3.742605800628662 and perplexity is 42.20783218528295
At time: 96.97136187553406 and batch: 150, loss is 3.7490069246292115 and perplexity is 42.47887632096805
At time: 97.89783191680908 and batch: 200, loss is 3.6414936208724975 and perplexity is 38.14877399856852
At time: 98.8184461593628 and batch: 250, loss is 3.793042612075806 and perplexity is 44.39126039336681
At time: 99.74262356758118 and batch: 300, loss is 3.767223734855652 and perplexity is 43.259797298422676
At time: 100.66711115837097 and batch: 350, loss is 3.760279755592346 and perplexity is 42.960442721910226
At time: 101.58866882324219 and batch: 400, loss is 3.6974849605560305 and perplexity is 40.345705610185874
At time: 102.50680899620056 and batch: 450, loss is 3.729194974899292 and perplexity is 41.645568937553016
At time: 103.42641282081604 and batch: 500, loss is 3.600649662017822 and perplexity is 36.62201865152709
At time: 104.34900116920471 and batch: 550, loss is 3.679406123161316 and perplexity is 39.62285598744915
At time: 105.27444863319397 and batch: 600, loss is 3.71257465839386 and perplexity is 40.95912664637141
At time: 106.19386839866638 and batch: 650, loss is 3.542387523651123 and perplexity is 34.54930808919202
At time: 107.11365032196045 and batch: 700, loss is 3.5541301584243774 and perplexity is 34.95739933972684
At time: 108.02904152870178 and batch: 750, loss is 3.6703265953063964 and perplexity is 39.26472744214585
At time: 108.95600152015686 and batch: 800, loss is 3.6237298107147216 and perplexity is 37.47708991633246
At time: 109.8761465549469 and batch: 850, loss is 3.7015543794631958 and perplexity is 40.51022370690833
At time: 110.80100059509277 and batch: 900, loss is 3.658153200149536 and perplexity is 38.78963997995297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385128909594392 and perplexity of 80.2485673564255
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 113.03630352020264 and batch: 50, loss is 3.769951095581055 and perplexity is 43.37794341086097
At time: 113.97123050689697 and batch: 100, loss is 3.6432638597488403 and perplexity is 38.21636625095065
At time: 114.88646459579468 and batch: 150, loss is 3.6471767044067382 and perplexity is 38.366193890459925
At time: 115.81124782562256 and batch: 200, loss is 3.528612675666809 and perplexity is 34.07665942762469
At time: 116.7304458618164 and batch: 250, loss is 3.6736250209808348 and perplexity is 39.39445305477283
At time: 117.6520619392395 and batch: 300, loss is 3.631900815963745 and perplexity is 37.7845699143475
At time: 118.57326126098633 and batch: 350, loss is 3.612775139808655 and perplexity is 37.06878125652238
At time: 119.5067069530487 and batch: 400, loss is 3.5452364540100096 and perplexity is 34.64787700320287
At time: 120.43043375015259 and batch: 450, loss is 3.5541622686386107 and perplexity is 34.958521847330566
At time: 121.35134291648865 and batch: 500, loss is 3.419822959899902 and perplexity is 30.56400348779828
At time: 122.27234220504761 and batch: 550, loss is 3.4760553455352783 and perplexity is 32.331931898005834
At time: 123.19155955314636 and batch: 600, loss is 3.502401895523071 and perplexity is 33.19508741423302
At time: 124.11603140830994 and batch: 650, loss is 3.3140282249450683 and perplexity is 27.495661399775507
At time: 125.0389301776886 and batch: 700, loss is 3.303880310058594 and perplexity is 27.218048745316946
At time: 125.96026802062988 and batch: 750, loss is 3.403936619758606 and perplexity is 30.082289797112516
At time: 126.88441801071167 and batch: 800, loss is 3.339732623100281 and perplexity is 28.211582571405863
At time: 127.80870413780212 and batch: 850, loss is 3.394263310432434 and perplexity is 29.792697419154095
At time: 128.73054933547974 and batch: 900, loss is 3.341291642189026 and perplexity is 28.255599269679447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358041005591824 and perplexity of 78.10397919844367
finished 7 epochs...
Completing Train Step...
At time: 130.97076535224915 and batch: 50, loss is 3.6521118593215944 and perplexity is 38.55600498914181
At time: 131.89472723007202 and batch: 100, loss is 3.5168279933929445 and perplexity is 33.67743381712146
At time: 132.81984448432922 and batch: 150, loss is 3.5207533645629883 and perplexity is 33.809890044757175
At time: 133.74560618400574 and batch: 200, loss is 3.407088737487793 and perplexity is 30.177262319766733
At time: 134.6681830883026 and batch: 250, loss is 3.550623269081116 and perplexity is 34.835022315274486
At time: 135.59207248687744 and batch: 300, loss is 3.5140639877319337 and perplexity is 33.58447772416613
At time: 136.5155942440033 and batch: 350, loss is 3.4983268070220945 and perplexity is 33.060089745763705
At time: 137.4379415512085 and batch: 400, loss is 3.4369569873809813 and perplexity is 31.092200109160398
At time: 138.3621904850006 and batch: 450, loss is 3.452311143875122 and perplexity is 31.573278440221657
At time: 139.28569197654724 and batch: 500, loss is 3.320492835044861 and perplexity is 27.673985908338263
At time: 140.2158761024475 and batch: 550, loss is 3.3829010725021362 and perplexity is 29.456101564799315
At time: 141.1407186985016 and batch: 600, loss is 3.4162868070602417 and perplexity is 30.456115366938768
At time: 142.07839250564575 and batch: 650, loss is 3.2330335521698 and perplexity is 25.356460564174018
At time: 143.00093913078308 and batch: 700, loss is 3.2290860271453856 and perplexity is 25.25656260613639
At time: 143.92534399032593 and batch: 750, loss is 3.3355851697921755 and perplexity is 28.094818653766325
At time: 144.8508379459381 and batch: 800, loss is 3.277600574493408 and perplexity is 26.512082553438063
At time: 145.77234530448914 and batch: 850, loss is 3.3404224634170534 and perplexity is 28.231050772666595
At time: 146.69867086410522 and batch: 900, loss is 3.295116453170776 and perplexity is 26.980555858239654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379463509337543 and perplexity of 79.79521253317981
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 148.9430046081543 and batch: 50, loss is 3.5971333599090576 and perplexity is 36.493470709267015
At time: 149.87491369247437 and batch: 100, loss is 3.474100694656372 and perplexity is 32.26879598334194
At time: 150.79612493515015 and batch: 150, loss is 3.4866014432907106 and perplexity is 32.67471193129523
At time: 151.7229163646698 and batch: 200, loss is 3.3715390872955324 and perplexity is 29.12331590749802
At time: 152.64414954185486 and batch: 250, loss is 3.510204167366028 and perplexity is 33.455097525858044
At time: 153.57170844078064 and batch: 300, loss is 3.4777972364425658 and perplexity is 32.38829967520078
At time: 154.4926860332489 and batch: 350, loss is 3.449943723678589 and perplexity is 31.498619632426106
At time: 155.41965794563293 and batch: 400, loss is 3.388868799209595 and perplexity is 29.63241309505974
At time: 156.34465265274048 and batch: 450, loss is 3.3946071004867555 and perplexity is 29.8029416130433
At time: 157.27029848098755 and batch: 500, loss is 3.2608064460754393 and perplexity is 26.070553162518372
At time: 158.19604563713074 and batch: 550, loss is 3.3147806835174563 and perplexity is 27.516358531789898
At time: 159.11560130119324 and batch: 600, loss is 3.3426771354675293 and perplexity is 28.294774344682544
At time: 160.04134798049927 and batch: 650, loss is 3.156319456100464 and perplexity is 23.484002768729113
At time: 160.96893095970154 and batch: 700, loss is 3.142342143058777 and perplexity is 23.15804283929888
At time: 161.89454174041748 and batch: 750, loss is 3.243853840827942 and perplexity is 25.632314505082647
At time: 162.81697750091553 and batch: 800, loss is 3.1776494932174684 and perplexity is 23.990297870466335
At time: 163.74266290664673 and batch: 850, loss is 3.2318724632263183 and perplexity is 25.32703654342532
At time: 164.6663007736206 and batch: 900, loss is 3.188317127227783 and perplexity is 24.24758748373685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374066914597603 and perplexity of 79.36574996893619
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 166.90811276435852 and batch: 50, loss is 3.575883936882019 and perplexity is 35.726186556490475
At time: 167.84024453163147 and batch: 100, loss is 3.449970507621765 and perplexity is 31.499463300962805
At time: 168.763605594635 and batch: 150, loss is 3.4618820428848265 and perplexity is 31.87691381352532
At time: 169.68590807914734 and batch: 200, loss is 3.3482402420043944 and perplexity is 28.452619837359286
At time: 170.6067509651184 and batch: 250, loss is 3.4852912759780885 and perplexity is 32.631930623228854
At time: 171.52936792373657 and batch: 300, loss is 3.456232829093933 and perplexity is 31.697342009719307
At time: 172.4503378868103 and batch: 350, loss is 3.4253395652770995 and perplexity is 30.733078966346433
At time: 173.3763689994812 and batch: 400, loss is 3.36608832359314 and perplexity is 28.965003448125664
At time: 174.29685711860657 and batch: 450, loss is 3.370436234474182 and perplexity is 29.091214880989853
At time: 175.21937108039856 and batch: 500, loss is 3.2353540420532227 and perplexity is 25.415368295356487
At time: 176.13837385177612 and batch: 550, loss is 3.2886505222320555 and perplexity is 26.80666423881478
At time: 177.05614972114563 and batch: 600, loss is 3.3157329177856445 and perplexity is 27.54257303051079
At time: 177.9797339439392 and batch: 650, loss is 3.129477038383484 and perplexity is 22.862020456467164
At time: 178.90060663223267 and batch: 700, loss is 3.113521628379822 and perplexity is 22.500142181761802
At time: 179.8242814540863 and batch: 750, loss is 3.213393225669861 and perplexity is 24.863310070485998
At time: 180.74490594863892 and batch: 800, loss is 3.1434006261825562 and perplexity is 23.182568214382023
At time: 181.67170095443726 and batch: 850, loss is 3.1976330375671385 and perplexity is 24.474531287448198
At time: 182.5939803123474 and batch: 900, loss is 3.154976749420166 and perplexity is 23.452491801058958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370081235284674 and perplexity of 79.05005309230556
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.8131058216095 and batch: 50, loss is 3.566333169937134 and perplexity is 35.386598320262635
At time: 185.73184084892273 and batch: 100, loss is 3.4385419273376465 and perplexity is 31.14151845243635
At time: 186.65343499183655 and batch: 150, loss is 3.4499631452560426 and perplexity is 31.499231391247626
At time: 187.57741928100586 and batch: 200, loss is 3.337020435333252 and perplexity is 28.135171130195197
At time: 188.5073025226593 and batch: 250, loss is 3.4745782136917116 and perplexity is 32.28420862729404
At time: 189.43157839775085 and batch: 300, loss is 3.4450344848632812 and perplexity is 31.344364334150846
At time: 190.35217761993408 and batch: 350, loss is 3.4157971715927125 and perplexity is 30.441206622874745
At time: 191.2747712135315 and batch: 400, loss is 3.3567379570007323 and perplexity is 28.695432306068895
At time: 192.19624638557434 and batch: 450, loss is 3.3617510843276976 and perplexity is 28.839647344067938
At time: 193.12036418914795 and batch: 500, loss is 3.2259521198272707 and perplexity is 25.17753477738018
At time: 194.04199361801147 and batch: 550, loss is 3.280055522918701 and perplexity is 26.577248305305577
At time: 194.96521544456482 and batch: 600, loss is 3.3071376132965087 and perplexity is 27.306850732609238
At time: 195.8878390789032 and batch: 650, loss is 3.1209694004058837 and perplexity is 22.66834369705462
At time: 196.81313729286194 and batch: 700, loss is 3.1040597057342527 and perplexity is 22.28825160389448
At time: 197.73555970191956 and batch: 750, loss is 3.2037617492675783 and perplexity is 24.624989219409468
At time: 198.65426540374756 and batch: 800, loss is 3.132645082473755 and perplexity is 22.93456319388828
At time: 199.57559418678284 and batch: 850, loss is 3.1872465562820436 and perplexity is 24.22164261146371
At time: 200.49738836288452 and batch: 900, loss is 3.1445008945465087 and perplexity is 23.20808929822943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369838348806721 and perplexity of 79.0308552348723
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.73197269439697 and batch: 50, loss is 3.563298740386963 and perplexity is 35.27938293167909
At time: 203.67036390304565 and batch: 100, loss is 3.4355053663253785 and perplexity is 31.047098759770943
At time: 204.59762501716614 and batch: 150, loss is 3.447167353630066 and perplexity is 31.41128909534954
At time: 205.52373218536377 and batch: 200, loss is 3.334240050315857 and perplexity is 28.057053171248782
At time: 206.45078992843628 and batch: 250, loss is 3.471400442123413 and perplexity is 32.18177962120085
At time: 207.37683081626892 and batch: 300, loss is 3.4416013669967653 and perplexity is 31.236939942750702
At time: 208.3031439781189 and batch: 350, loss is 3.4125115871429443 and perplexity is 30.341353595366314
At time: 209.2277808189392 and batch: 400, loss is 3.353809576034546 and perplexity is 28.611524065896347
At time: 210.15366458892822 and batch: 450, loss is 3.3590993547439574 and perplexity is 28.763273703894416
At time: 211.07515621185303 and batch: 500, loss is 3.223337187767029 and perplexity is 25.111783239861392
At time: 212.01225686073303 and batch: 550, loss is 3.2772493886947633 and perplexity is 26.502773521248425
At time: 212.93823313713074 and batch: 600, loss is 3.3047547245025637 and perplexity is 27.241859008778558
At time: 213.86169123649597 and batch: 650, loss is 3.118563847541809 and perplexity is 22.613879332668223
At time: 214.78741598129272 and batch: 700, loss is 3.101573100090027 and perplexity is 22.232898361021103
At time: 215.7095308303833 and batch: 750, loss is 3.2012389087677002 and perplexity is 24.562942599070148
At time: 216.6329689025879 and batch: 800, loss is 3.1297404623031615 and perplexity is 22.868043652799717
At time: 217.5591537952423 and batch: 850, loss is 3.1846069955825804 and perplexity is 24.157792420848793
At time: 218.48387598991394 and batch: 900, loss is 3.1413900423049927 and perplexity is 23.136004542259826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369388946115154 and perplexity of 78.99534653526314
Annealing...
Model not improving. Stopping early with 78.10397919844367 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9e58990b70>
ELAPSED
687.9938435554504


RESULTS SO FAR:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}, {'best_accuracy': -79.27615153589069, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}}, {'best_accuracy': -78.10397919844367, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.45979727262087977, 'dropout': 0.13450648890583938}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9332850524599525, 'dropout': 0.403477051599481}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.447831392288208 and batch: 50, loss is 7.080528202056885 and perplexity is 1188.596171673338
At time: 2.596980571746826 and batch: 100, loss is 6.192907600402832 and perplexity is 489.2666319091539
At time: 3.746485710144043 and batch: 150, loss is 6.060314245223999 and perplexity is 428.51007294766447
At time: 4.894594669342041 and batch: 200, loss is 5.93474235534668 and perplexity is 377.9426087341039
At time: 6.0440897941589355 and batch: 250, loss is 6.001071853637695 and perplexity is 403.8614419390754
At time: 7.193339824676514 and batch: 300, loss is 5.927974195480346 and perplexity is 375.39326963684135
At time: 8.350942373275757 and batch: 350, loss is 5.93220775604248 and perplexity is 376.98588862422093
At time: 9.503469228744507 and batch: 400, loss is 5.811008729934692 and perplexity is 333.9558270652571
At time: 10.652487993240356 and batch: 450, loss is 5.828141078948975 and perplexity is 339.7265668636948
At time: 11.801932573318481 and batch: 500, loss is 5.783707609176636 and perplexity is 324.961790971464
At time: 12.956230163574219 and batch: 550, loss is 5.8286983680725095 and perplexity is 339.9159455488231
At time: 14.106417894363403 and batch: 600, loss is 5.766735687255859 and perplexity is 319.49310316796686
At time: 15.25440263748169 and batch: 650, loss is 5.678859672546387 and perplexity is 292.61556204062396
At time: 16.40557551383972 and batch: 700, loss is 5.769888429641724 and perplexity is 320.50197213262777
At time: 17.55599308013916 and batch: 750, loss is 5.728936033248901 and perplexity is 307.64177356818567
At time: 18.707358360290527 and batch: 800, loss is 5.733286142349243 and perplexity is 308.9829638941561
At time: 19.859063386917114 and batch: 850, loss is 5.776790513992309 and perplexity is 322.7217555326443
At time: 21.005382537841797 and batch: 900, loss is 5.650917081832886 and perplexity is 284.55230394914423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.636888895949272 and perplexity of 280.58841939460115
finished 1 epochs...
Completing Train Step...
At time: 23.380115032196045 and batch: 50, loss is 5.56399188041687 and perplexity is 260.86209092511865
At time: 24.3053138256073 and batch: 100, loss is 5.413284368515015 and perplexity is 224.36728371319595
At time: 25.230634927749634 and batch: 150, loss is 5.333808307647705 and perplexity is 207.225652380579
At time: 26.151952505111694 and batch: 200, loss is 5.155834550857544 and perplexity is 173.44049128792602
At time: 27.078887701034546 and batch: 250, loss is 5.1853573513031 and perplexity is 178.6372746480761
At time: 27.99712324142456 and batch: 300, loss is 5.102693424224854 and perplexity is 164.46428336419808
At time: 28.922467470169067 and batch: 350, loss is 5.056119155883789 and perplexity is 156.9801172931642
At time: 29.841747045516968 and batch: 400, loss is 4.89956521987915 and perplexity is 134.23140584910644
At time: 30.767911672592163 and batch: 450, loss is 4.902976093292236 and perplexity is 134.69003389898717
At time: 31.685609102249146 and batch: 500, loss is 4.806302928924561 and perplexity is 122.27870773979603
At time: 32.608410120010376 and batch: 550, loss is 4.863669519424438 and perplexity is 129.498528666246
At time: 33.5329065322876 and batch: 600, loss is 4.790000953674316 and perplexity is 120.3014833915957
At time: 34.4558527469635 and batch: 650, loss is 4.657638921737671 and perplexity is 105.38696131199823
At time: 35.377410650253296 and batch: 700, loss is 4.713687086105347 and perplexity is 111.46237456511635
At time: 36.30250382423401 and batch: 750, loss is 4.7309450435638425 and perplexity is 113.40268219915956
At time: 37.221731185913086 and batch: 800, loss is 4.666406574249268 and perplexity is 106.31502007127487
At time: 38.146408557891846 and batch: 850, loss is 4.717555446624756 and perplexity is 111.89438626405753
At time: 39.06872057914734 and batch: 900, loss is 4.646373205184936 and perplexity is 104.20636429955336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.761339579543022 and perplexity of 116.90242114857061
finished 2 epochs...
Completing Train Step...
At time: 41.289034366607666 and batch: 50, loss is 4.690147905349732 and perplexity is 108.86928096573918
At time: 42.219494581222534 and batch: 100, loss is 4.55959041595459 and perplexity is 95.54433837812742
At time: 43.145668029785156 and batch: 150, loss is 4.559164304733276 and perplexity is 95.50363453620926
At time: 44.06618404388428 and batch: 200, loss is 4.450817451477051 and perplexity is 85.69696849096208
At time: 44.993088722229004 and batch: 250, loss is 4.582113752365112 and perplexity is 97.72073348668569
At time: 45.91706824302673 and batch: 300, loss is 4.539967765808106 and perplexity is 93.68778012091806
At time: 46.835492849349976 and batch: 350, loss is 4.532403860092163 and perplexity is 92.98180790486403
At time: 47.75819969177246 and batch: 400, loss is 4.42688060760498 and perplexity is 83.67000981428014
At time: 48.676310539245605 and batch: 450, loss is 4.450398893356323 and perplexity is 85.66110683449573
At time: 49.60174798965454 and batch: 500, loss is 4.342140188217163 and perplexity is 76.87188371137009
At time: 50.52152442932129 and batch: 550, loss is 4.417830457687378 and perplexity is 82.9161998734141
At time: 51.44715642929077 and batch: 600, loss is 4.394727935791016 and perplexity is 81.0225844185132
At time: 52.36487698554993 and batch: 650, loss is 4.25189480304718 and perplexity is 70.2383742241306
At time: 53.285741567611694 and batch: 700, loss is 4.283630542755127 and perplexity is 72.50318868497116
At time: 54.19902014732361 and batch: 750, loss is 4.363766813278199 and perplexity is 78.55247032453335
At time: 55.11998391151428 and batch: 800, loss is 4.304107398986816 and perplexity is 74.0031306941386
At time: 56.03868007659912 and batch: 850, loss is 4.375941877365112 and perplexity is 79.51469738693973
At time: 56.96598434448242 and batch: 900, loss is 4.3183571434021 and perplexity is 75.0652055686238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5281739953446065 and perplexity of 92.58933806577595
finished 3 epochs...
Completing Train Step...
At time: 59.215261936187744 and batch: 50, loss is 4.39752423286438 and perplexity is 81.2494646986942
At time: 60.13896369934082 and batch: 100, loss is 4.254445281028747 and perplexity is 70.41774429352401
At time: 61.06730890274048 and batch: 150, loss is 4.262585625648499 and perplexity is 70.99330846645816
At time: 61.99400758743286 and batch: 200, loss is 4.155857162475586 and perplexity is 63.80663376642233
At time: 62.928600788116455 and batch: 250, loss is 4.307571115493775 and perplexity is 74.25990099253788
At time: 63.85344982147217 and batch: 300, loss is 4.2696889305114745 and perplexity is 71.49939088061775
At time: 64.77941060066223 and batch: 350, loss is 4.270711870193481 and perplexity is 71.57256786622897
At time: 65.70340728759766 and batch: 400, loss is 4.178677577972412 and perplexity is 65.27946910058387
At time: 66.63099718093872 and batch: 450, loss is 4.2079945039749145 and perplexity is 67.2215919012415
At time: 67.55373525619507 and batch: 500, loss is 4.08601173877716 and perplexity is 59.502107897759004
At time: 68.48083996772766 and batch: 550, loss is 4.166344375610351 and perplexity is 64.47930861123112
At time: 69.40938520431519 and batch: 600, loss is 4.164547972679138 and perplexity is 64.36358176937973
At time: 70.3325207233429 and batch: 650, loss is 4.014705066680908 and perplexity is 55.406951636304875
At time: 71.26280069351196 and batch: 700, loss is 4.03261402130127 and perplexity is 56.40817085357733
At time: 72.18720602989197 and batch: 750, loss is 4.139132804870606 and perplexity is 62.74838275622503
At time: 73.11713767051697 and batch: 800, loss is 4.0862557363510135 and perplexity is 59.51662803909279
At time: 74.04470729827881 and batch: 850, loss is 4.158492259979248 and perplexity is 63.974992190789436
At time: 74.97003650665283 and batch: 900, loss is 4.107580833435058 and perplexity is 60.79945550578469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429566683834547 and perplexity of 83.8950559489973
finished 4 epochs...
Completing Train Step...
At time: 77.19737195968628 and batch: 50, loss is 4.201493849754334 and perplexity is 66.78602484541109
At time: 78.13381147384644 and batch: 100, loss is 4.057974805831909 and perplexity is 57.85702065029719
At time: 79.0574312210083 and batch: 150, loss is 4.066064791679382 and perplexity is 58.32698155390565
At time: 79.98144483566284 and batch: 200, loss is 3.9599637174606324 and perplexity is 52.455422698632766
At time: 80.901850938797 and batch: 250, loss is 4.115684471130371 and perplexity is 61.29415398671386
At time: 81.82286739349365 and batch: 300, loss is 4.079118676185608 and perplexity is 59.09336650212329
At time: 82.74224925041199 and batch: 350, loss is 4.082410173416138 and perplexity is 59.288192613215514
At time: 83.6770887374878 and batch: 400, loss is 4.000579133033752 and perplexity is 54.62977878315315
At time: 84.61429810523987 and batch: 450, loss is 4.0306360721588135 and perplexity is 56.296708630054
At time: 85.56124067306519 and batch: 500, loss is 3.9077650451660157 and perplexity is 49.78755459990272
At time: 86.50051712989807 and batch: 550, loss is 3.987939739227295 and perplexity is 53.943636839156675
At time: 87.42560577392578 and batch: 600, loss is 4.00281750202179 and perplexity is 54.75219734364664
At time: 88.36007714271545 and batch: 650, loss is 3.8435477447509765 and perplexity is 46.690828090357414
At time: 89.28329110145569 and batch: 700, loss is 3.8566460180282593 and perplexity is 47.306420111353624
At time: 90.20157527923584 and batch: 750, loss is 3.9727124643325804 and perplexity is 53.1284445819671
At time: 91.12759113311768 and batch: 800, loss is 3.921112160682678 and perplexity is 50.45652935288573
At time: 92.04479575157166 and batch: 850, loss is 3.9975373458862307 and perplexity is 54.46385909829162
At time: 92.9625346660614 and batch: 900, loss is 3.9494563436508177 and perplexity is 51.9071395165802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3866815436376285 and perplexity of 80.37326079060992
finished 5 epochs...
Completing Train Step...
At time: 95.15833234786987 and batch: 50, loss is 4.048111481666565 and perplexity is 57.289163185179426
At time: 96.098562002182 and batch: 100, loss is 3.904567222595215 and perplexity is 49.62859712841238
At time: 97.01959657669067 and batch: 150, loss is 3.9123313140869143 and perplexity is 50.01541780953837
At time: 97.94540357589722 and batch: 200, loss is 3.8097956371307373 and perplexity is 45.141212735854886
At time: 98.87107872962952 and batch: 250, loss is 3.9639193534851076 and perplexity is 52.6633281864649
At time: 99.79813385009766 and batch: 300, loss is 3.9298152208328245 and perplexity is 50.89757198927791
At time: 100.72225141525269 and batch: 350, loss is 3.9348592567443847 and perplexity is 51.154949735811336
At time: 101.64804244041443 and batch: 400, loss is 3.8589017486572263 and perplexity is 47.41325109781053
At time: 102.57208585739136 and batch: 450, loss is 3.8883562803268434 and perplexity is 48.830556775201636
At time: 103.49307918548584 and batch: 500, loss is 3.7662225914001466 and perplexity is 43.216509707630536
At time: 104.41564321517944 and batch: 550, loss is 3.843661413192749 and perplexity is 46.6961356656777
At time: 105.33596158027649 and batch: 600, loss is 3.8620320320129395 and perplexity is 47.561900544626255
At time: 106.26136136054993 and batch: 650, loss is 3.70525634765625 and perplexity is 40.6604691969901
At time: 107.18451428413391 and batch: 700, loss is 3.7173522233963014 and perplexity is 41.15527973032073
At time: 108.1087121963501 and batch: 750, loss is 3.8378161859512328 and perplexity is 46.42398231538063
At time: 109.04272890090942 and batch: 800, loss is 3.785954647064209 and perplexity is 44.07772915473272
At time: 109.96422624588013 and batch: 850, loss is 3.8627752828598023 and perplexity is 47.59726410785564
At time: 110.88378024101257 and batch: 900, loss is 3.81622727394104 and perplexity is 45.43248028083457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370438667192851 and perplexity of 79.07831315384682
finished 6 epochs...
Completing Train Step...
At time: 113.11413407325745 and batch: 50, loss is 3.9181280088424684 and perplexity is 50.30618384644748
At time: 114.03842639923096 and batch: 100, loss is 3.776872658729553 and perplexity is 43.679228062975156
At time: 114.96675658226013 and batch: 150, loss is 3.786256766319275 and perplexity is 44.09104789725292
At time: 115.8874294757843 and batch: 200, loss is 3.6830236864089967 and perplexity is 39.76645375543898
At time: 116.80823683738708 and batch: 250, loss is 3.8372188711166384 and perplexity is 46.39626086210456
At time: 117.73198914527893 and batch: 300, loss is 3.8023729515075684 and perplexity is 44.807384189966136
At time: 118.64824652671814 and batch: 350, loss is 3.809833288192749 and perplexity is 45.142912382451435
At time: 119.57183647155762 and batch: 400, loss is 3.736849298477173 and perplexity is 41.96556069569277
At time: 120.49247694015503 and batch: 450, loss is 3.7649484062194825 and perplexity is 43.1614789385417
At time: 121.41519832611084 and batch: 500, loss is 3.646051454544067 and perplexity is 38.323046616338324
At time: 122.33787250518799 and batch: 550, loss is 3.72275671005249 and perplexity is 41.37830501619358
At time: 123.26001000404358 and batch: 600, loss is 3.7467839908599854 and perplexity is 42.384553467883904
At time: 124.18491053581238 and batch: 650, loss is 3.5872645568847656 and perplexity is 36.13509511285897
At time: 125.10572719573975 and batch: 700, loss is 3.5996921634674073 and perplexity is 36.586969903995204
At time: 126.03079319000244 and batch: 750, loss is 3.72132613658905 and perplexity is 41.31915263207894
At time: 126.9615159034729 and batch: 800, loss is 3.6705889558792113 and perplexity is 39.27503031000305
At time: 127.88870787620544 and batch: 850, loss is 3.7467638540267942 and perplexity is 42.38369998579408
At time: 128.81061959266663 and batch: 900, loss is 3.7030991888046265 and perplexity is 40.572852641340695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37482149307042 and perplexity of 79.4256602560055
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 131.04685831069946 and batch: 50, loss is 3.824093403816223 and perplexity is 45.79126735410945
At time: 131.9771444797516 and batch: 100, loss is 3.6823844146728515 and perplexity is 39.741040309420754
At time: 132.89567732810974 and batch: 150, loss is 3.6967356634140014 and perplexity is 40.315486011422394
At time: 133.82023525238037 and batch: 200, loss is 3.579690070152283 and perplexity is 35.86242428877655
At time: 134.74334716796875 and batch: 250, loss is 3.726924738883972 and perplexity is 41.55113090594369
At time: 135.6698009967804 and batch: 300, loss is 3.678592143058777 and perplexity is 39.59061689383999
At time: 136.5959827899933 and batch: 350, loss is 3.6711806440353394 and perplexity is 39.298275756618594
At time: 137.52101683616638 and batch: 400, loss is 3.5896574878692626 and perplexity is 36.22166744107528
At time: 138.44450497627258 and batch: 450, loss is 3.6020860147476195 and perplexity is 36.674658583692874
At time: 139.3655490875244 and batch: 500, loss is 3.4721275568008423 and perplexity is 32.20518797475662
At time: 140.29006552696228 and batch: 550, loss is 3.5328744745254514 and perplexity is 34.222197201957336
At time: 141.2189326286316 and batch: 600, loss is 3.5484400177001953 and perplexity is 34.75905166638903
At time: 142.145423412323 and batch: 650, loss is 3.371882743835449 and perplexity is 29.133326045399855
At time: 143.07432460784912 and batch: 700, loss is 3.367013792991638 and perplexity is 28.991822080441896
At time: 143.99706435203552 and batch: 750, loss is 3.4755290126800538 and perplexity is 32.314919017586746
At time: 144.92433071136475 and batch: 800, loss is 3.401141672134399 and perplexity is 29.998328760719723
At time: 145.8479130268097 and batch: 850, loss is 3.4598967123031614 and perplexity is 31.813690382154576
At time: 146.77190828323364 and batch: 900, loss is 3.4022275590896607 and perplexity is 30.03092124727601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348826369194136 and perplexity of 77.38758515328846
finished 8 epochs...
Completing Train Step...
At time: 149.0043020248413 and batch: 50, loss is 3.711647319793701 and perplexity is 40.921161273302104
At time: 149.93817520141602 and batch: 100, loss is 3.5651873970031738 and perplexity is 35.34607653249926
At time: 150.865971326828 and batch: 150, loss is 3.5787414836883547 and perplexity is 35.828421808228626
At time: 151.79031229019165 and batch: 200, loss is 3.4638387966156006 and perplexity is 31.939350149900772
At time: 152.7149019241333 and batch: 250, loss is 3.613720316886902 and perplexity is 37.10383438198036
At time: 153.6378526687622 and batch: 300, loss is 3.569231324195862 and perplexity is 35.48930289585079
At time: 154.5596239566803 and batch: 350, loss is 3.5651147079467775 and perplexity is 35.343507352925556
At time: 155.49311447143555 and batch: 400, loss is 3.489933977127075 and perplexity is 32.78378315522848
At time: 156.41697645187378 and batch: 450, loss is 3.5071771717071534 and perplexity is 33.35398220590477
At time: 157.3468053340912 and batch: 500, loss is 3.3819178915023804 and perplexity is 29.427155117544146
At time: 158.27220392227173 and batch: 550, loss is 3.446073670387268 and perplexity is 31.376953874181776
At time: 159.19491934776306 and batch: 600, loss is 3.4685080575942995 and perplexity is 32.088832024599455
At time: 160.12102937698364 and batch: 650, loss is 3.298614206314087 and perplexity is 27.075092418694638
At time: 161.04523539543152 and batch: 700, loss is 3.299255313873291 and perplexity is 27.09245603048707
At time: 161.9702386856079 and batch: 750, loss is 3.4127459001541136 and perplexity is 30.34846380226466
At time: 162.89543890953064 and batch: 800, loss is 3.3435647821426393 and perplexity is 28.31990125731299
At time: 163.820246219635 and batch: 850, loss is 3.408708152770996 and perplexity is 30.226171430964975
At time: 164.74556374549866 and batch: 900, loss is 3.3580103302001953 and perplexity is 28.73196684293601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365767805543665 and perplexity of 78.70981057590019
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 167.00600147247314 and batch: 50, loss is 3.661540541648865 and perplexity is 38.92125652644783
At time: 167.93034029006958 and batch: 100, loss is 3.526971321105957 and perplexity is 34.0207734241745
At time: 168.86509370803833 and batch: 150, loss is 3.543833770751953 and perplexity is 34.59931107547302
At time: 169.78756189346313 and batch: 200, loss is 3.427072687149048 and perplexity is 30.786389321022092
At time: 170.70939660072327 and batch: 250, loss is 3.5793901777267454 and perplexity is 35.85167103186167
At time: 171.62977409362793 and batch: 300, loss is 3.5319627380371093 and perplexity is 34.19100979556567
At time: 172.54951095581055 and batch: 350, loss is 3.5213938903808595 and perplexity is 33.83155308935926
At time: 173.4732894897461 and batch: 400, loss is 3.444734354019165 and perplexity is 31.33495833521076
At time: 174.40186071395874 and batch: 450, loss is 3.453541488647461 and perplexity is 31.612148365097358
At time: 175.3250970840454 and batch: 500, loss is 3.3237955570220947 and perplexity is 27.765536489634574
At time: 176.24742984771729 and batch: 550, loss is 3.3804289627075197 and perplexity is 29.383372781417737
At time: 177.17293238639832 and batch: 600, loss is 3.4042048597335817 and perplexity is 30.09036015212242
At time: 178.10397720336914 and batch: 650, loss is 3.2259256792068483 and perplexity is 25.176869076540772
At time: 179.03195714950562 and batch: 700, loss is 3.220063443183899 and perplexity is 25.02970809533975
At time: 179.95310139656067 and batch: 750, loss is 3.3282240533828737 and perplexity is 27.88876873230491
At time: 180.8776888847351 and batch: 800, loss is 3.252406153678894 and perplexity is 25.852470155940374
At time: 181.80326676368713 and batch: 850, loss is 3.3096890211105348 and perplexity is 27.376610600037417
At time: 182.72957825660706 and batch: 900, loss is 3.26076566696167 and perplexity is 26.069490050141468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358789313329409 and perplexity of 78.16244688359636
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.96374583244324 and batch: 50, loss is 3.6468375062942506 and perplexity is 38.35318235677753
At time: 185.896094083786 and batch: 100, loss is 3.5122740840911866 and perplexity is 33.52441851135147
At time: 186.81681728363037 and batch: 150, loss is 3.5231744337081907 and perplexity is 33.89184529598
At time: 187.74112343788147 and batch: 200, loss is 3.403295569419861 and perplexity is 30.06301171481863
At time: 188.66599249839783 and batch: 250, loss is 3.5587361097335815 and perplexity is 35.118782795842975
At time: 189.58767914772034 and batch: 300, loss is 3.514267535209656 and perplexity is 33.59131445567362
At time: 190.51113557815552 and batch: 350, loss is 3.5022789907455443 and perplexity is 33.19100783010472
At time: 191.43631958961487 and batch: 400, loss is 3.4307250690460207 and perplexity is 30.89903856585524
At time: 192.3585171699524 and batch: 450, loss is 3.43717339515686 and perplexity is 31.09892943114574
At time: 193.28406882286072 and batch: 500, loss is 3.30287832736969 and perplexity is 27.190790390128953
At time: 194.20736622810364 and batch: 550, loss is 3.357142653465271 and perplexity is 28.707047596246472
At time: 195.12867903709412 and batch: 600, loss is 3.383867931365967 and perplexity is 29.484595230147896
At time: 196.04840993881226 and batch: 650, loss is 3.203979802131653 and perplexity is 24.630359354301937
At time: 196.97244310379028 and batch: 700, loss is 3.1967737770080564 and perplexity is 24.453510320548126
At time: 197.89678144454956 and batch: 750, loss is 3.300414638519287 and perplexity is 27.12388319610768
At time: 198.8201286792755 and batch: 800, loss is 3.2267110919952393 and perplexity is 25.196651078987067
At time: 199.74070048332214 and batch: 850, loss is 3.2814288425445555 and perplexity is 26.61377243591675
At time: 200.6637828350067 and batch: 900, loss is 3.235110945701599 and perplexity is 25.409190662959084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353289460482663 and perplexity of 77.73374490711568
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.90245985984802 and batch: 50, loss is 3.6373786544799804 and perplexity is 37.99211561869013
At time: 203.83331632614136 and batch: 100, loss is 3.5026598310470582 and perplexity is 33.20365071084073
At time: 204.76795482635498 and batch: 150, loss is 3.5127932119369505 and perplexity is 33.54182648860567
At time: 205.69367718696594 and batch: 200, loss is 3.3945092725753785 and perplexity is 29.8000261961193
At time: 206.6159291267395 and batch: 250, loss is 3.549927625656128 and perplexity is 34.810797987765525
At time: 207.54098868370056 and batch: 300, loss is 3.5046173620223997 and perplexity is 33.268711544124145
At time: 208.47034883499146 and batch: 350, loss is 3.492123146057129 and perplexity is 32.85563100957274
At time: 209.39700746536255 and batch: 400, loss is 3.423478193283081 and perplexity is 30.675926481387336
At time: 210.32454991340637 and batch: 450, loss is 3.430390033721924 and perplexity is 30.88868803044936
At time: 211.24984455108643 and batch: 500, loss is 3.294786014556885 and perplexity is 26.971641913596702
At time: 212.17592859268188 and batch: 550, loss is 3.3481161642074584 and perplexity is 28.449089717982055
At time: 213.09758710861206 and batch: 600, loss is 3.377449827194214 and perplexity is 29.295965995041016
At time: 214.02653408050537 and batch: 650, loss is 3.198010425567627 and perplexity is 24.483769424945137
At time: 214.95081877708435 and batch: 700, loss is 3.189750175476074 and perplexity is 24.282360356156786
At time: 215.87716364860535 and batch: 750, loss is 3.290692367553711 and perplexity is 26.861455219097472
At time: 216.80249500274658 and batch: 800, loss is 3.216732816696167 and perplexity is 24.946482160764223
At time: 217.73131608963013 and batch: 850, loss is 3.2714449739456177 and perplexity is 26.34938602470295
At time: 218.6565818786621 and batch: 900, loss is 3.2256121969223024 and perplexity is 25.168977811055534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351459241893194 and perplexity of 77.59160527513308
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 220.89913773536682 and batch: 50, loss is 3.6337346410751343 and perplexity is 37.85392377947327
At time: 221.82059597969055 and batch: 100, loss is 3.4992671155929567 and perplexity is 33.091191051625756
At time: 222.74936842918396 and batch: 150, loss is 3.5100612831115723 and perplexity is 33.45031766068219
At time: 223.67038941383362 and batch: 200, loss is 3.392253999710083 and perplexity is 29.73289473399055
At time: 224.61011481285095 and batch: 250, loss is 3.5475608682632447 and perplexity is 34.728506694452385
At time: 225.53331923484802 and batch: 300, loss is 3.5007873296737673 and perplexity is 33.141535003309734
At time: 226.45844674110413 and batch: 350, loss is 3.4884862995147703 and perplexity is 32.73635714337863
At time: 227.38283252716064 and batch: 400, loss is 3.4199471426010133 and perplexity is 30.567799243986915
At time: 228.3140094280243 and batch: 450, loss is 3.427814955711365 and perplexity is 30.80924957313598
At time: 229.24213409423828 and batch: 500, loss is 3.29234263420105 and perplexity is 26.905820379848485
At time: 230.16704726219177 and batch: 550, loss is 3.345244026184082 and perplexity is 28.367497234199373
At time: 231.09123492240906 and batch: 600, loss is 3.37532009601593 and perplexity is 29.23363985538287
At time: 232.00765323638916 and batch: 650, loss is 3.1959106397628783 and perplexity is 24.432412691399687
At time: 232.92840600013733 and batch: 700, loss is 3.187787637710571 and perplexity is 24.234752038763194
At time: 233.85160160064697 and batch: 750, loss is 3.2879854440689087 and perplexity is 26.78884163917061
At time: 234.77768993377686 and batch: 800, loss is 3.2137453746795654 and perplexity is 24.872067202322448
At time: 235.70390605926514 and batch: 850, loss is 3.268696331977844 and perplexity is 26.27706044055353
At time: 236.63103008270264 and batch: 900, loss is 3.2228204441070556 and perplexity is 25.098810237228676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350718459037886 and perplexity of 77.53414802852855
Annealing...
Model not improving. Stopping early with 77.38758515328846 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9e58990b70>
ELAPSED
933.1548652648926


RESULTS SO FAR:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}, {'best_accuracy': -79.27615153589069, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}}, {'best_accuracy': -78.10397919844367, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.45979727262087977, 'dropout': 0.13450648890583938}}, {'best_accuracy': -77.38758515328846, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9332850524599525, 'dropout': 0.403477051599481}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.8608962530057973, 'dropout': 0.3355775493962274}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.418348789215088 and batch: 50, loss is 6.988054904937744 and perplexity is 1083.6116972290993
At time: 2.563580274581909 and batch: 100, loss is 6.117350997924805 and perplexity is 453.6613515125736
At time: 3.7114691734313965 and batch: 150, loss is 5.944519748687744 and perplexity is 381.65602647025224
At time: 4.861014127731323 and batch: 200, loss is 5.782844152450561 and perplexity is 324.681321631327
At time: 6.014425992965698 and batch: 250, loss is 5.8230228519439695 and perplexity is 337.99221137354044
At time: 7.163803339004517 and batch: 300, loss is 5.719142866134644 and perplexity is 304.64369059327464
At time: 8.31766676902771 and batch: 350, loss is 5.7008857250213625 and perplexity is 299.13223256884083
At time: 9.46423888206482 and batch: 400, loss is 5.568559789657593 and perplexity is 262.056410976908
At time: 10.610918283462524 and batch: 450, loss is 5.576304979324341 and perplexity is 264.0939680304986
At time: 11.757789611816406 and batch: 500, loss is 5.526352024078369 and perplexity is 251.22577180039772
At time: 12.909684419631958 and batch: 550, loss is 5.575907802581787 and perplexity is 263.98909687611837
At time: 14.059185028076172 and batch: 600, loss is 5.498000659942627 and perplexity is 244.20319861751088
At time: 15.207463264465332 and batch: 650, loss is 5.403200168609619 and perplexity is 222.11608899793956
At time: 16.366001844406128 and batch: 700, loss is 5.503298215866089 and perplexity is 245.50031144923406
At time: 17.51253318786621 and batch: 750, loss is 5.471593036651611 and perplexity is 237.8387770207714
At time: 18.660805463790894 and batch: 800, loss is 5.460298652648926 and perplexity is 235.16764731939458
At time: 19.807766914367676 and batch: 850, loss is 5.495068740844727 and perplexity is 243.48826317429666
At time: 20.960885047912598 and batch: 900, loss is 5.3920871448516845 and perplexity is 219.6613725479277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.361035072640197 and perplexity of 212.94524621496535
finished 1 epochs...
Completing Train Step...
At time: 23.349076986312866 and batch: 50, loss is 5.2798637104034425 and perplexity is 196.34311400439864
At time: 24.272828340530396 and batch: 100, loss is 5.1290866565704345 and perplexity is 168.86281784589966
At time: 25.19293713569641 and batch: 150, loss is 5.085784635543823 and perplexity is 161.70677035268804
At time: 26.113731622695923 and batch: 200, loss is 4.9453963279724125 and perplexity is 140.52653441269476
At time: 27.03335976600647 and batch: 250, loss is 5.01236704826355 and perplexity is 150.2599881779246
At time: 27.95332169532776 and batch: 300, loss is 4.947602691650391 and perplexity is 140.83692935015512
At time: 28.87337374687195 and batch: 350, loss is 4.914693231582642 and perplexity is 136.27749775725647
At time: 29.797506093978882 and batch: 400, loss is 4.772198352813721 and perplexity is 118.1787552043478
At time: 30.717241525650024 and batch: 450, loss is 4.780865993499756 and perplexity is 119.20753831148059
At time: 31.642564058303833 and batch: 500, loss is 4.695709896087647 and perplexity is 109.47649800048406
At time: 32.565739154815674 and batch: 550, loss is 4.753271074295044 and perplexity is 115.96298835571982
At time: 33.489590883255005 and batch: 600, loss is 4.691520700454712 and perplexity is 109.01883881438526
At time: 34.41049885749817 and batch: 650, loss is 4.556877069473266 and perplexity is 95.28544487650109
At time: 35.3302435874939 and batch: 700, loss is 4.61288950920105 and perplexity is 100.77491939996762
At time: 36.25283479690552 and batch: 750, loss is 4.639531869888305 and perplexity is 103.49588670016584
At time: 37.1705641746521 and batch: 800, loss is 4.573730840682983 and perplexity is 96.90497320870473
At time: 38.09579396247864 and batch: 850, loss is 4.633253135681152 and perplexity is 102.84809930659071
At time: 39.01400876045227 and batch: 900, loss is 4.559185333251953 and perplexity is 95.50564285728781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.678199663554152 and perplexity of 107.57622473000393
finished 2 epochs...
Completing Train Step...
At time: 41.252410888671875 and batch: 50, loss is 4.600510091781616 and perplexity is 99.53507471853575
At time: 42.18949317932129 and batch: 100, loss is 4.46619601726532 and perplexity is 87.02505078500411
At time: 43.110647201538086 and batch: 150, loss is 4.463995571136475 and perplexity is 86.83376738048746
At time: 44.036303997039795 and batch: 200, loss is 4.362280473709107 and perplexity is 78.43580140593397
At time: 44.95770716667175 and batch: 250, loss is 4.497791013717651 and perplexity is 89.81850415539078
At time: 45.881561517715454 and batch: 300, loss is 4.4604767894744874 and perplexity is 86.52875526242872
At time: 46.8037633895874 and batch: 350, loss is 4.44898027420044 and perplexity is 85.5396725023264
At time: 47.733094453811646 and batch: 400, loss is 4.351948866844177 and perplexity is 77.62960536319912
At time: 48.658955097198486 and batch: 450, loss is 4.371069984436035 and perplexity is 79.12825241860357
At time: 49.58523750305176 and batch: 500, loss is 4.263025741577149 and perplexity is 71.02456062912464
At time: 50.510631799697876 and batch: 550, loss is 4.339753580093384 and perplexity is 76.68863940237097
At time: 51.435747146606445 and batch: 600, loss is 4.328020415306091 and perplexity is 75.79409712731162
At time: 52.36069703102112 and batch: 650, loss is 4.179383816719056 and perplexity is 65.32558827467997
At time: 53.28603553771973 and batch: 700, loss is 4.214258608818054 and perplexity is 67.64399661310664
At time: 54.20717096328735 and batch: 750, loss is 4.29005072593689 and perplexity is 72.97016988598635
At time: 55.133830547332764 and batch: 800, loss is 4.225176339149475 and perplexity is 68.38656171349167
At time: 56.05968403816223 and batch: 850, loss is 4.304736804962158 and perplexity is 74.04972336810708
At time: 56.985004901885986 and batch: 900, loss is 4.2456271266937256 and perplexity is 69.79951956304627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.47614528708262 and perplexity of 87.89520803283631
finished 3 epochs...
Completing Train Step...
At time: 59.214301109313965 and batch: 50, loss is 4.3168254613876345 and perplexity is 74.95031755175783
At time: 60.14575505256653 and batch: 100, loss is 4.176882276535034 and perplexity is 65.16237791431362
At time: 61.0753378868103 and batch: 150, loss is 4.182704300880432 and perplexity is 65.542861382204
At time: 61.99658823013306 and batch: 200, loss is 4.081354660987854 and perplexity is 59.2256462041248
At time: 62.91926193237305 and batch: 250, loss is 4.233066148757935 and perplexity is 68.92825277488015
At time: 63.85432267189026 and batch: 300, loss is 4.196751356124878 and perplexity is 66.47004241261865
At time: 64.77551245689392 and batch: 350, loss is 4.189479908943176 and perplexity is 65.9884620251448
At time: 65.69606757164001 and batch: 400, loss is 4.109944953918457 and perplexity is 60.94336278397183
At time: 66.61652946472168 and batch: 450, loss is 4.131980447769165 and perplexity is 62.30118508036096
At time: 67.54031944274902 and batch: 500, loss is 4.0133543300628665 and perplexity is 55.3321619597841
At time: 68.4674973487854 and batch: 550, loss is 4.094730472564697 and perplexity is 60.02315908893433
At time: 69.3923089504242 and batch: 600, loss is 4.1008200407028195 and perplexity is 60.38978938317723
At time: 70.3194305896759 and batch: 650, loss is 3.947572774887085 and perplexity is 51.809460871073775
At time: 71.24407410621643 and batch: 700, loss is 3.9698686361312867 and perplexity is 52.977571043865964
At time: 72.17303371429443 and batch: 750, loss is 4.0665446519851685 and perplexity is 58.35497707354179
At time: 73.10079979896545 and batch: 800, loss is 4.006319379806518 and perplexity is 54.944268956563086
At time: 74.02443838119507 and batch: 850, loss is 4.089037775993347 and perplexity is 59.68243619316168
At time: 74.94891166687012 and batch: 900, loss is 4.035066237449646 and perplexity is 56.54666562119605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.392477793236301 and perplexity of 80.84047701406656
finished 4 epochs...
Completing Train Step...
At time: 77.20711588859558 and batch: 50, loss is 4.114027853012085 and perplexity is 61.19269704158428
At time: 78.12932753562927 and batch: 100, loss is 3.9795781135559083 and perplexity is 53.494460878252426
At time: 79.05117011070251 and batch: 150, loss is 3.9851265478134157 and perplexity is 53.79209631935347
At time: 79.97296690940857 and batch: 200, loss is 3.881129083633423 and perplexity is 48.478920937981194
At time: 80.90035939216614 and batch: 250, loss is 4.034972338676453 and perplexity is 56.54135620794364
At time: 81.82039022445679 and batch: 300, loss is 4.006800346374511 and perplexity is 54.970701669149996
At time: 82.7401750087738 and batch: 350, loss is 3.994853038787842 and perplexity is 54.31785741904839
At time: 83.66108274459839 and batch: 400, loss is 3.9295825386047363 and perplexity is 50.885730406539516
At time: 84.58544254302979 and batch: 450, loss is 3.9563954639434815 and perplexity is 52.268581997833316
At time: 85.51286363601685 and batch: 500, loss is 3.8291007804870607 and perplexity is 46.02113651842388
At time: 86.4468355178833 and batch: 550, loss is 3.914029431343079 and perplexity is 50.10042200671554
At time: 87.37096762657166 and batch: 600, loss is 3.9257837200164794 and perplexity is 50.69279145006673
At time: 88.29843664169312 and batch: 650, loss is 3.7695935440063475 and perplexity is 43.36243633134241
At time: 89.21933794021606 and batch: 700, loss is 3.7899221658706663 and perplexity is 44.25295575165296
At time: 90.1459846496582 and batch: 750, loss is 3.892914528846741 and perplexity is 49.05364665162437
At time: 91.06822800636292 and batch: 800, loss is 3.834204592704773 and perplexity is 46.256620178275774
At time: 91.99162364006042 and batch: 850, loss is 3.919283514022827 and perplexity is 50.36434649963813
At time: 92.91972351074219 and batch: 900, loss is 3.8716567373275756 and perplexity is 48.02187985337981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368564553456764 and perplexity of 78.93025018769475
finished 5 epochs...
Completing Train Step...
At time: 95.15241003036499 and batch: 50, loss is 3.9551748418807984 and perplexity is 52.20482073557991
At time: 96.08306121826172 and batch: 100, loss is 3.8213087558746337 and perplexity is 45.66393216984099
At time: 97.00674104690552 and batch: 150, loss is 3.8261693048477174 and perplexity is 45.886424227196635
At time: 97.93256521224976 and batch: 200, loss is 3.7247553396224977 and perplexity is 41.46108761847775
At time: 98.8622236251831 and batch: 250, loss is 3.8764319086074828 and perplexity is 48.25174093109196
At time: 99.78511214256287 and batch: 300, loss is 3.8506726360321046 and perplexity is 47.024683092304464
At time: 100.7078583240509 and batch: 350, loss is 3.8399442195892335 and perplexity is 46.52287930209345
At time: 101.6318006515503 and batch: 400, loss is 3.78106360912323 and perplexity is 43.8626696698602
At time: 102.551584482193 and batch: 450, loss is 3.803012557029724 and perplexity is 44.83605240752518
At time: 103.47715783119202 and batch: 500, loss is 3.6817424964904784 and perplexity is 39.71553799913375
At time: 104.4026608467102 and batch: 550, loss is 3.763009548187256 and perplexity is 43.077876031692455
At time: 105.32701802253723 and batch: 600, loss is 3.783054747581482 and perplexity is 43.950093325739594
At time: 106.25416779518127 and batch: 650, loss is 3.624051923751831 and perplexity is 37.489163720047834
At time: 107.17530870437622 and batch: 700, loss is 3.644520788192749 and perplexity is 38.264431689794826
At time: 108.09886932373047 and batch: 750, loss is 3.748650884628296 and perplexity is 42.46375483389072
At time: 109.02165746688843 and batch: 800, loss is 3.6937920522689818 and perplexity is 40.19698738997802
At time: 109.95122218132019 and batch: 850, loss is 3.7784048557281493 and perplexity is 43.746204542591784
At time: 110.87605619430542 and batch: 900, loss is 3.73446412563324 and perplexity is 41.865584857165814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36781248327804 and perplexity of 78.87091141658631
finished 6 epochs...
Completing Train Step...
At time: 113.11334657669067 and batch: 50, loss is 3.82147406578064 and perplexity is 45.671481494147706
At time: 114.04532933235168 and batch: 100, loss is 3.685910911560059 and perplexity is 39.881434368475304
At time: 114.9722056388855 and batch: 150, loss is 3.6887523221969603 and perplexity is 39.99491504655774
At time: 115.89778971672058 and batch: 200, loss is 3.5900483179092406 and perplexity is 36.23582672356559
At time: 116.82125687599182 and batch: 250, loss is 3.7423487424850466 and perplexity is 42.19698371269921
At time: 117.74667406082153 and batch: 300, loss is 3.720718431472778 and perplexity is 41.294050399774434
At time: 118.6719479560852 and batch: 350, loss is 3.7061004781723024 and perplexity is 40.69480643035062
At time: 119.59678030014038 and batch: 400, loss is 3.652195887565613 and perplexity is 38.55924491865831
At time: 120.52567481994629 and batch: 450, loss is 3.673593683242798 and perplexity is 39.393218541066425
At time: 121.45133543014526 and batch: 500, loss is 3.5553260612487794 and perplexity is 34.99923000004792
At time: 122.380850315094 and batch: 550, loss is 3.6335926151275633 and perplexity is 37.84854792184388
At time: 123.30666875839233 and batch: 600, loss is 3.658516626358032 and perplexity is 38.803739713691016
At time: 124.22966432571411 and batch: 650, loss is 3.4996163606643678 and perplexity is 33.10275000534398
At time: 125.15954732894897 and batch: 700, loss is 3.5190724182128905 and perplexity is 33.753105173021524
At time: 126.08809161186218 and batch: 750, loss is 3.62782235622406 and perplexity is 37.63078089151457
At time: 127.01007294654846 and batch: 800, loss is 3.5713676738739015 and perplexity is 35.56520150077959
At time: 127.93726778030396 and batch: 850, loss is 3.652360315322876 and perplexity is 38.565585650103884
At time: 128.86246132850647 and batch: 900, loss is 3.6083754110336304 and perplexity is 36.90604692893711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3834859769638275 and perplexity of 80.11683261184665
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 131.10968494415283 and batch: 50, loss is 3.7218338584899904 and perplexity is 41.34013659735714
At time: 132.03232145309448 and batch: 100, loss is 3.6074552059173586 and perplexity is 36.87210141654418
At time: 132.9688320159912 and batch: 150, loss is 3.601275963783264 and perplexity is 36.64496227052716
At time: 133.89240550994873 and batch: 200, loss is 3.485388689041138 and perplexity is 32.63510955437628
At time: 134.81785607337952 and batch: 250, loss is 3.637581262588501 and perplexity is 37.99981390921604
At time: 135.73971962928772 and batch: 300, loss is 3.6009089183807372 and perplexity is 36.63151437374531
At time: 136.665860414505 and batch: 350, loss is 3.5740812158584596 and perplexity is 35.661840225561335
At time: 137.58947491645813 and batch: 400, loss is 3.5081128025054933 and perplexity is 33.385203822579335
At time: 138.51453924179077 and batch: 450, loss is 3.5146560955047605 and perplexity is 33.604369242853195
At time: 139.43731927871704 and batch: 500, loss is 3.3798139762878416 and perplexity is 29.365307961571624
At time: 140.36763310432434 and batch: 550, loss is 3.439389624595642 and perplexity is 31.167928224402456
At time: 141.2925877571106 and batch: 600, loss is 3.464102854728699 and perplexity is 31.947785108045498
At time: 142.21443486213684 and batch: 650, loss is 3.2798470306396483 and perplexity is 26.571707731838917
At time: 143.1344268321991 and batch: 700, loss is 3.279722638130188 and perplexity is 26.56840261600359
At time: 144.06114864349365 and batch: 750, loss is 3.377747435569763 and perplexity is 29.30468601740243
At time: 144.9802303314209 and batch: 800, loss is 3.299168577194214 and perplexity is 27.090106222731684
At time: 145.90891909599304 and batch: 850, loss is 3.359604444503784 and perplexity is 28.777805408499997
At time: 146.83111572265625 and batch: 900, loss is 3.3031160640716553 and perplexity is 27.19725540741563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36078340713292 and perplexity of 78.31846564091322
finished 8 epochs...
Completing Train Step...
At time: 149.06827330589294 and batch: 50, loss is 3.609706606864929 and perplexity is 36.95520881955064
At time: 150.01593780517578 and batch: 100, loss is 3.4851451921463013 and perplexity is 32.62716397393847
At time: 150.9384241104126 and batch: 150, loss is 3.4772130727767943 and perplexity is 32.36938513246659
At time: 151.86375522613525 and batch: 200, loss is 3.3650370979309083 and perplexity is 28.9345706918441
At time: 152.78563714027405 and batch: 250, loss is 3.517258229255676 and perplexity is 33.6919261742567
At time: 153.70822072029114 and batch: 300, loss is 3.484876832962036 and perplexity is 32.61840934957401
At time: 154.6307771205902 and batch: 350, loss is 3.463895707130432 and perplexity is 31.941167886484855
At time: 155.55910062789917 and batch: 400, loss is 3.403312783241272 and perplexity is 30.06352921858746
At time: 156.4827697277069 and batch: 450, loss is 3.4149526023864745 and perplexity is 30.41550777091741
At time: 157.40379095077515 and batch: 500, loss is 3.285614290237427 and perplexity is 26.725396423585977
At time: 158.32525515556335 and batch: 550, loss is 3.3480499601364135 and perplexity is 28.44720633476971
At time: 159.25082778930664 and batch: 600, loss is 3.379654674530029 and perplexity is 29.3606303889773
At time: 160.1756489276886 and batch: 650, loss is 3.200590534210205 and perplexity is 24.547021773918424
At time: 161.1000165939331 and batch: 700, loss is 3.2067732572555543 and perplexity is 24.69925934755888
At time: 162.0267677307129 and batch: 750, loss is 3.3115476036071776 and perplexity is 27.427539602531898
At time: 162.9463245868683 and batch: 800, loss is 3.2394043684005736 and perplexity is 25.51851758431732
At time: 163.8678114414215 and batch: 850, loss is 3.3071296644210815 and perplexity is 27.30663367471714
At time: 164.79448556900024 and batch: 900, loss is 3.25811713218689 and perplexity is 26.000535453814194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.383319175406678 and perplexity of 80.10347011388681
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 167.0315957069397 and batch: 50, loss is 3.5570332384109498 and perplexity is 35.05903091705353
At time: 167.96836352348328 and batch: 100, loss is 3.453920269012451 and perplexity is 31.624124694249176
At time: 168.8911771774292 and batch: 150, loss is 3.450484480857849 and perplexity is 31.515657343346195
At time: 169.81107783317566 and batch: 200, loss is 3.334116826057434 and perplexity is 28.053596074681668
At time: 170.73084354400635 and batch: 250, loss is 3.4864591693878175 and perplexity is 32.6700635031857
At time: 171.65126085281372 and batch: 300, loss is 3.4515259790420534 and perplexity is 31.548497941986657
At time: 172.57535219192505 and batch: 350, loss is 3.426615524291992 and perplexity is 30.77231814397648
At time: 173.49654531478882 and batch: 400, loss is 3.3598730516433717 and perplexity is 28.78553637074373
At time: 174.42316484451294 and batch: 450, loss is 3.3661021423339843 and perplexity is 28.96540371076743
At time: 175.34693098068237 and batch: 500, loss is 3.2328634786605837 and perplexity is 25.35214846864157
At time: 176.26918935775757 and batch: 550, loss is 3.2876611375808715 and perplexity is 26.780155252621917
At time: 177.19210624694824 and batch: 600, loss is 3.3110115766525268 and perplexity is 27.41284164160873
At time: 178.10926055908203 and batch: 650, loss is 3.128405532836914 and perplexity is 22.83753679427471
At time: 179.0369508266449 and batch: 700, loss is 3.1278018808364867 and perplexity is 22.823755029617413
At time: 179.9542999267578 and batch: 750, loss is 3.2234978342056273 and perplexity is 25.11581768245647
At time: 180.87963914871216 and batch: 800, loss is 3.1448576736450193 and perplexity is 23.21637093667731
At time: 181.80031728744507 and batch: 850, loss is 3.2084782266616823 and perplexity is 24.741406748905483
At time: 182.72530961036682 and batch: 900, loss is 3.158123412132263 and perplexity is 23.526405111656683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378021658283391 and perplexity of 79.68024262653098
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.97980403900146 and batch: 50, loss is 3.539293189048767 and perplexity is 34.44256620241135
At time: 185.90697240829468 and batch: 100, loss is 3.432617354393005 and perplexity is 30.957563719450974
At time: 186.83191108703613 and batch: 150, loss is 3.4350068044662474 and perplexity is 31.031623718445875
At time: 187.75666403770447 and batch: 200, loss is 3.3192052459716797 and perplexity is 27.63837611678031
At time: 188.68622970581055 and batch: 250, loss is 3.4609926748275757 and perplexity is 31.848576107808842
At time: 189.6136929988861 and batch: 300, loss is 3.4270943784713745 and perplexity is 30.78705712575889
At time: 190.54201292991638 and batch: 350, loss is 3.4057470178604126 and perplexity is 30.13680004519573
At time: 191.47284054756165 and batch: 400, loss is 3.340400414466858 and perplexity is 28.230428314496447
At time: 192.39448618888855 and batch: 450, loss is 3.342601189613342 and perplexity is 28.29262555547275
At time: 193.31925058364868 and batch: 500, loss is 3.2117681884765625 and perplexity is 24.82293907793928
At time: 194.2451467514038 and batch: 550, loss is 3.2658539152145387 and perplexity is 26.202476133525145
At time: 195.1670515537262 and batch: 600, loss is 3.2886981534957886 and perplexity is 26.80794110451807
At time: 196.0971817970276 and batch: 650, loss is 3.1051543045043943 and perplexity is 22.312661653856807
At time: 197.02162790298462 and batch: 700, loss is 3.1021646451950073 and perplexity is 22.2460540139135
At time: 197.95237016677856 and batch: 750, loss is 3.1939675998687744 and perplexity is 24.38498563008854
At time: 198.88216137886047 and batch: 800, loss is 3.114546356201172 and perplexity is 22.52321052080558
At time: 199.8043897151947 and batch: 850, loss is 3.174432940483093 and perplexity is 23.91325578364459
At time: 200.73363161087036 and batch: 900, loss is 3.129270620346069 and perplexity is 22.857301810096818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376933894745291 and perplexity of 79.5936164868146
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.98136973381042 and batch: 50, loss is 3.5323378944396975 and perplexity is 34.20383917816348
At time: 203.91810965538025 and batch: 100, loss is 3.419255185127258 and perplexity is 30.546654943165947
At time: 204.84812688827515 and batch: 150, loss is 3.424872078895569 and perplexity is 30.71871502820149
At time: 205.77086544036865 and batch: 200, loss is 3.3106231021881105 and perplexity is 27.402194520835046
At time: 206.6957278251648 and batch: 250, loss is 3.4523449993133544 and perplexity is 31.574347385494388
At time: 207.6249554157257 and batch: 300, loss is 3.416135606765747 and perplexity is 30.451510741445265
At time: 208.55163192749023 and batch: 350, loss is 3.3950697422027587 and perplexity is 29.81673288705645
At time: 209.4719259738922 and batch: 400, loss is 3.330675220489502 and perplexity is 27.95721261429317
At time: 210.39532208442688 and batch: 450, loss is 3.3328783464431764 and perplexity is 28.018873773705703
At time: 211.32319498062134 and batch: 500, loss is 3.2025740146636963 and perplexity is 24.59575863012088
At time: 212.2426688671112 and batch: 550, loss is 3.2564918184280396 and perplexity is 25.958310749295467
At time: 213.16814613342285 and batch: 600, loss is 3.2801772689819337 and perplexity is 26.580484177631664
At time: 214.0960075855255 and batch: 650, loss is 3.0968779373168944 and perplexity is 22.12875595837792
At time: 215.01755499839783 and batch: 700, loss is 3.093923740386963 and perplexity is 22.06347972236415
At time: 215.94049549102783 and batch: 750, loss is 3.1850587701797486 and perplexity is 24.16870876346567
At time: 216.87339282035828 and batch: 800, loss is 3.10509850025177 and perplexity is 22.311416547190603
At time: 217.7959177494049 and batch: 850, loss is 3.1630613708496096 and perplexity is 23.642864828862933
At time: 218.71682953834534 and batch: 900, loss is 3.1184120082855227 and perplexity is 22.610445918718703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376473204730308 and perplexity of 79.55695694743295
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 220.96221780776978 and batch: 50, loss is 3.5300437068939208 and perplexity is 34.125459099980915
At time: 221.89397287368774 and batch: 100, loss is 3.4154291105270387 and perplexity is 30.430004461590904
At time: 222.8132300376892 and batch: 150, loss is 3.4210360193252565 and perplexity is 30.601101937068254
At time: 223.73378324508667 and batch: 200, loss is 3.307162570953369 and perplexity is 27.307532256124336
At time: 224.65898966789246 and batch: 250, loss is 3.449172887802124 and perplexity is 31.474348722000364
At time: 225.5893998146057 and batch: 300, loss is 3.4134805965423585 and perplexity is 30.37076890174366
At time: 226.51179790496826 and batch: 350, loss is 3.391842789649963 and perplexity is 29.72067078203771
At time: 227.4351954460144 and batch: 400, loss is 3.3275237131118773 and perplexity is 27.86924394224959
At time: 228.35700678825378 and batch: 450, loss is 3.3304468059539794 and perplexity is 27.950827509812463
At time: 229.28149676322937 and batch: 500, loss is 3.1998732614517214 and perplexity is 24.529421176867082
At time: 230.20959973335266 and batch: 550, loss is 3.2538171195983887 and perplexity is 25.88897285624233
At time: 231.13191413879395 and batch: 600, loss is 3.2775833654403685 and perplexity is 26.51162630952899
At time: 232.05831837654114 and batch: 650, loss is 3.0943894720077516 and perplexity is 22.073757775756935
At time: 232.9823100566864 and batch: 700, loss is 3.0913999891281128 and perplexity is 22.007867193253222
At time: 233.89960551261902 and batch: 750, loss is 3.182537989616394 and perplexity is 24.10786147572382
At time: 234.82564854621887 and batch: 800, loss is 3.10243932723999 and perplexity is 22.25216544483459
At time: 235.74260330200195 and batch: 850, loss is 3.160123195648193 and perplexity is 23.573499902786697
At time: 236.66672229766846 and batch: 900, loss is 3.1152513074874877 and perplexity is 22.53909388487537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376034253264127 and perplexity of 79.5220429678678
Annealing...
Model not improving. Stopping early with 78.31846564091322 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9e58990b70>
ELAPSED
1178.221022605896


RESULTS SO FAR:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}, {'best_accuracy': -79.27615153589069, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}}, {'best_accuracy': -78.10397919844367, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.45979727262087977, 'dropout': 0.13450648890583938}}, {'best_accuracy': -77.38758515328846, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9332850524599525, 'dropout': 0.403477051599481}}, {'best_accuracy': -78.31846564091322, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.8608962530057973, 'dropout': 0.3355775493962274}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9564027780737974, 'dropout': 0.40676761254723304}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3930540084838867 and batch: 50, loss is 7.0662269783020015 and perplexity is 1171.7187633180347
At time: 2.5354907512664795 and batch: 100, loss is 6.267900476455688 and perplexity is 527.3689911293816
At time: 3.6852867603302 and batch: 150, loss is 6.136117401123047 and perplexity is 462.2553301688302
At time: 4.835639238357544 and batch: 200, loss is 6.03548677444458 and perplexity is 418.00223292845044
At time: 5.981819152832031 and batch: 250, loss is 6.096726045608521 and perplexity is 444.4004389976664
At time: 7.132453918457031 and batch: 300, loss is 6.005999536514282 and perplexity is 405.8564544085085
At time: 8.278551816940308 and batch: 350, loss is 6.019821500778198 and perplexity is 411.50513582408087
At time: 9.42132019996643 and batch: 400, loss is 5.911781349182129 and perplexity is 369.3635351553444
At time: 10.564998388290405 and batch: 450, loss is 5.919975509643555 and perplexity is 372.40259349224624
At time: 11.709509372711182 and batch: 500, loss is 5.899497346878052 and perplexity is 364.8540267499652
At time: 12.862366914749146 and batch: 550, loss is 5.946517019271851 and perplexity is 382.4190585622941
At time: 14.02084231376648 and batch: 600, loss is 5.88833625793457 and perplexity is 360.8044991124449
At time: 15.173642635345459 and batch: 650, loss is 5.818791160583496 and perplexity is 336.5649546376114
At time: 16.325525045394897 and batch: 700, loss is 5.923066740036011 and perplexity is 373.55555682701504
At time: 17.4768226146698 and batch: 750, loss is 5.897588033676147 and perplexity is 364.1580707503367
At time: 18.628822326660156 and batch: 800, loss is 5.907680034637451 and perplexity is 367.85176136535546
At time: 19.77727222442627 and batch: 850, loss is 5.951914949417114 and perplexity is 384.4889113611556
At time: 20.924916744232178 and batch: 900, loss is 5.825596084594727 and perplexity is 338.8630639401467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.896942556720891 and perplexity of 363.92309095277744
finished 1 epochs...
Completing Train Step...
At time: 23.301157236099243 and batch: 50, loss is 5.830876140594483 and perplexity is 340.6570117987905
At time: 24.22227454185486 and batch: 100, loss is 5.599566278457641 and perplexity is 270.3091430994998
At time: 25.13758897781372 and batch: 150, loss is 5.4695205402374265 and perplexity is 237.34636744303538
At time: 26.066641092300415 and batch: 200, loss is 5.263325853347778 and perplexity is 193.1227222384276
At time: 26.98518466949463 and batch: 250, loss is 5.290660066604614 and perplexity is 198.47438849816444
At time: 27.912014722824097 and batch: 300, loss is 5.186500129699707 and perplexity is 178.84153415581972
At time: 28.835691452026367 and batch: 350, loss is 5.132489919662476 and perplexity is 169.438481453226
At time: 29.75939154624939 and batch: 400, loss is 4.962920618057251 and perplexity is 143.01086666788203
At time: 30.68434429168701 and batch: 450, loss is 4.956287641525268 and perplexity is 142.06541798128785
At time: 31.607789278030396 and batch: 500, loss is 4.865513639450073 and perplexity is 129.73755982958622
At time: 32.533716917037964 and batch: 550, loss is 4.9180668926239015 and perplexity is 136.73802824232106
At time: 33.459410667419434 and batch: 600, loss is 4.835751600265503 and perplexity is 125.93319907162216
At time: 34.38657259941101 and batch: 650, loss is 4.698075428009033 and perplexity is 109.73577469388937
At time: 35.31058597564697 and batch: 700, loss is 4.7620979690551755 and perplexity is 116.99111234570097
At time: 36.23851013183594 and batch: 750, loss is 4.773012170791626 and perplexity is 118.27497034543504
At time: 37.164597511291504 and batch: 800, loss is 4.705596389770508 and perplexity is 110.56420464923734
At time: 38.09867024421692 and batch: 850, loss is 4.753713026046753 and perplexity is 116.01424972824935
At time: 39.0224027633667 and batch: 900, loss is 4.678938417434693 and perplexity is 107.65572644596197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.778439038420377 and perplexity of 118.91857775984867
finished 2 epochs...
Completing Train Step...
At time: 41.25068950653076 and batch: 50, loss is 4.729914178848267 and perplexity is 113.28583961022711
At time: 42.18332099914551 and batch: 100, loss is 4.596136856079101 and perplexity is 99.10073480377125
At time: 43.10469031333923 and batch: 150, loss is 4.594356842041016 and perplexity is 98.924491009385
At time: 44.028635025024414 and batch: 200, loss is 4.481743183135986 and perplexity is 88.3886160057953
At time: 44.95282959938049 and batch: 250, loss is 4.615409669876098 and perplexity is 101.02920867920433
At time: 45.87571477890015 and batch: 300, loss is 4.5736578178405765 and perplexity is 96.89789719047631
At time: 46.80267930030823 and batch: 350, loss is 4.561938591003418 and perplexity is 95.7689568280728
At time: 47.728328704833984 and batch: 400, loss is 4.45587532043457 and perplexity is 86.13151052961784
At time: 48.650882720947266 and batch: 450, loss is 4.47404067993164 and perplexity is 87.71041767323845
At time: 49.57346057891846 and batch: 500, loss is 4.368366956710815 and perplexity is 78.91465536789431
At time: 50.501712799072266 and batch: 550, loss is 4.4457721996307376 and perplexity is 85.26569456008144
At time: 51.42794609069824 and batch: 600, loss is 4.422515163421631 and perplexity is 83.30554915175207
At time: 52.351574182510376 and batch: 650, loss is 4.276678347587586 and perplexity is 72.00088046253359
At time: 53.280428647994995 and batch: 700, loss is 4.317156472206116 and perplexity is 74.97513102425343
At time: 54.20553183555603 and batch: 750, loss is 4.388263378143311 and perplexity is 80.50049859590692
At time: 55.13351130485535 and batch: 800, loss is 4.332396421432495 and perplexity is 76.12649932727518
At time: 56.05508518218994 and batch: 850, loss is 4.402820501327515 and perplexity is 81.68092523308009
At time: 56.97937870025635 and batch: 900, loss is 4.339089260101319 and perplexity is 76.63771052446057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.540077941058433 and perplexity of 93.69810276418497
finished 3 epochs...
Completing Train Step...
At time: 59.20839524269104 and batch: 50, loss is 4.423060321807862 and perplexity is 83.35097625184896
At time: 60.14236879348755 and batch: 100, loss is 4.283670773506165 and perplexity is 72.5061056013791
At time: 61.066892862319946 and batch: 150, loss is 4.290594606399536 and perplexity is 73.0098677302057
At time: 62.00744915008545 and batch: 200, loss is 4.183987040519714 and perplexity is 65.62698975443429
At time: 62.92859506607056 and batch: 250, loss is 4.333521060943603 and perplexity is 76.21216235724606
At time: 63.85173273086548 and batch: 300, loss is 4.302065072059631 and perplexity is 73.85214633981306
At time: 64.7750198841095 and batch: 350, loss is 4.298666391372681 and perplexity is 73.60157252774475
At time: 65.69755268096924 and batch: 400, loss is 4.202305326461792 and perplexity is 66.8402421440192
At time: 66.6247148513794 and batch: 450, loss is 4.230811324119568 and perplexity is 68.77300674430673
At time: 67.54872679710388 and batch: 500, loss is 4.107749457359314 and perplexity is 60.809708612999984
At time: 68.47440648078918 and batch: 550, loss is 4.1944578266143795 and perplexity is 66.31776610064017
At time: 69.39115905761719 and batch: 600, loss is 4.194854822158813 and perplexity is 66.34409918501166
At time: 70.31823086738586 and batch: 650, loss is 4.04033438205719 and perplexity is 56.84534769214837
At time: 71.24116897583008 and batch: 700, loss is 4.063913774490357 and perplexity is 58.20165405312631
At time: 72.16907000541687 and batch: 750, loss is 4.159360809326172 and perplexity is 64.03058176612626
At time: 73.09151268005371 and batch: 800, loss is 4.1034360599517825 and perplexity is 60.547977055428376
At time: 74.01434326171875 and batch: 850, loss is 4.185990090370178 and perplexity is 65.75857562898568
At time: 74.93770122528076 and batch: 900, loss is 4.127242875099182 and perplexity is 62.00672674823967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431683684048587 and perplexity of 84.07284992894361
finished 4 epochs...
Completing Train Step...
At time: 77.18177437782288 and batch: 50, loss is 4.222021360397338 and perplexity is 68.1711435628853
At time: 78.10711693763733 and batch: 100, loss is 4.080732336044312 and perplexity is 59.188800073516816
At time: 79.02789974212646 and batch: 150, loss is 4.085098271369934 and perplexity is 59.44777947893081
At time: 79.95142221450806 and batch: 200, loss is 3.97963342666626 and perplexity is 53.49741990510592
At time: 80.87279510498047 and batch: 250, loss is 4.139850082397461 and perplexity is 62.793406906497815
At time: 81.79193711280823 and batch: 300, loss is 4.110704998970032 and perplexity is 60.98970009227246
At time: 82.71748995780945 and batch: 350, loss is 4.104850416183472 and perplexity is 60.63367405284834
At time: 83.64207625389099 and batch: 400, loss is 4.021855635643005 and perplexity is 55.80456274360698
At time: 84.5712673664093 and batch: 450, loss is 4.050994086265564 and perplexity is 57.45454343887209
At time: 85.48964858055115 and batch: 500, loss is 3.9258701610565185 and perplexity is 50.697173577077244
At time: 86.41174006462097 and batch: 550, loss is 4.010429716110229 and perplexity is 55.17057315450153
At time: 87.33874177932739 and batch: 600, loss is 4.022773532867432 and perplexity is 55.85580911271159
At time: 88.26193141937256 and batch: 650, loss is 3.8676400136947633 and perplexity is 47.82937610967136
At time: 89.18350577354431 and batch: 700, loss is 3.887289881706238 and perplexity is 48.77851169214771
At time: 90.09884810447693 and batch: 750, loss is 3.9897047472000122 and perplexity is 54.03893186175347
At time: 91.0239417552948 and batch: 800, loss is 3.938068470954895 and perplexity is 51.31938063330432
At time: 91.9444088935852 and batch: 850, loss is 4.018922290802002 and perplexity is 55.64110856831309
At time: 92.87060308456421 and batch: 900, loss is 3.967383346557617 and perplexity is 52.84606991579009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.394854401888913 and perplexity of 81.03283167655785
finished 5 epochs...
Completing Train Step...
At time: 95.10461521148682 and batch: 50, loss is 4.0604295873641965 and perplexity is 57.99922146065632
At time: 96.04426336288452 and batch: 100, loss is 3.926607060432434 and perplexity is 50.73454606083567
At time: 96.96524572372437 and batch: 150, loss is 3.928082504272461 and perplexity is 50.80945728435962
At time: 97.8908531665802 and batch: 200, loss is 3.820756211280823 and perplexity is 45.63870778043016
At time: 98.8140320777893 and batch: 250, loss is 3.981599850654602 and perplexity is 53.60272201528742
At time: 99.73966836929321 and batch: 300, loss is 3.9612517404556273 and perplexity is 52.52303001982622
At time: 100.66428685188293 and batch: 350, loss is 3.9493764638900757 and perplexity is 51.902993352294324
At time: 101.58765482902527 and batch: 400, loss is 3.8747108697891237 and perplexity is 48.16876923113438
At time: 102.51473951339722 and batch: 450, loss is 3.906872444152832 and perplexity is 49.74313400610572
At time: 103.44377613067627 and batch: 500, loss is 3.7832533359527587 and perplexity is 43.958822169885394
At time: 104.37215948104858 and batch: 550, loss is 3.8627704191207886 and perplexity is 47.59703260774823
At time: 105.2955892086029 and batch: 600, loss is 3.881921982765198 and perplexity is 48.51737507541376
At time: 106.22428631782532 and batch: 650, loss is 3.7292254114151 and perplexity is 41.64683650286036
At time: 107.15672063827515 and batch: 700, loss is 3.7448682022094726 and perplexity is 42.30343135261845
At time: 108.09827089309692 and batch: 750, loss is 3.8489042949676513 and perplexity is 46.94160089463042
At time: 109.0234682559967 and batch: 800, loss is 3.8031849384307863 and perplexity is 44.84378197525501
At time: 109.94788098335266 and batch: 850, loss is 3.880072913169861 and perplexity is 48.42774596308674
At time: 110.87034177780151 and batch: 900, loss is 3.8302564573287965 and perplexity is 46.074352824632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378106940282534 and perplexity of 79.68703820668063
finished 6 epochs...
Completing Train Step...
At time: 113.09378409385681 and batch: 50, loss is 3.927639980316162 and perplexity is 50.786977856514255
At time: 114.02395415306091 and batch: 100, loss is 3.7980627393722535 and perplexity is 44.61467047572542
At time: 114.95421719551086 and batch: 150, loss is 3.8002364683151244 and perplexity is 44.711756156964505
At time: 115.88130044937134 and batch: 200, loss is 3.6914293432235716 and perplexity is 40.102125713673416
At time: 116.8097894191742 and batch: 250, loss is 3.854906301498413 and perplexity is 47.22419189794851
At time: 117.73474931716919 and batch: 300, loss is 3.8318972539901734 and perplexity is 46.15001352385854
At time: 118.66029167175293 and batch: 350, loss is 3.824643039703369 and perplexity is 45.81644279599454
At time: 119.58533382415771 and batch: 400, loss is 3.749526138305664 and perplexity is 42.500937661293435
At time: 120.50634551048279 and batch: 450, loss is 3.7853092622756956 and perplexity is 44.049291236511046
At time: 121.42999982833862 and batch: 500, loss is 3.661799192428589 and perplexity is 38.93132484182891
At time: 122.35740160942078 and batch: 550, loss is 3.739647374153137 and perplexity is 42.08314794259316
At time: 123.2794234752655 and batch: 600, loss is 3.764395580291748 and perplexity is 43.137624748120096
At time: 124.20145416259766 and batch: 650, loss is 3.6095394229888917 and perplexity is 36.94903102092897
At time: 125.1226544380188 and batch: 700, loss is 3.6268950366973876 and perplexity is 37.59590130834868
At time: 126.0431661605835 and batch: 750, loss is 3.732126708030701 and perplexity is 41.76784177987655
At time: 126.96950054168701 and batch: 800, loss is 3.6867143011093138 and perplexity is 39.913487569935754
At time: 127.89344239234924 and batch: 850, loss is 3.7623554182052614 and perplexity is 43.049706715621035
At time: 128.82024431228638 and batch: 900, loss is 3.712774510383606 and perplexity is 40.96731322735495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380710131501498 and perplexity of 79.89474904302028
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 131.06151676177979 and batch: 50, loss is 3.8356857299804688 and perplexity is 46.32518334586685
At time: 131.9817967414856 and batch: 100, loss is 3.7095076560974123 and perplexity is 40.833697355169484
At time: 132.90554475784302 and batch: 150, loss is 3.7088428688049317 and perplexity is 40.80656065312951
At time: 133.82250952720642 and batch: 200, loss is 3.5813856506347657 and perplexity is 35.92328349671688
At time: 134.74841237068176 and batch: 250, loss is 3.740815215110779 and perplexity is 42.13232307516501
At time: 135.67239260673523 and batch: 300, loss is 3.70660804271698 and perplexity is 40.71546691406813
At time: 136.59669637680054 and batch: 350, loss is 3.6827750825881957 and perplexity is 39.756568891854016
At time: 137.52265405654907 and batch: 400, loss is 3.6001892375946043 and perplexity is 36.605160860879465
At time: 138.44848465919495 and batch: 450, loss is 3.6236752605438234 and perplexity is 37.47504559043243
At time: 139.37121772766113 and batch: 500, loss is 3.489397020339966 and perplexity is 32.766184405663026
At time: 140.29374647140503 and batch: 550, loss is 3.5477545833587647 and perplexity is 34.735234782088995
At time: 141.21439862251282 and batch: 600, loss is 3.5648603105545043 and perplexity is 35.334517200405344
At time: 142.13732767105103 and batch: 650, loss is 3.395626220703125 and perplexity is 29.83332987536455
At time: 143.05722832679749 and batch: 700, loss is 3.389810266494751 and perplexity is 29.66032417919366
At time: 143.98064923286438 and batch: 750, loss is 3.487027268409729 and perplexity is 32.68862860721706
At time: 144.90131616592407 and batch: 800, loss is 3.423291301727295 and perplexity is 30.67019394546002
At time: 145.82248449325562 and batch: 850, loss is 3.4767350006103515 and perplexity is 32.35391392886047
At time: 146.74775385856628 and batch: 900, loss is 3.4122301864624025 and perplexity is 30.33281671901376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357482074058219 and perplexity of 78.06033661931266
finished 8 epochs...
Completing Train Step...
At time: 148.99284386634827 and batch: 50, loss is 3.7285951519012452 and perplexity is 41.62059645782009
At time: 149.92420172691345 and batch: 100, loss is 3.591555938720703 and perplexity is 36.29049781133418
At time: 150.84683680534363 and batch: 150, loss is 3.5906062602996824 and perplexity is 36.25604986849766
At time: 151.77304697036743 and batch: 200, loss is 3.4703412675857543 and perplexity is 32.147711544853976
At time: 152.6973741054535 and batch: 250, loss is 3.6301873874664308 and perplexity is 37.71988418850607
At time: 153.62950134277344 and batch: 300, loss is 3.6012496328353882 and perplexity is 36.643997386638915
At time: 154.55126476287842 and batch: 350, loss is 3.5795470237731934 and perplexity is 35.85729466573242
At time: 155.47815322875977 and batch: 400, loss is 3.502831211090088 and perplexity is 33.209341641570354
At time: 156.39810729026794 and batch: 450, loss is 3.531793818473816 and perplexity is 34.18523475289501
At time: 157.31969618797302 and batch: 500, loss is 3.4019395780563353 and perplexity is 30.022274156702306
At time: 158.24595546722412 and batch: 550, loss is 3.4626412105560305 and perplexity is 31.90112292417467
At time: 159.16668033599854 and batch: 600, loss is 3.4857078075408934 and perplexity is 32.64552568347798
At time: 160.08862972259521 and batch: 650, loss is 3.322095971107483 and perplexity is 27.718386653853255
At time: 161.009192943573 and batch: 700, loss is 3.3205472469329833 and perplexity is 27.675491743130692
At time: 161.92962002754211 and batch: 750, loss is 3.42492084980011 and perplexity is 30.720213244254136
At time: 162.8499174118042 and batch: 800, loss is 3.366629195213318 and perplexity is 28.980674033974672
At time: 163.77655816078186 and batch: 850, loss is 3.4257771825790404 and perplexity is 30.746531236702467
At time: 164.69990944862366 and batch: 900, loss is 3.3705269145965575 and perplexity is 29.093852995525776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373227054125642 and perplexity of 79.29912179581062
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 166.9281506538391 and batch: 50, loss is 3.6864671421051027 and perplexity is 39.903623811101994
At time: 167.86306071281433 and batch: 100, loss is 3.5641795110702517 and perplexity is 35.31046966602231
At time: 168.7864372730255 and batch: 150, loss is 3.570416684150696 and perplexity is 35.531395436806115
At time: 169.710999250412 and batch: 200, loss is 3.4395450162887573 and perplexity is 31.172771837859038
At time: 170.62860679626465 and batch: 250, loss is 3.5973791742324828 and perplexity is 36.50244242972237
At time: 171.55302333831787 and batch: 300, loss is 3.571466021537781 and perplexity is 35.56869942726622
At time: 172.48098731040955 and batch: 350, loss is 3.541058034896851 and perplexity is 34.50340569274068
At time: 173.40509676933289 and batch: 400, loss is 3.4626581859588623 and perplexity is 31.9016644631835
At time: 174.32973790168762 and batch: 450, loss is 3.4813025426864623 and perplexity is 32.50202979728321
At time: 175.255117893219 and batch: 500, loss is 3.3488709783554076 and perplexity is 28.47057159979169
At time: 176.17626476287842 and batch: 550, loss is 3.4025963306427003 and perplexity is 30.041997838984013
At time: 177.1080710887909 and batch: 600, loss is 3.4283890199661253 and perplexity is 30.826941139594012
At time: 178.0334758758545 and batch: 650, loss is 3.258404908180237 and perplexity is 26.008018860452715
At time: 178.9576597213745 and batch: 700, loss is 3.245757884979248 and perplexity is 25.681166056597153
At time: 179.88810181617737 and batch: 750, loss is 3.344784069061279 and perplexity is 28.354452402052118
At time: 180.812255859375 and batch: 800, loss is 3.277258429527283 and perplexity is 26.50301312946826
At time: 181.73935532569885 and batch: 850, loss is 3.3376260471343993 and perplexity is 28.152215282407866
At time: 182.66227793693542 and batch: 900, loss is 3.2788640689849853 and perplexity is 26.54560159480744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368478435359589 and perplexity of 78.92345315741689
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 184.89631271362305 and batch: 50, loss is 3.6709806060791017 and perplexity is 39.29041539606402
At time: 185.8183012008667 and batch: 100, loss is 3.550347909927368 and perplexity is 34.82543149352984
At time: 186.74499654769897 and batch: 150, loss is 3.5628081655502317 and perplexity is 35.26207999869618
At time: 187.66916275024414 and batch: 200, loss is 3.434959444999695 and perplexity is 31.030154112100472
At time: 188.59327721595764 and batch: 250, loss is 3.58485249042511 and perplexity is 36.048039895613336
At time: 189.51693201065063 and batch: 300, loss is 3.556702370643616 and perplexity is 35.04743293257454
At time: 190.44294929504395 and batch: 350, loss is 3.525443367958069 and perplexity is 33.96883096925178
At time: 191.3736126422882 and batch: 400, loss is 3.4472796058654787 and perplexity is 31.41481528067508
At time: 192.3023326396942 and batch: 450, loss is 3.4672842168807985 and perplexity is 32.049584426821795
At time: 193.22702813148499 and batch: 500, loss is 3.3312727069854735 and perplexity is 27.973921662513465
At time: 194.1510922908783 and batch: 550, loss is 3.3871651697158813 and perplexity is 29.58197341959995
At time: 195.07492780685425 and batch: 600, loss is 3.416032843589783 and perplexity is 30.44838160827115
At time: 195.99796223640442 and batch: 650, loss is 3.241436023712158 and perplexity is 25.570415117209322
At time: 196.9239103794098 and batch: 700, loss is 3.224362025260925 and perplexity is 25.13753192872144
At time: 197.84532475471497 and batch: 750, loss is 3.3190169620513914 and perplexity is 27.63317274484565
At time: 198.77445340156555 and batch: 800, loss is 3.2511166095733643 and perplexity is 25.819153741547183
At time: 199.70768547058105 and batch: 850, loss is 3.3153602647781373 and perplexity is 27.532311120021145
At time: 200.63401293754578 and batch: 900, loss is 3.266307349205017 and perplexity is 26.21435992089174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3613172557255995 and perplexity of 78.36028700572157
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 202.86077976226807 and batch: 50, loss is 3.656709246635437 and perplexity is 38.7336699617703
At time: 203.79301476478577 and batch: 100, loss is 3.5378029012680052 and perplexity is 34.39127509560963
At time: 204.71314191818237 and batch: 150, loss is 3.5529313707351684 and perplexity is 34.91551794821551
At time: 205.6383821964264 and batch: 200, loss is 3.4267351961135866 and perplexity is 30.77600094370274
At time: 206.55911564826965 and batch: 250, loss is 3.577532105445862 and perplexity is 35.78511788512012
At time: 207.48051476478577 and batch: 300, loss is 3.5478119325637816 and perplexity is 34.73722687731182
At time: 208.4032461643219 and batch: 350, loss is 3.5165721940994263 and perplexity is 33.66882025506205
At time: 209.32278847694397 and batch: 400, loss is 3.4404382944107055 and perplexity is 31.2006302337376
At time: 210.24750971794128 and batch: 450, loss is 3.46183650970459 and perplexity is 31.875462389307497
At time: 211.16835165023804 and batch: 500, loss is 3.3268843841552735 and perplexity is 27.85143202204354
At time: 212.08897757530212 and batch: 550, loss is 3.381816020011902 and perplexity is 29.424157482081174
At time: 213.01175212860107 and batch: 600, loss is 3.408875598907471 and perplexity is 30.231233110359014
At time: 213.93373727798462 and batch: 650, loss is 3.235133714675903 and perplexity is 25.409769210754824
At time: 214.85813403129578 and batch: 700, loss is 3.2201974391937256 and perplexity is 25.03306220106502
At time: 215.78337144851685 and batch: 750, loss is 3.3128748321533203 and perplexity is 27.463966384027707
At time: 216.7099871635437 and batch: 800, loss is 3.2416726112365724 and perplexity is 25.576465474111405
At time: 217.64047741889954 and batch: 850, loss is 3.3042630481719972 and perplexity is 27.228468123763683
At time: 218.56645846366882 and batch: 900, loss is 3.2591650676727295 and perplexity is 26.027796619043514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361195603462114 and perplexity of 78.35075487925413
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 220.79907751083374 and batch: 50, loss is 3.6535921621322633 and perplexity is 38.61312181636535
At time: 221.72848868370056 and batch: 100, loss is 3.534187912940979 and perplexity is 34.26717548206054
At time: 222.6524875164032 and batch: 150, loss is 3.549562249183655 and perplexity is 34.79808126452999
At time: 223.58619236946106 and batch: 200, loss is 3.423169484138489 and perplexity is 30.66645800394169
At time: 224.51189184188843 and batch: 250, loss is 3.5740125799179077 and perplexity is 35.65939262561326
At time: 225.43504095077515 and batch: 300, loss is 3.5450886583328245 and perplexity is 34.64275657515604
At time: 226.3567123413086 and batch: 350, loss is 3.512690567970276 and perplexity is 33.53838379917404
At time: 227.28103733062744 and batch: 400, loss is 3.437516508102417 and perplexity is 31.109601707219866
At time: 228.20340418815613 and batch: 450, loss is 3.460122618675232 and perplexity is 31.82087810937711
At time: 229.12497544288635 and batch: 500, loss is 3.3245635509490965 and perplexity is 27.786868443395537
At time: 230.05087113380432 and batch: 550, loss is 3.378782196044922 and perplexity is 29.335025042335026
At time: 230.97404217720032 and batch: 600, loss is 3.405786008834839 and perplexity is 30.137975131304316
At time: 231.90515565872192 and batch: 650, loss is 3.232529683113098 and perplexity is 25.34368744656785
At time: 232.82871079444885 and batch: 700, loss is 3.218437366485596 and perplexity is 24.989040943155615
At time: 233.745543718338 and batch: 750, loss is 3.3118250703811647 and perplexity is 27.435150889354585
At time: 234.66461777687073 and batch: 800, loss is 3.239469499588013 and perplexity is 25.52017968979614
At time: 235.5843985080719 and batch: 850, loss is 3.3010300254821776 and perplexity is 27.14058001718561
At time: 236.51031804084778 and batch: 900, loss is 3.2557988452911375 and perplexity is 25.940328568570546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360525470890411 and perplexity of 78.2982670752498
Annealing...
Model not improving. Stopping early with 78.06033661931266 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9e58990b70>
ELAPSED
1423.4044253826141


RESULTS SO FAR:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}, {'best_accuracy': -79.27615153589069, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}}, {'best_accuracy': -78.10397919844367, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.45979727262087977, 'dropout': 0.13450648890583938}}, {'best_accuracy': -77.38758515328846, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9332850524599525, 'dropout': 0.403477051599481}}, {'best_accuracy': -78.31846564091322, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.8608962530057973, 'dropout': 0.3355775493962274}}, {'best_accuracy': -78.06033661931266, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9564027780737974, 'dropout': 0.40676761254723304}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -78.73929858294282, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.10080085173586983, 'dropout': 0.970719280841491}}, {'best_accuracy': -79.27615153589069, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.6046664374636176, 'dropout': 0.9602320762484412}}, {'best_accuracy': -78.10397919844367, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.45979727262087977, 'dropout': 0.13450648890583938}}, {'best_accuracy': -77.38758515328846, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9332850524599525, 'dropout': 0.403477051599481}}, {'best_accuracy': -78.31846564091322, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.8608962530057973, 'dropout': 0.3355775493962274}}, {'best_accuracy': -78.06033661931266, 'params': {'batch_size': 32, 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35, 'data': 'ptb', 'wordvec_source': 'None', 'tune_wordvecs': True, 'tie_weights': True, 'rnn_dropout': 0.9564027780737974, 'dropout': 0.40676761254723304}}]
