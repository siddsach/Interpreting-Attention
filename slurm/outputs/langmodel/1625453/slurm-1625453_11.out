Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1928753852844238 and batch: 50, loss is 7.139270181655884 and perplexity is 1260.5081113787783
At time: 1.9755067825317383 and batch: 100, loss is 6.339850559234619 and perplexity is 566.7116152356938
At time: 2.780191659927368 and batch: 150, loss is 6.179051170349121 and perplexity is 482.5338966081609
At time: 3.5642547607421875 and batch: 200, loss is 6.043086824417114 and perplexity is 421.1911734915358
At time: 4.3475635051727295 and batch: 250, loss is 6.08925500869751 and perplexity is 441.09267850061906
At time: 5.128493785858154 and batch: 300, loss is 5.986879911422729 and perplexity is 398.1703430858669
At time: 5.916317462921143 and batch: 350, loss is 5.977913875579834 and perplexity is 394.6162902100182
At time: 6.702534914016724 and batch: 400, loss is 5.843643722534179 and perplexity is 345.0342620996521
At time: 7.486084699630737 and batch: 450, loss is 5.842136335372925 and perplexity is 344.5145536820758
At time: 8.267910480499268 and batch: 500, loss is 5.7993987274169925 and perplexity is 330.10101953451914
At time: 9.050191879272461 and batch: 550, loss is 5.838708295822143 and perplexity is 343.3355661287831
At time: 9.831687927246094 and batch: 600, loss is 5.761540441513062 and perplexity is 317.8375621786844
At time: 10.611323833465576 and batch: 650, loss is 5.672446422576904 and perplexity is 290.74495005889725
At time: 11.400429010391235 and batch: 700, loss is 5.775450382232666 and perplexity is 322.2895555256894
At time: 12.184807062149048 and batch: 750, loss is 5.724537267684936 and perplexity is 306.2915014709944
At time: 12.968940734863281 and batch: 800, loss is 5.721470832824707 and perplexity is 305.35371709567556
At time: 13.752628803253174 and batch: 850, loss is 5.753055229187011 and perplexity is 315.15205265045495
At time: 14.535706043243408 and batch: 900, loss is 5.639473009109497 and perplexity is 281.3144292635912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.560787253183861 and perplexity of 260.0274632141032
finished 1 epochs...
Completing Train Step...
At time: 16.361995220184326 and batch: 50, loss is 5.365980377197266 and perplexity is 214.00093350757473
At time: 17.088268041610718 and batch: 100, loss is 5.159813470840454 and perplexity is 174.1319718852587
At time: 17.801132440567017 and batch: 150, loss is 5.092413759231567 and perplexity is 162.78230552361921
At time: 18.51628088951111 and batch: 200, loss is 4.934111661911011 and perplexity is 138.94965342057338
At time: 19.230028867721558 and batch: 250, loss is 4.989696998596191 and perplexity is 146.89190829752926
At time: 19.944521188735962 and batch: 300, loss is 4.903486366271973 and perplexity is 134.75878012210404
At time: 20.659260511398315 and batch: 350, loss is 4.874614934921265 and perplexity is 130.92372934705782
At time: 21.378607034683228 and batch: 400, loss is 4.72545615196228 and perplexity is 112.78193234207222
At time: 22.09319758415222 and batch: 450, loss is 4.7366392993927 and perplexity is 114.05026809335078
At time: 22.805748462677002 and batch: 500, loss is 4.635638475418091 and perplexity is 103.09371979242597
At time: 23.51899814605713 and batch: 550, loss is 4.6986909675598145 and perplexity is 109.80334219645083
At time: 24.232697248458862 and batch: 600, loss is 4.632630033493042 and perplexity is 102.78403439243667
At time: 24.94701838493347 and batch: 650, loss is 4.493612623214721 and perplexity is 89.44399034799311
At time: 25.662877559661865 and batch: 700, loss is 4.538622760772705 and perplexity is 93.56185428932247
At time: 26.378564596176147 and batch: 750, loss is 4.571105804443359 and perplexity is 96.65092772741124
At time: 27.115694522857666 and batch: 800, loss is 4.51550332069397 and perplexity is 91.42356981869209
At time: 27.83135724067688 and batch: 850, loss is 4.571993856430054 and perplexity is 96.73679689829606
At time: 28.545992612838745 and batch: 900, loss is 4.498135166168213 and perplexity is 89.84942073340453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.58740694228917 and perplexity of 98.23935927003622
finished 2 epochs...
Completing Train Step...
At time: 30.3177592754364 and batch: 50, loss is 4.5402442073822025 and perplexity is 93.71368289846566
At time: 31.04536008834839 and batch: 100, loss is 4.404632353782654 and perplexity is 81.82905317046225
At time: 31.76163363456726 and batch: 150, loss is 4.398686943054199 and perplexity is 81.34398922088032
At time: 32.47592067718506 and batch: 200, loss is 4.281863923072815 and perplexity is 72.3752161974505
At time: 33.190624952316284 and batch: 250, loss is 4.416974115371704 and perplexity is 82.8452256162747
At time: 33.91560912132263 and batch: 300, loss is 4.370892210006714 and perplexity is 79.11418668898722
At time: 34.631009101867676 and batch: 350, loss is 4.368257780075073 and perplexity is 78.90604020160646
At time: 35.34843444824219 and batch: 400, loss is 4.264089498519898 and perplexity is 71.10015369779707
At time: 36.06888794898987 and batch: 450, loss is 4.305280933380127 and perplexity is 74.09002689108605
At time: 36.792967081069946 and batch: 500, loss is 4.177080607414245 and perplexity is 65.17530290768715
At time: 37.51503920555115 and batch: 550, loss is 4.251886744499206 and perplexity is 70.23780820710294
At time: 38.25263810157776 and batch: 600, loss is 4.243060379028321 and perplexity is 69.62059153893988
At time: 38.98674464225769 and batch: 650, loss is 4.088593559265137 and perplexity is 59.65593014427535
At time: 39.715009689331055 and batch: 700, loss is 4.11098433971405 and perplexity is 61.00673938024654
At time: 40.43885326385498 and batch: 750, loss is 4.195573329925537 and perplexity is 66.39178506483839
At time: 41.16284918785095 and batch: 800, loss is 4.156563835144043 and perplexity is 63.851740106403
At time: 41.88768815994263 and batch: 850, loss is 4.231737494468689 and perplexity is 68.8367317695324
At time: 42.613948583602905 and batch: 900, loss is 4.161357550621033 and perplexity is 64.15856200206251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.408800517043022 and perplexity of 82.17084184417327
finished 3 epochs...
Completing Train Step...
At time: 44.40614461898804 and batch: 50, loss is 4.242109770774841 and perplexity is 69.55444107658005
At time: 45.14049220085144 and batch: 100, loss is 4.110507493019104 and perplexity is 60.97765545304202
At time: 45.86280918121338 and batch: 150, loss is 4.1096059656143185 and perplexity is 60.92270719797204
At time: 46.58908128738403 and batch: 200, loss is 3.990246319770813 and perplexity is 54.06820579126829
At time: 47.316285371780396 and batch: 250, loss is 4.140068173408508 and perplexity is 62.807103077548746
At time: 48.055912256240845 and batch: 300, loss is 4.101889152526855 and perplexity is 60.454387346127845
At time: 48.78855609893799 and batch: 350, loss is 4.105424671173096 and perplexity is 60.668503242173195
At time: 49.51669907569885 and batch: 400, loss is 4.017473735809326 and perplexity is 55.56056771071246
At time: 50.24406862258911 and batch: 450, loss is 4.062671394348144 and perplexity is 58.12939037266257
At time: 50.97351861000061 and batch: 500, loss is 3.932061176300049 and perplexity is 51.012014137232754
At time: 51.70238995552063 and batch: 550, loss is 4.005335507392883 and perplexity is 54.8902373904916
At time: 52.432469844818115 and batch: 600, loss is 4.014448018074035 and perplexity is 55.39271118689795
At time: 53.16110825538635 and batch: 650, loss is 3.8580937147140504 and perplexity is 47.37495505590174
At time: 53.89044976234436 and batch: 700, loss is 3.875354948043823 and perplexity is 48.19980368118615
At time: 54.620458126068115 and batch: 750, loss is 3.9722505950927736 and perplexity is 53.10391185354833
At time: 55.34989356994629 and batch: 800, loss is 3.942719006538391 and perplexity is 51.55859905481237
At time: 56.08230543136597 and batch: 850, loss is 4.012485728263855 and perplexity is 55.28412121152328
At time: 56.81691122055054 and batch: 900, loss is 3.951844096183777 and perplexity is 52.03122900898289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346835201733733 and perplexity of 77.2336468212515
finished 4 epochs...
Completing Train Step...
At time: 58.655303955078125 and batch: 50, loss is 4.043196549415589 and perplexity is 57.00828165169035
At time: 59.387330532073975 and batch: 100, loss is 3.9164653491973875 and perplexity is 50.22261128028716
At time: 60.11720156669617 and batch: 150, loss is 3.9147057819366453 and perplexity is 50.1343189186838
At time: 60.85478401184082 and batch: 200, loss is 3.799908833503723 and perplexity is 44.69710942868865
At time: 61.58765435218811 and batch: 250, loss is 3.950004243850708 and perplexity is 51.93558724124243
At time: 62.31355595588684 and batch: 300, loss is 3.915232515335083 and perplexity is 50.16073329492257
At time: 63.05946183204651 and batch: 350, loss is 3.9201712083816527 and perplexity is 50.4090744953694
At time: 63.78782510757446 and batch: 400, loss is 3.841161618232727 and perplexity is 46.579550681077976
At time: 64.51605319976807 and batch: 450, loss is 3.887122197151184 and perplexity is 48.770332974859876
At time: 65.24411916732788 and batch: 500, loss is 3.7614280223846435 and perplexity is 43.00980110454272
At time: 65.97028160095215 and batch: 550, loss is 3.83079638004303 and perplexity is 46.09923613119234
At time: 66.70728254318237 and batch: 600, loss is 3.8478182649612425 and perplexity is 46.89064858038909
At time: 67.43928933143616 and batch: 650, loss is 3.692580051422119 and perplexity is 40.148298118899085
At time: 68.17416858673096 and batch: 700, loss is 3.707071213722229 and perplexity is 40.73432950577347
At time: 68.9057023525238 and batch: 750, loss is 3.809366588592529 and perplexity is 45.12184911878047
At time: 69.64130783081055 and batch: 800, loss is 3.780964455604553 and perplexity is 43.85832074743291
At time: 70.36927342414856 and batch: 850, loss is 3.849298286437988 and perplexity is 46.960099128820815
At time: 71.09679698944092 and batch: 900, loss is 3.7962804365158083 and perplexity is 44.535224440575575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33906032614512 and perplexity of 76.6354931243803
finished 5 epochs...
Completing Train Step...
At time: 72.91306567192078 and batch: 50, loss is 3.8864044094085695 and perplexity is 48.735338788348834
At time: 73.65493321418762 and batch: 100, loss is 3.7663941287994387 and perplexity is 43.22392359117324
At time: 74.38000106811523 and batch: 150, loss is 3.766158699989319 and perplexity is 43.21374863205971
At time: 75.10986137390137 and batch: 200, loss is 3.6535632848739623 and perplexity is 38.612006791372366
At time: 75.84666037559509 and batch: 250, loss is 3.7996434497833254 and perplexity is 44.68524911733382
At time: 76.57682180404663 and batch: 300, loss is 3.7705564403533938 and perplexity is 43.40420997150003
At time: 77.30286169052124 and batch: 350, loss is 3.7771663808822633 and perplexity is 43.69205950421755
At time: 78.02866673469543 and batch: 400, loss is 3.7013931226730348 and perplexity is 40.50369168494525
At time: 78.75479912757874 and batch: 450, loss is 3.745644602775574 and perplexity is 42.336288514177554
At time: 79.48116374015808 and batch: 500, loss is 3.6292078781127928 and perplexity is 37.68295529817462
At time: 80.20653653144836 and batch: 550, loss is 3.691131663322449 and perplexity is 40.090189893471184
At time: 80.92980456352234 and batch: 600, loss is 3.7129803037643434 and perplexity is 40.97574489680486
At time: 81.67637920379639 and batch: 650, loss is 3.55944251537323 and perplexity is 35.14359966642424
At time: 82.40257143974304 and batch: 700, loss is 3.574814796447754 and perplexity is 35.68801065721604
At time: 83.12905812263489 and batch: 750, loss is 3.6793320894241335 and perplexity is 39.6199226679262
At time: 83.85483980178833 and batch: 800, loss is 3.6504997968673707 and perplexity is 38.49390037282356
At time: 84.58192777633667 and batch: 850, loss is 3.7194459104537962 and perplexity is 41.24153627243363
At time: 85.30844283103943 and batch: 900, loss is 3.6682911348342895 and perplexity is 39.1848869251631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3412412878585185 and perplexity of 76.80281459530961
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 87.09834027290344 and batch: 50, loss is 3.790680012702942 and perplexity is 44.286505425139346
At time: 87.83604097366333 and batch: 100, loss is 3.6765938282012938 and perplexity is 39.51158137111457
At time: 88.55999374389648 and batch: 150, loss is 3.6818004179000856 and perplexity is 39.71783844569988
At time: 89.28484964370728 and batch: 200, loss is 3.557597432136536 and perplexity is 35.07881658327159
At time: 90.01043581962585 and batch: 250, loss is 3.692861533164978 and perplexity is 40.15960072249007
At time: 90.7358763217926 and batch: 300, loss is 3.6538232564926147 and perplexity is 38.622046122191335
At time: 91.46026039123535 and batch: 350, loss is 3.644896149635315 and perplexity is 38.27879737806695
At time: 92.18551993370056 and batch: 400, loss is 3.5664561557769776 and perplexity is 35.39095063840753
At time: 92.91344165802002 and batch: 450, loss is 3.598483529090881 and perplexity is 36.542776346742386
At time: 93.65184926986694 and batch: 500, loss is 3.4707964086532592 and perplexity is 32.1623466188633
At time: 94.37844061851501 and batch: 550, loss is 3.513783025741577 and perplexity is 33.5750430879088
At time: 95.10549783706665 and batch: 600, loss is 3.528420386314392 and perplexity is 34.07010747880601
At time: 95.83230566978455 and batch: 650, loss is 3.3656619596481323 and perplexity is 28.952656447333393
At time: 96.5598497390747 and batch: 700, loss is 3.356673712730408 and perplexity is 28.69358884817518
At time: 97.2865343093872 and batch: 750, loss is 3.4538005065917967 and perplexity is 31.620337539308665
At time: 98.0141761302948 and batch: 800, loss is 3.401533603668213 and perplexity is 30.010088356050343
At time: 98.74623775482178 and batch: 850, loss is 3.456293087005615 and perplexity is 31.69925208290262
At time: 99.48840403556824 and batch: 900, loss is 3.395896415710449 and perplexity is 29.841391781243036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317210994354666 and perplexity of 74.97921894092472
finished 7 epochs...
Completing Train Step...
At time: 101.3078715801239 and batch: 50, loss is 3.6957832193374633 and perplexity is 40.277106045864855
At time: 102.03437209129333 and batch: 100, loss is 3.5729550313949585 and perplexity is 35.62170102147581
At time: 102.76938700675964 and batch: 150, loss is 3.5763677835464476 and perplexity is 35.743476735248926
At time: 103.50109839439392 and batch: 200, loss is 3.455657877922058 and perplexity is 31.67912282385852
At time: 104.23054575920105 and batch: 250, loss is 3.5909985399246214 and perplexity is 36.27027516810702
At time: 104.95960569381714 and batch: 300, loss is 3.557643508911133 and perplexity is 35.08043293923438
At time: 105.6880669593811 and batch: 350, loss is 3.553406524658203 and perplexity is 34.932112135627804
At time: 106.4181547164917 and batch: 400, loss is 3.481517643928528 and perplexity is 32.50902177622697
At time: 107.14965057373047 and batch: 450, loss is 3.517187376022339 and perplexity is 33.68953907691767
At time: 107.87868642807007 and batch: 500, loss is 3.3938962268829345 and perplexity is 29.781763017078653
At time: 108.60796976089478 and batch: 550, loss is 3.441106762886047 and perplexity is 31.221493844015615
At time: 109.33746004104614 and batch: 600, loss is 3.46252366065979 and perplexity is 31.897373170880847
At time: 110.06749701499939 and batch: 650, loss is 3.3044352293014527 and perplexity is 27.23315675579397
At time: 110.79477977752686 and batch: 700, loss is 3.3014867162704467 and perplexity is 27.15297770080621
At time: 111.52762031555176 and batch: 750, loss is 3.4059182262420653 and perplexity is 30.141960159674483
At time: 112.26491403579712 and batch: 800, loss is 3.360123119354248 and perplexity is 28.79273560404069
At time: 112.9927670955658 and batch: 850, loss is 3.421951026916504 and perplexity is 30.629114991765597
At time: 113.72118735313416 and batch: 900, loss is 3.3706637716293333 and perplexity is 29.097834966392902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32932480067423 and perplexity of 75.89302634548139
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 115.53441166877747 and batch: 50, loss is 3.663469338417053 and perplexity is 38.99640016535767
At time: 116.27199268341064 and batch: 100, loss is 3.553509244918823 and perplexity is 34.935700555588944
At time: 116.99705839157104 and batch: 150, loss is 3.56034734249115 and perplexity is 35.175412939009114
At time: 117.73465466499329 and batch: 200, loss is 3.4379438972473144 and perplexity is 31.122900454959396
At time: 118.46089696884155 and batch: 250, loss is 3.574149794578552 and perplexity is 35.664285952781604
At time: 119.18652009963989 and batch: 300, loss is 3.53302668094635 and perplexity is 34.227406436538146
At time: 119.91238951683044 and batch: 350, loss is 3.5243187713623048 and perplexity is 33.930651210019185
At time: 120.63794946670532 and batch: 400, loss is 3.4487074518203737 and perplexity is 31.45970283622849
At time: 121.37285804748535 and batch: 450, loss is 3.4782823657989503 and perplexity is 32.40401600209413
At time: 122.10511088371277 and batch: 500, loss is 3.3498141384124756 and perplexity is 28.497436572719728
At time: 122.82909560203552 and batch: 550, loss is 3.390898427963257 and perplexity is 29.69261696780651
At time: 123.55155944824219 and batch: 600, loss is 3.4170443630218506 and perplexity is 30.479196320155467
At time: 124.27554249763489 and batch: 650, loss is 3.2480225276947023 and perplexity is 25.739390626271145
At time: 124.9994866847992 and batch: 700, loss is 3.236744623184204 and perplexity is 25.450735011389842
At time: 125.72434115409851 and batch: 750, loss is 3.337714433670044 and perplexity is 28.154703669155726
At time: 126.44978213310242 and batch: 800, loss is 3.287991280555725 and perplexity is 26.78899799234794
At time: 127.16963768005371 and batch: 850, loss is 3.3436315965652468 and perplexity is 28.32179349837759
At time: 127.88998937606812 and batch: 900, loss is 3.2991931200027467 and perplexity is 27.090771098180756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322571636879281 and perplexity of 75.38223497778948
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 129.66943502426147 and batch: 50, loss is 3.6520968008041383 and perplexity is 38.555424397239086
At time: 130.40778851509094 and batch: 100, loss is 3.539803705215454 and perplexity is 34.460154178370914
At time: 131.1323688030243 and batch: 150, loss is 3.545565333366394 and perplexity is 34.659273848682645
At time: 131.85788941383362 and batch: 200, loss is 3.4214580869674682 and perplexity is 30.614020398052322
At time: 132.58545207977295 and batch: 250, loss is 3.559594931602478 and perplexity is 35.14895652959366
At time: 133.31164836883545 and batch: 300, loss is 3.518423390388489 and perplexity is 33.7312055760969
At time: 134.03628587722778 and batch: 350, loss is 3.5099556255340576 and perplexity is 33.446783567856194
At time: 134.76044249534607 and batch: 400, loss is 3.4350644540786743 and perplexity is 31.03341273109367
At time: 135.4840636253357 and batch: 450, loss is 3.4617030239105224 and perplexity is 31.871207751872333
At time: 136.23539519309998 and batch: 500, loss is 3.331691679954529 and perplexity is 27.98564443511947
At time: 136.9733326435089 and batch: 550, loss is 3.3698542642593385 and perplexity is 29.074289585902015
At time: 137.6987111568451 and batch: 600, loss is 3.3970475149154664 and perplexity is 29.87576196154752
At time: 138.41640162467957 and batch: 650, loss is 3.2279070663452147 and perplexity is 25.22680365464228
At time: 139.136159658432 and batch: 700, loss is 3.2175224208831787 and perplexity is 24.96618778632797
At time: 139.85910820960999 and batch: 750, loss is 3.3166139554977416 and perplexity is 27.5668497688295
At time: 140.5825982093811 and batch: 800, loss is 3.268851342201233 and perplexity is 26.281133969273558
At time: 141.31014251708984 and batch: 850, loss is 3.317447690963745 and perplexity is 27.58984281289401
At time: 142.039954662323 and batch: 900, loss is 3.2768724250793455 and perplexity is 26.492784822729494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317696767310574 and perplexity of 75.01565066579822
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 143.8930218219757 and batch: 50, loss is 3.6439941930770874 and perplexity is 38.24428713144195
At time: 144.63190388679504 and batch: 100, loss is 3.531763014793396 and perplexity is 34.18418173806706
At time: 145.3673231601715 and batch: 150, loss is 3.5368473291397096 and perplexity is 34.35842744831931
At time: 146.10464715957642 and batch: 200, loss is 3.4131271266937255 and perplexity is 30.360035647709687
At time: 146.83625864982605 and batch: 250, loss is 3.553628740310669 and perplexity is 34.93987546025222
At time: 147.57878923416138 and batch: 300, loss is 3.5111081981658936 and perplexity is 33.48535563950651
At time: 148.3108048439026 and batch: 350, loss is 3.50371066570282 and perplexity is 33.238560596752016
At time: 149.04075479507446 and batch: 400, loss is 3.4294466161727906 and perplexity is 30.859560841802544
At time: 149.78187370300293 and batch: 450, loss is 3.455283980369568 and perplexity is 31.66728029145455
At time: 150.52107453346252 and batch: 500, loss is 3.3258477783203126 and perplexity is 27.822576023829473
At time: 151.2626941204071 and batch: 550, loss is 3.362873520851135 and perplexity is 28.8720361913821
At time: 151.99288368225098 and batch: 600, loss is 3.3906102132797242 and perplexity is 29.684060352734168
At time: 152.72359204292297 and batch: 650, loss is 3.221255130767822 and perplexity is 25.05955346735382
At time: 153.452561378479 and batch: 700, loss is 3.2114114332199097 and perplexity is 24.814084943411082
At time: 154.2026126384735 and batch: 750, loss is 3.309273338317871 and perplexity is 27.365232978994005
At time: 154.93827843666077 and batch: 800, loss is 3.263165488243103 and perplexity is 26.132127296196554
At time: 155.66827917099 and batch: 850, loss is 3.3079516220092775 and perplexity is 27.329087796377813
At time: 156.4020745754242 and batch: 900, loss is 3.268276038169861 and perplexity is 26.26601867531368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314551784567637 and perplexity of 74.78009833716078
finished 11 epochs...
Completing Train Step...
At time: 158.2162687778473 and batch: 50, loss is 3.639816541671753 and perplexity is 38.08484910170199
At time: 158.96408653259277 and batch: 100, loss is 3.5260083961486814 and perplexity is 33.988029739764066
At time: 159.69106650352478 and batch: 150, loss is 3.530402240753174 and perplexity is 34.13769642614761
At time: 160.4172613620758 and batch: 200, loss is 3.408098998069763 and perplexity is 30.207764623403392
At time: 161.14418172836304 and batch: 250, loss is 3.5478383827209474 and perplexity is 34.7381456945736
At time: 161.87322330474854 and batch: 300, loss is 3.5064761209487916 and perplexity is 33.330607565746874
At time: 162.60215377807617 and batch: 350, loss is 3.498517928123474 and perplexity is 33.0664088303636
At time: 163.33039951324463 and batch: 400, loss is 3.424732174873352 and perplexity is 30.714417657028473
At time: 164.05778694152832 and batch: 450, loss is 3.451218295097351 and perplexity is 31.538792468876125
At time: 164.78380393981934 and batch: 500, loss is 3.322708306312561 and perplexity is 27.735364795455787
At time: 165.51314997673035 and batch: 550, loss is 3.359941062927246 and perplexity is 28.78749417860508
At time: 166.24262142181396 and batch: 600, loss is 3.388339433670044 and perplexity is 29.616730867895015
At time: 166.96890115737915 and batch: 650, loss is 3.2197643327713013 and perplexity is 25.02222256857946
At time: 167.695237159729 and batch: 700, loss is 3.210756368637085 and perplexity is 24.797835438028244
At time: 168.42522501945496 and batch: 750, loss is 3.308955764770508 and perplexity is 27.356543884668724
At time: 169.15110564231873 and batch: 800, loss is 3.2636485481262207 and perplexity is 26.14475372796967
At time: 169.87920379638672 and batch: 850, loss is 3.309858174324036 and perplexity is 27.38124183337814
At time: 170.60754323005676 and batch: 900, loss is 3.271557550430298 and perplexity is 26.352352512930366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313718194830908 and perplexity of 74.71778838875014
finished 12 epochs...
Completing Train Step...
At time: 172.41662573814392 and batch: 50, loss is 3.6373270034790037 and perplexity is 37.99015333856651
At time: 173.16523575782776 and batch: 100, loss is 3.5226907920837403 and perplexity is 33.875457752034045
At time: 173.8934564590454 and batch: 150, loss is 3.5266751766204836 and perplexity is 34.010699851423524
At time: 174.6251003742218 and batch: 200, loss is 3.4046859836578367 and perplexity is 30.10484082750141
At time: 175.35355496406555 and batch: 250, loss is 3.544266366958618 and perplexity is 34.61428184411288
At time: 176.07860684394836 and batch: 300, loss is 3.5030718517303465 and perplexity is 33.21733412042459
At time: 176.8039517402649 and batch: 350, loss is 3.495103702545166 and perplexity is 32.9537051589127
At time: 177.53043699264526 and batch: 400, loss is 3.4215102624893188 and perplexity is 30.615617742213306
At time: 178.257248878479 and batch: 450, loss is 3.4482258129119874 and perplexity is 31.444554267659147
At time: 178.98450136184692 and batch: 500, loss is 3.320374174118042 and perplexity is 27.670702282344507
At time: 179.71121549606323 and batch: 550, loss is 3.3578507137298583 and perplexity is 28.727381113791385
At time: 180.4368531703949 and batch: 600, loss is 3.3867230653762816 and perplexity is 29.568897991341984
At time: 181.17359948158264 and batch: 650, loss is 3.2186722564697265 and perplexity is 24.99491130800416
At time: 181.90262365341187 and batch: 700, loss is 3.210146679878235 and perplexity is 24.78272108451178
At time: 182.6284830570221 and batch: 750, loss is 3.308758783340454 and perplexity is 27.351155684238126
At time: 183.3548505306244 and batch: 800, loss is 3.2640147495269773 and perplexity is 26.15432972667234
At time: 184.0829474925995 and batch: 850, loss is 3.3110791635513306 and perplexity is 27.414694453174874
At time: 184.81364822387695 and batch: 900, loss is 3.2734777116775513 and perplexity is 26.403001890923967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313405912216395 and perplexity of 74.69445896531779
finished 13 epochs...
Completing Train Step...
At time: 186.64235711097717 and batch: 50, loss is 3.6352258014678953 and perplexity is 37.91041215747267
At time: 187.36998462677002 and batch: 100, loss is 3.5200763273239137 and perplexity is 33.78700723727834
At time: 188.10571789741516 and batch: 150, loss is 3.5237167072296143 and perplexity is 33.910228930304754
At time: 188.84176516532898 and batch: 200, loss is 3.4018451452255247 and perplexity is 30.01943920222481
At time: 189.57787561416626 and batch: 250, loss is 3.541322708129883 and perplexity is 34.512539029297535
At time: 190.3086338043213 and batch: 300, loss is 3.500197129249573 and perplexity is 33.12198062636073
At time: 191.05716919898987 and batch: 350, loss is 3.4922952365875246 and perplexity is 32.8612856390798
At time: 191.78380036354065 and batch: 400, loss is 3.4188466119766234 and perplexity is 30.534176949374668
At time: 192.51421904563904 and batch: 450, loss is 3.4457196807861328 and perplexity is 31.36584872446474
At time: 193.2466208934784 and batch: 500, loss is 3.318349251747131 and perplexity is 27.614727949239835
At time: 193.98084354400635 and batch: 550, loss is 3.3560504388809203 and perplexity is 28.67571045674386
At time: 194.71059823036194 and batch: 600, loss is 3.385321617126465 and perplexity is 29.52748773494636
At time: 195.43834257125854 and batch: 650, loss is 3.2176920270919798 and perplexity is 24.97042256589893
At time: 196.1665461063385 and batch: 700, loss is 3.209533939361572 and perplexity is 24.767540358588036
At time: 196.89186644554138 and batch: 750, loss is 3.308536286354065 and perplexity is 27.345070811482156
At time: 197.61432099342346 and batch: 800, loss is 3.2642292308807375 and perplexity is 26.15993994434084
At time: 198.33580112457275 and batch: 850, loss is 3.3119153642654418 and perplexity is 27.437628227536674
At time: 199.05967593193054 and batch: 900, loss is 3.2747383975982665 and perplexity is 26.436308774020464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313292620933219 and perplexity of 74.68599721354587
finished 14 epochs...
Completing Train Step...
At time: 200.86752605438232 and batch: 50, loss is 3.6333091354370115 and perplexity is 37.83782014781604
At time: 201.61361241340637 and batch: 100, loss is 3.5178018760681153 and perplexity is 33.7102476622843
At time: 202.35662579536438 and batch: 150, loss is 3.521142954826355 and perplexity is 33.8230646148996
At time: 203.09213733673096 and batch: 200, loss is 3.3993283319473266 and perplexity is 29.94398087612674
At time: 203.84045577049255 and batch: 250, loss is 3.5387171411514284 and perplexity is 34.42273134803307
At time: 204.569983959198 and batch: 300, loss is 3.497644553184509 and perplexity is 33.03754206515916
At time: 205.2978961467743 and batch: 350, loss is 3.4898211431503294 and perplexity is 32.78008423928793
At time: 206.02650809288025 and batch: 400, loss is 3.416501245498657 and perplexity is 30.46264702905294
At time: 206.75591325759888 and batch: 450, loss is 3.4435099267959597 and perplexity is 31.29661443863303
At time: 207.485937833786 and batch: 500, loss is 3.316512470245361 and perplexity is 27.5640522820776
At time: 208.21613836288452 and batch: 550, loss is 3.3544137620925905 and perplexity is 28.628815973074968
At time: 208.94674730300903 and batch: 600, loss is 3.384037194252014 and perplexity is 29.489586300220033
At time: 209.69644975662231 and batch: 650, loss is 3.216760182380676 and perplexity is 24.947164847665704
At time: 210.42473220825195 and batch: 700, loss is 3.2089097547531127 and perplexity is 24.752085664899543
At time: 211.1520607471466 and batch: 750, loss is 3.308269085884094 and perplexity is 27.337765171788586
At time: 211.88459706306458 and batch: 800, loss is 3.2643145227432253 and perplexity is 26.16217126949685
At time: 212.6119258403778 and batch: 850, loss is 3.312494730949402 and perplexity is 27.453529281044126
At time: 213.33988308906555 and batch: 900, loss is 3.275610384941101 and perplexity is 26.45937095416597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31328885849208 and perplexity of 74.68571621240606
finished 15 epochs...
Completing Train Step...
At time: 215.14978694915771 and batch: 50, loss is 3.631509289741516 and perplexity is 37.7697791601034
At time: 215.88668394088745 and batch: 100, loss is 3.515730652809143 and perplexity is 33.64049847126897
At time: 216.62449765205383 and batch: 150, loss is 3.5188127422332762 and perplexity is 33.74434144028539
At time: 217.3543210029602 and batch: 200, loss is 3.397032070159912 and perplexity is 29.875300541270292
At time: 218.08864545822144 and batch: 250, loss is 3.536340069770813 and perplexity is 34.34100323376727
At time: 218.81610918045044 and batch: 300, loss is 3.4953175830841063 and perplexity is 32.96075406891763
At time: 219.54003429412842 and batch: 350, loss is 3.4875698328018188 and perplexity is 32.706369105371685
At time: 220.26483058929443 and batch: 400, loss is 3.4143697690963744 and perplexity is 30.397785765442272
At time: 220.99010610580444 and batch: 450, loss is 3.441502499580383 and perplexity is 31.233851779864736
At time: 221.71587014198303 and batch: 500, loss is 3.3148092555999757 and perplexity is 27.51714474268829
At time: 222.44062662124634 and batch: 550, loss is 3.3528886461257934 and perplexity is 28.585186986812392
At time: 223.17308473587036 and batch: 600, loss is 3.382828640937805 and perplexity is 29.453968090550248
At time: 223.89741230010986 and batch: 650, loss is 3.21585328578949 and perplexity is 24.9245506048454
At time: 224.63321733474731 and batch: 700, loss is 3.2082721042633056 and perplexity is 24.73630751635826
At time: 225.36159491539001 and batch: 750, loss is 3.3079563331604005 and perplexity is 27.32921654814376
At time: 226.08714413642883 and batch: 800, loss is 3.2642940521240233 and perplexity is 26.16163571913284
At time: 226.81274938583374 and batch: 850, loss is 3.312887201309204 and perplexity is 27.464306092210037
At time: 227.54796504974365 and batch: 900, loss is 3.276224055290222 and perplexity is 26.475613268780652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313363271216824 and perplexity of 74.69127398683088
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 229.3790214061737 and batch: 50, loss is 3.630766258239746 and perplexity is 37.741725448059135
At time: 230.1032154560089 and batch: 100, loss is 3.516225051879883 and perplexity is 33.65713441451096
At time: 230.83541297912598 and batch: 150, loss is 3.5187685012817385 and perplexity is 33.74284859153382
At time: 231.56211304664612 and batch: 200, loss is 3.397021927833557 and perplexity is 29.87499753775884
At time: 232.2872235774994 and batch: 250, loss is 3.536594729423523 and perplexity is 34.349749615352465
At time: 233.0146610736847 and batch: 300, loss is 3.4953583335876464 and perplexity is 32.962097263610744
At time: 233.74124717712402 and batch: 350, loss is 3.4871254444122313 and perplexity is 32.69183800364128
At time: 234.47766017913818 and batch: 400, loss is 3.4137190532684327 and perplexity is 30.378011879398375
At time: 235.2025601863861 and batch: 450, loss is 3.440997166633606 and perplexity is 31.218072272793805
At time: 235.92691111564636 and batch: 500, loss is 3.3138845109939576 and perplexity is 27.49171017356731
At time: 236.65151858329773 and batch: 550, loss is 3.3510214281082153 and perplexity is 28.531862010805913
At time: 237.37602639198303 and batch: 600, loss is 3.3809789419174194 and perplexity is 29.3995374702782
At time: 238.10051894187927 and batch: 650, loss is 3.213377528190613 and perplexity is 24.862919782255407
At time: 238.82515025138855 and batch: 700, loss is 3.2057798528671264 and perplexity is 24.67473517815766
At time: 239.55291175842285 and batch: 750, loss is 3.3053895616531372 and perplexity is 27.25915864357244
At time: 240.2862424850464 and batch: 800, loss is 3.261914677619934 and perplexity is 26.099461387460185
At time: 241.0188329219818 and batch: 850, loss is 3.3093192338943482 and perplexity is 27.366488950958566
At time: 241.7477147579193 and batch: 900, loss is 3.2724283027648924 and perplexity is 26.375308878607086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311910132839255 and perplexity of 74.58281605141036
finished 17 epochs...
Completing Train Step...
At time: 243.57761144638062 and batch: 50, loss is 3.629862108230591 and perplexity is 37.70761668869091
At time: 244.3232123851776 and batch: 100, loss is 3.5149537038803103 and perplexity is 33.61437167292459
At time: 245.05427622795105 and batch: 150, loss is 3.5176038932800293 and perplexity is 33.703574274094855
At time: 245.808180809021 and batch: 200, loss is 3.396092643737793 and perplexity is 29.84724807325099
At time: 246.54767560958862 and batch: 250, loss is 3.5354670476913452 and perplexity is 34.311035862704756
At time: 247.27907514572144 and batch: 300, loss is 3.4943721675872803 and perplexity is 32.92960718692621
At time: 248.0125093460083 and batch: 350, loss is 3.486273431777954 and perplexity is 32.66399600717507
At time: 248.74673461914062 and batch: 400, loss is 3.412926268577576 and perplexity is 30.353938200524627
At time: 249.48184323310852 and batch: 450, loss is 3.4402453994750974 and perplexity is 31.19461237060405
At time: 250.22243118286133 and batch: 500, loss is 3.3132820320129395 and perplexity is 27.47515198451678
At time: 250.9524862766266 and batch: 550, loss is 3.350620245933533 and perplexity is 28.520417832110414
At time: 251.68612909317017 and batch: 600, loss is 3.3806820249557497 and perplexity is 29.39080954473672
At time: 252.4271776676178 and batch: 650, loss is 3.2132274770736693 and perplexity is 24.859189353256117
At time: 253.16105222702026 and batch: 700, loss is 3.205814871788025 and perplexity is 24.675599275886857
At time: 253.89792847633362 and batch: 750, loss is 3.3053497076034546 and perplexity is 27.258072277357744
At time: 254.62996554374695 and batch: 800, loss is 3.262076554298401 and perplexity is 26.10368662355373
At time: 255.36025524139404 and batch: 850, loss is 3.30969087600708 and perplexity is 27.376661380864938
At time: 256.1072826385498 and batch: 900, loss is 3.2729742765426635 and perplexity is 26.389713037424112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311277624678938 and perplexity of 74.53565672753926
finished 18 epochs...
Completing Train Step...
At time: 257.9393992424011 and batch: 50, loss is 3.6291943073272703 and perplexity is 37.68244391434037
At time: 258.68212389945984 and batch: 100, loss is 3.5140836811065674 and perplexity is 33.58513912238038
At time: 259.4076473712921 and batch: 150, loss is 3.516728959083557 and perplexity is 33.67409876086666
At time: 260.13455843925476 and batch: 200, loss is 3.3953506708145142 and perplexity is 29.825110437125154
At time: 260.86493706703186 and batch: 250, loss is 3.534602608680725 and perplexity is 34.28138888066031
At time: 261.59756875038147 and batch: 300, loss is 3.4936253309249876 and perplexity is 32.90502333020954
At time: 262.3228497505188 and batch: 350, loss is 3.485551929473877 and perplexity is 32.640437358627125
At time: 263.0497121810913 and batch: 400, loss is 3.412287812232971 and perplexity is 30.334564721315218
At time: 263.77590584754944 and batch: 450, loss is 3.4396453714370727 and perplexity is 31.1759003429784
At time: 264.5130362510681 and batch: 500, loss is 3.31280131816864 and perplexity is 27.461947472633714
At time: 265.24326753616333 and batch: 550, loss is 3.3502490949630737 and perplexity is 28.50983441549829
At time: 265.97220253944397 and batch: 600, loss is 3.380391545295715 and perplexity is 29.382273352227195
At time: 266.7005832195282 and batch: 650, loss is 3.213046703338623 and perplexity is 24.85469587091054
At time: 267.4266691207886 and batch: 700, loss is 3.2057720947265627 and perplexity is 24.674543748836346
At time: 268.1525273323059 and batch: 750, loss is 3.3053200578689577 and perplexity is 27.25726409473308
At time: 268.877813577652 and batch: 800, loss is 3.2621857643127443 and perplexity is 26.10653756321705
At time: 269.61265802383423 and batch: 850, loss is 3.309959301948547 and perplexity is 27.38401097333683
At time: 270.3406386375427 and batch: 900, loss is 3.2733828592300416 and perplexity is 26.400497620342986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31098415427012 and perplexity of 74.51378592726118
finished 19 epochs...
Completing Train Step...
At time: 272.1521644592285 and batch: 50, loss is 3.628625111579895 and perplexity is 37.661001330606275
At time: 272.8817172050476 and batch: 100, loss is 3.513381152153015 and perplexity is 33.56155287571849
At time: 273.61178731918335 and batch: 150, loss is 3.515987343788147 and perplexity is 33.64913479214114
At time: 274.34020376205444 and batch: 200, loss is 3.394692134857178 and perplexity is 29.805475995173367
At time: 275.06768250465393 and batch: 250, loss is 3.533867244720459 and perplexity is 34.25618884950256
At time: 275.79584312438965 and batch: 300, loss is 3.4929734659194946 and perplexity is 32.883580686609434
At time: 276.53033685684204 and batch: 350, loss is 3.484900407791138 and perplexity is 32.61917833206427
At time: 277.25839924812317 and batch: 400, loss is 3.411712408065796 and perplexity is 30.31711510713693
At time: 277.9843592643738 and batch: 450, loss is 3.4391058540344237 and perplexity is 31.159084938709615
At time: 278.71457982063293 and batch: 500, loss is 3.3123676872253416 and perplexity is 27.450041703987743
At time: 279.44743967056274 and batch: 550, loss is 3.3498880529403685 and perplexity is 28.499543025137918
At time: 280.1780388355255 and batch: 600, loss is 3.380105285644531 and perplexity is 29.37386359665067
At time: 280.9186134338379 and batch: 650, loss is 3.2128541898727416 and perplexity is 24.849911467810788
At time: 281.6570014953613 and batch: 700, loss is 3.205688772201538 and perplexity is 24.672487889198262
At time: 282.4044587612152 and batch: 750, loss is 3.3052835273742676 and perplexity is 27.256268391578676
At time: 283.1401128768921 and batch: 800, loss is 3.262259006500244 and perplexity is 26.108449733161127
At time: 283.87064838409424 and batch: 850, loss is 3.310162353515625 and perplexity is 27.3895719042366
At time: 284.6022434234619 and batch: 900, loss is 3.2736976099014283 and perplexity is 26.408808502553196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310844943947988 and perplexity of 74.50341356110593
Finished Training.
Improved accuracyfrom -10000000 to -74.50341356110593
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc267f18b70>
ELAPSED
296.35830307006836


RESULTS SO FAR:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9985671043395996 and batch: 50, loss is 7.073140201568603 and perplexity is 1179.8471811449422
At time: 1.7998559474945068 and batch: 100, loss is 6.181170263290405 and perplexity is 483.5575149705657
At time: 2.585993528366089 and batch: 150, loss is 5.987476015090943 and perplexity is 398.40776464485486
At time: 3.367476224899292 and batch: 200, loss is 5.830624389648437 and perplexity is 340.57126186805914
At time: 4.14896559715271 and batch: 250, loss is 5.86161527633667 and perplexity is 351.29111821717447
At time: 4.930222511291504 and batch: 300, loss is 5.763226556777954 and perplexity is 318.3739250017291
At time: 5.710738658905029 and batch: 350, loss is 5.742623443603516 and perplexity is 311.8815423009744
At time: 6.491397857666016 and batch: 400, loss is 5.59603515625 and perplexity is 269.3563317181232
At time: 7.277033805847168 and batch: 450, loss is 5.598453264236451 and perplexity is 270.0084525465843
At time: 8.056763648986816 and batch: 500, loss is 5.54493480682373 and perplexity is 255.93789229524555
At time: 8.836673021316528 and batch: 550, loss is 5.590166387557983 and perplexity is 267.78017129590813
At time: 9.62769079208374 and batch: 600, loss is 5.509426155090332 and perplexity is 247.00934133683805
At time: 10.419867515563965 and batch: 650, loss is 5.418695850372314 and perplexity is 225.58473433335536
At time: 11.206223487854004 and batch: 700, loss is 5.51092833518982 and perplexity is 247.38067268723708
At time: 11.993201971054077 and batch: 750, loss is 5.475220527648926 and perplexity is 238.70310176055108
At time: 12.779993295669556 and batch: 800, loss is 5.459782218933105 and perplexity is 235.04623017210142
At time: 13.579688549041748 and batch: 850, loss is 5.483429002761841 and perplexity is 240.67055407533022
At time: 14.373425006866455 and batch: 900, loss is 5.380233163833618 and perplexity is 217.07288307037314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.258021942556721 and perplexity of 192.10112816477528
finished 1 epochs...
Completing Train Step...
At time: 16.230250120162964 and batch: 50, loss is 5.157207107543945 and perplexity is 173.678711642007
At time: 16.95132064819336 and batch: 100, loss is 5.000356121063232 and perplexity is 148.46602156675368
At time: 17.672096252441406 and batch: 150, loss is 4.970419473648072 and perplexity is 144.0873155279091
At time: 18.392158031463623 and batch: 200, loss is 4.828552007675171 and perplexity is 125.02978734518872
At time: 19.130380630493164 and batch: 250, loss is 4.909843301773071 and perplexity is 135.61816161790648
At time: 19.855792760849 and batch: 300, loss is 4.828253564834594 and perplexity is 124.99247866782748
At time: 20.579609394073486 and batch: 350, loss is 4.813889799118042 and perplexity is 123.20994855755201
At time: 21.314935445785522 and batch: 400, loss is 4.671172370910645 and perplexity is 106.82290511611896
At time: 22.045645236968994 and batch: 450, loss is 4.689641036987305 and perplexity is 108.81411255431978
At time: 22.77608060836792 and batch: 500, loss is 4.5889036655426025 and perplexity is 98.3865064953252
At time: 23.509581089019775 and batch: 550, loss is 4.649568538665772 and perplexity is 104.53987093300901
At time: 24.238341093063354 and batch: 600, loss is 4.593416290283203 and perplexity is 98.8314911479229
At time: 24.967119216918945 and batch: 650, loss is 4.4568390274047855 and perplexity is 86.21455607602911
At time: 25.697057485580444 and batch: 700, loss is 4.501692152023315 and perplexity is 90.16958292066653
At time: 26.424460649490356 and batch: 750, loss is 4.5367289066314695 and perplexity is 93.3848294666227
At time: 27.163140296936035 and batch: 800, loss is 4.486674489974976 and perplexity is 88.82556386891964
At time: 27.89271092414856 and batch: 850, loss is 4.535287475585937 and perplexity is 93.25031864157705
At time: 28.619690895080566 and batch: 900, loss is 4.471451787948609 and perplexity is 87.48363855607798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.572273149882277 and perplexity of 96.7638186255789
finished 2 epochs...
Completing Train Step...
At time: 30.443206310272217 and batch: 50, loss is 4.5112401962280275 and perplexity is 91.03464935862586
At time: 31.18751358985901 and batch: 100, loss is 4.378365955352783 and perplexity is 79.70768102376461
At time: 31.913749933242798 and batch: 150, loss is 4.378430228233338 and perplexity is 79.71280423066621
At time: 32.63918375968933 and batch: 200, loss is 4.266329894065857 and perplexity is 71.25962473785884
At time: 33.3644962310791 and batch: 250, loss is 4.397076721191406 and perplexity is 81.21311274938617
At time: 34.08884310722351 and batch: 300, loss is 4.3528936576843265 and perplexity is 77.70298376143162
At time: 34.818562030792236 and batch: 350, loss is 4.352691297531128 and perplexity is 77.68726136458064
At time: 35.55107927322388 and batch: 400, loss is 4.258498201370239 and perplexity is 70.70372093057982
At time: 36.28231406211853 and batch: 450, loss is 4.296456589698791 and perplexity is 73.43910722373526
At time: 37.01911640167236 and batch: 500, loss is 4.172716941833496 and perplexity is 64.89151930036095
At time: 37.743096351623535 and batch: 550, loss is 4.24391987323761 and perplexity is 69.68045575700401
At time: 38.46510434150696 and batch: 600, loss is 4.237834415435791 and perplexity is 69.25770590171906
At time: 39.18921685218811 and batch: 650, loss is 4.084286732673645 and perplexity is 59.39955487618753
At time: 39.91153311729431 and batch: 700, loss is 4.1029462718963625 and perplexity is 60.51832864079916
At time: 40.63352036476135 and batch: 750, loss is 4.187920289039612 and perplexity is 65.88562532029574
At time: 41.35796117782593 and batch: 800, loss is 4.1493478202819825 and perplexity is 63.39264341722152
At time: 42.08314609527588 and batch: 850, loss is 4.210564661026001 and perplexity is 67.39458416306447
At time: 42.80643582344055 and batch: 900, loss is 4.161001410484314 and perplexity is 64.13571663133759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.407631651995933 and perplexity of 82.07485133015896
finished 3 epochs...
Completing Train Step...
At time: 44.60253858566284 and batch: 50, loss is 4.2302515411376955 and perplexity is 68.73451955875984
At time: 45.340731620788574 and batch: 100, loss is 4.09997784614563 and perplexity is 60.33895084222509
At time: 46.07663607597351 and batch: 150, loss is 4.105714106559754 and perplexity is 60.68606539529873
At time: 46.805948972702026 and batch: 200, loss is 3.9932713174819945 and perplexity is 54.23200961822665
At time: 47.534693479537964 and batch: 250, loss is 4.134963574409485 and perplexity is 62.487314891665584
At time: 48.26389789581299 and batch: 300, loss is 4.101757965087891 and perplexity is 60.44645701006935
At time: 48.99096632003784 and batch: 350, loss is 4.099967021942138 and perplexity is 60.33829772467744
At time: 49.71621251106262 and batch: 400, loss is 4.020849409103394 and perplexity is 55.74843895290222
At time: 50.439491987228394 and batch: 450, loss is 4.066897854804993 and perplexity is 58.37559185638037
At time: 51.16223931312561 and batch: 500, loss is 3.938140697479248 and perplexity is 51.32308738766079
At time: 51.884384870529175 and batch: 550, loss is 4.009358706474305 and perplexity is 55.11151656976437
At time: 52.60813355445862 and batch: 600, loss is 4.018894820213318 and perplexity is 55.63958009529983
At time: 53.33278727531433 and batch: 650, loss is 3.86476921081543 and perplexity is 47.692264303670065
At time: 54.05709719657898 and batch: 700, loss is 3.8734660816192625 and perplexity is 48.10884662025475
At time: 54.78202700614929 and batch: 750, loss is 3.9703310441970827 and perplexity is 53.00207396476045
At time: 55.525535106658936 and batch: 800, loss is 3.9405232667922974 and perplexity is 51.44551398773754
At time: 56.2482635974884 and batch: 850, loss is 4.0037247133255 and perplexity is 54.801891694209715
At time: 56.970579385757446 and batch: 900, loss is 3.9595529365539552 and perplexity is 52.433879437619574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349159972308433 and perplexity of 77.4134061994548
finished 4 epochs...
Completing Train Step...
At time: 58.78152656555176 and batch: 50, loss is 4.036205530166626 and perplexity is 56.611125537892235
At time: 59.51061964035034 and batch: 100, loss is 3.9130492210388184 and perplexity is 50.05133711750397
At time: 60.239338874816895 and batch: 150, loss is 3.9206745862960815 and perplexity is 50.434455697789595
At time: 60.96871876716614 and batch: 200, loss is 3.807938647270203 and perplexity is 45.05746374609133
At time: 61.691627979278564 and batch: 250, loss is 3.9528037786483763 and perplexity is 52.08118643487298
At time: 62.41393756866455 and batch: 300, loss is 3.924999952316284 and perplexity is 50.65307564351171
At time: 63.13668632507324 and batch: 350, loss is 3.92051607131958 and perplexity is 50.42646171482955
At time: 63.8599066734314 and batch: 400, loss is 3.848733983039856 and perplexity is 46.93360686084723
At time: 64.58466458320618 and batch: 450, loss is 3.898068404197693 and perplexity is 49.30711564471187
At time: 65.31223773956299 and batch: 500, loss is 3.7728605461120606 and perplexity is 43.50433316456325
At time: 66.0400288105011 and batch: 550, loss is 3.8397840213775636 and perplexity is 46.51542701696483
At time: 66.76859664916992 and batch: 600, loss is 3.8565099143981936 and perplexity is 47.299981973987926
At time: 67.495276927948 and batch: 650, loss is 3.704540100097656 and perplexity is 40.631356662332
At time: 68.2223527431488 and batch: 700, loss is 3.7128963232040406 and perplexity is 40.97230387528108
At time: 68.94871687889099 and batch: 750, loss is 3.8125566911697386 and perplexity is 45.26602228727304
At time: 69.68098950386047 and batch: 800, loss is 3.7825057744979858 and perplexity is 43.92597252892649
At time: 70.40947914123535 and batch: 850, loss is 3.847818236351013 and perplexity is 46.8906472388369
At time: 71.13673686981201 and batch: 900, loss is 3.806594772338867 and perplexity is 44.99695281875944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338699967893835 and perplexity of 76.60788186736264
finished 5 epochs...
Completing Train Step...
At time: 72.9454300403595 and batch: 50, loss is 3.8864601373672487 and perplexity is 48.73805478497282
At time: 73.68455839157104 and batch: 100, loss is 3.7705111503601074 and perplexity is 43.402244239636154
At time: 74.41007804870605 and batch: 150, loss is 3.776640224456787 and perplexity is 43.669076693174176
At time: 75.14399695396423 and batch: 200, loss is 3.665908842086792 and perplexity is 39.09164815834539
At time: 75.86870455741882 and batch: 250, loss is 3.809617447853088 and perplexity is 45.13316977237149
At time: 76.59385204315186 and batch: 300, loss is 3.7832389545440672 and perplexity is 43.958189984644044
At time: 77.33040308952332 and batch: 350, loss is 3.7832436990737914 and perplexity is 43.95839854607781
At time: 78.07005310058594 and batch: 400, loss is 3.714043893814087 and perplexity is 41.01934947594563
At time: 78.80782771110535 and batch: 450, loss is 3.764328818321228 and perplexity is 43.13474489142189
At time: 79.54576706886292 and batch: 500, loss is 3.641601572036743 and perplexity is 38.15289242542669
At time: 80.27533793449402 and batch: 550, loss is 3.7040351915359495 and perplexity is 40.61084672073771
At time: 81.00430488586426 and batch: 600, loss is 3.7243468523025514 and perplexity is 41.44415474858119
At time: 81.74057006835938 and batch: 650, loss is 3.5809016752243044 and perplexity is 35.90590171735762
At time: 82.4713990688324 and batch: 700, loss is 3.5817512321472167 and perplexity is 35.93641878589296
At time: 83.19932341575623 and batch: 750, loss is 3.6841116333007813 and perplexity is 39.80974108808934
At time: 83.92869210243225 and batch: 800, loss is 3.6575812292099 and perplexity is 38.767459776945955
At time: 84.65779089927673 and batch: 850, loss is 3.720703101158142 and perplexity is 41.29341735384162
At time: 85.40095496177673 and batch: 900, loss is 3.681770839691162 and perplexity is 39.71666368055016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344582335589683 and perplexity of 77.05984560209689
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 87.29938435554504 and batch: 50, loss is 3.7931895065307617 and perplexity is 44.39778170232743
At time: 88.05121898651123 and batch: 100, loss is 3.6886672306060793 and perplexity is 39.99151196039835
At time: 88.78248476982117 and batch: 150, loss is 3.6998354625701904 and perplexity is 40.440649812041386
At time: 89.51718711853027 and batch: 200, loss is 3.5733364391326905 and perplexity is 35.635290005183606
At time: 90.25267887115479 and batch: 250, loss is 3.700648922920227 and perplexity is 40.47356006096973
At time: 90.9816164970398 and batch: 300, loss is 3.667127070426941 and perplexity is 39.13929973135014
At time: 91.71274423599243 and batch: 350, loss is 3.655027985572815 and perplexity is 38.66860326303132
At time: 92.46099209785461 and batch: 400, loss is 3.582380247116089 and perplexity is 35.9590304420265
At time: 93.20113325119019 and batch: 450, loss is 3.61481641292572 and perplexity is 37.144526044782616
At time: 93.93313431739807 and batch: 500, loss is 3.4852554178237916 and perplexity is 32.6307605234045
At time: 94.66117787361145 and batch: 550, loss is 3.526556749343872 and perplexity is 34.00667229535542
At time: 95.3921570777893 and batch: 600, loss is 3.5416422367095945 and perplexity is 34.52356853390294
At time: 96.11947989463806 and batch: 650, loss is 3.383714237213135 and perplexity is 29.48006396848553
At time: 96.84933400154114 and batch: 700, loss is 3.368017678260803 and perplexity is 29.020941157215333
At time: 97.57806587219238 and batch: 750, loss is 3.459434943199158 and perplexity is 31.79900319415756
At time: 98.30518388748169 and batch: 800, loss is 3.4146048259735107 and perplexity is 30.40493181386717
At time: 99.03270769119263 and batch: 850, loss is 3.455882053375244 and perplexity is 31.68622530164494
At time: 99.75935077667236 and batch: 900, loss is 3.411797466278076 and perplexity is 30.31969393642319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304755641989512 and perplexity of 74.05111825790947
finished 7 epochs...
Completing Train Step...
At time: 101.56329584121704 and batch: 50, loss is 3.6963437032699584 and perplexity is 40.2996870442032
At time: 102.28992938995361 and batch: 100, loss is 3.5861130952835083 and perplexity is 36.09351088429452
At time: 103.01620149612427 and batch: 150, loss is 3.5943748712539674 and perplexity is 36.392942600899225
At time: 103.74181318283081 and batch: 200, loss is 3.4728665113449098 and perplexity is 32.228994939802284
At time: 104.46906590461731 and batch: 250, loss is 3.6010468339920045 and perplexity is 36.63656677983665
At time: 105.19593453407288 and batch: 300, loss is 3.5716376161575316 and perplexity is 35.5748033484045
At time: 105.92201542854309 and batch: 350, loss is 3.564499740600586 and perplexity is 35.321778931823125
At time: 106.64781332015991 and batch: 400, loss is 3.497033166885376 and perplexity is 33.01734953792971
At time: 107.37474536895752 and batch: 450, loss is 3.5352894115447997 and perplexity is 34.30494152381358
At time: 108.10212850570679 and batch: 500, loss is 3.4095614767074585 and perplexity is 30.25197515445631
At time: 108.83111548423767 and batch: 550, loss is 3.4552909755706787 and perplexity is 31.667501811223605
At time: 109.56083512306213 and batch: 600, loss is 3.4759374237060547 and perplexity is 32.32811948224203
At time: 110.33277702331543 and batch: 650, loss is 3.324799294471741 and perplexity is 27.79341978983404
At time: 111.06451439857483 and batch: 700, loss is 3.3139527320861815 and perplexity is 27.49358575203873
At time: 111.79183626174927 and batch: 750, loss is 3.4118514013290406 and perplexity is 30.32132927476151
At time: 112.52101182937622 and batch: 800, loss is 3.3735226345062257 and perplexity is 29.181140709685913
At time: 113.24616384506226 and batch: 850, loss is 3.4205421304702757 and perplexity is 30.585992125462322
At time: 113.97309875488281 and batch: 900, loss is 3.3856025743484497 and perplexity is 29.535784861386777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3151487585616435 and perplexity of 74.82475343877829
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 115.7766547203064 and batch: 50, loss is 3.660518517494202 and perplexity is 38.88149838250042
At time: 116.51659607887268 and batch: 100, loss is 3.5648176002502443 and perplexity is 35.33300808465246
At time: 117.24408960342407 and batch: 150, loss is 3.5763267564773558 and perplexity is 35.74201031524099
At time: 117.97063899040222 and batch: 200, loss is 3.449925923347473 and perplexity is 31.498058951557127
At time: 118.70260787010193 and batch: 250, loss is 3.581388006210327 and perplexity is 35.92336811682523
At time: 119.43669056892395 and batch: 300, loss is 3.5492552137374878 and perplexity is 34.787398660176194
At time: 120.16391944885254 and batch: 350, loss is 3.53719162940979 and perplexity is 34.370259100873014
At time: 120.89329099655151 and batch: 400, loss is 3.4689486408233643 and perplexity is 32.10297294072627
At time: 121.62126469612122 and batch: 450, loss is 3.4967671060562133 and perplexity is 33.00856608305336
At time: 122.34964299201965 and batch: 500, loss is 3.367458291053772 and perplexity is 29.004711753679537
At time: 123.07734417915344 and batch: 550, loss is 3.410012240409851 and perplexity is 30.265614720661357
At time: 123.8050057888031 and batch: 600, loss is 3.4294011306762697 and perplexity is 30.8581572112779
At time: 124.53528785705566 and batch: 650, loss is 3.2686801767349243 and perplexity is 26.276635931687924
At time: 125.2722156047821 and batch: 700, loss is 3.250494289398193 and perplexity is 25.803090959882926
At time: 126.01111674308777 and batch: 750, loss is 3.3453193426132204 and perplexity is 28.36963385325489
At time: 126.7516860961914 and batch: 800, loss is 3.298042726516724 and perplexity is 27.059623970739356
At time: 127.48595595359802 and batch: 850, loss is 3.3429988336563112 and perplexity is 28.303878186607438
At time: 128.21948957443237 and batch: 900, loss is 3.312994222640991 and perplexity is 27.467245516112367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304319616866438 and perplexity of 74.01883714815615
finished 9 epochs...
Completing Train Step...
At time: 130.0411992073059 and batch: 50, loss is 3.636141037940979 and perplexity is 37.94512503221023
At time: 130.77849221229553 and batch: 100, loss is 3.5282473230361937 and perplexity is 34.06421170450289
At time: 131.50331020355225 and batch: 150, loss is 3.5376221132278443 and perplexity is 34.38505812638023
At time: 132.2284595966339 and batch: 200, loss is 3.4126939153671265 and perplexity is 30.346886184844923
At time: 132.95532870292664 and batch: 250, loss is 3.5448371505737306 and perplexity is 34.63404474867289
At time: 133.68113660812378 and batch: 300, loss is 3.512994689941406 and perplexity is 33.54858510970628
At time: 134.41051959991455 and batch: 350, loss is 3.5024443006515504 and perplexity is 33.19649508602575
At time: 135.14085841178894 and batch: 400, loss is 3.4366083097457887 and perplexity is 31.08136084416786
At time: 135.86844420433044 and batch: 450, loss is 3.4671264266967774 and perplexity is 32.04452771595745
At time: 136.59278655052185 and batch: 500, loss is 3.3411655044555664 and perplexity is 28.25203539720415
At time: 137.3267319202423 and batch: 550, loss is 3.3860586643218995 and perplexity is 29.549258909175506
At time: 138.05848002433777 and batch: 600, loss is 3.408630299568176 and perplexity is 30.223818318309952
At time: 138.78386664390564 and batch: 650, loss is 3.251055054664612 and perplexity is 25.81756449480803
At time: 139.5093994140625 and batch: 700, loss is 3.2366190338134766 and perplexity is 25.447538870300086
At time: 140.2352375984192 and batch: 750, loss is 3.3355936002731323 and perplexity is 28.095055507598367
At time: 140.9597635269165 and batch: 800, loss is 3.2916336393356325 and perplexity is 26.886751052175835
At time: 141.68481636047363 and batch: 850, loss is 3.340258550643921 and perplexity is 28.226423722072717
At time: 142.4040069580078 and batch: 900, loss is 3.3131500387191775 and perplexity is 27.471525688038408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307023139849101 and perplexity of 74.21921952266341
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 144.18806338310242 and batch: 50, loss is 3.6281864929199217 and perplexity is 37.644486134870675
At time: 144.914391040802 and batch: 100, loss is 3.5268672132492065 and perplexity is 34.017231778728075
At time: 145.6388897895813 and batch: 150, loss is 3.537813873291016 and perplexity is 34.39165243954145
At time: 146.375337600708 and batch: 200, loss is 3.410315685272217 and perplexity is 30.27480005950614
At time: 147.11161065101624 and batch: 250, loss is 3.5425605249404906 and perplexity is 34.55528568108885
At time: 147.8337426185608 and batch: 300, loss is 3.5112519216537477 and perplexity is 33.49016861747176
At time: 148.5579535961151 and batch: 350, loss is 3.4979396629333497 and perplexity is 33.04729320465771
At time: 149.28838276863098 and batch: 400, loss is 3.432885808944702 and perplexity is 30.965875533963207
At time: 150.0117597579956 and batch: 450, loss is 3.457858557701111 and perplexity is 31.748915196039228
At time: 150.7378957271576 and batch: 500, loss is 3.33127815246582 and perplexity is 27.974073994368858
At time: 151.46090960502625 and batch: 550, loss is 3.371766057014465 and perplexity is 29.129926768528193
At time: 152.18439602851868 and batch: 600, loss is 3.3971348905563357 and perplexity is 29.878372489441983
At time: 152.908052444458 and batch: 650, loss is 3.23417103767395 and perplexity is 25.38531958074717
At time: 153.63155794143677 and batch: 700, loss is 3.2150483417510984 and perplexity is 24.90449580900347
At time: 154.35577869415283 and batch: 750, loss is 3.313674502372742 and perplexity is 27.48593728361844
At time: 155.08133053779602 and batch: 800, loss is 3.269231653213501 and perplexity is 26.291130874783104
At time: 155.80695724487305 and batch: 850, loss is 3.314040598869324 and perplexity is 27.496001631112016
At time: 156.53108286857605 and batch: 900, loss is 3.2873517417907716 and perplexity is 26.77187086697164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303191302573844 and perplexity of 73.93536773500198
finished 11 epochs...
Completing Train Step...
At time: 158.3385362625122 and batch: 50, loss is 3.618760657310486 and perplexity is 37.29132244318456
At time: 159.07885003089905 and batch: 100, loss is 3.513491268157959 and perplexity is 33.56524874332424
At time: 159.80602192878723 and batch: 150, loss is 3.5232370042800905 and perplexity is 33.89396599446887
At time: 160.5331199169159 and batch: 200, loss is 3.396436905860901 and perplexity is 29.857525119138934
At time: 161.26114797592163 and batch: 250, loss is 3.5291640663146975 and perplexity is 34.09545416008804
At time: 161.98994135856628 and batch: 300, loss is 3.49897093296051 and perplexity is 33.08139146685434
At time: 162.71697521209717 and batch: 350, loss is 3.48558735370636 and perplexity is 32.64159364154856
At time: 163.4453477859497 and batch: 400, loss is 3.421504912376404 and perplexity is 30.61545394563959
At time: 164.17260813713074 and batch: 450, loss is 3.448017120361328 and perplexity is 31.4379927081228
At time: 164.90172266960144 and batch: 500, loss is 3.3225120210647585 and perplexity is 27.72992128676172
At time: 165.64080810546875 and batch: 550, loss is 3.3642015790939332 and perplexity is 28.910405409671302
At time: 166.36718487739563 and batch: 600, loss is 3.391384859085083 and perplexity is 29.707063894220568
At time: 167.0931704044342 and batch: 650, loss is 3.229958243370056 and perplexity is 25.278601399720348
At time: 167.81999969482422 and batch: 700, loss is 3.2125760555267333 and perplexity is 24.84300081502582
At time: 168.55273175239563 and batch: 750, loss is 3.3125221014022825 and perplexity is 27.454280706857105
At time: 169.28594374656677 and batch: 800, loss is 3.2695342302322388 and perplexity is 26.299087170417845
At time: 170.02324080467224 and batch: 850, loss is 3.3164796876907348 and perplexity is 27.563148676839262
At time: 170.75539445877075 and batch: 900, loss is 3.2907559871673584 and perplexity is 26.86316418886194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303310446543236 and perplexity of 73.94417721298002
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 172.56724762916565 and batch: 50, loss is 3.6162230014801025 and perplexity is 37.19680987227157
At time: 173.30535197257996 and batch: 100, loss is 3.5128087329864504 and perplexity is 33.54234709699511
At time: 174.03077244758606 and batch: 150, loss is 3.5237652826309205 and perplexity is 33.91187617329085
At time: 174.7660608291626 and batch: 200, loss is 3.395559105873108 and perplexity is 29.83132768369025
At time: 175.4964988231659 and batch: 250, loss is 3.5273270750045778 and perplexity is 34.03287860005833
At time: 176.22368907928467 and batch: 300, loss is 3.4972714138031007 and perplexity is 33.025216756821685
At time: 176.94921731948853 and batch: 350, loss is 3.4828953456878664 and perplexity is 32.55384037896734
At time: 177.67487907409668 and batch: 400, loss is 3.419580569267273 and perplexity is 30.55659595745145
At time: 178.39898252487183 and batch: 450, loss is 3.445539059638977 and perplexity is 31.360183900495514
At time: 179.1335117816925 and batch: 500, loss is 3.3194034242630006 and perplexity is 27.643853985713445
At time: 179.86004877090454 and batch: 550, loss is 3.359283437728882 and perplexity is 28.76856902054793
At time: 180.5921106338501 and batch: 600, loss is 3.3870830869674684 and perplexity is 29.579545349570886
At time: 181.3177318572998 and batch: 650, loss is 3.2244802856445314 and perplexity is 25.14050487867788
At time: 182.0449891090393 and batch: 700, loss is 3.2051054859161376 and perplexity is 24.658100961642443
At time: 182.77077388763428 and batch: 750, loss is 3.3049905729293823 and perplexity is 27.248284716086086
At time: 183.50831174850464 and batch: 800, loss is 3.261289482116699 and perplexity is 26.083149221241705
At time: 184.23341727256775 and batch: 850, loss is 3.306123414039612 and perplexity is 27.279170184056603
At time: 184.9584813117981 and batch: 900, loss is 3.280012512207031 and perplexity is 26.57610522352429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303797891695205 and perplexity of 73.98022972977726
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 186.76134133338928 and batch: 50, loss is 3.61384063243866 and perplexity is 37.10829881885157
At time: 187.487726688385 and batch: 100, loss is 3.510379786491394 and perplexity is 33.46097339676712
At time: 188.21265745162964 and batch: 150, loss is 3.5219531631469727 and perplexity is 33.8504794476425
At time: 188.93891024589539 and batch: 200, loss is 3.3938063192367554 and perplexity is 29.779085529231843
At time: 189.66690039634705 and batch: 250, loss is 3.5255328702926634 and perplexity is 33.971871394987545
At time: 190.3945779800415 and batch: 300, loss is 3.495239028930664 and perplexity is 32.95816496647863
At time: 191.13184309005737 and batch: 350, loss is 3.4807911014556883 and perplexity is 32.48541116924906
At time: 191.86765027046204 and batch: 400, loss is 3.4181201791763307 and perplexity is 30.512003976271235
At time: 192.60423803329468 and batch: 450, loss is 3.4446978855133055 and perplexity is 31.333815616935844
At time: 193.334486246109 and batch: 500, loss is 3.3175989627838134 and perplexity is 27.59401669431908
At time: 194.06982040405273 and batch: 550, loss is 3.357392039299011 and perplexity is 28.714207620012623
At time: 194.8047001361847 and batch: 600, loss is 3.3852978324890137 and perplexity is 29.526785442707656
At time: 195.5342390537262 and batch: 650, loss is 3.2225882053375243 and perplexity is 25.09298199722032
At time: 196.26795363426208 and batch: 700, loss is 3.202974195480347 and perplexity is 24.60560335059838
At time: 197.00591468811035 and batch: 750, loss is 3.3028761196136474 and perplexity is 27.190730359563435
At time: 197.74665117263794 and batch: 800, loss is 3.2587865352630616 and perplexity is 26.01794611895535
At time: 198.48487067222595 and batch: 850, loss is 3.3033882331848146 and perplexity is 27.20465866772393
At time: 199.21914291381836 and batch: 900, loss is 3.27677321434021 and perplexity is 26.490156584342365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303226836740154 and perplexity of 73.93799501333405
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 201.08006834983826 and batch: 50, loss is 3.6131957530975343 and perplexity is 37.08437615800292
At time: 201.84082984924316 and batch: 100, loss is 3.5095982503890992 and perplexity is 33.43483265433716
At time: 202.57835626602173 and batch: 150, loss is 3.521334147453308 and perplexity is 33.82953195370898
At time: 203.32133722305298 and batch: 200, loss is 3.3932538509368895 and perplexity is 29.762638072245586
At time: 204.05624437332153 and batch: 250, loss is 3.5251068830490113 and perplexity is 33.957402893047714
At time: 204.80281972885132 and batch: 300, loss is 3.4945646572113036 and perplexity is 32.93594640473011
At time: 205.54204773902893 and batch: 350, loss is 3.4801944398880007 and perplexity is 32.46603415422878
At time: 206.28018498420715 and batch: 400, loss is 3.4176624393463135 and perplexity is 30.498040612795272
At time: 207.0128993988037 and batch: 450, loss is 3.4444052600860595 and perplexity is 31.324647887174528
At time: 207.74925756454468 and batch: 500, loss is 3.317090096473694 and perplexity is 27.57997860092178
At time: 208.4862778186798 and batch: 550, loss is 3.356895394325256 and perplexity is 28.699950393805707
At time: 209.22665333747864 and batch: 600, loss is 3.3847930240631103 and perplexity is 29.51188383417156
At time: 209.96351051330566 and batch: 650, loss is 3.2220608139038087 and perplexity is 25.07975166255785
At time: 210.6913206577301 and batch: 700, loss is 3.202396101951599 and perplexity is 24.591383121237772
At time: 211.41675877571106 and batch: 750, loss is 3.302283344268799 and perplexity is 27.174617141228968
At time: 212.14325070381165 and batch: 800, loss is 3.258118257522583 and perplexity is 26.000564713161243
At time: 212.8694784641266 and batch: 850, loss is 3.302679805755615 and perplexity is 27.185392966303105
At time: 213.60297536849976 and batch: 900, loss is 3.2759609746932985 and perplexity is 26.468648964764775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303060871281036 and perplexity of 73.92572487828286
finished 15 epochs...
Completing Train Step...
At time: 215.4525055885315 and batch: 50, loss is 3.6129773330688475 and perplexity is 37.076277072032156
At time: 216.20946264266968 and batch: 100, loss is 3.509298529624939 and perplexity is 33.42481304236231
At time: 216.9445996284485 and batch: 150, loss is 3.521009950637817 and perplexity is 33.81856630479031
At time: 217.6941635608673 and batch: 200, loss is 3.392946720123291 and perplexity is 29.753498452600876
At time: 218.43629217147827 and batch: 250, loss is 3.5248443412780763 and perplexity is 33.94848882656443
At time: 219.1765956878662 and batch: 300, loss is 3.494288191795349 and perplexity is 32.926842013190154
At time: 219.9094729423523 and batch: 350, loss is 3.479950132369995 and perplexity is 32.458103426812684
At time: 220.651132106781 and batch: 400, loss is 3.4174186897277834 and perplexity is 30.49060763295979
At time: 221.3804111480713 and batch: 450, loss is 3.4441723680496215 and perplexity is 31.317353475575537
At time: 222.10878324508667 and batch: 500, loss is 3.316899175643921 and perplexity is 27.574713511145823
At time: 222.83930277824402 and batch: 550, loss is 3.3567303037643432 and perplexity is 28.69521269398254
At time: 223.57744097709656 and batch: 600, loss is 3.3846913814544677 and perplexity is 29.50888432175441
At time: 224.30771565437317 and batch: 650, loss is 3.2219875764846804 and perplexity is 25.07791495353244
At time: 225.0359764099121 and batch: 700, loss is 3.2023625564575195 and perplexity is 24.590558204977064
At time: 225.76427912712097 and batch: 750, loss is 3.3022768259048463 and perplexity is 27.17444000776148
At time: 226.49171137809753 and batch: 800, loss is 3.258200149536133 and perplexity is 26.002694038945236
At time: 227.21900916099548 and batch: 850, loss is 3.3028568410873413 and perplexity is 27.190206167405762
At time: 227.94584226608276 and batch: 900, loss is 3.276191487312317 and perplexity is 26.474751025633523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302921242909889 and perplexity of 73.91540347032971
finished 16 epochs...
Completing Train Step...
At time: 229.78538632392883 and batch: 50, loss is 3.612771625518799 and perplexity is 37.06865098630934
At time: 230.51416730880737 and batch: 100, loss is 3.509021224975586 and perplexity is 33.41554547133157
At time: 231.25039792060852 and batch: 150, loss is 3.520708646774292 and perplexity is 33.80837817504229
At time: 231.98796224594116 and batch: 200, loss is 3.392659478187561 and perplexity is 29.744953227442878
At time: 232.71825122833252 and batch: 250, loss is 3.52459107875824 and perplexity is 33.93989203540802
At time: 233.44373774528503 and batch: 300, loss is 3.4940265226364136 and perplexity is 32.91822720129861
At time: 234.16852974891663 and batch: 350, loss is 3.479718189239502 and perplexity is 32.45057586571079
At time: 234.90087819099426 and batch: 400, loss is 3.4171886587142946 and perplexity is 30.483594654216315
At time: 235.63045740127563 and batch: 450, loss is 3.4439543151855467 and perplexity is 31.310525381424743
At time: 236.36174631118774 and batch: 500, loss is 3.316719994544983 and perplexity is 27.569773086304586
At time: 237.0869128704071 and batch: 550, loss is 3.3565744829177855 and perplexity is 28.690741729992155
At time: 237.8129346370697 and batch: 600, loss is 3.3845952558517456 and perplexity is 29.506047898791945
At time: 238.55204057693481 and batch: 650, loss is 3.2219199895858766 and perplexity is 25.076220072308793
At time: 239.28501653671265 and batch: 700, loss is 3.2023352575302124 and perplexity is 24.58988691827893
At time: 240.0099127292633 and batch: 750, loss is 3.302275834083557 and perplexity is 27.174413055586726
At time: 240.73484063148499 and batch: 800, loss is 3.2582776069641115 and perplexity is 26.0047082187516
At time: 241.45992064476013 and batch: 850, loss is 3.303024744987488 and perplexity is 27.194771892357846
At time: 242.18358945846558 and batch: 900, loss is 3.2764078617095946 and perplexity is 26.480480103720325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3028020989404965 and perplexity of 73.90659742036529
finished 17 epochs...
Completing Train Step...
At time: 243.98492336273193 and batch: 50, loss is 3.61257643699646 and perplexity is 37.06141631718339
At time: 244.72980976104736 and batch: 100, loss is 3.5087621116638186 and perplexity is 33.406888180338086
At time: 245.4592092037201 and batch: 150, loss is 3.5204260635375975 and perplexity is 33.798825843838095
At time: 246.18714880943298 and batch: 200, loss is 3.392388663291931 and perplexity is 29.73689894169857
At time: 246.91429686546326 and batch: 250, loss is 3.5243459844589236 and perplexity is 33.93157458067274
At time: 247.64148116111755 and batch: 300, loss is 3.4937770223617552 and perplexity is 32.91001511907161
At time: 248.3680820465088 and batch: 350, loss is 3.479496273994446 and perplexity is 32.44337538719287
At time: 249.09251403808594 and batch: 400, loss is 3.4169697904586793 and perplexity is 30.476923493108796
At time: 249.82754778862 and batch: 450, loss is 3.4437483024597166 and perplexity is 31.30407567912693
At time: 250.55685019493103 and batch: 500, loss is 3.3165501451492307 and perplexity is 27.565090774660014
At time: 251.28112840652466 and batch: 550, loss is 3.3564260721206667 and perplexity is 28.68648403009421
At time: 252.0154480934143 and batch: 600, loss is 3.384503335952759 and perplexity is 29.503335830498024
At time: 252.73910999298096 and batch: 650, loss is 3.221856589317322 and perplexity is 25.07463028361892
At time: 253.4619903564453 and batch: 700, loss is 3.202312026023865 and perplexity is 24.589315664800466
At time: 254.1857419013977 and batch: 750, loss is 3.3022785663604735 and perplexity is 27.174487303709665
At time: 254.9084711074829 and batch: 800, loss is 3.2583509492874145 and perplexity is 26.006615534411804
At time: 255.64303946495056 and batch: 850, loss is 3.3031840229034426 and perplexity is 27.199103763926303
At time: 256.3696472644806 and batch: 900, loss is 3.276610879898071 and perplexity is 26.48585666857273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302700094980736 and perplexity of 73.89905903925488
finished 18 epochs...
Completing Train Step...
At time: 258.2011909484863 and batch: 50, loss is 3.612390151023865 and perplexity is 37.054512938220284
At time: 258.94613885879517 and batch: 100, loss is 3.508518223762512 and perplexity is 33.39874163795255
At time: 259.6788282394409 and batch: 150, loss is 3.520159478187561 and perplexity is 33.78981677291622
At time: 260.409152507782 and batch: 200, loss is 3.392131724357605 and perplexity is 29.72925935606985
At time: 261.1362919807434 and batch: 250, loss is 3.5241081953048705 and perplexity is 33.92350697948955
At time: 261.8669664859772 and batch: 300, loss is 3.493537883758545 and perplexity is 32.90214600496638
At time: 262.5979142189026 and batch: 350, loss is 3.4792827463150022 and perplexity is 32.43644856809325
At time: 263.33091044425964 and batch: 400, loss is 3.416759886741638 and perplexity is 30.47052694493672
At time: 264.07110118865967 and batch: 450, loss is 3.443552303314209 and perplexity is 31.297940708287072
At time: 264.799298286438 and batch: 500, loss is 3.316388154029846 and perplexity is 27.560625836399424
At time: 265.5283558368683 and batch: 550, loss is 3.35628381729126 and perplexity is 28.68240352944457
At time: 266.2587242126465 and batch: 600, loss is 3.384414792060852 and perplexity is 29.500723605969323
At time: 266.99555373191833 and batch: 650, loss is 3.2217965221405027 and perplexity is 25.07312416660255
At time: 267.72385573387146 and batch: 700, loss is 3.202291769981384 and perplexity is 24.588817587622344
At time: 268.45751309394836 and batch: 750, loss is 3.3022838544845583 and perplexity is 27.174631006150424
At time: 269.19075107574463 and batch: 800, loss is 3.258420162200928 and perplexity is 26.0084155903364
At time: 269.92665672302246 and batch: 850, loss is 3.303334879875183 and perplexity is 27.20320724786598
At time: 270.6542043685913 and batch: 900, loss is 3.2768017292022704 and perplexity is 26.490911958272967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302610632491438 and perplexity of 73.89244814119459
finished 19 epochs...
Completing Train Step...
At time: 272.4709916114807 and batch: 50, loss is 3.612211112976074 and perplexity is 37.047879364410754
At time: 273.1969168186188 and batch: 100, loss is 3.508286919593811 and perplexity is 33.3910172631568
At time: 273.9247086048126 and batch: 150, loss is 3.5199065351486207 and perplexity is 33.7812709548246
At time: 274.65652871131897 and batch: 200, loss is 3.391886577606201 and perplexity is 29.721972217962676
At time: 275.40329456329346 and batch: 250, loss is 3.5238773012161255 and perplexity is 33.91567514645526
At time: 276.13087368011475 and batch: 300, loss is 3.493307704925537 and perplexity is 32.89457349894424
At time: 276.8577778339386 and batch: 350, loss is 3.4790764665603637 and perplexity is 32.42975827550096
At time: 277.58496475219727 and batch: 400, loss is 3.4165579223632814 and perplexity is 30.4643736053027
At time: 278.31338024139404 and batch: 450, loss is 3.443364729881287 and perplexity is 31.292070596660736
At time: 279.04072666168213 and batch: 500, loss is 3.316232671737671 and perplexity is 27.556340980238865
At time: 279.76642513275146 and batch: 550, loss is 3.356146831512451 and perplexity is 28.678474717160835
At time: 280.4993414878845 and batch: 600, loss is 3.384328842163086 and perplexity is 29.498188130754844
At time: 281.22801303863525 and batch: 650, loss is 3.2217391204833983 and perplexity is 25.07168496903316
At time: 281.952360868454 and batch: 700, loss is 3.2022732496261597 and perplexity is 24.58836219820307
At time: 282.6782217025757 and batch: 750, loss is 3.3022908115386964 and perplexity is 27.174820062187155
At time: 283.40296626091003 and batch: 800, loss is 3.258485975265503 and perplexity is 26.010127340198277
At time: 284.1276316642761 and batch: 850, loss is 3.3034780073165892 and perplexity is 27.20710105196587
At time: 284.8517520427704 and batch: 900, loss is 3.2769813394546508 and perplexity is 26.49567042497749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302532875374572 and perplexity of 73.88670270084627
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc267f18b70>
ELAPSED
587.9964320659637


RESULTS SO FAR:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.88670270084627, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.23850096980998603, 'data': 'ptb', 'dropout': 0.3583059732624252, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0229535102844238 and batch: 50, loss is 7.048715553283691 and perplexity is 1151.3789077449328
At time: 1.8347406387329102 and batch: 100, loss is 6.0984412288665775 and perplexity is 445.16332124491345
At time: 2.6247212886810303 and batch: 150, loss is 5.840707540512085 and perplexity is 344.0226645463057
At time: 3.415287733078003 and batch: 200, loss is 5.603260307312012 and perplexity is 271.30951944567175
At time: 4.2053139209747314 and batch: 250, loss is 5.5900389671325685 and perplexity is 267.7460528063064
At time: 4.999018669128418 and batch: 300, loss is 5.46019172668457 and perplexity is 235.1425031362267
At time: 5.790956735610962 and batch: 350, loss is 5.407593507766723 and perplexity is 223.0940670307196
At time: 6.578669786453247 and batch: 400, loss is 5.240018882751465 and perplexity is 188.67366504995468
At time: 7.365718364715576 and batch: 450, loss is 5.215303201675415 and perplexity is 184.0676221311375
At time: 8.153410196304321 and batch: 500, loss is 5.141400794982911 and perplexity is 170.95507368283506
At time: 8.939726829528809 and batch: 550, loss is 5.1884771347045895 and perplexity is 179.19545449979614
At time: 9.736541032791138 and batch: 600, loss is 5.08215482711792 and perplexity is 161.12086975189976
At time: 10.526411294937134 and batch: 650, loss is 4.967270965576172 and perplexity is 143.6343688791765
At time: 11.322452306747437 and batch: 700, loss is 5.042360143661499 and perplexity is 154.8350170118775
At time: 12.105328798294067 and batch: 750, loss is 5.024592485427856 and perplexity is 152.1082571494553
At time: 12.889331579208374 and batch: 800, loss is 4.981900215148926 and perplexity is 145.7510770845755
At time: 13.672610998153687 and batch: 850, loss is 5.0163896369934085 and perplexity is 150.86563963960842
At time: 14.462753295898438 and batch: 900, loss is 4.9276957511901855 and perplexity is 138.06101859868258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.892247030179795 and perplexity of 133.25266064714697
finished 1 epochs...
Completing Train Step...
At time: 16.31265616416931 and batch: 50, loss is 4.832617511749268 and perplexity is 125.53913112307642
At time: 17.031415224075317 and batch: 100, loss is 4.709787216186523 and perplexity is 111.02853231749293
At time: 17.76390027999878 and batch: 150, loss is 4.695567941665649 and perplexity is 109.46095843046952
At time: 18.485475540161133 and batch: 200, loss is 4.576799364089966 and perplexity is 97.20278507491088
At time: 19.204357624053955 and batch: 250, loss is 4.69095398902893 and perplexity is 108.95707409584261
At time: 19.924257040023804 and batch: 300, loss is 4.623647079467774 and perplexity is 101.86486474719696
At time: 20.647162437438965 and batch: 350, loss is 4.616556043624878 and perplexity is 101.14509232218634
At time: 21.373976945877075 and batch: 400, loss is 4.496417584419251 and perplexity is 89.69522946415944
At time: 22.100990295410156 and batch: 450, loss is 4.521865425109863 and perplexity is 92.00707029326136
At time: 22.82775378227234 and batch: 500, loss is 4.415986700057983 and perplexity is 82.76346334513497
At time: 23.5647931098938 and batch: 550, loss is 4.482408590316773 and perplexity is 88.44744999769487
At time: 24.294064044952393 and batch: 600, loss is 4.448326454162598 and perplexity is 85.483763229707
At time: 25.02633762359619 and batch: 650, loss is 4.3014716529846195 and perplexity is 73.808334068249
At time: 25.7624351978302 and batch: 700, loss is 4.339067769050598 and perplexity is 76.63606351723462
At time: 26.49746060371399 and batch: 750, loss is 4.395796890258789 and perplexity is 81.10924017939776
At time: 27.22913646697998 and batch: 800, loss is 4.352516222000122 and perplexity is 77.67366141658864
At time: 27.954299211502075 and batch: 850, loss is 4.4144371509552 and perplexity is 82.63531660523928
At time: 28.679905652999878 and batch: 900, loss is 4.34669545173645 and perplexity is 77.2228541734719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5055040333369005 and perplexity of 90.51395460385844
finished 2 epochs...
Completing Train Step...
At time: 30.492063522338867 and batch: 50, loss is 4.398416137695312 and perplexity is 81.32196381511964
At time: 31.22044587135315 and batch: 100, loss is 4.272859792709351 and perplexity is 71.7264654171067
At time: 31.938649892807007 and batch: 150, loss is 4.275376954078674 and perplexity is 71.90723992887106
At time: 32.65841197967529 and batch: 200, loss is 4.159303488731385 and perplexity is 64.02691160028391
At time: 33.384764671325684 and batch: 250, loss is 4.305373830795288 and perplexity is 74.09690998277922
At time: 34.107341051101685 and batch: 300, loss is 4.259427213668824 and perplexity is 70.76943607724337
At time: 34.828150272369385 and batch: 350, loss is 4.2591518115997316 and perplexity is 70.7499487116728
At time: 35.553693532943726 and batch: 400, loss is 4.168945345878601 and perplexity is 64.64723566781743
At time: 36.27380180358887 and batch: 450, loss is 4.206496729850769 and perplexity is 67.120984502695
At time: 36.99289608001709 and batch: 500, loss is 4.082317447662353 and perplexity is 59.282695325738935
At time: 37.72475028038025 and batch: 550, loss is 4.155459623336792 and perplexity is 63.78127317343365
At time: 38.45523762702942 and batch: 600, loss is 4.154958901405334 and perplexity is 63.74934448550362
At time: 39.180171966552734 and batch: 650, loss is 4.001106128692627 and perplexity is 54.65857602676189
At time: 39.89833617210388 and batch: 700, loss is 4.01782078742981 and perplexity is 55.57985344215005
At time: 40.618919372558594 and batch: 750, loss is 4.107082958221436 and perplexity is 60.769192498112446
At time: 41.33981943130493 and batch: 800, loss is 4.072863049507141 and perplexity is 58.724854301233876
At time: 42.06186580657959 and batch: 850, loss is 4.14263078212738 and perplexity is 62.96825950974913
At time: 42.780664682388306 and batch: 900, loss is 4.082994480133056 and perplexity is 59.32284522528446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380718074432791 and perplexity of 79.89538364404291
finished 3 epochs...
Completing Train Step...
At time: 44.56704044342041 and batch: 50, loss is 4.154327740669251 and perplexity is 63.709121097360246
At time: 45.29585886001587 and batch: 100, loss is 4.032925209999084 and perplexity is 56.42572717033336
At time: 46.01324105262756 and batch: 150, loss is 4.036790208816528 and perplexity is 56.644234532459116
At time: 46.734177350997925 and batch: 200, loss is 3.924615840911865 and perplexity is 50.63362295572693
At time: 47.45332932472229 and batch: 250, loss is 4.077432312965393 and perplexity is 58.9937976005585
At time: 48.179563760757446 and batch: 300, loss is 4.039403877258301 and perplexity is 56.79247742515964
At time: 48.89494729042053 and batch: 350, loss is 4.040286993980407 and perplexity is 56.842653964273
At time: 49.61024856567383 and batch: 400, loss is 3.967281656265259 and perplexity is 52.840696256719475
At time: 50.32534837722778 and batch: 450, loss is 4.003626127243042 and perplexity is 54.79648925670322
At time: 51.04455876350403 and batch: 500, loss is 3.877833843231201 and perplexity is 48.31943415700941
At time: 51.765992641448975 and batch: 550, loss is 3.94433557510376 and perplexity is 51.64201447052254
At time: 52.4915611743927 and batch: 600, loss is 3.9613518285751343 and perplexity is 52.52828721421868
At time: 53.207963705062866 and batch: 650, loss is 3.8070433855056764 and perplexity is 45.01714357284067
At time: 53.92387342453003 and batch: 700, loss is 3.818677821159363 and perplexity is 45.54395124565605
At time: 54.63925051689148 and batch: 750, loss is 3.9152195978164674 and perplexity is 50.160085346901404
At time: 55.3548858165741 and batch: 800, loss is 3.886152639389038 and perplexity is 48.72307023564191
At time: 56.07004141807556 and batch: 850, loss is 3.954295802116394 and perplexity is 52.15895078596826
At time: 56.785988569259644 and batch: 900, loss is 3.8993384075164794 and perplexity is 49.369775625990464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342895925861516 and perplexity of 76.93000064549445
finished 4 epochs...
Completing Train Step...
At time: 58.581621408462524 and batch: 50, loss is 3.980545735359192 and perplexity is 53.5462483362492
At time: 59.297755002975464 and batch: 100, loss is 3.8596296882629395 and perplexity is 47.44777764621664
At time: 60.01240277290344 and batch: 150, loss is 3.8685730838775636 and perplexity is 47.874025101463346
At time: 60.72647404670715 and batch: 200, loss is 3.7560536956787107 and perplexity is 42.7792724049935
At time: 61.442190408706665 and batch: 250, loss is 3.911903462409973 and perplexity is 49.994023206340444
At time: 62.155256271362305 and batch: 300, loss is 3.8782225275039672 and perplexity is 48.3382188115486
At time: 62.86900520324707 and batch: 350, loss is 3.880205054283142 and perplexity is 48.434145682175696
At time: 63.58241820335388 and batch: 400, loss is 3.813041772842407 and perplexity is 45.28798533158367
At time: 64.29622483253479 and batch: 450, loss is 3.844042158126831 and perplexity is 46.713918367895694
At time: 65.00985479354858 and batch: 500, loss is 3.7243704271316527 and perplexity is 41.44513179896349
At time: 65.74323868751526 and batch: 550, loss is 3.7860813474655153 and perplexity is 44.083314174509944
At time: 66.45811486244202 and batch: 600, loss is 3.811838541030884 and perplexity is 45.233526157028884
At time: 67.17341113090515 and batch: 650, loss is 3.6595465898513795 and perplexity is 38.84372673805418
At time: 67.88855504989624 and batch: 700, loss is 3.668294267654419 and perplexity is 39.185009684557926
At time: 68.60266041755676 and batch: 750, loss is 3.7664629650115966 and perplexity is 43.22689906475685
At time: 69.31492304801941 and batch: 800, loss is 3.739648241996765 and perplexity is 42.0831844642008
At time: 70.027756690979 and batch: 850, loss is 3.8072669506073 and perplexity is 45.027208960211084
At time: 70.74169635772705 and batch: 900, loss is 3.7560847997665405 and perplexity is 42.7806030359336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340599582619863 and perplexity of 76.75354563660544
finished 5 epochs...
Completing Train Step...
At time: 72.53175520896912 and batch: 50, loss is 3.8445278215408325 and perplexity is 46.73661111904509
At time: 73.27237010002136 and batch: 100, loss is 3.7227224588394163 and perplexity is 41.37688778332295
At time: 73.99431943893433 and batch: 150, loss is 3.7359445524215698 and perplexity is 41.92760969078194
At time: 74.71102833747864 and batch: 200, loss is 3.6222233390808105 and perplexity is 37.42067424842308
At time: 75.42884016036987 and batch: 250, loss is 3.781129856109619 and perplexity is 43.865575535792175
At time: 76.14692854881287 and batch: 300, loss is 3.749099316596985 and perplexity is 42.48280120925143
At time: 76.86395049095154 and batch: 350, loss is 3.7502272081375123 and perplexity is 42.530744233536495
At time: 77.58402109146118 and batch: 400, loss is 3.6876230382919313 and perplexity is 39.949774925515946
At time: 78.30277943611145 and batch: 450, loss is 3.7155457639694216 and perplexity is 41.081001497792016
At time: 79.02084851264954 and batch: 500, loss is 3.603453483581543 and perplexity is 36.7248443422107
At time: 79.73994302749634 and batch: 550, loss is 3.658632836341858 and perplexity is 38.808249357683266
At time: 80.4571213722229 and batch: 600, loss is 3.6874238634109497 and perplexity is 39.94181872621375
At time: 81.17437958717346 and batch: 650, loss is 3.5391715955734253 and perplexity is 34.438378465693
At time: 81.89197158813477 and batch: 700, loss is 3.5473727655410765 and perplexity is 34.72197478216082
At time: 82.6090784072876 and batch: 750, loss is 3.6465032148361205 and perplexity is 38.34036335828378
At time: 83.3249864578247 and batch: 800, loss is 3.618974642753601 and perplexity is 37.299303097183355
At time: 84.05255770683289 and batch: 850, loss is 3.6873786640167237 and perplexity is 39.94001342100269
At time: 84.76902437210083 and batch: 900, loss is 3.6383449411392212 and perplexity is 38.02884463569052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34989364833048 and perplexity of 77.47022339952832
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 86.5545723438263 and batch: 50, loss is 3.7635988140106202 and perplexity is 43.103267832306464
At time: 87.29190635681152 and batch: 100, loss is 3.6483645391464234 and perplexity is 38.411793665529395
At time: 88.00957536697388 and batch: 150, loss is 3.6672316074371336 and perplexity is 39.14339145058941
At time: 88.73281526565552 and batch: 200, loss is 3.535545587539673 and perplexity is 34.31373075208409
At time: 89.44257020950317 and batch: 250, loss is 3.692379188537598 and perplexity is 40.14023462578566
At time: 90.15354490280151 and batch: 300, loss is 3.639064755439758 and perplexity is 38.05622819624907
At time: 90.86412858963013 and batch: 350, loss is 3.6383708715438843 and perplexity is 38.02983075180593
At time: 91.57718849182129 and batch: 400, loss is 3.5616914796829224 and perplexity is 35.222725309811715
At time: 92.2933464050293 and batch: 450, loss is 3.5780427932739256 and perplexity is 35.803397576460526
At time: 93.0101387500763 and batch: 500, loss is 3.4592830419540403 and perplexity is 31.79417325282521
At time: 93.72695326805115 and batch: 550, loss is 3.491048803329468 and perplexity is 32.82035175573311
At time: 94.44356322288513 and batch: 600, loss is 3.5189275598526 and perplexity is 33.74821610767099
At time: 95.15892815589905 and batch: 650, loss is 3.3564052629470824 and perplexity is 28.685887094279398
At time: 95.87512254714966 and batch: 700, loss is 3.3426636028289796 and perplexity is 28.294391444319324
At time: 96.59861016273499 and batch: 750, loss is 3.42718695640564 and perplexity is 30.789907459846685
At time: 97.32398080825806 and batch: 800, loss is 3.384286413192749 and perplexity is 29.49693657955685
At time: 98.04524683952332 and batch: 850, loss is 3.432028555870056 and perplexity is 30.939341316847177
At time: 98.76437830924988 and batch: 900, loss is 3.3734194374084474 and perplexity is 29.178129456033805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311410982314855 and perplexity of 74.5455972893221
finished 7 epochs...
Completing Train Step...
At time: 100.58899855613708 and batch: 50, loss is 3.6721830415725707 and perplexity is 39.337688001522956
At time: 101.30687284469604 and batch: 100, loss is 3.5437198781967165 and perplexity is 34.595370695919414
At time: 102.0508713722229 and batch: 150, loss is 3.561789698600769 and perplexity is 35.22618501767685
At time: 102.77422451972961 and batch: 200, loss is 3.433693361282349 and perplexity is 30.990892198911535
At time: 103.49378371238708 and batch: 250, loss is 3.592302942276001 and perplexity is 36.31761707005304
At time: 104.21452355384827 and batch: 300, loss is 3.5444424247741697 and perplexity is 34.620376495451005
At time: 104.93294382095337 and batch: 350, loss is 3.5475776290893553 and perplexity is 34.729088777792256
At time: 105.65721678733826 and batch: 400, loss is 3.4744686222076417 and perplexity is 32.280670746822885
At time: 106.38240766525269 and batch: 450, loss is 3.4948971033096314 and perplexity is 32.94689765185593
At time: 107.10296201705933 and batch: 500, loss is 3.383026428222656 and perplexity is 29.45979428708177
At time: 107.82378506660461 and batch: 550, loss is 3.4158908843994142 and perplexity is 30.444059487459647
At time: 108.54406309127808 and batch: 600, loss is 3.4518980979919434 and perplexity is 31.560239920482296
At time: 109.26232886314392 and batch: 650, loss is 3.2950491094589234 and perplexity is 26.978738948639712
At time: 109.98013997077942 and batch: 700, loss is 3.2865795135498046 and perplexity is 26.75120485269156
At time: 110.69905734062195 and batch: 750, loss is 3.379076056480408 and perplexity is 29.343646712290763
At time: 111.41995048522949 and batch: 800, loss is 3.343693175315857 and perplexity is 28.32353757273467
At time: 112.13829779624939 and batch: 850, loss is 3.399343605041504 and perplexity is 29.9444382168592
At time: 112.85785150527954 and batch: 900, loss is 3.346588282585144 and perplexity is 28.405656065819212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323024802011986 and perplexity of 75.41640331967265
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 114.6573417186737 and batch: 50, loss is 3.639357624053955 and perplexity is 38.067375303301695
At time: 115.3899552822113 and batch: 100, loss is 3.5252773571014404 and perplexity is 33.96319224258178
At time: 116.10988688468933 and batch: 150, loss is 3.5479594469070435 and perplexity is 34.74235149448934
At time: 116.83037638664246 and batch: 200, loss is 3.419785761833191 and perplexity is 30.56286658710296
At time: 117.54832243919373 and batch: 250, loss is 3.575852746963501 and perplexity is 35.725072277020054
At time: 118.2675552368164 and batch: 300, loss is 3.516869888305664 and perplexity is 33.6788447598273
At time: 118.98668909072876 and batch: 350, loss is 3.52129271030426 and perplexity is 33.828130183394066
At time: 119.72779941558838 and batch: 400, loss is 3.442850284576416 and perplexity is 31.275976677938008
At time: 120.44729781150818 and batch: 450, loss is 3.4599333810806274 and perplexity is 31.814856972676164
At time: 121.16798448562622 and batch: 500, loss is 3.3465185546875 and perplexity is 28.403675468192805
At time: 121.88677191734314 and batch: 550, loss is 3.377621474266052 and perplexity is 29.300994993414815
At time: 122.6079773902893 and batch: 600, loss is 3.4108792781829833 and perplexity is 30.291867531295832
At time: 123.32797288894653 and batch: 650, loss is 3.2428793334960937 and perplexity is 25.607347793762653
At time: 124.04782366752625 and batch: 700, loss is 3.2252360868453978 and perplexity is 25.15951328483882
At time: 124.76633739471436 and batch: 750, loss is 3.3117875480651855 and perplexity is 27.43412147826704
At time: 125.48563623428345 and batch: 800, loss is 3.272842540740967 and perplexity is 26.38623679639831
At time: 126.2048122882843 and batch: 850, loss is 3.32334436416626 and perplexity is 27.753011703691474
At time: 126.92474365234375 and batch: 900, loss is 3.2750353717803957 and perplexity is 26.44416084107003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317549196008134 and perplexity of 75.00458132530467
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 128.71763348579407 and batch: 50, loss is 3.6249687480926513 and perplexity is 37.523550458754194
At time: 129.4499933719635 and batch: 100, loss is 3.509553337097168 and perplexity is 33.43333101965941
At time: 130.16914343833923 and batch: 150, loss is 3.530485601425171 and perplexity is 34.14054228607689
At time: 130.88795614242554 and batch: 200, loss is 3.4067613649368287 and perplexity is 30.167384729334202
At time: 131.60707545280457 and batch: 250, loss is 3.5627306509017944 and perplexity is 35.25934677689564
At time: 132.32663488388062 and batch: 300, loss is 3.5010635662078857 and perplexity is 33.150691170650155
At time: 133.04961037635803 and batch: 350, loss is 3.506107168197632 and perplexity is 33.318312414692045
At time: 133.77783465385437 and batch: 400, loss is 3.427594666481018 and perplexity is 30.8024633747506
At time: 134.49816608428955 and batch: 450, loss is 3.4389583587646486 and perplexity is 31.154489459984653
At time: 135.2177631855011 and batch: 500, loss is 3.3310428524017333 and perplexity is 27.967492467312447
At time: 135.93753147125244 and batch: 550, loss is 3.360710663795471 and perplexity is 28.809657586514017
At time: 136.65979743003845 and batch: 600, loss is 3.39954439163208 and perplexity is 29.950451262164716
At time: 137.39226126670837 and batch: 650, loss is 3.223780145645142 and perplexity is 25.12290916605736
At time: 138.13937139511108 and batch: 700, loss is 3.203243780136108 and perplexity is 24.612237537904825
At time: 138.86046409606934 and batch: 750, loss is 3.289327311515808 and perplexity is 26.824812842602302
At time: 139.5819411277771 and batch: 800, loss is 3.2512306213378905 and perplexity is 25.822097596637697
At time: 140.30284881591797 and batch: 850, loss is 3.2998998737335206 and perplexity is 27.109924369249573
At time: 141.02215766906738 and batch: 900, loss is 3.25606330871582 and perplexity is 25.947189743926163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31285157922196 and perplexity of 74.65306483633756
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 142.83918690681458 and batch: 50, loss is 3.614106092453003 and perplexity is 37.11815089599669
At time: 143.5626835823059 and batch: 100, loss is 3.49816397190094 and perplexity is 33.054706840319504
At time: 144.2835991382599 and batch: 150, loss is 3.5203163909912107 and perplexity is 33.79511924380277
At time: 145.01538753509521 and batch: 200, loss is 3.397811689376831 and perplexity is 29.898600981238705
At time: 145.74633264541626 and batch: 250, loss is 3.554658408164978 and perplexity is 34.97587045511156
At time: 146.4713101387024 and batch: 300, loss is 3.4973067378997804 and perplexity is 33.02638336337581
At time: 147.1912145614624 and batch: 350, loss is 3.50098699092865 and perplexity is 33.14815274440849
At time: 147.92090892791748 and batch: 400, loss is 3.422071852684021 and perplexity is 30.63281600167702
At time: 148.65266489982605 and batch: 450, loss is 3.431866178512573 and perplexity is 30.934317876219424
At time: 149.38279223442078 and batch: 500, loss is 3.3257697105407713 and perplexity is 27.820404061879398
At time: 150.10697960853577 and batch: 550, loss is 3.354308133125305 and perplexity is 28.625792100516207
At time: 150.83466982841492 and batch: 600, loss is 3.394473795890808 and perplexity is 29.798969008742613
At time: 151.55522680282593 and batch: 650, loss is 3.2167443323135374 and perplexity is 24.9467694365616
At time: 152.29445672035217 and batch: 700, loss is 3.195789771080017 and perplexity is 24.42945975632084
At time: 153.0240592956543 and batch: 750, loss is 3.282339401245117 and perplexity is 26.638016874280066
At time: 153.74078679084778 and batch: 800, loss is 3.2442118072509767 and perplexity is 25.641491655477985
At time: 154.45543718338013 and batch: 850, loss is 3.2911744928359985 and perplexity is 26.87440892818204
At time: 155.1711564064026 and batch: 900, loss is 3.247185525894165 and perplexity is 25.71785572360561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311087830425942 and perplexity of 74.52151163062057
finished 11 epochs...
Completing Train Step...
At time: 156.99628067016602 and batch: 50, loss is 3.61022931098938 and perplexity is 36.97453050894401
At time: 157.73596024513245 and batch: 100, loss is 3.492986969947815 and perplexity is 32.88402475041262
At time: 158.46147966384888 and batch: 150, loss is 3.51494619846344 and perplexity is 33.614119383999125
At time: 159.17757964134216 and batch: 200, loss is 3.392321996688843 and perplexity is 29.734916549740152
At time: 159.89321041107178 and batch: 250, loss is 3.549025683403015 and perplexity is 34.77941481322887
At time: 160.61100435256958 and batch: 300, loss is 3.4917413663864134 and perplexity is 32.8430897917281
At time: 161.32662868499756 and batch: 350, loss is 3.495511450767517 and perplexity is 32.96714471340215
At time: 162.0505566596985 and batch: 400, loss is 3.4173045206069945 and perplexity is 30.48712674580271
At time: 162.76897406578064 and batch: 450, loss is 3.4281158447265625 and perplexity is 30.818521132684282
At time: 163.48801159858704 and batch: 500, loss is 3.3218165683746337 and perplexity is 27.71064314270146
At time: 164.21469378471375 and batch: 550, loss is 3.3512685441970826 and perplexity is 28.538913564194385
At time: 164.9332721233368 and batch: 600, loss is 3.392285776138306 and perplexity is 29.73383955419735
At time: 165.6503369808197 and batch: 650, loss is 3.2149649143218992 and perplexity is 24.90241817760955
At time: 166.36721634864807 and batch: 700, loss is 3.195036950111389 and perplexity is 24.411075667596016
At time: 167.08532238006592 and batch: 750, loss is 3.282115731239319 and perplexity is 26.63205941516864
At time: 167.8024604320526 and batch: 800, loss is 3.2448172903060915 and perplexity is 25.657021845342186
At time: 168.5210461616516 and batch: 850, loss is 3.2932986783981324 and perplexity is 26.93155583344569
At time: 169.23716735839844 and batch: 900, loss is 3.250994391441345 and perplexity is 25.815998365632574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310290192904538 and perplexity of 74.4620941767378
finished 12 epochs...
Completing Train Step...
At time: 171.03187680244446 and batch: 50, loss is 3.6075705814361574 and perplexity is 36.87635579979544
At time: 171.76333355903625 and batch: 100, loss is 3.4895991277694702 and perplexity is 32.772807364219936
At time: 172.4895944595337 and batch: 150, loss is 3.5112601137161255 and perplexity is 33.49044297214588
At time: 173.2163245677948 and batch: 200, loss is 3.3886825323104857 and perplexity is 29.626894071381066
At time: 173.93752241134644 and batch: 250, loss is 3.545170102119446 and perplexity is 34.645578127327916
At time: 174.67091536521912 and batch: 300, loss is 3.4882319498062135 and perplexity is 32.728031719309485
At time: 175.39139103889465 and batch: 350, loss is 3.4919789266586303 and perplexity is 32.850892931900816
At time: 176.11624002456665 and batch: 400, loss is 3.414105930328369 and perplexity is 30.389766709011337
At time: 176.83667135238647 and batch: 450, loss is 3.425372796058655 and perplexity is 30.734100267549312
At time: 177.564594745636 and batch: 500, loss is 3.319064345359802 and perplexity is 27.634482127013367
At time: 178.28467297554016 and batch: 550, loss is 3.3490161323547363 and perplexity is 28.47470451707005
At time: 179.0041983127594 and batch: 600, loss is 3.3904991483688356 and perplexity is 29.68076367829209
At time: 179.72480392456055 and batch: 650, loss is 3.213583846092224 and perplexity is 24.868049976900107
At time: 180.44538497924805 and batch: 700, loss is 3.1944239854812624 and perplexity is 24.396117126625057
At time: 181.16631507873535 and batch: 750, loss is 3.2819490575790407 and perplexity is 26.62762092224537
At time: 181.88823342323303 and batch: 800, loss is 3.245342450141907 and perplexity is 25.67049942135305
At time: 182.60387444496155 and batch: 850, loss is 3.294771866798401 and perplexity is 26.971260328020293
At time: 183.3194284439087 and batch: 900, loss is 3.253240747451782 and perplexity is 25.87405547277887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309939867829623 and perplexity of 74.43601280676272
finished 13 epochs...
Completing Train Step...
At time: 185.13203930854797 and batch: 50, loss is 3.6052985095977785 and perplexity is 36.792665181831865
At time: 185.86351871490479 and batch: 100, loss is 3.4868184089660645 and perplexity is 32.68180199135842
At time: 186.5889768600464 and batch: 150, loss is 3.5082427072525024 and perplexity is 33.389541000739676
At time: 187.31472420692444 and batch: 200, loss is 3.385672960281372 and perplexity is 29.537863838323354
At time: 188.0360939502716 and batch: 250, loss is 3.542017407417297 and perplexity is 34.53652319549506
At time: 188.7571313381195 and batch: 300, loss is 3.485327863693237 and perplexity is 32.63312457285306
At time: 189.48507285118103 and batch: 350, loss is 3.48907977104187 and perplexity is 32.755791005395686
At time: 190.2099711894989 and batch: 400, loss is 3.411444778442383 and perplexity is 30.309002434682
At time: 190.93050146102905 and batch: 450, loss is 3.423021321296692 and perplexity is 30.66191471095788
At time: 191.65636658668518 and batch: 500, loss is 3.316781554222107 and perplexity is 27.571470324874362
At time: 192.4024477005005 and batch: 550, loss is 3.3470897388458254 and perplexity is 28.419903831909377
At time: 193.13435649871826 and batch: 600, loss is 3.3889258193969725 and perplexity is 29.63410278897974
At time: 193.8679075241089 and batch: 650, loss is 3.212374062538147 and perplexity is 24.83798320982334
At time: 194.58943438529968 and batch: 700, loss is 3.193800539970398 and perplexity is 24.38091221712872
At time: 195.3122420310974 and batch: 750, loss is 3.2817283487319946 and perplexity is 26.621744619231926
At time: 196.04567193984985 and batch: 800, loss is 3.2456893634796145 and perplexity is 25.679406404874296
At time: 196.796959400177 and batch: 850, loss is 3.2958052492141725 and perplexity is 26.99914636016052
At time: 197.51943564414978 and batch: 900, loss is 3.254731435775757 and perplexity is 25.91265438749404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3098207238602315 and perplexity of 74.42714473303047
finished 14 epochs...
Completing Train Step...
At time: 199.35075521469116 and batch: 50, loss is 3.603250880241394 and perplexity is 36.717404519772415
At time: 200.09090971946716 and batch: 100, loss is 3.4843734312057495 and perplexity is 32.60199331729657
At time: 200.80925750732422 and batch: 150, loss is 3.505603494644165 and perplexity is 33.30153508738533
At time: 201.5272159576416 and batch: 200, loss is 3.3830309915542602 and perplexity is 29.45992872219883
At time: 202.2448706626892 and batch: 250, loss is 3.539262261390686 and perplexity is 34.441500990972735
At time: 202.96458411216736 and batch: 300, loss is 3.482761220932007 and perplexity is 32.54947439587397
At time: 203.6833255290985 and batch: 350, loss is 3.4865323734283447 and perplexity is 32.67245517137723
At time: 204.40373611450195 and batch: 400, loss is 3.4090788745880127 and perplexity is 30.237379009477895
At time: 205.12685084342957 and batch: 450, loss is 3.420905795097351 and perplexity is 30.59711719165627
At time: 205.8451647758484 and batch: 500, loss is 3.314763751029968 and perplexity is 27.51589261533791
At time: 206.5724356174469 and batch: 550, loss is 3.345354800224304 and perplexity is 28.370639790532607
At time: 207.293696641922 and batch: 600, loss is 3.3874890565872193 and perplexity is 29.591556184200883
At time: 208.02260541915894 and batch: 650, loss is 3.2112568616867065 and perplexity is 24.81024968867458
At time: 208.74481582641602 and batch: 700, loss is 3.1931577348709106 and perplexity is 24.36524507842319
At time: 209.46660470962524 and batch: 750, loss is 3.2814520454406737 and perplexity is 26.61438995967804
At time: 210.18691968917847 and batch: 800, loss is 3.2458862829208375 and perplexity is 25.684463677155755
At time: 210.92875266075134 and batch: 850, loss is 3.2965351581573485 and perplexity is 27.018860472425093
At time: 211.64770770072937 and batch: 900, loss is 3.2557767963409425 and perplexity is 25.939756617863377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309827412644478 and perplexity of 74.42764256180861
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 213.4542510509491 and batch: 50, loss is 3.6021779203414916 and perplexity is 36.6780293448636
At time: 214.2107436656952 and batch: 100, loss is 3.4838143301010134 and perplexity is 32.58377060146124
At time: 214.9378752708435 and batch: 150, loss is 3.5055429267883302 and perplexity is 33.29951814589061
At time: 215.65223383903503 and batch: 200, loss is 3.3826804161071777 and perplexity is 29.449602604663422
At time: 216.38004779815674 and batch: 250, loss is 3.538903660774231 and perplexity is 34.429152461713166
At time: 217.0948519706726 and batch: 300, loss is 3.4830779600143433 and perplexity is 32.559785719438025
At time: 217.8219804763794 and batch: 350, loss is 3.486335997581482 and perplexity is 32.6660397202643
At time: 218.53608059883118 and batch: 400, loss is 3.4083010482788088 and perplexity is 30.21386872520711
At time: 219.25156044960022 and batch: 450, loss is 3.4201770973205567 and perplexity is 30.574829261951052
At time: 219.9881386756897 and batch: 500, loss is 3.3141264152526855 and perplexity is 27.49836133977829
At time: 220.70821690559387 and batch: 550, loss is 3.343474016189575 and perplexity is 28.317330891137317
At time: 221.42378187179565 and batch: 600, loss is 3.384845371246338 and perplexity is 29.513428738597366
At time: 222.139493227005 and batch: 650, loss is 3.208656973838806 and perplexity is 24.745829600794288
At time: 222.86217665672302 and batch: 700, loss is 3.190539588928223 and perplexity is 24.30153674612611
At time: 223.5839684009552 and batch: 750, loss is 3.2789040756225587 and perplexity is 26.546663616313424
At time: 224.30246591567993 and batch: 800, loss is 3.2431365871429443 and perplexity is 25.613936224781387
At time: 225.01861238479614 and batch: 850, loss is 3.2932354831695556 and perplexity is 26.929853941395162
At time: 225.73609828948975 and batch: 900, loss is 3.251636257171631 and perplexity is 25.832574089402026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309066981485445 and perplexity of 74.37106697695631
finished 16 epochs...
Completing Train Step...
At time: 227.5342996120453 and batch: 50, loss is 3.601489109992981 and perplexity is 36.65277383781375
At time: 228.2544436454773 and batch: 100, loss is 3.4827356052398684 and perplexity is 32.548640629237376
At time: 228.98378348350525 and batch: 150, loss is 3.5044493007659914 and perplexity is 33.26312083246712
At time: 229.70160102844238 and batch: 200, loss is 3.381669797897339 and perplexity is 29.419855334097157
At time: 230.41714429855347 and batch: 250, loss is 3.537935109138489 and perplexity is 34.395822193427676
At time: 231.13299250602722 and batch: 300, loss is 3.4819263505935667 and perplexity is 32.522311145642746
At time: 231.85103273391724 and batch: 350, loss is 3.485408945083618 and perplexity is 32.635770619237015
At time: 232.5705156326294 and batch: 400, loss is 3.4075137758255005 and perplexity is 30.190091539441653
At time: 233.28811287879944 and batch: 450, loss is 3.4194021129608156 and perplexity is 30.551143426732686
At time: 234.0076940059662 and batch: 500, loss is 3.3133763217926027 and perplexity is 27.477742732682234
At time: 234.72575688362122 and batch: 550, loss is 3.342980451583862 and perplexity is 28.30335790744994
At time: 235.44345617294312 and batch: 600, loss is 3.3847127103805543 and perplexity is 29.509513721279195
At time: 236.16199779510498 and batch: 650, loss is 3.2085073614120483 and perplexity is 24.742127594115587
At time: 236.8785960674286 and batch: 700, loss is 3.1904904365539553 and perplexity is 24.300342297251923
At time: 237.59559178352356 and batch: 750, loss is 3.278938775062561 and perplexity is 26.547584786656792
At time: 238.31319880485535 and batch: 800, loss is 3.2433497714996338 and perplexity is 25.619397297384698
At time: 239.0313172340393 and batch: 850, loss is 3.2936096000671387 and perplexity is 26.939930739636797
At time: 239.74812245368958 and batch: 900, loss is 3.2522863483428956 and perplexity is 25.84937307759391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308686556881422 and perplexity of 74.34277977416589
finished 17 epochs...
Completing Train Step...
At time: 241.532470703125 and batch: 50, loss is 3.600888524055481 and perplexity is 36.630767306344545
At time: 242.25966024398804 and batch: 100, loss is 3.4819127464294435 and perplexity is 32.52186870979374
At time: 242.97374653816223 and batch: 150, loss is 3.503573427200317 and perplexity is 33.233999299470334
At time: 243.6882529258728 and batch: 200, loss is 3.3808641719818113 and perplexity is 29.396163480875806
At time: 244.40098023414612 and batch: 250, loss is 3.537101583480835 and perplexity is 34.36716433830133
At time: 245.11576557159424 and batch: 300, loss is 3.4810507678985596 and perplexity is 32.493847635698806
At time: 245.83085083961487 and batch: 350, loss is 3.484619393348694 and perplexity is 32.61001315968553
At time: 246.55649971961975 and batch: 400, loss is 3.4068352127075197 and perplexity is 30.169612605704877
At time: 247.27182459831238 and batch: 450, loss is 3.4187770223617555 and perplexity is 30.53205216169289
At time: 247.98586797714233 and batch: 500, loss is 3.312774157524109 and perplexity is 27.46120159856953
At time: 248.6993100643158 and batch: 550, loss is 3.3425343513488768 and perplexity is 28.290734588678628
At time: 249.41256713867188 and batch: 600, loss is 3.3845001220703126 and perplexity is 29.503241010397257
At time: 250.12642431259155 and batch: 650, loss is 3.2083143424987792 and perplexity is 24.73735235640583
At time: 250.83953142166138 and batch: 700, loss is 3.1904220008850097 and perplexity is 24.298679343974516
At time: 251.55318570137024 and batch: 750, loss is 3.278959584236145 and perplexity is 26.548137225704743
At time: 252.26541566848755 and batch: 800, loss is 3.243502287864685 and perplexity is 25.623304972719936
At time: 252.97921562194824 and batch: 850, loss is 3.293913178443909 and perplexity is 26.948110361596395
At time: 253.69289112091064 and batch: 900, loss is 3.2527863216400146 and perplexity is 25.862300305245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308476696275685 and perplexity of 74.3271797903386
finished 18 epochs...
Completing Train Step...
At time: 255.47138571739197 and batch: 50, loss is 3.600326895713806 and perplexity is 36.61020020532204
At time: 256.1989459991455 and batch: 100, loss is 3.4812035083770754 and perplexity is 32.49881114058991
At time: 256.91189336776733 and batch: 150, loss is 3.5028070831298828 and perplexity is 33.20854037756325
At time: 257.6305561065674 and batch: 200, loss is 3.380149345397949 and perplexity is 29.375157830349274
At time: 258.34920144081116 and batch: 250, loss is 3.53634295463562 and perplexity is 34.34110230306183
At time: 259.0742721557617 and batch: 300, loss is 3.480300154685974 and perplexity is 32.46946647589079
At time: 259.78962302207947 and batch: 350, loss is 3.483908748626709 and perplexity is 32.58684725828745
At time: 260.5060257911682 and batch: 400, loss is 3.4062122249603273 and perplexity is 30.150823160118875
At time: 261.2222216129303 and batch: 450, loss is 3.4182209825515746 and perplexity is 30.515079844283832
At time: 261.9399371147156 and batch: 500, loss is 3.3122401285171508 and perplexity is 27.44654043544159
At time: 262.66229820251465 and batch: 550, loss is 3.3421089267730713 and perplexity is 28.27870157466844
At time: 263.3810336589813 and batch: 600, loss is 3.38424298286438 and perplexity is 29.495655545733385
At time: 264.1016194820404 and batch: 650, loss is 3.208099021911621 and perplexity is 24.732026468578937
At time: 264.8320634365082 and batch: 700, loss is 3.1903331661224366 and perplexity is 24.296520872439224
At time: 265.5581135749817 and batch: 750, loss is 3.278958282470703 and perplexity is 26.548102666279643
At time: 266.4553279876709 and batch: 800, loss is 3.2436162042617798 and perplexity is 25.626224053566524
At time: 267.18403577804565 and batch: 850, loss is 3.2941658782958982 and perplexity is 26.954921005584286
At time: 267.90186834335327 and batch: 900, loss is 3.253186798095703 and perplexity is 25.872659621799933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308358388404324 and perplexity of 74.31838682006241
finished 19 epochs...
Completing Train Step...
At time: 269.7042033672333 and batch: 50, loss is 3.599790782928467 and perplexity is 36.590578269173534
At time: 270.42593717575073 and batch: 100, loss is 3.480555648803711 and perplexity is 32.47776329343034
At time: 271.1462585926056 and batch: 150, loss is 3.502106952667236 and perplexity is 33.18529820405115
At time: 271.8667995929718 and batch: 200, loss is 3.3794854879379272 and perplexity is 29.355663384166984
At time: 272.59940338134766 and batch: 250, loss is 3.5356356573104857 and perplexity is 34.316821521139005
At time: 273.32000207901 and batch: 300, loss is 3.4796180868148805 and perplexity is 32.44732764696627
At time: 274.04033970832825 and batch: 350, loss is 3.483249840736389 and perplexity is 32.56538259990129
At time: 274.7605767250061 and batch: 400, loss is 3.4056244707107544 and perplexity is 30.133107092535408
At time: 275.481258392334 and batch: 450, loss is 3.4177031087875367 and perplexity is 30.49928097628767
At time: 276.20345997810364 and batch: 500, loss is 3.311743879318237 and perplexity is 27.432923490715957
At time: 276.92764139175415 and batch: 550, loss is 3.3416973638534544 and perplexity is 28.267065504337182
At time: 277.6566638946533 and batch: 600, loss is 3.3839630937576293 and perplexity is 29.487401188255916
At time: 278.377295255661 and batch: 650, loss is 3.207872018814087 and perplexity is 24.72641285913974
At time: 279.0974802970886 and batch: 700, loss is 3.1902280330657957 and perplexity is 24.293966639203695
At time: 279.8171555995941 and batch: 750, loss is 3.2789373540878297 and perplexity is 26.547547063236436
At time: 280.53830909729004 and batch: 800, loss is 3.243702793121338 and perplexity is 25.628443095152853
At time: 281.2654912471771 and batch: 850, loss is 3.294381422996521 and perplexity is 26.96073162216405
At time: 281.9923496246338 and batch: 900, loss is 3.253518090248108 and perplexity is 25.88123245087156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308290246414812 and perplexity of 74.31332278986565
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc267f18b70>
ELAPSED
877.1328301429749


RESULTS SO FAR:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.88670270084627, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.31332278986565, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.23850096980998603, 'data': 'ptb', 'dropout': 0.3583059732624252, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.26724264066418335, 'data': 'ptb', 'dropout': 0.9244537941766774, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0233678817749023 and batch: 50, loss is 7.269048271179199 and perplexity is 1435.1838969664616
At time: 1.8141133785247803 and batch: 100, loss is 6.531941900253296 and perplexity is 686.7304795995386
At time: 2.6022801399230957 and batch: 150, loss is 6.325306262969971 and perplexity is 558.5288441761239
At time: 3.3889031410217285 and batch: 200, loss is 6.160538740158081 and perplexity is 473.6831982671599
At time: 4.175807952880859 and batch: 250, loss is 6.202841653823852 and perplexity is 494.15125464926274
At time: 4.961894989013672 and batch: 300, loss is 6.099610080718994 and perplexity is 445.68395543041987
At time: 5.748696565628052 and batch: 350, loss is 6.096699380874634 and perplexity is 444.38858933620617
At time: 6.535134553909302 and batch: 400, loss is 5.973694257736206 and perplexity is 392.9546684403509
At time: 7.32246470451355 and batch: 450, loss is 5.961675930023193 and perplexity is 388.2602763348209
At time: 8.10865592956543 and batch: 500, loss is 5.920338926315307 and perplexity is 372.53795539822204
At time: 8.895005941390991 and batch: 550, loss is 5.957620267868042 and perplexity is 386.6888126426576
At time: 9.68050479888916 and batch: 600, loss is 5.883547077178955 and perplexity is 359.08067230269035
At time: 10.465951204299927 and batch: 650, loss is 5.808319053649902 and perplexity is 333.0588008928276
At time: 11.253108263015747 and batch: 700, loss is 5.90607442855835 and perplexity is 367.26161024299665
At time: 12.040840864181519 and batch: 750, loss is 5.850216913223266 and perplexity is 347.3097083769464
At time: 12.828769207000732 and batch: 800, loss is 5.8592146873474125 and perplexity is 350.44882403233106
At time: 13.615636587142944 and batch: 850, loss is 5.892981376647949 and perplexity is 362.4843774396923
At time: 14.402101755142212 and batch: 900, loss is 5.769592304229736 and perplexity is 320.40707740515063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.771333041256422 and perplexity of 320.96530759342016
finished 1 epochs...
Completing Train Step...
At time: 16.23478651046753 and batch: 50, loss is 5.537643165588379 and perplexity is 254.0784723561152
At time: 16.965807676315308 and batch: 100, loss is 5.291008386611939 and perplexity is 198.54353314015185
At time: 17.687453031539917 and batch: 150, loss is 5.197817440032959 and perplexity is 180.87703577428854
At time: 18.40733242034912 and batch: 200, loss is 5.022054109573364 and perplexity is 151.7226388513829
At time: 19.125860691070557 and batch: 250, loss is 5.063218774795533 and perplexity is 158.0985819410744
At time: 19.844480991363525 and batch: 300, loss is 4.969207696914673 and perplexity is 143.912819617769
At time: 20.562549114227295 and batch: 350, loss is 4.935486249923706 and perplexity is 139.1407832808459
At time: 21.282150506973267 and batch: 400, loss is 4.782283945083618 and perplexity is 119.3766887244447
At time: 22.000414848327637 and batch: 450, loss is 4.785982532501221 and perplexity is 119.81903136051615
At time: 22.71817636489868 and batch: 500, loss is 4.689213848114013 and perplexity is 108.76763830352768
At time: 23.441304683685303 and batch: 550, loss is 4.7417905807495115 and perplexity is 114.63928891660613
At time: 24.1675283908844 and batch: 600, loss is 4.670054359436035 and perplexity is 106.70354261920168
At time: 24.894914150238037 and batch: 650, loss is 4.5341723346710205 and perplexity is 93.14638935460091
At time: 25.62029194831848 and batch: 700, loss is 4.585156755447388 and perplexity is 98.01855087969793
At time: 26.34649634361267 and batch: 750, loss is 4.608264331817627 and perplexity is 100.30989376396545
At time: 27.07077646255493 and batch: 800, loss is 4.55187180519104 and perplexity is 94.80970763117256
At time: 27.795519590377808 and batch: 850, loss is 4.600319480895996 and perplexity is 99.51610405785803
At time: 28.525397539138794 and batch: 900, loss is 4.528026933670044 and perplexity is 92.57572272384539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.60704563088613 and perplexity of 100.18772046447525
finished 2 epochs...
Completing Train Step...
At time: 30.330265283584595 and batch: 50, loss is 4.577144079208374 and perplexity is 97.23629812037244
At time: 31.063795566558838 and batch: 100, loss is 4.443141040802002 and perplexity is 85.04164186342649
At time: 31.782591104507446 and batch: 150, loss is 4.434837198257446 and perplexity is 84.33839333316156
At time: 32.5021014213562 and batch: 200, loss is 4.318768272399902 and perplexity is 75.09607339625379
At time: 33.2214777469635 and batch: 250, loss is 4.453505239486694 and perplexity is 85.9276135990606
At time: 33.94150948524475 and batch: 300, loss is 4.40161723613739 and perplexity is 81.58270052608654
At time: 34.659945487976074 and batch: 350, loss is 4.40109938621521 and perplexity is 81.5404638680368
At time: 35.39289999008179 and batch: 400, loss is 4.29970413684845 and perplexity is 73.67799187170105
At time: 36.11889290809631 and batch: 450, loss is 4.328772001266479 and perplexity is 75.8510843192969
At time: 36.839858531951904 and batch: 500, loss is 4.205917868614197 and perplexity is 67.08214200989717
At time: 37.55964112281799 and batch: 550, loss is 4.27742483139038 and perplexity is 72.0546480193942
At time: 38.2808198928833 and batch: 600, loss is 4.266451501846314 and perplexity is 71.26829098959057
At time: 39.00270223617554 and batch: 650, loss is 4.1111705493927 and perplexity is 61.01810048332286
At time: 39.72212791442871 and batch: 700, loss is 4.137910904884339 and perplexity is 62.67175733203303
At time: 40.44059181213379 and batch: 750, loss is 4.221133141517639 and perplexity is 68.11061954938691
At time: 41.15908336639404 and batch: 800, loss is 4.180167722702026 and perplexity is 65.37681747098291
At time: 41.88541865348816 and batch: 850, loss is 4.244633312225342 and perplexity is 69.73018624855035
At time: 42.60619497299194 and batch: 900, loss is 4.185477514266967 and perplexity is 65.72487799157669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4123292687821065 and perplexity of 82.46131454684435
finished 3 epochs...
Completing Train Step...
At time: 44.40674924850464 and batch: 50, loss is 4.262803287506103 and perplexity is 71.00876268369225
At time: 45.13348364830017 and batch: 100, loss is 4.136811537742615 and perplexity is 62.60289592023036
At time: 45.85346174240112 and batch: 150, loss is 4.132454314231873 and perplexity is 62.3307145185101
At time: 46.572813272476196 and batch: 200, loss is 4.018993501663208 and perplexity is 55.64507096065371
At time: 47.29205632209778 and batch: 250, loss is 4.169655919075012 and perplexity is 64.69318858522723
At time: 48.01181983947754 and batch: 300, loss is 4.127911863327026 and perplexity is 62.04822239698402
At time: 48.739885568618774 and batch: 350, loss is 4.127040705680847 and perplexity is 61.994192151458876
At time: 49.461785316467285 and batch: 400, loss is 4.045867528915405 and perplexity is 57.16075313676129
At time: 50.18228816986084 and batch: 450, loss is 4.079440488815307 and perplexity is 59.11238655407743
At time: 50.90286421775818 and batch: 500, loss is 3.95472553730011 and perplexity is 52.18137013911411
At time: 51.6193151473999 and batch: 550, loss is 4.026955676078797 and perplexity is 56.08989525530693
At time: 52.33496975898743 and batch: 600, loss is 4.033980784416198 and perplexity is 56.48532017129165
At time: 53.06062030792236 and batch: 650, loss is 3.877788224220276 and perplexity is 48.31722992249259
At time: 53.77594876289368 and batch: 700, loss is 3.8947889471054076 and perplexity is 49.145679930044956
At time: 54.49228644371033 and batch: 750, loss is 3.9917097902297973 and perplexity is 54.14739094164541
At time: 55.21211338043213 and batch: 800, loss is 3.960469913482666 and perplexity is 52.48198214651323
At time: 55.93215227127075 and batch: 850, loss is 4.02478593826294 and perplexity is 55.96832682199047
At time: 56.65237307548523 and batch: 900, loss is 3.971319456100464 and perplexity is 53.05448774450584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346156290132705 and perplexity of 77.18122979771087
finished 4 epochs...
Completing Train Step...
At time: 58.46286177635193 and batch: 50, loss is 4.0562849044799805 and perplexity is 57.75933055943831
At time: 59.193774461746216 and batch: 100, loss is 3.936865758895874 and perplexity is 51.257695297639444
At time: 59.916029930114746 and batch: 150, loss is 3.9375086545944216 and perplexity is 51.290659244518196
At time: 60.635218381881714 and batch: 200, loss is 3.825841898918152 and perplexity is 45.871403198939134
At time: 61.35046887397766 and batch: 250, loss is 3.9757150506973264 and perplexity is 53.288207055532915
At time: 62.065940618515015 and batch: 300, loss is 3.9425279712677 and perplexity is 51.54875048462758
At time: 62.7811381816864 and batch: 350, loss is 3.941481938362122 and perplexity is 51.49485698747904
At time: 63.49731159210205 and batch: 400, loss is 3.867242684364319 and perplexity is 47.81037587061223
At time: 64.21975994110107 and batch: 450, loss is 3.899655833244324 and perplexity is 49.38544935044215
At time: 64.93845343589783 and batch: 500, loss is 3.782746443748474 and perplexity is 43.936545432047296
At time: 65.65694665908813 and batch: 550, loss is 3.8503262519836428 and perplexity is 47.00839731292725
At time: 66.3849983215332 and batch: 600, loss is 3.865739917755127 and perplexity is 47.738581992413465
At time: 67.1019856929779 and batch: 650, loss is 3.7111077404022215 and perplexity is 40.899087013946904
At time: 67.8184163570404 and batch: 700, loss is 3.7248447513580323 and perplexity is 41.46479489201327
At time: 68.54311084747314 and batch: 750, loss is 3.8285875129699707 and perplexity is 45.99752142489805
At time: 69.26921701431274 and batch: 800, loss is 3.798135533332825 and perplexity is 44.6179182724975
At time: 69.98655343055725 and batch: 850, loss is 3.8594080209732056 and perplexity is 47.43726119156197
At time: 70.71036291122437 and batch: 900, loss is 3.8093474674224854 and perplexity is 45.120986344479434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334532855308219 and perplexity of 76.28931241705047
finished 5 epochs...
Completing Train Step...
At time: 72.50871324539185 and batch: 50, loss is 3.8998065900802614 and perplexity is 49.39289510576273
At time: 73.23901200294495 and batch: 100, loss is 3.7831245279312133 and perplexity is 43.953160285628115
At time: 73.95542860031128 and batch: 150, loss is 3.78730119228363 and perplexity is 44.137121788668466
At time: 74.67261338233948 and batch: 200, loss is 3.677037935256958 and perplexity is 39.529132640214684
At time: 75.39169239997864 and batch: 250, loss is 3.825373010635376 and perplexity is 45.849899677233324
At time: 76.11091494560242 and batch: 300, loss is 3.794657998085022 and perplexity is 44.46302736441205
At time: 76.82946467399597 and batch: 350, loss is 3.797432165145874 and perplexity is 44.58654648246956
At time: 77.54714465141296 and batch: 400, loss is 3.725595064163208 and perplexity is 41.495918133207816
At time: 78.26532435417175 and batch: 450, loss is 3.758274416923523 and perplexity is 42.87437880737944
At time: 78.98246121406555 and batch: 500, loss is 3.6476850843429567 and perplexity is 38.38570345237763
At time: 79.7001953125 and batch: 550, loss is 3.7128775119781494 and perplexity is 40.97153314326683
At time: 80.41780591011047 and batch: 600, loss is 3.7301601123809816 and perplexity is 41.685782039545416
At time: 81.13449287414551 and batch: 650, loss is 3.5782902383804323 and perplexity is 35.81225804818211
At time: 81.85103845596313 and batch: 700, loss is 3.5907648754119874 and perplexity is 36.26180108202192
At time: 82.56677913665771 and batch: 750, loss is 3.695649991035461 and perplexity is 40.27174035285583
At time: 83.28625988960266 and batch: 800, loss is 3.666620683670044 and perplexity is 39.11948512564235
At time: 84.00512766838074 and batch: 850, loss is 3.73013108253479 and perplexity is 41.68457192526924
At time: 84.72320222854614 and batch: 900, loss is 3.6785673332214355 and perplexity is 39.58963466925908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346741140705266 and perplexity of 77.22638248664995
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 86.53550910949707 and batch: 50, loss is 3.804148874282837 and perplexity is 44.88702934493889
At time: 87.25524520874023 and batch: 100, loss is 3.6944395208358767 and perplexity is 40.22302210321662
At time: 87.97437357902527 and batch: 150, loss is 3.6974729681015015 and perplexity is 40.34522176904713
At time: 88.69862914085388 and batch: 200, loss is 3.570446138381958 and perplexity is 35.53244200215718
At time: 89.44457936286926 and batch: 250, loss is 3.7113994121551515 and perplexity is 40.91101786221063
At time: 90.1695945262909 and batch: 300, loss is 3.667123556137085 and perplexity is 39.13916218474782
At time: 90.89055180549622 and batch: 350, loss is 3.665179762840271 and perplexity is 39.06315763614711
At time: 91.60950350761414 and batch: 400, loss is 3.5855694723129274 and perplexity is 36.073894955006864
At time: 92.3289086818695 and batch: 450, loss is 3.6060955286026 and perplexity is 36.82200132439845
At time: 93.04852724075317 and batch: 500, loss is 3.4924054527282715 and perplexity is 32.864907682763096
At time: 93.76825904846191 and batch: 550, loss is 3.5364675092697144 and perplexity is 34.345379912886116
At time: 94.48767518997192 and batch: 600, loss is 3.5446706771850587 and perplexity is 34.62827958176691
At time: 95.20810008049011 and batch: 650, loss is 3.3830712175369264 and perplexity is 29.461113800616268
At time: 95.93161964416504 and batch: 700, loss is 3.372937412261963 and perplexity is 29.16406825310758
At time: 96.66261887550354 and batch: 750, loss is 3.4681360912323 and perplexity is 32.076898278104174
At time: 97.38313889503479 and batch: 800, loss is 3.4191365337371824 and perplexity is 30.543030755106503
At time: 98.1032030582428 and batch: 850, loss is 3.4699763011932374 and perplexity is 32.13598085132844
At time: 98.82345938682556 and batch: 900, loss is 3.3985635232925415 and perplexity is 29.921088215762236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307798620772688 and perplexity of 74.27679743396722
finished 7 epochs...
Completing Train Step...
At time: 100.61544466018677 and batch: 50, loss is 3.7074872064590454 and perplexity is 40.75127821601273
At time: 101.35081315040588 and batch: 100, loss is 3.5926185035705567 and perplexity is 36.3290793127358
At time: 102.07982611656189 and batch: 150, loss is 3.593633532524109 and perplexity is 36.36597310105821
At time: 102.80429816246033 and batch: 200, loss is 3.471060833930969 and perplexity is 32.17085228080625
At time: 103.52342796325684 and batch: 250, loss is 3.612638483047485 and perplexity is 37.063715903050635
At time: 104.24169611930847 and batch: 300, loss is 3.573544125556946 and perplexity is 35.64269173973537
At time: 104.96432447433472 and batch: 350, loss is 3.5747632217407226 and perplexity is 35.68617010598522
At time: 105.68594431877136 and batch: 400, loss is 3.499598574638367 and perplexity is 33.10216124420756
At time: 106.40439438819885 and batch: 450, loss is 3.5245365953445433 and perplexity is 33.938042924602826
At time: 107.12233328819275 and batch: 500, loss is 3.416719923019409 and perplexity is 30.469309253593615
At time: 107.85452485084534 and batch: 550, loss is 3.4626323699951174 and perplexity is 31.90084090160088
At time: 108.57890605926514 and batch: 600, loss is 3.4794943523406983 and perplexity is 32.44331304231888
At time: 109.30559372901917 and batch: 650, loss is 3.3242701625823976 and perplexity is 27.778717295231743
At time: 110.02421021461487 and batch: 700, loss is 3.318857879638672 and perplexity is 27.62877714269508
At time: 110.74502992630005 and batch: 750, loss is 3.421490430831909 and perplexity is 30.615010589791297
At time: 111.46958112716675 and batch: 800, loss is 3.378017578125 and perplexity is 29.31260352954392
At time: 112.18976974487305 and batch: 850, loss is 3.435349450111389 and perplexity is 31.04225839103275
At time: 112.91053867340088 and batch: 900, loss is 3.3726175212860108 and perplexity is 29.154740422875257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318249010059931 and perplexity of 75.05708895591124
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 114.6898467540741 and batch: 50, loss is 3.6744211196899412 and perplexity is 39.42582741488246
At time: 115.42198085784912 and batch: 100, loss is 3.5711752367019653 and perplexity is 35.55835809246762
At time: 116.14064693450928 and batch: 150, loss is 3.577718415260315 and perplexity is 35.79178562490687
At time: 116.85965323448181 and batch: 200, loss is 3.447125082015991 and perplexity is 31.409961317523162
At time: 117.57729887962341 and batch: 250, loss is 3.589137988090515 and perplexity is 36.202855179759666
At time: 118.29489183425903 and batch: 300, loss is 3.546783990859985 and perplexity is 34.701537379634566
At time: 119.01026606559753 and batch: 350, loss is 3.541550703048706 and perplexity is 34.52040860991001
At time: 119.72898292541504 and batch: 400, loss is 3.46512442111969 and perplexity is 31.98043856757713
At time: 120.44654750823975 and batch: 450, loss is 3.4853466844558714 and perplexity is 32.633738758924366
At time: 121.16371083259583 and batch: 500, loss is 3.3751479148864747 and perplexity is 29.228606807564837
At time: 121.87814497947693 and batch: 550, loss is 3.412434606552124 and perplexity is 30.339017989939546
At time: 122.59208059310913 and batch: 600, loss is 3.4303446054458617 and perplexity is 30.88728484247476
At time: 123.30636286735535 and batch: 650, loss is 3.269488797187805 and perplexity is 26.297892349964236
At time: 124.02086973190308 and batch: 700, loss is 3.2562050819396973 and perplexity is 25.95086862144371
At time: 124.74004554748535 and batch: 750, loss is 3.3547368907928465 and perplexity is 28.638068259929426
At time: 125.47008085250854 and batch: 800, loss is 3.3047791624069216 and perplexity is 27.242524750858184
At time: 126.1900634765625 and batch: 850, loss is 3.358818144798279 and perplexity is 28.755186322437957
At time: 126.90825772285461 and batch: 900, loss is 3.2988284540176394 and perplexity is 27.080893816514703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31619764354131 and perplexity of 74.90327717275096
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 128.70708537101746 and batch: 50, loss is 3.665075159072876 and perplexity is 39.05907169639809
At time: 129.4265751838684 and batch: 100, loss is 3.5599080753326415 and perplexity is 35.159964928468284
At time: 130.14530205726624 and batch: 150, loss is 3.566718111038208 and perplexity is 35.400222698506646
At time: 130.8656508922577 and batch: 200, loss is 3.4325020122528076 and perplexity is 30.953993213715112
At time: 131.5830955505371 and batch: 250, loss is 3.5718894815444946 and perplexity is 35.58376453847541
At time: 132.30194520950317 and batch: 300, loss is 3.5319021558761596 and perplexity is 34.18893849304983
At time: 133.02300596237183 and batch: 350, loss is 3.5258655071258547 and perplexity is 33.98317357036166
At time: 133.74276971817017 and batch: 400, loss is 3.4533190441131594 and perplexity is 31.605117197527395
At time: 134.46312379837036 and batch: 450, loss is 3.4673795175552367 and perplexity is 32.052638919378424
At time: 135.19028329849243 and batch: 500, loss is 3.3548728370666505 and perplexity is 28.641961763245945
At time: 135.90885996818542 and batch: 550, loss is 3.3923502445220945 and perplexity is 29.735756508568052
At time: 136.62768006324768 and batch: 600, loss is 3.4138752794265748 and perplexity is 30.382758090217774
At time: 137.34987044334412 and batch: 650, loss is 3.252080492973328 and perplexity is 25.84405239301046
At time: 138.0705282688141 and batch: 700, loss is 3.2378029346466066 and perplexity is 25.47768407371445
At time: 138.7907383441925 and batch: 750, loss is 3.3328432035446167 and perplexity is 28.017889126568697
At time: 139.512681722641 and batch: 800, loss is 3.2823804950714113 and perplexity is 26.63911155481047
At time: 140.23150777816772 and batch: 850, loss is 3.335134353637695 and perplexity is 28.082155910159162
At time: 140.95024299621582 and batch: 900, loss is 3.27847843170166 and perplexity is 26.53536659474983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31382814172196 and perplexity of 74.7260038289126
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 142.74020385742188 and batch: 50, loss is 3.6565857791900633 and perplexity is 38.728887909710124
At time: 143.47268223762512 and batch: 100, loss is 3.5522917461395265 and perplexity is 34.89319226495547
At time: 144.19250202178955 and batch: 150, loss is 3.561191840171814 and perplexity is 35.20513104032013
At time: 144.91150641441345 and batch: 200, loss is 3.4279774379730226 and perplexity is 30.814255936398197
At time: 145.63039708137512 and batch: 250, loss is 3.5647165822982787 and perplexity is 35.32943899681297
At time: 146.3582842350006 and batch: 300, loss is 3.5269778299331667 and perplexity is 34.0209948602309
At time: 147.08339881896973 and batch: 350, loss is 3.519621396064758 and perplexity is 33.77163996732356
At time: 147.8040111064911 and batch: 400, loss is 3.4485156106948853 and perplexity is 31.453668150297837
At time: 148.52867579460144 and batch: 450, loss is 3.463005332946777 and perplexity is 31.91274095238506
At time: 149.2501277923584 and batch: 500, loss is 3.3478195333480834 and perplexity is 28.44065209154263
At time: 149.9723937511444 and batch: 550, loss is 3.3856933403015135 and perplexity is 29.538465826717555
At time: 150.6917860507965 and batch: 600, loss is 3.4086204862594602 and perplexity is 30.223521724105513
At time: 151.4121651649475 and batch: 650, loss is 3.2456056928634642 and perplexity is 25.677257883003357
At time: 152.13259768486023 and batch: 700, loss is 3.23090735912323 and perplexity is 25.30260510787388
At time: 152.85285234451294 and batch: 750, loss is 3.3260451316833497 and perplexity is 27.828067444633486
At time: 153.57253766059875 and batch: 800, loss is 3.2749888277053834 and perplexity is 26.442930050707428
At time: 154.29227709770203 and batch: 850, loss is 3.3266782569885254 and perplexity is 27.845691676911823
At time: 155.01226830482483 and batch: 900, loss is 3.2721884775161745 and perplexity is 26.368984172037738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309829502889555 and perplexity of 74.42779813398462
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 156.82773327827454 and batch: 50, loss is 3.651486096382141 and perplexity is 38.531885617417764
At time: 157.55843091011047 and batch: 100, loss is 3.5475548982620237 and perplexity is 34.728299365843895
At time: 158.27575850486755 and batch: 150, loss is 3.558254818916321 and perplexity is 35.10188451498986
At time: 158.9938840866089 and batch: 200, loss is 3.4263520574569704 and perplexity is 30.76421172664176
At time: 159.7142550945282 and batch: 250, loss is 3.562990665435791 and perplexity is 35.26851591151934
At time: 160.43191766738892 and batch: 300, loss is 3.525809922218323 and perplexity is 33.98128467129873
At time: 161.14809584617615 and batch: 350, loss is 3.5163001537323 and perplexity is 33.65966222257289
At time: 161.87480807304382 and batch: 400, loss is 3.445915951728821 and perplexity is 31.372005533343682
At time: 162.5914328098297 and batch: 450, loss is 3.46147656917572 and perplexity is 31.863991183116976
At time: 163.30746722221375 and batch: 500, loss is 3.3458924293518066 and perplexity is 28.38589677378168
At time: 164.02690649032593 and batch: 550, loss is 3.3831229162216188 and perplexity is 29.462636940821163
At time: 164.74314284324646 and batch: 600, loss is 3.40699857711792 and perplexity is 30.1745416492844
At time: 165.4609580039978 and batch: 650, loss is 3.2433577299118044 and perplexity is 25.619601187919272
At time: 166.17934560775757 and batch: 700, loss is 3.228889994621277 and perplexity is 25.251611983675765
At time: 166.89661765098572 and batch: 750, loss is 3.324073543548584 and perplexity is 27.77325600759072
At time: 167.61271333694458 and batch: 800, loss is 3.2729153299331664 and perplexity is 26.388157499162258
At time: 168.33645153045654 and batch: 850, loss is 3.32416916847229 and perplexity is 27.77591195006263
At time: 169.0685362815857 and batch: 900, loss is 3.2697649669647215 and perplexity is 26.305156035987004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307294871709118 and perplexity of 74.23938998958843
finished 12 epochs...
Completing Train Step...
At time: 170.86770057678223 and batch: 50, loss is 3.6495989561080933 and perplexity is 38.45923911286604
At time: 171.5850658416748 and batch: 100, loss is 3.5452571964263915 and perplexity is 34.648595691348056
At time: 172.30228877067566 and batch: 150, loss is 3.555936827659607 and perplexity is 35.020612883445544
At time: 173.02341961860657 and batch: 200, loss is 3.42443621635437 and perplexity is 30.705328808494727
At time: 173.74403977394104 and batch: 250, loss is 3.561345281600952 and perplexity is 35.21053338040075
At time: 174.46066308021545 and batch: 300, loss is 3.523914785385132 and perplexity is 33.91694647118155
At time: 175.17600417137146 and batch: 350, loss is 3.5150361585617067 and perplexity is 33.61714344950263
At time: 175.89421463012695 and batch: 400, loss is 3.444336371421814 and perplexity is 31.322490048349792
At time: 176.61820435523987 and batch: 450, loss is 3.4599664115905764 and perplexity is 31.815907850981354
At time: 177.34506678581238 and batch: 500, loss is 3.34499493598938 and perplexity is 28.36043204876083
At time: 178.07041382789612 and batch: 550, loss is 3.382328453063965 and perplexity is 29.439239256773984
At time: 178.7898554801941 and batch: 600, loss is 3.406330180168152 and perplexity is 30.15437981648976
At time: 179.51821899414062 and batch: 650, loss is 3.243011403083801 and perplexity is 25.61072996896438
At time: 180.23489570617676 and batch: 700, loss is 3.228815612792969 and perplexity is 25.249733792461207
At time: 180.95155310630798 and batch: 750, loss is 3.3240353727340697 and perplexity is 27.772195900019906
At time: 181.66791915893555 and batch: 800, loss is 3.273162536621094 and perplexity is 26.394681634549862
At time: 182.3851923942566 and batch: 850, loss is 3.32503529548645 and perplexity is 27.799979839165385
At time: 183.10511827468872 and batch: 900, loss is 3.2713345766067503 and perplexity is 26.346477283166653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306075422731165 and perplexity of 74.1489140179692
finished 13 epochs...
Completing Train Step...
At time: 184.8911576271057 and batch: 50, loss is 3.6483132219314576 and perplexity is 38.40982252983367
At time: 185.63279747962952 and batch: 100, loss is 3.5436786365509034 and perplexity is 34.59394395531517
At time: 186.35293102264404 and batch: 150, loss is 3.5543273067474366 and perplexity is 34.96429181178251
At time: 187.07228469848633 and batch: 200, loss is 3.4230500936508177 and perplexity is 30.66279693911795
At time: 187.80234551429749 and batch: 250, loss is 3.5600978994369505 and perplexity is 35.166639770821305
At time: 188.52836084365845 and batch: 300, loss is 3.5225562238693238 and perplexity is 33.87089949887668
At time: 189.2519450187683 and batch: 350, loss is 3.513927364349365 and perplexity is 33.57988961264645
At time: 189.97955322265625 and batch: 400, loss is 3.443096055984497 and perplexity is 31.2836643634332
At time: 190.70506715774536 and batch: 450, loss is 3.4588205337524416 and perplexity is 31.77947158701531
At time: 191.4248011112213 and batch: 500, loss is 3.3442529249191284 and perplexity is 28.339396099642258
At time: 192.1534993648529 and batch: 550, loss is 3.3816577672958372 and perplexity is 29.419501397670434
At time: 192.8732659816742 and batch: 600, loss is 3.4058025455474854 and perplexity is 30.138473518459637
At time: 193.602055311203 and batch: 650, loss is 3.242684392929077 and perplexity is 25.6023563693953
At time: 194.33252382278442 and batch: 700, loss is 3.228720254898071 and perplexity is 25.247326145795913
At time: 195.05419254302979 and batch: 750, loss is 3.3240071058273317 and perplexity is 27.77141087704363
At time: 195.77209734916687 and batch: 800, loss is 3.2733959054946897 and perplexity is 26.40084205046854
At time: 196.49117469787598 and batch: 850, loss is 3.325733528137207 and perplexity is 27.819397471002855
At time: 197.21410632133484 and batch: 900, loss is 3.27257541179657 and perplexity is 26.379189210165325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305392748688998 and perplexity of 74.09831175350577
finished 14 epochs...
Completing Train Step...
At time: 198.9910590648651 and batch: 50, loss is 3.647303371429443 and perplexity is 38.371053929809406
At time: 199.71810054779053 and batch: 100, loss is 3.5424363946914674 and perplexity is 34.5509965910806
At time: 200.43180394172668 and batch: 150, loss is 3.553046441078186 and perplexity is 34.91953592001271
At time: 201.1536045074463 and batch: 200, loss is 3.4219110012054443 and perplexity is 30.627889064193397
At time: 201.8722062110901 and batch: 250, loss is 3.559037461280823 and perplexity is 35.12936749015264
At time: 202.59097933769226 and batch: 300, loss is 3.521443419456482 and perplexity is 33.83322877640793
At time: 203.31979537010193 and batch: 350, loss is 3.512926712036133 and perplexity is 33.546304624677816
At time: 204.0453143119812 and batch: 400, loss is 3.442037558555603 and perplexity is 31.250568204327905
At time: 204.76866555213928 and batch: 450, loss is 3.457858943939209 and perplexity is 31.74892745868221
At time: 205.48727416992188 and batch: 500, loss is 3.34358717918396 and perplexity is 28.32053554641472
At time: 206.20496797561646 and batch: 550, loss is 3.381050624847412 and perplexity is 29.40164499080022
At time: 206.9224236011505 and batch: 600, loss is 3.4053381872177124 and perplexity is 30.124481716091108
At time: 207.64112901687622 and batch: 650, loss is 3.242373719215393 and perplexity is 25.59440362568114
At time: 208.36662554740906 and batch: 700, loss is 3.2286102390289306 and perplexity is 25.244548692051303
At time: 209.09945368766785 and batch: 750, loss is 3.3239746952056883 and perplexity is 27.77051080293925
At time: 209.82206964492798 and batch: 800, loss is 3.2736082077026367 and perplexity is 26.406447602542002
At time: 210.5409219264984 and batch: 850, loss is 3.3263143205642702 and perplexity is 27.835559459305372
At time: 211.2595100402832 and batch: 900, loss is 3.2735780477523804 and perplexity is 26.405651197405692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304962994301156 and perplexity of 74.06647452048327
finished 15 epochs...
Completing Train Step...
At time: 213.08722257614136 and batch: 50, loss is 3.6464487648010255 and perplexity is 38.33827578098821
At time: 213.8075611591339 and batch: 100, loss is 3.5413860368728636 and perplexity is 34.51472473421958
At time: 214.52706360816956 and batch: 150, loss is 3.5519533061981203 and perplexity is 34.881385013145355
At time: 215.24750900268555 and batch: 200, loss is 3.420914044380188 and perplexity is 30.597369596971067
At time: 215.9783799648285 and batch: 250, loss is 3.5580861949920655 and perplexity is 35.095965996490115
At time: 216.70437121391296 and batch: 300, loss is 3.5204665851593018 and perplexity is 33.80019545482224
At time: 217.43354558944702 and batch: 350, loss is 3.5120086908340453 and perplexity is 33.51552253724696
At time: 218.15279054641724 and batch: 400, loss is 3.441096429824829 and perplexity is 31.221171232075186
At time: 218.87288522720337 and batch: 450, loss is 3.457009963989258 and perplexity is 31.721984694388972
At time: 219.5949957370758 and batch: 500, loss is 3.34296977519989 and perplexity is 28.303055731546298
At time: 220.3152666091919 and batch: 550, loss is 3.3804859352111816 and perplexity is 29.38504687341928
At time: 221.03586721420288 and batch: 600, loss is 3.4049104928970335 and perplexity is 30.111600401176712
At time: 221.75456714630127 and batch: 650, loss is 3.2420775175094603 and perplexity is 25.586823642322297
At time: 222.47410225868225 and batch: 700, loss is 3.228490643501282 and perplexity is 25.241529737460557
At time: 223.19194102287292 and batch: 750, loss is 3.3239355659484864 and perplexity is 27.76942418473883
At time: 223.9096918106079 and batch: 800, loss is 3.273799443244934 and perplexity is 26.411497936755726
At time: 224.62814903259277 and batch: 850, loss is 3.3268089056015016 and perplexity is 27.849329915567015
At time: 225.34748673439026 and batch: 900, loss is 3.2744066190719603 and perplexity is 26.4275392293082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304670778039384 and perplexity of 74.04483425414912
finished 16 epochs...
Completing Train Step...
At time: 227.1359806060791 and batch: 50, loss is 3.6456925964355467 and perplexity is 38.309296547626225
At time: 227.87433767318726 and batch: 100, loss is 3.5404601669311524 and perplexity is 34.48278337709473
At time: 228.60769534111023 and batch: 150, loss is 3.5509826946258545 and perplexity is 34.84754516252691
At time: 229.3307638168335 and batch: 200, loss is 3.420010585784912 and perplexity is 30.569738624015304
At time: 230.0495481491089 and batch: 250, loss is 3.5572075748443606 and perplexity is 35.06514351627134
At time: 230.76860165596008 and batch: 300, loss is 3.519576735496521 and perplexity is 33.770131740371696
At time: 231.48739075660706 and batch: 350, loss is 3.511154742240906 and perplexity is 33.48691422068229
At time: 232.20742511749268 and batch: 400, loss is 3.4402389192581175 and perplexity is 31.194410223402265
At time: 232.9361400604248 and batch: 450, loss is 3.4562377166748046 and perplexity is 31.697496933420396
At time: 233.66526889801025 and batch: 500, loss is 3.3423871994018555 and perplexity is 28.286571858285694
At time: 234.40711736679077 and batch: 550, loss is 3.3799532651901245 and perplexity is 29.369398507960035
At time: 235.13151121139526 and batch: 600, loss is 3.4045069789886475 and perplexity is 30.099452402719077
At time: 235.858740568161 and batch: 650, loss is 3.241792469024658 and perplexity is 25.579531196411857
At time: 236.5832815170288 and batch: 700, loss is 3.228363118171692 and perplexity is 25.238311008300542
At time: 237.30279636383057 and batch: 750, loss is 3.323889760971069 and perplexity is 27.768152236022175
At time: 238.02262449264526 and batch: 800, loss is 3.2739708471298217 and perplexity is 26.41602535810551
At time: 238.74175190925598 and batch: 850, loss is 3.327236785888672 and perplexity is 27.86124864456154
At time: 239.4607493877411 and batch: 900, loss is 3.2751050901412966 and perplexity is 26.446004548888666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304462171580694 and perplexity of 74.02938963447079
finished 17 epochs...
Completing Train Step...
At time: 241.26633739471436 and batch: 50, loss is 3.645004014968872 and perplexity is 38.28292655600787
At time: 242.00020480155945 and batch: 100, loss is 3.539622015953064 and perplexity is 34.45389370712381
At time: 242.72040486335754 and batch: 150, loss is 3.5500992345809936 and perplexity is 34.816772343993684
At time: 243.44100284576416 and batch: 200, loss is 3.419174437522888 and perplexity is 30.544188473539865
At time: 244.16144585609436 and batch: 250, loss is 3.5563823080062864 and perplexity is 35.03621735369813
At time: 244.88945150375366 and batch: 300, loss is 3.518748736381531 and perplexity is 33.74218167408949
At time: 245.62104725837708 and batch: 350, loss is 3.510351686477661 and perplexity is 33.4600331561656
At time: 246.3466649055481 and batch: 400, loss is 3.4394448947906495 and perplexity is 31.169650929480078
At time: 247.06842947006226 and batch: 450, loss is 3.455521836280823 and perplexity is 31.674813437131338
At time: 247.79101300239563 and batch: 500, loss is 3.3418312788009645 and perplexity is 28.27085114040143
At time: 248.51131176948547 and batch: 550, loss is 3.3794457578659056 and perplexity is 29.354497104714824
At time: 249.23338103294373 and batch: 600, loss is 3.404120750427246 and perplexity is 30.087829379234904
At time: 249.95658993721008 and batch: 650, loss is 3.241515693664551 and perplexity is 25.57245239211809
At time: 250.6912384033203 and batch: 700, loss is 3.228228712081909 and perplexity is 25.234919053559988
At time: 251.41354036331177 and batch: 750, loss is 3.3238380002975463 and perplexity is 27.766714974957075
At time: 252.1487078666687 and batch: 800, loss is 3.2741234827041628 and perplexity is 26.42005769103875
At time: 252.87966322898865 and batch: 850, loss is 3.3276111459732056 and perplexity is 27.87168073651691
At time: 253.62024354934692 and batch: 900, loss is 3.275703592300415 and perplexity is 26.461837277199656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304308747592038 and perplexity of 74.0180326214767
finished 18 epochs...
Completing Train Step...
At time: 255.43762350082397 and batch: 50, loss is 3.64436457157135 and perplexity is 38.25845461642777
At time: 256.16526556015015 and batch: 100, loss is 3.538849277496338 and perplexity is 34.42728014245911
At time: 256.8884320259094 and batch: 150, loss is 3.5492812538146974 and perplexity is 34.78830453851775
At time: 257.6077971458435 and batch: 200, loss is 3.4183897495269777 and perplexity is 30.52023021660749
At time: 258.3303062915802 and batch: 250, loss is 3.5555988264083864 and perplexity is 35.00877787270932
At time: 259.06444692611694 and batch: 300, loss is 3.5179684448242186 and perplexity is 33.715863203979254
At time: 259.792937040329 and batch: 350, loss is 3.5095898580551146 and perplexity is 33.43455205923223
At time: 260.5244507789612 and batch: 400, loss is 3.438701047897339 and perplexity is 31.146474102544627
At time: 261.248149394989 and batch: 450, loss is 3.454848971366882 and perplexity is 31.65350773524735
At time: 261.9669361114502 and batch: 500, loss is 3.3412969589233397 and perplexity is 28.255749497592998
At time: 262.6921932697296 and batch: 550, loss is 3.378959016799927 and perplexity is 29.34021254222641
At time: 263.42708349227905 and batch: 600, loss is 3.4037476634979247 and perplexity is 30.076606097121978
At time: 264.160728931427 and batch: 650, loss is 3.2412449884414674 and perplexity is 25.56553073259545
At time: 264.8907718658447 and batch: 700, loss is 3.228088083267212 and perplexity is 25.231370546321536
At time: 265.61219024658203 and batch: 750, loss is 3.3237809467315675 and perplexity is 27.765130830043226
At time: 266.3408935070038 and batch: 800, loss is 3.274258728027344 and perplexity is 26.4236311219191
At time: 267.07181096076965 and batch: 850, loss is 3.327941541671753 and perplexity is 27.880890941365788
At time: 267.80500888824463 and batch: 900, loss is 3.276223783493042 and perplexity is 26.475606072784597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304194202161815 and perplexity of 74.00955467964985
finished 19 epochs...
Completing Train Step...
At time: 269.6226234436035 and batch: 50, loss is 3.643762402534485 and perplexity is 38.23542349466959
At time: 270.3535737991333 and batch: 100, loss is 3.5381272840499878 and perplexity is 34.40243284269375
At time: 271.07956194877625 and batch: 150, loss is 3.5485150480270384 and perplexity is 34.76165974723767
At time: 271.79609632492065 and batch: 200, loss is 3.4176461791992185 and perplexity is 30.4975447142005
At time: 272.51691222190857 and batch: 250, loss is 3.554849681854248 and perplexity is 34.982561058736756
At time: 273.23773860931396 and batch: 300, loss is 3.5172265625 and perplexity is 33.69085927715495
At time: 273.95277667045593 and batch: 350, loss is 3.508862547874451 and perplexity is 33.410243610096444
At time: 274.67538952827454 and batch: 400, loss is 3.437998037338257 and perplexity is 31.12458549723418
At time: 275.40004301071167 and batch: 450, loss is 3.454210653305054 and perplexity is 31.63330917677789
At time: 276.1206383705139 and batch: 500, loss is 3.340780634880066 and perplexity is 28.241164140475977
At time: 276.8355348110199 and batch: 550, loss is 3.378489694595337 and perplexity is 29.326445759773993
At time: 277.5505917072296 and batch: 600, loss is 3.403385238647461 and perplexity is 30.06570756272392
At time: 278.2668435573578 and batch: 650, loss is 3.2409788513183595 and perplexity is 25.558727701105322
At time: 278.9816174507141 and batch: 700, loss is 3.227942118644714 and perplexity is 25.227687927617204
At time: 279.6962332725525 and batch: 750, loss is 3.323719425201416 and perplexity is 27.763422729252728
At time: 280.41455841064453 and batch: 800, loss is 3.274377856254578 and perplexity is 26.42677910975512
At time: 281.13174295425415 and batch: 850, loss is 3.3282351160049437 and perplexity is 27.889077256919936
At time: 281.84532594680786 and batch: 900, loss is 3.27668110370636 and perplexity is 26.487716671601273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304106829917594 and perplexity of 74.00308858124657
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc267f18b70>
ELAPSED
1166.071611404419


RESULTS SO FAR:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.88670270084627, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.31332278986565, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.23850096980998603, 'data': 'ptb', 'dropout': 0.3583059732624252, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.00308858124657, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.26724264066418335, 'data': 'ptb', 'dropout': 0.9244537941766774, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.3239422609632767, 'data': 'ptb', 'dropout': 0.8373723041896595, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0128092765808105 and batch: 50, loss is 7.069748010635376 and perplexity is 1175.8516947920825
At time: 1.8190860748291016 and batch: 100, loss is 6.258085479736328 and perplexity is 522.2181851333457
At time: 2.606131076812744 and batch: 150, loss is 6.113598728179932 and perplexity is 451.96228142702125
At time: 3.399707794189453 and batch: 200, loss is 5.972416553497315 and perplexity is 392.4529092129997
At time: 4.182034254074097 and batch: 250, loss is 6.0131448554992675 and perplexity is 408.7668135771865
At time: 4.964406490325928 and batch: 300, loss is 5.911703948974609 and perplexity is 369.3349474474347
At time: 5.756251573562622 and batch: 350, loss is 5.8980121898651126 and perplexity is 364.31256341200026
At time: 6.5397069454193115 and batch: 400, loss is 5.765369052886963 and perplexity is 319.0567711336405
At time: 7.321783065795898 and batch: 450, loss is 5.760325260162354 and perplexity is 317.45156647545923
At time: 8.103275060653687 and batch: 500, loss is 5.70942681312561 and perplexity is 301.6980893280616
At time: 8.893802165985107 and batch: 550, loss is 5.754827308654785 and perplexity is 315.71102225518723
At time: 9.692850589752197 and batch: 600, loss is 5.681595382690429 and perplexity is 293.41716938473155
At time: 10.480798482894897 and batch: 650, loss is 5.579248418807984 and perplexity is 264.8724578000952
At time: 11.266056060791016 and batch: 700, loss is 5.684265480041504 and perplexity is 294.2016686701122
At time: 12.054746866226196 and batch: 750, loss is 5.635386037826538 and perplexity is 280.16705151674563
At time: 12.843525648117065 and batch: 800, loss is 5.628216791152954 and perplexity is 278.16564766591665
At time: 13.625652313232422 and batch: 850, loss is 5.656141595840454 and perplexity is 286.04284172433626
At time: 14.409351110458374 and batch: 900, loss is 5.54691590309143 and perplexity is 256.44543247541606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.430863889929366 and perplexity of 228.34642641859293
finished 1 epochs...
Completing Train Step...
At time: 16.25405526161194 and batch: 50, loss is 5.263561754226685 and perplexity is 193.1682854323279
At time: 16.969462871551514 and batch: 100, loss is 5.0766034507751465 and perplexity is 160.22890527318071
At time: 17.686826944351196 and batch: 150, loss is 5.022478103637695 and perplexity is 151.78698198927086
At time: 18.404136419296265 and batch: 200, loss is 4.876983251571655 and perplexity is 131.23416565584
At time: 19.124598026275635 and batch: 250, loss is 4.943259830474854 and perplexity is 140.22662032051798
At time: 19.85065507888794 and batch: 300, loss is 4.8621980094909665 and perplexity is 129.3081104304664
At time: 20.566514015197754 and batch: 350, loss is 4.834082345962525 and perplexity is 125.72315989035886
At time: 21.280693769454956 and batch: 400, loss is 4.69469126701355 and perplexity is 109.36503883410172
At time: 21.997608184814453 and batch: 450, loss is 4.704412021636963 and perplexity is 110.4333334436808
At time: 22.714600801467896 and batch: 500, loss is 4.60571439743042 and perplexity is 100.0544359552242
At time: 23.43736171722412 and batch: 550, loss is 4.667360258102417 and perplexity is 106.41645935207781
At time: 24.158100605010986 and batch: 600, loss is 4.6085585021972655 and perplexity is 100.33940630414025
At time: 24.875049591064453 and batch: 650, loss is 4.465769958496094 and perplexity is 86.98798089651113
At time: 25.591607570648193 and batch: 700, loss is 4.513253717422486 and perplexity is 91.21813421787459
At time: 26.315545558929443 and batch: 750, loss is 4.548396434783935 and perplexity is 94.48078068152874
At time: 27.040027141571045 and batch: 800, loss is 4.496590347290039 and perplexity is 89.71072680814193
At time: 27.771181106567383 and batch: 850, loss is 4.548992128372192 and perplexity is 94.53707904341603
At time: 28.50757884979248 and batch: 900, loss is 4.478417234420776 and perplexity is 88.0951283349676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.574441152076199 and perplexity of 96.97383036735118
finished 2 epochs...
Completing Train Step...
At time: 30.302491903305054 and batch: 50, loss is 4.5202755355834965 and perplexity is 91.8609054396244
At time: 31.03120994567871 and batch: 100, loss is 4.383521995544434 and perplexity is 80.11971835840993
At time: 31.74761176109314 and batch: 150, loss is 4.3850045585632325 and perplexity is 80.23858898474893
At time: 32.46378779411316 and batch: 200, loss is 4.2695789766311645 and perplexity is 71.49152967734345
At time: 33.181442975997925 and batch: 250, loss is 4.3989479541778564 and perplexity is 81.36522367800386
At time: 33.89961576461792 and batch: 300, loss is 4.3528979825973515 and perplexity is 77.7033198208049
At time: 34.626638889312744 and batch: 350, loss is 4.349426651000977 and perplexity is 77.43405345837506
At time: 35.34795689582825 and batch: 400, loss is 4.253580651283264 and perplexity is 70.35688533122398
At time: 36.07036519050598 and batch: 450, loss is 4.285770998001099 and perplexity is 72.65854472255019
At time: 36.790194034576416 and batch: 500, loss is 4.162937140464782 and perplexity is 64.25998629829597
At time: 37.508469104766846 and batch: 550, loss is 4.237464427947998 and perplexity is 69.23208615688696
At time: 38.23619484901428 and batch: 600, loss is 4.231726489067078 and perplexity is 68.83597419782237
At time: 38.96758675575256 and batch: 650, loss is 4.071990990638733 and perplexity is 58.67366509448942
At time: 39.69501543045044 and batch: 700, loss is 4.096023015975952 and perplexity is 60.10079178874655
At time: 40.4153938293457 and batch: 750, loss is 4.184276642799378 and perplexity is 65.6459982325911
At time: 41.131656885147095 and batch: 800, loss is 4.140661501884461 and perplexity is 62.844379377748375
At time: 41.84924864768982 and batch: 850, loss is 4.210323724746704 and perplexity is 67.37834831869199
At time: 42.56587338447571 and batch: 900, loss is 4.153809266090393 and perplexity is 63.67609809916221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.400775230094178 and perplexity of 81.51403631112072
finished 3 epochs...
Completing Train Step...
At time: 44.33705925941467 and batch: 50, loss is 4.223976211547852 and perplexity is 68.30453834229996
At time: 45.06772494316101 and batch: 100, loss is 4.091529431343079 and perplexity is 59.831329673173556
At time: 45.784271478652954 and batch: 150, loss is 4.0999283123016355 and perplexity is 60.33596209606993
At time: 46.51203179359436 and batch: 200, loss is 3.981749062538147 and perplexity is 53.610720775142624
At time: 47.22797989845276 and batch: 250, loss is 4.126433334350586 and perplexity is 61.95655008901532
At time: 47.945207595825195 and batch: 300, loss is 4.087702593803406 and perplexity is 59.602802441917774
At time: 48.66223883628845 and batch: 350, loss is 4.08602475643158 and perplexity is 59.50288248067845
At time: 49.37976312637329 and batch: 400, loss is 4.009453225135803 and perplexity is 55.11672588272846
At time: 50.096696615219116 and batch: 450, loss is 4.043953075408935 and perplexity is 57.051426216530515
At time: 50.81430792808533 and batch: 500, loss is 3.919938154220581 and perplexity is 50.39732781966147
At time: 51.531084060668945 and batch: 550, loss is 3.9966436862945556 and perplexity is 54.41520868989864
At time: 52.24775457382202 and batch: 600, loss is 4.004252543449402 and perplexity is 54.830825418866716
At time: 52.96485924720764 and batch: 650, loss is 3.8437394666671754 and perplexity is 46.69978060355688
At time: 53.680638551712036 and batch: 700, loss is 3.8596365308761595 and perplexity is 47.44810231411802
At time: 54.39664077758789 and batch: 750, loss is 3.957968792915344 and perplexity is 52.35088239803375
At time: 55.11286997795105 and batch: 800, loss is 3.9248166561126707 and perplexity is 50.64379197790123
At time: 55.82930374145508 and batch: 850, loss is 3.9940636920928956 and perplexity is 54.274998715231526
At time: 56.54568362236023 and batch: 900, loss is 3.9471255683898927 and perplexity is 51.786296523565625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.35078367468429 and perplexity of 77.53920463299369
finished 4 epochs...
Completing Train Step...
At time: 58.34057950973511 and batch: 50, loss is 4.0248404169082646 and perplexity is 55.97137598367317
At time: 59.056602001190186 and batch: 100, loss is 3.8979594039916994 and perplexity is 49.30174145184903
At time: 59.777260541915894 and batch: 150, loss is 3.909733519554138 and perplexity is 49.885656649988476
At time: 60.497952461242676 and batch: 200, loss is 3.7915265655517576 and perplexity is 44.32401216595029
At time: 61.214327812194824 and batch: 250, loss is 3.937033314704895 and perplexity is 51.26628454181236
At time: 61.93263030052185 and batch: 300, loss is 3.9036998987197875 and perplexity is 49.58557172239705
At time: 62.64886116981506 and batch: 350, loss is 3.9049147939682007 and perplexity is 49.64584960611567
At time: 63.36588001251221 and batch: 400, loss is 3.8354204654693604 and perplexity is 46.3128965484523
At time: 64.09383058547974 and batch: 450, loss is 3.8692133808135987 and perplexity is 47.90468850884676
At time: 64.81053924560547 and batch: 500, loss is 3.748155975341797 and perplexity is 42.44274432686017
At time: 65.52876615524292 and batch: 550, loss is 3.822315502166748 and perplexity is 45.70992731307146
At time: 66.24606990814209 and batch: 600, loss is 3.8367592191696165 and perplexity is 46.37493963101284
At time: 66.96151089668274 and batch: 650, loss is 3.6795714712142944 and perplexity is 39.62940809121399
At time: 67.67985558509827 and batch: 700, loss is 3.690955414772034 and perplexity is 40.083124678251856
At time: 68.39851260185242 and batch: 750, loss is 3.7920719623565673 and perplexity is 44.34819293401937
At time: 69.11815309524536 and batch: 800, loss is 3.7645635652542113 and perplexity is 43.14487182907744
At time: 69.83711504936218 and batch: 850, loss is 3.8304093360900877 and perplexity is 46.08139715306951
At time: 70.5567364692688 and batch: 900, loss is 3.790919075012207 and perplexity is 44.297093925000745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3439439747431505 and perplexity of 77.01066931160385
finished 5 epochs...
Completing Train Step...
At time: 72.35186576843262 and batch: 50, loss is 3.872221755981445 and perplexity is 48.04902077813318
At time: 73.08452916145325 and batch: 100, loss is 3.748602132797241 and perplexity is 42.46168469855096
At time: 73.80050039291382 and batch: 150, loss is 3.7623323249816893 and perplexity is 43.048712570598184
At time: 74.51744103431702 and batch: 200, loss is 3.6465323829650877 and perplexity is 38.341481691256625
At time: 75.23524045944214 and batch: 250, loss is 3.7921711587905884 and perplexity is 44.35259233481254
At time: 75.95265817642212 and batch: 300, loss is 3.7584299659729004 and perplexity is 42.88104839495613
At time: 76.6726233959198 and batch: 350, loss is 3.7635325479507444 and perplexity is 43.10041164321469
At time: 77.38860130310059 and batch: 400, loss is 3.6955355644226073 and perplexity is 40.26713245765146
At time: 78.10642457008362 and batch: 450, loss is 3.7289242553710937 and perplexity is 41.63429619472335
At time: 78.82138013839722 and batch: 500, loss is 3.615900182723999 and perplexity is 37.18480398234507
At time: 79.53654384613037 and batch: 550, loss is 3.6856990671157837 and perplexity is 39.87298660301231
At time: 80.25085520744324 and batch: 600, loss is 3.703369154930115 and perplexity is 40.58380741581076
At time: 80.96663689613342 and batch: 650, loss is 3.5462804079055785 and perplexity is 34.6840666762619
At time: 81.69132471084595 and batch: 700, loss is 3.5568440055847166 and perplexity is 35.05239722522402
At time: 82.42603182792664 and batch: 750, loss is 3.6596282529830932 and perplexity is 38.84689896795239
At time: 83.14760184288025 and batch: 800, loss is 3.631502366065979 and perplexity is 37.76951765531268
At time: 83.86636161804199 and batch: 850, loss is 3.700355257987976 and perplexity is 40.461676140727185
At time: 84.59271788597107 and batch: 900, loss is 3.6635928773880004 and perplexity is 39.00121803809614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356535611087328 and perplexity of 77.98649035309805
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 86.39885306358337 and batch: 50, loss is 3.780164203643799 and perplexity is 43.82323708001851
At time: 87.13572382926941 and batch: 100, loss is 3.6657751274108885 and perplexity is 39.0864213807376
At time: 87.86482763290405 and batch: 150, loss is 3.6770445728302 and perplexity is 39.52939501859856
At time: 88.5887553691864 and batch: 200, loss is 3.5460066843032836 and perplexity is 34.67457412781558
At time: 89.31187391281128 and batch: 250, loss is 3.687583303451538 and perplexity is 39.94818755912263
At time: 90.02950358390808 and batch: 300, loss is 3.6456419801712037 and perplexity is 38.307357523218876
At time: 90.74648523330688 and batch: 350, loss is 3.6360771560668947 and perplexity is 37.94270110393417
At time: 91.46447372436523 and batch: 400, loss is 3.560545744895935 and perplexity is 35.18239251788525
At time: 92.18181848526001 and batch: 450, loss is 3.5843533515930175 and perplexity is 36.03005140883023
At time: 92.90490341186523 and batch: 500, loss is 3.465712456703186 and perplexity is 31.99924973370614
At time: 93.62143421173096 and batch: 550, loss is 3.510971369743347 and perplexity is 33.48077420455836
At time: 94.34132790565491 and batch: 600, loss is 3.5244691038131712 and perplexity is 33.935752471408044
At time: 95.06362438201904 and batch: 650, loss is 3.35000807762146 and perplexity is 28.502963878990098
At time: 95.77976775169373 and batch: 700, loss is 3.3409222030639647 and perplexity is 28.24516247380625
At time: 96.5023365020752 and batch: 750, loss is 3.4340354061126708 and perplexity is 31.001494286466492
At time: 97.21946549415588 and batch: 800, loss is 3.3874143600463866 and perplexity is 29.589345879868137
At time: 97.93603539466858 and batch: 850, loss is 3.4339593839645386 and perplexity is 30.999137575857763
At time: 98.66582703590393 and batch: 900, loss is 3.3879598140716554 and perplexity is 29.60548991019628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326912657855308 and perplexity of 75.71018213880151
finished 7 epochs...
Completing Train Step...
At time: 100.4806261062622 and batch: 50, loss is 3.6862346363067626 and perplexity is 39.89434706568147
At time: 101.19826817512512 and batch: 100, loss is 3.5622057914733887 and perplexity is 35.24084543202773
At time: 101.91428422927856 and batch: 150, loss is 3.5732166528701783 and perplexity is 35.63102164263105
At time: 102.63051724433899 and batch: 200, loss is 3.447531666755676 and perplexity is 31.42273472502928
At time: 103.34832429885864 and batch: 250, loss is 3.5878925132751465 and perplexity is 36.15779350280573
At time: 104.06417632102966 and batch: 300, loss is 3.549405608177185 and perplexity is 34.79263088494512
At time: 104.78182291984558 and batch: 350, loss is 3.544150824546814 and perplexity is 34.61028265754803
At time: 105.49815511703491 and batch: 400, loss is 3.473411645889282 and perplexity is 32.246568867916814
At time: 106.2153594493866 and batch: 450, loss is 3.5042674684524537 and perplexity is 33.257073072106465
At time: 106.93184351921082 and batch: 500, loss is 3.390020899772644 and perplexity is 29.666572288505673
At time: 107.64833736419678 and batch: 550, loss is 3.436728777885437 and perplexity is 31.085105383430804
At time: 108.36984515190125 and batch: 600, loss is 3.459320025444031 and perplexity is 31.795349134057425
At time: 109.09870743751526 and batch: 650, loss is 3.2910040330886843 and perplexity is 26.86982831364321
At time: 109.81847071647644 and batch: 700, loss is 3.2851812553405764 and perplexity is 26.71382589970352
At time: 110.5362069606781 and batch: 750, loss is 3.386698842048645 and perplexity is 29.568181742913083
At time: 111.2541971206665 and batch: 800, loss is 3.346074938774109 and perplexity is 28.391077940194855
At time: 111.9708993434906 and batch: 850, loss is 3.3999758434295653 and perplexity is 29.963376226246258
At time: 112.68851470947266 and batch: 900, loss is 3.3625253343582155 and perplexity is 28.86198508828789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338610505404538 and perplexity of 76.60102864210883
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 114.46716165542603 and batch: 50, loss is 3.6568333768844603 and perplexity is 38.738478280290956
At time: 115.2074134349823 and batch: 100, loss is 3.5459370374679566 and perplexity is 34.672159237556976
At time: 115.92339730262756 and batch: 150, loss is 3.55936044216156 and perplexity is 35.140715436690584
At time: 116.6392092704773 and batch: 200, loss is 3.429489183425903 and perplexity is 30.860874476498555
At time: 117.3556797504425 and batch: 250, loss is 3.5680862140655516 and perplexity is 35.448686994865156
At time: 118.08187890052795 and batch: 300, loss is 3.522397322654724 and perplexity is 33.865517799397345
At time: 118.79739689826965 and batch: 350, loss is 3.5159645318984984 and perplexity is 33.64836720054664
At time: 119.51227068901062 and batch: 400, loss is 3.4405883836746214 and perplexity is 31.205313464805652
At time: 120.23501944541931 and batch: 450, loss is 3.47109245300293 and perplexity is 32.17186950938138
At time: 120.95246052742004 and batch: 500, loss is 3.351042938232422 and perplexity is 28.5324757413023
At time: 121.66849088668823 and batch: 550, loss is 3.389249534606934 and perplexity is 29.64369735165524
At time: 122.38383340835571 and batch: 600, loss is 3.40888765335083 and perplexity is 30.23159753324267
At time: 123.09886622428894 and batch: 650, loss is 3.239530611038208 and perplexity is 25.52173931264114
At time: 123.81467247009277 and batch: 700, loss is 3.2197243309020998 and perplexity is 25.02122165292452
At time: 124.53012108802795 and batch: 750, loss is 3.322257628440857 and perplexity is 27.722867896528264
At time: 125.24506855010986 and batch: 800, loss is 3.2733775186538696 and perplexity is 26.400356626850957
At time: 125.96146845817566 and batch: 850, loss is 3.322645163536072 and perplexity is 27.733613562805086
At time: 126.67755317687988 and batch: 900, loss is 3.290182752609253 and perplexity is 26.847769707554715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328213208342252 and perplexity of 75.80871111011162
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 128.478013753891 and batch: 50, loss is 3.641780037879944 and perplexity is 38.159702021166
At time: 129.22596907615662 and batch: 100, loss is 3.5286716985702515 and perplexity is 34.078670790361386
At time: 129.9523482322693 and batch: 150, loss is 3.5449804735183714 and perplexity is 34.63900895768512
At time: 130.68597745895386 and batch: 200, loss is 3.4163966083526613 and perplexity is 30.459459671369256
At time: 131.41346955299377 and batch: 250, loss is 3.554796586036682 and perplexity is 34.98070368036673
At time: 132.14445996284485 and batch: 300, loss is 3.508114609718323 and perplexity is 33.38526415680252
At time: 132.86875557899475 and batch: 350, loss is 3.4992096185684205 and perplexity is 33.0892884612991
At time: 133.59371089935303 and batch: 400, loss is 3.424434041976929 and perplexity is 30.705262043593027
At time: 134.31793689727783 and batch: 450, loss is 3.453224663734436 and perplexity is 31.60213443535602
At time: 135.04103302955627 and batch: 500, loss is 3.3324534606933596 and perplexity is 28.006971482249412
At time: 135.7649610042572 and batch: 550, loss is 3.372044448852539 and perplexity is 29.138037431303015
At time: 136.5200252532959 and batch: 600, loss is 3.3946181964874267 and perplexity is 29.803272308338133
At time: 137.24613070487976 and batch: 650, loss is 3.220344371795654 and perplexity is 25.03674064426346
At time: 137.9689700603485 and batch: 700, loss is 3.1974730396270754 and perplexity is 24.470615726107425
At time: 138.69883370399475 and batch: 750, loss is 3.3003566312789916 and perplexity is 27.122309860130255
At time: 139.4211301803589 and batch: 800, loss is 3.2522535848617555 and perplexity is 25.848526176020396
At time: 140.14670944213867 and batch: 850, loss is 3.2987755823135374 and perplexity is 27.07946204136055
At time: 140.8818085193634 and batch: 900, loss is 3.270467824935913 and perplexity is 26.323651323609095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321878093562714 and perplexity of 75.32997225785769
finished 10 epochs...
Completing Train Step...
At time: 142.72871088981628 and batch: 50, loss is 3.627732400894165 and perplexity is 37.62739595445411
At time: 143.45667624473572 and batch: 100, loss is 3.5095897006988523 and perplexity is 33.4345467980965
At time: 144.18295288085938 and batch: 150, loss is 3.5253929328918456 and perplexity is 33.96711779221478
At time: 144.91401553153992 and batch: 200, loss is 3.39922625541687 and perplexity is 29.94092445444796
At time: 145.64955520629883 and batch: 250, loss is 3.537793312072754 and perplexity is 34.39094531253898
At time: 146.37671041488647 and batch: 300, loss is 3.494209704399109 and perplexity is 32.92425777251062
At time: 147.10386180877686 and batch: 350, loss is 3.485661735534668 and perplexity is 32.64402167326211
At time: 147.83980059623718 and batch: 400, loss is 3.4117507314682007 and perplexity is 30.31827698440234
At time: 148.5762197971344 and batch: 450, loss is 3.44166428565979 and perplexity is 31.23890539107992
At time: 149.30499958992004 and batch: 500, loss is 3.3220951843261717 and perplexity is 27.718364845553236
At time: 150.030455827713 and batch: 550, loss is 3.363033862113953 and perplexity is 28.8766659412852
At time: 150.7554738521576 and batch: 600, loss is 3.3876189517974855 and perplexity is 29.595400235270038
At time: 151.47880840301514 and batch: 650, loss is 3.21503324508667 and perplexity is 24.904119837025455
At time: 152.2025501728058 and batch: 700, loss is 3.1940582275390623 and perplexity is 24.387195684670736
At time: 152.92696976661682 and batch: 750, loss is 3.2984787225723267 and perplexity is 27.07142443234712
At time: 153.6513476371765 and batch: 800, loss is 3.2527182340621947 and perplexity is 25.86053946380676
At time: 154.38571429252625 and batch: 850, loss is 3.302218885421753 and perplexity is 27.172865553192413
At time: 155.11032724380493 and batch: 900, loss is 3.2758023166656494 and perplexity is 26.46444983424718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321549925085616 and perplexity of 75.30525539145204
finished 11 epochs...
Completing Train Step...
At time: 156.9168417453766 and batch: 50, loss is 3.620436906814575 and perplexity is 37.353884424032515
At time: 157.65597319602966 and batch: 100, loss is 3.501252365112305 and perplexity is 33.15695057568972
At time: 158.38444542884827 and batch: 150, loss is 3.516297378540039 and perplexity is 33.659568810668404
At time: 159.11929440498352 and batch: 200, loss is 3.3904230403900146 and perplexity is 29.678504821318285
At time: 159.86646676063538 and batch: 250, loss is 3.528383803367615 and perplexity is 34.06886111667536
At time: 160.60422372817993 and batch: 300, loss is 3.4857018327713014 and perplexity is 32.64533063456649
At time: 161.32938599586487 and batch: 350, loss is 3.4772407484054564 and perplexity is 32.37028098794616
At time: 162.06103587150574 and batch: 400, loss is 3.4036497688293457 and perplexity is 30.07366190184898
At time: 162.79713439941406 and batch: 450, loss is 3.434104743003845 and perplexity is 31.003643908225257
At time: 163.52520108222961 and batch: 500, loss is 3.315425419807434 and perplexity is 27.534105046999823
At time: 164.2543089389801 and batch: 550, loss is 3.3569619846343994 and perplexity is 28.701861596007895
At time: 164.98410177230835 and batch: 600, loss is 3.382743229866028 and perplexity is 29.451452502998592
At time: 165.7147834300995 and batch: 650, loss is 3.210902757644653 and perplexity is 24.801465834266427
At time: 166.438152551651 and batch: 700, loss is 3.1911656284332275 and perplexity is 24.316755231350943
At time: 167.16975736618042 and batch: 750, loss is 3.2967536306381224 and perplexity is 27.02476399475529
At time: 167.90032267570496 and batch: 800, loss is 3.2522748804092405 and perplexity is 25.849076640398195
At time: 168.62991380691528 and batch: 850, loss is 3.303134036064148 and perplexity is 27.19774420067792
At time: 169.35524439811707 and batch: 900, loss is 3.2773687648773193 and perplexity is 26.505937510027234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3220540921982025 and perplexity of 75.34323139694743
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 171.1412661075592 and batch: 50, loss is 3.617937364578247 and perplexity is 37.26063340320675
At time: 171.87942385673523 and batch: 100, loss is 3.5011882162094117 and perplexity is 33.15482366190733
At time: 172.59855484962463 and batch: 150, loss is 3.5172697496414185 and perplexity is 33.69231432047842
At time: 173.32827949523926 and batch: 200, loss is 3.3923905181884764 and perplexity is 29.73695410062084
At time: 174.0553855895996 and batch: 250, loss is 3.5294384002685546 and perplexity is 34.10480898395057
At time: 174.77627849578857 and batch: 300, loss is 3.485950689315796 and perplexity is 32.65345564968175
At time: 175.51471662521362 and batch: 350, loss is 3.4752639865875246 and perplexity is 32.30635585564992
At time: 176.24566078186035 and batch: 400, loss is 3.402462821006775 and perplexity is 30.037987210524786
At time: 176.97022080421448 and batch: 450, loss is 3.4308882570266723 and perplexity is 30.904081329010854
At time: 177.70103406906128 and batch: 500, loss is 3.3113623046875 and perplexity is 27.422457779916797
At time: 178.43328309059143 and batch: 550, loss is 3.3521331691741945 and perplexity is 28.563599692265186
At time: 179.17034721374512 and batch: 600, loss is 3.380035834312439 and perplexity is 29.371823613535785
At time: 179.8986976146698 and batch: 650, loss is 3.203617181777954 and perplexity is 24.62142950385176
At time: 180.62327933311462 and batch: 700, loss is 3.1828298330307008 and perplexity is 24.114898223092773
At time: 181.34880328178406 and batch: 750, loss is 3.2889944076538087 and perplexity is 26.81588424507435
At time: 182.07355451583862 and batch: 800, loss is 3.2434687566757203 and perplexity is 25.6224458072435
At time: 182.7973711490631 and batch: 850, loss is 3.2931697225570677 and perplexity is 26.928083075933003
At time: 183.52174377441406 and batch: 900, loss is 3.2679058837890627 and perplexity is 26.256297992622518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320369772715111 and perplexity of 75.21643613622626
finished 13 epochs...
Completing Train Step...
At time: 185.33545875549316 and batch: 50, loss is 3.6143995094299317 and perplexity is 37.12904358959424
At time: 186.0614197254181 and batch: 100, loss is 3.4958496713638305 and perplexity is 32.978296766566594
At time: 186.78682398796082 and batch: 150, loss is 3.512029404640198 and perplexity is 33.51621677847406
At time: 187.51048135757446 and batch: 200, loss is 3.38751736164093 and perplexity is 29.59239378664219
At time: 188.23624968528748 and batch: 250, loss is 3.524990825653076 and perplexity is 33.95346211397793
At time: 188.9653832912445 and batch: 300, loss is 3.4815059328079223 and perplexity is 32.50864106138148
At time: 189.68948554992676 and batch: 350, loss is 3.47148145198822 and perplexity is 32.18438676841811
At time: 190.4133529663086 and batch: 400, loss is 3.3986802387237547 and perplexity is 29.924580672283525
At time: 191.15549039840698 and batch: 450, loss is 3.4279889965057375 and perplexity is 30.81461210604192
At time: 191.8795359134674 and batch: 500, loss is 3.308557276725769 and perplexity is 27.34564480070686
At time: 192.6078863143921 and batch: 550, loss is 3.3499506664276124 and perplexity is 28.501327536778245
At time: 193.33364009857178 and batch: 600, loss is 3.3783836793899535 and perplexity is 29.32333687540103
At time: 194.05911827087402 and batch: 650, loss is 3.202640371322632 and perplexity is 24.59739077663886
At time: 194.7840974330902 and batch: 700, loss is 3.18248206615448 and perplexity is 24.106513318347915
At time: 195.51045155525208 and batch: 750, loss is 3.288616666793823 and perplexity is 26.805756702812026
At time: 196.23417615890503 and batch: 800, loss is 3.243869843482971 and perplexity is 25.632724693451312
At time: 196.95934200286865 and batch: 850, loss is 3.2946500635147093 and perplexity is 26.967975340012263
At time: 197.6853370666504 and batch: 900, loss is 3.270431160926819 and perplexity is 26.322686210710142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31971761625107 and perplexity of 75.16739924279943
finished 14 epochs...
Completing Train Step...
At time: 199.49277472496033 and batch: 50, loss is 3.612218346595764 and perplexity is 37.04814735564967
At time: 200.23320269584656 and batch: 100, loss is 3.4931700658798217 and perplexity is 32.89004623281007
At time: 200.96097469329834 and batch: 150, loss is 3.509204511642456 and perplexity is 33.421670656597904
At time: 201.69268012046814 and batch: 200, loss is 3.384732890129089 and perplexity is 29.51010922185398
At time: 202.4208447933197 and batch: 250, loss is 3.5221501398086548 and perplexity is 33.85714785881956
At time: 203.1483325958252 and batch: 300, loss is 3.478931427001953 and perplexity is 32.4250550187687
At time: 203.87587118148804 and batch: 350, loss is 3.4690841150283815 and perplexity is 32.10732236007451
At time: 204.60359907150269 and batch: 400, loss is 3.3964163398742677 and perplexity is 29.856911075990656
At time: 205.33114433288574 and batch: 450, loss is 3.425982103347778 and perplexity is 30.752832485125797
At time: 206.05945801734924 and batch: 500, loss is 3.3068048620223998 and perplexity is 27.297765854822217
At time: 206.7883276939392 and batch: 550, loss is 3.3484219789505003 and perplexity is 28.45779119949679
At time: 207.51733326911926 and batch: 600, loss is 3.3772112798690794 and perplexity is 29.28897835419032
At time: 208.2466881275177 and batch: 650, loss is 3.201881184577942 and perplexity is 24.57872385034536
At time: 208.9759237766266 and batch: 700, loss is 3.182038116455078 and perplexity is 24.09581361424755
At time: 209.71421027183533 and batch: 750, loss is 3.2883138704299926 and perplexity is 26.797641245880122
At time: 210.45176458358765 and batch: 800, loss is 3.243995985984802 and perplexity is 25.635958273414538
At time: 211.18082523345947 and batch: 850, loss is 3.295389232635498 and perplexity is 26.987916603709028
At time: 211.9097146987915 and batch: 900, loss is 3.271625919342041 and perplexity is 26.354154256184458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319632752300942 and perplexity of 75.16102051104482
finished 15 epochs...
Completing Train Step...
At time: 213.71327114105225 and batch: 50, loss is 3.610360851287842 and perplexity is 36.97939446961906
At time: 214.45353984832764 and batch: 100, loss is 3.4910495471954346 and perplexity is 32.82037616968487
At time: 215.1790008544922 and batch: 150, loss is 3.506955146789551 and perplexity is 33.34657761278629
At time: 215.90676045417786 and batch: 200, loss is 3.3825150775909423 and perplexity is 29.444733853572174
At time: 216.63561034202576 and batch: 250, loss is 3.519827046394348 and perplexity is 33.77858583039867
At time: 217.3674840927124 and batch: 300, loss is 3.476819829940796 and perplexity is 32.35665860612924
At time: 218.09268617630005 and batch: 350, loss is 3.4670706748962403 and perplexity is 32.04274122564041
At time: 218.8196656703949 and batch: 400, loss is 3.394535722732544 and perplexity is 29.800814421920023
At time: 219.5466639995575 and batch: 450, loss is 3.4242402935028076 and perplexity is 30.699313522201667
At time: 220.2894628047943 and batch: 500, loss is 3.3053140830993653 and perplexity is 27.25710123934691
At time: 221.02334308624268 and batch: 550, loss is 3.347082004547119 and perplexity is 28.41968402473396
At time: 221.75348162651062 and batch: 600, loss is 3.3761559772491454 and perplexity is 29.2580859218973
At time: 222.48318338394165 and batch: 650, loss is 3.201091170310974 and perplexity is 24.559313975888106
At time: 223.2200367450714 and batch: 700, loss is 3.181493377685547 and perplexity is 24.082691264839205
At time: 223.9481019973755 and batch: 750, loss is 3.2879618978500367 and perplexity is 26.78821087066818
At time: 224.67453145980835 and batch: 800, loss is 3.2439580583572387 and perplexity is 25.634985980775404
At time: 225.39922666549683 and batch: 850, loss is 3.295765132904053 and perplexity is 26.99806327575699
At time: 226.1309084892273 and batch: 900, loss is 3.272249436378479 and perplexity is 26.37059164430734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31972764942744 and perplexity of 75.16815341435665
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 227.94997906684875 and batch: 50, loss is 3.609642810821533 and perplexity is 36.95285129864613
At time: 228.67678380012512 and batch: 100, loss is 3.490849690437317 and perplexity is 32.81381745112734
At time: 229.40194869041443 and batch: 150, loss is 3.50704607963562 and perplexity is 33.34961004986728
At time: 230.1320300102234 and batch: 200, loss is 3.3830011320114135 and perplexity is 29.45904907532788
At time: 230.8615095615387 and batch: 250, loss is 3.52016996383667 and perplexity is 33.79017108293594
At time: 231.58690190315247 and batch: 300, loss is 3.476597213745117 and perplexity is 32.34945629159113
At time: 232.3140206336975 and batch: 350, loss is 3.466269083023071 and perplexity is 32.017066316453935
At time: 233.0406105518341 and batch: 400, loss is 3.393740386962891 and perplexity is 29.777122191133678
At time: 233.76857113838196 and batch: 450, loss is 3.4235904693603514 and perplexity is 30.679370847435642
At time: 234.5052194595337 and batch: 500, loss is 3.304407625198364 and perplexity is 27.23240501930301
At time: 235.2372694015503 and batch: 550, loss is 3.3454600048065184 and perplexity is 28.373624668847608
At time: 235.96818614006042 and batch: 600, loss is 3.3749209690093993 and perplexity is 29.22197424840165
At time: 236.69942569732666 and batch: 650, loss is 3.198960838317871 and perplexity is 24.507050172988105
At time: 237.42910528182983 and batch: 700, loss is 3.17889253616333 and perplexity is 24.02013738305893
At time: 238.1567976474762 and batch: 750, loss is 3.285646185874939 and perplexity is 26.726248860737112
At time: 238.88896441459656 and batch: 800, loss is 3.2406492137908938 and perplexity is 25.550303973765363
At time: 239.61991953849792 and batch: 850, loss is 3.292275867462158 and perplexity is 26.904024025933527
At time: 240.34515929222107 and batch: 900, loss is 3.2682111644744873 and perplexity is 26.264314756889807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319017802199272 and perplexity of 75.11481444250933
finished 17 epochs...
Completing Train Step...
At time: 242.1465392112732 and batch: 50, loss is 3.608884963989258 and perplexity is 36.92485730626496
At time: 242.88565754890442 and batch: 100, loss is 3.4898368310928345 and perplexity is 32.780598495398586
At time: 243.61458802223206 and batch: 150, loss is 3.5060999679565428 and perplexity is 33.31807251567364
At time: 244.34426641464233 and batch: 200, loss is 3.3820643854141235 and perplexity is 29.43146633238444
At time: 245.072039604187 and batch: 250, loss is 3.519398980140686 and perplexity is 33.76412945207286
At time: 245.8086175918579 and batch: 300, loss is 3.475691442489624 and perplexity is 32.32016835004154
At time: 246.5310137271881 and batch: 350, loss is 3.465574893951416 and perplexity is 31.994848131613395
At time: 247.25410556793213 and batch: 400, loss is 3.392990007400513 and perplexity is 29.754786428412995
At time: 247.97735810279846 and batch: 450, loss is 3.422913842201233 and perplexity is 30.658619373192845
At time: 248.7003767490387 and batch: 500, loss is 3.303805027008057 and perplexity is 27.215999764705458
At time: 249.43015503883362 and batch: 550, loss is 3.3450280380249025 and perplexity is 28.36137085232801
At time: 250.15891194343567 and batch: 600, loss is 3.374618420600891 and perplexity is 29.213134523889043
At time: 250.88718962669373 and batch: 650, loss is 3.198731055259705 and perplexity is 24.501419514992403
At time: 251.6162931919098 and batch: 700, loss is 3.1788226747512818 and perplexity is 24.01845936095894
At time: 252.34349489212036 and batch: 750, loss is 3.285568842887878 and perplexity is 26.72418185275261
At time: 253.08084845542908 and batch: 800, loss is 3.240793323516846 and perplexity is 25.553986286391172
At time: 253.80940508842468 and batch: 850, loss is 3.292603611946106 and perplexity is 26.91284311652923
At time: 254.5465121269226 and batch: 900, loss is 3.268839569091797 and perplexity is 26.2808245604286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318622327830694 and perplexity of 75.08511433190013
finished 18 epochs...
Completing Train Step...
At time: 256.3785593509674 and batch: 50, loss is 3.608268938064575 and perplexity is 36.902117641729646
At time: 257.1295986175537 and batch: 100, loss is 3.4890691661834716 and perplexity is 32.75544363671224
At time: 257.86233162879944 and batch: 150, loss is 3.50533718585968 and perplexity is 33.29266777682848
At time: 258.59830808639526 and batch: 200, loss is 3.381308116912842 and perplexity is 29.40921665587594
At time: 259.3334722518921 and batch: 250, loss is 3.5187153387069703 and perplexity is 33.74105478250486
At time: 260.0615699291229 and batch: 300, loss is 3.474985318183899 and perplexity is 32.29735434931261
At time: 260.7928133010864 and batch: 350, loss is 3.4649758100509644 and perplexity is 31.975686273553784
At time: 261.52302408218384 and batch: 400, loss is 3.392392454147339 and perplexity is 29.737011670196402
At time: 262.25144481658936 and batch: 450, loss is 3.4223785972595215 and perplexity is 30.64221389312026
At time: 262.9790291786194 and batch: 500, loss is 3.3033181762695314 and perplexity is 27.20275286001462
At time: 263.7171914577484 and batch: 550, loss is 3.344660830497742 and perplexity is 28.350958255379435
At time: 264.4619104862213 and batch: 600, loss is 3.374364275932312 and perplexity is 29.20571110484846
At time: 265.18903827667236 and batch: 650, loss is 3.1985463523864746 and perplexity is 24.49689445031885
At time: 265.91863918304443 and batch: 700, loss is 3.1787492656707763 and perplexity is 24.016696252656963
At time: 266.64873218536377 and batch: 750, loss is 3.285515127182007 and perplexity is 26.722746383014538
At time: 267.3768870830536 and batch: 800, loss is 3.2408889389038085 and perplexity is 25.55642975749324
At time: 268.1039791107178 and batch: 850, loss is 3.2928613424301147 and perplexity is 26.919780270531337
At time: 268.8323223590851 and batch: 900, loss is 3.269280180931091 and perplexity is 26.292406754314833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318402852097603 and perplexity of 75.06863677966551
finished 19 epochs...
Completing Train Step...
At time: 270.6478581428528 and batch: 50, loss is 3.607723145484924 and perplexity is 36.88198223512498
At time: 271.3738775253296 and batch: 100, loss is 3.4884220695495607 and perplexity is 32.73425455582351
At time: 272.0988030433655 and batch: 150, loss is 3.5046707725524904 and perplexity is 33.270488491096565
At time: 272.8225073814392 and batch: 200, loss is 3.3806483936309815 and perplexity is 29.389821109497007
At time: 273.547358751297 and batch: 250, loss is 3.5180850076675414 and perplexity is 33.719793449915265
At time: 274.27243638038635 and batch: 300, loss is 3.4743704795837402 and perplexity is 32.2775027925527
At time: 274.995733499527 and batch: 350, loss is 3.464426121711731 and perplexity is 31.95811444162823
At time: 275.7195043563843 and batch: 400, loss is 3.3918636226654053 and perplexity is 29.721289959680707
At time: 276.4466178417206 and batch: 450, loss is 3.4219036006927492 and perplexity is 30.627662402950257
At time: 277.1717793941498 and batch: 500, loss is 3.302890868186951 and perplexity is 27.191131387000606
At time: 277.89710545539856 and batch: 550, loss is 3.3443178796768187 and perplexity is 28.34123693803396
At time: 278.63079619407654 and batch: 600, loss is 3.3741217374801638 and perplexity is 29.198628455825286
At time: 279.36139392852783 and batch: 650, loss is 3.1983703804016113 and perplexity is 24.49258406244432
At time: 280.0918276309967 and batch: 700, loss is 3.178660192489624 and perplexity is 24.014557104392757
At time: 280.8157813549042 and batch: 750, loss is 3.2854584217071534 and perplexity is 26.72123109995433
At time: 281.5388867855072 and batch: 800, loss is 3.2409469175338743 and perplexity is 25.557911527235028
At time: 282.2731580734253 and batch: 850, loss is 3.2930595874786377 and perplexity is 26.92511751270073
At time: 282.99702620506287 and batch: 900, loss is 3.269606938362122 and perplexity is 26.300999397380018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318282035932149 and perplexity of 75.05956782267326
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc267f18b70>
ELAPSED
1456.0194926261902


RESULTS SO FAR:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.88670270084627, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.31332278986565, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.23850096980998603, 'data': 'ptb', 'dropout': 0.3583059732624252, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.00308858124657, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.26724264066418335, 'data': 'ptb', 'dropout': 0.9244537941766774, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -75.05956782267326, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.3239422609632767, 'data': 'ptb', 'dropout': 0.8373723041896595, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8627506693955694, 'data': 'ptb', 'dropout': 0.6116306336471359, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0322530269622803 and batch: 50, loss is 7.061911115646362 and perplexity is 1166.6726829919737
At time: 1.8174657821655273 and batch: 100, loss is 6.1893790912628175 and perplexity is 487.54329232431496
At time: 2.6015467643737793 and batch: 150, loss is 5.997245082855224 and perplexity is 402.3189101131826
At time: 3.387758731842041 and batch: 200, loss is 5.829792165756226 and perplexity is 340.28794823347164
At time: 4.1751158237457275 and batch: 250, loss is 5.863247346878052 and perplexity is 351.86491821633655
At time: 4.96122145652771 and batch: 300, loss is 5.773087520599365 and perplexity is 321.5289288817699
At time: 5.7474353313446045 and batch: 350, loss is 5.744804515838623 and perplexity is 312.5625208352607
At time: 6.5327723026275635 and batch: 400, loss is 5.600021123886108 and perplexity is 270.4321199431189
At time: 7.3182313442230225 and batch: 450, loss is 5.602609529495239 and perplexity is 271.13301466786345
At time: 8.103881597518921 and batch: 500, loss is 5.552914943695068 and perplexity is 257.9884828206227
At time: 8.890401601791382 and batch: 550, loss is 5.597272691726684 and perplexity is 269.68987607841484
At time: 9.682095289230347 and batch: 600, loss is 5.520482234954834 and perplexity is 249.7554489616271
At time: 10.467453956604004 and batch: 650, loss is 5.420117807388306 and perplexity is 225.90573429903043
At time: 11.25972032546997 and batch: 700, loss is 5.518666582107544 and perplexity is 249.30239119187095
At time: 12.045515060424805 and batch: 750, loss is 5.48306562423706 and perplexity is 240.5831154520533
At time: 12.831238031387329 and batch: 800, loss is 5.47210729598999 and perplexity is 237.96111928801344
At time: 13.61581015586853 and batch: 850, loss is 5.495784759521484 and perplexity is 243.66266774931606
At time: 14.400022029876709 and batch: 900, loss is 5.395122880935669 and perplexity is 220.32921969377915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.277188235766267 and perplexity of 195.81850508456188
finished 1 epochs...
Completing Train Step...
At time: 16.264118671417236 and batch: 50, loss is 5.172458267211914 and perplexity is 176.34781513047423
At time: 16.98430347442627 and batch: 100, loss is 5.0169202327728275 and perplexity is 150.94570955176076
At time: 17.717050313949585 and batch: 150, loss is 4.97725754737854 and perplexity is 145.07597161432204
At time: 18.438034296035767 and batch: 200, loss is 4.838057651519775 and perplexity is 126.22394258917767
At time: 19.157029390335083 and batch: 250, loss is 4.9176968002319335 and perplexity is 136.68743190161376
At time: 19.876939296722412 and batch: 300, loss is 4.8387657642364506 and perplexity is 126.313355021378
At time: 20.59869360923767 and batch: 350, loss is 4.824590816497802 and perplexity is 124.53550008414055
At time: 21.32606077194214 and batch: 400, loss is 4.681068153381347 and perplexity is 107.88524904094248
At time: 22.055580377578735 and batch: 450, loss is 4.69690318107605 and perplexity is 109.60721263652678
At time: 22.7851459980011 and batch: 500, loss is 4.6014816856384275 and perplexity is 99.63182938117616
At time: 23.51423478126526 and batch: 550, loss is 4.662181692123413 and perplexity is 105.86679915025923
At time: 24.24400758743286 and batch: 600, loss is 4.604281663894653 and perplexity is 99.91118725254327
At time: 24.971431016921997 and batch: 650, loss is 4.468342361450195 and perplexity is 87.21203709339886
At time: 25.699445962905884 and batch: 700, loss is 4.515689516067505 and perplexity is 91.44059404929172
At time: 26.427130222320557 and batch: 750, loss is 4.547438678741455 and perplexity is 94.39033446255291
At time: 27.15339756011963 and batch: 800, loss is 4.4957116508483885 and perplexity is 89.63193293473562
At time: 27.885047912597656 and batch: 850, loss is 4.5500836849212645 and perplexity is 94.64032795193546
At time: 28.618958473205566 and batch: 900, loss is 4.4830997276306155 and perplexity is 88.50860045995148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.586653617963399 and perplexity of 98.16538103924889
finished 2 epochs...
Completing Train Step...
At time: 30.41099715232849 and batch: 50, loss is 4.520737924575806 and perplexity is 91.90339073273222
At time: 31.14519453048706 and batch: 100, loss is 4.392149858474731 and perplexity is 80.8139709578779
At time: 31.866652965545654 and batch: 150, loss is 4.388444213867188 and perplexity is 80.51505727816821
At time: 32.588321685791016 and batch: 200, loss is 4.2759483575820925 and perplexity is 71.94833971885129
At time: 33.30905866622925 and batch: 250, loss is 4.414049091339112 and perplexity is 82.60325539723452
At time: 34.033286809921265 and batch: 300, loss is 4.367399616241455 and perplexity is 78.83835493832783
At time: 34.759175300598145 and batch: 350, loss is 4.365344524383545 and perplexity is 78.67650124607005
At time: 35.49677658081055 and batch: 400, loss is 4.266400227546692 and perplexity is 71.26463685156729
At time: 36.22333025932312 and batch: 450, loss is 4.303515958786011 and perplexity is 73.95937520831275
At time: 36.949718952178955 and batch: 500, loss is 4.183964633941651 and perplexity is 65.62551929463935
At time: 37.67624497413635 and batch: 550, loss is 4.254730668067932 and perplexity is 70.43784347296024
At time: 38.40134024620056 and batch: 600, loss is 4.2447392988204955 and perplexity is 69.73757710522922
At time: 39.129655838012695 and batch: 650, loss is 4.093566403388977 and perplexity is 59.95332863123661
At time: 39.855238914489746 and batch: 700, loss is 4.1144693040847775 and perplexity is 61.21971658679905
At time: 40.577316761016846 and batch: 750, loss is 4.200540614128113 and perplexity is 66.722392360389
At time: 41.29999542236328 and batch: 800, loss is 4.159477195739746 and perplexity is 64.03803448958656
At time: 42.022238969802856 and batch: 850, loss is 4.22717670917511 and perplexity is 68.52349705671558
At time: 42.749738931655884 and batch: 900, loss is 4.172042679786682 and perplexity is 64.84778015921515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411066342706549 and perplexity of 82.35723773697033
finished 3 epochs...
Completing Train Step...
At time: 44.54817986488342 and batch: 50, loss is 4.239252347946167 and perplexity is 69.35597830986735
At time: 45.28743624687195 and batch: 100, loss is 4.113469939231873 and perplexity is 61.15856631454323
At time: 46.01292657852173 and batch: 150, loss is 4.115867028236389 and perplexity is 61.30534469152175
At time: 46.7427864074707 and batch: 200, loss is 4.002715249061584 and perplexity is 54.74659905561612
At time: 47.47517681121826 and batch: 250, loss is 4.14856689453125 and perplexity is 63.34315779438346
At time: 48.203596115112305 and batch: 300, loss is 4.113627004623413 and perplexity is 61.1681729631237
At time: 48.930832624435425 and batch: 350, loss is 4.105439372062683 and perplexity is 60.66939512969655
At time: 49.66216254234314 and batch: 400, loss is 4.030986127853393 and perplexity is 56.31641906316945
At time: 50.38962531089783 and batch: 450, loss is 4.073128504753113 and perplexity is 58.74044519112724
At time: 51.115554094314575 and batch: 500, loss is 3.94969847202301 and perplexity is 51.91970922945717
At time: 51.841652154922485 and batch: 550, loss is 4.019500827789306 and perplexity is 55.67330832111035
At time: 52.56765699386597 and batch: 600, loss is 4.023562273979187 and perplexity is 55.89988226455765
At time: 53.295000314712524 and batch: 650, loss is 3.8706793928146364 and perplexity is 47.97496886043898
At time: 54.03549647331238 and batch: 700, loss is 3.883857536315918 and perplexity is 48.61137399363634
At time: 54.76403784751892 and batch: 750, loss is 3.9844586420059205 and perplexity is 53.75618026143144
At time: 55.49111247062683 and batch: 800, loss is 3.9547722482681276 and perplexity is 52.183807638354324
At time: 56.21884322166443 and batch: 850, loss is 4.018354892730713 and perplexity is 55.60954686549845
At time: 56.946263551712036 and batch: 900, loss is 3.9692376136779783 and perplexity is 52.94415155234925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356324914383562 and perplexity of 77.97006058755181
finished 4 epochs...
Completing Train Step...
At time: 58.7725396156311 and batch: 50, loss is 4.046814947128296 and perplexity is 57.21493393723147
At time: 59.49930167198181 and batch: 100, loss is 3.9221052169799804 and perplexity is 50.50666041446305
At time: 60.22563099861145 and batch: 150, loss is 3.9254343557357787 and perplexity is 50.67508429274931
At time: 60.95287370681763 and batch: 200, loss is 3.8166374015808104 and perplexity is 45.4511172182414
At time: 61.680726051330566 and batch: 250, loss is 3.9667503213882447 and perplexity is 52.812627609458545
At time: 62.407944202423096 and batch: 300, loss is 3.937083058357239 and perplexity is 51.268834777476066
At time: 63.136775732040405 and batch: 350, loss is 3.922064299583435 and perplexity is 50.50459385569008
At time: 63.866968870162964 and batch: 400, loss is 3.8584144830703737 and perplexity is 47.39015387988648
At time: 64.59609532356262 and batch: 450, loss is 3.9034757900238035 and perplexity is 49.57446040969612
At time: 65.32958197593689 and batch: 500, loss is 3.783470129966736 and perplexity is 43.96835321249248
At time: 66.05825519561768 and batch: 550, loss is 3.8483955240249634 and perplexity is 46.917724446428544
At time: 66.78617930412292 and batch: 600, loss is 3.860365142822266 and perplexity is 47.482686165857935
At time: 67.51277756690979 and batch: 650, loss is 3.7085927438735964 and perplexity is 40.79635519132154
At time: 68.24051690101624 and batch: 700, loss is 3.7195307350158693 and perplexity is 41.24503471606203
At time: 68.9677197933197 and batch: 750, loss is 3.8214732933044435 and perplexity is 45.67144621402903
At time: 69.7053017616272 and batch: 800, loss is 3.795580759048462 and perplexity is 44.50407504606679
At time: 70.43206334114075 and batch: 850, loss is 3.8604737091064454 and perplexity is 47.48784146449857
At time: 71.16084003448486 and batch: 900, loss is 3.8125490951538086 and perplexity is 45.26567844715257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.339170691085188 and perplexity of 76.64395146272952
finished 5 epochs...
Completing Train Step...
At time: 72.97415781021118 and batch: 50, loss is 3.8980137300491333 and perplexity is 49.304419893840674
At time: 73.71618723869324 and batch: 100, loss is 3.774003038406372 and perplexity is 43.55406493364487
At time: 74.44445967674255 and batch: 150, loss is 3.7799508142471314 and perplexity is 43.813886663573335
At time: 75.18067049980164 and batch: 200, loss is 3.673601222038269 and perplexity is 39.3935155196034
At time: 75.90901803970337 and batch: 250, loss is 3.8221317434310915 and perplexity is 45.701528486323994
At time: 76.63692784309387 and batch: 300, loss is 3.7961056089401244 and perplexity is 44.5274391358169
At time: 77.3660659790039 and batch: 350, loss is 3.7781918239593506 and perplexity is 43.736886203846176
At time: 78.0947892665863 and batch: 400, loss is 3.7226004076004027 and perplexity is 41.37183799107543
At time: 78.84144353866577 and batch: 450, loss is 3.768577480316162 and perplexity is 43.31839971006267
At time: 79.57338333129883 and batch: 500, loss is 3.654506607055664 and perplexity is 38.64844753883913
At time: 80.30353426933289 and batch: 550, loss is 3.714764895439148 and perplexity is 41.04893515795748
At time: 81.03484964370728 and batch: 600, loss is 3.729319496154785 and perplexity is 41.65075501896482
At time: 81.76408624649048 and batch: 650, loss is 3.5805825424194335 and perplexity is 35.89444479446835
At time: 82.49615049362183 and batch: 700, loss is 3.5880056476593016 and perplexity is 36.161884423913634
At time: 83.22456812858582 and batch: 750, loss is 3.693319225311279 and perplexity is 40.17798566333992
At time: 83.95424747467041 and batch: 800, loss is 3.669169211387634 and perplexity is 39.21930936617503
At time: 84.68716216087341 and batch: 850, loss is 3.734002594947815 and perplexity is 41.846267063311195
At time: 85.41845440864563 and batch: 900, loss is 3.6853410387039185 and perplexity is 39.85871349618386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347427995237585 and perplexity of 77.27944399815931
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 87.24869060516357 and batch: 50, loss is 3.802625732421875 and perplexity is 44.81871207318539
At time: 87.99460697174072 and batch: 100, loss is 3.687215690612793 and perplexity is 39.93350479144359
At time: 88.74037313461304 and batch: 150, loss is 3.688947763442993 and perplexity is 40.002732466487686
At time: 89.46918725967407 and batch: 200, loss is 3.5729457473754884 and perplexity is 35.62137031044513
At time: 90.19360947608948 and batch: 250, loss is 3.71328471660614 and perplexity is 40.988220338499616
At time: 90.939457654953 and batch: 300, loss is 3.678561358451843 and perplexity is 39.58939813102031
At time: 91.66432905197144 and batch: 350, loss is 3.649494285583496 and perplexity is 38.45521377480329
At time: 92.39144206047058 and batch: 400, loss is 3.5895097827911377 and perplexity is 36.21631771195689
At time: 93.11759972572327 and batch: 450, loss is 3.620057587623596 and perplexity is 37.3397180657685
At time: 93.84579658508301 and batch: 500, loss is 3.4970224714279174 and perplexity is 33.01699640416081
At time: 94.57545137405396 and batch: 550, loss is 3.5406670713424684 and perplexity is 34.48991875523505
At time: 95.30855202674866 and batch: 600, loss is 3.548847312927246 and perplexity is 34.77321174570017
At time: 96.03579878807068 and batch: 650, loss is 3.387342267036438 and perplexity is 29.587212771753265
At time: 96.7657561302185 and batch: 700, loss is 3.372840332984924 and perplexity is 29.161237163868325
At time: 97.49709630012512 and batch: 750, loss is 3.4713703918457033 and perplexity is 32.18081256431628
At time: 98.22119951248169 and batch: 800, loss is 3.424423141479492 and perplexity is 30.704927342787034
At time: 98.95079016685486 and batch: 850, loss is 3.474892692565918 and perplexity is 32.294362925450244
At time: 99.68282151222229 and batch: 900, loss is 3.417645993232727 and perplexity is 30.497539042679637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3101304981806505 and perplexity of 74.45020392259887
finished 7 epochs...
Completing Train Step...
At time: 101.49337220191956 and batch: 50, loss is 3.708309350013733 and perplexity is 40.78479539282083
At time: 102.2180323600769 and batch: 100, loss is 3.5841043519973756 and perplexity is 36.02108105745175
At time: 102.94205284118652 and batch: 150, loss is 3.584523196220398 and perplexity is 36.03617143919869
At time: 103.66753625869751 and batch: 200, loss is 3.4749817419052125 and perplexity is 32.29723884517916
At time: 104.38965821266174 and batch: 250, loss is 3.613702335357666 and perplexity is 37.10316720429611
At time: 105.11387920379639 and batch: 300, loss is 3.5858018350601197 and perplexity is 36.08227815827489
At time: 105.84089684486389 and batch: 350, loss is 3.558621129989624 and perplexity is 35.11474507932127
At time: 106.57836389541626 and batch: 400, loss is 3.504401707649231 and perplexity is 33.261537774545154
At time: 107.30535340309143 and batch: 450, loss is 3.540333333015442 and perplexity is 34.47841006800223
At time: 108.02605533599854 and batch: 500, loss is 3.4226877975463865 and perplexity is 30.651689939368804
At time: 108.75677990913391 and batch: 550, loss is 3.468314280509949 and perplexity is 32.08261454671133
At time: 109.47973775863647 and batch: 600, loss is 3.4844719219207763 and perplexity is 32.605204469061405
At time: 110.20232677459717 and batch: 650, loss is 3.3284987735748293 and perplexity is 27.896431392700027
At time: 110.9301552772522 and batch: 700, loss is 3.31831476688385 and perplexity is 27.613775675541575
At time: 111.65749454498291 and batch: 750, loss is 3.423946747779846 and perplexity is 30.690303192555934
At time: 112.38661551475525 and batch: 800, loss is 3.383986473083496 and perplexity is 29.488090591876126
At time: 113.11518025398254 and batch: 850, loss is 3.441486077308655 and perplexity is 31.23333885327541
At time: 113.84536123275757 and batch: 900, loss is 3.3917250442504883 and perplexity is 29.71717151579911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320612241144049 and perplexity of 75.23467595852773
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 115.66429829597473 and batch: 50, loss is 3.6734113025665285 and perplexity is 39.386034634351276
At time: 116.40732622146606 and batch: 100, loss is 3.562626085281372 and perplexity is 35.25566005417981
At time: 117.13931131362915 and batch: 150, loss is 3.5709269857406616 and perplexity is 35.549531791502964
At time: 117.86882472038269 and batch: 200, loss is 3.4562883377075195 and perplexity is 31.69910153406257
At time: 118.59990930557251 and batch: 250, loss is 3.5973263216018676 and perplexity is 36.50051323059816
At time: 119.33016633987427 and batch: 300, loss is 3.5609655809402465 and perplexity is 35.19716645548812
At time: 120.06306338310242 and batch: 350, loss is 3.5284631729125975 and perplexity is 34.07156525399192
At time: 120.79598188400269 and batch: 400, loss is 3.4753828191757203 and perplexity is 32.31019513164245
At time: 121.52497935295105 and batch: 450, loss is 3.503031916618347 and perplexity is 33.21600760895353
At time: 122.25538539886475 and batch: 500, loss is 3.3814601564407347 and perplexity is 29.41368835922122
At time: 122.98630094528198 and batch: 550, loss is 3.4198027849197388 and perplexity is 30.563386865854387
At time: 123.72241997718811 and batch: 600, loss is 3.4364386415481567 and perplexity is 31.076087773042367
At time: 124.45955538749695 and batch: 650, loss is 3.27257954120636 and perplexity is 26.379298140872415
At time: 125.19370985031128 and batch: 700, loss is 3.257740616798401 and perplexity is 25.990747694843446
At time: 125.92539381980896 and batch: 750, loss is 3.356861000061035 and perplexity is 28.69896329710406
At time: 126.65591144561768 and batch: 800, loss is 3.314095516204834 and perplexity is 27.497511679722322
At time: 127.41484808921814 and batch: 850, loss is 3.366230773925781 and perplexity is 28.969129816396094
At time: 128.14647722244263 and batch: 900, loss is 3.3163527727127073 and perplexity is 27.559650722406676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310279741679152 and perplexity of 74.46131596067553
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 129.97962188720703 and batch: 50, loss is 3.6589756488800047 and perplexity is 38.821555592788414
At time: 130.72176957130432 and batch: 100, loss is 3.545454239845276 and perplexity is 34.65542364178189
At time: 131.44864201545715 and batch: 150, loss is 3.5569200468063356 and perplexity is 35.05506275367344
At time: 132.17702317237854 and batch: 200, loss is 3.439127264022827 and perplexity is 31.159752061498338
At time: 132.90377855300903 and batch: 250, loss is 3.5841795873641966 and perplexity is 36.023791218647105
At time: 133.62950944900513 and batch: 300, loss is 3.546936831474304 and perplexity is 34.706841589264485
At time: 134.35764122009277 and batch: 350, loss is 3.5129404306411742 and perplexity is 33.54676483633828
At time: 135.08449172973633 and batch: 400, loss is 3.458505153656006 and perplexity is 31.76945055450168
At time: 135.81163883209229 and batch: 450, loss is 3.4848405838012697 and perplexity is 32.617226981039614
At time: 136.5411250591278 and batch: 500, loss is 3.3631556129455564 and perplexity is 28.880181913409416
At time: 137.26962184906006 and batch: 550, loss is 3.400481686592102 and perplexity is 29.97853682935726
At time: 137.9992425441742 and batch: 600, loss is 3.419493765830994 and perplexity is 30.55394365503749
At time: 138.72749590873718 and batch: 650, loss is 3.253408284187317 and perplexity is 25.87839069071182
At time: 139.45378017425537 and batch: 700, loss is 3.2402712392807005 and perplexity is 25.540648435024305
At time: 140.1796772480011 and batch: 750, loss is 3.334850492477417 and perplexity is 28.07418560808251
At time: 140.90664887428284 and batch: 800, loss is 3.2902270364761352 and perplexity is 26.848958656939978
At time: 141.63210439682007 and batch: 850, loss is 3.3426158666610717 and perplexity is 28.29304081073578
At time: 142.36097860336304 and batch: 900, loss is 3.2953147172927855 and perplexity is 26.985905664778034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307182416523973 and perplexity of 74.2310418546464
finished 10 epochs...
Completing Train Step...
At time: 144.1741442680359 and batch: 50, loss is 3.6458479070663454 and perplexity is 38.3152468506992
At time: 144.90041780471802 and batch: 100, loss is 3.5297184562683106 and perplexity is 34.114361577895245
At time: 145.63806581497192 and batch: 150, loss is 3.5388470697402954 and perplexity is 34.42720413550725
At time: 146.3662359714508 and batch: 200, loss is 3.4231859683990478 and perplexity is 30.666963521992372
At time: 147.09491348266602 and batch: 250, loss is 3.5689873027801515 and perplexity is 35.48064380246065
At time: 147.8232297897339 and batch: 300, loss is 3.5327330732345583 and perplexity is 34.21735848120452
At time: 148.55241870880127 and batch: 350, loss is 3.499182033538818 and perplexity is 33.08837570488664
At time: 149.27936816215515 and batch: 400, loss is 3.4468107271194457 and perplexity is 31.400088994170588
At time: 150.0149440765381 and batch: 450, loss is 3.47415328502655 and perplexity is 32.27049305589236
At time: 150.7447543144226 and batch: 500, loss is 3.352922310829163 and perplexity is 28.586149314851216
At time: 151.47199010849 and batch: 550, loss is 3.3914181852340697 and perplexity is 29.708053932754854
At time: 152.19984531402588 and batch: 600, loss is 3.412201542854309 and perplexity is 30.331947890142548
At time: 152.92521953582764 and batch: 650, loss is 3.247897753715515 and perplexity is 25.736179220434956
At time: 153.651225566864 and batch: 700, loss is 3.2364138889312746 and perplexity is 25.442318973372082
At time: 154.37703227996826 and batch: 750, loss is 3.3331469678878785 and perplexity is 28.02640125503391
At time: 155.10306334495544 and batch: 800, loss is 3.2911261320114136 and perplexity is 26.873109291032055
At time: 155.83827781677246 and batch: 850, loss is 3.3458827209472655 and perplexity is 28.38562119335026
At time: 156.5695300102234 and batch: 900, loss is 3.3003952312469482 and perplexity is 27.12335680062757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306497234187714 and perplexity of 74.1801974767895
finished 11 epochs...
Completing Train Step...
At time: 158.38694429397583 and batch: 50, loss is 3.638633270263672 and perplexity is 38.03981104005911
At time: 159.13295459747314 and batch: 100, loss is 3.521617045402527 and perplexity is 33.83910361275924
At time: 159.8630187511444 and batch: 150, loss is 3.529740152359009 and perplexity is 34.11510173420737
At time: 160.5908544063568 and batch: 200, loss is 3.4141533470153806 and perplexity is 30.391207725231567
At time: 161.31934523582458 and batch: 250, loss is 3.5600265312194823 and perplexity is 35.164130079983664
At time: 162.0479097366333 and batch: 300, loss is 3.524042935371399 and perplexity is 33.9212932059171
At time: 162.77365636825562 and batch: 350, loss is 3.4906875324249267 and perplexity is 32.8084968591105
At time: 163.5158727169037 and batch: 400, loss is 3.439210591316223 and perplexity is 31.162348627481418
At time: 164.24912929534912 and batch: 450, loss is 3.466985445022583 and perplexity is 32.040010343232154
At time: 164.9767951965332 and batch: 500, loss is 3.34630211353302 and perplexity is 28.39752840914527
At time: 165.7053849697113 and batch: 550, loss is 3.3854254341125487 and perplexity is 29.530553348858273
At time: 166.43267440795898 and batch: 600, loss is 3.407245087623596 and perplexity is 30.181980907694914
At time: 167.15966796875 and batch: 650, loss is 3.2438328456878662 and perplexity is 25.631776356698417
At time: 167.8891990184784 and batch: 700, loss is 3.2334458446502685 and perplexity is 25.366916997602242
At time: 168.61810064315796 and batch: 750, loss is 3.331401858329773 and perplexity is 27.977534765415037
At time: 169.3511803150177 and batch: 800, loss is 3.2906862401962282 and perplexity is 26.86129062986309
At time: 170.08395504951477 and batch: 850, loss is 3.346551547050476 and perplexity is 28.404612588022513
At time: 170.81513667106628 and batch: 900, loss is 3.3018513250350954 and perplexity is 27.162879719533947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306775654831978 and perplexity of 74.20085365058179
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 172.63275599479675 and batch: 50, loss is 3.635759973526001 and perplexity is 37.930668249996124
At time: 173.3775978088379 and batch: 100, loss is 3.522094407081604 and perplexity is 33.85526096022072
At time: 174.10514116287231 and batch: 150, loss is 3.530657272338867 and perplexity is 34.146403727270304
At time: 174.8393840789795 and batch: 200, loss is 3.412982063293457 and perplexity is 30.35563183712995
At time: 175.56684064865112 and batch: 250, loss is 3.5601049184799196 and perplexity is 35.16688660784322
At time: 176.2981345653534 and batch: 300, loss is 3.523132472038269 and perplexity is 33.89042316739229
At time: 177.027184009552 and batch: 350, loss is 3.4891836261749267 and perplexity is 32.7591930390852
At time: 177.75811958312988 and batch: 400, loss is 3.436281290054321 and perplexity is 31.071198288902618
At time: 178.48759031295776 and batch: 450, loss is 3.462475256919861 and perplexity is 31.89582925609139
At time: 179.217511177063 and batch: 500, loss is 3.3423497009277345 and perplexity is 28.285511174890026
At time: 179.9473054409027 and batch: 550, loss is 3.37948456287384 and perplexity is 29.355636228309592
At time: 180.67626237869263 and batch: 600, loss is 3.4025929975509643 and perplexity is 30.041897706416158
At time: 181.40479683876038 and batch: 650, loss is 3.2386956310272215 and perplexity is 25.500438064765163
At time: 182.14291143417358 and batch: 700, loss is 3.2268917036056517 and perplexity is 25.201202297704544
At time: 182.86876797676086 and batch: 750, loss is 3.3240305185317993 and perplexity is 27.772061088490716
At time: 183.59798288345337 and batch: 800, loss is 3.280748782157898 and perplexity is 26.595679616345805
At time: 184.3320336341858 and batch: 850, loss is 3.3352268314361573 and perplexity is 28.084753006198817
At time: 185.06574010849 and batch: 900, loss is 3.2921393585205077 and perplexity is 26.90035163675029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305164493926584 and perplexity of 74.08140039108407
finished 13 epochs...
Completing Train Step...
At time: 186.86148023605347 and batch: 50, loss is 3.6319860506057737 and perplexity is 37.787790605893655
At time: 187.58153581619263 and batch: 100, loss is 3.517943444252014 and perplexity is 33.715020298643395
At time: 188.3048915863037 and batch: 150, loss is 3.526283874511719 and perplexity is 33.99739399632546
At time: 189.04314351081848 and batch: 200, loss is 3.4093651151657105 and perplexity is 30.246035413161525
At time: 189.7775011062622 and batch: 250, loss is 3.556102089881897 and perplexity is 35.02640094601749
At time: 190.511554479599 and batch: 300, loss is 3.5198942279815673 and perplexity is 33.780855205638034
At time: 191.24380564689636 and batch: 350, loss is 3.4858815622329713 and perplexity is 32.6511984895649
At time: 191.9757239818573 and batch: 400, loss is 3.433755302429199 and perplexity is 30.992811869768943
At time: 192.70864081382751 and batch: 450, loss is 3.46022668838501 and perplexity is 31.824189871250887
At time: 193.4408802986145 and batch: 500, loss is 3.3400551414489748 and perplexity is 28.220682791845988
At time: 194.17619943618774 and batch: 550, loss is 3.377696237564087 and perplexity is 29.30318571432796
At time: 194.91414999961853 and batch: 600, loss is 3.401221766471863 and perplexity is 30.00073155321058
At time: 195.653329372406 and batch: 650, loss is 3.2374347257614136 and perplexity is 25.46830469095637
At time: 196.39195728302002 and batch: 700, loss is 3.2261085987091063 and perplexity is 25.181474838129635
At time: 197.13124799728394 and batch: 750, loss is 3.3234886217117308 and perplexity is 27.757015573826568
At time: 197.86618542671204 and batch: 800, loss is 3.281227540969849 and perplexity is 26.60841558080629
At time: 198.60442924499512 and batch: 850, loss is 3.3366549968719483 and perplexity is 28.124891334979537
At time: 199.3387689590454 and batch: 900, loss is 3.2942302417755127 and perplexity is 26.95665597392664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304914918664384 and perplexity of 74.06291381314956
finished 14 epochs...
Completing Train Step...
At time: 201.160795211792 and batch: 50, loss is 3.629722409248352 and perplexity is 37.70234934094698
At time: 201.9025616645813 and batch: 100, loss is 3.5155553579330445 and perplexity is 33.634601981084906
At time: 202.63042068481445 and batch: 150, loss is 3.5235721588134767 and perplexity is 33.90532761466926
At time: 203.35630750656128 and batch: 200, loss is 3.4068181467056275 and perplexity is 30.169097735432466
At time: 204.08671188354492 and batch: 250, loss is 3.5535058307647707 and perplexity is 34.93558127992893
At time: 204.81495547294617 and batch: 300, loss is 3.5175195264816286 and perplexity is 33.70073093138223
At time: 205.54232668876648 and batch: 350, loss is 3.48351270198822 and perplexity is 32.57394390230589
At time: 206.26964569091797 and batch: 400, loss is 3.431805772781372 and perplexity is 30.93244932256515
At time: 206.99781489372253 and batch: 450, loss is 3.4584621810913085 and perplexity is 31.7680853690653
At time: 207.73429107666016 and batch: 500, loss is 3.338348093032837 and perplexity is 28.1725498143337
At time: 208.46556997299194 and batch: 550, loss is 3.376230182647705 and perplexity is 29.260257110380184
At time: 209.19306182861328 and batch: 600, loss is 3.4000263833999633 and perplexity is 29.964890612661957
At time: 209.91948413848877 and batch: 650, loss is 3.236510376930237 and perplexity is 25.444773970255746
At time: 210.6458842754364 and batch: 700, loss is 3.225504856109619 and perplexity is 25.166276297516813
At time: 211.37384557724 and batch: 750, loss is 3.3231656551361084 and perplexity is 27.748052433032548
At time: 212.10047841072083 and batch: 800, loss is 3.281408553123474 and perplexity is 26.613232463359118
At time: 212.82737064361572 and batch: 850, loss is 3.3373688888549804 and perplexity is 28.144976637942584
At time: 213.55595088005066 and batch: 900, loss is 3.2952412033081053 and perplexity is 26.98392189624067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304927878183861 and perplexity of 74.06387363914308
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 215.37504291534424 and batch: 50, loss is 3.628666567802429 and perplexity is 37.66256264582118
At time: 216.11825847625732 and batch: 100, loss is 3.515505843162537 and perplexity is 33.6329366127172
At time: 216.8471565246582 and batch: 150, loss is 3.523614068031311 and perplexity is 33.90674859020579
At time: 217.57701706886292 and batch: 200, loss is 3.406550922393799 and perplexity is 30.161036896125708
At time: 218.30555701255798 and batch: 250, loss is 3.5533844137191775 and perplexity is 34.93133976236532
At time: 219.05213236808777 and batch: 300, loss is 3.5173952913284303 and perplexity is 33.69654437597614
At time: 219.78858947753906 and batch: 350, loss is 3.482939805984497 and perplexity is 32.555287764542406
At time: 220.51840782165527 and batch: 400, loss is 3.4311287212371826 and perplexity is 30.911513548084713
At time: 221.2582643032074 and batch: 450, loss is 3.4572735404968262 and perplexity is 31.730346966328657
At time: 221.99099946022034 and batch: 500, loss is 3.3371490669250488 and perplexity is 28.138790434816617
At time: 222.72264313697815 and batch: 550, loss is 3.3739152574539184 and perplexity is 29.19260014463974
At time: 223.4548990726471 and batch: 600, loss is 3.3980006837844847 and perplexity is 29.904252183626827
At time: 224.1846706867218 and batch: 650, loss is 3.2351281785964967 and perplexity is 25.409628540644153
At time: 224.91432118415833 and batch: 700, loss is 3.223505072593689 and perplexity is 25.11599948114931
At time: 225.64314556121826 and batch: 750, loss is 3.321106543540955 and perplexity is 27.690974881216263
At time: 226.3723921775818 and batch: 800, loss is 3.278035821914673 and perplexity is 26.523624380594853
At time: 227.1036696434021 and batch: 850, loss is 3.33359347820282 and perplexity is 28.038918126533133
At time: 227.83277225494385 and batch: 900, loss is 3.2917978668212893 and perplexity is 26.89116695829527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303964275203339 and perplexity of 73.99253984400384
finished 16 epochs...
Completing Train Step...
At time: 229.6841881275177 and batch: 50, loss is 3.62765501499176 and perplexity is 37.62448423712742
At time: 230.41799426078796 and batch: 100, loss is 3.514391598701477 and perplexity is 33.5954821699651
At time: 231.14450407028198 and batch: 150, loss is 3.5226450777053833 and perplexity is 33.87390919193735
At time: 231.87081360816956 and batch: 200, loss is 3.4056800365448 and perplexity is 30.134781510283165
At time: 232.5975501537323 and batch: 250, loss is 3.5523932504653932 and perplexity is 34.896734254674264
At time: 233.32464003562927 and batch: 300, loss is 3.51651153087616 and perplexity is 33.66677785785162
At time: 234.05121040344238 and batch: 350, loss is 3.482205572128296 and perplexity is 32.53139334318733
At time: 234.7797966003418 and batch: 400, loss is 3.430444531440735 and perplexity is 30.89037143935464
At time: 235.507896900177 and batch: 450, loss is 3.4567167472839357 and perplexity is 31.712684642082582
At time: 236.23611545562744 and batch: 500, loss is 3.3366421365737917 and perplexity is 28.124529642817087
At time: 236.97488570213318 and batch: 550, loss is 3.3735928773880004 and perplexity is 29.183190549095304
At time: 237.70769238471985 and batch: 600, loss is 3.3978369426727295 and perplexity is 29.899356028989914
At time: 238.43665409088135 and batch: 650, loss is 3.234896354675293 and perplexity is 25.4037386636531
At time: 239.16358089447021 and batch: 700, loss is 3.223404769897461 and perplexity is 25.11348040502006
At time: 239.89042448997498 and batch: 750, loss is 3.3209446907043456 and perplexity is 27.686493381064803
At time: 240.6174190044403 and batch: 800, loss is 3.278215517997742 and perplexity is 26.528391000263458
At time: 241.34515190124512 and batch: 850, loss is 3.333923478126526 and perplexity is 28.048172494262023
At time: 242.07355189323425 and batch: 900, loss is 3.292413282394409 and perplexity is 26.90772129459647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303563784246576 and perplexity of 73.96291243407926
finished 17 epochs...
Completing Train Step...
At time: 243.88410258293152 and batch: 50, loss is 3.626932692527771 and perplexity is 37.597317039886576
At time: 244.623694896698 and batch: 100, loss is 3.513617091178894 and perplexity is 33.5694722900237
At time: 245.35110211372375 and batch: 150, loss is 3.521885838508606 and perplexity is 33.84820055306888
At time: 246.07920169830322 and batch: 200, loss is 3.4049827909469603 and perplexity is 30.113777489864578
At time: 246.80778789520264 and batch: 250, loss is 3.5516250276565553 and perplexity is 34.86993608226746
At time: 247.53513145446777 and batch: 300, loss is 3.515822949409485 and perplexity is 33.6436035182021
At time: 248.2634928226471 and batch: 350, loss is 3.4815761613845826 and perplexity is 32.510924177141426
At time: 248.99651622772217 and batch: 400, loss is 3.429892964363098 and perplexity is 30.873338025419724
At time: 249.72508263587952 and batch: 450, loss is 3.4562471437454225 and perplexity is 31.697795749370876
At time: 250.45145392417908 and batch: 500, loss is 3.3362018156051634 and perplexity is 28.11214854871026
At time: 251.17734384536743 and batch: 550, loss is 3.3732752656936644 and perplexity is 29.173923098302158
At time: 251.9066824913025 and batch: 600, loss is 3.397626070976257 and perplexity is 29.89305176577935
At time: 252.63431978225708 and batch: 650, loss is 3.2346826457977294 and perplexity is 25.398310239249273
At time: 253.35985445976257 and batch: 700, loss is 3.223297290802002 and perplexity is 25.110781375909507
At time: 254.08785796165466 and batch: 750, loss is 3.320834927558899 and perplexity is 27.683454591241528
At time: 254.8163275718689 and batch: 800, loss is 3.2783390426635743 and perplexity is 26.531668113294813
At time: 255.55007219314575 and batch: 850, loss is 3.334181056022644 and perplexity is 28.055398014050194
At time: 256.2724018096924 and batch: 900, loss is 3.292846746444702 and perplexity is 26.91938735267871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303386531464041 and perplexity of 73.94980346388101
finished 18 epochs...
Completing Train Step...
At time: 258.0518162250519 and batch: 50, loss is 3.626332712173462 and perplexity is 37.57476615400864
At time: 258.7954008579254 and batch: 100, loss is 3.5129823732376098 and perplexity is 33.54817190426537
At time: 259.5246572494507 and batch: 150, loss is 3.521218419075012 and perplexity is 33.82561714336916
At time: 260.25339913368225 and batch: 200, loss is 3.404359178543091 and perplexity is 30.09500401898759
At time: 260.9824867248535 and batch: 250, loss is 3.55096302986145 and perplexity is 34.846859900499005
At time: 261.7103238105774 and batch: 300, loss is 3.515220355987549 and perplexity is 33.623336211118065
At time: 262.44252848625183 and batch: 350, loss is 3.480998148918152 and perplexity is 32.492137887549696
At time: 263.17304968833923 and batch: 400, loss is 3.429399318695068 and perplexity is 30.858101296927778
At time: 263.9011015892029 and batch: 450, loss is 3.455813946723938 and perplexity is 31.684067332433973
At time: 264.6292128562927 and batch: 500, loss is 3.3357927942276 and perplexity is 28.10065243022399
At time: 265.36267375946045 and batch: 550, loss is 3.3729542541503905 and perplexity is 29.164559435227396
At time: 266.0996963977814 and batch: 600, loss is 3.3973901653289795 and perplexity is 29.886000657784237
At time: 266.8332426548004 and batch: 650, loss is 3.2344758033752443 and perplexity is 25.39305733451286
At time: 267.5639510154724 and batch: 700, loss is 3.2231801080703737 and perplexity is 25.10783899835584
At time: 268.29309821128845 and batch: 750, loss is 3.3207439613342284 and perplexity is 27.680936446426323
At time: 269.0212941169739 and batch: 800, loss is 3.2784197473526 and perplexity is 26.533809429725213
At time: 269.74925541877747 and batch: 850, loss is 3.3343849992752075 and perplexity is 28.061120306664797
At time: 270.47684383392334 and batch: 900, loss is 3.293169016838074 and perplexity is 26.928064072280012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3033079382491435 and perplexity of 73.94399173946977
finished 19 epochs...
Completing Train Step...
At time: 272.2944269180298 and batch: 50, loss is 3.625794630050659 and perplexity is 37.554553282652726
At time: 273.0238506793976 and batch: 100, loss is 3.51241587638855 and perplexity is 33.52917235269104
At time: 273.763130903244 and batch: 150, loss is 3.520603060722351 and perplexity is 33.80480867031654
At time: 274.4911878108978 and batch: 200, loss is 3.4037768411636353 and perplexity is 30.077483675083165
At time: 275.2200093269348 and batch: 250, loss is 3.5503617238998415 and perplexity is 34.82591257440468
At time: 275.9481348991394 and batch: 300, loss is 3.514664440155029 and perplexity is 33.604649660732015
At time: 276.67797565460205 and batch: 350, loss is 3.4804532051086428 and perplexity is 32.474436321767556
At time: 277.40744805336 and batch: 400, loss is 3.42893705368042 and perplexity is 30.8438399727893
At time: 278.13636660575867 and batch: 450, loss is 3.4554002809524538 and perplexity is 31.670963428782212
At time: 278.86566066741943 and batch: 500, loss is 3.335402836799622 and perplexity is 28.089696508388172
At time: 279.5930268764496 and batch: 550, loss is 3.3726324796676637 and perplexity is 29.15517653387124
At time: 280.32484459877014 and batch: 600, loss is 3.3971427345275877 and perplexity is 29.878606855456027
At time: 281.05377769470215 and batch: 650, loss is 3.234272446632385 and perplexity is 25.387894010098286
At time: 281.7815451622009 and batch: 700, loss is 3.2230559635162352 and perplexity is 25.104722190349385
At time: 282.5094702243805 and batch: 750, loss is 3.3206597995758056 and perplexity is 27.678606868172277
At time: 283.24220538139343 and batch: 800, loss is 3.278470501899719 and perplexity is 26.5351561753826
At time: 283.97782945632935 and batch: 850, loss is 3.3345487928390503 and perplexity is 28.065716914002316
At time: 284.71284437179565 and batch: 900, loss is 3.2934193897247312 and perplexity is 26.934806973498482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303279510916096 and perplexity of 73.941889738867
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fc267f18b70>
ELAPSED
1748.149599313736


RESULTS SO FAR:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.88670270084627, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.31332278986565, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.23850096980998603, 'data': 'ptb', 'dropout': 0.3583059732624252, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.00308858124657, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.26724264066418335, 'data': 'ptb', 'dropout': 0.9244537941766774, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -75.05956782267326, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.3239422609632767, 'data': 'ptb', 'dropout': 0.8373723041896595, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.941889738867, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8627506693955694, 'data': 'ptb', 'dropout': 0.6116306336471359, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -74.50341356110593, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.29170551276891343, 'data': 'ptb', 'dropout': 0.8782291202070783, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.88670270084627, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8263504649821455, 'data': 'ptb', 'dropout': 0.6348104197738138, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.31332278986565, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.23850096980998603, 'data': 'ptb', 'dropout': 0.3583059732624252, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -74.00308858124657, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.26724264066418335, 'data': 'ptb', 'dropout': 0.9244537941766774, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -75.05956782267326, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.3239422609632767, 'data': 'ptb', 'dropout': 0.8373723041896595, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}, {'best_accuracy': -73.941889738867, 'params': {'tune_wordvecs': True, 'tie_weights': True, 'wordvec_dim': 300, 'rnn_dropout': 0.8627506693955694, 'data': 'ptb', 'dropout': 0.6116306336471359, 'wordvec_source': 'gigavec', 'num_layers': 2, 'batch_size': 32, 'seq_len': 35}}]
