Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0505645275115967 and batch: 50, loss is 6.637935094833374 and perplexity is 763.516774412641
At time: 1.6823632717132568 and batch: 100, loss is 5.865267868041992 and perplexity is 352.57658745970866
At time: 2.3257253170013428 and batch: 150, loss is 5.750938043594361 and perplexity is 314.4855230974476
At time: 2.958852529525757 and batch: 200, loss is 5.591596183776855 and perplexity is 268.1633162166229
At time: 3.5931556224823 and batch: 250, loss is 5.637292156219482 and perplexity is 280.7015923739735
At time: 4.228838920593262 and batch: 300, loss is 5.563035392761231 and perplexity is 260.61269884456124
At time: 4.8628785610198975 and batch: 350, loss is 5.543472394943238 and perplexity is 255.56387922811808
At time: 5.49875020980835 and batch: 400, loss is 5.4022086715698245 and perplexity is 221.89597069456312
At time: 6.132494688034058 and batch: 450, loss is 5.403648920059204 and perplexity is 222.21578628283117
At time: 6.767984628677368 and batch: 500, loss is 5.3485722160339355 and perplexity is 210.3078092832981
At time: 7.405465841293335 and batch: 550, loss is 5.413324174880981 and perplexity is 224.37621513716485
At time: 8.043140649795532 and batch: 600, loss is 5.33815842628479 and perplexity is 208.12907211994718
At time: 8.681580543518066 and batch: 650, loss is 5.238055419921875 and perplexity is 188.30357476985816
At time: 9.316748857498169 and batch: 700, loss is 5.34062497138977 and perplexity is 208.64306549732535
At time: 9.954782962799072 and batch: 750, loss is 5.319582405090332 and perplexity is 204.2985501425429
At time: 10.589612007141113 and batch: 800, loss is 5.281938724517822 and perplexity is 196.75095172539773
At time: 11.225079536437988 and batch: 850, loss is 5.323522720336914 and perplexity is 205.10513889791255
At time: 11.860889196395874 and batch: 900, loss is 5.248142070770264 and perplexity is 190.21253852260617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.09055526942423 and perplexity of 162.48005721768337
finished 1 epochs...
Completing Train Step...
At time: 13.423073053359985 and batch: 50, loss is 5.037913017272949 and perplexity is 154.14797493562443
At time: 14.046964645385742 and batch: 100, loss is 4.877808351516723 and perplexity is 131.34249164245088
At time: 14.67226767539978 and batch: 150, loss is 4.8422793197631835 and perplexity is 126.75794459571745
At time: 15.298794746398926 and batch: 200, loss is 4.7281920433044435 and perplexity is 113.09091393161734
At time: 15.928568363189697 and batch: 250, loss is 4.834239768981933 and perplexity is 125.74295316771625
At time: 16.566524982452393 and batch: 300, loss is 4.787573385238647 and perplexity is 120.00979749473248
At time: 17.21209716796875 and batch: 350, loss is 4.76396183013916 and perplexity is 117.20937086576967
At time: 17.844892263412476 and batch: 400, loss is 4.654247388839722 and perplexity is 105.03014338758302
At time: 18.476638793945312 and batch: 450, loss is 4.66089204788208 and perplexity is 105.7303566424447
At time: 19.108978986740112 and batch: 500, loss is 4.561017589569092 and perplexity is 95.68079408670778
At time: 19.74175786972046 and batch: 550, loss is 4.6417733001708985 and perplexity is 103.72812569126225
At time: 20.388046264648438 and batch: 600, loss is 4.607758064270019 and perplexity is 100.25912297293512
At time: 21.02186942100525 and batch: 650, loss is 4.468494958877564 and perplexity is 87.22534644135521
At time: 21.655976057052612 and batch: 700, loss is 4.498900871276856 and perplexity is 89.91824524014423
At time: 22.28840398788452 and batch: 750, loss is 4.567444953918457 and perplexity is 96.29774998780027
At time: 22.922119617462158 and batch: 800, loss is 4.502350206375122 and perplexity is 90.22893893470784
At time: 23.575846672058105 and batch: 850, loss is 4.565857849121094 and perplexity is 96.1450365849623
At time: 24.20801591873169 and batch: 900, loss is 4.5150649452209475 and perplexity is 91.38350075131729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.637128856084118 and perplexity of 103.24748323376141
finished 2 epochs...
Completing Train Step...
At time: 25.76907753944397 and batch: 50, loss is 4.511276912689209 and perplexity is 91.0379918901578
At time: 26.418125867843628 and batch: 100, loss is 4.360773983001709 and perplexity is 78.3177275608947
At time: 27.05722212791443 and batch: 150, loss is 4.358746042251587 and perplexity is 78.15906478344857
At time: 27.701515197753906 and batch: 200, loss is 4.261099934577942 and perplexity is 70.8879126541953
At time: 28.34642767906189 and batch: 250, loss is 4.405477685928345 and perplexity is 81.89825514476446
At time: 28.985246181488037 and batch: 300, loss is 4.383913168907165 and perplexity is 80.15106518868409
At time: 29.63139796257019 and batch: 350, loss is 4.366927242279052 and perplexity is 78.80112254671481
At time: 30.269753456115723 and batch: 400, loss is 4.291466655731202 and perplexity is 73.07356370554457
At time: 30.90625500679016 and batch: 450, loss is 4.304260883331299 and perplexity is 74.01448988784976
At time: 31.54432702064514 and batch: 500, loss is 4.195712761878967 and perplexity is 66.40104284652199
At time: 32.183939695358276 and batch: 550, loss is 4.285952806472778 and perplexity is 72.67175586243282
At time: 32.82439470291138 and batch: 600, loss is 4.2879744815826415 and perplexity is 72.81882315350012
At time: 33.464900970458984 and batch: 650, loss is 4.138255424499512 and perplexity is 62.69335270155189
At time: 34.105194330215454 and batch: 700, loss is 4.1572883558273315 and perplexity is 63.898018775676164
At time: 34.744677782058716 and batch: 750, loss is 4.254290270805359 and perplexity is 70.40682966921158
At time: 35.38076567649841 and batch: 800, loss is 4.198559293746948 and perplexity is 66.59032480180205
At time: 36.016769886016846 and batch: 850, loss is 4.269659543037415 and perplexity is 71.49728972499695
At time: 36.652703523635864 and batch: 900, loss is 4.232815380096436 and perplexity is 68.91096989628505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.537663707994435 and perplexity of 93.47216654753954
finished 3 epochs...
Completing Train Step...
At time: 38.245229959487915 and batch: 50, loss is 4.26157322883606 and perplexity is 70.92147143718769
At time: 38.89093494415283 and batch: 100, loss is 4.118338522911071 and perplexity is 61.45704791408922
At time: 39.547399282455444 and batch: 150, loss is 4.117523159980774 and perplexity is 61.40695853869952
At time: 40.191484451293945 and batch: 200, loss is 4.027037191390991 and perplexity is 56.094467626986244
At time: 40.83534288406372 and batch: 250, loss is 4.1740350341796875 and perplexity is 64.97710871026486
At time: 41.48256754875183 and batch: 300, loss is 4.161971569061279 and perplexity is 64.19796863919431
At time: 42.12799119949341 and batch: 350, loss is 4.138387470245362 and perplexity is 62.70163163865636
At time: 42.773298501968384 and batch: 400, loss is 4.076916518211365 and perplexity is 58.96337675536657
At time: 43.41880249977112 and batch: 450, loss is 4.096662845611572 and perplexity is 60.13925836117145
At time: 44.064162254333496 and batch: 500, loss is 3.982838134765625 and perplexity is 53.66913852703551
At time: 44.71048545837402 and batch: 550, loss is 4.074309062957764 and perplexity is 58.809832655553826
At time: 45.35519194602966 and batch: 600, loss is 4.086816716194153 and perplexity is 59.5500250343991
At time: 45.99971532821655 and batch: 650, loss is 3.935599808692932 and perplexity is 51.192846664106135
At time: 46.643752098083496 and batch: 700, loss is 3.9527645826339723 and perplexity is 52.07914509994566
At time: 47.28750681877136 and batch: 750, loss is 4.054923267364502 and perplexity is 57.680736831395045
At time: 47.931570053100586 and batch: 800, loss is 4.00302788734436 and perplexity is 54.76371761415164
At time: 48.57576513290405 and batch: 850, loss is 4.082756619453431 and perplexity is 59.30873633104396
At time: 49.218934297561646 and batch: 900, loss is 4.047304825782776 and perplexity is 57.24296917845726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.503887855843322 and perplexity of 90.3677861364829
finished 4 epochs...
Completing Train Step...
At time: 50.80955219268799 and batch: 50, loss is 4.084579548835754 and perplexity is 59.41695057262362
At time: 51.45799803733826 and batch: 100, loss is 3.9497717094421385 and perplexity is 51.92351183420781
At time: 52.1042537689209 and batch: 150, loss is 3.946146774291992 and perplexity is 51.735633200701706
At time: 52.75171518325806 and batch: 200, loss is 3.859547719955444 and perplexity is 47.44388859158036
At time: 53.40109992027283 and batch: 250, loss is 4.006400589942932 and perplexity is 54.94873116932621
At time: 54.05056428909302 and batch: 300, loss is 4.000075435638427 and perplexity is 54.602268834799204
At time: 54.699864864349365 and batch: 350, loss is 3.9733356046676636 and perplexity is 53.16156137585976
At time: 55.35018491744995 and batch: 400, loss is 3.919594769477844 and perplexity is 50.38002511712555
At time: 56.0140905380249 and batch: 450, loss is 3.940952458381653 and perplexity is 51.46759870860036
At time: 56.66339921951294 and batch: 500, loss is 3.826336159706116 and perplexity is 45.8940812388005
At time: 57.3134925365448 and batch: 550, loss is 3.9183216381072996 and perplexity is 50.315925538950076
At time: 57.96282386779785 and batch: 600, loss is 3.9379128885269163 and perplexity is 51.311396860546516
At time: 58.61185312271118 and batch: 650, loss is 3.784087882041931 and perplexity is 43.995523145209525
At time: 59.260169982910156 and batch: 700, loss is 3.80037757396698 and perplexity is 44.7180656836071
At time: 59.908658266067505 and batch: 750, loss is 3.905069799423218 and perplexity is 49.653545580067146
At time: 60.55701184272766 and batch: 800, loss is 3.851879873275757 and perplexity is 47.08148732229649
At time: 61.206520557403564 and batch: 850, loss is 3.936874008178711 and perplexity is 51.25811813860958
At time: 61.85662841796875 and batch: 900, loss is 3.9040543365478517 and perplexity is 49.603149839732495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.494282761665239 and perplexity of 89.50395029358165
finished 5 epochs...
Completing Train Step...
At time: 63.43615460395813 and batch: 50, loss is 3.9471272897720335 and perplexity is 51.786385667648325
At time: 64.0969865322113 and batch: 100, loss is 3.814642643928528 and perplexity is 45.360543620587165
At time: 64.73443555831909 and batch: 150, loss is 3.8127847003936766 and perplexity is 45.276344534624606
At time: 65.37338829040527 and batch: 200, loss is 3.727338881492615 and perplexity is 41.568342563483505
At time: 66.01198744773865 and batch: 250, loss is 3.8726242923736574 and perplexity is 48.06836615095361
At time: 66.65111184120178 and batch: 300, loss is 3.872267694473267 and perplexity is 48.051228128382014
At time: 67.29017066955566 and batch: 350, loss is 3.841276178359985 and perplexity is 46.58488714599885
At time: 67.94692492485046 and batch: 400, loss is 3.795468544960022 and perplexity is 44.499081342040846
At time: 68.59196448326111 and batch: 450, loss is 3.8138199758529665 and perplexity is 45.32324229486906
At time: 69.23614072799683 and batch: 500, loss is 3.7005147171020507 and perplexity is 40.46812863819966
At time: 69.8913164138794 and batch: 550, loss is 3.7899557638168333 and perplexity is 44.254442585055166
At time: 70.54412055015564 and batch: 600, loss is 3.8133748054504393 and perplexity is 45.303070219191476
At time: 71.19277667999268 and batch: 650, loss is 3.6604990768432617 and perplexity is 38.8807425082097
At time: 71.83925223350525 and batch: 700, loss is 3.6756052780151367 and perplexity is 39.47254148961511
At time: 72.48869895935059 and batch: 750, loss is 3.7804284286499024 and perplexity is 43.83481780499464
At time: 73.12914609909058 and batch: 800, loss is 3.72686577796936 and perplexity is 41.54868108548485
At time: 73.76600933074951 and batch: 850, loss is 3.814937057495117 and perplexity is 45.37390034612105
At time: 74.40389108657837 and batch: 900, loss is 3.787369441986084 and perplexity is 44.140134236895896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499811041845034 and perplexity of 90.00012343641389
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.97138738632202 and batch: 50, loss is 3.84350643157959 and perplexity is 46.688899184019235
At time: 76.6226258277893 and batch: 100, loss is 3.70521390914917 and perplexity is 40.65874366399498
At time: 77.26099801063538 and batch: 150, loss is 3.7069936084747312 and perplexity is 40.73116843071012
At time: 77.90036678314209 and batch: 200, loss is 3.599087414741516 and perplexity is 36.5648506695251
At time: 78.53903460502625 and batch: 250, loss is 3.740359835624695 and perplexity is 42.11314124737244
At time: 79.17786073684692 and batch: 300, loss is 3.72271116733551 and perplexity is 41.37642057867065
At time: 79.81712889671326 and batch: 350, loss is 3.676219115257263 and perplexity is 39.49677864369568
At time: 80.4741747379303 and batch: 400, loss is 3.6241797637939452 and perplexity is 37.493956642673815
At time: 81.11397361755371 and batch: 450, loss is 3.6268762063980104 and perplexity is 37.59519337293703
At time: 81.75470089912415 and batch: 500, loss is 3.499682984352112 and perplexity is 33.10495550609229
At time: 82.39362788200378 and batch: 550, loss is 3.5697481346130373 and perplexity is 35.507648877574546
At time: 83.03214168548584 and batch: 600, loss is 3.582961497306824 and perplexity is 35.97993771091795
At time: 83.67152690887451 and batch: 650, loss is 3.4122600078582765 and perplexity is 30.33372129943697
At time: 84.31008505821228 and batch: 700, loss is 3.414825644493103 and perplexity is 30.411646527237856
At time: 84.94851899147034 and batch: 750, loss is 3.505263066291809 and perplexity is 33.29020023012747
At time: 85.58751773834229 and batch: 800, loss is 3.4288143062591554 and perplexity is 30.840054203322214
At time: 86.22462964057922 and batch: 850, loss is 3.4942924118041994 and perplexity is 32.92698096504805
At time: 86.86204552650452 and batch: 900, loss is 3.4559710884094237 and perplexity is 31.689046611393533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.435528480843322 and perplexity of 84.39671515189987
finished 7 epochs...
Completing Train Step...
At time: 88.45790672302246 and batch: 50, loss is 3.7328999757766725 and perplexity is 41.80015199535755
At time: 89.09789657592773 and batch: 100, loss is 3.5959235191345216 and perplexity is 36.44934611764582
At time: 89.74714994430542 and batch: 150, loss is 3.5977818775177 and perplexity is 36.51714504340401
At time: 90.4021303653717 and batch: 200, loss is 3.498440189361572 and perplexity is 33.063838388593354
At time: 91.05325722694397 and batch: 250, loss is 3.641286506652832 and perplexity is 38.14087366317476
At time: 91.69559407234192 and batch: 300, loss is 3.6295448780059814 and perplexity is 37.69565659013195
At time: 92.33896088600159 and batch: 350, loss is 3.58890606880188 and perplexity is 36.194460012878004
At time: 93.00255918502808 and batch: 400, loss is 3.5413533878326415 and perplexity is 34.51359787997893
At time: 93.65374302864075 and batch: 450, loss is 3.5504069709777832 and perplexity is 34.827488380835376
At time: 94.30248093605042 and batch: 500, loss is 3.4282992696762085 and perplexity is 30.824174536843042
At time: 94.94436550140381 and batch: 550, loss is 3.502767791748047 and perplexity is 33.20723559375662
At time: 95.59018802642822 and batch: 600, loss is 3.5236671209335326 and perplexity is 33.90854748934138
At time: 96.23356008529663 and batch: 650, loss is 3.359700002670288 and perplexity is 28.780555494215328
At time: 96.87692070007324 and batch: 700, loss is 3.367134985923767 and perplexity is 28.99533589728817
At time: 97.52047038078308 and batch: 750, loss is 3.465912342071533 and perplexity is 32.005646554820174
At time: 98.16663455963135 and batch: 800, loss is 3.395605001449585 and perplexity is 29.832696841090264
At time: 98.81326031684875 and batch: 850, loss is 3.471051344871521 and perplexity is 32.17054701112482
At time: 99.45903491973877 and batch: 900, loss is 3.4420193910598753 and perplexity is 31.250000464920777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442233987050514 and perplexity of 84.96453949642354
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.07931804656982 and batch: 50, loss is 3.689213719367981 and perplexity is 40.01337284507599
At time: 101.7175874710083 and batch: 100, loss is 3.557773804664612 and perplexity is 35.08500406846946
At time: 102.35810971260071 and batch: 150, loss is 3.5609939193725584 and perplexity is 35.19816390214026
At time: 102.99810695648193 and batch: 200, loss is 3.4579736185073853 and perplexity is 31.75256846198985
At time: 103.63867712020874 and batch: 250, loss is 3.5996796131134032 and perplexity is 36.58651072745239
At time: 104.27821564674377 and batch: 300, loss is 3.5855292081832886 and perplexity is 36.07244250026493
At time: 104.94088840484619 and batch: 350, loss is 3.5394777059555054 and perplexity is 34.448922024547436
At time: 105.58138465881348 and batch: 400, loss is 3.4915925216674806 and perplexity is 32.838201635056336
At time: 106.22185015678406 and batch: 450, loss is 3.4934479570388794 and perplexity is 32.89918735593899
At time: 106.86138010025024 and batch: 500, loss is 3.366956601142883 and perplexity is 28.99016403195222
At time: 107.5018720626831 and batch: 550, loss is 3.4328789615631106 and perplexity is 30.965663499523053
At time: 108.14154195785522 and batch: 600, loss is 3.4506992292404175 and perplexity is 31.522426006539987
At time: 108.78437304496765 and batch: 650, loss is 3.2808719968795774 and perplexity is 26.598956797501945
At time: 109.42622494697571 and batch: 700, loss is 3.2809787940979005 and perplexity is 26.601797643792754
At time: 110.068528175354 and batch: 750, loss is 3.3738907289505007 and perplexity is 29.19188410262908
At time: 110.71149516105652 and batch: 800, loss is 3.2984887075424196 and perplexity is 27.07169474105996
At time: 111.35427331924438 and batch: 850, loss is 3.365777916908264 and perplexity is 28.95601391270604
At time: 111.9967041015625 and batch: 900, loss is 3.3346350955963135 and perplexity is 28.06813916727863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429974699673587 and perplexity of 83.92929344488701
finished 9 epochs...
Completing Train Step...
At time: 113.59528303146362 and batch: 50, loss is 3.65804181098938 and perplexity is 38.78531947516601
At time: 114.25164031982422 and batch: 100, loss is 3.5243763399124144 and perplexity is 33.93260460464013
At time: 114.89422821998596 and batch: 150, loss is 3.5265548467636108 and perplexity is 34.00660759499351
At time: 115.53909087181091 and batch: 200, loss is 3.4259696006774902 and perplexity is 30.752447995004395
At time: 116.18432950973511 and batch: 250, loss is 3.5672353649139406 and perplexity is 35.418538337408485
At time: 116.82861423492432 and batch: 300, loss is 3.554458975791931 and perplexity is 34.96889582977347
At time: 117.47262835502625 and batch: 350, loss is 3.509906349182129 and perplexity is 33.44513547298461
At time: 118.1186273097992 and batch: 400, loss is 3.464053707122803 and perplexity is 31.946214989477856
At time: 118.77210307121277 and batch: 450, loss is 3.4688721656799317 and perplexity is 32.1005179551399
At time: 119.41565775871277 and batch: 500, loss is 3.3442669773101805 and perplexity is 28.33979433871653
At time: 120.059401512146 and batch: 550, loss is 3.413354172706604 and perplexity is 30.366929555341855
At time: 120.70451617240906 and batch: 600, loss is 3.4347128009796144 and perplexity is 31.022501653901866
At time: 121.376633644104 and batch: 650, loss is 3.267863311767578 and perplexity is 26.255180232733093
At time: 122.03720831871033 and batch: 700, loss is 3.271100792884827 and perplexity is 26.340318625572817
At time: 122.68469524383545 and batch: 750, loss is 3.367558035850525 and perplexity is 29.007604967047385
At time: 123.32799243927002 and batch: 800, loss is 3.2953738832473753 and perplexity is 26.987502358881663
At time: 123.97607469558716 and batch: 850, loss is 3.366719436645508 and perplexity is 28.983289409511173
At time: 124.62243127822876 and batch: 900, loss is 3.3387280035018922 and perplexity is 28.183254894305367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432376809316139 and perplexity of 84.13114314546134
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 126.22996735572815 and batch: 50, loss is 3.6437084674835205 and perplexity is 38.23336132076698
At time: 126.89022755622864 and batch: 100, loss is 3.5117103815078736 and perplexity is 33.50552603540158
At time: 127.53989481925964 and batch: 150, loss is 3.5143751096725464 and perplexity is 33.59492821765474
At time: 128.18902778625488 and batch: 200, loss is 3.413824987411499 and perplexity is 30.381230118512576
At time: 128.83777356147766 and batch: 250, loss is 3.554793553352356 and perplexity is 34.98059759509584
At time: 129.4946870803833 and batch: 300, loss is 3.540233197212219 and perplexity is 34.47495771757106
At time: 130.15180826187134 and batch: 350, loss is 3.4938729763031007 and perplexity is 32.913173116240735
At time: 130.80069613456726 and batch: 400, loss is 3.448774428367615 and perplexity is 31.46180996906515
At time: 131.4520502090454 and batch: 450, loss is 3.451884055137634 and perplexity is 31.55979672774298
At time: 132.10077738761902 and batch: 500, loss is 3.324430475234985 and perplexity is 27.78317093206445
At time: 132.7522702217102 and batch: 550, loss is 3.392993426322937 and perplexity is 29.754888157893447
At time: 133.40336871147156 and batch: 600, loss is 3.412071008682251 and perplexity is 30.327988792842163
At time: 134.05446696281433 and batch: 650, loss is 3.243311867713928 and perplexity is 25.61842624364305
At time: 134.70561337471008 and batch: 700, loss is 3.244338970184326 and perplexity is 25.644752510097877
At time: 135.35691714286804 and batch: 750, loss is 3.3383923721313478 and perplexity is 28.17379729706073
At time: 136.00851964950562 and batch: 800, loss is 3.2642491054534912 and perplexity is 26.160459867137103
At time: 136.6594376564026 and batch: 850, loss is 3.3332938671112062 and perplexity is 28.03051861402196
At time: 137.33240795135498 and batch: 900, loss is 3.304342269897461 and perplexity is 27.230625295436482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431658601107663 and perplexity of 84.07074116106267
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.94298148155212 and batch: 50, loss is 3.636252841949463 and perplexity is 37.9493676864597
At time: 139.5933539867401 and batch: 100, loss is 3.5047332525253294 and perplexity is 33.272567295254966
At time: 140.24363374710083 and batch: 150, loss is 3.5077830171585083 and perplexity is 33.374195686813245
At time: 140.8941307067871 and batch: 200, loss is 3.4074941301345825 and perplexity is 30.189498440060426
At time: 141.5445041656494 and batch: 250, loss is 3.5485221433639524 and perplexity is 34.76190639380029
At time: 142.19546747207642 and batch: 300, loss is 3.5334865283966064 and perplexity is 34.24314944152981
At time: 142.84716868400574 and batch: 350, loss is 3.487408313751221 and perplexity is 32.70108683029079
At time: 143.49649453163147 and batch: 400, loss is 3.4421958923339844 and perplexity is 31.255516616608325
At time: 144.14664220809937 and batch: 450, loss is 3.445643367767334 and perplexity is 31.363455193191346
At time: 144.79808640480042 and batch: 500, loss is 3.317709183692932 and perplexity is 27.597058299546536
At time: 145.4507131576538 and batch: 550, loss is 3.3866186141967773 and perplexity is 29.565809646363594
At time: 146.10283732414246 and batch: 600, loss is 3.405497965812683 and perplexity is 30.129295348001357
At time: 146.75549983978271 and batch: 650, loss is 3.2362626075744627 and perplexity is 25.438470315959748
At time: 147.4097034931183 and batch: 700, loss is 3.2368990087509157 and perplexity is 25.454664540861316
At time: 148.06292510032654 and batch: 750, loss is 3.3303159141540526 and perplexity is 27.947169215115874
At time: 148.71665692329407 and batch: 800, loss is 3.255974950790405 and perplexity is 25.94489720535351
At time: 149.3687686920166 and batch: 850, loss is 3.324570589065552 and perplexity is 27.787064011299787
At time: 150.0297794342041 and batch: 900, loss is 3.2951100873947143 and perplexity is 26.9803841066097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431561613736087 and perplexity of 84.06258775624607
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.65729975700378 and batch: 50, loss is 3.634228677749634 and perplexity is 37.87262962640079
At time: 152.30401849746704 and batch: 100, loss is 3.502871160507202 and perplexity is 33.21066836191235
At time: 152.9500994682312 and batch: 150, loss is 3.50588915348053 and perplexity is 33.31104932399574
At time: 153.59714150428772 and batch: 200, loss is 3.4056976938247683 and perplexity is 30.13531361325481
At time: 154.2571530342102 and batch: 250, loss is 3.5467424774169922 and perplexity is 34.7000968292421
At time: 154.9047303199768 and batch: 300, loss is 3.5315554761886596 and perplexity is 34.17708793682936
At time: 155.55262970924377 and batch: 350, loss is 3.485663948059082 and perplexity is 32.64409389903693
At time: 156.1994435787201 and batch: 400, loss is 3.4403431034088134 and perplexity is 31.197660355841055
At time: 156.84680128097534 and batch: 450, loss is 3.443866639137268 and perplexity is 31.307780318629653
At time: 157.4975905418396 and batch: 500, loss is 3.3158574724197387 and perplexity is 27.546003799271258
At time: 158.1587769985199 and batch: 550, loss is 3.3848157358169555 and perplexity is 29.512554108424233
At time: 158.814213514328 and batch: 600, loss is 3.4037304592132567 and perplexity is 30.076088655079957
At time: 159.46627521514893 and batch: 650, loss is 3.234394040107727 and perplexity is 25.390981200049858
At time: 160.1131808757782 and batch: 700, loss is 3.2349151754379273 and perplexity is 25.404216785887808
At time: 160.76066064834595 and batch: 750, loss is 3.328305888175964 and perplexity is 27.891051097310825
At time: 161.40732836723328 and batch: 800, loss is 3.253826961517334 and perplexity is 25.889227654668606
At time: 162.05551195144653 and batch: 850, loss is 3.3223554563522337 and perplexity is 27.725580099454362
At time: 162.70314955711365 and batch: 900, loss is 3.2927439403533936 and perplexity is 26.916620017936438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431512702001284 and perplexity of 84.05847620979917
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.32803678512573 and batch: 50, loss is 3.6337228870391844 and perplexity is 37.853478845707215
At time: 164.9891710281372 and batch: 100, loss is 3.5024096632003783 and perplexity is 33.195345263961684
At time: 165.6373143196106 and batch: 150, loss is 3.505408992767334 and perplexity is 33.29505850618351
At time: 166.28606343269348 and batch: 200, loss is 3.4052417850494385 and perplexity is 30.12157779071015
At time: 166.93361592292786 and batch: 250, loss is 3.5462453174591064 and perplexity is 34.682849618230435
At time: 167.58193111419678 and batch: 300, loss is 3.53106879234314 and perplexity is 34.160458547199475
At time: 168.22833228111267 and batch: 350, loss is 3.4851909112930297 and perplexity is 32.62865569413536
At time: 168.8754472732544 and batch: 400, loss is 3.439884910583496 and perplexity is 31.183369086027987
At time: 169.5232937335968 and batch: 450, loss is 3.4434102058410643 and perplexity is 31.293493665962018
At time: 170.17040705680847 and batch: 500, loss is 3.315385522842407 and perplexity is 27.533006541687378
At time: 170.83118224143982 and batch: 550, loss is 3.3843559074401854 and perplexity is 29.498986518194776
At time: 171.47909903526306 and batch: 600, loss is 3.4032766675949095 and perplexity is 30.062443474404088
At time: 172.1251060962677 and batch: 650, loss is 3.233912172317505 and perplexity is 25.378749051423323
At time: 172.77190732955933 and batch: 700, loss is 3.2344083642959593 and perplexity is 25.391344907848875
At time: 173.42089819908142 and batch: 750, loss is 3.3277940559387207 and perplexity is 27.87677921094595
At time: 174.06872749328613 and batch: 800, loss is 3.253288564682007 and perplexity is 25.875292728027084
At time: 174.71748185157776 and batch: 850, loss is 3.321801881790161 and perplexity is 27.710236170988676
At time: 175.36406183242798 and batch: 900, loss is 3.292142753601074 and perplexity is 26.9004429657664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431498488334761 and perplexity of 84.05728143914088
Annealing...
Model not improving. Stopping early with 83.92929344488701 lossat 13 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -83.92929344488701
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f692cfc5f60>
ELAPSED
186.45936965942383


RESULTS SO FAR:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8588635921478271 and batch: 50, loss is 6.590871324539185 and perplexity is 728.415279578144
At time: 1.5223665237426758 and batch: 100, loss is 5.789135236740112 and perplexity is 326.7303577644512
At time: 2.1705355644226074 and batch: 150, loss is 5.620610542297364 and perplexity is 276.05787680363204
At time: 2.819768190383911 and batch: 200, loss is 5.440737838745117 and perplexity is 230.61227537459436
At time: 3.4697961807250977 and batch: 250, loss is 5.489051694869995 and perplexity is 242.02758200033344
At time: 4.118475675582886 and batch: 300, loss is 5.406851034164429 and perplexity is 222.92848705210903
At time: 4.768113374710083 and batch: 350, loss is 5.383042869567871 and perplexity is 217.68365163284625
At time: 5.417507648468018 and batch: 400, loss is 5.235751161575317 and perplexity is 187.8701742112009
At time: 6.069923639297485 and batch: 450, loss is 5.240248489379883 and perplexity is 188.7169907478014
At time: 6.722668647766113 and batch: 500, loss is 5.175742244720459 and perplexity is 176.92788934305963
At time: 7.378097295761108 and batch: 550, loss is 5.239740219116211 and perplexity is 188.62109588547258
At time: 8.042508363723755 and batch: 600, loss is 5.166537103652954 and perplexity is 175.30671617417988
At time: 8.697571516036987 and batch: 650, loss is 5.058837957382202 and perplexity is 157.4074957866653
At time: 9.350664138793945 and batch: 700, loss is 5.146930713653564 and perplexity is 171.90306006464874
At time: 10.00424575805664 and batch: 750, loss is 5.139233112335205 and perplexity is 170.58489869197737
At time: 10.658204078674316 and batch: 800, loss is 5.09201639175415 and perplexity is 162.71763397954032
At time: 11.31560492515564 and batch: 850, loss is 5.1354000282287595 and perplexity is 169.9322839905681
At time: 11.987425327301025 and batch: 900, loss is 5.0776171875 and perplexity is 160.39141755725714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.964129304232663 and perplexity of 143.1838264313379
finished 1 epochs...
Completing Train Step...
At time: 13.626250743865967 and batch: 50, loss is 4.923700571060181 and perplexity is 137.5105403225873
At time: 14.272984743118286 and batch: 100, loss is 4.773293447494507 and perplexity is 118.30824301832166
At time: 14.926210880279541 and batch: 150, loss is 4.746841192245483 and perplexity is 115.21975204005722
At time: 15.574257612228394 and batch: 200, loss is 4.636400527954102 and perplexity is 103.17231256514809
At time: 16.22042417526245 and batch: 250, loss is 4.752822580337525 and perplexity is 115.91099131719682
At time: 16.86648392677307 and batch: 300, loss is 4.712712383270263 and perplexity is 111.35378480259436
At time: 17.509449005126953 and batch: 350, loss is 4.6849620914459225 and perplexity is 108.30616650019196
At time: 18.15239691734314 and batch: 400, loss is 4.588003959655762 and perplexity is 98.29802738480237
At time: 18.799250841140747 and batch: 450, loss is 4.591172046661377 and perplexity is 98.60993790699267
At time: 19.450443029403687 and batch: 500, loss is 4.493630151748658 and perplexity is 89.44555818375422
At time: 20.094502925872803 and batch: 550, loss is 4.5773122692108155 and perplexity is 97.25265366897204
At time: 20.73897671699524 and batch: 600, loss is 4.55382755279541 and perplexity is 94.9953129291408
At time: 21.383955001831055 and batch: 650, loss is 4.409682712554932 and perplexity is 82.2433645769692
At time: 22.028538703918457 and batch: 700, loss is 4.434832935333252 and perplexity is 84.33803380575046
At time: 22.673252820968628 and batch: 750, loss is 4.515414247512817 and perplexity is 91.41542679316397
At time: 23.32084083557129 and batch: 800, loss is 4.450625734329224 and perplexity is 85.68054048740085
At time: 23.965351581573486 and batch: 850, loss is 4.5173566532135006 and perplexity is 91.59316520346509
At time: 24.61102604866028 and batch: 900, loss is 4.465102224349976 and perplexity is 86.92991543966625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.623218275096319 and perplexity of 101.821194011666
finished 2 epochs...
Completing Train Step...
At time: 26.242259740829468 and batch: 50, loss is 4.4723030376434325 and perplexity is 87.55814068215811
At time: 26.889856576919556 and batch: 100, loss is 4.327769470214844 and perplexity is 75.77507935699984
At time: 27.540468215942383 and batch: 150, loss is 4.323062539100647 and perplexity is 75.4192493688717
At time: 28.19577383995056 and batch: 200, loss is 4.2189479207992555 and perplexity is 67.96194531457704
At time: 28.85607600212097 and batch: 250, loss is 4.3699679183959965 and perplexity is 79.04109589374984
At time: 29.503790140151978 and batch: 300, loss is 4.344965634346008 and perplexity is 77.08938820653883
At time: 30.15071678161621 and batch: 350, loss is 4.32014536857605 and perplexity is 75.19955915033545
At time: 30.79699182510376 and batch: 400, loss is 4.253332161903382 and perplexity is 70.33940456439997
At time: 31.451465606689453 and batch: 450, loss is 4.263523659706116 and perplexity is 71.05993385121724
At time: 32.09809231758118 and batch: 500, loss is 4.15786690235138 and perplexity is 63.93499744824219
At time: 32.750417947769165 and batch: 550, loss is 4.25494873046875 and perplexity is 70.4532049930405
At time: 33.40234851837158 and batch: 600, loss is 4.255954341888428 and perplexity is 70.52408917553261
At time: 34.06154131889343 and batch: 650, loss is 4.106666197776795 and perplexity is 60.743871579171675
At time: 34.70796823501587 and batch: 700, loss is 4.1110393953323365 and perplexity is 61.010098236463875
At time: 35.35729503631592 and batch: 750, loss is 4.225219798088074 and perplexity is 68.38953378545925
At time: 36.00414752960205 and batch: 800, loss is 4.170928854942321 and perplexity is 64.77559130090725
At time: 36.66378331184387 and batch: 850, loss is 4.2434435558319095 and perplexity is 69.64727364632557
At time: 37.312575817108154 and batch: 900, loss is 4.202075614929199 and perplexity is 66.8248899329149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.528154347040882 and perplexity of 92.58751886021219
finished 3 epochs...
Completing Train Step...
At time: 38.93243169784546 and batch: 50, loss is 4.23476318359375 and perplexity is 69.04532573135694
At time: 39.60872483253479 and batch: 100, loss is 4.097329564094544 and perplexity is 60.17936768560126
At time: 40.260148763656616 and batch: 150, loss is 4.100181035995483 and perplexity is 60.351212350251814
At time: 40.91078066825867 and batch: 200, loss is 3.996298704147339 and perplexity is 54.396439652041195
At time: 41.56598258018494 and batch: 250, loss is 4.150103936195373 and perplexity is 63.44059372941376
At time: 42.2257297039032 and batch: 300, loss is 4.13401246547699 and perplexity is 62.427910902614876
At time: 42.87615895271301 and batch: 350, loss is 4.107011876106262 and perplexity is 60.76487304889191
At time: 43.5271270275116 and batch: 400, loss is 4.0513225317001345 and perplexity is 57.473417220693996
At time: 44.17865180969238 and batch: 450, loss is 4.06329710483551 and perplexity is 58.16577392344053
At time: 44.829259395599365 and batch: 500, loss is 3.957485179901123 and perplexity is 52.325570950966025
At time: 45.50145220756531 and batch: 550, loss is 4.0526219177246094 and perplexity is 57.54814591600619
At time: 46.15163850784302 and batch: 600, loss is 4.06401967048645 and perplexity is 58.20781770160366
At time: 46.80311679840088 and batch: 650, loss is 3.912597556114197 and perplexity is 50.02873578859557
At time: 47.464067220687866 and batch: 700, loss is 3.916313428878784 and perplexity is 50.21498202471451
At time: 48.12597846984863 and batch: 750, loss is 4.036617994308472 and perplexity is 56.63448041339937
At time: 48.779266119003296 and batch: 800, loss is 3.984854483604431 and perplexity is 53.777463405854995
At time: 49.43078112602234 and batch: 850, loss is 4.062252569198608 and perplexity is 58.10504941970474
At time: 50.09857702255249 and batch: 900, loss is 4.0234398794174195 and perplexity is 55.89304084165005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493922821462971 and perplexity of 89.47174002084296
finished 4 epochs...
Completing Train Step...
At time: 51.72517800331116 and batch: 50, loss is 4.064445595741272 and perplexity is 58.232615161748235
At time: 52.39157295227051 and batch: 100, loss is 3.931992435455322 and perplexity is 51.00850764881062
At time: 53.045546531677246 and batch: 150, loss is 3.937226028442383 and perplexity is 51.27616521115332
At time: 53.70017385482788 and batch: 200, loss is 3.8338603258132933 and perplexity is 46.24069829628231
At time: 54.35375690460205 and batch: 250, loss is 3.986219983100891 and perplexity is 53.8509466643148
At time: 55.00724267959595 and batch: 300, loss is 3.97434700012207 and perplexity is 53.21535593658593
At time: 55.65980243682861 and batch: 350, loss is 3.947971153259277 and perplexity is 51.83010475152447
At time: 56.31323504447937 and batch: 400, loss is 3.9016236591339113 and perplexity is 49.482726997588124
At time: 56.96810173988342 and batch: 450, loss is 3.912091279029846 and perplexity is 50.003413796619775
At time: 57.622541189193726 and batch: 500, loss is 3.809920897483826 and perplexity is 45.14686749425215
At time: 58.282071590423584 and batch: 550, loss is 3.899111423492432 and perplexity is 49.35857074736487
At time: 58.93934869766235 and batch: 600, loss is 3.9177518033981324 and perplexity is 50.28726194568489
At time: 59.59438514709473 and batch: 650, loss is 3.766498284339905 and perplexity is 43.22842583675879
At time: 60.24827432632446 and batch: 700, loss is 3.7702069997787477 and perplexity is 43.38904542913304
At time: 60.90201544761658 and batch: 750, loss is 3.8888309621810913 and perplexity is 48.85374125662663
At time: 61.576011419296265 and batch: 800, loss is 3.840771446228027 and perplexity is 46.561380189449814
At time: 62.2291476726532 and batch: 850, loss is 3.918621563911438 and perplexity is 50.3310188467014
At time: 62.882795095443726 and batch: 900, loss is 3.882178544998169 and perplexity is 48.52982439844577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.487802583877355 and perplexity of 88.92582398667672
finished 5 epochs...
Completing Train Step...
At time: 64.50755286216736 and batch: 50, loss is 3.9301967239379882 and perplexity is 50.916993275442046
At time: 65.15511655807495 and batch: 100, loss is 3.8019678878784178 and perplexity is 44.78923802373749
At time: 65.80391597747803 and batch: 150, loss is 3.8069670581817627 and perplexity is 45.01370766586998
At time: 66.4510612487793 and batch: 200, loss is 3.704392728805542 and perplexity is 40.6253692080006
At time: 67.11404180526733 and batch: 250, loss is 3.8539244174957275 and perplexity is 47.17784597626719
At time: 67.77660965919495 and batch: 300, loss is 3.8478923416137696 and perplexity is 46.89412221132664
At time: 68.42676877975464 and batch: 350, loss is 3.8212840509414674 and perplexity is 45.662804059383625
At time: 69.08564257621765 and batch: 400, loss is 3.777273392677307 and perplexity is 43.696735320113525
At time: 69.73619246482849 and batch: 450, loss is 3.7890905952453613 and perplexity is 44.21617159000083
At time: 70.38200116157532 and batch: 500, loss is 3.6892935180664064 and perplexity is 40.01656598755126
At time: 71.02773404121399 and batch: 550, loss is 3.7732812118530275 and perplexity is 43.52263779690512
At time: 71.6759524345398 and batch: 600, loss is 3.796185736656189 and perplexity is 44.53100716076399
At time: 72.33564591407776 and batch: 650, loss is 3.6464178943634034 and perplexity is 38.337092279904866
At time: 72.9891107082367 and batch: 700, loss is 3.6508721685409546 and perplexity is 38.508237080054336
At time: 73.64010882377625 and batch: 750, loss is 3.766328744888306 and perplexity is 43.22109753438467
At time: 74.28976774215698 and batch: 800, loss is 3.720497851371765 and perplexity is 41.28494275848516
At time: 74.94389128684998 and batch: 850, loss is 3.8007434511184695 and perplexity is 44.73442999558092
At time: 75.59479832649231 and batch: 900, loss is 3.766049008369446 and perplexity is 43.209008705941315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.495456225251498 and perplexity of 89.60904156841025
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 77.22043681144714 and batch: 50, loss is 3.830477724075317 and perplexity is 46.084548674739246
At time: 77.86301064491272 and batch: 100, loss is 3.6992179346084595 and perplexity is 40.41568428923945
At time: 78.52895307540894 and batch: 150, loss is 3.69857940196991 and perplexity is 40.38988579317889
At time: 79.1729462146759 and batch: 200, loss is 3.579194416999817 and perplexity is 35.84465336959263
At time: 79.81683492660522 and batch: 250, loss is 3.7215316534042358 and perplexity is 41.32764528539569
At time: 80.46105575561523 and batch: 300, loss is 3.704833121299744 and perplexity is 40.64326425580713
At time: 81.10565423965454 and batch: 350, loss is 3.6584120702743532 and perplexity is 38.79968275872781
At time: 81.74899363517761 and batch: 400, loss is 3.611475257873535 and perplexity is 37.02062752127581
At time: 82.39430570602417 and batch: 450, loss is 3.603991904258728 and perplexity is 36.744623081934364
At time: 83.03876185417175 and batch: 500, loss is 3.4898158073425294 and perplexity is 32.779909331525396
At time: 83.68304967880249 and batch: 550, loss is 3.5560500192642213 and perplexity is 35.02457714716885
At time: 84.32833623886108 and batch: 600, loss is 3.5664192771911623 and perplexity is 35.38964549426341
At time: 84.9752151966095 and batch: 650, loss is 3.402072687149048 and perplexity is 30.026270660356058
At time: 85.62220883369446 and batch: 700, loss is 3.389301619529724 and perplexity is 29.64524138155301
At time: 86.26831483840942 and batch: 750, loss is 3.487294244766235 and perplexity is 32.697356863248956
At time: 86.91572904586792 and batch: 800, loss is 3.421492323875427 and perplexity is 30.615068545393502
At time: 87.5717716217041 and batch: 850, loss is 3.4817721033096314 and perplexity is 32.517295054350406
At time: 88.222008228302 and batch: 900, loss is 3.432674918174744 and perplexity is 30.959345805183382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431018986114084 and perplexity of 84.01698544780749
finished 7 epochs...
Completing Train Step...
At time: 89.83089804649353 and batch: 50, loss is 3.716078691482544 and perplexity is 41.102900528536516
At time: 90.49302172660828 and batch: 100, loss is 3.586269564628601 and perplexity is 36.09915885416023
At time: 91.14051246643066 and batch: 150, loss is 3.587993931770325 and perplexity is 36.16146075777235
At time: 91.78330206871033 and batch: 200, loss is 3.4766916704177855 and perplexity is 32.35251205791156
At time: 92.43638348579407 and batch: 250, loss is 3.6209906768798827 and perplexity is 37.37457561560199
At time: 93.0811710357666 and batch: 300, loss is 3.611416301727295 and perplexity is 37.018444992083154
At time: 93.72505974769592 and batch: 350, loss is 3.569464364051819 and perplexity is 35.49757428162949
At time: 94.37032794952393 and batch: 400, loss is 3.527772364616394 and perplexity is 34.0480364619279
At time: 95.02827024459839 and batch: 450, loss is 3.5281034326553344 and perplexity is 34.05931054473038
At time: 95.67326283454895 and batch: 500, loss is 3.4195245695114136 and perplexity is 30.55488484344936
At time: 96.31846833229065 and batch: 550, loss is 3.489305362701416 and perplexity is 32.763181272208264
At time: 96.9623064994812 and batch: 600, loss is 3.5074610710144043 and perplexity is 33.363452722620046
At time: 97.60681009292603 and batch: 650, loss is 3.3495196294784546 and perplexity is 28.48904505880093
At time: 98.25048756599426 and batch: 700, loss is 3.3422371196746825 and perplexity is 28.282326935844914
At time: 98.89486861228943 and batch: 750, loss is 3.4477255821228026 and perplexity is 31.428828667005003
At time: 99.53993821144104 and batch: 800, loss is 3.390021300315857 and perplexity is 29.66658417125223
At time: 100.18374300003052 and batch: 850, loss is 3.4599644231796263 and perplexity is 31.81584458794469
At time: 100.83274054527283 and batch: 900, loss is 3.418564624786377 and perplexity is 30.525567916485763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.437630849341824 and perplexity of 84.57433479275855
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 102.42652606964111 and batch: 50, loss is 3.6710494232177733 and perplexity is 39.29311934306667
At time: 103.08221650123596 and batch: 100, loss is 3.548385672569275 and perplexity is 34.7571627325032
At time: 103.72664213180542 and batch: 150, loss is 3.551129689216614 and perplexity is 34.852667939668095
At time: 104.37225031852722 and batch: 200, loss is 3.4368843173980714 and perplexity is 31.089940721605657
At time: 105.0172348022461 and batch: 250, loss is 3.5814333057403562 and perplexity is 35.924995465376774
At time: 105.66017889976501 and batch: 300, loss is 3.566156015396118 and perplexity is 35.38032997892806
At time: 106.3040623664856 and batch: 350, loss is 3.518067774772644 and perplexity is 33.71921236526568
At time: 106.94686818122864 and batch: 400, loss is 3.4767224311828615 and perplexity is 32.35350726124112
At time: 107.59024357795715 and batch: 450, loss is 3.470310230255127 and perplexity is 32.14671378118589
At time: 108.23377871513367 and batch: 500, loss is 3.357697458267212 and perplexity is 28.722978823054344
At time: 108.87597966194153 and batch: 550, loss is 3.4183676481246947 and perplexity is 30.51955568417579
At time: 109.51887893676758 and batch: 600, loss is 3.43420126914978 and perplexity is 31.00663671492138
At time: 110.16137361526489 and batch: 650, loss is 3.2711020135879516 and perplexity is 26.340350779301698
At time: 110.82680773735046 and batch: 700, loss is 3.255901327133179 and perplexity is 25.94298711744958
At time: 111.47476816177368 and batch: 750, loss is 3.3562491941452026 and perplexity is 28.68141047158939
At time: 112.12487006187439 and batch: 800, loss is 3.294039239883423 and perplexity is 26.95150769331253
At time: 112.7695848941803 and batch: 850, loss is 3.3527342462539673 and perplexity is 28.580773778313684
At time: 113.41338562965393 and batch: 900, loss is 3.3099692916870116 and perplexity is 27.38428453381096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426657898785317 and perplexity of 83.65137783998269
finished 9 epochs...
Completing Train Step...
At time: 115.02473092079163 and batch: 50, loss is 3.6398632955551147 and perplexity is 38.08662975792072
At time: 115.67268776893616 and batch: 100, loss is 3.5139170503616333 and perplexity is 33.579543271863024
At time: 116.32178902626038 and batch: 150, loss is 3.515558948516846 and perplexity is 33.63472274915876
At time: 116.9702639579773 and batch: 200, loss is 3.40301965713501 and perplexity is 30.05471810477395
At time: 117.61774182319641 and batch: 250, loss is 3.5478240919113158 and perplexity is 34.73764926189375
At time: 118.262033700943 and batch: 300, loss is 3.534879765510559 and perplexity is 34.29089151852445
At time: 118.90857291221619 and batch: 350, loss is 3.488814797401428 and perplexity is 32.747112734014785
At time: 119.55334234237671 and batch: 400, loss is 3.449426727294922 and perplexity is 31.482339168818676
At time: 120.19753742218018 and batch: 450, loss is 3.4458680295944215 and perplexity is 31.370502155900947
At time: 120.84222888946533 and batch: 500, loss is 3.335656175613403 and perplexity is 28.096813620263358
At time: 121.48661851882935 and batch: 550, loss is 3.399249863624573 and perplexity is 29.941631314355114
At time: 122.13217568397522 and batch: 600, loss is 3.4183334636688234 and perplexity is 30.518512407603314
At time: 122.77752494812012 and batch: 650, loss is 3.2581556844711304 and perplexity is 26.001537853169687
At time: 123.42384672164917 and batch: 700, loss is 3.2462924337387085 and perplexity is 25.694897561806773
At time: 124.06824350357056 and batch: 750, loss is 3.349955077171326 and perplexity is 28.501453249106742
At time: 124.71247911453247 and batch: 800, loss is 3.291383743286133 and perplexity is 26.880032998746483
At time: 125.35678696632385 and batch: 850, loss is 3.354008479118347 and perplexity is 28.617215552273642
At time: 126.00193285942078 and batch: 900, loss is 3.3141806364059447 and perplexity is 27.499852373065277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42870090432363 and perplexity of 83.82245276220037
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 127.6279079914093 and batch: 50, loss is 3.6249833631515505 and perplexity is 37.524098871661785
At time: 128.2902569770813 and batch: 100, loss is 3.50074095249176 and perplexity is 33.13999802794945
At time: 128.94598197937012 and batch: 150, loss is 3.503626923561096 and perplexity is 33.23577724504337
At time: 129.60381650924683 and batch: 200, loss is 3.3900985050201418 and perplexity is 29.668874659527408
At time: 130.2585108280182 and batch: 250, loss is 3.5347038650512697 and perplexity is 34.28486026542254
At time: 130.9080512523651 and batch: 300, loss is 3.5206413412094117 and perplexity is 33.806102759626455
At time: 131.558034658432 and batch: 350, loss is 3.4728234386444092 and perplexity is 32.227606779851925
At time: 132.2075207233429 and batch: 400, loss is 3.433870792388916 and perplexity is 30.99639143506124
At time: 132.85846138000488 and batch: 450, loss is 3.4288913869857787 and perplexity is 30.842431468728805
At time: 133.50816440582275 and batch: 500, loss is 3.3163980531692503 and perplexity is 27.560898664227025
At time: 134.15683555603027 and batch: 550, loss is 3.377757821083069 and perplexity is 29.30499036318938
At time: 134.80483651161194 and batch: 600, loss is 3.3961743116378784 and perplexity is 29.849685734862454
At time: 135.45432114601135 and batch: 650, loss is 3.2333984661102293 and perplexity is 25.36571517858004
At time: 136.10373735427856 and batch: 700, loss is 3.219273581504822 and perplexity is 25.009945893807036
At time: 136.7544105052948 and batch: 750, loss is 3.3207055377960204 and perplexity is 27.679872867340507
At time: 137.40277862548828 and batch: 800, loss is 3.2603662824630737 and perplexity is 26.059080378798182
At time: 138.05184388160706 and batch: 850, loss is 3.319753050804138 and perplexity is 27.653520700537747
At time: 138.70114278793335 and batch: 900, loss is 3.279825963973999 and perplexity is 26.571147960452677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.427523678296233 and perplexity of 83.72383284948195
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 140.3034212589264 and batch: 50, loss is 3.61793701171875 and perplexity is 37.2606202554407
At time: 140.96647810935974 and batch: 100, loss is 3.4930879211425783 and perplexity is 32.88734459956835
At time: 141.61872005462646 and batch: 150, loss is 3.497047634124756 and perplexity is 33.01782721128448
At time: 142.27163910865784 and batch: 200, loss is 3.383321175575256 and perplexity is 29.46847876325634
At time: 142.92274069786072 and batch: 250, loss is 3.5281254625320435 and perplexity is 34.060060875407295
At time: 143.5871603488922 and batch: 300, loss is 3.513655352592468 and perplexity is 33.57075673005853
At time: 144.2394037246704 and batch: 350, loss is 3.466254458427429 and perplexity is 32.016598083229276
At time: 144.89090013504028 and batch: 400, loss is 3.4271639919281007 and perplexity is 30.789200393827123
At time: 145.54268217086792 and batch: 450, loss is 3.4223046779632567 and perplexity is 30.639948925946705
At time: 146.19465112686157 and batch: 500, loss is 3.3094810104370116 and perplexity is 27.37091656505828
At time: 146.84253692626953 and batch: 550, loss is 3.3709071111679076 and perplexity is 29.104916481698528
At time: 147.49885487556458 and batch: 600, loss is 3.3897538805007934 and perplexity is 29.658651799483554
At time: 148.15015196800232 and batch: 650, loss is 3.2261554288864134 and perplexity is 25.182654118673902
At time: 148.80068230628967 and batch: 700, loss is 3.2117939472198485 and perplexity is 24.823578493889833
At time: 149.45217275619507 and batch: 750, loss is 3.3128747510910035 and perplexity is 27.463964157735052
At time: 150.10482168197632 and batch: 800, loss is 3.2519519901275635 and perplexity is 25.840731572103916
At time: 150.75649857521057 and batch: 850, loss is 3.3108093214035033 and perplexity is 27.407297811149366
At time: 151.40713739395142 and batch: 900, loss is 3.270579233169556 and perplexity is 26.326584158474034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.427425436777611 and perplexity of 83.71560809701063
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 153.015216588974 and batch: 50, loss is 3.616016592979431 and perplexity is 37.18913292683673
At time: 153.67211842536926 and batch: 100, loss is 3.491089692115784 and perplexity is 32.82169376751929
At time: 154.31945967674255 and batch: 150, loss is 3.495153560638428 and perplexity is 32.95534820877716
At time: 154.9654438495636 and batch: 200, loss is 3.3814725255966187 and perplexity is 29.414052183967755
At time: 155.61428308486938 and batch: 250, loss is 3.5264304494857788 and perplexity is 34.00237752869021
At time: 156.26866817474365 and batch: 300, loss is 3.511717720031738 and perplexity is 33.505771917406186
At time: 156.9163691997528 and batch: 350, loss is 3.464438223838806 and perplexity is 31.95850120513061
At time: 157.56522011756897 and batch: 400, loss is 3.425308928489685 and perplexity is 30.73213741796263
At time: 158.2110733985901 and batch: 450, loss is 3.42050003528595 and perplexity is 30.584704629584888
At time: 158.85890579223633 and batch: 500, loss is 3.3075968647003173 and perplexity is 27.319394322251892
At time: 159.50530433654785 and batch: 550, loss is 3.3690266704559324 and perplexity is 29.050237837913862
At time: 160.16572332382202 and batch: 600, loss is 3.388009743690491 and perplexity is 29.606968137926298
At time: 160.81243205070496 and batch: 650, loss is 3.2243140745162964 and perplexity is 25.136326594245904
At time: 161.45916748046875 and batch: 700, loss is 3.209822015762329 and perplexity is 24.774676330273312
At time: 162.10572862625122 and batch: 750, loss is 3.3109315967559816 and perplexity is 27.410649253044898
At time: 162.75137901306152 and batch: 800, loss is 3.2497780656814577 and perplexity is 25.784616790781136
At time: 163.39762449264526 and batch: 850, loss is 3.308506603240967 and perplexity is 27.344259136699158
At time: 164.04468607902527 and batch: 900, loss is 3.2682100439071657 and perplexity is 26.264285325973454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4273965913955475 and perplexity of 83.71319332313811
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 165.66218996047974 and batch: 50, loss is 3.6155388450622556 and perplexity is 37.17137013944457
At time: 166.30901956558228 and batch: 100, loss is 3.49061514377594 and perplexity is 32.80612198230569
At time: 166.95522904396057 and batch: 150, loss is 3.494674415588379 and perplexity is 32.939561599150245
At time: 167.60244846343994 and batch: 200, loss is 3.3810028982162477 and perplexity is 29.400241782819577
At time: 168.24911093711853 and batch: 250, loss is 3.5259933948516844 and perplexity is 33.98751987905989
At time: 168.9062213897705 and batch: 300, loss is 3.511235475540161 and perplexity is 33.48961783888374
At time: 169.56487488746643 and batch: 350, loss is 3.463954405784607 and perplexity is 31.943042845080807
At time: 170.21651148796082 and batch: 400, loss is 3.4248456716537476 and perplexity is 30.71790384237594
At time: 170.86375284194946 and batch: 450, loss is 3.4200351524353025 and perplexity is 30.57048962932141
At time: 171.5114712715149 and batch: 500, loss is 3.3071140909194945 and perplexity is 27.306208418125646
At time: 172.15861105918884 and batch: 550, loss is 3.368544111251831 and perplexity is 29.036222760088204
At time: 172.8056902885437 and batch: 600, loss is 3.3875585079193113 and perplexity is 29.593611428565453
At time: 173.45227313041687 and batch: 650, loss is 3.223835949897766 and perplexity is 25.124311170345656
At time: 174.10888767242432 and batch: 700, loss is 3.209318313598633 and perplexity is 24.762200414537535
At time: 174.75456023216248 and batch: 750, loss is 3.31043598651886 and perplexity is 27.397067620544995
At time: 175.4027464389801 and batch: 800, loss is 3.2492358541488646 and perplexity is 25.77063986376162
At time: 176.04901146888733 and batch: 850, loss is 3.3079281091690063 and perplexity is 27.328445219456132
At time: 176.70936679840088 and batch: 900, loss is 3.267609486579895 and perplexity is 26.248516852385517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42738906651327 and perplexity of 83.71256339358338
Annealing...
Model not improving. Stopping early with 83.65137783998269 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f692cfc5f60>
ELAPSED
369.8433668613434


RESULTS SO FAR:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.65137783998269, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.24010280535543527, 'tie_weights': True, 'dropout': 0.7302462644798332, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8715004920959473 and batch: 50, loss is 6.919599761962891 and perplexity is 1011.9149066385762
At time: 1.5430951118469238 and batch: 100, loss is 6.099433460235596 and perplexity is 445.6052454658693
At time: 2.192258358001709 and batch: 150, loss is 6.014777565002442 and perplexity is 409.43475616802397
At time: 2.8422844409942627 and batch: 200, loss is 5.863388748168945 and perplexity is 351.9146758878089
At time: 3.493955135345459 and batch: 250, loss is 5.9266286468505855 and perplexity is 374.8884994098609
At time: 4.1475114822387695 and batch: 300, loss is 5.837029333114624 and perplexity is 342.7596021633968
At time: 4.7982494831085205 and batch: 350, loss is 5.84084153175354 and perplexity is 344.06876365858665
At time: 5.44855809211731 and batch: 400, loss is 5.710552120208741 and perplexity is 302.03778341918616
At time: 6.1004416942596436 and batch: 450, loss is 5.71803318977356 and perplexity is 304.30582218823145
At time: 6.752060174942017 and batch: 500, loss is 5.674926233291626 and perplexity is 291.46683720310745
At time: 7.402848482131958 and batch: 550, loss is 5.729564113616943 and perplexity is 307.835058019284
At time: 8.053584814071655 and batch: 600, loss is 5.664158020019531 and perplexity is 288.3450980795875
At time: 8.705503463745117 and batch: 650, loss is 5.572532167434693 and perplexity is 263.09946837763715
At time: 9.358582019805908 and batch: 700, loss is 5.68138557434082 and perplexity is 293.3556144702692
At time: 10.012651205062866 and batch: 750, loss is 5.637202262878418 and perplexity is 280.6763603041075
At time: 10.66543698310852 and batch: 800, loss is 5.627020282745361 and perplexity is 277.8330191658628
At time: 11.316455125808716 and batch: 850, loss is 5.663316745758056 and perplexity is 288.10262277856634
At time: 11.969584226608276 and batch: 900, loss is 5.570432386398315 and perplexity is 262.5475967108576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.359299751177226 and perplexity of 212.57603819871926
finished 1 epochs...
Completing Train Step...
At time: 13.597816705703735 and batch: 50, loss is 5.253926649093628 and perplexity is 191.31602637872825
At time: 14.24005126953125 and batch: 100, loss is 5.068863687515258 and perplexity is 158.9935582924021
At time: 14.883619546890259 and batch: 150, loss is 5.013454484939575 and perplexity is 150.42347527483048
At time: 15.52593445777893 and batch: 200, loss is 4.87528920173645 and perplexity is 131.01203664107013
At time: 16.168713331222534 and batch: 250, loss is 4.950611333847046 and perplexity is 141.26129534124837
At time: 16.811706066131592 and batch: 300, loss is 4.901089286804199 and perplexity is 134.43613946930407
At time: 17.473580598831177 and batch: 350, loss is 4.863739538192749 and perplexity is 129.5075963111702
At time: 18.116427421569824 and batch: 400, loss is 4.745801057815552 and perplexity is 115.09997031429978
At time: 18.759151220321655 and batch: 450, loss is 4.74512957572937 and perplexity is 115.02270868891546
At time: 19.401158571243286 and batch: 500, loss is 4.647308568954468 and perplexity is 104.30388075684455
At time: 20.043962001800537 and batch: 550, loss is 4.72654372215271 and perplexity is 112.904657333631
At time: 20.686662435531616 and batch: 600, loss is 4.68429856300354 and perplexity is 108.2343261149389
At time: 21.33082389831543 and batch: 650, loss is 4.539720935821533 and perplexity is 93.66465802113999
At time: 21.977089166641235 and batch: 700, loss is 4.579851608276368 and perplexity is 97.49992495160409
At time: 22.631909370422363 and batch: 750, loss is 4.635088586807251 and perplexity is 103.03704531381628
At time: 23.27998924255371 and batch: 800, loss is 4.567707166671753 and perplexity is 96.3230037967515
At time: 23.934465169906616 and batch: 850, loss is 4.624693794250488 and perplexity is 101.97154402861759
At time: 24.579427242279053 and batch: 900, loss is 4.576264123916626 and perplexity is 97.15077216032077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.67482015531357 and perplexity of 107.21328361870786
finished 2 epochs...
Completing Train Step...
At time: 26.271108865737915 and batch: 50, loss is 4.5759162330627445 and perplexity is 97.11698017354102
At time: 26.917647123336792 and batch: 100, loss is 4.428418083190918 and perplexity is 83.79874935323393
At time: 27.56496024131775 and batch: 150, loss is 4.417584505081177 and perplexity is 82.89580892566507
At time: 28.21246910095215 and batch: 200, loss is 4.32052255153656 and perplexity is 75.22792849256473
At time: 28.861358404159546 and batch: 250, loss is 4.455703220367432 and perplexity is 86.11668856633996
At time: 29.509242057800293 and batch: 300, loss is 4.4360399293899535 and perplexity is 84.439890769317
At time: 30.156068563461304 and batch: 350, loss is 4.409590587615967 and perplexity is 82.2357882610165
At time: 30.804708242416382 and batch: 400, loss is 4.336095743179321 and perplexity is 76.40863727952315
At time: 31.45296835899353 and batch: 450, loss is 4.347309246063232 and perplexity is 77.27026767284136
At time: 32.10868859291077 and batch: 500, loss is 4.2411478233337405 and perplexity is 69.48756553050633
At time: 32.75925421714783 and batch: 550, loss is 4.335281610488892 and perplexity is 76.34645582549452
At time: 33.41453504562378 and batch: 600, loss is 4.331340551376343 and perplexity is 76.04616205647885
At time: 34.079317569732666 and batch: 650, loss is 4.185719833374024 and perplexity is 65.74080631511363
At time: 34.72695231437683 and batch: 700, loss is 4.206224870681763 and perplexity is 67.10273952776947
At time: 35.37081241607666 and batch: 750, loss is 4.297522177696228 and perplexity is 73.51740476398616
At time: 36.01782560348511 and batch: 800, loss is 4.2434919071197506 and perplexity is 69.65064126311465
At time: 36.66374588012695 and batch: 850, loss is 4.31260190486908 and perplexity is 74.63442820732497
At time: 37.31063771247864 and batch: 900, loss is 4.278048281669617 and perplexity is 72.09958451620236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5552915808272685 and perplexity of 95.13449058530001
finished 3 epochs...
Completing Train Step...
At time: 38.920233488082886 and batch: 50, loss is 4.312860312461853 and perplexity is 74.65371680230842
At time: 39.58483910560608 and batch: 100, loss is 4.167899417877197 and perplexity is 64.57965466241299
At time: 40.235010862350464 and batch: 150, loss is 4.161103558540344 and perplexity is 64.14226830472826
At time: 40.88631200790405 and batch: 200, loss is 4.069966659545899 and perplexity is 58.555010308713946
At time: 41.536531925201416 and batch: 250, loss is 4.212112879753112 and perplexity is 67.4990065338484
At time: 42.19389724731445 and batch: 300, loss is 4.201502676010132 and perplexity is 66.78661431855154
At time: 42.84763550758362 and batch: 350, loss is 4.176946034431458 and perplexity is 65.16653266290302
At time: 43.4975049495697 and batch: 400, loss is 4.113977522850036 and perplexity is 61.18961728072907
At time: 44.14767932891846 and batch: 450, loss is 4.127676138877868 and perplexity is 62.0335978376891
At time: 44.79829216003418 and batch: 500, loss is 4.024589085578919 and perplexity is 55.95731039097798
At time: 45.454288482666016 and batch: 550, loss is 4.116180810928345 and perplexity is 61.32458426597521
At time: 46.10572028160095 and batch: 600, loss is 4.123917164802552 and perplexity is 61.80085286682934
At time: 46.75873041152954 and batch: 650, loss is 3.9757656955718996 and perplexity is 53.29090589843617
At time: 47.4121630191803 and batch: 700, loss is 3.987569365501404 and perplexity is 53.92366123284067
At time: 48.08340501785278 and batch: 750, loss is 4.092080483436584 and perplexity is 59.86430893845922
At time: 48.740288734436035 and batch: 800, loss is 4.045338025093079 and perplexity is 57.13049431127718
At time: 49.39739537239075 and batch: 850, loss is 4.114700722694397 and perplexity is 61.2338856079172
At time: 50.06059217453003 and batch: 900, loss is 4.085967278480529 and perplexity is 59.49946247520027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.509189135407748 and perplexity of 90.84812310976967
finished 4 epochs...
Completing Train Step...
At time: 51.689430236816406 and batch: 50, loss is 4.1316826009750365 and perplexity is 62.282631635293576
At time: 52.35449528694153 and batch: 100, loss is 3.9903636312484743 and perplexity is 54.07454898444157
At time: 53.0066442489624 and batch: 150, loss is 3.985608081817627 and perplexity is 53.818005280411285
At time: 53.658337116241455 and batch: 200, loss is 3.8964454317092896 and perplexity is 49.227156455871125
At time: 54.31082081794739 and batch: 250, loss is 4.039328718185425 and perplexity is 56.78820911561316
At time: 54.96416997909546 and batch: 300, loss is 4.033345475196838 and perplexity is 56.44944592345512
At time: 55.61765933036804 and batch: 350, loss is 4.0054492139816285 and perplexity is 54.89647912699726
At time: 56.270350217819214 and batch: 400, loss is 3.9535703372955324 and perplexity is 52.12112502436865
At time: 56.926785945892334 and batch: 450, loss is 3.967363228797913 and perplexity is 52.84500678194818
At time: 57.586639165878296 and batch: 500, loss is 3.8674739837646483 and perplexity is 47.821435660892746
At time: 58.24625277519226 and batch: 550, loss is 3.9538606548309327 and perplexity is 52.13625889763611
At time: 58.90292549133301 and batch: 600, loss is 3.9696702432632445 and perplexity is 52.96706171412716
At time: 59.54966449737549 and batch: 650, loss is 3.8195571756362914 and perplexity is 45.58401813699921
At time: 60.20087122917175 and batch: 700, loss is 3.825409145355225 and perplexity is 45.85155648044713
At time: 60.854796171188354 and batch: 750, loss is 3.9384239530563354 and perplexity is 51.337626997512274
At time: 61.509790897369385 and batch: 800, loss is 3.893739414215088 and perplexity is 49.09412698053152
At time: 62.16808557510376 and batch: 850, loss is 3.9648681354522703 and perplexity is 52.71331791350476
At time: 62.82388663291931 and batch: 900, loss is 3.934907064437866 and perplexity is 51.15739539442855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.491615190897902 and perplexity of 89.26551034133405
finished 5 epochs...
Completing Train Step...
At time: 64.47177410125732 and batch: 50, loss is 3.991383442878723 and perplexity is 54.12972296714916
At time: 65.11896634101868 and batch: 100, loss is 3.8512690973281862 and perplexity is 47.05273986228642
At time: 65.76741695404053 and batch: 150, loss is 3.8500298118591307 and perplexity is 46.994464203043144
At time: 66.41453981399536 and batch: 200, loss is 3.7589602994918825 and perplexity is 42.90379568354073
At time: 67.08473682403564 and batch: 250, loss is 3.9032904291152954 and perplexity is 49.565272094279344
At time: 67.73105597496033 and batch: 300, loss is 3.9031282567977907 and perplexity is 49.5572346309807
At time: 68.37695264816284 and batch: 350, loss is 3.8674546241760255 and perplexity is 47.820509866532525
At time: 69.02441930770874 and batch: 400, loss is 3.8204038763046264 and perplexity is 45.62263049987116
At time: 69.6830575466156 and batch: 450, loss is 3.838990635871887 and perplexity is 46.4785369873217
At time: 70.33859848976135 and batch: 500, loss is 3.739480333328247 and perplexity is 42.07611892592958
At time: 70.98849272727966 and batch: 550, loss is 3.8232003355026247 and perplexity is 45.75039087965486
At time: 71.63510346412659 and batch: 600, loss is 3.8450504684448243 and perplexity is 46.76104424854309
At time: 72.29728579521179 and batch: 650, loss is 3.6954487562179565 and perplexity is 40.26363709189179
At time: 72.9465217590332 and batch: 700, loss is 3.7002198266983033 and perplexity is 40.456196734795014
At time: 73.5932126045227 and batch: 750, loss is 3.8115111112594606 and perplexity is 45.21871777838294
At time: 74.23993468284607 and batch: 800, loss is 3.7676753187179566 and perplexity is 43.2793371363749
At time: 74.8871808052063 and batch: 850, loss is 3.8415009355545044 and perplexity is 46.595358611265276
At time: 75.54453468322754 and batch: 900, loss is 3.811507387161255 and perplexity is 45.218549379750755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.495141852391909 and perplexity of 89.58087534534738
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 77.16892433166504 and batch: 50, loss is 3.8848656463623046 and perplexity is 48.66040431795685
At time: 77.81592512130737 and batch: 100, loss is 3.739413175582886 and perplexity is 42.07329328353191
At time: 78.47112989425659 and batch: 150, loss is 3.7353941869735716 and perplexity is 41.90454053190727
At time: 79.11820340156555 and batch: 200, loss is 3.6255929708480834 and perplexity is 37.54698082493821
At time: 79.76424098014832 and batch: 250, loss is 3.7627207040786743 and perplexity is 43.06543503783088
At time: 80.40943431854248 and batch: 300, loss is 3.7499610233306884 and perplexity is 42.51942470220908
At time: 81.05753874778748 and batch: 350, loss is 3.7016696691513062 and perplexity is 40.51489438720025
At time: 81.70380330085754 and batch: 400, loss is 3.6474951267242433 and perplexity is 38.37841248806621
At time: 82.35002875328064 and batch: 450, loss is 3.649524440765381 and perplexity is 38.4563734162536
At time: 82.99707102775574 and batch: 500, loss is 3.5392451429367067 and perplexity is 34.44091141076941
At time: 83.66674399375916 and batch: 550, loss is 3.598879418373108 and perplexity is 36.557246104262845
At time: 84.31914591789246 and batch: 600, loss is 3.6148598289489744 and perplexity is 37.14613874739746
At time: 84.96686625480652 and batch: 650, loss is 3.4499579000473024 and perplexity is 31.49906617163713
At time: 85.61337327957153 and batch: 700, loss is 3.4355764865875242 and perplexity is 31.049306916095
At time: 86.26014924049377 and batch: 750, loss is 3.5267461252212526 and perplexity is 34.01311294859108
At time: 86.9061439037323 and batch: 800, loss is 3.4683762121200563 and perplexity is 32.08460153621476
At time: 87.55445575714111 and batch: 850, loss is 3.5243106269836426 and perplexity is 33.9303748670728
At time: 88.2011399269104 and batch: 900, loss is 3.48074098110199 and perplexity is 32.48378302975302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430813724047517 and perplexity of 83.99974171755015
finished 7 epochs...
Completing Train Step...
At time: 89.79994487762451 and batch: 50, loss is 3.770818076133728 and perplexity is 43.41556755155559
At time: 90.46397709846497 and batch: 100, loss is 3.6290695333480834 and perplexity is 37.67774241918591
At time: 91.11651134490967 and batch: 150, loss is 3.6275757360458374 and perplexity is 37.62150152591087
At time: 91.7699224948883 and batch: 200, loss is 3.5244656133651735 and perplexity is 33.9356340206355
At time: 92.42201256752014 and batch: 250, loss is 3.6632673072814943 and perplexity is 38.9885224741458
At time: 93.06795048713684 and batch: 300, loss is 3.657784399986267 and perplexity is 38.77533699202948
At time: 93.714040517807 and batch: 350, loss is 3.6138598012924192 and perplexity is 37.10901014922256
At time: 94.36047601699829 and batch: 400, loss is 3.565046091079712 and perplexity is 35.341082275381446
At time: 95.00889372825623 and batch: 450, loss is 3.5743686962127685 and perplexity is 35.672093777801486
At time: 95.65384817123413 and batch: 500, loss is 3.467148585319519 and perplexity is 32.045237786425105
At time: 96.30019021034241 and batch: 550, loss is 3.5330759477615357 and perplexity is 34.22909275338473
At time: 96.94670605659485 and batch: 600, loss is 3.5557886123657227 and perplexity is 35.01542267765846
At time: 97.59416151046753 and batch: 650, loss is 3.396026463508606 and perplexity is 29.84527284089429
At time: 98.24124312400818 and batch: 700, loss is 3.3887139987945556 and perplexity is 29.62782634023894
At time: 98.8882508277893 and batch: 750, loss is 3.487304344177246 and perplexity is 32.69768708896243
At time: 99.53535032272339 and batch: 800, loss is 3.436229724884033 and perplexity is 31.069596138579733
At time: 100.20503544807434 and batch: 850, loss is 3.5011485528945925 and perplexity is 33.153508657777536
At time: 100.85144305229187 and batch: 900, loss is 3.4657743787765503 and perplexity is 32.001231254945075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436273862237799 and perplexity of 84.4596463440853
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 102.45579981803894 and batch: 50, loss is 3.7252515029907225 and perplexity is 41.48166419561058
At time: 103.11763572692871 and batch: 100, loss is 3.588774938583374 and perplexity is 36.18971413659851
At time: 103.76532864570618 and batch: 150, loss is 3.5918061876297 and perplexity is 36.299580605250654
At time: 104.4135971069336 and batch: 200, loss is 3.4824693107604983 and perplexity is 32.53997425988118
At time: 105.05999302864075 and batch: 250, loss is 3.6200153541564943 and perplexity is 37.3381411133143
At time: 105.70663714408875 and batch: 300, loss is 3.6094855260849 and perplexity is 36.94703963621666
At time: 106.35655426979065 and batch: 350, loss is 3.562093024253845 and perplexity is 35.23687164393473
At time: 107.00179195404053 and batch: 400, loss is 3.511268162727356 and perplexity is 33.49071253818234
At time: 107.64630937576294 and batch: 450, loss is 3.5150154495239256 and perplexity is 33.61644727801739
At time: 108.29111409187317 and batch: 500, loss is 3.401865577697754 and perplexity is 30.02005257984904
At time: 108.93473672866821 and batch: 550, loss is 3.4632191896438598 and perplexity is 31.91956643557162
At time: 109.58767986297607 and batch: 600, loss is 3.482230339050293 and perplexity is 32.53219905564619
At time: 110.23425793647766 and batch: 650, loss is 3.318355498313904 and perplexity is 27.61490044702064
At time: 110.879549741745 and batch: 700, loss is 3.304059796333313 and perplexity is 27.222934449936716
At time: 111.5286750793457 and batch: 750, loss is 3.394799060821533 and perplexity is 29.808663144828703
At time: 112.17879438400269 and batch: 800, loss is 3.33798677444458 and perplexity is 28.162372387164716
At time: 112.82948040962219 and batch: 850, loss is 3.395866355895996 and perplexity is 29.840494768025163
At time: 113.48063206672668 and batch: 900, loss is 3.3590493869781493 and perplexity is 28.761836503277255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424594826894264 and perplexity of 83.47897693259486
finished 9 epochs...
Completing Train Step...
At time: 115.13990640640259 and batch: 50, loss is 3.6944769048690795 and perplexity is 40.22452583011796
At time: 115.78419089317322 and batch: 100, loss is 3.5555001640319825 and perplexity is 35.00532399387615
At time: 116.44603538513184 and batch: 150, loss is 3.556689715385437 and perplexity is 35.04698940106878
At time: 117.09167408943176 and batch: 200, loss is 3.4502296829223633 and perplexity is 31.507628241862317
At time: 117.73835062980652 and batch: 250, loss is 3.5875799036026 and perplexity is 36.146491993391436
At time: 118.38601016998291 and batch: 300, loss is 3.579507088661194 and perplexity is 35.855862729246404
At time: 119.03353190422058 and batch: 350, loss is 3.533952488899231 and perplexity is 34.25910911465493
At time: 119.68006682395935 and batch: 400, loss is 3.484977388381958 and perplexity is 32.621689472337636
At time: 120.32704472541809 and batch: 450, loss is 3.491533150672913 and perplexity is 32.836252056240234
At time: 120.9750747680664 and batch: 500, loss is 3.380277991294861 and perplexity is 29.37893706696171
At time: 121.62098407745361 and batch: 550, loss is 3.4440832471847536 and perplexity is 31.314562570314212
At time: 122.26807355880737 and batch: 600, loss is 3.466738667488098 and perplexity is 32.03210456399925
At time: 122.91457676887512 and batch: 650, loss is 3.3055462074279784 and perplexity is 27.263429010058385
At time: 123.57668566703796 and batch: 700, loss is 3.2942011165618896 and perplexity is 26.955870866996094
At time: 124.23083806037903 and batch: 750, loss is 3.3881941270828246 and perplexity is 29.612427674456754
At time: 124.87786507606506 and batch: 800, loss is 3.3346733951568606 and perplexity is 28.06921418526034
At time: 125.5242977142334 and batch: 850, loss is 3.3963504934310915 and perplexity is 29.854945169316764
At time: 126.18507385253906 and batch: 900, loss is 3.362853364944458 and perplexity is 28.8714542551798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426434660611087 and perplexity of 83.63270574336181
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 127.82818651199341 and batch: 50, loss is 3.6799912452697754 and perplexity is 39.64604698060746
At time: 128.47960543632507 and batch: 100, loss is 3.5420826292037964 and perplexity is 34.538775802696065
At time: 129.14994859695435 and batch: 150, loss is 3.5444962692260744 and perplexity is 34.62224066083517
At time: 129.80643367767334 and batch: 200, loss is 3.437545294761658 and perplexity is 31.110497261613276
At time: 130.4587550163269 and batch: 250, loss is 3.573621311187744 and perplexity is 35.64544294955622
At time: 131.11195921897888 and batch: 300, loss is 3.5651731586456297 and perplexity is 35.34557326600666
At time: 131.76602625846863 and batch: 350, loss is 3.5188772249221802 and perplexity is 33.746517436313034
At time: 132.42234706878662 and batch: 400, loss is 3.469996757507324 and perplexity is 32.136638241770086
At time: 133.0903718471527 and batch: 450, loss is 3.473912491798401 and perplexity is 32.262723475164215
At time: 133.74129605293274 and batch: 500, loss is 3.3607005071640015 and perplexity is 28.8093649789251
At time: 134.39349341392517 and batch: 550, loss is 3.421775894165039 and perplexity is 30.623751300274886
At time: 135.04455828666687 and batch: 600, loss is 3.4438533926010133 and perplexity is 31.3073656017294
At time: 135.7027885913849 and batch: 650, loss is 3.281478290557861 and perplexity is 26.61508846662756
At time: 136.3543357849121 and batch: 700, loss is 3.2673721265792848 and perplexity is 26.242287243768622
At time: 137.0052034854889 and batch: 750, loss is 3.360078811645508 and perplexity is 28.791459892159857
At time: 137.6555299758911 and batch: 800, loss is 3.3048222208023073 and perplexity is 27.243697795514738
At time: 138.30688858032227 and batch: 850, loss is 3.363020887374878 and perplexity is 28.87629127650986
At time: 138.9576518535614 and batch: 900, loss is 3.3287722730636595 and perplexity is 27.90406209587526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425699730441995 and perplexity of 83.5712641252031
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 140.68101716041565 and batch: 50, loss is 3.672763915061951 and perplexity is 39.36054485944528
At time: 141.3536822795868 and batch: 100, loss is 3.535245189666748 and perplexity is 34.30342452841538
At time: 142.0084526538849 and batch: 150, loss is 3.538283519744873 and perplexity is 34.40780815059123
At time: 142.6621823310852 and batch: 200, loss is 3.4316597604751586 and perplexity is 30.92793313402031
At time: 143.3267469406128 and batch: 250, loss is 3.5676594972610474 and perplexity is 35.43356367134446
At time: 143.98506450653076 and batch: 300, loss is 3.5581518411636353 and perplexity is 35.09826998791861
At time: 144.65421199798584 and batch: 350, loss is 3.5127871513366697 and perplexity is 33.54162320561865
At time: 145.30895256996155 and batch: 400, loss is 3.463777847290039 and perplexity is 31.937403527373526
At time: 145.96230959892273 and batch: 450, loss is 3.4679543590545654 and perplexity is 32.07106940318824
At time: 146.61579656600952 and batch: 500, loss is 3.3544055604934693 and perplexity is 28.628581171965916
At time: 147.26931428909302 and batch: 550, loss is 3.414863829612732 and perplexity is 30.412807821770556
At time: 147.92103672027588 and batch: 600, loss is 3.4374889135360718 and perplexity is 31.108743263095835
At time: 148.57423615455627 and batch: 650, loss is 3.274234914779663 and perplexity is 26.42300189693854
At time: 149.22721791267395 and batch: 700, loss is 3.259632148742676 and perplexity is 26.039956549752333
At time: 149.8930160999298 and batch: 750, loss is 3.3525427293777468 and perplexity is 28.57530060191925
At time: 150.54564356803894 and batch: 800, loss is 3.2972393083572387 and perplexity is 27.037892508347117
At time: 151.19862508773804 and batch: 850, loss is 3.354305467605591 and perplexity is 28.62571579800472
At time: 151.85208582878113 and batch: 900, loss is 3.319781017303467 and perplexity is 27.654294083520213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42526746776006 and perplexity of 83.53514719300301
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 153.46133399009705 and batch: 50, loss is 3.670502395629883 and perplexity is 39.271630800720345
At time: 154.11959600448608 and batch: 100, loss is 3.5332422733306883 and perplexity is 34.23478640020491
At time: 154.76694917678833 and batch: 150, loss is 3.5363552618026732 and perplexity is 34.341524947345434
At time: 155.41435480117798 and batch: 200, loss is 3.4299513912200927 and perplexity is 30.87514191022263
At time: 156.06161737442017 and batch: 250, loss is 3.5660113430023195 and perplexity is 35.37521179213572
At time: 156.70924067497253 and batch: 300, loss is 3.5561445808410643 and perplexity is 35.02788928301007
At time: 157.35427403450012 and batch: 350, loss is 3.511117057800293 and perplexity is 33.4856523088294
At time: 158.00108289718628 and batch: 400, loss is 3.4620042419433594 and perplexity is 31.880809380394723
At time: 158.64692306518555 and batch: 450, loss is 3.466293659210205 and perplexity is 32.01785318353626
At time: 159.29213857650757 and batch: 500, loss is 3.3527258348464968 and perplexity is 28.58053337479068
At time: 159.93893218040466 and batch: 550, loss is 3.4129981088638304 and perplexity is 30.356118914464524
At time: 160.58679127693176 and batch: 600, loss is 3.4358050870895385 and perplexity is 31.056405614595317
At time: 161.23295950889587 and batch: 650, loss is 3.272408137321472 and perplexity is 26.374777014170856
At time: 161.87931299209595 and batch: 700, loss is 3.2575777912139894 and perplexity is 25.986516080677617
At time: 162.52492666244507 and batch: 750, loss is 3.350540347099304 and perplexity is 28.518139175006148
At time: 163.17179942131042 and batch: 800, loss is 3.2952990198135375 and perplexity is 26.985482057408664
At time: 163.8198480606079 and batch: 850, loss is 3.3520892238616944 and perplexity is 28.562344483531053
At time: 164.4674150943756 and batch: 900, loss is 3.3174594020843506 and perplexity is 27.59016592276267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425173406731592 and perplexity of 83.52729016067093
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 166.08048582077026 and batch: 50, loss is 3.669953856468201 and perplexity is 39.25009468052569
At time: 166.72819542884827 and batch: 100, loss is 3.5327384948730467 and perplexity is 34.21754399585513
At time: 167.37501168251038 and batch: 150, loss is 3.5358463096618653 and perplexity is 34.32405120173218
At time: 168.02146911621094 and batch: 200, loss is 3.429510545730591 and perplexity is 30.861533742943756
At time: 168.66786289215088 and batch: 250, loss is 3.5655670070648195 and perplexity is 35.359496805864936
At time: 169.31297755241394 and batch: 300, loss is 3.5556413459777834 and perplexity is 35.010266462516455
At time: 169.95902585983276 and batch: 350, loss is 3.5106638717651366 and perplexity is 33.47048051690671
At time: 170.60642409324646 and batch: 400, loss is 3.461560764312744 and perplexity is 31.86667408916301
At time: 171.25321435928345 and batch: 450, loss is 3.4658623790740966 and perplexity is 32.0040474967306
At time: 171.90635514259338 and batch: 500, loss is 3.352300691604614 and perplexity is 28.56838513673062
At time: 172.5577847957611 and batch: 550, loss is 3.4125241661071777 and perplexity is 30.341735260568456
At time: 173.20493865013123 and batch: 600, loss is 3.4353714847564696 and perplexity is 31.042942403714857
At time: 173.85153198242188 and batch: 650, loss is 3.271947922706604 and perplexity is 26.36264174895244
At time: 174.49923300743103 and batch: 700, loss is 3.2570610094070434 and perplexity is 25.973090191367678
At time: 175.14697217941284 and batch: 750, loss is 3.3500210285186767 and perplexity is 28.503333020336022
At time: 175.79371881484985 and batch: 800, loss is 3.2948065328598024 and perplexity is 26.97219533159325
At time: 176.44200611114502 and batch: 850, loss is 3.351534757614136 and perplexity is 28.546512017253498
At time: 177.08939719200134 and batch: 900, loss is 3.316871929168701 and perplexity is 27.573962207632675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425152504280822 and perplexity of 83.5255442538473
Annealing...
Model not improving. Stopping early with 83.47897693259486 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f692cfc5f60>
ELAPSED
554.0118403434753


RESULTS SO FAR:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.65137783998269, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.47897693259486, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.24010280535543527, 'tie_weights': True, 'dropout': 0.7302462644798332, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.04256940005503396, 'tie_weights': True, 'dropout': 0.5685987769311317, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9209191799163818 and batch: 50, loss is 6.646939706802368 and perplexity is 770.4229939468764
At time: 1.5848724842071533 and batch: 100, loss is 5.890734100341797 and perplexity is 361.67068952015694
At time: 2.263174295425415 and batch: 150, loss is 5.774085655212402 and perplexity is 321.8500182533297
At time: 2.927274703979492 and batch: 200, loss is 5.612421712875366 and perplexity is 273.806516523301
At time: 3.591049909591675 and batch: 250, loss is 5.67293833732605 and perplexity is 290.8880069710887
At time: 4.249579429626465 and batch: 300, loss is 5.5925204372406006 and perplexity is 268.41128166429564
At time: 4.907680511474609 and batch: 350, loss is 5.573885612487793 and perplexity is 263.4558001349187
At time: 5.5648088455200195 and batch: 400, loss is 5.431720857620239 and perplexity is 228.5421958003965
At time: 6.219033241271973 and batch: 450, loss is 5.437644453048706 and perplexity is 229.90000489217317
At time: 6.872734546661377 and batch: 500, loss is 5.387822999954223 and perplexity is 218.72669883533788
At time: 7.527074337005615 and batch: 550, loss is 5.442063655853271 and perplexity is 230.9182278483316
At time: 8.181946754455566 and batch: 600, loss is 5.370530290603638 and perplexity is 214.9768376800825
At time: 8.837074995040894 and batch: 650, loss is 5.275845775604248 and perplexity is 195.55580291555142
At time: 9.491413354873657 and batch: 700, loss is 5.368838205337524 and perplexity is 214.61338612271902
At time: 10.14623236656189 and batch: 750, loss is 5.348697719573974 and perplexity is 210.33420531422396
At time: 10.79995059967041 and batch: 800, loss is 5.315833053588867 and perplexity is 203.5339972512346
At time: 11.457504510879517 and batch: 850, loss is 5.3585687923431395 and perplexity is 212.42071064162374
At time: 12.112987995147705 and batch: 900, loss is 5.278954153060913 and perplexity is 196.16460987557764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.119783166336687 and perplexity of 167.29908961341286
finished 1 epochs...
Completing Train Step...
At time: 13.744298219680786 and batch: 50, loss is 5.064170579910279 and perplexity is 158.24913261608555
At time: 14.401851177215576 and batch: 100, loss is 4.902702379226684 and perplexity is 134.65317238720667
At time: 15.045364379882812 and batch: 150, loss is 4.865907468795776 and perplexity is 129.7886643504565
At time: 15.690380811691284 and batch: 200, loss is 4.747344942092895 and perplexity is 115.27780859432869
At time: 16.33559012413025 and batch: 250, loss is 4.843476495742798 and perplexity is 126.90978703513787
At time: 16.97978401184082 and batch: 300, loss is 4.803604078292847 and perplexity is 121.94914069831788
At time: 17.62325644493103 and batch: 350, loss is 4.772729873657227 and perplexity is 118.24158637255701
At time: 18.267200231552124 and batch: 400, loss is 4.665999269485473 and perplexity is 106.27172627462025
At time: 18.914652824401855 and batch: 450, loss is 4.6687353897094725 and perplexity is 106.56289665104698
At time: 19.562265872955322 and batch: 500, loss is 4.5751726436614994 and perplexity is 97.04479185896209
At time: 20.207136154174805 and batch: 550, loss is 4.651123161315918 and perplexity is 104.7025173783711
At time: 20.85194420814514 and batch: 600, loss is 4.616999616622925 and perplexity is 101.18996750599956
At time: 21.520142316818237 and batch: 650, loss is 4.474783420562744 and perplexity is 87.77558796353073
At time: 22.165029048919678 and batch: 700, loss is 4.51094533920288 and perplexity is 91.00781110964823
At time: 22.810368537902832 and batch: 750, loss is 4.569994792938233 and perplexity is 96.54360706301063
At time: 23.45639705657959 and batch: 800, loss is 4.509657344818115 and perplexity is 90.89066901534328
At time: 24.09962224960327 and batch: 850, loss is 4.57923544883728 and perplexity is 97.439867956777
At time: 24.744348764419556 and batch: 900, loss is 4.525543508529663 and perplexity is 92.3461030862474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.645600619381422 and perplexity of 104.12588703365923
finished 2 epochs...
Completing Train Step...
At time: 26.37338376045227 and batch: 50, loss is 4.523173131942749 and perplexity is 92.12746727257641
At time: 27.020177602767944 and batch: 100, loss is 4.372273392677307 and perplexity is 79.2235333290993
At time: 27.66652488708496 and batch: 150, loss is 4.368926496505737 and perplexity is 78.95882361376843
At time: 28.313251495361328 and batch: 200, loss is 4.267561616897583 and perplexity is 71.3474509222833
At time: 28.959758520126343 and batch: 250, loss is 4.411189937591553 and perplexity is 82.36741729935562
At time: 29.605380535125732 and batch: 300, loss is 4.392130813598633 and perplexity is 80.81243188046982
At time: 30.262348890304565 and batch: 350, loss is 4.36353325843811 and perplexity is 78.53412615715608
At time: 30.909992456436157 and batch: 400, loss is 4.295194120407104 and perplexity is 73.34645110611449
At time: 31.5579092502594 and batch: 450, loss is 4.309540281295776 and perplexity is 74.40627512019711
At time: 32.205671072006226 and batch: 500, loss is 4.202173280715942 and perplexity is 66.83141675708275
At time: 32.85380029678345 and batch: 550, loss is 4.291101140975952 and perplexity is 73.04685912054877
At time: 33.500855684280396 and batch: 600, loss is 4.289221515655518 and perplexity is 72.90968735071905
At time: 34.14810347557068 and batch: 650, loss is 4.1412057256698604 and perplexity is 62.87859009208228
At time: 34.795307874679565 and batch: 700, loss is 4.160949292182923 and perplexity is 64.13237407383323
At time: 35.44269824028015 and batch: 750, loss is 4.256935887336731 and perplexity is 70.59334575793983
At time: 36.09998965263367 and batch: 800, loss is 4.205157361030579 and perplexity is 67.03114492646557
At time: 36.746800661087036 and batch: 850, loss is 4.282898445129394 and perplexity is 72.45012869759212
At time: 37.39418601989746 and batch: 900, loss is 4.241667971611023 and perplexity is 69.52371876971702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.537411206389127 and perplexity of 93.44856765493918
finished 3 epochs...
Completing Train Step...
At time: 39.02603602409363 and batch: 50, loss is 4.271882200241089 and perplexity is 71.65638042760044
At time: 39.676513671875 and batch: 100, loss is 4.123912792205811 and perplexity is 61.80058263721232
At time: 40.33510971069336 and batch: 150, loss is 4.125020475387573 and perplexity is 61.869076030709444
At time: 40.98884153366089 and batch: 200, loss is 4.029133877754211 and perplexity is 56.212203516797665
At time: 41.6393768787384 and batch: 250, loss is 4.179142169952392 and perplexity is 65.30980446462317
At time: 42.2895450592041 and batch: 300, loss is 4.163561840057373 and perplexity is 64.30014202688413
At time: 42.93946313858032 and batch: 350, loss is 4.136502156257629 and perplexity is 62.58353073909488
At time: 43.589986085891724 and batch: 400, loss is 4.082284917831421 and perplexity is 59.28076690104862
At time: 44.24313044548035 and batch: 450, loss is 4.097504425048828 and perplexity is 60.18989162734941
At time: 44.89759540557861 and batch: 500, loss is 3.9903181695938112 and perplexity is 54.07209072184835
At time: 45.561503410339355 and batch: 550, loss is 4.075733094215393 and perplexity is 58.893639353027304
At time: 46.22069835662842 and batch: 600, loss is 4.086138195991516 and perplexity is 59.50963284435392
At time: 46.873204469680786 and batch: 650, loss is 3.940957055091858 and perplexity is 51.467835290780314
At time: 47.525177240371704 and batch: 700, loss is 3.9498570585250854 and perplexity is 51.92794364744915
At time: 48.17766809463501 and batch: 750, loss is 4.059268803596496 and perplexity is 57.93193596535607
At time: 48.82784366607666 and batch: 800, loss is 4.0120281314849855 and perplexity is 55.25882916295555
At time: 49.47971796989441 and batch: 850, loss is 4.092346711158752 and perplexity is 59.88024859876265
At time: 50.13152360916138 and batch: 900, loss is 4.054178347587586 and perplexity is 57.63778530948748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.496874665560788 and perplexity of 89.73623683316691
finished 4 epochs...
Completing Train Step...
At time: 51.75206446647644 and batch: 50, loss is 4.096500654220581 and perplexity is 60.12950508217498
At time: 52.41753673553467 and batch: 100, loss is 3.9532001781463624 and perplexity is 52.10183548339613
At time: 53.078264236450195 and batch: 150, loss is 3.9572635746002196 and perplexity is 52.31397661180145
At time: 53.736417055130005 and batch: 200, loss is 3.863699097633362 and perplexity is 47.6412554804308
At time: 54.39051008224487 and batch: 250, loss is 4.010608034133911 and perplexity is 55.18041193926211
At time: 55.04512858390808 and batch: 300, loss is 4.001577472686767 and perplexity is 54.68434509087007
At time: 55.69805455207825 and batch: 350, loss is 3.972011466026306 and perplexity is 53.09121468287282
At time: 56.35111856460571 and batch: 400, loss is 3.9257230949401856 and perplexity is 50.68971828887378
At time: 57.00569701194763 and batch: 450, loss is 3.9416590404510496 and perplexity is 51.503977641840194
At time: 57.658573389053345 and batch: 500, loss is 3.832487998008728 and perplexity is 46.17728442257105
At time: 58.309561014175415 and batch: 550, loss is 3.9173894214630125 and perplexity is 50.26904205186871
At time: 58.961275577545166 and batch: 600, loss is 3.9319659423828126 and perplexity is 51.00715629461971
At time: 59.62149691581726 and batch: 650, loss is 3.7911766576766968 and perplexity is 44.308505558139444
At time: 60.27566313743591 and batch: 700, loss is 3.7974256706237792 and perplexity is 44.5862569150986
At time: 60.92592167854309 and batch: 750, loss is 3.9089952754974364 and perplexity is 49.84884245105574
At time: 61.57649111747742 and batch: 800, loss is 3.8632349395751953 and perplexity is 47.61914753898489
At time: 62.22710824012756 and batch: 850, loss is 3.944506297111511 and perplexity is 51.65083165153927
At time: 62.87759041786194 and batch: 900, loss is 3.9081617641448974 and perplexity is 49.8073101861735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.48307235926798 and perplexity of 88.50617815762509
finished 5 epochs...
Completing Train Step...
At time: 64.49011611938477 and batch: 50, loss is 3.9557540035247802 and perplexity is 52.235064522556215
At time: 65.14486074447632 and batch: 100, loss is 3.8190304136276243 and perplexity is 45.56001253121789
At time: 65.79135322570801 and batch: 150, loss is 3.8234697246551512 and perplexity is 45.762717198895494
At time: 66.43642091751099 and batch: 200, loss is 3.728587074279785 and perplexity is 41.62026026375479
At time: 67.08084106445312 and batch: 250, loss is 3.874875502586365 and perplexity is 48.176700043170534
At time: 67.73363304138184 and batch: 300, loss is 3.8718236589431765 and perplexity is 48.02989641219899
At time: 68.39680933952332 and batch: 350, loss is 3.8394885587692262 and perplexity is 46.50168547772595
At time: 69.06192135810852 and batch: 400, loss is 3.7981250143051146 and perplexity is 44.61744893784729
At time: 69.71628189086914 and batch: 450, loss is 3.8134154653549195 and perplexity is 45.304912275147906
At time: 70.36416101455688 and batch: 500, loss is 3.7077694749832153 and perplexity is 40.762782642765195
At time: 71.00966215133667 and batch: 550, loss is 3.7877055549621583 and perplexity is 44.15497280235608
At time: 71.65538883209229 and batch: 600, loss is 3.807924361228943 and perplexity is 45.05682005790308
At time: 72.30221962928772 and batch: 650, loss is 3.668049120903015 and perplexity is 39.1754047840832
At time: 72.94867706298828 and batch: 700, loss is 3.672167491912842 and perplexity is 39.337076318615765
At time: 73.6037266254425 and batch: 750, loss is 3.7838894653320314 and perplexity is 43.986794564233534
At time: 74.25289344787598 and batch: 800, loss is 3.7393647813796997 and perplexity is 42.07125722929504
At time: 74.89942073822021 and batch: 850, loss is 3.8218216276168824 and perplexity is 45.687357916979245
At time: 75.54612398147583 and batch: 900, loss is 3.7872595310211183 and perplexity is 44.135283018754116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.486665490555437 and perplexity of 88.82476449400153
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 77.16614866256714 and batch: 50, loss is 3.8489033937454225 and perplexity is 46.9415585898353
At time: 77.81333684921265 and batch: 100, loss is 3.712649874687195 and perplexity is 40.962207555921836
At time: 78.47027897834778 and batch: 150, loss is 3.712123408317566 and perplexity is 40.9406480069046
At time: 79.12045502662659 and batch: 200, loss is 3.598945517539978 and perplexity is 36.55966258763629
At time: 79.7752115726471 and batch: 250, loss is 3.738649845123291 and perplexity is 42.0411897116091
At time: 80.42632126808167 and batch: 300, loss is 3.720784754753113 and perplexity is 41.29678924747894
At time: 81.07694935798645 and batch: 350, loss is 3.67224009513855 and perplexity is 39.33993242092628
At time: 81.72432851791382 and batch: 400, loss is 3.624135298728943 and perplexity is 37.49228950851939
At time: 82.37130808830261 and batch: 450, loss is 3.6262104368209838 and perplexity is 37.57017196711651
At time: 83.0255196094513 and batch: 500, loss is 3.5052305459976196 and perplexity is 33.28911764062552
At time: 83.68797397613525 and batch: 550, loss is 3.568505606651306 and perplexity is 35.46355702933868
At time: 84.33986973762512 and batch: 600, loss is 3.578378686904907 and perplexity is 35.81542572965089
At time: 84.99596118927002 and batch: 650, loss is 3.423183183670044 and perplexity is 30.6668781229285
At time: 85.65602803230286 and batch: 700, loss is 3.4081085109710694 and perplexity is 30.20805198825377
At time: 86.30395579338074 and batch: 750, loss is 3.50566472530365 and perplexity is 33.303574224768816
At time: 86.95023655891418 and batch: 800, loss is 3.4399960708618162 and perplexity is 31.186835630682022
At time: 87.59561324119568 and batch: 850, loss is 3.5028739404678344 and perplexity is 33.210760686391296
At time: 88.2484176158905 and batch: 900, loss is 3.456808671951294 and perplexity is 31.715599954061826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426144952643408 and perplexity of 83.6084801914865
finished 7 epochs...
Completing Train Step...
At time: 89.88013529777527 and batch: 50, loss is 3.7348506355285647 and perplexity is 41.88176944753735
At time: 90.52554130554199 and batch: 100, loss is 3.5984940671920778 and perplexity is 36.54316144024661
At time: 91.17412304878235 and batch: 150, loss is 3.602495059967041 and perplexity is 36.689663246043935
At time: 91.83084654808044 and batch: 200, loss is 3.4964094877243044 and perplexity is 32.99676372520758
At time: 92.49251508712769 and batch: 250, loss is 3.638890476226807 and perplexity is 38.04959636466312
At time: 93.14244365692139 and batch: 300, loss is 3.6286265802383424 and perplexity is 37.66105664179463
At time: 93.78859829902649 and batch: 350, loss is 3.5846133852005004 and perplexity is 36.03942165131204
At time: 94.43524599075317 and batch: 400, loss is 3.5425374507904053 and perplexity is 34.55448835643963
At time: 95.08780074119568 and batch: 450, loss is 3.5503357315063475 and perplexity is 34.82500737734543
At time: 95.74275374412537 and batch: 500, loss is 3.4332521629333494 and perplexity is 30.97722208427711
At time: 96.39283442497253 and batch: 550, loss is 3.501444573402405 and perplexity is 33.16332422897968
At time: 97.03990077972412 and batch: 600, loss is 3.518996067047119 and perplexity is 33.75052818247253
At time: 97.68582153320312 and batch: 650, loss is 3.3709095764160155 and perplexity is 29.104988232627257
At time: 98.33248805999756 and batch: 700, loss is 3.36140380859375 and perplexity is 28.829633773205455
At time: 98.97966480255127 and batch: 750, loss is 3.466788105964661 and perplexity is 32.03368822159648
At time: 99.62954187393188 and batch: 800, loss is 3.4078246116638184 and perplexity is 30.199477160472334
At time: 100.27954816818237 and batch: 850, loss is 3.4794379615783693 and perplexity is 32.44148359074651
At time: 100.93094992637634 and batch: 900, loss is 3.443337597846985 and perplexity is 31.29122159064561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43236301369863 and perplexity of 84.12998251239577
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 102.56661915779114 and batch: 50, loss is 3.6889454412460325 and perplexity is 40.0026395723718
At time: 103.2103898525238 and batch: 100, loss is 3.5577432441711427 and perplexity is 35.0839318698153
At time: 103.8553946018219 and batch: 150, loss is 3.5648679065704347 and perplexity is 35.33478560298028
At time: 104.50084447860718 and batch: 200, loss is 3.455521149635315 and perplexity is 31.674791687770444
At time: 105.14573550224304 and batch: 250, loss is 3.595515513420105 and perplexity is 36.43447760956745
At time: 105.79923701286316 and batch: 300, loss is 3.5847871780395506 and perplexity is 36.04568558901654
At time: 106.44813871383667 and batch: 350, loss is 3.534951434135437 and perplexity is 34.29334918763325
At time: 107.09310865402222 and batch: 400, loss is 3.4898262214660645 and perplexity is 32.78025070732821
At time: 107.73819375038147 and batch: 450, loss is 3.4930233097076417 and perplexity is 32.88521976968741
At time: 108.3822774887085 and batch: 500, loss is 3.369968056678772 and perplexity is 29.077598207901783
At time: 109.02634286880493 and batch: 550, loss is 3.432563457489014 and perplexity is 30.955895247574517
At time: 109.67062473297119 and batch: 600, loss is 3.4475736522674563 and perplexity is 31.424054052324365
At time: 110.31636881828308 and batch: 650, loss is 3.2932831621170044 and perplexity is 26.9311379590961
At time: 110.96177959442139 and batch: 700, loss is 3.276146011352539 and perplexity is 26.473547088296062
At time: 111.60634303092957 and batch: 750, loss is 3.373741583824158 and perplexity is 29.18753060004632
At time: 112.2507815361023 and batch: 800, loss is 3.3104214668273926 and perplexity is 27.396669826463963
At time: 112.89711809158325 and batch: 850, loss is 3.3735147523880005 and perplexity is 29.18091070139137
At time: 113.54211950302124 and batch: 900, loss is 3.33636926651001 and perplexity is 28.116856347573513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.421011728783176 and perplexity of 83.1803988048384
finished 9 epochs...
Completing Train Step...
At time: 115.17893290519714 and batch: 50, loss is 3.6594462156295777 and perplexity is 38.839828024879395
At time: 115.8321921825409 and batch: 100, loss is 3.5244219398498533 and perplexity is 33.93415196456673
At time: 116.47920751571655 and batch: 150, loss is 3.5298957681655883 and perplexity is 34.12041099637212
At time: 117.12585783004761 and batch: 200, loss is 3.422539782524109 and perplexity is 30.647153364548483
At time: 117.77321910858154 and batch: 250, loss is 3.5630563402175905 and perplexity is 35.27083223966756
At time: 118.42034363746643 and batch: 300, loss is 3.5542611932754515 and perplexity is 34.96198027746796
At time: 119.10300159454346 and batch: 350, loss is 3.5057842445373537 and perplexity is 33.30755488031747
At time: 119.765789270401 and batch: 400, loss is 3.4627912759780886 and perplexity is 31.905910538869136
At time: 120.42052674293518 and batch: 450, loss is 3.4689010047912596 and perplexity is 32.10144371889993
At time: 121.06821846961975 and batch: 500, loss is 3.347808017730713 and perplexity is 28.440324581761125
At time: 121.71609926223755 and batch: 550, loss is 3.4126486587524414 and perplexity is 30.34551281858715
At time: 122.36487913131714 and batch: 600, loss is 3.4313935661315917 and perplexity is 30.91970138883243
At time: 123.01203489303589 and batch: 650, loss is 3.2804998540878296 and perplexity is 26.589060029081473
At time: 123.66890454292297 and batch: 700, loss is 3.266218752861023 and perplexity is 26.212037527321915
At time: 124.31709456443787 and batch: 750, loss is 3.3674476671218874 and perplexity is 29.004403611234384
At time: 124.96460962295532 and batch: 800, loss is 3.3073976278305053 and perplexity is 27.31395183383215
At time: 125.6121187210083 and batch: 850, loss is 3.3743927478790283 and perplexity is 29.20654266013679
At time: 126.2656466960907 and batch: 900, loss is 3.340801076889038 and perplexity is 28.241741452507412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423416764768835 and perplexity of 83.38069141619019
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 127.9086537361145 and batch: 50, loss is 3.6449230480194093 and perplexity is 38.27982702970941
At time: 128.57518434524536 and batch: 100, loss is 3.511559081077576 and perplexity is 33.50045701837697
At time: 129.2289969921112 and batch: 150, loss is 3.5177752447128294 and perplexity is 33.709349924656074
At time: 129.89077258110046 and batch: 200, loss is 3.409609489440918 and perplexity is 30.253427669345356
At time: 130.5468270778656 and batch: 250, loss is 3.5496364212036133 and perplexity is 34.80066240423105
At time: 131.20012545585632 and batch: 300, loss is 3.539938597679138 and perplexity is 34.46480290699914
At time: 131.85217189788818 and batch: 350, loss is 3.4913751077651978 and perplexity is 32.83106292954994
At time: 132.51364755630493 and batch: 400, loss is 3.446822471618652 and perplexity is 31.40045777465643
At time: 133.16588687896729 and batch: 450, loss is 3.451770181655884 and perplexity is 31.55620310841912
At time: 133.81569027900696 and batch: 500, loss is 3.328715615272522 and perplexity is 27.902481158139793
At time: 134.46622037887573 and batch: 550, loss is 3.3906361627578736 and perplexity is 29.68483064860402
At time: 135.11849665641785 and batch: 600, loss is 3.408737931251526 and perplexity is 30.227071533824205
At time: 135.78282070159912 and batch: 650, loss is 3.256354660987854 and perplexity is 25.95475061799639
At time: 136.43389129638672 and batch: 700, loss is 3.2399314260482788 and perplexity is 25.531970859182504
At time: 137.09697103500366 and batch: 750, loss is 3.337866349220276 and perplexity is 28.158981131353524
At time: 137.75499486923218 and batch: 800, loss is 3.2764466619491577 and perplexity is 26.481507572624984
At time: 138.41066145896912 and batch: 850, loss is 3.340744347572327 and perplexity is 28.240139363255217
At time: 139.07559823989868 and batch: 900, loss is 3.3069974756240845 and perplexity is 27.303024282227526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.422944787430437 and perplexity of 83.34134690497093
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 140.78367233276367 and batch: 50, loss is 3.637658395767212 and perplexity is 38.00274506869665
At time: 141.45644783973694 and batch: 100, loss is 3.5040176916122436 and perplexity is 33.24876726282246
At time: 142.11294269561768 and batch: 150, loss is 3.511495623588562 and perplexity is 33.49833123094305
At time: 142.76773047447205 and batch: 200, loss is 3.403115835189819 and perplexity is 30.057608848109915
At time: 143.4220631122589 and batch: 250, loss is 3.543782629966736 and perplexity is 34.59754168478104
At time: 144.07737159729004 and batch: 300, loss is 3.5333084154129026 and perplexity is 34.23705083514798
At time: 144.7416615486145 and batch: 350, loss is 3.4850629281997683 and perplexity is 32.624480045062604
At time: 145.40077948570251 and batch: 400, loss is 3.440246572494507 and perplexity is 31.194648962511298
At time: 146.05521774291992 and batch: 450, loss is 3.445558090209961 and perplexity is 31.360780708380084
At time: 146.7111041545868 and batch: 500, loss is 3.3224613904953 and perplexity is 27.728517340597534
At time: 147.37726855278015 and batch: 550, loss is 3.383683862686157 and perplexity is 29.47916853908641
At time: 148.04274940490723 and batch: 600, loss is 3.4020244121551513 and perplexity is 30.024821177310365
At time: 148.7009539604187 and batch: 650, loss is 3.249318828582764 and perplexity is 25.772778256730284
At time: 149.35668087005615 and batch: 700, loss is 3.2326397466659547 and perplexity is 25.346477016362872
At time: 150.01387882232666 and batch: 750, loss is 3.329972653388977 and perplexity is 27.937577694719884
At time: 150.67143630981445 and batch: 800, loss is 3.268201012611389 and perplexity is 26.264048126515426
At time: 151.3295452594757 and batch: 850, loss is 3.331933689117432 and perplexity is 27.992418037106326
At time: 151.989727973938 and batch: 900, loss is 3.297839198112488 and perplexity is 27.054117129071724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423092358732877 and perplexity of 83.35364660359977
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 153.6925585269928 and batch: 50, loss is 3.6356099128723143 and perplexity is 37.924976776167576
At time: 154.34902501106262 and batch: 100, loss is 3.5021640634536744 and perplexity is 33.18719349665008
At time: 155.0085723400116 and batch: 150, loss is 3.509620442390442 and perplexity is 33.43557464842146
At time: 155.6579830646515 and batch: 200, loss is 3.4012577629089353 and perplexity is 30.001811492092923
At time: 156.31014490127563 and batch: 250, loss is 3.5420463705062866 and perplexity is 34.53752349437555
At time: 156.95636105537415 and batch: 300, loss is 3.531472282409668 and perplexity is 34.174244733999
At time: 157.60369396209717 and batch: 350, loss is 3.4832072925567625 and perplexity is 32.56399703162973
At time: 158.24811840057373 and batch: 400, loss is 3.4383662366867065 and perplexity is 31.13604765938585
At time: 158.90209984779358 and batch: 450, loss is 3.443847951889038 and perplexity is 31.30719526783383
At time: 159.55357456207275 and batch: 500, loss is 3.3207505130767823 and perplexity is 27.681117805389675
At time: 160.20127367973328 and batch: 550, loss is 3.381768832206726 and perplexity is 29.422769053429143
At time: 160.8498957157135 and batch: 600, loss is 3.4002196073532103 and perplexity is 29.970681106697807
At time: 161.4959418773651 and batch: 650, loss is 3.2475057792663575 and perplexity is 25.72609327260767
At time: 162.14176869392395 and batch: 700, loss is 3.2306803321838378 and perplexity is 25.29686138689196
At time: 162.78875350952148 and batch: 750, loss is 3.327990436553955 and perplexity is 27.882254207572156
At time: 163.43430185317993 and batch: 800, loss is 3.2660662078857423 and perplexity is 26.208039317666397
At time: 164.0814347267151 and batch: 850, loss is 3.329667816162109 and perplexity is 27.929062578938105
At time: 164.72828483581543 and batch: 900, loss is 3.2954891204833983 and perplexity is 26.99061250325944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42311033484054 and perplexity of 83.35514499119276
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 166.38767313957214 and batch: 50, loss is 3.63508273601532 and perplexity is 37.90498887515147
At time: 167.03516578674316 and batch: 100, loss is 3.5017144870758057 and perplexity is 33.17227667178352
At time: 167.6882200241089 and batch: 150, loss is 3.5091467475891114 and perplexity is 33.41974014118896
At time: 168.33667373657227 and batch: 200, loss is 3.4007987499237062 and perplexity is 29.988043431138774
At time: 168.99657702445984 and batch: 250, loss is 3.5415734910964964 and perplexity is 34.521195271594365
At time: 169.65137839317322 and batch: 300, loss is 3.531002540588379 and perplexity is 34.1581954318458
At time: 170.3035171031952 and batch: 350, loss is 3.4827087831497194 and perplexity is 32.547767618372234
At time: 170.95741987228394 and batch: 400, loss is 3.4378957033157347 and perplexity is 31.121400556167607
At time: 171.60425066947937 and batch: 450, loss is 3.443411011695862 and perplexity is 31.29351888398418
At time: 172.2521631717682 and batch: 500, loss is 3.3203146123886107 and perplexity is 27.669054216543305
At time: 172.89956307411194 and batch: 550, loss is 3.3812846517562867 and perplexity is 29.408526572099436
At time: 173.547434091568 and batch: 600, loss is 3.3997552824020385 and perplexity is 29.956768201960728
At time: 174.19512104988098 and batch: 650, loss is 3.2470432329177856 and perplexity is 25.714196513714707
At time: 174.84212064743042 and batch: 700, loss is 3.23017626285553 and perplexity is 25.28411322821219
At time: 175.48665046691895 and batch: 750, loss is 3.3274874830245973 and perplexity is 27.868234255399763
At time: 176.1455581188202 and batch: 800, loss is 3.265531258583069 and perplexity is 26.1940230946276
At time: 176.79301381111145 and batch: 850, loss is 3.3290985345840456 and perplexity is 27.913167602907123
At time: 177.4450421333313 and batch: 900, loss is 3.294891805648804 and perplexity is 26.974495423980922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423111588987585 and perplexity of 83.35524953086716
Annealing...
Model not improving. Stopping early with 83.1803988048384 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f692cfc5f60>
ELAPSED
738.6604266166687


RESULTS SO FAR:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.65137783998269, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.47897693259486, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.24010280535543527, 'tie_weights': True, 'dropout': 0.7302462644798332, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.1803988048384, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.04256940005503396, 'tie_weights': True, 'dropout': 0.5685987769311317, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.7628700018886103, 'tie_weights': True, 'dropout': 0.6120487313382243, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8714025020599365 and batch: 50, loss is 6.722602128982544 and perplexity is 830.9770099776375
At time: 1.5387675762176514 and batch: 100, loss is 5.939554233551025 and perplexity is 379.7656050361549
At time: 2.2021188735961914 and batch: 150, loss is 5.838358573913574 and perplexity is 343.2155151527686
At time: 2.855215549468994 and batch: 200, loss is 5.676101865768433 and perplexity is 291.8096965816566
At time: 3.5090749263763428 and batch: 250, loss is 5.7385803318023685 and perplexity is 310.62311605001514
At time: 4.165050268173218 and batch: 300, loss is 5.6491797733306885 and perplexity is 284.05837798729226
At time: 4.822315454483032 and batch: 350, loss is 5.645918588638306 and perplexity is 283.13352003922887
At time: 5.479407548904419 and batch: 400, loss is 5.501901454925537 and perplexity is 245.15764557017647
At time: 6.135083913803101 and batch: 450, loss is 5.508872089385986 and perplexity is 246.87251983969998
At time: 6.789132833480835 and batch: 500, loss is 5.459145135879517 and perplexity is 234.89653389159386
At time: 7.442539691925049 and batch: 550, loss is 5.515486440658569 and perplexity is 248.5108336239335
At time: 8.102777004241943 and batch: 600, loss is 5.447355375289917 and perplexity is 232.14342115350848
At time: 8.767239809036255 and batch: 650, loss is 5.350587558746338 and perplexity is 210.73207897491275
At time: 9.433093309402466 and batch: 700, loss is 5.449767541885376 and perplexity is 232.7040656715149
At time: 10.086530685424805 and batch: 750, loss is 5.423396244049072 and perplexity is 226.64756730204624
At time: 10.740884065628052 and batch: 800, loss is 5.394364528656006 and perplexity is 220.1621958672023
At time: 11.395163536071777 and batch: 850, loss is 5.433158702850342 and perplexity is 228.8710404637162
At time: 12.049421548843384 and batch: 900, loss is 5.349721441268921 and perplexity is 210.5496392567792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.1729364264501285 and perplexity of 176.43215763038364
finished 1 epochs...
Completing Train Step...
At time: 13.673327922821045 and batch: 50, loss is 5.105140037536621 and perplexity is 164.86715650561888
At time: 14.34143614768982 and batch: 100, loss is 4.948473510742187 and perplexity is 140.95962625271213
At time: 14.986490488052368 and batch: 150, loss is 4.906178970336914 and perplexity is 135.12212110843367
At time: 15.642486572265625 and batch: 200, loss is 4.784706563949585 and perplexity is 119.66624354156812
At time: 16.294813871383667 and batch: 250, loss is 4.879302930831909 and perplexity is 131.53894018105748
At time: 16.949449062347412 and batch: 300, loss is 4.831088590621948 and perplexity is 125.34733834857072
At time: 17.599188804626465 and batch: 350, loss is 4.7991264533996585 and perplexity is 121.40431885441592
At time: 18.251622200012207 and batch: 400, loss is 4.688144598007202 and perplexity is 108.65140064929065
At time: 18.897810220718384 and batch: 450, loss is 4.698470115661621 and perplexity is 109.77909459756144
At time: 19.54479479789734 and batch: 500, loss is 4.596958637237549 and perplexity is 99.18220739215529
At time: 20.19000792503357 and batch: 550, loss is 4.677187929153442 and perplexity is 107.46744120204409
At time: 20.83544421195984 and batch: 600, loss is 4.638653993606567 and perplexity is 103.40506998472702
At time: 21.488471508026123 and batch: 650, loss is 4.494370927810669 and perplexity is 89.51184185975916
At time: 22.14583468437195 and batch: 700, loss is 4.535108623504638 and perplexity is 93.23364211936602
At time: 22.796243906021118 and batch: 750, loss is 4.598545379638672 and perplexity is 99.33970893019995
At time: 23.450026988983154 and batch: 800, loss is 4.530134134292602 and perplexity is 92.77100402057901
At time: 24.097657918930054 and batch: 850, loss is 4.590099229812622 and perplexity is 98.50420423072707
At time: 24.754968643188477 and batch: 900, loss is 4.538152990341186 and perplexity is 93.51791201885702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.655530119595462 and perplexity of 105.16495522794807
finished 2 epochs...
Completing Train Step...
At time: 26.397396564483643 and batch: 50, loss is 4.535936317443848 and perplexity is 93.31084298481548
At time: 27.043609380722046 and batch: 100, loss is 4.391666326522827 and perplexity is 80.77490426651747
At time: 27.690752029418945 and batch: 150, loss is 4.3878115367889405 and perplexity is 80.46413335787177
At time: 28.336519479751587 and batch: 200, loss is 4.285339984893799 and perplexity is 72.62723468542033
At time: 28.98332381248474 and batch: 250, loss is 4.432762594223022 and perplexity is 84.16360593210962
At time: 29.630237340927124 and batch: 300, loss is 4.4061676788330075 and perplexity is 81.95478385968198
At time: 30.278695821762085 and batch: 350, loss is 4.378025007247925 and perplexity is 79.68050947328445
At time: 30.925661087036133 and batch: 400, loss is 4.309363741874694 and perplexity is 74.39314063887352
At time: 31.5731840133667 and batch: 450, loss is 4.324901132583618 and perplexity is 75.55804226200804
At time: 32.220354318618774 and batch: 500, loss is 4.214463949203491 and perplexity is 67.65788808363484
At time: 32.86752891540527 and batch: 550, loss is 4.305226006507874 and perplexity is 74.08595746940509
At time: 33.51562738418579 and batch: 600, loss is 4.305481381416321 and perplexity is 74.10487958002439
At time: 34.163511753082275 and batch: 650, loss is 4.156160655021668 and perplexity is 63.82600154300027
At time: 34.80873513221741 and batch: 700, loss is 4.17429190158844 and perplexity is 64.99380135561397
At time: 35.45392680168152 and batch: 750, loss is 4.278086485862732 and perplexity is 72.1023390752703
At time: 36.101160764694214 and batch: 800, loss is 4.220477533340454 and perplexity is 68.065980304783
At time: 36.74819374084473 and batch: 850, loss is 4.2859108257293705 and perplexity is 72.6687051121338
At time: 37.39567255973816 and batch: 900, loss is 4.2495880031585695 and perplexity is 70.07653508731214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.53904159754923 and perplexity of 93.60104964243334
finished 3 epochs...
Completing Train Step...
At time: 39.02619028091431 and batch: 50, loss is 4.280362830162049 and perplexity is 72.26665577352743
At time: 39.676676988601685 and batch: 100, loss is 4.143488998413086 and perplexity is 63.02232309134773
At time: 40.327905893325806 and batch: 150, loss is 4.142878336906433 and perplexity is 62.98384953293216
At time: 40.9800124168396 and batch: 200, loss is 4.041370325088501 and perplexity is 56.90426674707545
At time: 41.635122299194336 and batch: 250, loss is 4.195118098258972 and perplexity is 66.36156830021301
At time: 42.285638093948364 and batch: 300, loss is 4.173155899047852 and perplexity is 64.92001015358366
At time: 42.96745038032532 and batch: 350, loss is 4.151146464347839 and perplexity is 63.50676682204839
At time: 43.63154149055481 and batch: 400, loss is 4.092863883972168 and perplexity is 59.91122504479593
At time: 44.28483176231384 and batch: 450, loss is 4.112025375366211 and perplexity is 61.070282640624455
At time: 44.93691325187683 and batch: 500, loss is 4.000747518539429 and perplexity is 54.63897842059962
At time: 45.589730978012085 and batch: 550, loss is 4.092531080245972 and perplexity is 59.89128968332534
At time: 46.24030351638794 and batch: 600, loss is 4.103346090316773 and perplexity is 60.542529821078645
At time: 46.89014768600464 and batch: 650, loss is 3.952117838859558 and perplexity is 52.045474126495336
At time: 47.542811155319214 and batch: 700, loss is 3.963033514022827 and perplexity is 52.61669758880338
At time: 48.19188070297241 and batch: 750, loss is 4.078065314292908 and perplexity is 59.03115257438148
At time: 48.84798884391785 and batch: 800, loss is 4.022987442016602 and perplexity is 55.86775845930679
At time: 49.50499486923218 and batch: 850, loss is 4.0929757022857665 and perplexity is 59.91792459150509
At time: 50.16537308692932 and batch: 900, loss is 4.0623546314239505 and perplexity is 58.1109800529938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.494077081549658 and perplexity of 89.48554300381188
finished 4 epochs...
Completing Train Step...
At time: 51.79746699333191 and batch: 50, loss is 4.105438222885132 and perplexity is 60.66932540982967
At time: 52.45720624923706 and batch: 100, loss is 3.9708305644989013 and perplexity is 53.028556190398355
At time: 53.123496770858765 and batch: 150, loss is 3.9712534761428833 and perplexity is 53.05098732713496
At time: 53.78252172470093 and batch: 200, loss is 3.8696132373809813 and perplexity is 47.92384734329306
At time: 54.44232225418091 and batch: 250, loss is 4.023576679229737 and perplexity is 55.900687522167324
At time: 55.107698917388916 and batch: 300, loss is 4.005924282073974 and perplexity is 54.92256488837846
At time: 55.77527070045471 and batch: 350, loss is 3.9885050106048583 and perplexity is 53.97413825303947
At time: 56.43517303466797 and batch: 400, loss is 3.9318825817108154 and perplexity is 51.00290448101381
At time: 57.10195755958557 and batch: 450, loss is 3.9530969858169556 and perplexity is 52.09645925102397
At time: 57.7615225315094 and batch: 500, loss is 3.846099781990051 and perplexity is 46.81013699800102
At time: 58.42094898223877 and batch: 550, loss is 3.928969392776489 and perplexity is 50.85453959645815
At time: 59.08232498168945 and batch: 600, loss is 3.9497972440719606 and perplexity is 51.92483769878922
At time: 59.7616286277771 and batch: 650, loss is 3.7984838485717773 and perplexity is 44.63346208027755
At time: 60.4144446849823 and batch: 700, loss is 3.8057832145690917 and perplexity is 44.960450006157224
At time: 61.06925296783447 and batch: 750, loss is 3.9256024312973024 and perplexity is 50.6836022518074
At time: 61.72255802154541 and batch: 800, loss is 3.874132905006409 and perplexity is 48.14093742256941
At time: 62.37606620788574 and batch: 850, loss is 3.9454723358154298 and perplexity is 51.700752462839866
At time: 63.028754234313965 and batch: 900, loss is 3.916536865234375 and perplexity is 50.226203130849015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.483423520440924 and perplexity of 88.5372635486315
finished 5 epochs...
Completing Train Step...
At time: 64.68761873245239 and batch: 50, loss is 3.965232639312744 and perplexity is 52.7325356236346
At time: 65.35212969779968 and batch: 100, loss is 3.8335111856460573 and perplexity is 46.22455662916219
At time: 66.01020073890686 and batch: 150, loss is 3.8399216508865357 and perplexity is 46.521829352909876
At time: 66.66437554359436 and batch: 200, loss is 3.7365955448150636 and perplexity is 41.95491313197004
At time: 67.32095623016357 and batch: 250, loss is 3.88862428188324 and perplexity is 48.84364519419709
At time: 67.97278022766113 and batch: 300, loss is 3.8745493602752688 and perplexity is 48.160990144848
At time: 68.62339282035828 and batch: 350, loss is 3.856949219703674 and perplexity is 47.32076567187835
At time: 69.2732903957367 and batch: 400, loss is 3.8022099208831786 and perplexity is 44.8000798095794
At time: 69.92240452766418 and batch: 450, loss is 3.8244865274429323 and perplexity is 45.80927252209989
At time: 70.57077503204346 and batch: 500, loss is 3.7187573099136353 and perplexity is 41.213147103799564
At time: 71.22013759613037 and batch: 550, loss is 3.798199706077576 and perplexity is 44.62078161865153
At time: 71.86406874656677 and batch: 600, loss is 3.8241301822662352 and perplexity is 45.792951516917086
At time: 72.51095867156982 and batch: 650, loss is 3.6707731342315673 and perplexity is 39.282264586552394
At time: 73.15761685371399 and batch: 700, loss is 3.678839864730835 and perplexity is 39.60042556251449
At time: 73.80365204811096 and batch: 750, loss is 3.8001907062530518 and perplexity is 44.70971010162009
At time: 74.45084023475647 and batch: 800, loss is 3.7505114126205443 and perplexity is 42.54283337952773
At time: 75.09797310829163 and batch: 850, loss is 3.8243024730682373 and perplexity is 45.800841900960755
At time: 75.74975299835205 and batch: 900, loss is 3.7941506004333494 and perplexity is 44.44047265132921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.490407865341395 and perplexity of 89.15780284147611
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 77.39073848724365 and batch: 50, loss is 3.8599089860916136 and perplexity is 47.461031558296916
At time: 78.04985761642456 and batch: 100, loss is 3.7280082654953004 and perplexity is 41.59617706195832
At time: 78.70375108718872 and batch: 150, loss is 3.729441499710083 and perplexity is 41.655836869153575
At time: 79.35150098800659 and batch: 200, loss is 3.6074614095687867 and perplexity is 36.87233015891831
At time: 79.99792718887329 and batch: 250, loss is 3.7506596183776857 and perplexity is 42.54913893960829
At time: 80.64491963386536 and batch: 300, loss is 3.7267878580093385 and perplexity is 41.54544374004427
At time: 81.29186820983887 and batch: 350, loss is 3.6898967456817626 and perplexity is 40.04071236737603
At time: 81.93590784072876 and batch: 400, loss is 3.63190354347229 and perplexity is 37.784672972225366
At time: 82.58155107498169 and batch: 450, loss is 3.6376888942718506 and perplexity is 38.00390411326788
At time: 83.22387290000916 and batch: 500, loss is 3.5211523151397706 and perplexity is 33.823381210866785
At time: 83.86739301681519 and batch: 550, loss is 3.5800350856781007 and perplexity is 35.87479951664976
At time: 84.51267695426941 and batch: 600, loss is 3.595157332420349 and perplexity is 36.42142980882833
At time: 85.15735936164856 and batch: 650, loss is 3.4271065902709963 and perplexity is 30.78743309342707
At time: 85.80211663246155 and batch: 700, loss is 3.414183540344238 and perplexity is 30.39212535081381
At time: 86.45189142227173 and batch: 750, loss is 3.5191716146469116 and perplexity is 33.75645352676146
At time: 87.10078239440918 and batch: 800, loss is 3.4501906967163087 and perplexity is 31.506399902919682
At time: 87.74463486671448 and batch: 850, loss is 3.508928394317627 and perplexity is 33.412443628234705
At time: 88.38885569572449 and batch: 900, loss is 3.464288802146912 and perplexity is 31.95372626855857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429915754762415 and perplexity of 83.92434638594354
finished 7 epochs...
Completing Train Step...
At time: 90.0159502029419 and batch: 50, loss is 3.7478718900680543 and perplexity is 42.43068868071654
At time: 90.66441988945007 and batch: 100, loss is 3.6158318662643434 and perplexity is 37.182263734955356
At time: 91.3191020488739 and batch: 150, loss is 3.6219097661972044 and perplexity is 37.4089419792494
At time: 91.9707133769989 and batch: 200, loss is 3.504223098754883 and perplexity is 33.25559749856779
At time: 92.61456727981567 and batch: 250, loss is 3.650689373016357 and perplexity is 38.501198589977804
At time: 93.27279210090637 and batch: 300, loss is 3.6350392484664917 and perplexity is 37.90334051593874
At time: 93.91769647598267 and batch: 350, loss is 3.602824258804321 and perplexity is 36.7017434288067
At time: 94.56227493286133 and batch: 400, loss is 3.5494133329391477 and perplexity is 34.79289965077483
At time: 95.20659732818604 and batch: 450, loss is 3.560575833320618 and perplexity is 35.18345111657839
At time: 95.85001063346863 and batch: 500, loss is 3.450228600502014 and perplexity is 31.507594137382807
At time: 96.49519968032837 and batch: 550, loss is 3.5128590774536135 and perplexity is 33.544035811095455
At time: 97.14159202575684 and batch: 600, loss is 3.535406002998352 and perplexity is 34.30894141998219
At time: 97.79028725624084 and batch: 650, loss is 3.37387282371521 and perplexity is 29.19136141975503
At time: 98.4471743106842 and batch: 700, loss is 3.3675538969039915 and perplexity is 29.007484906369825
At time: 99.09639096260071 and batch: 750, loss is 3.480709962844849 and perplexity is 32.48277545504478
At time: 99.74057793617249 and batch: 800, loss is 3.418475661277771 and perplexity is 30.522852375655543
At time: 100.38413310050964 and batch: 850, loss is 3.486973915100098 and perplexity is 32.68688460721806
At time: 101.03980541229248 and batch: 900, loss is 3.4505314731597903 and perplexity is 31.51713837143014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436051042112585 and perplexity of 84.44082913161606
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 102.68667507171631 and batch: 50, loss is 3.702349991798401 and perplexity is 40.542466965457756
At time: 103.333242893219 and batch: 100, loss is 3.576062650680542 and perplexity is 35.7325718895533
At time: 103.98286628723145 and batch: 150, loss is 3.582739791870117 and perplexity is 35.97196164731608
At time: 104.62894988059998 and batch: 200, loss is 3.462581181526184 and perplexity is 31.899207988190966
At time: 105.27608299255371 and batch: 250, loss is 3.608416895866394 and perplexity is 36.90757800187986
At time: 105.92521286010742 and batch: 300, loss is 3.5900465393066407 and perplexity is 36.235762274487286
At time: 106.57552337646484 and batch: 350, loss is 3.551407642364502 and perplexity is 34.86235669488184
At time: 107.22211027145386 and batch: 400, loss is 3.4990619802474976 and perplexity is 33.084403574917225
At time: 107.87846255302429 and batch: 450, loss is 3.5011686754226683 and perplexity is 33.154175796898556
At time: 108.53064775466919 and batch: 500, loss is 3.386834831237793 and perplexity is 29.572202969388353
At time: 109.17694211006165 and batch: 550, loss is 3.441214418411255 and perplexity is 31.224855191263718
At time: 109.84296536445618 and batch: 600, loss is 3.4630314111709595 and perplexity is 31.91357319084949
At time: 110.48950004577637 and batch: 650, loss is 3.2954497861862184 and perplexity is 26.989550867365658
At time: 111.13541412353516 and batch: 700, loss is 3.281084065437317 and perplexity is 26.60459819806806
At time: 111.79024314880371 and batch: 750, loss is 3.388032302856445 and perplexity is 29.607636053967713
At time: 112.44852018356323 and batch: 800, loss is 3.3196991205215456 and perplexity is 27.652029378565757
At time: 113.0985496044159 and batch: 850, loss is 3.381799650192261 and perplexity is 29.42367581787249
At time: 113.74790406227112 and batch: 900, loss is 3.344553174972534 and perplexity is 28.347906282362135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4239568840967465 and perplexity of 83.42573910366686
finished 9 epochs...
Completing Train Step...
At time: 115.40426206588745 and batch: 50, loss is 3.672189612388611 and perplexity is 39.33794648308348
At time: 116.05364775657654 and batch: 100, loss is 3.5421153116226196 and perplexity is 34.53990463187883
At time: 116.70286893844604 and batch: 150, loss is 3.548113589286804 and perplexity is 34.74770717598532
At time: 117.34959816932678 and batch: 200, loss is 3.4295452308654784 and perplexity is 30.86260419796879
At time: 118.00754165649414 and batch: 250, loss is 3.5757254695892335 and perplexity is 35.72052557297753
At time: 118.66018295288086 and batch: 300, loss is 3.5596051168441774 and perplexity is 35.149314532034566
At time: 119.32108306884766 and batch: 350, loss is 3.522460446357727 and perplexity is 33.86765558375667
At time: 119.97779107093811 and batch: 400, loss is 3.471937909126282 and perplexity is 32.19908091486158
At time: 120.62751030921936 and batch: 450, loss is 3.477363076210022 and perplexity is 32.37424101555839
At time: 121.27888131141663 and batch: 500, loss is 3.365415735244751 and perplexity is 28.9455284743498
At time: 121.937659740448 and batch: 550, loss is 3.4219838047027586 and perplexity is 30.63011896280385
At time: 122.60307931900024 and batch: 600, loss is 3.4471769094467164 and perplexity is 31.411589257303035
At time: 123.25968980789185 and batch: 650, loss is 3.282539420127869 and perplexity is 26.64334551355045
At time: 123.91582775115967 and batch: 700, loss is 3.2710645246505736 and perplexity is 26.33936332605022
At time: 124.57631635665894 and batch: 750, loss is 3.3817007541656494 and perplexity is 29.420766077129066
At time: 125.23014616966248 and batch: 800, loss is 3.316589274406433 and perplexity is 27.566169397289457
At time: 125.87701272964478 and batch: 850, loss is 3.3829189491271974 and perplexity is 29.456628145189477
At time: 126.5466160774231 and batch: 900, loss is 3.348806390762329 and perplexity is 28.468732813480514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426151641427654 and perplexity of 83.609039432442
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 128.20040917396545 and batch: 50, loss is 3.6578425359725952 and perplexity is 38.77759130001829
At time: 128.85542178153992 and batch: 100, loss is 3.529077453613281 and perplexity is 34.09250118858148
At time: 129.50986862182617 and batch: 150, loss is 3.5357157087326048 and perplexity is 34.31956874146236
At time: 130.16788983345032 and batch: 200, loss is 3.41652792930603 and perplexity is 30.463459899303523
At time: 130.81852960586548 and batch: 250, loss is 3.562637434005737 and perplexity is 35.256060163218436
At time: 131.48281455039978 and batch: 300, loss is 3.5449341106414796 and perplexity is 34.637403030805146
At time: 132.14640522003174 and batch: 350, loss is 3.5067328071594237 and perplexity is 33.33916417123533
At time: 132.81012153625488 and batch: 400, loss is 3.456483826637268 and perplexity is 31.705298963240114
At time: 133.47142839431763 and batch: 450, loss is 3.459520001411438 and perplexity is 31.801708075556103
At time: 134.127947807312 and batch: 500, loss is 3.3454200220108032 and perplexity is 28.37249023468785
At time: 134.787926197052 and batch: 550, loss is 3.4000927877426146 and perplexity is 29.966880477592813
At time: 135.44985461235046 and batch: 600, loss is 3.4243513488769532 and perplexity is 30.70272303526976
At time: 136.10288453102112 and batch: 650, loss is 3.259078402519226 and perplexity is 26.02554101379677
At time: 136.75415897369385 and batch: 700, loss is 3.2441544246673586 and perplexity is 25.640020322653815
At time: 137.40608644485474 and batch: 750, loss is 3.35287383556366 and perplexity is 28.584763627259527
At time: 138.05674195289612 and batch: 800, loss is 3.28596031665802 and perplexity is 26.734645717007027
At time: 138.7082908153534 and batch: 850, loss is 3.3491775798797607 and perplexity is 28.479302058761014
At time: 139.3608431816101 and batch: 900, loss is 3.3146573209762575 and perplexity is 27.5129642532446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4256830084813785 and perplexity of 83.56986666149996
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 140.9981460571289 and batch: 50, loss is 3.650869197845459 and perplexity is 38.50812268397781
At time: 141.66402649879456 and batch: 100, loss is 3.521668667793274 and perplexity is 33.840850513277665
At time: 142.3160011768341 and batch: 150, loss is 3.5289710474014284 and perplexity is 34.08887372767303
At time: 142.9819929599762 and batch: 200, loss is 3.4098451805114744 and perplexity is 30.26055897245996
At time: 143.6351580619812 and batch: 250, loss is 3.556334261894226 and perplexity is 35.03453404011164
At time: 144.28847074508667 and batch: 300, loss is 3.5383775758743288 and perplexity is 34.411044568048986
At time: 144.94254112243652 and batch: 350, loss is 3.500223240852356 and perplexity is 33.12284550565388
At time: 145.59492540359497 and batch: 400, loss is 3.449759159088135 and perplexity is 31.492806639046655
At time: 146.248761177063 and batch: 450, loss is 3.453294038772583 and perplexity is 31.604326910688652
At time: 146.90191650390625 and batch: 500, loss is 3.3390123224258423 and perplexity is 28.19126906624671
At time: 147.56527543067932 and batch: 550, loss is 3.3933374738693236 and perplexity is 29.765127015383076
At time: 148.21892070770264 and batch: 600, loss is 3.417866883277893 and perplexity is 30.504276389535264
At time: 148.8857421875 and batch: 650, loss is 3.25228750705719 and perplexity is 25.849403029649345
At time: 149.5525243282318 and batch: 700, loss is 3.2369244861602784 and perplexity is 25.455313068031373
At time: 150.20633149147034 and batch: 750, loss is 3.3451086139678954 and perplexity is 28.363656188599336
At time: 150.86190104484558 and batch: 800, loss is 3.277937664985657 and perplexity is 26.521021030849777
At time: 151.5167098045349 and batch: 850, loss is 3.340235776901245 and perplexity is 28.22578090808187
At time: 152.1689488887787 and batch: 900, loss is 3.305528931617737 and perplexity is 27.262958016300686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425675901648116 and perplexity of 83.56927274650226
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 153.79222679138184 and batch: 50, loss is 3.6490484142303465 and perplexity is 38.438071518506355
At time: 154.45058369636536 and batch: 100, loss is 3.519689130783081 and perplexity is 33.7739275573178
At time: 155.09691429138184 and batch: 150, loss is 3.527067766189575 and perplexity is 34.024054718741965
At time: 155.74412202835083 and batch: 200, loss is 3.407960171699524 and perplexity is 30.203571280167733
At time: 156.39125561714172 and batch: 250, loss is 3.5546104669570924 and perplexity is 34.974193709828
At time: 157.03777599334717 and batch: 300, loss is 3.536558303833008 and perplexity is 34.348498428226435
At time: 157.68372917175293 and batch: 350, loss is 3.4984676742553713 and perplexity is 33.0647471571687
At time: 158.33161807060242 and batch: 400, loss is 3.44780779838562 and perplexity is 31.431412734067443
At time: 158.9788737297058 and batch: 450, loss is 3.4515730237960813 and perplexity is 31.549982168224545
At time: 159.64853525161743 and batch: 500, loss is 3.3372752904891967 and perplexity is 28.142342437404608
At time: 160.29576444625854 and batch: 550, loss is 3.3914581108093262 and perplexity is 29.709240067576275
At time: 160.9437735080719 and batch: 600, loss is 3.4161423015594483 and perplexity is 30.45171460870999
At time: 161.5908224582672 and batch: 650, loss is 3.2504988527297973 and perplexity is 25.803208708212047
At time: 162.23886919021606 and batch: 700, loss is 3.234998779296875 and perplexity is 25.406340765229853
At time: 162.88627362251282 and batch: 750, loss is 3.3431565666198733 and perplexity is 28.30834299330845
At time: 163.53350687026978 and batch: 800, loss is 3.2758615493774412 and perplexity is 26.466017441803306
At time: 164.1779179573059 and batch: 850, loss is 3.337973961830139 and perplexity is 28.162011555857177
At time: 164.82434916496277 and batch: 900, loss is 3.303186445236206 and perplexity is 27.19916964928628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42566921286387 and perplexity of 83.56871377153666
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 166.45934915542603 and batch: 50, loss is 3.6485899686813354 and perplexity is 38.42045379439829
At time: 167.10642743110657 and batch: 100, loss is 3.5192138147354126 and perplexity is 33.75787808214574
At time: 167.75345611572266 and batch: 150, loss is 3.5265940189361573 and perplexity is 34.00793973378516
At time: 168.40069246292114 and batch: 200, loss is 3.4074841642379763 and perplexity is 30.18919757613957
At time: 169.04872608184814 and batch: 250, loss is 3.5541607666015627 and perplexity is 34.95846933837504
At time: 169.7006492614746 and batch: 300, loss is 3.536086263656616 and perplexity is 34.33228838316684
At time: 170.34863805770874 and batch: 350, loss is 3.4979996871948242 and perplexity is 33.04927690356049
At time: 170.99562907218933 and batch: 400, loss is 3.447324819564819 and perplexity is 31.416235692798818
At time: 171.64966225624084 and batch: 450, loss is 3.4511298894882203 and perplexity is 31.53600438595957
At time: 172.29734897613525 and batch: 500, loss is 3.336830973625183 and perplexity is 28.12984109755055
At time: 172.94364762306213 and batch: 550, loss is 3.3909770154953005 and perplexity is 29.69495052898714
At time: 173.5910291671753 and batch: 600, loss is 3.415699453353882 and perplexity is 30.43823210711027
At time: 174.23697590827942 and batch: 650, loss is 3.2500391674041746 and perplexity is 25.791350077642953
At time: 174.88468050956726 and batch: 700, loss is 3.234505772590637 and perplexity is 25.39381835592115
At time: 175.5452709197998 and batch: 750, loss is 3.3426598262786866 and perplexity is 28.294284589328797
At time: 176.20647764205933 and batch: 800, loss is 3.2753408479690553 and perplexity is 26.452240136489422
At time: 176.85216188430786 and batch: 850, loss is 3.3374079847335816 and perplexity is 28.14607701204236
At time: 177.49870204925537 and batch: 900, loss is 3.3025925016403197 and perplexity is 27.18301967322186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425666286520762 and perplexity of 83.56846922116495
Annealing...
Model not improving. Stopping early with 83.42573910366686 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f692cfc5f60>
ELAPSED
922.9982843399048


RESULTS SO FAR:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.65137783998269, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.47897693259486, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.24010280535543527, 'tie_weights': True, 'dropout': 0.7302462644798332, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.1803988048384, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.04256940005503396, 'tie_weights': True, 'dropout': 0.5685987769311317, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.42573910366686, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.7628700018886103, 'tie_weights': True, 'dropout': 0.6120487313382243, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.040540033485182286, 'tie_weights': True, 'dropout': 0.5745371205662028, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8833661079406738 and batch: 50, loss is 6.695193986892701 and perplexity is 808.5107594889662
At time: 1.5456085205078125 and batch: 100, loss is 5.902341279983521 and perplexity is 365.89312405657046
At time: 2.1973185539245605 and batch: 150, loss is 5.789819145202637 and perplexity is 326.95388784943594
At time: 2.848355531692505 and batch: 200, loss is 5.622798824310303 and perplexity is 276.6626307347206
At time: 3.4987123012542725 and batch: 250, loss is 5.680286388397217 and perplexity is 293.03333925500783
At time: 4.1470091342926025 and batch: 300, loss is 5.605535106658936 and perplexity is 271.9273966702297
At time: 4.797651290893555 and batch: 350, loss is 5.585562601089477 and perplexity is 266.55020199718604
At time: 5.462116003036499 and batch: 400, loss is 5.448357706069946 and perplexity is 232.37622230229974
At time: 6.124185085296631 and batch: 450, loss is 5.445967063903809 and perplexity is 231.82135741281905
At time: 6.789806604385376 and batch: 500, loss is 5.397381315231323 and perplexity is 220.82738108051018
At time: 7.444409132003784 and batch: 550, loss is 5.457925081253052 and perplexity is 234.61012204325692
At time: 8.099141359329224 and batch: 600, loss is 5.38936294555664 and perplexity is 219.06378553431554
At time: 8.755253791809082 and batch: 650, loss is 5.285266408920288 and perplexity is 197.4067673671855
At time: 9.409974098205566 and batch: 700, loss is 5.389101753234863 and perplexity is 219.00657522732624
At time: 10.072413444519043 and batch: 750, loss is 5.360093650817871 and perplexity is 212.74486924764187
At time: 10.737488508224487 and batch: 800, loss is 5.330535860061645 and perplexity is 206.54862566608023
At time: 11.389363527297974 and batch: 850, loss is 5.372643394470215 and perplexity is 215.43158636339703
At time: 12.04326343536377 and batch: 900, loss is 5.290799741744995 and perplexity is 198.50211237236286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.126707730227953 and perplexity of 168.46158308345872
finished 1 epochs...
Completing Train Step...
At time: 13.666398286819458 and batch: 50, loss is 5.068797998428344 and perplexity is 158.98311449375822
At time: 14.312346696853638 and batch: 100, loss is 4.909817409515381 and perplexity is 135.61465020297788
At time: 14.958725214004517 and batch: 150, loss is 4.872662343978882 and perplexity is 130.66833827727876
At time: 15.616838216781616 and batch: 200, loss is 4.752821216583252 and perplexity is 115.9108332431949
At time: 16.277249574661255 and batch: 250, loss is 4.855474700927735 and perplexity is 128.44164811494682
At time: 16.934123516082764 and batch: 300, loss is 4.807823858261108 and perplexity is 122.46482651450697
At time: 17.590511322021484 and batch: 350, loss is 4.781814594268798 and perplexity is 119.32067232502109
At time: 18.245895385742188 and batch: 400, loss is 4.6691670322418215 and perplexity is 106.60890365818811
At time: 18.892447471618652 and batch: 450, loss is 4.673825740814209 and perplexity is 107.10672216686086
At time: 19.534756660461426 and batch: 500, loss is 4.575636949539184 and perplexity is 97.08986078829564
At time: 20.176518440246582 and batch: 550, loss is 4.656391735076904 and perplexity is 105.2556060289363
At time: 20.81878900527954 and batch: 600, loss is 4.620262479782104 and perplexity is 101.52067575753772
At time: 21.461856842041016 and batch: 650, loss is 4.477792634963989 and perplexity is 88.04012134611992
At time: 22.106300115585327 and batch: 700, loss is 4.514739198684692 and perplexity is 91.35373774033872
At time: 22.749335765838623 and batch: 750, loss is 4.580117797851562 and perplexity is 97.52588186978569
At time: 23.391967058181763 and batch: 800, loss is 4.5136361503601075 and perplexity is 91.25302570831026
At time: 24.035327672958374 and batch: 850, loss is 4.578776531219482 and perplexity is 97.39516134380652
At time: 24.678317070007324 and batch: 900, loss is 4.5238249969482425 and perplexity is 92.18754152255951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.650722555918236 and perplexity of 104.66058138521186
finished 2 epochs...
Completing Train Step...
At time: 26.281543970108032 and batch: 50, loss is 4.524874696731567 and perplexity is 92.2843615720232
At time: 26.9252028465271 and batch: 100, loss is 4.3755555343627925 and perplexity is 79.48398337347683
At time: 27.577515602111816 and batch: 150, loss is 4.372776041030884 and perplexity is 79.26336491749417
At time: 28.2327561378479 and batch: 200, loss is 4.267103424072266 and perplexity is 71.31476752038795
At time: 28.877795696258545 and batch: 250, loss is 4.414225120544433 and perplexity is 82.617797262498
At time: 29.521859884262085 and batch: 300, loss is 4.391185755729675 and perplexity is 80.73609553262625
At time: 30.16849970817566 and batch: 350, loss is 4.3725425910949705 and perplexity is 79.24486304974788
At time: 30.812740087509155 and batch: 400, loss is 4.2971329545974735 and perplexity is 73.48879565991477
At time: 31.456575393676758 and batch: 450, loss is 4.310866594314575 and perplexity is 74.50502660478298
At time: 32.11418557167053 and batch: 500, loss is 4.199023265838623 and perplexity is 66.62122802264727
At time: 32.76004886627197 and batch: 550, loss is 4.2898709583282475 and perplexity is 72.95705339203941
At time: 33.403703451156616 and batch: 600, loss is 4.28814474105835 and perplexity is 72.83122230365838
At time: 34.048457622528076 and batch: 650, loss is 4.141350336074829 and perplexity is 62.887683647954916
At time: 34.7038094997406 and batch: 700, loss is 4.161613397598266 and perplexity is 64.17497887622865
At time: 35.35166645050049 and batch: 750, loss is 4.262558336257935 and perplexity is 70.99137112877047
At time: 35.99709939956665 and batch: 800, loss is 4.2075521802902225 and perplexity is 67.19186477401178
At time: 36.642908334732056 and batch: 850, loss is 4.27855299949646 and perplexity is 72.13598364668803
At time: 37.288625717163086 and batch: 900, loss is 4.238854222297668 and perplexity is 69.32837141189722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.538210934155608 and perplexity of 93.52333096039175
finished 3 epochs...
Completing Train Step...
At time: 38.89996314048767 and batch: 50, loss is 4.2704265546798705 and perplexity is 71.55215001517112
At time: 39.54979968070984 and batch: 100, loss is 4.12700846195221 and perplexity is 61.99219325977609
At time: 40.20003128051758 and batch: 150, loss is 4.131951661109924 and perplexity is 62.2993916631891
At time: 40.84995913505554 and batch: 200, loss is 4.024724102020263 and perplexity is 55.964866057952335
At time: 41.49981069564819 and batch: 250, loss is 4.177069997787475 and perplexity is 65.17461142571688
At time: 42.149242877960205 and batch: 300, loss is 4.165801234245301 and perplexity is 64.4442967405928
At time: 42.79882597923279 and batch: 350, loss is 4.146307854652405 and perplexity is 63.20022458157332
At time: 43.44791913032532 and batch: 400, loss is 4.082864346504212 and perplexity is 59.31512583044925
At time: 44.097017765045166 and batch: 450, loss is 4.09821855545044 and perplexity is 60.23289041037192
At time: 44.7466824054718 and batch: 500, loss is 3.986192545890808 and perplexity is 53.84946916484732
At time: 45.39708471298218 and batch: 550, loss is 4.079205617904663 and perplexity is 59.09850440433753
At time: 46.04714775085449 and batch: 600, loss is 4.087687554359436 and perplexity is 59.60190605565063
At time: 46.697200298309326 and batch: 650, loss is 3.9398559856414797 and perplexity is 51.41119681683339
At time: 47.349392890930176 and batch: 700, loss is 3.952796263694763 and perplexity is 52.08079504864343
At time: 48.00357723236084 and batch: 750, loss is 4.060099110603333 and perplexity is 57.98005723265581
At time: 48.67392086982727 and batch: 800, loss is 4.014073033332824 and perplexity is 55.37194365942486
At time: 49.323649644851685 and batch: 850, loss is 4.084889602661133 and perplexity is 59.43537588171217
At time: 49.974329233169556 and batch: 900, loss is 4.051201539039612 and perplexity is 57.466463779701286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499301858144264 and perplexity of 89.95430850559063
finished 4 epochs...
Completing Train Step...
At time: 51.65600657463074 and batch: 50, loss is 4.095190367698669 and perplexity is 60.05076979625331
At time: 52.317065715789795 and batch: 100, loss is 3.9558654594421387 and perplexity is 52.2408867540459
At time: 52.98262166976929 and batch: 150, loss is 3.9596908378601072 and perplexity is 52.44111063666507
At time: 53.63295531272888 and batch: 200, loss is 3.8586995220184326 and perplexity is 47.403663844838384
At time: 54.288382053375244 and batch: 250, loss is 4.0073530912399296 and perplexity is 55.00109484130715
At time: 54.942424297332764 and batch: 300, loss is 4.006423354148865 and perplexity is 54.949982047795864
At time: 55.595454454422 and batch: 350, loss is 3.979951710700989 and perplexity is 53.514449989819184
At time: 56.25261330604553 and batch: 400, loss is 3.9244967126846313 and perplexity is 50.627591421255914
At time: 56.91454601287842 and batch: 450, loss is 3.9409661531448363 and perplexity is 51.4683035500026
At time: 57.57417106628418 and batch: 500, loss is 3.832162628173828 and perplexity is 46.16226217118859
At time: 58.22992563247681 and batch: 550, loss is 3.9217776966094973 and perplexity is 50.49012116295098
At time: 58.88651895523071 and batch: 600, loss is 3.935493745803833 and perplexity is 51.187417290820335
At time: 59.5502667427063 and batch: 650, loss is 3.7866381454467772 and perplexity is 44.10786650956063
At time: 60.20807075500488 and batch: 700, loss is 3.800174422264099 and perplexity is 44.708982055122476
At time: 60.877310276031494 and batch: 750, loss is 3.9086248207092287 and perplexity is 49.83037912880735
At time: 61.53341293334961 and batch: 800, loss is 3.8628577613830566 and perplexity is 47.60119002180977
At time: 62.187339067459106 and batch: 850, loss is 3.9358046197891237 and perplexity is 51.20333260093003
At time: 62.8412766456604 and batch: 900, loss is 3.9056908226013185 and perplexity is 49.68439115966582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.488884076680223 and perplexity of 89.02204864906842
finished 5 epochs...
Completing Train Step...
At time: 64.49018621444702 and batch: 50, loss is 3.9555809926986694 and perplexity is 52.226028072615584
At time: 65.15319657325745 and batch: 100, loss is 3.8215509176254274 and perplexity is 45.67499156663076
At time: 65.80161261558533 and batch: 150, loss is 3.8240214204788208 and perplexity is 45.78797126449461
At time: 66.4529538154602 and batch: 200, loss is 3.725260705947876 and perplexity is 41.48204595134547
At time: 67.10776209831238 and batch: 250, loss is 3.8702710103988647 and perplexity is 47.95538072675609
At time: 67.75616502761841 and batch: 300, loss is 3.876553859710693 and perplexity is 48.25762564294659
At time: 68.40750765800476 and batch: 350, loss is 3.8478637599945067 and perplexity is 46.89278192053386
At time: 69.05716753005981 and batch: 400, loss is 3.797505645751953 and perplexity is 44.58982284930129
At time: 69.70163774490356 and batch: 450, loss is 3.813959813117981 and perplexity is 45.3295806162696
At time: 70.34676742553711 and batch: 500, loss is 3.704529504776001 and perplexity is 40.630926162319525
At time: 71.00389361381531 and batch: 550, loss is 3.7907740449905396 and perplexity is 44.29066998235253
At time: 71.65056538581848 and batch: 600, loss is 3.809449415206909 and perplexity is 45.12558656355535
At time: 72.29662227630615 and batch: 650, loss is 3.663442778587341 and perplexity is 38.99536444136428
At time: 72.94322752952576 and batch: 700, loss is 3.6778582191467284 and perplexity is 39.561571053431045
At time: 73.59411954879761 and batch: 750, loss is 3.7834739112854003 and perplexity is 43.968519471161464
At time: 74.24218678474426 and batch: 800, loss is 3.7387098121643065 and perplexity is 42.04371087294942
At time: 74.88899111747742 and batch: 850, loss is 3.8146785640716554 and perplexity is 45.36217300707008
At time: 75.53671360015869 and batch: 900, loss is 3.784930410385132 and perplexity is 44.03260624001889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498321951252141 and perplexity of 89.86620483245797
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 77.16692471504211 and batch: 50, loss is 3.8525489234924315 and perplexity is 47.112997741441795
At time: 77.82577729225159 and batch: 100, loss is 3.711457567214966 and perplexity is 40.91339711408357
At time: 78.47275257110596 and batch: 150, loss is 3.7098375511169435 and perplexity is 40.84717041078078
At time: 79.1204080581665 and batch: 200, loss is 3.5930959463119505 and perplexity is 36.34642850924959
At time: 79.7677218914032 and batch: 250, loss is 3.7369636487960816 and perplexity is 41.970359745322696
At time: 80.4139449596405 and batch: 300, loss is 3.727781434059143 and perplexity is 41.58674281140946
At time: 81.0603768825531 and batch: 350, loss is 3.6830270528793334 and perplexity is 39.76658762825129
At time: 81.72171425819397 and batch: 400, loss is 3.6264643478393555 and perplexity is 37.57971265893321
At time: 82.3688600063324 and batch: 450, loss is 3.6252234983444214 and perplexity is 37.53311081038016
At time: 83.01665806770325 and batch: 500, loss is 3.504579906463623 and perplexity is 33.267465469279834
At time: 83.66395044326782 and batch: 550, loss is 3.5705062532424927 and perplexity is 35.534578094157396
At time: 84.31138181686401 and batch: 600, loss is 3.577487006187439 and perplexity is 35.78350403923283
At time: 84.95893430709839 and batch: 650, loss is 3.416489014625549 and perplexity is 30.4622744465611
At time: 85.60591864585876 and batch: 700, loss is 3.4172141313552857 and perplexity is 30.48437116176887
At time: 86.25258898735046 and batch: 750, loss is 3.503498020172119 and perplexity is 33.231493316833564
At time: 86.90009951591492 and batch: 800, loss is 3.4383351373672486 and perplexity is 31.135079364549767
At time: 87.54684281349182 and batch: 850, loss is 3.4964674711227417 and perplexity is 32.99867704517566
At time: 88.19370937347412 and batch: 900, loss is 3.452563271522522 and perplexity is 31.581239940250455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430836716743364 and perplexity of 84.00167312026673
finished 7 epochs...
Completing Train Step...
At time: 89.8658561706543 and batch: 50, loss is 3.7407501888275148 and perplexity is 42.12958345586474
At time: 90.52138137817383 and batch: 100, loss is 3.5985584926605223 and perplexity is 36.54551582638124
At time: 91.17031002044678 and batch: 150, loss is 3.6025868225097657 and perplexity is 36.693030137310004
At time: 91.8220522403717 and batch: 200, loss is 3.4922738599777223 and perplexity is 32.86058318370718
At time: 92.466632604599 and batch: 250, loss is 3.63597149848938 and perplexity is 37.938692381830826
At time: 93.1092300415039 and batch: 300, loss is 3.636297330856323 and perplexity is 37.95105604990056
At time: 93.75947284698486 and batch: 350, loss is 3.5941688203811646 and perplexity is 36.385444575826284
At time: 94.41128778457642 and batch: 400, loss is 3.5437058782577515 and perplexity is 34.5948863662315
At time: 95.0651798248291 and batch: 450, loss is 3.5490102577209472 and perplexity is 34.77887832117135
At time: 95.72159957885742 and batch: 500, loss is 3.432887315750122 and perplexity is 30.96592219354745
At time: 96.36977982521057 and batch: 550, loss is 3.5041512393951417 and perplexity is 33.253207858483755
At time: 97.01899337768555 and batch: 600, loss is 3.5185788965225218 and perplexity is 33.73645139334011
At time: 97.66558861732483 and batch: 650, loss is 3.364110884666443 and perplexity is 28.907783515901535
At time: 98.33899593353271 and batch: 700, loss is 3.369595084190369 and perplexity is 29.066755085960178
At time: 98.98853492736816 and batch: 750, loss is 3.463693881034851 and perplexity is 31.934721975780406
At time: 99.63576316833496 and batch: 800, loss is 3.4053632640838623 and perplexity is 30.125237153158892
At time: 100.28586673736572 and batch: 850, loss is 3.4732286882400514 and perplexity is 32.24066965115086
At time: 100.94261360168457 and batch: 900, loss is 3.439113335609436 and perplexity is 31.159318058612953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436505879441353 and perplexity of 84.47924470853413
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 102.59477472305298 and batch: 50, loss is 3.69658522605896 and perplexity is 40.309421512514646
At time: 103.24273920059204 and batch: 100, loss is 3.558952031135559 and perplexity is 35.12636651137378
At time: 103.88832759857178 and batch: 150, loss is 3.565758533477783 and perplexity is 35.36626973202878
At time: 104.53410267829895 and batch: 200, loss is 3.450134768486023 and perplexity is 31.50463785500501
At time: 105.17944979667664 and batch: 250, loss is 3.5934821557998657 and perplexity is 36.36046855581678
At time: 105.82488584518433 and batch: 300, loss is 3.5916071224212645 and perplexity is 36.292355340844594
At time: 106.47086882591248 and batch: 350, loss is 3.5431649923324584 and perplexity is 34.576179538694824
At time: 107.11752462387085 and batch: 400, loss is 3.493064579963684 and perplexity is 32.88657697913332
At time: 107.76222348213196 and batch: 450, loss is 3.490888686180115 and perplexity is 32.81509707526267
At time: 108.41735243797302 and batch: 500, loss is 3.3707375383377074 and perplexity is 29.099981497069464
At time: 109.07190370559692 and batch: 550, loss is 3.4349303674697875 and perplexity is 31.02925184498416
At time: 109.72035360336304 and batch: 600, loss is 3.447265348434448 and perplexity is 31.414367389305777
At time: 110.37360215187073 and batch: 650, loss is 3.2861152362823485 and perplexity is 26.738787759111577
At time: 111.03127527236938 and batch: 700, loss is 3.285794153213501 and perplexity is 26.730203765241917
At time: 111.67824196815491 and batch: 750, loss is 3.372890796661377 and perplexity is 29.162708784236898
At time: 112.32388138771057 and batch: 800, loss is 3.307621245384216 and perplexity is 27.320060395888806
At time: 112.97524833679199 and batch: 850, loss is 3.368044352531433 and perplexity is 29.021715279978235
At time: 113.62386798858643 and batch: 900, loss is 3.331875891685486 and perplexity is 27.99080019398387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424581031276755 and perplexity of 83.47782529650286
finished 9 epochs...
Completing Train Step...
At time: 115.26226472854614 and batch: 50, loss is 3.665778594017029 and perplexity is 39.086556878200824
At time: 115.91189360618591 and batch: 100, loss is 3.526006164550781 and perplexity is 33.987953892232895
At time: 116.57071137428284 and batch: 150, loss is 3.531462526321411 and perplexity is 34.17391132867763
At time: 117.22296643257141 and batch: 200, loss is 3.4179335021972657 and perplexity is 30.506308619156293
At time: 117.88302898406982 and batch: 250, loss is 3.5605975914001466 and perplexity is 35.18421664923411
At time: 118.54570317268372 and batch: 300, loss is 3.56138521194458 and perplexity is 35.21193937716873
At time: 119.19589686393738 and batch: 350, loss is 3.5140462255477907 and perplexity is 33.583881195786276
At time: 119.84362173080444 and batch: 400, loss is 3.465492525100708 and perplexity is 31.992212861277796
At time: 120.49120616912842 and batch: 450, loss is 3.4667856025695802 and perplexity is 32.03360802871935
At time: 121.1393690109253 and batch: 500, loss is 3.3488515710830686 and perplexity is 28.470019069016583
At time: 121.7861180305481 and batch: 550, loss is 3.415620069503784 and perplexity is 30.43581589896066
At time: 122.43373036384583 and batch: 600, loss is 3.4304818677902222 and perplexity is 30.891524794589394
At time: 123.08179807662964 and batch: 650, loss is 3.271479802131653 and perplexity is 26.350303742002456
At time: 123.74001240730286 and batch: 700, loss is 3.2750702714920044 and perplexity is 26.445083750761665
At time: 124.39097881317139 and batch: 750, loss is 3.366724662780762 and perplexity is 28.983440880497533
At time: 125.03885412216187 and batch: 800, loss is 3.304730625152588 and perplexity is 27.241202505595002
At time: 125.68662095069885 and batch: 850, loss is 3.3687815999984743 and perplexity is 29.0431193551382
At time: 126.3351616859436 and batch: 900, loss is 3.3361002159118653 and perplexity is 28.109292508128643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.426486080639983 and perplexity of 83.63700625007294
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 127.98126602172852 and batch: 50, loss is 3.6512744283676146 and perplexity is 38.52373051281101
At time: 128.63230872154236 and batch: 100, loss is 3.513601064682007 and perplexity is 33.56893429329164
At time: 129.2839469909668 and batch: 150, loss is 3.5200515365600586 and perplexity is 33.786169641942905
At time: 129.9343638420105 and batch: 200, loss is 3.4048967599868774 and perplexity is 30.111186884113145
At time: 130.58631324768066 and batch: 250, loss is 3.5470378732681276 and perplexity is 34.71034860797136
At time: 131.24084091186523 and batch: 300, loss is 3.5472232055664064 and perplexity is 34.716782152806346
At time: 131.9082703590393 and batch: 350, loss is 3.4985991382598876 and perplexity is 33.06909426697627
At time: 132.55998015403748 and batch: 400, loss is 3.4506839179992674 and perplexity is 31.521943362768713
At time: 133.21250677108765 and batch: 450, loss is 3.450293154716492 and perplexity is 31.509628151023804
At time: 133.86484956741333 and batch: 500, loss is 3.3301760864257814 and perplexity is 27.943261699128822
At time: 134.51798033714294 and batch: 550, loss is 3.393885278701782 and perplexity is 29.78143696272696
At time: 135.1703381538391 and batch: 600, loss is 3.409141125679016 and perplexity is 30.23926137789945
At time: 135.82328701019287 and batch: 650, loss is 3.2488356494903563 and perplexity is 25.760328397121523
At time: 136.47491216659546 and batch: 700, loss is 3.248506016731262 and perplexity is 25.751838348372775
At time: 137.1293613910675 and batch: 750, loss is 3.3371961164474486 and perplexity is 28.140114382612794
At time: 137.79113221168518 and batch: 800, loss is 3.2738457107543946 and perplexity is 26.412719959256137
At time: 138.44148015975952 and batch: 850, loss is 3.335036931037903 and perplexity is 28.079420206784455
At time: 139.09312176704407 and batch: 900, loss is 3.3022079944610594 and perplexity is 27.172569616193258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424451854130993 and perplexity of 83.46704256575279
finished 11 epochs...
Completing Train Step...
At time: 140.71630692481995 and batch: 50, loss is 3.6426576519012452 and perplexity is 38.193206210431164
At time: 141.3834159374237 and batch: 100, loss is 3.5047670936584474 and perplexity is 33.27369329568644
At time: 142.03712272644043 and batch: 150, loss is 3.510626583099365 and perplexity is 33.469232470614635
At time: 142.69037342071533 and batch: 200, loss is 3.3961347484588624 and perplexity is 29.848504809762886
At time: 143.3452787399292 and batch: 250, loss is 3.538040528297424 and perplexity is 34.39944836320471
At time: 144.00226974487305 and batch: 300, loss is 3.538813438415527 and perplexity is 34.42604632249362
At time: 144.67285776138306 and batch: 350, loss is 3.4906417751312255 and perplexity is 32.80699566542937
At time: 145.3295702934265 and batch: 400, loss is 3.44317063331604 and perplexity is 31.28599750264087
At time: 145.98434114456177 and batch: 450, loss is 3.443627161979675 and perplexity is 31.300283718056537
At time: 146.6399962902069 and batch: 500, loss is 3.3242303943634033 and perplexity is 27.77761260708485
At time: 147.2943103313446 and batch: 550, loss is 3.389042253494263 and perplexity is 29.63755340986805
At time: 147.9488172531128 and batch: 600, loss is 3.40544554233551 and perplexity is 30.127715906974696
At time: 148.62561106681824 and batch: 650, loss is 3.246128673553467 and perplexity is 25.690690105138632
At time: 149.27966499328613 and batch: 700, loss is 3.2470493078231812 and perplexity is 25.714352725500337
At time: 149.9334237575531 and batch: 750, loss is 3.336588544845581 and perplexity is 28.12302244105555
At time: 150.58736896514893 and batch: 800, loss is 3.2744254779815676 and perplexity is 26.428037628581286
At time: 151.24202346801758 and batch: 850, loss is 3.3368661212921142 and perplexity is 28.130829813211694
At time: 151.89618849754333 and batch: 900, loss is 3.305082573890686 and perplexity is 27.250791699794537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424670493766053 and perplexity of 83.48529376462392
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 153.51152157783508 and batch: 50, loss is 3.6378307914733887 and perplexity is 38.00929714352798
At time: 154.16830039024353 and batch: 100, loss is 3.5005462694168092 and perplexity is 33.133546859216565
At time: 154.8135769367218 and batch: 150, loss is 3.506673913002014 and perplexity is 33.3372007470704
At time: 155.4569606781006 and batch: 200, loss is 3.391923327445984 and perplexity is 29.723064515750476
At time: 156.10141801834106 and batch: 250, loss is 3.5337499713897706 and perplexity is 34.25217174769355
At time: 156.74547791481018 and batch: 300, loss is 3.5340217256546023 and perplexity is 34.26148118632826
At time: 157.38974332809448 and batch: 350, loss is 3.485887928009033 and perplexity is 32.6514063404442
At time: 158.03406739234924 and batch: 400, loss is 3.4384037160873415 and perplexity is 31.13721464165903
At time: 158.67707920074463 and batch: 450, loss is 3.438280658721924 and perplexity is 31.133383213806326
At time: 159.32162642478943 and batch: 500, loss is 3.3186022090911864 and perplexity is 27.621714181050756
At time: 159.96653246879578 and batch: 550, loss is 3.3823279571533202 and perplexity is 29.43922465754549
At time: 160.61865329742432 and batch: 600, loss is 3.399008746147156 and perplexity is 29.934412734040755
At time: 161.26982188224792 and batch: 650, loss is 3.2387439584732056 and perplexity is 25.501670465587463
At time: 161.9153401851654 and batch: 700, loss is 3.2390010261535647 and perplexity is 25.508226963555085
At time: 162.5626084804535 and batch: 750, loss is 3.32830237865448 and perplexity is 27.89095321323955
At time: 163.20912337303162 and batch: 800, loss is 3.265627236366272 and perplexity is 26.196537259547455
At time: 163.8572256565094 and batch: 850, loss is 3.3275516700744627 and perplexity is 27.870023092551055
At time: 164.50288248062134 and batch: 900, loss is 3.2951935386657714 and perplexity is 26.982635747906862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424421336552868 and perplexity of 83.46449539262741
finished 13 epochs...
Completing Train Step...
At time: 166.15211629867554 and batch: 50, loss is 3.635811324119568 and perplexity is 37.932616062335384
At time: 166.80016922950745 and batch: 100, loss is 3.4984703493118285 and perplexity is 33.0648356073524
At time: 167.44854354858398 and batch: 150, loss is 3.504395704269409 and perplexity is 33.26133809349982
At time: 168.09673357009888 and batch: 200, loss is 3.3898304224014284 and perplexity is 29.66092201594479
At time: 168.7427008152008 and batch: 250, loss is 3.5316189098358155 and perplexity is 34.17925598292916
At time: 169.38831210136414 and batch: 300, loss is 3.5320456409454346 and perplexity is 34.19384444721511
At time: 170.03569054603577 and batch: 350, loss is 3.4839789485931396 and perplexity is 32.589134934167504
At time: 170.68526148796082 and batch: 400, loss is 3.4366733694076537 and perplexity is 31.083383052776075
At time: 171.3323483467102 and batch: 450, loss is 3.4367849445343017 and perplexity is 31.08685137866269
At time: 171.979430437088 and batch: 500, loss is 3.3172102689743044 and perplexity is 27.58329315507218
At time: 172.6259183883667 and batch: 550, loss is 3.3812176513671877 and perplexity is 29.40655625538301
At time: 173.2729721069336 and batch: 600, loss is 3.39817138671875 and perplexity is 29.90935736294373
At time: 173.9238510131836 and batch: 650, loss is 3.23830575466156 and perplexity is 25.490497984476793
At time: 174.58263421058655 and batch: 700, loss is 3.23882981300354 and perplexity is 25.50385999351713
At time: 175.2363350391388 and batch: 750, loss is 3.328239092826843 and perplexity is 27.88918816703366
At time: 175.8845570087433 and batch: 800, loss is 3.265843696594238 and perplexity is 26.202208381738632
At time: 176.54416728019714 and batch: 850, loss is 3.328107919692993 and perplexity is 27.885530094746905
At time: 177.19057536125183 and batch: 900, loss is 3.2960451221466065 and perplexity is 27.005623501385518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424426353141053 and perplexity of 83.4649141006791
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 178.82623624801636 and batch: 50, loss is 3.634392490386963 and perplexity is 37.87883414991828
At time: 179.4751489162445 and batch: 100, loss is 3.497185969352722 and perplexity is 33.02239505587832
At time: 180.12335729599 and batch: 150, loss is 3.503180966377258 and perplexity is 33.22095881585857
At time: 180.78106808662415 and batch: 200, loss is 3.388597273826599 and perplexity is 29.624368234986292
At time: 181.43681263923645 and batch: 250, loss is 3.5304379510879516 and perplexity is 34.13891551648246
At time: 182.0821249485016 and batch: 300, loss is 3.530713219642639 and perplexity is 34.1483141799342
At time: 182.7287564277649 and batch: 350, loss is 3.4826783323287964 and perplexity is 32.54677652721889
At time: 183.37500071525574 and batch: 400, loss is 3.4352728128433228 and perplexity is 31.039879488312522
At time: 184.02117252349854 and batch: 450, loss is 3.435289225578308 and perplexity is 31.04038894180929
At time: 184.66825079917908 and batch: 500, loss is 3.315695700645447 and perplexity is 27.541547993783496
At time: 185.31487226486206 and batch: 550, loss is 3.3793754053115843 and perplexity is 29.352432013505762
At time: 185.96233916282654 and batch: 600, loss is 3.396430196762085 and perplexity is 29.857324802724477
At time: 186.60945057868958 and batch: 650, loss is 3.236318836212158 and perplexity is 25.439900726705314
At time: 187.25618433952332 and batch: 700, loss is 3.2366433048248293 and perplexity is 25.448156515300308
At time: 187.90230107307434 and batch: 750, loss is 3.326156249046326 and perplexity is 27.831159797908548
At time: 188.54948949813843 and batch: 800, loss is 3.263554906845093 and perplexity is 26.142305614359845
At time: 189.1959900856018 and batch: 850, loss is 3.325700783729553 and perplexity is 27.818486556225146
At time: 189.84246826171875 and batch: 900, loss is 3.293458285331726 and perplexity is 26.935854639539674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424383294092466 and perplexity of 83.46132025826178
finished 15 epochs...
Completing Train Step...
At time: 191.4952564239502 and batch: 50, loss is 3.633918128013611 and perplexity is 37.860870117318534
At time: 192.1403226852417 and batch: 100, loss is 3.496709747314453 and perplexity is 33.006672807533995
At time: 192.78450298309326 and batch: 150, loss is 3.5026445102691652 and perplexity is 33.203142008979825
At time: 193.42643857002258 and batch: 200, loss is 3.3880991554260254 and perplexity is 29.60961546668079
At time: 194.07056188583374 and batch: 250, loss is 3.5299148082733156 and perplexity is 34.121060658858
At time: 194.721910238266 and batch: 300, loss is 3.530233464241028 and perplexity is 34.13193527099962
At time: 195.36997890472412 and batch: 350, loss is 3.4822059202194215 and perplexity is 32.53140466707863
At time: 196.01458978652954 and batch: 400, loss is 3.4348727750778196 and perplexity is 31.027464847608652
At time: 196.659117937088 and batch: 450, loss is 3.4349298095703125 and perplexity is 31.029234533785672
At time: 197.30800223350525 and batch: 500, loss is 3.3153705644607543 and perplexity is 27.532594695547758
At time: 197.97055959701538 and batch: 550, loss is 3.379111671447754 and perplexity is 29.344691803920572
At time: 198.6145088672638 and batch: 600, loss is 3.396234655380249 and perplexity is 29.851487030956207
At time: 199.2595067024231 and batch: 650, loss is 3.2362333059310915 and perplexity is 25.437724937895094
At time: 199.90442419052124 and batch: 700, loss is 3.2366158676147463 and perplexity is 25.44745829846238
At time: 200.5513129234314 and batch: 750, loss is 3.326146183013916 and perplexity is 27.83087964996201
At time: 201.196617603302 and batch: 800, loss is 3.263605580329895 and perplexity is 26.143630369650783
At time: 201.84330296516418 and batch: 850, loss is 3.325841717720032 and perplexity is 27.822407402828386
At time: 202.48866200447083 and batch: 900, loss is 3.2936749935150145 and perplexity is 26.941692492196307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424361137494649 and perplexity of 83.4594710598416
finished 16 epochs...
Completing Train Step...
At time: 204.10969257354736 and batch: 50, loss is 3.6334708976745604 and perplexity is 37.843941373345345
At time: 204.75356125831604 and batch: 100, loss is 3.49624801158905 and perplexity is 32.99143596548994
At time: 205.39954209327698 and batch: 150, loss is 3.50213677406311 and perplexity is 33.18628785072234
At time: 206.0517258644104 and batch: 200, loss is 3.387628426551819 and perplexity is 29.595680645745077
At time: 206.7088978290558 and batch: 250, loss is 3.5294262075424196 and perplexity is 34.10439315588979
At time: 207.37455821037292 and batch: 300, loss is 3.529771018028259 and perplexity is 34.11615473590466
At time: 208.03002762794495 and batch: 350, loss is 3.481771240234375 and perplexity is 32.51726698948974
At time: 208.68412828445435 and batch: 400, loss is 3.4344777059555054 and perplexity is 31.015209275362164
At time: 209.3471279144287 and batch: 450, loss is 3.4345791864395143 and perplexity is 31.018356873518254
At time: 210.00440669059753 and batch: 500, loss is 3.3150529956817625 and perplexity is 27.523852591250982
At time: 210.65824675559998 and batch: 550, loss is 3.378859691619873 and perplexity is 29.33729846505586
At time: 211.3058750629425 and batch: 600, loss is 3.396050200462341 and perplexity is 29.845981285163045
At time: 211.9535665512085 and batch: 650, loss is 3.2361386823654175 and perplexity is 25.435318043535098
At time: 212.60170078277588 and batch: 700, loss is 3.2365799617767332 and perplexity is 25.44654460255048
At time: 213.24914646148682 and batch: 750, loss is 3.3261349058151244 and perplexity is 27.830565797369342
At time: 213.8965060710907 and batch: 800, loss is 3.263655061721802 and perplexity is 26.144924024876644
At time: 214.5631227493286 and batch: 850, loss is 3.3259734106063843 and perplexity is 27.82607165723736
At time: 215.215754032135 and batch: 900, loss is 3.2938709592819215 and perplexity is 26.94697265897638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424352358465326 and perplexity of 83.45873836991404
finished 17 epochs...
Completing Train Step...
At time: 216.835999250412 and batch: 50, loss is 3.6330418491363528 and perplexity is 37.82770796832746
At time: 217.50189352035522 and batch: 100, loss is 3.4958022308349608 and perplexity is 32.97673229583672
At time: 218.15400791168213 and batch: 150, loss is 3.5016500997543334 and perplexity is 33.17014086650162
At time: 218.80707359313965 and batch: 200, loss is 3.38717483997345 and perplexity is 29.58225948628547
At time: 219.46277165412903 and batch: 250, loss is 3.528958158493042 and perplexity is 34.08843436213404
At time: 220.11971759796143 and batch: 300, loss is 3.5293231916427614 and perplexity is 34.10088004210293
At time: 220.77176189422607 and batch: 350, loss is 3.481354875564575 and perplexity is 32.50373076655492
At time: 221.43473863601685 and batch: 400, loss is 3.434090504646301 and perplexity is 31.003202470400804
At time: 222.0927131175995 and batch: 450, loss is 3.4342358446121217 and perplexity is 31.007708802255312
At time: 222.759272813797 and batch: 500, loss is 3.314743285179138 and perplexity is 27.515329484946683
At time: 223.4152946472168 and batch: 550, loss is 3.378615174293518 and perplexity is 29.33012586422103
At time: 224.06822633743286 and batch: 600, loss is 3.395871109962463 and perplexity is 29.84063663205792
At time: 224.72031497955322 and batch: 650, loss is 3.23603636264801 and perplexity is 25.432715642121476
At time: 225.3723931312561 and batch: 700, loss is 3.2365365028381348 and perplexity is 25.445438746760885
At time: 226.02508068084717 and batch: 750, loss is 3.326119475364685 and perplexity is 27.83013636251631
At time: 226.67542219161987 and batch: 800, loss is 3.263700909614563 and perplexity is 26.146122742028698
At time: 227.32616448402405 and batch: 850, loss is 3.32609543800354 and perplexity is 27.829467407517832
At time: 227.97894859313965 and batch: 900, loss is 3.2940524578094483 and perplexity is 26.951863938701898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424356120906464 and perplexity of 83.45905237909534
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 229.61718344688416 and batch: 50, loss is 3.6326903343200683 and perplexity is 37.814413305282926
At time: 230.28727889060974 and batch: 100, loss is 3.4954890203475952 and perplexity is 32.966405254795816
At time: 230.94908022880554 and batch: 150, loss is 3.501348462104797 and perplexity is 33.16013701202129
At time: 231.61594605445862 and batch: 200, loss is 3.386867504119873 and perplexity is 29.5731691942734
At time: 232.27355241775513 and batch: 250, loss is 3.5286701822280886 and perplexity is 34.078619115475185
At time: 232.9278814792633 and batch: 300, loss is 3.5289982557296753 and perplexity is 34.08980124155697
At time: 233.58235239982605 and batch: 350, loss is 3.481022000312805 and perplexity is 32.492912879595856
At time: 234.2357633113861 and batch: 400, loss is 3.433736367225647 and perplexity is 30.99222502012361
At time: 234.89121890068054 and batch: 450, loss is 3.433850259780884 and perplexity is 30.995755004839307
At time: 235.54538798332214 and batch: 500, loss is 3.314355020523071 and perplexity is 27.504648328699552
At time: 236.19860100746155 and batch: 550, loss is 3.378150238990784 and perplexity is 29.316492422852797
At time: 236.8595371246338 and batch: 600, loss is 3.3954199886322023 and perplexity is 29.827177920348845
At time: 237.5147361755371 and batch: 650, loss is 3.2355084943771364 and perplexity is 25.419294061216853
At time: 238.16933012008667 and batch: 700, loss is 3.235966892242432 and perplexity is 25.430948882420328
At time: 238.83414959907532 and batch: 750, loss is 3.325578827857971 and perplexity is 27.81509413531812
At time: 239.49245047569275 and batch: 800, loss is 3.2631213569641115 and perplexity is 26.130974077442897
At time: 240.1559226512909 and batch: 850, loss is 3.325481767654419 and perplexity is 27.81239452763388
At time: 240.82207345962524 and batch: 900, loss is 3.2933835554122926 and perplexity is 26.933841800503174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424352358465326 and perplexity of 83.45873836991404
finished 19 epochs...
Completing Train Step...
At time: 242.4492220878601 and batch: 50, loss is 3.632579655647278 and perplexity is 37.81022828780632
At time: 243.09446620941162 and batch: 100, loss is 3.495376229286194 and perplexity is 32.96268714864505
At time: 243.73789358139038 and batch: 150, loss is 3.5012261629104615 and perplexity is 33.15608180196017
At time: 244.3820116519928 and batch: 200, loss is 3.3867524671554565 and perplexity is 29.56976738233193
At time: 245.02531361579895 and batch: 250, loss is 3.5285486602783203 and perplexity is 34.074478066854105
At time: 245.6705105304718 and batch: 300, loss is 3.5288848781585695 and perplexity is 34.08593644178767
At time: 246.31822538375854 and batch: 350, loss is 3.4809161806106568 and perplexity is 32.48947467115131
At time: 246.96211314201355 and batch: 400, loss is 3.4336394262313843 and perplexity is 30.989220748636633
At time: 247.60624837875366 and batch: 450, loss is 3.4337655210494997 and perplexity is 30.99312857516363
At time: 248.2643265724182 and batch: 500, loss is 3.314278998374939 and perplexity is 27.5025574457277
At time: 248.90777611732483 and batch: 550, loss is 3.378087868690491 and perplexity is 29.314664001437038
At time: 249.55159711837769 and batch: 600, loss is 3.395375256538391 and perplexity is 29.82584371806904
At time: 250.2046205997467 and batch: 650, loss is 3.2354864263534546 and perplexity is 25.41873311382306
At time: 250.86355590820312 and batch: 700, loss is 3.2359570455551148 and perplexity is 25.430698473051358
At time: 251.51744747161865 and batch: 750, loss is 3.3255778312683106 and perplexity is 27.815066415096712
At time: 252.17232847213745 and batch: 800, loss is 3.2631326055526735 and perplexity is 26.13126801567221
At time: 252.82486605644226 and batch: 850, loss is 3.3255125522613525 and perplexity is 27.813250734446218
At time: 253.47085404396057 and batch: 900, loss is 3.2934306144714354 and perplexity is 26.935109311581115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424349850171233 and perplexity of 83.45852903111617
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f692cfc5f60>
ELAPSED
1183.71107172966


RESULTS SO FAR:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.65137783998269, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.47897693259486, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.24010280535543527, 'tie_weights': True, 'dropout': 0.7302462644798332, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.1803988048384, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.04256940005503396, 'tie_weights': True, 'dropout': 0.5685987769311317, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.42573910366686, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.7628700018886103, 'tie_weights': True, 'dropout': 0.6120487313382243, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.45852903111617, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.040540033485182286, 'tie_weights': True, 'dropout': 0.5745371205662028, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -83.92929344488701, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.1539617413418658, 'tie_weights': True, 'dropout': 0.5432825584425187, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.65137783998269, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.13114107369323613, 'tie_weights': True, 'dropout': 0.40368688016744314, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.47897693259486, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.24010280535543527, 'tie_weights': True, 'dropout': 0.7302462644798332, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.1803988048384, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.04256940005503396, 'tie_weights': True, 'dropout': 0.5685987769311317, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.42573910366686, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.7628700018886103, 'tie_weights': True, 'dropout': 0.6120487313382243, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}, {'best_accuracy': -83.45852903111617, 'params': {'num_layers': 1, 'wordvec_dim': 300, 'data': 'ptb', 'tune_wordvecs': True, 'rnn_dropout': 0.040540033485182286, 'tie_weights': True, 'dropout': 0.5745371205662028, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'None'}}]
Exception ignored in: <bound method DropoutDescriptor.__del__ of <torch.backends.cudnn.DropoutDescriptor object at 0x7f692382d5c0>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 215, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroyDropoutDescriptor'
Exception ignored in: <bound method CuDNNHandle.__del__ of <torch.backends.cudnn.CuDNNHandle object at 0x7f692382d048>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 91, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroy'
