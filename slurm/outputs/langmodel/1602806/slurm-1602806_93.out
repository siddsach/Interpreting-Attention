Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'anneal': 7.345038311625543, 'num_layers': 1, 'seq_len': 50, 'lr': 25.145775354217726, 'wordvec_dim': 200, 'data': 'gigasmall', 'batch_size': 50, 'tune_wordvecs': True, 'wordvec_source': '', 'dropout': 0.09492371834341329}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 8576 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 10.586524963378906 and batch: 50, loss is 7.969990520477295 and perplexity is 2892.8299414433086
At time: 13.281921148300171 and batch: 100, loss is 6.639324455261231 and perplexity is 764.5783116631801
At time: 15.981642007827759 and batch: 150, loss is 6.415234060287475 and perplexity is 611.0837745244619
At time: 18.661360025405884 and batch: 200, loss is 6.234667272567749 and perplexity is 510.1308556777707
At time: 21.347217321395874 and batch: 250, loss is 6.117927284240722 and perplexity is 453.92286568779366
At time: 24.03031849861145 and batch: 300, loss is 6.142349510192871 and perplexity is 465.1451512915795
At time: 26.732956886291504 and batch: 350, loss is 6.074329652786255 and perplexity is 434.5581000452474
At time: 30.064589500427246 and batch: 400, loss is 5.996164817810058 and perplexity is 401.88453372063674
At time: 32.78669023513794 and batch: 450, loss is 6.021497859954834 and perplexity is 412.19554475974866
At time: 35.49504780769348 and batch: 500, loss is 5.947996463775635 and perplexity is 382.9852450541026
At time: 38.20872139930725 and batch: 550, loss is 5.920684080123902 and perplexity is 372.6665604853648
At time: 40.922175884246826 and batch: 600, loss is 5.927563333511353 and perplexity is 375.23906649920303
At time: 43.62808179855347 and batch: 650, loss is 5.935860939025879 and perplexity is 378.36560570255494
At time: 46.3249409198761 and batch: 700, loss is 5.9270031452178955 and perplexity is 375.0289208329689
At time: 49.013306617736816 and batch: 750, loss is 5.932637395858765 and perplexity is 377.14789157111755
At time: 51.715214252471924 and batch: 800, loss is 5.9009065914154055 and perplexity is 365.36855775894736
At time: 54.42429733276367 and batch: 850, loss is 5.908834838867188 and perplexity is 368.27680350829314
At time: 57.13728213310242 and batch: 900, loss is 5.889940700531006 and perplexity is 361.38385386624617
At time: 59.85046172142029 and batch: 950, loss is 5.986590509414673 and perplexity is 398.0551284615052
At time: 62.57010293006897 and batch: 1000, loss is 5.988322496414185 and perplexity is 398.74515215262755
At time: 65.28067827224731 and batch: 1050, loss is 5.919554424285889 and perplexity is 372.24581322422404
At time: 68.0046033859253 and batch: 1100, loss is 5.960086278915405 and perplexity is 387.6435682616389
At time: 70.74236965179443 and batch: 1150, loss is 5.883906421661377 and perplexity is 359.2097291475704
At time: 73.47377419471741 and batch: 1200, loss is 5.946038932800293 and perplexity is 382.2362728810917
At time: 76.18544340133667 and batch: 1250, loss is 5.902252912521362 and perplexity is 365.86079243832893
At time: 78.89403963088989 and batch: 1300, loss is 5.858697347640991 and perplexity is 350.267569829631
At time: 81.60282945632935 and batch: 1350, loss is 5.862593536376953 and perplexity is 351.6349404269776
At time: 84.31940460205078 and batch: 1400, loss is 5.892912950515747 and perplexity is 362.4595748843416
At time: 87.03097486495972 and batch: 1450, loss is 5.846868515014648 and perplexity is 346.14872197869533
At time: 89.73919415473938 and batch: 1500, loss is 5.869498062133789 and perplexity is 354.0712139080682
At time: 92.44960236549377 and batch: 1550, loss is 5.875922565460205 and perplexity is 356.35326828340465
At time: 95.15825891494751 and batch: 1600, loss is 5.872417631149292 and perplexity is 355.106459753964
At time: 97.8733651638031 and batch: 1650, loss is 5.850181541442871 and perplexity is 347.29742363148046
At time: 100.58313131332397 and batch: 1700, loss is 5.891943483352661 and perplexity is 362.10835250533546
At time: 103.3010401725769 and batch: 1750, loss is 5.920539455413818 and perplexity is 372.61266758931305
At time: 106.02574062347412 and batch: 1800, loss is 5.833543577194214 and perplexity is 341.56690578276726
At time: 108.73212766647339 and batch: 1850, loss is 5.85971529006958 and perplexity is 350.62430358672884
At time: 111.44100999832153 and batch: 1900, loss is 5.831287040710449 and perplexity is 340.7970165664611
At time: 114.15867137908936 and batch: 1950, loss is 5.808283405303955 and perplexity is 333.0469281090964
At time: 116.86596274375916 and batch: 2000, loss is 5.831598224639893 and perplexity is 340.90308362354494
At time: 119.58312201499939 and batch: 2050, loss is 5.79709909439087 and perplexity is 329.34278049795614
At time: 122.29977416992188 and batch: 2100, loss is 5.845322694778442 and perplexity is 345.6140516381088
At time: 125.01506304740906 and batch: 2150, loss is 5.8244006633758545 and perplexity is 338.45822186983213
At time: 127.72789740562439 and batch: 2200, loss is 5.823596820831299 and perplexity is 338.1862640718069
At time: 130.43503594398499 and batch: 2250, loss is 5.807942571640015 and perplexity is 332.9334338467564
At time: 133.14016485214233 and batch: 2300, loss is 5.834279279708863 and perplexity is 341.8182898748882
At time: 135.83612823486328 and batch: 2350, loss is 5.847473421096802 and perplexity is 346.3581727885976
At time: 138.53121519088745 and batch: 2400, loss is 5.793705463409424 and perplexity is 328.2270069671924
At time: 141.24809741973877 and batch: 2450, loss is 5.767971858978272 and perplexity is 319.8882957202128
At time: 143.95552015304565 and batch: 2500, loss is 5.783763751983643 and perplexity is 324.9800357507309
At time: 146.66042518615723 and batch: 2550, loss is 5.755987167358398 and perplexity is 316.07741487293515
At time: 149.36860728263855 and batch: 2600, loss is 5.851697301864624 and perplexity is 347.8242424855167
At time: 152.0750720500946 and batch: 2650, loss is 5.8615651512146 and perplexity is 351.2735101482987
At time: 154.79043078422546 and batch: 2700, loss is 5.847981357574463 and perplexity is 346.5341454265706
At time: 157.49957084655762 and batch: 2750, loss is 5.8825297451019285 and perplexity is 358.7155537714039
At time: 160.2112331390381 and batch: 2800, loss is 5.854162654876709 and perplexity is 348.682809930461
At time: 162.9122428894043 and batch: 2850, loss is 5.818729515075684 and perplexity is 336.5442075595596
At time: 165.6313488483429 and batch: 2900, loss is 5.811820802688598 and perplexity is 334.22713363885333
At time: 168.33997082710266 and batch: 2950, loss is 5.829119625091553 and perplexity is 340.05916769131886
At time: 171.04870295524597 and batch: 3000, loss is 5.843550462722778 and perplexity is 345.0020857698442
At time: 173.75542497634888 and batch: 3050, loss is 5.8279464817047115 and perplexity is 339.66046344196053
At time: 176.46285676956177 and batch: 3100, loss is 5.893322439193725 and perplexity is 362.6080283694178
At time: 179.17296171188354 and batch: 3150, loss is 5.79231035232544 and perplexity is 327.7694131030646
At time: 181.8811550140381 and batch: 3200, loss is 5.792905836105347 and perplexity is 327.96465259732395
At time: 184.58728909492493 and batch: 3250, loss is 5.822167015075683 and perplexity is 337.70306892488594
At time: 187.29612135887146 and batch: 3300, loss is 5.789062566757202 and perplexity is 326.7066151376351
At time: 190.00916957855225 and batch: 3350, loss is 5.792721424102783 and perplexity is 327.9041775553016
At time: 192.72689580917358 and batch: 3400, loss is 5.798303146362304 and perplexity is 329.7395651488268
At time: 195.43290066719055 and batch: 3450, loss is 5.8136445426940915 and perplexity is 334.83723319577587
At time: 198.1416518688202 and batch: 3500, loss is 5.787062206268311 and perplexity is 326.05373734642734
At time: 200.8493618965149 and batch: 3550, loss is 5.827815275192261 and perplexity is 339.6159007006599
At time: 203.56516075134277 and batch: 3600, loss is 5.821891574859619 and perplexity is 337.61006472775193
At time: 206.26892709732056 and batch: 3650, loss is 5.873804988861084 and perplexity is 355.59946134511466
At time: 208.97401809692383 and batch: 3700, loss is 5.821343746185303 and perplexity is 337.4251629054405
At time: 211.6823182106018 and batch: 3750, loss is 5.83653507232666 and perplexity is 342.5902313924625
At time: 214.39278388023376 and batch: 3800, loss is 5.7579961681365965 and perplexity is 316.7130529302709
At time: 217.10669326782227 and batch: 3850, loss is 5.753648471832276 and perplexity is 315.339069755606
At time: 219.81103110313416 and batch: 3900, loss is 5.813868732452392 and perplexity is 334.91230868942006
At time: 222.53106117248535 and batch: 3950, loss is 5.802790994644165 and perplexity is 331.2227118691614
At time: 225.25748586654663 and batch: 4000, loss is 5.7740703964233395 and perplexity is 321.8451072492595
At time: 227.9917230606079 and batch: 4050, loss is 5.7573691082000735 and perplexity is 316.51451711683717
At time: 230.7132227420807 and batch: 4100, loss is 5.778576154708862 and perplexity is 323.29853544703474
At time: 233.40866470336914 and batch: 4150, loss is 5.797960910797119 and perplexity is 329.62673585058485
At time: 236.1363866329193 and batch: 4200, loss is 5.81338828086853 and perplexity is 334.7514381885932
At time: 238.86974024772644 and batch: 4250, loss is 5.823384504318238 and perplexity is 338.11446916534567
At time: 241.5879921913147 and batch: 4300, loss is 5.819287166595459 and perplexity is 336.73193428655935
At time: 244.3053126335144 and batch: 4350, loss is 5.8145316696166995 and perplexity is 335.1344081164823
At time: 247.0174057483673 and batch: 4400, loss is 5.775952243804932 and perplexity is 322.4513408622764
At time: 249.72684597969055 and batch: 4450, loss is 5.738938598632813 and perplexity is 310.73442194667996
At time: 252.43968796730042 and batch: 4500, loss is 5.781108884811402 and perplexity is 324.11840119092574
At time: 255.15904664993286 and batch: 4550, loss is 5.811803283691407 and perplexity is 334.22127836592716
At time: 257.87676429748535 and batch: 4600, loss is 5.792170886993408 and perplexity is 327.72370382054066
At time: 260.58595418930054 and batch: 4650, loss is 5.729420557022094 and perplexity is 307.79086943843663
At time: 263.29647040367126 and batch: 4700, loss is 5.832134056091308 and perplexity is 341.0857991656303
At time: 266.01146507263184 and batch: 4750, loss is 5.845466213226318 and perplexity is 345.66365718993484
At time: 268.7230384349823 and batch: 4800, loss is 5.809162740707397 and perplexity is 333.3399168628034
At time: 271.42687940597534 and batch: 4850, loss is 5.85682970046997 and perplexity is 349.6140040988081
At time: 274.13129353523254 and batch: 4900, loss is 5.80916259765625 and perplexity is 333.3398691781494
At time: 276.84203910827637 and batch: 4950, loss is 5.839316892623901 and perplexity is 343.5445826532015
At time: 279.5539360046387 and batch: 5000, loss is 5.832298355102539 and perplexity is 341.1418438290934
At time: 282.2584421634674 and batch: 5050, loss is 5.830716705322265 and perplexity is 340.602703384835
At time: 284.96473836898804 and batch: 5100, loss is 5.804996175765991 and perplexity is 331.95392387163434
At time: 287.6700351238251 and batch: 5150, loss is 5.862231321334839 and perplexity is 351.5075960266431
At time: 290.3908772468567 and batch: 5200, loss is 5.758269023895264 and perplexity is 316.7994817013658
At time: 293.09525299072266 and batch: 5250, loss is 5.76770697593689 and perplexity is 319.80357395671456
At time: 295.8072667121887 and batch: 5300, loss is 5.770960950851441 and perplexity is 320.84590169840453
At time: 298.51284766197205 and batch: 5350, loss is 5.776275386810303 and perplexity is 322.55555559487266
At time: 301.23307609558105 and batch: 5400, loss is 5.84299446105957 and perplexity is 344.8103173530656
At time: 303.9610376358032 and batch: 5450, loss is 5.731055269241333 and perplexity is 308.2944304101763
At time: 306.68777799606323 and batch: 5500, loss is 5.727661180496216 and perplexity is 307.249825497434
At time: 309.4104118347168 and batch: 5550, loss is 5.787132396697998 and perplexity is 326.07662400155544
At time: 312.1174292564392 and batch: 5600, loss is 5.842067680358887 and perplexity is 344.4909018424744
At time: 314.82629799842834 and batch: 5650, loss is 5.814317445755005 and perplexity is 335.062622018811
At time: 317.5470814704895 and batch: 5700, loss is 5.783831148147583 and perplexity is 325.0019388965834
At time: 320.24964356422424 and batch: 5750, loss is 5.845113706588745 and perplexity is 345.54182993012796
At time: 322.95883226394653 and batch: 5800, loss is 5.823752279281616 and perplexity is 338.23884207107926
At time: 325.65793442726135 and batch: 5850, loss is 5.81349404335022 and perplexity is 334.78684420372633
At time: 328.346045255661 and batch: 5900, loss is 5.773201875686645 and perplexity is 321.56569945290624
At time: 331.060435295105 and batch: 5950, loss is 5.735282754898071 and perplexity is 309.60049944170447
At time: 333.7771170139313 and batch: 6000, loss is 5.807836637496949 and perplexity is 332.89816669677424
At time: 336.48550248146057 and batch: 6050, loss is 5.8010440444946285 and perplexity is 330.64458742744995
At time: 339.19507908821106 and batch: 6100, loss is 5.763051338195801 and perplexity is 318.31814486099717
At time: 341.8992917537689 and batch: 6150, loss is 5.796072511672974 and perplexity is 329.0048563744432
At time: 344.61584281921387 and batch: 6200, loss is 5.770968132019043 and perplexity is 320.8482057548719
At time: 347.3281879425049 and batch: 6250, loss is 5.779108028411866 and perplexity is 323.4705351732696
At time: 350.03161215782166 and batch: 6300, loss is 5.816077642440796 and perplexity is 335.6529175012877
At time: 352.74189352989197 and batch: 6350, loss is 5.7251824855804445 and perplexity is 306.48918999821507
At time: 355.449280500412 and batch: 6400, loss is 5.789019880294799 and perplexity is 326.6926694856386
At time: 358.1583254337311 and batch: 6450, loss is 5.752168998718262 and perplexity is 314.8728790234664
At time: 360.86511397361755 and batch: 6500, loss is 5.764931983947754 and perplexity is 318.91735179928344
At time: 363.57666993141174 and batch: 6550, loss is 5.7768118286132815 and perplexity is 322.72863429785184
At time: 366.2848970890045 and batch: 6600, loss is 5.810304536819458 and perplexity is 333.7207404539465
At time: 368.98892760276794 and batch: 6650, loss is 5.800500402450561 and perplexity is 330.4648839796819
At time: 371.6990373134613 and batch: 6700, loss is 5.791788759231568 and perplexity is 327.59849541942435
At time: 374.4081873893738 and batch: 6750, loss is 5.74520359992981 and perplexity is 312.6872844587602
At time: 377.1241970062256 and batch: 6800, loss is 5.829689445495606 and perplexity is 340.2529955619477
At time: 379.8343801498413 and batch: 6850, loss is 5.792644720077515 and perplexity is 327.8790269495695
At time: 382.54218220710754 and batch: 6900, loss is 5.836427373886108 and perplexity is 342.55333694556623
At time: 385.2635359764099 and batch: 6950, loss is 5.805822439193726 and perplexity is 332.2283186041776
At time: 387.9710125923157 and batch: 7000, loss is 5.7954749584198 and perplexity is 328.8083171793725
At time: 390.6883316040039 and batch: 7050, loss is 5.734297742843628 and perplexity is 309.2956893631389
At time: 393.4037775993347 and batch: 7100, loss is 5.751661310195923 and perplexity is 314.71306224879396
At time: 396.1162576675415 and batch: 7150, loss is 5.814465970993042 and perplexity is 335.1123909703847
At time: 398.82720947265625 and batch: 7200, loss is 5.792322015762329 and perplexity is 327.7732360432228
At time: 401.54413533210754 and batch: 7250, loss is 5.758730411529541 and perplexity is 316.9456827898618
At time: 404.2490723133087 and batch: 7300, loss is 5.752144794464112 and perplexity is 314.8652578525102
At time: 406.9518070220947 and batch: 7350, loss is 5.744026966094971 and perplexity is 312.3195823879136
At time: 409.6384334564209 and batch: 7400, loss is 5.825105180740357 and perplexity is 338.6967555800021
At time: 412.3439791202545 and batch: 7450, loss is 5.837680788040161 and perplexity is 342.9829673427747
At time: 415.06350588798523 and batch: 7500, loss is 5.823810977935791 and perplexity is 338.2586968186162
At time: 417.78194785118103 and batch: 7550, loss is 5.7912619972229 and perplexity is 327.4259744206839
At time: 420.4967963695526 and batch: 7600, loss is 5.843263759613037 and perplexity is 344.90318677698866
At time: 423.2022285461426 and batch: 7650, loss is 5.792581529617309 and perplexity is 327.858308777567
At time: 425.9161443710327 and batch: 7700, loss is 5.778327007293701 and perplexity is 323.21799648605634
At time: 428.6307210922241 and batch: 7750, loss is 5.782271938323975 and perplexity is 324.4955875373548
At time: 431.342257976532 and batch: 7800, loss is 5.817497396469117 and perplexity is 336.12980053122476
At time: 434.0480988025665 and batch: 7850, loss is 5.73711067199707 and perplexity is 310.1669410349095
At time: 436.75488567352295 and batch: 7900, loss is 5.765176906585693 and perplexity is 318.99547144461485
At time: 439.4692294597626 and batch: 7950, loss is 5.772100849151611 and perplexity is 321.21184192406355
At time: 442.1828019618988 and batch: 8000, loss is 5.798049974441528 and perplexity is 329.6560949163676
At time: 444.90446066856384 and batch: 8050, loss is 5.810258903503418 and perplexity is 333.70551201739306
At time: 447.6147418022156 and batch: 8100, loss is 5.808474931716919 and perplexity is 333.11072150145066
At time: 450.3304924964905 and batch: 8150, loss is 5.824041795730591 and perplexity is 338.3367819564608
At time: 453.03290486335754 and batch: 8200, loss is 5.822205753326416 and perplexity is 337.71615120443374
At time: 455.74996519088745 and batch: 8250, loss is 5.74840892791748 and perplexity is 313.69115777514963
At time: 458.4790711402893 and batch: 8300, loss is 5.821682672500611 and perplexity is 337.53954455497694
At time: 461.20200395584106 and batch: 8350, loss is 5.824665498733521 and perplexity is 338.54786944447585
At time: 463.92375683784485 and batch: 8400, loss is 5.781513242721558 and perplexity is 324.24948753138773
At time: 466.635710477829 and batch: 8450, loss is 5.825507249832153 and perplexity is 338.8329624573654
At time: 469.3396394252777 and batch: 8500, loss is 5.808729648590088 and perplexity is 333.19558122999405
At time: 472.0465099811554 and batch: 8550, loss is 5.79255313873291 and perplexity is 327.84900072235587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 569 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
