Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'seq_len': 50, 'data': 'gigasmall', 'num_layers': 1, 'anneal': 6.108449030227687, 'lr': 24.83303219627586, 'dropout': 0.8992631823973549, 'wordvec_source': 'glove', 'batch_size': 50, 'wordvec_dim': 200, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 8576 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 9.885857343673706 and batch: 50, loss is 8.806469097137452 and perplexity is 6677.300693978343
At time: 13.596132516860962 and batch: 100, loss is 7.866246252059937 and perplexity is 2607.7583252652203
At time: 17.308202743530273 and batch: 150, loss is 7.534146842956543 and perplexity is 1870.847550923982
At time: 21.023540258407593 and batch: 200, loss is 7.3204931545257566 and perplexity is 1510.948917357808
At time: 24.73304009437561 and batch: 250, loss is 7.1961031341552735 and perplexity is 1334.2213392051551
At time: 28.456342697143555 and batch: 300, loss is 7.190967330932617 and perplexity is 1327.3866068946465
At time: 32.18310785293579 and batch: 350, loss is 7.174992389678955 and perplexity is 1306.3501592076236
At time: 36.323506116867065 and batch: 400, loss is 7.13902946472168 and perplexity is 1260.2047222475833
At time: 40.04964733123779 and batch: 450, loss is 7.153757057189941 and perplexity is 1278.9018476929627
At time: 43.776899576187134 and batch: 500, loss is 7.117783689498902 and perplexity is 1233.7131098858388
At time: 47.49927854537964 and batch: 550, loss is 7.100556144714355 and perplexity is 1212.6412910386348
At time: 51.223474979400635 and batch: 600, loss is 7.102754945755005 and perplexity is 1215.3105815152037
At time: 54.94586229324341 and batch: 650, loss is 7.102093057632446 and perplexity is 1214.506448028638
At time: 58.67603898048401 and batch: 700, loss is 7.111375465393066 and perplexity is 1225.8324771939597
At time: 62.40629196166992 and batch: 750, loss is 7.145099706649781 and perplexity is 1267.8777347505213
At time: 66.1353530883789 and batch: 800, loss is 7.1003304290771485 and perplexity is 1212.3676098251515
At time: 69.8671875 and batch: 850, loss is 7.093331317901612 and perplexity is 1203.9117404426497
At time: 73.60524916648865 and batch: 900, loss is 7.10463586807251 and perplexity is 1217.5986374638542
At time: 77.33651614189148 and batch: 950, loss is 7.16309326171875 and perplexity is 1290.8978483375297
At time: 81.07307863235474 and batch: 1000, loss is 7.166559200286866 and perplexity is 1295.3797835469427
At time: 84.80794715881348 and batch: 1050, loss is 7.127463607788086 and perplexity is 1245.7133388975117
At time: 88.53898215293884 and batch: 1100, loss is 7.1484650707244874 and perplexity is 1272.151792776957
At time: 92.26729583740234 and batch: 1150, loss is 7.12491997718811 and perplexity is 1242.5487308331144
At time: 95.994558095932 and batch: 1200, loss is 7.144499197006225 and perplexity is 1267.1165905040577
At time: 99.720778465271 and batch: 1250, loss is 7.159743194580078 and perplexity is 1286.5804896349807
At time: 103.45192909240723 and batch: 1300, loss is 7.116630067825318 and perplexity is 1232.2906923269034
At time: 107.17574763298035 and batch: 1350, loss is 7.246867198944091 and perplexity is 1403.700438521298
At time: 110.89878463745117 and batch: 1400, loss is 7.405245532989502 and perplexity is 1644.5885872601216
At time: 114.61479258537292 and batch: 1450, loss is 7.445674743652344 and perplexity is 1712.440360684607
At time: 118.32382893562317 and batch: 1500, loss is 7.408364706039428 and perplexity is 1649.726352283273
At time: 122.03943204879761 and batch: 1550, loss is 7.409594240188599 and perplexity is 1651.7559946718122
At time: 125.7644739151001 and batch: 1600, loss is 7.246084098815918 and perplexity is 1402.6016308223532
At time: 129.49839568138123 and batch: 1650, loss is 7.128529863357544 and perplexity is 1247.04229606114
At time: 133.23182606697083 and batch: 1700, loss is 7.147064895629883 and perplexity is 1270.371803958964
At time: 136.96263766288757 and batch: 1750, loss is 7.160043363571167 and perplexity is 1286.9667391696755
At time: 140.69445776939392 and batch: 1800, loss is 7.109016971588135 and perplexity is 1222.9447655541935
At time: 144.42377161979675 and batch: 1850, loss is 7.128865699768067 and perplexity is 1247.4611686020075
At time: 148.1563799381256 and batch: 1900, loss is 7.118545560836792 and perplexity is 1234.653398687502
At time: 151.8890483379364 and batch: 1950, loss is 7.095541982650757 and perplexity is 1206.5761296388846
At time: 155.6238718032837 and batch: 2000, loss is 7.12082989692688 and perplexity is 1237.4769857879971
At time: 159.35890769958496 and batch: 2050, loss is 7.095996084213257 and perplexity is 1207.124162166425
At time: 163.09476709365845 and batch: 2100, loss is 7.152410278320312 and perplexity is 1277.1806090322564
At time: 166.83156991004944 and batch: 2150, loss is 7.160320882797241 and perplexity is 1287.3239467468056
At time: 170.56530499458313 and batch: 2200, loss is 7.150590181350708 and perplexity is 1274.85813068517
At time: 174.301655292511 and batch: 2250, loss is 7.140387744903564 and perplexity is 1261.9175963650698
At time: 178.03768491744995 and batch: 2300, loss is 7.1262373447418215 and perplexity is 1244.1867028837498
At time: 181.77513694763184 and batch: 2350, loss is 7.146586713790893 and perplexity is 1269.7644804506551
At time: 185.51130890846252 and batch: 2400, loss is 7.121140327453613 and perplexity is 1237.8611960527273
At time: 189.24168515205383 and batch: 2450, loss is 7.1494667434692385 and perplexity is 1273.4267109737739
At time: 192.96068024635315 and batch: 2500, loss is 7.268567037582398 and perplexity is 1434.493404415086
At time: 196.67397212982178 and batch: 2550, loss is 7.465294218063354 and perplexity is 1746.3692860322424
At time: 200.38987588882446 and batch: 2600, loss is 7.667867622375488 and perplexity is 2138.5164551974894
At time: 204.10904836654663 and batch: 2650, loss is 7.699954509735107 and perplexity is 2208.247535836986
At time: 207.82148480415344 and batch: 2700, loss is 7.454745302200317 and perplexity is 1728.0438102565092
At time: 211.5442271232605 and batch: 2750, loss is 7.870543279647827 and perplexity is 2618.988044659249
At time: 215.2663254737854 and batch: 2800, loss is 7.63219183921814 and perplexity is 2063.5680760405357
At time: 218.9896981716156 and batch: 2850, loss is 7.667654037475586 and perplexity is 2138.0597491489607
At time: 222.7164192199707 and batch: 2900, loss is 7.80396071434021 and perplexity is 2450.2876733511707
At time: 226.43796753883362 and batch: 2950, loss is 7.629706230163574 and perplexity is 2058.4452218898705
At time: 230.162189245224 and batch: 3000, loss is 7.303400564193725 and perplexity is 1485.3423511977117
At time: 233.89985489845276 and batch: 3050, loss is 7.144756307601929 and perplexity is 1267.442421490973
At time: 237.63470125198364 and batch: 3100, loss is 7.139259071350097 and perplexity is 1260.494106826012
At time: 241.36916995048523 and batch: 3150, loss is 7.121989469528199 and perplexity is 1238.9127624782552
At time: 245.0999219417572 and batch: 3200, loss is 7.101917066574097 and perplexity is 1214.2927245607384
At time: 248.82775354385376 and batch: 3250, loss is 7.134251680374145 and perplexity is 1254.1980964584895
At time: 252.5552270412445 and batch: 3300, loss is 7.1329027366638185 and perplexity is 1252.5073944120875
At time: 256.286607503891 and batch: 3350, loss is 7.110548667907715 and perplexity is 1224.8193808548185
At time: 260.0165944099426 and batch: 3400, loss is 7.136641626358032 and perplexity is 1257.1991469078932
At time: 263.7464904785156 and batch: 3450, loss is 7.118527297973633 and perplexity is 1234.6308505873292
At time: 267.47902607917786 and batch: 3500, loss is 7.148734216690063 and perplexity is 1272.4942333807696
At time: 271.2155222892761 and batch: 3550, loss is 7.162849950790405 and perplexity is 1290.5837969912643
At time: 274.94642877578735 and batch: 3600, loss is 7.179313793182373 and perplexity is 1312.0076406882283
At time: 278.6796917915344 and batch: 3650, loss is 7.20679723739624 and perplexity is 1348.5662059851625
At time: 282.4138114452362 and batch: 3700, loss is 7.145606174468994 and perplexity is 1268.5200366607637
At time: 286.14385414123535 and batch: 3750, loss is 7.168548107147217 and perplexity is 1297.9587370842496
At time: 289.8743612766266 and batch: 3800, loss is 7.115459117889404 and perplexity is 1230.8485861015683
At time: 293.60755586624146 and batch: 3850, loss is 7.093464832305909 and perplexity is 1204.0724907325018
At time: 297.34028601646423 and batch: 3900, loss is 7.1529703712463375 and perplexity is 1277.8961492224257
At time: 301.06903886795044 and batch: 3950, loss is 7.18199631690979 and perplexity is 1315.5318570986624
At time: 304.7981479167938 and batch: 4000, loss is 7.155891714096069 and perplexity is 1281.6347802533055
At time: 308.52336049079895 and batch: 4050, loss is 7.139976491928101 and perplexity is 1261.3987356975845
At time: 312.24978399276733 and batch: 4100, loss is 7.113751163482666 and perplexity is 1228.748147072557
At time: 315.9773426055908 and batch: 4150, loss is 7.141023712158203 and perplexity is 1262.7203898825476
At time: 319.70952916145325 and batch: 4200, loss is 7.161502981185913 and perplexity is 1288.8465900897693
At time: 323.440069437027 and batch: 4250, loss is 7.148856506347657 and perplexity is 1272.64985578017
At time: 327.18417167663574 and batch: 4300, loss is 7.15947979927063 and perplexity is 1286.2416549944703
At time: 330.9097008705139 and batch: 4350, loss is 7.3606007862091065 and perplexity is 1572.7811844170737
At time: 334.62851548194885 and batch: 4400, loss is 7.467735414505005 and perplexity is 1750.637724447656
At time: 338.3499674797058 and batch: 4450, loss is 7.509739484786987 and perplexity is 1825.7378483720056
At time: 342.07125425338745 and batch: 4500, loss is 7.471851091384888 and perplexity is 1757.8576308648007
At time: 345.78910779953003 and batch: 4550, loss is 7.400815763473511 and perplexity is 1637.3195508345157
At time: 349.50661039352417 and batch: 4600, loss is 7.537208700180054 and perplexity is 1876.5845975386244
At time: 353.2199320793152 and batch: 4650, loss is 7.363730459213257 and perplexity is 1577.7111858535302
At time: 356.93863677978516 and batch: 4700, loss is 7.415374336242675 and perplexity is 1661.3309483971732
At time: 360.655207157135 and batch: 4750, loss is 7.483605117797851 and perplexity is 1778.6414433431798
At time: 364.3723990917206 and batch: 4800, loss is 7.319718046188354 and perplexity is 1509.7782220210454
At time: 368.09050130844116 and batch: 4850, loss is 7.25303469657898 and perplexity is 1412.3845096229002
At time: 371.7967402935028 and batch: 4900, loss is 7.210737714767456 and perplexity is 1353.890684206816
At time: 375.5174615383148 and batch: 4950, loss is 7.25275580406189 and perplexity is 1411.9906610751714
At time: 379.2386701107025 and batch: 5000, loss is 7.245416498184204 and perplexity is 1401.6655655812124
At time: 382.958003282547 and batch: 5050, loss is 7.499639215469361 and perplexity is 1807.3902183803898
At time: 386.673011302948 and batch: 5100, loss is 7.722909231185913 and perplexity is 2259.5235041972287
At time: 390.3912034034729 and batch: 5150, loss is 7.549940977096558 and perplexity is 1900.6305472956253
At time: 394.1135768890381 and batch: 5200, loss is 7.6188265895843506 and perplexity is 2036.1714626762748
At time: 397.83322620391846 and batch: 5250, loss is 7.63090669631958 and perplexity is 2060.9177995386945
At time: 401.5554313659668 and batch: 5300, loss is 7.604497537612915 and perplexity is 2007.2030962302308
At time: 405.2730784416199 and batch: 5350, loss is 7.54994140625 and perplexity is 1900.6313629579429
At time: 408.9925081729889 and batch: 5400, loss is 7.618721170425415 and perplexity is 2035.9568225070227
At time: 412.7093608379364 and batch: 5450, loss is 7.744501447677612 and perplexity is 2308.8421582833
At time: 416.42572140693665 and batch: 5500, loss is 7.769758281707763 and perplexity is 2367.8988547069475
At time: 420.14249062538147 and batch: 5550, loss is 7.834140577316284 and perplexity is 2525.36422117343
At time: 423.8548595905304 and batch: 5600, loss is 7.584835729598999 and perplexity is 1968.1233029725706
At time: 427.56779766082764 and batch: 5650, loss is 7.605878763198852 and perplexity is 2009.9774120398392
At time: 431.2788977622986 and batch: 5700, loss is 7.526800537109375 and perplexity is 1857.1540923730126
At time: 434.9898450374603 and batch: 5750, loss is 7.506914911270141 and perplexity is 1820.588193809494
At time: 438.6999001502991 and batch: 5800, loss is 7.549435577392578 and perplexity is 1899.670211876707
At time: 442.4104518890381 and batch: 5850, loss is 7.662590532302857 and perplexity is 2127.2610352936917
At time: 446.1258182525635 and batch: 5900, loss is 7.614662866592408 and perplexity is 2027.7110344043608
At time: 449.839950799942 and batch: 5950, loss is 7.560117158889771 and perplexity is 1920.0704535190496
At time: 453.56003522872925 and batch: 6000, loss is 7.83298113822937 and perplexity is 2522.4379119531727
At time: 457.2800078392029 and batch: 6050, loss is 7.7017644500732425 and perplexity is 2212.247951293262
At time: 460.9983606338501 and batch: 6100, loss is 7.6517023086547855 and perplexity is 2104.224581998421
At time: 464.7170271873474 and batch: 6150, loss is 7.799482250213623 and perplexity is 2439.3386835352967
At time: 468.4435875415802 and batch: 6200, loss is 7.713809204101563 and perplexity is 2239.055052095581
At time: 472.16013526916504 and batch: 6250, loss is 7.621358165740967 and perplexity is 2041.3327160985325
At time: 475.87708020210266 and batch: 6300, loss is 7.759052143096924 and perplexity is 2342.6830242954907
At time: 479.59738993644714 and batch: 6350, loss is 7.662887897491455 and perplexity is 2127.8937027346237
At time: 483.31870889663696 and batch: 6400, loss is 7.602720365524292 and perplexity is 2003.639118750085
At time: 487.0332176685333 and batch: 6450, loss is 7.488329610824585 and perplexity is 1787.064504117788
At time: 490.74826169013977 and batch: 6500, loss is 7.661729145050049 and perplexity is 2125.4294287290204
At time: 494.45747470855713 and batch: 6550, loss is 7.6838650894165035 and perplexity is 2173.002410206177
At time: 498.16450595855713 and batch: 6600, loss is 7.593855829238891 and perplexity is 1985.956277963954
At time: 501.8771629333496 and batch: 6650, loss is 7.683940868377686 and perplexity is 2173.1670843108072
At time: 505.59008502960205 and batch: 6700, loss is 7.489008245468139 and perplexity is 1788.2776796054457
At time: 509.30555868148804 and batch: 6750, loss is 7.652845363616944 and perplexity is 2106.631201535536
At time: 513.0257782936096 and batch: 6800, loss is 7.698675260543824 and perplexity is 2205.4244430671774
At time: 516.738822221756 and batch: 6850, loss is 7.60558632850647 and perplexity is 2009.3897108499502
At time: 520.4589402675629 and batch: 6900, loss is 7.789554033279419 and perplexity is 2415.2402252175475
At time: 524.1744055747986 and batch: 6950, loss is 7.691081581115722 and perplexity is 2188.7405829576605
At time: 527.898814201355 and batch: 7000, loss is 7.843372821807861 and perplexity is 2548.7869569369873
At time: 531.6179616451263 and batch: 7050, loss is 7.664530658721924 and perplexity is 2131.3921968204886
At time: 535.3356680870056 and batch: 7100, loss is 7.600585260391235 and perplexity is 1999.3657023029698
At time: 539.0534791946411 and batch: 7150, loss is 7.642640008926391 and perplexity is 2085.2416127340557
At time: 542.7710809707642 and batch: 7200, loss is 7.695522499084473 and perplexity is 2198.4822152261777
At time: 546.4853947162628 and batch: 7250, loss is 7.4858216953277585 and perplexity is 1782.5883126551862
At time: 550.1991426944733 and batch: 7300, loss is 7.459522686004639 and perplexity is 1736.3190901236335
At time: 553.9076766967773 and batch: 7350, loss is 7.449313173294067 and perplexity is 1718.682303000588
At time: 557.6172113418579 and batch: 7400, loss is 7.518753900527954 and perplexity is 1842.2702112161041
At time: 561.325079202652 and batch: 7450, loss is 7.6217211151123045 and perplexity is 2042.0737509954722
At time: 565.0382435321808 and batch: 7500, loss is 7.71766848564148 and perplexity is 2247.712891699412
At time: 568.7543139457703 and batch: 7550, loss is 7.775815105438232 and perplexity is 2382.2843218231064
At time: 572.4712221622467 and batch: 7600, loss is 7.7215359592437744 and perplexity is 2256.422693581733
At time: 576.1901323795319 and batch: 7650, loss is 7.783552865982056 and perplexity is 2400.7893689351795
At time: 579.9080040454865 and batch: 7700, loss is 7.571251230239868 and perplexity is 1941.5681110885039
At time: 583.6276524066925 and batch: 7750, loss is 7.865257701873779 and perplexity is 2605.1816990593134
At time: 587.3454444408417 and batch: 7800, loss is 7.845570812225342 and perplexity is 2554.3953275590643
At time: 591.0633676052094 and batch: 7850, loss is 7.5321063137054445 and perplexity is 1867.0339240038702
At time: 594.7859261035919 and batch: 7900, loss is 7.532306547164917 and perplexity is 1867.407804095829
At time: 598.503940820694 and batch: 7950, loss is 7.629019680023194 and perplexity is 2057.0324810480197
At time: 602.2118275165558 and batch: 8000, loss is 7.5162271308898925 and perplexity is 1837.6210948778505
At time: 605.908855676651 and batch: 8050, loss is 7.454043407440185 and perplexity is 1726.8313309270336
At time: 609.620210647583 and batch: 8100, loss is 7.509149799346924 and perplexity is 1824.6615547139857
At time: 613.3265705108643 and batch: 8150, loss is 7.53972357749939 and perplexity is 1881.309916889538
At time: 617.030202627182 and batch: 8200, loss is 7.480107431411743 and perplexity is 1772.4311804882147
At time: 620.7370035648346 and batch: 8250, loss is 7.548707704544068 and perplexity is 1898.2879966078656
At time: 624.4457294940948 and batch: 8300, loss is 7.596924238204956 and perplexity is 1992.0593626048415
At time: 628.161438703537 and batch: 8350, loss is 7.650862350463867 and perplexity is 2102.457863413985
At time: 631.8735268115997 and batch: 8400, loss is 7.656025695800781 and perplexity is 2113.3416536225473
At time: 635.5877413749695 and batch: 8450, loss is 7.6111420726776124 and perplexity is 2020.5844347396032
At time: 639.2969715595245 and batch: 8500, loss is 7.493829641342163 and perplexity is 1796.9204927007424
At time: 643.013466835022 and batch: 8550, loss is 7.662966613769531 and perplexity is 2128.061209199701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 569 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
