Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'anneal': 2.2257311753662283, 'num_layers': 1, 'dropout': 0.3983140465493671, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 29.077239322071772, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.214442491531372 and batch: 50, loss is 6.871644916534424 and perplexity is 964.5338392935523
At time: 5.716282844543457 and batch: 100, loss is 6.184109621047973 and perplexity is 484.9809544775479
At time: 8.220850467681885 and batch: 150, loss is 6.16110860824585 and perplexity is 473.9532121344003
At time: 10.723633766174316 and batch: 200, loss is 6.150623044967651 and perplexity is 469.0095097849358
At time: 13.232959985733032 and batch: 250, loss is 6.084526233673095 and perplexity is 439.0117744003862
At time: 15.741483449935913 and batch: 300, loss is 6.09117317199707 and perplexity is 441.9395782745861
At time: 18.249206066131592 and batch: 350, loss is 6.063481092453003 and perplexity is 429.8692499017413
At time: 20.759580612182617 and batch: 400, loss is 6.070474424362183 and perplexity is 432.88600453211643
At time: 23.263331174850464 and batch: 450, loss is 6.058503408432006 and perplexity is 427.7348132880369
At time: 25.762325286865234 and batch: 500, loss is 6.0584207534790036 and perplexity is 427.6994603482148
At time: 28.27221655845642 and batch: 550, loss is 6.043464727401734 and perplexity is 421.3503729721786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.548191833496094 and perplexity of 256.7728478350353
Finished 1 epochs...
Completing Train Step...
At time: 32.35464262962341 and batch: 50, loss is 5.87640866279602 and perplexity is 356.52653276601626
At time: 34.851778507232666 and batch: 100, loss is 5.868167343139649 and perplexity is 353.60035797639154
At time: 37.34894299507141 and batch: 150, loss is 5.8490827274322506 and perplexity is 346.916017941875
At time: 39.851311922073364 and batch: 200, loss is 5.84909309387207 and perplexity is 346.9196142445378
At time: 42.34262704849243 and batch: 250, loss is 5.824099349975586 and perplexity is 338.3562552348798
At time: 44.83975172042847 and batch: 300, loss is 5.75001482963562 and perplexity is 314.1953196532497
At time: 47.33820366859436 and batch: 350, loss is 5.75040906906128 and perplexity is 314.3192122556843
At time: 49.837212800979614 and batch: 400, loss is 5.802715740203857 and perplexity is 331.19778682723427
At time: 52.33851671218872 and batch: 450, loss is 5.747837677001953 and perplexity is 313.5120125873757
At time: 54.84110236167908 and batch: 500, loss is 5.739750051498413 and perplexity is 310.98667061392007
At time: 57.334155797958374 and batch: 550, loss is 5.795534963607788 and perplexity is 328.8280479762261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.646422322591146 and perplexity of 283.2761799347596
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 61.370383739471436 and batch: 50, loss is 5.736468639373779 and perplexity is 309.9678676527399
At time: 63.9065899848938 and batch: 100, loss is 5.691385173797608 and perplexity is 296.30376872828714
At time: 66.41162538528442 and batch: 150, loss is 5.724137954711914 and perplexity is 306.1692197169007
At time: 68.91914772987366 and batch: 200, loss is 5.730800476074219 and perplexity is 308.21588910216764
At time: 71.4855580329895 and batch: 250, loss is 5.662277698516846 and perplexity is 287.8034260098895
At time: 74.0038492679596 and batch: 300, loss is 5.675046434402466 and perplexity is 291.50187394639784
At time: 76.52188611030579 and batch: 350, loss is 5.646982574462891 and perplexity is 283.4349304107621
At time: 79.04264760017395 and batch: 400, loss is 5.643710317611695 and perplexity is 282.5089743271618
At time: 81.56529426574707 and batch: 450, loss is 5.626259546279908 and perplexity is 277.6217418302523
At time: 84.08751273155212 and batch: 500, loss is 5.614041776657104 and perplexity is 274.25046005524484
At time: 86.60149335861206 and batch: 550, loss is 5.603906803131103 and perplexity is 271.48497662572856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.422375996907552 and perplexity of 226.41644868847393
Finished 3 epochs...
Completing Train Step...
At time: 90.67685317993164 and batch: 50, loss is 5.594431858062745 and perplexity is 268.9248192137976
At time: 93.19250798225403 and batch: 100, loss is 5.583216371536255 and perplexity is 265.925547114266
At time: 95.70655345916748 and batch: 150, loss is 5.6226534652709965 and perplexity is 276.62241824320034
At time: 98.22542595863342 and batch: 200, loss is 5.646283473968506 and perplexity is 283.23685015785475
At time: 100.73770475387573 and batch: 250, loss is 5.569828100204468 and perplexity is 262.38899074946016
At time: 103.25525879859924 and batch: 300, loss is 5.580054273605347 and perplexity is 265.0859925683557
At time: 105.77157402038574 and batch: 350, loss is 5.529364728927613 and perplexity is 251.98378215876397
At time: 108.28454899787903 and batch: 400, loss is 5.565839462280273 and perplexity is 261.34450050152566
At time: 110.80211687088013 and batch: 450, loss is 5.553495225906372 and perplexity is 258.13823239222893
At time: 113.31920671463013 and batch: 500, loss is 5.515198802947998 and perplexity is 248.43936281604022
At time: 115.83615803718567 and batch: 550, loss is 5.581809120178223 and perplexity is 265.55158621715873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.481469217936198 and perplexity of 240.1993534522037
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 120.01034283638 and batch: 50, loss is 5.5590966796875 and perplexity is 259.5882390514417
At time: 122.52583527565002 and batch: 100, loss is 5.520137090682983 and perplexity is 249.66926217334822
At time: 125.07590842247009 and batch: 150, loss is 5.53859697341919 and perplexity is 254.32093000330894
At time: 127.59216403961182 and batch: 200, loss is 5.557145948410034 and perplexity is 259.08234574563426
At time: 130.11093139648438 and batch: 250, loss is 5.502353191375732 and perplexity is 245.2684172326396
At time: 132.6241910457611 and batch: 300, loss is 5.485552740097046 and perplexity is 241.18221824434244
At time: 135.14079523086548 and batch: 350, loss is 5.410572986602784 and perplexity is 223.75976230190986
At time: 137.65866923332214 and batch: 400, loss is 5.434611511230469 and perplexity is 229.2037878798614
At time: 140.17825198173523 and batch: 450, loss is 5.417711296081543 and perplexity is 225.3627432143215
At time: 142.69593620300293 and batch: 500, loss is 5.387138214111328 and perplexity is 218.576969160744
At time: 145.21241402626038 and batch: 550, loss is 5.4105596923828125 and perplexity is 223.75678761018227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.291644795735677 and perplexity of 198.66992827134285
Finished 5 epochs...
Completing Train Step...
At time: 149.28910756111145 and batch: 50, loss is 5.455772504806519 and perplexity is 234.10564897364543
At time: 151.8372938632965 and batch: 100, loss is 5.450237102508545 and perplexity is 232.81335999575293
At time: 154.35971236228943 and batch: 150, loss is 5.4710352325439455 and perplexity is 237.7061465683316
At time: 156.88019371032715 and batch: 200, loss is 5.48667462348938 and perplexity is 241.4529484049569
At time: 159.3994846343994 and batch: 250, loss is 5.444154844284058 and perplexity is 231.4016266376823
At time: 161.91856288909912 and batch: 300, loss is 5.429798049926758 and perplexity is 228.10317531926754
At time: 164.43959712982178 and batch: 350, loss is 5.352456340789795 and perplexity is 211.12625950411086
At time: 166.96422576904297 and batch: 400, loss is 5.37832857131958 and perplexity is 216.6598411454139
At time: 169.48466420173645 and batch: 450, loss is 5.3739121055603025 and perplexity is 215.70508026185723
At time: 172.0132999420166 and batch: 500, loss is 5.356967220306396 and perplexity is 212.08077585911798
At time: 174.5356891155243 and batch: 550, loss is 5.374358854293823 and perplexity is 215.8014677621719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.292555745442709 and perplexity of 198.85098904040407
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 178.68657159805298 and batch: 50, loss is 5.407502126693726 and perplexity is 223.07368138694017
At time: 181.24966764450073 and batch: 100, loss is 5.371882705688477 and perplexity is 215.26777228608307
At time: 183.7757704257965 and batch: 150, loss is 5.38663179397583 and perplexity is 218.4663054059469
At time: 186.33398365974426 and batch: 200, loss is 5.403116340637207 and perplexity is 222.09747023695627
At time: 188.85288834571838 and batch: 250, loss is 5.360698938369751 and perplexity is 212.8736800485783
At time: 191.3710618019104 and batch: 300, loss is 5.346609134674072 and perplexity is 209.89536290840886
At time: 193.8889865875244 and batch: 350, loss is 5.26607388496399 and perplexity is 193.6541594536455
At time: 196.4092662334442 and batch: 400, loss is 5.281974620819092 and perplexity is 196.7580144835987
At time: 198.92864990234375 and batch: 450, loss is 5.264833507537841 and perplexity is 193.41410411616812
At time: 201.4551968574524 and batch: 500, loss is 5.250167293548584 and perplexity is 190.59815163277113
At time: 203.97623896598816 and batch: 550, loss is 5.279458665847779 and perplexity is 196.2636023989876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.230780537923177 and perplexity of 186.93865930267188
Finished 7 epochs...
Completing Train Step...
At time: 208.19654035568237 and batch: 50, loss is 5.337779064178466 and perplexity is 208.05013081137795
At time: 210.7168321609497 and batch: 100, loss is 5.31939619064331 and perplexity is 204.2605103428907
At time: 213.24190592765808 and batch: 150, loss is 5.342275009155274 and perplexity is 208.98761861956422
At time: 215.76465797424316 and batch: 200, loss is 5.363748302459717 and perplexity is 213.5238001268595
At time: 218.2938437461853 and batch: 250, loss is 5.316557292938232 and perplexity is 203.68145797296057
At time: 220.8094780445099 and batch: 300, loss is 5.30537675857544 and perplexity is 201.416873654616
At time: 223.33624744415283 and batch: 350, loss is 5.23646167755127 and perplexity is 188.00370640414346
At time: 225.85534024238586 and batch: 400, loss is 5.261677684783936 and perplexity is 192.80468559966013
At time: 228.38198256492615 and batch: 450, loss is 5.2520331764221195 and perplexity is 190.9541174516176
At time: 230.90519213676453 and batch: 500, loss is 5.2362743091583255 and perplexity is 187.96848375171592
At time: 233.42298436164856 and batch: 550, loss is 5.260291624069214 and perplexity is 192.53763171853726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.222136942545573 and perplexity of 185.32980035083605
Finished 8 epochs...
Completing Train Step...
At time: 237.56798791885376 and batch: 50, loss is 5.317585306167603 and perplexity is 203.89095286964715
At time: 240.12813234329224 and batch: 100, loss is 5.303483152389527 and perplexity is 201.03583030360002
At time: 242.64843559265137 and batch: 150, loss is 5.3304104995727535 and perplexity is 206.52273425230084
At time: 245.1689338684082 and batch: 200, loss is 5.357257404327393 and perplexity is 212.1423272416147
At time: 247.6932282447815 and batch: 250, loss is 5.3081949520111085 and perplexity is 201.98530596566866
At time: 250.25717949867249 and batch: 300, loss is 5.297964267730713 and perplexity is 199.92939270294534
At time: 252.77997541427612 and batch: 350, loss is 5.232873182296753 and perplexity is 187.3302650391273
At time: 255.30087971687317 and batch: 400, loss is 5.256932373046875 and perplexity is 191.89193461887012
At time: 257.82164239883423 and batch: 450, loss is 5.244944334030151 and perplexity is 189.60526037504212
At time: 260.3356411457062 and batch: 500, loss is 5.2298235416412355 and perplexity is 186.7598452765942
At time: 262.85140204429626 and batch: 550, loss is 5.247565288543701 and perplexity is 190.10285894477306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.21996103922526 and perplexity of 184.92697903195398
Finished 9 epochs...
Completing Train Step...
At time: 266.99793672561646 and batch: 50, loss is 5.30268705368042 and perplexity is 200.87584962726532
At time: 269.5104501247406 and batch: 100, loss is 5.287164392471314 and perplexity is 197.78179795286866
At time: 272.0280075073242 and batch: 150, loss is 5.312666101455688 and perplexity is 202.89043442868123
At time: 274.5456349849701 and batch: 200, loss is 5.340015897750854 and perplexity is 208.51602519856417
At time: 277.07440161705017 and batch: 250, loss is 5.293269968032837 and perplexity is 198.9930636392505
At time: 279.59100890159607 and batch: 300, loss is 5.284663610458374 and perplexity is 197.28780672969194
At time: 282.10468196868896 and batch: 350, loss is 5.223492603302002 and perplexity is 185.58121506619568
At time: 284.62101340293884 and batch: 400, loss is 5.248967723846436 and perplexity is 190.36965294224026
At time: 287.13682770729065 and batch: 450, loss is 5.234422378540039 and perplexity is 187.62070129527058
At time: 289.65451312065125 and batch: 500, loss is 5.219072303771973 and perplexity is 184.76270088014076
At time: 292.1784291267395 and batch: 550, loss is 5.23407452583313 and perplexity is 187.55544827633042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.217406209309896 and perplexity of 184.45512506373746
Finished 10 epochs...
Completing Train Step...
At time: 296.3277258872986 and batch: 50, loss is 5.28759391784668 and perplexity is 197.86676850107185
At time: 298.86948251724243 and batch: 100, loss is 5.272232513427735 and perplexity is 194.85048354874178
At time: 301.3901369571686 and batch: 150, loss is 5.297395391464233 and perplexity is 199.81568996092707
At time: 303.90963196754456 and batch: 200, loss is 5.322566022872925 and perplexity is 204.9090091650461
At time: 306.42624855041504 and batch: 250, loss is 5.280794715881347 and perplexity is 196.52599563777716
At time: 308.9435887336731 and batch: 300, loss is 5.272518920898437 and perplexity is 194.9062981753816
At time: 311.4601078033447 and batch: 350, loss is 5.215226840972901 and perplexity is 184.05356713483303
At time: 314.0069041252136 and batch: 400, loss is 5.239258861541748 and perplexity is 188.53032354099506
At time: 316.5245349407196 and batch: 450, loss is 5.22551025390625 and perplexity is 185.9560311136696
At time: 319.0468587875366 and batch: 500, loss is 5.209993124008179 and perplexity is 183.09279924479978
At time: 321.56094455718994 and batch: 550, loss is 5.2214033317565915 and perplexity is 185.1938902684257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2208094278971355 and perplexity of 185.08393555674337
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 325.63988041877747 and batch: 50, loss is 5.2706735324859615 and perplexity is 194.5469520198042
At time: 328.15998339653015 and batch: 100, loss is 5.242816410064697 and perplexity is 189.20222376522557
At time: 330.67466473579407 and batch: 150, loss is 5.2574119377136235 and perplexity is 191.9839812799464
At time: 333.2020831108093 and batch: 200, loss is 5.274898653030395 and perplexity is 195.37067528328097
At time: 335.72778725624084 and batch: 250, loss is 5.226160955429077 and perplexity is 186.07707236288599
At time: 338.2482509613037 and batch: 300, loss is 5.215587348937988 and perplexity is 184.1199318735776
At time: 340.77178859710693 and batch: 350, loss is 5.15235990524292 and perplexity is 172.83889282147703
At time: 343.29592061042786 and batch: 400, loss is 5.170868673324585 and perplexity is 176.06771640209197
At time: 345.8173568248749 and batch: 450, loss is 5.159395914077759 and perplexity is 174.05927708095635
At time: 348.33936882019043 and batch: 500, loss is 5.1490374946594235 and perplexity is 172.26560393261278
At time: 350.8673689365387 and batch: 550, loss is 5.173762168884277 and perplexity is 176.57790531643047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.181978352864584 and perplexity of 178.03467823549983
Finished 12 epochs...
Completing Train Step...
At time: 355.08972334861755 and batch: 50, loss is 5.233464698791504 and perplexity is 187.44110675998525
At time: 357.63943696022034 and batch: 100, loss is 5.213731060028076 and perplexity is 183.77846911076804
At time: 360.15509510040283 and batch: 150, loss is 5.234983072280884 and perplexity is 187.7259285455134
At time: 362.671484708786 and batch: 200, loss is 5.25708312034607 and perplexity is 191.92086399020866
At time: 365.19597339630127 and batch: 250, loss is 5.215648097991943 and perplexity is 184.13111732500258
At time: 367.7181625366211 and batch: 300, loss is 5.208895483016968 and perplexity is 182.89193933935258
At time: 370.23385667800903 and batch: 350, loss is 5.150743236541748 and perplexity is 172.55969533883277
At time: 372.75298523902893 and batch: 400, loss is 5.17148609161377 and perplexity is 176.1764573962185
At time: 375.277667760849 and batch: 450, loss is 5.159769697189331 and perplexity is 174.1243496598995
At time: 377.8527340888977 and batch: 500, loss is 5.146734409332275 and perplexity is 171.869318063074
At time: 380.36428713798523 and batch: 550, loss is 5.165304069519043 and perplexity is 175.09069022029078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1803639729817705 and perplexity of 177.7474945067026
Finished 13 epochs...
Completing Train Step...
At time: 384.44292545318604 and batch: 50, loss is 5.222105808258057 and perplexity is 185.32403032936986
At time: 386.9649920463562 and batch: 100, loss is 5.203235063552857 and perplexity is 181.8596186863704
At time: 389.48460626602173 and batch: 150, loss is 5.226024875640869 and perplexity is 186.0517527570708
At time: 392.00534558296204 and batch: 200, loss is 5.249084615707398 and perplexity is 190.39190690587168
At time: 394.5257866382599 and batch: 250, loss is 5.211486597061157 and perplexity is 183.36644769909893
At time: 397.0464997291565 and batch: 300, loss is 5.205678949356079 and perplexity is 182.3046063547031
At time: 399.57881331443787 and batch: 350, loss is 5.149087085723877 and perplexity is 172.2741469791081
At time: 402.0967538356781 and batch: 400, loss is 5.1702354145050045 and perplexity is 175.9562552634363
At time: 404.61335921287537 and batch: 450, loss is 5.1572795104980464 and perplexity is 173.6912869490336
At time: 407.1333816051483 and batch: 500, loss is 5.143037605285644 and perplexity is 171.23512384071094
At time: 409.6510217189789 and batch: 550, loss is 5.158850221633911 and perplexity is 173.96432015966525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.180175272623698 and perplexity of 177.7139566552445
Finished 14 epochs...
Completing Train Step...
At time: 413.6877360343933 and batch: 50, loss is 5.2136180973052975 and perplexity is 183.7577101670247
At time: 416.2316448688507 and batch: 100, loss is 5.195144557952881 and perplexity is 180.39421833116498
At time: 418.7563970088959 and batch: 150, loss is 5.219267435073853 and perplexity is 184.7987573842643
At time: 421.2746682167053 and batch: 200, loss is 5.24284686088562 and perplexity is 189.2079852159796
At time: 423.7952399253845 and batch: 250, loss is 5.206883296966553 and perplexity is 182.5242967369841
At time: 426.31476402282715 and batch: 300, loss is 5.201975708007812 and perplexity is 181.63073691916952
At time: 428.8322341442108 and batch: 350, loss is 5.146298751831055 and perplexity is 171.7944582132418
At time: 431.35439229011536 and batch: 400, loss is 5.167675132751465 and perplexity is 175.50633388218677
At time: 433.8765950202942 and batch: 450, loss is 5.154029865264892 and perplexity is 173.12776800059723
At time: 436.3985786437988 and batch: 500, loss is 5.13907470703125 and perplexity is 170.5578792793156
At time: 438.91406750679016 and batch: 550, loss is 5.152843856811524 and perplexity is 172.92255871826174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.180006408691407 and perplexity of 177.68394971131718
Finished 15 epochs...
Completing Train Step...
At time: 442.9924373626709 and batch: 50, loss is 5.2065458011627195 and perplexity is 182.46270594663895
At time: 445.5060794353485 and batch: 100, loss is 5.188170003890991 and perplexity is 179.14042650489145
At time: 448.0195255279541 and batch: 150, loss is 5.2132354354858395 and perplexity is 183.68740655942383
At time: 450.5349109172821 and batch: 200, loss is 5.2367223072052 and perplexity is 188.0527121309767
At time: 453.05980825424194 and batch: 250, loss is 5.202720441818237 and perplexity is 181.76605385125862
At time: 455.57189655303955 and batch: 300, loss is 5.198177404403687 and perplexity is 180.94215678256833
At time: 458.0901792049408 and batch: 350, loss is 5.143312187194824 and perplexity is 171.2821483636799
At time: 460.61472821235657 and batch: 400, loss is 5.164806680679321 and perplexity is 175.0036237197839
At time: 463.140079498291 and batch: 450, loss is 5.15022795677185 and perplexity is 172.47080172323268
At time: 465.65938901901245 and batch: 500, loss is 5.134946317672729 and perplexity is 169.85520140742437
At time: 468.18061208724976 and batch: 550, loss is 5.147119512557984 and perplexity is 171.9355182379927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.18003184000651 and perplexity of 177.68846850529036
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 472.2149906158447 and batch: 50, loss is 5.1994912052154545 and perplexity is 181.18003496308935
At time: 474.76365900039673 and batch: 100, loss is 5.1781815147399906 and perplexity is 177.35999103124814
At time: 477.2882950305939 and batch: 150, loss is 5.1960702419281 and perplexity is 180.56128368122526
At time: 479.80709195137024 and batch: 200, loss is 5.21302864074707 and perplexity is 183.64942489750842
At time: 482.329026222229 and batch: 250, loss is 5.174854316711426 and perplexity is 176.7708598402968
At time: 484.84469509124756 and batch: 300, loss is 5.16927128791809 and perplexity is 175.7866929125229
At time: 487.3591995239258 and batch: 350, loss is 5.106885185241699 and perplexity is 165.15512524634588
At time: 489.87955927848816 and batch: 400, loss is 5.128094186782837 and perplexity is 168.69530973805672
At time: 492.403769493103 and batch: 450, loss is 5.114758615493774 and perplexity is 166.4605951248317
At time: 494.927969455719 and batch: 500, loss is 5.104630107879639 and perplexity is 164.7831072844708
At time: 497.44802236557007 and batch: 550, loss is 5.128172159194946 and perplexity is 168.70846383109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1655832926432295 and perplexity of 175.13958641598666
Finished 17 epochs...
Completing Train Step...
At time: 501.5363404750824 and batch: 50, loss is 5.183906736373902 and perplexity is 178.37832861146157
At time: 504.06548285484314 and batch: 100, loss is 5.165527458190918 and perplexity is 175.12980786609359
At time: 506.61892104148865 and batch: 150, loss is 5.185279989242554 and perplexity is 178.6234554349673
At time: 509.1467547416687 and batch: 200, loss is 5.205286512374878 and perplexity is 182.23307732156312
At time: 511.67016768455505 and batch: 250, loss is 5.170771656036377 and perplexity is 176.05063561828464
At time: 514.1889765262604 and batch: 300, loss is 5.16732310295105 and perplexity is 175.4445612960322
At time: 516.7149293422699 and batch: 350, loss is 5.10673942565918 and perplexity is 165.1310540585848
At time: 519.2396097183228 and batch: 400, loss is 5.130118627548217 and perplexity is 169.03716932050094
At time: 521.7565994262695 and batch: 450, loss is 5.117191486358642 and perplexity is 166.86606528513585
At time: 524.2762999534607 and batch: 500, loss is 5.105415458679199 and perplexity is 164.9125706599593
At time: 526.8053288459778 and batch: 550, loss is 5.124627141952515 and perplexity is 168.11144825834612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.164946492513021 and perplexity of 175.0280930078284
Finished 18 epochs...
Completing Train Step...
At time: 530.8432631492615 and batch: 50, loss is 5.177541561126709 and perplexity is 177.24652517445108
At time: 533.3921728134155 and batch: 100, loss is 5.160230731964111 and perplexity is 174.20464554840484
At time: 535.9112422466278 and batch: 150, loss is 5.180639009475708 and perplexity is 177.79638827787545
At time: 538.4382677078247 and batch: 200, loss is 5.202095060348511 and perplexity is 181.65241626647833
At time: 540.9614803791046 and batch: 250, loss is 5.16881178855896 and perplexity is 175.70593759471524
At time: 543.4808139801025 and batch: 300, loss is 5.1661146068573 and perplexity is 175.2326652925713
At time: 546.001211643219 and batch: 350, loss is 5.106062021255493 and perplexity is 165.01923143422516
At time: 548.5261344909668 and batch: 400, loss is 5.130260620117188 and perplexity is 169.06117304655925
At time: 551.0634162425995 and batch: 450, loss is 5.117163925170899 and perplexity is 166.86146632155922
At time: 553.588873386383 and batch: 500, loss is 5.104888210296631 and perplexity is 164.82564369187898
At time: 556.1099371910095 and batch: 550, loss is 5.121844997406006 and perplexity is 167.64438792531433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.164832560221354 and perplexity of 175.00815279202476
Finished 19 epochs...
Completing Train Step...
At time: 560.2410726547241 and batch: 50, loss is 5.173188152313233 and perplexity is 176.4765757579037
At time: 562.7652051448822 and batch: 100, loss is 5.156447563171387 and perplexity is 173.54684503955363
At time: 565.2895131111145 and batch: 150, loss is 5.177113056182861 and perplexity is 177.17059043250202
At time: 567.809681892395 and batch: 200, loss is 5.199382209777832 and perplexity is 181.16028824206097
At time: 570.3571302890778 and batch: 250, loss is 5.167040662765503 and perplexity is 175.39501569874975
At time: 572.8767881393433 and batch: 300, loss is 5.164709644317627 and perplexity is 174.98664282875006
At time: 575.3978662490845 and batch: 350, loss is 5.104943418502808 and perplexity is 164.8347436711937
At time: 577.9188280105591 and batch: 400, loss is 5.129569997787476 and perplexity is 168.94445593372734
At time: 580.4451296329498 and batch: 450, loss is 5.116579313278198 and perplexity is 166.76394563256892
At time: 582.9648711681366 and batch: 500, loss is 5.103905420303345 and perplexity is 164.6637342731321
At time: 585.4816029071808 and batch: 550, loss is 5.11922251701355 and perplexity is 167.2053197805126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.164807637532552 and perplexity of 175.0037911726467
Finished 20 epochs...
Completing Train Step...
At time: 589.52152967453 and batch: 50, loss is 5.169510383605957 and perplexity is 175.8287277777598
At time: 592.0740532875061 and batch: 100, loss is 5.153246746063233 and perplexity is 172.99224139480546
At time: 594.5945267677307 and batch: 150, loss is 5.174038887023926 and perplexity is 176.62677438706
At time: 597.1121096611023 and batch: 200, loss is 5.196838970184326 and perplexity is 180.70013960637004
At time: 599.628764629364 and batch: 250, loss is 5.165211048126221 and perplexity is 175.07440379792087
At time: 602.1558055877686 and batch: 300, loss is 5.163315114974975 and perplexity is 174.7427888910078
At time: 604.6848609447479 and batch: 350, loss is 5.103498954772949 and perplexity is 164.59681774159674
At time: 607.2022230625153 and batch: 400, loss is 5.128646965026856 and perplexity is 168.7885866134864
At time: 609.7237372398376 and batch: 450, loss is 5.1157411766052245 and perplexity is 166.6242332112352
At time: 612.2459495067596 and batch: 500, loss is 5.102767391204834 and perplexity is 164.47644874046563
At time: 614.7717425823212 and batch: 550, loss is 5.116679105758667 and perplexity is 166.7805882507467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.164935811360677 and perplexity of 175.0262235160867
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 618.8351447582245 and batch: 50, loss is 5.166019134521484 and perplexity is 175.21593621929915
At time: 621.3816900253296 and batch: 100, loss is 5.14794771194458 and perplexity is 172.07797411152237
At time: 623.9051506519318 and batch: 150, loss is 5.166008567810058 and perplexity is 175.21408477284578
At time: 626.4287219047546 and batch: 200, loss is 5.18542896270752 and perplexity is 178.65006757224953
At time: 628.9446408748627 and batch: 250, loss is 5.150767593383789 and perplexity is 172.5638983992612
At time: 631.4865355491638 and batch: 300, loss is 5.148796195983887 and perplexity is 172.22404148522673
At time: 634.0030767917633 and batch: 350, loss is 5.0860482120513915 and perplexity is 161.74939807605418
At time: 636.531580209732 and batch: 400, loss is 5.1083666038513185 and perplexity is 165.39997043669237
At time: 639.052182674408 and batch: 450, loss is 5.097151889801025 and perplexity is 163.5554194525987
At time: 641.5724296569824 and batch: 500, loss is 5.086184043884277 and perplexity is 161.77137028549194
At time: 644.0894656181335 and batch: 550, loss is 5.106648645401001 and perplexity is 165.11606409927043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1544342041015625 and perplexity of 173.19778443513385
Finished 22 epochs...
Completing Train Step...
At time: 648.1777126789093 and batch: 50, loss is 5.1595729732513425 and perplexity is 174.09009860124675
At time: 650.7067544460297 and batch: 100, loss is 5.142491893768311 and perplexity is 171.14170435382232
At time: 653.2258803844452 and batch: 150, loss is 5.1618420505523686 and perplexity is 174.48557100135827
At time: 655.7484800815582 and batch: 200, loss is 5.182605295181275 and perplexity is 178.146330705296
At time: 658.2746036052704 and batch: 250, loss is 5.1500012397766115 and perplexity is 172.43170409351558
At time: 660.8000526428223 and batch: 300, loss is 5.149594135284424 and perplexity is 172.3615206591486
At time: 663.3201580047607 and batch: 350, loss is 5.087534189224243 and perplexity is 161.98993265950693
At time: 665.8488068580627 and batch: 400, loss is 5.109294490814209 and perplexity is 165.55351413750574
At time: 668.3714010715485 and batch: 450, loss is 5.098194885253906 and perplexity is 163.72609600334258
At time: 670.8955476284027 and batch: 500, loss is 5.0861861038208005 and perplexity is 161.77170352458927
At time: 673.4224321842194 and batch: 550, loss is 5.1047702980041505 and perplexity is 164.80620986813764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.154045104980469 and perplexity of 173.13040643864443
Finished 23 epochs...
Completing Train Step...
At time: 677.5027780532837 and batch: 50, loss is 5.156063394546509 and perplexity is 173.48018659160485
At time: 680.050940990448 and batch: 100, loss is 5.139245643615722 and perplexity is 170.58703635258865
At time: 682.5730133056641 and batch: 150, loss is 5.159320678710937 and perplexity is 174.04618216000318
At time: 685.0990753173828 and batch: 200, loss is 5.180905351638794 and perplexity is 177.84374925935128
At time: 687.6168491840363 and batch: 250, loss is 5.149750242233276 and perplexity is 172.38842959051914
At time: 690.1333065032959 and batch: 300, loss is 5.1501318359375 and perplexity is 172.45422448259177
At time: 692.6548471450806 and batch: 350, loss is 5.088235187530517 and perplexity is 162.10352713805156
At time: 695.213143825531 and batch: 400, loss is 5.109389343261719 and perplexity is 165.56921803828047
At time: 697.7343168258667 and batch: 450, loss is 5.098404836654663 and perplexity is 163.76047413528119
At time: 700.2528944015503 and batch: 500, loss is 5.085678586959839 and perplexity is 161.68962248794088
At time: 702.7710154056549 and batch: 550, loss is 5.1030942153930665 and perplexity is 164.53021240746347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.153923034667969 and perplexity of 173.1092736456968
Finished 24 epochs...
Completing Train Step...
At time: 706.8348968029022 and batch: 50, loss is 5.153450317382813 and perplexity is 173.0274612384167
At time: 709.3621571063995 and batch: 100, loss is 5.136792697906494 and perplexity is 170.16910840056
At time: 711.8839676380157 and batch: 150, loss is 5.1574538135528565 and perplexity is 173.7215645096017
At time: 714.4025747776031 and batch: 200, loss is 5.179618883132934 and perplexity is 177.6151059796881
At time: 716.9228212833405 and batch: 250, loss is 5.149532861709595 and perplexity is 172.35095977616967
At time: 719.4479880332947 and batch: 300, loss is 5.150447006225586 and perplexity is 172.50858549624184
At time: 721.9733848571777 and batch: 350, loss is 5.08859601020813 and perplexity is 162.16202832040594
At time: 724.4870796203613 and batch: 400, loss is 5.10918041229248 and perplexity is 165.5346291145542
At time: 727.0060911178589 and batch: 450, loss is 5.098209190368652 and perplexity is 163.72843814068506
At time: 729.5264773368835 and batch: 500, loss is 5.084983510971069 and perplexity is 161.57727496327294
At time: 732.0506794452667 and batch: 550, loss is 5.101603212356568 and perplexity is 164.2850801530486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.154004414876302 and perplexity of 173.1233618876947
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 736.1172215938568 and batch: 50, loss is 5.151007013320923 and perplexity is 172.60521858320803
At time: 738.6533126831055 and batch: 100, loss is 5.133529291152954 and perplexity is 169.61468253356585
At time: 741.1741898059845 and batch: 150, loss is 5.153026361465454 and perplexity is 172.95412077002032
At time: 743.7045555114746 and batch: 200, loss is 5.1748159217834475 and perplexity is 176.76407286615807
At time: 746.2303383350372 and batch: 250, loss is 5.143189239501953 and perplexity is 171.2610909132176
At time: 748.7581510543823 and batch: 300, loss is 5.1429032135009765 and perplexity is 171.21211279310265
At time: 751.2827343940735 and batch: 350, loss is 5.079693403244018 and perplexity is 160.72477068034826
At time: 753.803510427475 and batch: 400, loss is 5.099094190597534 and perplexity is 163.87340198296926
At time: 756.3228549957275 and batch: 450, loss is 5.088732013702392 and perplexity is 162.18408442271362
At time: 758.8671369552612 and batch: 500, loss is 5.077618246078491 and perplexity is 160.39158734425186
At time: 761.3899834156036 and batch: 550, loss is 5.096041202545166 and perplexity is 163.37386137837018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152355448404948 and perplexity of 172.83812250825298
Finished 26 epochs...
Completing Train Step...
At time: 765.4463765621185 and batch: 50, loss is 5.148380870819092 and perplexity is 172.15252735865144
At time: 767.9664425849915 and batch: 100, loss is 5.131727733612061 and perplexity is 169.30938700950048
At time: 770.4867544174194 and batch: 150, loss is 5.15212721824646 and perplexity is 172.7986801373007
At time: 773.0096046924591 and batch: 200, loss is 5.174250860214233 and perplexity is 176.66421849635395
At time: 775.5330970287323 and batch: 250, loss is 5.142913475036621 and perplexity is 171.2138697013151
At time: 778.0599012374878 and batch: 300, loss is 5.142524118423462 and perplexity is 171.14721942508731
At time: 780.5861322879791 and batch: 350, loss is 5.079776945114136 and perplexity is 160.73819848914945
At time: 783.09894323349 and batch: 400, loss is 5.099486761093139 and perplexity is 163.93774647465034
At time: 785.6210489273071 and batch: 450, loss is 5.088809309005737 and perplexity is 162.19662097521882
At time: 788.1457636356354 and batch: 500, loss is 5.07725004196167 and perplexity is 160.33254137264024
At time: 790.6651287078857 and batch: 550, loss is 5.094802436828613 and perplexity is 163.17160474007275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152239990234375 and perplexity of 172.81816808679562
Finished 27 epochs...
Completing Train Step...
At time: 794.7270431518555 and batch: 50, loss is 5.146876163482666 and perplexity is 171.89368297910752
At time: 797.2842404842377 and batch: 100, loss is 5.130873279571533 and perplexity is 169.1647817077611
At time: 799.8120672702789 and batch: 150, loss is 5.151685676574707 and perplexity is 172.72239916105033
At time: 802.3259074687958 and batch: 200, loss is 5.174024410247803 and perplexity is 176.62421741929828
At time: 804.846512556076 and batch: 250, loss is 5.142737836837768 and perplexity is 171.18380064633683
At time: 807.3653011322021 and batch: 300, loss is 5.142311534881592 and perplexity is 171.1108402099489
At time: 809.8900945186615 and batch: 350, loss is 5.079775686264038 and perplexity is 160.73799614397993
At time: 812.4150011539459 and batch: 400, loss is 5.099645013809204 and perplexity is 163.96369212122588
At time: 814.9335350990295 and batch: 450, loss is 5.088694181442261 and perplexity is 162.17794874830648
At time: 817.4508237838745 and batch: 500, loss is 5.076779155731201 and perplexity is 160.25706075939726
At time: 819.9762179851532 and batch: 550, loss is 5.09371994972229 and perplexity is 162.99506914781324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152249654134114 and perplexity of 172.81983819231496
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 824.0711529254913 and batch: 50, loss is 5.145676736831665 and perplexity is 171.68763271039788
At time: 826.5911483764648 and batch: 100, loss is 5.1299778079986575 and perplexity is 169.01336725839553
At time: 829.1119220256805 and batch: 150, loss is 5.150568466186524 and perplexity is 172.52953965481754
At time: 831.6296372413635 and batch: 200, loss is 5.172468423843384 and perplexity is 176.34960623933887
At time: 834.1492748260498 and batch: 250, loss is 5.139425745010376 and perplexity is 170.617762082536
At time: 836.6714687347412 and batch: 300, loss is 5.13833517074585 and perplexity is 170.43179216757133
At time: 839.1908650398254 and batch: 350, loss is 5.075675563812256 and perplexity is 160.08029991604838
At time: 841.7112064361572 and batch: 400, loss is 5.094995555877685 and perplexity is 163.20311932815144
At time: 844.2290999889374 and batch: 450, loss is 5.084214105606079 and perplexity is 161.45300435442277
At time: 846.7564997673035 and batch: 500, loss is 5.072507028579712 and perplexity is 159.573882570016
At time: 849.2755105495453 and batch: 550, loss is 5.091155309677124 and perplexity is 162.57758104849242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.151655578613282 and perplexity of 172.7172006471758
Finished 29 epochs...
Completing Train Step...
At time: 853.3449673652649 and batch: 50, loss is 5.144773502349853 and perplexity is 171.53262853353002
At time: 855.8999049663544 and batch: 100, loss is 5.129582262039184 and perplexity is 168.94652792376533
At time: 858.4275481700897 and batch: 150, loss is 5.150213146209717 and perplexity is 172.46824735262348
At time: 860.9483826160431 and batch: 200, loss is 5.1719896030426025 and perplexity is 176.2651865922163
At time: 863.4715287685394 and batch: 250, loss is 5.139013309478759 and perplexity is 170.54740776443577
At time: 865.9927318096161 and batch: 300, loss is 5.138226194381714 and perplexity is 170.41322014250196
At time: 868.5198545455933 and batch: 350, loss is 5.075818767547608 and perplexity is 160.1032256544391
At time: 871.0375199317932 and batch: 400, loss is 5.095111360549927 and perplexity is 163.22202010627217
At time: 873.5644178390503 and batch: 450, loss is 5.084416065216065 and perplexity is 161.48561463308195
At time: 876.0867354869843 and batch: 500, loss is 5.072410478591919 and perplexity is 159.55847645734372
At time: 878.6104824542999 and batch: 550, loss is 5.090779581069946 and perplexity is 162.5165074746697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.151592000325521 and perplexity of 172.70621993236304
Finished 30 epochs...
Completing Train Step...
At time: 882.6735203266144 and batch: 50, loss is 5.144160432815552 and perplexity is 171.42749933388495
At time: 885.1945395469666 and batch: 100, loss is 5.1291764545440675 and perplexity is 168.8779820656121
At time: 887.7407560348511 and batch: 150, loss is 5.14993836402893 and perplexity is 172.42086266203188
At time: 890.263484954834 and batch: 200, loss is 5.171666555404663 and perplexity is 176.2082537365441
At time: 892.7914087772369 and batch: 250, loss is 5.138729181289673 and perplexity is 170.49895732170847
At time: 895.3244245052338 and batch: 300, loss is 5.138127384185791 and perplexity is 170.39638241071526
At time: 897.847846031189 and batch: 350, loss is 5.075939893722534 and perplexity is 160.12261952028467
At time: 900.3636338710785 and batch: 400, loss is 5.095194158554077 and perplexity is 163.23553512327075
At time: 902.8798151016235 and batch: 450, loss is 5.084467477798462 and perplexity is 161.49391723897767
At time: 905.4057106971741 and batch: 500, loss is 5.072234296798706 and perplexity is 159.53036763504332
At time: 907.9288935661316 and batch: 550, loss is 5.090422620773316 and perplexity is 162.45850588670208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.151589965820312 and perplexity of 172.70586856101644
Finished 31 epochs...
Completing Train Step...
At time: 911.9898953437805 and batch: 50, loss is 5.143629789352417 and perplexity is 171.33655658318398
At time: 914.5363545417786 and batch: 100, loss is 5.128841190338135 and perplexity is 168.82137281311367
At time: 917.0640239715576 and batch: 150, loss is 5.149706163406372 and perplexity is 172.38083107823894
At time: 919.5927588939667 and batch: 200, loss is 5.171398849487304 and perplexity is 176.16108805787877
At time: 922.1184418201447 and batch: 250, loss is 5.138490886688232 and perplexity is 170.4583331810764
At time: 924.6369426250458 and batch: 300, loss is 5.138032522201538 and perplexity is 170.38021903842719
At time: 927.152321100235 and batch: 350, loss is 5.07603328704834 and perplexity is 160.13757460259993
At time: 929.6701834201813 and batch: 400, loss is 5.095238819122314 and perplexity is 163.24282547782036
At time: 932.1919388771057 and batch: 450, loss is 5.0844683265686035 and perplexity is 161.49405431025082
At time: 934.7138726711273 and batch: 500, loss is 5.072022218704223 and perplexity is 159.49653832601757
At time: 937.2356503009796 and batch: 550, loss is 5.090077934265136 and perplexity is 162.4025182812244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.151550801595052 and perplexity of 172.69910480192593
Finished 32 epochs...
Completing Train Step...
At time: 941.2977919578552 and batch: 50, loss is 5.143158617019654 and perplexity is 171.25584655379063
At time: 943.8165667057037 and batch: 100, loss is 5.128520946502686 and perplexity is 168.7673174650862
At time: 946.3399276733398 and batch: 150, loss is 5.149497117996216 and perplexity is 172.3447994229604
At time: 948.8597238063812 and batch: 200, loss is 5.171102056503296 and perplexity is 176.10881244078976
At time: 951.4214777946472 and batch: 250, loss is 5.138277244567871 and perplexity is 170.42191999117648
At time: 953.9564294815063 and batch: 300, loss is 5.137990818023682 and perplexity is 170.37311361963313
At time: 956.4775726795197 and batch: 350, loss is 5.076135749816895 and perplexity is 160.1539835824838
At time: 958.9972836971283 and batch: 400, loss is 5.095259265899658 and perplexity is 163.2461633016497
At time: 961.5168130397797 and batch: 450, loss is 5.084474000930786 and perplexity is 161.49497068860526
At time: 964.0348381996155 and batch: 500, loss is 5.0718310546875 and perplexity is 159.4660512412074
At time: 966.558810710907 and batch: 550, loss is 5.0897546482086184 and perplexity is 162.3500242972644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.15157216389974 and perplexity of 172.70279409222775
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 970.6101360321045 and batch: 50, loss is 5.142795219421386 and perplexity is 171.19362389693052
At time: 973.1572823524475 and batch: 100, loss is 5.127868032455444 and perplexity is 168.6571628774499
At time: 975.6751787662506 and batch: 150, loss is 5.148887071609497 and perplexity is 172.23969316390912
At time: 978.1985681056976 and batch: 200, loss is 5.170408973693847 and perplexity is 175.98679673867184
At time: 980.7196846008301 and batch: 250, loss is 5.136647434234619 and perplexity is 170.1443908063617
At time: 983.23450756073 and batch: 300, loss is 5.136338148117066 and perplexity is 170.0917756452946
At time: 985.7490494251251 and batch: 350, loss is 5.074175682067871 and perplexity is 159.8403783685214
At time: 988.2711136341095 and batch: 400, loss is 5.093147382736206 and perplexity is 162.90177026480373
At time: 990.7996921539307 and batch: 450, loss is 5.082464981079101 and perplexity is 161.17084977823322
At time: 993.3168635368347 and batch: 500, loss is 5.070063419342041 and perplexity is 159.18442239451446
At time: 995.833838224411 and batch: 550, loss is 5.088464164733887 and perplexity is 162.1406494002657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.151670328776041 and perplexity of 172.71974827278564
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 999.9106545448303 and batch: 50, loss is 5.142463254928589 and perplexity is 171.1368031241647
At time: 1002.4392132759094 and batch: 100, loss is 5.127270269393921 and perplexity is 168.55637598176324
At time: 1004.9577550888062 and batch: 150, loss is 5.148452577590942 and perplexity is 172.1648723032569
At time: 1007.4757282733917 and batch: 200, loss is 5.169965696334839 and perplexity is 175.90880306388536
At time: 1009.9956653118134 and batch: 250, loss is 5.135976057052613 and perplexity is 170.030198082212
At time: 1012.5192458629608 and batch: 300, loss is 5.13563138961792 and perplexity is 169.97160430828166
At time: 1015.0917127132416 and batch: 350, loss is 5.073257942199707 and perplexity is 159.69375377265996
At time: 1017.6138107776642 and batch: 400, loss is 5.092146339416504 and perplexity is 162.73878012961637
At time: 1020.1333348751068 and batch: 450, loss is 5.081500825881958 and perplexity is 161.01553095354944
At time: 1022.6547548770905 and batch: 500, loss is 5.069259424209594 and perplexity is 159.0564903290182
At time: 1025.1755418777466 and batch: 550, loss is 5.087856159210205 and perplexity is 162.04209695306912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152203877766927 and perplexity of 172.8119273090117
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1029.2274343967438 and batch: 50, loss is 5.142188186645508 and perplexity is 171.08973529128815
At time: 1031.770732164383 and batch: 100, loss is 5.127101554870605 and perplexity is 168.52794047194686
At time: 1034.2868611812592 and batch: 150, loss is 5.148340320587158 and perplexity is 172.14554667527412
At time: 1036.8089847564697 and batch: 200, loss is 5.169818134307861 and perplexity is 175.88284751941546
At time: 1039.3402526378632 and batch: 250, loss is 5.1356706714630125 and perplexity is 169.97828123765237
At time: 1041.870730638504 and batch: 300, loss is 5.135353651046753 and perplexity is 169.92440319285063
At time: 1044.3930387496948 and batch: 350, loss is 5.07281831741333 and perplexity is 159.62356387001293
At time: 1046.9155776500702 and batch: 400, loss is 5.091634817123413 and perplexity is 162.65555690271225
At time: 1049.441641330719 and batch: 450, loss is 5.0809986114501955 and perplexity is 160.93468693238302
At time: 1051.9682459831238 and batch: 500, loss is 5.068856611251831 and perplexity is 158.992433216081
At time: 1054.481663942337 and batch: 550, loss is 5.087637910842895 and perplexity is 162.0067353889158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152647399902344 and perplexity of 172.88859022363047
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1058.5425169467926 and batch: 50, loss is 5.142082185745239 and perplexity is 171.071600586485
At time: 1061.0501158237457 and batch: 100, loss is 5.127049102783203 and perplexity is 168.51910106150828
At time: 1063.5618557929993 and batch: 150, loss is 5.148282909393311 and perplexity is 172.13566387761838
At time: 1066.0968551635742 and batch: 200, loss is 5.169706039428711 and perplexity is 175.8631330578439
At time: 1068.617957353592 and batch: 250, loss is 5.135519256591797 and perplexity is 169.95254594649143
At time: 1071.1362943649292 and batch: 300, loss is 5.135227031707764 and perplexity is 169.90288883933565
At time: 1073.6546840667725 and batch: 350, loss is 5.072562370300293 and perplexity is 159.58271390760572
At time: 1076.1773676872253 and batch: 400, loss is 5.0913967037200925 and perplexity is 162.6168310452449
At time: 1078.755492925644 and batch: 450, loss is 5.08077449798584 and perplexity is 160.8986233434785
At time: 1081.2720248699188 and batch: 500, loss is 5.068671474456787 and perplexity is 158.96300059117414
At time: 1083.787219285965 and batch: 550, loss is 5.0875390529632565 and perplexity is 161.99072053817818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152787272135416 and perplexity of 172.9127742281137
Annealing...
Model not improving. Stopping early with 172.69910480192593loss at 36 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -172.69910480192593
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f2316d9e860>
SETTINGS FOR THIS RUN
{'anneal': 5.722128771470329, 'num_layers': 1, 'dropout': 0.39802325653507376, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 26.98880515309043, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.0263988971710205 and batch: 50, loss is 6.95765887260437 and perplexity is 1051.1697520227713
At time: 5.524892330169678 and batch: 100, loss is 6.193981771469116 and perplexity is 489.7924703385007
At time: 8.030447006225586 and batch: 150, loss is 6.141165361404419 and perplexity is 464.5946762107467
At time: 10.5421462059021 and batch: 200, loss is 6.125329427719116 and perplexity is 457.29533421710255
At time: 13.047447443008423 and batch: 250, loss is 6.066034536361695 and perplexity is 430.96829950303453
At time: 15.556653022766113 and batch: 300, loss is 6.0648040676116945 and perplexity is 430.43833259897883
At time: 18.07638454437256 and batch: 350, loss is 6.041508207321167 and perplexity is 420.52679843927814
At time: 20.598218202590942 and batch: 400, loss is 6.0541465473175045 and perplexity is 425.8752859054742
At time: 23.117063999176025 and batch: 450, loss is 6.030723762512207 and perplexity is 416.0160172447407
At time: 25.62851309776306 and batch: 500, loss is 6.027945680618286 and perplexity is 414.86189454560684
At time: 28.138715744018555 and batch: 550, loss is 6.014020786285401 and perplexity is 409.1250218734825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.528511555989583 and perplexity of 251.76888809893717
Finished 1 epochs...
Completing Train Step...
At time: 32.22563624382019 and batch: 50, loss is 5.848315143585205 and perplexity is 346.6498329829284
At time: 34.76490068435669 and batch: 100, loss is 5.847736501693726 and perplexity is 346.4493048904626
At time: 37.28278303146362 and batch: 150, loss is 5.860088682174682 and perplexity is 350.7552483788994
At time: 39.79690146446228 and batch: 200, loss is 5.825707702636719 and perplexity is 338.9008892828032
At time: 42.310524702072144 and batch: 250, loss is 5.793666505813599 and perplexity is 328.21422028118707
At time: 44.82105541229248 and batch: 300, loss is 5.749795274734497 and perplexity is 314.1263441031471
At time: 47.33026576042175 and batch: 350, loss is 5.79756199836731 and perplexity is 329.4952698719063
At time: 49.83819532394409 and batch: 400, loss is 5.836707468032837 and perplexity is 342.6492975685641
At time: 52.34490489959717 and batch: 450, loss is 5.746445178985596 and perplexity is 313.07575154855357
At time: 54.85094118118286 and batch: 500, loss is 5.827848100662232 and perplexity is 339.6270489351819
At time: 57.35749077796936 and batch: 550, loss is 5.815126562118531 and perplexity is 335.33383637628316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5851598103841145 and perplexity of 266.4428596730055
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 61.39439249038696 and batch: 50, loss is 5.786440114974976 and perplexity is 325.85096523318737
At time: 63.8989794254303 and batch: 100, loss is 5.728213062286377 and perplexity is 307.419437879844
At time: 66.40581774711609 and batch: 150, loss is 5.734968509674072 and perplexity is 309.50322424834025
At time: 68.91198778152466 and batch: 200, loss is 5.73473744392395 and perplexity is 309.43171691543
At time: 71.44309186935425 and batch: 250, loss is 5.706594858169556 and perplexity is 300.8449025921703
At time: 73.95160603523254 and batch: 300, loss is 5.676630249023438 and perplexity is 291.9639246811628
At time: 76.4603590965271 and batch: 350, loss is 5.601997489929199 and perplexity is 270.96712130705595
At time: 78.96802544593811 and batch: 400, loss is 5.580716609954834 and perplexity is 265.26162681502575
At time: 81.48043012619019 and batch: 450, loss is 5.523726139068604 and perplexity is 250.56694718955976
At time: 83.98647403717041 and batch: 500, loss is 5.51806531906128 and perplexity is 249.15253993122914
At time: 86.49306631088257 and batch: 550, loss is 5.510163841247558 and perplexity is 247.19162393405588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3520563761393225 and perplexity of 211.0418333483872
Finished 3 epochs...
Completing Train Step...
At time: 90.50246095657349 and batch: 50, loss is 5.585047521591187 and perplexity is 266.4129428056045
At time: 93.03586435317993 and batch: 100, loss is 5.55294584274292 and perplexity is 257.9964545422573
At time: 95.54039764404297 and batch: 150, loss is 5.574164476394653 and perplexity is 263.5292786934122
At time: 98.04318737983704 and batch: 200, loss is 5.578977088928223 and perplexity is 264.80059973702913
At time: 100.55212354660034 and batch: 250, loss is 5.558253908157349 and perplexity is 259.36955763618596
At time: 103.07266688346863 and batch: 300, loss is 5.540730371475219 and perplexity is 254.86407694929903
At time: 105.58908224105835 and batch: 350, loss is 5.487394361495972 and perplexity is 241.62679382280027
At time: 108.09930348396301 and batch: 400, loss is 5.496513891220093 and perplexity is 243.8403947094459
At time: 110.61190485954285 and batch: 450, loss is 5.461278047561645 and perplexity is 235.39808214174207
At time: 113.12198662757874 and batch: 500, loss is 5.45846022605896 and perplexity is 234.73570603129417
At time: 115.6215980052948 and batch: 550, loss is 5.447936849594116 and perplexity is 232.27844584068663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.335903930664062 and perplexity of 207.66037457511058
Finished 4 epochs...
Completing Train Step...
At time: 119.6932761669159 and batch: 50, loss is 5.516216917037964 and perplexity is 248.6924312362276
At time: 122.1988878250122 and batch: 100, loss is 5.484825868606567 and perplexity is 241.0069734638233
At time: 124.73800706863403 and batch: 150, loss is 5.514778814315796 and perplexity is 248.33504301596452
At time: 127.24039125442505 and batch: 200, loss is 5.524417686462402 and perplexity is 250.74028603796708
At time: 129.7457377910614 and batch: 250, loss is 5.500358972549439 and perplexity is 244.77978571851816
At time: 132.25211429595947 and batch: 300, loss is 5.479192333221436 and perplexity is 239.65306936478092
At time: 134.75546193122864 and batch: 350, loss is 5.434343309402466 and perplexity is 229.14232324779852
At time: 137.27969670295715 and batch: 400, loss is 5.4431602668762205 and perplexity is 231.1715942191769
At time: 139.80386781692505 and batch: 450, loss is 5.412591705322265 and perplexity is 224.2119265654111
At time: 142.31978821754456 and batch: 500, loss is 5.414524583816529 and perplexity is 224.64572007644583
At time: 144.8338921070099 and batch: 550, loss is 5.402100353240967 and perplexity is 221.8719365955275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.319313049316406 and perplexity of 204.24352855898053
Finished 5 epochs...
Completing Train Step...
At time: 148.87301969528198 and batch: 50, loss is 5.469802303314209 and perplexity is 237.41325230818947
At time: 151.4375514984131 and batch: 100, loss is 5.44510407447815 and perplexity is 231.62138433246753
At time: 153.94893288612366 and batch: 150, loss is 5.4775481796264645 and perplexity is 239.25936665179384
At time: 156.46363854408264 and batch: 200, loss is 5.488328123092652 and perplexity is 241.85252101489135
At time: 158.97890067100525 and batch: 250, loss is 5.461258172988892 and perplexity is 235.3934037519231
At time: 161.49646735191345 and batch: 300, loss is 5.44964903831482 and perplexity is 232.6764910427281
At time: 164.01559972763062 and batch: 350, loss is 5.408267297744751 and perplexity is 223.24443623018328
At time: 166.5359070301056 and batch: 400, loss is 5.4149915218353275 and perplexity is 224.7506401976004
At time: 169.05410027503967 and batch: 450, loss is 5.3866456031799315 and perplexity is 218.46932227257767
At time: 171.5691590309143 and batch: 500, loss is 5.400516004562378 and perplexity is 221.5206924060733
At time: 174.0797824859619 and batch: 550, loss is 5.373056497573852 and perplexity is 215.52060020502657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.308564758300781 and perplexity of 202.06001521535998
Finished 6 epochs...
Completing Train Step...
At time: 178.17129063606262 and batch: 50, loss is 5.4370945453643795 and perplexity is 229.77361586718933
At time: 180.69635367393494 and batch: 100, loss is 5.409281234741211 and perplexity is 223.47090681738223
At time: 183.21917533874512 and batch: 150, loss is 5.4440986442565915 and perplexity is 231.3886222253371
At time: 185.73513412475586 and batch: 200, loss is 5.455666723251343 and perplexity is 234.08088622376576
At time: 188.27830576896667 and batch: 250, loss is 5.4286260986328125 and perplexity is 227.83600609303838
At time: 190.79176497459412 and batch: 300, loss is 5.420723485946655 and perplexity is 226.04260200323628
At time: 193.31017637252808 and batch: 350, loss is 5.379741411209107 and perplexity is 216.96616315246348
At time: 195.82744312286377 and batch: 400, loss is 5.389513683319092 and perplexity is 219.096809208076
At time: 198.3413610458374 and batch: 450, loss is 5.357983274459839 and perplexity is 212.29637092187667
At time: 200.85723853111267 and batch: 500, loss is 5.359032983779907 and perplexity is 212.5193374055744
At time: 203.386221408844 and batch: 550, loss is 5.338040084838867 and perplexity is 208.10444328194885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.301866149902343 and perplexity of 200.71101754827666
Finished 7 epochs...
Completing Train Step...
At time: 207.42209577560425 and batch: 50, loss is 5.4013610935211185 and perplexity is 221.70797622195465
At time: 209.96488642692566 and batch: 100, loss is 5.376296262741089 and perplexity is 216.2199686213913
At time: 212.4831666946411 and batch: 150, loss is 5.407033205032349 and perplexity is 222.9691018273814
At time: 215.00171756744385 and batch: 200, loss is 5.419567184448242 and perplexity is 225.78137965883417
At time: 217.50983667373657 and batch: 250, loss is 5.395302600860596 and perplexity is 220.36882080304935
At time: 220.0215973854065 and batch: 300, loss is 5.395477771759033 and perplexity is 220.4074263885658
At time: 222.5371720790863 and batch: 350, loss is 5.3534865570068355 and perplexity is 211.3438772779277
At time: 225.06212949752808 and batch: 400, loss is 5.365520210266113 and perplexity is 213.9024800089982
At time: 227.58020973205566 and batch: 450, loss is 5.338068380355835 and perplexity is 208.11033178806358
At time: 230.10730719566345 and batch: 500, loss is 5.338222417831421 and perplexity is 208.14239104731593
At time: 232.62082862854004 and batch: 550, loss is 5.314564485549926 and perplexity is 203.2759642283104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.303810628255208 and perplexity of 201.10167546692418
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 236.75732922554016 and batch: 50, loss is 5.374491462707519 and perplexity is 215.8300867500025
At time: 239.270516872406 and batch: 100, loss is 5.3365896034240725 and perplexity is 207.80281046391426
At time: 241.78642678260803 and batch: 150, loss is 5.3491341018676755 and perplexity is 210.42601146701568
At time: 244.30037832260132 and batch: 200, loss is 5.340635747909546 and perplexity is 208.6453139555619
At time: 246.8236792087555 and batch: 250, loss is 5.3028434371948245 and perplexity is 200.90726575500727
At time: 249.3490695953369 and batch: 300, loss is 5.287070655822754 and perplexity is 197.76325941886606
At time: 251.9332253932953 and batch: 350, loss is 5.21269136428833 and perplexity is 183.58749471421282
At time: 254.44801688194275 and batch: 400, loss is 5.205528860092163 and perplexity is 182.27724644379268
At time: 256.966915845871 and batch: 450, loss is 5.165579919815063 and perplexity is 175.13899570125278
At time: 259.4786629676819 and batch: 500, loss is 5.1770790004730225 and perplexity is 177.16455686502158
At time: 261.99328684806824 and batch: 550, loss is 5.204945278167725 and perplexity is 182.1709037703808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2006373087565105 and perplexity of 181.38780508302122
Finished 9 epochs...
Completing Train Step...
At time: 266.0529086589813 and batch: 50, loss is 5.304152545928955 and perplexity is 201.17044744049295
At time: 268.6003186702728 and batch: 100, loss is 5.278386030197144 and perplexity is 196.0531959270474
At time: 271.11618208885193 and batch: 150, loss is 5.298193550109863 and perplexity is 199.975238245353
At time: 273.6287543773651 and batch: 200, loss is 5.293211059570313 and perplexity is 198.98134160908535
At time: 276.13550329208374 and batch: 250, loss is 5.265601215362548 and perplexity is 193.56264664864344
At time: 278.640043258667 and batch: 300, loss is 5.253967056274414 and perplexity is 191.32375707618021
At time: 281.142737865448 and batch: 350, loss is 5.190697040557861 and perplexity is 179.593693401011
At time: 283.6471085548401 and batch: 400, loss is 5.2003944206237795 and perplexity is 181.34375348776584
At time: 286.1552429199219 and batch: 450, loss is 5.17147050857544 and perplexity is 176.1737120531205
At time: 288.66144037246704 and batch: 500, loss is 5.184201583862305 and perplexity is 178.43093076806403
At time: 291.17738223075867 and batch: 550, loss is 5.199050683975219 and perplexity is 181.10023888660527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1959279378255205 and perplexity of 180.53559089792793
Finished 10 epochs...
Completing Train Step...
At time: 295.2424051761627 and batch: 50, loss is 5.282560386657715 and perplexity is 196.87330236951456
At time: 297.7618615627289 and batch: 100, loss is 5.255792255401611 and perplexity is 191.67327990795962
At time: 300.2789692878723 and batch: 150, loss is 5.277827587127685 and perplexity is 195.94374194329063
At time: 302.7921612262726 and batch: 200, loss is 5.275042867660522 and perplexity is 195.39885262469815
At time: 305.3053512573242 and batch: 250, loss is 5.252038507461548 and perplexity is 190.95513543826027
At time: 307.82097458839417 and batch: 300, loss is 5.244043292999268 and perplexity is 189.43449520054065
At time: 310.34194803237915 and batch: 350, loss is 5.186462574005127 and perplexity is 178.8348177639049
At time: 312.8584854602814 and batch: 400, loss is 5.201684932708741 and perplexity is 181.5779308650414
At time: 315.4207091331482 and batch: 450, loss is 5.173639364242554 and perplexity is 176.55622206146128
At time: 317.93085265159607 and batch: 500, loss is 5.183952045440674 and perplexity is 178.3864109501636
At time: 320.43529534339905 and batch: 550, loss is 5.192627630233765 and perplexity is 179.940750035531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.194144185384115 and perplexity of 180.21384713782157
Finished 11 epochs...
Completing Train Step...
At time: 324.4844505786896 and batch: 50, loss is 5.268991937637329 and perplexity is 194.22007777951677
At time: 327.03490447998047 and batch: 100, loss is 5.243005657196045 and perplexity is 189.23803313162085
At time: 329.54966497421265 and batch: 150, loss is 5.266124086380005 and perplexity is 193.66388141069328
At time: 332.0663249492645 and batch: 200, loss is 5.265014944076538 and perplexity is 193.44919968546702
At time: 334.58702087402344 and batch: 250, loss is 5.244742307662964 and perplexity is 189.56695898216486
At time: 337.11024498939514 and batch: 300, loss is 5.239092922210693 and perplexity is 188.49904154075202
At time: 339.63063383102417 and batch: 350, loss is 5.184022397994995 and perplexity is 178.39896133130057
At time: 342.14976930618286 and batch: 400, loss is 5.200814924240112 and perplexity is 181.42002522705403
At time: 344.6644821166992 and batch: 450, loss is 5.172531719207764 and perplexity is 176.3607687051865
At time: 347.1818776130676 and batch: 500, loss is 5.1815588092803955 and perplexity is 177.96000059484098
At time: 349.70366382598877 and batch: 550, loss is 5.186739311218262 and perplexity is 178.884314861513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.193071492513021 and perplexity of 180.0206366749943
Finished 12 epochs...
Completing Train Step...
At time: 353.7793185710907 and batch: 50, loss is 5.259076375961303 and perplexity is 192.30379284079493
At time: 356.2935423851013 and batch: 100, loss is 5.232822217941284 and perplexity is 187.32071811618817
At time: 358.8034989833832 and batch: 150, loss is 5.257218723297119 and perplexity is 191.94689079034924
At time: 361.3170733451843 and batch: 200, loss is 5.257323551177978 and perplexity is 191.9670132308263
At time: 363.83468890190125 and batch: 250, loss is 5.239014749526977 and perplexity is 188.4843066407378
At time: 366.35186767578125 and batch: 300, loss is 5.234588756561279 and perplexity is 187.65191985326473
At time: 368.87048053741455 and batch: 350, loss is 5.180960025787353 and perplexity is 177.85347298073435
At time: 371.3855097293854 and batch: 400, loss is 5.19790885925293 and perplexity is 180.89357216767027
At time: 373.9015419483185 and batch: 450, loss is 5.169759759902954 and perplexity is 175.87258076251896
At time: 376.41837453842163 and batch: 500, loss is 5.17809627532959 and perplexity is 177.34487361449303
At time: 378.9615728855133 and batch: 550, loss is 5.180795469284058 and perplexity is 177.82420844302268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.192124938964843 and perplexity of 179.85031812313576
Finished 13 epochs...
Completing Train Step...
At time: 383.026978969574 and batch: 50, loss is 5.251042289733887 and perplexity is 190.76499727237967
At time: 385.5708611011505 and batch: 100, loss is 5.2250621891021725 and perplexity is 185.87272942469284
At time: 388.08852791786194 and batch: 150, loss is 5.250108804702759 and perplexity is 190.58700409287226
At time: 390.6060254573822 and batch: 200, loss is 5.2507638740539555 and perplexity is 190.71189269887302
At time: 393.1310842037201 and batch: 250, loss is 5.233634958267212 and perplexity is 187.47302310150087
At time: 395.65436458587646 and batch: 300, loss is 5.229988451004028 and perplexity is 186.79064626328983
At time: 398.17177963256836 and batch: 350, loss is 5.176986284255982 and perplexity is 177.1481315989714
At time: 400.6871852874756 and batch: 400, loss is 5.194751873016357 and perplexity is 180.32339414569577
At time: 403.2077443599701 and batch: 450, loss is 5.166558179855347 and perplexity is 175.31041101294727
At time: 405.73242259025574 and batch: 500, loss is 5.174592151641845 and perplexity is 176.72452276977302
At time: 408.2525887489319 and batch: 550, loss is 5.175511484146118 and perplexity is 176.88706607207985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.191557312011719 and perplexity of 179.74825920347817
Finished 14 epochs...
Completing Train Step...
At time: 412.3244078159332 and batch: 50, loss is 5.244083404541016 and perplexity is 189.44209386259934
At time: 414.83871722221375 and batch: 100, loss is 5.218527288436889 and perplexity is 184.66202981093988
At time: 417.3609850406647 and batch: 150, loss is 5.244066934585572 and perplexity is 189.43897378544807
At time: 419.8779296875 and batch: 200, loss is 5.244971408843994 and perplexity is 189.61039397166562
At time: 422.3931632041931 and batch: 250, loss is 5.228598852157592 and perplexity is 186.53126245817433
At time: 424.9099838733673 and batch: 300, loss is 5.225657606124878 and perplexity is 185.9834341663223
At time: 427.43221402168274 and batch: 350, loss is 5.17335693359375 and perplexity is 176.50636421413756
At time: 429.94862818717957 and batch: 400, loss is 5.191119451522827 and perplexity is 179.66957177114176
At time: 432.4671802520752 and batch: 450, loss is 5.162884674072266 and perplexity is 174.6675886330101
At time: 434.98017716407776 and batch: 500, loss is 5.170617990493774 and perplexity is 176.02358478028094
At time: 437.5037376880646 and batch: 550, loss is 5.170553121566773 and perplexity is 176.0121666895529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.191304524739583 and perplexity of 179.70282687396218
Finished 15 epochs...
Completing Train Step...
At time: 441.56299233436584 and batch: 50, loss is 5.237643489837646 and perplexity is 188.22602283652645
At time: 444.10686111450195 and batch: 100, loss is 5.212574863433838 and perplexity is 183.56610786002227
At time: 446.6269602775574 and batch: 150, loss is 5.238468103408813 and perplexity is 188.3813005826737
At time: 449.15031003952026 and batch: 200, loss is 5.239614400863648 and perplexity is 188.59736540168703
At time: 451.6655752658844 and batch: 250, loss is 5.224015674591064 and perplexity is 185.67831266385497
At time: 454.18463015556335 and batch: 300, loss is 5.221512041091919 and perplexity is 185.21402366746784
At time: 456.70947194099426 and batch: 350, loss is 5.169745979309082 and perplexity is 175.87015715060966
At time: 459.240136384964 and batch: 400, loss is 5.187594366073609 and perplexity is 179.03733617496613
At time: 461.75623655319214 and batch: 450, loss is 5.159300413131714 and perplexity is 174.04265504904964
At time: 464.26844668388367 and batch: 500, loss is 5.166890630722046 and perplexity is 175.36870280007057
At time: 466.7950394153595 and batch: 550, loss is 5.1658001899719235 and perplexity is 175.17757784439956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.190857442220052 and perplexity of 179.62250283843053
Finished 16 epochs...
Completing Train Step...
At time: 470.8974895477295 and batch: 50, loss is 5.231633720397949 and perplexity is 187.09822014822578
At time: 473.4168391227722 and batch: 100, loss is 5.207052726745605 and perplexity is 182.55522440821213
At time: 475.9385414123535 and batch: 150, loss is 5.23331223487854 and perplexity is 187.4125309338448
At time: 478.4653899669647 and batch: 200, loss is 5.2345179843902585 and perplexity is 187.63863978943556
At time: 480.9917633533478 and batch: 250, loss is 5.21924222946167 and perplexity is 184.79409947715666
At time: 483.5115239620209 and batch: 300, loss is 5.217188081741333 and perplexity is 184.41489470363314
At time: 486.03270959854126 and batch: 350, loss is 5.1660121631622316 and perplexity is 175.21471473031875
At time: 488.555636882782 and batch: 400, loss is 5.184187822341919 and perplexity is 178.4284753040682
At time: 491.07802176475525 and batch: 450, loss is 5.155772714614868 and perplexity is 173.42976671120414
At time: 493.59861493110657 and batch: 500, loss is 5.1632889556884765 and perplexity is 174.73821780411805
At time: 496.11832666397095 and batch: 550, loss is 5.161661729812622 and perplexity is 174.4541104706986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.190422058105469 and perplexity of 179.54431507616295
Finished 17 epochs...
Completing Train Step...
At time: 500.1979410648346 and batch: 50, loss is 5.22656065940857 and perplexity is 186.15146297532323
At time: 502.746285200119 and batch: 100, loss is 5.201748533248901 and perplexity is 181.5894796867774
At time: 505.26240158081055 and batch: 150, loss is 5.2285622882843015 and perplexity is 186.52444227741591
At time: 507.80737566947937 and batch: 200, loss is 5.229873380661011 and perplexity is 186.769153436169
At time: 510.3341829776764 and batch: 250, loss is 5.214904251098633 and perplexity is 183.99420289341893
At time: 512.8552572727203 and batch: 300, loss is 5.213383903503418 and perplexity is 183.7146802891181
At time: 515.3721632957458 and batch: 350, loss is 5.162214517593384 and perplexity is 174.55057323053148
At time: 517.892915725708 and batch: 400, loss is 5.18054045677185 and perplexity is 177.77886682648182
At time: 520.4121825695038 and batch: 450, loss is 5.15232177734375 and perplexity is 172.83230296322836
At time: 522.9364242553711 and batch: 500, loss is 5.1597569465637205 and perplexity is 174.1221294796617
At time: 525.4570524692535 and batch: 550, loss is 5.157298126220703 and perplexity is 173.69452036795545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1902715047200525 and perplexity of 179.51728610639847
Finished 18 epochs...
Completing Train Step...
At time: 529.5042946338654 and batch: 50, loss is 5.221349802017212 and perplexity is 185.18397715307051
At time: 532.0572156906128 and batch: 100, loss is 5.197012548446655 and perplexity is 180.7315079449576
At time: 534.5858716964722 and batch: 150, loss is 5.223922615051269 and perplexity is 185.66103432949797
At time: 537.1080937385559 and batch: 200, loss is 5.225099039077759 and perplexity is 185.8795789564361
At time: 539.627206325531 and batch: 250, loss is 5.210663681030273 and perplexity is 183.215614579769
At time: 542.1526412963867 and batch: 300, loss is 5.209497509002685 and perplexity is 183.00207818931145
At time: 544.680915594101 and batch: 350, loss is 5.1587723350524906 and perplexity is 173.95077120112668
At time: 547.2029287815094 and batch: 400, loss is 5.177375679016113 and perplexity is 177.21712558525968
At time: 549.7282950878143 and batch: 450, loss is 5.148896636962891 and perplexity is 172.24134070532233
At time: 552.2500383853912 and batch: 500, loss is 5.156567525863648 and perplexity is 173.56766543512833
At time: 554.7737231254578 and batch: 550, loss is 5.153480653762817 and perplexity is 173.03271034485107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.190174865722656 and perplexity of 179.4999385740914
Finished 19 epochs...
Completing Train Step...
At time: 558.8649451732635 and batch: 50, loss is 5.2167151737213135 and perplexity is 184.32770403912394
At time: 561.3843376636505 and batch: 100, loss is 5.192661085128784 and perplexity is 179.94677003513186
At time: 563.9017159938812 and batch: 150, loss is 5.219592027664184 and perplexity is 184.85875142789794
At time: 566.4294044971466 and batch: 200, loss is 5.220858497619629 and perplexity is 185.0930177969288
At time: 568.9808292388916 and batch: 250, loss is 5.206664133071899 and perplexity is 182.48429838449974
At time: 571.4995582103729 and batch: 300, loss is 5.205754384994507 and perplexity is 182.31835913779037
At time: 574.0195376873016 and batch: 350, loss is 5.1550184345245365 and perplexity is 173.2990014141328
At time: 576.5466589927673 and batch: 400, loss is 5.1736999607086185 and perplexity is 176.56692106873766
At time: 579.0750844478607 and batch: 450, loss is 5.14565107345581 and perplexity is 171.68322668268704
At time: 581.5961899757385 and batch: 500, loss is 5.152908868789673 and perplexity is 172.93380112131211
At time: 584.1122138500214 and batch: 550, loss is 5.149291725158691 and perplexity is 172.30940467061305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1898040771484375 and perplexity of 179.43339438547
Finished 20 epochs...
Completing Train Step...
At time: 588.1621251106262 and batch: 50, loss is 5.2119440174102785 and perplexity is 183.45034242973833
At time: 590.7086515426636 and batch: 100, loss is 5.188401823043823 and perplexity is 179.18195950068775
At time: 593.2439272403717 and batch: 150, loss is 5.215211601257324 and perplexity is 184.05076223219206
At time: 595.7784357070923 and batch: 200, loss is 5.21649287223816 and perplexity is 184.2867322713408
At time: 598.2981631755829 and batch: 250, loss is 5.2028865718841555 and perplexity is 181.79625316620425
At time: 600.820903301239 and batch: 300, loss is 5.202039346694947 and perplexity is 181.64229602860965
At time: 603.3404512405396 and batch: 350, loss is 5.151635093688965 and perplexity is 172.71366258463092
At time: 605.8743743896484 and batch: 400, loss is 5.170231781005859 and perplexity is 175.95561592769474
At time: 608.4125330448151 and batch: 450, loss is 5.142303466796875 and perplexity is 171.10945967876324
At time: 610.9371819496155 and batch: 500, loss is 5.14940860748291 and perplexity is 172.32954577136218
At time: 613.4677588939667 and batch: 550, loss is 5.14526873588562 and perplexity is 171.61759828185583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.189643351236979 and perplexity of 179.40455710712234
Finished 21 epochs...
Completing Train Step...
At time: 617.6163988113403 and batch: 50, loss is 5.207508344650268 and perplexity is 182.63841878802762
At time: 620.1315305233002 and batch: 100, loss is 5.1841042041778564 and perplexity is 178.41355606631518
At time: 622.663711309433 and batch: 150, loss is 5.210994548797608 and perplexity is 183.27624475083738
At time: 625.1892011165619 and batch: 200, loss is 5.212386589050293 and perplexity is 183.53155031747727
At time: 627.7057321071625 and batch: 250, loss is 5.198946943283081 and perplexity is 181.0814523969554
At time: 630.2258021831512 and batch: 300, loss is 5.198172483444214 and perplexity is 180.94126637573873
At time: 632.7741129398346 and batch: 350, loss is 5.148224582672119 and perplexity is 172.12562406154188
At time: 635.3009474277496 and batch: 400, loss is 5.167122859954834 and perplexity is 175.4094332685965
At time: 637.8291392326355 and batch: 450, loss is 5.138892459869385 and perplexity is 170.52679842216813
At time: 640.3460779190063 and batch: 500, loss is 5.14604489326477 and perplexity is 171.75085225349156
At time: 642.8708004951477 and batch: 550, loss is 5.141548528671264 and perplexity is 170.98033137207287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.189577229817709 and perplexity of 179.39269501535634
Finished 22 epochs...
Completing Train Step...
At time: 646.9223279953003 and batch: 50, loss is 5.2031812000274655 and perplexity is 181.84982334998972
At time: 649.4853637218475 and batch: 100, loss is 5.1801026248931885 and perplexity is 177.70104660856208
At time: 652.0070226192474 and batch: 150, loss is 5.206958961486817 and perplexity is 182.53810787283302
At time: 654.5267407894135 and batch: 200, loss is 5.2084956741333 and perplexity is 182.81883213268273
At time: 657.054995059967 and batch: 250, loss is 5.195358972549439 and perplexity is 180.43290163169374
At time: 659.5814366340637 and batch: 300, loss is 5.194533996582031 and perplexity is 180.28411020723135
At time: 662.1004292964935 and batch: 350, loss is 5.145012788772583 and perplexity is 171.57367887379186
At time: 664.6307923793793 and batch: 400, loss is 5.16391092300415 and perplexity is 174.8469330695575
At time: 667.1557116508484 and batch: 450, loss is 5.135622501373291 and perplexity is 169.97009356579647
At time: 669.6811945438385 and batch: 500, loss is 5.142687549591065 and perplexity is 171.17519250076384
At time: 672.2013955116272 and batch: 550, loss is 5.137651414871216 and perplexity is 170.31529825968784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1892552693684895 and perplexity of 179.33494695947635
Finished 23 epochs...
Completing Train Step...
At time: 676.315691947937 and batch: 50, loss is 5.199065504074096 and perplexity is 181.10292282994033
At time: 678.845871925354 and batch: 100, loss is 5.176039657592773 and perplexity is 176.98051780062417
At time: 681.3715994358063 and batch: 150, loss is 5.203163900375366 and perplexity is 181.8466774385231
At time: 683.903774023056 and batch: 200, loss is 5.204725341796875 and perplexity is 182.13084216859426
At time: 686.4375224113464 and batch: 250, loss is 5.191710004806518 and perplexity is 179.77570756306628
At time: 688.9707410335541 and batch: 300, loss is 5.1910914707183835 and perplexity is 179.66454454232291
At time: 691.4960198402405 and batch: 350, loss is 5.141701154708862 and perplexity is 171.00642941412704
At time: 694.0168414115906 and batch: 400, loss is 5.160523042678833 and perplexity is 174.2555748760845
At time: 696.5662589073181 and batch: 450, loss is 5.132495899200439 and perplexity is 169.43949462008743
At time: 699.0940611362457 and batch: 500, loss is 5.139458074569702 and perplexity is 170.62327816876308
At time: 701.6180090904236 and batch: 550, loss is 5.134108171463013 and perplexity is 169.71289755821064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.18927001953125 and perplexity of 179.3375921986415
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 705.7076256275177 and batch: 50, loss is 5.196523685455322 and perplexity is 180.64317659207933
At time: 708.2557504177094 and batch: 100, loss is 5.169766664505005 and perplexity is 175.87379509689305
At time: 710.7821638584137 and batch: 150, loss is 5.192078323364258 and perplexity is 179.8419344879465
At time: 713.3091330528259 and batch: 200, loss is 5.187992334365845 and perplexity is 179.10860153762695
At time: 715.8324139118195 and batch: 250, loss is 5.167185039520263 and perplexity is 175.42034049002908
At time: 718.3522162437439 and batch: 300, loss is 5.162983713150024 and perplexity is 174.68488840656528
At time: 720.8771057128906 and batch: 350, loss is 5.105201272964478 and perplexity is 164.87725252560085
At time: 723.403317451477 and batch: 400, loss is 5.1133128356933595 and perplexity is 166.22010364958493
At time: 725.9251716136932 and batch: 450, loss is 5.0878527736663814 and perplexity is 162.04154835337724
At time: 728.4476490020752 and batch: 500, loss is 5.101211223602295 and perplexity is 164.22069486911187
At time: 730.9653565883636 and batch: 550, loss is 5.113020248413086 and perplexity is 166.17147687566603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.172883605957031 and perplexity of 176.42283864293825
Finished 25 epochs...
Completing Train Step...
At time: 735.1022696495056 and batch: 50, loss is 5.185519828796386 and perplexity is 178.6663015427131
At time: 737.6361081600189 and batch: 100, loss is 5.159913787841797 and perplexity is 174.14944115873396
At time: 740.161473274231 and batch: 150, loss is 5.183353576660156 and perplexity is 178.27968419184324
At time: 742.683441400528 and batch: 200, loss is 5.181065835952759 and perplexity is 177.87229268176867
At time: 745.2083857059479 and batch: 250, loss is 5.162047243118286 and perplexity is 174.5213778169081
At time: 747.7470715045929 and batch: 300, loss is 5.159370822906494 and perplexity is 174.05490978461535
At time: 750.2745337486267 and batch: 350, loss is 5.103864107131958 and perplexity is 164.65693163257714
At time: 752.8006844520569 and batch: 400, loss is 5.114481363296509 and perplexity is 166.41444995629553
At time: 755.3254587650299 and batch: 450, loss is 5.090970306396485 and perplexity is 162.54750644467606
At time: 757.8487312793732 and batch: 500, loss is 5.1032373046875 and perplexity is 164.55375660389092
At time: 760.4228734970093 and batch: 550, loss is 5.111070680618286 and perplexity is 165.84782990430114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.17110341389974 and perplexity of 176.1090514904278
Finished 26 epochs...
Completing Train Step...
At time: 764.5040047168732 and batch: 50, loss is 5.180437746047974 and perplexity is 177.7606079680865
At time: 767.0797154903412 and batch: 100, loss is 5.15510850906372 and perplexity is 173.3146119448713
At time: 769.6014802455902 and batch: 150, loss is 5.179119997024536 and perplexity is 177.52651837007437
At time: 772.1296494007111 and batch: 200, loss is 5.177787456512451 and perplexity is 177.2901146361344
At time: 774.6517813205719 and batch: 250, loss is 5.160091695785522 and perplexity is 174.18042648389746
At time: 777.1757638454437 and batch: 300, loss is 5.158436260223389 and perplexity is 173.89232054786993
At time: 779.6951620578766 and batch: 350, loss is 5.104112329483033 and perplexity is 164.69780823630114
At time: 782.2153398990631 and batch: 400, loss is 5.115831880569458 and perplexity is 166.6393473751718
At time: 784.7347798347473 and batch: 450, loss is 5.092865409851075 and perplexity is 162.8558428580951
At time: 787.2573339939117 and batch: 500, loss is 5.103954420089722 and perplexity is 164.6718029586158
At time: 789.7808089256287 and batch: 550, loss is 5.109311103820801 and perplexity is 165.55626450197326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.170401509602865 and perplexity of 175.98548316211156
Finished 27 epochs...
Completing Train Step...
At time: 793.921528339386 and batch: 50, loss is 5.176759214401245 and perplexity is 177.10791116505916
At time: 796.4492020606995 and batch: 100, loss is 5.151608934402466 and perplexity is 172.7091445775433
At time: 798.9746963977814 and batch: 150, loss is 5.176238946914673 and perplexity is 177.01579164273932
At time: 801.5038831233978 and batch: 200, loss is 5.175699272155762 and perplexity is 176.920286461247
At time: 804.0303370952606 and batch: 250, loss is 5.159136562347412 and perplexity is 174.01414035965888
At time: 806.5522525310516 and batch: 300, loss is 5.1580812358856205 and perplexity is 173.83059549952
At time: 809.0737464427948 and batch: 350, loss is 5.104566783905029 and perplexity is 164.77267289354603
At time: 811.5967943668365 and batch: 400, loss is 5.116741542816162 and perplexity is 166.79100186501867
At time: 814.121178150177 and batch: 450, loss is 5.0939469814300535 and perplexity is 163.0320783976965
At time: 816.653377532959 and batch: 500, loss is 5.104067497253418 and perplexity is 164.69042463185818
At time: 819.178370475769 and batch: 550, loss is 5.107736616134644 and perplexity is 165.29580330243164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.17007090250651 and perplexity of 175.92731072916223
Finished 28 epochs...
Completing Train Step...
At time: 823.3011717796326 and batch: 50, loss is 5.173805608749389 and perplexity is 176.5855760034248
At time: 825.8901801109314 and batch: 100, loss is 5.148887453079223 and perplexity is 172.23975886815023
At time: 828.4161059856415 and batch: 150, loss is 5.1740320205688475 and perplexity is 176.62556159141187
At time: 830.9421107769012 and batch: 200, loss is 5.174147577285766 and perplexity is 176.64597304075065
At time: 833.4755041599274 and batch: 250, loss is 5.158498544692993 and perplexity is 173.9031516761255
At time: 835.9956049919128 and batch: 300, loss is 5.157929716110229 and perplexity is 173.80425872205544
At time: 838.5219349861145 and batch: 350, loss is 5.104923868179322 and perplexity is 164.8315211301341
At time: 841.0519349575043 and batch: 400, loss is 5.117335262298584 and perplexity is 166.890058335287
At time: 843.5781350135803 and batch: 450, loss is 5.094639663696289 and perplexity is 163.145046948369
At time: 846.1000339984894 and batch: 500, loss is 5.1040357398986815 and perplexity is 164.6851945826682
At time: 848.6244511604309 and batch: 550, loss is 5.106309823989868 and perplexity is 165.06012871801912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169882202148438 and perplexity of 175.89411631463037
Finished 29 epochs...
Completing Train Step...
At time: 852.7606728076935 and batch: 50, loss is 5.171382675170898 and perplexity is 176.15823879574455
At time: 855.2866547107697 and batch: 100, loss is 5.146638078689575 and perplexity is 171.8527625786176
At time: 857.8080494403839 and batch: 150, loss is 5.172270650863648 and perplexity is 176.31473250089286
At time: 860.3301515579224 and batch: 200, loss is 5.172893466949463 and perplexity is 176.42457835579265
At time: 862.8569490909576 and batch: 250, loss is 5.158025913238525 and perplexity is 173.82097899683842
At time: 865.3849232196808 and batch: 300, loss is 5.157749137878418 and perplexity is 173.7728762899115
At time: 867.9072375297546 and batch: 350, loss is 5.105177612304687 and perplexity is 164.87335146717265
At time: 870.4257521629333 and batch: 400, loss is 5.117725820541382 and perplexity is 166.95525135321734
At time: 872.9575276374817 and batch: 450, loss is 5.095030164718628 and perplexity is 163.20876769669076
At time: 875.4909052848816 and batch: 500, loss is 5.103834924697876 and perplexity is 164.65212661263496
At time: 878.011266708374 and batch: 550, loss is 5.104961881637573 and perplexity is 164.83778706537527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169780476888021 and perplexity of 175.87622434989026
Finished 30 epochs...
Completing Train Step...
At time: 882.107522726059 and batch: 50, loss is 5.169278135299683 and perplexity is 175.78789659520916
At time: 884.6574099063873 and batch: 100, loss is 5.144757099151612 and perplexity is 171.52981487289586
At time: 887.1872460842133 and batch: 150, loss is 5.170831823348999 and perplexity is 176.06122843058267
At time: 889.758978843689 and batch: 200, loss is 5.1718419075012205 and perplexity is 176.23915493248313
At time: 892.2786300182343 and batch: 250, loss is 5.157608957290649 and perplexity is 173.74851841326551
At time: 894.8055377006531 and batch: 300, loss is 5.157489395141601 and perplexity is 173.7277459088375
At time: 897.3282775878906 and batch: 350, loss is 5.105336284637451 and perplexity is 164.89951438207098
At time: 899.855409860611 and batch: 400, loss is 5.117908391952515 and perplexity is 166.98573539172529
At time: 902.3846349716187 and batch: 450, loss is 5.095205698013306 and perplexity is 163.23741878394122
At time: 904.9089665412903 and batch: 500, loss is 5.103515272140503 and perplexity is 164.59950355028266
At time: 907.43679022789 and batch: 550, loss is 5.103698120117188 and perplexity is 164.62960298819698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169684855143229 and perplexity of 175.85940756248866
Finished 31 epochs...
Completing Train Step...
At time: 911.5346102714539 and batch: 50, loss is 5.167386503219604 and perplexity is 175.45568488095014
At time: 914.0534961223602 and batch: 100, loss is 5.1430387210845945 and perplexity is 171.23531490478894
At time: 916.5821590423584 and batch: 150, loss is 5.169499683380127 and perplexity is 175.8268463807309
At time: 919.102842092514 and batch: 200, loss is 5.170889701843262 and perplexity is 176.0714188842834
At time: 921.6237576007843 and batch: 250, loss is 5.157201795578003 and perplexity is 173.67778906905627
At time: 924.1459219455719 and batch: 300, loss is 5.1572380638122555 and perplexity is 173.6840881700226
At time: 926.6699323654175 and batch: 350, loss is 5.1054285049438475 and perplexity is 164.91472216703445
At time: 929.1964540481567 and batch: 400, loss is 5.118003759384155 and perplexity is 167.00166115181756
At time: 931.7190518379211 and batch: 450, loss is 5.095240612030029 and perplexity is 163.24311815740393
At time: 934.2353808879852 and batch: 500, loss is 5.103150033950806 and perplexity is 164.53939650294373
At time: 936.7549295425415 and batch: 550, loss is 5.10248966217041 and perplexity is 164.43077519789955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169602966308593 and perplexity of 175.84500723016487
Finished 32 epochs...
Completing Train Step...
At time: 940.8290967941284 and batch: 50, loss is 5.165726318359375 and perplexity is 175.1646376722034
At time: 943.386248588562 and batch: 100, loss is 5.141534719467163 and perplexity is 170.97797028608207
At time: 945.9139750003815 and batch: 150, loss is 5.168306818008423 and perplexity is 175.61723366899542
At time: 948.4422633647919 and batch: 200, loss is 5.170060396194458 and perplexity is 175.92546239164682
At time: 950.9721267223358 and batch: 250, loss is 5.156764574050904 and perplexity is 173.60186999883155
At time: 953.539491891861 and batch: 300, loss is 5.157094068527222 and perplexity is 173.65908028079303
At time: 956.058470249176 and batch: 350, loss is 5.105384283065796 and perplexity is 164.90742948955096
At time: 958.5848486423492 and batch: 400, loss is 5.117886486053467 and perplexity is 166.98207745912867
At time: 961.1106245517731 and batch: 450, loss is 5.0951128101348875 and perplexity is 163.2222567106292
At time: 963.6308312416077 and batch: 500, loss is 5.1026689815521244 and perplexity is 164.46026346667284
At time: 966.1498477458954 and batch: 550, loss is 5.101236295700073 and perplexity is 164.22481227804667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169454956054688 and perplexity of 175.81898229202307
Finished 33 epochs...
Completing Train Step...
At time: 970.344560623169 and batch: 50, loss is 5.164138679504394 and perplexity is 174.88676013037573
At time: 972.8581120967865 and batch: 100, loss is 5.140090293884278 and perplexity is 170.73118360709458
At time: 975.3776640892029 and batch: 150, loss is 5.167189826965332 and perplexity is 175.4211803072834
At time: 977.9014077186584 and batch: 200, loss is 5.169200744628906 and perplexity is 175.7742927783883
At time: 980.4298915863037 and batch: 250, loss is 5.156358613967895 and perplexity is 173.53140887244643
At time: 982.9499216079712 and batch: 300, loss is 5.1567467212677 and perplexity is 173.59877074994787
At time: 985.4757609367371 and batch: 350, loss is 5.1052664852142335 and perplexity is 164.88800489276082
At time: 987.9983315467834 and batch: 400, loss is 5.1177218151092525 and perplexity is 166.9545826266287
At time: 990.5227928161621 and batch: 450, loss is 5.094959001541138 and perplexity is 163.19715365543826
At time: 993.0469946861267 and batch: 500, loss is 5.102150764465332 and perplexity is 164.3750594270474
At time: 995.5683128833771 and batch: 550, loss is 5.10009648323059 and perplexity is 164.03773342687128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169271341959635 and perplexity of 175.78670241230756
Finished 34 epochs...
Completing Train Step...
At time: 999.6586961746216 and batch: 50, loss is 5.162658853530884 and perplexity is 174.6281495568275
At time: 1002.2155842781067 and batch: 100, loss is 5.138754873275757 and perplexity is 170.50333783481918
At time: 1004.7362720966339 and batch: 150, loss is 5.166218347549439 and perplexity is 175.2508449935252
At time: 1007.2579696178436 and batch: 200, loss is 5.16837797164917 and perplexity is 175.62972991912036
At time: 1009.7846269607544 and batch: 250, loss is 5.155933408737183 and perplexity is 173.45763809467277
At time: 1012.3104434013367 and batch: 300, loss is 5.156449565887451 and perplexity is 173.5471926049561
At time: 1014.8375797271729 and batch: 350, loss is 5.105080604553223 and perplexity is 164.85735824981595
At time: 1017.4146287441254 and batch: 400, loss is 5.11753288269043 and perplexity is 166.9230424730721
At time: 1019.9310519695282 and batch: 450, loss is 5.094781541824341 and perplexity is 163.16819530431488
At time: 1022.4453160762787 and batch: 500, loss is 5.101663103103638 and perplexity is 164.29491960387452
At time: 1024.9548280239105 and batch: 550, loss is 5.099058389663696 and perplexity is 163.86753526716464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169232177734375 and perplexity of 175.7798179971088
Finished 35 epochs...
Completing Train Step...
At time: 1029.0166296958923 and batch: 50, loss is 5.161249713897705 and perplexity is 174.3822474061417
At time: 1031.533182144165 and batch: 100, loss is 5.137448492050171 and perplexity is 170.28074090525448
At time: 1034.0400924682617 and batch: 150, loss is 5.165230321884155 and perplexity is 175.0777781721185
At time: 1036.5529117584229 and batch: 200, loss is 5.167628698348999 and perplexity is 175.4981845396503
At time: 1039.0741407871246 and batch: 250, loss is 5.155542459487915 and perplexity is 173.38983821529936
At time: 1041.595113992691 and batch: 300, loss is 5.156091184616089 and perplexity is 173.48500768504988
At time: 1044.1105480194092 and batch: 350, loss is 5.104929227828979 and perplexity is 164.83240457170731
At time: 1046.6264719963074 and batch: 400, loss is 5.117393064498901 and perplexity is 166.89970522667286
At time: 1049.148071527481 and batch: 450, loss is 5.094514741897583 and perplexity is 163.1246678485776
At time: 1051.6733920574188 and batch: 500, loss is 5.1010917282104495 and perplexity is 164.2010724252485
At time: 1054.1938273906708 and batch: 550, loss is 5.098029108047485 and perplexity is 163.69895619817163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169154357910156 and perplexity of 175.766139374812
Finished 36 epochs...
Completing Train Step...
At time: 1058.2847378253937 and batch: 50, loss is 5.1599031257629395 and perplexity is 174.147584373558
At time: 1060.8401508331299 and batch: 100, loss is 5.136257829666138 and perplexity is 170.07811468598015
At time: 1063.361388683319 and batch: 150, loss is 5.164274682998657 and perplexity is 174.910546958363
At time: 1065.8868794441223 and batch: 200, loss is 5.166889362335205 and perplexity is 175.36848036485668
At time: 1068.4131908416748 and batch: 250, loss is 5.155085506439209 and perplexity is 173.31062529978232
At time: 1070.9473898410797 and batch: 300, loss is 5.155795669555664 and perplexity is 173.43374782692408
At time: 1073.4577078819275 and batch: 350, loss is 5.104693212509155 and perplexity is 164.79350618951264
At time: 1075.968454360962 and batch: 400, loss is 5.117171592712403 and perplexity is 166.86274574368267
At time: 1078.4805459976196 and batch: 450, loss is 5.094271802902222 and perplexity is 163.0850433190185
At time: 1081.035272359848 and batch: 500, loss is 5.1005480766296385 and perplexity is 164.1118285136485
At time: 1083.5449028015137 and batch: 550, loss is 5.097062187194824 and perplexity is 163.5407487632249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1690729777018225 and perplexity of 175.75183607178244
Finished 37 epochs...
Completing Train Step...
At time: 1087.5600690841675 and batch: 50, loss is 5.15865686416626 and perplexity is 173.93068611105952
At time: 1090.072738647461 and batch: 100, loss is 5.1350682353973385 and perplexity is 169.87591102950236
At time: 1092.5830445289612 and batch: 150, loss is 5.163384408950805 and perplexity is 174.7548979331347
At time: 1095.093491077423 and batch: 200, loss is 5.166187410354614 and perplexity is 175.24542330785675
At time: 1097.6021416187286 and batch: 250, loss is 5.1547113704681395 and perplexity is 173.24579568898997
At time: 1100.1112267971039 and batch: 300, loss is 5.1553887939453125 and perplexity is 173.36319621875913
At time: 1102.6223633289337 and batch: 350, loss is 5.1044854736328125 and perplexity is 164.7592757273302
At time: 1105.1428072452545 and batch: 400, loss is 5.11692512512207 and perplexity is 166.82162455255158
At time: 1107.6654212474823 and batch: 450, loss is 5.093956031799316 and perplexity is 163.03355390488463
At time: 1110.1928942203522 and batch: 500, loss is 5.100093259811401 and perplexity is 164.0372046653458
At time: 1112.7126393318176 and batch: 550, loss is 5.0961001682281495 and perplexity is 163.38349511371513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.168972778320312 and perplexity of 175.73422672874614
Finished 38 epochs...
Completing Train Step...
At time: 1116.782948255539 and batch: 50, loss is 5.157345399856568 and perplexity is 173.702731733552
At time: 1119.3373029232025 and batch: 100, loss is 5.133942203521729 and perplexity is 169.68473299526542
At time: 1121.8638331890106 and batch: 150, loss is 5.162532701492309 and perplexity is 174.60612124925558
At time: 1124.3863468170166 and batch: 200, loss is 5.165539293289185 and perplexity is 175.13188055684435
At time: 1126.9135646820068 and batch: 250, loss is 5.1543076133728025 and perplexity is 173.1758605890897
At time: 1129.433177471161 and batch: 300, loss is 5.155051755905151 and perplexity is 173.3047760723283
At time: 1131.9528744220734 and batch: 350, loss is 5.1041977596282955 and perplexity is 164.71187899500816
At time: 1134.473887681961 and batch: 400, loss is 5.11659049987793 and perplexity is 166.76581116451285
At time: 1137.0024154186249 and batch: 450, loss is 5.09361403465271 and perplexity is 162.97780642793037
At time: 1139.5281114578247 and batch: 500, loss is 5.099519958496094 and perplexity is 163.9431888724158
At time: 1142.0463044643402 and batch: 550, loss is 5.095202922821045 and perplexity is 163.23696576934853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.168962605794271 and perplexity of 175.7324390768409
Finished 39 epochs...
Completing Train Step...
At time: 1146.1505053043365 and batch: 50, loss is 5.156219978332519 and perplexity is 173.50735290286553
At time: 1148.7043824195862 and batch: 100, loss is 5.132907018661499 and perplexity is 169.50916881502536
At time: 1151.2394123077393 and batch: 150, loss is 5.161796560287476 and perplexity is 174.4776337870478
At time: 1153.7613809108734 and batch: 200, loss is 5.164896364212036 and perplexity is 175.019319366807
At time: 1156.2845206260681 and batch: 250, loss is 5.153868761062622 and perplexity is 173.09987863624988
At time: 1158.8060722351074 and batch: 300, loss is 5.154672327041626 and perplexity is 173.23903171154257
At time: 1161.335401058197 and batch: 350, loss is 5.103910551071167 and perplexity is 164.66457912668875
At time: 1163.8564796447754 and batch: 400, loss is 5.116312665939331 and perplexity is 166.71948439824538
At time: 1166.377722978592 and batch: 450, loss is 5.09327467918396 and perplexity is 162.92250840140875
At time: 1168.9020054340363 and batch: 500, loss is 5.098977041244507 and perplexity is 163.85420544440157
At time: 1171.427386045456 and batch: 550, loss is 5.094370164871216 and perplexity is 163.10108547394884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.168925476074219 and perplexity of 175.72591430170618
Finished 40 epochs...
Completing Train Step...
At time: 1175.5043604373932 and batch: 50, loss is 5.155138654708862 and perplexity is 173.31983670441238
At time: 1178.0266118049622 and batch: 100, loss is 5.131904611587524 and perplexity is 169.33933675974922
At time: 1180.5527095794678 and batch: 150, loss is 5.16098843574524 and perplexity is 174.33669108641132
At time: 1183.0730426311493 and batch: 200, loss is 5.164255447387696 and perplexity is 174.9071824794876
At time: 1185.5902841091156 and batch: 250, loss is 5.1534356594085695 and perplexity is 173.02492502493442
At time: 1188.1138045787811 and batch: 300, loss is 5.1543647575378415 and perplexity is 173.1857568618024
At time: 1190.64324092865 and batch: 350, loss is 5.103602199554444 and perplexity is 164.61381238136806
At time: 1193.1628398895264 and batch: 400, loss is 5.115881013870239 and perplexity is 166.64753511749205
At time: 1195.6835820674896 and batch: 450, loss is 5.092881946563721 and perplexity is 162.85853598063883
At time: 1198.2053644657135 and batch: 500, loss is 5.098394346237183 and perplexity is 163.7587562285515
At time: 1200.7315254211426 and batch: 550, loss is 5.09346682548523 and perplexity is 162.95381636655023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.168875122070313 and perplexity of 175.71706602110606
Finished 41 epochs...
Completing Train Step...
At time: 1204.807805299759 and batch: 50, loss is 5.1540765380859375 and perplexity is 173.13584855050067
At time: 1207.35688996315 and batch: 100, loss is 5.1308860588073735 and perplexity is 169.16694351821553
At time: 1209.8787026405334 and batch: 150, loss is 5.1602832794189455 and perplexity is 174.21379979966295
At time: 1212.40651845932 and batch: 200, loss is 5.163641195297242 and perplexity is 174.7997783669915
At time: 1214.9289319515228 and batch: 250, loss is 5.153127899169922 and perplexity is 172.97168302602216
At time: 1217.4579327106476 and batch: 300, loss is 5.153960781097412 and perplexity is 173.11580802600443
At time: 1219.9769673347473 and batch: 350, loss is 5.103313817977905 and perplexity is 164.56634763494162
At time: 1222.4989936351776 and batch: 400, loss is 5.115567092895508 and perplexity is 166.59522917122777
At time: 1225.0293018817902 and batch: 450, loss is 5.092522230148315 and perplexity is 162.79996362720777
At time: 1227.5465774536133 and batch: 500, loss is 5.0978529834747315 and perplexity is 163.6701273282617
At time: 1230.0668363571167 and batch: 550, loss is 5.092642335891724 and perplexity is 162.81951801213958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.16881103515625 and perplexity of 175.70580521743557
Finished 42 epochs...
Completing Train Step...
At time: 1234.146859407425 and batch: 50, loss is 5.153075256347656 and perplexity is 172.96257754812675
At time: 1236.6706590652466 and batch: 100, loss is 5.129956750869751 and perplexity is 169.0098083596045
At time: 1239.189804315567 and batch: 150, loss is 5.159523687362671 and perplexity is 174.0815186274658
At time: 1241.7084481716156 and batch: 200, loss is 5.162994785308838 and perplexity is 174.68682255609968
At time: 1244.2320349216461 and batch: 250, loss is 5.152698659896851 and perplexity is 172.8974527189594
At time: 1246.76314163208 and batch: 300, loss is 5.1535799694061275 and perplexity is 173.0498960531834
At time: 1249.2793304920197 and batch: 350, loss is 5.1029689979553225 and perplexity is 164.50961164564856
At time: 1251.7991156578064 and batch: 400, loss is 5.115172872543335 and perplexity is 166.52956688486876
At time: 1254.3219065666199 and batch: 450, loss is 5.092090311050415 and perplexity is 162.72966239709464
At time: 1256.845764875412 and batch: 500, loss is 5.097268648147583 and perplexity is 163.57451702782356
At time: 1259.377913236618 and batch: 550, loss is 5.091774950027466 and perplexity is 162.6783518953882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.168783569335938 and perplexity of 175.70097937963473
Finished 43 epochs...
Completing Train Step...
At time: 1263.438675403595 and batch: 50, loss is 5.152087411880493 and perplexity is 172.7918017867025
At time: 1265.985019683838 and batch: 100, loss is 5.129027414321899 and perplexity is 168.85281432919479
At time: 1268.5116238594055 and batch: 150, loss is 5.158852434158325 and perplexity is 173.96470506039648
At time: 1271.0619592666626 and batch: 200, loss is 5.162355012893677 and perplexity is 174.57509848853528
At time: 1273.5768504142761 and batch: 250, loss is 5.152276611328125 and perplexity is 172.82449699301577
At time: 1276.094059228897 and batch: 300, loss is 5.1530958271026615 and perplexity is 172.96613555552986
At time: 1278.609619140625 and batch: 350, loss is 5.102629222869873 and perplexity is 164.45372487329848
At time: 1281.1296181678772 and batch: 400, loss is 5.114768371582032 and perplexity is 166.4622191370111
At time: 1283.6509654521942 and batch: 450, loss is 5.091677665710449 and perplexity is 162.6625266128191
At time: 1286.1627826690674 and batch: 500, loss is 5.096706447601318 and perplexity is 163.4825811906059
At time: 1288.6783316135406 and batch: 550, loss is 5.090975656509399 and perplexity is 162.54837609451587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1687876383463545 and perplexity of 175.70169431020457
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1292.7166819572449 and batch: 50, loss is 5.151200141906738 and perplexity is 172.63855680415546
At time: 1295.231338262558 and batch: 100, loss is 5.1277288150787355 and perplexity is 168.6336845040077
At time: 1297.7403576374054 and batch: 150, loss is 5.156414194107056 and perplexity is 173.54105404033768
At time: 1300.2521243095398 and batch: 200, loss is 5.159271020889282 and perplexity is 174.0375396203163
At time: 1302.767349243164 and batch: 250, loss is 5.146444416046142 and perplexity is 171.81948434081514
At time: 1305.2815194129944 and batch: 300, loss is 5.145605154037476 and perplexity is 171.67534326978225
At time: 1307.79501247406 and batch: 350, loss is 5.094282627105713 and perplexity is 163.08680859426752
At time: 1310.3118057250977 and batch: 400, loss is 5.105044651031494 and perplexity is 164.85143115375462
At time: 1312.8254895210266 and batch: 450, loss is 5.082023057937622 and perplexity is 161.0996403856825
At time: 1315.3399834632874 and batch: 500, loss is 5.0886454772949214 and perplexity is 162.170050201943
At time: 1317.8515856266022 and batch: 550, loss is 5.086436948776245 and perplexity is 161.81228823037364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169123840332031 and perplexity of 175.76077549976858
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1321.8862211704254 and batch: 50, loss is 5.150288763046265 and perplexity is 172.48128934898426
At time: 1324.427666425705 and batch: 100, loss is 5.1268362712860105 and perplexity is 168.4832387053787
At time: 1326.936435699463 and batch: 150, loss is 5.155616235733032 and perplexity is 173.4026307383908
At time: 1329.4519882202148 and batch: 200, loss is 5.158497343063354 and perplexity is 173.90294270906978
At time: 1331.9671080112457 and batch: 250, loss is 5.145232458114624 and perplexity is 171.61137249085607
At time: 1334.5069200992584 and batch: 300, loss is 5.1444294643402095 and perplexity is 171.47362493974185
At time: 1337.0211136341095 and batch: 350, loss is 5.092644376754761 and perplexity is 162.81985030481462
At time: 1339.545684337616 and batch: 400, loss is 5.103309001922607 and perplexity is 164.5655550762198
At time: 1342.0695705413818 and batch: 450, loss is 5.080326557159424 and perplexity is 160.82656642099235
At time: 1344.5924355983734 and batch: 500, loss is 5.087156581878662 and perplexity is 161.92877561842514
At time: 1347.1108903884888 and batch: 550, loss is 5.085595922470093 and perplexity is 161.6762570502319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.170213826497396 and perplexity of 175.95245675946018
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1351.1836647987366 and batch: 50, loss is 5.149943904876709 and perplexity is 172.42181802243255
At time: 1353.701809644699 and batch: 100, loss is 5.126632347106933 and perplexity is 168.44888440219302
At time: 1356.2217304706573 and batch: 150, loss is 5.1554982948303225 and perplexity is 173.38218068156203
At time: 1358.7645120620728 and batch: 200, loss is 5.158219223022461 and perplexity is 173.85458354067072
At time: 1361.2863321304321 and batch: 250, loss is 5.144903583526611 and perplexity is 171.55494315102646
At time: 1363.8071029186249 and batch: 300, loss is 5.144205083847046 and perplexity is 171.43515391944803
At time: 1366.3349940776825 and batch: 350, loss is 5.092235946655274 and perplexity is 162.75336335571606
At time: 1368.8603851795197 and batch: 400, loss is 5.1029582023620605 and perplexity is 164.5078356763799
At time: 1371.3820362091064 and batch: 450, loss is 5.080013151168823 and perplexity is 160.77617030926248
At time: 1373.9069736003876 and batch: 500, loss is 5.086875162124634 and perplexity is 161.883212073762
At time: 1376.4244978427887 and batch: 550, loss is 5.085456495285034 and perplexity is 161.65371655623585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.170246887207031 and perplexity of 175.95827396870263
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1380.5030870437622 and batch: 50, loss is 5.149902105331421 and perplexity is 172.4146110194672
At time: 1383.0499999523163 and batch: 100, loss is 5.12659176826477 and perplexity is 168.44204908018602
At time: 1385.5740413665771 and batch: 150, loss is 5.155445232391357 and perplexity is 173.37298084426698
At time: 1388.0988302230835 and batch: 200, loss is 5.15815468788147 and perplexity is 173.8433641726356
At time: 1390.6184203624725 and batch: 250, loss is 5.144845571517944 and perplexity is 171.54499119284682
At time: 1393.138795375824 and batch: 300, loss is 5.144158554077149 and perplexity is 171.42717726676116
At time: 1395.6566083431244 and batch: 350, loss is 5.092167615890503 and perplexity is 162.7422426738756
At time: 1398.2084929943085 and batch: 400, loss is 5.102898473739624 and perplexity is 164.49801014341062
At time: 1400.73037981987 and batch: 450, loss is 5.07995379447937 and perplexity is 160.76662745126896
At time: 1403.2544212341309 and batch: 500, loss is 5.0868252086639405 and perplexity is 161.8751256490649
At time: 1405.779171705246 and batch: 550, loss is 5.085434408187866 and perplexity is 161.65014613432098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.170256042480469 and perplexity of 175.95988492218885
Annealing...
Model not improving. Stopping early with 175.70097937963473loss at 47 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f2316d9e860>
SETTINGS FOR THIS RUN
{'anneal': 3.8276469565730373, 'num_layers': 1, 'dropout': 0.8979697863465091, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 17.403461512516476, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.011042833328247 and batch: 50, loss is 7.772159290313721 and perplexity is 2373.591030982707
At time: 5.511240005493164 and batch: 100, loss is 7.055845985412597 and perplexity is 1159.618076409057
At time: 8.020387649536133 and batch: 150, loss is 6.98544153213501 and perplexity is 1080.7835130502838
At time: 10.529562950134277 and batch: 200, loss is 6.926741695404052 and perplexity is 1019.1678045779275
At time: 13.032907962799072 and batch: 250, loss is 6.812788219451904 and perplexity is 909.4028901332604
At time: 15.541221618652344 and batch: 300, loss is 6.793237762451172 and perplexity is 891.7963171035069
At time: 18.060728311538696 and batch: 350, loss is 6.735360202789306 and perplexity is 841.6465929439121
At time: 20.57688069343567 and batch: 400, loss is 6.741236438751221 and perplexity is 846.606866516325
At time: 23.08320689201355 and batch: 450, loss is 6.718259410858154 and perplexity is 827.3761355060052
At time: 25.590775728225708 and batch: 500, loss is 6.6735336875915525 and perplexity is 791.1864763579275
At time: 28.099417209625244 and batch: 550, loss is 6.626280908584595 and perplexity is 754.6702573261034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.0032597859700525 and perplexity of 404.74603080309384
Finished 1 epochs...
Completing Train Step...
At time: 32.16537857055664 and batch: 50, loss is 6.159293556213379 and perplexity is 473.09374262031804
At time: 34.69882583618164 and batch: 100, loss is 5.933866720199585 and perplexity is 377.61181375130445
At time: 37.2103533744812 and batch: 150, loss is 5.862172660827636 and perplexity is 351.4869770175412
At time: 39.72804260253906 and batch: 200, loss is 5.817057113647461 and perplexity is 335.9818409286303
At time: 42.25055980682373 and batch: 250, loss is 5.724723625183105 and perplexity is 306.3485865078661
At time: 44.76286339759827 and batch: 300, loss is 5.732776136398315 and perplexity is 308.82542092117643
At time: 47.27837657928467 and batch: 350, loss is 5.7059948539733885 and perplexity is 300.66444853023035
At time: 49.79435849189758 and batch: 400, loss is 5.6832665348052975 and perplexity is 293.90792405652843
At time: 52.326571226119995 and batch: 450, loss is 5.639307384490967 and perplexity is 281.2678405267847
At time: 54.85057973861694 and batch: 500, loss is 5.63526032447815 and perplexity is 280.13183299235715
At time: 57.36681032180786 and batch: 550, loss is 5.601765155792236 and perplexity is 270.904173707503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.412865193684896 and perplexity of 224.27325430391878
Finished 2 epochs...
Completing Train Step...
At time: 61.447827100753784 and batch: 50, loss is 5.645094509124756 and perplexity is 282.90029161834593
At time: 63.96297645568848 and batch: 100, loss is 5.5966410636901855 and perplexity is 269.51958617712336
At time: 66.48630738258362 and batch: 150, loss is 5.628264884948731 and perplexity is 278.1790260294732
At time: 69.0015959739685 and batch: 200, loss is 5.613071146011353 and perplexity is 273.9843933012654
At time: 71.54773879051208 and batch: 250, loss is 5.581899156570435 and perplexity is 265.57549660031407
At time: 74.0674102306366 and batch: 300, loss is 5.571307344436645 and perplexity is 262.77741536722283
At time: 76.58905339241028 and batch: 350, loss is 5.51863995552063 and perplexity is 249.29575320845785
At time: 79.10393452644348 and batch: 400, loss is 5.5225475025177 and perplexity is 250.27179380060844
At time: 81.62163615226746 and batch: 450, loss is 5.500810022354126 and perplexity is 244.89021849652715
At time: 84.14474773406982 and batch: 500, loss is 5.465380802154541 and perplexity is 236.365846596941
At time: 86.66767930984497 and batch: 550, loss is 5.438326969146728 and perplexity is 230.0569689055873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.296315511067708 and perplexity of 199.60002937919492
Finished 3 epochs...
Completing Train Step...
At time: 90.68517851829529 and batch: 50, loss is 5.480629148483277 and perplexity is 239.99765404537422
At time: 93.232257604599 and batch: 100, loss is 5.463130197525024 and perplexity is 235.83447870214368
At time: 95.75370192527771 and batch: 150, loss is 5.512478189468384 and perplexity is 247.7643739449477
At time: 98.27214741706848 and batch: 200, loss is 5.535899000167847 and perplexity is 253.63570371365384
At time: 100.79920506477356 and batch: 250, loss is 5.483475694656372 and perplexity is 240.68179170180886
At time: 103.32155799865723 and batch: 300, loss is 5.507476673126221 and perplexity is 246.52827015309146
At time: 105.83944368362427 and batch: 350, loss is 5.491490278244019 and perplexity is 242.618506654536
At time: 108.35926985740662 and batch: 400, loss is 5.5162794876098635 and perplexity is 248.7079925507123
At time: 110.8845489025116 and batch: 450, loss is 5.525688419342041 and perplexity is 251.0591124924124
At time: 113.41250920295715 and batch: 500, loss is 5.471530714035034 and perplexity is 237.82395474775024
At time: 115.93087482452393 and batch: 550, loss is 5.4373565292358395 and perplexity is 229.83382073464136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.335553487141927 and perplexity of 207.58761409200113
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 120.01547646522522 and batch: 50, loss is 5.44124547958374 and perplexity is 230.72937330279282
At time: 122.53468704223633 and batch: 100, loss is 5.367339448928833 and perplexity is 214.29197385439014
At time: 125.08198189735413 and batch: 150, loss is 5.360890350341797 and perplexity is 212.91443051941263
At time: 127.60338473320007 and batch: 200, loss is 5.36558892250061 and perplexity is 213.91717823133212
At time: 130.12709736824036 and batch: 250, loss is 5.311771211624145 and perplexity is 202.70895105791658
At time: 132.64599609375 and batch: 300, loss is 5.293861923217773 and perplexity is 199.11089348659192
At time: 135.1697554588318 and batch: 350, loss is 5.247433624267578 and perplexity is 190.07783083715125
At time: 137.68881964683533 and batch: 400, loss is 5.2408823394775395 and perplexity is 188.8366469488576
At time: 140.20548462867737 and batch: 450, loss is 5.234586963653564 and perplexity is 187.65158341099155
At time: 142.72952604293823 and batch: 500, loss is 5.201481409072876 and perplexity is 181.54097922475265
At time: 145.24937081336975 and batch: 550, loss is 5.200498304367065 and perplexity is 181.3625931342492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.167212931315104 and perplexity of 175.4252333464119
Finished 5 epochs...
Completing Train Step...
At time: 149.29045152664185 and batch: 50, loss is 5.284268455505371 and perplexity is 197.2098628766597
At time: 151.8453242778778 and batch: 100, loss is 5.246205520629883 and perplexity is 189.8445388443491
At time: 154.3654329776764 and batch: 150, loss is 5.2548047637939455 and perplexity is 191.48409757599254
At time: 156.88843607902527 and batch: 200, loss is 5.2608763694763185 and perplexity is 192.65025013772225
At time: 159.40761494636536 and batch: 250, loss is 5.219092264175415 and perplexity is 184.76638885499798
At time: 161.9334955215454 and batch: 300, loss is 5.21649715423584 and perplexity is 184.28752138839033
At time: 164.46418285369873 and batch: 350, loss is 5.184592390060425 and perplexity is 178.50067630935177
At time: 166.9852991104126 and batch: 400, loss is 5.207069511413574 and perplexity is 182.55828856275522
At time: 169.50981163978577 and batch: 450, loss is 5.202899332046509 and perplexity is 181.79857293071015
At time: 172.03646755218506 and batch: 500, loss is 5.169025344848633 and perplexity is 175.7434647097545
At time: 174.56036114692688 and batch: 550, loss is 5.149453840255737 and perplexity is 172.33734089085354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.152113342285157 and perplexity of 172.79628240613727
Finished 6 epochs...
Completing Train Step...
At time: 178.662207365036 and batch: 50, loss is 5.220523376464843 and perplexity is 185.03099960344667
At time: 181.18755722045898 and batch: 100, loss is 5.186768045425415 and perplexity is 178.88945503432174
At time: 183.71668457984924 and batch: 150, loss is 5.201061763763428 and perplexity is 181.46481238699414
At time: 186.2537751197815 and batch: 200, loss is 5.207247619628906 and perplexity is 182.59080658950305
At time: 188.80750679969788 and batch: 250, loss is 5.172376832962036 and perplexity is 176.3334549631442
At time: 191.33589243888855 and batch: 300, loss is 5.170585947036743 and perplexity is 176.01794446647366
At time: 193.86193704605103 and batch: 350, loss is 5.138728294372559 and perplexity is 170.49880610333238
At time: 196.39228105545044 and batch: 400, loss is 5.15705153465271 and perplexity is 173.6516940443482
At time: 198.91764187812805 and batch: 450, loss is 5.154280166625977 and perplexity is 173.17110754031597
At time: 201.4415512084961 and batch: 500, loss is 5.129091033935547 and perplexity is 168.86355702172506
At time: 203.962562084198 and batch: 550, loss is 5.108876791000366 and perplexity is 165.4843769057791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.156975301106771 and perplexity of 173.63845646453314
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 208.05422592163086 and batch: 50, loss is 5.171606454849243 and perplexity is 176.19766384085753
At time: 210.61100029945374 and batch: 100, loss is 5.123214740753173 and perplexity is 167.8741750491299
At time: 213.13653802871704 and batch: 150, loss is 5.12062141418457 and perplexity is 167.4393865088044
At time: 215.65810537338257 and batch: 200, loss is 5.1097258186340335 and perplexity is 165.62493737613053
At time: 218.18565917015076 and batch: 250, loss is 5.0681508445739745 and perplexity is 158.88026124296232
At time: 220.71042442321777 and batch: 300, loss is 5.05685977935791 and perplexity is 157.09642351724523
At time: 223.24237632751465 and batch: 350, loss is 5.006821918487549 and perplexity is 149.42908291119272
At time: 225.7652690410614 and batch: 400, loss is 5.009671182632446 and perplexity is 149.85545297134874
At time: 228.293940782547 and batch: 450, loss is 5.000284166336059 and perplexity is 148.4553391190083
At time: 230.81391167640686 and batch: 500, loss is 4.984096174240112 and perplexity is 146.07149216779732
At time: 233.33517384529114 and batch: 550, loss is 5.001955480575561 and perplexity is 148.70366209626874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0661565144856775 and perplexity of 158.56371730899212
Finished 8 epochs...
Completing Train Step...
At time: 237.43823289871216 and batch: 50, loss is 5.101651544570923 and perplexity is 164.29302060664622
At time: 239.9594955444336 and batch: 100, loss is 5.064075860977173 and perplexity is 158.2341441369368
At time: 242.48272275924683 and batch: 150, loss is 5.074733266830444 and perplexity is 159.92952777981057
At time: 245.0012857913971 and batch: 200, loss is 5.068731632232666 and perplexity is 158.97256373938274
At time: 247.5298089981079 and batch: 250, loss is 5.036832399368286 and perplexity is 153.9814898435281
At time: 250.05624222755432 and batch: 300, loss is 5.029188728332519 and perplexity is 152.80899278911255
At time: 252.60230946540833 and batch: 350, loss is 4.986542997360229 and perplexity is 146.42934088975164
At time: 255.128112077713 and batch: 400, loss is 5.002519645690918 and perplexity is 148.78757918427027
At time: 257.64895510673523 and batch: 450, loss is 4.999446105957031 and perplexity is 148.33097670012424
At time: 260.17206025123596 and batch: 500, loss is 4.980059614181519 and perplexity is 145.48305424830644
At time: 262.6978795528412 and batch: 550, loss is 4.989951210021973 and perplexity is 146.92925464570646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.061049397786459 and perplexity of 157.75597826487675
Finished 9 epochs...
Completing Train Step...
At time: 266.75541377067566 and batch: 50, loss is 5.075881443023682 and perplexity is 160.1132605147945
At time: 269.31071043014526 and batch: 100, loss is 5.039424085617066 and perplexity is 154.38107913469298
At time: 271.82922887802124 and batch: 150, loss is 5.053762578964234 and perplexity is 156.6106171207113
At time: 274.35047936439514 and batch: 200, loss is 5.050341463088989 and perplexity is 156.07574949598444
At time: 276.878760099411 and batch: 250, loss is 5.0220011043548585 and perplexity is 151.71459697289092
At time: 279.4031147956848 and batch: 300, loss is 5.016481018066406 and perplexity is 150.8794265335581
At time: 281.92560935020447 and batch: 350, loss is 4.977225847244263 and perplexity is 145.0713727594341
At time: 284.4548296928406 and batch: 400, loss is 4.995977468490601 and perplexity is 147.81736160415346
At time: 286.98170018196106 and batch: 450, loss is 4.992938299179077 and perplexity is 147.3688015852398
At time: 289.5052354335785 and batch: 500, loss is 4.974230327606201 and perplexity is 144.63745883671507
At time: 292.0314824581146 and batch: 550, loss is 4.977675294876098 and perplexity is 145.1365893990022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.059601338704427 and perplexity of 157.52770360529115
Finished 10 epochs...
Completing Train Step...
At time: 296.1771020889282 and batch: 50, loss is 5.058368310928345 and perplexity is 157.3335872712531
At time: 298.70546436309814 and batch: 100, loss is 5.022412967681885 and perplexity is 151.7770955211052
At time: 301.2248501777649 and batch: 150, loss is 5.039597129821777 and perplexity is 154.40779619730407
At time: 303.7443006038666 and batch: 200, loss is 5.03724534034729 and perplexity is 154.04508824098107
At time: 306.2654814720154 and batch: 250, loss is 5.010694189071655 and perplexity is 150.0088345064614
At time: 308.78101682662964 and batch: 300, loss is 5.006210622787475 and perplexity is 149.33776546916667
At time: 311.3110272884369 and batch: 350, loss is 4.968210592269897 and perplexity is 143.76939499344786
At time: 313.845148563385 and batch: 400, loss is 4.98839093208313 and perplexity is 146.70018292536025
At time: 316.39613246917725 and batch: 450, loss is 4.984946193695069 and perplexity is 146.1957085636518
At time: 318.92344903945923 and batch: 500, loss is 4.965547637939453 and perplexity is 143.38705296608612
At time: 321.44590640068054 and batch: 550, loss is 4.9662296581268315 and perplexity is 143.48487918675434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.057914225260417 and perplexity of 157.2621605623392
Finished 11 epochs...
Completing Train Step...
At time: 325.5263931751251 and batch: 50, loss is 5.042830829620361 and perplexity is 154.90791283449946
At time: 328.08425545692444 and batch: 100, loss is 5.00792818069458 and perplexity is 149.5944821288323
At time: 330.60543060302734 and batch: 150, loss is 5.026828804016113 and perplexity is 152.44880031192338
At time: 333.1284222602844 and batch: 200, loss is 5.025133848190308 and perplexity is 152.1906251892205
At time: 335.64871764183044 and batch: 250, loss is 5.0005850315094 and perplexity is 148.50001088009722
At time: 338.17459893226624 and batch: 300, loss is 4.996699457168579 and perplexity is 147.9241226011182
At time: 340.6977286338806 and batch: 350, loss is 4.959320697784424 and perplexity is 142.49696450812095
At time: 343.21772027015686 and batch: 400, loss is 4.980619049072265 and perplexity is 145.56446531483763
At time: 345.7382833957672 and batch: 450, loss is 4.9769477367401125 and perplexity is 145.0310324966036
At time: 348.264529466629 and batch: 500, loss is 4.957323369979858 and perplexity is 142.21263540257996
At time: 350.78698229789734 and batch: 550, loss is 4.955701112747192 and perplexity is 141.98211695688457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.05734608968099 and perplexity of 157.17283970920604
Finished 12 epochs...
Completing Train Step...
At time: 354.89088344573975 and batch: 50, loss is 5.030237741470337 and perplexity is 152.9693755372225
At time: 357.41679191589355 and batch: 100, loss is 4.995355882644653 and perplexity is 147.7255089744997
At time: 359.9425804615021 and batch: 150, loss is 5.015765371322632 and perplexity is 150.7714887905108
At time: 362.4664149284363 and batch: 200, loss is 5.014875860214233 and perplexity is 150.63743550618344
At time: 364.9870538711548 and batch: 250, loss is 4.9914443302154545 and perplexity is 147.14880154699878
At time: 367.5090973377228 and batch: 300, loss is 4.98746322631836 and perplexity is 146.56415142832572
At time: 370.0363473892212 and batch: 350, loss is 4.950525236129761 and perplexity is 141.24913358973575
At time: 372.5618212223053 and batch: 400, loss is 4.9728991985321045 and perplexity is 144.44505579510536
At time: 375.0838313102722 and batch: 450, loss is 4.968108348846435 and perplexity is 143.7546962697514
At time: 377.60340094566345 and batch: 500, loss is 4.94868655204773 and perplexity is 140.9896596745839
At time: 380.15713381767273 and batch: 550, loss is 4.945746650695801 and perplexity is 140.57577267507608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.056416829427083 and perplexity of 157.02685307656355
Finished 13 epochs...
Completing Train Step...
At time: 384.2087416648865 and batch: 50, loss is 5.0179305744171145 and perplexity is 151.09829335605355
At time: 386.7782382965088 and batch: 100, loss is 4.98393590927124 and perplexity is 146.0480839004647
At time: 389.30096912384033 and batch: 150, loss is 5.005348262786865 and perplexity is 149.20903806630463
At time: 391.82554626464844 and batch: 200, loss is 5.004969654083252 and perplexity is 149.15255691863672
At time: 394.34446024894714 and batch: 250, loss is 4.983026933670044 and perplexity is 145.91539007246013
At time: 396.86323046684265 and batch: 300, loss is 4.978956079483032 and perplexity is 145.32259720114476
At time: 399.3805432319641 and batch: 350, loss is 4.942161226272583 and perplexity is 140.0726513571246
At time: 401.90533781051636 and batch: 400, loss is 4.96569034576416 and perplexity is 143.40751688065166
At time: 404.42551732063293 and batch: 450, loss is 4.959979610443115 and perplexity is 142.59088850231657
At time: 406.94298219680786 and batch: 500, loss is 4.9409119987487795 and perplexity is 139.89777799676642
At time: 409.4715209007263 and batch: 550, loss is 4.936206531524658 and perplexity is 139.24103992916034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0559132893880205 and perplexity of 156.94780367277173
Finished 14 epochs...
Completing Train Step...
At time: 413.64184617996216 and batch: 50, loss is 5.006841764450074 and perplexity is 149.43204850459975
At time: 416.17928290367126 and batch: 100, loss is 4.973695869445801 and perplexity is 144.56017682038197
At time: 418.7069458961487 and batch: 150, loss is 4.996169319152832 and perplexity is 147.8457231833734
At time: 421.2268080711365 and batch: 200, loss is 4.9955016708374025 and perplexity is 147.74704717944587
At time: 423.7569406032562 and batch: 250, loss is 4.974559555053711 and perplexity is 144.68508529764105
At time: 426.2824852466583 and batch: 300, loss is 4.970653991699219 and perplexity is 144.12111056696028
At time: 428.8079855442047 and batch: 350, loss is 4.9345070743560795 and perplexity is 139.00460670666442
At time: 431.3304636478424 and batch: 400, loss is 4.958587999343872 and perplexity is 142.3925954446367
At time: 433.86357164382935 and batch: 450, loss is 4.951873874664306 and perplexity is 141.4397561258224
At time: 436.38836145401 and batch: 500, loss is 4.93281135559082 and perplexity is 138.76909372493975
At time: 438.91091203689575 and batch: 550, loss is 4.9270881652832035 and perplexity is 137.9771601476647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.055101013183593 and perplexity of 156.8203704689974
Finished 15 epochs...
Completing Train Step...
At time: 443.02488684654236 and batch: 50, loss is 4.9963712406158445 and perplexity is 147.87557942230498
At time: 445.5864145755768 and batch: 100, loss is 4.963478164672852 and perplexity is 143.09062412477076
At time: 448.11319279670715 and batch: 150, loss is 4.986792707443238 and perplexity is 146.46591033831012
At time: 450.63507318496704 and batch: 200, loss is 4.98606918334961 and perplexity is 146.35997705054945
At time: 453.1635756492615 and batch: 250, loss is 4.966586246490478 and perplexity is 143.53605334854464
At time: 455.69185185432434 and batch: 300, loss is 4.962197351455688 and perplexity is 142.90746908087866
At time: 458.21491956710815 and batch: 350, loss is 4.926589097976684 and perplexity is 137.9083174379904
At time: 460.734739780426 and batch: 400, loss is 4.951493234634399 and perplexity is 141.38592873789983
At time: 463.25093364715576 and batch: 450, loss is 4.944123477935791 and perplexity is 140.34777899677425
At time: 465.7726731300354 and batch: 500, loss is 4.92501317024231 and perplexity is 137.69115505679306
At time: 468.29846024513245 and batch: 550, loss is 4.918747863769531 and perplexity is 136.8311746054577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0547841389973955 and perplexity of 156.77068601400214
Finished 16 epochs...
Completing Train Step...
At time: 472.38985228538513 and batch: 50, loss is 4.98671441078186 and perplexity is 146.4544429954582
At time: 474.91841864585876 and batch: 100, loss is 4.95452576637268 and perplexity is 141.81533682188694
At time: 477.4400546550751 and batch: 150, loss is 4.978104028701782 and perplexity is 145.19882770510216
At time: 479.96949553489685 and batch: 200, loss is 4.977457513809204 and perplexity is 145.1049848392798
At time: 482.4917366504669 and batch: 250, loss is 4.958698530197143 and perplexity is 142.40833508955282
At time: 485.0120692253113 and batch: 300, loss is 4.9540673828125 and perplexity is 141.75034589942956
At time: 487.5345673561096 and batch: 350, loss is 4.919264221191407 and perplexity is 136.90184664245433
At time: 490.06184124946594 and batch: 400, loss is 4.94457706451416 and perplexity is 140.41145330544532
At time: 492.5881407260895 and batch: 450, loss is 4.936723165512085 and perplexity is 139.31299516849296
At time: 495.1107587814331 and batch: 500, loss is 4.917585010528565 and perplexity is 136.67215250820257
At time: 497.62917733192444 and batch: 550, loss is 4.910783071517944 and perplexity is 135.74567136856885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.054571533203125 and perplexity of 156.7373592006462
Finished 17 epochs...
Completing Train Step...
At time: 501.69480323791504 and batch: 50, loss is 4.977566862106324 and perplexity is 145.1208526898209
At time: 504.26092553138733 and batch: 100, loss is 4.945520706176758 and perplexity is 140.54401393772136
At time: 506.77835273742676 and batch: 150, loss is 4.970127153396606 and perplexity is 144.04520204321315
At time: 509.3294007778168 and batch: 200, loss is 4.969417533874512 and perplexity is 143.94302101489333
At time: 511.85776138305664 and batch: 250, loss is 4.951156215667725 and perplexity is 141.3382870268253
At time: 514.3856613636017 and batch: 300, loss is 4.947050228118896 and perplexity is 140.75914357168284
At time: 516.9079194068909 and batch: 350, loss is 4.911955318450928 and perplexity is 135.90489212028248
At time: 519.4308643341064 and batch: 400, loss is 4.937765140533447 and perplexity is 139.45823148277512
At time: 521.9528303146362 and batch: 450, loss is 4.9294759559631345 and perplexity is 138.30701437941562
At time: 524.4795053005219 and batch: 500, loss is 4.910249814987183 and perplexity is 135.67330339984989
At time: 527.0158274173737 and batch: 550, loss is 4.9027978134155275 and perplexity is 134.6660235166974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.054652913411458 and perplexity of 156.75011503862086
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 531.0721673965454 and batch: 50, loss is 4.9700376987457275 and perplexity is 144.03231710627097
At time: 533.6222972869873 and batch: 100, loss is 4.934706449508667 and perplexity is 139.0323235342685
At time: 536.1490232944489 and batch: 150, loss is 4.951275930404663 and perplexity is 141.3552083155197
At time: 538.6745159626007 and batch: 200, loss is 4.942999048233032 and perplexity is 140.19005647591564
At time: 541.2074935436249 and batch: 250, loss is 4.91787992477417 and perplexity is 136.71246501702922
At time: 543.726859331131 and batch: 300, loss is 4.909110851287842 and perplexity is 135.51886439921088
At time: 546.2446715831757 and batch: 350, loss is 4.863030853271485 and perplexity is 129.4158487443738
At time: 548.7667989730835 and batch: 400, loss is 4.881933860778808 and perplexity is 131.88546556008032
At time: 551.2976174354553 and batch: 450, loss is 4.871746158599853 and perplexity is 130.54867668072004
At time: 553.8179929256439 and batch: 500, loss is 4.857990322113037 and perplexity is 128.76516539887567
At time: 556.3507001399994 and batch: 550, loss is 4.8734129047393795 and perplexity is 130.76644961928503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0314580281575525 and perplexity of 153.15615596940174
Finished 19 epochs...
Completing Train Step...
At time: 560.5281338691711 and batch: 50, loss is 4.953214311599732 and perplexity is 141.62947432330398
At time: 563.0544481277466 and batch: 100, loss is 4.919853420257568 and perplexity is 136.98253285043683
At time: 565.5779583454132 and batch: 150, loss is 4.938331747055054 and perplexity is 139.53727181649475
At time: 568.0984845161438 and batch: 200, loss is 4.932479486465454 and perplexity is 138.72304818814612
At time: 570.6528584957123 and batch: 250, loss is 4.910590791702271 and perplexity is 135.71957272510127
At time: 573.1816267967224 and batch: 300, loss is 4.904321012496948 and perplexity is 134.8713029810727
At time: 575.7043459415436 and batch: 350, loss is 4.86022102355957 and perplexity is 129.05272264753827
At time: 578.2255580425262 and batch: 400, loss is 4.882331104278564 and perplexity is 131.93786661128576
At time: 580.7487666606903 and batch: 450, loss is 4.874547243118286 and perplexity is 130.9148671837169
At time: 583.2711567878723 and batch: 500, loss is 4.860498323440551 and perplexity is 129.08851391439637
At time: 585.8026580810547 and batch: 550, loss is 4.870754718780518 and perplexity is 130.4193096646995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.029811096191406 and perplexity of 152.90412579556545
Finished 20 epochs...
Completing Train Step...
At time: 589.9054527282715 and batch: 50, loss is 4.945559034347534 and perplexity is 140.5494008359234
At time: 592.4556992053986 and batch: 100, loss is 4.9128443717956545 and perplexity is 136.0257725457979
At time: 594.9774782657623 and batch: 150, loss is 4.931809253692627 and perplexity is 138.6301026059709
At time: 597.5030181407928 and batch: 200, loss is 4.927350311279297 and perplexity is 138.01333504909542
At time: 600.0310025215149 and batch: 250, loss is 4.907310380935669 and perplexity is 135.27508622524178
At time: 602.5451381206512 and batch: 300, loss is 4.902186164855957 and perplexity is 134.58368042247494
At time: 605.0711252689362 and batch: 350, loss is 4.8587586784362795 and perplexity is 128.86414094729815
At time: 607.595766544342 and batch: 400, loss is 4.882123794555664 and perplexity is 131.91051744369062
At time: 610.1148114204407 and batch: 450, loss is 4.875180215835571 and perplexity is 130.9977589542782
At time: 612.6397423744202 and batch: 500, loss is 4.860833368301392 and perplexity is 129.1317716038055
At time: 615.1677360534668 and batch: 550, loss is 4.867966299057007 and perplexity is 130.05615244198927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0291290283203125 and perplexity of 152.79987036268494
Finished 21 epochs...
Completing Train Step...
At time: 619.2976162433624 and batch: 50, loss is 4.9399947166442875 and perplexity is 139.76951110600726
At time: 621.8201026916504 and batch: 100, loss is 4.907808799743652 and perplexity is 135.34252667786686
At time: 624.3462717533112 and batch: 150, loss is 4.927348022460937 and perplexity is 138.01301916200177
At time: 626.8746919631958 and batch: 200, loss is 4.923629188537598 and perplexity is 137.5007248236691
At time: 629.4077858924866 and batch: 250, loss is 4.90491623878479 and perplexity is 134.95160582289287
At time: 631.9330625534058 and batch: 300, loss is 4.900652551651001 and perplexity is 134.3774393004843
At time: 634.5131237506866 and batch: 350, loss is 4.8572848033905025 and perplexity is 128.67435120320116
At time: 637.0372049808502 and batch: 400, loss is 4.881494493484497 and perplexity is 131.82753212787784
At time: 639.5700218677521 and batch: 450, loss is 4.874918870925903 and perplexity is 130.96352783005744
At time: 642.0985355377197 and batch: 500, loss is 4.860485067367554 and perplexity is 129.08680271897467
At time: 644.6158125400543 and batch: 550, loss is 4.865354118347168 and perplexity is 129.71686560158082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028917948404948 and perplexity of 152.76762078272017
Finished 22 epochs...
Completing Train Step...
At time: 648.7047145366669 and batch: 50, loss is 4.935524339675903 and perplexity is 139.1460832197377
At time: 651.2517790794373 and batch: 100, loss is 4.90382269859314 and perplexity is 134.80411147819294
At time: 653.7766880989075 and batch: 150, loss is 4.923832559585572 and perplexity is 137.52869133386645
At time: 656.3009448051453 and batch: 200, loss is 4.920632982254029 and perplexity is 137.08936086134457
At time: 658.8265833854675 and batch: 250, loss is 4.902771959304809 and perplexity is 134.6625418914228
At time: 661.3475949764252 and batch: 300, loss is 4.899248628616333 and perplexity is 134.18891608511788
At time: 663.863942861557 and batch: 350, loss is 4.855564794540405 and perplexity is 128.45322040832988
At time: 666.381842136383 and batch: 400, loss is 4.8804035568237305 and perplexity is 131.6837950584561
At time: 668.9076828956604 and batch: 450, loss is 4.873997974395752 and perplexity is 130.84297948649728
At time: 671.4327688217163 and batch: 500, loss is 4.859306669235229 and perplexity is 128.93477666294532
At time: 673.949542760849 and batch: 550, loss is 4.862845125198365 and perplexity is 129.3918148201116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028691101074219 and perplexity of 152.73296978611532
Finished 23 epochs...
Completing Train Step...
At time: 678.0731909275055 and batch: 50, loss is 4.931572504043579 and perplexity is 138.597285862661
At time: 680.5999701023102 and batch: 100, loss is 4.900280723571777 and perplexity is 134.32748328343862
At time: 683.1245350837708 and batch: 150, loss is 4.9206426811218265 and perplexity is 137.09069047937993
At time: 685.6497287750244 and batch: 200, loss is 4.917977046966553 and perplexity is 136.72574347616379
At time: 688.1777517795563 and batch: 250, loss is 4.900795526504517 and perplexity is 134.3966532687085
At time: 690.7021191120148 and batch: 300, loss is 4.897810564041138 and perplexity is 133.99608244535727
At time: 693.2322678565979 and batch: 350, loss is 4.853842172622681 and perplexity is 128.2321345537929
At time: 695.7581307888031 and batch: 400, loss is 4.879030466079712 and perplexity is 131.5031053384042
At time: 698.318085193634 and batch: 450, loss is 4.872732486724853 and perplexity is 130.67750403478902
At time: 700.8352882862091 and batch: 500, loss is 4.857988061904908 and perplexity is 128.76487436313099
At time: 703.3574457168579 and batch: 550, loss is 4.860417852401733 and perplexity is 129.078126445533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028590393066406 and perplexity of 152.71758912749166
Finished 24 epochs...
Completing Train Step...
At time: 707.4205739498138 and batch: 50, loss is 4.928000602722168 and perplexity is 138.1031131276743
At time: 709.9726705551147 and batch: 100, loss is 4.897161779403686 and perplexity is 133.9091760404066
At time: 712.4968252182007 and batch: 150, loss is 4.917764158248901 and perplexity is 136.69663920606047
At time: 715.0194761753082 and batch: 200, loss is 4.915632476806641 and perplexity is 136.4055558757374
At time: 717.5464262962341 and batch: 250, loss is 4.899004669189453 and perplexity is 134.15618342693867
At time: 720.0763912200928 and batch: 300, loss is 4.896256780624389 and perplexity is 133.7880432203481
At time: 722.598564863205 and batch: 350, loss is 4.8521757984161376 and perplexity is 128.0186297711589
At time: 725.118280172348 and batch: 400, loss is 4.877674131393433 and perplexity is 131.32486402004665
At time: 727.6515679359436 and batch: 450, loss is 4.871450672149658 and perplexity is 130.51010701435752
At time: 730.1757678985596 and batch: 500, loss is 4.8566446781158445 and perplexity is 128.5920098559962
At time: 732.6941902637482 and batch: 550, loss is 4.858129663467407 and perplexity is 128.7831089615292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028395080566407 and perplexity of 152.68776438603246
Finished 25 epochs...
Completing Train Step...
At time: 736.8001415729523 and batch: 50, loss is 4.924928035736084 and perplexity is 137.67943328726622
At time: 739.3280372619629 and batch: 100, loss is 4.894365882873535 and perplexity is 133.53530273884152
At time: 741.8491432666779 and batch: 150, loss is 4.915197057723999 and perplexity is 136.3461752224037
At time: 744.3715813159943 and batch: 200, loss is 4.913319110870361 and perplexity is 136.09036462617166
At time: 746.8978426456451 and batch: 250, loss is 4.897276620864869 and perplexity is 133.92455524891733
At time: 749.4299962520599 and batch: 300, loss is 4.894810018539428 and perplexity is 133.59462370174631
At time: 751.9591243267059 and batch: 350, loss is 4.85044020652771 and perplexity is 127.79663437818819
At time: 754.4918475151062 and batch: 400, loss is 4.876218481063843 and perplexity is 131.1338400042686
At time: 757.0160310268402 and batch: 450, loss is 4.870080518722534 and perplexity is 130.33141059267064
At time: 759.5406785011292 and batch: 500, loss is 4.855185451507569 and perplexity is 128.40450181523622
At time: 762.0921499729156 and batch: 550, loss is 4.855912036895752 and perplexity is 128.49783255230108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028189086914063 and perplexity of 152.6563149150853
Finished 26 epochs...
Completing Train Step...
At time: 766.1475365161896 and batch: 50, loss is 4.921899948120117 and perplexity is 137.2631584767346
At time: 768.7014529705048 and batch: 100, loss is 4.891602687835693 and perplexity is 133.16682797121123
At time: 771.2283101081848 and batch: 150, loss is 4.91266788482666 and perplexity is 136.00176788781312
At time: 773.7512390613556 and batch: 200, loss is 4.911058712005615 and perplexity is 135.78309352891844
At time: 776.274787902832 and batch: 250, loss is 4.8954565143585205 and perplexity is 133.68101999183367
At time: 778.7998504638672 and batch: 300, loss is 4.893101749420166 and perplexity is 133.3666029474342
At time: 781.328644990921 and batch: 350, loss is 4.848671388626099 and perplexity is 127.57078520544908
At time: 783.8495810031891 and batch: 400, loss is 4.8746585941314695 and perplexity is 130.92944549845856
At time: 786.3689711093903 and batch: 450, loss is 4.868613443374634 and perplexity is 130.14034478137324
At time: 788.8970954418182 and batch: 500, loss is 4.853571195602417 and perplexity is 128.19739129959373
At time: 791.4222283363342 and batch: 550, loss is 4.853677759170532 and perplexity is 128.21105319895116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028128560384115 and perplexity of 152.6470754376884
Finished 27 epochs...
Completing Train Step...
At time: 795.5412375926971 and batch: 50, loss is 4.91899697303772 and perplexity is 136.8652647651401
At time: 798.0718483924866 and batch: 100, loss is 4.889067115783692 and perplexity is 132.82960159551303
At time: 800.6048369407654 and batch: 150, loss is 4.910370302200318 and perplexity is 135.68965128290895
At time: 803.1389708518982 and batch: 200, loss is 4.908835372924805 and perplexity is 135.4815370259634
At time: 805.6598782539368 and batch: 250, loss is 4.893724937438964 and perplexity is 133.44974131922686
At time: 808.1874568462372 and batch: 300, loss is 4.891643390655518 and perplexity is 133.17224834692834
At time: 810.7109663486481 and batch: 350, loss is 4.846957960128784 and perplexity is 127.35238894327424
At time: 813.2295708656311 and batch: 400, loss is 4.87317476272583 and perplexity is 130.73531234136212
At time: 815.7545657157898 and batch: 450, loss is 4.867158327102661 and perplexity is 129.95111315843795
At time: 818.278199672699 and batch: 500, loss is 4.852056770324707 and perplexity is 128.00339286481767
At time: 820.8016669750214 and batch: 550, loss is 4.851680545806885 and perplexity is 127.95524390803445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.028058878580729 and perplexity of 152.6364390847747
Finished 28 epochs...
Completing Train Step...
At time: 824.9546368122101 and batch: 50, loss is 4.916298017501831 and perplexity is 136.49636954096562
At time: 827.5118870735168 and batch: 100, loss is 4.886630601882935 and perplexity is 132.50635438287281
At time: 830.0227744579315 and batch: 150, loss is 4.908055047988892 and perplexity is 135.37585864136798
At time: 832.5349621772766 and batch: 200, loss is 4.906788644790649 and perplexity is 135.2045267315571
At time: 835.0465767383575 and batch: 250, loss is 4.892018766403198 and perplexity is 133.2222473628437
At time: 837.5584065914154 and batch: 300, loss is 4.890100927352905 and perplexity is 132.96699338074836
At time: 840.0700249671936 and batch: 350, loss is 4.8453447628021244 and perplexity is 127.14711003210083
At time: 842.5818407535553 and batch: 400, loss is 4.871605262756348 and perplexity is 130.53028421054236
At time: 845.0944118499756 and batch: 450, loss is 4.86574990272522 and perplexity is 129.7682156716633
At time: 847.6065692901611 and batch: 500, loss is 4.850549669265747 and perplexity is 127.81062411336245
At time: 850.1180195808411 and batch: 550, loss is 4.849568157196045 and perplexity is 127.68523798722323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.027840169270833 and perplexity of 152.603059724839
Finished 29 epochs...
Completing Train Step...
At time: 854.1591567993164 and batch: 50, loss is 4.913679141998291 and perplexity is 136.1393702148877
At time: 856.6669900417328 and batch: 100, loss is 4.884200353622436 and perplexity is 132.18472202716484
At time: 859.1766486167908 and batch: 150, loss is 4.905763711929321 and perplexity is 135.06602216017777
At time: 861.6847043037415 and batch: 200, loss is 4.904597511291504 and perplexity is 134.90859988980472
At time: 864.1975998878479 and batch: 250, loss is 4.890349349975586 and perplexity is 133.00002949326296
At time: 866.7060678005219 and batch: 300, loss is 4.888461408615112 and perplexity is 132.74917011504823
At time: 869.2136688232422 and batch: 350, loss is 4.843672361373901 and perplexity is 126.93464673516965
At time: 871.7217121124268 and batch: 400, loss is 4.87000864982605 and perplexity is 130.32204415459597
At time: 874.2395467758179 and batch: 450, loss is 4.864042091369629 and perplexity is 129.54678517392273
At time: 876.7529218196869 and batch: 500, loss is 4.848883895874024 and perplexity is 127.59789780263837
At time: 879.2583920955658 and batch: 550, loss is 4.847452306747437 and perplexity is 127.41536072978815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0277654012044275 and perplexity of 152.59165031567082
Finished 30 epochs...
Completing Train Step...
At time: 883.2650015354156 and batch: 50, loss is 4.911084947586059 and perplexity is 135.78665592392224
At time: 885.8090460300446 and batch: 100, loss is 4.881874380111694 and perplexity is 131.87762115790358
At time: 888.3279304504395 and batch: 150, loss is 4.903627042770386 and perplexity is 134.7777388489144
At time: 890.8728070259094 and batch: 200, loss is 4.902685499191284 and perplexity is 134.6508994560736
At time: 893.398862361908 and batch: 250, loss is 4.888664865493775 and perplexity is 132.7761815945874
At time: 895.9272203445435 and batch: 300, loss is 4.88691484451294 and perplexity is 132.54402369089289
At time: 898.4522287845612 and batch: 350, loss is 4.842083559036255 and perplexity is 126.73313279700658
At time: 900.9742527008057 and batch: 400, loss is 4.868385515213013 and perplexity is 130.11068551205426
At time: 903.4943220615387 and batch: 450, loss is 4.86247633934021 and perplexity is 129.34410574640157
At time: 906.0113360881805 and batch: 500, loss is 4.847325973510742 and perplexity is 127.39926495159978
At time: 908.5300314426422 and batch: 550, loss is 4.845443677902222 and perplexity is 127.15968742325344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.027793884277344 and perplexity of 152.59599665667147
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 912.6248071193695 and batch: 50, loss is 4.909176349639893 and perplexity is 135.5277409521974
At time: 915.1511936187744 and batch: 100, loss is 4.878504886627197 and perplexity is 131.4340081679135
At time: 917.6786892414093 and batch: 150, loss is 4.898284149169922 and perplexity is 134.05955602620438
At time: 920.2039837837219 and batch: 200, loss is 4.894994325637818 and perplexity is 133.6192484083856
At time: 922.733008146286 and batch: 250, loss is 4.877416000366211 and perplexity is 131.29096937281568
At time: 925.2547345161438 and batch: 300, loss is 4.873903284072876 and perplexity is 130.83059050909105
At time: 927.7753870487213 and batch: 350, loss is 4.827113752365112 and perplexity is 124.85009184460412
At time: 930.3034827709198 and batch: 400, loss is 4.850290002822876 and perplexity is 127.77744029178679
At time: 932.8291420936584 and batch: 450, loss is 4.843926677703857 and perplexity is 126.9669323938827
At time: 935.3531532287598 and batch: 500, loss is 4.830682525634765 and perplexity is 125.29644951601664
At time: 937.8836779594421 and batch: 550, loss is 4.83522310256958 and perplexity is 125.86666125016401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.02375233968099 and perplexity of 151.98051771153436
Finished 32 epochs...
Completing Train Step...
At time: 941.9549839496613 and batch: 50, loss is 4.905405912399292 and perplexity is 135.017704245505
At time: 944.5070731639862 and batch: 100, loss is 4.875632581710815 and perplexity is 131.05703127553954
At time: 947.0290637016296 and batch: 150, loss is 4.89610878944397 and perplexity is 133.76824523490563
At time: 949.5598647594452 and batch: 200, loss is 4.892974624633789 and perplexity is 133.34964982412862
At time: 952.0791573524475 and batch: 250, loss is 4.875972290039062 and perplexity is 131.10156000350082
At time: 954.6262254714966 and batch: 300, loss is 4.873184127807617 and perplexity is 130.73653669398774
At time: 957.1487085819244 and batch: 350, loss is 4.82688868522644 and perplexity is 124.82199535358677
At time: 959.675133228302 and batch: 400, loss is 4.850423517227173 and perplexity is 127.79450155954709
At time: 962.1993520259857 and batch: 450, loss is 4.844351005554199 and perplexity is 127.0208194314468
At time: 964.7239265441895 and batch: 500, loss is 4.831014461517334 and perplexity is 125.33804680700429
At time: 967.2448120117188 and batch: 550, loss is 4.834374694824219 and perplexity is 125.7599202862095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023404439290364 and perplexity of 151.92765282644532
Finished 33 epochs...
Completing Train Step...
At time: 971.3354172706604 and batch: 50, loss is 4.903532638549804 and perplexity is 134.76501586208795
At time: 973.8587143421173 and batch: 100, loss is 4.874164772033692 and perplexity is 130.8648056066387
At time: 976.3807532787323 and batch: 150, loss is 4.894944372177124 and perplexity is 133.61257383122287
At time: 978.9026350975037 and batch: 200, loss is 4.891881551742554 and perplexity is 133.20396857146926
At time: 981.423445224762 and batch: 250, loss is 4.875078115463257 and perplexity is 130.9843847170856
At time: 983.9569079875946 and batch: 300, loss is 4.872763891220092 and perplexity is 130.68160796028283
At time: 986.4808790683746 and batch: 350, loss is 4.826862897872925 and perplexity is 124.81877656616824
At time: 988.9991035461426 and batch: 400, loss is 4.850541954040527 and perplexity is 127.8096380294159
At time: 991.5213267803192 and batch: 450, loss is 4.844505729675293 and perplexity is 127.04047413658324
At time: 994.0529563426971 and batch: 500, loss is 4.830968294143677 and perplexity is 125.33226041213622
At time: 996.5733721256256 and batch: 550, loss is 4.833574657440185 and perplexity is 125.6593478847723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023253885904948 and perplexity of 151.9047813257081
Finished 34 epochs...
Completing Train Step...
At time: 1000.6547627449036 and batch: 50, loss is 4.902076711654663 and perplexity is 134.5689506139379
At time: 1003.2129065990448 and batch: 100, loss is 4.87303900718689 and perplexity is 130.71756550321976
At time: 1005.7359318733215 and batch: 150, loss is 4.894095849990845 and perplexity is 133.49924868421132
At time: 1008.2673218250275 and batch: 200, loss is 4.891055345535278 and perplexity is 133.09396007691015
At time: 1010.7870960235596 and batch: 250, loss is 4.87436164855957 and perplexity is 130.89057235127663
At time: 1013.3120651245117 and batch: 300, loss is 4.872404384613037 and perplexity is 130.63463550276083
At time: 1015.8456275463104 and batch: 350, loss is 4.826837720870972 and perplexity is 124.81563404314673
At time: 1018.4032206535339 and batch: 400, loss is 4.850564804077148 and perplexity is 127.8125585176919
At time: 1020.9251754283905 and batch: 450, loss is 4.844487724304199 and perplexity is 127.03818674629511
At time: 1023.4462509155273 and batch: 500, loss is 4.830712518692017 and perplexity is 125.30020759595821
At time: 1025.9625306129456 and batch: 550, loss is 4.832783670425415 and perplexity is 125.55999227199092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023190816243489 and perplexity of 151.89520104469162
Finished 35 epochs...
Completing Train Step...
At time: 1030.0085785388947 and batch: 50, loss is 4.900840559005737 and perplexity is 134.40270562243612
At time: 1032.526221036911 and batch: 100, loss is 4.872094326019287 and perplexity is 130.59413739010014
At time: 1035.0456199645996 and batch: 150, loss is 4.89338490486145 and perplexity is 133.40437177371635
At time: 1037.5571494102478 and batch: 200, loss is 4.890367431640625 and perplexity is 133.0024343769885
At time: 1040.0676720142365 and batch: 250, loss is 4.873739604949951 and perplexity is 130.8091780252209
At time: 1042.580227136612 and batch: 300, loss is 4.872087106704712 and perplexity is 130.5931945933439
At time: 1045.0905067920685 and batch: 350, loss is 4.826782474517822 and perplexity is 124.80873862502484
At time: 1047.6036005020142 and batch: 400, loss is 4.850505170822143 and perplexity is 127.80493686605116
At time: 1050.1173400878906 and batch: 450, loss is 4.8443532371521 and perplexity is 127.02110289115701
At time: 1052.6311249732971 and batch: 500, loss is 4.8304167652130126 and perplexity is 125.2631551031137
At time: 1055.1410021781921 and batch: 550, loss is 4.83201494216919 and perplexity is 125.46350784798295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0231470743815105 and perplexity of 151.88855701108454
Finished 36 epochs...
Completing Train Step...
At time: 1059.1560010910034 and batch: 50, loss is 4.8997334861755375 and perplexity is 134.25399437101768
At time: 1061.6915969848633 and batch: 100, loss is 4.871263809204102 and perplexity is 130.48572178975436
At time: 1064.2004930973053 and batch: 150, loss is 4.892737989425659 and perplexity is 133.31809833523226
At time: 1066.7087032794952 and batch: 200, loss is 4.889727716445923 and perplexity is 132.91737790764378
At time: 1069.217523574829 and batch: 250, loss is 4.873162202835083 and perplexity is 130.73367033043408
At time: 1071.7274055480957 and batch: 300, loss is 4.871740226745605 and perplexity is 130.54790228729453
At time: 1074.237356185913 and batch: 350, loss is 4.826666488647461 and perplexity is 124.7942634143229
At time: 1076.7476689815521 and batch: 400, loss is 4.850372552871704 and perplexity is 127.7879887611043
At time: 1079.2603611946106 and batch: 450, loss is 4.844153242111206 and perplexity is 126.99570184061669
At time: 1081.799350976944 and batch: 500, loss is 4.830075998306274 and perplexity is 125.2204768373042
At time: 1084.3173305988312 and batch: 550, loss is 4.831265316009522 and perplexity is 125.36949236305244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0231272379557295 and perplexity of 151.88554411487905
Finished 37 epochs...
Completing Train Step...
At time: 1088.3755729198456 and batch: 50, loss is 4.898733673095703 and perplexity is 134.1198325509727
At time: 1090.893135547638 and batch: 100, loss is 4.870538053512573 and perplexity is 130.39105539099626
At time: 1093.408546447754 and batch: 150, loss is 4.89218113899231 and perplexity is 133.24388076036317
At time: 1095.9230234622955 and batch: 200, loss is 4.8891418838500975 and perplexity is 132.83953337927127
At time: 1098.4399089813232 and batch: 250, loss is 4.87263484954834 and perplexity is 130.66474567511645
At time: 1100.965410709381 and batch: 300, loss is 4.871398658752441 and perplexity is 130.5033189168581
At time: 1103.493157863617 and batch: 350, loss is 4.82648042678833 and perplexity is 124.7710461216565
At time: 1106.016084909439 and batch: 400, loss is 4.8501966381073 and perplexity is 127.7655109443156
At time: 1108.5357356071472 and batch: 450, loss is 4.843872537612915 and perplexity is 126.96005857869245
At time: 1111.0733745098114 and batch: 500, loss is 4.82968695640564 and perplexity is 125.17177030005332
At time: 1113.6003901958466 and batch: 550, loss is 4.830534629821777 and perplexity is 125.27792006599873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0231068929036455 and perplexity of 151.88245402700736
Finished 38 epochs...
Completing Train Step...
At time: 1117.672369003296 and batch: 50, loss is 4.897800788879395 and perplexity is 133.99477261838032
At time: 1120.2232637405396 and batch: 100, loss is 4.869815292358399 and perplexity is 130.2968478501844
At time: 1122.7519190311432 and batch: 150, loss is 4.891612195968628 and perplexity is 133.1680941451336
At time: 1125.2734849452972 and batch: 200, loss is 4.88857400894165 and perplexity is 132.76411855653478
At time: 1127.7887308597565 and batch: 250, loss is 4.872130374908448 and perplexity is 130.59884524854002
At time: 1130.3073422908783 and batch: 300, loss is 4.871038570404052 and perplexity is 130.4563346520409
At time: 1132.823725938797 and batch: 350, loss is 4.826285648345947 and perplexity is 124.74674577830676
At time: 1135.3470001220703 and batch: 400, loss is 4.849965629577636 and perplexity is 127.7359994303276
At time: 1137.8692209720612 and batch: 450, loss is 4.84356369972229 and perplexity is 126.92085455616282
At time: 1140.394588470459 and batch: 500, loss is 4.829278974533081 and perplexity is 125.12071290276934
At time: 1142.9139473438263 and batch: 550, loss is 4.829805364608765 and perplexity is 125.18659254197675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023064676920573 and perplexity of 151.87604229523882
Finished 39 epochs...
Completing Train Step...
At time: 1146.9747111797333 and batch: 50, loss is 4.896920967102051 and perplexity is 133.87693294593691
At time: 1149.5225939750671 and batch: 100, loss is 4.869182462692261 and perplexity is 130.2144182241678
At time: 1152.0410096645355 and batch: 150, loss is 4.891119432449341 and perplexity is 133.10248993141496
At time: 1154.5639069080353 and batch: 200, loss is 4.88802396774292 and perplexity is 132.6911129015152
At time: 1157.0888962745667 and batch: 250, loss is 4.871640348434449 and perplexity is 130.53486403441917
At time: 1159.6079487800598 and batch: 300, loss is 4.87067590713501 and perplexity is 130.40903150932337
At time: 1162.1311967372894 and batch: 350, loss is 4.826097269058227 and perplexity is 124.72324828848134
At time: 1164.6505808830261 and batch: 400, loss is 4.849724292755127 and perplexity is 127.70517574969921
At time: 1167.1799325942993 and batch: 450, loss is 4.843281078338623 and perplexity is 126.88498907704628
At time: 1169.7070879936218 and batch: 500, loss is 4.828864126205445 and perplexity is 125.06881754936346
At time: 1172.2242739200592 and batch: 550, loss is 4.829136762619019 and perplexity is 125.10292051187352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023054504394532 and perplexity of 151.8744973401016
Finished 40 epochs...
Completing Train Step...
At time: 1176.330798149109 and batch: 50, loss is 4.896056671142578 and perplexity is 133.76127364285915
At time: 1178.8500883579254 and batch: 100, loss is 4.868568525314331 and perplexity is 130.1344992608043
At time: 1181.3788590431213 and batch: 150, loss is 4.89062120437622 and perplexity is 133.03619105168187
At time: 1183.9009897708893 and batch: 200, loss is 4.887524003982544 and perplexity is 132.6247887349619
At time: 1186.420218706131 and batch: 250, loss is 4.871184320449829 and perplexity is 130.4753500545025
At time: 1188.9420809745789 and batch: 300, loss is 4.870332956314087 and perplexity is 130.36431529308126
At time: 1191.4712381362915 and batch: 350, loss is 4.825907545089722 and perplexity is 124.69958754342737
At time: 1193.9956638813019 and batch: 400, loss is 4.849467134475708 and perplexity is 127.67233952866448
At time: 1196.5192730426788 and batch: 450, loss is 4.842959651947021 and perplexity is 126.8442114467132
At time: 1199.0370419025421 and batch: 500, loss is 4.828459444046021 and perplexity is 125.01821466993287
At time: 1201.5484676361084 and batch: 550, loss is 4.828460359573365 and perplexity is 125.01832912757925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023049926757812 and perplexity of 151.87380211541708
Finished 41 epochs...
Completing Train Step...
At time: 1205.618988752365 and batch: 50, loss is 4.895253705978393 and perplexity is 133.65391110976253
At time: 1208.2027912139893 and batch: 100, loss is 4.867988653182984 and perplexity is 130.05905976610026
At time: 1210.7263188362122 and batch: 150, loss is 4.890178203582764 and perplexity is 132.97726896571658
At time: 1213.246908903122 and batch: 200, loss is 4.887023324966431 and perplexity is 132.55840290660825
At time: 1215.7665684223175 and batch: 250, loss is 4.870721759796143 and perplexity is 130.41501124754623
At time: 1218.2918095588684 and batch: 300, loss is 4.869976282119751 and perplexity is 130.31782599721288
At time: 1220.8158168792725 and batch: 350, loss is 4.825676393508911 and perplexity is 124.67076636779878
At time: 1223.3404755592346 and batch: 400, loss is 4.849182634353638 and perplexity is 127.63602189891367
At time: 1225.864145040512 and batch: 450, loss is 4.842645902633667 and perplexity is 126.80442040500121
At time: 1228.3887341022491 and batch: 500, loss is 4.828062314987182 and perplexity is 124.96857616110283
At time: 1230.915680885315 and batch: 550, loss is 4.827850427627563 and perplexity is 124.94209970457702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023047892252604 and perplexity of 151.87349312769004
Finished 42 epochs...
Completing Train Step...
At time: 1235.0222268104553 and batch: 50, loss is 4.8944759178161625 and perplexity is 133.54999709664875
At time: 1237.552484512329 and batch: 100, loss is 4.867413663864136 and perplexity is 129.98429869138485
At time: 1240.0816168785095 and batch: 150, loss is 4.889692277908325 and perplexity is 132.9126675936132
At time: 1242.6029188632965 and batch: 200, loss is 4.886532535552979 and perplexity is 132.49336060813891
At time: 1245.124491930008 and batch: 250, loss is 4.8702403259277345 and perplexity is 130.35224015546007
At time: 1247.6461102962494 and batch: 300, loss is 4.869609441757202 and perplexity is 130.2700289261609
At time: 1250.168270111084 and batch: 350, loss is 4.8254533386230465 and perplexity is 124.64296104540773
At time: 1252.6974050998688 and batch: 400, loss is 4.848866281509399 and perplexity is 127.59565026653566
At time: 1255.2218232154846 and batch: 450, loss is 4.84227614402771 and perplexity is 126.75754204665542
At time: 1257.7429931163788 and batch: 500, loss is 4.82762041091919 and perplexity is 124.91336423901099
At time: 1260.2665309906006 and batch: 550, loss is 4.82717188835144 and perplexity is 124.85735033882449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.02304941813151 and perplexity of 151.8737248684264
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 1264.3407163619995 and batch: 50, loss is 4.893824214935303 and perplexity is 133.46299053309556
At time: 1266.8863682746887 and batch: 100, loss is 4.866658658981323 and perplexity is 129.88619694949097
At time: 1269.4068672657013 and batch: 150, loss is 4.888522710800171 and perplexity is 132.757308178679
At time: 1271.9589037895203 and batch: 200, loss is 4.885129804611206 and perplexity is 132.30763836125868
At time: 1274.4787182807922 and batch: 250, loss is 4.866859502792359 and perplexity is 129.9122864081527
At time: 1276.9989874362946 and batch: 300, loss is 4.8658210563659665 and perplexity is 129.77744948116722
At time: 1279.5234570503235 and batch: 350, loss is 4.820987272262573 and perplexity is 124.08753851015618
At time: 1282.0479192733765 and batch: 400, loss is 4.843623790740967 and perplexity is 126.92848158876018
At time: 1284.5680451393127 and batch: 450, loss is 4.836778736114502 and perplexity is 126.06261602798736
At time: 1287.087197303772 and batch: 500, loss is 4.82257082939148 and perplexity is 124.28419388278577
At time: 1289.6198394298553 and batch: 550, loss is 4.824383726119995 and perplexity is 124.50971265063457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.02283681233724 and perplexity of 151.84143906671784
Finished 44 epochs...
Completing Train Step...
At time: 1293.7570796012878 and batch: 50, loss is 4.893247537612915 and perplexity is 133.3860476408199
At time: 1296.2789075374603 and batch: 100, loss is 4.8659930992126466 and perplexity is 129.79977868374556
At time: 1298.8074643611908 and batch: 150, loss is 4.887989435195923 and perplexity is 132.68653081853878
At time: 1301.3373544216156 and batch: 200, loss is 4.884595689773559 and perplexity is 132.23698975738924
At time: 1303.8645949363708 and batch: 250, loss is 4.866517066955566 and perplexity is 129.86780740168447
At time: 1306.3837842941284 and batch: 300, loss is 4.865560617446899 and perplexity is 129.74365478342335
At time: 1308.913442850113 and batch: 350, loss is 4.820885257720947 and perplexity is 124.07488042245909
At time: 1311.4440071582794 and batch: 400, loss is 4.8435955905914305 and perplexity is 126.92490223706831
At time: 1313.9688742160797 and batch: 450, loss is 4.8369177722930905 and perplexity is 126.0801445109012
At time: 1316.4924817085266 and batch: 500, loss is 4.822820672988891 and perplexity is 124.31524937223291
At time: 1319.014087677002 and batch: 550, loss is 4.824272947311401 and perplexity is 124.4959203769687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022732543945312 and perplexity of 151.82560762941196
Finished 45 epochs...
Completing Train Step...
At time: 1323.0735304355621 and batch: 50, loss is 4.892831687927246 and perplexity is 133.33059062652603
At time: 1325.646134853363 and batch: 100, loss is 4.865595273971557 and perplexity is 129.74815132551137
At time: 1328.1658263206482 and batch: 150, loss is 4.8877321720123295 and perplexity is 132.65239984970952
At time: 1330.6882028579712 and batch: 200, loss is 4.884311294555664 and perplexity is 132.19938753707262
At time: 1333.2162549495697 and batch: 250, loss is 4.86626277923584 and perplexity is 129.83478781149051
At time: 1335.7651793956757 and batch: 300, loss is 4.865352354049683 and perplexity is 129.71663674264298
At time: 1338.2848446369171 and batch: 350, loss is 4.8207979488372805 and perplexity is 124.0640480560461
At time: 1340.8034029006958 and batch: 400, loss is 4.843578901290893 and perplexity is 126.9227839669055
At time: 1343.329792022705 and batch: 450, loss is 4.837001218795776 and perplexity is 126.09066589699907
At time: 1345.855456352234 and batch: 500, loss is 4.822961053848267 and perplexity is 124.33270207875753
At time: 1348.374897480011 and batch: 550, loss is 4.824155931472778 and perplexity is 124.48135323475042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022693888346354 and perplexity of 151.8197388330435
Finished 46 epochs...
Completing Train Step...
At time: 1352.445520401001 and batch: 50, loss is 4.892500696182251 and perplexity is 133.28646660441967
At time: 1354.9603748321533 and batch: 100, loss is 4.865277853012085 and perplexity is 129.7069730785929
At time: 1357.484757900238 and batch: 150, loss is 4.887537040710449 and perplexity is 132.62651773951637
At time: 1360.0123488903046 and batch: 200, loss is 4.884083166122436 and perplexity is 132.1692325376573
At time: 1362.5317404270172 and batch: 250, loss is 4.866042366027832 and perplexity is 129.80617366298193
At time: 1365.0574560165405 and batch: 300, loss is 4.8651652336120605 and perplexity is 129.69236637962013
At time: 1367.5811786651611 and batch: 350, loss is 4.820715856552124 and perplexity is 124.05386377286669
At time: 1370.1085057258606 and batch: 400, loss is 4.843551912307739 and perplexity is 126.91935849625234
At time: 1372.6421823501587 and batch: 450, loss is 4.8370498752594 and perplexity is 126.09680117215673
At time: 1375.1644480228424 and batch: 500, loss is 4.823048620223999 and perplexity is 124.34358991956104
At time: 1377.6930079460144 and batch: 550, loss is 4.824031524658203 and perplexity is 124.4658678693829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022672526041666 and perplexity of 151.81649564816598
Finished 47 epochs...
Completing Train Step...
At time: 1381.7986402511597 and batch: 50, loss is 4.892207527160645 and perplexity is 133.2473968687099
At time: 1384.3761599063873 and batch: 100, loss is 4.86500020980835 and perplexity is 129.6709658178575
At time: 1386.9011461734772 and batch: 150, loss is 4.887368993759155 and perplexity is 132.60423213011632
At time: 1389.427405834198 and batch: 200, loss is 4.883881072998047 and perplexity is 132.14252474332739
At time: 1391.9560029506683 and batch: 250, loss is 4.865838241577149 and perplexity is 129.77967975320703
At time: 1394.4860923290253 and batch: 300, loss is 4.86498950958252 and perplexity is 129.66957831666298
At time: 1397.0132756233215 and batch: 350, loss is 4.820634546279908 and perplexity is 124.04377732950509
At time: 1399.5956687927246 and batch: 400, loss is 4.8435159683227536 and perplexity is 126.91479659072317
At time: 1402.1301243305206 and batch: 450, loss is 4.837075328826904 and perplexity is 126.10001082644585
At time: 1404.660462141037 and batch: 500, loss is 4.823106107711792 and perplexity is 124.35073832563859
At time: 1407.185560464859 and batch: 550, loss is 4.8239058494567875 and perplexity is 124.45022657925081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022656758626302 and perplexity of 151.8141019132915
Finished 48 epochs...
Completing Train Step...
At time: 1411.2647895812988 and batch: 50, loss is 4.891937580108642 and perplexity is 133.21143198126424
At time: 1413.7880656719208 and batch: 100, loss is 4.864745826721191 and perplexity is 129.63798391245524
At time: 1416.3080263137817 and batch: 150, loss is 4.88721884727478 and perplexity is 132.58432356548659
At time: 1418.829030752182 and batch: 200, loss is 4.883694105148315 and perplexity is 132.1178206491256
At time: 1421.3591403961182 and batch: 250, loss is 4.865644636154175 and perplexity is 129.75455613552785
At time: 1423.883290529251 and batch: 300, loss is 4.864821720123291 and perplexity is 129.64782295345015
At time: 1426.4040582180023 and batch: 350, loss is 4.820552616119385 and perplexity is 124.0336148192304
At time: 1428.926857471466 and batch: 400, loss is 4.843475103378296 and perplexity is 126.90961033057847
At time: 1431.4452278614044 and batch: 450, loss is 4.837086353302002 and perplexity is 126.10140102053816
At time: 1433.9668378829956 and batch: 500, loss is 4.82314398765564 and perplexity is 124.3554488138398
At time: 1436.4963521957397 and batch: 550, loss is 4.8237793922424315 and perplexity is 124.43448994529804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022645568847656 and perplexity of 151.8124031566001
Finished 49 epochs...
Completing Train Step...
At time: 1440.567224264145 and batch: 50, loss is 4.891683235168457 and perplexity is 133.17755463601324
At time: 1443.1155035495758 and batch: 100, loss is 4.864508562088012 and perplexity is 129.6072290524208
At time: 1445.642825603485 and batch: 150, loss is 4.887081212997437 and perplexity is 132.56607667365333
At time: 1448.1712296009064 and batch: 200, loss is 4.883516893386841 and perplexity is 132.0944098917983
At time: 1450.6929354667664 and batch: 250, loss is 4.865459280014038 and perplexity is 129.73050756068173
At time: 1453.212958574295 and batch: 300, loss is 4.864660015106201 and perplexity is 129.62685994498133
At time: 1455.7309868335724 and batch: 350, loss is 4.820471105575561 and perplexity is 124.02350518386056
At time: 1458.2626540660858 and batch: 400, loss is 4.843432092666626 and perplexity is 126.90415197530517
At time: 1460.7853698730469 and batch: 450, loss is 4.837083072662353 and perplexity is 126.10098732796078
At time: 1463.307609319687 and batch: 500, loss is 4.823167219161987 and perplexity is 124.35833781179598
At time: 1465.8861813545227 and batch: 550, loss is 4.823652572631836 and perplexity is 124.41871021234951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022634887695313 and perplexity of 151.81078163385433
Finished Training.
Improved accuracyfrom -172.69910480192593 to -151.81078163385433
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f2315076898>
SETTINGS FOR THIS RUN
{'anneal': 5.014970910694949, 'num_layers': 1, 'dropout': 0.4120799441113109, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 4.082977578885308, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.035163164138794 and batch: 50, loss is 7.364459629058838 and perplexity is 1578.862024802829
At time: 5.544519424438477 and batch: 100, loss is 6.439095706939697 and perplexity is 625.8406005331158
At time: 8.07431149482727 and batch: 150, loss is 6.211547317504883 and perplexity is 498.4719492485825
At time: 10.587824583053589 and batch: 200, loss is 6.081040906906128 and perplexity is 437.48433826528867
At time: 13.11294412612915 and batch: 250, loss is 5.9123464012146 and perplexity is 369.57230374860876
At time: 15.650727272033691 and batch: 300, loss is 5.862043228149414 and perplexity is 351.4414860608172
At time: 18.17844033241272 and batch: 350, loss is 5.781818885803222 and perplexity is 324.3486072908476
At time: 20.702248096466064 and batch: 400, loss is 5.761415948867798 and perplexity is 317.7979962026922
At time: 23.223507404327393 and batch: 450, loss is 5.709905796051025 and perplexity is 301.84263217549454
At time: 25.753883361816406 and batch: 500, loss is 5.6775307369232175 and perplexity is 292.2269530716782
At time: 28.281500339508057 and batch: 550, loss is 5.640970115661621 and perplexity is 281.7359023551906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.213245646158854 and perplexity of 183.68928214104457
Finished 1 epochs...
Completing Train Step...
At time: 32.36624455451965 and batch: 50, loss is 5.457973728179931 and perplexity is 234.62153538231888
At time: 34.88262939453125 and batch: 100, loss is 5.356262426376343 and perplexity is 211.9313552771563
At time: 37.39793133735657 and batch: 150, loss is 5.32742733001709 and perplexity is 205.90755995975587
At time: 39.93065118789673 and batch: 200, loss is 5.295369119644165 and perplexity is 199.41121898161438
At time: 42.43659567832947 and batch: 250, loss is 5.225487270355225 and perplexity is 185.95175723285476
At time: 44.93605089187622 and batch: 300, loss is 5.202674341201782 and perplexity is 181.75767451727322
At time: 47.44204115867615 and batch: 350, loss is 5.132573957443237 and perplexity is 169.45272128551653
At time: 49.95591592788696 and batch: 400, loss is 5.126829919815063 and perplexity is 168.48216859238133
At time: 52.46893501281738 and batch: 450, loss is 5.063691864013672 and perplexity is 158.17339437067537
At time: 54.99034333229065 and batch: 500, loss is 5.053884534835816 and perplexity is 156.62971786972182
At time: 57.51312565803528 and batch: 550, loss is 5.022767076492309 and perplexity is 151.8308506448696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.868287658691406 and perplexity of 130.09795395588924
Finished 2 epochs...
Completing Train Step...
At time: 61.63232731819153 and batch: 50, loss is 5.028534555435181 and perplexity is 152.7090619771385
At time: 64.14938020706177 and batch: 100, loss is 4.984669618606567 and perplexity is 146.15528006363294
At time: 66.66757321357727 and batch: 150, loss is 4.97646954536438 and perplexity is 144.96169648691233
At time: 69.18328380584717 and batch: 200, loss is 4.966566305160523 and perplexity is 143.5331910772832
At time: 71.69973874092102 and batch: 250, loss is 4.933830966949463 and perplexity is 138.91065642633532
At time: 74.21637749671936 and batch: 300, loss is 4.9237923908233645 and perplexity is 137.52316708751943
At time: 76.73943781852722 and batch: 350, loss is 4.865465345382691 and perplexity is 129.7312944264219
At time: 79.27990698814392 and batch: 400, loss is 4.866007881164551 and perplexity is 129.80169739201233
At time: 81.79369068145752 and batch: 450, loss is 4.8107763767242435 and perplexity is 122.82694048691373
At time: 84.30968713760376 and batch: 500, loss is 4.813370742797852 and perplexity is 123.14601224975073
At time: 86.82732796669006 and batch: 550, loss is 4.801423854827881 and perplexity is 121.6835539446739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.76857655843099 and perplexity of 117.75151021636762
Finished 3 epochs...
Completing Train Step...
At time: 90.88229870796204 and batch: 50, loss is 4.82648777961731 and perplexity is 124.77196354519305
At time: 93.42065286636353 and batch: 100, loss is 4.794365320205689 and perplexity is 120.8276705609993
At time: 95.93686771392822 and batch: 150, loss is 4.7841228485107425 and perplexity is 119.59641289030203
At time: 98.45813012123108 and batch: 200, loss is 4.779905958175659 and perplexity is 119.09314978110356
At time: 100.9690842628479 and batch: 250, loss is 4.762870721817016 and perplexity is 117.0815524903122
At time: 103.49108052253723 and batch: 300, loss is 4.7549779415130615 and perplexity is 116.16109079823414
At time: 106.01862692832947 and batch: 350, loss is 4.703724632263183 and perplexity is 110.35744882788781
At time: 108.53971910476685 and batch: 400, loss is 4.7070748996734615 and perplexity is 110.72779582651047
At time: 111.06090140342712 and batch: 450, loss is 4.652871599197388 and perplexity is 104.88574335898603
At time: 113.58014130592346 and batch: 500, loss is 4.66220989227295 and perplexity is 105.8697846519218
At time: 116.10456776618958 and batch: 550, loss is 4.6566269588470455 and perplexity is 105.28036756155127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.713151550292968 and perplexity of 111.40269845257792
Finished 4 epochs...
Completing Train Step...
At time: 120.35359406471252 and batch: 50, loss is 4.687294311523438 and perplexity is 108.55905509752745
At time: 122.87231373786926 and batch: 100, loss is 4.662491540908814 and perplexity is 105.89960693185336
At time: 125.39168190956116 and batch: 150, loss is 4.6547533226013185 and perplexity is 105.08329512760423
At time: 127.90864086151123 and batch: 200, loss is 4.648250074386596 and perplexity is 104.4021296708547
At time: 130.43970727920532 and batch: 250, loss is 4.638610601425171 and perplexity is 103.40058311052125
At time: 132.96431851387024 and batch: 300, loss is 4.633290634155274 and perplexity is 102.85195602569108
At time: 135.51520133018494 and batch: 350, loss is 4.5845651435852055 and perplexity is 97.96057909237298
At time: 138.03782415390015 and batch: 400, loss is 4.587698488235474 and perplexity is 98.26800473253023
At time: 140.55430555343628 and batch: 450, loss is 4.534867792129517 and perplexity is 93.21119123667383
At time: 143.05811858177185 and batch: 500, loss is 4.548179750442505 and perplexity is 94.46031039366443
At time: 145.5676817893982 and batch: 550, loss is 4.544944086074829 and perplexity is 94.15516247751304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.689684041341146 and perplexity of 108.81879213553948
Finished 5 epochs...
Completing Train Step...
At time: 149.5728578567505 and batch: 50, loss is 4.580611219406128 and perplexity is 97.57401511604233
At time: 152.11499524116516 and batch: 100, loss is 4.557203588485717 and perplexity is 95.31656246582892
At time: 154.64131474494934 and batch: 150, loss is 4.551288013458252 and perplexity is 94.75437466070626
At time: 157.15812158584595 and batch: 200, loss is 4.543770608901977 and perplexity is 94.04473834640973
At time: 159.67284393310547 and batch: 250, loss is 4.539701614379883 and perplexity is 93.66284830239859
At time: 162.19140553474426 and batch: 300, loss is 4.535092887878418 and perplexity is 93.23217504116516
At time: 164.7115294933319 and batch: 350, loss is 4.489720697402954 and perplexity is 89.09655750322801
At time: 167.2261528968811 and batch: 400, loss is 4.493450498580932 and perplexity is 89.42949044924046
At time: 169.73944854736328 and batch: 450, loss is 4.439078931808472 and perplexity is 84.69689412069062
At time: 172.25811910629272 and batch: 500, loss is 4.453915948867798 and perplexity is 85.96291212428312
At time: 174.78916025161743 and batch: 550, loss is 4.454396514892578 and perplexity is 86.00423290712818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.674144490559896 and perplexity of 107.14086784898755
Finished 6 epochs...
Completing Train Step...
At time: 178.88560605049133 and batch: 50, loss is 4.493361110687256 and perplexity is 89.42149689272557
At time: 181.406809091568 and batch: 100, loss is 4.472455043792724 and perplexity is 87.57145106956709
At time: 183.9244818687439 and batch: 150, loss is 4.467677946090698 and perplexity is 87.1541113219381
At time: 186.44739627838135 and batch: 200, loss is 4.4589282894134525 and perplexity is 86.39486916768978
At time: 188.9701533317566 and batch: 250, loss is 4.457653303146362 and perplexity is 86.28478708743651
At time: 191.48989629745483 and batch: 300, loss is 4.453824806213379 and perplexity is 85.95507759332591
At time: 194.0110490322113 and batch: 350, loss is 4.411094856262207 and perplexity is 82.35958606813185
At time: 196.53639221191406 and batch: 400, loss is 4.41281873703003 and perplexity is 82.50168662161535
At time: 199.08741188049316 and batch: 450, loss is 4.357589120864868 and perplexity is 78.0686931763371
At time: 201.60863614082336 and batch: 500, loss is 4.375238466262817 and perplexity is 79.45878553281696
At time: 204.12590336799622 and batch: 550, loss is 4.376502275466919 and perplexity is 79.55926976039136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.665363057454427 and perplexity of 106.20413642682131
Finished 7 epochs...
Completing Train Step...
At time: 208.1782190799713 and batch: 50, loss is 4.418727149963379 and perplexity is 82.99058353403885
At time: 210.7240068912506 and batch: 100, loss is 4.397478923797608 and perplexity is 81.24578344467068
At time: 213.2476246356964 and batch: 150, loss is 4.394515943527222 and perplexity is 81.00541007790187
At time: 215.7651846408844 and batch: 200, loss is 4.385685396194458 and perplexity is 80.29323703672037
At time: 218.28635478019714 and batch: 250, loss is 4.387349967956543 and perplexity is 80.42700219174161
At time: 220.8057882785797 and batch: 300, loss is 4.381944789886474 and perplexity is 79.99345268494287
At time: 223.32310509681702 and batch: 350, loss is 4.342543096542358 and perplexity is 76.90286227363205
At time: 225.83762764930725 and batch: 400, loss is 4.343591871261597 and perplexity is 76.98355836003277
At time: 228.35011649131775 and batch: 450, loss is 4.286724109649658 and perplexity is 72.72782944068356
At time: 230.87348794937134 and batch: 500, loss is 4.3061751365661625 and perplexity is 74.15630805918484
At time: 233.38609409332275 and batch: 550, loss is 4.309096364974976 and perplexity is 74.3732522905301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.661012776692709 and perplexity of 105.74312211321403
Finished 8 epochs...
Completing Train Step...
At time: 237.4801254272461 and batch: 50, loss is 4.355180835723877 and perplexity is 77.88090771381387
At time: 240.00428223609924 and batch: 100, loss is 4.333547658920288 and perplexity is 76.21418947352204
At time: 242.5238082408905 and batch: 150, loss is 4.332665233612061 and perplexity is 76.14696580817846
At time: 245.04238843917847 and batch: 200, loss is 4.321659164428711 and perplexity is 75.31348213741657
At time: 247.5632245540619 and batch: 250, loss is 4.325243425369263 and perplexity is 75.58390966163132
At time: 250.07613849639893 and batch: 300, loss is 4.3183486366271975 and perplexity is 75.06456700853307
At time: 252.60089421272278 and batch: 350, loss is 4.283086996078492 and perplexity is 72.46379052605518
At time: 255.1299819946289 and batch: 400, loss is 4.282217969894409 and perplexity is 72.40084494932877
At time: 257.64691638946533 and batch: 450, loss is 4.224560813903809 and perplexity is 68.34448101048883
At time: 260.1728744506836 and batch: 500, loss is 4.245735177993774 and perplexity is 69.80706189935012
At time: 262.7421541213989 and batch: 550, loss is 4.249170446395874 and perplexity is 70.04728026437982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.6561767578125 and perplexity of 105.23298089872043
Finished 9 epochs...
Completing Train Step...
At time: 266.80801725387573 and batch: 50, loss is 4.296377668380737 and perplexity is 73.43331154130091
At time: 269.36456298828125 and batch: 100, loss is 4.276146383285522 and perplexity is 71.96258875022538
At time: 271.88093733787537 and batch: 150, loss is 4.275403299331665 and perplexity is 71.90913436825349
At time: 274.4056668281555 and batch: 200, loss is 4.264276733398438 and perplexity is 71.11346737279212
At time: 276.93194007873535 and batch: 250, loss is 4.269307918548584 and perplexity is 71.47215394648124
At time: 279.45147466659546 and batch: 300, loss is 4.265388345718383 and perplexity is 71.19256193233655
At time: 281.9694893360138 and batch: 350, loss is 4.228667259216309 and perplexity is 68.6257109165887
At time: 284.48718881607056 and batch: 400, loss is 4.229195671081543 and perplexity is 68.66198313898953
At time: 287.01126646995544 and batch: 450, loss is 4.174385757446289 and perplexity is 64.99990169086671
At time: 289.5404016971588 and batch: 500, loss is 4.189043169021606 and perplexity is 65.95964852187699
At time: 292.0556013584137 and batch: 550, loss is 4.195789699554443 and perplexity is 66.40615178494019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.656075032552083 and perplexity of 105.22227659079255
Finished 10 epochs...
Completing Train Step...
At time: 296.1414489746094 and batch: 50, loss is 4.246327133178711 and perplexity is 69.84839678457942
At time: 298.6616599559784 and batch: 100, loss is 4.225164003372193 and perplexity is 68.3857181173005
At time: 301.1812918186188 and batch: 150, loss is 4.224921417236328 and perplexity is 68.36913070221392
At time: 303.6958291530609 and batch: 200, loss is 4.215254902839661 and perplexity is 67.7114235054574
At time: 306.2233588695526 and batch: 250, loss is 4.221269512176514 and perplexity is 68.11990847280514
At time: 308.7548966407776 and batch: 300, loss is 4.212307596206665 and perplexity is 67.51215098069753
At time: 311.28458857536316 and batch: 350, loss is 4.178958139419556 and perplexity is 65.29778657237155
At time: 313.81641244888306 and batch: 400, loss is 4.179084510803222 and perplexity is 65.30603886542694
At time: 316.34573554992676 and batch: 450, loss is 4.124766416549683 and perplexity is 61.85335964168005
At time: 318.8666386604309 and batch: 500, loss is 4.139884672164917 and perplexity is 62.79557895340554
At time: 321.3987166881561 and batch: 550, loss is 4.148096380233764 and perplexity is 63.31336094346522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.659283447265625 and perplexity of 105.56041544597326
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 325.5107333660126 and batch: 50, loss is 4.217809000015259 and perplexity is 67.88458610390599
At time: 328.06156945228577 and batch: 100, loss is 4.206141271591187 and perplexity is 67.09713003424731
At time: 330.59189343452454 and batch: 150, loss is 4.193279905319214 and perplexity is 66.23969498154622
At time: 333.1197009086609 and batch: 200, loss is 4.165941209793091 and perplexity is 64.45331799769399
At time: 335.6449749469757 and batch: 250, loss is 4.15799726486206 and perplexity is 63.943332718321464
At time: 338.1701545715332 and batch: 300, loss is 4.124127469062805 and perplexity is 61.81385121623075
At time: 340.6994104385376 and batch: 350, loss is 4.06252724647522 and perplexity is 58.12101174858089
At time: 343.2268681526184 and batch: 400, loss is 4.045636587142944 and perplexity is 57.14755385530807
At time: 345.75627088546753 and batch: 450, loss is 3.9700595712661744 and perplexity is 52.98768728928185
At time: 348.2829236984253 and batch: 500, loss is 3.951098713874817 and perplexity is 51.99246030191533
At time: 350.80605912208557 and batch: 550, loss is 3.958497323989868 and perplexity is 52.37855877942938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.591935221354166 and perplexity of 98.68522324033731
Finished 12 epochs...
Completing Train Step...
At time: 354.90458130836487 and batch: 50, loss is 4.151547613143921 and perplexity is 63.53224759555124
At time: 357.4315781593323 and batch: 100, loss is 4.135790238380432 and perplexity is 62.538992260494275
At time: 359.9514548778534 and batch: 150, loss is 4.128098115921021 and perplexity is 62.05978011565236
At time: 362.4735827445984 and batch: 200, loss is 4.10582275390625 and perplexity is 60.692659133462826
At time: 364.99781918525696 and batch: 250, loss is 4.105508456230163 and perplexity is 60.67358656912998
At time: 367.5254576206207 and batch: 300, loss is 4.0798260736465455 and perplexity is 59.13518378852242
At time: 370.0518352985382 and batch: 350, loss is 4.028166432380676 and perplexity is 56.157847578009395
At time: 372.5724091529846 and batch: 400, loss is 4.020676822662353 and perplexity is 55.73881835844573
At time: 375.0879635810852 and batch: 450, loss is 3.9562849187850953 and perplexity is 52.262804278513066
At time: 377.60777473449707 and batch: 500, loss is 3.94932288646698 and perplexity is 51.90021259815303
At time: 380.13358783721924 and batch: 550, loss is 3.962855186462402 and perplexity is 52.60731541805942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.589449055989584 and perplexity of 98.44018019130968
Finished 13 epochs...
Completing Train Step...
At time: 384.23183012008667 and batch: 50, loss is 4.122789769172669 and perplexity is 61.731218115716885
At time: 386.7843096256256 and batch: 100, loss is 4.1061956357955935 and perplexity is 60.715294546775354
At time: 389.304669380188 and batch: 150, loss is 4.0997085809707645 and perplexity is 60.32270585127867
At time: 391.85667848587036 and batch: 200, loss is 4.078888416290283 and perplexity is 59.079761236171784
At time: 394.3873255252838 and batch: 250, loss is 4.081327395439148 and perplexity is 59.224031406397856
At time: 396.8981018066406 and batch: 300, loss is 4.058487052917481 and perplexity is 57.88666533255795
At time: 399.42132449150085 and batch: 350, loss is 4.010627989768982 and perplexity is 55.181513110413064
At time: 401.94541025161743 and batch: 400, loss is 4.006546492576599 and perplexity is 54.95674891881164
At time: 404.46393752098083 and batch: 450, loss is 3.946492609977722 and perplexity is 51.75352832309396
At time: 406.97999024391174 and batch: 500, loss is 3.9442282485961915 and perplexity is 51.63647221088662
At time: 409.5018835067749 and batch: 550, loss is 3.958196749687195 and perplexity is 52.36281749648032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.589727783203125 and perplexity of 98.4676219726429
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 413.5860333442688 and batch: 50, loss is 4.11823341846466 and perplexity is 61.450588844534565
At time: 416.10461235046387 and batch: 100, loss is 4.1153362321853635 and perplexity is 61.27281269134516
At time: 418.62693524360657 and batch: 150, loss is 4.111199989318847 and perplexity is 61.019896878137494
At time: 421.15636801719666 and batch: 200, loss is 4.086208930015564 and perplexity is 59.51384234903046
At time: 423.6758737564087 and batch: 250, loss is 4.085960516929626 and perplexity is 59.49906016791615
At time: 426.18983459472656 and batch: 300, loss is 4.05517183303833 and perplexity is 57.69507606465923
At time: 428.712788105011 and batch: 350, loss is 3.9946006870269777 and perplexity is 54.30415194145576
At time: 431.2388083934784 and batch: 400, loss is 3.979055347442627 and perplexity is 53.466503095184514
At time: 433.75812578201294 and batch: 450, loss is 3.9062903785705565 and perplexity is 49.71418866470696
At time: 436.27890825271606 and batch: 500, loss is 3.8961796045303343 and perplexity is 49.214072278884444
At time: 438.79655838012695 and batch: 550, loss is 3.9113424682617186 and perplexity is 49.965984717323465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.572150675455729 and perplexity of 96.75196825808044
Finished 15 epochs...
Completing Train Step...
At time: 442.9096257686615 and batch: 50, loss is 4.1049255800247195 and perplexity is 60.638231683981516
At time: 445.4774353504181 and batch: 100, loss is 4.093140668869019 and perplexity is 59.92780986214777
At time: 447.99999165534973 and batch: 150, loss is 4.0875730800628665 and perplexity is 59.59508355988833
At time: 450.52525758743286 and batch: 200, loss is 4.063502836227417 and perplexity is 58.177741680100524
At time: 453.0586996078491 and batch: 250, loss is 4.066316552162171 and perplexity is 58.341667831575855
At time: 455.63059306144714 and batch: 300, loss is 4.039388437271118 and perplexity is 56.791600556805534
At time: 458.15551710128784 and batch: 350, loss is 3.983855118751526 and perplexity is 53.72374694469114
At time: 460.6756134033203 and batch: 400, loss is 3.9743720293045044 and perplexity is 53.21668789010675
At time: 463.1966161727905 and batch: 450, loss is 3.9073947525024413 and perplexity is 49.7691220466273
At time: 465.7269160747528 and batch: 500, loss is 3.9026085567474365 and perplexity is 49.531486424899086
At time: 468.2497844696045 and batch: 550, loss is 3.918612518310547 and perplexity is 50.33056357445158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.570583089192708 and perplexity of 96.60042001522432
Finished 16 epochs...
Completing Train Step...
At time: 472.2834482192993 and batch: 50, loss is 4.098200845718384 and perplexity is 60.2318237114673
At time: 474.78863859176636 and batch: 100, loss is 4.08374589920044 and perplexity is 59.36743829426112
At time: 477.2967720031738 and batch: 150, loss is 4.077550907135009 and perplexity is 59.000794335875256
At time: 479.80383706092834 and batch: 200, loss is 4.053762540817261 and perplexity is 57.61382411008898
At time: 482.30623841285706 and batch: 250, loss is 4.057896666526794 and perplexity is 57.85249991953321
At time: 484.81991052627563 and batch: 300, loss is 4.032542815208435 and perplexity is 56.40415439112691
At time: 487.3385760784149 and batch: 350, loss is 3.979204607009888 and perplexity is 53.47448407790366
At time: 489.8596715927124 and batch: 400, loss is 3.972121391296387 and perplexity is 53.09705106976307
At time: 492.3824152946472 and batch: 450, loss is 3.9075850105285643 and perplexity is 49.77859192238114
At time: 494.9027178287506 and batch: 500, loss is 3.9047478866577148 and perplexity is 49.637564042360935
At time: 497.41685366630554 and batch: 550, loss is 3.9208412408828734 and perplexity is 50.44286153157991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.570259602864583 and perplexity of 96.5691761538117
Finished 17 epochs...
Completing Train Step...
At time: 501.4802646636963 and batch: 50, loss is 4.092184281349182 and perplexity is 59.87052305126651
At time: 504.02554750442505 and batch: 100, loss is 4.076748361587525 and perplexity is 58.95346250659781
At time: 506.54670691490173 and batch: 150, loss is 4.070576086044311 and perplexity is 58.590706159504784
At time: 509.0600850582123 and batch: 200, loss is 4.047063403129577 and perplexity is 57.229151097027284
At time: 511.57404017448425 and batch: 250, loss is 4.052147521972656 and perplexity is 57.52085179467033
At time: 514.0888154506683 and batch: 300, loss is 4.027869186401367 and perplexity is 56.14115736428255
At time: 516.6017911434174 and batch: 350, loss is 3.9758256912231444 and perplexity is 53.29410321695262
At time: 519.176287651062 and batch: 400, loss is 3.970170011520386 and perplexity is 52.99353958609479
At time: 521.6983480453491 and batch: 450, loss is 3.907002873420715 and perplexity is 49.74962238978471
At time: 524.2144448757172 and batch: 500, loss is 3.905295739173889 and perplexity is 49.66476555724712
At time: 526.7303864955902 and batch: 550, loss is 3.921304473876953 and perplexity is 50.46623374232876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.570285034179688 and perplexity of 96.57163206618824
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 530.7966368198395 and batch: 50, loss is 4.0918925714492795 and perplexity is 59.85306077406352
At time: 533.3543086051941 and batch: 100, loss is 4.081225161552429 and perplexity is 59.217977012967374
At time: 535.8748795986176 and batch: 150, loss is 4.076613388061523 and perplexity is 58.945505886871906
At time: 538.3913698196411 and batch: 200, loss is 4.051479792594909 and perplexity is 57.48245625243637
At time: 540.9194903373718 and batch: 250, loss is 4.055067801475525 and perplexity is 57.68907426792363
At time: 543.4474520683289 and batch: 300, loss is 4.027908301353454 and perplexity is 56.14335336591094
At time: 545.9724633693695 and batch: 350, loss is 3.9718008613586426 and perplexity is 53.08003460257855
At time: 548.4937591552734 and batch: 400, loss is 3.9600390672683714 and perplexity is 52.45937535356201
At time: 551.0188119411469 and batch: 450, loss is 3.893768458366394 and perplexity is 49.09555289849096
At time: 553.5416581630707 and batch: 500, loss is 3.888116159439087 and perplexity is 48.818832946183576
At time: 556.0638282299042 and batch: 550, loss is 3.906496286392212 and perplexity is 49.72442625896447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.566383361816406 and perplexity of 96.19557530048421
Finished 19 epochs...
Completing Train Step...
At time: 560.1594445705414 and batch: 50, loss is 4.088962087631225 and perplexity is 59.67791909826669
At time: 562.6798100471497 and batch: 100, loss is 4.076182565689087 and perplexity is 58.92011631377202
At time: 565.1961379051208 and batch: 150, loss is 4.070907349586487 and perplexity is 58.610118339461984
At time: 567.7182910442352 and batch: 200, loss is 4.045932989120484 and perplexity is 57.16449501385476
At time: 570.2354910373688 and batch: 250, loss is 4.050459094047547 and perplexity is 57.42381392600063
At time: 572.7461721897125 and batch: 300, loss is 4.024324035644531 and perplexity is 55.94248087489827
At time: 575.2597980499268 and batch: 350, loss is 3.9698219108581543 and perplexity is 52.97509571021982
At time: 577.7791335582733 and batch: 400, loss is 3.959954571723938 and perplexity is 52.45494295734238
At time: 580.2938857078552 and batch: 450, loss is 3.895276050567627 and perplexity is 49.16962479223119
At time: 582.8565356731415 and batch: 500, loss is 3.890773015022278 and perplexity is 48.948709990938376
At time: 585.380616903305 and batch: 550, loss is 3.9087195777893067 and perplexity is 49.835101133750946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565730285644531 and perplexity of 96.13277277206525
Finished 20 epochs...
Completing Train Step...
At time: 589.4350111484528 and batch: 50, loss is 4.087244257926941 and perplexity is 59.57549059869774
At time: 591.9771294593811 and batch: 100, loss is 4.073321995735168 and perplexity is 58.751812037209284
At time: 594.4970219135284 and batch: 150, loss is 4.067695298194885 and perplexity is 58.42216165215732
At time: 597.0202081203461 and batch: 200, loss is 4.042816586494446 and perplexity is 56.98662473313823
At time: 599.5360314846039 and batch: 250, loss is 4.047870182991028 and perplexity is 57.27534105367831
At time: 602.0539517402649 and batch: 300, loss is 4.022365446090698 and perplexity is 55.83301974594482
At time: 604.5793082714081 and batch: 350, loss is 3.968837523460388 and perplexity is 52.92297335211241
At time: 607.0979607105255 and batch: 400, loss is 3.9601021003723145 and perplexity is 52.46268213503871
At time: 609.6202776432037 and batch: 450, loss is 3.8962644815444945 and perplexity is 49.21824959967089
At time: 612.1384251117706 and batch: 500, loss is 3.8923519372940065 and perplexity is 49.02605724589834
At time: 614.6647880077362 and batch: 550, loss is 3.9099643087387084 and perplexity is 49.897171048656716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565458170572916 and perplexity of 96.10661715454808
Finished 21 epochs...
Completing Train Step...
At time: 618.7239274978638 and batch: 50, loss is 4.085761742591858 and perplexity is 59.4872344569963
At time: 621.2366433143616 and batch: 100, loss is 4.071157336235046 and perplexity is 58.62477191804053
At time: 623.7504765987396 and batch: 150, loss is 4.065356421470642 and perplexity is 58.28567908825231
At time: 626.2723348140717 and batch: 200, loss is 4.040588340759277 and perplexity is 56.859785896144146
At time: 628.7944440841675 and batch: 250, loss is 4.046028642654419 and perplexity is 57.16996326134289
At time: 631.3084859848022 and batch: 300, loss is 4.020977058410645 and perplexity is 55.75555565672636
At time: 633.824743270874 and batch: 350, loss is 3.9681169033050536 and perplexity is 52.8848497288101
At time: 636.3459000587463 and batch: 400, loss is 3.9601389884948732 and perplexity is 52.46461742058138
At time: 638.8692245483398 and batch: 450, loss is 3.8968453550338746 and perplexity is 49.24684748112852
At time: 641.382383108139 and batch: 500, loss is 3.893338508605957 and perplexity is 49.07444881445677
At time: 643.8930730819702 and batch: 550, loss is 3.9106966495513915 and perplexity is 49.93372616722192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565334065755208 and perplexity of 96.09469060043232
Finished 22 epochs...
Completing Train Step...
At time: 648.0052394866943 and batch: 50, loss is 4.084388780593872 and perplexity is 59.40561678652246
At time: 650.5654354095459 and batch: 100, loss is 4.069359498023987 and perplexity is 58.519468750364524
At time: 653.0820980072021 and batch: 150, loss is 4.063488998413086 and perplexity is 58.17693663288303
At time: 655.5972483158112 and batch: 200, loss is 4.038815889358521 and perplexity is 56.7590939511363
At time: 658.118412733078 and batch: 250, loss is 4.044562802314759 and perplexity is 57.08622261318195
At time: 660.6373035907745 and batch: 300, loss is 4.019862599372864 and perplexity is 55.69345298568747
At time: 663.1656646728516 and batch: 350, loss is 3.967506432533264 and perplexity is 52.85257492619434
At time: 665.68665599823 and batch: 400, loss is 3.9600728797912597 and perplexity is 52.46114916738024
At time: 668.209041595459 and batch: 450, loss is 3.897173719406128 and perplexity is 49.26302104655313
At time: 670.7240207195282 and batch: 500, loss is 3.893974504470825 and perplexity is 49.105669888157955
At time: 673.2478613853455 and batch: 550, loss is 3.911137194633484 and perplexity is 49.955729070995105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565280151367188 and perplexity of 96.0895098536562
Finished 23 epochs...
Completing Train Step...
At time: 677.3044538497925 and batch: 50, loss is 4.083094525337219 and perplexity is 59.3287804883389
At time: 679.8179981708527 and batch: 100, loss is 4.067777576446534 and perplexity is 58.42696872323156
At time: 682.3395166397095 and batch: 150, loss is 4.061883273124695 and perplexity is 58.083595414763245
At time: 684.8560557365417 and batch: 200, loss is 4.0372958278656 and perplexity is 56.67288217824622
At time: 687.3704419136047 and batch: 250, loss is 4.043309850692749 and perplexity is 57.014741128736794
At time: 689.8837521076202 and batch: 300, loss is 4.018893895149231 and perplexity is 55.63952862514624
At time: 692.4022235870361 and batch: 350, loss is 3.9669311380386354 and perplexity is 52.82217787527874
At time: 694.9286286830902 and batch: 400, loss is 3.959915680885315 and perplexity is 52.4529029802893
At time: 697.4479246139526 and batch: 450, loss is 3.8973403358459473 and perplexity is 49.271229759568925
At time: 699.9611659049988 and batch: 500, loss is 3.8943996667861938 and perplexity is 49.12655220733836
At time: 702.4715631008148 and batch: 550, loss is 3.9114100551605224 and perplexity is 49.96936187740079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565263366699218 and perplexity of 96.08789703667331
Finished 24 epochs...
Completing Train Step...
At time: 706.479752779007 and batch: 50, loss is 4.08186038017273 and perplexity is 59.25560532445933
At time: 709.0161590576172 and batch: 100, loss is 4.066341109275818 and perplexity is 58.34310055213484
At time: 711.5259776115417 and batch: 150, loss is 4.060453572273254 and perplexity is 58.0006125833914
At time: 714.0352075099945 and batch: 200, loss is 4.035945014953613 and perplexity is 56.59637939934396
At time: 716.5449905395508 and batch: 250, loss is 4.0421957635879515 and perplexity is 56.95125711079109
At time: 719.051766872406 and batch: 300, loss is 4.018014616966248 and perplexity is 55.59062750350997
At time: 721.5613346099854 and batch: 350, loss is 3.966422004699707 and perplexity is 52.79529118852272
At time: 724.0763266086578 and batch: 400, loss is 3.9597050666809084 and perplexity is 52.44185681713956
At time: 726.5819351673126 and batch: 450, loss is 3.8973638010025025 and perplexity is 49.272385930253705
At time: 729.0881071090698 and batch: 500, loss is 3.89463002204895 and perplexity is 49.13787007069504
At time: 731.5985100269318 and batch: 550, loss is 3.9115373849868775 and perplexity is 49.97572487266266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565264383951823 and perplexity of 96.08799478238657
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 735.6582975387573 and batch: 50, loss is 4.081792674064636 and perplexity is 59.251593493854344
At time: 738.1755990982056 and batch: 100, loss is 4.067273998260498 and perplexity is 58.397553583339395
At time: 740.6877167224884 and batch: 150, loss is 4.0616706848144535 and perplexity is 58.07124883377715
At time: 743.1993315219879 and batch: 200, loss is 4.036478595733643 and perplexity is 56.62658619777755
At time: 745.7135081291199 and batch: 250, loss is 4.042232356071472 and perplexity is 56.95334113685801
At time: 748.2254509925842 and batch: 300, loss is 4.016966967582703 and perplexity is 55.53241851350532
At time: 750.736930847168 and batch: 350, loss is 3.964245104789734 and perplexity is 52.68048612878692
At time: 753.2507996559143 and batch: 400, loss is 3.9555899810791018 and perplexity is 52.22649750213408
At time: 755.7664523124695 and batch: 450, loss is 3.8928559494018553 and perplexity is 49.050773200397316
At time: 758.2826812267303 and batch: 500, loss is 3.8897213315963746 and perplexity is 48.897258504001854
At time: 760.7928459644318 and batch: 550, loss is 3.9077792310714723 and perplexity is 49.78826088645483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.56560312906901 and perplexity of 96.12054963502676
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 764.8731729984283 and batch: 50, loss is 4.081588592529297 and perplexity is 59.23950257149178
At time: 767.4118938446045 and batch: 100, loss is 4.067093014717102 and perplexity is 58.3869855435157
At time: 769.9335730075836 and batch: 150, loss is 4.061527390480041 and perplexity is 58.06292814899475
At time: 772.4860248565674 and batch: 200, loss is 4.036290311813355 and perplexity is 56.6159253258026
At time: 774.9986934661865 and batch: 250, loss is 4.041993703842163 and perplexity is 56.93975071678562
At time: 777.5177946090698 and batch: 300, loss is 4.016670618057251 and perplexity is 55.51596394590373
At time: 780.0395629405975 and batch: 350, loss is 3.9637083911895754 and perplexity is 52.652219381670996
At time: 782.574896812439 and batch: 400, loss is 3.9547368240356446 and perplexity is 52.18195909976242
At time: 785.0927708148956 and batch: 450, loss is 3.8919624614715578 and perplexity is 49.00696649986441
At time: 787.6127574443817 and batch: 500, loss is 3.888750195503235 and perplexity is 48.84979566158292
At time: 790.1235327720642 and batch: 550, loss is 3.9070181131362913 and perplexity is 49.750380565657125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.56563720703125 and perplexity of 96.12382528330107
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 794.2193915843964 and batch: 50, loss is 4.081545162200928 and perplexity is 59.23692983631044
At time: 796.7395286560059 and batch: 100, loss is 4.06704508304596 and perplexity is 58.384187024795004
At time: 799.2591552734375 and batch: 150, loss is 4.061481962203979 and perplexity is 58.060290510177985
At time: 801.7848904132843 and batch: 200, loss is 4.036245574951172 and perplexity is 56.613392563608286
At time: 804.3045489788055 and batch: 250, loss is 4.041932935714722 and perplexity is 56.93629069988803
At time: 806.8375430107117 and batch: 300, loss is 4.016610264778137 and perplexity is 55.51261347654335
At time: 809.3573913574219 and batch: 350, loss is 3.9636010408401487 and perplexity is 52.646567450896086
At time: 811.8791406154633 and batch: 400, loss is 3.954564166069031 and perplexity is 52.17295024655785
At time: 814.3982148170471 and batch: 450, loss is 3.8917823743820192 and perplexity is 48.99814177253392
At time: 816.9164044857025 and batch: 500, loss is 3.8885550451278688 and perplexity is 48.84026353575222
At time: 819.4439840316772 and batch: 550, loss is 3.9068662405014036 and perplexity is 49.74282541799857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565642293294271 and perplexity of 96.12431419560245
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 823.4760055541992 and batch: 50, loss is 4.081536717414856 and perplexity is 59.23642959522267
At time: 826.0170454978943 and batch: 100, loss is 4.0670356178283695 and perplexity is 58.38363440837632
At time: 828.5311777591705 and batch: 150, loss is 4.061472430229187 and perplexity is 58.05973708359002
At time: 831.0620334148407 and batch: 200, loss is 4.036236848831177 and perplexity is 56.612898550506834
At time: 833.5857768058777 and batch: 250, loss is 4.041921005249024 and perplexity is 56.93561142747689
At time: 836.1288688182831 and batch: 300, loss is 4.016598253250122 and perplexity is 55.51194668923597
At time: 838.6411287784576 and batch: 350, loss is 3.963579730987549 and perplexity is 52.645445572257394
At time: 841.1588838100433 and batch: 400, loss is 3.95452956199646 and perplexity is 52.17114488123796
At time: 843.6776371002197 and batch: 450, loss is 3.891746096611023 and perplexity is 48.996364261409724
At time: 846.1995651721954 and batch: 500, loss is 3.8885157012939455 and perplexity is 48.83834201033524
At time: 848.7180190086365 and batch: 550, loss is 3.9068357706069947 and perplexity is 49.741309782451225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.565643310546875 and perplexity of 96.12441197836111
Annealing...
Model not improving. Stopping early with 96.08789703667331loss at 28 epochs.
Finished Training.
Improved accuracyfrom -151.81078163385433 to -96.08789703667331
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f230e50feb8>
SETTINGS FOR THIS RUN
{'anneal': 7.164073193929745, 'num_layers': 1, 'dropout': 0.7817805766891953, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 28.173359026361044, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.034071922302246 and batch: 50, loss is 7.306287393569947 and perplexity is 1489.636476352023
At time: 5.541814804077148 and batch: 100, loss is 6.747125797271728 and perplexity is 851.6075488308044
At time: 8.051424741744995 and batch: 150, loss is 6.68043083190918 and perplexity is 796.6622656147041
At time: 10.557965993881226 and batch: 200, loss is 6.629111242294312 and perplexity is 756.8092516010158
At time: 13.084712266921997 and batch: 250, loss is 6.530204391479492 and perplexity is 685.538315363828
At time: 15.612201690673828 and batch: 300, loss is 6.5177707862854 and perplexity is 677.0673739058524
At time: 18.137499570846558 and batch: 350, loss is 6.4703457736968994 and perplexity is 645.7069568509103
At time: 20.680373907089233 and batch: 400, loss is 6.485981168746949 and perplexity is 655.8821797324425
At time: 23.217926502227783 and batch: 450, loss is 6.471157665252686 and perplexity is 646.2314137489446
At time: 25.748013734817505 and batch: 500, loss is 6.435781269073487 and perplexity is 623.7697245392022
At time: 28.2809841632843 and batch: 550, loss is 6.4038754749298095 and perplexity is 604.1819987196137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.832437133789062 and perplexity of 341.1891903313647
Finished 1 epochs...
Completing Train Step...
At time: 32.415056467056274 and batch: 50, loss is 6.175947437286377 and perplexity is 481.0385619595572
At time: 34.93213701248169 and batch: 100, loss is 6.154469776153564 and perplexity is 470.8171377932245
At time: 37.44825792312622 and batch: 150, loss is 6.104445419311523 and perplexity is 447.8442068246736
At time: 39.958977937698364 and batch: 200, loss is 6.077388753890991 and perplexity is 435.8894926031151
At time: 42.478912591934204 and batch: 250, loss is 6.080318613052368 and perplexity is 437.1684601088021
At time: 45.01250410079956 and batch: 300, loss is 6.030655250549317 and perplexity is 415.98751614714956
At time: 47.52486562728882 and batch: 350, loss is 6.008928165435791 and perplexity is 407.04679954768517
At time: 50.03832006454468 and batch: 400, loss is 6.000601968765259 and perplexity is 403.67171813461147
At time: 52.56015133857727 and batch: 450, loss is 5.994955711364746 and perplexity is 401.39890618749706
At time: 55.081730365753174 and batch: 500, loss is 5.967566680908203 and perplexity is 390.5541706478772
At time: 57.598222970962524 and batch: 550, loss is 5.918570718765259 and perplexity is 371.87981301045346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.6482177734375 and perplexity of 283.7852452559024
Finished 2 epochs...
Completing Train Step...
At time: 61.682212114334106 and batch: 50, loss is 5.93919976234436 and perplexity is 379.6310129198123
At time: 64.2030200958252 and batch: 100, loss is 5.961743640899658 and perplexity is 388.2865666684888
At time: 66.72821545600891 and batch: 150, loss is 5.98300537109375 and perplexity is 396.6306008566838
At time: 69.26907396316528 and batch: 200, loss is 5.930651302337647 and perplexity is 376.3995839376152
At time: 71.78905200958252 and batch: 250, loss is 5.891263275146485 and perplexity is 361.86212718417187
At time: 74.30671787261963 and batch: 300, loss is 5.8975173854827885 and perplexity is 364.13234454930677
At time: 76.82966876029968 and batch: 350, loss is 5.868578329086303 and perplexity is 353.7457126215641
At time: 79.34766554832458 and batch: 400, loss is 5.8511370658874515 and perplexity is 347.6294334057666
At time: 81.87148785591125 and batch: 450, loss is 5.789317922592163 and perplexity is 326.79005223075166
At time: 84.38378834724426 and batch: 500, loss is 5.817380380630493 and perplexity is 336.0904703219029
At time: 86.9044873714447 and batch: 550, loss is 5.860621395111084 and perplexity is 350.9421500152616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.656930033365885 and perplexity of 286.26845756494265
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 90.93847560882568 and batch: 50, loss is 5.810071287155151 and perplexity is 333.64290928066384
At time: 93.48075199127197 and batch: 100, loss is 5.738625574111938 and perplexity is 310.6371696750979
At time: 95.99447774887085 and batch: 150, loss is 5.7369969367980955 and perplexity is 310.1316661421921
At time: 98.52649664878845 and batch: 200, loss is 5.7257969856262205 and perplexity is 306.6775854980921
At time: 101.05506920814514 and batch: 250, loss is 5.681981029510498 and perplexity is 293.53034660487333
At time: 103.56808662414551 and batch: 300, loss is 5.685350065231323 and perplexity is 294.52092854374104
At time: 106.08043098449707 and batch: 350, loss is 5.625217399597168 and perplexity is 277.33256995887245
At time: 108.59395241737366 and batch: 400, loss is 5.636248559951782 and perplexity is 280.4088060417689
At time: 111.11424016952515 and batch: 450, loss is 5.586993141174316 and perplexity is 266.93178561602207
At time: 113.63638019561768 and batch: 500, loss is 5.545871791839599 and perplexity is 256.1778146495917
At time: 116.15189719200134 and batch: 550, loss is 5.535190896987915 and perplexity is 253.45616703805726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.409915669759115 and perplexity of 223.61272957005303
Finished 4 epochs...
Completing Train Step...
At time: 120.23121500015259 and batch: 50, loss is 5.644518575668335 and perplexity is 282.7374067854864
At time: 122.74546599388123 and batch: 100, loss is 5.62561487197876 and perplexity is 277.442823905945
At time: 125.2616822719574 and batch: 150, loss is 5.642453517913818 and perplexity is 282.15414015802475
At time: 127.78340673446655 and batch: 200, loss is 5.626988220214844 and perplexity is 277.82411127901247
At time: 130.29553198814392 and batch: 250, loss is 5.590098285675049 and perplexity is 267.76193558298075
At time: 132.8136019706726 and batch: 300, loss is 5.5988939762115475 and perplexity is 270.12747473032334
At time: 135.33665204048157 and batch: 350, loss is 5.550022182464599 and perplexity is 257.2432621320411
At time: 137.86173701286316 and batch: 400, loss is 5.564098558425903 and perplexity is 260.8899206579947
At time: 140.389790058136 and batch: 450, loss is 5.521121559143066 and perplexity is 249.91517471411916
At time: 142.91387796401978 and batch: 500, loss is 5.487844352722168 and perplexity is 241.73554822746317
At time: 145.4431300163269 and batch: 550, loss is 5.4802368545532225 and perplexity is 239.90352288721314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.375475565592448 and perplexity of 216.04259030655908
Finished 5 epochs...
Completing Train Step...
At time: 149.50999569892883 and batch: 50, loss is 5.570118751525879 and perplexity is 262.46526554049257
At time: 152.06309390068054 and batch: 100, loss is 5.550552968978882 and perplexity is 257.3798396300113
At time: 154.5823254585266 and batch: 150, loss is 5.573203344345092 and perplexity is 263.2761139395306
At time: 157.1065742969513 and batch: 200, loss is 5.569919271469116 and perplexity is 262.4129141761245
At time: 159.6234745979309 and batch: 250, loss is 5.5369706630706785 and perplexity is 253.90766138560375
At time: 162.1539580821991 and batch: 300, loss is 5.543766136169434 and perplexity is 255.63895990197872
At time: 164.6765694618225 and batch: 350, loss is 5.49834490776062 and perplexity is 244.2872795072829
At time: 167.20368194580078 and batch: 400, loss is 5.513528738021851 and perplexity is 248.02479922034277
At time: 169.72132110595703 and batch: 450, loss is 5.474562578201294 and perplexity is 238.54609884225522
At time: 172.25093173980713 and batch: 500, loss is 5.451002330780029 and perplexity is 232.99158354295238
At time: 174.7806715965271 and batch: 550, loss is 5.44019660949707 and perplexity is 230.4874950366331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.369792683919271 and perplexity of 214.8183277937881
Finished 6 epochs...
Completing Train Step...
At time: 178.9120798110962 and batch: 50, loss is 5.523665704727173 and perplexity is 250.55180479868687
At time: 181.42832374572754 and batch: 100, loss is 5.501676321029663 and perplexity is 245.10245848680051
At time: 183.98335814476013 and batch: 150, loss is 5.524345045089722 and perplexity is 250.72207258093624
At time: 186.50815296173096 and batch: 200, loss is 5.525693016052246 and perplexity is 251.06026654104934
At time: 189.0270869731903 and batch: 250, loss is 5.491771278381347 and perplexity is 242.68669206783096
At time: 191.54478430747986 and batch: 300, loss is 5.501050662994385 and perplexity is 244.9491561266094
At time: 194.06693315505981 and batch: 350, loss is 5.458660764694214 and perplexity is 234.7827843297782
At time: 196.5939164161682 and batch: 400, loss is 5.472449550628662 and perplexity is 238.04257652367653
At time: 199.11730790138245 and batch: 450, loss is 5.433859090805054 and perplexity is 229.03139513232355
At time: 201.63787484169006 and batch: 500, loss is 5.414141750335693 and perplexity is 224.55973463357753
At time: 204.1655650138855 and batch: 550, loss is 5.402253065109253 and perplexity is 221.9058216607452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.357761637369792 and perplexity of 212.24932338593004
Finished 7 epochs...
Completing Train Step...
At time: 208.2797567844391 and batch: 50, loss is 5.483112297058105 and perplexity is 240.5943444067888
At time: 210.82729840278625 and batch: 100, loss is 5.462497663497925 and perplexity is 235.68535253825993
At time: 213.34464311599731 and batch: 150, loss is 5.487680358886719 and perplexity is 241.6959083381825
At time: 215.85257291793823 and batch: 200, loss is 5.490668983459472 and perplexity is 242.41932714412874
At time: 218.36574292182922 and batch: 250, loss is 5.458211259841919 and perplexity is 234.67727204490532
At time: 220.87648844718933 and batch: 300, loss is 5.465948038101196 and perplexity is 236.49995983502117
At time: 223.38175988197327 and batch: 350, loss is 5.42412073135376 and perplexity is 226.81183008315992
At time: 225.89186573028564 and batch: 400, loss is 5.442827720642089 and perplexity is 231.09473175695078
At time: 228.40766429901123 and batch: 450, loss is 5.407949810028076 and perplexity is 223.17357011402015
At time: 230.93373012542725 and batch: 500, loss is 5.383673944473267 and perplexity is 217.8210696786862
At time: 233.4653356075287 and batch: 550, loss is 5.368380689620972 and perplexity is 214.51521958366502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.35162353515625 and perplexity of 210.9505055604019
Finished 8 epochs...
Completing Train Step...
At time: 237.59278225898743 and batch: 50, loss is 5.446943664550782 and perplexity is 232.04786488612038
At time: 240.1103093624115 and batch: 100, loss is 5.423267135620117 and perplexity is 226.61830707961673
At time: 242.62919640541077 and batch: 150, loss is 5.449818725585938 and perplexity is 232.7159766315526
At time: 245.14821910858154 and batch: 200, loss is 5.456811685562133 and perplexity is 234.3490535075889
At time: 247.71844601631165 and batch: 250, loss is 5.4278889083862305 and perplexity is 227.66810950499357
At time: 250.2337396144867 and batch: 300, loss is 5.437455091476441 and perplexity is 229.85647478737764
At time: 252.74892854690552 and batch: 350, loss is 5.397489347457886 and perplexity is 220.8512388428547
At time: 255.26649141311646 and batch: 400, loss is 5.412991046905518 and perplexity is 224.30148159148254
At time: 257.7904267311096 and batch: 450, loss is 5.360222759246827 and perplexity is 212.77233817667582
At time: 260.3115372657776 and batch: 500, loss is 5.341140651702881 and perplexity is 208.75068636526592
At time: 262.82942366600037 and batch: 550, loss is 5.33520586013794 and perplexity is 207.51546357311045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.336263020833333 and perplexity of 207.7349567642316
Finished 9 epochs...
Completing Train Step...
At time: 266.88505935668945 and batch: 50, loss is 5.416460599899292 and perplexity is 225.08105907911647
At time: 269.43484330177307 and batch: 100, loss is 5.3931914234161376 and perplexity is 219.90407387334318
At time: 271.95543384552 and batch: 150, loss is 5.418208074569702 and perplexity is 225.47472639029638
At time: 274.47105646133423 and batch: 200, loss is 5.424362373352051 and perplexity is 226.86664396941902
At time: 276.98683285713196 and batch: 250, loss is 5.39834508895874 and perplexity is 221.04031130048386
At time: 279.5037202835083 and batch: 300, loss is 5.411050033569336 and perplexity is 223.86653168273227
At time: 282.0262117385864 and batch: 350, loss is 5.379095916748047 and perplexity is 216.826157887084
At time: 284.5481333732605 and batch: 400, loss is 5.404065027236938 and perplexity is 222.30827110697436
At time: 287.06279945373535 and batch: 450, loss is 5.348593521118164 and perplexity is 210.31228995661917
At time: 289.5790274143219 and batch: 500, loss is 5.331250381469727 and perplexity is 206.69626181924326
At time: 292.0926048755646 and batch: 550, loss is 5.319908971786499 and perplexity is 204.3652781400724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.333993530273437 and perplexity of 207.26403881494033
Finished 10 epochs...
Completing Train Step...
At time: 296.1989858150482 and batch: 50, loss is 5.391952114105225 and perplexity is 219.63171351131015
At time: 298.72504448890686 and batch: 100, loss is 5.368626899719239 and perplexity is 214.5680418993853
At time: 301.24406003952026 and batch: 150, loss is 5.397097339630127 and perplexity is 220.76468039536107
At time: 303.76523303985596 and batch: 200, loss is 5.404972915649414 and perplexity is 222.51019385810312
At time: 306.2837736606598 and batch: 250, loss is 5.3776864814758305 and perplexity is 216.5207707144997
At time: 308.80697536468506 and batch: 300, loss is 5.386026744842529 and perplexity is 218.33416253770076
At time: 311.3779249191284 and batch: 350, loss is 5.35375792503357 and perplexity is 211.4012370313165
At time: 313.89275097846985 and batch: 400, loss is 5.3774418449401855 and perplexity is 216.46780830179162
At time: 316.4155683517456 and batch: 450, loss is 5.32712306022644 and perplexity is 205.8449180400988
At time: 318.93824768066406 and batch: 500, loss is 5.311324148178101 and perplexity is 202.6183475499799
At time: 321.46176290512085 and batch: 550, loss is 5.298175716400147 and perplexity is 199.97167197680375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.331709798177084 and perplexity of 206.79124335165207
Finished 11 epochs...
Completing Train Step...
At time: 325.4940757751465 and batch: 50, loss is 5.372515640258789 and perplexity is 215.4040658289345
At time: 328.0446207523346 and batch: 100, loss is 5.348392858505249 and perplexity is 210.2700923768682
At time: 330.5662133693695 and batch: 150, loss is 5.377438344955444 and perplexity is 216.4670506690915
At time: 333.0914273262024 and batch: 200, loss is 5.384583978652954 and perplexity is 218.01938452010555
At time: 335.61643242836 and batch: 250, loss is 5.356966571807861 and perplexity is 212.08063832509015
At time: 338.1334750652313 and batch: 300, loss is 5.364401988983154 and perplexity is 213.66342338738554
At time: 340.65258622169495 and batch: 350, loss is 5.331638832092285 and perplexity is 206.7765687074479
At time: 343.1677825450897 and batch: 400, loss is 5.353797025680542 and perplexity is 211.40950311805886
At time: 345.6875925064087 and batch: 450, loss is 5.303272371292114 and perplexity is 200.99346021623302
At time: 348.21306824684143 and batch: 500, loss is 5.290710973739624 and perplexity is 198.48449251783686
At time: 350.7343096733093 and batch: 550, loss is 5.2761749744415285 and perplexity is 195.62019025603203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.330348205566406 and perplexity of 206.5098695244844
Finished 12 epochs...
Completing Train Step...
At time: 354.8438501358032 and batch: 50, loss is 5.349484825134278 and perplexity is 210.4998257085648
At time: 357.35998940467834 and batch: 100, loss is 5.325216512680054 and perplexity is 205.4528387943003
At time: 359.8876473903656 and batch: 150, loss is 5.356335391998291 and perplexity is 211.94681954447748
At time: 362.4124653339386 and batch: 200, loss is 5.361726083755493 and perplexity is 213.0924445990519
At time: 364.9338674545288 and batch: 250, loss is 5.3347515106201175 and perplexity is 207.4212004381219
At time: 367.45442366600037 and batch: 300, loss is 5.344419012069702 and perplexity is 209.4361693581865
At time: 369.97398710250854 and batch: 350, loss is 5.3219661521911625 and perplexity is 204.7861271183724
At time: 372.51150274276733 and batch: 400, loss is 5.336619358062745 and perplexity is 207.80899365344348
At time: 375.0651288032532 and batch: 450, loss is 5.286846294403076 and perplexity is 197.71889395035845
At time: 377.5850477218628 and batch: 500, loss is 5.254859228134155 and perplexity is 191.49452691503856
At time: 380.10389971733093 and batch: 550, loss is 5.241005992889404 and perplexity is 188.85999868826926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.321812438964844 and perplexity of 204.75465120126205
Finished 13 epochs...
Completing Train Step...
At time: 384.1654989719391 and batch: 50, loss is 5.32199501991272 and perplexity is 204.79203891259846
At time: 386.7177724838257 and batch: 100, loss is 5.301317796707154 and perplexity is 200.60098719100037
At time: 389.239018201828 and batch: 150, loss is 5.327652587890625 and perplexity is 205.95394748323838
At time: 391.75433111190796 and batch: 200, loss is 5.337478609085083 and perplexity is 207.98763047963823
At time: 394.2759175300598 and batch: 250, loss is 5.317499437332153 and perplexity is 203.87344574263466
At time: 396.8034405708313 and batch: 300, loss is 5.326825370788574 and perplexity is 205.78364930214047
At time: 399.3215148448944 and batch: 350, loss is 5.297472743988037 and perplexity is 199.8311468066465
At time: 401.8391602039337 and batch: 400, loss is 5.325953311920166 and perplexity is 205.60427207091365
At time: 404.3591320514679 and batch: 450, loss is 5.289287176132202 and perplexity is 198.2020918606765
At time: 406.8799695968628 and batch: 500, loss is 5.253601322174072 and perplexity is 191.253796248322
At time: 409.39969849586487 and batch: 550, loss is 5.238621044158935 and perplexity is 188.41011396340767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.334004720052083 and perplexity of 207.26635806663185
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 413.5097041130066 and batch: 50, loss is 5.315793399810791 and perplexity is 203.5259265192952
At time: 416.03928661346436 and batch: 100, loss is 5.279994974136352 and perplexity is 196.3688884260604
At time: 418.5590877532959 and batch: 150, loss is 5.294079656600952 and perplexity is 199.15425129510848
At time: 421.0864956378937 and batch: 200, loss is 5.288759489059448 and perplexity is 198.0975307692032
At time: 423.61462211608887 and batch: 250, loss is 5.250243396759033 and perplexity is 190.6126573159736
At time: 426.13422989845276 and batch: 300, loss is 5.239140110015869 and perplexity is 188.5079366066678
At time: 428.65397810935974 and batch: 350, loss is 5.1864962387084965 and perplexity is 178.84083828633612
At time: 431.17631316185 and batch: 400, loss is 5.190325412750244 and perplexity is 179.52696379052796
At time: 433.69948148727417 and batch: 450, loss is 5.137852220535279 and perplexity is 170.34950197028942
At time: 436.22734665870667 and batch: 500, loss is 5.098035287857056 and perplexity is 163.6999678296737
At time: 438.77677297592163 and batch: 550, loss is 5.132283668518067 and perplexity is 169.40353817618868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.237414042154948 and perplexity of 188.18283976608032
Finished 15 epochs...
Completing Train Step...
At time: 442.8640511035919 and batch: 50, loss is 5.25847131729126 and perplexity is 192.1874729573801
At time: 445.42610716819763 and batch: 100, loss is 5.230922479629516 and perplexity is 186.96519557821156
At time: 447.9524211883545 and batch: 150, loss is 5.2494949626922605 and perplexity is 190.47004968254265
At time: 450.48115706443787 and batch: 200, loss is 5.248597402572631 and perplexity is 190.2991680616999
At time: 453.0135805606842 and batch: 250, loss is 5.217140645980835 and perplexity is 184.4061470503331
At time: 455.5379412174225 and batch: 300, loss is 5.210366249084473 and perplexity is 183.16112850637458
At time: 458.0701127052307 and batch: 350, loss is 5.163043632507324 and perplexity is 174.69535572640302
At time: 460.59332060813904 and batch: 400, loss is 5.177697038650512 and perplexity is 177.27408516771214
At time: 463.1215922832489 and batch: 450, loss is 5.13860013961792 and perplexity is 170.47695727071581
At time: 465.6452302932739 and batch: 500, loss is 5.108237657546997 and perplexity is 165.37864409677562
At time: 468.16809344291687 and batch: 550, loss is 5.132146053314209 and perplexity is 169.38022727775208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.231504821777344 and perplexity of 187.07410499996914
Finished 16 epochs...
Completing Train Step...
At time: 472.27869629859924 and batch: 50, loss is 5.241050157546997 and perplexity is 188.86833980963442
At time: 474.80292534828186 and batch: 100, loss is 5.212312326431275 and perplexity is 183.51792128994805
At time: 477.3201823234558 and batch: 150, loss is 5.2318880653381346 and perplexity is 187.14581368616945
At time: 479.8487515449524 and batch: 200, loss is 5.2316828632354735 and perplexity is 187.1074149115862
At time: 482.377375125885 and batch: 250, loss is 5.20359317779541 and perplexity is 181.9247568687266
At time: 484.8985493183136 and batch: 300, loss is 5.1991453456878665 and perplexity is 181.11738295680982
At time: 487.42159152030945 and batch: 350, loss is 5.154995727539062 and perplexity is 173.29506636090176
At time: 489.948602437973 and batch: 400, loss is 5.175226488113403 and perplexity is 176.83666114295434
At time: 492.47647857666016 and batch: 450, loss is 5.140374994277954 and perplexity is 170.7797977621865
At time: 495.00385212898254 and batch: 500, loss is 5.1130508613586425 and perplexity is 166.17656395190542
At time: 497.5210099220276 and batch: 550, loss is 5.129876203536988 and perplexity is 168.99619561857259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2298126220703125 and perplexity of 186.75780595035243
Finished 17 epochs...
Completing Train Step...
At time: 501.62252593040466 and batch: 50, loss is 5.229690322875976 and perplexity is 186.73496701776872
At time: 504.17161774635315 and batch: 100, loss is 5.200558338165283 and perplexity is 181.37348134639683
At time: 506.7075252532959 and batch: 150, loss is 5.22073184967041 and perplexity is 185.06957763016598
At time: 509.2386419773102 and batch: 200, loss is 5.2213036155700685 and perplexity is 185.175424360611
At time: 511.7612192630768 and batch: 250, loss is 5.195675849914551 and perplexity is 180.4900857938471
At time: 514.2866234779358 and batch: 300, loss is 5.193257761001587 and perplexity is 180.05417197008583
At time: 516.8124649524689 and batch: 350, loss is 5.1511476039886475 and perplexity is 172.6294869720559
At time: 519.333564043045 and batch: 400, loss is 5.174577341079712 and perplexity is 176.7219053996305
At time: 521.8661205768585 and batch: 450, loss is 5.1414564418792725 and perplexity is 170.96458706679553
At time: 524.3895690441132 and batch: 500, loss is 5.114821290969848 and perplexity is 166.47102844883207
At time: 526.9051992893219 and batch: 550, loss is 5.127231931686401 and perplexity is 168.54991404058904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.228992207845052 and perplexity of 186.60465002391325
Finished 18 epochs...
Completing Train Step...
At time: 530.9487302303314 and batch: 50, loss is 5.221454744338989 and perplexity is 185.20341180933005
At time: 533.4969408512115 and batch: 100, loss is 5.192171421051025 and perplexity is 179.85867813541648
At time: 536.0162851810455 and batch: 150, loss is 5.213148155212402 and perplexity is 183.67137497198323
At time: 538.5329792499542 and batch: 200, loss is 5.214270648956298 and perplexity is 183.87766069688
At time: 541.052978515625 and batch: 250, loss is 5.190592222213745 and perplexity is 179.57486967400814
At time: 543.5853252410889 and batch: 300, loss is 5.18929762840271 and perplexity is 179.3425435755231
At time: 546.1070430278778 and batch: 350, loss is 5.148310823440552 and perplexity is 172.14046894773585
At time: 548.6265394687653 and batch: 400, loss is 5.173303174972534 and perplexity is 176.49687573040777
At time: 551.1442627906799 and batch: 450, loss is 5.140798320770264 and perplexity is 170.8521086794142
At time: 553.6739346981049 and batch: 500, loss is 5.114812049865723 and perplexity is 166.4694900798324
At time: 556.2011120319366 and batch: 550, loss is 5.1243191337585445 and perplexity is 168.05967652825018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.228529357910157 and perplexity of 186.51830005887788
Finished 19 epochs...
Completing Train Step...
At time: 560.4227991104126 and batch: 50, loss is 5.214833660125732 and perplexity is 183.9812150220472
At time: 562.9540829658508 and batch: 100, loss is 5.185521955490112 and perplexity is 178.6666815116197
At time: 565.5225355625153 and batch: 150, loss is 5.206807947158813 and perplexity is 182.51054408445347
At time: 568.0437970161438 and batch: 200, loss is 5.208511934280396 and perplexity is 182.8218048179531
At time: 570.5698447227478 and batch: 250, loss is 5.186191854476928 and perplexity is 178.7864102391412
At time: 573.0928831100464 and batch: 300, loss is 5.18577615737915 and perplexity is 178.71210469265267
At time: 575.6167516708374 and batch: 350, loss is 5.145632743835449 and perplexity is 171.68007982316004
At time: 578.1389801502228 and batch: 400, loss is 5.1716384601593015 and perplexity is 176.20330319196438
At time: 580.657470703125 and batch: 450, loss is 5.139448461532592 and perplexity is 170.62163796874196
At time: 583.1807432174683 and batch: 500, loss is 5.114180965423584 and perplexity is 166.36446691727983
At time: 585.7123172283173 and batch: 550, loss is 5.121608619689941 and perplexity is 167.60476521093332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.227955627441406 and perplexity of 186.4113195190867
Finished 20 epochs...
Completing Train Step...
At time: 589.7719864845276 and batch: 50, loss is 5.209062376022339 and perplexity is 182.92246527197855
At time: 592.3218078613281 and batch: 100, loss is 5.1800001525878905 and perplexity is 177.6828381056118
At time: 594.842257976532 and batch: 150, loss is 5.201981372833252 and perplexity is 181.63176582850298
At time: 597.3767611980438 and batch: 200, loss is 5.203911304473877 and perplexity is 181.98264119414605
At time: 599.8969068527222 and batch: 250, loss is 5.1825417137146 and perplexity is 178.13500426038684
At time: 602.4161403179169 and batch: 300, loss is 5.182499628067017 and perplexity is 178.12750749112968
At time: 604.9438729286194 and batch: 350, loss is 5.1429688835144045 and perplexity is 171.22335666403737
At time: 607.4717516899109 and batch: 400, loss is 5.169975728988647 and perplexity is 175.91056790486132
At time: 609.9954254627228 and batch: 450, loss is 5.137617664337158 and perplexity is 170.3095501244153
At time: 612.5114357471466 and batch: 500, loss is 5.112817163467407 and perplexity is 166.13773337682562
At time: 615.0236246585846 and batch: 550, loss is 5.118464202880859 and perplexity is 167.0785736862129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.227667744954427 and perplexity of 186.35766268862403
Finished 21 epochs...
Completing Train Step...
At time: 619.0678746700287 and batch: 50, loss is 5.204008750915527 and perplexity is 182.0003756190368
At time: 621.5864291191101 and batch: 100, loss is 5.174845180511475 and perplexity is 176.76924483375325
At time: 624.1040170192719 and batch: 150, loss is 5.197039604187012 and perplexity is 180.73639783586032
At time: 626.6269760131836 and batch: 200, loss is 5.199328851699829 and perplexity is 181.1506221351546
At time: 629.1819427013397 and batch: 250, loss is 5.1783321094512935 and perplexity is 177.38670251914812
At time: 631.7025768756866 and batch: 300, loss is 5.179350776672363 and perplexity is 177.56749260529176
At time: 634.2234692573547 and batch: 350, loss is 5.1403281784057615 and perplexity is 170.77180274414977
At time: 636.7460041046143 and batch: 400, loss is 5.167465534210205 and perplexity is 175.46955186548635
At time: 639.2767403125763 and batch: 450, loss is 5.135409774780274 and perplexity is 169.93394025239934
At time: 641.8009223937988 and batch: 500, loss is 5.1111620426177975 and perplexity is 165.86298278584408
At time: 644.3328065872192 and batch: 550, loss is 5.115452632904053 and perplexity is 166.57616177396832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.227191162109375 and perplexity of 186.26886898400144
Finished 22 epochs...
Completing Train Step...
At time: 648.3837599754333 and batch: 50, loss is 5.199209203720093 and perplexity is 181.1289491257799
At time: 650.9364521503448 and batch: 100, loss is 5.1706822395324705 and perplexity is 176.03489448970606
At time: 653.4551632404327 and batch: 150, loss is 5.193570346832275 and perplexity is 180.11046315045428
At time: 655.9783411026001 and batch: 200, loss is 5.196060972213745 and perplexity is 180.55960993745953
At time: 658.4982900619507 and batch: 250, loss is 5.176167106628418 and perplexity is 177.00307523437672
At time: 661.0248465538025 and batch: 300, loss is 5.176728754043579 and perplexity is 177.10251647690217
At time: 663.5467202663422 and batch: 350, loss is 5.137714309692383 and perplexity is 170.32601054678406
At time: 666.0638179779053 and batch: 400, loss is 5.165028114318847 and perplexity is 175.0423796998949
At time: 668.5823588371277 and batch: 450, loss is 5.132884492874146 and perplexity is 169.50535053053923
At time: 671.1162068843842 and batch: 500, loss is 5.109367370605469 and perplexity is 165.56558008273484
At time: 673.6378540992737 and batch: 550, loss is 5.112573585510254 and perplexity is 166.0972708152177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.226820373535157 and perplexity of 186.19981541857726
Finished 23 epochs...
Completing Train Step...
At time: 677.7035965919495 and batch: 50, loss is 5.194916162490845 and perplexity is 180.3530218150483
At time: 680.2230861186981 and batch: 100, loss is 5.166583805084229 and perplexity is 175.31490343991436
At time: 682.7500290870667 and batch: 150, loss is 5.18958197593689 and perplexity is 179.39354643648758
At time: 685.2731368541718 and batch: 200, loss is 5.192290086746215 and perplexity is 179.88002245688773
At time: 687.7966215610504 and batch: 250, loss is 5.173007640838623 and perplexity is 176.4447225859976
At time: 690.3171157836914 and batch: 300, loss is 5.1738080978393555 and perplexity is 176.58601554135723
At time: 692.8663821220398 and batch: 350, loss is 5.13513861656189 and perplexity is 169.8878675147006
At time: 695.3893585205078 and batch: 400, loss is 5.162958869934082 and perplexity is 174.6805487260668
At time: 697.9224424362183 and batch: 450, loss is 5.131034660339355 and perplexity is 169.1920838530829
At time: 700.439891576767 and batch: 500, loss is 5.107881298065186 and perplexity is 165.3197203485044
At time: 702.9611639976501 and batch: 550, loss is 5.1099040317535405 and perplexity is 165.65445654316179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.226541646321615 and perplexity of 186.14792369501737
Finished 24 epochs...
Completing Train Step...
At time: 707.0032374858856 and batch: 50, loss is 5.190962343215943 and perplexity is 179.64134640619992
At time: 709.551459312439 and batch: 100, loss is 5.162856187820434 and perplexity is 174.66261307896136
At time: 712.0718822479248 and batch: 150, loss is 5.186167545318604 and perplexity is 178.78206414481357
At time: 714.5919878482819 and batch: 200, loss is 5.188922691345215 and perplexity is 179.2753140141657
At time: 717.1214473247528 and batch: 250, loss is 5.170186538696289 and perplexity is 175.94765546932436
At time: 719.6380791664124 and batch: 300, loss is 5.170795621871949 and perplexity is 176.05485486942897
At time: 722.1622774600983 and batch: 350, loss is 5.132510824203491 and perplexity is 169.4420235239337
At time: 724.6947770118713 and batch: 400, loss is 5.160697927474976 and perplexity is 174.28605219170527
At time: 727.219398021698 and batch: 450, loss is 5.128384199142456 and perplexity is 168.74424055781876
At time: 729.7437658309937 and batch: 500, loss is 5.105881261825561 and perplexity is 164.98940534777776
At time: 732.2722053527832 and batch: 550, loss is 5.1070314884185795 and perplexity is 165.17928973347495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.226123046875 and perplexity of 186.07001858382594
Finished 25 epochs...
Completing Train Step...
At time: 736.3357546329498 and batch: 50, loss is 5.187043752670288 and perplexity is 178.9387829528258
At time: 738.8593440055847 and batch: 100, loss is 5.158857221603394 and perplexity is 173.96553790885946
At time: 741.3787229061127 and batch: 150, loss is 5.182403221130371 and perplexity is 178.11033559155842
At time: 743.8984377384186 and batch: 200, loss is 5.18540735244751 and perplexity is 178.6462069395533
At time: 746.4184107780457 and batch: 250, loss is 5.167073087692261 and perplexity is 175.4007029614915
At time: 748.9408700466156 and batch: 300, loss is 5.16780870437622 and perplexity is 175.52977811406552
At time: 751.4666199684143 and batch: 350, loss is 5.130035037994385 and perplexity is 169.02304016946928
At time: 753.9936335086823 and batch: 400, loss is 5.158187971115113 and perplexity is 173.84915033823324
At time: 756.5358273983002 and batch: 450, loss is 5.126147203445434 and perplexity is 168.36718231386635
At time: 759.0545108318329 and batch: 500, loss is 5.103985567092895 and perplexity is 164.67693207166292
At time: 761.5731270313263 and batch: 550, loss is 5.104150438308716 and perplexity is 164.70408479596108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2257542928059895 and perplexity of 186.00141715665734
Finished 26 epochs...
Completing Train Step...
At time: 765.6122741699219 and batch: 50, loss is 5.183458461761474 and perplexity is 178.29838405523404
At time: 768.1599378585815 and batch: 100, loss is 5.155448961257934 and perplexity is 173.37362733018597
At time: 770.6881716251373 and batch: 150, loss is 5.179239835739136 and perplexity is 177.5477941946515
At time: 773.2195794582367 and batch: 200, loss is 5.182187900543213 and perplexity is 178.07198889808447
At time: 775.7506687641144 and batch: 250, loss is 5.1641754722595214 and perplexity is 174.89319481449033
At time: 778.2697365283966 and batch: 300, loss is 5.1651214981079105 and perplexity is 175.058726583813
At time: 780.788045167923 and batch: 350, loss is 5.127434577941894 and perplexity is 168.58407351056056
At time: 783.313591003418 and batch: 400, loss is 5.155732021331787 and perplexity is 173.42270942820565
At time: 785.8393340110779 and batch: 450, loss is 5.123460302352905 and perplexity is 167.91540356197117
At time: 788.3607223033905 and batch: 500, loss is 5.101981859207154 and perplexity is 164.34729795979172
At time: 790.8784611225128 and batch: 550, loss is 5.101428728103638 and perplexity is 164.25641749423463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.225330607096354 and perplexity of 185.9226277063968
Finished 27 epochs...
Completing Train Step...
At time: 794.974137544632 and batch: 50, loss is 5.179844942092895 and perplexity is 177.65526200446283
At time: 797.5012924671173 and batch: 100, loss is 5.151997499465942 and perplexity is 172.7762663570137
At time: 800.0266020298004 and batch: 150, loss is 5.175695848464966 and perplexity is 176.91968074192755
At time: 802.552220582962 and batch: 200, loss is 5.178839015960693 and perplexity is 177.47664378731724
At time: 805.0755774974823 and batch: 250, loss is 5.16122802734375 and perplexity is 174.37846569713074
At time: 807.6028618812561 and batch: 300, loss is 5.1622495937347415 and perplexity is 174.55669589849134
At time: 810.1251695156097 and batch: 350, loss is 5.124841003417969 and perplexity is 168.147404663738
At time: 812.6426293849945 and batch: 400, loss is 5.153244953155518 and perplexity is 172.9919312359593
At time: 815.163676738739 and batch: 450, loss is 5.120916042327881 and perplexity is 167.48872613243256
At time: 817.6895680427551 and batch: 500, loss is 5.09972583770752 and perplexity is 163.97694484156636
At time: 820.2485072612762 and batch: 550, loss is 5.098643865585327 and perplexity is 163.79962230488488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.224940490722656 and perplexity of 185.85011039110339
Finished 28 epochs...
Completing Train Step...
At time: 824.3371012210846 and batch: 50, loss is 5.1765846920013425 and perplexity is 177.07700456438644
At time: 826.8966853618622 and batch: 100, loss is 5.148601131439209 and perplexity is 172.1904499573564
At time: 829.4204490184784 and batch: 150, loss is 5.172677774429321 and perplexity is 176.38652899750053
At time: 831.9404942989349 and batch: 200, loss is 5.1757094669342045 and perplexity is 176.9220901335635
At time: 834.4567844867706 and batch: 250, loss is 5.158356103897095 and perplexity is 173.87838253690146
At time: 836.9852380752563 and batch: 300, loss is 5.1595674419403075 and perplexity is 174.08913565742645
At time: 839.5121352672577 and batch: 350, loss is 5.122430400848389 and perplexity is 167.74255625833848
At time: 842.0405032634735 and batch: 400, loss is 5.150681648254395 and perplexity is 172.54906800999345
At time: 844.5623528957367 and batch: 450, loss is 5.118412122726441 and perplexity is 167.06987243487856
At time: 847.0819990634918 and batch: 500, loss is 5.097682666778565 and perplexity is 163.64225394663248
At time: 849.6027555465698 and batch: 550, loss is 5.096196422576904 and perplexity is 163.39922224252467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.224559020996094 and perplexity of 185.77922772096684
Finished 29 epochs...
Completing Train Step...
At time: 853.6678459644318 and batch: 50, loss is 5.173184490203857 and perplexity is 176.4759294825644
At time: 856.1980588436127 and batch: 100, loss is 5.1453601837158205 and perplexity is 171.6332930564586
At time: 858.7216529846191 and batch: 150, loss is 5.169391393661499 and perplexity is 175.80780717190302
At time: 861.2410428524017 and batch: 200, loss is 5.172318162918091 and perplexity is 176.32310977507186
At time: 863.7593202590942 and batch: 250, loss is 5.155420484542847 and perplexity is 173.36869028909254
At time: 866.2795839309692 and batch: 300, loss is 5.156736688613892 and perplexity is 173.5970291023161
At time: 868.8043782711029 and batch: 350, loss is 5.119859952926635 and perplexity is 167.31193643323124
At time: 871.3290073871613 and batch: 400, loss is 5.1485299968719485 and perplexity is 172.1782016998547
At time: 873.8508851528168 and batch: 450, loss is 5.115960607528686 and perplexity is 166.6607997323659
At time: 876.3721206188202 and batch: 500, loss is 5.095730133056641 and perplexity is 163.323048658426
At time: 878.8915548324585 and batch: 550, loss is 5.093526201248169 and perplexity is 162.96349216097187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.224504597981771 and perplexity of 185.7691173305172
Finished 30 epochs...
Completing Train Step...
At time: 882.9252910614014 and batch: 50, loss is 5.170169820785523 and perplexity is 175.94471401670828
At time: 885.4721529483795 and batch: 100, loss is 5.142431650161743 and perplexity is 171.13139447087335
At time: 887.9917418956757 and batch: 150, loss is 5.166692190170288 and perplexity is 175.33390599058907
At time: 890.5127472877502 and batch: 200, loss is 5.1694859027862545 and perplexity is 175.8244233990642
At time: 893.0306923389435 and batch: 250, loss is 5.152920303344726 and perplexity is 172.93577855368707
At time: 895.5570397377014 and batch: 300, loss is 5.154339981079102 and perplexity is 173.18146598519976
At time: 898.0872495174408 and batch: 350, loss is 5.117326402664185 and perplexity is 166.8885797569351
At time: 900.6143634319305 and batch: 400, loss is 5.146218156814575 and perplexity is 171.78061299398624
At time: 903.134238243103 and batch: 450, loss is 5.113577356338501 and perplexity is 166.26407811445833
At time: 905.653605222702 and batch: 500, loss is 5.093787355422974 and perplexity is 163.00605631495176
At time: 908.1798076629639 and batch: 550, loss is 5.091063871383667 and perplexity is 162.5627159115587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.224383036295573 and perplexity of 185.74653629589312
Finished 31 epochs...
Completing Train Step...
At time: 912.2404894828796 and batch: 50, loss is 5.167000484466553 and perplexity is 175.38796876694246
At time: 914.7603423595428 and batch: 100, loss is 5.139586544036865 and perplexity is 170.6451994584731
At time: 917.2829058170319 and batch: 150, loss is 5.163789625167847 and perplexity is 174.82572580111582
At time: 919.8086137771606 and batch: 200, loss is 5.1664026260375975 and perplexity is 175.28314293010519
At time: 922.3414669036865 and batch: 250, loss is 5.1501298427581785 and perplexity is 172.45388075074024
At time: 924.8653955459595 and batch: 300, loss is 5.151613569259643 and perplexity is 172.70994506161668
At time: 927.3926208019257 and batch: 350, loss is 5.1146996879577635 and perplexity is 166.45078630112585
At time: 929.9202065467834 and batch: 400, loss is 5.143603525161743 and perplexity is 171.3320566262866
At time: 932.4446399211884 and batch: 450, loss is 5.111230068206787 and perplexity is 165.87426609671218
At time: 934.9763374328613 and batch: 500, loss is 5.091713714599609 and perplexity is 162.6683905219045
At time: 937.5015108585358 and batch: 550, loss is 5.088671159744263 and perplexity is 162.1742151795252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.22391357421875 and perplexity of 185.6593558067689
Finished 32 epochs...
Completing Train Step...
At time: 941.5341355800629 and batch: 50, loss is 5.163789739608765 and perplexity is 174.82574580833352
At time: 944.0835344791412 and batch: 100, loss is 5.136718740463257 and perplexity is 170.15652359376057
At time: 946.6051790714264 and batch: 150, loss is 5.160963106155395 and perplexity is 174.33227526545699
At time: 949.1569964885712 and batch: 200, loss is 5.163465490341187 and perplexity is 174.76906787768672
At time: 951.6781120300293 and batch: 250, loss is 5.147700710296631 and perplexity is 172.0354758171315
At time: 954.1988053321838 and batch: 300, loss is 5.149091749191284 and perplexity is 172.27495037585106
At time: 956.7213280200958 and batch: 350, loss is 5.112251329421997 and perplexity is 166.04375358204487
At time: 959.2506654262543 and batch: 400, loss is 5.140852537155151 and perplexity is 170.86137191420445
At time: 961.7770450115204 and batch: 450, loss is 5.108477020263672 and perplexity is 165.4182343163289
At time: 964.3002574443817 and batch: 500, loss is 5.089384450912475 and perplexity is 162.28993388058194
At time: 966.8108739852905 and batch: 550, loss is 5.0861931324005125 and perplexity is 161.7728405538985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.223975118001302 and perplexity of 185.67078233740378
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 970.8707501888275 and batch: 50, loss is 5.162454481124878 and perplexity is 174.5924640284391
At time: 973.390602350235 and batch: 100, loss is 5.134122867584228 and perplexity is 169.71539169785197
At time: 975.9108793735504 and batch: 150, loss is 5.154387817382813 and perplexity is 173.18975054455353
At time: 978.4366142749786 and batch: 200, loss is 5.153093471527099 and perplexity is 172.96572812120775
At time: 980.9646770954132 and batch: 250, loss is 5.13214602470398 and perplexity is 169.3802224317451
At time: 983.4879055023193 and batch: 300, loss is 5.130749616622925 and perplexity is 169.14386358547944
At time: 986.0153846740723 and batch: 350, loss is 5.08716682434082 and perplexity is 161.9304341762756
At time: 988.5343370437622 and batch: 400, loss is 5.107410068511963 and perplexity is 165.2418351628875
At time: 991.0588572025299 and batch: 450, loss is 5.073212051391602 and perplexity is 159.68642546540232
At time: 993.5864598751068 and batch: 500, loss is 5.056552820205688 and perplexity is 157.0482087326286
At time: 996.1163876056671 and batch: 550, loss is 5.068129796981811 and perplexity is 158.8769172312127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.212790934244792 and perplexity of 183.6057754231581
Finished 34 epochs...
Completing Train Step...
At time: 1000.2175943851471 and batch: 50, loss is 5.155279359817505 and perplexity is 173.34422540663306
At time: 1002.7664020061493 and batch: 100, loss is 5.127938184738159 and perplexity is 168.66899497744163
At time: 1005.2934443950653 and batch: 150, loss is 5.1492803764343265 and perplexity is 172.30744918977013
At time: 1007.8162393569946 and batch: 200, loss is 5.14900146484375 and perplexity is 172.259397346468
At time: 1010.3382785320282 and batch: 250, loss is 5.1287157917022705 and perplexity is 168.80020417054533
At time: 1012.8857333660126 and batch: 300, loss is 5.128116407394409 and perplexity is 168.69905829265588
At time: 1015.4033901691437 and batch: 350, loss is 5.0853966617584225 and perplexity is 161.64404453364293
At time: 1017.9281301498413 and batch: 400, loss is 5.107216157913208 and perplexity is 165.2097961261465
At time: 1020.453289270401 and batch: 450, loss is 5.074336643218994 and perplexity is 159.8661085305423
At time: 1022.9736950397491 and batch: 500, loss is 5.058028993606567 and perplexity is 157.28021031616933
At time: 1025.4941778182983 and batch: 550, loss is 5.067567806243897 and perplexity is 158.7876549598946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.211473083496093 and perplexity of 183.36396978142028
Finished 35 epochs...
Completing Train Step...
At time: 1029.5852227210999 and batch: 50, loss is 5.152241916656494 and perplexity is 172.8185010078578
At time: 1032.1169865131378 and batch: 100, loss is 5.125255098342896 and perplexity is 168.21704806914406
At time: 1034.6510109901428 and batch: 150, loss is 5.146953964233399 and perplexity is 171.90705695693546
At time: 1037.1802868843079 and batch: 200, loss is 5.147167491912842 and perplexity is 171.94376779113747
At time: 1039.7087361812592 and batch: 250, loss is 5.127079448699951 and perplexity is 168.52421500571288
At time: 1042.2434673309326 and batch: 300, loss is 5.126988801956177 and perplexity is 168.50893952672217
At time: 1044.7701683044434 and batch: 350, loss is 5.0848722171783445 and perplexity is 161.55929341616
At time: 1047.298305273056 and batch: 400, loss is 5.107566757202148 and perplexity is 165.26772871816365
At time: 1049.8206415176392 and batch: 450, loss is 5.075336866378784 and perplexity is 160.02609031016277
At time: 1052.3445508480072 and batch: 500, loss is 5.058854856491089 and perplexity is 157.4101558555525
At time: 1054.8814504146576 and batch: 550, loss is 5.066936130523682 and perplexity is 158.68738432618304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.210929361979167 and perplexity of 183.26429794493762
Finished 36 epochs...
Completing Train Step...
At time: 1058.9256324768066 and batch: 50, loss is 5.15007981300354 and perplexity is 172.4452531412202
At time: 1061.469288110733 and batch: 100, loss is 5.123413763046265 and perplexity is 167.9075890773569
At time: 1063.9867992401123 and batch: 150, loss is 5.145398778915405 and perplexity is 171.6399174054927
At time: 1066.5089201927185 and batch: 200, loss is 5.14598783493042 and perplexity is 171.74105271551397
At time: 1069.035296678543 and batch: 250, loss is 5.126020383834839 and perplexity is 168.34583140725374
At time: 1071.557037115097 and batch: 300, loss is 5.126319828033448 and perplexity is 168.39624913810036
At time: 1074.0762515068054 and batch: 350, loss is 5.084623231887817 and perplexity is 161.51907253596292
At time: 1076.630858182907 and batch: 400, loss is 5.107886905670166 and perplexity is 165.32064739879084
At time: 1079.1635398864746 and batch: 450, loss is 5.076025724411011 and perplexity is 160.13636354477987
At time: 1081.6860144138336 and batch: 500, loss is 5.059288063049316 and perplexity is 157.47836173995228
At time: 1084.2018475532532 and batch: 550, loss is 5.066290111541748 and perplexity is 158.58490236992355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.21059824625651 and perplexity of 183.20362629970862
Finished 37 epochs...
Completing Train Step...
At time: 1088.2591032981873 and batch: 50, loss is 5.148306722640991 and perplexity is 172.13976303562387
At time: 1090.7726774215698 and batch: 100, loss is 5.1219460391998295 and perplexity is 167.66132787080156
At time: 1093.284019947052 and batch: 150, loss is 5.144225025177002 and perplexity is 171.4385725985048
At time: 1095.7966084480286 and batch: 200, loss is 5.14511552810669 and perplexity is 171.59130714485252
At time: 1098.31147813797 and batch: 250, loss is 5.12528016090393 and perplexity is 168.22126407201003
At time: 1100.8189249038696 and batch: 300, loss is 5.125877981185913 and perplexity is 168.32186022174633
At time: 1103.3468325138092 and batch: 350, loss is 5.084435615539551 and perplexity is 161.48877175994775
At time: 1105.859554052353 and batch: 400, loss is 5.108116884231567 and perplexity is 165.35867197570062
At time: 1108.3940513134003 and batch: 450, loss is 5.076471138000488 and perplexity is 160.207706344619
At time: 1110.9143524169922 and batch: 500, loss is 5.059495840072632 and perplexity is 157.51108552469876
At time: 1113.4781603813171 and batch: 550, loss is 5.065643377304077 and perplexity is 158.48237324210567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.210410563151042 and perplexity of 183.16924530065884
Finished 38 epochs...
Completing Train Step...
At time: 1117.6150665283203 and batch: 50, loss is 5.1467928981781 and perplexity is 171.87937079510374
At time: 1120.2236683368683 and batch: 100, loss is 5.120717248916626 and perplexity is 167.45543378647707
At time: 1122.7427294254303 and batch: 150, loss is 5.143274354934692 and perplexity is 171.2756684954619
At time: 1125.2627582550049 and batch: 200, loss is 5.144399280548096 and perplexity is 171.4684492936044
At time: 1127.7831408977509 and batch: 250, loss is 5.124684143066406 and perplexity is 168.12103107126762
At time: 1130.2991394996643 and batch: 300, loss is 5.125517044067383 and perplexity is 168.26111757732926
At time: 1132.8163998126984 and batch: 350, loss is 5.0842678928375244 and perplexity is 161.46168869808687
At time: 1135.3332521915436 and batch: 400, loss is 5.108259086608887 and perplexity is 165.38218804394677
At time: 1137.8494811058044 and batch: 450, loss is 5.076753625869751 and perplexity is 160.25296947106483
At time: 1140.42542386055 and batch: 500, loss is 5.05960446357727 and perplexity is 157.5281958601033
At time: 1142.9429652690887 and batch: 550, loss is 5.065021638870239 and perplexity is 158.38386928459641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.210237630208334 and perplexity of 183.13757204280918
Finished 39 epochs...
Completing Train Step...
At time: 1147.0139961242676 and batch: 50, loss is 5.145498533248901 and perplexity is 171.6570400850713
At time: 1149.5566370487213 and batch: 100, loss is 5.1196369934082036 and perplexity is 167.27463680276455
At time: 1152.0738008022308 and batch: 150, loss is 5.14242847442627 and perplexity is 171.13085100369622
At time: 1154.589295387268 and batch: 200, loss is 5.1437666130065915 and perplexity is 171.36000108079372
At time: 1157.1080391407013 and batch: 250, loss is 5.124189949035644 and perplexity is 168.0379671877949
At time: 1159.6337451934814 and batch: 300, loss is 5.125177841186524 and perplexity is 168.2040526003599
At time: 1162.1620016098022 and batch: 350, loss is 5.084074354171753 and perplexity is 161.4304426420405
At time: 1164.6844885349274 and batch: 400, loss is 5.108322086334229 and perplexity is 165.39260740457502
At time: 1167.2040579319 and batch: 450, loss is 5.0769057464599605 and perplexity is 160.27734910163852
At time: 1169.7223160266876 and batch: 500, loss is 5.05960602760315 and perplexity is 157.52844223847103
At time: 1172.2401714324951 and batch: 550, loss is 5.064419574737549 and perplexity is 158.28854073753257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.210081481933594 and perplexity of 183.1089776594346
Finished 40 epochs...
Completing Train Step...
At time: 1176.3220283985138 and batch: 50, loss is 5.144339237213135 and perplexity is 171.45815406515126
At time: 1178.8402500152588 and batch: 100, loss is 5.118680191040039 and perplexity is 167.1146645772441
At time: 1181.3623712062836 and batch: 150, loss is 5.141702604293823 and perplexity is 171.00667730265508
At time: 1183.8864860534668 and batch: 200, loss is 5.143259334564209 and perplexity is 171.2730958907871
At time: 1186.414095401764 and batch: 250, loss is 5.123725805282593 and perplexity is 167.95999151241043
At time: 1188.9418201446533 and batch: 300, loss is 5.124858446121216 and perplexity is 168.1503376345987
At time: 1191.4661691188812 and batch: 350, loss is 5.083888597488404 and perplexity is 161.40045864337407
At time: 1193.9867832660675 and batch: 400, loss is 5.108313570022583 and perplexity is 165.39119887558414
At time: 1196.5139033794403 and batch: 450, loss is 5.076986856460572 and perplexity is 160.29034972475523
At time: 1199.0383491516113 and batch: 500, loss is 5.059551229476929 and perplexity is 157.51981021152153
At time: 1201.5671315193176 and batch: 550, loss is 5.063837718963623 and perplexity is 158.1964664257408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.210008748372396 and perplexity of 183.09565997572926
Finished 41 epochs...
Completing Train Step...
At time: 1205.5992028713226 and batch: 50, loss is 5.143276405334473 and perplexity is 171.27601967941504
At time: 1208.148146390915 and batch: 100, loss is 5.117811765670776 and perplexity is 166.9696009605286
At time: 1210.6757111549377 and batch: 150, loss is 5.141038188934326 and perplexity is 170.89309557657862
At time: 1213.1955308914185 and batch: 200, loss is 5.142792558670044 and perplexity is 171.19316839387184
At time: 1215.7166919708252 and batch: 250, loss is 5.123342008590698 and perplexity is 167.89554139195988
At time: 1218.246750831604 and batch: 300, loss is 5.124584007263183 and perplexity is 168.1041969796444
At time: 1220.7693364620209 and batch: 350, loss is 5.083685503005982 and perplexity is 161.36768242921002
At time: 1223.2880523204803 and batch: 400, loss is 5.108235340118409 and perplexity is 165.37826084402192
At time: 1225.8073360919952 and batch: 450, loss is 5.076999921798706 and perplexity is 160.2924439860551
At time: 1228.3225820064545 and batch: 500, loss is 5.059450426101685 and perplexity is 157.5039324832621
At time: 1230.8466720581055 and batch: 550, loss is 5.063282871246338 and perplexity is 158.1087158238237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209953816731771 and perplexity of 183.08560250697457
Finished 42 epochs...
Completing Train Step...
At time: 1234.9027426242828 and batch: 50, loss is 5.142312307357788 and perplexity is 171.11097238905086
At time: 1237.4245021343231 and batch: 100, loss is 5.11701452255249 and perplexity is 166.8365386437559
At time: 1239.9546592235565 and batch: 150, loss is 5.140442266464233 and perplexity is 170.79128687899896
At time: 1242.4819922447205 and batch: 200, loss is 5.142389068603515 and perplexity is 171.12410758457966
At time: 1245.007197380066 and batch: 250, loss is 5.1229551315307615 and perplexity is 167.83059902169637
At time: 1247.5274381637573 and batch: 300, loss is 5.124330739974976 and perplexity is 168.06162707654858
At time: 1250.0499255657196 and batch: 350, loss is 5.083440485000611 and perplexity is 161.32814928489373
At time: 1252.5805847644806 and batch: 400, loss is 5.108083896636963 and perplexity is 165.3532172808343
At time: 1255.0997421741486 and batch: 450, loss is 5.076958827972412 and perplexity is 160.285857091547
At time: 1257.6209020614624 and batch: 500, loss is 5.059320631027222 and perplexity is 157.48349057527548
At time: 1260.141515493393 and batch: 550, loss is 5.062759189605713 and perplexity is 158.02593886840162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2098943074544275 and perplexity of 183.07470753925645
Finished 43 epochs...
Completing Train Step...
At time: 1264.1998298168182 and batch: 50, loss is 5.141410846710205 and perplexity is 170.9567920852517
At time: 1266.7367715835571 and batch: 100, loss is 5.116281871795654 and perplexity is 166.7143504935366
At time: 1269.2643547058105 and batch: 150, loss is 5.139862117767334 and perplexity is 170.69223127274304
At time: 1271.7850432395935 and batch: 200, loss is 5.142005538940429 and perplexity is 171.0584889973935
At time: 1274.316623210907 and batch: 250, loss is 5.122614831924438 and perplexity is 167.7734960515305
At time: 1276.8395540714264 and batch: 300, loss is 5.124082489013672 and perplexity is 168.0199107943363
At time: 1279.3596789836884 and batch: 350, loss is 5.083246984481812 and perplexity is 161.29693522437603
At time: 1281.8803267478943 and batch: 400, loss is 5.107960805892945 and perplexity is 165.3328650829027
At time: 1284.4068486690521 and batch: 450, loss is 5.076856307983398 and perplexity is 160.26942542954023
At time: 1286.936198234558 and batch: 500, loss is 5.059076347351074 and perplexity is 157.445024627758
At time: 1289.4588108062744 and batch: 550, loss is 5.062203235626221 and perplexity is 157.93810813600962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2098642985026045 and perplexity of 183.06921374160987
Finished 44 epochs...
Completing Train Step...
At time: 1293.5437309741974 and batch: 50, loss is 5.140499505996704 and perplexity is 170.80106317220262
At time: 1296.0701053142548 and batch: 100, loss is 5.115559558868409 and perplexity is 166.5939740429847
At time: 1298.5913693904877 and batch: 150, loss is 5.139328718185425 and perplexity is 170.60120838589012
At time: 1301.1143534183502 and batch: 200, loss is 5.1416153144836425 and perplexity is 170.99175081372834
At time: 1303.645174741745 and batch: 250, loss is 5.1222453212738035 and perplexity is 167.71151341017355
At time: 1306.1709263324738 and batch: 300, loss is 5.123840827941894 and perplexity is 167.97931182840614
At time: 1308.6974592208862 and batch: 350, loss is 5.083118886947632 and perplexity is 161.27627480800547
At time: 1311.2119903564453 and batch: 400, loss is 5.107954368591309 and perplexity is 165.33180078880548
At time: 1313.7298877239227 and batch: 450, loss is 5.076765298843384 and perplexity is 160.25484011067002
At time: 1316.2524840831757 and batch: 500, loss is 5.058899145126343 and perplexity is 157.41712749091147
At time: 1318.7747514247894 and batch: 550, loss is 5.061730651855469 and perplexity is 157.8634867831302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209840901692709 and perplexity of 183.06493055612484
Finished 45 epochs...
Completing Train Step...
At time: 1322.789404630661 and batch: 50, loss is 5.139723243713379 and perplexity is 170.66852819651695
At time: 1325.3247163295746 and batch: 100, loss is 5.114931945800781 and perplexity is 166.48945029155445
At time: 1327.8445336818695 and batch: 150, loss is 5.1388582992553715 and perplexity is 170.52097322152514
At time: 1330.3798077106476 and batch: 200, loss is 5.141253795623779 and perplexity is 170.92994524354089
At time: 1332.8903093338013 and batch: 250, loss is 5.121920480728149 and perplexity is 167.65704275826198
At time: 1335.4063203334808 and batch: 300, loss is 5.123586091995239 and perplexity is 167.93652690904895
At time: 1337.9207165241241 and batch: 350, loss is 5.0828814029693605 and perplexity is 161.2379788241846
At time: 1340.437085390091 and batch: 400, loss is 5.107760238647461 and perplexity is 165.29970805078352
At time: 1342.9508979320526 and batch: 450, loss is 5.076635150909424 and perplexity is 160.23398463150076
At time: 1345.4602105617523 and batch: 500, loss is 5.058684892654419 and perplexity is 157.38340409501282
At time: 1347.9831912517548 and batch: 550, loss is 5.061259756088257 and perplexity is 157.78916703520733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209820556640625 and perplexity of 183.06120612846487
Finished 46 epochs...
Completing Train Step...
At time: 1351.997356414795 and batch: 50, loss is 5.138964710235595 and perplexity is 170.5391194908984
At time: 1354.5065207481384 and batch: 100, loss is 5.1143231868743895 and perplexity is 166.3881291957292
At time: 1357.0134224891663 and batch: 150, loss is 5.138401794433594 and perplexity is 170.44314734033182
At time: 1359.5201547145844 and batch: 200, loss is 5.140897617340088 and perplexity is 170.86907455006576
At time: 1362.0266077518463 and batch: 250, loss is 5.1215455532073975 and perplexity is 167.59419530124092
At time: 1364.5347464084625 and batch: 300, loss is 5.123264741897583 and perplexity is 167.882569159855
At time: 1367.0416831970215 and batch: 350, loss is 5.082570343017578 and perplexity is 161.1878319460132
At time: 1369.5465579032898 and batch: 400, loss is 5.1074557209014895 and perplexity is 165.24937901970867
At time: 1372.0544397830963 and batch: 450, loss is 5.0764032649993895 and perplexity is 160.1968329357999
At time: 1374.5577528476715 and batch: 500, loss is 5.058420143127441 and perplexity is 157.34174242842113
At time: 1377.0678308010101 and batch: 550, loss is 5.06076340675354 and perplexity is 157.71086792058813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209738159179688 and perplexity of 183.0461229712993
Finished 47 epochs...
Completing Train Step...
At time: 1381.0565354824066 and batch: 50, loss is 5.138180055618286 and perplexity is 170.40535766863664
At time: 1383.589703798294 and batch: 100, loss is 5.113680362701416 and perplexity is 166.28120525451536
At time: 1386.0982987880707 and batch: 150, loss is 5.1379603099823 and perplexity is 170.3679159489182
At time: 1388.6059265136719 and batch: 200, loss is 5.140535860061646 and perplexity is 170.80727259801353
At time: 1391.1164140701294 and batch: 250, loss is 5.121254663467408 and perplexity is 167.54545095930413
At time: 1393.6523759365082 and batch: 300, loss is 5.122995643615723 and perplexity is 167.83739832690915
At time: 1396.1572000980377 and batch: 350, loss is 5.082332019805908 and perplexity is 161.1494217214275
At time: 1398.6656167507172 and batch: 400, loss is 5.107248573303223 and perplexity is 165.21515155292087
At time: 1401.1777942180634 and batch: 450, loss is 5.0762367439270015 and perplexity is 160.1701590083334
At time: 1403.6920037269592 and batch: 500, loss is 5.058192272186279 and perplexity is 157.30589290217756
At time: 1406.2018988132477 and batch: 550, loss is 5.060292892456054 and perplexity is 157.63668015693307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209714762369791 and perplexity of 183.04184032605815
Finished 48 epochs...
Completing Train Step...
At time: 1410.2009987831116 and batch: 50, loss is 5.137494020462036 and perplexity is 170.28849369344374
At time: 1412.706776380539 and batch: 100, loss is 5.113134536743164 and perplexity is 166.19046942155722
At time: 1415.2157208919525 and batch: 150, loss is 5.137537832260132 and perplexity is 170.29595450198207
At time: 1417.7290697097778 and batch: 200, loss is 5.140203542709351 and perplexity is 170.7505198079218
At time: 1420.238273859024 and batch: 250, loss is 5.120945196151734 and perplexity is 167.49360914043038
At time: 1422.7470533847809 and batch: 300, loss is 5.122721691131591 and perplexity is 167.79142515222753
At time: 1425.2503821849823 and batch: 350, loss is 5.082083072662353 and perplexity is 161.10930902638205
At time: 1427.760425567627 and batch: 400, loss is 5.107038793563842 and perplexity is 165.1804963965883
At time: 1430.2768919467926 and batch: 450, loss is 5.076053113937378 and perplexity is 160.1407496639981
At time: 1432.785284280777 and batch: 500, loss is 5.057950458526611 and perplexity is 157.2678587872972
At time: 1435.290453195572 and batch: 550, loss is 5.059764471054077 and perplexity is 157.55340356590574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2096303304036455 and perplexity of 183.0263863960045
Finished 49 epochs...
Completing Train Step...
At time: 1439.2678689956665 and batch: 50, loss is 5.13673921585083 and perplexity is 170.1600076501978
At time: 1441.802457332611 and batch: 100, loss is 5.112587375640869 and perplexity is 166.09956133407033
At time: 1444.305804014206 and batch: 150, loss is 5.13712405204773 and perplexity is 170.22550398227767
At time: 1446.8149726390839 and batch: 200, loss is 5.139901580810547 and perplexity is 170.69896744055598
At time: 1449.322143316269 and batch: 250, loss is 5.120649156570434 and perplexity is 167.4440317413084
At time: 1451.8307285308838 and batch: 300, loss is 5.122443046569824 and perplexity is 167.74467749739006
At time: 1454.3450660705566 and batch: 350, loss is 5.081809005737305 and perplexity is 161.06516034358367
At time: 1456.8843803405762 and batch: 400, loss is 5.106823949813843 and perplexity is 165.1450122112299
At time: 1459.4068324565887 and batch: 450, loss is 5.075847759246826 and perplexity is 160.10786738628664
At time: 1461.943307876587 and batch: 500, loss is 5.057680864334106 and perplexity is 157.22546600058172
At time: 1464.462955713272 and batch: 550, loss is 5.059311485290527 and perplexity is 157.48205027932315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209613037109375 and perplexity of 183.02322129421293
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f230e50feb8>
SETTINGS FOR THIS RUN
{'anneal': 2.0, 'num_layers': 1, 'dropout': 0.0, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 0.0, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9904675483703613 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 5.482476711273193 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 7.981460332870483 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 10.467126846313477 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 12.95678448677063 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 15.436108112335205 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 17.910842418670654 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 20.39495539665222 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 22.87017059326172 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 25.345682621002197 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 27.838345050811768 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 1 epochs...
Completing Train Step...
At time: 31.84379553794861 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 34.33717608451843 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 36.83415699005127 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 39.32904767990112 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 41.82993197441101 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 44.32393407821655 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 46.82686471939087 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 49.32267236709595 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 51.820783615112305 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 54.32548236846924 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 56.82323169708252 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 2 epochs...
Completing Train Step...
At time: 60.81347703933716 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 63.32319188117981 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 65.83482432365417 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 68.37290811538696 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 70.8867290019989 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 73.3994505405426 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 75.9191677570343 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 78.43993759155273 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 80.95188641548157 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 83.46479845046997 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 85.97677779197693 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 3 epochs...
Completing Train Step...
At time: 90.04028964042664 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 92.57752394676208 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 95.08848690986633 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 97.58857274055481 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 100.0952820777893 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 102.59270429611206 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 105.09584021568298 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 107.6002881526947 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 110.10453367233276 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 112.6063883304596 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 115.0993070602417 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 4 epochs...
Completing Train Step...
At time: 119.0977189540863 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 121.61313796043396 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 124.12436079978943 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 126.63633847236633 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 129.1542272567749 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 131.6732144355774 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 134.18844628334045 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 136.6983082294464 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 139.20878863334656 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 141.72614073753357 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 144.2410011291504 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 5 epochs...
Completing Train Step...
At time: 148.26766896247864 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 150.80189514160156 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 153.3156464099884 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 155.83073711395264 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 158.34184002876282 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 160.85812616348267 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 163.37229704856873 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 165.88244032859802 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 168.3928484916687 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 170.90803933143616 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 173.42607069015503 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 6 epochs...
Completing Train Step...
At time: 177.45877742767334 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 179.97040009498596 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 182.51449465751648 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 185.03134894371033 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 187.5420434474945 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 190.05613207817078 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 192.5713131427765 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 195.0896017551422 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 197.60667157173157 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 200.11239194869995 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 202.63117361068726 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 7 epochs...
Completing Train Step...
At time: 206.67665839195251 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 209.22543787956238 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 211.74118828773499 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 214.25434160232544 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 216.78007173538208 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 219.2999496459961 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 221.81461596488953 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 224.33032965660095 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 226.8507661819458 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 229.3701012134552 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 231.8869993686676 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 8 epochs...
Completing Train Step...
At time: 235.94801139831543 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 238.4704942703247 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 240.9901406764984 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 243.50423502922058 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 246.044992685318 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 248.5636444091797 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 251.0852267742157 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 253.59863591194153 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 256.11048674583435 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 258.6241137981415 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 261.14733481407166 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 9 epochs...
Completing Train Step...
At time: 265.20225715637207 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 267.7566785812378 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 270.27817153930664 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 272.8078758716583 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 275.33499217033386 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 277.85878252983093 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 280.37878131866455 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 282.8986027240753 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 285.4202651977539 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 287.9461462497711 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 290.4652554988861 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 10 epochs...
Completing Train Step...
At time: 294.52800154685974 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 297.04608130455017 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 299.5600574016571 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 302.0778431892395 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 304.59710001945496 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 307.1150104999542 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 309.6593005657196 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 312.1725091934204 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 314.6909568309784 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 317.20908284187317 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 319.7200131416321 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 11 epochs...
Completing Train Step...
At time: 323.76210594177246 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 326.31105637550354 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 328.8259987831116 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 331.34280920028687 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 333.8619387149811 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 336.38481426239014 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 338.9031984806061 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 341.4180941581726 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 343.93829131126404 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 346.46395802497864 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 348.98183131217957 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 12 epochs...
Completing Train Step...
At time: 353.03946900367737 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 355.5549991130829 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 358.07880663871765 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 360.5926868915558 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 363.10691022872925 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 365.63069915771484 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 368.1525571346283 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 370.667977809906 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 373.21483421325684 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 375.72905945777893 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 378.2493939399719 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 13 epochs...
Completing Train Step...
At time: 382.27440309524536 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 384.82875895500183 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 387.3508369922638 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 389.8697443008423 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 392.383508682251 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 394.90185356140137 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 397.4194474220276 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 399.94167041778564 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 402.4596710205078 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 404.9694628715515 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 407.47775626182556 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 14 epochs...
Completing Train Step...
At time: 411.4871232509613 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 414.00551295280457 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 416.52060556411743 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 419.0354690551758 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 421.5524470806122 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 424.0708179473877 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 426.5953588485718 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 429.1128098964691 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 431.6294219493866 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 434.1436641216278 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 436.68581318855286 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 15 epochs...
Completing Train Step...
At time: 440.7162079811096 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 443.2590594291687 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 445.77206015586853 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 448.2834312915802 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 450.799432516098 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 453.319256067276 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 455.83534002304077 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 458.3493733406067 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 460.8702013492584 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 463.3904139995575 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 465.90648341178894 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 16 epochs...
Completing Train Step...
At time: 469.99662947654724 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 472.51861238479614 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 475.04286456108093 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 477.5604667663574 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 480.0788917541504 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 482.5966613292694 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 485.12265181541443 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 487.6435227394104 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 490.15942311286926 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 492.6808841228485 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 495.2002408504486 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 17 epochs...
Completing Train Step...
At time: 499.2359118461609 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 501.7792582511902 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 504.2922565937042 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 506.8100655078888 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 509.33185720443726 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 511.8443605899811 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 514.3594298362732 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 516.8798930644989 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 519.3996307849884 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 521.9137978553772 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 524.4261569976807 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 18 epochs...
Completing Train Step...
At time: 528.4930844306946 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 531.0547761917114 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 533.5797617435455 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 536.1017961502075 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 538.619791507721 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 541.1353764533997 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 543.6549179553986 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 546.1785988807678 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 548.7097299098969 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 551.2274954319 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 553.7427842617035 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 19 epochs...
Completing Train Step...
At time: 557.7820127010345 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 560.2969839572906 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 562.8344986438751 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 565.3463475704193 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 567.8639645576477 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 570.3861875534058 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 572.9010696411133 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 575.4162616729736 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 577.9312739372253 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 580.4433476924896 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 582.959974527359 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 20 epochs...
Completing Train Step...
At time: 586.990583896637 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 589.5282557010651 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 592.0417308807373 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 594.5529534816742 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 597.0742201805115 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 599.5911521911621 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 602.1032185554504 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 604.6209735870361 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 607.1430263519287 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 609.6650578975677 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 612.1827149391174 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 21 epochs...
Completing Train Step...
At time: 616.2466676235199 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 618.7693135738373 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 621.2898104190826 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 623.8027696609497 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 626.3459820747375 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 628.8608028888702 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 631.3821740150452 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 633.9102010726929 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 636.4249360561371 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 638.9339680671692 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 641.4510481357574 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 22 epochs...
Completing Train Step...
At time: 645.5105872154236 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 648.0547389984131 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 650.563467502594 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 653.0843315124512 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 655.6050243377686 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 658.1216387748718 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 660.6294665336609 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 663.1461629867554 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 665.6590573787689 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 668.172112941742 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 670.6900682449341 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 23 epochs...
Completing Train Step...
At time: 674.777378320694 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 677.29692029953 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 679.8142938613892 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 682.3412964344025 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 684.8700938224792 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 687.3972165584564 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 689.9705743789673 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 692.4900739192963 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 695.0102078914642 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 697.5368206501007 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 700.0687890052795 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 24 epochs...
Completing Train Step...
At time: 704.1478924751282 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 706.7099244594574 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 709.230477809906 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 711.7570788860321 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 714.2742111682892 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 716.7891662120819 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 719.3002398014069 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 721.823322057724 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 724.3471553325653 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 726.8641619682312 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 729.3760147094727 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 25 epochs...
Completing Train Step...
At time: 733.4731419086456 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 735.9966275691986 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 738.5150370597839 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 741.0317966938019 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 743.5519108772278 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 746.0735199451447 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 748.5961451530457 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 751.1117205619812 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 753.6570358276367 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 756.182559967041 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 758.7052850723267 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 26 epochs...
Completing Train Step...
At time: 762.7193853855133 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 765.258106470108 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 767.7721698284149 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 770.2838289737701 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 772.8017001152039 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 775.3173501491547 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 777.8338847160339 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 780.3561275005341 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 782.8810861110687 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 785.4003434181213 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 787.9166321754456 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 27 epochs...
Completing Train Step...
At time: 792.0276510715485 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 794.5529680252075 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 797.0749971866608 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 799.5998818874359 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 802.1235954761505 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 804.6474139690399 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 807.1708407402039 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 809.6897158622742 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 812.2097291946411 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 814.7385952472687 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 817.3134679794312 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 28 epochs...
Completing Train Step...
At time: 821.407977104187 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 823.9769539833069 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 826.49933385849 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 829.0154120922089 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 831.530210018158 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 834.0532939434052 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 836.592588186264 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 839.1599123477936 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 841.6836905479431 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 844.2064945697784 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 846.7199883460999 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 29 epochs...
Completing Train Step...
At time: 851.0425839424133 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 853.5615234375 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 856.0742552280426 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 858.584778547287 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 861.1087033748627 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 863.6464037895203 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 866.170577287674 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 868.6807613372803 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 871.1938078403473 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 873.7155530452728 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 876.2326626777649 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 30 epochs...
Completing Train Step...
At time: 880.4014115333557 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 882.9554831981659 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 885.4801912307739 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 887.9917151927948 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 890.5037055015564 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 893.0150439739227 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 895.5300650596619 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 898.0413467884064 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 900.5545406341553 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 903.0649333000183 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 905.5773057937622 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 31 epochs...
Completing Train Step...
At time: 909.7642531394958 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 912.2752311229706 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 914.7961294651031 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 917.3111748695374 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 919.8330230712891 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 922.3540859222412 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 924.8715043067932 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 927.3932936191559 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 929.9095575809479 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 932.4226231575012 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 934.9362623691559 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 32 epochs...
Completing Train Step...
At time: 939.0178546905518 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 941.5716784000397 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 944.0887260437012 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 946.6351087093353 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 949.1590547561646 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 951.6840229034424 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 954.2094705104828 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 956.7290029525757 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 959.2490036487579 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 961.766664981842 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 964.2823774814606 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 33 epochs...
Completing Train Step...
At time: 968.351701259613 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 970.8725748062134 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 973.3843858242035 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 975.8954975605011 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 978.4145035743713 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 980.9321599006653 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 983.4443204402924 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 985.9588322639465 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 988.4723017215729 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 990.9926545619965 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 993.5051145553589 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 34 epochs...
Completing Train Step...
At time: 997.5492088794708 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1000.0939729213715 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1002.614066362381 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1005.1359157562256 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1007.6513001918793 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1010.1924710273743 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1012.7070150375366 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1015.2229108810425 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1017.7464737892151 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1020.2632586956024 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1022.7766988277435 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 35 epochs...
Completing Train Step...
At time: 1026.8744349479675 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1029.392016172409 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1031.919248342514 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1034.4364528656006 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1036.957079410553 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1039.470377922058 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1041.9729330539703 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1044.4737620353699 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1046.9789643287659 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1049.4815328121185 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1051.9952549934387 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 36 epochs...
Completing Train Step...
At time: 1056.0662870407104 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1058.6285803318024 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1061.151472568512 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1063.665910243988 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1066.1929326057434 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1068.7123005390167 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1071.2292592525482 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1073.8045172691345 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1076.3227236270905 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1078.8384988307953 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1081.3520185947418 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 37 epochs...
Completing Train Step...
At time: 1085.3982503414154 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1087.9162139892578 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1090.436707019806 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1092.9576978683472 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1095.4770247936249 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1097.9927034378052 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1100.5078301429749 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1103.0337400436401 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1105.5426247119904 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1108.0462172031403 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1110.556027173996 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 38 epochs...
Completing Train Step...
At time: 1114.6194100379944 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1117.1778502464294 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1119.694922208786 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1122.2126107215881 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1124.730979681015 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1127.2508363723755 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1129.7772374153137 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1132.3019869327545 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1134.8189623355865 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1137.3834660053253 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1139.9073190689087 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 39 epochs...
Completing Train Step...
At time: 1143.9870383739471 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1146.5380368232727 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1149.0580341815948 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1151.5790300369263 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1154.1058852672577 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1156.6293783187866 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1159.1464352607727 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1161.6631989479065 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1164.182318687439 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1166.7085387706757 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1169.227900505066 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 40 epochs...
Completing Train Step...
At time: 1173.3170685768127 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1175.8277311325073 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1178.3512902259827 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1180.8780689239502 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1183.3931102752686 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1185.9069859981537 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1188.430392742157 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1190.9521133899689 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1193.472644329071 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1195.985594511032 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1198.4987134933472 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 41 epochs...
Completing Train Step...
At time: 1202.5021290779114 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1205.04230594635 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1207.5606112480164 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1210.08554148674 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1212.604326248169 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1215.1280915737152 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1217.6487929821014 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1220.1683249473572 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1222.6872684955597 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1225.210669517517 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1227.735846042633 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 42 epochs...
Completing Train Step...
At time: 1231.8417384624481 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1234.362187385559 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1236.8975191116333 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1239.418369293213 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1241.939694404602 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1244.4587669372559 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1246.9744729995728 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1249.5017592906952 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1252.0271589756012 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1254.5441219806671 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1257.0673367977142 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 43 epochs...
Completing Train Step...
At time: 1261.1357221603394 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1263.675538301468 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1266.1934230327606 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1268.7170369625092 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1271.2351415157318 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1273.758047580719 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1276.2779908180237 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1278.7959985733032 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1281.3071925640106 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1283.8299942016602 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1286.3472578525543 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 44 epochs...
Completing Train Step...
At time: 1290.4605784416199 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1292.9855337142944 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1295.5110170841217 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1298.0363295078278 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1300.5568552017212 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1303.0760867595673 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1305.6027536392212 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1308.1223850250244 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1310.638684272766 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1313.173675775528 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1315.703973531723 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 45 epochs...
Completing Train Step...
At time: 1319.7629868984222 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1322.2973835468292 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1324.810021162033 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1327.36066031456 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1329.87988615036 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1332.3963203430176 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1334.923415184021 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1337.4453899860382 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1339.963250875473 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1342.4783141613007 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1344.9894371032715 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 46 epochs...
Completing Train Step...
At time: 1349.0769646167755 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1351.592617034912 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1354.1086127758026 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1356.6387412548065 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1359.1723310947418 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1361.6897022724152 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1364.2062194347382 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1366.7244246006012 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1369.2510492801666 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1371.7779114246368 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1374.2949631214142 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 47 epochs...
Completing Train Step...
At time: 1378.358107805252 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1380.904995918274 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1383.4221122264862 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1385.9451293945312 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1388.467448234558 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1391.0145139694214 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1393.5340695381165 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1396.0493142604828 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1398.5628023147583 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1401.0778348445892 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1403.5983951091766 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 48 epochs...
Completing Train Step...
At time: 1407.662080526352 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1410.1759147644043 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1412.703599691391 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1415.2275199890137 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1417.7532110214233 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1420.2763855457306 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1422.8026132583618 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1425.320029258728 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1427.8359797000885 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1430.3510944843292 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1432.872972726822 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished 49 epochs...
Completing Train Step...
At time: 1436.9281153678894 and batch: 50, loss is 9.935029983520508 and perplexity is 20640.90330717817
At time: 1439.469527721405 and batch: 100, loss is 9.934891147613525 and perplexity is 20638.037807568304
At time: 1441.9850444793701 and batch: 150, loss is 9.935097560882568 and perplexity is 20642.29821210567
At time: 1444.5090601444244 and batch: 200, loss is 9.93510850906372 and perplexity is 20642.524208963034
At time: 1447.0302748680115 and batch: 250, loss is 9.935816707611083 and perplexity is 20657.14839242324
At time: 1449.5523171424866 and batch: 300, loss is 9.93595890045166 and perplexity is 20660.085899872673
At time: 1452.07515335083 and batch: 350, loss is 9.935483875274658 and perplexity is 20650.274169505195
At time: 1454.6198544502258 and batch: 400, loss is 9.935691413879395 and perplexity is 20654.560343351663
At time: 1457.1324450969696 and batch: 450, loss is 9.935016479492187 and perplexity is 20640.624573717367
At time: 1459.6577818393707 and batch: 500, loss is 9.934725570678712 and perplexity is 20634.62090741529
At time: 1462.1769535541534 and batch: 550, loss is 9.935495414733886 and perplexity is 20650.512463876916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 10.103114827473958 and perplexity of 24418.951911434415
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f230e50feb8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'anneal': 2.2257311753662283, 'num_layers': 1, 'dropout': 0.3983140465493671, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 29.077239322071772, 'tune_wordvecs': True}, 'best_accuracy': -172.69910480192593}, {'params': {'anneal': 5.722128771470329, 'num_layers': 1, 'dropout': 0.39802325653507376, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 26.98880515309043, 'tune_wordvecs': True}, 'best_accuracy': -175.70097937963473}, {'params': {'anneal': 3.8276469565730373, 'num_layers': 1, 'dropout': 0.8979697863465091, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 17.403461512516476, 'tune_wordvecs': True}, 'best_accuracy': -151.81078163385433}, {'params': {'anneal': 5.014970910694949, 'num_layers': 1, 'dropout': 0.4120799441113109, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 4.082977578885308, 'tune_wordvecs': True}, 'best_accuracy': -96.08789703667331}, {'params': {'anneal': 7.164073193929745, 'num_layers': 1, 'dropout': 0.7817805766891953, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 28.173359026361044, 'tune_wordvecs': True}, 'best_accuracy': -183.02322129421293}, {'params': {'anneal': 2.0, 'num_layers': 1, 'dropout': 0.0, 'seq_len': 50, 'wordvec_source': '', 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'lr': 0.0, 'tune_wordvecs': True}, 'best_accuracy': -24418.951911434415}]
