Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'seq_len': 20, 'lr': 24.508440653647003, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.8051793355431327, 'anneal': 7.835234642237461, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1375 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5936532020568848 and batch: 50, loss is 7.397116823196411 and perplexity is 1631.274390836918
At time: 2.3607826232910156 and batch: 100, loss is 6.7332597255706785 and perplexity is 839.8805888243098
At time: 3.0940001010894775 and batch: 150, loss is 6.578927612304687 and perplexity is 719.767045906073
At time: 3.8306117057800293 and batch: 200, loss is 6.562546701431274 and perplexity is 708.0726500053511
At time: 4.567253828048706 and batch: 250, loss is 6.5864456748962406 and perplexity is 725.198691751653
At time: 5.308905839920044 and batch: 300, loss is 6.609423694610595 and perplexity is 742.0552450273759
At time: 6.046759128570557 and batch: 350, loss is 6.612201948165893 and perplexity is 744.1197291523542
At time: 6.782712697982788 and batch: 400, loss is 6.650892601013184 and perplexity is 773.474421551478
At time: 7.518014430999756 and batch: 450, loss is 6.652341947555542 and perplexity is 774.5962668048463
At time: 8.260544776916504 and batch: 500, loss is 6.636618251800537 and perplexity is 762.5120043752091
At time: 9.001039266586304 and batch: 550, loss is 6.624021768569946 and perplexity is 752.9672759109367
At time: 9.743118047714233 and batch: 600, loss is 6.569872617721558 and perplexity is 713.2789782442887
At time: 10.488618850708008 and batch: 650, loss is 6.607713966369629 and perplexity is 740.7876161778084
At time: 11.23237419128418 and batch: 700, loss is 6.626814517974854 and perplexity is 755.0730639230952
At time: 11.969745397567749 and batch: 750, loss is 6.631068820953369 and perplexity is 758.2922162773544
At time: 12.70519757270813 and batch: 800, loss is 6.597394809722901 and perplexity is 733.182618788912
At time: 13.443012714385986 and batch: 850, loss is 6.6001611328125 and perplexity is 735.213646740709
At time: 14.177531480789185 and batch: 900, loss is 6.6492273807525635 and perplexity is 772.1874880848645
At time: 14.914671897888184 and batch: 950, loss is 6.637177324295044 and perplexity is 762.9384230519561
At time: 15.647947311401367 and batch: 1000, loss is 6.663232545852662 and perplexity is 783.0781863452181
At time: 16.382256984710693 and batch: 1050, loss is 6.665770778656006 and perplexity is 785.0683457608515
At time: 17.11682939529419 and batch: 1100, loss is 6.682841768264771 and perplexity is 798.5852848410037
At time: 17.854618072509766 and batch: 1150, loss is 6.684631109237671 and perplexity is 800.015505406054
At time: 18.591441869735718 and batch: 1200, loss is 6.618778438568115 and perplexity is 749.0295524161129
At time: 19.327352046966553 and batch: 1250, loss is 6.651463460922241 and perplexity is 773.9160931437312
At time: 20.059481859207153 and batch: 1300, loss is 6.728576793670654 and perplexity is 835.956680092272
At time: 20.793517589569092 and batch: 1350, loss is 6.7127377223968505 and perplexity is 822.8202120159795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 6.435330403645834 and perplexity of 623.4885517259044
Finished 1 epochs...
Completing Train Step...
At time: 23.36953043937683 and batch: 50, loss is 6.959158477783203 and perplexity is 1052.747274161291
At time: 24.093241691589355 and batch: 100, loss is 7.13269721031189 and perplexity is 1252.2499975883716
At time: 24.819948434829712 and batch: 150, loss is 6.978578910827637 and perplexity is 1073.3918970251252
At time: 25.544625282287598 and batch: 200, loss is 7.079794540405273 and perplexity is 1187.7244640512556
At time: 26.271188259124756 and batch: 250, loss is 7.077782621383667 and perplexity is 1185.3372608446211
At time: 26.996792793273926 and batch: 300, loss is 7.091225109100342 and perplexity is 1201.3787194109702
At time: 27.72248363494873 and batch: 350, loss is 6.857948265075684 and perplexity is 951.4130162661739
At time: 28.446321964263916 and batch: 400, loss is 6.880624580383301 and perplexity is 973.2340328870843
At time: 29.1755473613739 and batch: 450, loss is 7.140635318756104 and perplexity is 1262.2300528424203
At time: 29.901986837387085 and batch: 500, loss is 7.357634992599487 and perplexity is 1568.1235502358638
At time: 30.628394603729248 and batch: 550, loss is 7.2612947082519534 and perplexity is 1424.0991370335018
At time: 31.35243034362793 and batch: 600, loss is 6.998830871582031 and perplexity is 1095.3518026195509
At time: 32.078861474990845 and batch: 650, loss is 7.118256702423095 and perplexity is 1234.2968101695678
At time: 32.80397033691406 and batch: 700, loss is 6.948850383758545 and perplexity is 1041.9511953689107
At time: 33.529404163360596 and batch: 750, loss is 7.091249542236328 and perplexity is 1201.4080732191942
At time: 34.255770683288574 and batch: 800, loss is 7.182544727325439 and perplexity is 1316.2535063331054
At time: 34.98051714897156 and batch: 850, loss is 7.32815260887146 and perplexity is 1522.5663965878223
At time: 35.70974040031433 and batch: 900, loss is 7.172355213165283 and perplexity is 1302.9096218952186
At time: 36.43596124649048 and batch: 950, loss is 7.265232620239257 and perplexity is 1429.7181704646234
At time: 37.16156816482544 and batch: 1000, loss is 7.222616701126099 and perplexity is 1370.0694365159682
At time: 37.88635802268982 and batch: 1050, loss is 7.130743322372436 and perplexity is 1249.8056302085247
At time: 38.61403441429138 and batch: 1100, loss is 7.166207809448242 and perplexity is 1294.9246789229462
At time: 39.34217429161072 and batch: 1150, loss is 7.171144542694091 and perplexity is 1301.3331821564311
At time: 40.06936597824097 and batch: 1200, loss is 7.112078180313111 and perplexity is 1226.6941906991674
At time: 40.79648017883301 and batch: 1250, loss is 6.966918144226074 and perplexity is 1060.9480182278467
At time: 41.52235436439514 and batch: 1300, loss is 6.985234651565552 and perplexity is 1080.5599430685834
At time: 42.25000333786011 and batch: 1350, loss is 7.136479616165161 and perplexity is 1256.9954843297776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 7.3986669921875 and perplexity of 1633.8051028223929
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 44.79771375656128 and batch: 50, loss is 6.8078106498718265 and perplexity is 904.8875210759327
At time: 45.55313158035278 and batch: 100, loss is 6.561421613693238 and perplexity is 707.2764541282872
At time: 46.281548738479614 and batch: 150, loss is 6.511891679763794 and perplexity is 673.0985008377604
At time: 47.01084232330322 and batch: 200, loss is 6.520927505493164 and perplexity is 679.2080624887541
At time: 47.74030566215515 and batch: 250, loss is 6.549729528427124 and perplexity is 699.0550737186225
At time: 48.472376108169556 and batch: 300, loss is 6.565776729583741 and perplexity is 710.3634422689431
At time: 49.20279145240784 and batch: 350, loss is 6.593751602172851 and perplexity is 730.5163421852167
At time: 49.9314067363739 and batch: 400, loss is 6.590332374572754 and perplexity is 728.0228059589898
At time: 50.66176748275757 and batch: 450, loss is 6.550388135910034 and perplexity is 699.5156282668278
At time: 51.38874936103821 and batch: 500, loss is 6.535444974899292 and perplexity is 689.1403662732937
At time: 52.11605381965637 and batch: 550, loss is 6.512380361557007 and perplexity is 673.4275122045434
At time: 52.84612846374512 and batch: 600, loss is 6.463057107925415 and perplexity is 641.0177245390912
At time: 53.57628917694092 and batch: 650, loss is 6.504148225784302 and perplexity is 667.9065214426441
At time: 54.30664896965027 and batch: 700, loss is 6.506340570449829 and perplexity is 669.3724090202714
At time: 55.03695845603943 and batch: 750, loss is 6.495921745300293 and perplexity is 662.4345399131366
At time: 55.76548981666565 and batch: 800, loss is 6.46822681427002 and perplexity is 644.3401785929279
At time: 56.49283480644226 and batch: 850, loss is 6.470529384613037 and perplexity is 645.8255265818296
At time: 57.22158360481262 and batch: 900, loss is 6.523344640731811 and perplexity is 680.851785981664
At time: 57.98092555999756 and batch: 950, loss is 6.49496639251709 and perplexity is 661.8019834371469
At time: 58.71063828468323 and batch: 1000, loss is 6.509887657165527 and perplexity is 671.7509469464406
At time: 59.441153049468994 and batch: 1050, loss is 6.507005968093872 and perplexity is 669.8179560607573
At time: 60.17136192321777 and batch: 1100, loss is 6.507227411270142 and perplexity is 669.9662991006406
At time: 60.90244960784912 and batch: 1150, loss is 6.503502216339111 and perplexity is 667.4751868593955
At time: 61.633764028549194 and batch: 1200, loss is 6.478651494979858 and perplexity is 651.0923527041978
At time: 62.365288734436035 and batch: 1250, loss is 6.495599689483643 and perplexity is 662.2212333665586
At time: 63.094491720199585 and batch: 1300, loss is 6.448860349655152 and perplexity is 631.9816441478115
At time: 63.82532238960266 and batch: 1350, loss is 6.4547394752502445 and perplexity is 635.7080869868587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 6.114079996744792 and perplexity of 452.1798490155956
Finished 3 epochs...
Completing Train Step...
At time: 66.36589312553406 and batch: 50, loss is 6.507615489959717 and perplexity is 670.2263492007423
At time: 67.1269166469574 and batch: 100, loss is 6.5171787071228025 and perplexity is 676.6666150742722
At time: 67.86236929893494 and batch: 150, loss is 6.478018712997437 and perplexity is 650.6804835199567
At time: 68.59368085861206 and batch: 200, loss is 6.4909068202972415 and perplexity is 659.1207964085246
At time: 69.32568192481995 and batch: 250, loss is 6.51915997505188 and perplexity is 678.0086019161532
At time: 70.05621409416199 and batch: 300, loss is 6.535992431640625 and perplexity is 689.5177441021275
At time: 70.78834223747253 and batch: 350, loss is 6.559712324142456 and perplexity is 706.0685465017664
At time: 71.51927375793457 and batch: 400, loss is 6.581674280166626 and perplexity is 721.7467244342531
At time: 72.2495768070221 and batch: 450, loss is 6.558271169662476 and perplexity is 705.0517255268421
At time: 72.98328185081482 and batch: 500, loss is 6.543610620498657 and perplexity is 694.7906801068182
At time: 73.71399593353271 and batch: 550, loss is 6.516904277801514 and perplexity is 676.4809433924006
At time: 74.44114398956299 and batch: 600, loss is 6.463773488998413 and perplexity is 641.4771020294307
At time: 75.17280411720276 and batch: 650, loss is 6.505859880447388 and perplexity is 669.0507257165134
At time: 75.90302062034607 and batch: 700, loss is 6.510350513458252 and perplexity is 672.061943067082
At time: 76.66506958007812 and batch: 750, loss is 6.49866364479065 and perplexity is 664.2533612136245
At time: 77.39942073822021 and batch: 800, loss is 6.4723020362854005 and perplexity is 646.9713655682306
At time: 78.13190126419067 and batch: 850, loss is 6.468384313583374 and perplexity is 644.4416697208044
At time: 78.86374688148499 and batch: 900, loss is 6.523649158477784 and perplexity is 681.0591490041298
At time: 79.5936508178711 and batch: 950, loss is 6.495684118270874 and perplexity is 662.277146262466
At time: 80.32423162460327 and batch: 1000, loss is 6.509715385437012 and perplexity is 671.6352332170667
At time: 81.05474019050598 and batch: 1050, loss is 6.505276908874512 and perplexity is 668.6608018309222
At time: 81.78522634506226 and batch: 1100, loss is 6.504923124313354 and perplexity is 668.424281803617
At time: 82.51651287078857 and batch: 1150, loss is 6.4980159187316895 and perplexity is 663.8232463151663
At time: 83.25049924850464 and batch: 1200, loss is 6.4745133590698245 and perplexity is 648.4036110847194
At time: 83.99258041381836 and batch: 1250, loss is 6.491030311584472 and perplexity is 659.202197110149
At time: 84.72762060165405 and batch: 1300, loss is 6.443088111877441 and perplexity is 628.3442040108959
At time: 85.47377514839172 and batch: 1350, loss is 6.4482409954071045 and perplexity is 631.5903448207794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 6.114435221354166 and perplexity of 452.3405029582569
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 88.1035304069519 and batch: 50, loss is 6.5012742614746095 and perplexity is 665.9897376412576
At time: 88.83787608146667 and batch: 100, loss is 6.514638843536377 and perplexity is 674.950154888584
At time: 89.56972074508667 and batch: 150, loss is 6.468615446090698 and perplexity is 644.5906383548352
At time: 90.3015661239624 and batch: 200, loss is 6.47925910949707 and perplexity is 651.4880860842445
At time: 91.03199005126953 and batch: 250, loss is 6.504313707351685 and perplexity is 668.0170568062077
At time: 91.76420211791992 and batch: 300, loss is 6.51455491065979 and perplexity is 674.8935067578846
At time: 92.49220418930054 and batch: 350, loss is 6.529580421447754 and perplexity is 685.1106934249445
At time: 93.22502374649048 and batch: 400, loss is 6.562745723724365 and perplexity is 708.2135862720946
At time: 93.95475625991821 and batch: 450, loss is 6.538464622497559 and perplexity is 691.2244723745508
At time: 94.68623065948486 and batch: 500, loss is 6.526155157089233 and perplexity is 682.7680226089752
At time: 95.47683119773865 and batch: 550, loss is 6.488920364379883 and perplexity is 657.8127815892533
At time: 96.20597243309021 and batch: 600, loss is 6.4426787567138675 and perplexity is 628.087040705637
At time: 96.93538331985474 and batch: 650, loss is 6.475725193023681 and perplexity is 649.1898448926291
At time: 97.6657726764679 and batch: 700, loss is 6.47516098022461 and perplexity is 648.8236669839589
At time: 98.3948962688446 and batch: 750, loss is 6.465581521987915 and perplexity is 642.637962913061
At time: 99.1263017654419 and batch: 800, loss is 6.441090707778931 and perplexity is 627.0903993166974
At time: 99.85487508773804 and batch: 850, loss is 6.413603172302246 and perplexity is 610.0879775759792
At time: 100.5842764377594 and batch: 900, loss is 6.460365333557129 and perplexity is 639.2945696710607
At time: 101.31607794761658 and batch: 950, loss is 6.435899877548218 and perplexity is 623.8437133025741
At time: 102.04744935035706 and batch: 1000, loss is 6.444635257720948 and perplexity is 629.3170965436774
At time: 102.77777099609375 and batch: 1050, loss is 6.4322495269775395 and perplexity is 621.5706163721397
At time: 103.50534152984619 and batch: 1100, loss is 6.420654592514038 and perplexity is 614.4051675438767
At time: 104.23492455482483 and batch: 1150, loss is 6.404703283309937 and perplexity is 604.6823527113166
At time: 104.96965169906616 and batch: 1200, loss is 6.373949708938599 and perplexity is 586.3692490322371
At time: 105.69888663291931 and batch: 1250, loss is 6.383040981292725 and perplexity is 591.7243972466475
At time: 106.42952418327332 and batch: 1300, loss is 6.345813751220703 and perplexity is 570.1011214922265
At time: 107.15678334236145 and batch: 1350, loss is 6.369848861694336 and perplexity is 583.9695620510445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.993142496744792 and perplexity of 400.6717432728056
Finished 5 epochs...
Completing Train Step...
At time: 109.74976372718811 and batch: 50, loss is 6.445236921310425 and perplexity is 629.6958476558544
At time: 110.48230266571045 and batch: 100, loss is 6.46379807472229 and perplexity is 641.4928734022093
At time: 111.21561408042908 and batch: 150, loss is 6.423148965835571 and perplexity is 615.9396363758599
At time: 111.94973421096802 and batch: 200, loss is 6.436532068252563 and perplexity is 624.2382261896387
At time: 112.6805272102356 and batch: 250, loss is 6.464304361343384 and perplexity is 641.8177348911133
At time: 113.41187882423401 and batch: 300, loss is 6.478405561447143 and perplexity is 650.9322469502674
At time: 114.20642709732056 and batch: 350, loss is 6.492662630081177 and perplexity is 660.2791037377489
At time: 114.94162845611572 and batch: 400, loss is 6.526361074447632 and perplexity is 682.908630872934
At time: 115.67532277107239 and batch: 450, loss is 6.504400825500488 and perplexity is 668.0752557506211
At time: 116.41252422332764 and batch: 500, loss is 6.493800687789917 and perplexity is 661.0309672125397
At time: 117.14427328109741 and batch: 550, loss is 6.4609182262420655 and perplexity is 639.6481286932434
At time: 117.87719583511353 and batch: 600, loss is 6.41521333694458 and perplexity is 611.0711109570809
At time: 118.60952830314636 and batch: 650, loss is 6.447317981719971 and perplexity is 631.0076472481353
At time: 119.34197854995728 and batch: 700, loss is 6.449327774047852 and perplexity is 632.2771168342157
At time: 120.07239818572998 and batch: 750, loss is 6.44039216041565 and perplexity is 626.6524999362515
At time: 120.8030755519867 and batch: 800, loss is 6.414482564926147 and perplexity is 610.6247204126457
At time: 121.53706407546997 and batch: 850, loss is 6.393724870681763 and perplexity is 598.0802071836588
At time: 122.26953840255737 and batch: 900, loss is 6.442463550567627 and perplexity is 627.9518870575728
At time: 123.00220847129822 and batch: 950, loss is 6.420050249099732 and perplexity is 614.0339680044524
At time: 123.73517870903015 and batch: 1000, loss is 6.430230674743652 and perplexity is 620.3170229804468
At time: 124.46859312057495 and batch: 1050, loss is 6.422755241394043 and perplexity is 615.6971736214014
At time: 125.20426535606384 and batch: 1100, loss is 6.418701829910279 and perplexity is 613.2065507970091
At time: 125.93843913078308 and batch: 1150, loss is 6.40868987083435 and perplexity is 607.0977832985795
At time: 126.67993998527527 and batch: 1200, loss is 6.381700668334961 and perplexity is 590.9318326305549
At time: 127.41814541816711 and batch: 1250, loss is 6.395421123504638 and perplexity is 599.0955633303103
At time: 128.1536042690277 and batch: 1300, loss is 6.35656400680542 and perplexity is 576.2629153447232
At time: 128.88878345489502 and batch: 1350, loss is 6.373519716262817 and perplexity is 586.1171687500711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.9886702473958335 and perplexity of 398.8838402837856
Finished 6 epochs...
Completing Train Step...
At time: 131.41021966934204 and batch: 50, loss is 6.43789119720459 and perplexity is 625.0872232533684
At time: 132.18891596794128 and batch: 100, loss is 6.453275556564331 and perplexity is 634.7781428868817
At time: 132.91914796829224 and batch: 150, loss is 6.411360845565796 and perplexity is 608.7214936165923
At time: 133.71205711364746 and batch: 200, loss is 6.423776588439941 and perplexity is 616.3263353523928
At time: 134.44522213935852 and batch: 250, loss is 6.451768102645874 and perplexity is 633.8219649661825
At time: 135.17937350273132 and batch: 300, loss is 6.4676860904693605 and perplexity is 643.9918627024745
At time: 135.91305589675903 and batch: 350, loss is 6.480357637405396 and perplexity is 652.2041571687744
At time: 136.6460509300232 and batch: 400, loss is 6.514904975891113 and perplexity is 675.1298048669095
At time: 137.37784266471863 and batch: 450, loss is 6.493857803344727 and perplexity is 661.0687234412025
At time: 138.1106677055359 and batch: 500, loss is 6.483305530548096 and perplexity is 654.1296219692044
At time: 138.84227919578552 and batch: 550, loss is 6.451526775360107 and perplexity is 633.6690248867669
At time: 139.57642889022827 and batch: 600, loss is 6.406325950622558 and perplexity is 605.6643475095403
At time: 140.31020855903625 and batch: 650, loss is 6.438207187652588 and perplexity is 625.2847760558375
At time: 141.048104763031 and batch: 700, loss is 6.440581722259521 and perplexity is 626.7713005992869
At time: 141.7819275856018 and batch: 750, loss is 6.431678428649902 and perplexity is 621.2157397769813
At time: 142.5143780708313 and batch: 800, loss is 6.405175704956054 and perplexity is 604.9680852315622
At time: 143.24655747413635 and batch: 850, loss is 6.387286520004272 and perplexity is 594.2419264340833
At time: 143.97910618782043 and batch: 900, loss is 6.437711009979248 and perplexity is 624.9746006678938
At time: 144.7107117176056 and batch: 950, loss is 6.4164169406890865 and perplexity is 611.8070412297616
At time: 145.44200897216797 and batch: 1000, loss is 6.427749996185303 and perplexity is 618.7801229078319
At time: 146.1767270565033 and batch: 1050, loss is 6.4219810581207275 and perplexity is 615.2206956325991
At time: 146.9123854637146 and batch: 1100, loss is 6.420742883682251 and perplexity is 614.4594164886913
At time: 147.64523243904114 and batch: 1150, loss is 6.412978076934815 and perplexity is 609.7067335768136
At time: 148.38102006912231 and batch: 1200, loss is 6.387378063201904 and perplexity is 594.2963277301924
At time: 149.11600399017334 and batch: 1250, loss is 6.40095856666565 and perplexity is 602.4222230543152
At time: 149.84729504585266 and batch: 1300, loss is 6.360512084960938 and perplexity is 578.5425434865582
At time: 150.57793879508972 and batch: 1350, loss is 6.375031385421753 and perplexity is 587.0038540161283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.9875732421875 and perplexity of 398.4465025582139
Finished 7 epochs...
Completing Train Step...
At time: 153.15504026412964 and batch: 50, loss is 6.433528003692627 and perplexity is 622.3657881279318
At time: 153.9165449142456 and batch: 100, loss is 6.447183589935303 and perplexity is 630.9228507023897
At time: 154.6501603126526 and batch: 150, loss is 6.404485759735107 and perplexity is 604.550834349009
At time: 155.3857614994049 and batch: 200, loss is 6.416982793807984 and perplexity is 612.1533321178543
At time: 156.12456679344177 and batch: 250, loss is 6.44506425857544 and perplexity is 629.5871320344278
At time: 156.85578870773315 and batch: 300, loss is 6.461995115280152 and perplexity is 640.3373297812088
At time: 157.58850955963135 and batch: 350, loss is 6.474149341583252 and perplexity is 648.1676237862853
At time: 158.3219211101532 and batch: 400, loss is 6.509296932220459 and perplexity is 671.3542440878794
At time: 159.05487179756165 and batch: 450, loss is 6.488838901519776 and perplexity is 657.7591964612784
At time: 159.78699493408203 and batch: 500, loss is 6.478086376190186 and perplexity is 650.7245121284726
At time: 160.51796221733093 and batch: 550, loss is 6.446789054870606 and perplexity is 630.67397861228
At time: 161.24941205978394 and batch: 600, loss is 6.401715612411499 and perplexity is 602.8784569086429
At time: 161.98320150375366 and batch: 650, loss is 6.43359528541565 and perplexity is 622.4076633792114
At time: 162.71262335777283 and batch: 700, loss is 6.436655626296997 and perplexity is 624.3153606093185
At time: 163.44827151298523 and batch: 750, loss is 6.427765388488769 and perplexity is 618.7896474325647
At time: 164.1796534061432 and batch: 800, loss is 6.400980854034424 and perplexity is 602.4356496101791
At time: 164.91592144966125 and batch: 850, loss is 6.3845185279846195 and perplexity is 592.5993439004019
At time: 165.64842247962952 and batch: 900, loss is 6.436338567733765 and perplexity is 624.117447454768
At time: 166.38489055633545 and batch: 950, loss is 6.415671234130859 and perplexity is 611.3509827707725
At time: 167.11597776412964 and batch: 1000, loss is 6.427501811981201 and perplexity is 618.6265705109419
At time: 167.8498854637146 and batch: 1050, loss is 6.422903699874878 and perplexity is 615.7885858737458
At time: 168.57967901229858 and batch: 1100, loss is 6.422698841094971 and perplexity is 615.6624490959174
At time: 169.3144130706787 and batch: 1150, loss is 6.416065645217896 and perplexity is 611.5921539336118
At time: 170.05151271820068 and batch: 1200, loss is 6.390774049758911 and perplexity is 596.3179808805731
At time: 170.85191893577576 and batch: 1250, loss is 6.404145889282226 and perplexity is 604.3454002956094
At time: 171.60053753852844 and batch: 1300, loss is 6.362867174148559 and perplexity is 579.9066684627898
At time: 172.3543553352356 and batch: 1350, loss is 6.375509738922119 and perplexity is 587.2847165348542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.987218831380209 and perplexity of 398.30531383246296
Finished 8 epochs...
Completing Train Step...
At time: 174.94106912612915 and batch: 50, loss is 6.430938234329224 and perplexity is 620.7560895507124
At time: 175.6738350391388 and batch: 100, loss is 6.443674945831299 and perplexity is 628.7130459384178
At time: 176.40707683563232 and batch: 150, loss is 6.401038780212402 and perplexity is 602.470547415577
At time: 177.13936686515808 and batch: 200, loss is 6.413121204376221 and perplexity is 609.7940055869594
At time: 177.87610793113708 and batch: 250, loss is 6.441479320526123 and perplexity is 627.3341419972028
At time: 178.6087703704834 and batch: 300, loss is 6.458825750350952 and perplexity is 638.3110797643321
At time: 179.34401631355286 and batch: 350, loss is 6.470332536697388 and perplexity is 645.6984096848073
At time: 180.07821369171143 and batch: 400, loss is 6.5060012245178225 and perplexity is 669.1452987529249
At time: 180.81343698501587 and batch: 450, loss is 6.4859131336212155 and perplexity is 655.8375582238104
At time: 181.54623532295227 and batch: 500, loss is 6.475192489624024 and perplexity is 648.8441113501243
At time: 182.2827911376953 and batch: 550, loss is 6.444375514984131 and perplexity is 629.1536572257023
At time: 183.01395750045776 and batch: 600, loss is 6.399482164382935 and perplexity is 601.5334617550858
At time: 183.74578046798706 and batch: 650, loss is 6.431509685516358 and perplexity is 621.1109227302715
At time: 184.47802019119263 and batch: 700, loss is 6.434934387207031 and perplexity is 623.2416888941117
At time: 185.20786142349243 and batch: 750, loss is 6.425721435546875 and perplexity is 617.5261622043325
At time: 185.94018626213074 and batch: 800, loss is 6.398781776428223 and perplexity is 601.1123024687782
At time: 186.67148423194885 and batch: 850, loss is 6.383480386734009 and perplexity is 591.9844612991338
At time: 187.4032187461853 and batch: 900, loss is 6.4358843898773195 and perplexity is 623.8340514912699
At time: 188.14228081703186 and batch: 950, loss is 6.415445137023926 and perplexity is 611.2127737071705
At time: 188.88081908226013 and batch: 1000, loss is 6.427862892150879 and perplexity is 618.849984630766
At time: 189.61640405654907 and batch: 1050, loss is 6.423828783035279 and perplexity is 616.3585050955983
At time: 190.3799991607666 and batch: 1100, loss is 6.424166879653931 and perplexity is 616.5669290537809
At time: 191.111741065979 and batch: 1150, loss is 6.417937889099121 and perplexity is 612.7382761770064
At time: 191.84897661209106 and batch: 1200, loss is 6.392442188262939 and perplexity is 597.3135520095278
At time: 192.5822765827179 and batch: 1250, loss is 6.405499668121338 and perplexity is 605.164104357123
At time: 193.31783723831177 and batch: 1300, loss is 6.363729677200317 and perplexity is 580.4070554957251
At time: 194.0509808063507 and batch: 1350, loss is 6.37479600906372 and perplexity is 586.8657034461457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.987139892578125 and perplexity of 398.2738733290797
Finished 9 epochs...
Completing Train Step...
At time: 196.5933084487915 and batch: 50, loss is 6.428757410049439 and perplexity is 619.4038046825875
At time: 197.32651495933533 and batch: 100, loss is 6.441076040267944 and perplexity is 627.0812015288305
At time: 198.0582070350647 and batch: 150, loss is 6.3988881683349605 and perplexity is 601.1762593549887
At time: 198.79013442993164 and batch: 200, loss is 6.410546951293945 and perplexity is 608.2262602409266
At time: 199.5188751220703 and batch: 250, loss is 6.439075126647949 and perplexity is 625.827720683505
At time: 200.25406694412231 and batch: 300, loss is 6.456222219467163 and perplexity is 636.6513786337827
At time: 200.99230647087097 and batch: 350, loss is 6.467139225006104 and perplexity is 643.6397820732869
At time: 201.73279690742493 and batch: 400, loss is 6.503922843933106 and perplexity is 667.7560043969576
At time: 202.4640347957611 and batch: 450, loss is 6.483934736251831 and perplexity is 654.5413335703903
At time: 203.19749236106873 and batch: 500, loss is 6.472993774414062 and perplexity is 647.419055154073
At time: 203.9322702884674 and batch: 550, loss is 6.442440853118897 and perplexity is 627.9376343135623
At time: 204.6638741493225 and batch: 600, loss is 6.397608194351196 and perplexity is 600.4072616368836
At time: 205.39278888702393 and batch: 650, loss is 6.430112199783325 and perplexity is 620.2435352990702
At time: 206.1235692501068 and batch: 700, loss is 6.433921604156494 and perplexity is 622.6107998060263
At time: 206.85539746284485 and batch: 750, loss is 6.424614820480347 and perplexity is 616.8431764201398
At time: 207.5906958580017 and batch: 800, loss is 6.397827415466309 and perplexity is 600.5388980145109
At time: 208.3256630897522 and batch: 850, loss is 6.383028392791748 and perplexity is 591.71694837038
At time: 209.11674189567566 and batch: 900, loss is 6.435690727233887 and perplexity is 623.713249837556
At time: 209.84804773330688 and batch: 950, loss is 6.415299987792968 and perplexity is 611.124063081411
At time: 210.58114790916443 and batch: 1000, loss is 6.427781686782837 and perplexity is 618.7997327303913
At time: 211.3153338432312 and batch: 1050, loss is 6.4241877460479735 and perplexity is 616.5797947165058
At time: 212.05097723007202 and batch: 1100, loss is 6.425029764175415 and perplexity is 617.0991847178823
At time: 212.78286600112915 and batch: 1150, loss is 6.419065589904785 and perplexity is 613.4296513836324
At time: 213.51792240142822 and batch: 1200, loss is 6.39350583076477 and perplexity is 597.949218091166
At time: 214.25039792060852 and batch: 1250, loss is 6.406327228546143 and perplexity is 605.665121502789
At time: 214.9835660457611 and batch: 1300, loss is 6.3642066478729244 and perplexity is 580.6839586714681
At time: 215.71644949913025 and batch: 1350, loss is 6.374306583404541 and perplexity is 586.5785465890862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.987140299479167 and perplexity of 398.27403538716663
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 218.24446272850037 and batch: 50, loss is 6.428330039978027 and perplexity is 619.1391465918493
At time: 219.00949811935425 and batch: 100, loss is 6.44158260345459 and perplexity is 627.3989382506311
At time: 219.74290680885315 and batch: 150, loss is 6.400212659835815 and perplexity is 601.9730397489687
At time: 220.47595977783203 and batch: 200, loss is 6.412380781173706 and perplexity is 609.3426670678436
At time: 221.20654463768005 and batch: 250, loss is 6.44087329864502 and perplexity is 626.9540789552809
At time: 221.93766283988953 and batch: 300, loss is 6.45919753074646 and perplexity is 638.5484354293815
At time: 222.6682586669922 and batch: 350, loss is 6.469395637512207 and perplexity is 645.0937386730209
At time: 223.40008974075317 and batch: 400, loss is 6.506259469985962 and perplexity is 669.3181248086671
At time: 224.13120985031128 and batch: 450, loss is 6.482144994735718 and perplexity is 653.3709214516869
At time: 224.86341881752014 and batch: 500, loss is 6.46805661201477 and perplexity is 644.2305197737373
At time: 225.59322047233582 and batch: 550, loss is 6.435101833343506 and perplexity is 623.346057044785
At time: 226.32385444641113 and batch: 600, loss is 6.39392783164978 and perplexity is 598.2016064406993
At time: 227.05410242080688 and batch: 650, loss is 6.429216527938843 and perplexity is 619.6882493419017
At time: 227.81567192077637 and batch: 700, loss is 6.427875614166259 and perplexity is 618.8578576998693
At time: 228.54595184326172 and batch: 750, loss is 6.418482637405395 and perplexity is 613.0721552469156
At time: 229.2762815952301 and batch: 800, loss is 6.389839372634888 and perplexity is 595.7608765021175
At time: 230.00743198394775 and batch: 850, loss is 6.371965951919556 and perplexity is 585.2071879232467
At time: 230.7394459247589 and batch: 900, loss is 6.41676812171936 and perplexity is 612.0219339877509
At time: 231.46911096572876 and batch: 950, loss is 6.396814451217652 and perplexity is 599.9308815823387
At time: 232.20056891441345 and batch: 1000, loss is 6.410661144256592 and perplexity is 608.2957193653392
At time: 232.93229985237122 and batch: 1050, loss is 6.405859041213989 and perplexity is 605.3816231357246
At time: 233.66354608535767 and batch: 1100, loss is 6.404396305084228 and perplexity is 604.4967568839604
At time: 234.39464282989502 and batch: 1150, loss is 6.396228446960449 and perplexity is 599.579422520015
At time: 235.12448048591614 and batch: 1200, loss is 6.368990421295166 and perplexity is 583.4684740949389
At time: 235.85598134994507 and batch: 1250, loss is 6.3831334400177 and perplexity is 591.7791098592443
At time: 236.58824229240417 and batch: 1300, loss is 6.3471010875701905 and perplexity is 570.8355059873139
At time: 237.32051825523376 and batch: 1350, loss is 6.361708612442016 and perplexity is 579.2351998473599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.979991455078125 and perplexity of 395.4369891627083
Finished 11 epochs...
Completing Train Step...
At time: 239.82925128936768 and batch: 50, loss is 6.423720026016236 and perplexity is 616.2914754269618
At time: 240.58698296546936 and batch: 100, loss is 6.436609315872192 and perplexity is 624.2864489692166
At time: 241.31491255760193 and batch: 150, loss is 6.394522705078125 and perplexity is 598.5575665462713
At time: 242.05229711532593 and batch: 200, loss is 6.407005205154419 and perplexity is 606.0758875167567
At time: 242.7790012359619 and batch: 250, loss is 6.435100402832031 and perplexity is 623.3451653417354
At time: 243.50429487228394 and batch: 300, loss is 6.453482046127319 and perplexity is 634.9092314819483
At time: 244.23251914978027 and batch: 350, loss is 6.4635351943969725 and perplexity is 641.3242597105424
At time: 244.96078896522522 and batch: 400, loss is 6.500738430023193 and perplexity is 665.6329749843711
At time: 245.6926555633545 and batch: 450, loss is 6.47729660987854 and perplexity is 650.210794715686
At time: 246.42031359672546 and batch: 500, loss is 6.463805274963379 and perplexity is 641.4974923221831
At time: 247.17828273773193 and batch: 550, loss is 6.431971673965454 and perplexity is 621.3979350952743
At time: 247.90719199180603 and batch: 600, loss is 6.39065468788147 and perplexity is 596.2468074946021
At time: 248.6342647075653 and batch: 650, loss is 6.425968132019043 and perplexity is 617.6785225226233
At time: 249.359925031662 and batch: 700, loss is 6.4256205749511714 and perplexity is 617.4638812886478
At time: 250.08830189704895 and batch: 750, loss is 6.416557016372681 and perplexity is 611.8927465217633
At time: 250.81522679328918 and batch: 800, loss is 6.387977914810181 and perplexity is 594.6529242799885
At time: 251.54330563545227 and batch: 850, loss is 6.370544004440307 and perplexity is 584.3756453825613
At time: 252.27136278152466 and batch: 900, loss is 6.416300992965699 and perplexity is 611.7361077085479
At time: 252.99797987937927 and batch: 950, loss is 6.39620451927185 and perplexity is 599.5650761419414
At time: 253.72587704658508 and batch: 1000, loss is 6.411030435562134 and perplexity is 608.5203991692952
At time: 254.4536099433899 and batch: 1050, loss is 6.406825075149536 and perplexity is 605.9667248962058
At time: 255.1796441078186 and batch: 1100, loss is 6.406352634429932 and perplexity is 605.6805091559485
At time: 255.90553426742554 and batch: 1150, loss is 6.398535261154175 and perplexity is 600.9641373680321
At time: 256.6319525241852 and batch: 1200, loss is 6.371928930282593 and perplexity is 585.1855229962251
At time: 257.3599967956543 and batch: 1250, loss is 6.386068925857544 and perplexity is 593.5188212563685
At time: 258.0868799686432 and batch: 1300, loss is 6.349462966918946 and perplexity is 572.1853440302375
At time: 258.81495690345764 and batch: 1350, loss is 6.36245213508606 and perplexity is 579.666034482479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.97857421875 and perplexity of 394.8769584378648
Finished 12 epochs...
Completing Train Step...
At time: 261.41444396972656 and batch: 50, loss is 6.422108564376831 and perplexity is 615.2991451214707
At time: 262.14821910858154 and batch: 100, loss is 6.434384336471558 and perplexity is 622.8989686101711
At time: 262.8803427219391 and batch: 150, loss is 6.391941137313843 and perplexity is 597.0143424532612
At time: 263.61190485954285 and batch: 200, loss is 6.404486541748047 and perplexity is 604.5513071157692
At time: 264.342077255249 and batch: 250, loss is 6.4324017524719235 and perplexity is 621.6652424685809
At time: 265.07333040237427 and batch: 300, loss is 6.450862417221069 and perplexity is 633.2481815234629
At time: 265.8336572647095 and batch: 350, loss is 6.460668668746949 and perplexity is 639.4885196251385
At time: 266.5642397403717 and batch: 400, loss is 6.498064622879029 and perplexity is 663.8555780477003
At time: 267.29597997665405 and batch: 450, loss is 6.474932842254638 and perplexity is 648.6756625530609
At time: 268.0263078212738 and batch: 500, loss is 6.461618318557739 and perplexity is 640.0960982245928
At time: 268.7575354576111 and batch: 550, loss is 6.430293292999267 and perplexity is 620.3558673665256
At time: 269.48758697509766 and batch: 600, loss is 6.389092721939087 and perplexity is 595.3162172523731
At time: 270.22019600868225 and batch: 650, loss is 6.42441276550293 and perplexity is 616.718552776896
At time: 270.95126700401306 and batch: 700, loss is 6.424658222198486 and perplexity is 616.8699490548039
At time: 271.68073415756226 and batch: 750, loss is 6.415902318954468 and perplexity is 611.4922730291777
At time: 272.4117684364319 and batch: 800, loss is 6.387462434768676 and perplexity is 594.3464715578168
At time: 273.14229822158813 and batch: 850, loss is 6.37025673866272 and perplexity is 584.207798367893
At time: 273.872838973999 and batch: 900, loss is 6.416624269485474 and perplexity is 611.9338995974828
At time: 274.6063005924225 and batch: 950, loss is 6.396463928222656 and perplexity is 599.7206288642949
At time: 275.3381233215332 and batch: 1000, loss is 6.411849155426025 and perplexity is 609.0188109096335
At time: 276.0672490596771 and batch: 1050, loss is 6.408057012557983 and perplexity is 606.7136979904574
At time: 276.7999303340912 and batch: 1100, loss is 6.408085527420044 and perplexity is 606.730998594527
At time: 277.53300166130066 and batch: 1150, loss is 6.4003816509246825 and perplexity is 602.0747764244784
At time: 278.2640118598938 and batch: 1200, loss is 6.373956880569458 and perplexity is 586.3734542711175
At time: 278.99661779403687 and batch: 1250, loss is 6.388018770217895 and perplexity is 594.6772195639522
At time: 279.7292945384979 and batch: 1300, loss is 6.350854053497314 and perplexity is 572.9818572636455
At time: 280.4598741531372 and batch: 1350, loss is 6.36286919593811 and perplexity is 579.9078409132178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.977908935546875 and perplexity of 394.6143407973536
Finished 13 epochs...
Completing Train Step...
At time: 282.99233961105347 and batch: 50, loss is 6.420985565185547 and perplexity is 614.6085525191456
At time: 283.7211585044861 and batch: 100, loss is 6.432800025939941 and perplexity is 621.9128845520072
At time: 284.4823703765869 and batch: 150, loss is 6.390164928436279 and perplexity is 595.9548614864602
At time: 285.2184679508209 and batch: 200, loss is 6.402744073867797 and perplexity is 603.4988131159496
At time: 285.9519362449646 and batch: 250, loss is 6.430506706237793 and perplexity is 620.4882736493429
At time: 286.6891231536865 and batch: 300, loss is 6.449100704193115 and perplexity is 632.1335620602404
At time: 287.421124458313 and batch: 350, loss is 6.458794288635254 and perplexity is 638.2909977185229
At time: 288.15235805511475 and batch: 400, loss is 6.496342039108276 and perplexity is 662.7130155651531
At time: 288.8865568637848 and batch: 450, loss is 6.473394613265992 and perplexity is 647.6786178827958
At time: 289.62094354629517 and batch: 500, loss is 6.460169172286987 and perplexity is 639.1691771352607
At time: 290.35350728034973 and batch: 550, loss is 6.429173345565796 and perplexity is 619.6614903105091
At time: 291.0845401287079 and batch: 600, loss is 6.388137798309327 and perplexity is 594.7480070711819
At time: 291.81884598731995 and batch: 650, loss is 6.423472747802735 and perplexity is 616.1390988124073
At time: 292.5519344806671 and batch: 700, loss is 6.424123077392578 and perplexity is 616.5399226194891
At time: 293.28198742866516 and batch: 750, loss is 6.415637140274048 and perplexity is 611.3301398132148
At time: 294.014582157135 and batch: 800, loss is 6.387336254119873 and perplexity is 594.2714812656831
At time: 294.74873876571655 and batch: 850, loss is 6.370291290283203 and perplexity is 584.2279840427473
At time: 295.4822702407837 and batch: 900, loss is 6.417052402496338 and perplexity is 612.1959447914635
At time: 296.2133758068085 and batch: 950, loss is 6.396877374649048 and perplexity is 599.9686324797038
At time: 296.9483313560486 and batch: 1000, loss is 6.412621688842774 and perplexity is 609.4894800729098
At time: 297.6787931919098 and batch: 1050, loss is 6.409131088256836 and perplexity is 607.3657045191321
At time: 298.41634011268616 and batch: 1100, loss is 6.4094593334198 and perplexity is 607.5651020976459
At time: 299.14713191986084 and batch: 1150, loss is 6.401806163787842 and perplexity is 602.9330508544247
At time: 299.8803517818451 and batch: 1200, loss is 6.375438585281372 and perplexity is 587.2429305757466
At time: 300.61324095726013 and batch: 1250, loss is 6.389430112838745 and perplexity is 595.5171054135062
At time: 301.34864020347595 and batch: 1300, loss is 6.3517911338806154 and perplexity is 573.5190389739871
At time: 302.0812714099884 and batch: 1350, loss is 6.36311676979065 and perplexity is 580.0514287050686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.977543131510417 and perplexity of 394.47001567761066
Finished 14 epochs...
Completing Train Step...
At time: 304.59411358833313 and batch: 50, loss is 6.420115976333618 and perplexity is 614.0743280850454
At time: 305.3584234714508 and batch: 100, loss is 6.431560287475586 and perplexity is 621.1423529550683
At time: 306.09147000312805 and batch: 150, loss is 6.3887897396087645 and perplexity is 595.1358742793379
At time: 306.82532715797424 and batch: 200, loss is 6.401399250030518 and perplexity is 602.6877590109841
At time: 307.5604546070099 and batch: 250, loss is 6.429047718048095 and perplexity is 619.5836486652954
At time: 308.2920949459076 and batch: 300, loss is 6.447793045043945 and perplexity is 631.3074870542986
At time: 309.03015995025635 and batch: 350, loss is 6.457407550811768 and perplexity is 637.4064688961319
At time: 309.76416754722595 and batch: 400, loss is 6.495085096359253 and perplexity is 661.8805465381106
At time: 310.4995529651642 and batch: 450, loss is 6.4722413730621335 and perplexity is 646.9321193902456
At time: 311.23092007637024 and batch: 500, loss is 6.459083700180054 and perplexity is 638.4757532361051
At time: 311.9618754386902 and batch: 550, loss is 6.428336315155029 and perplexity is 619.1430318117733
At time: 312.6957278251648 and batch: 600, loss is 6.387464113235474 and perplexity is 594.3474691494728
At time: 313.4276626110077 and batch: 650, loss is 6.422825946807861 and perplexity is 615.7407082838989
At time: 314.1660621166229 and batch: 700, loss is 6.42378677368164 and perplexity is 616.3326128170524
At time: 314.89872646331787 and batch: 750, loss is 6.415512886047363 and perplexity is 611.2541841784454
At time: 315.6331989765167 and batch: 800, loss is 6.387355947494507 and perplexity is 594.2831845918366
At time: 316.36894845962524 and batch: 850, loss is 6.370422105789185 and perplexity is 584.304415121165
At time: 317.10352396965027 and batch: 900, loss is 6.41744707107544 and perplexity is 612.437606980226
At time: 317.8308906555176 and batch: 950, loss is 6.397272329330445 and perplexity is 600.2056397002679
At time: 318.57953429222107 and batch: 1000, loss is 6.413282251358032 and perplexity is 609.8922189793584
At time: 319.33063793182373 and batch: 1050, loss is 6.410020666122437 and perplexity is 607.9062439965284
At time: 320.07965111732483 and batch: 1100, loss is 6.41055121421814 and perplexity is 608.2288530688941
At time: 320.811625957489 and batch: 1150, loss is 6.402915182113648 and perplexity is 603.6020855743673
At time: 321.5512475967407 and batch: 1200, loss is 6.37653356552124 and perplexity is 587.8863021560304
At time: 322.3483142852783 and batch: 1250, loss is 6.390454349517822 and perplexity is 596.1273683493794
At time: 323.08093190193176 and batch: 1300, loss is 6.352439432144165 and perplexity is 573.8909709194573
At time: 323.8142600059509 and batch: 1350, loss is 6.363255414962769 and perplexity is 580.1318556105116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.977305501302084 and perplexity of 394.3762888222112
Finished 15 epochs...
Completing Train Step...
At time: 326.3370666503906 and batch: 50, loss is 6.419392433166504 and perplexity is 613.6301795005722
At time: 327.1012201309204 and batch: 100, loss is 6.4305210781097415 and perplexity is 620.4971912914388
At time: 327.8325140476227 and batch: 150, loss is 6.387561540603638 and perplexity is 594.4053776800591
At time: 328.5646553039551 and batch: 200, loss is 6.400263948440552 and perplexity is 602.0039148980314
At time: 329.2964630126953 and batch: 250, loss is 6.427808923721313 and perplexity is 618.8165871701713
At time: 330.03366017341614 and batch: 300, loss is 6.446751136779785 and perplexity is 630.6500651124607
At time: 330.7703266143799 and batch: 350, loss is 6.456278371810913 and perplexity is 636.6871291045715
At time: 331.50841760635376 and batch: 400, loss is 6.494068584442139 and perplexity is 661.2080789184282
At time: 332.24089527130127 and batch: 450, loss is 6.471208066940307 and perplexity is 646.2639857236261
At time: 332.9748320579529 and batch: 500, loss is 6.458145523071289 and perplexity is 637.877030797676
At time: 333.71383142471313 and batch: 550, loss is 6.4275497341156 and perplexity is 618.6562171269559
At time: 334.4484553337097 and batch: 600, loss is 6.3868605899810795 and perplexity is 593.9888748515341
At time: 335.18365597724915 and batch: 650, loss is 6.422272434234619 and perplexity is 615.3999823667458
At time: 335.9162428379059 and batch: 700, loss is 6.423523836135864 and perplexity is 616.1705771360221
At time: 336.65037178993225 and batch: 750, loss is 6.4153922653198245 and perplexity is 611.1804587005386
At time: 337.38292622566223 and batch: 800, loss is 6.387409391403199 and perplexity is 594.314946256817
At time: 338.11912846565247 and batch: 850, loss is 6.370541496276855 and perplexity is 584.3741796747634
At time: 338.85453033447266 and batch: 900, loss is 6.4177741527557375 and perplexity is 612.637956865397
At time: 339.58574199676514 and batch: 950, loss is 6.397603206634521 and perplexity is 600.4042669830411
At time: 340.32067108154297 and batch: 1000, loss is 6.413816137313843 and perplexity is 610.2179188051749
At time: 341.11363768577576 and batch: 1050, loss is 6.410755300521851 and perplexity is 608.3529969149256
At time: 341.84784483909607 and batch: 1100, loss is 6.411432113647461 and perplexity is 608.7648775756377
At time: 342.580917596817 and batch: 1150, loss is 6.4037776374816895 and perplexity is 604.1228899862299
At time: 343.3155801296234 and batch: 1200, loss is 6.3773283195495605 and perplexity is 588.3537128765372
At time: 344.0526773929596 and batch: 1250, loss is 6.391225576400757 and perplexity is 596.5872951326568
At time: 344.78488302230835 and batch: 1300, loss is 6.352910194396973 and perplexity is 574.1612007277993
At time: 345.52026748657227 and batch: 1350, loss is 6.363330984115601 and perplexity is 580.1756973398914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.977146402994792 and perplexity of 394.3135492132386
Finished 16 epochs...
Completing Train Step...
At time: 348.12716150283813 and batch: 50, loss is 6.418781814575195 and perplexity is 613.2555998790601
At time: 348.8615822792053 and batch: 100, loss is 6.429632472991943 and perplexity is 619.9460592173339
At time: 349.5938467979431 and batch: 150, loss is 6.386561250686645 and perplexity is 593.8110972500725
At time: 350.3276026248932 and batch: 200, loss is 6.399318256378174 and perplexity is 601.4348736854803
At time: 351.0611400604248 and batch: 250, loss is 6.426798210144043 and perplexity is 618.1914568107658
At time: 351.79500246047974 and batch: 300, loss is 6.445901279449463 and perplexity is 630.1143302131173
At time: 352.5290262699127 and batch: 350, loss is 6.455337038040161 and perplexity is 636.0880760072336
At time: 353.2633225917816 and batch: 400, loss is 6.493258590698242 and perplexity is 660.6727213584979
At time: 353.99502635002136 and batch: 450, loss is 6.470371370315552 and perplexity is 645.723484977177
At time: 354.7293722629547 and batch: 500, loss is 6.45739260673523 and perplexity is 637.396943516249
At time: 355.46321749687195 and batch: 550, loss is 6.426903619766235 and perplexity is 618.256623573212
At time: 356.19394993782043 and batch: 600, loss is 6.386399183273316 and perplexity is 593.7148676195891
At time: 356.93586349487305 and batch: 650, loss is 6.421793813705444 and perplexity is 615.105509777466
At time: 357.6671018600464 and batch: 700, loss is 6.4231782722473145 and perplexity is 615.9576876209601
At time: 358.3965108394623 and batch: 750, loss is 6.4152444648742675 and perplexity is 611.0901326317079
At time: 359.13074564933777 and batch: 800, loss is 6.387462110519409 and perplexity is 594.34627884144
At time: 359.9248676300049 and batch: 850, loss is 6.370647497177124 and perplexity is 584.4361271470888
At time: 360.65578603744507 and batch: 900, loss is 6.418100509643555 and perplexity is 612.8379281115801
At time: 361.3909480571747 and batch: 950, loss is 6.397918691635132 and perplexity is 600.5937154060736
At time: 362.12305784225464 and batch: 1000, loss is 6.4142991733551025 and perplexity is 610.5127472536311
At time: 362.85596656799316 and batch: 1050, loss is 6.411355781555176 and perplexity is 608.7184110522886
At time: 363.59959983825684 and batch: 1100, loss is 6.412158126831055 and perplexity is 609.2070093798055
At time: 364.3347818851471 and batch: 1150, loss is 6.404465847015381 and perplexity is 604.5387962175407
At time: 365.0679852962494 and batch: 1200, loss is 6.37799105644226 and perplexity is 588.7437658249804
At time: 365.8022882938385 and batch: 1250, loss is 6.391881532669068 and perplexity is 596.9787586859428
At time: 366.5344099998474 and batch: 1300, loss is 6.353264360427857 and perplexity is 574.3645851351469
At time: 367.2673714160919 and batch: 1350, loss is 6.363354139328003 and perplexity is 580.1891315869294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.977036539713541 and perplexity of 394.270231012464
Finished 17 epochs...
Completing Train Step...
At time: 369.8336343765259 and batch: 50, loss is 6.418218507766723 and perplexity is 612.9102461035136
At time: 370.5668888092041 and batch: 100, loss is 6.428843441009522 and perplexity is 619.4570948788539
At time: 371.29956698417664 and batch: 150, loss is 6.38573884010315 and perplexity is 593.3229414788473
At time: 372.03175592422485 and batch: 200, loss is 6.398527641296386 and perplexity is 600.9595581242161
At time: 372.7647273540497 and batch: 250, loss is 6.425967445373535 and perplexity is 617.6780983965864
At time: 373.49703431129456 and batch: 300, loss is 6.445168857574463 and perplexity is 629.6529896624974
At time: 374.22872591018677 and batch: 350, loss is 6.454529581069946 and perplexity is 635.5746695612918
At time: 374.96200466156006 and batch: 400, loss is 6.492558383941651 and perplexity is 660.2102757777615
At time: 375.6920063495636 and batch: 450, loss is 6.46971583366394 and perplexity is 645.3003282784846
At time: 376.42840933799744 and batch: 500, loss is 6.456775856018067 and perplexity is 637.0039496962853
At time: 377.159964799881 and batch: 550, loss is 6.426366567611694 and perplexity is 617.9246766658423
At time: 377.89254355430603 and batch: 600, loss is 6.3859619045257565 and perplexity is 593.4553054805086
At time: 378.6270203590393 and batch: 650, loss is 6.421292238235473 and perplexity is 614.7970653028751
At time: 379.41708540916443 and batch: 700, loss is 6.42283000946045 and perplexity is 615.7432098295627
At time: 380.1489462852478 and batch: 750, loss is 6.415176486968994 and perplexity is 611.0485934164485
At time: 380.88225841522217 and batch: 800, loss is 6.387558526992798 and perplexity is 594.4035863762688
At time: 381.61493015289307 and batch: 850, loss is 6.370793838500976 and perplexity is 584.5216605620262
At time: 382.3548045158386 and batch: 900, loss is 6.418387422561645 and perplexity is 613.0137844563734
At time: 383.089129447937 and batch: 950, loss is 6.398206214904786 and perplexity is 600.7664249026703
At time: 383.8205621242523 and batch: 1000, loss is 6.4147196388244625 and perplexity is 610.7695007566638
At time: 384.55550956726074 and batch: 1050, loss is 6.411867666244507 and perplexity is 609.0300844506351
At time: 385.2913281917572 and batch: 1100, loss is 6.41277774810791 and perplexity is 609.5846039755685
At time: 386.0260851383209 and batch: 1150, loss is 6.4050484466552735 and perplexity is 604.8911029194321
At time: 386.76402974128723 and batch: 1200, loss is 6.3785828113555905 and perplexity is 589.0922609427794
At time: 387.4983477592468 and batch: 1250, loss is 6.3924238395690915 and perplexity is 597.3025921865803
At time: 388.23131132125854 and batch: 1300, loss is 6.353552436828613 and perplexity is 574.5300698525308
At time: 388.9654881954193 and batch: 1350, loss is 6.363373317718506 and perplexity is 580.2002587873612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976951904296875 and perplexity of 394.2368631992525
Finished 18 epochs...
Completing Train Step...
At time: 391.5615658760071 and batch: 50, loss is 6.417715663909912 and perplexity is 612.6021254262711
At time: 392.3210883140564 and batch: 100, loss is 6.428161859512329 and perplexity is 619.0350282374258
At time: 393.0477740764618 and batch: 150, loss is 6.385027008056641 and perplexity is 592.9007454793223
At time: 393.7851617336273 and batch: 200, loss is 6.397846937179565 and perplexity is 600.5506216771099
At time: 394.5093846321106 and batch: 250, loss is 6.425278635025024 and perplexity is 617.2527818284052
At time: 395.24739480018616 and batch: 300, loss is 6.4445147037506105 and perplexity is 629.2412344419188
At time: 395.9976737499237 and batch: 350, loss is 6.45382942199707 and perplexity is 635.1298219401301
At time: 396.74431324005127 and batch: 400, loss is 6.4919272136688235 and perplexity is 659.793702156148
At time: 397.4731431007385 and batch: 450, loss is 6.469162578582764 and perplexity is 644.9434113351399
At time: 398.2645421028137 and batch: 500, loss is 6.4562211608886715 and perplexity is 636.6507046886833
At time: 398.99149656295776 and batch: 550, loss is 6.425916490554809 and perplexity is 617.6466255229055
At time: 399.7196581363678 and batch: 600, loss is 6.385679273605347 and perplexity is 593.2876003617412
At time: 400.4481508731842 and batch: 650, loss is 6.421043128967285 and perplexity is 614.6439327300166
At time: 401.17500853538513 and batch: 700, loss is 6.422688093185425 and perplexity is 615.6558320471632
At time: 401.90038442611694 and batch: 750, loss is 6.415181741714478 and perplexity is 611.0518043297216
At time: 402.6323175430298 and batch: 800, loss is 6.387666311264038 and perplexity is 594.4676571864994
At time: 403.36088967323303 and batch: 850, loss is 6.3709365749359135 and perplexity is 584.6050990547134
At time: 404.0894057750702 and batch: 900, loss is 6.41862151145935 and perplexity is 613.1573009746112
At time: 404.81760144233704 and batch: 950, loss is 6.398449106216431 and perplexity is 600.912363570507
At time: 405.54462122917175 and batch: 1000, loss is 6.415070552825927 and perplexity is 610.9838659358186
At time: 406.27470421791077 and batch: 1050, loss is 6.4122953033447265 and perplexity is 609.2905840055635
At time: 407.0027754306793 and batch: 1100, loss is 6.413301448822022 and perplexity is 609.9039274756565
At time: 407.73362731933594 and batch: 1150, loss is 6.405551700592041 and perplexity is 605.1955933598721
At time: 408.4618327617645 and batch: 1200, loss is 6.37908821105957 and perplexity is 589.3900632453355
At time: 409.1920073032379 and batch: 1250, loss is 6.39287371635437 and perplexity is 597.5713652094306
At time: 409.9232211112976 and batch: 1300, loss is 6.353796863555909 and perplexity is 574.6705175211206
At time: 410.65073561668396 and batch: 1350, loss is 6.363376903533935 and perplexity is 580.2023392821316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976885172526042 and perplexity of 394.2105559530177
Finished 19 epochs...
Completing Train Step...
At time: 413.2129900455475 and batch: 50, loss is 6.417286796569824 and perplexity is 612.3394567112498
At time: 413.9992046356201 and batch: 100, loss is 6.427580080032349 and perplexity is 618.6749911018719
At time: 414.7301490306854 and batch: 150, loss is 6.3844032478332515 and perplexity is 592.5310328958635
At time: 415.4629330635071 and batch: 200, loss is 6.397233285903931 and perplexity is 600.1822060729484
At time: 416.1942901611328 and batch: 250, loss is 6.424684038162232 and perplexity is 616.885874352607
At time: 416.98288345336914 and batch: 300, loss is 6.443933219909668 and perplexity is 628.8754471920302
At time: 417.71867513656616 and batch: 350, loss is 6.453228626251221 and perplexity is 634.7483532489044
At time: 418.4492154121399 and batch: 400, loss is 6.491392650604248 and perplexity is 659.4410950664958
At time: 419.18263959884644 and batch: 450, loss is 6.468687553405761 and perplexity is 644.637119730885
At time: 419.91430020332336 and batch: 500, loss is 6.45576587677002 and perplexity is 636.3609137073316
At time: 420.6444833278656 and batch: 550, loss is 6.425597171783448 and perplexity is 617.4494308469644
At time: 421.37564969062805 and batch: 600, loss is 6.385472183227539 and perplexity is 593.1647489295473
At time: 422.1080689430237 and batch: 650, loss is 6.420869274139404 and perplexity is 614.5370832033061
At time: 422.8402826786041 and batch: 700, loss is 6.422592868804932 and perplexity is 615.597209393157
At time: 423.57233810424805 and batch: 750, loss is 6.415203800201416 and perplexity is 611.065283356629
At time: 424.30479311943054 and batch: 800, loss is 6.387771692276001 and perplexity is 594.5303060907374
At time: 425.03614807128906 and batch: 850, loss is 6.371049919128418 and perplexity is 584.671364402925
At time: 425.7694170475006 and batch: 900, loss is 6.41881067276001 and perplexity is 613.2732975778608
At time: 426.49874091148376 and batch: 950, loss is 6.398656444549561 and perplexity is 601.0369686555858
At time: 427.234010219574 and batch: 1000, loss is 6.415349006652832 and perplexity is 611.1540204204498
At time: 427.96539282798767 and batch: 1050, loss is 6.412655687332153 and perplexity is 609.5102021467825
At time: 428.7009365558624 and batch: 1100, loss is 6.413749771118164 and perplexity is 610.1774223071831
At time: 429.4314272403717 and batch: 1150, loss is 6.405981416702271 and perplexity is 605.4557115406624
At time: 430.1619625091553 and batch: 1200, loss is 6.3795139026641845 and perplexity is 589.6410150573522
At time: 430.89332485198975 and batch: 1250, loss is 6.393248596191406 and perplexity is 597.7954246604988
At time: 431.62673926353455 and batch: 1300, loss is 6.354000616073608 and perplexity is 574.7876200154722
At time: 432.36172008514404 and batch: 1350, loss is 6.363361520767212 and perplexity is 580.19341423354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.97681884765625 and perplexity of 394.1844108562682
Finished 20 epochs...
Completing Train Step...
At time: 434.9358983039856 and batch: 50, loss is 6.416910228729248 and perplexity is 612.1089127747767
At time: 435.66930413246155 and batch: 100, loss is 6.427056064605713 and perplexity is 618.3508807892794
At time: 436.4313094615936 and batch: 150, loss is 6.383833389282227 and perplexity is 592.1934702106468
At time: 437.1658215522766 and batch: 200, loss is 6.396660280227661 and perplexity is 599.8383967738198
At time: 437.89873123168945 and batch: 250, loss is 6.4241406440734865 and perplexity is 616.5507532747058
At time: 438.6324644088745 and batch: 300, loss is 6.443440790176392 and perplexity is 628.5658464578652
At time: 439.3677554130554 and batch: 350, loss is 6.452707824707031 and perplexity is 634.4178613941646
At time: 440.10186862945557 and batch: 400, loss is 6.490952062606811 and perplexity is 659.150617230216
At time: 440.833696603775 and batch: 450, loss is 6.468267793655396 and perplexity is 644.3665837984505
At time: 441.5655858516693 and batch: 500, loss is 6.45537522315979 and perplexity is 636.1123655702575
At time: 442.2979350090027 and batch: 550, loss is 6.42534104347229 and perplexity is 617.2913048181572
At time: 443.03083181381226 and batch: 600, loss is 6.385294094085693 and perplexity is 593.059122134209
At time: 443.7632794380188 and batch: 650, loss is 6.420720367431641 and perplexity is 614.4455813222388
At time: 444.49330830574036 and batch: 700, loss is 6.4225012397766115 and perplexity is 615.5408054031844
At time: 445.22364044189453 and batch: 750, loss is 6.41522403717041 and perplexity is 611.0776495909487
At time: 445.9553837776184 and batch: 800, loss is 6.387867336273193 and perplexity is 594.5871720650653
At time: 446.6857752799988 and batch: 850, loss is 6.37108811378479 and perplexity is 584.6936961512529
At time: 447.4165177345276 and batch: 900, loss is 6.418944854736328 and perplexity is 613.355593322132
At time: 448.149254322052 and batch: 950, loss is 6.398836107254028 and perplexity is 601.1449622837825
At time: 448.8812038898468 and batch: 1000, loss is 6.41554012298584 and perplexity is 611.2708330977854
At time: 449.61094880104065 and batch: 1050, loss is 6.41295449256897 and perplexity is 609.6923541997155
At time: 450.34599256515503 and batch: 1100, loss is 6.414136953353882 and perplexity is 610.4137179075141
At time: 451.0816445350647 and batch: 1150, loss is 6.406346216201782 and perplexity is 605.6766217727298
At time: 451.8124077320099 and batch: 1200, loss is 6.3798681640625 and perplexity is 589.8499391125293
At time: 452.5432801246643 and batch: 1250, loss is 6.393554105758667 and perplexity is 597.9780847827839
At time: 453.27634716033936 and batch: 1300, loss is 6.354157381057739 and perplexity is 574.8777336507492
At time: 454.0075159072876 and batch: 1350, loss is 6.363329639434815 and perplexity is 580.174917189303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976759847005209 and perplexity of 394.16115440547696
Finished 21 epochs...
Completing Train Step...
At time: 456.6096887588501 and batch: 50, loss is 6.41656795501709 and perplexity is 611.8994398355418
At time: 457.3433392047882 and batch: 100, loss is 6.426573209762573 and perplexity is 618.0523791440124
At time: 458.0754597187042 and batch: 150, loss is 6.383307266235351 and perplexity is 591.8819855245808
At time: 458.8074941635132 and batch: 200, loss is 6.3961279010772705 and perplexity is 599.519140308057
At time: 459.53834652900696 and batch: 250, loss is 6.423638582229614 and perplexity is 616.2412843594436
At time: 460.2697992324829 and batch: 300, loss is 6.442999286651611 and perplexity is 628.2883936737954
At time: 461.00218296051025 and batch: 350, loss is 6.4522292518615725 and perplexity is 634.1143188724252
At time: 461.7363381385803 and batch: 400, loss is 6.4905486583709715 and perplexity is 658.8847667053881
At time: 462.466335773468 and batch: 450, loss is 6.467859249114991 and perplexity is 644.1033851164733
At time: 463.2058401107788 and batch: 500, loss is 6.454996814727783 and perplexity is 635.871700825078
At time: 463.9420599937439 and batch: 550, loss is 6.425111789703369 and perplexity is 617.1498046803449
At time: 464.6752893924713 and batch: 600, loss is 6.3851332855224605 and perplexity is 592.9637608165303
At time: 465.4118881225586 and batch: 650, loss is 6.420575666427612 and perplexity is 614.3566768621379
At time: 466.1460020542145 and batch: 700, loss is 6.422421159744263 and perplexity is 615.4915148491968
At time: 466.87736105918884 and batch: 750, loss is 6.4152384376525875 and perplexity is 611.0864494671117
At time: 467.6226749420166 and batch: 800, loss is 6.387948503494263 and perplexity is 594.6354350121635
At time: 468.35514879226685 and batch: 850, loss is 6.371126480102539 and perplexity is 584.716129125718
At time: 469.0913052558899 and batch: 900, loss is 6.419061603546143 and perplexity is 613.4272060379142
At time: 469.83000349998474 and batch: 950, loss is 6.398988466262818 and perplexity is 601.2365591119989
At time: 470.56784081459045 and batch: 1000, loss is 6.415739803314209 and perplexity is 611.3929040456387
At time: 471.2991509437561 and batch: 1050, loss is 6.413222427368164 and perplexity is 609.8557338847827
At time: 472.03229999542236 and batch: 1100, loss is 6.414475708007813 and perplexity is 610.6205334231598
At time: 472.7647907733917 and batch: 1150, loss is 6.406663446426392 and perplexity is 605.8687911828569
At time: 473.525422334671 and batch: 1200, loss is 6.380175266265869 and perplexity is 590.0311111462297
At time: 474.25634479522705 and batch: 1250, loss is 6.393804254531861 and perplexity is 598.1276869777118
At time: 474.9903440475464 and batch: 1300, loss is 6.35425329208374 and perplexity is 574.932873408222
At time: 475.72049832344055 and batch: 1350, loss is 6.363276195526123 and perplexity is 580.1439112025516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976719156901042 and perplexity of 394.1451162733445
Finished 22 epochs...
Completing Train Step...
At time: 478.2517194747925 and batch: 50, loss is 6.416252689361572 and perplexity is 611.7065593634229
At time: 479.0199637413025 and batch: 100, loss is 6.426143579483032 and perplexity is 617.7869021601488
At time: 479.75169920921326 and batch: 150, loss is 6.382829113006592 and perplexity is 591.5990428925148
At time: 480.4876334667206 and batch: 200, loss is 6.395656509399414 and perplexity is 599.2365985737432
At time: 481.22085523605347 and batch: 250, loss is 6.423181858062744 and perplexity is 615.9598963355003
At time: 481.95778250694275 and batch: 300, loss is 6.44259931564331 and perplexity is 628.0371467805599
At time: 482.6922118663788 and batch: 350, loss is 6.451739559173584 and perplexity is 633.8038737446826
At time: 483.42646980285645 and batch: 400, loss is 6.490114183425903 and perplexity is 658.5985599618991
At time: 484.16030740737915 and batch: 450, loss is 6.4674060344696045 and perplexity is 643.811534169753
At time: 484.89656925201416 and batch: 500, loss is 6.45459529876709 and perplexity is 635.6164394374332
At time: 485.6284890174866 and batch: 550, loss is 6.4249059677124025 and perplexity is 617.022794749988
At time: 486.3596770763397 and batch: 600, loss is 6.384997339248657 and perplexity is 592.8831550818957
At time: 487.0913894176483 and batch: 650, loss is 6.420435914993286 and perplexity is 614.2708256344143
At time: 487.8232662677765 and batch: 700, loss is 6.422368631362915 and perplexity is 615.4591849253153
At time: 488.55525183677673 and batch: 750, loss is 6.415247001647949 and perplexity is 611.0916828310397
At time: 489.2889266014099 and batch: 800, loss is 6.388011875152588 and perplexity is 594.6731192398227
At time: 490.02347326278687 and batch: 850, loss is 6.371199607849121 and perplexity is 584.7588896621028
At time: 490.75521206855774 and batch: 900, loss is 6.419171733856201 and perplexity is 613.4947666864825
At time: 491.48574805259705 and batch: 950, loss is 6.399111547470093 and perplexity is 601.3105645878004
At time: 492.2817189693451 and batch: 1000, loss is 6.415928993225098 and perplexity is 611.5085843571019
At time: 493.0168147087097 and batch: 1050, loss is 6.413465375900269 and perplexity is 610.0039154396422
At time: 493.7488832473755 and batch: 1100, loss is 6.414776535034179 and perplexity is 610.8042522148713
At time: 494.4801847934723 and batch: 1150, loss is 6.406945476531982 and perplexity is 606.0396885199723
At time: 495.21197962760925 and batch: 1200, loss is 6.380462217330932 and perplexity is 590.2004454961676
At time: 495.9459855556488 and batch: 1250, loss is 6.394007692337036 and perplexity is 598.249381139742
At time: 496.6799645423889 and batch: 1300, loss is 6.3542831516265865 and perplexity is 574.9500408972947
At time: 497.4145896434784 and batch: 1350, loss is 6.363202333450317 and perplexity is 580.1010621514835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976684977213542 and perplexity of 394.13164474666826
Finished 23 epochs...
Completing Train Step...
At time: 499.9486765861511 and batch: 50, loss is 6.415953702926636 and perplexity is 611.5236947383951
At time: 500.7141010761261 and batch: 100, loss is 6.425749158859253 and perplexity is 617.5432823123405
At time: 501.4463369846344 and batch: 150, loss is 6.3824001121521 and perplexity is 591.3453008292698
At time: 502.18208360671997 and batch: 200, loss is 6.395258255004883 and perplexity is 598.9979974801181
At time: 502.91468143463135 and batch: 250, loss is 6.422761545181275 and perplexity is 615.7010548576163
At time: 503.6497573852539 and batch: 300, loss is 6.442232904434204 and perplexity is 627.8070690843418
At time: 504.3868582248688 and batch: 350, loss is 6.451306715011596 and perplexity is 633.5295948023889
At time: 505.1236217021942 and batch: 400, loss is 6.48975513458252 and perplexity is 658.3621333575803
At time: 505.85868191719055 and batch: 450, loss is 6.4670592212677 and perplexity is 643.5882905443204
At time: 506.5906150341034 and batch: 500, loss is 6.45429835319519 and perplexity is 635.4277239708134
At time: 507.32346510887146 and batch: 550, loss is 6.424729814529419 and perplexity is 616.9141137932487
At time: 508.0536971092224 and batch: 600, loss is 6.3848936557769775 and perplexity is 592.821686084791
At time: 508.78767108917236 and batch: 650, loss is 6.420314083099365 and perplexity is 614.1959924149759
At time: 509.520126581192 and batch: 700, loss is 6.4223365974426265 and perplexity is 615.4394696706246
At time: 510.2502224445343 and batch: 750, loss is 6.415268783569336 and perplexity is 611.104993727003
At time: 510.98288202285767 and batch: 800, loss is 6.388073253631592 and perplexity is 594.70962049157
At time: 511.7451732158661 and batch: 850, loss is 6.37128511428833 and perplexity is 584.8088924503033
At time: 512.4759352207184 and batch: 900, loss is 6.419276256561279 and perplexity is 613.5588941703784
At time: 513.2103095054626 and batch: 950, loss is 6.3992173290252685 and perplexity is 601.3741755188383
At time: 513.9420711994171 and batch: 1000, loss is 6.416112852096558 and perplexity is 611.6210259716872
At time: 514.6740825176239 and batch: 1050, loss is 6.413691148757935 and perplexity is 610.1416533149704
At time: 515.4042720794678 and batch: 1100, loss is 6.415052661895752 and perplexity is 610.9729349639177
At time: 516.1379990577698 and batch: 1150, loss is 6.407192802429199 and perplexity is 606.1895963669666
At time: 516.8703396320343 and batch: 1200, loss is 6.380704278945923 and perplexity is 590.3433276616188
At time: 517.6031000614166 and batch: 1250, loss is 6.394191837310791 and perplexity is 598.3595559000837
At time: 518.3334317207336 and batch: 1300, loss is 6.354314823150634 and perplexity is 574.9682507297063
At time: 519.0639836788177 and batch: 1350, loss is 6.363128156661987 and perplexity is 580.0580337136614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976656901041666 and perplexity of 394.12057919420874
Finished 24 epochs...
Completing Train Step...
At time: 521.6034252643585 and batch: 50, loss is 6.415666522979737 and perplexity is 611.3481026106882
At time: 522.33984541893 and batch: 100, loss is 6.425379667282105 and perplexity is 617.3151474205578
At time: 523.0709362030029 and batch: 150, loss is 6.3820147132873535 and perplexity is 591.1174409329503
At time: 523.8041880130768 and batch: 200, loss is 6.394885559082031 and perplexity is 598.774794964578
At time: 524.5345706939697 and batch: 250, loss is 6.422362251281738 and perplexity is 615.4552582582805
At time: 525.2660357952118 and batch: 300, loss is 6.441878595352173 and perplexity is 627.5846707392054
At time: 525.9977967739105 and batch: 350, loss is 6.450947933197021 and perplexity is 633.3023366752582
At time: 526.7290523052216 and batch: 400, loss is 6.489473400115966 and perplexity is 658.1766761792061
At time: 527.4632589817047 and batch: 450, loss is 6.466771583557129 and perplexity is 643.4031969031099
At time: 528.1946227550507 and batch: 500, loss is 6.454036235809326 and perplexity is 635.261189143691
At time: 528.9326994419098 and batch: 550, loss is 6.424563035964966 and perplexity is 616.8112343222431
At time: 529.6700685024261 and batch: 600, loss is 6.384793100357055 and perplexity is 592.7620776482337
At time: 530.4600393772125 and batch: 650, loss is 6.4202046298980715 and perplexity is 614.1287703762853
At time: 531.1907546520233 and batch: 700, loss is 6.422301378250122 and perplexity is 615.4177947711557
At time: 531.9243884086609 and batch: 750, loss is 6.4152881526947025 and perplexity is 611.1168304108716
At time: 532.6581296920776 and batch: 800, loss is 6.388123035430908 and perplexity is 594.739226943474
At time: 533.3905715942383 and batch: 850, loss is 6.371367931365967 and perplexity is 584.8573266193125
At time: 534.1233537197113 and batch: 900, loss is 6.4193667221069335 and perplexity is 613.614402621294
At time: 534.8609211444855 and batch: 950, loss is 6.399306211471558 and perplexity is 601.4276295022189
At time: 535.5924696922302 and batch: 1000, loss is 6.416282329559326 and perplexity is 611.7246907355172
At time: 536.3236577510834 and batch: 1050, loss is 6.413894414901733 and perplexity is 610.2656870615142
At time: 537.0577964782715 and batch: 1100, loss is 6.415290956497192 and perplexity is 611.1185438641643
At time: 537.7926409244537 and batch: 1150, loss is 6.407357807159424 and perplexity is 606.2896287704633
At time: 538.5269753932953 and batch: 1200, loss is 6.380924215316773 and perplexity is 590.4731799097527
At time: 539.258930683136 and batch: 1250, loss is 6.394361362457276 and perplexity is 598.4610014899946
At time: 539.9919710159302 and batch: 1300, loss is 6.354373807907105 and perplexity is 575.0021660921886
At time: 540.7246882915497 and batch: 1350, loss is 6.363050622940063 and perplexity is 580.013061398834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976630859375 and perplexity of 394.1103157710978
Finished 25 epochs...
Completing Train Step...
At time: 543.2890701293945 and batch: 50, loss is 6.415383424758911 and perplexity is 611.1750555463477
At time: 544.028647184372 and batch: 100, loss is 6.425035524368286 and perplexity is 617.1027393384445
At time: 544.7737834453583 and batch: 150, loss is 6.381660385131836 and perplexity is 590.9080284829648
At time: 545.5299029350281 and batch: 200, loss is 6.394542379379272 and perplexity is 598.5693428639346
At time: 546.2638413906097 and batch: 250, loss is 6.421990261077881 and perplexity is 615.226357508354
At time: 546.9956378936768 and batch: 300, loss is 6.44154920578003 and perplexity is 627.3779849349701
At time: 547.7381315231323 and batch: 350, loss is 6.450619010925293 and perplexity is 633.0940636866783
At time: 548.4739491939545 and batch: 400, loss is 6.489219799041748 and perplexity is 658.0097830301368
At time: 549.203206539154 and batch: 450, loss is 6.466510467529297 and perplexity is 643.235215948254
At time: 549.989336013794 and batch: 500, loss is 6.453794050216675 and perplexity is 635.1073566648666
At time: 550.7171175479889 and batch: 550, loss is 6.424405717849732 and perplexity is 616.7142063737313
At time: 551.4465770721436 and batch: 600, loss is 6.384697437286377 and perplexity is 592.705374919926
At time: 552.1785223484039 and batch: 650, loss is 6.420102500915528 and perplexity is 614.0660532324894
At time: 552.9116435050964 and batch: 700, loss is 6.422262992858887 and perplexity is 615.3941721717146
At time: 553.6486592292786 and batch: 750, loss is 6.415301847457886 and perplexity is 611.1251995684479
At time: 554.3788042068481 and batch: 800, loss is 6.38816614151001 and perplexity is 594.7648643721961
At time: 555.1086180210114 and batch: 850, loss is 6.371442880630493 and perplexity is 584.9011628885229
At time: 555.8369176387787 and batch: 900, loss is 6.419441432952881 and perplexity is 613.6602479849508
At time: 556.5643880367279 and batch: 950, loss is 6.39938383102417 and perplexity is 601.4743138575353
At time: 557.2931377887726 and batch: 1000, loss is 6.416429824829102 and perplexity is 611.8149238881236
At time: 558.0224978923798 and batch: 1050, loss is 6.414065284729004 and perplexity is 610.3699719633694
At time: 558.7532494068146 and batch: 1100, loss is 6.415496597290039 and perplexity is 611.2442276884655
At time: 559.4847240447998 and batch: 1150, loss is 6.407551355361939 and perplexity is 606.4069863951274
At time: 560.2142753601074 and batch: 1200, loss is 6.381117582321167 and perplexity is 590.5873689795704
At time: 560.9486601352692 and batch: 1250, loss is 6.394508323669434 and perplexity is 598.5489585071795
At time: 561.6791620254517 and batch: 1300, loss is 6.354433660507202 and perplexity is 575.0365824968362
At time: 562.406641960144 and batch: 1350, loss is 6.362953968048096 and perplexity is 579.9570030082473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976605224609375 and perplexity of 394.10021297501464
Finished 26 epochs...
Completing Train Step...
At time: 564.9508748054504 and batch: 50, loss is 6.415083560943604 and perplexity is 610.9918137375379
At time: 565.7158029079437 and batch: 100, loss is 6.42471981048584 and perplexity is 616.9079421884402
At time: 566.446879863739 and batch: 150, loss is 6.381331977844238 and perplexity is 590.7140018417361
At time: 567.178870677948 and batch: 200, loss is 6.394218788146973 and perplexity is 598.3756824077633
At time: 567.9092881679535 and batch: 250, loss is 6.421635293960572 and perplexity is 615.0080111369201
At time: 568.6730127334595 and batch: 300, loss is 6.441236238479615 and perplexity is 627.1816668628561
At time: 569.4049303531647 and batch: 350, loss is 6.450309715270996 and perplexity is 632.8982807229894
At time: 570.1364812850952 and batch: 400, loss is 6.48898247718811 and perplexity is 657.8536414573531
At time: 570.8683376312256 and batch: 450, loss is 6.4662667083740235 and perplexity is 643.078440583864
At time: 571.6072835922241 and batch: 500, loss is 6.45356427192688 and perplexity is 634.9614395475512
At time: 572.3409461975098 and batch: 550, loss is 6.424252786636353 and perplexity is 616.6198987333171
At time: 573.0766100883484 and batch: 600, loss is 6.384605093002319 and perplexity is 592.6506444934818
At time: 573.810061454773 and batch: 650, loss is 6.419994087219238 and perplexity is 613.9994836704838
At time: 574.5426714420319 and batch: 700, loss is 6.422216758728028 and perplexity is 615.3657206147501
At time: 575.2734804153442 and batch: 750, loss is 6.415301971435547 and perplexity is 611.1252753343254
At time: 576.0055811405182 and batch: 800, loss is 6.388203678131103 and perplexity is 594.7871902545661
At time: 576.7406280040741 and batch: 850, loss is 6.371506252288818 and perplexity is 584.9382302196683
At time: 577.4746334552765 and batch: 900, loss is 6.41949990272522 and perplexity is 613.6961296089291
At time: 578.2094702720642 and batch: 950, loss is 6.399448947906494 and perplexity is 601.5134812648678
At time: 578.9431962966919 and batch: 1000, loss is 6.416553115844726 and perplexity is 611.8903598216549
At time: 579.6789722442627 and batch: 1050, loss is 6.414181966781616 and perplexity is 610.4411953397148
At time: 580.4109811782837 and batch: 1100, loss is 6.415624351501465 and perplexity is 611.3223217010756
At time: 581.1469805240631 and batch: 1150, loss is 6.407719964981079 and perplexity is 606.509241066466
At time: 581.8822195529938 and batch: 1200, loss is 6.381270999908447 and perplexity is 590.6779824194674
At time: 582.6153562068939 and batch: 1250, loss is 6.394579019546509 and perplexity is 598.5912749465548
At time: 583.3459787368774 and batch: 1300, loss is 6.354436798095703 and perplexity is 575.0383867278356
At time: 584.0792496204376 and batch: 1350, loss is 6.362730255126953 and perplexity is 579.8272736445756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976575927734375 and perplexity of 394.0886672394655
Finished 27 epochs...
Completing Train Step...
At time: 586.6179854869843 and batch: 50, loss is 6.414598608016968 and perplexity is 610.695583304029
At time: 587.4065709114075 and batch: 100, loss is 6.424423561096192 and perplexity is 616.7252106554868
At time: 588.1363894939423 and batch: 150, loss is 6.381015844345093 and perplexity is 590.5272868723234
At time: 588.8699939250946 and batch: 200, loss is 6.393899393081665 and perplexity is 598.1845946854644
At time: 589.5991077423096 and batch: 250, loss is 6.421272468566895 and perplexity is 614.7849110887937
At time: 590.3310010433197 and batch: 300, loss is 6.440914688110351 and perplexity is 626.9800287864174
At time: 591.0656859874725 and batch: 350, loss is 6.450001907348633 and perplexity is 632.7034995972576
At time: 591.7992525100708 and batch: 400, loss is 6.488745994567871 and perplexity is 657.6980888979509
At time: 592.5334322452545 and batch: 450, loss is 6.4660267162322995 and perplexity is 642.9241253295767
At time: 593.26540184021 and batch: 500, loss is 6.453330230712891 and perplexity is 634.8128497901449
At time: 593.9982073307037 and batch: 550, loss is 6.424088201522827 and perplexity is 616.5184206284038
At time: 594.7308030128479 and batch: 600, loss is 6.38450662612915 and perplexity is 592.5922909106315
At time: 595.4612336158752 and batch: 650, loss is 6.419854955673218 and perplexity is 613.9140629155637
At time: 596.1946246623993 and batch: 700, loss is 6.42214415550232 and perplexity is 615.3210447002708
At time: 596.9243891239166 and batch: 750, loss is 6.41526577949524 and perplexity is 611.1031579250789
At time: 597.656769990921 and batch: 800, loss is 6.388233575820923 and perplexity is 594.8049732833235
At time: 598.3876578807831 and batch: 850, loss is 6.37155439376831 and perplexity is 584.9663906893203
At time: 599.1215345859528 and batch: 900, loss is 6.419541482925415 and perplexity is 613.7216477473781
At time: 599.8508603572845 and batch: 950, loss is 6.399496545791626 and perplexity is 601.5421127158472
At time: 600.5859267711639 and batch: 1000, loss is 6.4166553020477295 and perplexity is 611.9528897689739
At time: 601.3205096721649 and batch: 1050, loss is 6.414224100112915 and perplexity is 610.4669158026775
At time: 602.0540549755096 and batch: 1100, loss is 6.415648517608642 and perplexity is 611.3370951603297
At time: 602.7868883609772 and batch: 1150, loss is 6.407865571975708 and perplexity is 606.5975594840058
At time: 603.5205488204956 and batch: 1200, loss is 6.381411504745484 and perplexity is 590.7609813638682
At time: 604.2524764537811 and batch: 1250, loss is 6.394626665115356 and perplexity is 598.6197958477987
At time: 604.983327627182 and batch: 1300, loss is 6.354465026855468 and perplexity is 575.0546195774259
At time: 605.7165462970734 and batch: 1350, loss is 6.362544822692871 and perplexity is 579.719764829995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976551513671875 and perplexity of 394.07904605155977
Finished 28 epochs...
Completing Train Step...
At time: 608.2912693023682 and batch: 50, loss is 6.414261264801025 and perplexity is 610.4896040368027
At time: 609.0236823558807 and batch: 100, loss is 6.424155921936035 and perplexity is 616.5601729243244
At time: 609.7556335926056 and batch: 150, loss is 6.380722141265869 and perplexity is 590.3538726571941
At time: 610.488817691803 and batch: 200, loss is 6.393600673675537 and perplexity is 598.0059320249153
At time: 611.2187855243683 and batch: 250, loss is 6.420956745147705 and perplexity is 614.5908397326466
At time: 611.9560835361481 and batch: 300, loss is 6.440624008178711 and perplexity is 626.7978047602267
At time: 612.6882028579712 and batch: 350, loss is 6.449717979431153 and perplexity is 632.523882910537
At time: 613.4207468032837 and batch: 400, loss is 6.488528890609741 and perplexity is 657.555315538488
At time: 614.1532514095306 and batch: 450, loss is 6.465817260742187 and perplexity is 642.789475443872
At time: 614.8842272758484 and batch: 500, loss is 6.453111143112182 and perplexity is 634.6737854001835
At time: 615.6133341789246 and batch: 550, loss is 6.423948898315429 and perplexity is 616.4325436166009
At time: 616.3460495471954 and batch: 600, loss is 6.384420652389526 and perplexity is 592.5413457253147
At time: 617.0775582790375 and batch: 650, loss is 6.41973690032959 and perplexity is 613.8415913578197
At time: 617.8094148635864 and batch: 700, loss is 6.422086162567139 and perplexity is 615.2853614615077
At time: 618.5403871536255 and batch: 750, loss is 6.415236463546753 and perplexity is 611.0852431189772
At time: 619.2715575695038 and batch: 800, loss is 6.3882534885406494 and perplexity is 594.8168175859741
At time: 620.0009074211121 and batch: 850, loss is 6.3716047000885006 and perplexity is 584.9958189360781
At time: 620.7487573623657 and batch: 900, loss is 6.419586935043335 and perplexity is 613.7495433300335
At time: 621.5026071071625 and batch: 950, loss is 6.399548683166504 and perplexity is 601.5734763600847
At time: 622.2526741027832 and batch: 1000, loss is 6.4167564010620115 and perplexity is 612.0147607304104
At time: 622.9986088275909 and batch: 1050, loss is 6.414348077774048 and perplexity is 610.5426047548704
At time: 623.7344810962677 and batch: 1100, loss is 6.41579574584961 and perplexity is 611.4271078715324
At time: 624.4807579517365 and batch: 1150, loss is 6.407996292114258 and perplexity is 606.6768591839467
At time: 625.2462074756622 and batch: 1200, loss is 6.381550321578979 and perplexity is 590.8429946249329
At time: 626.0052151679993 and batch: 1250, loss is 6.394690322875976 and perplexity is 598.6579038563876
At time: 626.7429845333099 and batch: 1300, loss is 6.354503870010376 and perplexity is 575.0769569469187
At time: 627.4743371009827 and batch: 1350, loss is 6.362426166534424 and perplexity is 579.6509815905825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976534830729166 and perplexity of 394.0724717082515
Finished 29 epochs...
Completing Train Step...
At time: 630.0083978176117 and batch: 50, loss is 6.413992033004761 and perplexity is 610.3252629480232
At time: 630.7520163059235 and batch: 100, loss is 6.423895959854126 and perplexity is 616.3999114899993
At time: 631.4885084629059 and batch: 150, loss is 6.3804378318786625 and perplexity is 590.1860533668544
At time: 632.2214624881744 and batch: 200, loss is 6.393313436508179 and perplexity is 597.834187161872
At time: 632.9533550739288 and batch: 250, loss is 6.4206601333618165 and perplexity is 614.408571878816
At time: 633.6906518936157 and batch: 300, loss is 6.440341167449951 and perplexity is 626.6205458815401
At time: 634.4242537021637 and batch: 350, loss is 6.449446926116943 and perplexity is 632.3524584493911
At time: 635.1602008342743 and batch: 400, loss is 6.488324041366577 and perplexity is 657.4206296253861
At time: 635.8957407474518 and batch: 450, loss is 6.465623054504395 and perplexity is 642.6646538391127
At time: 636.6272370815277 and batch: 500, loss is 6.452902603149414 and perplexity is 634.5414443522842
At time: 637.3616321086884 and batch: 550, loss is 6.4238178920745845 and perplexity is 616.3517923958997
At time: 638.0987136363983 and batch: 600, loss is 6.384344434738159 and perplexity is 592.4961853366369
At time: 638.8302805423737 and batch: 650, loss is 6.419650936126709 and perplexity is 613.7888252227556
At time: 639.5667762756348 and batch: 700, loss is 6.4220262622833255 and perplexity is 615.2485067975437
At time: 640.2979800701141 and batch: 750, loss is 6.415182571411133 and perplexity is 611.0523113175702
At time: 641.0294313430786 and batch: 800, loss is 6.388255786895752 and perplexity is 594.8181846878131
At time: 641.7656128406525 and batch: 850, loss is 6.371643667221069 and perplexity is 585.0186149898523
At time: 642.498295545578 and batch: 900, loss is 6.419626169204712 and perplexity is 613.7736237510459
At time: 643.2311503887177 and batch: 950, loss is 6.3995949649810795 and perplexity is 601.6013189164681
At time: 643.9972939491272 and batch: 1000, loss is 6.416844654083252 and perplexity is 612.0687752655265
At time: 644.7309150695801 and batch: 1050, loss is 6.414468870162964 and perplexity is 610.616358108966
At time: 645.4632716178894 and batch: 1100, loss is 6.415935220718384 and perplexity is 611.5123925345632
At time: 646.2013816833496 and batch: 1150, loss is 6.408120107650757 and perplexity is 606.7519798552053
At time: 646.9387645721436 and batch: 1200, loss is 6.3816831588745115 and perplexity is 590.9214858235869
At time: 647.672798871994 and batch: 1250, loss is 6.394762535095214 and perplexity is 598.7011358331092
At time: 648.4086465835571 and batch: 1300, loss is 6.354540462493897 and perplexity is 575.0980008260103
At time: 649.1483163833618 and batch: 1350, loss is 6.362314519882202 and perplexity is 579.5862691115638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976511637369792 and perplexity of 394.06333194978686
Finished 30 epochs...
Completing Train Step...
At time: 651.7167980670929 and batch: 50, loss is 6.4137459659576415 and perplexity is 610.1751004885613
At time: 652.4789431095123 and batch: 100, loss is 6.423643770217896 and perplexity is 616.2444814202987
At time: 653.2139992713928 and batch: 150, loss is 6.380158014297486 and perplexity is 590.02093203596
At time: 653.9518504142761 and batch: 200, loss is 6.393027877807617 and perplexity is 597.6634947807416
At time: 654.6860284805298 and batch: 250, loss is 6.420370607376099 and perplexity is 614.2307103804147
At time: 655.4194440841675 and batch: 300, loss is 6.440060586929321 and perplexity is 626.4447530257181
At time: 656.1488697528839 and batch: 350, loss is 6.449187231063843 and perplexity is 632.188260965676
At time: 656.8825967311859 and batch: 400, loss is 6.488131170272827 and perplexity is 657.2938444164904
At time: 657.6129560470581 and batch: 450, loss is 6.465442905426025 and perplexity is 642.5488888218166
At time: 658.3447456359863 and batch: 500, loss is 6.452702922821045 and perplexity is 634.4147515577625
At time: 659.0760869979858 and batch: 550, loss is 6.42369402885437 and perplexity is 616.2754538059778
At time: 659.8070995807648 and batch: 600, loss is 6.384275236129761 and perplexity is 592.4551868436662
At time: 660.5412378311157 and batch: 650, loss is 6.419580202102662 and perplexity is 613.7454110046813
At time: 661.2726018428802 and batch: 700, loss is 6.421950178146362 and perplexity is 615.2016979266149
At time: 662.0024795532227 and batch: 750, loss is 6.4150940990448 and perplexity is 610.998252465027
At time: 662.7897210121155 and batch: 800, loss is 6.3882371520996095 and perplexity is 594.8071004754759
At time: 663.5194585323334 and batch: 850, loss is 6.371667499542236 and perplexity is 585.0325575075141
At time: 664.2518837451935 and batch: 900, loss is 6.419659767150879 and perplexity is 613.79424563064
At time: 664.979737997055 and batch: 950, loss is 6.39963812828064 and perplexity is 601.6272865748333
At time: 665.7145929336548 and batch: 1000, loss is 6.416924390792847 and perplexity is 612.1175815615129
At time: 666.4472944736481 and batch: 1050, loss is 6.414582071304321 and perplexity is 610.6854844901538
At time: 667.1780743598938 and batch: 1100, loss is 6.416051511764526 and perplexity is 611.5835100855069
At time: 667.9093019962311 and batch: 1150, loss is 6.408237466812134 and perplexity is 606.823191937338
At time: 668.6423468589783 and batch: 1200, loss is 6.38180814743042 and perplexity is 590.9953488626766
At time: 669.3760595321655 and batch: 1250, loss is 6.394844808578491 and perplexity is 598.7503950873333
At time: 670.1120157241821 and batch: 1300, loss is 6.354568691253662 and perplexity is 575.1142353584564
At time: 670.8430962562561 and batch: 1350, loss is 6.36220272064209 and perplexity is 579.5214754291076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976483561197917 and perplexity of 394.0522683152622
Finished 31 epochs...
Completing Train Step...
At time: 673.3850729465485 and batch: 50, loss is 6.413516397476196 and perplexity is 610.035039594725
At time: 674.1460583209991 and batch: 100, loss is 6.423397102355957 and perplexity is 616.092492457802
At time: 674.8761866092682 and batch: 150, loss is 6.379859418869018 and perplexity is 589.8447807832422
At time: 675.6070547103882 and batch: 200, loss is 6.392732400894165 and perplexity is 597.486925103437
At time: 676.3408751487732 and batch: 250, loss is 6.420072793960571 and perplexity is 614.0478114708607
At time: 677.0754537582397 and batch: 300, loss is 6.439770746231079 and perplexity is 626.2632101516186
At time: 677.8087320327759 and batch: 350, loss is 6.44893557548523 and perplexity is 632.0291872798
At time: 678.5408358573914 and batch: 400, loss is 6.487940378189087 and perplexity is 657.1684499168041
At time: 679.2775368690491 and batch: 450, loss is 6.465272331237793 and perplexity is 642.4392959138329
At time: 680.0081386566162 and batch: 500, loss is 6.452511758804321 and perplexity is 634.2934858767726
At time: 680.7402200698853 and batch: 550, loss is 6.423572835922241 and perplexity is 616.2007701023919
At time: 681.4749979972839 and batch: 600, loss is 6.38420560836792 and perplexity is 592.4139369511009
At time: 682.2384700775146 and batch: 650, loss is 6.419511404037475 and perplexity is 613.7031879603359
At time: 682.9701981544495 and batch: 700, loss is 6.421856460571289 and perplexity is 615.1440454168729
At time: 683.7026510238647 and batch: 750, loss is 6.415014286041259 and perplexity is 610.9494888053528
At time: 684.440060377121 and batch: 800, loss is 6.38821403503418 and perplexity is 594.7933504397469
At time: 685.1718575954437 and batch: 850, loss is 6.371670951843262 and perplexity is 585.0345772194987
At time: 685.9053816795349 and batch: 900, loss is 6.419690914154053 and perplexity is 613.8133637796918
At time: 686.6394181251526 and batch: 950, loss is 6.39967827796936 and perplexity is 601.6514422080322
At time: 687.3884198665619 and batch: 1000, loss is 6.417007732391357 and perplexity is 612.1685985451255
At time: 688.1317358016968 and batch: 1050, loss is 6.414687147140503 and perplexity is 610.7496561494673
At time: 688.8693301677704 and batch: 1100, loss is 6.416142959594726 and perplexity is 611.6394406278147
At time: 689.6038365364075 and batch: 1150, loss is 6.4083458518981935 and perplexity is 606.8889660856232
At time: 690.3528218269348 and batch: 1200, loss is 6.381920700073242 and perplexity is 591.0618706946202
At time: 691.0968070030212 and batch: 1250, loss is 6.394924201965332 and perplexity is 598.7979337961763
At time: 691.8356456756592 and batch: 1300, loss is 6.354581432342529 and perplexity is 575.121562986719
At time: 692.5687339305878 and batch: 1350, loss is 6.362081060409546 and perplexity is 579.4509750002787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976452229817708 and perplexity of 394.03992230723134
Finished 32 epochs...
Completing Train Step...
At time: 695.1777007579803 and batch: 50, loss is 6.413286247253418 and perplexity is 609.8946560497315
At time: 695.9100289344788 and batch: 100, loss is 6.42313946723938 and perplexity is 615.9337858417617
At time: 696.6386337280273 and batch: 150, loss is 6.379525804519654 and perplexity is 589.6480329212549
At time: 697.3669159412384 and batch: 200, loss is 6.3924292469024655 and perplexity is 597.3058220095537
At time: 698.093424320221 and batch: 250, loss is 6.419765338897705 and perplexity is 613.8590483819529
At time: 698.8223855495453 and batch: 300, loss is 6.439457807540894 and perplexity is 626.0672588248974
At time: 699.5533921718597 and batch: 350, loss is 6.4486321926116945 and perplexity is 631.8374695322159
At time: 700.2838037014008 and batch: 400, loss is 6.4876737213134765 and perplexity is 656.9932347933963
At time: 701.0758898258209 and batch: 450, loss is 6.465103921890258 and perplexity is 642.3311122410039
At time: 701.8051564693451 and batch: 500, loss is 6.4523317241668705 and perplexity is 634.1793013578982
At time: 702.5323674678802 and batch: 550, loss is 6.423431177139282 and perplexity is 616.1134860336642
At time: 703.2653214931488 and batch: 600, loss is 6.38411060333252 and perplexity is 592.3576573175158
At time: 703.9966056346893 and batch: 650, loss is 6.419425678253174 and perplexity is 613.65058002818
At time: 704.7325439453125 and batch: 700, loss is 6.421731967926025 and perplexity is 615.0674692741229
At time: 705.4635787010193 and batch: 750, loss is 6.414932813644409 and perplexity is 610.8997153137557
At time: 706.1924138069153 and batch: 800, loss is 6.388186740875244 and perplexity is 594.777116277056
At time: 706.9232833385468 and batch: 850, loss is 6.371651678085327 and perplexity is 585.023301513337
At time: 707.6503751277924 and batch: 900, loss is 6.419707889556885 and perplexity is 613.8237835972457
At time: 708.378007888794 and batch: 950, loss is 6.39969985961914 and perplexity is 601.6644269788636
At time: 709.1080980300903 and batch: 1000, loss is 6.417093114852905 and perplexity is 612.2208692384202
At time: 709.8390054702759 and batch: 1050, loss is 6.414782333374023 and perplexity is 610.8077938757716
At time: 710.5694224834442 and batch: 1100, loss is 6.416233615875244 and perplexity is 611.6948920979942
At time: 711.3004007339478 and batch: 1150, loss is 6.408441791534424 and perplexity is 606.9471935853795
At time: 712.0350766181946 and batch: 1200, loss is 6.382006511688233 and perplexity is 591.1125928445476
At time: 712.7640254497528 and batch: 1250, loss is 6.394983682632446 and perplexity is 598.8335517560247
At time: 713.4915318489075 and batch: 1300, loss is 6.35453122138977 and perplexity is 575.0926863100578
At time: 714.2222473621368 and batch: 1350, loss is 6.361881361007691 and perplexity is 579.3352705406071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976415608723959 and perplexity of 394.02549239851663
Finished 33 epochs...
Completing Train Step...
At time: 716.7881331443787 and batch: 50, loss is 6.412982902526855 and perplexity is 609.7096757798731
At time: 717.5212955474854 and batch: 100, loss is 6.422799348831177 and perplexity is 615.7243310446981
At time: 718.2521183490753 and batch: 150, loss is 6.3791319942474365 and perplexity is 589.4158691861302
At time: 718.9847469329834 and batch: 200, loss is 6.392109785079956 and perplexity is 597.1150360790923
At time: 719.7702832221985 and batch: 250, loss is 6.419460020065308 and perplexity is 613.6716542629766
At time: 720.5045187473297 and batch: 300, loss is 6.439136142730713 and perplexity is 625.8659074044965
At time: 721.2368152141571 and batch: 350, loss is 6.448316764831543 and perplexity is 631.6382018707152
At time: 721.9693758487701 and batch: 400, loss is 6.487447395324707 and perplexity is 656.8445569753783
At time: 722.701375246048 and batch: 450, loss is 6.464947652816773 and perplexity is 642.2307435956848
At time: 723.4334192276001 and batch: 500, loss is 6.452160587310791 and perplexity is 634.0707791924092
At time: 724.1657693386078 and batch: 550, loss is 6.423249931335449 and perplexity is 616.0018281687012
At time: 724.9017496109009 and batch: 600, loss is 6.383989839553833 and perplexity is 592.2861262877499
At time: 725.6354613304138 and batch: 650, loss is 6.419334163665772 and perplexity is 613.5944246180983
At time: 726.368198633194 and batch: 700, loss is 6.421619920730591 and perplexity is 614.998556549999
At time: 727.1023237705231 and batch: 750, loss is 6.414860897064209 and perplexity is 610.8557830751321
At time: 727.8337690830231 and batch: 800, loss is 6.388189992904663 and perplexity is 594.779050512881
At time: 728.5648295879364 and batch: 850, loss is 6.3716489028930665 and perplexity is 585.021677963451
At time: 729.2954330444336 and batch: 900, loss is 6.419729127883911 and perplexity is 613.8368203259366
At time: 730.0237772464752 and batch: 950, loss is 6.399715385437012 and perplexity is 601.6737683836928
At time: 730.7587351799011 and batch: 1000, loss is 6.417174243927002 and perplexity is 612.2705401655353
At time: 731.4923141002655 and batch: 1050, loss is 6.414861469268799 and perplexity is 610.8561326097152
At time: 732.2255055904388 and batch: 1100, loss is 6.416327352523804 and perplexity is 611.7522330145512
At time: 732.9605965614319 and batch: 1150, loss is 6.408540363311768 and perplexity is 607.0070243977715
At time: 733.6940605640411 and batch: 1200, loss is 6.382087059020996 and perplexity is 591.1602073048474
At time: 734.4257965087891 and batch: 1250, loss is 6.395052671432495 and perplexity is 598.8748659892825
At time: 735.159654378891 and batch: 1300, loss is 6.354527854919434 and perplexity is 575.0907502808476
At time: 735.8930556774139 and batch: 1350, loss is 6.361741027832031 and perplexity is 579.2539762865945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.97637939453125 and perplexity of 394.0112233417756
Finished 34 epochs...
Completing Train Step...
At time: 738.4331104755402 and batch: 50, loss is 6.412753477096557 and perplexity is 609.5698089202712
At time: 739.1945841312408 and batch: 100, loss is 6.422539005279541 and perplexity is 615.5640520502319
At time: 739.924699306488 and batch: 150, loss is 6.3788134765625 and perplexity is 589.2281597039938
At time: 740.6553151607513 and batch: 200, loss is 6.391851902008057 and perplexity is 596.9610700727774
At time: 741.3868927955627 and batch: 250, loss is 6.419198760986328 and perplexity is 613.5113479134444
At time: 742.1174314022064 and batch: 300, loss is 6.438885746002197 and perplexity is 625.7092122475888
At time: 742.8499743938446 and batch: 350, loss is 6.448067255020142 and perplexity is 631.4806216018217
At time: 743.5827198028564 and batch: 400, loss is 6.4872913646698 and perplexity is 656.7420770841918
At time: 744.3122098445892 and batch: 450, loss is 6.464796915054321 and perplexity is 642.1339424663936
At time: 745.0436971187592 and batch: 500, loss is 6.45200098991394 and perplexity is 633.969591221514
At time: 745.7757785320282 and batch: 550, loss is 6.42310564994812 and perplexity is 615.9129569817189
At time: 746.5083928108215 and batch: 600, loss is 6.383907575607299 and perplexity is 592.2374044975751
At time: 747.2384510040283 and batch: 650, loss is 6.419261484146118 and perplexity is 613.5498304906147
At time: 747.9709458351135 and batch: 700, loss is 6.421558618545532 and perplexity is 614.9608569502201
At time: 748.7010281085968 and batch: 750, loss is 6.414833869934082 and perplexity is 610.8392736194968
At time: 749.4321627616882 and batch: 800, loss is 6.388184366226196 and perplexity is 594.77570389182
At time: 750.1664769649506 and batch: 850, loss is 6.371667413711548 and perplexity is 585.0325072937692
At time: 750.9012610912323 and batch: 900, loss is 6.419757871627808 and perplexity is 613.8544645478739
At time: 751.6345956325531 and batch: 950, loss is 6.399731035232544 and perplexity is 601.6831845288254
At time: 752.367701292038 and batch: 1000, loss is 6.417248783111572 and perplexity is 612.3161800132925
At time: 753.1009652614594 and batch: 1050, loss is 6.414891138076782 and perplexity is 610.8742562518709
At time: 753.8347001075745 and batch: 1100, loss is 6.4164193248748775 and perplexity is 611.808499893155
At time: 754.5681626796722 and batch: 1150, loss is 6.408633050918579 and perplexity is 607.0632890336574
At time: 755.2989735603333 and batch: 1200, loss is 6.382162733078003 and perplexity is 591.2049444887757
At time: 756.0295596122742 and batch: 1250, loss is 6.395117111206055 and perplexity is 598.9134585934735
At time: 756.7610538005829 and batch: 1300, loss is 6.354528388977051 and perplexity is 575.0910574125252
At time: 757.4927864074707 and batch: 1350, loss is 6.361611366271973 and perplexity is 579.1788741813923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976343994140625 and perplexity of 393.9972754374407
Finished 35 epochs...
Completing Train Step...
At time: 760.035769701004 and batch: 50, loss is 6.4125414562225345 and perplexity is 609.4405810965881
At time: 760.7985696792603 and batch: 100, loss is 6.422295665740966 and perplexity is 615.4142792014101
At time: 761.5297751426697 and batch: 150, loss is 6.378534488677978 and perplexity is 589.0637951151481
At time: 762.2629344463348 and batch: 200, loss is 6.391617546081543 and perplexity is 596.8211851001842
At time: 762.9951095581055 and batch: 250, loss is 6.418961610794067 and perplexity is 613.3658708299736
At time: 763.7340552806854 and batch: 300, loss is 6.438658924102783 and perplexity is 625.5673037901672
At time: 764.4678990840912 and batch: 350, loss is 6.447849388122559 and perplexity is 631.3430578637459
At time: 765.2009930610657 and batch: 400, loss is 6.48713791847229 and perplexity is 656.6413102410604
At time: 765.9382140636444 and batch: 450, loss is 6.464637861251831 and perplexity is 642.0318167430943
At time: 766.6741161346436 and batch: 500, loss is 6.451846055984497 and perplexity is 633.871375430274
At time: 767.4063906669617 and batch: 550, loss is 6.422975358963012 and perplexity is 615.8327143033756
At time: 768.1430461406708 and batch: 600, loss is 6.383842926025391 and perplexity is 592.1991178346055
At time: 768.8792679309845 and batch: 650, loss is 6.419194421768188 and perplexity is 613.5086857596506
At time: 769.6097793579102 and batch: 700, loss is 6.421506233215332 and perplexity is 614.9286428664489
At time: 770.3440911769867 and batch: 750, loss is 6.414806108474732 and perplexity is 610.8223160652171
At time: 771.0782861709595 and batch: 800, loss is 6.388056402206421 and perplexity is 594.6995988713414
At time: 771.8155519962311 and batch: 850, loss is 6.371681613922119 and perplexity is 585.0408149375486
At time: 772.5484948158264 and batch: 900, loss is 6.419782838821411 and perplexity is 613.8697909624625
At time: 773.2799997329712 and batch: 950, loss is 6.399733543395996 and perplexity is 601.6846936504911
At time: 774.0160853862762 and batch: 1000, loss is 6.417314214706421 and perplexity is 612.356246148284
At time: 774.7475135326385 and batch: 1050, loss is 6.414910507202149 and perplexity is 610.886088466513
At time: 775.4791052341461 and batch: 1100, loss is 6.41649489402771 and perplexity is 611.854735490158
At time: 776.2717916965485 and batch: 1150, loss is 6.408711700439453 and perplexity is 607.1110361480997
At time: 776.9998881816864 and batch: 1200, loss is 6.382230825424195 and perplexity is 591.245202391138
At time: 777.7442717552185 and batch: 1250, loss is 6.3951719284057615 and perplexity is 598.9462902520017
At time: 778.4924194812775 and batch: 1300, loss is 6.354526109695435 and perplexity is 575.0897466195444
At time: 779.2436249256134 and batch: 1350, loss is 6.3614790153503415 and perplexity is 579.1022243960518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.9763134765625 and perplexity of 393.9852517782739
Finished 36 epochs...
Completing Train Step...
At time: 781.9330024719238 and batch: 50, loss is 6.4123299694061275 and perplexity is 609.311706076467
At time: 782.6806819438934 and batch: 100, loss is 6.422050619125367 and perplexity is 615.263492490741
At time: 783.4126660823822 and batch: 150, loss is 6.37827425956726 and perplexity is 588.9105235113207
At time: 784.1503932476044 and batch: 200, loss is 6.391390953063965 and perplexity is 596.6859649074523
At time: 784.8832635879517 and batch: 250, loss is 6.418732614517212 and perplexity is 613.2254284102125
At time: 785.6205439567566 and batch: 300, loss is 6.438430490493775 and perplexity is 625.4244195136918
At time: 786.3546769618988 and batch: 350, loss is 6.44763840675354 and perplexity is 631.2098702916194
At time: 787.0876359939575 and batch: 400, loss is 6.486980648040771 and perplexity is 656.5380480990995
At time: 787.8203086853027 and batch: 450, loss is 6.464461212158203 and perplexity is 641.9184124212662
At time: 788.5544786453247 and batch: 500, loss is 6.451703157424927 and perplexity is 633.7808025952903
At time: 789.2893736362457 and batch: 550, loss is 6.422818489074707 and perplexity is 615.7361162711277
At time: 790.0284905433655 and batch: 600, loss is 6.383792123794556 and perplexity is 592.1690335625013
At time: 790.7647783756256 and batch: 650, loss is 6.419137868881226 and perplexity is 613.4739910533464
At time: 791.5007684230804 and batch: 700, loss is 6.4214715480804445 and perplexity is 614.907314353418
At time: 792.235543012619 and batch: 750, loss is 6.414783821105957 and perplexity is 610.808702594708
At time: 792.9745907783508 and batch: 800, loss is 6.3878586387634275 and perplexity is 594.5820006598188
At time: 793.7069594860077 and batch: 850, loss is 6.371695289611816 and perplexity is 585.0488158289025
At time: 794.4394130706787 and batch: 900, loss is 6.419800281524658 and perplexity is 613.8804986044435
At time: 795.2279739379883 and batch: 950, loss is 6.399747924804688 and perplexity is 601.6933467861959
At time: 795.9617853164673 and batch: 1000, loss is 6.417364253997802 and perplexity is 612.3868887875758
At time: 796.6941664218903 and batch: 1050, loss is 6.414974842071533 and perplexity is 610.9253910074717
At time: 797.4271175861359 and batch: 1100, loss is 6.4165659999847415 and perplexity is 611.8982435535123
At time: 798.159029006958 and batch: 1150, loss is 6.408780689239502 and perplexity is 607.1529214547718
At time: 798.8927817344666 and batch: 1200, loss is 6.3822902488708495 and perplexity is 591.2803372627895
At time: 799.625152349472 and batch: 1250, loss is 6.395215530395507 and perplexity is 598.9724060713544
At time: 800.3587107658386 and batch: 1300, loss is 6.354520645141601 and perplexity is 575.0866040192514
At time: 801.0912883281708 and batch: 1350, loss is 6.361356239318848 and perplexity is 579.031128887613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976275634765625 and perplexity of 393.9703429504947
Finished 37 epochs...
Completing Train Step...
At time: 803.6557493209839 and batch: 50, loss is 6.412109394073486 and perplexity is 609.1773217656936
At time: 804.3875269889832 and batch: 100, loss is 6.4217892932891845 and perplexity is 615.1027292508024
At time: 805.1197085380554 and batch: 150, loss is 6.377991008758545 and perplexity is 588.7437377514908
At time: 805.8546347618103 and batch: 200, loss is 6.391148872375489 and perplexity is 596.5415362406642
At time: 806.5884337425232 and batch: 250, loss is 6.4185120964050295 and perplexity is 613.0902160053375
At time: 807.3236179351807 and batch: 300, loss is 6.438202180862427 and perplexity is 625.2816453940101
At time: 808.0556836128235 and batch: 350, loss is 6.447431163787842 and perplexity is 631.0790700403065
At time: 808.7880127429962 and batch: 400, loss is 6.486819953918457 and perplexity is 656.4325547700001
At time: 809.5206162929535 and batch: 450, loss is 6.4642934989929195 and perplexity is 641.8107632798068
At time: 810.255551815033 and batch: 500, loss is 6.451567573547363 and perplexity is 633.6948779616783
At time: 810.986763715744 and batch: 550, loss is 6.422705469131469 and perplexity is 615.666529742624
At time: 811.7210922241211 and batch: 600, loss is 6.3837346172332765 and perplexity is 592.1349809368193
At time: 812.4565694332123 and batch: 650, loss is 6.419060440063476 and perplexity is 613.4264923264079
At time: 813.1944451332092 and batch: 700, loss is 6.421418924331665 and perplexity is 614.8749564767884
At time: 813.9293518066406 and batch: 750, loss is 6.414732217788696 and perplexity is 610.7771836526903
At time: 814.7189135551453 and batch: 800, loss is 6.387772064208985 and perplexity is 594.5305272162094
At time: 815.4530227184296 and batch: 850, loss is 6.37168683052063 and perplexity is 585.0438668685528
At time: 816.1860287189484 and batch: 900, loss is 6.419799108505249 and perplexity is 613.8797785111258
At time: 816.9200944900513 and batch: 950, loss is 6.3997530746459965 and perplexity is 601.6964454194273
At time: 817.6556301116943 and batch: 1000, loss is 6.417398042678833 and perplexity is 612.4075808824059
At time: 818.387305021286 and batch: 1050, loss is 6.4150211620330815 and perplexity is 610.9536897034842
At time: 819.1212480068207 and batch: 1100, loss is 6.416629791259766 and perplexity is 611.937278567687
At time: 819.8520278930664 and batch: 1150, loss is 6.408831329345703 and perplexity is 607.1836685217052
At time: 820.5849828720093 and batch: 1200, loss is 6.382334394454956 and perplexity is 591.3064402548104
At time: 821.3153281211853 and batch: 1250, loss is 6.395235195159912 and perplexity is 598.9841848384174
At time: 822.047812461853 and batch: 1300, loss is 6.354508266448975 and perplexity is 575.0794852430073
At time: 822.778064250946 and batch: 1350, loss is 6.361241188049316 and perplexity is 578.9645144532471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976233723958333 and perplexity of 393.95383168137516
Finished 38 epochs...
Completing Train Step...
At time: 825.3170433044434 and batch: 50, loss is 6.41189266204834 and perplexity is 609.0453078374139
At time: 826.0766866207123 and batch: 100, loss is 6.421531610488891 and perplexity is 614.9442482768487
At time: 826.8087294101715 and batch: 150, loss is 6.377733430862427 and perplexity is 588.5921099069969
At time: 827.5421941280365 and batch: 200, loss is 6.3909282302856445 and perplexity is 596.4099285890568
At time: 828.2756416797638 and batch: 250, loss is 6.418299264907837 and perplexity is 612.9597449814153
At time: 829.0127160549164 and batch: 300, loss is 6.438002481460571 and perplexity is 625.156789490673
At time: 829.7470772266388 and batch: 350, loss is 6.447232370376587 and perplexity is 630.9536281481255
At time: 830.4837779998779 and batch: 400, loss is 6.486665105819702 and perplexity is 656.330915306475
At time: 831.2227079868317 and batch: 450, loss is 6.464126310348511 and perplexity is 641.7034687777881
At time: 831.9584956169128 and batch: 500, loss is 6.451442918777466 and perplexity is 633.6158897957089
At time: 832.6928052902222 and batch: 550, loss is 6.422599277496338 and perplexity is 615.6011545783441
At time: 833.4836847782135 and batch: 600, loss is 6.383683376312256 and perplexity is 592.1046401723783
At time: 834.2190387248993 and batch: 650, loss is 6.4189869403839115 and perplexity is 613.381407332672
At time: 834.9536519050598 and batch: 700, loss is 6.421375303268433 and perplexity is 614.8481355624149
At time: 835.6857523918152 and batch: 750, loss is 6.41468542098999 and perplexity is 610.7486019045448
At time: 836.4194734096527 and batch: 800, loss is 6.387710237503052 and perplexity is 594.4937704884206
At time: 837.1527695655823 and batch: 850, loss is 6.371683492660522 and perplexity is 585.0419140772275
At time: 837.886082649231 and batch: 900, loss is 6.419794502258301 and perplexity is 613.8769508357824
At time: 838.6187906265259 and batch: 950, loss is 6.399754600524902 and perplexity is 601.6973635360415
At time: 839.3534762859344 and batch: 1000, loss is 6.4174348258972165 and perplexity is 612.4301076184934
At time: 840.0912997722626 and batch: 1050, loss is 6.415068912506103 and perplexity is 610.9828637276931
At time: 840.8303329944611 and batch: 1100, loss is 6.416695079803467 and perplexity is 611.9772323656896
At time: 841.5649101734161 and batch: 1150, loss is 6.408886699676514 and perplexity is 607.2172894130856
At time: 842.2984964847565 and batch: 1200, loss is 6.38237813949585 and perplexity is 591.3323075449987
At time: 843.0297088623047 and batch: 1250, loss is 6.39524393081665 and perplexity is 598.9894173815028
At time: 843.7601983547211 and batch: 1300, loss is 6.354501476287842 and perplexity is 575.0755803738956
At time: 844.4927518367767 and batch: 1350, loss is 6.361140737533569 and perplexity is 578.9063600900378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976205240885417 and perplexity of 393.9426108254647
Finished 39 epochs...
Completing Train Step...
At time: 847.0288355350494 and batch: 50, loss is 6.411704816818237 and perplexity is 608.9309123260824
At time: 847.8119478225708 and batch: 100, loss is 6.421309251785278 and perplexity is 614.8075252723456
At time: 848.5442926883698 and batch: 150, loss is 6.377520141601562 and perplexity is 588.4665829181794
At time: 849.2772705554962 and batch: 200, loss is 6.390715579986573 and perplexity is 596.2831153232903
At time: 850.004501581192 and batch: 250, loss is 6.4180926322937015 and perplexity is 612.8331005918311
At time: 850.7336974143982 and batch: 300, loss is 6.437820472717285 and perplexity is 625.0430158432746
At time: 851.4672269821167 and batch: 350, loss is 6.447048177719116 and perplexity is 630.8374218251206
At time: 852.1967380046844 and batch: 400, loss is 6.486520900726318 and perplexity is 656.2362758694511
At time: 852.9830861091614 and batch: 450, loss is 6.463963651657105 and perplexity is 641.599098619873
At time: 853.7173819541931 and batch: 500, loss is 6.451322784423828 and perplexity is 633.5397753324064
At time: 854.4503901004791 and batch: 550, loss is 6.422499074935913 and perplexity is 615.5394728568396
At time: 855.1776640415192 and batch: 600, loss is 6.3836359119415285 and perplexity is 592.0765369651837
At time: 855.9116237163544 and batch: 650, loss is 6.418923473358154 and perplexity is 613.342479074437
At time: 856.663421869278 and batch: 700, loss is 6.421335372924805 and perplexity is 614.8235849552432
At time: 857.4108157157898 and batch: 750, loss is 6.414642171859741 and perplexity is 610.7221881299023
At time: 858.1691398620605 and batch: 800, loss is 6.387676095962524 and perplexity is 594.4734739017422
At time: 858.9352433681488 and batch: 850, loss is 6.3716918563842775 and perplexity is 585.0468072266445
At time: 859.6712584495544 and batch: 900, loss is 6.4197981071472165 and perplexity is 613.8791637979866
At time: 860.4032008647919 and batch: 950, loss is 6.399750881195068 and perplexity is 601.6951256292479
At time: 861.1311883926392 and batch: 1000, loss is 6.4174713611602785 and perplexity is 612.452483322331
At time: 861.8638961315155 and batch: 1050, loss is 6.415115785598755 and perplexity is 611.0115030552749
At time: 862.593362569809 and batch: 1100, loss is 6.4167529296875 and perplexity is 612.012636201657
At time: 863.3230946063995 and batch: 1150, loss is 6.408942823410034 and perplexity is 607.2513696707722
At time: 864.0512022972107 and batch: 1200, loss is 6.38241961479187 and perplexity is 591.3568337361128
At time: 864.7843654155731 and batch: 1250, loss is 6.395250959396362 and perplexity is 598.9936274411648
At time: 865.5117273330688 and batch: 1300, loss is 6.354493770599365 and perplexity is 575.071149037696
At time: 866.2392036914825 and batch: 1350, loss is 6.3610459899902345 and perplexity is 578.8515127329657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976187744140625 and perplexity of 393.9357181724402
Finished 40 epochs...
Completing Train Step...
At time: 868.830703496933 and batch: 50, loss is 6.411526002883911 and perplexity is 608.8220367284437
At time: 869.5622837543488 and batch: 100, loss is 6.421099433898926 and perplexity is 614.6785411889366
At time: 870.2960364818573 and batch: 150, loss is 6.377320041656494 and perplexity is 588.3488425675749
At time: 871.0281596183777 and batch: 200, loss is 6.390511722564697 and perplexity is 596.1615709739723
At time: 871.8163814544678 and batch: 250, loss is 6.417895364761352 and perplexity is 612.712220441592
At time: 872.5517339706421 and batch: 300, loss is 6.43765043258667 and perplexity is 624.9367424828447
At time: 873.2876555919647 and batch: 350, loss is 6.446871452331543 and perplexity is 630.7259466877861
At time: 874.0237145423889 and batch: 400, loss is 6.486384840011596 and perplexity is 656.1469939667476
At time: 874.7567520141602 and batch: 450, loss is 6.463809022903442 and perplexity is 641.4998966208407
At time: 875.4923799037933 and batch: 500, loss is 6.451207571029663 and perplexity is 633.4667872692335
At time: 876.2268033027649 and batch: 550, loss is 6.422406597137451 and perplexity is 615.4825517535332
At time: 876.9575004577637 and batch: 600, loss is 6.383591775894165 and perplexity is 592.0504056237767
At time: 877.6959838867188 and batch: 650, loss is 6.418866586685181 and perplexity is 613.3075890538074
At time: 878.430332660675 and batch: 700, loss is 6.4212986087799075 and perplexity is 614.8009819073731
At time: 879.1640486717224 and batch: 750, loss is 6.414598436355591 and perplexity is 610.6954784711929
At time: 879.8973233699799 and batch: 800, loss is 6.387651996612549 and perplexity is 594.459147650071
At time: 880.6297569274902 and batch: 850, loss is 6.371701755523682 and perplexity is 585.0525987152126
At time: 881.3630123138428 and batch: 900, loss is 6.419798965454102 and perplexity is 613.8796906949257
At time: 882.0946464538574 and batch: 950, loss is 6.399745693206787 and perplexity is 601.6920040500848
At time: 882.8251240253448 and batch: 1000, loss is 6.417503347396851 and perplexity is 612.4720736856611
At time: 883.55615401268 and batch: 1050, loss is 6.415158710479736 and perplexity is 611.0377312142381
At time: 884.2893838882446 and batch: 1100, loss is 6.416800918579102 and perplexity is 612.0420067144381
At time: 885.0204527378082 and batch: 1150, loss is 6.408993167877197 and perplexity is 607.2819421869845
At time: 885.7574491500854 and batch: 1200, loss is 6.3824512386322025 and perplexity is 591.3755350059035
At time: 886.493492603302 and batch: 1250, loss is 6.395249652862549 and perplexity is 598.9928448362477
At time: 887.2263157367706 and batch: 1300, loss is 6.354467916488647 and perplexity is 575.0562812767351
At time: 887.959780216217 and batch: 1350, loss is 6.360943145751953 and perplexity is 578.791984251194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976170247395833 and perplexity of 393.92882564001366
Finished 41 epochs...
Completing Train Step...
At time: 890.5239217281342 and batch: 50, loss is 6.4113530254364015 and perplexity is 608.7167333543598
At time: 891.2543587684631 and batch: 100, loss is 6.4208972454071045 and perplexity is 614.5542728249787
At time: 891.9838809967041 and batch: 150, loss is 6.377128343582154 and perplexity is 588.2360680370897
At time: 892.7151644229889 and batch: 200, loss is 6.3903120994567875 and perplexity is 596.0425752259039
At time: 893.4465107917786 and batch: 250, loss is 6.417709035873413 and perplexity is 612.5980650904803
At time: 894.1920108795166 and batch: 300, loss is 6.43748685836792 and perplexity is 624.8345273035366
At time: 894.9272627830505 and batch: 350, loss is 6.446695327758789 and perplexity is 630.6148701318438
At time: 895.6609079837799 and batch: 400, loss is 6.486250152587891 and perplexity is 656.0586251697744
At time: 896.3944890499115 and batch: 450, loss is 6.463662900924683 and perplexity is 641.4061662347739
At time: 897.127765417099 and batch: 500, loss is 6.451098527908325 and perplexity is 633.3977158394364
At time: 897.8582441806793 and batch: 550, loss is 6.422321996688843 and perplexity is 615.430483856059
At time: 898.5890009403229 and batch: 600, loss is 6.383547506332397 and perplexity is 592.0241963919153
At time: 899.3284192085266 and batch: 650, loss is 6.418811960220337 and perplexity is 613.2740871434091
At time: 900.0616896152496 and batch: 700, loss is 6.421262102127075 and perplexity is 614.7785379910442
At time: 900.7949833869934 and batch: 750, loss is 6.414549207687378 and perplexity is 610.6654154860905
At time: 901.5295519828796 and batch: 800, loss is 6.387627439498901 and perplexity is 594.4445496284667
At time: 902.2646787166595 and batch: 850, loss is 6.371711177825928 and perplexity is 585.058111283598
At time: 902.9962637424469 and batch: 900, loss is 6.419795846939087 and perplexity is 613.8777763048778
At time: 903.7290198802948 and batch: 950, loss is 6.399740839004517 and perplexity is 601.6890833224817
At time: 904.4631462097168 and batch: 1000, loss is 6.4175292491912845 and perplexity is 612.4879380168666
At time: 905.196364402771 and batch: 1050, loss is 6.415197439193726 and perplexity is 611.0613963780248
At time: 905.9304585456848 and batch: 1100, loss is 6.416819877624512 and perplexity is 612.0536105566349
At time: 906.6633422374725 and batch: 1150, loss is 6.409019403457641 and perplexity is 607.2978747902306
At time: 907.394425868988 and batch: 1200, loss is 6.382425107955933 and perplexity is 591.3600821651419
At time: 908.1256356239319 and batch: 1250, loss is 6.395191583633423 and perplexity is 598.9580627933892
At time: 908.8585119247437 and batch: 1300, loss is 6.354326152801514 and perplexity is 574.9747649561558
At time: 909.5890848636627 and batch: 1350, loss is 6.3607642936706545 and perplexity is 578.688475356838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976151936848958 and perplexity of 393.92161265382333
Finished 42 epochs...
Completing Train Step...
At time: 912.3387336730957 and batch: 50, loss is 6.411171236038208 and perplexity is 608.6060851633717
At time: 913.0735313892365 and batch: 100, loss is 6.4206956768035885 and perplexity is 614.4304104622217
At time: 913.8086307048798 and batch: 150, loss is 6.376939907073974 and perplexity is 588.1252333294242
At time: 914.5441513061523 and batch: 200, loss is 6.390107660293579 and perplexity is 595.9207332356499
At time: 915.2758460044861 and batch: 250, loss is 6.417529287338257 and perplexity is 612.4879613814277
At time: 916.0103662014008 and batch: 300, loss is 6.437313508987427 and perplexity is 624.7262220129168
At time: 916.7452692985535 and batch: 350, loss is 6.446508131027222 and perplexity is 630.4968321377853
At time: 917.4816806316376 and batch: 400, loss is 6.486100597381592 and perplexity is 655.9605155233298
At time: 918.244845867157 and batch: 450, loss is 6.463518514633178 and perplexity is 641.3135626625873
At time: 918.9787752628326 and batch: 500, loss is 6.450989542007446 and perplexity is 633.32868818035
At time: 919.7127330303192 and batch: 550, loss is 6.422237825393677 and perplexity is 615.3786844551903
At time: 920.4445843696594 and batch: 600, loss is 6.38349687576294 and perplexity is 591.9942226285199
At time: 921.1803681850433 and batch: 650, loss is 6.418754892349243 and perplexity is 613.2390898954777
At time: 921.9131889343262 and batch: 700, loss is 6.421215257644653 and perplexity is 614.7497396831541
At time: 922.6461071968079 and batch: 750, loss is 6.414477605819702 and perplexity is 610.6216922671674
At time: 923.3803052902222 and batch: 800, loss is 6.387597064971924 and perplexity is 594.4264939306756
At time: 924.1133451461792 and batch: 850, loss is 6.371715822219849 and perplexity is 585.0608285302435
At time: 924.8466658592224 and batch: 900, loss is 6.419787645339966 and perplexity is 613.872741546094
At time: 925.5776450634003 and batch: 950, loss is 6.399735822677612 and perplexity is 601.6860650609149
At time: 926.3112947940826 and batch: 1000, loss is 6.417550868988037 and perplexity is 612.5011800247444
At time: 927.0498967170715 and batch: 1050, loss is 6.415236787796021 and perplexity is 611.0854412629518
At time: 927.7894148826599 and batch: 1100, loss is 6.416758012771607 and perplexity is 612.0157471212676
At time: 928.5303938388824 and batch: 1150, loss is 6.408995275497436 and perplexity is 607.2832221080455
At time: 929.2693502902985 and batch: 1200, loss is 6.382279644012451 and perplexity is 591.2740668517874
At time: 930.0084543228149 and batch: 1250, loss is 6.39509051322937 and perplexity is 598.897528919115
At time: 930.7425765991211 and batch: 1300, loss is 6.3541473960876464 and perplexity is 574.8719935424293
At time: 931.4750061035156 and batch: 1350, loss is 6.360634546279908 and perplexity is 578.6133969078269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976146647135416 and perplexity of 393.9195289268456
Finished 43 epochs...
Completing Train Step...
At time: 934.0592577457428 and batch: 50, loss is 6.411010675430298 and perplexity is 608.5083748447839
At time: 934.7979421615601 and batch: 100, loss is 6.420502233505249 and perplexity is 614.3115645123686
At time: 935.5311601161957 and batch: 150, loss is 6.376759214401245 and perplexity is 588.0189730096341
At time: 936.2671139240265 and batch: 200, loss is 6.389932098388672 and perplexity is 595.8161214397412
At time: 937.0046870708466 and batch: 250, loss is 6.417361154556274 and perplexity is 612.3849907331628
At time: 937.7380423545837 and batch: 300, loss is 6.43715991973877 and perplexity is 624.630278150023
At time: 938.475839138031 and batch: 350, loss is 6.446341848373413 and perplexity is 630.39200016743
At time: 939.2079410552979 and batch: 400, loss is 6.485979986190796 and perplexity is 655.8814041153942
At time: 939.9403908252716 and batch: 450, loss is 6.46339602470398 and perplexity is 641.2350130205709
At time: 940.673056602478 and batch: 500, loss is 6.450889167785644 and perplexity is 633.2651214964108
At time: 941.4046466350555 and batch: 550, loss is 6.422160911560058 and perplexity is 615.3313551416045
At time: 942.1932806968689 and batch: 600, loss is 6.383453569412231 and perplexity is 591.9685860742144
At time: 942.9279806613922 and batch: 650, loss is 6.4187043762207034 and perplexity is 613.2081122132298
At time: 943.6612992286682 and batch: 700, loss is 6.421177749633789 and perplexity is 614.7266820756644
At time: 944.3942673206329 and batch: 750, loss is 6.41444128036499 and perplexity is 610.5995115594035
At time: 945.1282639503479 and batch: 800, loss is 6.387589731216431 and perplexity is 594.4221345680955
At time: 945.8633589744568 and batch: 850, loss is 6.371720457077027 and perplexity is 585.0635402099083
At time: 946.5958309173584 and batch: 900, loss is 6.419781093597412 and perplexity is 613.8687196231059
At time: 947.3298954963684 and batch: 950, loss is 6.399731206893921 and perplexity is 601.6832878145981
At time: 948.0621836185455 and batch: 1000, loss is 6.417568368911743 and perplexity is 612.5118988424534
At time: 948.8000254631042 and batch: 1050, loss is 6.415267963409423 and perplexity is 611.1044925233905
At time: 949.5358486175537 and batch: 1100, loss is 6.416787052154541 and perplexity is 612.0335199389649
At time: 950.2672827243805 and batch: 1150, loss is 6.409010362625122 and perplexity is 607.2923843366746
At time: 951.0037188529968 and batch: 1200, loss is 6.382258472442627 and perplexity is 591.2615487841098
At time: 951.7361664772034 and batch: 1250, loss is 6.39504903793335 and perplexity is 598.8726899819221
At time: 952.4699604511261 and batch: 1300, loss is 6.354094219207764 and perplexity is 574.8414244562723
At time: 953.2061257362366 and batch: 1350, loss is 6.360543155670166 and perplexity is 578.5605194929655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976141357421875 and perplexity of 393.91744521089055
Finished 44 epochs...
Completing Train Step...
At time: 955.7804379463196 and batch: 50, loss is 6.410859861373901 and perplexity is 608.4166101482995
At time: 956.5457262992859 and batch: 100, loss is 6.4203173828125 and perplexity is 614.1980190889032
At time: 957.2791519165039 and batch: 150, loss is 6.376583614349365 and perplexity is 587.9157259128305
At time: 958.0117247104645 and batch: 200, loss is 6.389767570495605 and perplexity is 595.7181011323839
At time: 958.7421967983246 and batch: 250, loss is 6.417200765609741 and perplexity is 612.2867788258892
At time: 959.4745297431946 and batch: 300, loss is 6.437011375427246 and perplexity is 624.5374997664192
At time: 960.2084169387817 and batch: 350, loss is 6.446185712814331 and perplexity is 630.2935812435899
At time: 961.0010335445404 and batch: 400, loss is 6.485870990753174 and perplexity is 655.8099199305201
At time: 961.7333817481995 and batch: 450, loss is 6.463280439376831 and perplexity is 641.1608999450863
At time: 962.4655029773712 and batch: 500, loss is 6.4507965564727785 and perplexity is 633.2064766977447
At time: 963.1997995376587 and batch: 550, loss is 6.422089405059815 and perplexity is 615.2873565230205
At time: 963.9293773174286 and batch: 600, loss is 6.383413600921631 and perplexity is 591.9449264561687
At time: 964.6637074947357 and batch: 650, loss is 6.418663148880005 and perplexity is 613.1828317945942
At time: 965.3956382274628 and batch: 700, loss is 6.421141815185547 and perplexity is 614.7045926084132
At time: 966.1256153583527 and batch: 750, loss is 6.414413175582886 and perplexity is 610.5823510343257
At time: 966.8609435558319 and batch: 800, loss is 6.387590341567993 and perplexity is 594.4224973746847
At time: 967.5929386615753 and batch: 850, loss is 6.371723728179932 and perplexity is 585.0654540160845
At time: 968.3326785564423 and batch: 900, loss is 6.419771127700805 and perplexity is 613.8626019014005
At time: 969.0665898323059 and batch: 950, loss is 6.3997206878662105 and perplexity is 601.6769587247088
At time: 969.8065700531006 and batch: 1000, loss is 6.417577333450318 and perplexity is 612.5173897536097
At time: 970.5419809818268 and batch: 1050, loss is 6.415289726257324 and perplexity is 611.1177920422299
At time: 971.2746212482452 and batch: 1100, loss is 6.416824903488159 and perplexity is 612.0566866623562
At time: 972.0084502696991 and batch: 1150, loss is 6.4090176200866695 and perplexity is 607.2967917537951
At time: 972.7400085926056 and batch: 1200, loss is 6.382266035079956 and perplexity is 591.2660202976784
At time: 973.4723381996155 and batch: 1250, loss is 6.395007181167602 and perplexity is 598.8476236326268
At time: 974.203831911087 and batch: 1300, loss is 6.354050779342652 and perplexity is 574.8164539646943
At time: 974.9381401538849 and batch: 1350, loss is 6.360457792282104 and perplexity is 578.5111337147212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976134033203125 and perplexity of 393.91456008391776
Finished 45 epochs...
Completing Train Step...
At time: 977.4969191551208 and batch: 50, loss is 6.410711832046509 and perplexity is 608.3265533124157
At time: 978.2578461170197 and batch: 100, loss is 6.420134201049804 and perplexity is 614.0855195173718
At time: 978.9932606220245 and batch: 150, loss is 6.376411046981811 and perplexity is 587.8142795970797
At time: 979.7879557609558 and batch: 200, loss is 6.389609680175782 and perplexity is 595.6240504359139
At time: 980.5233428478241 and batch: 250, loss is 6.417046432495117 and perplexity is 612.1922899918354
At time: 981.2584927082062 and batch: 300, loss is 6.436866216659546 and perplexity is 624.4468492520882
At time: 981.9898965358734 and batch: 350, loss is 6.446032886505127 and perplexity is 630.1972631620093
At time: 982.7213356494904 and batch: 400, loss is 6.485766363143921 and perplexity is 655.7413076959031
At time: 983.4540431499481 and batch: 450, loss is 6.463169937133789 and perplexity is 641.0900541418737
At time: 984.1873347759247 and batch: 500, loss is 6.450710391998291 and perplexity is 633.151919144933
At time: 984.9212906360626 and batch: 550, loss is 6.422019548416138 and perplexity is 615.244376114648
At time: 985.6589815616608 and batch: 600, loss is 6.383373203277588 and perplexity is 591.9210137587486
At time: 986.392086982727 and batch: 650, loss is 6.418625316619873 and perplexity is 613.159634141006
At time: 987.1236093044281 and batch: 700, loss is 6.421104478836059 and perplexity is 614.6816422113567
At time: 987.8550810813904 and batch: 750, loss is 6.414385223388672 and perplexity is 610.5652841563957
At time: 988.5867702960968 and batch: 800, loss is 6.387593441009521 and perplexity is 594.4243397553136
At time: 989.3186564445496 and batch: 850, loss is 6.371724920272827 and perplexity is 585.0661514688712
At time: 990.0519049167633 and batch: 900, loss is 6.419760599136352 and perplexity is 613.8561388434543
At time: 990.784215927124 and batch: 950, loss is 6.399708662033081 and perplexity is 601.6697231015127
At time: 991.5161831378937 and batch: 1000, loss is 6.4175846385955815 and perplexity is 612.521864298462
At time: 992.2490301132202 and batch: 1050, loss is 6.415310106277466 and perplexity is 611.130246762054
At time: 992.9824540615082 and batch: 1100, loss is 6.416864385604859 and perplexity is 612.0808524329406
At time: 993.7196130752563 and batch: 1150, loss is 6.409018564224243 and perplexity is 607.2973651257852
At time: 994.453063249588 and batch: 1200, loss is 6.382275791168213 and perplexity is 591.2717887692943
At time: 995.1935396194458 and batch: 1250, loss is 6.394967832565308 and perplexity is 598.8240602792448
At time: 995.9270470142365 and batch: 1300, loss is 6.354015188217163 and perplexity is 574.7959959642122
At time: 996.665961265564 and batch: 1350, loss is 6.360376358032227 and perplexity is 578.4640250126582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976126302083333 and perplexity of 393.9115146950383
Finished 46 epochs...
Completing Train Step...
At time: 999.2337830066681 and batch: 50, loss is 6.410569410324097 and perplexity is 608.239920566243
At time: 999.9622209072113 and batch: 100, loss is 6.419955358505249 and perplexity is 613.9757047205585
At time: 1000.6909852027893 and batch: 150, loss is 6.3762431240081785 and perplexity is 587.7155803624723
At time: 1001.4229946136475 and batch: 200, loss is 6.389458408355713 and perplexity is 595.5339561162656
At time: 1002.1531608104706 and batch: 250, loss is 6.416897468566894 and perplexity is 612.1011022155038
At time: 1002.8820652961731 and batch: 300, loss is 6.436725015640259 and perplexity is 624.3586829452162
At time: 1003.6092355251312 and batch: 350, loss is 6.445886936187744 and perplexity is 630.1052923831825
At time: 1004.342761516571 and batch: 400, loss is 6.485668115615844 and perplexity is 655.6768858980572
At time: 1005.0761480331421 and batch: 450, loss is 6.463067617416382 and perplexity is 641.0244613444767
At time: 1005.8415217399597 and batch: 500, loss is 6.450628929138183 and perplexity is 633.1003428795206
At time: 1006.5972678661346 and batch: 550, loss is 6.421952686309814 and perplexity is 615.2032409549644
At time: 1007.3480250835419 and batch: 600, loss is 6.383334417343139 and perplexity is 591.898055994332
At time: 1008.0759692192078 and batch: 650, loss is 6.418587694168091 and perplexity is 613.1365660061783
At time: 1008.8241214752197 and batch: 700, loss is 6.421068105697632 and perplexity is 614.6592847175043
At time: 1009.5612165927887 and batch: 750, loss is 6.4143595790863035 and perplexity is 610.5496268363944
At time: 1010.2884700298309 and batch: 800, loss is 6.387596998214722 and perplexity is 594.4264542484271
At time: 1011.0230128765106 and batch: 850, loss is 6.371724309921265 and perplexity is 585.0657943729407
At time: 1011.7532243728638 and batch: 900, loss is 6.419752101898194 and perplexity is 613.8509227838086
At time: 1012.4804661273956 and batch: 950, loss is 6.399698657989502 and perplexity is 601.6637040014901
At time: 1013.2122721672058 and batch: 1000, loss is 6.417593469619751 and perplexity is 612.5272735177346
At time: 1013.9426832199097 and batch: 1050, loss is 6.415330390930176 and perplexity is 611.1426434526012
At time: 1014.6733868122101 and batch: 1100, loss is 6.416901540756226 and perplexity is 612.1035948121573
At time: 1015.4055950641632 and batch: 1150, loss is 6.409033336639404 and perplexity is 607.306336440853
At time: 1016.1358082294464 and batch: 1200, loss is 6.382294406890869 and perplexity is 591.2827958233805
At time: 1016.8654601573944 and batch: 1250, loss is 6.3949511623382564 and perplexity is 598.8140778294013
At time: 1017.5958878993988 and batch: 1300, loss is 6.353992357254028 and perplexity is 574.782872967824
At time: 1018.3243434429169 and batch: 1350, loss is 6.360295152664184 and perplexity is 578.4170525358422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.976121012369791 and perplexity of 393.90943102147577
Finished 47 epochs...
Completing Train Step...
At time: 1020.8898029327393 and batch: 50, loss is 6.410431671142578 and perplexity is 608.1561478669408
At time: 1021.6213698387146 and batch: 100, loss is 6.419781503677368 and perplexity is 613.8689713584149
At time: 1022.3521029949188 and batch: 150, loss is 6.376080522537231 and perplexity is 587.6200247135619
At time: 1023.0847375392914 and batch: 200, loss is 6.389312372207642 and perplexity is 595.446992981304
At time: 1023.8143701553345 and batch: 250, loss is 6.416754207611084 and perplexity is 612.0134183075381
At time: 1024.5494091510773 and batch: 300, loss is 6.436587591171264 and perplexity is 624.2728866801383
At time: 1025.2819905281067 and batch: 350, loss is 6.44574631690979 and perplexity is 630.0166936614226
At time: 1026.0139071941376 and batch: 400, loss is 6.485575761795044 and perplexity is 655.6163344285567
At time: 1026.744645357132 and batch: 450, loss is 6.462972230911255 and perplexity is 640.9633191775237
At time: 1027.478700876236 and batch: 500, loss is 6.4505504131317135 and perplexity is 633.0506363203087
At time: 1028.211282491684 and batch: 550, loss is 6.421889324188232 and perplexity is 615.1642616073332
At time: 1028.943933725357 and batch: 600, loss is 6.3832975769042966 and perplexity is 591.8762506118615
At time: 1029.675076007843 and batch: 650, loss is 6.418549795150756 and perplexity is 613.1133291731643
At time: 1030.4119474887848 and batch: 700, loss is 6.421032924652099 and perplexity is 614.6376607416007
At time: 1031.141639471054 and batch: 750, loss is 6.414337072372437 and perplexity is 610.5358855252781
At time: 1031.8793177604675 and batch: 800, loss is 6.387598791122437 and perplexity is 594.4275200011582
At time: 1032.6102509498596 and batch: 850, loss is 6.3717222499847415 and perplexity is 585.0645891757835
At time: 1033.3428239822388 and batch: 900, loss is 6.419745874404907 and perplexity is 613.8471000432112
At time: 1034.0742874145508 and batch: 950, loss is 6.39969160079956 and perplexity is 601.6594579614325
At time: 1034.8111789226532 and batch: 1000, loss is 6.417605142593384 and perplexity is 612.534423574179
At time: 1035.543609380722 and batch: 1050, loss is 6.415350742340088 and perplexity is 611.1550811936152
At time: 1036.3382930755615 and batch: 1100, loss is 6.416935329437256 and perplexity is 612.1242773346953
At time: 1037.0692081451416 and batch: 1150, loss is 6.409060163497925 and perplexity is 607.3226287805544
At time: 1037.8027272224426 and batch: 1200, loss is 6.382318058013916 and perplexity is 591.2967804909159
At time: 1038.5344803333282 and batch: 1250, loss is 6.394947948455811 and perplexity is 598.812153314441
At time: 1039.2680230140686 and batch: 1300, loss is 6.353974924087525 and perplexity is 574.7728527696385
At time: 1040.0026161670685 and batch: 1350, loss is 6.360214347839356 and perplexity is 578.3703155355472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.97611572265625 and perplexity of 393.9073473589352
Finished 48 epochs...
Completing Train Step...
At time: 1042.528077840805 and batch: 50, loss is 6.410296773910522 and perplexity is 608.0741148190758
At time: 1043.3187892436981 and batch: 100, loss is 6.419611930847168 and perplexity is 613.7648846849551
At time: 1044.051204442978 and batch: 150, loss is 6.375923013687133 and perplexity is 587.5274766479358
At time: 1044.7856285572052 and batch: 200, loss is 6.38917103767395 and perplexity is 595.3628417050936
At time: 1045.5154116153717 and batch: 250, loss is 6.416615571975708 and perplexity is 611.9285773195604
At time: 1046.2490265369415 and batch: 300, loss is 6.436454076766967 and perplexity is 624.1895428214812
At time: 1046.9845161437988 and batch: 350, loss is 6.445609302520752 and perplexity is 629.9303782224204
At time: 1047.7189500331879 and batch: 400, loss is 6.485487337112427 and perplexity is 655.5583643252982
At time: 1048.4509570598602 and batch: 450, loss is 6.462881088256836 and perplexity is 640.9049027413834
At time: 1049.191759109497 and batch: 500, loss is 6.450473384857178 and perplexity is 633.0018754001081
At time: 1049.923092365265 and batch: 550, loss is 6.4218277168273925 and perplexity is 615.1263641280866
At time: 1050.658718585968 and batch: 600, loss is 6.383261547088623 and perplexity is 591.8549258038173
At time: 1051.390501499176 and batch: 650, loss is 6.418511571884156 and perplexity is 613.089894426806
At time: 1052.127897977829 and batch: 700, loss is 6.420997943878174 and perplexity is 614.616160616592
At time: 1052.858737230301 and batch: 750, loss is 6.414315910339355 and perplexity is 610.5229654813788
At time: 1053.591013431549 and batch: 800, loss is 6.387598152160645 and perplexity is 594.4271401848063
At time: 1054.3238582611084 and batch: 850, loss is 6.371718149185181 and perplexity is 585.0621899480929
At time: 1055.1160342693329 and batch: 900, loss is 6.419741153717041 and perplexity is 613.844202269494
At time: 1055.8488256931305 and batch: 950, loss is 6.399686441421509 and perplexity is 601.6563537808388
At time: 1056.5876922607422 and batch: 1000, loss is 6.417618885040283 and perplexity is 612.5428413538089
At time: 1057.321058511734 and batch: 1050, loss is 6.415371131896973 and perplexity is 611.1675425019483
At time: 1058.0543353557587 and batch: 1100, loss is 6.416967067718506 and perplexity is 612.1437054154748
At time: 1058.7871041297913 and batch: 1150, loss is 6.409090137481689 and perplexity is 607.3408329319933
At time: 1059.5219767093658 and batch: 1200, loss is 6.382343606948853 and perplexity is 591.3118876868743
At time: 1060.2537887096405 and batch: 1250, loss is 6.394949283599853 and perplexity is 598.8129528154539
At time: 1060.9880664348602 and batch: 1300, loss is 6.353959808349609 and perplexity is 574.7641647194984
At time: 1061.7182450294495 and batch: 1350, loss is 6.360133237838745 and perplexity is 578.3234058213503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.97611083984375 and perplexity of 393.9054239879117
Finished 49 epochs...
Completing Train Step...
At time: 1064.3174104690552 and batch: 50, loss is 6.4101646137237545 and perplexity is 607.9937569406659
At time: 1065.0814714431763 and batch: 100, loss is 6.419446010589599 and perplexity is 613.6630571050642
At time: 1065.8138463497162 and batch: 150, loss is 6.375770206451416 and perplexity is 587.4377050573702
At time: 1066.5543098449707 and batch: 200, loss is 6.38903338432312 and perplexity is 595.2808936553146
At time: 1067.2872784137726 and batch: 250, loss is 6.416480751037597 and perplexity is 611.8460820958762
At time: 1068.0194244384766 and batch: 300, loss is 6.43632435798645 and perplexity is 624.1085789665545
At time: 1068.7522399425507 and batch: 350, loss is 6.445474853515625 and perplexity is 629.8456904029935
At time: 1069.4853899478912 and batch: 400, loss is 6.4854019737243656 and perplexity is 655.5024060306762
At time: 1070.2183303833008 and batch: 450, loss is 6.462792644500732 and perplexity is 640.8482212110803
At time: 1070.951012134552 and batch: 500, loss is 6.450398378372192 and perplexity is 632.9543979350262
At time: 1071.68243765831 and batch: 550, loss is 6.42176778793335 and perplexity is 615.0895013899705
At time: 1072.416960477829 and batch: 600, loss is 6.383226490020752 and perplexity is 591.8341774692032
At time: 1073.1499395370483 and batch: 650, loss is 6.418473701477051 and perplexity is 613.0666769025438
At time: 1073.8848085403442 and batch: 700, loss is 6.4209620475769045 and perplexity is 614.5940985657008
At time: 1074.6706492900848 and batch: 750, loss is 6.4142950248718265 and perplexity is 610.5102145569628
At time: 1075.4085056781769 and batch: 800, loss is 6.387596092224121 and perplexity is 594.4259157038907
At time: 1076.1402788162231 and batch: 850, loss is 6.371711759567261 and perplexity is 585.0584516361828
At time: 1076.878199338913 and batch: 900, loss is 6.419736604690552 and perplexity is 613.8414098823092
At time: 1077.6082139015198 and batch: 950, loss is 6.39968243598938 and perplexity is 601.653943891975
At time: 1078.3413727283478 and batch: 1000, loss is 6.417633848190308 and perplexity is 612.5520069928139
At time: 1079.0742530822754 and batch: 1050, loss is 6.4153929042816165 and perplexity is 611.1808492216244
At time: 1079.8088841438293 and batch: 1100, loss is 6.4169972801208495 and perplexity is 612.1622000267768
At time: 1080.5450761318207 and batch: 1150, loss is 6.409118938446045 and perplexity is 607.35832518557
At time: 1081.2775423526764 and batch: 1200, loss is 6.38236969947815 and perplexity is 591.3273167109179
At time: 1082.0098934173584 and batch: 1250, loss is 6.394952611923218 and perplexity is 598.8149458619124
At time: 1082.7585849761963 and batch: 1300, loss is 6.353946056365967 and perplexity is 574.7562606264554
At time: 1083.5406005382538 and batch: 1350, loss is 6.360052394866943 and perplexity is 578.2766543283512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.97610595703125 and perplexity of 393.90350062627937
Finished Training.
Improved accuracyfrom -10000000 to -393.90350062627937
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc67f09898>
SETTINGS FOR THIS RUN
{'seq_len': 20, 'lr': 25.35419659816711, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.025124451429033634, 'anneal': 2.101066051940741, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1375 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.325207233428955 and batch: 50, loss is 7.7672718811035155 and perplexity is 2362.018622899158
At time: 2.0439260005950928 and batch: 100, loss is 7.176519918441772 and perplexity is 1308.347171508679
At time: 2.7599895000457764 and batch: 150, loss is 7.056946201324463 and perplexity is 1160.8946087702493
At time: 3.482612371444702 and batch: 200, loss is 6.79886757850647 and perplexity is 896.8311255522027
At time: 4.200526237487793 and batch: 250, loss is 6.279094305038452 and perplexity is 533.305432985104
At time: 4.916506290435791 and batch: 300, loss is 6.17762378692627 and perplexity is 481.8456270523138
At time: 5.6590895652771 and batch: 350, loss is 6.165795631408692 and perplexity is 476.1798559069707
At time: 6.373345613479614 and batch: 400, loss is 6.2421957206726075 and perplexity is 513.9858421767534
At time: 7.087623834609985 and batch: 450, loss is 6.241991672515869 and perplexity is 513.8809750124072
At time: 7.803596019744873 and batch: 500, loss is 6.298023281097412 and perplexity is 543.4965077716158
At time: 8.519418716430664 and batch: 550, loss is 6.291765766143799 and perplexity is 540.1061887955866
At time: 9.235317468643188 and batch: 600, loss is 6.253204555511474 and perplexity is 519.6754881465052
At time: 9.950413465499878 and batch: 650, loss is 6.31611216545105 and perplexity is 553.4172099551542
At time: 10.665704011917114 and batch: 700, loss is 6.3287307643890385 and perplexity is 560.4448057297185
At time: 11.378703355789185 and batch: 750, loss is 6.379641761779785 and perplexity is 589.7164108559768
At time: 12.092042446136475 and batch: 800, loss is 6.416568975448609 and perplexity is 611.9000642373351
At time: 12.808803796768188 and batch: 850, loss is 6.486910800933838 and perplexity is 656.4921924173104
At time: 13.525351285934448 and batch: 900, loss is 6.493881540298462 and perplexity is 661.0844153851452
At time: 14.24171257019043 and batch: 950, loss is 6.43680627822876 and perplexity is 624.4094220095112
At time: 14.956360101699829 and batch: 1000, loss is 6.450819730758667 and perplexity is 633.2211509756945
At time: 15.671307563781738 and batch: 1050, loss is 6.479843854904175 and perplexity is 651.8691521528244
At time: 16.38430953025818 and batch: 1100, loss is 6.4515225696563725 and perplexity is 633.6663598681865
At time: 17.096307516098022 and batch: 1150, loss is 6.445636310577393 and perplexity is 629.947391647504
At time: 17.809648752212524 and batch: 1200, loss is 6.436541719436645 and perplexity is 624.2442508567433
At time: 18.522122621536255 and batch: 1250, loss is 6.4660640144348145 and perplexity is 642.9481056910143
At time: 19.239155769348145 and batch: 1300, loss is 6.431757984161377 and perplexity is 621.2651628788093
At time: 19.95631766319275 and batch: 1350, loss is 6.379437818527221 and perplexity is 589.596154436216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 6.063944091796875 and perplexity of 430.0683251646984
Finished 1 epochs...
Completing Train Step...
At time: 22.49845051765442 and batch: 50, loss is 6.353089542388916 and perplexity is 574.2641846209026
At time: 23.20664095878601 and batch: 100, loss is 6.438825912475586 and perplexity is 625.6717749788003
At time: 23.914057970046997 and batch: 150, loss is 6.449722528457642 and perplexity is 632.5267602849798
At time: 24.62512230873108 and batch: 200, loss is 6.477455177307129 and perplexity is 650.3139051442068
At time: 25.3348548412323 and batch: 250, loss is 6.5186410331726075 and perplexity is 677.6568461364085
At time: 26.043859481811523 and batch: 300, loss is 6.453534994125366 and perplexity is 634.9428495446938
At time: 26.75554895401001 and batch: 350, loss is 6.512503414154053 and perplexity is 673.5103843075517
At time: 27.464476108551025 and batch: 400, loss is 6.5548177909851075 and perplexity is 702.6211142576177
At time: 28.204166650772095 and batch: 450, loss is 6.5377597236633305 and perplexity is 690.7374007380665
At time: 28.913145303726196 and batch: 500, loss is 6.565722188949585 and perplexity is 710.3246996528537
At time: 29.622527599334717 and batch: 550, loss is 6.582690944671631 and perplexity is 722.4808718381573
At time: 30.32836890220642 and batch: 600, loss is 6.451002740859986 and perplexity is 633.3370474474807
At time: 31.033620357513428 and batch: 650, loss is 6.445979223251343 and perplexity is 630.1634456337299
At time: 31.74259352684021 and batch: 700, loss is 6.406244382858277 and perplexity is 605.6149468375875
At time: 32.44881224632263 and batch: 750, loss is 6.428756170272827 and perplexity is 619.4030367607131
At time: 33.15736389160156 and batch: 800, loss is 6.474045124053955 and perplexity is 648.100076877811
At time: 33.86621594429016 and batch: 850, loss is 6.508940267562866 and perplexity is 671.1148384525906
At time: 34.580392837524414 and batch: 900, loss is 6.4982100772857665 and perplexity is 663.9521457898965
At time: 35.289559841156006 and batch: 950, loss is 6.469107713699341 and perplexity is 644.9080275607347
At time: 36.00111365318298 and batch: 1000, loss is 6.468954362869263 and perplexity is 644.8091379619791
At time: 36.71197485923767 and batch: 1050, loss is 6.400929183959961 and perplexity is 602.4045225194807
At time: 37.42281365394592 and batch: 1100, loss is 6.401331405639649 and perplexity is 602.6468714141026
At time: 38.133886098861694 and batch: 1150, loss is 6.449383087158203 and perplexity is 632.3120910154083
At time: 38.845234394073486 and batch: 1200, loss is 6.461349992752075 and perplexity is 639.9243669643294
At time: 39.56206750869751 and batch: 1250, loss is 6.477415437698364 and perplexity is 650.2880624375351
At time: 40.281890869140625 and batch: 1300, loss is 6.333930969238281 and perplexity is 563.366824489471
At time: 40.998270988464355 and batch: 1350, loss is 6.293614492416382 and perplexity is 541.1056208502924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 6.167285970052084 and perplexity of 476.890054233987
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 43.487592697143555 and batch: 50, loss is 6.257679042816162 and perplexity is 522.0059795095464
At time: 44.22827172279358 and batch: 100, loss is 6.195082311630249 and perplexity is 490.3318033471107
At time: 44.93830847740173 and batch: 150, loss is 6.1017992973327635 and perplexity is 446.6607229378557
At time: 45.70362734794617 and batch: 200, loss is 6.096936168670655 and perplexity is 444.49382758992715
At time: 46.415326833724976 and batch: 250, loss is 6.102913980484009 and perplexity is 447.1588857152835
At time: 47.128730058670044 and batch: 300, loss is 6.116847763061523 and perplexity is 453.433110738543
At time: 47.83795666694641 and batch: 350, loss is 6.088038730621338 and perplexity is 440.5565132753003
At time: 48.548847675323486 and batch: 400, loss is 6.144534168243408 and perplexity is 466.1624452060809
At time: 49.25940179824829 and batch: 450, loss is 6.105713472366333 and perplexity is 448.41245724891616
At time: 49.96945333480835 and batch: 500, loss is 6.125834302902222 and perplexity is 457.52626957455885
At time: 50.68134880065918 and batch: 550, loss is 6.083288249969482 and perplexity is 438.46862125472546
At time: 51.39327144622803 and batch: 600, loss is 6.055915975570679 and perplexity is 426.629508743502
At time: 52.10233449935913 and batch: 650, loss is 6.087628850936889 and perplexity is 440.3759751126475
At time: 52.81145215034485 and batch: 700, loss is 6.076470804214478 and perplexity is 435.48955157533646
At time: 53.519216537475586 and batch: 750, loss is 6.054752159118652 and perplexity is 426.1332791184176
At time: 54.22741222381592 and batch: 800, loss is 6.028409824371338 and perplexity is 415.05449479598343
At time: 54.937772274017334 and batch: 850, loss is 6.038219223022461 and perplexity is 419.14596441748927
At time: 55.65082311630249 and batch: 900, loss is 6.0788799667358395 and perplexity is 436.53998150157776
At time: 56.36104488372803 and batch: 950, loss is 6.05421067237854 and perplexity is 425.90259605979963
At time: 57.076777935028076 and batch: 1000, loss is 6.0652580833435055 and perplexity is 430.6338027434559
At time: 57.787888526916504 and batch: 1050, loss is 6.021803007125855 and perplexity is 412.3213442568438
At time: 58.49751925468445 and batch: 1100, loss is 6.003736476898194 and perplexity is 404.9390155575653
At time: 59.20717430114746 and batch: 1150, loss is 6.018539752960205 and perplexity is 410.97802789602827
At time: 59.91765379905701 and batch: 1200, loss is 6.016268882751465 and perplexity is 410.0458090106893
At time: 60.6298713684082 and batch: 1250, loss is 6.0156477928161625 and perplexity is 409.7912127574693
At time: 61.344850301742554 and batch: 1300, loss is 5.998411197662353 and perplexity is 402.78833379921457
At time: 62.06292366981506 and batch: 1350, loss is 5.99140703201294 and perplexity is 399.9769946235676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.7693880208333335 and perplexity of 320.3416302442587
Finished 3 epochs...
Completing Train Step...
At time: 64.60857653617859 and batch: 50, loss is 6.04440447807312 and perplexity is 421.74652338021957
At time: 65.31489634513855 and batch: 100, loss is 6.063859367370606 and perplexity is 430.03188941611296
At time: 66.02349376678467 and batch: 150, loss is 6.010294933319091 and perplexity is 407.60351840637
At time: 66.72913074493408 and batch: 200, loss is 6.005531539916992 and perplexity is 405.6665594074304
At time: 67.4399003982544 and batch: 250, loss is 6.022280626296997 and perplexity is 412.5183238724077
At time: 68.14932656288147 and batch: 300, loss is 6.016943712234497 and perplexity is 410.32261339938
At time: 68.8574948310852 and batch: 350, loss is 6.031182107925415 and perplexity is 416.2067399831729
At time: 69.56978273391724 and batch: 400, loss is 6.080579643249512 and perplexity is 437.28258917304726
At time: 70.27687788009644 and batch: 450, loss is 6.062441835403442 and perplexity is 429.42273731425684
At time: 70.98373293876648 and batch: 500, loss is 6.08647123336792 and perplexity is 439.8664831022529
At time: 71.68966889381409 and batch: 550, loss is 6.026470403671265 and perplexity is 414.2503095958983
At time: 72.39889240264893 and batch: 600, loss is 5.988659248352051 and perplexity is 398.8794529670904
At time: 73.10243320465088 and batch: 650, loss is 6.017393712997436 and perplexity is 410.50730043999914
At time: 73.81133913993835 and batch: 700, loss is 6.017006359100342 and perplexity is 410.3483196302938
At time: 74.5173089504242 and batch: 750, loss is 5.985866575241089 and perplexity is 397.76706703237863
At time: 75.22447443008423 and batch: 800, loss is 5.954949398040771 and perplexity is 385.65739516447655
At time: 75.930899143219 and batch: 850, loss is 5.988094511032105 and perplexity is 398.6542544488292
At time: 76.6377444267273 and batch: 900, loss is 6.011063747406006 and perplexity is 407.9170102262188
At time: 77.34428524971008 and batch: 950, loss is 5.984856414794922 and perplexity is 397.365461351712
At time: 78.04866409301758 and batch: 1000, loss is 5.986479682922363 and perplexity is 398.01101585234005
At time: 78.75403237342834 and batch: 1050, loss is 5.969547300338745 and perplexity is 391.3284763763031
At time: 79.46354866027832 and batch: 1100, loss is 5.939223833084107 and perplexity is 379.640151029104
At time: 80.1705265045166 and batch: 1150, loss is 5.954745988845826 and perplexity is 385.5789568820058
At time: 80.87819695472717 and batch: 1200, loss is 5.960274248123169 and perplexity is 387.71644016468235
At time: 81.58768105506897 and batch: 1250, loss is 5.9631640625 and perplexity is 388.8384891834417
At time: 82.2969400882721 and batch: 1300, loss is 5.940868673324585 and perplexity is 380.26511226621085
At time: 83.01535654067993 and batch: 1350, loss is 5.955569486618042 and perplexity is 385.8966110698184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.761093343098958 and perplexity of 317.6954892713441
Finished 4 epochs...
Completing Train Step...
At time: 85.53405833244324 and batch: 50, loss is 6.013890819549561 and perplexity is 409.0718526850269
At time: 86.24720168113708 and batch: 100, loss is 6.024859819412232 and perplexity is 413.5836615584871
At time: 86.96256995201111 and batch: 150, loss is 5.974100551605225 and perplexity is 393.114355950772
At time: 87.67282009124756 and batch: 200, loss is 5.982295799255371 and perplexity is 396.34926277866947
At time: 88.38288927078247 and batch: 250, loss is 6.002114334106445 and perplexity is 404.2826791320025
At time: 89.09196400642395 and batch: 300, loss is 5.996456899642944 and perplexity is 402.0019340362695
At time: 89.80968618392944 and batch: 350, loss is 5.995473022460938 and perplexity is 401.606608014265
At time: 90.52187275886536 and batch: 400, loss is 6.047780704498291 and perplexity is 423.1728415688878
At time: 91.23610401153564 and batch: 450, loss is 6.024241123199463 and perplexity is 413.32785805390745
At time: 91.94635009765625 and batch: 500, loss is 6.036897535324097 and perplexity is 418.59235028558413
At time: 92.65726590156555 and batch: 550, loss is 5.994044246673584 and perplexity is 401.0332119414501
At time: 93.36613774299622 and batch: 600, loss is 5.951830577850342 and perplexity is 384.45647279776296
At time: 94.07565712928772 and batch: 650, loss is 5.992925281524658 and perplexity is 400.58472072354266
At time: 94.7921085357666 and batch: 700, loss is 5.992346611022949 and perplexity is 400.3529812190988
At time: 95.50335741043091 and batch: 750, loss is 5.959783411026001 and perplexity is 387.526181249554
At time: 96.21163201332092 and batch: 800, loss is 5.921436557769775 and perplexity is 372.947089274079
At time: 96.91955351829529 and batch: 850, loss is 5.949873905181885 and perplexity is 383.7049528043142
At time: 97.62971687316895 and batch: 900, loss is 5.975880746841431 and perplexity is 393.81479953270264
At time: 98.34012031555176 and batch: 950, loss is 5.959730777740479 and perplexity is 387.5057850101743
At time: 99.05370426177979 and batch: 1000, loss is 5.959529409408569 and perplexity is 387.42776147263993
At time: 99.76465106010437 and batch: 1050, loss is 5.933092069625855 and perplexity is 377.31940981319593
At time: 100.53085088729858 and batch: 1100, loss is 5.924757604598999 and perplexity is 374.1877229834223
At time: 101.24113082885742 and batch: 1150, loss is 5.94330062866211 and perplexity is 381.1910254690869
At time: 101.95775246620178 and batch: 1200, loss is 5.947512836456299 and perplexity is 382.80006770871273
At time: 102.67242407798767 and batch: 1250, loss is 5.939616632461548 and perplexity is 379.78930273551094
At time: 103.38757038116455 and batch: 1300, loss is 5.916270818710327 and perplexity is 371.025509391126
At time: 104.09844446182251 and batch: 1350, loss is 5.92428768157959 and perplexity is 374.0119248678378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.73057861328125 and perplexity of 308.1475150492838
Finished 5 epochs...
Completing Train Step...
At time: 106.58638882637024 and batch: 50, loss is 5.968950519561767 and perplexity is 391.09500873554674
At time: 107.32978320121765 and batch: 100, loss is 5.989473648071289 and perplexity is 399.2044325952939
At time: 108.04168772697449 and batch: 150, loss is 5.941965494155884 and perplexity is 380.6824237789049
At time: 108.75275683403015 and batch: 200, loss is 5.937378425598144 and perplexity is 378.94020629269124
At time: 109.46207785606384 and batch: 250, loss is 5.964108333587647 and perplexity is 389.20583153462775
At time: 110.1729006767273 and batch: 300, loss is 5.95663272857666 and perplexity is 386.30713074059184
At time: 110.88583159446716 and batch: 350, loss is 5.9638707637786865 and perplexity is 389.1133789619887
At time: 111.59647178649902 and batch: 400, loss is 6.018874216079712 and perplexity is 411.1155078789975
At time: 112.31047487258911 and batch: 450, loss is 5.995273876190185 and perplexity is 401.52663751914673
At time: 113.01912045478821 and batch: 500, loss is 6.021340160369873 and perplexity is 412.13054681862644
At time: 113.72699117660522 and batch: 550, loss is 5.975719585418701 and perplexity is 393.7513368933199
At time: 114.44015979766846 and batch: 600, loss is 5.930232925415039 and perplexity is 376.2421399757733
At time: 115.15456891059875 and batch: 650, loss is 5.961786870956421 and perplexity is 388.3033526816336
At time: 115.8655207157135 and batch: 700, loss is 5.961230001449585 and perplexity is 388.0871785810918
At time: 116.5768666267395 and batch: 750, loss is 5.942456731796264 and perplexity is 380.8694752540975
At time: 117.30813431739807 and batch: 800, loss is 5.889927968978882 and perplexity is 361.37925291816265
At time: 118.05847072601318 and batch: 850, loss is 5.924206027984619 and perplexity is 373.98138669640315
At time: 118.84790229797363 and batch: 900, loss is 5.950779571533203 and perplexity is 384.05261887987217
At time: 119.56260704994202 and batch: 950, loss is 5.931976861953736 and perplexity is 376.89885485917506
At time: 120.27339458465576 and batch: 1000, loss is 5.9298616313934325 and perplexity is 376.1024694495441
At time: 121.00343823432922 and batch: 1050, loss is 5.908786811828613 and perplexity is 368.2591166887711
At time: 121.72134804725647 and batch: 1100, loss is 5.900110731124878 and perplexity is 365.07789111278254
At time: 122.43481063842773 and batch: 1150, loss is 5.912939462661743 and perplexity is 369.7915478400771
At time: 123.14805889129639 and batch: 1200, loss is 5.922781791687012 and perplexity is 373.449127951924
At time: 123.85972332954407 and batch: 1250, loss is 5.908885641098022 and perplexity is 368.29551326672066
At time: 124.57337975502014 and batch: 1300, loss is 5.888676652908325 and perplexity is 360.9273360558038
At time: 125.28279209136963 and batch: 1350, loss is 5.908549661636353 and perplexity is 368.1717943231138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.709140625 and perplexity of 301.6117592712844
Finished 6 epochs...
Completing Train Step...
At time: 127.75155258178711 and batch: 50, loss is 5.957484827041626 and perplexity is 386.6364427368993
At time: 128.49455666542053 and batch: 100, loss is 5.972668828964234 and perplexity is 392.5519279433883
At time: 129.20808267593384 and batch: 150, loss is 5.92754942893982 and perplexity is 375.23384899703444
At time: 129.91889762878418 and batch: 200, loss is 5.923678884506225 and perplexity is 373.7842967992565
At time: 130.62857604026794 and batch: 250, loss is 5.945236358642578 and perplexity is 381.9296229974324
At time: 131.33939218521118 and batch: 300, loss is 5.938273820877075 and perplexity is 379.27965951412966
At time: 132.05470037460327 and batch: 350, loss is 5.943291664123535 and perplexity is 381.1876082827516
At time: 132.7664053440094 and batch: 400, loss is 5.997912921905518 and perplexity is 402.5876841309303
At time: 133.47806572914124 and batch: 450, loss is 5.969520206451416 and perplexity is 391.3178739102871
At time: 134.19204139709473 and batch: 500, loss is 5.977197370529175 and perplexity is 394.3336469147767
At time: 134.90472412109375 and batch: 550, loss is 5.937082643508911 and perplexity is 378.82813914132475
At time: 135.61512994766235 and batch: 600, loss is 5.905028162002563 and perplexity is 366.8775576487019
At time: 136.3286464214325 and batch: 650, loss is 5.949561223983765 and perplexity is 383.58499423531856
At time: 137.0451397895813 and batch: 700, loss is 5.946123552322388 and perplexity is 382.2686189003634
At time: 137.7883231639862 and batch: 750, loss is 5.921497354507446 and perplexity is 372.96976392969634
At time: 138.49810457229614 and batch: 800, loss is 5.889677743911744 and perplexity is 361.2888380828396
At time: 139.21161079406738 and batch: 850, loss is 5.900223693847656 and perplexity is 365.1191336347788
At time: 139.9226415157318 and batch: 900, loss is 5.928422327041626 and perplexity is 375.5615329080829
At time: 140.63373494148254 and batch: 950, loss is 5.906862030029297 and perplexity is 367.5509799664979
At time: 141.343585729599 and batch: 1000, loss is 5.915902891159058 and perplexity is 370.88902399390594
At time: 142.05333018302917 and batch: 1050, loss is 5.900962152481079 and perplexity is 365.38885858934276
At time: 142.76303434371948 and batch: 1100, loss is 5.876856813430786 and perplexity is 356.6863461655496
At time: 143.48060607910156 and batch: 1150, loss is 5.890557813644409 and perplexity is 361.60693740824905
At time: 144.1951003074646 and batch: 1200, loss is 5.895304985046387 and perplexity is 363.327628496629
At time: 144.91207814216614 and batch: 1250, loss is 5.887776412963867 and perplexity is 360.60256106048547
At time: 145.62379717826843 and batch: 1300, loss is 5.872186517715454 and perplexity is 355.02439936366653
At time: 146.33830070495605 and batch: 1350, loss is 5.8844203281402585 and perplexity is 359.3943767964045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.712297770182292 and perplexity of 302.5654961346592
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 148.82480692863464 and batch: 50, loss is 5.858493099212646 and perplexity is 350.19603553462554
At time: 149.5392415523529 and batch: 100, loss is 5.831121110916138 and perplexity is 340.74047287887146
At time: 150.2562313079834 and batch: 150, loss is 5.7623866748809816 and perplexity is 318.1066407649599
At time: 150.9715850353241 and batch: 200, loss is 5.74589898109436 and perplexity is 312.9047969250382
At time: 151.68489861488342 and batch: 250, loss is 5.776390657424927 and perplexity is 322.5927389150474
At time: 152.4015862941742 and batch: 300, loss is 5.754762840270996 and perplexity is 315.69066953189883
At time: 153.11550426483154 and batch: 350, loss is 5.7525232219696045 and perplexity is 314.9844340749637
At time: 153.82919263839722 and batch: 400, loss is 5.78366982460022 and perplexity is 324.94951265980757
At time: 154.5426845550537 and batch: 450, loss is 5.749576301574707 and perplexity is 314.05756639551623
At time: 155.2635793685913 and batch: 500, loss is 5.768662719726563 and perplexity is 320.1093703446608
At time: 156.02635502815247 and batch: 550, loss is 5.720514068603515 and perplexity is 305.06170529984536
At time: 156.7468967437744 and batch: 600, loss is 5.676367740631104 and perplexity is 291.8872917595079
At time: 157.46272230148315 and batch: 650, loss is 5.70986083984375 and perplexity is 301.8290627805746
At time: 158.1773030757904 and batch: 700, loss is 5.727092943191528 and perplexity is 307.0752842798397
At time: 158.90905833244324 and batch: 750, loss is 5.700229930877685 and perplexity is 298.93612771177374
At time: 159.6295108795166 and batch: 800, loss is 5.679078722000122 and perplexity is 292.6796663403931
At time: 160.33968305587769 and batch: 850, loss is 5.6704975128173825 and perplexity is 290.178866190561
At time: 161.04585886001587 and batch: 900, loss is 5.691483840942383 and perplexity is 296.3330056174702
At time: 161.7613503932953 and batch: 950, loss is 5.668027067184449 and perplexity is 289.46287984462634
At time: 162.47015929222107 and batch: 1000, loss is 5.6727407360076905 and perplexity is 290.83053279608964
At time: 163.18970561027527 and batch: 1050, loss is 5.6378394222259525 and perplexity is 280.855252856186
At time: 163.91536736488342 and batch: 1100, loss is 5.625451583862304 and perplexity is 277.3975244883341
At time: 164.65017199516296 and batch: 1150, loss is 5.630035257339477 and perplexity is 278.67194269112247
At time: 165.36217308044434 and batch: 1200, loss is 5.62582766532898 and perplexity is 277.5018681758295
At time: 166.0800485610962 and batch: 1250, loss is 5.640410451889038 and perplexity is 281.5782690921344
At time: 166.79634475708008 and batch: 1300, loss is 5.608865842819214 and perplexity is 272.8346251138634
At time: 167.51688051223755 and batch: 1350, loss is 5.603829469680786 and perplexity is 271.4639825675588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.447970377604166 and perplexity of 232.28623380530993
Finished 8 epochs...
Completing Train Step...
At time: 170.08868741989136 and batch: 50, loss is 5.671875038146973 and perplexity is 290.5788703735441
At time: 170.80276942253113 and batch: 100, loss is 5.685328693389892 and perplexity is 294.5146341564197
At time: 171.51350212097168 and batch: 150, loss is 5.6307187843322755 and perplexity is 278.86248759996295
At time: 172.22719359397888 and batch: 200, loss is 5.617969369888305 and perplexity is 275.3297223701025
At time: 172.93700218200684 and batch: 250, loss is 5.6705991077423095 and perplexity is 290.2083483882825
At time: 173.65031456947327 and batch: 300, loss is 5.658297424316406 and perplexity is 286.66016621163254
At time: 174.42105555534363 and batch: 350, loss is 5.655763444900512 and perplexity is 285.93469480409306
At time: 175.1353349685669 and batch: 400, loss is 5.691070718765259 and perplexity is 296.2106091651209
At time: 175.84539937973022 and batch: 450, loss is 5.665289144515992 and perplexity is 288.6714368135333
At time: 176.5626769065857 and batch: 500, loss is 5.6860720157623295 and perplexity is 294.7336348569748
At time: 177.27293729782104 and batch: 550, loss is 5.646166515350342 and perplexity is 283.2037251044152
At time: 177.9912781715393 and batch: 600, loss is 5.605298843383789 and perplexity is 271.8631578018318
At time: 178.7044370174408 and batch: 650, loss is 5.647465925216675 and perplexity is 283.57196201252106
At time: 179.416344165802 and batch: 700, loss is 5.64925175666809 and perplexity is 284.078826193313
At time: 180.1287019252777 and batch: 750, loss is 5.625909967422485 and perplexity is 277.52470810040603
At time: 180.8440580368042 and batch: 800, loss is 5.593443078994751 and perplexity is 268.6590434001294
At time: 181.55750608444214 and batch: 850, loss is 5.600069885253906 and perplexity is 270.44530690468827
At time: 182.27624082565308 and batch: 900, loss is 5.62939829826355 and perplexity is 278.49449658697176
At time: 182.98742294311523 and batch: 950, loss is 5.607687692642212 and perplexity is 272.5133742300451
At time: 183.701030254364 and batch: 1000, loss is 5.623383541107177 and perplexity is 276.8244473258077
At time: 184.4130895137787 and batch: 1050, loss is 5.586930837631225 and perplexity is 266.91515533808257
At time: 185.12866497039795 and batch: 1100, loss is 5.577182502746582 and perplexity is 264.3258183852983
At time: 185.84130001068115 and batch: 1150, loss is 5.5910727977752686 and perplexity is 268.0230000137474
At time: 186.5566976070404 and batch: 1200, loss is 5.58836672782898 and perplexity is 267.2986914852645
At time: 187.26815056800842 and batch: 1250, loss is 5.605717134475708 and perplexity is 271.9768995258779
At time: 187.98008370399475 and batch: 1300, loss is 5.571591796875 and perplexity is 262.8521736758562
At time: 188.69488954544067 and batch: 1350, loss is 5.565648622512818 and perplexity is 261.2946303365809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.4318310546875 and perplexity of 228.56738186780888
Finished 9 epochs...
Completing Train Step...
At time: 191.2162048816681 and batch: 50, loss is 5.636436977386475 and perplexity is 280.46164492739587
At time: 191.95560359954834 and batch: 100, loss is 5.652100067138672 and perplexity is 284.88912433094447
At time: 192.67038488388062 and batch: 150, loss is 5.6026692199707036 and perplexity is 271.1491992094498
At time: 193.41752982139587 and batch: 200, loss is 5.593127307891845 and perplexity is 268.5742220304629
At time: 194.1300654411316 and batch: 250, loss is 5.637854986190796 and perplexity is 280.8596241114844
At time: 194.848042011261 and batch: 300, loss is 5.629090328216552 and perplexity is 278.40874182939547
At time: 195.5638828277588 and batch: 350, loss is 5.620642032623291 and perplexity is 276.0665700930239
At time: 196.27698016166687 and batch: 400, loss is 5.6565343856811525 and perplexity is 286.1552185153561
At time: 196.99043917655945 and batch: 450, loss is 5.63014889717102 and perplexity is 278.70361272320014
At time: 197.70363450050354 and batch: 500, loss is 5.6480260181427 and perplexity is 283.73083314961013
At time: 198.4168736934662 and batch: 550, loss is 5.613727979660034 and perplexity is 274.1644145855454
At time: 199.13057661056519 and batch: 600, loss is 5.572319488525391 and perplexity is 263.0435186195424
At time: 199.84274053573608 and batch: 650, loss is 5.612642593383789 and perplexity is 273.8670017256355
At time: 200.55149722099304 and batch: 700, loss is 5.620255918502807 and perplexity is 275.95999746803926
At time: 201.26440978050232 and batch: 750, loss is 5.596511878967285 and perplexity is 269.48477061293477
At time: 201.98138236999512 and batch: 800, loss is 5.563666162490844 and perplexity is 260.7771373020939
At time: 202.698548078537 and batch: 850, loss is 5.568058843612671 and perplexity is 261.92516772991996
At time: 203.4149031639099 and batch: 900, loss is 5.5963730621337895 and perplexity is 269.4473641867839
At time: 204.13313817977905 and batch: 950, loss is 5.574523277282715 and perplexity is 263.623850197793
At time: 204.87612318992615 and batch: 1000, loss is 5.58654034614563 and perplexity is 266.81094758998995
At time: 205.63434982299805 and batch: 1050, loss is 5.556189756393433 and perplexity is 258.8347316771539
At time: 206.38196086883545 and batch: 1100, loss is 5.54792516708374 and perplexity is 256.70438426978563
At time: 207.10155963897705 and batch: 1150, loss is 5.560381574630737 and perplexity is 259.92199704318233
At time: 207.81750440597534 and batch: 1200, loss is 5.554762773513794 and perplexity is 258.4656423511
At time: 208.53212881088257 and batch: 1250, loss is 5.569056377410889 and perplexity is 262.1865772980974
At time: 209.24440670013428 and batch: 1300, loss is 5.535265960693359 and perplexity is 253.47519311119768
At time: 209.95625734329224 and batch: 1350, loss is 5.523381567001342 and perplexity is 250.48062369176645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.400594482421875 and perplexity of 221.5380775580135
Finished 10 epochs...
Completing Train Step...
At time: 212.47984385490417 and batch: 50, loss is 5.591858911514282 and perplexity is 268.2337794138663
At time: 213.2233395576477 and batch: 100, loss is 5.612118692398071 and perplexity is 273.72356011135815
At time: 213.93081140518188 and batch: 150, loss is 5.563016672134399 and perplexity is 260.60782005714566
At time: 214.64047837257385 and batch: 200, loss is 5.551119241714478 and perplexity is 257.5256280900051
At time: 215.34900569915771 and batch: 250, loss is 5.593199062347412 and perplexity is 268.59349411896477
At time: 216.05751633644104 and batch: 300, loss is 5.58662971496582 and perplexity is 266.83479323510267
At time: 216.76933813095093 and batch: 350, loss is 5.577354421615601 and perplexity is 264.3712648874913
At time: 217.47694396972656 and batch: 400, loss is 5.613627634048462 and perplexity is 274.1369047699602
At time: 218.18361592292786 and batch: 450, loss is 5.5872807121276855 and perplexity is 267.0085584823903
At time: 218.890554189682 and batch: 500, loss is 5.611849451065064 and perplexity is 273.6498723355009
At time: 219.60026478767395 and batch: 550, loss is 5.579834442138672 and perplexity is 265.02772473060134
At time: 220.313236951828 and batch: 600, loss is 5.539412498474121 and perplexity is 254.52841968873884
At time: 221.02335929870605 and batch: 650, loss is 5.578551750183106 and perplexity is 264.6879937317768
At time: 221.73280882835388 and batch: 700, loss is 5.584751348495484 and perplexity is 266.3340501431172
At time: 222.44328904151917 and batch: 750, loss is 5.564121408462524 and perplexity is 260.8958820703448
At time: 223.15673899650574 and batch: 800, loss is 5.527828216552734 and perplexity is 251.59690325743236
At time: 223.86577558517456 and batch: 850, loss is 5.526482820510864 and perplexity is 251.25863338414226
At time: 224.57968473434448 and batch: 900, loss is 5.562230958938598 and perplexity is 260.4031374754306
At time: 225.28989958763123 and batch: 950, loss is 5.540573081970215 and perplexity is 254.82399265729407
At time: 225.9983034133911 and batch: 1000, loss is 5.555586223602295 and perplexity is 258.6785635601314
At time: 226.7097611427307 and batch: 1050, loss is 5.522788381576538 and perplexity is 250.33208629606108
At time: 227.418860912323 and batch: 1100, loss is 5.516857452392578 and perplexity is 248.85177855920833
At time: 228.1323208808899 and batch: 1150, loss is 5.530878810882569 and perplexity is 252.36559523147343
At time: 228.84088921546936 and batch: 1200, loss is 5.526329107284546 and perplexity is 251.22001457715126
At time: 229.6104438304901 and batch: 1250, loss is 5.5385751628875735 and perplexity is 254.31538318911413
At time: 230.31901621818542 and batch: 1300, loss is 5.507775945663452 and perplexity is 246.6020603351361
At time: 231.02843689918518 and batch: 1350, loss is 5.501785125732422 and perplexity is 245.12912823781247
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.38562744140625 and perplexity of 218.24699835997825
Finished 11 epochs...
Completing Train Step...
At time: 233.54673099517822 and batch: 50, loss is 5.56311861038208 and perplexity is 260.6343873157412
At time: 234.25903344154358 and batch: 100, loss is 5.588736476898194 and perplexity is 267.3975432016821
At time: 234.97171354293823 and batch: 150, loss is 5.540403079986572 and perplexity is 254.78067575514663
At time: 235.68354773521423 and batch: 200, loss is 5.531891202926635 and perplexity is 252.62121752543723
At time: 236.401762008667 and batch: 250, loss is 5.5722699642181395 and perplexity is 263.0304918940782
At time: 237.11327075958252 and batch: 300, loss is 5.566578769683838 and perplexity is 261.537785865474
At time: 237.82752895355225 and batch: 350, loss is 5.559488105773926 and perplexity is 259.6898685488563
At time: 238.53943967819214 and batch: 400, loss is 5.594628591537475 and perplexity is 268.9777309325053
At time: 239.2544448375702 and batch: 450, loss is 5.57190170288086 and perplexity is 262.9336457668246
At time: 239.96823501586914 and batch: 500, loss is 5.594061479568482 and perplexity is 268.8252336874944
At time: 240.68075823783875 and batch: 550, loss is 5.5648919868469235 and perplexity is 261.09700027636626
At time: 241.39384174346924 and batch: 600, loss is 5.528526420593262 and perplexity is 251.7726305714925
At time: 242.10832023620605 and batch: 650, loss is 5.563364305496216 and perplexity is 260.6984317786641
At time: 242.82147884368896 and batch: 700, loss is 5.5703801345825195 and perplexity is 262.53387848060095
At time: 243.53281378746033 and batch: 750, loss is 5.551058549880981 and perplexity is 257.509998861752
At time: 244.24412059783936 and batch: 800, loss is 5.513731908798218 and perplexity is 248.0751957307344
At time: 244.95900011062622 and batch: 850, loss is 5.512373142242431 and perplexity is 247.73834835175703
At time: 245.67836570739746 and batch: 900, loss is 5.549081573486328 and perplexity is 257.0014105717858
At time: 246.39049363136292 and batch: 950, loss is 5.531292333602905 and perplexity is 252.46997571929708
At time: 247.1030056476593 and batch: 1000, loss is 5.543608160018921 and perplexity is 255.59857823292654
At time: 247.81853032112122 and batch: 1050, loss is 5.51324836730957 and perplexity is 247.95527007814516
At time: 248.5628125667572 and batch: 1100, loss is 5.505981454849243 and perplexity is 246.15993201934538
At time: 249.27768516540527 and batch: 1150, loss is 5.51808382987976 and perplexity is 249.1571519913562
At time: 249.99198460578918 and batch: 1200, loss is 5.5162326526641845 and perplexity is 248.696344598159
At time: 250.70472025871277 and batch: 1250, loss is 5.528860111236572 and perplexity is 251.85665876148641
At time: 251.4205493927002 and batch: 1300, loss is 5.498269891738891 and perplexity is 244.2689547347497
At time: 252.13137817382812 and batch: 1350, loss is 5.489019184112549 and perplexity is 242.0197136282238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.382525227864583 and perplexity of 217.5709986560921
Finished 12 epochs...
Completing Train Step...
At time: 254.6415500640869 and batch: 50, loss is 5.553981552124023 and perplexity is 258.2638023139171
At time: 255.3539834022522 and batch: 100, loss is 5.578313570022583 and perplexity is 264.62495781019237
At time: 256.0752522945404 and batch: 150, loss is 5.5274975299835205 and perplexity is 251.51371729566753
At time: 256.78653836250305 and batch: 200, loss is 5.519243783950806 and perplexity is 249.4463305286255
At time: 257.4978759288788 and batch: 250, loss is 5.555276508331299 and perplexity is 258.59845926411975
At time: 258.2090585231781 and batch: 300, loss is 5.551800060272217 and perplexity is 257.7010160136042
At time: 258.92060017585754 and batch: 350, loss is 5.542403898239136 and perplexity is 255.2909559002795
At time: 259.63282680511475 and batch: 400, loss is 5.581510143280029 and perplexity is 265.47220429487294
At time: 260.3430802822113 and batch: 450, loss is 5.557817535400391 and perplexity is 259.25640051838656
At time: 261.0554316043854 and batch: 500, loss is 5.579473142623901 and perplexity is 264.93198763817924
At time: 261.7670953273773 and batch: 550, loss is 5.550026950836181 and perplexity is 257.24448876642646
At time: 262.47748947143555 and batch: 600, loss is 5.513597421646118 and perplexity is 248.04183504749585
At time: 263.1889410018921 and batch: 650, loss is 5.552637271881103 and perplexity is 257.9168566353624
At time: 263.896986246109 and batch: 700, loss is 5.553289861679077 and perplexity is 258.0852254766469
At time: 264.6085021495819 and batch: 750, loss is 5.532365579605102 and perplexity is 252.74108356809694
At time: 265.3189721107483 and batch: 800, loss is 5.500167579650879 and perplexity is 244.73294108882834
At time: 266.0367362499237 and batch: 850, loss is 5.498116283416748 and perplexity is 244.23143587213983
At time: 266.7791576385498 and batch: 900, loss is 5.535010070800781 and perplexity is 253.41033966928518
At time: 267.4908883571625 and batch: 950, loss is 5.5164293193817135 and perplexity is 248.74525970174136
At time: 268.2021291255951 and batch: 1000, loss is 5.529773626327515 and perplexity is 252.08683874045443
At time: 268.91394209861755 and batch: 1050, loss is 5.500308542251587 and perplexity is 244.76744171227494
At time: 269.62902998924255 and batch: 1100, loss is 5.493194904327392 and perplexity is 243.03243318403733
At time: 270.3411853313446 and batch: 1150, loss is 5.510102186203003 and perplexity is 247.17638379328918
At time: 271.0546598434448 and batch: 1200, loss is 5.502154922485351 and perplexity is 245.21979295620926
At time: 271.76630306243896 and batch: 1250, loss is 5.516167945861817 and perplexity is 248.68025277357034
At time: 272.47881865501404 and batch: 1300, loss is 5.4864481163024905 and perplexity is 241.39826377031275
At time: 273.19138979911804 and batch: 1350, loss is 5.473947696685791 and perplexity is 238.399466340909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.3749092610677085 and perplexity of 215.9202790461109
Finished 13 epochs...
Completing Train Step...
At time: 275.6828992366791 and batch: 50, loss is 5.5379945755004885 and perplexity is 254.16777373953153
At time: 276.42748761177063 and batch: 100, loss is 5.564215803146363 and perplexity is 260.9205104170218
At time: 277.1460633277893 and batch: 150, loss is 5.514793510437012 and perplexity is 248.33869260467617
At time: 277.85991859436035 and batch: 200, loss is 5.505154790878296 and perplexity is 245.9565245588474
At time: 278.5758376121521 and batch: 250, loss is 5.541070899963379 and perplexity is 254.95088020676093
At time: 279.28265380859375 and batch: 300, loss is 5.5383857727050785 and perplexity is 254.26722291296636
At time: 279.99423480033875 and batch: 350, loss is 5.5306566619873045 and perplexity is 252.3095387199667
At time: 280.7133755683899 and batch: 400, loss is 5.571102027893066 and perplexity is 262.72346835486553
At time: 281.42371368408203 and batch: 450, loss is 5.547544536590576 and perplexity is 256.6066933466552
At time: 282.13620948791504 and batch: 500, loss is 5.5701266956329345 and perplexity is 262.4673506009424
At time: 282.8490426540375 and batch: 550, loss is 5.544034976959228 and perplexity is 255.70769532088914
At time: 283.5597913265228 and batch: 600, loss is 5.502200603485107 and perplexity is 245.23099509737207
At time: 284.27138113975525 and batch: 650, loss is 5.540301551818848 and perplexity is 254.75480965305678
At time: 285.0127818584442 and batch: 700, loss is 5.54486798286438 and perplexity is 255.92079008335983
At time: 285.7248523235321 and batch: 750, loss is 5.525684175491333 and perplexity is 251.0580470372809
At time: 286.4383432865143 and batch: 800, loss is 5.495364828109741 and perplexity is 243.56036762228067
At time: 287.14932656288147 and batch: 850, loss is 5.486203517913818 and perplexity is 241.33922536461003
At time: 287.86287093162537 and batch: 900, loss is 5.526584463119507 and perplexity is 251.28417326503148
At time: 288.57551431655884 and batch: 950, loss is 5.504308586120605 and perplexity is 245.7484830128678
At time: 289.2884705066681 and batch: 1000, loss is 5.516042556762695 and perplexity is 248.6490729355525
At time: 290.00002884864807 and batch: 1050, loss is 5.490542058944702 and perplexity is 242.38856014124477
At time: 290.7260024547577 and batch: 1100, loss is 5.486299800872803 and perplexity is 241.3624633380396
At time: 291.4393982887268 and batch: 1150, loss is 5.497265882492066 and perplexity is 244.02382952006593
At time: 292.15012383461 and batch: 1200, loss is 5.494410266876221 and perplexity is 243.3279852665659
At time: 292.8621861934662 and batch: 1250, loss is 5.504899377822876 and perplexity is 245.89371207331519
At time: 293.57288360595703 and batch: 1300, loss is 5.474189805984497 and perplexity is 238.45719205620097
At time: 294.28495478630066 and batch: 1350, loss is 5.461425981521606 and perplexity is 235.4329080881071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.365873616536458 and perplexity of 213.97808784603063
Finished 14 epochs...
Completing Train Step...
At time: 296.75150299072266 and batch: 50, loss is 5.526155242919922 and perplexity is 251.17634016575775
At time: 297.4964163303375 and batch: 100, loss is 5.553838739395141 and perplexity is 258.22692158911843
At time: 298.2104916572571 and batch: 150, loss is 5.506561937332154 and perplexity is 246.3028650289177
At time: 298.9282217025757 and batch: 200, loss is 5.493359127044678 and perplexity is 243.07234790796576
At time: 299.64218616485596 and batch: 250, loss is 5.527523851394653 and perplexity is 251.52033757875316
At time: 300.3566870689392 and batch: 300, loss is 5.528353805541992 and perplexity is 251.72917457664542
At time: 301.072390794754 and batch: 350, loss is 5.519881715774536 and perplexity is 249.605511048944
At time: 301.78496956825256 and batch: 400, loss is 5.556320343017578 and perplexity is 258.8685342380083
At time: 302.50014328956604 and batch: 450, loss is 5.531532793045044 and perplexity is 252.53069180840134
At time: 303.21348905563354 and batch: 500, loss is 5.553935079574585 and perplexity is 258.2518004154776
At time: 303.96014857292175 and batch: 550, loss is 5.527106161117554 and perplexity is 251.41530191696822
At time: 304.6745676994324 and batch: 600, loss is 5.488130798339844 and perplexity is 241.80480223416927
At time: 305.3892242908478 and batch: 650, loss is 5.525317058563233 and perplexity is 250.96589629436158
At time: 306.10373401641846 and batch: 700, loss is 5.529634847640991 and perplexity is 252.05185688750802
At time: 306.81722235679626 and batch: 750, loss is 5.508692359924316 and perplexity is 246.82815356169115
At time: 307.53055357933044 and batch: 800, loss is 5.482041311264038 and perplexity is 240.33680921467862
At time: 308.2457060813904 and batch: 850, loss is 5.478427581787109 and perplexity is 239.46986439830278
At time: 308.95822834968567 and batch: 900, loss is 5.510048971176148 and perplexity is 247.16323064536329
At time: 309.672301530838 and batch: 950, loss is 5.488963088989258 and perplexity is 242.00613788331898
At time: 310.3856780529022 and batch: 1000, loss is 5.503630952835083 and perplexity is 245.58201207040764
At time: 311.1012279987335 and batch: 1050, loss is 5.477145433425903 and perplexity is 239.16302525279463
At time: 311.81523084640503 and batch: 1100, loss is 5.46953914642334 and perplexity is 237.35078359475773
At time: 312.5287389755249 and batch: 1150, loss is 5.484872417449951 and perplexity is 241.01819232079595
At time: 313.24121379852295 and batch: 1200, loss is 5.481003036499024 and perplexity is 240.0874030690258
At time: 313.9567427635193 and batch: 1250, loss is 5.489147157669067 and perplexity is 242.05068773361523
At time: 314.6725444793701 and batch: 1300, loss is 5.462616033554077 and perplexity is 235.7132522778885
At time: 315.3856782913208 and batch: 1350, loss is 5.447683973312378 and perplexity is 232.2197155570353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.361086018880209 and perplexity of 212.9560952509448
Finished 15 epochs...
Completing Train Step...
At time: 317.9116470813751 and batch: 50, loss is 5.509996652603149 and perplexity is 247.15029975610403
At time: 318.6235258579254 and batch: 100, loss is 5.5375590419769285 and perplexity is 254.05709925643262
At time: 319.3383595943451 and batch: 150, loss is 5.489104242324829 and perplexity is 242.04030026792046
At time: 320.05409717559814 and batch: 200, loss is 5.477004451751709 and perplexity is 239.1293100257595
At time: 320.7679786682129 and batch: 250, loss is 5.510494165420532 and perplexity is 247.27329079032373
At time: 321.4821889400482 and batch: 300, loss is 5.509858264923095 and perplexity is 247.1160995659932
At time: 322.2254858016968 and batch: 350, loss is 5.50582290649414 and perplexity is 246.12090686080066
At time: 322.93969345092773 and batch: 400, loss is 5.54387674331665 and perplexity is 255.66723696184445
At time: 323.65152502059937 and batch: 450, loss is 5.515197219848633 and perplexity is 248.43896951215416
At time: 324.36289262771606 and batch: 500, loss is 5.539239950180054 and perplexity is 254.4845050329384
At time: 325.0803382396698 and batch: 550, loss is 5.510576839447022 and perplexity is 247.2937347139958
At time: 325.79658818244934 and batch: 600, loss is 5.468347539901734 and perplexity is 237.06812329649156
At time: 326.5098412036896 and batch: 650, loss is 5.510888280868531 and perplexity is 247.37076422073233
At time: 327.22618651390076 and batch: 700, loss is 5.508549184799194 and perplexity is 246.79281643968014
At time: 327.9467921257019 and batch: 750, loss is 5.48918116569519 and perplexity is 242.05891953969976
At time: 328.6604263782501 and batch: 800, loss is 5.4631234073638915 and perplexity is 235.8328773534693
At time: 329.37901282310486 and batch: 850, loss is 5.4606360912323 and perplexity is 235.2470153473258
At time: 330.0937600135803 and batch: 900, loss is 5.492241525650025 and perplexity is 242.8008416590835
At time: 330.8082094192505 and batch: 950, loss is 5.473038673400879 and perplexity is 238.18285414257656
At time: 331.5221095085144 and batch: 1000, loss is 5.491511192321777 and perplexity is 242.6235808499107
At time: 332.2345621585846 and batch: 1050, loss is 5.455650253295898 and perplexity is 234.0770309537473
At time: 332.94978308677673 and batch: 1100, loss is 5.453892478942871 and perplexity is 233.66593776248678
At time: 333.66557455062866 and batch: 1150, loss is 5.463784122467041 and perplexity is 235.98874718446143
At time: 334.3773639202118 and batch: 1200, loss is 5.46241828918457 and perplexity is 235.6666459176568
At time: 335.09099817276 and batch: 1250, loss is 5.471062965393067 and perplexity is 237.71273892844158
At time: 335.80764532089233 and batch: 1300, loss is 5.449962024688721 and perplexity is 232.74932701168987
At time: 336.524249792099 and batch: 1350, loss is 5.436658620834351 and perplexity is 229.67347374046997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.348647867838542 and perplexity of 210.32372005042478
Finished 16 epochs...
Completing Train Step...
At time: 339.06701159477234 and batch: 50, loss is 5.493102312088013 and perplexity is 243.0099313085725
At time: 339.7783534526825 and batch: 100, loss is 5.51805269241333 and perplexity is 249.14939398968312
At time: 340.54889464378357 and batch: 150, loss is 5.472507162094116 and perplexity is 238.05629090039943
At time: 341.2605586051941 and batch: 200, loss is 5.459349021911621 and perplexity is 234.94443089644335
At time: 341.9763123989105 and batch: 250, loss is 5.494076461791992 and perplexity is 243.2467747029523
At time: 342.6912908554077 and batch: 300, loss is 5.493512697219849 and perplexity is 243.10967943744467
At time: 343.41044068336487 and batch: 350, loss is 5.488348150253296 and perplexity is 241.85736468268428
At time: 344.12374544143677 and batch: 400, loss is 5.529022607803345 and perplexity is 251.89758792918866
At time: 344.8428258895874 and batch: 450, loss is 5.496447620391845 and perplexity is 243.8242357399684
At time: 345.55639266967773 and batch: 500, loss is 5.524328927993775 and perplexity is 250.7180317018002
At time: 346.27737736701965 and batch: 550, loss is 5.50124981880188 and perplexity is 244.99794403162912
At time: 346.99359369277954 and batch: 600, loss is 5.460707311630249 and perplexity is 235.26377033001634
At time: 347.7096531391144 and batch: 650, loss is 5.496375360488892 and perplexity is 243.8066176609041
At time: 348.42490315437317 and batch: 700, loss is 5.494168634414673 and perplexity is 243.26919642945415
At time: 349.1379108428955 and batch: 750, loss is 5.4762962055206295 and perplexity is 238.96000755418885
At time: 349.85236501693726 and batch: 800, loss is 5.448120460510254 and perplexity is 232.32109861456416
At time: 350.56723618507385 and batch: 850, loss is 5.448165493011475 and perplexity is 232.3315608502896
At time: 351.29003834724426 and batch: 900, loss is 5.481879854202271 and perplexity is 240.29800827205565
At time: 352.0055618286133 and batch: 950, loss is 5.460634288787841 and perplexity is 235.24659132802873
At time: 352.71880173683167 and batch: 1000, loss is 5.476162252426147 and perplexity is 238.92800026550498
At time: 353.4548029899597 and batch: 1050, loss is 5.442199945449829 and perplexity is 230.94970174517653
At time: 354.2059826850891 and batch: 1100, loss is 5.440108079910278 and perplexity is 230.4670909771316
At time: 354.95076060295105 and batch: 1150, loss is 5.452614974975586 and perplexity is 233.3676191921262
At time: 355.69153666496277 and batch: 1200, loss is 5.451637372970581 and perplexity is 233.13959001875008
At time: 356.40834069252014 and batch: 1250, loss is 5.458193788528442 and perplexity is 234.67317196053656
At time: 357.13566851615906 and batch: 1300, loss is 5.434901695251465 and perplexity is 229.27030880784437
At time: 357.8541407585144 and batch: 1350, loss is 5.423730382919311 and perplexity is 226.7233117179991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.340639241536459 and perplexity of 208.64604288571934
Finished 17 epochs...
Completing Train Step...
At time: 360.4270353317261 and batch: 50, loss is 5.4806969165802 and perplexity is 240.01391878076382
At time: 361.19123363494873 and batch: 100, loss is 5.509976406097412 and perplexity is 247.14529587679766
At time: 361.90252566337585 and batch: 150, loss is 5.46315673828125 and perplexity is 235.84073801061555
At time: 362.61823415756226 and batch: 200, loss is 5.448396902084351 and perplexity is 232.38533070256176
At time: 363.33106899261475 and batch: 250, loss is 5.480291070938111 and perplexity is 239.91652994154057
At time: 364.0474524497986 and batch: 300, loss is 5.482664394378662 and perplexity is 240.48660568530224
At time: 364.75995326042175 and batch: 350, loss is 5.47508770942688 and perplexity is 238.67139974432783
At time: 365.47343611717224 and batch: 400, loss is 5.515287790298462 and perplexity is 248.46147176038187
At time: 366.1826477050781 and batch: 450, loss is 5.489723072052002 and perplexity is 242.190128355156
At time: 366.89114451408386 and batch: 500, loss is 5.515967016220093 and perplexity is 248.63029055908973
At time: 367.60750937461853 and batch: 550, loss is 5.4901590919494625 and perplexity is 242.29575109522207
At time: 368.3176097869873 and batch: 600, loss is 5.44872631072998 and perplexity is 232.4618930490697
At time: 369.02848386764526 and batch: 650, loss is 5.488854942321777 and perplexity is 241.9799671411618
At time: 369.7365937232971 and batch: 700, loss is 5.4868360614776615 and perplexity is 241.4919312297818
At time: 370.4447581768036 and batch: 750, loss is 5.465770101547241 and perplexity is 236.4578815908975
At time: 371.1563470363617 and batch: 800, loss is 5.442305240631104 and perplexity is 230.97402091621038
At time: 371.8687393665314 and batch: 850, loss is 5.4371262550354 and perplexity is 229.78090202847795
At time: 372.57710456848145 and batch: 900, loss is 5.473362512588501 and perplexity is 238.2599995752506
At time: 373.285856962204 and batch: 950, loss is 5.4486658477783205 and perplexity is 232.44783814177242
At time: 373.99780559539795 and batch: 1000, loss is 5.463267278671265 and perplexity is 235.86680937871967
At time: 374.70681524276733 and batch: 1050, loss is 5.431908149719238 and perplexity is 228.5850039566473
At time: 375.41518473625183 and batch: 1100, loss is 5.431737356185913 and perplexity is 228.5459664499282
At time: 376.131290435791 and batch: 1150, loss is 5.444398822784424 and perplexity is 231.4580905472416
At time: 376.8454270362854 and batch: 1200, loss is 5.442519664764404 and perplexity is 231.023552630668
At time: 377.6108434200287 and batch: 1250, loss is 5.452187843322754 and perplexity is 233.2679617801559
At time: 378.32217025756836 and batch: 1300, loss is 5.426150455474853 and perplexity is 227.27266304931172
At time: 379.03322196006775 and batch: 1350, loss is 5.413750553131104 and perplexity is 224.4719046736631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.337044677734375 and perplexity of 207.89739770504443
Finished 18 epochs...
Completing Train Step...
At time: 381.52638387680054 and batch: 50, loss is 5.476928024291992 and perplexity is 239.11103467842858
At time: 382.29505228996277 and batch: 100, loss is 5.501067190170288 and perplexity is 244.95320447785392
At time: 383.0118329524994 and batch: 150, loss is 5.452014789581299 and perplexity is 233.2275973793143
At time: 383.72700357437134 and batch: 200, loss is 5.439298410415649 and perplexity is 230.28056432671096
At time: 384.43897891044617 and batch: 250, loss is 5.469581966400146 and perplexity is 237.36094716740658
At time: 385.15215253829956 and batch: 300, loss is 5.478602533340454 and perplexity is 239.5117636881246
At time: 385.8673779964447 and batch: 350, loss is 5.473772583007812 and perplexity is 238.35772298855255
At time: 386.5862171649933 and batch: 400, loss is 5.513175268173217 and perplexity is 247.93714542450465
At time: 387.29902839660645 and batch: 450, loss is 5.482153882980347 and perplexity is 240.36386586466224
At time: 388.0130805969238 and batch: 500, loss is 5.510718193054199 and perplexity is 247.32869304611498
At time: 388.72858715057373 and batch: 550, loss is 5.483780775070191 and perplexity is 240.7552302041746
At time: 389.44246768951416 and batch: 600, loss is 5.441673212051391 and perplexity is 230.8280848565585
At time: 390.15640807151794 and batch: 650, loss is 5.482032661437988 and perplexity is 240.3347303520764
At time: 390.86899423599243 and batch: 700, loss is 5.477839107513428 and perplexity is 239.32898400009304
At time: 391.58564710617065 and batch: 750, loss is 5.459192981719971 and perplexity is 234.90777298254739
At time: 392.2967040538788 and batch: 800, loss is 5.434824619293213 and perplexity is 229.25263826009018
At time: 393.01348996162415 and batch: 850, loss is 5.4284859561920165 and perplexity is 227.80407883627737
At time: 393.72500228881836 and batch: 900, loss is 5.462253770828247 and perplexity is 235.6278776175662
At time: 394.43835043907166 and batch: 950, loss is 5.440894327163696 and perplexity is 230.64836634870872
At time: 395.1513822078705 and batch: 1000, loss is 5.456842775344849 and perplexity is 234.35633948200112
At time: 395.91903257369995 and batch: 1050, loss is 5.424685735702514 and perplexity is 226.94001596288547
At time: 396.63961720466614 and batch: 1100, loss is 5.424838428497314 and perplexity is 226.97467071387288
At time: 397.3538501262665 and batch: 1150, loss is 5.434355220794678 and perplexity is 229.1450526681387
At time: 398.06994915008545 and batch: 1200, loss is 5.435600318908691 and perplexity is 229.43053843306348
At time: 398.7864441871643 and batch: 1250, loss is 5.44654691696167 and perplexity is 231.95581871592375
At time: 399.5003173351288 and batch: 1300, loss is 5.4211146068573 and perplexity is 226.13102928332816
At time: 400.2167820930481 and batch: 1350, loss is 5.409263715744019 and perplexity is 223.4669918654862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.330843505859375 and perplexity of 206.6121792582897
Finished 19 epochs...
Completing Train Step...
At time: 402.7298917770386 and batch: 50, loss is 5.469340934753418 and perplexity is 237.30374256177882
At time: 403.45090651512146 and batch: 100, loss is 5.494187889099121 and perplexity is 243.27388054616273
At time: 404.1648621559143 and batch: 150, loss is 5.445139350891114 and perplexity is 231.629555248192
At time: 404.8774530887604 and batch: 200, loss is 5.433827590942383 and perplexity is 229.0241807884555
At time: 405.59308195114136 and batch: 250, loss is 5.46495101928711 and perplexity is 236.2642824324601
At time: 406.30843901634216 and batch: 300, loss is 5.468075637817383 and perplexity is 237.0036727421512
At time: 407.02410888671875 and batch: 350, loss is 5.459574556350708 and perplexity is 234.9974249326475
At time: 407.7366096973419 and batch: 400, loss is 5.501045551300049 and perplexity is 244.94790402459554
At time: 408.45044469833374 and batch: 450, loss is 5.474833040237427 and perplexity is 238.6106252314357
At time: 409.16325330734253 and batch: 500, loss is 5.5030045986175535 and perplexity is 245.42823890466076
At time: 409.8772928714752 and batch: 550, loss is 5.4759171676635745 and perplexity is 238.869449828491
At time: 410.5900685787201 and batch: 600, loss is 5.436388921737671 and perplexity is 229.611539364269
At time: 411.30230593681335 and batch: 650, loss is 5.475665197372437 and perplexity is 238.80926940583154
At time: 412.01569414138794 and batch: 700, loss is 5.472539472579956 and perplexity is 238.06398273907848
At time: 412.73191809654236 and batch: 750, loss is 5.449813327789307 and perplexity is 232.71472048142817
At time: 413.44458532333374 and batch: 800, loss is 5.426542549133301 and perplexity is 227.3617926916738
At time: 414.18636536598206 and batch: 850, loss is 5.4239020347595215 and perplexity is 226.76223253199313
At time: 414.90005016326904 and batch: 900, loss is 5.455819854736328 and perplexity is 234.11673412213065
At time: 415.61953473091125 and batch: 950, loss is 5.432331018447876 and perplexity is 228.68168584708948
At time: 416.33467721939087 and batch: 1000, loss is 5.448641700744629 and perplexity is 232.44222528376048
At time: 417.0552644729614 and batch: 1050, loss is 5.4172719287872315 and perplexity is 225.2637479448312
At time: 417.7751181125641 and batch: 1100, loss is 5.412595252990723 and perplexity is 224.21272199640177
At time: 418.4903447628021 and batch: 1150, loss is 5.426898393630982 and perplexity is 227.44271253117185
At time: 419.20889949798584 and batch: 1200, loss is 5.42925009727478 and perplexity is 227.9782198174026
At time: 419.9286985397339 and batch: 1250, loss is 5.436462612152099 and perplexity is 229.62846015720473
At time: 420.6474449634552 and batch: 1300, loss is 5.410410861968995 and perplexity is 223.7234882729262
At time: 421.36344623565674 and batch: 1350, loss is 5.4023379039764405 and perplexity is 221.92464869789816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.318645426432291 and perplexity of 204.1072164129993
Finished 20 epochs...
Completing Train Step...
At time: 423.9357831478119 and batch: 50, loss is 5.460494146347046 and perplexity is 235.21362560653358
At time: 424.6540434360504 and batch: 100, loss is 5.4847407913208 and perplexity is 240.98647011686708
At time: 425.3684229850769 and batch: 150, loss is 5.433945693969727 and perplexity is 229.05123083485626
At time: 426.0894424915314 and batch: 200, loss is 5.426617603302002 and perplexity is 227.3788577824138
At time: 426.8042244911194 and batch: 250, loss is 5.456747188568115 and perplexity is 234.333939185506
At time: 427.52117013931274 and batch: 300, loss is 5.459565925598144 and perplexity is 234.99539673677234
At time: 428.2377414703369 and batch: 350, loss is 5.455067300796509 and perplexity is 233.94061492939764
At time: 428.95296597480774 and batch: 400, loss is 5.4929045295715335 and perplexity is 242.96187294553752
At time: 429.6649332046509 and batch: 450, loss is 5.466372127532959 and perplexity is 236.60027823908356
At time: 430.38067078590393 and batch: 500, loss is 5.4960558891296385 and perplexity is 243.72874086972956
At time: 431.0980348587036 and batch: 550, loss is 5.468413324356079 and perplexity is 237.08371920660386
At time: 431.8124985694885 and batch: 600, loss is 5.43056357383728 and perplexity is 228.27786060839247
At time: 432.533105134964 and batch: 650, loss is 5.468397359848023 and perplexity is 237.07993431187072
At time: 433.3147678375244 and batch: 700, loss is 5.462701778411866 and perplexity is 235.73346434371183
At time: 434.05358242988586 and batch: 750, loss is 5.4452527904510495 and perplexity is 231.65583269343023
At time: 434.7984390258789 and batch: 800, loss is 5.419407949447632 and perplexity is 225.7454302229866
At time: 435.53332471847534 and batch: 850, loss is 5.4107022190094 and perplexity is 223.788681183084
At time: 436.25194358825684 and batch: 900, loss is 5.444804391860962 and perplexity is 231.5519818296954
At time: 436.96950793266296 and batch: 950, loss is 5.425405321121215 and perplexity is 227.1033774585182
At time: 437.6886582374573 and batch: 1000, loss is 5.440457191467285 and perplexity is 230.54756374827141
At time: 438.4071214199066 and batch: 1050, loss is 5.410365180969238 and perplexity is 223.7132685937373
At time: 439.1258273124695 and batch: 1100, loss is 5.407316017150879 and perplexity is 223.03216910911138
At time: 439.84097933769226 and batch: 1150, loss is 5.419112234115601 and perplexity is 225.67868370760405
At time: 440.5552725791931 and batch: 1200, loss is 5.420193243026733 and perplexity is 225.92277628510016
At time: 441.26921916007996 and batch: 1250, loss is 5.426244611740112 and perplexity is 227.29406320192336
At time: 441.98341155052185 and batch: 1300, loss is 5.4031416606903075 and perplexity is 222.10309382789066
At time: 442.6987102031708 and batch: 1350, loss is 5.393103046417236 and perplexity is 219.8846402700025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.317417399088542 and perplexity of 203.8567210092701
Finished 21 epochs...
Completing Train Step...
At time: 445.2010316848755 and batch: 50, loss is 5.4539768028259275 and perplexity is 233.68564221246316
At time: 445.9523935317993 and batch: 100, loss is 5.479076671600342 and perplexity is 239.62535230520896
At time: 446.66841554641724 and batch: 150, loss is 5.429660406112671 and perplexity is 228.0717804889129
At time: 447.3806850910187 and batch: 200, loss is 5.418934211730957 and perplexity is 225.638511426056
At time: 448.0941643714905 and batch: 250, loss is 5.4464133644104 and perplexity is 231.92484249307523
At time: 448.8098201751709 and batch: 300, loss is 5.450438404083252 and perplexity is 232.86023040911866
At time: 449.52422285079956 and batch: 350, loss is 5.44905119895935 and perplexity is 232.53742945167804
At time: 450.2387070655823 and batch: 400, loss is 5.485561294555664 and perplexity is 241.18428143647253
At time: 450.9524257183075 and batch: 450, loss is 5.458634824752807 and perplexity is 234.77669415709914
At time: 451.72015953063965 and batch: 500, loss is 5.488284111022949 and perplexity is 241.84187681911698
At time: 452.4319784641266 and batch: 550, loss is 5.461937170028687 and perplexity is 235.55328945107345
At time: 453.1464252471924 and batch: 600, loss is 5.42098858833313 and perplexity is 226.10253438022943
At time: 453.85834431648254 and batch: 650, loss is 5.461171703338623 and perplexity is 235.37305024661808
At time: 454.57266497612 and batch: 700, loss is 5.455503091812134 and perplexity is 234.0425863650776
At time: 455.28759360313416 and batch: 750, loss is 5.4366796875 and perplexity is 229.67831224571518
At time: 456.0042014122009 and batch: 800, loss is 5.407757387161255 and perplexity is 223.13063054726993
At time: 456.72078347206116 and batch: 850, loss is 5.401420097351075 and perplexity is 221.72105822762398
At time: 457.4326994419098 and batch: 900, loss is 5.435112400054932 and perplexity is 229.31862225296112
At time: 458.1477553844452 and batch: 950, loss is 5.419655933380127 and perplexity is 225.80141840431375
At time: 458.86087918281555 and batch: 1000, loss is 5.431402606964111 and perplexity is 228.4694736691769
At time: 459.57755422592163 and batch: 1050, loss is 5.397730770111084 and perplexity is 220.90456377156224
At time: 460.2964713573456 and batch: 1100, loss is 5.389620819091797 and perplexity is 219.12028357147727
At time: 461.0186138153076 and batch: 1150, loss is 5.396381349563598 and perplexity is 220.6066716502537
At time: 461.72983837127686 and batch: 1200, loss is 5.391320829391479 and perplexity is 219.49310712256712
At time: 462.4448492527008 and batch: 1250, loss is 5.400384616851807 and perplexity is 221.49158922139628
At time: 463.1586015224457 and batch: 1300, loss is 5.378668746948242 and perplexity is 216.73355608038273
At time: 463.87346029281616 and batch: 1350, loss is 5.371487474441528 and perplexity is 215.18270854704565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.308824462890625 and perplexity of 202.11249794344306
Finished 22 epochs...
Completing Train Step...
At time: 466.35939621925354 and batch: 50, loss is 5.438741722106934 and perplexity is 230.15240550439464
At time: 467.1044716835022 and batch: 100, loss is 5.462750692367553 and perplexity is 235.74499528195042
At time: 467.81780910491943 and batch: 150, loss is 5.411776123046875 and perplexity is 224.02913784192683
At time: 468.5310072898865 and batch: 200, loss is 5.403593225479126 and perplexity is 222.203410412565
At time: 469.24458622932434 and batch: 250, loss is 5.431745090484619 and perplexity is 228.5477340995366
At time: 469.9888525009155 and batch: 300, loss is 5.43088394165039 and perplexity is 228.3510052033384
At time: 470.7067756652832 and batch: 350, loss is 5.429505825042725 and perplexity is 228.03652763384264
At time: 471.4198429584503 and batch: 400, loss is 5.468741512298584 and perplexity is 237.1615399938352
At time: 472.13578963279724 and batch: 450, loss is 5.441142044067383 and perplexity is 230.70550892515982
At time: 472.84936904907227 and batch: 500, loss is 5.474334964752197 and perplexity is 238.49180872074447
At time: 473.5641140937805 and batch: 550, loss is 5.446240816116333 and perplexity is 231.88482770949187
At time: 474.27740120887756 and batch: 600, loss is 5.401305046081543 and perplexity is 221.69555040577475
At time: 474.99518156051636 and batch: 650, loss is 5.437064476013184 and perplexity is 229.766706827514
At time: 475.7107746601105 and batch: 700, loss is 5.4313798427581785 and perplexity is 228.464272802226
At time: 476.4252059459686 and batch: 750, loss is 5.408237886428833 and perplexity is 223.23787041409756
At time: 477.14005517959595 and batch: 800, loss is 5.3861916065216064 and perplexity is 218.37016044159256
At time: 477.8537127971649 and batch: 850, loss is 5.379783964157104 and perplexity is 216.97539589876018
At time: 478.57022047042847 and batch: 900, loss is 5.418680353164673 and perplexity is 225.5812384269751
At time: 479.28389835357666 and batch: 950, loss is 5.401613788604736 and perplexity is 221.7640078167009
At time: 479.9985649585724 and batch: 1000, loss is 5.412861261367798 and perplexity is 224.2723723921003
At time: 480.7094964981079 and batch: 1050, loss is 5.372988605499268 and perplexity is 215.5059685610549
At time: 481.42218542099 and batch: 1100, loss is 5.361223430633545 and perplexity is 212.98535993198632
At time: 482.1392560005188 and batch: 1150, loss is 5.374287881851196 and perplexity is 215.78615234837497
At time: 482.8530571460724 and batch: 1200, loss is 5.373222999572754 and perplexity is 215.55648780336062
At time: 483.57194089889526 and batch: 1250, loss is 5.3875979137420655 and perplexity is 218.67747201154674
At time: 484.2867057323456 and batch: 1300, loss is 5.364678039550781 and perplexity is 213.7224134384367
At time: 485.00097250938416 and batch: 1350, loss is 5.35948353767395 and perplexity is 212.6151103944282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.311040852864584 and perplexity of 202.56095485161146
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 487.51906657218933 and batch: 50, loss is 5.403272914886474 and perplexity is 222.13224770417966
At time: 488.23427152633667 and batch: 100, loss is 5.397180051803589 and perplexity is 220.78294107708
At time: 488.97751545906067 and batch: 150, loss is 5.358916339874267 and perplexity is 212.49454976572702
At time: 489.6897075176239 and batch: 200, loss is 5.346727695465088 and perplexity is 209.92024974393814
At time: 490.40347051620483 and batch: 250, loss is 5.3583480167388915 and perplexity is 212.37381850738714
At time: 491.1220896244049 and batch: 300, loss is 5.357318630218506 and perplexity is 212.15531624227046
At time: 491.8339445590973 and batch: 350, loss is 5.353889255523682 and perplexity is 211.4290022825583
At time: 492.5504846572876 and batch: 400, loss is 5.385705862045288 and perplexity is 218.26411410016274
At time: 493.2640824317932 and batch: 450, loss is 5.356388244628906 and perplexity is 211.95802178747232
At time: 493.9844934940338 and batch: 500, loss is 5.389764451980591 and perplexity is 219.15175871117856
At time: 494.7013747692108 and batch: 550, loss is 5.359565048217774 and perplexity is 212.6324414740249
At time: 495.41585636138916 and batch: 600, loss is 5.311171827316284 and perplexity is 202.58748689908126
At time: 496.1337025165558 and batch: 650, loss is 5.340063819885254 and perplexity is 208.52601797098373
At time: 496.8471431732178 and batch: 700, loss is 5.33607177734375 and perplexity is 207.6952326048061
At time: 497.5622832775116 and batch: 750, loss is 5.3185885047912596 and perplexity is 204.09559862594952
At time: 498.28148126602173 and batch: 800, loss is 5.3030229091644285 and perplexity is 200.94332621352405
At time: 498.9952645301819 and batch: 850, loss is 5.285817108154297 and perplexity is 197.51550906199864
At time: 499.7103464603424 and batch: 900, loss is 5.323614654541015 and perplexity is 205.12399594240458
At time: 500.4248025417328 and batch: 950, loss is 5.304239444732666 and perplexity is 201.18792967129903
At time: 501.14143466949463 and batch: 1000, loss is 5.314632806777954 and perplexity is 203.28985276625062
At time: 501.8580400943756 and batch: 1050, loss is 5.263718738555908 and perplexity is 193.19861220639555
At time: 502.58117413520813 and batch: 1100, loss is 5.2511056041717525 and perplexity is 190.77707583331608
At time: 503.29761958122253 and batch: 1150, loss is 5.2632372951507564 and perplexity is 193.1056203956076
At time: 504.0172562599182 and batch: 1200, loss is 5.258239145278931 and perplexity is 192.14285758445996
At time: 504.73109126091003 and batch: 1250, loss is 5.278800029754638 and perplexity is 196.1343786670563
At time: 505.4479866027832 and batch: 1300, loss is 5.262334957122802 and perplexity is 192.9314524419144
At time: 506.1669647693634 and batch: 1350, loss is 5.257996034622193 and perplexity is 192.09615128579455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.23309326171875 and perplexity of 187.37149711257862
Finished 24 epochs...
Completing Train Step...
At time: 508.7202031612396 and batch: 50, loss is 5.347185506820678 and perplexity is 210.01637562011928
At time: 509.42976236343384 and batch: 100, loss is 5.352446384429932 and perplexity is 211.1241574655591
At time: 510.15914273262024 and batch: 150, loss is 5.318091659545899 and perplexity is 203.99421988503133
At time: 510.876727104187 and batch: 200, loss is 5.307842073440551 and perplexity is 201.9140422540838
At time: 511.5880615711212 and batch: 250, loss is 5.325219974517823 and perplexity is 205.4535500399284
At time: 512.3002209663391 and batch: 300, loss is 5.327161502838135 and perplexity is 205.85283140845667
At time: 513.0107715129852 and batch: 350, loss is 5.330000400543213 and perplexity is 206.43805684365424
At time: 513.7204654216766 and batch: 400, loss is 5.363080930709839 and perplexity is 213.38134791430662
At time: 514.437842130661 and batch: 450, loss is 5.33090238571167 and perplexity is 206.6243449110392
At time: 515.1539561748505 and batch: 500, loss is 5.369524660110474 and perplexity is 214.7607590826107
At time: 515.871675491333 and batch: 550, loss is 5.337093753814697 and perplexity is 207.90760074479354
At time: 516.5823752880096 and batch: 600, loss is 5.289770812988281 and perplexity is 198.297972881181
At time: 517.2930889129639 and batch: 650, loss is 5.3214238834381105 and perplexity is 204.6751081043692
At time: 518.0068001747131 and batch: 700, loss is 5.318470392227173 and perplexity is 204.07149379504673
At time: 518.7260358333588 and batch: 750, loss is 5.301524667739868 and perplexity is 200.6424900171024
At time: 519.4380657672882 and batch: 800, loss is 5.285815801620483 and perplexity is 197.51525100147592
At time: 520.1470332145691 and batch: 850, loss is 5.2725209045410155 and perplexity is 194.9066848001969
At time: 520.8571870326996 and batch: 900, loss is 5.3115236186981205 and perplexity is 202.6587679683381
At time: 521.5722889900208 and batch: 950, loss is 5.294603357315063 and perplexity is 199.2585758337633
At time: 522.2867648601532 and batch: 1000, loss is 5.30770055770874 and perplexity is 201.88547026237208
At time: 523.0002892017365 and batch: 1050, loss is 5.258209810256958 and perplexity is 192.1372211521837
At time: 523.7125232219696 and batch: 1100, loss is 5.247607793807983 and perplexity is 190.11093948876498
At time: 524.423424243927 and batch: 1150, loss is 5.2628548717498775 and perplexity is 193.03178640634954
At time: 525.1871902942657 and batch: 1200, loss is 5.254700536727905 and perplexity is 191.46414079034554
At time: 525.8980107307434 and batch: 1250, loss is 5.279052686691284 and perplexity is 196.18393963903932
At time: 526.611560344696 and batch: 1300, loss is 5.261827278137207 and perplexity is 192.83353005652103
At time: 527.3235125541687 and batch: 1350, loss is 5.250782022476196 and perplexity is 190.71535385023515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.226678466796875 and perplexity of 186.1733942848153
Finished 25 epochs...
Completing Train Step...
At time: 529.8485627174377 and batch: 50, loss is 5.334299917221069 and perplexity is 207.32755154038497
At time: 530.6224157810211 and batch: 100, loss is 5.342250270843506 and perplexity is 208.98244868264717
At time: 531.3366882801056 and batch: 150, loss is 5.307908124923706 and perplexity is 201.92737941650932
At time: 532.0544533729553 and batch: 200, loss is 5.297007389068604 and perplexity is 199.7381760333051
At time: 532.772536277771 and batch: 250, loss is 5.314350891113281 and perplexity is 203.2325502499047
At time: 533.4910750389099 and batch: 300, loss is 5.317826528549194 and perplexity is 203.94014186338788
At time: 534.2079689502716 and batch: 350, loss is 5.32321249961853 and perplexity is 205.04152090269972
At time: 534.924036026001 and batch: 400, loss is 5.354958705902099 and perplexity is 211.65523606039503
At time: 535.6418228149414 and batch: 450, loss is 5.322798023223877 and perplexity is 204.95655364203998
At time: 536.35617852211 and batch: 500, loss is 5.362904052734375 and perplexity is 213.34360879119407
At time: 537.0694103240967 and batch: 550, loss is 5.3286318016052245 and perplexity is 206.1557191858766
At time: 537.7843050956726 and batch: 600, loss is 5.2825436019897465 and perplexity is 196.86999794423437
At time: 538.4988033771515 and batch: 650, loss is 5.315089836120605 and perplexity is 203.38278342843012
At time: 539.2123045921326 and batch: 700, loss is 5.312466697692871 and perplexity is 202.84998134599903
At time: 539.9284057617188 and batch: 750, loss is 5.295438776016235 and perplexity is 199.42510972746263
At time: 540.6465094089508 and batch: 800, loss is 5.280610399246216 and perplexity is 196.48977596559195
At time: 541.359881401062 and batch: 850, loss is 5.26656867980957 and perplexity is 193.750002242872
At time: 542.077390909195 and batch: 900, loss is 5.307458744049073 and perplexity is 201.83665750000844
At time: 542.7914991378784 and batch: 950, loss is 5.2915534400939945 and perplexity is 198.6517794815724
At time: 543.5054020881653 and batch: 1000, loss is 5.3022580909729005 and perplexity is 200.7896998577742
At time: 544.2777976989746 and batch: 1050, loss is 5.25417308807373 and perplexity is 191.36317991514238
At time: 544.9916257858276 and batch: 1100, loss is 5.244538536071778 and perplexity is 189.52833455671063
At time: 545.7054834365845 and batch: 1150, loss is 5.258487901687622 and perplexity is 192.1906602970374
At time: 546.4226896762848 and batch: 1200, loss is 5.250997047424317 and perplexity is 190.75636681855028
At time: 547.1362512111664 and batch: 1250, loss is 5.27454514503479 and perplexity is 195.301622393555
At time: 547.8516240119934 and batch: 1300, loss is 5.259673204421997 and perplexity is 192.41859947398686
At time: 548.5688600540161 and batch: 1350, loss is 5.244602499008178 and perplexity is 189.5404577332329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.226037190755208 and perplexity of 186.0540440197875
Finished 26 epochs...
Completing Train Step...
At time: 551.0381422042847 and batch: 50, loss is 5.3274612140655515 and perplexity is 205.91453705970162
At time: 551.7820570468903 and batch: 100, loss is 5.335856380462647 and perplexity is 207.6505005172321
At time: 552.4964165687561 and batch: 150, loss is 5.301904077529907 and perplexity is 200.71863018536192
At time: 553.2110123634338 and batch: 200, loss is 5.289910888671875 and perplexity is 198.32575155080022
At time: 553.9324865341187 and batch: 250, loss is 5.306245174407959 and perplexity is 201.5918632274947
At time: 554.647796869278 and batch: 300, loss is 5.312533626556396 and perplexity is 202.86355831905723
At time: 555.3625817298889 and batch: 350, loss is 5.318113441467285 and perplexity is 203.99866331948527
At time: 556.0791490077972 and batch: 400, loss is 5.348489618301391 and perplexity is 210.29043905249569
At time: 556.7979738712311 and batch: 450, loss is 5.316066045761108 and perplexity is 203.5814246042655
At time: 557.5129883289337 and batch: 500, loss is 5.3564255428314205 and perplexity is 211.96592758812858
At time: 558.2291903495789 and batch: 550, loss is 5.319939336776733 and perplexity is 204.37148378396404
At time: 558.9464907646179 and batch: 600, loss is 5.278818941116333 and perplexity is 196.1380878703049
At time: 559.6637442111969 and batch: 650, loss is 5.309838075637817 and perplexity is 202.31746560909585
At time: 560.3863453865051 and batch: 700, loss is 5.308877801895141 and perplexity is 202.1232787103988
At time: 561.0985066890717 and batch: 750, loss is 5.290112819671631 and perplexity is 198.3658037118386
At time: 561.8135931491852 and batch: 800, loss is 5.276140079498291 and perplexity is 195.61336421969477
At time: 562.581670999527 and batch: 850, loss is 5.2623895645141605 and perplexity is 192.94198821290604
At time: 563.2958760261536 and batch: 900, loss is 5.300453863143921 and perplexity is 200.4277561061973
At time: 564.0133991241455 and batch: 950, loss is 5.284738512039184 and perplexity is 197.30258445172112
At time: 564.7294311523438 and batch: 1000, loss is 5.299881134033203 and perplexity is 200.3129981614242
At time: 565.4425156116486 and batch: 1050, loss is 5.251076450347901 and perplexity is 190.7715140331265
At time: 566.1587443351746 and batch: 1100, loss is 5.239526863098145 and perplexity is 188.58085673231656
At time: 566.8738782405853 and batch: 1150, loss is 5.253808488845825 and perplexity is 191.29342176515246
At time: 567.5887186527252 and batch: 1200, loss is 5.249036169052124 and perplexity is 190.3826832782196
At time: 568.30846118927 and batch: 1250, loss is 5.271183567047119 and perplexity is 194.64620299766509
At time: 569.0229725837708 and batch: 1300, loss is 5.257962045669555 and perplexity is 192.08962224976486
At time: 569.7344105243683 and batch: 1350, loss is 5.242112770080566 and perplexity is 189.0691403423975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.223026529947917 and perplexity of 185.49474076008477
Finished 27 epochs...
Completing Train Step...
At time: 572.2500965595245 and batch: 50, loss is 5.319825344085693 and perplexity is 204.34818825634076
At time: 572.9639799594879 and batch: 100, loss is 5.3292862033844 and perplexity is 206.2906720071661
At time: 573.6790196895599 and batch: 150, loss is 5.2918649101257325 and perplexity is 198.71366319459304
At time: 574.3927912712097 and batch: 200, loss is 5.281066188812256 and perplexity is 196.5793543682104
At time: 575.1099956035614 and batch: 250, loss is 5.297513484954834 and perplexity is 199.83928828660822
At time: 575.8217060565948 and batch: 300, loss is 5.302282133102417 and perplexity is 200.79452732777486
At time: 576.5402679443359 and batch: 350, loss is 5.308614807128906 and perplexity is 202.0701283354053
At time: 577.2566857337952 and batch: 400, loss is 5.338558673858643 and perplexity is 208.21239194927892
At time: 577.974371433258 and batch: 450, loss is 5.303846292495727 and perplexity is 201.10884773334277
At time: 578.6891918182373 and batch: 500, loss is 5.345188179016113 and perplexity is 209.59732270600895
At time: 579.4044840335846 and batch: 550, loss is 5.307075386047363 and perplexity is 201.7592966317189
At time: 580.1199553012848 and batch: 600, loss is 5.264520168304443 and perplexity is 193.35350938290537
At time: 580.8954224586487 and batch: 650, loss is 5.295044202804565 and perplexity is 199.34643744343836
At time: 581.6125221252441 and batch: 700, loss is 5.293800382614136 and perplexity is 199.09864045904928
At time: 582.7223281860352 and batch: 750, loss is 5.273534030914306 and perplexity is 195.10424996523943
At time: 583.4614686965942 and batch: 800, loss is 5.259926700592041 and perplexity is 192.46738303496053
At time: 584.2157647609711 and batch: 850, loss is 5.249342079162598 and perplexity is 190.44093217490254
At time: 584.9533219337463 and batch: 900, loss is 5.284447679519653 and perplexity is 197.24521078744309
At time: 585.6810166835785 and batch: 950, loss is 5.2682662677764895 and perplexity is 194.0791892481653
At time: 586.4027895927429 and batch: 1000, loss is 5.27694655418396 and perplexity is 195.77118507682556
At time: 587.1266865730286 and batch: 1050, loss is 5.227406597137451 and perplexity is 186.30900214590986
At time: 587.8701691627502 and batch: 1100, loss is 5.216804285049438 and perplexity is 184.34413045752012
At time: 588.5950214862823 and batch: 1150, loss is 5.233421001434326 and perplexity is 187.43291625794626
At time: 589.3108532428741 and batch: 1200, loss is 5.226533689498901 and perplexity is 186.1464425548826
At time: 590.0304908752441 and batch: 1250, loss is 5.24889009475708 and perplexity is 190.3548752930365
At time: 590.745459318161 and batch: 1300, loss is 5.2390159034729 and perplexity is 188.48452414156046
At time: 591.4678769111633 and batch: 1350, loss is 5.219784832000732 and perplexity is 184.8943964329674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.206243896484375 and perplexity of 182.40762791666614
Finished 28 epochs...
Completing Train Step...
At time: 594.0174453258514 and batch: 50, loss is 5.295013809204102 and perplexity is 199.34037867953927
At time: 594.7369182109833 and batch: 100, loss is 5.3098941421508785 and perplexity is 202.32880916191777
At time: 595.462553024292 and batch: 150, loss is 5.271617498397827 and perplexity is 194.7306844156843
At time: 596.1781725883484 and batch: 200, loss is 5.255364007949829 and perplexity is 191.59121388780056
At time: 596.8991055488586 and batch: 250, loss is 5.273565654754639 and perplexity is 195.11042000844816
At time: 597.6176171302795 and batch: 300, loss is 5.276865673065186 and perplexity is 195.75535152467916
At time: 598.3339283466339 and batch: 350, loss is 5.283226985931396 and perplexity is 197.00458172038006
At time: 599.0486993789673 and batch: 400, loss is 5.311346492767334 and perplexity is 202.62287502430905
At time: 599.8170611858368 and batch: 450, loss is 5.275719079971314 and perplexity is 195.5310284187705
At time: 600.5388262271881 and batch: 500, loss is 5.316565628051758 and perplexity is 203.68315568811116
At time: 601.257871389389 and batch: 550, loss is 5.284593563079834 and perplexity is 197.27398772001112
At time: 601.9870018959045 and batch: 600, loss is 5.240721797943115 and perplexity is 188.80633325716923
At time: 602.7078988552094 and batch: 650, loss is 5.268960456848145 and perplexity is 194.21396367443174
At time: 603.4287216663361 and batch: 700, loss is 5.269504842758178 and perplexity is 194.31971980324926
At time: 604.1473174095154 and batch: 750, loss is 5.250108089447021 and perplexity is 190.5868677744729
At time: 604.8697628974915 and batch: 800, loss is 5.24132887840271 and perplexity is 188.92098869175308
At time: 605.5878322124481 and batch: 850, loss is 5.2278519630432125 and perplexity is 186.39199630341434
At time: 606.3035981655121 and batch: 900, loss is 5.267966899871826 and perplexity is 194.02109686387288
At time: 607.0228898525238 and batch: 950, loss is 5.250544767379761 and perplexity is 190.6701110278231
At time: 607.7387390136719 and batch: 1000, loss is 5.254279651641846 and perplexity is 191.38357334497908
At time: 608.4556844234467 and batch: 1050, loss is 5.209978847503662 and perplexity is 183.09018533828322
At time: 609.1740794181824 and batch: 1100, loss is 5.199600334167481 and perplexity is 181.1998080293203
At time: 609.8935451507568 and batch: 1150, loss is 5.213180150985718 and perplexity is 183.67725177367723
At time: 610.6077802181244 and batch: 1200, loss is 5.207833480834961 and perplexity is 182.69781080141382
At time: 611.3231573104858 and batch: 1250, loss is 5.234949045181274 and perplexity is 187.71954088532078
At time: 612.0426440238953 and batch: 1300, loss is 5.229094285964965 and perplexity is 186.6236992479927
At time: 612.757043838501 and batch: 1350, loss is 5.207569046020508 and perplexity is 182.64950552679318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.2048331705729165 and perplexity of 182.15048217324448
Finished 29 epochs...
Completing Train Step...
At time: 615.2819356918335 and batch: 50, loss is 5.280225238800049 and perplexity is 196.41411044843136
At time: 616.028489112854 and batch: 100, loss is 5.2954972553253175 and perplexity is 199.43677231109976
At time: 616.7429940700531 and batch: 150, loss is 5.258936271667481 and perplexity is 192.2768521409975
At time: 617.4615817070007 and batch: 200, loss is 5.2410766220092775 and perplexity is 188.87333817482846
At time: 618.1768002510071 and batch: 250, loss is 5.256465454101562 and perplexity is 191.8023575533857
At time: 618.8887155056 and batch: 300, loss is 5.257249250411987 and perplexity is 191.95275046457195
At time: 619.6056799888611 and batch: 350, loss is 5.271241283416748 and perplexity is 194.65743759407104
At time: 620.3211579322815 and batch: 400, loss is 5.295029048919678 and perplexity is 199.34341659336152
At time: 621.0390758514404 and batch: 450, loss is 5.262492418289185 and perplexity is 192.96183404534628
At time: 621.7541697025299 and batch: 500, loss is 5.307095918655396 and perplexity is 201.7634393188035
At time: 622.466881275177 and batch: 550, loss is 5.271573619842529 and perplexity is 194.72214010203746
At time: 623.1849172115326 and batch: 600, loss is 5.230444116592407 and perplexity is 186.87577972774508
At time: 623.8990743160248 and batch: 650, loss is 5.255432205200195 and perplexity is 191.60428032732455
At time: 624.6153750419617 and batch: 700, loss is 5.2617714309692385 and perplexity is 192.82276115068728
At time: 625.3363211154938 and batch: 750, loss is 5.242212285995484 and perplexity is 189.08795666712768
At time: 626.0550951957703 and batch: 800, loss is 5.2310254192352295 and perplexity is 186.98444269235466
At time: 626.7724888324738 and batch: 850, loss is 5.214191083908081 and perplexity is 183.8630310439528
At time: 627.4909303188324 and batch: 900, loss is 5.256335124969483 and perplexity is 191.77736174747122
At time: 628.2074255943298 and batch: 950, loss is 5.241129951477051 and perplexity is 188.88341095801562
At time: 628.9378876686096 and batch: 1000, loss is 5.242893762588501 and perplexity is 189.21685960079526
At time: 629.6551735401154 and batch: 1050, loss is 5.198254327774048 and perplexity is 180.95607599845818
At time: 630.3686261177063 and batch: 1100, loss is 5.190884027481079 and perplexity is 179.6272782130319
At time: 631.0828628540039 and batch: 1150, loss is 5.204877767562866 and perplexity is 182.1586057176088
At time: 631.7971620559692 and batch: 1200, loss is 5.201644115447998 and perplexity is 181.5705195025489
At time: 632.564014673233 and batch: 1250, loss is 5.228752384185791 and perplexity is 186.55990317979973
At time: 633.28111743927 and batch: 1300, loss is 5.2213201904296875 and perplexity is 185.17849364271106
At time: 633.9925556182861 and batch: 1350, loss is 5.199264297485351 and perplexity is 181.13892847648202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.203428548177083 and perplexity of 181.89480913064904
Finished 30 epochs...
Completing Train Step...
At time: 636.4697554111481 and batch: 50, loss is 5.274319152832032 and perplexity is 195.25749073660094
At time: 637.2155554294586 and batch: 100, loss is 5.288759717941284 and perplexity is 198.09757611013495
At time: 637.933522939682 and batch: 150, loss is 5.250057773590088 and perplexity is 190.57727847414952
At time: 638.6475503444672 and batch: 200, loss is 5.2320661449432375 and perplexity is 187.17914350635988
At time: 639.3663182258606 and batch: 250, loss is 5.248355436325073 and perplexity is 190.2531276564272
At time: 640.0866348743439 and batch: 300, loss is 5.249308891296387 and perplexity is 190.4346119516024
At time: 640.802980184555 and batch: 350, loss is 5.26554967880249 and perplexity is 193.55267135272774
At time: 641.5159885883331 and batch: 400, loss is 5.288378772735595 and perplexity is 198.02212616031804
At time: 642.2312679290771 and batch: 450, loss is 5.255169258117676 and perplexity is 191.55390516410563
At time: 642.9493534564972 and batch: 500, loss is 5.298562068939209 and perplexity is 200.04894646664886
At time: 643.6655099391937 and batch: 550, loss is 5.264093656539917 and perplexity is 193.27105942063037
At time: 644.3790607452393 and batch: 600, loss is 5.2251607704162595 and perplexity is 185.89105390582333
At time: 645.0939245223999 and batch: 650, loss is 5.248671503067016 and perplexity is 190.31326984660248
At time: 645.8062040805817 and batch: 700, loss is 5.255541381835937 and perplexity is 191.6252001800032
At time: 646.524005651474 and batch: 750, loss is 5.2346549987792965 and perplexity is 187.6643507443714
At time: 647.2407145500183 and batch: 800, loss is 5.221652822494507 and perplexity is 185.24010019300056
At time: 647.959418296814 and batch: 850, loss is 5.204421415328979 and perplexity is 182.07549619601807
At time: 648.6750688552856 and batch: 900, loss is 5.2460472106933596 and perplexity is 189.81448694627514
At time: 649.3916726112366 and batch: 950, loss is 5.229947891235351 and perplexity is 186.78307023152843
At time: 650.109194278717 and batch: 1000, loss is 5.2339028644561765 and perplexity is 187.52325501307325
At time: 650.854089975357 and batch: 1050, loss is 5.1907874202728275 and perplexity is 179.60992576135732
At time: 651.5767621994019 and batch: 1100, loss is 5.1838044261932374 and perplexity is 178.36007962597915
At time: 652.291909456253 and batch: 1150, loss is 5.196086339950561 and perplexity is 180.56419038422155
At time: 653.0048615932465 and batch: 1200, loss is 5.191661586761475 and perplexity is 179.7670033854811
At time: 653.7234869003296 and batch: 1250, loss is 5.21914891242981 and perplexity is 184.776855844863
At time: 654.4402987957001 and batch: 1300, loss is 5.213806304931641 and perplexity is 183.79229802422466
At time: 655.1580338478088 and batch: 1350, loss is 5.19240288734436 and perplexity is 179.90031417545265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.197218424479167 and perplexity of 180.76872006117316
Finished 31 epochs...
Completing Train Step...
At time: 657.6780753135681 and batch: 50, loss is 5.266068105697632 and perplexity is 193.6530402779107
At time: 658.3903243541718 and batch: 100, loss is 5.284325895309448 and perplexity is 197.22119089788188
At time: 659.1014325618744 and batch: 150, loss is 5.243732566833496 and perplexity is 189.37564209025626
At time: 659.8128669261932 and batch: 200, loss is 5.223529748916626 and perplexity is 185.58810872252562
At time: 660.5204634666443 and batch: 250, loss is 5.237741365432739 and perplexity is 188.24444647212096
At time: 661.237562417984 and batch: 300, loss is 5.237194757461548 and perplexity is 188.1415786738874
At time: 661.9522039890289 and batch: 350, loss is 5.254776659011841 and perplexity is 191.47871603277764
At time: 662.6690983772278 and batch: 400, loss is 5.276063241958618 and perplexity is 195.59833434749754
At time: 663.4164340496063 and batch: 450, loss is 5.241627588272094 and perplexity is 188.97742968493037
At time: 664.1623952388763 and batch: 500, loss is 5.287646770477295 and perplexity is 197.87722655666389
At time: 664.9099576473236 and batch: 550, loss is 5.250387363433838 and perplexity is 190.64010116187526
At time: 665.620760679245 and batch: 600, loss is 5.20858868598938 and perplexity is 182.8358372424118
At time: 666.3368105888367 and batch: 650, loss is 5.233118171691895 and perplexity is 187.3761645896729
At time: 667.0605680942535 and batch: 700, loss is 5.239168615341186 and perplexity is 188.51331016331275
At time: 667.783093214035 and batch: 750, loss is 5.220736427307129 and perplexity is 185.0704248133991
At time: 668.4973046779633 and batch: 800, loss is 5.209376239776612 and perplexity is 182.9798870145001
At time: 669.2655200958252 and batch: 850, loss is 5.190509157180786 and perplexity is 179.55995390103828
At time: 669.9785223007202 and batch: 900, loss is 5.229982442855835 and perplexity is 186.78952400077733
At time: 670.7073767185211 and batch: 950, loss is 5.2143095588684085 and perplexity is 183.88481549969222
At time: 671.4279615879059 and batch: 1000, loss is 5.22075041770935 and perplexity is 185.07301404119366
At time: 672.1411690711975 and batch: 1050, loss is 5.176887073516846 and perplexity is 177.1305574736843
At time: 672.8651323318481 and batch: 1100, loss is 5.1669150066375735 and perplexity is 175.3729776248574
At time: 673.5894255638123 and batch: 1150, loss is 5.180287532806396 and perplexity is 177.73390797633513
At time: 674.3015463352203 and batch: 1200, loss is 5.174375247955322 and perplexity is 176.6861947261825
At time: 675.0188546180725 and batch: 1250, loss is 5.203302450180054 and perplexity is 181.87187400561476
At time: 675.7491314411163 and batch: 1300, loss is 5.200878992080688 and perplexity is 181.4316487886518
At time: 676.4621906280518 and batch: 1350, loss is 5.177439270019531 and perplexity is 177.22839535842274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.183451334635417 and perplexity of 178.29711330470346
Finished 32 epochs...
Completing Train Step...
At time: 678.9814085960388 and batch: 50, loss is 5.251198511123658 and perplexity is 190.79480117331602
At time: 679.6964111328125 and batch: 100, loss is 5.268294172286987 and perplexity is 194.0846050085008
At time: 680.4130544662476 and batch: 150, loss is 5.230168924331665 and perplexity is 186.82436003491918
At time: 681.126601934433 and batch: 200, loss is 5.209376630783081 and perplexity is 182.9799585608336
At time: 681.8438441753387 and batch: 250, loss is 5.225675973892212 and perplexity is 185.98685029814226
At time: 682.5634186267853 and batch: 300, loss is 5.224611158370972 and perplexity is 185.78891401471384
At time: 683.2796001434326 and batch: 350, loss is 5.244066352844238 and perplexity is 189.4388635809989
At time: 683.9954750537872 and batch: 400, loss is 5.263842000961303 and perplexity is 193.22242779980868
At time: 684.7079780101776 and batch: 450, loss is 5.2292862892150875 and perplexity is 186.6595350449828
At time: 685.4263143539429 and batch: 500, loss is 5.273170652389527 and perplexity is 195.03336615031685
At time: 686.1462993621826 and batch: 550, loss is 5.239695987701416 and perplexity is 188.61275309204956
At time: 686.8630721569061 and batch: 600, loss is 5.200598793029785 and perplexity is 181.38081893442848
At time: 687.580367565155 and batch: 650, loss is 5.222595129013062 and perplexity is 185.41473541394154
At time: 688.3548386096954 and batch: 700, loss is 5.229755697250366 and perplexity is 186.74717509845763
At time: 689.0754368305206 and batch: 750, loss is 5.211492252349854 and perplexity is 183.36748469223022
At time: 689.7928805351257 and batch: 800, loss is 5.19905460357666 and perplexity is 181.10094872875368
At time: 690.5136799812317 and batch: 850, loss is 5.1800783920288085 and perplexity is 177.69674045537485
At time: 691.2374088764191 and batch: 900, loss is 5.225429315567016 and perplexity is 185.94098075042476
At time: 691.9564576148987 and batch: 950, loss is 5.207299928665162 and perplexity is 182.6003579884349
At time: 692.6778767108917 and batch: 1000, loss is 5.20987226486206 and perplexity is 183.07067214258086
At time: 693.3963446617126 and batch: 1050, loss is 5.1647599506378175 and perplexity is 174.99544598425857
At time: 694.1167104244232 and batch: 1100, loss is 5.157124347686768 and perplexity is 173.66433861139996
At time: 694.83460521698 and batch: 1150, loss is 5.16998254776001 and perplexity is 175.9117674028937
At time: 695.5499622821808 and batch: 1200, loss is 5.168463983535767 and perplexity is 175.64483681321016
At time: 696.2673749923706 and batch: 1250, loss is 5.196262855529785 and perplexity is 180.59606559002668
At time: 696.9816863536835 and batch: 1300, loss is 5.193849315643311 and perplexity is 180.16071536128828
At time: 697.6964814662933 and batch: 1350, loss is 5.174676885604859 and perplexity is 176.73949797339415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.179803873697916 and perplexity of 177.6479661378085
Finished 33 epochs...
Completing Train Step...
At time: 700.2332212924957 and batch: 50, loss is 5.243587589263916 and perplexity is 189.34818886002765
At time: 700.9877953529358 and batch: 100, loss is 5.2589801406860355 and perplexity is 192.28528732281194
At time: 701.7034568786621 and batch: 150, loss is 5.222253141403198 and perplexity is 185.3513367131468
At time: 702.4168498516083 and batch: 200, loss is 5.201763172149658 and perplexity is 181.59213797660612
At time: 703.1310369968414 and batch: 250, loss is 5.21792384147644 and perplexity is 184.55062968575413
At time: 703.8467800617218 and batch: 300, loss is 5.21756121635437 and perplexity is 184.48371912359474
At time: 704.5653145313263 and batch: 350, loss is 5.2337104606628415 and perplexity is 187.4871782982299
At time: 705.285459280014 and batch: 400, loss is 5.256213502883911 and perplexity is 191.7540388030914
At time: 706.00253033638 and batch: 450, loss is 5.219780693054199 and perplexity is 184.89363116653
At time: 706.7737069129944 and batch: 500, loss is 5.261443328857422 and perplexity is 192.7595059731936
At time: 707.4895069599152 and batch: 550, loss is 5.231617870330811 and perplexity is 187.09525465237678
At time: 708.2057597637177 and batch: 600, loss is 5.192131471633911 and perplexity is 179.85149302958345
At time: 708.9246973991394 and batch: 650, loss is 5.214452171325684 and perplexity is 183.91104163512864
At time: 709.6393444538116 and batch: 700, loss is 5.222005910873413 and perplexity is 185.30551786811697
At time: 710.3562190532684 and batch: 750, loss is 5.202072811126709 and perplexity is 181.64837468653914
At time: 711.0732035636902 and batch: 800, loss is 5.18955249786377 and perplexity is 179.38825833835028
At time: 711.7874205112457 and batch: 850, loss is 5.172525815963745 and perplexity is 176.3597276076064
At time: 712.5006928443909 and batch: 900, loss is 5.214159212112427 and perplexity is 183.85717109238308
At time: 713.2158257961273 and batch: 950, loss is 5.19399353981018 and perplexity is 180.18670076418042
At time: 713.9370930194855 and batch: 1000, loss is 5.198982372283935 and perplexity is 181.08786804553634
At time: 714.6512570381165 and batch: 1050, loss is 5.155597686767578 and perplexity is 173.39941432881417
At time: 715.3678364753723 and batch: 1100, loss is 5.14804048538208 and perplexity is 172.09393911725155
At time: 716.085394859314 and batch: 1150, loss is 5.158795795440674 and perplexity is 173.95485220161459
At time: 716.7976896762848 and batch: 1200, loss is 5.1609228515625 and perplexity is 174.32525773193257
At time: 717.5153958797455 and batch: 1250, loss is 5.186281595230103 and perplexity is 178.80245538619442
At time: 718.2301654815674 and batch: 1300, loss is 5.184991436004639 and perplexity is 178.5719204941785
At time: 718.9455831050873 and batch: 1350, loss is 5.161964206695557 and perplexity is 174.50688678765442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.172123616536458 and perplexity of 176.28881008861498
Finished 34 epochs...
Completing Train Step...
At time: 721.4701387882233 and batch: 50, loss is 5.234548959732056 and perplexity is 187.64445205045536
At time: 722.2136027812958 and batch: 100, loss is 5.251847629547119 and perplexity is 190.9186897987028
At time: 722.9283435344696 and batch: 150, loss is 5.212941541671753 and perplexity is 183.63342989900218
At time: 723.6514592170715 and batch: 200, loss is 5.1926132011413575 and perplexity is 179.93815367255257
At time: 724.3727984428406 and batch: 250, loss is 5.210924243927002 and perplexity is 183.26335999110097
At time: 725.1438794136047 and batch: 300, loss is 5.212776374816895 and perplexity is 183.60310224756935
At time: 725.8612008094788 and batch: 350, loss is 5.227534418106079 and perplexity is 186.33281786507
At time: 726.5758030414581 and batch: 400, loss is 5.249602794647217 and perplexity is 190.49058954776754
At time: 727.2940757274628 and batch: 450, loss is 5.214193868637085 and perplexity is 183.863543053381
At time: 728.0112826824188 and batch: 500, loss is 5.259455938339233 and perplexity is 192.3767979798161
At time: 728.7276895046234 and batch: 550, loss is 5.2278044891357425 and perplexity is 186.3831477570679
At time: 729.4464273452759 and batch: 600, loss is 5.18732759475708 and perplexity is 178.98958051928972
At time: 730.1629889011383 and batch: 650, loss is 5.208247833251953 and perplexity is 182.77352776656895
At time: 730.8777639865875 and batch: 700, loss is 5.216916341781616 and perplexity is 184.36478861579658
At time: 731.591890335083 and batch: 750, loss is 5.196098871231079 and perplexity is 180.5664530989201
At time: 732.3061051368713 and batch: 800, loss is 5.1836597442626955 and perplexity is 178.3342760120306
At time: 733.0197958946228 and batch: 850, loss is 5.168561964035034 and perplexity is 175.66204742515336
At time: 733.7368729114532 and batch: 900, loss is 5.20967396736145 and perplexity is 183.03437328496528
At time: 734.4566826820374 and batch: 950, loss is 5.191804084777832 and perplexity is 179.79262165210278
At time: 735.170875787735 and batch: 1000, loss is 5.195120124816895 and perplexity is 180.38981078854258
At time: 735.8890569210052 and batch: 1050, loss is 5.149980516433716 and perplexity is 172.42813076921126
At time: 736.6036450862885 and batch: 1100, loss is 5.140404357910156 and perplexity is 170.7848125509814
At time: 737.3194444179535 and batch: 1150, loss is 5.154723834991455 and perplexity is 173.24795512870784
At time: 738.0350184440613 and batch: 1200, loss is 5.154831323623657 and perplexity is 173.26657831530895
At time: 738.7514762878418 and batch: 1250, loss is 5.178899393081665 and perplexity is 177.48735963960183
At time: 739.4682483673096 and batch: 1300, loss is 5.183058013916016 and perplexity is 178.22699914541099
At time: 740.1804912090302 and batch: 1350, loss is 5.158270292282104 and perplexity is 173.863462392254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.166856282552083 and perplexity of 175.36267930950922
Finished 35 epochs...
Completing Train Step...
At time: 742.7014269828796 and batch: 50, loss is 5.22808180809021 and perplexity is 186.43484250436964
At time: 743.4176421165466 and batch: 100, loss is 5.245842905044555 and perplexity is 189.77571073560148
At time: 744.1684281826019 and batch: 150, loss is 5.2038062477111815 and perplexity is 181.9635236912245
At time: 744.8986411094666 and batch: 200, loss is 5.187418346405029 and perplexity is 179.0058248557766
At time: 745.6177635192871 and batch: 250, loss is 5.20239483833313 and perplexity is 181.70687982480786
At time: 746.344230890274 and batch: 300, loss is 5.202156820297241 and perplexity is 181.66363545683842
At time: 747.0576748847961 and batch: 350, loss is 5.220596399307251 and perplexity is 185.04451158630692
At time: 747.7753789424896 and batch: 400, loss is 5.243641319274903 and perplexity is 189.35836281361634
At time: 748.4912443161011 and batch: 450, loss is 5.207009468078613 and perplexity is 182.54732748335832
At time: 749.217973947525 and batch: 500, loss is 5.253106651306152 and perplexity is 191.15921196291285
At time: 749.938065290451 and batch: 550, loss is 5.2198336219787596 and perplexity is 184.9034176465774
At time: 750.6529297828674 and batch: 600, loss is 5.17946120262146 and perplexity is 177.58710174685052
At time: 751.3680863380432 and batch: 650, loss is 5.1988171863555905 and perplexity is 181.05795734842243
At time: 752.0843505859375 and batch: 700, loss is 5.212569961547851 and perplexity is 183.5652080420959
At time: 752.8016574382782 and batch: 750, loss is 5.191899919509888 and perplexity is 179.8098528554851
At time: 753.5247051715851 and batch: 800, loss is 5.178752765655518 and perplexity is 177.46133703274472
At time: 754.2406198978424 and batch: 850, loss is 5.16147777557373 and perplexity is 174.42202184909482
At time: 754.9558804035187 and batch: 900, loss is 5.204273023605347 and perplexity is 182.0484797038678
At time: 755.6718652248383 and batch: 950, loss is 5.183220739364624 and perplexity is 178.25600357361634
At time: 756.3900315761566 and batch: 1000, loss is 5.187613277435303 and perplexity is 179.04072204680295
At time: 757.1044478416443 and batch: 1050, loss is 5.1433541965484615 and perplexity is 171.28934396716258
At time: 757.8200612068176 and batch: 1100, loss is 5.136167011260986 and perplexity is 170.0626691642326
At time: 758.5402221679688 and batch: 1150, loss is 5.149705324172974 and perplexity is 172.38068641054898
At time: 759.254900932312 and batch: 1200, loss is 5.1505913734436035 and perplexity is 172.53349187860337
At time: 759.969420671463 and batch: 1250, loss is 5.176238861083984 and perplexity is 177.01577644935273
At time: 760.6817674636841 and batch: 1300, loss is 5.1770792484283445 and perplexity is 177.16460079392178
At time: 761.4024047851562 and batch: 1350, loss is 5.153928470611572 and perplexity is 173.11021466050326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.165001627604167 and perplexity of 175.03774346372643
Finished 36 epochs...
Completing Train Step...
At time: 763.9253661632538 and batch: 50, loss is 5.223855581283569 and perplexity is 185.64858918797876
At time: 764.641464471817 and batch: 100, loss is 5.240690288543701 and perplexity is 188.80038417672918
At time: 765.3575885295868 and batch: 150, loss is 5.200904970169067 and perplexity is 181.43636209727993
At time: 766.0729627609253 and batch: 200, loss is 5.185728988647461 and perplexity is 178.70367526814098
At time: 766.7871367931366 and batch: 250, loss is 5.199371185302734 and perplexity is 181.15829105598345
At time: 767.5014479160309 and batch: 300, loss is 5.202734727859497 and perplexity is 181.768650587152
At time: 768.2167739868164 and batch: 350, loss is 5.2176869773864745 and perplexity is 184.5069214454616
At time: 768.931515455246 and batch: 400, loss is 5.237870073318481 and perplexity is 188.26867657609836
At time: 769.6488053798676 and batch: 450, loss is 5.202030124664307 and perplexity is 181.64062092551416
At time: 770.364648103714 and batch: 500, loss is 5.249933586120606 and perplexity is 190.55361263372583
At time: 771.0835254192352 and batch: 550, loss is 5.219129266738892 and perplexity is 184.77322581152174
At time: 771.7998299598694 and batch: 600, loss is 5.1748322486877445 and perplexity is 176.76695889981886
At time: 772.5185034275055 and batch: 650, loss is 5.1975603103637695 and perplexity is 180.83053290080636
At time: 773.2381930351257 and batch: 700, loss is 5.20890851020813 and perplexity is 182.89432192312754
At time: 773.9529731273651 and batch: 750, loss is 5.186606578826904 and perplexity is 178.86057269433715
At time: 774.6675486564636 and batch: 800, loss is 5.176868486404419 and perplexity is 177.12726515869568
At time: 775.3813781738281 and batch: 850, loss is 5.1603770923614505 and perplexity is 174.23014407548746
At time: 776.0970633029938 and batch: 900, loss is 5.19908878326416 and perplexity is 181.10713880837406
At time: 776.8106627464294 and batch: 950, loss is 5.176700696945191 and perplexity is 177.09754756387946
At time: 777.5292522907257 and batch: 1000, loss is 5.184197778701782 and perplexity is 178.43025181102203
At time: 778.2438180446625 and batch: 1050, loss is 5.139751815795899 and perplexity is 170.67340462145253
At time: 778.9592123031616 and batch: 1100, loss is 5.134228467941284 and perplexity is 169.7333146501316
At time: 779.676824092865 and batch: 1150, loss is 5.1440567207336425 and perplexity is 171.40972115295457
At time: 780.453604221344 and batch: 1200, loss is 5.14511209487915 and perplexity is 171.59071803386257
At time: 781.1714866161346 and batch: 1250, loss is 5.170512218475341 and perplexity is 176.0049673950435
At time: 781.8906428813934 and batch: 1300, loss is 5.174322099685669 and perplexity is 176.67680441020283
At time: 782.6057493686676 and batch: 1350, loss is 5.1503856754302975 and perplexity is 172.49800573193525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.1643448893229165 and perplexity of 174.92282721600992
Finished 37 epochs...
Completing Train Step...
At time: 785.0764169692993 and batch: 50, loss is 5.220284624099731 and perplexity is 184.98682828788517
At time: 785.8213255405426 and batch: 100, loss is 5.2403154373168945 and perplexity is 188.72962538393776
At time: 786.5366413593292 and batch: 150, loss is 5.194673223495483 and perplexity is 180.30921235484672
At time: 787.248722076416 and batch: 200, loss is 5.180813245773315 and perplexity is 177.8273695612505
At time: 787.9679956436157 and batch: 250, loss is 5.195114278793335 and perplexity is 180.38875622854133
At time: 788.6835653781891 and batch: 300, loss is 5.195182361602783 and perplexity is 180.40103801994286
At time: 789.396874666214 and batch: 350, loss is 5.2137297248840335 and perplexity is 183.77822374020369
At time: 790.1111204624176 and batch: 400, loss is 5.233472242355346 and perplexity is 187.4425207392739
At time: 790.8297092914581 and batch: 450, loss is 5.1965460681915285 and perplexity is 180.64721992589878
At time: 791.5464437007904 and batch: 500, loss is 5.244789323806763 and perplexity is 189.57587189909168
At time: 792.2645409107208 and batch: 550, loss is 5.215281496047973 and perplexity is 184.06362687126753
At time: 792.9796483516693 and batch: 600, loss is 5.172640295028686 and perplexity is 176.3799182599977
At time: 793.6966052055359 and batch: 650, loss is 5.19345344543457 and perplexity is 180.0894092162118
At time: 794.4126915931702 and batch: 700, loss is 5.204948863983154 and perplexity is 182.1715570027895
At time: 795.1276533603668 and batch: 750, loss is 5.187181415557862 and perplexity is 178.96341787800458
At time: 795.8440470695496 and batch: 800, loss is 5.17917576789856 and perplexity is 177.5364194552598
At time: 796.5572028160095 and batch: 850, loss is 5.160940923690796 and perplexity is 174.32840818882318
At time: 797.2703971862793 and batch: 900, loss is 5.198747434616089 and perplexity is 181.04532868138762
At time: 797.9879562854767 and batch: 950, loss is 5.176641807556153 and perplexity is 177.0871187045809
At time: 798.7571878433228 and batch: 1000, loss is 5.182228660583496 and perplexity is 178.07924726744986
At time: 799.4739949703217 and batch: 1050, loss is 5.138578958511353 and perplexity is 170.47334641835766
At time: 800.1901416778564 and batch: 1100, loss is 5.1330946254730225 and perplexity is 169.54097287294636
At time: 800.9060170650482 and batch: 1150, loss is 5.140968990325928 and perplexity is 170.88127042132675
At time: 801.6242430210114 and batch: 1200, loss is 5.143232040405273 and perplexity is 171.26842119948262
At time: 802.3381617069244 and batch: 1250, loss is 5.173849191665649 and perplexity is 176.59327228550848
At time: 803.0515792369843 and batch: 1300, loss is 5.173775758743286 and perplexity is 176.58030500157344
At time: 803.7647368907928 and batch: 1350, loss is 5.1466588306427 and perplexity is 171.85632889609488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.171272786458333 and perplexity of 176.13888205723856
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 806.304285287857 and batch: 50, loss is 5.214278211593628 and perplexity is 183.87905130219923
At time: 807.0799851417542 and batch: 100, loss is 5.230310926437378 and perplexity is 186.8508913711507
At time: 807.7961668968201 and batch: 150, loss is 5.1823680973052975 and perplexity is 178.10407978515192
At time: 808.5071799755096 and batch: 200, loss is 5.1695961666107175 and perplexity is 175.8438115413079
At time: 809.2166087627411 and batch: 250, loss is 5.174888944625854 and perplexity is 176.77698115248828
At time: 809.929322719574 and batch: 300, loss is 5.175919046401978 and perplexity is 176.9591732568459
At time: 810.6408257484436 and batch: 350, loss is 5.188692302703857 and perplexity is 179.23401577564624
At time: 811.3525428771973 and batch: 400, loss is 5.2101373195648195 and perplexity is 183.11920231646087
At time: 812.0675897598267 and batch: 450, loss is 5.1737633228302 and perplexity is 176.57810907790193
At time: 812.7818503379822 and batch: 500, loss is 5.220938739776611 and perplexity is 185.1078706558239
At time: 813.4941000938416 and batch: 550, loss is 5.190798273086548 and perplexity is 179.61187504500157
At time: 814.2054738998413 and batch: 600, loss is 5.141560487747192 and perplexity is 170.9823761510647
At time: 814.9155049324036 and batch: 650, loss is 5.15814323425293 and perplexity is 173.8413730467211
At time: 815.630122423172 and batch: 700, loss is 5.167012329101563 and perplexity is 175.3900461857207
At time: 816.3409357070923 and batch: 750, loss is 5.144062395095825 and perplexity is 171.4106937965536
At time: 817.0523383617401 and batch: 800, loss is 5.138599290847778 and perplexity is 170.47681257502597
At time: 817.8269355297089 and batch: 850, loss is 5.1081595134735105 and perplexity is 165.36572124078708
At time: 818.5385103225708 and batch: 900, loss is 5.149034957885743 and perplexity is 172.26516693431688
At time: 819.2492053508759 and batch: 950, loss is 5.128105659484863 and perplexity is 168.6972451401806
At time: 819.9565415382385 and batch: 1000, loss is 5.1384490203857425 and perplexity is 170.45119687032408
At time: 820.6681611537933 and batch: 1050, loss is 5.085308866500855 and perplexity is 161.6298535760773
At time: 821.3807680606842 and batch: 1100, loss is 5.078544731140137 and perplexity is 160.54025661323388
At time: 822.0887956619263 and batch: 1150, loss is 5.08724006652832 and perplexity is 161.94229474983936
At time: 822.8088080883026 and batch: 1200, loss is 5.087177066802979 and perplexity is 161.93209275111388
At time: 823.5469009876251 and batch: 1250, loss is 5.114450635910035 and perplexity is 166.40933655373794
At time: 824.2910685539246 and batch: 1300, loss is 5.114873275756836 and perplexity is 166.47968263472671
At time: 825.031653881073 and batch: 1350, loss is 5.092283887863159 and perplexity is 162.7611661355786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.12393798828125 and perplexity of 167.9956335482473
Finished 39 epochs...
Completing Train Step...
At time: 827.6417951583862 and batch: 50, loss is 5.18717737197876 and perplexity is 178.96269422673114
At time: 828.3558721542358 and batch: 100, loss is 5.204127235412598 and perplexity is 182.02194111957215
At time: 829.0717463493347 and batch: 150, loss is 5.15987006187439 and perplexity is 174.1418264724269
At time: 829.7863380908966 and batch: 200, loss is 5.148960170745849 and perplexity is 172.25228419691615
At time: 830.502156496048 and batch: 250, loss is 5.157602548599243 and perplexity is 173.74740491619679
At time: 831.2179815769196 and batch: 300, loss is 5.159884653091431 and perplexity is 174.14436743215066
At time: 831.9346704483032 and batch: 350, loss is 5.1752824783325195 and perplexity is 176.84656254354772
At time: 832.6528129577637 and batch: 400, loss is 5.194980344772339 and perplexity is 180.36459765494064
At time: 833.3698375225067 and batch: 450, loss is 5.159593639373779 and perplexity is 174.0936964057156
At time: 834.0840971469879 and batch: 500, loss is 5.2093045520782475 and perplexity is 182.9667700777202
At time: 834.799795627594 and batch: 550, loss is 5.180371208190918 and perplexity is 177.74878055165317
At time: 835.5151107311249 and batch: 600, loss is 5.132140789031983 and perplexity is 169.37933561477917
At time: 836.2870523929596 and batch: 650, loss is 5.148970136642456 and perplexity is 172.25400085392474
At time: 837.0050723552704 and batch: 700, loss is 5.1595867729187015 and perplexity is 174.0925010032741
At time: 837.7186985015869 and batch: 750, loss is 5.139234867095947 and perplexity is 170.5851980279234
At time: 838.4327535629272 and batch: 800, loss is 5.1308800411224365 and perplexity is 169.16592552791064
At time: 839.1476449966431 and batch: 850, loss is 5.103887329101562 and perplexity is 164.6607553352354
At time: 839.8618490695953 and batch: 900, loss is 5.145332012176514 and perplexity is 171.62845795050342
At time: 840.5749859809875 and batch: 950, loss is 5.125992984771728 and perplexity is 168.34121895238331
At time: 841.291668176651 and batch: 1000, loss is 5.1358202457427975 and perplexity is 170.003707518159
At time: 842.0065977573395 and batch: 1050, loss is 5.083867607116699 and perplexity is 161.39707082330975
At time: 842.7208573818207 and batch: 1100, loss is 5.080804290771485 and perplexity is 160.90341703308263
At time: 843.4348683357239 and batch: 1150, loss is 5.089761381149292 and perplexity is 162.35111739402623
At time: 844.1548812389374 and batch: 1200, loss is 5.090069131851196 and perplexity is 162.40108875332527
At time: 844.8749396800995 and batch: 1250, loss is 5.116127185821533 and perplexity is 166.6885641162774
At time: 845.592449426651 and batch: 1300, loss is 5.11698655128479 and perplexity is 166.83187207953645
At time: 846.3062601089478 and batch: 1350, loss is 5.090997152328491 and perplexity is 162.5518702425567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.122005208333333 and perplexity of 167.6712485397896
Finished 40 epochs...
Completing Train Step...
At time: 848.8298072814941 and batch: 50, loss is 5.180292711257935 and perplexity is 177.73482836514748
At time: 849.5466687679291 and batch: 100, loss is 5.196913537979126 and perplexity is 180.71361451969148
At time: 850.2642545700073 and batch: 150, loss is 5.152219896316528 and perplexity is 172.8146955276124
At time: 850.9774281978607 and batch: 200, loss is 5.142658987045288 and perplexity is 171.17030337131555
At time: 851.697687625885 and batch: 250, loss is 5.150718603134155 and perplexity is 172.55544465787798
At time: 852.4163699150085 and batch: 300, loss is 5.154986982345581 and perplexity is 173.2935508686437
At time: 853.1397700309753 and batch: 350, loss is 5.171672639846801 and perplexity is 176.20932586872996
At time: 853.8564274311066 and batch: 400, loss is 5.190957174301148 and perplexity is 179.6404178577865
At time: 854.5715036392212 and batch: 450, loss is 5.156870813369751 and perplexity is 173.62031432298966
At time: 855.3459737300873 and batch: 500, loss is 5.207382917404175 and perplexity is 182.6155123907014
At time: 856.0607113838196 and batch: 550, loss is 5.176037664413452 and perplexity is 176.9801650470674
At time: 856.7785534858704 and batch: 600, loss is 5.128703975677491 and perplexity is 168.79820963493376
At time: 857.5029199123383 and batch: 650, loss is 5.145695419311523 and perplexity is 171.69084029110059
At time: 858.2165546417236 and batch: 700, loss is 5.156704368591309 and perplexity is 173.59141853308256
At time: 858.9378633499146 and batch: 750, loss is 5.135737581253052 and perplexity is 169.98965482925928
At time: 859.6533229351044 and batch: 800, loss is 5.127909517288208 and perplexity is 168.66415973677704
At time: 860.3741118907928 and batch: 850, loss is 5.10181155204773 and perplexity is 164.31931082158985
At time: 861.0909206867218 and batch: 900, loss is 5.143420972824097 and perplexity is 171.30078241351288
At time: 861.8109958171844 and batch: 950, loss is 5.126259965896606 and perplexity is 168.38616888050677
At time: 862.526694059372 and batch: 1000, loss is 5.134418773651123 and perplexity is 169.76561894280755
At time: 863.2455167770386 and batch: 1050, loss is 5.081688804626465 and perplexity is 161.0458012959056
At time: 863.962956905365 and batch: 1100, loss is 5.079248905181885 and perplexity is 160.65334470677342
At time: 864.6802756786346 and batch: 1150, loss is 5.088954648971558 and perplexity is 162.22019633974742
At time: 865.396856546402 and batch: 1200, loss is 5.08871919631958 and perplexity is 162.18200566053974
At time: 866.1119213104248 and batch: 1250, loss is 5.115757160186767 and perplexity is 166.626896484538
At time: 866.8247632980347 and batch: 1300, loss is 5.112672271728516 and perplexity is 166.11366313557286
At time: 867.5427956581116 and batch: 1350, loss is 5.0904708576202395 and perplexity is 162.46634256178876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.12209716796875 and perplexity of 167.6866682356591
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 870.039642572403 and batch: 50, loss is 5.17633563041687 and perplexity is 177.03290697679142
At time: 870.7867386341095 and batch: 100, loss is 5.194502382278443 and perplexity is 180.2784107407319
At time: 871.5049068927765 and batch: 150, loss is 5.148551979064941 and perplexity is 172.18198659591366
At time: 872.227621793747 and batch: 200, loss is 5.1394429111480715 and perplexity is 170.62069095567182
At time: 872.9437749385834 and batch: 250, loss is 5.1454810047149655 and perplexity is 171.65403121518952
At time: 873.7135343551636 and batch: 300, loss is 5.148265218734741 and perplexity is 172.13261871129657
At time: 874.4307565689087 and batch: 350, loss is 5.163567819595337 and perplexity is 174.78695278110996
At time: 875.154637336731 and batch: 400, loss is 5.182843828201294 and perplexity is 178.18882955605267
At time: 875.8682880401611 and batch: 450, loss is 5.146198587417603 and perplexity is 171.77725138387086
At time: 876.5841691493988 and batch: 500, loss is 5.194949970245362 and perplexity is 180.35911924880585
At time: 877.3038198947906 and batch: 550, loss is 5.161810998916626 and perplexity is 174.48015302308414
At time: 878.0192415714264 and batch: 600, loss is 5.1177754211425786 and perplexity is 166.96353263943413
At time: 878.735342502594 and batch: 650, loss is 5.132553052902222 and perplexity is 169.44917899117937
At time: 879.4564199447632 and batch: 700, loss is 5.140638294219971 and perplexity is 170.82476999336814
At time: 880.1696984767914 and batch: 750, loss is 5.118381547927856 and perplexity is 167.06476438526852
At time: 880.8857672214508 and batch: 800, loss is 5.109414758682251 and perplexity is 165.5734261030588
At time: 881.6000397205353 and batch: 850, loss is 5.080087556838989 and perplexity is 160.78813341301685
At time: 882.3169519901276 and batch: 900, loss is 5.1161441898345945 and perplexity is 166.69139851489686
At time: 883.0314385890961 and batch: 950, loss is 5.0996019077301025 and perplexity is 163.95662444167456
At time: 883.7466578483582 and batch: 1000, loss is 5.1073384094238286 and perplexity is 165.2299945079084
At time: 884.4605877399445 and batch: 1050, loss is 5.057002201080322 and perplexity is 157.11879905381073
At time: 885.176242351532 and batch: 1100, loss is 5.054209871292114 and perplexity is 156.68068351712662
At time: 885.8913869857788 and batch: 1150, loss is 5.0607547855377195 and perplexity is 157.70950826701952
At time: 886.6118228435516 and batch: 1200, loss is 5.058655471801758 and perplexity is 157.37877380918337
At time: 887.3247354030609 and batch: 1250, loss is 5.084192485809326 and perplexity is 161.44951381101603
At time: 888.0469746589661 and batch: 1300, loss is 5.085367879867554 and perplexity is 161.63939217934566
At time: 888.7612812519073 and batch: 1350, loss is 5.066807708740234 and perplexity is 158.66700671776968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.097499186197917 and perplexity of 163.6122315252097
Finished 42 epochs...
Completing Train Step...
At time: 891.255806684494 and batch: 50, loss is 5.162834901809692 and perplexity is 174.65889524827207
At time: 892.0081315040588 and batch: 100, loss is 5.180370206832886 and perplexity is 177.7486025615732
At time: 892.729896068573 and batch: 150, loss is 5.134370822906494 and perplexity is 169.75747875013252
At time: 893.4426362514496 and batch: 200, loss is 5.125871963500977 and perplexity is 168.32084731687135
At time: 894.1610004901886 and batch: 250, loss is 5.133626499176025 and perplexity is 169.6311712429432
At time: 894.8767008781433 and batch: 300, loss is 5.136875982284546 and perplexity is 170.1832814191041
At time: 895.5897364616394 and batch: 350, loss is 5.1529289245605465 and perplexity is 172.93726947678383
At time: 896.3020799160004 and batch: 400, loss is 5.173571443557739 and perplexity is 176.5442306491876
At time: 897.0242533683777 and batch: 450, loss is 5.137982368469238 and perplexity is 170.37167404881575
At time: 897.7378559112549 and batch: 500, loss is 5.186802949905395 and perplexity is 178.89569918669744
At time: 898.4546523094177 and batch: 550, loss is 5.154817543029785 and perplexity is 173.26419061541355
At time: 899.1672518253326 and batch: 600, loss is 5.1117221641540525 and perplexity is 165.95591223803405
At time: 899.8810796737671 and batch: 650, loss is 5.126648168563843 and perplexity is 168.45154953004217
At time: 900.5946831703186 and batch: 700, loss is 5.136218070983887 and perplexity is 170.0713527386844
At time: 901.3079454898834 and batch: 750, loss is 5.114382038116455 and perplexity is 166.397921631943
At time: 902.0458424091339 and batch: 800, loss is 5.106198177337647 and perplexity is 165.04170133592947
At time: 902.8007862567902 and batch: 850, loss is 5.078241691589356 and perplexity is 160.4916139366872
At time: 903.5421249866486 and batch: 900, loss is 5.113715906143188 and perplexity is 166.2871155658879
At time: 904.269294500351 and batch: 950, loss is 5.099326343536377 and perplexity is 163.9114500911569
At time: 904.9822330474854 and batch: 1000, loss is 5.108231534957886 and perplexity is 165.37763155438975
At time: 905.699245929718 and batch: 1050, loss is 5.058490161895752 and perplexity is 157.3527596891328
At time: 906.4147891998291 and batch: 1100, loss is 5.057023677825928 and perplexity is 157.12217349052375
At time: 907.1304502487183 and batch: 1150, loss is 5.064086017608642 and perplexity is 158.23575127098619
At time: 907.846440076828 and batch: 1200, loss is 5.062995929718017 and perplexity is 158.06335437561722
At time: 908.5622045993805 and batch: 1250, loss is 5.086951398849488 and perplexity is 161.895553990107
At time: 909.2794272899628 and batch: 1300, loss is 5.087088174819947 and perplexity is 161.91769892603
At time: 909.9934313297272 and batch: 1350, loss is 5.0659593486785885 and perplexity is 158.53245704752248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.095170491536458 and perplexity of 163.23167187070104
Finished 43 epochs...
Completing Train Step...
At time: 912.5227999687195 and batch: 50, loss is 5.158432369232178 and perplexity is 173.89164393569536
At time: 913.2362287044525 and batch: 100, loss is 5.175918073654175 and perplexity is 176.95900112028258
At time: 913.9493012428284 and batch: 150, loss is 5.128709077835083 and perplexity is 168.79907087219772
At time: 914.6670231819153 and batch: 200, loss is 5.120942964553833 and perplexity is 167.49323536246084
At time: 915.3817915916443 and batch: 250, loss is 5.128809423446655 and perplexity is 168.81600996806492
At time: 916.0967600345612 and batch: 300, loss is 5.132979307174683 and perplexity is 169.52142282371665
At time: 916.8100383281708 and batch: 350, loss is 5.149585227966309 and perplexity is 172.3599853870909
At time: 917.5251579284668 and batch: 400, loss is 5.169668235778809 and perplexity is 175.8564849151939
At time: 918.2390787601471 and batch: 450, loss is 5.1342916011810305 and perplexity is 169.7440308024475
At time: 918.9553401470184 and batch: 500, loss is 5.184483051300049 and perplexity is 178.4811603336235
At time: 919.6712093353271 and batch: 550, loss is 5.153187847137451 and perplexity is 172.98205263767505
At time: 920.3871927261353 and batch: 600, loss is 5.1099679565429685 and perplexity is 165.66504630788475
At time: 921.0992245674133 and batch: 650, loss is 5.125379867553711 and perplexity is 168.23803768687912
At time: 921.8131725788116 and batch: 700, loss is 5.135287008285522 and perplexity is 169.91307933875208
At time: 922.5324552059174 and batch: 750, loss is 5.113244104385376 and perplexity is 166.2086795170502
At time: 923.2510168552399 and batch: 800, loss is 5.105743703842163 and perplexity is 164.96671129880556
At time: 923.9661357402802 and batch: 850, loss is 5.077167835235596 and perplexity is 160.31936150107526
At time: 924.678923368454 and batch: 900, loss is 5.112935962677002 and perplexity is 166.157471580653
At time: 925.391889333725 and batch: 950, loss is 5.097614421844482 and perplexity is 163.63108657286145
At time: 926.1049537658691 and batch: 1000, loss is 5.108180284500122 and perplexity is 165.3691560922562
At time: 926.8207960128784 and batch: 1050, loss is 5.059093236923218 and perplexity is 157.4476838293164
At time: 927.5388951301575 and batch: 1100, loss is 5.057456331253052 and perplexity is 157.19016764525298
At time: 928.2568185329437 and batch: 1150, loss is 5.065398817062378 and perplexity is 158.44361949360666
At time: 929.0292420387268 and batch: 1200, loss is 5.0647336292266845 and perplexity is 158.33825977116408
At time: 929.7449843883514 and batch: 1250, loss is 5.087860612869263 and perplexity is 162.04281863492898
At time: 930.4633896350861 and batch: 1300, loss is 5.086987771987915 and perplexity is 161.9014427465987
At time: 931.1804625988007 and batch: 1350, loss is 5.064790258407593 and perplexity is 158.34722659101075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.095565999348958 and perplexity of 163.29624404073041
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 933.7486622333527 and batch: 50, loss is 5.156827402114868 and perplexity is 173.6127774108664
At time: 934.4643788337708 and batch: 100, loss is 5.1751906776428225 and perplexity is 176.83032865228827
At time: 935.17950963974 and batch: 150, loss is 5.129828605651856 and perplexity is 168.98815194849817
At time: 935.8956816196442 and batch: 200, loss is 5.121605501174927 and perplexity is 167.60424253377155
At time: 936.6122450828552 and batch: 250, loss is 5.128417587280273 and perplexity is 168.74987470783472
At time: 937.3295085430145 and batch: 300, loss is 5.133175621032715 and perplexity is 169.55470549506813
At time: 938.0451321601868 and batch: 350, loss is 5.147119512557984 and perplexity is 171.9355182379927
At time: 938.7621471881866 and batch: 400, loss is 5.167783098220825 and perplexity is 175.52528352883542
At time: 939.4771296977997 and batch: 450, loss is 5.130092821121216 and perplexity is 169.03280713141683
At time: 940.1937041282654 and batch: 500, loss is 5.177589416503906 and perplexity is 177.25500757673282
At time: 940.9119246006012 and batch: 550, loss is 5.146682910919189 and perplexity is 171.86046729383781
At time: 941.6262102127075 and batch: 600, loss is 5.10488037109375 and perplexity is 164.82435159528268
At time: 942.3396022319794 and batch: 650, loss is 5.11992039680481 and perplexity is 167.32204972117427
At time: 943.0566890239716 and batch: 700, loss is 5.127552700042725 and perplexity is 168.603988191649
At time: 943.7729716300964 and batch: 750, loss is 5.104449243545532 and perplexity is 164.75330659252214
At time: 944.4909591674805 and batch: 800, loss is 5.096551370620728 and perplexity is 163.4572307712198
At time: 945.2050652503967 and batch: 850, loss is 5.063036851882934 and perplexity is 158.06982280262244
At time: 945.9181592464447 and batch: 900, loss is 5.09639952659607 and perplexity is 163.43241265172932
At time: 946.6326560974121 and batch: 950, loss is 5.083307247161866 and perplexity is 161.30665570286527
At time: 947.4038119316101 and batch: 1000, loss is 5.095965404510498 and perplexity is 163.361478430069
At time: 948.1233756542206 and batch: 1050, loss is 5.044886646270752 and perplexity is 155.226702675436
At time: 948.8409993648529 and batch: 1100, loss is 5.041604137420654 and perplexity is 154.71800500919045
At time: 949.563310623169 and batch: 1150, loss is 5.0470736217498775 and perplexity is 155.5665511525818
At time: 950.2816863059998 and batch: 1200, loss is 5.045659484863282 and perplexity is 155.34671423066212
At time: 950.9999361038208 and batch: 1250, loss is 5.068594360351563 and perplexity is 158.95074277425118
At time: 951.7252485752106 and batch: 1300, loss is 5.069836616516113 and perplexity is 159.14832301154743
At time: 952.4437563419342 and batch: 1350, loss is 5.051702604293824 and perplexity is 156.28833527651003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.083453369140625 and perplexity of 161.33022787274777
Finished 45 epochs...
Completing Train Step...
At time: 954.9639797210693 and batch: 50, loss is 5.150707111358643 and perplexity is 172.55346170083837
At time: 955.7058579921722 and batch: 100, loss is 5.1668116474151615 and perplexity is 175.35485214699182
At time: 956.4225876331329 and batch: 150, loss is 5.122156505584717 and perplexity is 167.69661865799586
At time: 957.134530544281 and batch: 200, loss is 5.114309854507447 and perplexity is 166.38591086292368
At time: 957.8429527282715 and batch: 250, loss is 5.121591577529907 and perplexity is 167.60190888804112
At time: 958.5531775951385 and batch: 300, loss is 5.12658766746521 and perplexity is 168.4413583345215
At time: 959.2628929615021 and batch: 350, loss is 5.141025609970093 and perplexity is 170.89094593196188
At time: 959.9757761955261 and batch: 400, loss is 5.162065391540527 and perplexity is 174.52454513330372
At time: 960.6870512962341 and batch: 450, loss is 5.1244761085510255 and perplexity is 168.08605973179237
At time: 961.4034984111786 and batch: 500, loss is 5.1733013534545895 and perplexity is 176.4965542384743
At time: 962.1150977611542 and batch: 550, loss is 5.143582620620728 and perplexity is 171.32847504571737
At time: 962.8237843513489 and batch: 600, loss is 5.102087316513061 and perplexity is 164.364630496973
At time: 963.5358216762543 and batch: 650, loss is 5.117609338760376 and perplexity is 166.93580524077237
At time: 964.247998714447 and batch: 700, loss is 5.125625972747803 and perplexity is 168.2794470371167
At time: 964.9595255851746 and batch: 750, loss is 5.102556476593017 and perplexity is 164.44176191223463
At time: 965.7246959209442 and batch: 800, loss is 5.0954212474822995 and perplexity is 163.27260831528616
At time: 966.4351868629456 and batch: 850, loss is 5.062808227539063 and perplexity is 158.03368832386644
At time: 967.1470165252686 and batch: 900, loss is 5.097235231399536 and perplexity is 163.56905099073026
At time: 967.8591537475586 and batch: 950, loss is 5.084504156112671 and perplexity is 161.49984067224713
At time: 968.5728635787964 and batch: 1000, loss is 5.097174701690673 and perplexity is 163.55915050333462
At time: 969.2862591743469 and batch: 1050, loss is 5.046262531280518 and perplexity is 155.44042376286725
At time: 969.9991433620453 and batch: 1100, loss is 5.0431843185424805 and perplexity is 154.9626807449896
At time: 970.7119204998016 and batch: 1150, loss is 5.048853397369385 and perplexity is 155.84367124017132
At time: 971.4218280315399 and batch: 1200, loss is 5.047842483520508 and perplexity is 155.68620631979928
At time: 972.1370832920074 and batch: 1250, loss is 5.070874328613281 and perplexity is 159.3135588703775
At time: 972.8560535907745 and batch: 1300, loss is 5.0720017051696775 and perplexity is 159.49326652182694
At time: 973.581357717514 and batch: 1350, loss is 5.0525216865539555 and perplexity is 156.41640072030725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.08276123046875 and perplexity of 161.2186036172828
Finished 46 epochs...
Completing Train Step...
At time: 976.0762956142426 and batch: 50, loss is 5.148080472946167 and perplexity is 172.10082087226232
At time: 976.8370389938354 and batch: 100, loss is 5.163596572875977 and perplexity is 174.79197855166885
At time: 977.5647277832031 and batch: 150, loss is 5.119230918884277 and perplexity is 167.20672462389592
At time: 978.280713558197 and batch: 200, loss is 5.111557102203369 and perplexity is 165.92852149207974
At time: 978.9979858398438 and batch: 250, loss is 5.118738451004028 and perplexity is 167.1244009552022
At time: 979.7136604785919 and batch: 300, loss is 5.12414098739624 and perplexity is 168.0297399748498
At time: 980.4259774684906 and batch: 350, loss is 5.138323945999145 and perplexity is 170.42987912461055
At time: 981.1420233249664 and batch: 400, loss is 5.1597590732574465 and perplexity is 174.12249978449577
At time: 981.8568489551544 and batch: 450, loss is 5.122469673156738 and perplexity is 167.74914402508873
At time: 982.5758271217346 and batch: 500, loss is 5.171445322036743 and perplexity is 176.16927490298303
At time: 983.2994947433472 and batch: 550, loss is 5.142423620223999 and perplexity is 171.13002030194696
At time: 984.0210030078888 and batch: 600, loss is 5.1011182308197025 and perplexity is 164.20542423977685
At time: 984.7667784690857 and batch: 650, loss is 5.116846780776978 and perplexity is 166.80855553357733
At time: 985.4822905063629 and batch: 700, loss is 5.12504879951477 and perplexity is 168.18234866860257
At time: 986.1981143951416 and batch: 750, loss is 5.1017729187011716 and perplexity is 164.31296273933327
At time: 986.9138700962067 and batch: 800, loss is 5.094821138381958 and perplexity is 163.17465633107784
At time: 987.6324734687805 and batch: 850, loss is 5.062448558807373 and perplexity is 157.97685876814245
At time: 988.3480851650238 and batch: 900, loss is 5.09728141784668 and perplexity is 163.57660583852257
At time: 989.06276512146 and batch: 950, loss is 5.085420122146607 and perplexity is 161.6478368101593
At time: 989.78053855896 and batch: 1000, loss is 5.097841491699219 and perplexity is 163.66824647870754
At time: 990.4963717460632 and batch: 1050, loss is 5.046762323379516 and perplexity is 155.51813107566184
At time: 991.2122933864594 and batch: 1100, loss is 5.043453464508056 and perplexity is 155.004393938544
At time: 991.9289889335632 and batch: 1150, loss is 5.049499168395996 and perplexity is 155.94434306971576
At time: 992.6448299884796 and batch: 1200, loss is 5.04846074104309 and perplexity is 155.7824902490836
At time: 993.3631517887115 and batch: 1250, loss is 5.071582946777344 and perplexity is 159.42649136025568
At time: 994.0787000656128 and batch: 1300, loss is 5.072856845855713 and perplexity is 159.6297140357739
At time: 994.7949070930481 and batch: 1350, loss is 5.052399015426635 and perplexity is 156.39721412094448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.082313232421875 and perplexity of 161.1463941737948
Finished 47 epochs...
Completing Train Step...
At time: 997.318122625351 and batch: 50, loss is 5.1461771202087405 and perplexity is 171.7735638453184
At time: 998.0345442295074 and batch: 100, loss is 5.161262903213501 and perplexity is 174.3845474038396
At time: 998.7506568431854 and batch: 150, loss is 5.117491674423218 and perplexity is 166.91616400545988
At time: 999.4642548561096 and batch: 200, loss is 5.109808673858643 and perplexity is 165.63866083604006
At time: 1000.1801671981812 and batch: 250, loss is 5.116655893325806 and perplexity is 166.77671691247113
At time: 1000.896066904068 and batch: 300, loss is 5.122480268478394 and perplexity is 167.750921390643
At time: 1001.6148862838745 and batch: 350, loss is 5.136867733001709 and perplexity is 170.18187753487203
At time: 1002.3316547870636 and batch: 400, loss is 5.158180770874023 and perplexity is 173.84789858694407
At time: 1003.1055686473846 and batch: 450, loss is 5.121169652938843 and perplexity is 167.53120843735013
At time: 1003.8208932876587 and batch: 500, loss is 5.170214500427246 and perplexity is 175.95257533911285
At time: 1004.5356965065002 and batch: 550, loss is 5.141742267608643 and perplexity is 171.01346012884693
At time: 1005.2515490055084 and batch: 600, loss is 5.100368432998657 and perplexity is 164.08234951682357
At time: 1005.9671337604523 and batch: 650, loss is 5.116572866439819 and perplexity is 166.7628705358295
At time: 1006.6832356452942 and batch: 700, loss is 5.124631757736206 and perplexity is 168.11222422621816
At time: 1007.3995623588562 and batch: 750, loss is 5.101257829666138 and perplexity is 164.22834872765796
At time: 1008.1138689517975 and batch: 800, loss is 5.094323396682739 and perplexity is 163.0934577100124
At time: 1008.8288559913635 and batch: 850, loss is 5.062176494598389 and perplexity is 157.9338847651331
At time: 1009.5420472621918 and batch: 900, loss is 5.097174091339111 and perplexity is 163.55905067478207
At time: 1010.2582850456238 and batch: 950, loss is 5.085667324066162 and perplexity is 161.68780140516958
At time: 1010.9722468852997 and batch: 1000, loss is 5.097982378005981 and perplexity is 163.691306717886
At time: 1011.6850881576538 and batch: 1050, loss is 5.046876096725464 and perplexity is 155.5358259003705
At time: 1012.4002492427826 and batch: 1100, loss is 5.0436739730834965 and perplexity is 155.03857750538467
At time: 1013.1128005981445 and batch: 1150, loss is 5.049661636352539 and perplexity is 155.96968108672095
At time: 1013.8293344974518 and batch: 1200, loss is 5.048735876083374 and perplexity is 155.82535736766647
At time: 1014.5432605743408 and batch: 1250, loss is 5.071986255645752 and perplexity is 159.49080244582424
At time: 1015.258599281311 and batch: 1300, loss is 5.073097381591797 and perplexity is 159.66811530479393
At time: 1015.9744582176208 and batch: 1350, loss is 5.052132520675659 and perplexity is 156.3555406374429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.082095133463541 and perplexity of 161.1112521454455
Finished 48 epochs...
Completing Train Step...
At time: 1018.5155522823334 and batch: 50, loss is 5.144614200592041 and perplexity is 171.50530526066305
At time: 1019.233834028244 and batch: 100, loss is 5.159646339416504 and perplexity is 174.10287139271338
At time: 1019.9490897655487 and batch: 150, loss is 5.11617374420166 and perplexity is 166.69632504647515
At time: 1020.6645205020905 and batch: 200, loss is 5.10848072052002 and perplexity is 165.41884640733286
At time: 1021.414523601532 and batch: 250, loss is 5.115260467529297 and perplexity is 166.54415467884183
At time: 1022.1339588165283 and batch: 300, loss is 5.121224737167358 and perplexity is 167.54043701889148
At time: 1022.8503265380859 and batch: 350, loss is 5.135758085250854 and perplexity is 169.99314033250164
At time: 1023.5640997886658 and batch: 400, loss is 5.156997528076172 and perplexity is 173.64231596408413
At time: 1024.2802150249481 and batch: 450, loss is 5.119750003814698 and perplexity is 167.29354164566766
At time: 1024.9966118335724 and batch: 500, loss is 5.169166555404663 and perplexity is 175.76828329440673
At time: 1025.7119884490967 and batch: 550, loss is 5.141353311538697 and perplexity is 170.94695633985197
At time: 1026.426605463028 and batch: 600, loss is 5.099749631881714 and perplexity is 163.980846583975
At time: 1027.1398677825928 and batch: 650, loss is 5.116276035308838 and perplexity is 166.71337747026737
At time: 1027.8522365093231 and batch: 700, loss is 5.124525756835937 and perplexity is 168.09440512354112
At time: 1028.5653367042542 and batch: 750, loss is 5.100897121429443 and perplexity is 164.16912089219935
At time: 1029.2781603336334 and batch: 800, loss is 5.093978614807129 and perplexity is 163.03723573447937
At time: 1029.9911813735962 and batch: 850, loss is 5.061618204116821 and perplexity is 157.84573638904098
At time: 1030.7048225402832 and batch: 900, loss is 5.097007522583008 and perplexity is 163.53180911602095
At time: 1031.4169278144836 and batch: 950, loss is 5.085802946090698 and perplexity is 161.70973131919501
At time: 1032.132051229477 and batch: 1000, loss is 5.097946653366089 and perplexity is 163.68545900935422
At time: 1032.8453392982483 and batch: 1050, loss is 5.046877937316895 and perplexity is 155.53611217854225
At time: 1033.561460018158 and batch: 1100, loss is 5.043741502761841 and perplexity is 155.04904756417042
At time: 1034.2745265960693 and batch: 1150, loss is 5.049645185470581 and perplexity is 155.96711526901342
At time: 1034.9874765872955 and batch: 1200, loss is 5.04877121925354 and perplexity is 155.83086482711312
At time: 1035.7030594348907 and batch: 1250, loss is 5.072030000686645 and perplexity is 159.4977795301047
At time: 1036.4166312217712 and batch: 1300, loss is 5.0731057453155515 and perplexity is 159.6694507303873
At time: 1037.1298496723175 and batch: 1350, loss is 5.051634340286255 and perplexity is 156.2776667725494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.081959228515625 and perplexity of 161.0893578169215
Finished 49 epochs...
Completing Train Step...
At time: 1039.6149640083313 and batch: 50, loss is 5.143278589248657 and perplexity is 171.27639373195234
At time: 1040.3567934036255 and batch: 100, loss is 5.158286991119385 and perplexity is 173.86636573416274
At time: 1041.073355436325 and batch: 150, loss is 5.115129127502441 and perplexity is 166.52228220149092
At time: 1041.7895467281342 and batch: 200, loss is 5.10735860824585 and perplexity is 165.23333199286657
At time: 1042.5044231414795 and batch: 250, loss is 5.114134073257446 and perplexity is 166.35666590996226
At time: 1043.219734430313 and batch: 300, loss is 5.12020938873291 and perplexity is 167.37041143067208
At time: 1043.9351036548615 and batch: 350, loss is 5.134886856079102 and perplexity is 169.8451018467337
At time: 1044.6476283073425 and batch: 400, loss is 5.15602972984314 and perplexity is 173.47434653088536
At time: 1045.36044049263 and batch: 450, loss is 5.118742752075195 and perplexity is 167.12511977069022
At time: 1046.0749578475952 and batch: 500, loss is 5.168208131790161 and perplexity is 175.5999035234803
At time: 1046.7880516052246 and batch: 550, loss is 5.140998868942261 and perplexity is 170.8863761935205
At time: 1047.5019962787628 and batch: 600, loss is 5.099281587600708 and perplexity is 163.90411424500383
At time: 1048.213659286499 and batch: 650, loss is 5.115899066925049 and perplexity is 166.65054364173398
At time: 1048.9231503009796 and batch: 700, loss is 5.1243829250335695 and perplexity is 168.07039761124832
At time: 1049.632292509079 and batch: 750, loss is 5.100443382263183 and perplexity is 164.09464782911056
At time: 1050.3681473731995 and batch: 800, loss is 5.093636960983276 and perplexity is 162.98154295382778
At time: 1051.1155540943146 and batch: 850, loss is 5.061209926605224 and perplexity is 157.7813046784761
At time: 1051.8692257404327 and batch: 900, loss is 5.096694107055664 and perplexity is 163.48056373879737
At time: 1052.6218917369843 and batch: 950, loss is 5.085765657424926 and perplexity is 161.70370149149457
At time: 1053.3609960079193 and batch: 1000, loss is 5.097874870300293 and perplexity is 163.6737095869902
At time: 1054.0738201141357 and batch: 1050, loss is 5.046780290603638 and perplexity is 155.5209253298803
At time: 1054.7919669151306 and batch: 1100, loss is 5.043711986541748 and perplexity is 155.04447116989658
At time: 1055.5153527259827 and batch: 1150, loss is 5.049540071487427 and perplexity is 155.95072180589264
At time: 1056.2380681037903 and batch: 1200, loss is 5.048705978393555 and perplexity is 155.8206986191092
At time: 1056.9506349563599 and batch: 1250, loss is 5.0721634674072265 and perplexity is 159.5190685963379
At time: 1057.6645731925964 and batch: 1300, loss is 5.073047504425049 and perplexity is 159.66015171018498
At time: 1058.379150390625 and batch: 1350, loss is 5.051298170089722 and perplexity is 156.2251397081082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.081798095703125 and perplexity of 161.06340312676477
Finished Training.
Improved accuracyfrom -393.90350062627937 to -161.06340312676477
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc57df2b70>
SETTINGS FOR THIS RUN
{'seq_len': 20, 'lr': 7.586349786777084, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.1501156749828374, 'anneal': 2.9661893316284225, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1375 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.3187079429626465 and batch: 50, loss is 7.418349237442016 and perplexity is 1666.280602543706
At time: 2.0334343910217285 and batch: 100, loss is 6.4672637367248536 and perplexity is 643.7199277582434
At time: 2.746795177459717 and batch: 150, loss is 6.081011629104614 and perplexity is 437.47152987316923
At time: 3.468867778778076 and batch: 200, loss is 5.850615911483764 and perplexity is 347.44831199590544
At time: 4.185349941253662 and batch: 250, loss is 5.742469310760498 and perplexity is 311.83347481665896
At time: 4.902034759521484 and batch: 300, loss is 5.6354418468475345 and perplexity is 280.1826878019251
At time: 5.6210713386535645 and batch: 350, loss is 5.575378007888794 and perplexity is 263.84927389559255
At time: 6.335250616073608 and batch: 400, loss is 5.544634065628052 and perplexity is 255.86093280052256
At time: 7.050981760025024 and batch: 450, loss is 5.47935941696167 and perplexity is 239.69311484134994
At time: 7.767770528793335 and batch: 500, loss is 5.477412567138672 and perplexity is 239.22692229383438
At time: 8.492351531982422 and batch: 550, loss is 5.421996259689331 and perplexity is 226.3304862586896
At time: 9.215192317962646 and batch: 600, loss is 5.337435903549195 and perplexity is 207.97874844607784
At time: 9.93399953842163 and batch: 650, loss is 5.350841684341431 and perplexity is 210.78563819498342
At time: 10.652852773666382 and batch: 700, loss is 5.343217248916626 and perplexity is 209.1846278638539
At time: 11.376734018325806 and batch: 750, loss is 5.285896415710449 and perplexity is 197.53117415549647
At time: 12.09509801864624 and batch: 800, loss is 5.229565544128418 and perplexity is 186.71166791610548
At time: 12.81368374824524 and batch: 850, loss is 5.214307889938355 and perplexity is 183.8845086090533
At time: 13.534703731536865 and batch: 900, loss is 5.258320741653442 and perplexity is 192.15853638468502
At time: 14.251425743103027 and batch: 950, loss is 5.204294052124023 and perplexity is 182.05230795397435
At time: 14.969268798828125 and batch: 1000, loss is 5.209542074203491 and perplexity is 183.01023389540288
At time: 15.687114000320435 and batch: 1050, loss is 5.142809715270996 and perplexity is 171.19610551194316
At time: 16.40459108352661 and batch: 1100, loss is 5.119394817352295 and perplexity is 167.2341317958395
At time: 17.119606494903564 and batch: 1150, loss is 5.125700168609619 and perplexity is 168.2919331389184
At time: 17.836677312850952 and batch: 1200, loss is 5.104823846817016 and perplexity is 164.81503528132205
At time: 18.556434154510498 and batch: 1250, loss is 5.1409714412689205 and perplexity is 170.8816892420923
At time: 19.275598287582397 and batch: 1300, loss is 5.123948087692261 and perplexity is 167.99733021376613
At time: 19.994012117385864 and batch: 1350, loss is 5.080608301162719 and perplexity is 160.87188472543258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.776589762369792 and perplexity of 118.69886769692057
Finished 1 epochs...
Completing Train Step...
At time: 22.522943258285522 and batch: 50, loss is 5.020683612823486 and perplexity is 151.51484589026717
At time: 23.26724934577942 and batch: 100, loss is 5.0166518497467045 and perplexity is 150.9052037212328
At time: 23.979859590530396 and batch: 150, loss is 4.934328384399414 and perplexity is 138.9797701985991
At time: 24.693097829818726 and batch: 200, loss is 4.914828443527222 and perplexity is 136.2959253485179
At time: 25.406235694885254 and batch: 250, loss is 4.909898138046264 and perplexity is 135.62559861637374
At time: 26.118688821792603 and batch: 300, loss is 4.875603647232055 and perplexity is 131.053239263512
At time: 26.831778526306152 and batch: 350, loss is 4.858553400039673 and perplexity is 128.83769063799585
At time: 27.545508861541748 and batch: 400, loss is 4.866453218460083 and perplexity is 129.859515802276
At time: 28.25990891456604 and batch: 450, loss is 4.812915477752686 and perplexity is 123.08996093499213
At time: 28.97250533103943 and batch: 500, loss is 4.883508195877075 and perplexity is 132.0932610043745
At time: 29.688252925872803 and batch: 550, loss is 4.846437940597534 and perplexity is 127.28618043002439
At time: 30.404937744140625 and batch: 600, loss is 4.7851310729980465 and perplexity is 119.717053728676
At time: 31.12684655189514 and batch: 650, loss is 4.8121312236785885 and perplexity is 122.99346497526903
At time: 31.83982253074646 and batch: 700, loss is 4.8232412528991695 and perplexity is 124.36754486510607
At time: 32.5570330619812 and batch: 750, loss is 4.773518772125244 and perplexity is 118.33490378304359
At time: 33.2706413269043 and batch: 800, loss is 4.71984281539917 and perplexity is 112.15062293138568
At time: 33.98422837257385 and batch: 850, loss is 4.700080699920655 and perplexity is 109.95604553824043
At time: 34.69775986671448 and batch: 900, loss is 4.748026666641235 and perplexity is 115.35642310000077
At time: 35.412946939468384 and batch: 950, loss is 4.703143281936645 and perplexity is 110.29331113401683
At time: 36.1257107257843 and batch: 1000, loss is 4.717380990982056 and perplexity is 111.87486735962833
At time: 36.840075731277466 and batch: 1050, loss is 4.65072208404541 and perplexity is 104.66053199873919
At time: 37.61518335342407 and batch: 1100, loss is 4.627769737243653 and perplexity is 102.28568557803793
At time: 38.33356714248657 and batch: 1150, loss is 4.651086797714234 and perplexity is 104.69871008695762
At time: 39.05826377868652 and batch: 1200, loss is 4.638732242584228 and perplexity is 103.41316164231613
At time: 39.78372931480408 and batch: 1250, loss is 4.685325222015381 and perplexity is 108.34550292180538
At time: 40.52221322059631 and batch: 1300, loss is 4.677491416931153 and perplexity is 107.5000612065867
At time: 41.26171398162842 and batch: 1350, loss is 4.630157842636108 and perplexity is 102.53024647771097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.614351399739584 and perplexity of 100.92234903790002
Finished 2 epochs...
Completing Train Step...
At time: 43.81084203720093 and batch: 50, loss is 4.676627836227417 and perplexity is 107.40726630178986
At time: 44.52441430091858 and batch: 100, loss is 4.689628562927246 and perplexity is 108.81275520901038
At time: 45.23585653305054 and batch: 150, loss is 4.632044887542724 and perplexity is 102.72390832393295
At time: 45.94617748260498 and batch: 200, loss is 4.620375604629516 and perplexity is 101.53216091810825
At time: 46.654170751571655 and batch: 250, loss is 4.622761631011963 and perplexity is 101.77470858021498
At time: 47.366596698760986 and batch: 300, loss is 4.607836475372315 and perplexity is 100.2669847095023
At time: 48.076316833496094 and batch: 350, loss is 4.595327396392822 and perplexity is 99.02054921194723
At time: 48.78653383255005 and batch: 400, loss is 4.608679294586182 and perplexity is 100.35152727277581
At time: 49.49958801269531 and batch: 450, loss is 4.559598035812378 and perplexity is 95.5450664151721
At time: 50.20941495895386 and batch: 500, loss is 4.6369819068908695 and perplexity is 103.23231221410714
At time: 50.923598766326904 and batch: 550, loss is 4.62129623413086 and perplexity is 101.62567746122376
At time: 51.629926681518555 and batch: 600, loss is 4.55289472579956 and perplexity is 94.90674005476818
At time: 52.33854293823242 and batch: 650, loss is 4.582584323883057 and perplexity is 97.76672890179461
At time: 53.04924917221069 and batch: 700, loss is 4.599071769714356 and perplexity is 99.392014132364
At time: 53.754443645477295 and batch: 750, loss is 4.550776643753052 and perplexity is 94.70593253104401
At time: 54.461270332336426 and batch: 800, loss is 4.507050552368164 and perplexity is 90.65404445513747
At time: 55.16557049751282 and batch: 850, loss is 4.484312953948975 and perplexity is 88.61604658848681
At time: 55.90809392929077 and batch: 900, loss is 4.534828433990478 and perplexity is 93.20752268984324
At time: 56.613605260849 and batch: 950, loss is 4.493446922302246 and perplexity is 89.42917062503172
At time: 57.32604718208313 and batch: 1000, loss is 4.512242794036865 and perplexity is 91.12596626801842
At time: 58.03189754486084 and batch: 1050, loss is 4.444961767196656 and perplexity is 85.19662046941781
At time: 58.738571643829346 and batch: 1100, loss is 4.427813777923584 and perplexity is 83.74812462553918
At time: 59.44574952125549 and batch: 1150, loss is 4.449563140869141 and perplexity is 85.58954525945154
At time: 60.15809607505798 and batch: 1200, loss is 4.430137329101562 and perplexity is 83.94294392782533
At time: 60.86605453491211 and batch: 1250, loss is 4.484355506896972 and perplexity is 88.61981754274106
At time: 61.57590842247009 and batch: 1300, loss is 4.477557363510132 and perplexity is 88.01941045520934
At time: 62.28695511817932 and batch: 1350, loss is 4.434685201644897 and perplexity is 84.32557515725146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.56445556640625 and perplexity of 96.01030854751761
Finished 3 epochs...
Completing Train Step...
At time: 64.78379583358765 and batch: 50, loss is 4.498756484985352 and perplexity is 89.9052632154112
At time: 65.55164980888367 and batch: 100, loss is 4.514814558029175 and perplexity is 91.36062235753775
At time: 66.26106190681458 and batch: 150, loss is 4.47078046798706 and perplexity is 87.42492875194301
At time: 66.97511291503906 and batch: 200, loss is 4.458556756973267 and perplexity is 86.36277663320637
At time: 67.686607837677 and batch: 250, loss is 4.458715572357177 and perplexity is 86.37649345992567
At time: 68.39864349365234 and batch: 300, loss is 4.456404724121094 and perplexity is 86.17712094091274
At time: 69.11148309707642 and batch: 350, loss is 4.452620248794556 and perplexity is 85.85160210052321
At time: 69.82564997673035 and batch: 400, loss is 4.459166803359985 and perplexity is 86.41547800654519
At time: 70.54329180717468 and batch: 450, loss is 4.411014995574951 and perplexity is 82.3530090376129
At time: 71.25617814064026 and batch: 500, loss is 4.489224796295166 and perplexity is 89.0523853750709
At time: 71.9699912071228 and batch: 550, loss is 4.474470329284668 and perplexity is 87.7481104942138
At time: 72.68900275230408 and batch: 600, loss is 4.412721853256226 and perplexity is 82.4936939340574
At time: 73.40418648719788 and batch: 650, loss is 4.443256187438965 and perplexity is 85.05143468628337
At time: 74.12100458145142 and batch: 700, loss is 4.463983945846557 and perplexity is 86.83275791863468
At time: 74.90177702903748 and batch: 750, loss is 4.41308219909668 and perplexity is 82.52342555004057
At time: 75.61585545539856 and batch: 800, loss is 4.377801237106323 and perplexity is 79.662681349172
At time: 76.33229064941406 and batch: 850, loss is 4.355374794006348 and perplexity is 77.89601482593875
At time: 77.05063819885254 and batch: 900, loss is 4.404432353973388 and perplexity is 81.81268901190458
At time: 77.76633906364441 and batch: 950, loss is 4.366954288482666 and perplexity is 78.80325384674184
At time: 78.48370623588562 and batch: 1000, loss is 4.382037658691406 and perplexity is 80.00088192626312
At time: 79.19807720184326 and batch: 1050, loss is 4.321303853988647 and perplexity is 75.28672722436703
At time: 79.91633915901184 and batch: 1100, loss is 4.301325340270996 and perplexity is 73.79753576058754
At time: 80.63378643989563 and batch: 1150, loss is 4.323356142044068 and perplexity is 75.44139593346615
At time: 81.34881782531738 and batch: 1200, loss is 4.305114526748657 and perplexity is 74.07769884504835
At time: 82.06419706344604 and batch: 1250, loss is 4.355810298919677 and perplexity is 77.92994631125221
At time: 82.78442096710205 and batch: 1300, loss is 4.354053802490235 and perplexity is 77.79318278629427
At time: 83.50041127204895 and batch: 1350, loss is 4.319709854125977 and perplexity is 75.16681578630796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.537468668619792 and perplexity of 93.45393757237134
Finished 4 epochs...
Completing Train Step...
At time: 86.04135155677795 and batch: 50, loss is 4.3871214485168455 and perplexity is 80.4086251580989
At time: 86.7581250667572 and batch: 100, loss is 4.400932130813598 and perplexity is 81.52682692546284
At time: 87.473299741745 and batch: 150, loss is 4.364717044830322 and perplexity is 78.62714883565256
At time: 88.18949317932129 and batch: 200, loss is 4.3497927856445315 and perplexity is 77.46240993876518
At time: 88.90696287155151 and batch: 250, loss is 4.350568437576294 and perplexity is 77.52251711478323
At time: 89.62418532371521 and batch: 300, loss is 4.352801704406739 and perplexity is 77.69583904589145
At time: 90.34660911560059 and batch: 350, loss is 4.351743021011353 and perplexity is 77.61362727699894
At time: 91.06409454345703 and batch: 400, loss is 4.3549831199646 and perplexity is 77.86551095315333
At time: 91.78133034706116 and batch: 450, loss is 4.311066217422486 and perplexity is 74.51990101433732
At time: 92.5009126663208 and batch: 500, loss is 4.388090181350708 and perplexity is 80.4865573750694
At time: 93.24368691444397 and batch: 550, loss is 4.376095819473266 and perplexity is 79.52693898930957
At time: 93.9610288143158 and batch: 600, loss is 4.315608768463135 and perplexity is 74.85918148425058
At time: 94.67654657363892 and batch: 650, loss is 4.346838521957397 and perplexity is 77.23390325465904
At time: 95.38995003700256 and batch: 700, loss is 4.364249601364135 and perplexity is 78.59040367747556
At time: 96.10454893112183 and batch: 750, loss is 4.3180740928649906 and perplexity is 75.04396132860799
At time: 96.819744348526 and batch: 800, loss is 4.286929378509521 and perplexity is 72.74275973162267
At time: 97.53763890266418 and batch: 850, loss is 4.267906570434571 and perplexity is 71.37206672324369
At time: 98.25431084632874 and batch: 900, loss is 4.310337400436401 and perplexity is 74.46560943139384
At time: 98.96961522102356 and batch: 950, loss is 4.275303707122803 and perplexity is 71.90197313533183
At time: 99.68574833869934 and batch: 1000, loss is 4.291672258377075 and perplexity is 73.08858936819094
At time: 100.40159177780151 and batch: 1050, loss is 4.227199859619141 and perplexity is 68.5250834244615
At time: 101.11726999282837 and batch: 1100, loss is 4.212320499420166 and perplexity is 67.51302211001567
At time: 101.83464622497559 and batch: 1150, loss is 4.233693532943725 and perplexity is 68.97151083891902
At time: 102.54957914352417 and batch: 1200, loss is 4.2176800632476805 and perplexity is 67.87583384906121
At time: 103.26323962211609 and batch: 1250, loss is 4.2617497158050535 and perplexity is 70.93398925730179
At time: 103.97864842414856 and batch: 1300, loss is 4.263830604553223 and perplexity is 71.08174867955196
At time: 104.69402432441711 and batch: 1350, loss is 4.226505689620971 and perplexity is 68.47753187376559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.547282307942709 and perplexity of 94.37557572450702
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 107.2228798866272 and batch: 50, loss is 4.287104301452636 and perplexity is 72.75548522220294
At time: 107.98236179351807 and batch: 100, loss is 4.291368551254273 and perplexity is 73.06639521343625
At time: 108.69495153427124 and batch: 150, loss is 4.250924081802368 and perplexity is 70.17022542416552
At time: 109.41063404083252 and batch: 200, loss is 4.2333917236328125 and perplexity is 68.95069773571227
At time: 110.13168907165527 and batch: 250, loss is 4.223723912239075 and perplexity is 68.28730732826777
At time: 110.84589505195618 and batch: 300, loss is 4.214729957580566 and perplexity is 67.67588804260409
At time: 111.56606984138489 and batch: 350, loss is 4.2095004749298095 and perplexity is 67.32290193202131
At time: 112.32904291152954 and batch: 400, loss is 4.207062187194825 and perplexity is 67.15894928904436
At time: 113.05750250816345 and batch: 450, loss is 4.146316947937012 and perplexity is 63.20079928181563
At time: 113.7825186252594 and batch: 500, loss is 4.2206789493560795 and perplexity is 68.07969126409266
At time: 114.50271725654602 and batch: 550, loss is 4.2010641765594485 and perplexity is 66.75733484484559
At time: 115.21978831291199 and batch: 600, loss is 4.1338517236709595 and perplexity is 62.41787693393043
At time: 115.937180519104 and batch: 650, loss is 4.1634771299362185 and perplexity is 64.29469538475892
At time: 116.67275166511536 and batch: 700, loss is 4.16623046875 and perplexity is 64.47196439391591
At time: 117.38925528526306 and batch: 750, loss is 4.113450288772583 and perplexity is 61.157364532433455
At time: 118.10951495170593 and batch: 800, loss is 4.086454887390136 and perplexity is 59.52848201773877
At time: 118.82388734817505 and batch: 850, loss is 4.052309255599976 and perplexity is 57.53015560302668
At time: 119.54606652259827 and batch: 900, loss is 4.083042840957642 and perplexity is 59.32571419636868
At time: 120.26351690292358 and batch: 950, loss is 4.048207297325134 and perplexity is 57.29465264706288
At time: 120.97885298728943 and batch: 1000, loss is 4.049727191925049 and perplexity is 57.381800691369556
At time: 121.69313478469849 and batch: 1050, loss is 3.9836269187927247 and perplexity is 53.71148858658331
At time: 122.4093885421753 and batch: 1100, loss is 3.9549608516693113 and perplexity is 52.19365061014143
At time: 123.1261157989502 and batch: 1150, loss is 3.970247058868408 and perplexity is 52.99762275507885
At time: 123.84078812599182 and batch: 1200, loss is 3.9511792135238646 and perplexity is 51.99664584518787
At time: 124.55941915512085 and batch: 1250, loss is 3.990257215499878 and perplexity is 54.06879490699905
At time: 125.27850794792175 and batch: 1300, loss is 3.9824049711227416 and perplexity is 53.64589604174394
At time: 125.99330878257751 and batch: 1350, loss is 3.940428686141968 and perplexity is 51.4406484676637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.428783365885416 and perplexity of 83.82936517756825
Finished 6 epochs...
Completing Train Step...
At time: 128.5154857635498 and batch: 50, loss is 4.162395749092102 and perplexity is 64.22520591185564
At time: 129.2653751373291 and batch: 100, loss is 4.171836566925049 and perplexity is 64.83441557502962
At time: 129.9860155582428 and batch: 150, loss is 4.134161858558655 and perplexity is 62.43723789728344
At time: 130.74065041542053 and batch: 200, loss is 4.120987501144409 and perplexity is 61.62006211150547
At time: 131.46250748634338 and batch: 250, loss is 4.1149884557724 and perplexity is 61.251507157331
At time: 132.18166327476501 and batch: 300, loss is 4.114868068695069 and perplexity is 61.24413371124493
At time: 132.90351510047913 and batch: 350, loss is 4.116196179389954 and perplexity is 61.325526737736354
At time: 133.62371969223022 and batch: 400, loss is 4.115044312477112 and perplexity is 61.25492856023271
At time: 134.34264183044434 and batch: 450, loss is 4.059440779685974 and perplexity is 57.94189972989933
At time: 135.06287717819214 and batch: 500, loss is 4.137978343963623 and perplexity is 62.67598400016435
At time: 135.78115439414978 and batch: 550, loss is 4.121430735588074 and perplexity is 61.647380299187695
At time: 136.4969367980957 and batch: 600, loss is 4.059711866378784 and perplexity is 57.95760913708165
At time: 137.22215485572815 and batch: 650, loss is 4.093381934165954 and perplexity is 59.94227010729638
At time: 137.9402585029602 and batch: 700, loss is 4.101596455574036 and perplexity is 60.436695120523666
At time: 138.65640568733215 and batch: 750, loss is 4.05150272846222 and perplexity is 57.4837746775452
At time: 139.37489819526672 and batch: 800, loss is 4.0296929693222046 and perplexity is 56.243640072940075
At time: 140.09123015403748 and batch: 850, loss is 4.001197237968444 and perplexity is 54.663556156905464
At time: 140.80696034431458 and batch: 900, loss is 4.032326316833496 and perplexity is 56.39194430514097
At time: 141.52209162712097 and batch: 950, loss is 4.002346158027649 and perplexity is 54.7263963053226
At time: 142.23825931549072 and batch: 1000, loss is 4.011083674430847 and perplexity is 55.206664209605854
At time: 142.95739769935608 and batch: 1050, loss is 3.9496302795410156 and perplexity is 51.91616881633653
At time: 143.6746051311493 and batch: 1100, loss is 3.922685704231262 and perplexity is 50.535987398085005
At time: 144.39154624938965 and batch: 1150, loss is 3.943301725387573 and perplexity is 51.58865197767094
At time: 145.10853099822998 and batch: 1200, loss is 3.9271470212936403 and perplexity is 50.76194812737555
At time: 145.82392477989197 and batch: 1250, loss is 3.9686025047302245 and perplexity is 52.91053692357278
At time: 146.54075574874878 and batch: 1300, loss is 3.9631130647659303 and perplexity is 52.62088345268828
At time: 147.2554008960724 and batch: 1350, loss is 3.925586977005005 and perplexity is 50.682818978655995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.422275390625 and perplexity of 83.28557714172513
Finished 7 epochs...
Completing Train Step...
At time: 149.79032349586487 and batch: 50, loss is 4.108236141204834 and perplexity is 60.83931091873041
At time: 150.51251339912415 and batch: 100, loss is 4.1161946058273315 and perplexity is 61.3254302382556
At time: 151.2402422428131 and batch: 150, loss is 4.081327509880066 and perplexity is 59.22403818405077
At time: 151.96178460121155 and batch: 200, loss is 4.068897604942322 and perplexity is 58.49244525406945
At time: 152.68044781684875 and batch: 250, loss is 4.063283953666687 and perplexity is 58.165008980557886
At time: 153.39620351791382 and batch: 300, loss is 4.066798505783081 and perplexity is 58.36979258650654
At time: 154.11547923088074 and batch: 350, loss is 4.069986343383789 and perplexity is 58.55616290738828
At time: 154.83558869361877 and batch: 400, loss is 4.0685731267929075 and perplexity is 58.4734688125655
At time: 155.55221390724182 and batch: 450, loss is 4.015160098075866 and perplexity is 55.43216927577205
At time: 156.26809191703796 and batch: 500, loss is 4.093779063224792 and perplexity is 59.966079652026814
At time: 156.98336243629456 and batch: 550, loss is 4.0797832441329955 and perplexity is 59.13265111160412
At time: 157.70444321632385 and batch: 600, loss is 4.017079181671143 and perplexity is 55.538650382872916
At time: 158.42206597328186 and batch: 650, loss is 4.053844418525696 and perplexity is 57.6185415911069
At time: 159.13744258880615 and batch: 700, loss is 4.062905931472779 and perplexity is 58.14302547164328
At time: 159.8547501564026 and batch: 750, loss is 4.014468789100647 and perplexity is 55.39386176232538
At time: 160.57650065422058 and batch: 800, loss is 3.9945524978637694 and perplexity is 54.30153513286638
At time: 161.29641723632812 and batch: 850, loss is 3.9691654729843138 and perplexity is 52.94033226229555
At time: 162.01364040374756 and batch: 900, loss is 3.999032096862793 and perplexity is 54.54532987896159
At time: 162.73569083213806 and batch: 950, loss is 3.973132429122925 and perplexity is 53.1507613438564
At time: 163.45463037490845 and batch: 1000, loss is 3.982951669692993 and perplexity is 53.67523219469559
At time: 164.17159175872803 and batch: 1050, loss is 3.923338198661804 and perplexity is 50.56897260856697
At time: 164.89108514785767 and batch: 1100, loss is 3.89645188331604 and perplexity is 49.22747405115053
At time: 165.6083676815033 and batch: 1150, loss is 3.91916419506073 and perplexity is 50.35833743659182
At time: 166.32397747039795 and batch: 1200, loss is 3.9036643648147584 and perplexity is 49.583809784705096
At time: 167.04440331459045 and batch: 1250, loss is 3.94525851726532 and perplexity is 51.68969906466157
At time: 167.76296257972717 and batch: 1300, loss is 3.941550531387329 and perplexity is 51.49838929664693
At time: 168.48205161094666 and batch: 1350, loss is 3.9059682655334473 and perplexity is 49.69817765522463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.422950439453125 and perplexity of 83.34181795348569
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 171.0120656490326 and batch: 50, loss is 4.078413171768188 and perplexity is 59.051690574020164
At time: 171.78037786483765 and batch: 100, loss is 4.08560613155365 and perplexity is 59.47797830688057
At time: 172.49647760391235 and batch: 150, loss is 4.058690056800843 and perplexity is 57.89841774326749
At time: 173.21750926971436 and batch: 200, loss is 4.043594155311585 and perplexity is 57.030952987425366
At time: 173.93522477149963 and batch: 250, loss is 4.033370366096497 and perplexity is 56.45085101843636
At time: 174.65334630012512 and batch: 300, loss is 4.038340525627136 and perplexity is 56.732119148375105
At time: 175.37152242660522 and batch: 350, loss is 4.034405632019043 and perplexity is 56.50932292255696
At time: 176.09036898612976 and batch: 400, loss is 4.033156147003174 and perplexity is 56.438759463481105
At time: 176.811425447464 and batch: 450, loss is 3.975502333641052 and perplexity is 53.2768729505154
At time: 177.5300076007843 and batch: 500, loss is 4.04874354839325 and perplexity is 57.3253852051893
At time: 178.24730849266052 and batch: 550, loss is 4.030485820770264 and perplexity is 56.28825060684234
At time: 178.96822881698608 and batch: 600, loss is 3.9684056854248047 and perplexity is 52.90012413319879
At time: 179.68466019630432 and batch: 650, loss is 3.999083843231201 and perplexity is 54.54815247472535
At time: 180.39976525306702 and batch: 700, loss is 4.000531024932862 and perplexity is 54.6271507114601
At time: 181.1171019077301 and batch: 750, loss is 3.9551774358749388 and perplexity is 52.20495615475463
At time: 181.83204913139343 and batch: 800, loss is 3.9330613470077513 and perplexity is 51.06306038273816
At time: 182.54723143577576 and batch: 850, loss is 3.9008141279220583 and perplexity is 49.442685395286134
At time: 183.267165184021 and batch: 900, loss is 3.9237489175796507 and perplexity is 50.589746508098116
At time: 183.98134541511536 and batch: 950, loss is 3.8968788146972657 and perplexity is 49.24849529163575
At time: 184.69780731201172 and batch: 1000, loss is 3.9027081394195555 and perplexity is 49.53641914827411
At time: 185.4130392074585 and batch: 1050, loss is 3.8422327184677125 and perplexity is 46.62946877762808
At time: 186.13113808631897 and batch: 1100, loss is 3.8080014848709105 and perplexity is 45.06029513796511
At time: 186.90693640708923 and batch: 1150, loss is 3.8279668521881103 and perplexity is 45.96898142503233
At time: 187.6427502632141 and batch: 1200, loss is 3.8055873012542722 and perplexity is 44.951642518141
At time: 188.36493372917175 and batch: 1250, loss is 3.84526864528656 and perplexity is 46.77124753851382
At time: 189.08204865455627 and batch: 1300, loss is 3.8405983543395994 and perplexity is 46.55332148969292
At time: 189.79564905166626 and batch: 1350, loss is 3.8036250829696656 and perplexity is 44.86352406536197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.385536702473958 and perplexity of 80.281298824165
Finished 9 epochs...
Completing Train Step...
At time: 192.40373015403748 and batch: 50, loss is 4.041851582527161 and perplexity is 56.931658939558496
At time: 193.12839365005493 and batch: 100, loss is 4.043580288887024 and perplexity is 57.03016217750101
At time: 193.83906531333923 and batch: 150, loss is 4.015581655502319 and perplexity is 55.455542044530624
At time: 194.56275248527527 and batch: 200, loss is 4.002241911888123 and perplexity is 54.72069158713019
At time: 195.2862958908081 and batch: 250, loss is 3.9930721282958985 and perplexity is 54.22120826416262
At time: 196.00088810920715 and batch: 300, loss is 4.000305571556091 and perplexity is 54.61483622409241
At time: 196.71667575836182 and batch: 350, loss is 3.9973510980606077 and perplexity is 54.45371626752894
At time: 197.4278974533081 and batch: 400, loss is 3.9970297479629515 and perplexity is 54.43622037179309
At time: 198.14190196990967 and batch: 450, loss is 3.9435847425460815 and perplexity is 51.603254517652154
At time: 198.8557710647583 and batch: 500, loss is 4.018751044273376 and perplexity is 55.63158103742345
At time: 199.56591820716858 and batch: 550, loss is 4.00256190776825 and perplexity is 54.738204784921905
At time: 200.27967429161072 and batch: 600, loss is 3.941703405380249 and perplexity is 51.506262662848776
At time: 200.99140763282776 and batch: 650, loss is 3.974062247276306 and perplexity is 53.2002048698017
At time: 201.7014091014862 and batch: 700, loss is 3.979228253364563 and perplexity is 53.47574856947049
At time: 202.41284656524658 and batch: 750, loss is 3.9357652711868285 and perplexity is 51.201317860998195
At time: 203.12595534324646 and batch: 800, loss is 3.9150154256820677 and perplexity is 50.1498451006365
At time: 203.83684182167053 and batch: 850, loss is 3.885784420967102 and perplexity is 48.70513280625651
At time: 204.5474123954773 and batch: 900, loss is 3.909886736869812 and perplexity is 49.89330058196744
At time: 205.31429195404053 and batch: 950, loss is 3.88525260925293 and perplexity is 48.67923773235195
At time: 206.0239598751068 and batch: 1000, loss is 3.8930708932876588 and perplexity is 49.06131749736616
At time: 206.73571634292603 and batch: 1050, loss is 3.8358528900146482 and perplexity is 46.33292771235456
At time: 207.4487009048462 and batch: 1100, loss is 3.803022513389587 and perplexity is 44.83649881362008
At time: 208.1611523628235 and batch: 1150, loss is 3.824406051635742 and perplexity is 45.80558613225147
At time: 208.87561106681824 and batch: 1200, loss is 3.8044049882888795 and perplexity is 44.89852701413798
At time: 209.59175539016724 and batch: 1250, loss is 3.845102491378784 and perplexity is 46.76347695853773
At time: 210.30260038375854 and batch: 1300, loss is 3.841602964401245 and perplexity is 46.60011292450228
At time: 211.01810717582703 and batch: 1350, loss is 3.805649285316467 and perplexity is 44.95442888990104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3834716796875 and perplexity of 80.11568716754073
Finished 10 epochs...
Completing Train Step...
At time: 213.57599306106567 and batch: 50, loss is 4.021801443099975 and perplexity is 55.801538634382055
At time: 214.291846036911 and batch: 100, loss is 4.021670017242432 and perplexity is 55.79420535121685
At time: 215.01149487495422 and batch: 150, loss is 3.9947160053253175 and perplexity is 54.31041456494103
At time: 215.72925329208374 and batch: 200, loss is 3.981500759124756 and perplexity is 53.5974107027164
At time: 216.44803261756897 and batch: 250, loss is 3.9727545833587645 and perplexity is 53.130682347441486
At time: 217.16633105278015 and batch: 300, loss is 3.980217204093933 and perplexity is 53.528659608915035
At time: 217.88165020942688 and batch: 350, loss is 3.9782239961624146 and perplexity is 53.422072120820594
At time: 218.59970784187317 and batch: 400, loss is 3.978348159790039 and perplexity is 53.4287056109009
At time: 219.31987476348877 and batch: 450, loss is 3.926399340629578 and perplexity is 50.72400858538897
At time: 220.0358157157898 and batch: 500, loss is 4.002060613632202 and perplexity is 54.7107717204371
At time: 220.7527461051941 and batch: 550, loss is 3.9864616632461547 and perplexity is 53.863962941751765
At time: 221.47687816619873 and batch: 600, loss is 3.9261923933029177 and perplexity is 50.713512473523345
At time: 222.1926007270813 and batch: 650, loss is 3.9593067073822024 and perplexity is 52.42097027628513
At time: 222.90730237960815 and batch: 700, loss is 3.96584792137146 and perplexity is 52.764990990292944
At time: 223.6254677772522 and batch: 750, loss is 3.923663339614868 and perplexity is 50.58541732579703
At time: 224.39657592773438 and batch: 800, loss is 3.902963705062866 and perplexity is 49.5490805729447
At time: 225.11462330818176 and batch: 850, loss is 3.8751004838943484 and perplexity is 48.18754011952213
At time: 226.10334300994873 and batch: 900, loss is 3.899385414123535 and perplexity is 49.372096386178846
At time: 226.81933045387268 and batch: 950, loss is 3.876323518753052 and perplexity is 48.246511215345585
At time: 227.5346999168396 and batch: 1000, loss is 3.884673709869385 and perplexity is 48.651065506866004
At time: 228.25680470466614 and batch: 1050, loss is 3.8290584802627565 and perplexity is 46.01918985519885
At time: 228.9798502922058 and batch: 1100, loss is 3.796976761817932 and perplexity is 44.566246243569026
At time: 229.69908666610718 and batch: 1150, loss is 3.8185161924362183 and perplexity is 45.536590629828744
At time: 230.41681742668152 and batch: 1200, loss is 3.799454479217529 and perplexity is 44.676805718327444
At time: 231.13544178009033 and batch: 1250, loss is 3.840605001449585 and perplexity is 46.55363093576951
At time: 231.85266041755676 and batch: 1300, loss is 3.837863874435425 and perplexity is 46.426196257516764
At time: 232.57460713386536 and batch: 1350, loss is 3.8022672939300537 and perplexity is 44.802650200393195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.38302734375 and perplexity of 80.08009679620075
Finished 11 epochs...
Completing Train Step...
At time: 235.1302330493927 and batch: 50, loss is 4.005230650901795 and perplexity is 54.88448209454918
At time: 235.85422253608704 and batch: 100, loss is 4.004189820289612 and perplexity is 54.82738636409784
At time: 236.56766438484192 and batch: 150, loss is 3.9783758020401 and perplexity is 53.43018252095429
At time: 237.28473615646362 and batch: 200, loss is 3.965283179283142 and perplexity is 52.73520079177222
At time: 237.9963002204895 and batch: 250, loss is 3.9568106937408447 and perplexity is 52.29028997713276
At time: 238.72815227508545 and batch: 300, loss is 3.964523015022278 and perplexity is 52.69512860949087
At time: 239.4423484802246 and batch: 350, loss is 3.963419404029846 and perplexity is 52.637005764714296
At time: 240.15700268745422 and batch: 400, loss is 3.963794479370117 and perplexity is 52.656752310552044
At time: 240.87774515151978 and batch: 450, loss is 3.9125670623779296 and perplexity is 50.02721024878047
At time: 241.59423780441284 and batch: 500, loss is 3.98869234085083 and perplexity is 53.98425018874066
At time: 242.3049247264862 and batch: 550, loss is 3.973477988243103 and perplexity is 53.16913124794433
At time: 243.07907152175903 and batch: 600, loss is 3.9135493421554566 and perplexity is 50.076375108604566
At time: 243.79368662834167 and batch: 650, loss is 3.9470944690704344 and perplexity is 51.78468603002923
At time: 244.5090296268463 and batch: 700, loss is 3.954474935531616 and perplexity is 52.16829503386548
At time: 245.22492599487305 and batch: 750, loss is 3.913251099586487 and perplexity is 50.06144242873858
At time: 245.94142961502075 and batch: 800, loss is 3.8926241397857666 and perplexity is 49.03940407727982
At time: 246.65795850753784 and batch: 850, loss is 3.8654689741134645 and perplexity is 47.725649279257944
At time: 247.37465500831604 and batch: 900, loss is 3.8899576616287233 and perplexity is 48.908815760295454
At time: 248.09149146080017 and batch: 950, loss is 3.8681555128097536 and perplexity is 47.85403846689194
At time: 248.8116753101349 and batch: 1000, loss is 3.8764027309417726 and perplexity is 48.250333078464166
At time: 249.53294968605042 and batch: 1050, loss is 3.8219712495803835 and perplexity is 45.694194260598785
At time: 250.24805521965027 and batch: 1100, loss is 3.7904400062561034 and perplexity is 44.27587765374721
At time: 250.9638271331787 and batch: 1150, loss is 3.8115602350234985 and perplexity is 45.22093914656572
At time: 251.68161535263062 and batch: 1200, loss is 3.793313307762146 and perplexity is 44.40327854262369
At time: 252.39802050590515 and batch: 1250, loss is 3.834491376876831 and perplexity is 46.26988774716925
At time: 253.11820340156555 and batch: 1300, loss is 3.8325311374664306 and perplexity is 46.17927652854809
At time: 253.8355348110199 and batch: 1350, loss is 3.7971377611160277 and perplexity is 44.573421955559795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.38322509765625 and perplexity of 80.09593451408871
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 256.3567774295807 and batch: 50, loss is 3.9985094022750856 and perplexity is 54.51682678010583
At time: 257.07488203048706 and batch: 100, loss is 4.001586847305298 and perplexity is 54.684857738147805
At time: 257.8191730976105 and batch: 150, loss is 3.9785675048828124 and perplexity is 53.44042622067201
At time: 258.537935256958 and batch: 200, loss is 3.9654607152938843 and perplexity is 52.74456402007699
At time: 259.2513620853424 and batch: 250, loss is 3.9579953289031984 and perplexity is 52.35227159884504
At time: 259.968953371048 and batch: 300, loss is 3.962907476425171 and perplexity is 52.61006632454576
At time: 260.686980009079 and batch: 350, loss is 3.9616337060928344 and perplexity is 52.54309584443957
At time: 261.40296483039856 and batch: 400, loss is 3.9603957891464234 and perplexity is 52.47809209859731
At time: 262.1185896396637 and batch: 450, loss is 3.905074291229248 and perplexity is 49.65376861466351
At time: 262.83608317375183 and batch: 500, loss is 3.9796214056015016 and perplexity is 53.49677681302217
At time: 263.5496337413788 and batch: 550, loss is 3.9635173511505126 and perplexity is 52.642161660367904
At time: 264.26839995384216 and batch: 600, loss is 3.9036535024642944 and perplexity is 49.58327119091108
At time: 264.9847447872162 and batch: 650, loss is 3.9329726028442384 and perplexity is 51.05852903522638
At time: 265.700514793396 and batch: 700, loss is 3.935382308959961 and perplexity is 51.181713444406704
At time: 266.41588592529297 and batch: 750, loss is 3.895096192359924 and perplexity is 49.16078202688844
At time: 267.1331343650818 and batch: 800, loss is 3.8728629779815673 and perplexity is 48.07984074750564
At time: 267.8534071445465 and batch: 850, loss is 3.8429471921920775 and perplexity is 46.66279621222122
At time: 268.57819175720215 and batch: 900, loss is 3.8614618253707884 and perplexity is 47.53478816358686
At time: 269.29308795928955 and batch: 950, loss is 3.8413992643356325 and perplexity is 46.5906214451823
At time: 270.01039457321167 and batch: 1000, loss is 3.8468370962142946 and perplexity is 46.84466350472655
At time: 270.72876930236816 and batch: 1050, loss is 3.7907650804519655 and perplexity is 44.29027293871266
At time: 271.45009183883667 and batch: 1100, loss is 3.7564629316329956 and perplexity is 42.79678280405783
At time: 272.16398215293884 and batch: 1150, loss is 3.775685887336731 and perplexity is 43.62742155197254
At time: 272.88205218315125 and batch: 1200, loss is 3.755738101005554 and perplexity is 42.765773624684584
At time: 273.60106325149536 and batch: 1250, loss is 3.7973188400268554 and perplexity is 44.58149399307519
At time: 274.3168601989746 and batch: 1300, loss is 3.795176377296448 and perplexity is 44.486082048493024
At time: 275.0314428806305 and batch: 1350, loss is 3.7583038997650147 and perplexity is 42.87564288452808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.370040283203125 and perplexity of 79.04681589437033
Finished 13 epochs...
Completing Train Step...
At time: 277.5565855503082 and batch: 50, loss is 3.9872206354141237 and perplexity is 53.904859708271665
At time: 278.2736132144928 and batch: 100, loss is 3.98611713886261 and perplexity is 53.84540868950367
At time: 278.9893751144409 and batch: 150, loss is 3.9624043130874633 and perplexity is 52.58360152659328
At time: 279.7297739982605 and batch: 200, loss is 3.9498007917404174 and perplexity is 51.92502191122481
At time: 280.47800397872925 and batch: 250, loss is 3.9416324949264525 and perplexity is 51.50261045988121
At time: 281.21151518821716 and batch: 300, loss is 3.947868013381958 and perplexity is 51.824759276549514
At time: 281.9664194583893 and batch: 350, loss is 3.9472659492492674 and perplexity is 51.79356683867004
At time: 282.73026394844055 and batch: 400, loss is 3.9465990686416625 and perplexity is 51.75903822785678
At time: 283.4753336906433 and batch: 450, loss is 3.8928231811523437 and perplexity is 49.049165918756394
At time: 284.2077293395996 and batch: 500, loss is 3.9682840394973753 and perplexity is 52.893689439922525
At time: 284.92651867866516 and batch: 550, loss is 3.953095736503601 and perplexity is 52.09639416626236
At time: 285.64408469200134 and batch: 600, loss is 3.8938190031051634 and perplexity is 49.098034483101955
At time: 286.3614981174469 and batch: 650, loss is 3.9242810153961183 and perplexity is 50.61667236471019
At time: 287.0810081958771 and batch: 700, loss is 3.928031949996948 and perplexity is 50.80688871398405
At time: 287.8049418926239 and batch: 750, loss is 3.8885587882995605 and perplexity is 48.840446353586266
At time: 288.526976108551 and batch: 800, loss is 3.867098445892334 and perplexity is 47.803480272368866
At time: 289.2489161491394 and batch: 850, loss is 3.8382676792144776 and perplexity is 46.44494716303663
At time: 289.9717891216278 and batch: 900, loss is 3.857959280014038 and perplexity is 47.36858664610791
At time: 290.68953824043274 and batch: 950, loss is 3.839029850959778 and perplexity is 46.48035968297304
At time: 291.41139459609985 and batch: 1000, loss is 3.844967637062073 and perplexity is 46.7571711269993
At time: 292.128347158432 and batch: 1050, loss is 3.7901936864852903 and perplexity is 44.26497297278538
At time: 292.84917092323303 and batch: 1100, loss is 3.756727180480957 and perplexity is 42.80809329893706
At time: 293.57140827178955 and batch: 1150, loss is 3.7763280153274534 and perplexity is 43.65544493685112
At time: 294.3478810787201 and batch: 1200, loss is 3.7576038026809693 and perplexity is 42.845636276962885
At time: 295.06959533691406 and batch: 1250, loss is 3.7994123983383177 and perplexity is 44.67492571861875
At time: 295.7902042865753 and batch: 1300, loss is 3.7972580337524415 and perplexity is 44.57878324093387
At time: 296.5099503993988 and batch: 1350, loss is 3.760771059989929 and perplexity is 42.981554562086906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.369329833984375 and perplexity of 78.99067709001962
Finished 14 epochs...
Completing Train Step...
At time: 299.06070041656494 and batch: 50, loss is 3.980037865638733 and perplexity is 53.5190607225423
At time: 299.7766432762146 and batch: 100, loss is 3.977767825126648 and perplexity is 53.39770807635663
At time: 300.4951972961426 and batch: 150, loss is 3.954321331977844 and perplexity is 52.16028241375328
At time: 301.21624279022217 and batch: 200, loss is 3.9416202449798585 and perplexity is 51.501979559517885
At time: 301.9359698295593 and batch: 250, loss is 3.933291015625 and perplexity is 51.07478931204068
At time: 302.6514747142792 and batch: 300, loss is 3.939774522781372 and perplexity is 51.40700888428192
At time: 303.3672173023224 and batch: 350, loss is 3.9395713710784914 and perplexity is 51.39656652361468
At time: 304.0822129249573 and batch: 400, loss is 3.9391653871536256 and perplexity is 51.37570457890644
At time: 304.8015875816345 and batch: 450, loss is 3.885881290435791 and perplexity is 48.70985107511827
At time: 305.51778531074524 and batch: 500, loss is 3.961936902999878 and perplexity is 52.55902916393064
At time: 306.239755153656 and batch: 550, loss is 3.9470278072357177 and perplexity is 51.78123408290608
At time: 306.9649569988251 and batch: 600, loss is 3.8881220293045042 and perplexity is 48.81911950700384
At time: 307.6824884414673 and batch: 650, loss is 3.9189806509017946 and perplexity is 50.349095306097105
At time: 308.4036521911621 and batch: 700, loss is 3.92335862159729 and perplexity is 50.57000538597828
At time: 309.11965250968933 and batch: 750, loss is 3.884190392494202 and perplexity is 48.627557283009516
At time: 309.83878231048584 and batch: 800, loss is 3.8630491876602173 and perplexity is 47.610303012609194
At time: 310.5574824810028 and batch: 850, loss is 3.8348403358459473 and perplexity is 46.28603685702312
At time: 311.27433824539185 and batch: 900, loss is 3.8548054265975953 and perplexity is 47.219428402537204
At time: 311.9930911064148 and batch: 950, loss is 3.8366717672348023 and perplexity is 46.370884230144135
At time: 312.76216745376587 and batch: 1000, loss is 3.8427873992919923 and perplexity is 46.65534042439523
At time: 313.47962975502014 and batch: 1050, loss is 3.788720564842224 and perplexity is 44.19981328892323
At time: 314.19590520858765 and batch: 1100, loss is 3.7553762197494507 and perplexity is 42.750300292730316
At time: 314.91994404792786 and batch: 1150, loss is 3.775408596992493 and perplexity is 43.615325766331964
At time: 315.6372549533844 and batch: 1200, loss is 3.7572738456726076 and perplexity is 42.83150139107616
At time: 316.3607940673828 and batch: 1250, loss is 3.799124336242676 and perplexity is 44.66205841927319
At time: 317.08407187461853 and batch: 1300, loss is 3.797119150161743 and perplexity is 44.57259240936081
At time: 317.8001024723053 and batch: 1350, loss is 3.760638403892517 and perplexity is 42.975853174968236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.369171549479167 and perplexity of 78.97817507924373
Finished 15 epochs...
Completing Train Step...
At time: 320.2968599796295 and batch: 50, loss is 3.973738446235657 and perplexity is 53.18298137674508
At time: 321.0421938896179 and batch: 100, loss is 3.970925531387329 and perplexity is 53.033592386509724
At time: 321.75813698768616 and batch: 150, loss is 3.9478003072738646 and perplexity is 51.821250542578724
At time: 322.48071932792664 and batch: 200, loss is 3.9350510454177856 and perplexity is 51.16476161663279
At time: 323.195693731308 and batch: 250, loss is 3.926730031967163 and perplexity is 50.74078534944815
At time: 323.9138677120209 and batch: 300, loss is 3.9333694171905518 and perplexity is 51.07879381246047
At time: 324.62966895103455 and batch: 350, loss is 3.933522562980652 and perplexity is 51.08661691371846
At time: 325.34438920021057 and batch: 400, loss is 3.933345317840576 and perplexity is 51.077562861564665
At time: 326.05971598625183 and batch: 450, loss is 3.8803180408477784 and perplexity is 48.43961839907334
At time: 326.7768397331238 and batch: 500, loss is 3.9568885803222655 and perplexity is 52.294362847669504
At time: 327.4975128173828 and batch: 550, loss is 3.942074155807495 and perplexity is 51.525362172093715
At time: 328.2198181152344 and batch: 600, loss is 3.8834620571136473 and perplexity is 48.592153007228774
At time: 328.9349732398987 and batch: 650, loss is 3.914669337272644 and perplexity is 50.132491823570014
At time: 329.65064454078674 and batch: 700, loss is 3.919318470954895 and perplexity is 50.36610711345002
At time: 330.36805534362793 and batch: 750, loss is 3.8803313302993776 and perplexity is 48.440262139315
At time: 331.1384677886963 and batch: 800, loss is 3.85939151763916 and perplexity is 47.43647832505429
At time: 331.8549964427948 and batch: 850, loss is 3.8316796684265135 and perplexity is 46.139973039524875
At time: 332.572979927063 and batch: 900, loss is 3.8517422008514406 and perplexity is 47.075005945960044
At time: 333.2903861999512 and batch: 950, loss is 3.834255242347717 and perplexity is 46.25896311890566
At time: 334.0093777179718 and batch: 1000, loss is 3.8403528785705565 and perplexity is 46.54189517979722
At time: 334.72448110580444 and batch: 1050, loss is 3.7867543649673463 and perplexity is 44.1129930025532
At time: 335.4417016506195 and batch: 1100, loss is 3.75362633228302 and perplexity is 42.67555749287878
At time: 336.1597681045532 and batch: 1150, loss is 3.773955502510071 and perplexity is 43.551994601338635
At time: 336.881028175354 and batch: 1200, loss is 3.7561378955841063 and perplexity is 42.782874567331625
At time: 337.59698605537415 and batch: 1250, loss is 3.7979562425613405 and perplexity is 44.60991940859106
At time: 338.3136057853699 and batch: 1300, loss is 3.7962182331085206 and perplexity is 44.53245428402853
At time: 339.028769493103 and batch: 1350, loss is 3.759729037284851 and perplexity is 42.93679013315979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.369232584635417 and perplexity of 78.98299567161133
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 341.50244998931885 and batch: 50, loss is 3.97234872341156 and perplexity is 53.109123106821094
At time: 342.2409303188324 and batch: 100, loss is 3.973190197944641 and perplexity is 53.153831889402596
At time: 342.95223021507263 and batch: 150, loss is 3.952120695114136 and perplexity is 52.04562278183136
At time: 343.66650915145874 and batch: 200, loss is 3.9400303506851198 and perplexity is 51.42016191398701
At time: 344.3836884498596 and batch: 250, loss is 3.930900077819824 and perplexity is 50.95281853775548
At time: 345.094938993454 and batch: 300, loss is 3.9371466398239137 and perplexity is 51.27209462881789
At time: 345.81006503105164 and batch: 350, loss is 3.9379929161071776 and perplexity is 51.31550335179118
At time: 346.52271914482117 and batch: 400, loss is 3.9380325841903687 and perplexity is 51.31753897982159
At time: 347.2357301712036 and batch: 450, loss is 3.8823001337051393 and perplexity is 48.535725435786354
At time: 347.95992374420166 and batch: 500, loss is 3.9579214811325074 and perplexity is 52.34840564304473
At time: 348.67073369026184 and batch: 550, loss is 3.9402978658676147 and perplexity is 51.43391942807525
At time: 349.38156604766846 and batch: 600, loss is 3.883970136642456 and perplexity is 48.61684795840083
At time: 350.1267743110657 and batch: 650, loss is 3.9127775716781614 and perplexity is 50.03774255033731
At time: 350.8385751247406 and batch: 700, loss is 3.914940161705017 and perplexity is 50.14607076588325
At time: 351.5516791343689 and batch: 750, loss is 3.876418070793152 and perplexity is 48.25107323707954
At time: 352.26290678977966 and batch: 800, loss is 3.8542345476150515 and perplexity is 47.19247951630273
At time: 352.98777627944946 and batch: 850, loss is 3.8241088342666627 and perplexity is 45.79197393944238
At time: 353.70892906188965 and batch: 900, loss is 3.8421736192703246 and perplexity is 46.626713094878816
At time: 354.4321074485779 and batch: 950, loss is 3.8247074604034426 and perplexity is 45.8193944183861
At time: 355.159539937973 and batch: 1000, loss is 3.8294232511520385 and perplexity is 46.03597937798479
At time: 355.871568441391 and batch: 1050, loss is 3.7752563095092775 and perplexity is 43.60868420386764
At time: 356.5808846950531 and batch: 1100, loss is 3.740945258140564 and perplexity is 42.137802446378885
At time: 357.3127934932709 and batch: 1150, loss is 3.7599076509475706 and perplexity is 42.944459915454466
At time: 358.02657651901245 and batch: 1200, loss is 3.741201252937317 and perplexity is 42.148590885384976
At time: 358.7374324798584 and batch: 1250, loss is 3.7841811656951903 and perplexity is 43.99962739976251
At time: 359.4464547634125 and batch: 1300, loss is 3.783348870277405 and perplexity is 43.96302194688188
At time: 360.1578447818756 and batch: 1350, loss is 3.7460515117645263 and perplexity is 42.3535190359222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3624275716145835 and perplexity of 78.44733999666563
Finished 17 epochs...
Completing Train Step...
At time: 362.6716499328613 and batch: 50, loss is 3.968943085670471 and perplexity is 52.928560313024406
At time: 363.3849642276764 and batch: 100, loss is 3.968205623626709 and perplexity is 52.88954189783104
At time: 364.100145816803 and batch: 150, loss is 3.9455017042160034 and perplexity is 51.7022708535444
At time: 364.8153066635132 and batch: 200, loss is 3.9329828214645386 and perplexity is 51.05905078561346
At time: 365.53094696998596 and batch: 250, loss is 3.9236578369140624 and perplexity is 50.58513897014621
At time: 366.2469937801361 and batch: 300, loss is 3.9297131967544554 and perplexity is 50.892379476289655
At time: 366.9680163860321 and batch: 350, loss is 3.9312793827056884 and perplexity is 50.97214885658584
At time: 367.6892967224121 and batch: 400, loss is 3.9316635799407957 and perplexity is 50.99173597766197
At time: 368.4360525608063 and batch: 450, loss is 3.876571063995361 and perplexity is 48.25845588801742
At time: 369.1544449329376 and batch: 500, loss is 3.952287721633911 and perplexity is 52.054316507095265
At time: 369.8723027706146 and batch: 550, loss is 3.935761694908142 and perplexity is 51.20113475114383
At time: 370.5932376384735 and batch: 600, loss is 3.879951043128967 and perplexity is 48.421844431323095
At time: 371.30748558044434 and batch: 650, loss is 3.9091326284408567 and perplexity is 49.85568980653394
At time: 372.02332878112793 and batch: 700, loss is 3.9119155406951904 and perplexity is 49.9946270520586
At time: 372.7387068271637 and batch: 750, loss is 3.87374577999115 and perplexity is 48.122304468309174
At time: 373.45710039138794 and batch: 800, loss is 3.851904578208923 and perplexity is 47.08265048166198
At time: 374.17518758773804 and batch: 850, loss is 3.8224976587295534 and perplexity is 45.7182544347138
At time: 374.8950717449188 and batch: 900, loss is 3.8412935304641724 and perplexity is 46.58569549882745
At time: 375.612042427063 and batch: 950, loss is 3.8242116022109984 and perplexity is 45.79668012828974
At time: 376.3310754299164 and batch: 1000, loss is 3.829287281036377 and perplexity is 46.02972028607854
At time: 377.0497090816498 and batch: 1050, loss is 3.77544219493866 and perplexity is 43.616791176316354
At time: 377.7660183906555 and batch: 1100, loss is 3.7415153408050537 and perplexity is 42.16183132564624
At time: 378.4805042743683 and batch: 1150, loss is 3.760900549888611 and perplexity is 42.98712059959738
At time: 379.20308351516724 and batch: 1200, loss is 3.742492790222168 and perplexity is 42.20306253052584
At time: 379.9207835197449 and batch: 1250, loss is 3.7856154251098633 and perplexity is 44.06277955706495
At time: 380.63582491874695 and batch: 1300, loss is 3.7848832416534424 and perplexity is 44.03052932681263
At time: 381.3489134311676 and batch: 1350, loss is 3.747475986480713 and perplexity is 42.41389354370134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.362055257161458 and perplexity of 78.41813835460965
Finished 18 epochs...
Completing Train Step...
At time: 383.9017262458801 and batch: 50, loss is 3.9663148593902586 and perplexity is 52.78963472374815
At time: 384.61631870269775 and batch: 100, loss is 3.965200433731079 and perplexity is 52.73083736899895
At time: 385.3320667743683 and batch: 150, loss is 3.9422670888900755 and perplexity is 51.535304078079086
At time: 386.04989671707153 and batch: 200, loss is 3.9296064805984496 and perplexity is 50.886948726961464
At time: 386.79901695251465 and batch: 250, loss is 3.9201444244384764 and perplexity is 50.40772435966361
At time: 387.52170038223267 and batch: 300, loss is 3.9263369989395143 and perplexity is 50.720846463533995
At time: 388.2400097846985 and batch: 350, loss is 3.928092427253723 and perplexity is 50.80996146815368
At time: 388.959125995636 and batch: 400, loss is 3.9285493183135984 and perplexity is 50.83318138938438
At time: 389.68635511398315 and batch: 450, loss is 3.8737196254730226 and perplexity is 48.12104586908373
At time: 390.40737414360046 and batch: 500, loss is 3.9495720052719117 and perplexity is 51.91314352769318
At time: 391.1282205581665 and batch: 550, loss is 3.9332983255386353 and perplexity is 51.075162665704084
At time: 391.84832978248596 and batch: 600, loss is 3.8777113008499144 and perplexity is 48.31351334126823
At time: 392.5638632774353 and batch: 650, loss is 3.907156171798706 and perplexity is 49.75724951080042
At time: 393.2802746295929 and batch: 700, loss is 3.910170121192932 and perplexity is 49.907441564752865
At time: 394.00332736968994 and batch: 750, loss is 3.872182607650757 and perplexity is 48.0471397759978
At time: 394.722149848938 and batch: 800, loss is 3.850439739227295 and perplexity is 47.013732469097164
At time: 395.43826627731323 and batch: 850, loss is 3.821367883682251 and perplexity is 45.66663225786173
At time: 396.15829205513 and batch: 900, loss is 3.8405153465271 and perplexity is 46.549457360690134
At time: 396.8750991821289 and batch: 950, loss is 3.823619074821472 and perplexity is 45.76955237872535
At time: 397.5922327041626 and batch: 1000, loss is 3.8289019680023193 and perplexity is 46.01198785138678
At time: 398.31095814704895 and batch: 1050, loss is 3.775265393257141 and perplexity is 43.609080335958794
At time: 399.02824687957764 and batch: 1100, loss is 3.7414754819869995 and perplexity is 42.160150838373944
At time: 399.74309945106506 and batch: 1150, loss is 3.761046276092529 and perplexity is 42.99338540596187
At time: 400.46177339553833 and batch: 1200, loss is 3.742805800437927 and perplexity is 42.21627458788081
At time: 401.17664074897766 and batch: 1250, loss is 3.7859606027603148 and perplexity is 44.07799166907432
At time: 401.89272475242615 and batch: 1300, loss is 3.785255722999573 and perplexity is 44.04693293247603
At time: 402.6093165874481 and batch: 1350, loss is 3.7477809953689576 and perplexity is 42.42683213130904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.362035725911459 and perplexity of 78.41660676530194
Finished 19 epochs...
Completing Train Step...
At time: 405.14460253715515 and batch: 50, loss is 3.9639213514328 and perplexity is 52.66343340514504
At time: 405.9178521633148 and batch: 100, loss is 3.962560982704163 and perplexity is 52.591840424664774
At time: 406.63256764411926 and batch: 150, loss is 3.9396378231048583 and perplexity is 51.39998204309131
At time: 407.35263204574585 and batch: 200, loss is 3.9269212293624878 and perplexity is 50.75048778295414
At time: 408.0694942474365 and batch: 250, loss is 3.9173743438720705 and perplexity is 50.268284121529504
At time: 408.7858712673187 and batch: 300, loss is 3.923667950630188 and perplexity is 50.58565057646904
At time: 409.5192348957062 and batch: 350, loss is 3.9255975770950315 and perplexity is 50.68335622394739
At time: 410.23834109306335 and batch: 400, loss is 3.926105694770813 and perplexity is 50.70911587702601
At time: 410.9543857574463 and batch: 450, loss is 3.871437783241272 and perplexity is 48.0113664175758
At time: 411.6759479045868 and batch: 500, loss is 3.947420873641968 and perplexity is 51.8015915471538
At time: 412.3918237686157 and batch: 550, loss is 3.9312848234176636 and perplexity is 50.97242618212095
At time: 413.10656785964966 and batch: 600, loss is 3.8758792209625246 and perplexity is 48.22508015825016
At time: 413.82117199897766 and batch: 650, loss is 3.9054480600357055 and perplexity is 49.672331113320105
At time: 414.5387372970581 and batch: 700, loss is 3.9086754512786865 and perplexity is 49.83290213314898
At time: 415.25318789482117 and batch: 750, loss is 3.8708054876327513 and perplexity is 47.981018636826214
At time: 415.96911096572876 and batch: 800, loss is 3.8491125011444094 and perplexity is 46.951375443409
At time: 416.6878731250763 and batch: 850, loss is 3.8202915000915527 and perplexity is 45.61750388948504
At time: 417.4071435928345 and batch: 900, loss is 3.8396281480789183 and perplexity is 46.50817706896913
At time: 418.1205544471741 and batch: 950, loss is 3.82289155960083 and perplexity is 45.73626644220871
At time: 418.8358416557312 and batch: 1000, loss is 3.8283522081375123 and perplexity is 45.98669925912956
At time: 419.5529022216797 and batch: 1050, loss is 3.7748805141448973 and perplexity is 43.59229934136762
At time: 420.2630753517151 and batch: 1100, loss is 3.741178045272827 and perplexity is 42.147612726379414
At time: 420.9795198440552 and batch: 1150, loss is 3.760843195915222 and perplexity is 42.9846551881277
At time: 421.7004177570343 and batch: 1200, loss is 3.7427191829681394 and perplexity is 42.212618079353184
At time: 422.41641068458557 and batch: 1250, loss is 3.7858949756622313 and perplexity is 44.07509905330976
At time: 423.13156843185425 and batch: 1300, loss is 3.785228424072266 and perplexity is 44.04573051486823
At time: 423.84801864624023 and batch: 1350, loss is 3.7477191495895386 and perplexity is 42.424208291945135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.361985270182291 and perplexity of 78.41265029804283
Finished 20 epochs...
Completing Train Step...
At time: 426.3738148212433 and batch: 50, loss is 3.961676173210144 and perplexity is 52.54532724563487
At time: 427.1279671192169 and batch: 100, loss is 3.960145936012268 and perplexity is 52.464981920689695
At time: 427.8426971435547 and batch: 150, loss is 3.937288565635681 and perplexity is 51.27937197887879
At time: 428.5628459453583 and batch: 200, loss is 3.9245383644104006 and perplexity is 50.62970019172682
At time: 429.2802276611328 and batch: 250, loss is 3.9149442386627196 and perplexity is 50.146275209709465
At time: 429.9997732639313 and batch: 300, loss is 3.921309370994568 and perplexity is 50.466480882016114
At time: 430.721804857254 and batch: 350, loss is 3.923403973579407 and perplexity is 50.57229888796523
At time: 431.4539477825165 and batch: 400, loss is 3.9239613389968873 and perplexity is 50.60049399520681
At time: 432.19563245773315 and batch: 450, loss is 3.86940318107605 and perplexity is 47.913781694215565
At time: 432.9347014427185 and batch: 500, loss is 3.9455085706710817 and perplexity is 51.7026258660835
At time: 433.6898396015167 and batch: 550, loss is 3.929474182128906 and perplexity is 50.88021690683971
At time: 434.44253849983215 and batch: 600, loss is 3.8742275762557985 and perplexity is 48.14549520100388
At time: 435.1714267730713 and batch: 650, loss is 3.9038864088058474 and perplexity is 49.59482079414209
At time: 435.8984489440918 and batch: 700, loss is 3.9072783184051514 and perplexity is 49.76332756117328
At time: 436.6494140625 and batch: 750, loss is 3.8695038986206054 and perplexity is 47.91860769568561
At time: 437.38141918182373 and batch: 800, loss is 3.847844467163086 and perplexity is 46.89187723472441
At time: 438.097186088562 and batch: 850, loss is 3.8192419147491456 and perplexity is 45.56964954404836
At time: 438.81894183158875 and batch: 900, loss is 3.8387486362457275 and perplexity is 46.467290559616856
At time: 439.5413177013397 and batch: 950, loss is 3.8221843528747557 and perplexity is 45.70393288155865
At time: 440.26069045066833 and batch: 1000, loss is 3.8276517677307127 and perplexity is 45.95449959508249
At time: 440.9780731201172 and batch: 1050, loss is 3.774343752861023 and perplexity is 43.56890696142969
At time: 441.69791078567505 and batch: 1100, loss is 3.7407326221466066 and perplexity is 42.12884338541569
At time: 442.47561740875244 and batch: 1150, loss is 3.7604819679260255 and perplexity is 42.96913073167068
At time: 443.19592213630676 and batch: 1200, loss is 3.7424360370635985 and perplexity is 42.200667441391005
At time: 443.91832065582275 and batch: 1250, loss is 3.785651931762695 and perplexity is 44.064388171023424
At time: 444.6369068622589 and batch: 1300, loss is 3.7850425481796264 and perplexity is 44.03754423623133
At time: 445.35643792152405 and batch: 1350, loss is 3.7474874687194824 and perplexity is 42.41438055295013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.361968994140625 and perplexity of 78.41137406086548
Finished 21 epochs...
Completing Train Step...
At time: 447.89014768600464 and batch: 50, loss is 3.9595756101608277 and perplexity is 52.43506831626677
At time: 448.60525941848755 and batch: 100, loss is 3.9579237508773804 and perplexity is 52.34852446070488
At time: 449.32921981811523 and batch: 150, loss is 3.9351383876800536 and perplexity is 51.16923065782603
At time: 450.0468668937683 and batch: 200, loss is 3.9223686599731447 and perplexity is 50.519967793048224
At time: 450.7648322582245 and batch: 250, loss is 3.9127328395843506 and perplexity is 50.03550430740449
At time: 451.48093461990356 and batch: 300, loss is 3.919153823852539 and perplexity is 50.357815162498426
At time: 452.19774413108826 and batch: 350, loss is 3.9214141511917116 and perplexity is 50.47176904687472
At time: 452.9131257534027 and batch: 400, loss is 3.92200581073761 and perplexity is 50.50163998667163
At time: 453.6297092437744 and batch: 450, loss is 3.8675306987762452 and perplexity is 47.824147931083324
At time: 454.3457431793213 and batch: 500, loss is 3.943743200302124 and perplexity is 51.6114321014512
At time: 455.06142807006836 and batch: 550, loss is 3.927794919013977 and perplexity is 50.794847334356874
At time: 455.7748908996582 and batch: 600, loss is 3.872688846588135 and perplexity is 48.071469266730226
At time: 456.49325251579285 and batch: 650, loss is 3.902415361404419 and perplexity is 49.521918096697036
At time: 457.2123692035675 and batch: 700, loss is 3.9059432315826417 and perplexity is 49.696933529062846
At time: 457.9305274486542 and batch: 750, loss is 3.8682476615905763 and perplexity is 47.858448361374194
At time: 458.65126967430115 and batch: 800, loss is 3.846600961685181 and perplexity is 46.83360316808346
At time: 459.37522625923157 and batch: 850, loss is 3.818191566467285 and perplexity is 45.52181066908296
At time: 460.0919609069824 and batch: 900, loss is 3.8377661228179933 and perplexity is 46.42165824354411
At time: 460.86804270744324 and batch: 950, loss is 3.821382608413696 and perplexity is 45.66730469170842
At time: 461.58509039878845 and batch: 1000, loss is 3.8269114303588867 and perplexity is 45.920490352338746
At time: 462.3099114894867 and batch: 1050, loss is 3.773740234375 and perplexity is 43.54262025371755
At time: 463.02456045150757 and batch: 1100, loss is 3.7401995611190797 and perplexity is 42.10639212534966
At time: 463.74260783195496 and batch: 1150, loss is 3.7600092267990113 and perplexity is 42.94882225708557
At time: 464.4595799446106 and batch: 1200, loss is 3.7420177602767946 and perplexity is 42.1830195729168
At time: 465.1776921749115 and batch: 1250, loss is 3.785282392501831 and perplexity is 44.04810765791794
At time: 465.8941099643707 and batch: 1300, loss is 3.784728889465332 and perplexity is 44.02373364274523
At time: 466.61902046203613 and batch: 1350, loss is 3.7471531867980956 and perplexity is 42.40020456184524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.361980794270833 and perplexity of 78.41229933074833
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 469.199414730072 and batch: 50, loss is 3.9593285799026487 and perplexity is 52.4221168675687
At time: 469.9211473464966 and batch: 100, loss is 3.959979648590088 and perplexity is 52.45625837941908
At time: 470.63767290115356 and batch: 150, loss is 3.9387698698043825 and perplexity is 51.35538861433896
At time: 471.35510325431824 and batch: 200, loss is 3.927017397880554 and perplexity is 50.75536861684289
At time: 472.07715225219727 and batch: 250, loss is 3.9166949796676636 and perplexity is 50.23414524635829
At time: 472.80117988586426 and batch: 300, loss is 3.922769236564636 and perplexity is 50.540208963347865
At time: 473.51687812805176 and batch: 350, loss is 3.925266423225403 and perplexity is 50.66657501314286
At time: 474.23444652557373 and batch: 400, loss is 3.926178126335144 and perplexity is 50.71278895063644
At time: 474.95839977264404 and batch: 450, loss is 3.871288571357727 and perplexity is 48.00420308560149
At time: 475.68018770217896 and batch: 500, loss is 3.9460675716400146 and perplexity is 51.73153576361656
At time: 476.3981819152832 and batch: 550, loss is 3.9284911489486696 and perplexity is 50.83022454150546
At time: 477.1191198825836 and batch: 600, loss is 3.8732827949523925 and perplexity is 48.10002971814574
At time: 477.8359417915344 and batch: 650, loss is 3.9022150564193727 and perplexity is 49.51199960302825
At time: 478.5531361103058 and batch: 700, loss is 3.905481481552124 and perplexity is 49.6739912656922
At time: 479.2677595615387 and batch: 750, loss is 3.867824149131775 and perplexity is 47.83818400364113
At time: 480.04113364219666 and batch: 800, loss is 3.845923867225647 and perplexity is 46.80190312803017
At time: 480.75586462020874 and batch: 850, loss is 3.8158989334106446 and perplexity is 45.41756540487511
At time: 481.4779860973358 and batch: 900, loss is 3.834372458457947 and perplexity is 46.264385732428345
At time: 482.1959300041199 and batch: 950, loss is 3.8172526264190676 and perplexity is 45.47908847790519
At time: 482.9181959629059 and batch: 1000, loss is 3.8226153564453127 and perplexity is 45.723635685503524
At time: 483.6348283290863 and batch: 1050, loss is 3.768928737640381 and perplexity is 43.33361828789627
At time: 484.35321950912476 and batch: 1100, loss is 3.73520676612854 and perplexity is 41.8966874834456
At time: 485.07217240333557 and batch: 1150, loss is 3.753739719390869 and perplexity is 42.680396625261174
At time: 485.7934000492096 and batch: 1200, loss is 3.7349683904647826 and perplexity is 41.886701523009755
At time: 486.51436138153076 and batch: 1250, loss is 3.77809455871582 and perplexity is 43.732632331838595
At time: 487.23022985458374 and batch: 1300, loss is 3.7781641769409178 and perplexity is 43.73567702606226
At time: 487.9481792449951 and batch: 1350, loss is 3.741366286277771 and perplexity is 42.15554738214538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.358683268229167 and perplexity of 78.15415857855623
Finished 23 epochs...
Completing Train Step...
At time: 490.45438051223755 and batch: 50, loss is 3.9582403087615967 and perplexity is 52.36509842202067
At time: 491.1983046531677 and batch: 100, loss is 3.9582814598083496 and perplexity is 52.36725334497242
At time: 491.91210293769836 and batch: 150, loss is 3.936041598320007 and perplexity is 51.2154681293392
At time: 492.62907004356384 and batch: 200, loss is 3.9238272953033446 and perplexity is 50.593711772663845
At time: 493.35029101371765 and batch: 250, loss is 3.913224139213562 and perplexity is 50.06009277177525
At time: 494.06624960899353 and batch: 300, loss is 3.919439334869385 and perplexity is 50.372194926204386
At time: 494.7836244106293 and batch: 350, loss is 3.921824836730957 and perplexity is 50.49250132949534
At time: 495.49768829345703 and batch: 400, loss is 3.922738428115845 and perplexity is 50.538651921893255
At time: 496.2076916694641 and batch: 450, loss is 3.868193335533142 and perplexity is 47.855848471181325
At time: 496.9166724681854 and batch: 500, loss is 3.943093514442444 and perplexity is 51.57791177383753
At time: 497.6337397098541 and batch: 550, loss is 3.9262858533859255 and perplexity is 50.71825238410151
At time: 498.39875841140747 and batch: 600, loss is 3.8713143825531007 and perplexity is 48.00544214745685
At time: 499.11494970321655 and batch: 650, loss is 3.9004877614974975 and perplexity is 49.42655159574144
At time: 499.8296608924866 and batch: 700, loss is 3.9039260864257814 and perplexity is 49.596788637631676
At time: 500.5425102710724 and batch: 750, loss is 3.8665780925750735 and perplexity is 47.778612043526344
At time: 501.25274682044983 and batch: 800, loss is 3.844757413864136 and perplexity is 46.747342718074464
At time: 501.9660065174103 and batch: 850, loss is 3.815278720855713 and perplexity is 45.38940559403337
At time: 502.67952132225037 and batch: 900, loss is 3.834127917289734 and perplexity is 46.253073568695925
At time: 503.39048504829407 and batch: 950, loss is 3.817079920768738 and perplexity is 45.471234660572044
At time: 504.10342836380005 and batch: 1000, loss is 3.8227051067352296 and perplexity is 45.727739579222416
At time: 504.81816005706787 and batch: 1050, loss is 3.769273066520691 and perplexity is 43.34854187332439
At time: 505.53191089630127 and batch: 1100, loss is 3.7358063745498655 and perplexity is 41.92181662315532
At time: 506.2474219799042 and batch: 1150, loss is 3.7545834589004516 and perplexity is 42.71642295846166
At time: 506.959938287735 and batch: 1200, loss is 3.7359472846984865 and perplexity is 41.927724248778574
At time: 507.6745328903198 and batch: 1250, loss is 3.7792729139328003 and perplexity is 43.78419528106785
At time: 508.39249563217163 and batch: 1300, loss is 3.7794653177261353 and perplexity is 43.7926203368084
At time: 509.10507369041443 and batch: 1350, loss is 3.74258225440979 and perplexity is 42.206838362128636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3583467610677085 and perplexity of 78.12786356897443
Finished 24 epochs...
Completing Train Step...
At time: 511.5884392261505 and batch: 50, loss is 3.95745388507843 and perplexity is 52.32393345712349
At time: 512.3889963626862 and batch: 100, loss is 3.9572647476196288 and perplexity is 52.31403797714738
At time: 513.1364009380341 and batch: 150, loss is 3.9347980785369874 and perplexity is 51.15182026341573
At time: 513.8661472797394 and batch: 200, loss is 3.9224288702011108 and perplexity is 50.523009703402025
At time: 514.601438999176 and batch: 250, loss is 3.9117318391799927 and perplexity is 49.98544380683124
At time: 515.3256723880768 and batch: 300, loss is 3.9180083227157594 and perplexity is 50.30016325445124
At time: 516.0431499481201 and batch: 350, loss is 3.920406994819641 and perplexity is 50.42096167284951
At time: 516.7597093582153 and batch: 400, loss is 3.9213583421707154 and perplexity is 50.468952345455676
At time: 517.5315508842468 and batch: 450, loss is 3.866917986869812 and perplexity is 47.79485448137007
At time: 518.2472951412201 and batch: 500, loss is 3.941848301887512 and perplexity is 51.51372628112386
At time: 518.9653921127319 and batch: 550, loss is 3.9252644681930544 and perplexity is 50.666475958446554
At time: 519.6829228401184 and batch: 600, loss is 3.8703856325149535 and perplexity is 47.960877789009274
At time: 520.3984718322754 and batch: 650, loss is 3.8996522092819212 and perplexity is 49.38527037975476
At time: 521.1131083965302 and batch: 700, loss is 3.9032261848449705 and perplexity is 49.56208791182402
At time: 521.8289532661438 and batch: 750, loss is 3.8659948825836183 and perplexity is 47.75075520358798
At time: 522.5456140041351 and batch: 800, loss is 3.844214973449707 and perplexity is 46.721991946380186
At time: 523.2666945457458 and batch: 850, loss is 3.814933214187622 and perplexity is 45.37372596060489
At time: 523.981437921524 and batch: 900, loss is 3.833957347869873 and perplexity is 46.245184881574055
At time: 524.6985814571381 and batch: 950, loss is 3.816948552131653 and perplexity is 45.465261558795824
At time: 525.4165344238281 and batch: 1000, loss is 3.822682466506958 and perplexity is 45.72670430447947
At time: 526.1397113800049 and batch: 1050, loss is 3.769357929229736 and perplexity is 43.35222070411649
At time: 526.8569192886353 and batch: 1100, loss is 3.7359806299209595 and perplexity is 41.929122361381495
At time: 527.5706663131714 and batch: 1150, loss is 3.7548541164398195 and perplexity is 42.727986045137904
At time: 528.2879314422607 and batch: 1200, loss is 3.7362514305114747 and perplexity is 41.94047833000885
At time: 529.0027847290039 and batch: 1250, loss is 3.779636263847351 and perplexity is 43.80010715529591
At time: 529.718602180481 and batch: 1300, loss is 3.7798611974716185 and perplexity is 43.80996038026091
At time: 530.4338352680206 and batch: 1350, loss is 3.7429354095458987 and perplexity is 42.221746556172825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.358266194661458 and perplexity of 78.12156934133371
Finished 25 epochs...
Completing Train Step...
At time: 532.9686636924744 and batch: 50, loss is 3.9566401481628417 and perplexity is 52.28137285981374
At time: 533.6832633018494 and batch: 100, loss is 3.9563384199142457 and perplexity is 52.26560047235364
At time: 534.4021093845367 and batch: 150, loss is 3.933811078071594 and perplexity is 51.1013583000972
At time: 535.1183938980103 and batch: 200, loss is 3.921369318962097 and perplexity is 50.469506335657336
At time: 535.892067193985 and batch: 250, loss is 3.910622253417969 and perplexity is 49.93001142925048
At time: 536.6111311912537 and batch: 300, loss is 3.9169626331329344 and perplexity is 50.247592388915265
At time: 537.3305070400238 and batch: 350, loss is 3.9194117641448973 and perplexity is 50.37080614744116
At time: 538.045996427536 and batch: 400, loss is 3.9203893756866455 and perplexity is 50.42007330704616
At time: 538.7633764743805 and batch: 450, loss is 3.866018214225769 and perplexity is 47.751869320117855
At time: 539.4791629314423 and batch: 500, loss is 3.940966987609863 and perplexity is 51.46834649851983
At time: 540.2008790969849 and batch: 550, loss is 3.9244948768615724 and perplexity is 50.62749847804148
At time: 540.9163439273834 and batch: 600, loss is 3.8696782541275025 and perplexity is 47.9269632972216
At time: 541.6366128921509 and batch: 650, loss is 3.8990035915374754 and perplexity is 49.3532486031411
At time: 542.3563103675842 and batch: 700, loss is 3.9026760149002073 and perplexity is 49.534827840178885
At time: 543.0708792209625 and batch: 750, loss is 3.86552095413208 and perplexity is 47.72813012387254
At time: 543.7877068519592 and batch: 800, loss is 3.843766884803772 and perplexity is 46.701061042074045
At time: 544.5105092525482 and batch: 850, loss is 3.8146022272109987 and perplexity is 45.358710333356655
At time: 545.2285439968109 and batch: 900, loss is 3.833739833831787 and perplexity is 46.235126998573456
At time: 545.948474407196 and batch: 950, loss is 3.816761589050293 and perplexity is 45.45676202797396
At time: 546.6698825359344 and batch: 1000, loss is 3.8225649547576905 and perplexity is 45.72133119517634
At time: 547.3861701488495 and batch: 1050, loss is 3.7693197441101076 and perplexity is 43.350565325988356
At time: 548.1077020168304 and batch: 1100, loss is 3.735991859436035 and perplexity is 41.92959320773684
At time: 548.8229782581329 and batch: 1150, loss is 3.754918990135193 and perplexity is 42.73075805740288
At time: 549.5477631092072 and batch: 1200, loss is 3.736337909698486 and perplexity is 41.944105465311296
At time: 550.2635204792023 and batch: 1250, loss is 3.7797490549087525 and perplexity is 43.80504769449055
At time: 550.981547832489 and batch: 1300, loss is 3.779991002082825 and perplexity is 43.81564748423322
At time: 551.6978785991669 and batch: 1350, loss is 3.7430376958847047 and perplexity is 42.226065484925996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.358236083984375 and perplexity of 78.11921708340026
Finished 26 epochs...
Completing Train Step...
At time: 554.227602481842 and batch: 50, loss is 3.9558466720581054 and perplexity is 52.239905293663774
At time: 554.940664768219 and batch: 100, loss is 3.9554690313339234 and perplexity is 52.220181102560794
At time: 555.6581342220306 and batch: 150, loss is 3.9329332447052003 and perplexity is 51.056519506087454
At time: 556.3751447200775 and batch: 200, loss is 3.9204517936706544 and perplexity is 50.42322052459604
At time: 557.099018573761 and batch: 250, loss is 3.909670715332031 and perplexity is 49.88252371850997
At time: 557.8120982646942 and batch: 300, loss is 3.9160655879974366 and perplexity is 50.20253824141546
At time: 558.5293297767639 and batch: 350, loss is 3.9185716772079466 and perplexity is 50.32850806071571
At time: 559.250349521637 and batch: 400, loss is 3.9195697927474975 and perplexity is 50.37876680453767
At time: 559.9672858715057 and batch: 450, loss is 3.8652541494369506 and perplexity is 47.71539773327206
At time: 560.6911380290985 and batch: 500, loss is 3.9402182340621947 and perplexity is 51.42982381528405
At time: 561.409325838089 and batch: 550, loss is 3.9238188076019287 and perplexity is 50.59328235016721
At time: 562.1203660964966 and batch: 600, loss is 3.8690551280975343 and perplexity is 47.897108061607014
At time: 562.8287589550018 and batch: 650, loss is 3.898426637649536 and perplexity is 49.324782267148336
At time: 563.5485475063324 and batch: 700, loss is 3.902175579071045 and perplexity is 49.51004503915428
At time: 564.2675693035126 and batch: 750, loss is 3.8650806093215944 and perplexity is 47.70711791610616
At time: 564.9854686260223 and batch: 800, loss is 3.843341951370239 and perplexity is 46.681220415627116
At time: 565.7041685581207 and batch: 850, loss is 3.814267568588257 and perplexity is 45.343533189549824
At time: 566.419460773468 and batch: 900, loss is 3.833488426208496 and perplexity is 46.22350459622367
At time: 567.1365699768066 and batch: 950, loss is 3.816537690162659 and perplexity is 45.44658544882533
At time: 567.8535168170929 and batch: 1000, loss is 3.822395586967468 and perplexity is 45.71358813007721
At time: 568.573489189148 and batch: 1050, loss is 3.769216985702515 and perplexity is 43.346110919794974
At time: 569.2894556522369 and batch: 1100, loss is 3.735927019119263 and perplexity is 41.92687456777082
At time: 570.0051929950714 and batch: 1150, loss is 3.7548877668380736 and perplexity is 42.72942388307669
At time: 570.7214705944061 and batch: 1200, loss is 3.736327347755432 and perplexity is 41.94366245639745
At time: 571.4379260540009 and batch: 1250, loss is 3.7797562408447267 and perplexity is 43.80536247588962
At time: 572.1576120853424 and batch: 1300, loss is 3.7800127553939817 and perplexity is 43.816600630013475
At time: 572.8775012493134 and batch: 1350, loss is 3.743039526939392 and perplexity is 42.226142803231916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3582246907552085 and perplexity of 78.11832705832786
Finished 27 epochs...
Completing Train Step...
At time: 575.3863990306854 and batch: 50, loss is 3.9550786447525024 and perplexity is 52.199799023282964
At time: 576.1649641990662 and batch: 100, loss is 3.954642095565796 and perplexity is 52.17701621674276
At time: 576.88649559021 and batch: 150, loss is 3.932119107246399 and perplexity is 51.014969397086624
At time: 577.6051635742188 and batch: 200, loss is 3.919613437652588 and perplexity is 50.38096562901682
At time: 578.3269884586334 and batch: 250, loss is 3.9088050365447997 and perplexity is 49.83936016145672
At time: 579.0533475875854 and batch: 300, loss is 3.9152458572387694 and perplexity is 50.16140253905951
At time: 579.7712090015411 and batch: 350, loss is 3.9178102922439577 and perplexity is 50.29020327561247
At time: 580.4899435043335 and batch: 400, loss is 3.918825879096985 and perplexity is 50.3413032887539
At time: 581.2120249271393 and batch: 450, loss is 3.864556951522827 and perplexity is 47.682142251674136
At time: 581.9304118156433 and batch: 500, loss is 3.939535799026489 and perplexity is 51.394738274795046
At time: 582.6501188278198 and batch: 550, loss is 3.923191337585449 and perplexity is 50.56154654013306
At time: 583.3672225475311 and batch: 600, loss is 3.8684754180908203 and perplexity is 47.86934967545554
At time: 584.0864150524139 and batch: 650, loss is 3.897885627746582 and perplexity is 49.29810428865673
At time: 584.8172013759613 and batch: 700, loss is 3.9016983461380006 and perplexity is 49.48642285223618
At time: 585.5407702922821 and batch: 750, loss is 3.864654564857483 and perplexity is 47.68679689175663
At time: 586.277918100357 and batch: 800, loss is 3.842925200462341 and perplexity is 46.661770027902
At time: 586.9936814308167 and batch: 850, loss is 3.813928275108337 and perplexity is 45.32815103406218
At time: 587.7243394851685 and batch: 900, loss is 3.833216905593872 and perplexity is 46.21095566556941
At time: 588.4408793449402 and batch: 950, loss is 3.816291317939758 and perplexity is 45.435390051719125
At time: 589.169837474823 and batch: 1000, loss is 3.822195725440979 and perplexity is 45.70445265551759
At time: 589.8978233337402 and batch: 1050, loss is 3.769075689315796 and perplexity is 43.339986703618685
At time: 590.6285419464111 and batch: 1100, loss is 3.7358187532424925 and perplexity is 41.922335563649554
At time: 591.399480342865 and batch: 1150, loss is 3.7548027849197387 and perplexity is 42.725792808955774
At time: 592.1141657829285 and batch: 1200, loss is 3.7362617206573487 and perplexity is 41.94090990586938
At time: 592.8293116092682 and batch: 1250, loss is 3.779706554412842 and perplexity is 43.80318599780194
At time: 593.5444111824036 and batch: 1300, loss is 3.779978413581848 and perplexity is 43.81509591438379
At time: 594.2611889839172 and batch: 1350, loss is 3.742990503311157 and perplexity is 42.22407277524589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.358221028645834 and perplexity of 78.11804098099383
Finished 28 epochs...
Completing Train Step...
At time: 596.757809638977 and batch: 50, loss is 3.9543336486816405 and perplexity is 52.16092486045811
At time: 597.5060932636261 and batch: 100, loss is 3.9538478660583496 and perplexity is 52.13559214314123
At time: 598.2239124774933 and batch: 150, loss is 3.9313483905792235 and perplexity is 50.97566645755763
At time: 598.9421603679657 and batch: 200, loss is 3.9188265705108645 and perplexity is 50.341338095441735
At time: 599.6706173419952 and batch: 250, loss is 3.9079941940307616 and perplexity is 49.79896466877
At time: 600.3870129585266 and batch: 300, loss is 3.914474940299988 and perplexity is 50.12274716612445
At time: 601.1044275760651 and batch: 350, loss is 3.917097067832947 and perplexity is 50.25434786299931
At time: 601.8206944465637 and batch: 400, loss is 3.9181287717819213 and perplexity is 50.3062222270345
At time: 602.5370099544525 and batch: 450, loss is 3.863899908065796 and perplexity is 47.65082330217258
At time: 603.2529664039612 and batch: 500, loss is 3.9388935279846193 and perplexity is 51.36173952090305
At time: 603.9699242115021 and batch: 550, loss is 3.9225949382781984 and perplexity is 50.53140065918781
At time: 604.6886129379272 and batch: 600, loss is 3.867922763824463 and perplexity is 47.8429017840728
At time: 605.4054493904114 and batch: 650, loss is 3.8973663997650148 and perplexity is 49.27251397764954
At time: 606.1212887763977 and batch: 700, loss is 3.9012345933914183 and perplexity is 49.46347870833601
At time: 606.8371632099152 and batch: 750, loss is 3.8642361831665037 and perplexity is 47.66684978207939
At time: 607.5532941818237 and batch: 800, loss is 3.8425124359130858 and perplexity is 46.6425136778717
At time: 608.2712662220001 and batch: 850, loss is 3.8135855436325072 and perplexity is 45.31261831189008
At time: 608.9852888584137 and batch: 900, loss is 3.832933521270752 and perplexity is 46.19786206052622
At time: 609.7563483715057 and batch: 950, loss is 3.8160301208496095 and perplexity is 45.42352400980312
At time: 610.4716680049896 and batch: 1000, loss is 3.821976113319397 and perplexity is 45.69441650577456
At time: 611.1884016990662 and batch: 1050, loss is 3.7689091253280638 and perplexity is 43.332768423774496
At time: 611.9074990749359 and batch: 1100, loss is 3.7356824922561644 and perplexity is 41.91662357402599
At time: 612.6221599578857 and batch: 1150, loss is 3.7546840572357176 and perplexity is 42.72072037565269
At time: 613.3372645378113 and batch: 1200, loss is 3.7361596918106077 and perplexity is 41.93663094149303
At time: 614.0563986301422 and batch: 1250, loss is 3.779620928764343 and perplexity is 43.79943548216703
At time: 614.7718749046326 and batch: 1300, loss is 3.7799098825454713 and perplexity is 43.81209332333834
At time: 615.4905498027802 and batch: 1350, loss is 3.7429108238220214 and perplexity is 42.22070851673091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.358224283854167 and perplexity of 78.11829527190568
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 617.9896574020386 and batch: 50, loss is 3.9542742443084715 and perplexity is 52.15782636544585
At time: 618.7075097560883 and batch: 100, loss is 3.954804482460022 and perplexity is 52.18548976833442
At time: 619.422033071518 and batch: 150, loss is 3.933045949935913 and perplexity is 51.06227416718188
At time: 620.1388046741486 and batch: 200, loss is 3.9211278772354126 and perplexity is 50.45732236182177
At time: 620.8537859916687 and batch: 250, loss is 3.910015959739685 and perplexity is 49.89974835404691
At time: 621.570778131485 and batch: 300, loss is 3.916643295288086 and perplexity is 50.231548992821075
At time: 622.2888214588165 and batch: 350, loss is 3.9191625356674193 and perplexity is 50.35825387237286
At time: 623.0050318241119 and batch: 400, loss is 3.920292549133301 and perplexity is 50.41519154147454
At time: 623.7223219871521 and batch: 450, loss is 3.8660648965835573 and perplexity is 47.75409854199877
At time: 624.4409289360046 and batch: 500, loss is 3.940059947967529 and perplexity is 51.421683833562945
At time: 625.1596372127533 and batch: 550, loss is 3.923300976753235 and perplexity is 50.567090369922575
At time: 625.8769156932831 and batch: 600, loss is 3.8680014705657957 and perplexity is 47.84666749115946
At time: 626.5922245979309 and batch: 650, loss is 3.8968927526473998 and perplexity is 49.24918171949099
At time: 627.3082294464111 and batch: 700, loss is 3.9011954784393312 and perplexity is 49.46154398457483
At time: 628.0787110328674 and batch: 750, loss is 3.863458790779114 and perplexity is 47.62980833566263
At time: 628.7963156700134 and batch: 800, loss is 3.841970067024231 and perplexity is 46.6172230885892
At time: 629.511146068573 and batch: 850, loss is 3.812024178504944 and perplexity is 45.24192397401815
At time: 630.2266087532043 and batch: 900, loss is 3.8308728408813475 and perplexity is 46.102761052190225
At time: 630.9475979804993 and batch: 950, loss is 3.8139949321746824 and perplexity is 45.33117257633542
At time: 631.6613028049469 and batch: 1000, loss is 3.819910206794739 and perplexity is 45.60011355665396
At time: 632.3826706409454 and batch: 1050, loss is 3.7663935089111327 and perplexity is 43.22389679717677
At time: 633.1009986400604 and batch: 1100, loss is 3.732919206619263 and perplexity is 41.80095585523026
At time: 633.8171932697296 and batch: 1150, loss is 3.751601219177246 and perplexity is 42.58922211106606
At time: 634.5331797599792 and batch: 1200, loss is 3.732507939338684 and perplexity is 41.78376802442857
At time: 635.2477645874023 and batch: 1250, loss is 3.775797476768494 and perplexity is 43.63229018279163
At time: 635.9678640365601 and batch: 1300, loss is 3.776128888130188 and perplexity is 43.646752815903035
At time: 636.684309720993 and batch: 1350, loss is 3.7399159622192384 and perplexity is 42.094452491980086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3575211588541665 and perplexity of 78.0633876512651
Finished 30 epochs...
Completing Train Step...
At time: 639.1828455924988 and batch: 50, loss is 3.9536581373214723 and perplexity is 52.12570146140051
At time: 639.8955309391022 and batch: 100, loss is 3.9538695764541627 and perplexity is 52.136724039769526
At time: 640.606938123703 and batch: 150, loss is 3.931777949333191 and perplexity is 50.9975682050307
At time: 641.3167090415955 and batch: 200, loss is 3.9197585105896 and perplexity is 50.388275073858615
At time: 642.027645111084 and batch: 250, loss is 3.9085438442230225 and perplexity is 49.82634420316843
At time: 642.742146730423 and batch: 300, loss is 3.915319952964783 and perplexity is 50.16511942229938
At time: 643.4538159370422 and batch: 350, loss is 3.917818236351013 and perplexity is 50.290602787958
At time: 644.1693062782288 and batch: 400, loss is 3.9189908790588377 and perplexity is 50.349610287184525
At time: 644.88014793396 and batch: 450, loss is 3.864788522720337 and perplexity is 47.69318534103657
At time: 645.5916478633881 and batch: 500, loss is 3.9389953136444094 and perplexity is 51.36696767551921
At time: 646.3063585758209 and batch: 550, loss is 3.9224651193618776 and perplexity is 50.524841153297245
At time: 647.0772211551666 and batch: 600, loss is 3.8673719215393065 and perplexity is 47.81655514781241
At time: 647.789379119873 and batch: 650, loss is 3.896302828788757 and perplexity is 49.220137020098925
At time: 648.5046484470367 and batch: 700, loss is 3.900571036338806 and perplexity is 49.43066775536589
At time: 649.216582775116 and batch: 750, loss is 3.8631175184249877 and perplexity is 47.61355637217601
At time: 649.9283299446106 and batch: 800, loss is 3.8416433048248293 and perplexity is 46.60199283071457
At time: 650.6385979652405 and batch: 850, loss is 3.811961851119995 and perplexity is 45.23910425108069
At time: 651.351407289505 and batch: 900, loss is 3.8309840822219847 and perplexity is 46.10788987039977
At time: 652.0612316131592 and batch: 950, loss is 3.814076247215271 and perplexity is 45.33485883234539
At time: 652.7711510658264 and batch: 1000, loss is 3.820074257850647 and perplexity is 45.607594917078224
At time: 653.4717390537262 and batch: 1050, loss is 3.76672749042511 and perplexity is 43.23833519061312
At time: 654.1852986812592 and batch: 1100, loss is 3.733408818244934 and perplexity is 41.82142710025197
At time: 654.896487236023 and batch: 1150, loss is 3.752064995765686 and perplexity is 42.60897857613992
At time: 655.6075224876404 and batch: 1200, loss is 3.7330001258850096 and perplexity is 41.80433849474408
At time: 656.3175835609436 and batch: 1250, loss is 3.7763157176971434 and perplexity is 43.654908081629294
At time: 657.031613111496 and batch: 1300, loss is 3.7766584396362304 and perplexity is 43.66987214048686
At time: 657.7422361373901 and batch: 1350, loss is 3.7404421281814577 and perplexity is 42.11660698803937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357255859375 and perplexity of 78.04268022213579
Finished 31 epochs...
Completing Train Step...
At time: 660.2344717979431 and batch: 50, loss is 3.9532833528518676 and perplexity is 52.10616921844481
At time: 660.9803664684296 and batch: 100, loss is 3.9533333444595335 and perplexity is 52.10877415472529
At time: 661.6930351257324 and batch: 150, loss is 3.9310981941223146 and perplexity is 50.96291412178252
At time: 662.4051694869995 and batch: 200, loss is 3.9190329360961913 and perplexity is 50.35172788715479
At time: 663.1183938980103 and batch: 250, loss is 3.907782096862793 and perplexity is 49.7884035694252
At time: 663.837340593338 and batch: 300, loss is 3.9146236896514894 and perplexity is 50.130203446805595
At time: 664.5753636360168 and batch: 350, loss is 3.917112274169922 and perplexity is 50.25511205335763
At time: 665.3985924720764 and batch: 400, loss is 3.9183037519454955 and perplexity is 50.31502558821292
At time: 666.1429042816162 and batch: 450, loss is 3.8641288757324217 and perplexity is 47.66173504916784
At time: 666.8800399303436 and batch: 500, loss is 3.9384225749969484 and perplexity is 51.33755625126223
At time: 667.6082973480225 and batch: 550, loss is 3.922004108428955 and perplexity is 50.50155401736597
At time: 668.3247389793396 and batch: 600, loss is 3.867010498046875 and perplexity is 47.799276244143776
At time: 669.0643262863159 and batch: 650, loss is 3.8959812211990354 and perplexity is 49.20430999564813
At time: 669.7883450984955 and batch: 700, loss is 3.90024338722229 and perplexity is 49.41447449374584
At time: 670.5032162666321 and batch: 750, loss is 3.8629251670837403 and perplexity is 47.60439872151768
At time: 671.2174022197723 and batch: 800, loss is 3.8414595890045167 and perplexity is 46.59343209376896
At time: 671.932639837265 and batch: 850, loss is 3.811915650367737 and perplexity is 45.23701421871371
At time: 672.6479504108429 and batch: 900, loss is 3.831030044555664 and perplexity is 46.11000914532228
At time: 673.3633499145508 and batch: 950, loss is 3.814113459587097 and perplexity is 45.3365458813583
At time: 674.0778620243073 and batch: 1000, loss is 3.8201593208312987 and perplexity is 45.61147460004859
At time: 674.8000528812408 and batch: 1050, loss is 3.766890869140625 and perplexity is 43.2453999913809
At time: 675.5141515731812 and batch: 1100, loss is 3.73364239692688 and perplexity is 41.83119683502761
At time: 676.2348718643188 and batch: 1150, loss is 3.752305059432983 and perplexity is 42.61920867168468
At time: 676.9502518177032 and batch: 1200, loss is 3.7332546377182005 and perplexity is 41.81497954764921
At time: 677.6674907207489 and batch: 1250, loss is 3.776595587730408 and perplexity is 43.6671274920499
At time: 678.3837080001831 and batch: 1300, loss is 3.7769412279129027 and perplexity is 43.682223214659146
At time: 679.0981290340424 and batch: 1350, loss is 3.740700750350952 and perplexity is 42.127500684925394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.35715087890625 and perplexity of 78.03448769501874
Finished 32 epochs...
Completing Train Step...
At time: 681.5698580741882 and batch: 50, loss is 3.9529631328582764 and perplexity is 52.089486452491776
At time: 682.314649105072 and batch: 100, loss is 3.952930197715759 and perplexity is 52.08777090608285
At time: 683.0306797027588 and batch: 150, loss is 3.930623393058777 and perplexity is 50.93872261948649
At time: 683.7789454460144 and batch: 200, loss is 3.9185328769683836 and perplexity is 50.32655534042936
At time: 684.4962785243988 and batch: 250, loss is 3.907259464263916 and perplexity is 49.76238932521194
At time: 685.2128202915192 and batch: 300, loss is 3.9141397428512574 and perplexity is 50.105948964665416
At time: 685.9305510520935 and batch: 350, loss is 3.9166295099258424 and perplexity is 50.230856537495036
At time: 686.6467685699463 and batch: 400, loss is 3.9178366327285765 and perplexity is 50.29152796138468
At time: 687.3612313270569 and batch: 450, loss is 3.8636855363845823 and perplexity is 47.640609409893834
At time: 688.0810604095459 and batch: 500, loss is 3.938021669387817 and perplexity is 51.316978862072986
At time: 688.7974288463593 and batch: 550, loss is 3.921669297218323 and perplexity is 50.48464836118614
At time: 689.5138251781464 and batch: 600, loss is 3.8667356395721435 and perplexity is 47.786140013366925
At time: 690.2293808460236 and batch: 650, loss is 3.8957357883453367 and perplexity is 49.19223512327743
At time: 690.947793006897 and batch: 700, loss is 3.9000115394592285 and perplexity is 49.40301918636662
At time: 691.661107301712 and batch: 750, loss is 3.862768669128418 and perplexity is 47.596949313377195
At time: 692.3766524791718 and batch: 800, loss is 3.8413105154037477 and perplexity is 46.58648676077032
At time: 693.0950372219086 and batch: 850, loss is 3.8118479537963865 and perplexity is 45.233951931607315
At time: 693.8098735809326 and batch: 900, loss is 3.8310238409042356 and perplexity is 46.109723095785455
At time: 694.5258712768555 and batch: 950, loss is 3.814106845855713 and perplexity is 45.33624603861349
At time: 695.2449803352356 and batch: 1000, loss is 3.820187430381775 and perplexity is 45.6127567361162
At time: 695.96320104599 and batch: 1050, loss is 3.7669668292999265 and perplexity is 43.24868504361827
At time: 696.6787950992584 and batch: 1100, loss is 3.7337569522857668 and perplexity is 41.83598909727814
At time: 697.3995652198792 and batch: 1150, loss is 3.7524330949783327 and perplexity is 42.624665794654724
At time: 698.1207413673401 and batch: 1200, loss is 3.7333900117874146 and perplexity is 41.820640594755524
At time: 698.8358612060547 and batch: 1250, loss is 3.77675009727478 and perplexity is 43.6738750012866
At time: 699.5560786724091 and batch: 1300, loss is 3.7770999097824096 and perplexity is 43.68915534148995
At time: 700.2732615470886 and batch: 1350, loss is 3.7408354902267456 and perplexity is 42.13317732156125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.35710205078125 and perplexity of 78.03067751032209
Finished 33 epochs...
Completing Train Step...
At time: 702.8093864917755 and batch: 50, loss is 3.9526683473587036 and perplexity is 52.0741334902319
At time: 703.5307867527008 and batch: 100, loss is 3.952586431503296 and perplexity is 52.069867967751776
At time: 704.2542278766632 and batch: 150, loss is 3.9302418184280397 and perplexity is 50.919289403059715
At time: 704.9711968898773 and batch: 200, loss is 3.918135766983032 and perplexity is 50.30657413040692
At time: 705.6908462047577 and batch: 250, loss is 3.9068427753448485 and perplexity is 49.74165820850707
At time: 706.4131498336792 and batch: 300, loss is 3.913752722740173 and perplexity is 50.08656070679599
At time: 707.1316261291504 and batch: 350, loss is 3.9162502002716066 and perplexity is 50.21180710171568
At time: 707.8513798713684 and batch: 400, loss is 3.9174708127975464 and perplexity is 50.27313368279644
At time: 708.5684192180634 and batch: 450, loss is 3.8633394336700437 and perplexity is 47.6241237186906
At time: 709.2843499183655 and batch: 500, loss is 3.93769917011261 and perplexity is 51.30043184192857
At time: 710.0026116371155 and batch: 550, loss is 3.9213929080963137 and perplexity is 50.47069688165805
At time: 710.7185142040253 and batch: 600, loss is 3.866500291824341 and perplexity is 47.77489497623759
At time: 711.4377162456512 and batch: 650, loss is 3.8955202960968016 and perplexity is 49.181635720005886
At time: 712.1543016433716 and batch: 700, loss is 3.8998180055618286 and perplexity is 49.393458952664645
At time: 712.8704738616943 and batch: 750, loss is 3.8626237297058106 and perplexity is 47.59005113894662
At time: 713.5914335250854 and batch: 800, loss is 3.8411719608306885 and perplexity is 46.5800324371352
At time: 714.3089635372162 and batch: 850, loss is 3.811763505935669 and perplexity is 45.23013218242198
At time: 715.023158788681 and batch: 900, loss is 3.8309866189956665 and perplexity is 46.10800683582967
At time: 715.7411570549011 and batch: 950, loss is 3.814073429107666 and perplexity is 45.334731074014954
At time: 716.4570763111115 and batch: 1000, loss is 3.820180597305298 and perplexity is 45.612445061725936
At time: 717.1720106601715 and batch: 1050, loss is 3.7669943618774413 and perplexity is 43.24987580778397
At time: 717.8894085884094 and batch: 1100, loss is 3.733809313774109 and perplexity is 41.838179749285935
At time: 718.6099681854248 and batch: 1150, loss is 3.752499289512634 and perplexity is 42.62748740794341
At time: 719.3283212184906 and batch: 1200, loss is 3.7334610414505005 and perplexity is 41.82361120626653
At time: 720.0482456684113 and batch: 1250, loss is 3.776835584640503 and perplexity is 43.677608725402216
At time: 720.762752532959 and batch: 1300, loss is 3.77719021320343 and perplexity is 43.69310079982029
At time: 721.4811692237854 and batch: 1350, loss is 3.7409086656570434 and perplexity is 42.13626054774839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357078857421875 and perplexity of 78.0288677377638
Finished 34 epochs...
Completing Train Step...
At time: 724.0176177024841 and batch: 50, loss is 3.9523879718780517 and perplexity is 52.05953522661826
At time: 724.7337491512299 and batch: 100, loss is 3.9522728443145754 and perplexity is 52.05354208416647
At time: 725.4528198242188 and batch: 150, loss is 3.9299079084396364 and perplexity is 50.90228978205521
At time: 726.1730391979218 and batch: 200, loss is 3.9177920246124267 and perplexity is 50.289284601100434
At time: 726.892776966095 and batch: 250, loss is 3.9064808797836306 and perplexity is 49.72366018009352
At time: 727.6066102981567 and batch: 300, loss is 3.9134171867370604 and perplexity is 50.06975768157438
At time: 728.3208918571472 and batch: 350, loss is 3.9159265995025634 and perplexity is 50.19556115106534
At time: 729.0372803211212 and batch: 400, loss is 3.9171585273742675 and perplexity is 50.257436567082536
At time: 729.7572178840637 and batch: 450, loss is 3.8630436706542968 and perplexity is 47.610040347010155
At time: 730.4761037826538 and batch: 500, loss is 3.937418637275696 and perplexity is 51.28604240469822
At time: 731.1937682628632 and batch: 550, loss is 3.92114755153656 and perplexity is 50.45831508414253
At time: 731.9081175327301 and batch: 600, loss is 3.866286392211914 and perplexity is 47.76467703756396
At time: 732.6233735084534 and batch: 650, loss is 3.8953197717666628 and perplexity is 49.17177459417886
At time: 733.3450119495392 and batch: 700, loss is 3.8996420431137087 and perplexity is 49.38476832334085
At time: 734.0647847652435 and batch: 750, loss is 3.8624831342697146 and perplexity is 47.583360665288964
At time: 734.7794718742371 and batch: 800, loss is 3.841036906242371 and perplexity is 46.573742014815345
At time: 735.4995019435883 and batch: 850, loss is 3.8116683197021484 and perplexity is 45.22582710139337
At time: 736.2154591083527 and batch: 900, loss is 3.8309310817718507 and perplexity is 46.10544619624039
At time: 736.9345433712006 and batch: 950, loss is 3.8140232276916506 and perplexity is 45.332455263445304
At time: 737.6523215770721 and batch: 1000, loss is 3.8201522874832152 and perplexity is 45.611153799799276
At time: 738.3689496517181 and batch: 1050, loss is 3.7669932222366334 and perplexity is 43.24982651848865
At time: 739.1396489143372 and batch: 1100, loss is 3.7338261890411375 and perplexity is 41.83888578569845
At time: 739.8670601844788 and batch: 1150, loss is 3.752528977394104 and perplexity is 42.62875294652243
At time: 740.5822196006775 and batch: 1200, loss is 3.733494210243225 and perplexity is 41.824998467964384
At time: 741.2979869842529 and batch: 1250, loss is 3.776880717277527 and perplexity is 43.67958005554823
At time: 742.0140011310577 and batch: 1300, loss is 3.777239637374878 and perplexity is 43.69526034849182
At time: 742.7334144115448 and batch: 1350, loss is 3.7409465599060057 and perplexity is 42.1378572999496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357066243489584 and perplexity of 78.02788349311697
Finished 35 epochs...
Completing Train Step...
At time: 745.336008310318 and batch: 50, loss is 3.9521166276931763 and perplexity is 52.04541109080492
At time: 746.1080257892609 and batch: 100, loss is 3.951976470947266 and perplexity is 52.03811708651075
At time: 746.8419666290283 and batch: 150, loss is 3.929601774215698 and perplexity is 50.88670923406727
At time: 747.555766582489 and batch: 200, loss is 3.9174793672561647 and perplexity is 50.27356374407761
At time: 748.2729685306549 and batch: 250, loss is 3.9061513137817383 and perplexity is 49.707275652248484
At time: 748.9952471256256 and batch: 300, loss is 3.9131117916107176 and perplexity is 50.05446895627119
At time: 749.7119228839874 and batch: 350, loss is 3.915635895729065 and perplexity is 50.18097123280076
At time: 750.4285323619843 and batch: 400, loss is 3.916877489089966 and perplexity is 50.243314287879834
At time: 751.1509368419647 and batch: 450, loss is 3.862776966094971 and perplexity is 47.59734422531195
At time: 751.8790786266327 and batch: 500, loss is 3.937163062095642 and perplexity is 51.27293664000181
At time: 752.6090312004089 and batch: 550, loss is 3.920920557975769 and perplexity is 50.446862671391216
At time: 753.3284966945648 and batch: 600, loss is 3.8660854482650757 and perplexity is 47.75507997910827
At time: 754.0473594665527 and batch: 650, loss is 3.8951279306411744 and perplexity is 49.16234233037544
At time: 754.7652518749237 and batch: 700, loss is 3.8994750022888183 and perplexity is 49.376519739847375
At time: 755.486884355545 and batch: 750, loss is 3.862344560623169 and perplexity is 47.57676732232902
At time: 756.2037444114685 and batch: 800, loss is 3.840902910232544 and perplexity is 46.567501737318125
At time: 756.9288883209229 and batch: 850, loss is 3.8115663862228395 and perplexity is 45.221217310432316
At time: 757.7047708034515 and batch: 900, loss is 3.830863800048828 and perplexity is 46.102344246733004
At time: 758.417594909668 and batch: 950, loss is 3.8139614486694335 and perplexity is 45.32965475519165
At time: 759.1593208312988 and batch: 1000, loss is 3.820110015869141 and perplexity is 45.609225783458825
At time: 759.8759989738464 and batch: 1050, loss is 3.7669737339019775 and perplexity is 43.24898365960864
At time: 760.5919342041016 and batch: 1100, loss is 3.733821220397949 and perplexity is 41.83867790372003
At time: 761.3102695941925 and batch: 1150, loss is 3.7525353813171387 and perplexity is 42.62902593864947
At time: 762.0290668010712 and batch: 1200, loss is 3.733503565788269 and perplexity is 41.825389765451916
At time: 762.7458119392395 and batch: 1250, loss is 3.776900749206543 and perplexity is 43.680455050559246
At time: 763.4674446582794 and batch: 1300, loss is 3.7772632360458376 and perplexity is 43.69629151073026
At time: 764.1890890598297 and batch: 1350, loss is 3.7409623050689698 and perplexity is 42.13852077260297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357059326171875 and perplexity of 78.0273437513235
Finished 36 epochs...
Completing Train Step...
At time: 766.7419326305389 and batch: 50, loss is 3.9518515825271607 and perplexity is 52.031618534088
At time: 767.5298342704773 and batch: 100, loss is 3.951691064834595 and perplexity is 52.02326720902608
At time: 768.249014377594 and batch: 150, loss is 3.929312815666199 and perplexity is 50.87200720861848
At time: 768.9644639492035 and batch: 200, loss is 3.9171862602233887 and perplexity is 50.25883036831502
At time: 769.6813011169434 and batch: 250, loss is 3.9058426570892335 and perplexity is 49.69193553648872
At time: 770.3965740203857 and batch: 300, loss is 3.912825846672058 and perplexity is 50.04015818036032
At time: 771.1147770881653 and batch: 350, loss is 3.915365982055664 and perplexity is 50.16742853028298
At time: 771.8361051082611 and batch: 400, loss is 3.9166160202026368 and perplexity is 50.230178941714264
At time: 772.5512270927429 and batch: 450, loss is 3.8625281286239623 and perplexity is 47.585501696041824
At time: 773.270210981369 and batch: 500, loss is 3.936923441886902 and perplexity is 51.2606520800946
At time: 773.9870252609253 and batch: 550, loss is 3.9207051467895506 and perplexity is 50.43599702319528
At time: 774.703409910202 and batch: 600, loss is 3.865892810821533 and perplexity is 47.74588144860406
At time: 775.4184701442719 and batch: 650, loss is 3.894942054748535 and perplexity is 49.15320508533363
At time: 776.1341338157654 and batch: 700, loss is 3.899312877655029 and perplexity is 49.36851523854773
At time: 776.8853416442871 and batch: 750, loss is 3.8622069263458254 and perplexity is 47.570219578947565
At time: 777.6031515598297 and batch: 800, loss is 3.8407691431045534 and perplexity is 46.56127295296562
At time: 778.3187177181244 and batch: 850, loss is 3.811459808349609 and perplexity is 45.21639798608771
At time: 779.034380197525 and batch: 900, loss is 3.830788688659668 and perplexity is 46.09888156565812
At time: 779.7504332065582 and batch: 950, loss is 3.813891682624817 and perplexity is 45.32649239478952
At time: 780.4658150672913 and batch: 1000, loss is 3.82005795955658 and perplexity is 45.606851597142004
At time: 781.1833152770996 and batch: 1050, loss is 3.7669419288635253 and perplexity is 43.247608145894574
At time: 781.9013683795929 and batch: 1100, loss is 3.733801870346069 and perplexity is 41.83786833096467
At time: 782.6174173355103 and batch: 1150, loss is 3.752526364326477 and perplexity is 42.62864155485366
At time: 783.3365862369537 and batch: 1200, loss is 3.733497071266174 and perplexity is 41.82511813041603
At time: 784.0561094284058 and batch: 1250, loss is 3.7769039249420167 and perplexity is 43.68059376835012
At time: 784.7743518352509 and batch: 1300, loss is 3.777269473075867 and perplexity is 43.696564046662495
At time: 785.4917297363281 and batch: 1350, loss is 3.740963144302368 and perplexity is 42.1385561366718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357056070963542 and perplexity of 78.02708975647731
Finished 37 epochs...
Completing Train Step...
At time: 788.0321829319 and batch: 50, loss is 3.9515915012359617 and perplexity is 52.01808784317253
At time: 788.7423014640808 and batch: 100, loss is 3.951413435935974 and perplexity is 52.008826051382286
At time: 789.4521472454071 and batch: 150, loss is 3.929035687446594 and perplexity is 50.85791109313894
At time: 790.1630339622498 and batch: 200, loss is 3.9169068002700804 and perplexity is 50.24478700029784
At time: 790.8756399154663 and batch: 250, loss is 3.9055487298965454 and perplexity is 49.67733187168948
At time: 791.5859823226929 and batch: 300, loss is 3.9125531578063963 and perplexity is 50.026514646692995
At time: 792.2985534667969 and batch: 350, loss is 3.915110397338867 and perplexity is 50.15460814068726
At time: 793.0088040828705 and batch: 400, loss is 3.916367974281311 and perplexity is 50.21772109582311
At time: 793.720116853714 and batch: 450, loss is 3.862291612625122 and perplexity is 47.574248294435094
At time: 794.4319574832916 and batch: 500, loss is 3.936694526672363 and perplexity is 51.24891907990866
At time: 795.1777009963989 and batch: 550, loss is 3.9204975986480712 and perplexity is 50.42553021197064
At time: 795.8962359428406 and batch: 600, loss is 3.86570631980896 and perplexity is 47.73697810104947
At time: 796.6072361469269 and batch: 650, loss is 3.894760217666626 and perplexity is 49.14426802252377
At time: 797.3184323310852 and batch: 700, loss is 3.899153847694397 and perplexity is 49.3606647897577
At time: 798.0293369293213 and batch: 750, loss is 3.8620699644088745 and perplexity is 47.56370471568719
At time: 798.7404634952545 and batch: 800, loss is 3.840635380744934 and perplexity is 46.55504522375571
At time: 799.4515700340271 and batch: 850, loss is 3.8113504219055176 and perplexity is 45.211452195603535
At time: 800.1629087924957 and batch: 900, loss is 3.8307079792022707 and perplexity is 46.09516110008073
At time: 800.8774547576904 and batch: 950, loss is 3.813815689086914 and perplexity is 45.32304800514905
At time: 801.591637134552 and batch: 1000, loss is 3.8199989128112795 and perplexity is 45.604158740494796
At time: 802.3049683570862 and batch: 1050, loss is 3.7669014310836793 and perplexity is 43.24585674924509
At time: 803.0189712047577 and batch: 1100, loss is 3.733772625923157 and perplexity is 41.83664482453991
At time: 803.7359421253204 and batch: 1150, loss is 3.7525063133239747 and perplexity is 42.62778681642438
At time: 804.4500660896301 and batch: 1200, loss is 3.733479380607605 and perplexity is 41.824378223076316
At time: 805.1605813503265 and batch: 1250, loss is 3.7768954133987425 and perplexity is 43.680221980668264
At time: 805.8736519813538 and batch: 1300, loss is 3.777263684272766 and perplexity is 43.696311096589184
At time: 806.5857515335083 and batch: 1350, loss is 3.740953412055969 and perplexity is 42.13814603585619
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3570556640625 and perplexity of 78.02705800717966
Finished 38 epochs...
Completing Train Step...
At time: 809.1297857761383 and batch: 50, loss is 3.9513354682922364 and perplexity is 52.00477120383701
At time: 809.8488593101501 and batch: 100, loss is 3.951141843795776 and perplexity is 51.99470278097724
At time: 810.5638651847839 and batch: 150, loss is 3.928767409324646 and perplexity is 50.84426885830305
At time: 811.2816951274872 and batch: 200, loss is 3.9166373109817503 and perplexity is 50.23124839274362
At time: 812.0003497600555 and batch: 250, loss is 3.9052655267715455 and perplexity is 49.66326508803432
At time: 812.7190532684326 and batch: 300, loss is 3.912290081977844 and perplexity is 50.013355610890756
At time: 813.4704718589783 and batch: 350, loss is 3.9148649644851683 and perplexity is 50.1423000625503
At time: 814.1852531433105 and batch: 400, loss is 3.9161296606063845 and perplexity is 50.20575495206681
At time: 814.9055359363556 and batch: 450, loss is 3.8620633840560914 and perplexity is 47.56339173076026
At time: 815.6205384731293 and batch: 500, loss is 3.9364736080169678 and perplexity is 51.237598488126054
At time: 816.337408542633 and batch: 550, loss is 3.92029589176178 and perplexity is 50.41536006101121
At time: 817.0521998405457 and batch: 600, loss is 3.865524320602417 and perplexity is 47.728290799477286
At time: 817.7673408985138 and batch: 650, loss is 3.894581923484802 and perplexity is 49.13550666653797
At time: 818.485387802124 and batch: 700, loss is 3.8989969730377196 and perplexity is 49.35292195975822
At time: 819.2023916244507 and batch: 750, loss is 3.861933560371399 and perplexity is 47.55721727679317
At time: 819.9170591831207 and batch: 800, loss is 3.8405012702941894 and perplexity is 46.54880212429811
At time: 820.6341557502747 and batch: 850, loss is 3.8112387800216676 and perplexity is 45.20640498565408
At time: 821.3578886985779 and batch: 900, loss is 3.8306229496002198 and perplexity is 46.09124181350597
At time: 822.0752420425415 and batch: 950, loss is 3.8137352752685545 and perplexity is 45.3194035523334
At time: 822.7961719036102 and batch: 1000, loss is 3.8199346923828124 and perplexity is 45.601230115920366
At time: 823.5195009708405 and batch: 1050, loss is 3.7668544244766236 and perplexity is 43.243823956027825
At time: 824.2338833808899 and batch: 1100, loss is 3.733736124038696 and perplexity is 41.83511773603527
At time: 824.9502010345459 and batch: 1150, loss is 3.7524783277511595 and perplexity is 42.6265938700852
At time: 825.6676893234253 and batch: 1200, loss is 3.7334536790847777 and perplexity is 41.82330328667848
At time: 826.3891959190369 and batch: 1250, loss is 3.776878333091736 and perplexity is 43.67947591543825
At time: 827.1050872802734 and batch: 1300, loss is 3.777249283790588 and perplexity is 43.69568185317071
At time: 827.8201787471771 and batch: 1350, loss is 3.7409359407424927 and perplexity is 42.13740983352871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357057291666667 and perplexity of 78.02718500444776
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 830.3385772705078 and batch: 50, loss is 3.951308698654175 and perplexity is 52.00337907356791
At time: 831.1089413166046 and batch: 100, loss is 3.9514721870422362 and perplexity is 52.0118817172092
At time: 831.8290221691132 and batch: 150, loss is 3.9293360185623167 and perplexity is 50.87318760021124
At time: 832.6157665252686 and batch: 200, loss is 3.9174446630477906 and perplexity is 50.27181907011967
At time: 833.3456916809082 and batch: 250, loss is 3.9059595918655394 and perplexity is 49.69774659160548
At time: 834.0639266967773 and batch: 300, loss is 3.913083386421204 and perplexity is 50.05304716978762
At time: 834.7805852890015 and batch: 350, loss is 3.915653119087219 and perplexity is 50.1818355250838
At time: 835.4960515499115 and batch: 400, loss is 3.9169642972946166 and perplexity is 50.24767600910272
At time: 836.2113335132599 and batch: 450, loss is 3.862870006561279 and perplexity is 47.60177291043397
At time: 836.9342119693756 and batch: 500, loss is 3.936907377243042 and perplexity is 51.25982860258936
At time: 837.6534953117371 and batch: 550, loss is 3.9205649662017823 and perplexity is 50.42892737101346
At time: 838.3695769309998 and batch: 600, loss is 3.8656013441085815 and perplexity is 47.73196714135846
At time: 839.0845775604248 and batch: 650, loss is 3.8943802881240845 and perplexity is 49.12560020970679
At time: 839.805965423584 and batch: 700, loss is 3.8989248991012575 and perplexity is 49.34936502857922
At time: 840.5235481262207 and batch: 750, loss is 3.8614381170272827 and perplexity is 47.53366120585981
At time: 841.2456016540527 and batch: 800, loss is 3.840048007965088 and perplexity is 46.52770808675781
At time: 841.9613816738129 and batch: 850, loss is 3.810380048751831 and perplexity is 45.16760149536578
At time: 842.6832382678986 and batch: 900, loss is 3.8293668270111083 and perplexity is 46.03338191067712
At time: 843.4017486572266 and batch: 950, loss is 3.8127095079421998 and perplexity is 45.272940223275995
At time: 844.1210284233093 and batch: 1000, loss is 3.8189108037948607 and perplexity is 45.55456343162777
At time: 844.8398914337158 and batch: 1050, loss is 3.7656465339660645 and perplexity is 43.19162168509182
At time: 845.5573291778564 and batch: 1100, loss is 3.7323269271850585 and perplexity is 41.7762053390815
At time: 846.2794563770294 and batch: 1150, loss is 3.751139454841614 and perplexity is 42.5695604670852
At time: 847.0038752555847 and batch: 1200, loss is 3.7319632816314696 and perplexity is 41.76101636963211
At time: 847.7205233573914 and batch: 1250, loss is 3.7754065990448 and perplexity is 43.61523862527952
At time: 848.439567565918 and batch: 1300, loss is 3.775715913772583 and perplexity is 43.62873154761431
At time: 849.1549642086029 and batch: 1350, loss is 3.739642996788025 and perplexity is 42.08296372969273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357164713541667 and perplexity of 78.03556728117377
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 851.6821439266205 and batch: 50, loss is 3.951204285621643 and perplexity is 51.9979495265196
At time: 852.4273664951324 and batch: 100, loss is 3.951354594230652 and perplexity is 52.00576585340015
At time: 853.1485774517059 and batch: 150, loss is 3.9293131256103515 and perplexity is 50.87202297610209
At time: 853.8705561161041 and batch: 200, loss is 3.917503614425659 and perplexity is 50.27478275047748
At time: 854.5889294147491 and batch: 250, loss is 3.9059474086761474 and perplexity is 49.697141118234704
At time: 855.3052694797516 and batch: 300, loss is 3.9130974054336547 and perplexity is 50.05374886899765
At time: 856.0209808349609 and batch: 350, loss is 3.9156864643096925 and perplexity is 50.18350887745251
At time: 856.7363793849945 and batch: 400, loss is 3.9170335292816163 and perplexity is 50.251154875977996
At time: 857.4518756866455 and batch: 450, loss is 3.8629445028305054 and perplexity is 47.60531919701528
At time: 858.1680028438568 and batch: 500, loss is 3.9369199800491335 and perplexity is 51.26047462434036
At time: 858.884604215622 and batch: 550, loss is 3.9205394887924196 and perplexity is 50.42764258895364
At time: 859.6002926826477 and batch: 600, loss is 3.8655491065979004 and perplexity is 47.72947380733844
At time: 860.3172922134399 and batch: 650, loss is 3.894265122413635 and perplexity is 49.11994295082476
At time: 861.0330266952515 and batch: 700, loss is 3.8988698148727416 and perplexity is 49.34664673174721
At time: 861.7473063468933 and batch: 750, loss is 3.8612813329696656 and perplexity is 47.526209269770206
At time: 862.4643864631653 and batch: 800, loss is 3.8398699617385863 and perplexity is 46.51942474133627
At time: 863.1802093982697 and batch: 850, loss is 3.810092978477478 and perplexity is 45.15463708055074
At time: 863.8979346752167 and batch: 900, loss is 3.828929719924927 and perplexity is 46.01326479023132
At time: 864.6175787448883 and batch: 950, loss is 3.8123562431335447 and perplexity is 45.25694971131964
At time: 865.3373765945435 and batch: 1000, loss is 3.8185684728622435 and perplexity is 45.53897136441898
At time: 866.0525858402252 and batch: 1050, loss is 3.765255403518677 and perplexity is 43.17473143014055
At time: 866.7694971561432 and batch: 1100, loss is 3.731877398490906 and perplexity is 41.75742995640157
At time: 867.4843220710754 and batch: 1150, loss is 3.750697569847107 and perplexity is 42.55075377259611
At time: 868.2004823684692 and batch: 1200, loss is 3.7314767694473265 and perplexity is 41.74070406783789
At time: 868.973052740097 and batch: 1250, loss is 3.774917221069336 and perplexity is 43.59389950997411
At time: 869.689691066742 and batch: 1300, loss is 3.7752069854736328 and perplexity is 43.606533300619596
At time: 870.4103991985321 and batch: 1350, loss is 3.7392016792297365 and perplexity is 42.064395876355206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357183837890625 and perplexity of 78.03705967486412
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 872.9187757968903 and batch: 50, loss is 3.9511653757095337 and perplexity is 51.99592633023511
At time: 873.6370573043823 and batch: 100, loss is 3.951298475265503 and perplexity is 52.002847425529005
At time: 874.3538107872009 and batch: 150, loss is 3.929286561012268 and perplexity is 50.870671599207505
At time: 875.0747921466827 and batch: 200, loss is 3.9175005197525024 and perplexity is 50.27462716669757
At time: 875.7925910949707 and batch: 250, loss is 3.9059253978729247 and perplexity is 49.69604725627925
At time: 876.5145485401154 and batch: 300, loss is 3.91308310508728 and perplexity is 50.05303308816945
At time: 877.2414772510529 and batch: 350, loss is 3.9156780147552492 and perplexity is 50.18308485095351
At time: 877.9589593410492 and batch: 400, loss is 3.917030720710754 and perplexity is 50.25101374224681
At time: 878.6780083179474 and batch: 450, loss is 3.8629532623291016 and perplexity is 47.60573619756831
At time: 879.3980050086975 and batch: 500, loss is 3.93691442489624 and perplexity is 51.26018986535738
At time: 880.1134004592896 and batch: 550, loss is 3.920517826080322 and perplexity is 50.426550201282566
At time: 880.8357918262482 and batch: 600, loss is 3.8655061292648316 and perplexity is 47.72742256592418
At time: 881.5575652122498 and batch: 650, loss is 3.894215269088745 and perplexity is 49.11749421938946
At time: 882.275557756424 and batch: 700, loss is 3.898838768005371 and perplexity is 49.3451146967335
At time: 882.9925293922424 and batch: 750, loss is 3.8612237691879274 and perplexity is 47.52347356017261
At time: 883.7126088142395 and batch: 800, loss is 3.8398071002960203 and perplexity is 46.51650055509994
At time: 884.428781747818 and batch: 850, loss is 3.8099954414367674 and perplexity is 45.15023304565729
At time: 885.1477460861206 and batch: 900, loss is 3.828785066604614 and perplexity is 46.00660930008196
At time: 885.8646228313446 and batch: 950, loss is 3.8122354412078856 and perplexity is 45.25148291485147
At time: 886.5831069946289 and batch: 1000, loss is 3.818451361656189 and perplexity is 45.5336385528321
At time: 887.3053212165833 and batch: 1050, loss is 3.76512423992157 and perplexity is 43.16906884843245
At time: 888.0779044628143 and batch: 1100, loss is 3.7317268514633177 and perplexity is 41.75114397262187
At time: 888.7960779666901 and batch: 1150, loss is 3.7505492115020753 and perplexity is 42.54444148143867
At time: 889.5151500701904 and batch: 1200, loss is 3.7313127613067625 and perplexity is 41.73385881393191
At time: 890.2311432361603 and batch: 1250, loss is 3.774749493598938 and perplexity is 43.586588228653014
At time: 890.9547595977783 and batch: 1300, loss is 3.7750348043441773 and perplexity is 43.59902572481425
At time: 891.6741135120392 and batch: 1350, loss is 3.739052109718323 and perplexity is 42.058104795704736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.357186279296875 and perplexity of 78.03725019526189
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 894.1959822177887 and batch: 50, loss is 3.9511506175994873 and perplexity is 51.99515897429474
At time: 894.9143967628479 and batch: 100, loss is 3.951278190612793 and perplexity is 52.00179257652771
At time: 895.634711265564 and batch: 150, loss is 3.9292753553390503 and perplexity is 50.870101562279025
At time: 896.3609354496002 and batch: 200, loss is 3.917496166229248 and perplexity is 50.274408295415526
At time: 897.0798783302307 and batch: 250, loss is 3.9059160137176514 and perplexity is 49.6955809030435
At time: 897.7945830821991 and batch: 300, loss is 3.9130767488479616 and perplexity is 50.05271494012364
At time: 898.5115783214569 and batch: 350, loss is 3.915672249794006 and perplexity is 50.1827955482482
At time: 899.2258424758911 and batch: 400, loss is 3.917026891708374 and perplexity is 50.250821331363966
At time: 899.9439594745636 and batch: 450, loss is 3.8629547786712646 and perplexity is 47.60580838420804
At time: 900.6602854728699 and batch: 500, loss is 3.93691123008728 and perplexity is 51.260026099105104
At time: 901.3782639503479 and batch: 550, loss is 3.920509247779846 and perplexity is 50.42611762903833
At time: 902.0951352119446 and batch: 600, loss is 3.865489559173584 and perplexity is 47.72663172472942
At time: 902.8115875720978 and batch: 650, loss is 3.8941974020004273 and perplexity is 49.116616640622205
At time: 903.5254898071289 and batch: 700, loss is 3.898826789855957 and perplexity is 49.34452363711674
At time: 904.239399433136 and batch: 750, loss is 3.8612030601501464 and perplexity is 47.52248940495365
At time: 904.9670789241791 and batch: 800, loss is 3.839784851074219 and perplexity is 46.51546561067506
At time: 905.6856927871704 and batch: 850, loss is 3.8099619483947755 and perplexity is 45.148720852330065
At time: 906.4598453044891 and batch: 900, loss is 3.828736433982849 and perplexity is 46.00437193245814
At time: 907.1909694671631 and batch: 950, loss is 3.812194390296936 and perplexity is 45.24962533838386
At time: 907.9335887432098 and batch: 1000, loss is 3.818411693572998 and perplexity is 45.531832356494405
At time: 908.6703042984009 and batch: 1050, loss is 3.7650794076919554 and perplexity is 43.167133526208325
At time: 909.4123046398163 and batch: 1100, loss is 3.731675705909729 and perplexity is 41.74900864185723
At time: 910.169867515564 and batch: 1150, loss is 3.7504991245269776 and perplexity is 42.542310612422476
At time: 910.9092137813568 and batch: 1200, loss is 3.7312571668624877 and perplexity is 41.73153870773681
At time: 911.6294643878937 and batch: 1250, loss is 3.7746923637390135 and perplexity is 43.584098204100975
At time: 912.3693261146545 and batch: 1300, loss is 3.7749764347076415 and perplexity is 43.596480939799186
At time: 913.1064281463623 and batch: 1350, loss is 3.7390012168884277 and perplexity is 42.05596439419766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.35718994140625 and perplexity of 78.0375359767307
Annealing...
Model not improving. Stopping early with 78.02705800717966loss at 42 epochs.
Finished Training.
Improved accuracyfrom -161.06340312676477 to -78.02705800717966
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc67f09898>
SETTINGS FOR THIS RUN
{'seq_len': 20, 'lr': 0.7211867437783614, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.9818069729096831, 'anneal': 6.186620571858408, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1375 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.312026023864746 and batch: 50, loss is 10.68269100189209 and perplexity is 43594.70608481626
At time: 2.0245399475097656 and batch: 100, loss is 10.456541976928712 and perplexity is 34771.10403202248
At time: 2.738701343536377 and batch: 150, loss is 10.258269691467286 and perplexity is 28517.399577660835
At time: 3.4525094032287598 and batch: 200, loss is 10.06679178237915 and perplexity is 23547.896677042223
At time: 4.168069839477539 and batch: 250, loss is 9.858981437683106 and perplexity is 19129.395028075774
At time: 4.884820222854614 and batch: 300, loss is 9.667204551696777 and perplexity is 15791.143939255064
At time: 5.599916219711304 and batch: 350, loss is 9.47327564239502 and perplexity is 13007.425197081615
At time: 6.314453601837158 and batch: 400, loss is 9.29539436340332 and perplexity is 10887.758496734523
At time: 7.032743453979492 and batch: 450, loss is 9.108606300354005 and perplexity is 9032.697247856857
At time: 7.783656358718872 and batch: 500, loss is 8.941548252105713 and perplexity is 7643.021223592071
At time: 8.49705719947815 and batch: 550, loss is 8.786642723083496 and perplexity is 6546.217776096904
At time: 9.210943222045898 and batch: 600, loss is 8.623592624664306 and perplexity is 5561.330319178131
At time: 9.927300930023193 and batch: 650, loss is 8.551233406066894 and perplexity is 5173.1310651161875
At time: 10.643065452575684 and batch: 700, loss is 8.446486625671387 and perplexity is 4658.676275295857
At time: 11.361031770706177 and batch: 750, loss is 8.362372913360595 and perplexity is 4282.84553958206
At time: 12.076550960540771 and batch: 800, loss is 8.268300704956054 and perplexity is 3898.318966465081
At time: 12.794883251190186 and batch: 850, loss is 8.18521203994751 and perplexity is 3587.504232820683
At time: 13.5082368850708 and batch: 900, loss is 8.154103574752808 and perplexity is 3477.6205007844646
At time: 14.222329139709473 and batch: 950, loss is 8.084702367782592 and perplexity is 3244.454037834538
At time: 14.939664602279663 and batch: 1000, loss is 8.053071546554566 and perplexity is 3143.4353663167917
At time: 15.660683393478394 and batch: 1050, loss is 8.002161664962768 and perplexity is 2987.4087892023676
At time: 16.373435020446777 and batch: 1100, loss is 7.962765493392944 and perplexity is 2872.0044895852197
At time: 17.090521097183228 and batch: 1150, loss is 7.913318147659302 and perplexity is 2733.4454107029264
At time: 17.804925441741943 and batch: 1200, loss is 7.831495294570923 and perplexity is 2518.6927466319667
At time: 18.523149967193604 and batch: 1250, loss is 7.8193257045745845 and perplexity is 2488.227042138921
At time: 19.245160818099976 and batch: 1300, loss is 7.766261081695557 and perplexity is 2359.6323021225367
At time: 19.963772535324097 and batch: 1350, loss is 7.732140588760376 and perplexity is 2280.47854654487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 7.010672200520833 and perplexity of 1108.3993211406878
Finished 1 epochs...
Completing Train Step...
At time: 22.506620407104492 and batch: 50, loss is 6.991585168838501 and perplexity is 1087.4438928371949
At time: 23.242146015167236 and batch: 100, loss is 6.618807821273804 and perplexity is 749.051561254342
At time: 23.948991775512695 and batch: 150, loss is 6.393125438690186 and perplexity is 597.7218062031824
At time: 24.657156229019165 and batch: 200, loss is 6.295538396835327 and perplexity is 542.147658414282
At time: 25.365717887878418 and batch: 250, loss is 6.250632953643799 and perplexity is 518.3408065609675
At time: 26.07453989982605 and batch: 300, loss is 6.2078453731536865 and perplexity is 496.6300452484002
At time: 26.786751985549927 and batch: 350, loss is 6.180217847824097 and perplexity is 483.0971865612032
At time: 27.500911235809326 and batch: 400, loss is 6.194280166625976 and perplexity is 489.93864384716517
At time: 28.206358194351196 and batch: 450, loss is 6.132176170349121 and perplexity is 460.4370606995942
At time: 28.915310621261597 and batch: 500, loss is 6.1017780780792235 and perplexity is 446.65124523128463
At time: 29.62289261817932 and batch: 550, loss is 6.05080620765686 and perplexity is 424.45509108454775
At time: 30.33153486251831 and batch: 600, loss is 5.978935613632202 and perplexity is 395.0196907395517
At time: 31.03836417198181 and batch: 650, loss is 5.986552963256836 and perplexity is 398.0401833013926
At time: 31.747507333755493 and batch: 700, loss is 5.985400943756104 and perplexity is 397.581897276058
At time: 32.50617074966431 and batch: 750, loss is 5.945254278182984 and perplexity is 381.9364670620647
At time: 33.21621298789978 and batch: 800, loss is 5.89631365776062 and perplexity is 363.69429205249514
At time: 33.923272132873535 and batch: 850, loss is 5.878717203140258 and perplexity is 357.3505394110377
At time: 34.63036227226257 and batch: 900, loss is 5.909485549926758 and perplexity is 368.51652328301464
At time: 35.343366861343384 and batch: 950, loss is 5.86593786239624 and perplexity is 352.8128909349043
At time: 36.051899671554565 and batch: 1000, loss is 5.870987453460693 and perplexity is 354.59895741391244
At time: 36.76205539703369 and batch: 1050, loss is 5.844859199523926 and perplexity is 345.4538982833074
At time: 37.47228670120239 and batch: 1100, loss is 5.828220911026001 and perplexity is 339.75368902374333
At time: 38.18042993545532 and batch: 1150, loss is 5.814323253631592 and perplexity is 335.06456802681964
At time: 38.88870048522949 and batch: 1200, loss is 5.790038518905639 and perplexity is 327.0256208023985
At time: 39.60203313827515 and batch: 1250, loss is 5.79689995765686 and perplexity is 329.2772027819611
At time: 40.311503648757935 and batch: 1300, loss is 5.745966100692749 and perplexity is 312.92579967418214
At time: 41.01947331428528 and batch: 1350, loss is 5.72457706451416 and perplexity is 306.30369114412474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.39426025390625 and perplexity of 220.13923970621738
Finished 2 epochs...
Completing Train Step...
At time: 43.55916118621826 and batch: 50, loss is 5.7657785987854 and perplexity is 319.187466286632
At time: 44.308064222335815 and batch: 100, loss is 5.77341926574707 and perplexity is 321.6356122386379
At time: 45.0537850856781 and batch: 150, loss is 5.717621402740479 and perplexity is 304.1805387933723
At time: 45.793275356292725 and batch: 200, loss is 5.70153133392334 and perplexity is 299.32541735525444
At time: 46.50913763046265 and batch: 250, loss is 5.718905181884765 and perplexity is 304.5712901907543
At time: 47.2291305065155 and batch: 300, loss is 5.717934350967408 and perplexity is 304.27574645041074
At time: 47.94920611381531 and batch: 350, loss is 5.717805633544922 and perplexity is 304.2365833811414
At time: 48.664557218551636 and batch: 400, loss is 5.760586338043213 and perplexity is 317.5344568776645
At time: 49.38062930107117 and batch: 450, loss is 5.719549999237061 and perplexity is 304.76774637605195
At time: 50.09905171394348 and batch: 500, loss is 5.723772077560425 and perplexity is 306.05721988525625
At time: 50.87037801742554 and batch: 550, loss is 5.684448862075806 and perplexity is 294.2556249177599
At time: 51.5810182094574 and batch: 600, loss is 5.632113466262817 and perplexity is 279.2516834116209
At time: 52.29585123062134 and batch: 650, loss is 5.654060888290405 and perplexity is 285.4482889836131
At time: 53.00754427909851 and batch: 700, loss is 5.658164548873901 and perplexity is 286.62207864570576
At time: 53.72630190849304 and batch: 750, loss is 5.62661298751831 and perplexity is 277.7198821448878
At time: 54.43924808502197 and batch: 800, loss is 5.590443143844604 and perplexity is 267.8542913978758
At time: 55.15088367462158 and batch: 850, loss is 5.571098871231079 and perplexity is 262.7226390269889
At time: 55.86485171318054 and batch: 900, loss is 5.611936016082764 and perplexity is 273.6735618668707
At time: 56.576921224594116 and batch: 950, loss is 5.57665545463562 and perplexity is 264.1865426677659
At time: 57.29346537590027 and batch: 1000, loss is 5.585234355926514 and perplexity is 266.4627225408332
At time: 58.00911068916321 and batch: 1050, loss is 5.559212923049927 and perplexity is 259.61841621510894
At time: 58.72398042678833 and batch: 1100, loss is 5.543057308197022 and perplexity is 255.4578200624388
At time: 59.437270641326904 and batch: 1150, loss is 5.538895559310913 and perplexity is 254.3968779829113
At time: 60.156938314437866 and batch: 1200, loss is 5.518546056747437 and perplexity is 249.27234574205266
At time: 60.866257429122925 and batch: 1250, loss is 5.538369159698487 and perplexity is 254.26299880500372
At time: 61.579554319381714 and batch: 1300, loss is 5.502258777618408 and perplexity is 245.2452616129374
At time: 62.29313111305237 and batch: 1350, loss is 5.481181755065918 and perplexity is 240.13031498009502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.191494954427084 and perplexity of 179.73705088565725
Finished 3 epochs...
Completing Train Step...
At time: 64.81263947486877 and batch: 50, loss is 5.538759355545044 and perplexity is 254.36223052971496
At time: 65.52709126472473 and batch: 100, loss is 5.546394853591919 and perplexity is 256.3118465166436
At time: 66.24583745002747 and batch: 150, loss is 5.495908031463623 and perplexity is 243.69270637101772
At time: 66.96175742149353 and batch: 200, loss is 5.483578367233276 and perplexity is 240.70650439021284
At time: 67.67688632011414 and batch: 250, loss is 5.502163152694703 and perplexity is 245.22181117474756
At time: 68.39363384246826 and batch: 300, loss is 5.501745529174805 and perplexity is 245.11942216032753
At time: 69.1389799118042 and batch: 350, loss is 5.5018925285339355 and perplexity is 245.15545720679503
At time: 69.85387301445007 and batch: 400, loss is 5.541129264831543 and perplexity is 254.96576081552072
At time: 70.56734561920166 and batch: 450, loss is 5.505238819122314 and perplexity is 245.97719272205342
At time: 71.27952218055725 and batch: 500, loss is 5.526041374206543 and perplexity is 251.14774066739693
At time: 71.99410581588745 and batch: 550, loss is 5.4888715744018555 and perplexity is 241.98399180482184
At time: 72.71061587333679 and batch: 600, loss is 5.436813259124756 and perplexity is 229.7089928000333
At time: 73.42420172691345 and batch: 650, loss is 5.463099699020386 and perplexity is 235.82728621288177
At time: 74.13698601722717 and batch: 700, loss is 5.471678981781006 and perplexity is 237.85921898366848
At time: 74.85391497612 and batch: 750, loss is 5.439692754745483 and perplexity is 230.37139206904973
At time: 75.5722336769104 and batch: 800, loss is 5.405948629379273 and perplexity is 222.72740606051562
At time: 76.29254984855652 and batch: 850, loss is 5.3842144966125485 and perplexity is 217.93884515290614
At time: 77.00605416297913 and batch: 900, loss is 5.431330442428589 and perplexity is 228.45298687061674
At time: 77.71966052055359 and batch: 950, loss is 5.3988010120391845 and perplexity is 221.14111165697287
At time: 78.43849778175354 and batch: 1000, loss is 5.406458234786987 and perplexity is 222.84093807689797
At time: 79.15903496742249 and batch: 1050, loss is 5.375733575820923 and perplexity is 216.09833869616244
At time: 79.87651681900024 and batch: 1100, loss is 5.359387464523316 and perplexity is 212.59468477209285
At time: 80.59142923355103 and batch: 1150, loss is 5.358771553039551 and perplexity is 212.4637855796503
At time: 81.30895018577576 and batch: 1200, loss is 5.345914363861084 and perplexity is 209.7495843836747
At time: 82.0223536491394 and batch: 1250, loss is 5.375254383087158 and perplexity is 215.99481074938166
At time: 82.73643279075623 and batch: 1300, loss is 5.344895648956299 and perplexity is 209.5360181558098
At time: 83.45202827453613 and batch: 1350, loss is 5.319793653488159 and perplexity is 204.34171244276175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.05311767578125 and perplexity of 156.5096509954282
Finished 4 epochs...
Completing Train Step...
At time: 85.94819736480713 and batch: 50, loss is 5.3892760181427 and perplexity is 219.0447437135913
At time: 86.71173405647278 and batch: 100, loss is 5.395281496047974 and perplexity is 220.3641700094558
At time: 87.42812609672546 and batch: 150, loss is 5.345886106491089 and perplexity is 209.74365749580198
At time: 88.18488693237305 and batch: 200, loss is 5.338730201721192 and perplexity is 208.24810923899383
At time: 88.90463066101074 and batch: 250, loss is 5.354577283859253 and perplexity is 211.57452148201605
At time: 89.62738990783691 and batch: 300, loss is 5.353157091140747 and perplexity is 211.2742581535501
At time: 90.34375381469727 and batch: 350, loss is 5.353299217224121 and perplexity is 211.30428787033176
At time: 91.05860304832458 and batch: 400, loss is 5.392321557998657 and perplexity is 219.71287009715041
At time: 91.7720422744751 and batch: 450, loss is 5.3580854129791256 and perplexity is 212.31805566625758
At time: 92.48902010917664 and batch: 500, loss is 5.390573892593384 and perplexity is 219.3292208578894
At time: 93.20237135887146 and batch: 550, loss is 5.353673582077026 and perplexity is 211.38340757787333
At time: 93.91908836364746 and batch: 600, loss is 5.3029707908630375 and perplexity is 200.93285366159412
At time: 94.63298344612122 and batch: 650, loss is 5.331961278915405 and perplexity is 206.84325390576075
At time: 95.35002088546753 and batch: 700, loss is 5.3404367065429685 and perplexity is 208.60378903986737
At time: 96.06855630874634 and batch: 750, loss is 5.3086179256439205 and perplexity is 202.07075849511702
At time: 96.78425455093384 and batch: 800, loss is 5.276567335128784 and perplexity is 195.69695898785332
At time: 97.50449061393738 and batch: 850, loss is 5.2534848499298095 and perplexity is 191.23152178665268
At time: 98.22197556495667 and batch: 900, loss is 5.304263505935669 and perplexity is 201.19277055315516
At time: 98.93840909004211 and batch: 950, loss is 5.271549062728882 and perplexity is 194.7173583470266
At time: 99.65174579620361 and batch: 1000, loss is 5.281417579650879 and perplexity is 196.64844269018863
At time: 100.36335062980652 and batch: 1050, loss is 5.245361700057983 and perplexity is 189.68441168580597
At time: 101.08537316322327 and batch: 1100, loss is 5.230326881408692 and perplexity is 186.85387259554514
At time: 101.80160284042358 and batch: 1150, loss is 5.2326366996765135 and perplexity is 187.28596992491939
At time: 102.51676511764526 and batch: 1200, loss is 5.222455549240112 and perplexity is 185.38885707335945
At time: 103.23218202590942 and batch: 1250, loss is 5.258487529754639 and perplexity is 192.19058881500507
At time: 103.94799900054932 and batch: 1300, loss is 5.234269227981567 and perplexity is 187.59196928030462
At time: 104.66249704360962 and batch: 1350, loss is 5.203995399475097 and perplexity is 181.99794566808524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.9608553059895835 and perplexity of 142.71580939669755
Finished 5 epochs...
Completing Train Step...
At time: 107.22318959236145 and batch: 50, loss is 5.279128313064575 and perplexity is 196.19877687992843
At time: 107.97010445594788 and batch: 100, loss is 5.284602766036987 and perplexity is 197.27580323242148
At time: 108.68618559837341 and batch: 150, loss is 5.235270738601685 and perplexity is 187.77993874078155
At time: 109.40141654014587 and batch: 200, loss is 5.231691370010376 and perplexity is 187.10900659901745
At time: 110.12055993080139 and batch: 250, loss is 5.24445839881897 and perplexity is 189.51314688520728
At time: 110.83625745773315 and batch: 300, loss is 5.242597208023072 and perplexity is 189.16075479670346
At time: 111.55424571037292 and batch: 350, loss is 5.242160873413086 and perplexity is 189.07823541687458
At time: 112.26912021636963 and batch: 400, loss is 5.281676082611084 and perplexity is 196.69928346570623
At time: 112.98794746398926 and batch: 450, loss is 5.246572904586792 and perplexity is 189.9142974955473
At time: 113.70235180854797 and batch: 500, loss is 5.287768096923828 and perplexity is 197.90123575385763
At time: 114.42001605033875 and batch: 550, loss is 5.2516130924224855 and perplexity is 190.87391752874356
At time: 115.1465334892273 and batch: 600, loss is 5.200370092391967 and perplexity is 181.33934176855817
At time: 115.87779974937439 and batch: 650, loss is 5.232228469848633 and perplexity is 187.2095298092817
At time: 116.61071157455444 and batch: 700, loss is 5.24208179473877 and perplexity is 189.06328395185454
At time: 117.34489870071411 and batch: 750, loss is 5.208545227050781 and perplexity is 182.82789156364402
At time: 118.06805205345154 and batch: 800, loss is 5.177611150741577 and perplexity is 177.25886012106176
At time: 118.7926721572876 and batch: 850, loss is 5.152074394226074 and perplexity is 172.7895524573809
At time: 119.50919485092163 and batch: 900, loss is 5.205548181533813 and perplexity is 182.2807683369979
At time: 120.23933935165405 and batch: 950, loss is 5.1721858882904055 and perplexity is 176.2997882438314
At time: 120.95703721046448 and batch: 1000, loss is 5.182641935348511 and perplexity is 178.15285813622793
At time: 121.67853784561157 and batch: 1050, loss is 5.142642946243286 and perplexity is 171.16755768439208
At time: 122.39645409584045 and batch: 1100, loss is 5.128957853317261 and perplexity is 168.84106916630148
At time: 123.11634731292725 and batch: 1150, loss is 5.133604574203491 and perplexity is 169.62745212494363
At time: 123.83270716667175 and batch: 1200, loss is 5.124272928237915 and perplexity is 168.05191142279617
At time: 124.60463237762451 and batch: 1250, loss is 5.1634817218780515 and perplexity is 174.7719046712775
At time: 125.31609654426575 and batch: 1300, loss is 5.144863891601562 and perplexity is 171.54813394021735
At time: 126.03651165962219 and batch: 1350, loss is 5.111608028411865 and perplexity is 165.93697181773044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.884808756510417 and perplexity of 132.26516806311193
Finished 6 epochs...
Completing Train Step...
At time: 128.55754709243774 and batch: 50, loss is 5.189773015975952 and perplexity is 179.42782106041477
At time: 129.270827293396 and batch: 100, loss is 5.196060523986817 and perplexity is 180.55952900579828
At time: 129.9851176738739 and batch: 150, loss is 5.143327503204346 and perplexity is 171.28477174278498
At time: 130.70071077346802 and batch: 200, loss is 5.143312768936157 and perplexity is 171.28224800561426
At time: 131.41634964942932 and batch: 250, loss is 5.1551025867462155 and perplexity is 173.31358552375053
At time: 132.13128542900085 and batch: 300, loss is 5.151990909576416 and perplexity is 172.77512778425714
At time: 132.84771132469177 and batch: 350, loss is 5.150721807479858 and perplexity is 172.5559975860614
At time: 133.5648319721222 and batch: 400, loss is 5.191150732040406 and perplexity is 179.67519201624154
At time: 134.2780945301056 and batch: 450, loss is 5.153989334106445 and perplexity is 173.12075107380383
At time: 134.9924235343933 and batch: 500, loss is 5.202806901931763 and perplexity is 181.78177004431183
At time: 135.7075490951538 and batch: 550, loss is 5.167661981582642 and perplexity is 175.50402578393746
At time: 136.41635417938232 and batch: 600, loss is 5.114239673614502 and perplexity is 166.37423416087145
At time: 137.13066291809082 and batch: 650, loss is 5.14652174949646 and perplexity is 171.83277224816433
At time: 137.8487708568573 and batch: 700, loss is 5.159944372177124 and perplexity is 174.1547674850901
At time: 138.56269693374634 and batch: 750, loss is 5.126180763244629 and perplexity is 168.37283277750947
At time: 139.2832522392273 and batch: 800, loss is 5.094927988052368 and perplexity is 163.19209242082974
At time: 139.99906587600708 and batch: 850, loss is 5.068969402313233 and perplexity is 159.01036715275316
At time: 140.71558833122253 and batch: 900, loss is 5.1222978115081785 and perplexity is 167.72031685786533
At time: 141.43293619155884 and batch: 950, loss is 5.091172847747803 and perplexity is 162.58043237060298
At time: 142.1490240097046 and batch: 1000, loss is 5.100608148574829 and perplexity is 164.12168732653217
At time: 142.91735339164734 and batch: 1050, loss is 5.057319993972778 and perplexity is 157.16873822615804
At time: 143.63215327262878 and batch: 1100, loss is 5.044834680557251 and perplexity is 155.2186364186634
At time: 144.35263633728027 and batch: 1150, loss is 5.0508535194396975 and perplexity is 156.15568953986232
At time: 145.06621646881104 and batch: 1200, loss is 5.042700929641724 and perplexity is 154.88779160683325
At time: 145.78667187690735 and batch: 1250, loss is 5.084910268783569 and perplexity is 161.5654411235778
At time: 146.50095081329346 and batch: 1300, loss is 5.069154682159424 and perplexity is 159.039831298594
At time: 147.2167398929596 and batch: 1350, loss is 5.033920240402222 and perplexity is 153.53372356700717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.834608968098959 and perplexity of 125.78938592593714
Finished 7 epochs...
Completing Train Step...
At time: 149.740403175354 and batch: 50, loss is 5.1151014423370365 and perplexity is 166.51767206838105
At time: 150.45780730247498 and batch: 100, loss is 5.121488847732544 and perplexity is 167.58469206225905
At time: 151.17415761947632 and batch: 150, loss is 5.067692909240723 and perplexity is 158.80752101401458
At time: 151.89202642440796 and batch: 200, loss is 5.069305534362793 and perplexity is 159.0638246172458
At time: 152.60534954071045 and batch: 250, loss is 5.080083904266357 and perplexity is 160.78754612375377
At time: 153.3235948085785 and batch: 300, loss is 5.076882438659668 and perplexity is 160.27361343272952
At time: 154.04009652137756 and batch: 350, loss is 5.075234203338623 and perplexity is 160.00966238851115
At time: 154.7558982372284 and batch: 400, loss is 5.114816541671753 and perplexity is 166.4702378301712
At time: 155.47221446037292 and batch: 450, loss is 5.075858020782471 and perplexity is 160.10951034730454
At time: 156.1871485710144 and batch: 500, loss is 5.139347200393677 and perplexity is 170.6043615020897
At time: 156.90048480033875 and batch: 550, loss is 5.097721452713013 and perplexity is 163.64860108745611
At time: 157.6169729232788 and batch: 600, loss is 5.043586263656616 and perplexity is 155.02497975694135
At time: 158.3393533229828 and batch: 650, loss is 5.076614828109741 and perplexity is 160.23072826141808
At time: 159.0533411502838 and batch: 700, loss is 5.091833276748657 and perplexity is 162.6878406669849
At time: 159.77054643630981 and batch: 750, loss is 5.056447353363037 and perplexity is 157.03164622732245
At time: 160.4881944656372 and batch: 800, loss is 5.026365098953247 and perplexity is 152.37812541881007
At time: 161.20376014709473 and batch: 850, loss is 4.999774246215821 and perplexity is 148.37965805192394
At time: 161.97976756095886 and batch: 900, loss is 5.052541971206665 and perplexity is 156.41957360485426
At time: 162.69722056388855 and batch: 950, loss is 5.019155406951905 and perplexity is 151.2834768479389
At time: 163.41509222984314 and batch: 1000, loss is 5.030905303955078 and perplexity is 153.07152624584225
At time: 164.13447260856628 and batch: 1050, loss is 4.986899204254151 and perplexity is 146.48150932127484
At time: 164.85211062431335 and batch: 1100, loss is 4.974311780929566 and perplexity is 144.64924051824235
At time: 165.57443118095398 and batch: 1150, loss is 4.981700134277344 and perplexity is 145.7219179992233
At time: 166.2923560142517 and batch: 1200, loss is 4.974120960235596 and perplexity is 144.6216410831399
At time: 167.00834703445435 and batch: 1250, loss is 5.017195844650269 and perplexity is 150.98731771575075
At time: 167.72476887702942 and batch: 1300, loss is 5.005053901672364 and perplexity is 149.16512319129865
At time: 168.4401397705078 and batch: 1350, loss is 4.967665557861328 and perplexity is 143.69105707664662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.784490152994792 and perplexity of 119.64034925755124
Finished 8 epochs...
Completing Train Step...
At time: 170.93368768692017 and batch: 50, loss is 5.0499641227722165 and perplexity is 156.0168669333102
At time: 171.67492127418518 and batch: 100, loss is 5.057842950820923 and perplexity is 157.2509521894316
At time: 172.38505911827087 and batch: 150, loss is 5.0033016014099125 and perplexity is 148.9039699830235
At time: 173.0963704586029 and batch: 200, loss is 5.00608060836792 and perplexity is 149.31835066840097
At time: 173.80710649490356 and batch: 250, loss is 5.014614372253418 and perplexity is 150.59805077989085
At time: 174.51794457435608 and batch: 300, loss is 5.0125054836273195 and perplexity is 150.280790913932
At time: 175.22821044921875 and batch: 350, loss is 5.011418905258179 and perplexity is 150.1175877398041
At time: 175.94202208518982 and batch: 400, loss is 5.048999338150025 and perplexity is 155.86641684692532
At time: 176.65199947357178 and batch: 450, loss is 5.010107250213623 and perplexity is 149.92081432620031
At time: 177.3642816543579 and batch: 500, loss is 5.0707234859466555 and perplexity is 159.28952940070806
At time: 178.0806005001068 and batch: 550, loss is 5.035832347869873 and perplexity is 153.827577396896
At time: 178.79328107833862 and batch: 600, loss is 4.982372570037842 and perplexity is 145.81993958088918
At time: 179.51032710075378 and batch: 650, loss is 5.016225109100342 and perplexity is 150.8408200755936
At time: 180.2803726196289 and batch: 700, loss is 5.03301739692688 and perplexity is 153.39516920230344
At time: 180.99081420898438 and batch: 750, loss is 4.99656662940979 and perplexity is 147.90447547631288
At time: 181.6996796131134 and batch: 800, loss is 4.96739935874939 and perplexity is 143.6528117355231
At time: 182.4100043773651 and batch: 850, loss is 4.939947633743286 and perplexity is 139.76293050687096
At time: 183.11930561065674 and batch: 900, loss is 4.9921535396575925 and perplexity is 147.2531978815125
At time: 183.83312940597534 and batch: 950, loss is 4.958026704788208 and perplexity is 142.31269368235652
At time: 184.54693245887756 and batch: 1000, loss is 4.9706331157684325 and perplexity is 144.11810193603526
At time: 185.26142287254333 and batch: 1050, loss is 4.926163053512573 and perplexity is 137.84957487715099
At time: 185.96979355812073 and batch: 1100, loss is 4.910865335464478 and perplexity is 135.7568388025528
At time: 186.67988657951355 and batch: 1150, loss is 4.921213960647583 and perplexity is 137.1690299587623
At time: 187.3977119922638 and batch: 1200, loss is 4.914411077499389 and perplexity is 136.23905192888765
At time: 188.11318469047546 and batch: 1250, loss is 4.958248643875122 and perplexity is 142.34428193685326
At time: 188.82305788993835 and batch: 1300, loss is 4.949226188659668 and perplexity is 141.0657633891775
At time: 189.53398847579956 and batch: 1350, loss is 4.9095970821380615 and perplexity is 135.58477387417997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.75046630859375 and perplexity of 115.63819504085616
Finished 9 epochs...
Completing Train Step...
At time: 192.02239656448364 and batch: 50, loss is 4.993450431823731 and perplexity is 147.44429328856796
At time: 192.785409450531 and batch: 100, loss is 5.001700344085694 and perplexity is 148.6657272053843
At time: 193.50573348999023 and batch: 150, loss is 4.947438850402832 and perplexity is 140.8138563421548
At time: 194.23380589485168 and batch: 200, loss is 4.95059287071228 and perplexity is 141.25868723899222
At time: 194.98717284202576 and batch: 250, loss is 4.956456899642944 and perplexity is 142.08946574160416
At time: 195.73154592514038 and batch: 300, loss is 4.9561881160736085 and perplexity is 142.0512795599761
At time: 196.47208309173584 and batch: 350, loss is 4.955554475784302 and perplexity is 141.96129865687269
At time: 197.20521926879883 and batch: 400, loss is 4.991151666641235 and perplexity is 147.10574275397337
At time: 197.9298553466797 and batch: 450, loss is 4.953101568222046 and perplexity is 141.61350743808606
At time: 198.64628767967224 and batch: 500, loss is 5.015380058288574 and perplexity is 150.71340576150817
At time: 199.46000123023987 and batch: 550, loss is 4.98172924041748 and perplexity is 145.72615946351556
At time: 200.18674063682556 and batch: 600, loss is 4.9294563579559325 and perplexity is 138.30430386411217
At time: 200.90659308433533 and batch: 650, loss is 4.963219175338745 and perplexity is 143.05356997783332
At time: 201.6241009235382 and batch: 700, loss is 4.9808110427856445 and perplexity is 145.59241546010972
At time: 202.34264206886292 and batch: 750, loss is 4.9436902332305905 and perplexity is 140.28698723444276
At time: 203.0585412979126 and batch: 800, loss is 4.915225238800049 and perplexity is 136.35001765847844
At time: 203.78037929534912 and batch: 850, loss is 4.886421089172363 and perplexity is 132.47859552541635
At time: 204.4972321987152 and batch: 900, loss is 4.93757981300354 and perplexity is 139.43238842799803
At time: 205.21765685081482 and batch: 950, loss is 4.904226331710816 and perplexity is 134.8585338645843
At time: 205.93129181861877 and batch: 1000, loss is 4.917639474868775 and perplexity is 136.6795964695274
At time: 206.65290760993958 and batch: 1050, loss is 4.872313585281372 and perplexity is 130.6227745036584
At time: 207.36981773376465 and batch: 1100, loss is 4.855445375442505 and perplexity is 128.4378815565205
At time: 208.08507537841797 and batch: 1150, loss is 4.8682667255401615 and perplexity is 130.09523062424657
At time: 208.798198223114 and batch: 1200, loss is 4.861172122955322 and perplexity is 129.17552300248315
At time: 209.51688766479492 and batch: 1250, loss is 4.904996423721314 and perplexity is 134.96242734269518
At time: 210.23219847679138 and batch: 1300, loss is 4.897427616119384 and perplexity is 133.9447787480108
At time: 210.94815278053284 and batch: 1350, loss is 4.857975950241089 and perplexity is 128.76331481570543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.71817626953125 and perplexity of 111.96387442984908
Finished 10 epochs...
Completing Train Step...
At time: 213.45905423164368 and batch: 50, loss is 4.9424922657012935 and perplexity is 140.11902860353285
At time: 214.17538571357727 and batch: 100, loss is 4.95061354637146 and perplexity is 141.26160788565875
At time: 214.88978052139282 and batch: 150, loss is 4.897650365829468 and perplexity is 133.9746182318889
At time: 215.60628175735474 and batch: 200, loss is 4.90230616569519 and perplexity is 134.59983154612758
At time: 216.3222894668579 and batch: 250, loss is 4.906611452102661 and perplexity is 135.1805716004371
At time: 217.0424988269806 and batch: 300, loss is 4.905441617965698 and perplexity is 135.02252521517713
At time: 217.78551983833313 and batch: 350, loss is 4.905748949050904 and perplexity is 135.06402821163252
At time: 218.4991476535797 and batch: 400, loss is 4.940032911300659 and perplexity is 139.77484965640662
At time: 219.2171492576599 and batch: 450, loss is 4.901417455673218 and perplexity is 134.48026446497792
At time: 219.93193864822388 and batch: 500, loss is 4.966037979125977 and perplexity is 143.4573787841831
At time: 220.65093278884888 and batch: 550, loss is 4.933972091674804 and perplexity is 138.93026153792093
At time: 221.37017822265625 and batch: 600, loss is 4.881583223342895 and perplexity is 131.83922968509017
At time: 222.08580684661865 and batch: 650, loss is 4.9151928424835205 and perplexity is 136.34560049169812
At time: 222.80421137809753 and batch: 700, loss is 4.933052062988281 and perplexity is 138.8025004928051
At time: 223.52333188056946 and batch: 750, loss is 4.894884233474731 and perplexity is 133.60453878602027
At time: 224.24122500419617 and batch: 800, loss is 4.86756799697876 and perplexity is 130.00436112116293
At time: 224.96054315567017 and batch: 850, loss is 4.836910371780395 and perplexity is 126.0792114566437
At time: 225.67645263671875 and batch: 900, loss is 4.888776330947876 and perplexity is 132.79098237683533
At time: 226.3912434577942 and batch: 950, loss is 4.855645818710327 and perplexity is 128.46362864554095
At time: 227.1094036102295 and batch: 1000, loss is 4.869588556289673 and perplexity is 130.26730820411368
At time: 227.8242371082306 and batch: 1050, loss is 4.823105297088623 and perplexity is 124.35063752408992
At time: 228.54638648033142 and batch: 1100, loss is 4.804869060516357 and perplexity is 122.10350180491776
At time: 229.27086305618286 and batch: 1150, loss is 4.819716558456421 and perplexity is 123.9299589022954
At time: 229.99166655540466 and batch: 1200, loss is 4.811795310974121 and perplexity is 122.95215684616849
At time: 230.70665383338928 and batch: 1250, loss is 4.855030679702759 and perplexity is 128.384629956586
At time: 231.42094564437866 and batch: 1300, loss is 4.849482803344727 and perplexity is 127.67434002550264
At time: 232.1385841369629 and batch: 1350, loss is 4.810138854980469 and perplexity is 122.74866059684932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.6900252278645835 and perplexity of 108.85592597533548
Finished 11 epochs...
Completing Train Step...
At time: 234.66727304458618 and batch: 50, loss is 4.8951097583770755 and perplexity is 133.63467333450055
At time: 235.38321018218994 and batch: 100, loss is 4.903510522842407 and perplexity is 134.76203547138664
At time: 236.1277937889099 and batch: 150, loss is 4.851829357147217 and perplexity is 127.9742865162258
At time: 236.84775471687317 and batch: 200, loss is 4.854831027984619 and perplexity is 128.35900030321966
At time: 237.56235671043396 and batch: 250, loss is 4.860156679153443 and perplexity is 129.04441909388683
At time: 238.27553057670593 and batch: 300, loss is 4.859364833831787 and perplexity is 128.94227632031738
At time: 238.99457550048828 and batch: 350, loss is 4.859942741394043 and perplexity is 129.01681457293284
At time: 239.7099485397339 and batch: 400, loss is 4.893248748779297 and perplexity is 133.3862091936145
At time: 240.42564630508423 and batch: 450, loss is 4.854978914260864 and perplexity is 128.37798424149446
At time: 241.14124536514282 and batch: 500, loss is 4.919652271270752 and perplexity is 136.95498172377592
At time: 241.85741186141968 and batch: 550, loss is 4.8916836357116695 and perplexity is 133.17760797938953
At time: 242.57369780540466 and batch: 600, loss is 4.837641248703003 and perplexity is 126.1713935254878
At time: 243.2957043647766 and batch: 650, loss is 4.870923681259155 and perplexity is 130.44134749625357
At time: 244.01468753814697 and batch: 700, loss is 4.889234008789063 and perplexity is 132.85177177689818
At time: 244.73381876945496 and batch: 750, loss is 4.850518980026245 and perplexity is 127.80670176269547
At time: 245.451584815979 and batch: 800, loss is 4.823661785125733 and perplexity is 124.4198564242377
At time: 246.1673686504364 and batch: 850, loss is 4.791485052108765 and perplexity is 120.48015518522918
At time: 246.88694739341736 and batch: 900, loss is 4.843011569976807 and perplexity is 126.85079711921732
At time: 247.60150909423828 and batch: 950, loss is 4.810681667327881 and perplexity is 122.81530817237656
At time: 248.3219017982483 and batch: 1000, loss is 4.825232305526733 and perplexity is 124.61541387032509
At time: 249.03641200065613 and batch: 1050, loss is 4.778315753936767 and perplexity is 118.90391784808637
At time: 249.7520215511322 and batch: 1100, loss is 4.759109001159668 and perplexity is 116.64195174170584
At time: 250.46848273277283 and batch: 1150, loss is 4.775741624832153 and perplexity is 118.59823741174502
At time: 251.18362498283386 and batch: 1200, loss is 4.766544418334961 and perplexity is 117.5124656194072
At time: 251.90012454986572 and batch: 1250, loss is 4.810045108795166 and perplexity is 122.73715391752944
At time: 252.61571502685547 and batch: 1300, loss is 4.8068421363830565 and perplexity is 122.34465911016201
At time: 253.33238697052002 and batch: 1350, loss is 4.766992855072021 and perplexity is 117.56517434343428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.66892822265625 and perplexity of 106.58344746979915
Finished 12 epochs...
Completing Train Step...
At time: 255.84291982650757 and batch: 50, loss is 4.853042936325073 and perplexity is 128.12968772240427
At time: 256.58898162841797 and batch: 100, loss is 4.86075101852417 and perplexity is 129.12113806902235
At time: 257.3066997528076 and batch: 150, loss is 4.8101831150054934 and perplexity is 122.75409357587007
At time: 258.0211515426636 and batch: 200, loss is 4.814270486831665 and perplexity is 123.25686200028312
At time: 258.7393147945404 and batch: 250, loss is 4.8170358848571775 and perplexity is 123.59818801671338
At time: 259.46113109588623 and batch: 300, loss is 4.817363300323486 and perplexity is 123.63866260071863
At time: 260.17914032936096 and batch: 350, loss is 4.819143371582031 and perplexity is 123.858944230792
At time: 260.9013991355896 and batch: 400, loss is 4.850284328460694 and perplexity is 127.7767152383689
At time: 261.6169557571411 and batch: 450, loss is 4.811983413696289 and perplexity is 122.9752866568897
At time: 262.3347816467285 and batch: 500, loss is 4.882650146484375 and perplexity is 131.97996707483463
At time: 263.062618970871 and batch: 550, loss is 4.851713790893554 and perplexity is 127.9594978619191
At time: 263.78082489967346 and batch: 600, loss is 4.79738829612732 and perplexity is 121.1934823412965
At time: 264.50143456459045 and batch: 650, loss is 4.830892887115478 and perplexity is 125.32280983516559
At time: 265.2173898220062 and batch: 700, loss is 4.850559329986572 and perplexity is 127.81185886208476
At time: 265.9302752017975 and batch: 750, loss is 4.810296821594238 and perplexity is 122.76805231869051
At time: 266.64678597450256 and batch: 800, loss is 4.783822059631348 and perplexity is 119.56044502892664
At time: 267.3625226020813 and batch: 850, loss is 4.750905122756958 and perplexity is 115.68894985379883
At time: 268.0846025943756 and batch: 900, loss is 4.802013578414917 and perplexity is 121.75533476993206
At time: 268.80016255378723 and batch: 950, loss is 4.770033674240112 and perplexity is 117.92321286839659
At time: 269.51974630355835 and batch: 1000, loss is 4.785246887207031 and perplexity is 119.73091946746486
At time: 270.23537039756775 and batch: 1050, loss is 4.738007230758667 and perplexity is 114.20638778854179
At time: 270.95228362083435 and batch: 1100, loss is 4.718129396438599 and perplexity is 111.95862645978454
At time: 271.66663789749146 and batch: 1150, loss is 4.735729970932007 and perplexity is 113.94660607717758
At time: 272.38531732559204 and batch: 1200, loss is 4.72553955078125 and perplexity is 112.79133861426124
At time: 273.1757574081421 and batch: 1250, loss is 4.769319801330567 and perplexity is 117.83906072186586
At time: 273.8923890590668 and batch: 1300, loss is 4.767629699707031 and perplexity is 117.64006893955853
At time: 274.6370458602905 and batch: 1350, loss is 4.727797737121582 and perplexity is 113.04633027541196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.6397314453125 and perplexity of 103.51654399693226
Finished 13 epochs...
Completing Train Step...
At time: 277.21357250213623 and batch: 50, loss is 4.813418312072754 and perplexity is 123.15187035559237
At time: 277.96702766418457 and batch: 100, loss is 4.821890621185303 and perplexity is 124.1996834997951
At time: 278.6830859184265 and batch: 150, loss is 4.771852893829346 and perplexity is 118.13793634262738
At time: 279.40009236335754 and batch: 200, loss is 4.7762235736846925 and perplexity is 118.6554094720766
At time: 280.11502623558044 and batch: 250, loss is 4.778843250274658 and perplexity is 118.96665577487066
At time: 280.8318808078766 and batch: 300, loss is 4.779131965637207 and perplexity is 119.00100823482678
At time: 281.54936599731445 and batch: 350, loss is 4.783870553970337 and perplexity is 119.56624317426501
At time: 282.2636773586273 and batch: 400, loss is 4.8126605606079105 and perplexity is 123.05858719262241
At time: 282.9824125766754 and batch: 450, loss is 4.773245944976806 and perplexity is 118.30262321239208
At time: 283.7020080089569 and batch: 500, loss is 4.84163119316101 and perplexity is 126.6758160175213
At time: 284.4177141189575 and batch: 550, loss is 4.813038110733032 and perplexity is 123.10505674935577
At time: 285.13346219062805 and batch: 600, loss is 4.760171537399292 and perplexity is 116.76595390922384
At time: 285.8491473197937 and batch: 650, loss is 4.793742656707764 and perplexity is 120.75245899917876
At time: 286.56994009017944 and batch: 700, loss is 4.81381049156189 and perplexity is 123.20017746510419
At time: 287.28714394569397 and batch: 750, loss is 4.77411039352417 and perplexity is 118.40493395799389
At time: 288.00277638435364 and batch: 800, loss is 4.747705173492432 and perplexity is 115.3193427611605
At time: 288.7219047546387 and batch: 850, loss is 4.714058799743652 and perplexity is 111.50381435128979
At time: 289.43854784965515 and batch: 900, loss is 4.764359941482544 and perplexity is 117.25604253550637
At time: 290.15683484077454 and batch: 950, loss is 4.732983636856079 and perplexity is 113.63409994960425
At time: 290.87268137931824 and batch: 1000, loss is 4.748591499328613 and perplexity is 115.42159858335563
At time: 291.65275168418884 and batch: 1050, loss is 4.701044683456421 and perplexity is 110.06209246132906
At time: 292.37600660324097 and batch: 1100, loss is 4.680110149383545 and perplexity is 107.78194403226384
At time: 293.09891152381897 and batch: 1150, loss is 4.6992538547515865 and perplexity is 109.86516648980603
At time: 293.82101559638977 and batch: 1200, loss is 4.688167085647583 and perplexity is 108.65384399038771
At time: 294.5434377193451 and batch: 1250, loss is 4.7320508289337155 and perplexity is 113.52815058380699
At time: 295.2641637325287 and batch: 1300, loss is 4.731890020370483 and perplexity is 113.50989575283107
At time: 295.9860534667969 and batch: 1350, loss is 4.691923303604126 and perplexity is 109.06273897881667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.62929931640625 and perplexity of 102.44225934679032
Finished 14 epochs...
Completing Train Step...
At time: 298.4639616012573 and batch: 50, loss is 4.778418235778808 and perplexity is 118.91610396503052
At time: 299.17940402030945 and batch: 100, loss is 4.786060237884522 and perplexity is 119.82834230602542
At time: 299.89871072769165 and batch: 150, loss is 4.736372480392456 and perplexity is 114.0198413742344
At time: 300.6196563243866 and batch: 200, loss is 4.74146279335022 and perplexity is 114.6017177602477
At time: 301.3385360240936 and batch: 250, loss is 4.743538551330566 and perplexity is 114.83985025766103
At time: 302.0560998916626 and batch: 300, loss is 4.744116172790528 and perplexity is 114.90620338127782
At time: 302.77409195899963 and batch: 350, loss is 4.748263034820557 and perplexity is 115.38369291043274
At time: 303.4969856739044 and batch: 400, loss is 4.776659774780273 and perplexity is 118.7071783816916
At time: 304.2142696380615 and batch: 450, loss is 4.737376766204834 and perplexity is 114.13440740214519
At time: 304.9306433200836 and batch: 500, loss is 4.806272096633911 and perplexity is 122.27493766525907
At time: 305.64434933662415 and batch: 550, loss is 4.778932628631591 and perplexity is 118.97728929428985
At time: 306.36106038093567 and batch: 600, loss is 4.725997972488403 and perplexity is 112.84305646564799
At time: 307.07992792129517 and batch: 650, loss is 4.758949632644653 and perplexity is 116.62336416824463
At time: 307.79528427124023 and batch: 700, loss is 4.780227012634278 and perplexity is 119.13139130630933
At time: 308.5097076892853 and batch: 750, loss is 4.739245281219483 and perplexity is 114.3478686217054
At time: 309.2274534702301 and batch: 800, loss is 4.7139662170410155 and perplexity is 111.49349150466873
At time: 309.99937868118286 and batch: 850, loss is 4.679977054595947 and perplexity is 107.76759977191035
At time: 310.71089696884155 and batch: 900, loss is 4.729745626449585 and perplexity is 113.26674661935404
At time: 311.4270417690277 and batch: 950, loss is 4.7018971824646 and perplexity is 110.15596029142156
At time: 312.14299488067627 and batch: 1000, loss is 4.715440502166748 and perplexity is 111.65798592685574
At time: 312.8593559265137 and batch: 1050, loss is 4.667169818878174 and perplexity is 106.39619541369765
At time: 313.57669711112976 and batch: 1100, loss is 4.645497159957886 and perplexity is 104.1151147866664
At time: 314.28993034362793 and batch: 1150, loss is 4.665827016830445 and perplexity is 106.25342226411745
At time: 315.00597071647644 and batch: 1200, loss is 4.653655080795288 and perplexity is 104.96795160891016
At time: 315.72480297088623 and batch: 1250, loss is 4.697633266448975 and perplexity is 109.68726447802322
At time: 316.44005131721497 and batch: 1300, loss is 4.698602113723755 and perplexity is 109.79358618172061
At time: 317.15625858306885 and batch: 1350, loss is 4.658842735290527 and perplexity is 105.5139039566279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.606986490885417 and perplexity of 100.1817955378173
Finished 15 epochs...
Completing Train Step...
At time: 319.7151017189026 and batch: 50, loss is 4.745372896194458 and perplexity is 115.0506994731165
At time: 320.42478919029236 and batch: 100, loss is 4.752766466140747 and perplexity is 115.90448724750833
At time: 321.1342439651489 and batch: 150, loss is 4.703506650924683 and perplexity is 110.33339558515308
At time: 321.85126519203186 and batch: 200, loss is 4.709871044158936 and perplexity is 111.03784000435384
At time: 322.56692910194397 and batch: 250, loss is 4.711277542114257 and perplexity is 111.19412438034699
At time: 323.28656029701233 and batch: 300, loss is 4.71145604133606 and perplexity is 111.21397421655536
At time: 324.00050234794617 and batch: 350, loss is 4.716253471374512 and perplexity is 111.74879733965476
At time: 324.71020913124084 and batch: 400, loss is 4.744364051818848 and perplexity is 114.93468974976075
At time: 325.41947293281555 and batch: 450, loss is 4.7045566368103025 and perplexity is 110.44930493417131
At time: 326.1347873210907 and batch: 500, loss is 4.773441486358642 and perplexity is 118.32575853269238
At time: 326.847177028656 and batch: 550, loss is 4.747343940734863 and perplexity is 115.27769316002693
At time: 327.5601909160614 and batch: 600, loss is 4.694202690124512 and perplexity is 109.31161865465386
At time: 328.2703514099121 and batch: 650, loss is 4.727087917327881 and perplexity is 112.96611622470562
At time: 329.03785252571106 and batch: 700, loss is 4.748700218200684 and perplexity is 115.4341477715206
At time: 329.75232219696045 and batch: 750, loss is 4.707286081314087 and perplexity is 110.75118197337146
At time: 330.46340775489807 and batch: 800, loss is 4.68263765335083 and perplexity is 108.05470788405833
At time: 331.1745607852936 and batch: 850, loss is 4.648618268966675 and perplexity is 104.4405770467715
At time: 331.88638854026794 and batch: 900, loss is 4.69791109085083 and perplexity is 109.71774251024252
At time: 332.5992646217346 and batch: 950, loss is 4.6698424434661865 and perplexity is 106.68093283025355
At time: 333.311559677124 and batch: 1000, loss is 4.6840957832336425 and perplexity is 108.2123806083219
At time: 334.02234959602356 and batch: 1050, loss is 4.635410318374634 and perplexity is 103.07020091721967
At time: 334.7346200942993 and batch: 1100, loss is 4.614215393066406 and perplexity is 100.90862385833724
At time: 335.44658756256104 and batch: 1150, loss is 4.635765829086304 and perplexity is 103.10684999188304
At time: 336.15545415878296 and batch: 1200, loss is 4.622015323638916 and perplexity is 101.69878170073092
At time: 336.8656520843506 and batch: 1250, loss is 4.666014289855957 and perplexity is 106.27332252730893
At time: 337.57709980010986 and batch: 1300, loss is 4.6683128070831295 and perplexity is 106.51787453576131
At time: 338.2867958545685 and batch: 1350, loss is 4.628545169830322 and perplexity is 102.36503199170649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589580485026041 and perplexity of 98.45311893958592
Finished 16 epochs...
Completing Train Step...
At time: 340.78610587120056 and batch: 50, loss is 4.714189081192017 and perplexity is 111.51834217605413
At time: 341.53210043907166 and batch: 100, loss is 4.7215633201599125 and perplexity is 112.34374469787977
At time: 342.24831891059875 and batch: 150, loss is 4.67334981918335 and perplexity is 107.05575988895833
At time: 342.96697640419006 and batch: 200, loss is 4.680378761291504 and perplexity is 107.81089943460272
At time: 343.6825394630432 and batch: 250, loss is 4.6806416416168215 and perplexity is 107.83924452443887
At time: 344.3953456878662 and batch: 300, loss is 4.681436681747437 and perplexity is 107.92501514251485
At time: 345.10886430740356 and batch: 350, loss is 4.686380615234375 and perplexity is 108.45991039271263
At time: 345.8263018131256 and batch: 400, loss is 4.714436731338501 and perplexity is 111.54596312985515
At time: 346.543585062027 and batch: 450, loss is 4.674102849960327 and perplexity is 107.13640653189864
At time: 347.2893147468567 and batch: 500, loss is 4.742873229980469 and perplexity is 114.76347026488007
At time: 348.0089190006256 and batch: 550, loss is 4.718220100402832 and perplexity is 111.96878201160204
At time: 348.7395131587982 and batch: 600, loss is 4.664819688796997 and perplexity is 106.14644410331717
At time: 349.4602749347687 and batch: 650, loss is 4.697038860321045 and perplexity is 109.62208506930068
At time: 350.19003534317017 and batch: 700, loss is 4.719234371185303 and perplexity is 112.08240628889669
At time: 350.90385270118713 and batch: 750, loss is 4.6775352573394775 and perplexity is 107.50477415647302
At time: 351.61861872673035 and batch: 800, loss is 4.653363151550293 and perplexity is 104.93731286643832
At time: 352.34106707572937 and batch: 850, loss is 4.619346809387207 and perplexity is 101.42775882740986
At time: 353.0774886608124 and batch: 900, loss is 4.668252944946289 and perplexity is 106.5114983390282
At time: 353.7974319458008 and batch: 950, loss is 4.640879278182983 and perplexity is 103.63543190744345
At time: 354.514080286026 and batch: 1000, loss is 4.654942646026611 and perplexity is 105.10319174036682
At time: 355.22784543037415 and batch: 1050, loss is 4.6058033847808835 and perplexity is 100.06333993054669
At time: 355.9431722164154 and batch: 1100, loss is 4.584039468765258 and perplexity is 97.90909721514547
At time: 356.6584062576294 and batch: 1150, loss is 4.606710214614868 and perplexity is 100.1541215079841
At time: 357.3766133785248 and batch: 1200, loss is 4.5921526050567625 and perplexity is 98.70667813144415
At time: 358.093008518219 and batch: 1250, loss is 4.636434335708618 and perplexity is 103.17580064832592
At time: 358.80918550491333 and batch: 1300, loss is 4.640366926193237 and perplexity is 103.58234768776148
At time: 359.5274999141693 and batch: 1350, loss is 4.599541254043579 and perplexity is 99.43868808093553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.5757454427083335 and perplexity of 97.10039494641657
Finished 17 epochs...
Completing Train Step...
At time: 362.10067892074585 and batch: 50, loss is 4.685363655090332 and perplexity is 108.34966705265944
At time: 362.8755314350128 and batch: 100, loss is 4.692629594802856 and perplexity is 109.13979624069641
At time: 363.5989956855774 and batch: 150, loss is 4.64529185295105 and perplexity is 104.09374141820938
At time: 364.32294178009033 and batch: 200, loss is 4.652433595657349 and perplexity is 104.83981309163929
At time: 365.0417699813843 and batch: 250, loss is 4.652319459915161 and perplexity is 104.82784780460774
At time: 365.83467841148376 and batch: 300, loss is 4.653661479949951 and perplexity is 104.96862331721634
At time: 366.55004477500916 and batch: 350, loss is 4.658582391738892 and perplexity is 105.4864376676157
At time: 367.26990699768066 and batch: 400, loss is 4.686544780731201 and perplexity is 108.47771722938205
At time: 367.9882616996765 and batch: 450, loss is 4.645538396835327 and perplexity is 104.11940825741873
At time: 368.70320200920105 and batch: 500, loss is 4.714175415039063 and perplexity is 111.51681815974649
At time: 369.41896533966064 and batch: 550, loss is 4.691004028320313 and perplexity is 108.96252636703423
At time: 370.1364622116089 and batch: 600, loss is 4.637363529205322 and perplexity is 103.27171548612628
At time: 370.8571357727051 and batch: 650, loss is 4.668989934921265 and perplexity is 106.59002517871615
At time: 371.5731554031372 and batch: 700, loss is 4.69183464050293 and perplexity is 109.05306956682023
At time: 372.28896832466125 and batch: 750, loss is 4.649755277633667 and perplexity is 104.55939442345242
At time: 373.0060794353485 and batch: 800, loss is 4.6259729766845705 and perplexity is 102.10206770051133
At time: 373.7215621471405 and batch: 850, loss is 4.592208967208863 and perplexity is 98.71224160903361
At time: 374.4383451938629 and batch: 900, loss is 4.641582193374634 and perplexity is 103.70830443552755
At time: 375.1538038253784 and batch: 950, loss is 4.617368841171265 and perplexity is 101.22733622434805
At time: 375.8683919906616 and batch: 1000, loss is 4.628058023452759 and perplexity is 102.31517738141493
At time: 376.5852909088135 and batch: 1050, loss is 4.578100147247315 and perplexity is 97.32930709159794
At time: 377.29960799217224 and batch: 1100, loss is 4.556373882293701 and perplexity is 95.23751052322747
At time: 378.01321268081665 and batch: 1150, loss is 4.579842052459717 and perplexity is 97.49899326464929
At time: 378.7296359539032 and batch: 1200, loss is 4.564238023757935 and perplexity is 95.98942448240044
At time: 379.4436023235321 and batch: 1250, loss is 4.608677959442138 and perplexity is 100.35139328912138
At time: 380.1598913669586 and batch: 1300, loss is 4.614092597961426 and perplexity is 100.8962335340283
At time: 380.88372349739075 and batch: 1350, loss is 4.57264476776123 and perplexity is 96.79978447297798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.563056233723958 and perplexity of 95.87605214152616
Finished 18 epochs...
Completing Train Step...
At time: 383.42127871513367 and batch: 50, loss is 4.65849612236023 and perplexity is 105.47733781070599
At time: 384.1375753879547 and batch: 100, loss is 4.665296630859375 and perplexity is 106.19708188196532
At time: 384.88674306869507 and batch: 150, loss is 4.618515167236328 and perplexity is 101.34344229335132
At time: 385.6059846878052 and batch: 200, loss is 4.625937480926513 and perplexity is 102.09844357454001
At time: 386.33002948760986 and batch: 250, loss is 4.625671482086181 and perplexity is 102.07128911863649
At time: 387.0466830730438 and batch: 300, loss is 4.627618265151978 and perplexity is 102.27019332464666
At time: 387.76345324516296 and batch: 350, loss is 4.632449836730957 and perplexity is 102.76551471088854
At time: 388.4803433418274 and batch: 400, loss is 4.660335464477539 and perplexity is 105.67152525438719
At time: 389.1946885585785 and batch: 450, loss is 4.618740739822388 and perplexity is 101.36630517423227
At time: 389.9113256931305 and batch: 500, loss is 4.687141904830932 and perplexity is 108.54251123172848
At time: 390.63303327560425 and batch: 550, loss is 4.665578174591064 and perplexity is 106.2269852140432
At time: 391.34841322898865 and batch: 600, loss is 4.611441698074341 and perplexity is 100.62912191944223
At time: 392.0638818740845 and batch: 650, loss is 4.642813510894776 and perplexity is 103.83608093834252
At time: 392.7822139263153 and batch: 700, loss is 4.666137580871582 and perplexity is 106.28642588092349
At time: 393.4982144832611 and batch: 750, loss is 4.623865442276001 and perplexity is 101.88711067387598
At time: 394.2132320404053 and batch: 800, loss is 4.600452890396118 and perplexity is 99.52938133719245
At time: 394.9302999973297 and batch: 850, loss is 4.56686936378479 and perplexity is 96.24233790186781
At time: 395.64646792411804 and batch: 900, loss is 4.614927768707275 and perplexity is 100.98053431451515
At time: 396.36292839050293 and batch: 950, loss is 4.5888911151885985 and perplexity is 98.38527171758793
At time: 397.078715801239 and batch: 1000, loss is 4.601908378601074 and perplexity is 99.67435065274722
At time: 397.79400849342346 and batch: 1050, loss is 4.551472558975219 and perplexity is 94.77186276938936
At time: 398.51063656806946 and batch: 1100, loss is 4.530324544906616 and perplexity is 92.78867028628612
At time: 399.22727942466736 and batch: 1150, loss is 4.554440898895264 and perplexity is 95.05359580579062
At time: 399.95118975639343 and batch: 1200, loss is 4.538141469955445 and perplexity is 93.51683466264262
At time: 400.66846323013306 and batch: 1250, loss is 4.5826926517486575 and perplexity is 97.77732033652669
At time: 401.3869881629944 and batch: 1300, loss is 4.589417104721069 and perplexity is 98.43703495292722
At time: 402.10172414779663 and batch: 1350, loss is 4.547554779052734 and perplexity is 94.4012938459474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.551368001302083 and perplexity of 94.76195416195888
Finished 19 epochs...
Completing Train Step...
At time: 404.68040227890015 and batch: 50, loss is 4.633645620346069 and perplexity is 102.88847353099807
At time: 405.3982377052307 and batch: 100, loss is 4.639596166610718 and perplexity is 103.502541360398
At time: 406.1141724586487 and batch: 150, loss is 4.594219999313355 and perplexity is 98.91095483838716
At time: 406.8300635814667 and batch: 200, loss is 4.600879993438721 and perplexity is 99.57189971800845
At time: 407.54845571517944 and batch: 250, loss is 4.600889701843261 and perplexity is 99.5728664069843
At time: 408.26587176322937 and batch: 300, loss is 4.603451719284058 and perplexity is 99.82830090144274
At time: 408.9831655025482 and batch: 350, loss is 4.607947978973389 and perplexity is 100.27816546270179
At time: 409.7035143375397 and batch: 400, loss is 4.635495166778565 and perplexity is 103.07894663028502
At time: 410.41855359077454 and batch: 450, loss is 4.593356485366821 and perplexity is 98.82558071559713
At time: 411.1337380409241 and batch: 500, loss is 4.661801776885986 and perplexity is 105.82658637933032
At time: 411.8510174751282 and batch: 550, loss is 4.641947555541992 and perplexity is 103.74620244923776
At time: 412.56978964805603 and batch: 600, loss is 4.586964254379272 and perplexity is 98.19587951810162
At time: 413.2841260433197 and batch: 650, loss is 4.618579702377319 and perplexity is 101.34998271772957
At time: 414.0019476413727 and batch: 700, loss is 4.642052812576294 and perplexity is 103.75712304155213
At time: 414.71738147735596 and batch: 750, loss is 4.599579410552979 and perplexity is 99.44248238656021
At time: 415.4459397792816 and batch: 800, loss is 4.576391324996949 and perplexity is 97.16313062948235
At time: 416.1698589324951 and batch: 850, loss is 4.542802619934082 and perplexity is 93.95374812307283
At time: 416.88794207572937 and batch: 900, loss is 4.5899678993225095 and perplexity is 98.4912684747555
At time: 417.60368609428406 and batch: 950, loss is 4.564263715744018 and perplexity is 95.99189067303892
At time: 418.32537937164307 and batch: 1000, loss is 4.577647514343262 and perplexity is 97.28526261342162
At time: 419.0408661365509 and batch: 1050, loss is 4.526626644134521 and perplexity is 92.44618062746
At time: 419.75743794441223 and batch: 1100, loss is 4.50569519996643 and perplexity is 90.53125950549263
At time: 420.47478199005127 and batch: 1150, loss is 4.530545339584351 and perplexity is 92.80915979274317
At time: 421.2477996349335 and batch: 1200, loss is 4.513462915420532 and perplexity is 91.23721886510388
At time: 421.96415281295776 and batch: 1250, loss is 4.558201646804809 and perplexity is 95.41174144312943
At time: 422.6843135356903 and batch: 1300, loss is 4.566040410995483 and perplexity is 96.16259060535316
At time: 423.4002387523651 and batch: 1350, loss is 4.524200353622437 and perplexity is 92.22215122663292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.538812255859375 and perplexity of 93.57958548094373
Finished 20 epochs...
Completing Train Step...
At time: 425.84328722953796 and batch: 50, loss is 4.6109054660797115 and perplexity is 100.57517582977988
At time: 426.63609886169434 and batch: 100, loss is 4.6160181045532225 and perplexity is 101.09069705710097
At time: 427.37308406829834 and batch: 150, loss is 4.5706845569610595 and perplexity is 96.61022234156671
At time: 428.1187083721161 and batch: 200, loss is 4.577038125991821 and perplexity is 97.2259961675931
At time: 428.85982942581177 and batch: 250, loss is 4.577264575958252 and perplexity is 97.2480154842041
At time: 429.5996286869049 and batch: 300, loss is 4.580510740280151 and perplexity is 97.56421145682522
At time: 430.32672810554504 and batch: 350, loss is 4.584899196624756 and perplexity is 97.99330858796425
At time: 431.0427131652832 and batch: 400, loss is 4.611922693252564 and perplexity is 100.67753568433463
At time: 431.7819926738739 and batch: 450, loss is 4.569228534698486 and perplexity is 96.46965806423682
At time: 432.5209195613861 and batch: 500, loss is 4.637896299362183 and perplexity is 103.32675023331878
At time: 433.2376410961151 and batch: 550, loss is 4.6195041275024415 and perplexity is 101.44371650644412
At time: 433.9558322429657 and batch: 600, loss is 4.563999509811401 and perplexity is 95.96653239609108
At time: 434.67656326293945 and batch: 650, loss is 4.595258827209473 and perplexity is 99.01375968653174
At time: 435.39204931259155 and batch: 700, loss is 4.621637582778931 and perplexity is 101.66037317016422
At time: 436.1076967716217 and batch: 750, loss is 4.576922130584717 and perplexity is 97.21471905264592
At time: 436.8249011039734 and batch: 800, loss is 4.553966884613037 and perplexity is 95.00854972088894
At time: 437.5477247238159 and batch: 850, loss is 4.519946489334107 and perplexity is 91.83068392563453
At time: 438.2632007598877 and batch: 900, loss is 4.565897340774536 and perplexity is 96.14883358640176
At time: 438.9803922176361 and batch: 950, loss is 4.541375732421875 and perplexity is 93.8197822929656
At time: 439.69640278816223 and batch: 1000, loss is 4.55463677406311 and perplexity is 95.07221626840708
At time: 440.47800731658936 and batch: 1050, loss is 4.503210611343384 and perplexity is 90.30660576972224
At time: 441.19864416122437 and batch: 1100, loss is 4.482612419128418 and perplexity is 88.46547997377282
At time: 441.91870045661926 and batch: 1150, loss is 4.508042507171631 and perplexity is 90.74401378536575
At time: 442.64171051979065 and batch: 1200, loss is 4.4900533485412595 and perplexity is 89.12620050461626
At time: 443.3600149154663 and batch: 1250, loss is 4.53492956161499 and perplexity is 93.21694902182274
At time: 444.07338762283325 and batch: 1300, loss is 4.543530921936036 and perplexity is 94.02219974962455
At time: 444.7916920185089 and batch: 1350, loss is 4.502082824707031 and perplexity is 90.20481659558476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.528773193359375 and perplexity of 92.6448340382115
Finished 21 epochs...
Completing Train Step...
At time: 447.31272149086 and batch: 50, loss is 4.5877550601959225 and perplexity is 98.2735641034581
At time: 448.0594730377197 and batch: 100, loss is 4.592988166809082 and perplexity is 98.7891881226853
At time: 448.7811508178711 and batch: 150, loss is 4.548672618865967 and perplexity is 94.5068783729285
At time: 449.49722266197205 and batch: 200, loss is 4.554370365142822 and perplexity is 95.04689155543615
At time: 450.2160596847534 and batch: 250, loss is 4.555006761550903 and perplexity is 95.10739830691881
At time: 450.9324264526367 and batch: 300, loss is 4.558630933761597 and perplexity is 95.45270925209923
At time: 451.6489107608795 and batch: 350, loss is 4.562864990234375 and perplexity is 95.85771822392422
At time: 452.36363458633423 and batch: 400, loss is 4.589568061828613 and perplexity is 98.45189584464931
At time: 453.0805563926697 and batch: 450, loss is 4.546399917602539 and perplexity is 94.29233635834699
At time: 453.7976562976837 and batch: 500, loss is 4.6150342464447025 and perplexity is 100.99128706578729
At time: 454.51767659187317 and batch: 550, loss is 4.598240804672241 and perplexity is 99.30945714888878
At time: 455.2358491420746 and batch: 600, loss is 4.542295923233032 and perplexity is 93.90615412772614
At time: 455.9540250301361 and batch: 650, loss is 4.572988681793213 and perplexity is 96.83308100239478
At time: 456.6751198768616 and batch: 700, loss is 4.598377103805542 and perplexity is 99.32299386432707
At time: 457.38766527175903 and batch: 750, loss is 4.554713296890259 and perplexity is 95.07949174154555
At time: 458.1069676876068 and batch: 800, loss is 4.532201166152954 and perplexity is 92.96296296588699
At time: 458.8789849281311 and batch: 850, loss is 4.498648805618286 and perplexity is 89.89558279477244
At time: 459.5950229167938 and batch: 900, loss is 4.54305549621582 and perplexity is 93.97750980180945
At time: 460.31632256507874 and batch: 950, loss is 4.519539089202881 and perplexity is 91.79327971271063
At time: 461.0348162651062 and batch: 1000, loss is 4.532906503677368 and perplexity is 93.02855636206996
At time: 461.7538471221924 and batch: 1050, loss is 4.480875759124756 and perplexity is 88.31197884121615
At time: 462.479061126709 and batch: 1100, loss is 4.460946769714355 and perplexity is 86.5694316253771
At time: 463.1965181827545 and batch: 1150, loss is 4.486574420928955 and perplexity is 88.81667562420749
At time: 463.9110999107361 and batch: 1200, loss is 4.467740497589111 and perplexity is 87.15956311270119
At time: 464.62880969047546 and batch: 1250, loss is 4.512954607009887 and perplexity is 91.19085400421362
At time: 465.34521532058716 and batch: 1300, loss is 4.522115564346313 and perplexity is 92.03008775023709
At time: 466.0661416053772 and batch: 1350, loss is 4.480964956283569 and perplexity is 88.31985637013929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.51954345703125 and perplexity of 91.79368065087748
Finished 22 epochs...
Completing Train Step...
At time: 468.5831732749939 and batch: 50, loss is 4.566447649002075 and perplexity is 96.20175964207893
At time: 469.29403424263 and batch: 100, loss is 4.571476068496704 and perplexity is 96.68672071769103
At time: 470.0087356567383 and batch: 150, loss is 4.527796335220337 and perplexity is 92.55437736690247
At time: 470.7300362586975 and batch: 200, loss is 4.532826509475708 and perplexity is 93.02111491461248
At time: 471.44400429725647 and batch: 250, loss is 4.533846158981323 and perplexity is 93.11601222122086
At time: 472.1519820690155 and batch: 300, loss is 4.537730731964111 and perplexity is 93.47843163314893
At time: 472.87347197532654 and batch: 350, loss is 4.541895399093628 and perplexity is 93.86854997734702
At time: 473.58615946769714 and batch: 400, loss is 4.568385610580444 and perplexity is 96.38837572502999
At time: 474.29501581192017 and batch: 450, loss is 4.524647274017334 and perplexity is 92.26337639837624
At time: 475.01416778564453 and batch: 500, loss is 4.593474206924438 and perplexity is 98.8372153016987
At time: 475.7399728298187 and batch: 550, loss is 4.5779703426361085 and perplexity is 97.31667411865874
At time: 476.4546568393707 and batch: 600, loss is 4.5220002269744874 and perplexity is 92.0194738538883
At time: 477.2205693721771 and batch: 650, loss is 4.551945791244507 and perplexity is 94.81672248676809
At time: 477.9355146884918 and batch: 700, loss is 4.577505970001221 and perplexity is 97.27149340943417
At time: 478.6425099372864 and batch: 750, loss is 4.533872728347778 and perplexity is 93.11848628753947
At time: 479.3499116897583 and batch: 800, loss is 4.511571178436279 and perplexity is 91.06478519483568
At time: 480.0566666126251 and batch: 850, loss is 4.477940340042114 and perplexity is 88.05312627954518
At time: 480.77311754226685 and batch: 900, loss is 4.522175130844116 and perplexity is 92.03556982352917
At time: 481.4766957759857 and batch: 950, loss is 4.498891553878784 and perplexity is 89.91740743996249
At time: 482.1826112270355 and batch: 1000, loss is 4.512370748519897 and perplexity is 91.13762698992797
At time: 482.8951416015625 and batch: 1050, loss is 4.459624843597412 and perplexity is 86.45506883901362
At time: 483.61279034614563 and batch: 1100, loss is 4.440352220535278 and perplexity is 84.80480640834014
At time: 484.33017444610596 and batch: 1150, loss is 4.467223014831543 and perplexity is 87.11447120978056
At time: 485.0416171550751 and batch: 1200, loss is 4.446760692596436 and perplexity is 85.35002077043593
At time: 485.7537477016449 and batch: 1250, loss is 4.492909049987793 and perplexity is 89.38108208395481
At time: 486.4634246826172 and batch: 1300, loss is 4.50191216468811 and perplexity is 90.18942355340357
At time: 487.1771285533905 and batch: 1350, loss is 4.461084356307984 and perplexity is 86.5813432380071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.510941162109375 and perplexity of 91.00743096230543
Finished 23 epochs...
Completing Train Step...
At time: 489.70171570777893 and batch: 50, loss is 4.5464621257781985 and perplexity is 94.29820229502324
At time: 490.416898727417 and batch: 100, loss is 4.550943202972412 and perplexity is 94.72170799097287
At time: 491.1342661380768 and batch: 150, loss is 4.507928142547607 and perplexity is 90.73363647375679
At time: 491.8537759780884 and batch: 200, loss is 4.512324705123901 and perplexity is 91.13343080068245
At time: 492.58597564697266 and batch: 250, loss is 4.513771858215332 and perplexity is 91.26541030103613
At time: 493.30415892601013 and batch: 300, loss is 4.517933216094971 and perplexity is 91.64598964956184
At time: 494.0213167667389 and batch: 350, loss is 4.521900634765625 and perplexity is 92.01030988756627
At time: 494.7383065223694 and batch: 400, loss is 4.54820873260498 and perplexity is 94.46304809739982
At time: 495.4550573825836 and batch: 450, loss is 4.503824539184571 and perplexity is 90.36206453134433
At time: 496.22809886932373 and batch: 500, loss is 4.572861289978027 and perplexity is 96.82074604613878
At time: 496.94397497177124 and batch: 550, loss is 4.5585678195953365 and perplexity is 95.44668502404654
At time: 497.6604673862457 and batch: 600, loss is 4.502739019393921 and perplexity is 90.26402794192744
At time: 498.3759572505951 and batch: 650, loss is 4.531993322372436 and perplexity is 92.94364320003214
At time: 499.09272384643555 and batch: 700, loss is 4.557751779556274 and perplexity is 95.3688284788208
At time: 499.80959463119507 and batch: 750, loss is 4.514122591018677 and perplexity is 91.29742568833453
At time: 500.5271346569061 and batch: 800, loss is 4.491879930496216 and perplexity is 89.28914558513952
At time: 501.2453382015228 and batch: 850, loss is 4.458032169342041 and perplexity is 86.31748366988913
At time: 501.96657824516296 and batch: 900, loss is 4.5009668254852295 and perplexity is 90.1042042425575
At time: 502.6848406791687 and batch: 950, loss is 4.4791451358795165 and perplexity is 88.15927625123908
At time: 503.40021204948425 and batch: 1000, loss is 4.492680654525757 and perplexity is 89.36067018149669
At time: 504.1214134693146 and batch: 1050, loss is 4.439292879104614 and perplexity is 84.71501673075198
At time: 504.8387415409088 and batch: 1100, loss is 4.421321363449096 and perplexity is 83.20615832764828
At time: 505.5560853481293 and batch: 1150, loss is 4.447161016464233 and perplexity is 85.38419526084303
At time: 506.28465843200684 and batch: 1200, loss is 4.426778926849365 and perplexity is 83.66150261697634
At time: 507.0043194293976 and batch: 1250, loss is 4.472648468017578 and perplexity is 87.58839114786748
At time: 507.730477809906 and batch: 1300, loss is 4.48241813659668 and perplexity is 88.4482943458398
At time: 508.45041036605835 and batch: 1350, loss is 4.442104959487915 and perplexity is 84.95357743620416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.5040869140625 and perplexity of 90.38577637755557
Finished 24 epochs...
Completing Train Step...
At time: 511.01753544807434 and batch: 50, loss is 4.527444047927856 and perplexity is 92.52177737851062
At time: 511.79050159454346 and batch: 100, loss is 4.531434440612793 and perplexity is 92.89171320588784
At time: 512.5072813034058 and batch: 150, loss is 4.489182682037353 and perplexity is 89.04863507892541
At time: 513.2227337360382 and batch: 200, loss is 4.493424015045166 and perplexity is 89.42712207149324
At time: 513.9434008598328 and batch: 250, loss is 4.4947364807128904 and perplexity is 89.54456915474854
At time: 514.7188358306885 and batch: 300, loss is 4.4990935802459715 and perplexity is 89.93557496223195
At time: 515.4353058338165 and batch: 350, loss is 4.5030357837677 and perplexity is 90.29081906478316
At time: 516.1592552661896 and batch: 400, loss is 4.528980112075805 and perplexity is 92.66400597180213
At time: 516.8736877441406 and batch: 450, loss is 4.4839847373962405 and perplexity is 88.58696610776721
At time: 517.5896055698395 and batch: 500, loss is 4.553230829238892 and perplexity is 94.93864389771167
At time: 518.3136446475983 and batch: 550, loss is 4.540145473480225 and perplexity is 93.7044306376478
At time: 519.030003786087 and batch: 600, loss is 4.48443525314331 and perplexity is 88.626884922336
At time: 519.7756493091583 and batch: 650, loss is 4.513123941421509 and perplexity is 91.20629706130528
At time: 520.5169088840485 and batch: 700, loss is 4.539149379730224 and perplexity is 93.61113871140833
At time: 521.2577931880951 and batch: 750, loss is 4.495367317199707 and perplexity is 89.60107495725474
At time: 522.0187857151031 and batch: 800, loss is 4.47329083442688 and perplexity is 87.64467306305625
At time: 522.7477324008942 and batch: 850, loss is 4.439693965911865 and perplexity is 84.74900162132879
At time: 523.4633674621582 and batch: 900, loss is 4.481172780990601 and perplexity is 88.33821332586292
At time: 524.1807398796082 and batch: 950, loss is 4.460392065048218 and perplexity is 86.52142447383612
At time: 524.8970081806183 and batch: 1000, loss is 4.474155406951905 and perplexity is 87.72048100536126
At time: 525.6141157150269 and batch: 1050, loss is 4.42013111114502 and perplexity is 83.10718092171328
At time: 526.3313834667206 and batch: 1100, loss is 4.402379236221313 and perplexity is 81.64489024200928
At time: 527.0505995750427 and batch: 1150, loss is 4.428193845748901 and perplexity is 83.7799606426797
At time: 527.7644951343536 and batch: 1200, loss is 4.407621564865113 and perplexity is 82.07402343457208
At time: 528.4868528842926 and batch: 1250, loss is 4.453703927993774 and perplexity is 85.94468812452335
At time: 529.203940153122 and batch: 1300, loss is 4.463871278762817 and perplexity is 86.82297527612815
At time: 529.9217267036438 and batch: 1350, loss is 4.423825798034668 and perplexity is 83.41480386879583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.497408447265625 and perplexity of 89.78414918087907
Finished 25 epochs...
Completing Train Step...
At time: 532.5023865699768 and batch: 50, loss is 4.509234523773193 and perplexity is 90.85224665116104
At time: 533.2755830287933 and batch: 100, loss is 4.5128278732299805 and perplexity is 91.1792977748905
At time: 533.9953322410583 and batch: 150, loss is 4.471409120559692 and perplexity is 87.47990593727901
At time: 534.7109854221344 and batch: 200, loss is 4.475059642791748 and perplexity is 87.79983688098095
At time: 535.4274361133575 and batch: 250, loss is 4.47639479637146 and perplexity is 87.91714143986569
At time: 536.1459243297577 and batch: 300, loss is 4.481126565933227 and perplexity is 88.33413086460209
At time: 536.8635504245758 and batch: 350, loss is 4.4849589157104495 and perplexity is 88.67330765827568
At time: 537.5793082714081 and batch: 400, loss is 4.510634450912476 and perplexity is 90.97952224440522
At time: 538.2950565814972 and batch: 450, loss is 4.465011310577393 and perplexity is 86.92201267234394
At time: 539.0125322341919 and batch: 500, loss is 4.5344846820831295 and perplexity is 93.17548793245929
At time: 539.7324900627136 and batch: 550, loss is 4.522586622238159 and perplexity is 92.07344946149546
At time: 540.4487860202789 and batch: 600, loss is 4.466907434463501 and perplexity is 87.08698393034538
At time: 541.1658163070679 and batch: 650, loss is 4.495064449310303 and perplexity is 89.57394177788586
At time: 541.8898551464081 and batch: 700, loss is 4.521303243637085 and perplexity is 91.9553601595794
At time: 542.607962846756 and batch: 750, loss is 4.477491407394409 and perplexity is 88.01360522823472
At time: 543.3273651599884 and batch: 800, loss is 4.455547208786011 and perplexity is 86.10325441353896
At time: 544.0448863506317 and batch: 850, loss is 4.42185546875 and perplexity is 83.25061104803892
At time: 544.7589812278748 and batch: 900, loss is 4.462501525878906 and perplexity is 86.704130667808
At time: 545.4744057655334 and batch: 950, loss is 4.442529993057251 and perplexity is 84.98969323311904
At time: 546.1873967647552 and batch: 1000, loss is 4.456199903488159 and perplexity is 86.15947189596308
At time: 546.9058232307434 and batch: 1050, loss is 4.401755094528198 and perplexity is 81.59394816117145
At time: 547.6223020553589 and batch: 1100, loss is 4.384558544158936 and perplexity is 80.20280939797998
At time: 548.3393940925598 and batch: 1150, loss is 4.410411701202393 and perplexity is 82.30334091445333
At time: 549.0543887615204 and batch: 1200, loss is 4.389416379928589 and perplexity is 80.59336934428448
At time: 549.7704319953918 and batch: 1250, loss is 4.435654335021972 and perplexity is 84.4073374995855
At time: 550.4882209300995 and batch: 1300, loss is 4.446165399551392 and perplexity is 85.29922761658253
At time: 551.2053096294403 and batch: 1350, loss is 4.406484146118164 and perplexity is 81.98072397201759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.491448160807292 and perplexity of 89.25060156019556
Finished 26 epochs...
Completing Train Step...
At time: 553.7311110496521 and batch: 50, loss is 4.491800270080566 and perplexity is 89.2820330579864
At time: 554.4573941230774 and batch: 100, loss is 4.495492315292358 and perplexity is 89.61227562074005
At time: 555.17129778862 and batch: 150, loss is 4.454321660995483 and perplexity is 85.99779539606783
At time: 555.8877272605896 and batch: 200, loss is 4.457508878707886 and perplexity is 86.27232635535117
At time: 556.6039471626282 and batch: 250, loss is 4.458898124694824 and perplexity is 86.39226313007576
At time: 557.3215453624725 and batch: 300, loss is 4.464004850387573 and perplexity is 86.83457313655718
At time: 558.037868976593 and batch: 350, loss is 4.467772951126099 and perplexity is 87.16239179470661
At time: 558.7590079307556 and batch: 400, loss is 4.493112745285035 and perplexity is 89.39929044445344
At time: 559.4760146141052 and batch: 450, loss is 4.446876258850097 and perplexity is 85.3598849225569
At time: 560.1907844543457 and batch: 500, loss is 4.516587390899658 and perplexity is 91.52273312711279
At time: 560.9092793464661 and batch: 550, loss is 4.505869884490966 and perplexity is 90.54707529686178
At time: 561.6373727321625 and batch: 600, loss is 4.450005378723144 and perplexity is 85.62740456706469
At time: 562.3534317016602 and batch: 650, loss is 4.477933435440064 and perplexity is 88.05251830984785
At time: 563.0730361938477 and batch: 700, loss is 4.504167327880859 and perplexity is 90.39304493520174
At time: 563.7888791561127 and batch: 750, loss is 4.46024790763855 and perplexity is 86.50895266837637
At time: 564.504293680191 and batch: 800, loss is 4.4385827541351315 and perplexity is 84.65487983696342
At time: 565.2221403121948 and batch: 850, loss is 4.4048585605621335 and perplexity is 81.84756555078181
At time: 565.9387011528015 and batch: 900, loss is 4.445072336196899 and perplexity is 85.20604109537224
At time: 566.6546812057495 and batch: 950, loss is 4.425363130569458 and perplexity is 83.54313878210985
At time: 567.3700070381165 and batch: 1000, loss is 4.439091806411743 and perplexity is 84.69798456662026
At time: 568.0861954689026 and batch: 1050, loss is 4.384336986541748 and perplexity is 80.18504182298146
At time: 568.8020992279053 and batch: 1100, loss is 4.36747636795044 and perplexity is 78.8444061490203
At time: 569.5171377658844 and batch: 1150, loss is 4.393343296051025 and perplexity is 80.91047496180668
At time: 570.2920422554016 and batch: 1200, loss is 4.371947526931763 and perplexity is 79.19772129920291
At time: 571.0085530281067 and batch: 1250, loss is 4.418273496627807 and perplexity is 82.95294311749339
At time: 571.7234952449799 and batch: 1300, loss is 4.429186887741089 and perplexity is 83.86319898444575
At time: 572.4387192726135 and batch: 1350, loss is 4.389943857192993 and perplexity is 80.63589172808472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.485425211588542 and perplexity of 88.71466529783257
Finished 27 epochs...
Completing Train Step...
At time: 574.938601732254 and batch: 50, loss is 4.475578155517578 and perplexity is 87.84537401850196
At time: 575.6559855937958 and batch: 100, loss is 4.4778548526763915 and perplexity is 88.04559917147684
At time: 576.3708429336548 and batch: 150, loss is 4.438309001922607 and perplexity is 84.63170854804758
At time: 577.0850858688354 and batch: 200, loss is 4.440565595626831 and perplexity is 84.82290357234501
At time: 577.8040874004364 and batch: 250, loss is 4.442400159835816 and perplexity is 84.9786594637482
At time: 578.5256628990173 and batch: 300, loss is 4.447526540756225 and perplexity is 85.41541096306426
At time: 579.2413518428802 and batch: 350, loss is 4.451373748779297 and perplexity is 85.74465474598195
At time: 579.9561128616333 and batch: 400, loss is 4.476364374160767 and perplexity is 87.91446684674894
At time: 580.6736922264099 and batch: 450, loss is 4.429428396224975 and perplexity is 83.88345510440027
At time: 581.3965313434601 and batch: 500, loss is 4.499447555541992 and perplexity is 89.96741556906363
At time: 582.1134572029114 and batch: 550, loss is 4.489901847839356 and perplexity is 89.11269884546304
At time: 582.8290839195251 and batch: 600, loss is 4.4336694145202635 and perplexity is 84.239961813557
At time: 583.5495553016663 and batch: 650, loss is 4.461459808349609 and perplexity is 86.61385648329271
At time: 584.2642743587494 and batch: 700, loss is 4.487565956115723 and perplexity is 88.90478415739749
At time: 584.9812693595886 and batch: 750, loss is 4.443528509140014 and perplexity is 85.07459919160945
At time: 585.6985111236572 and batch: 800, loss is 4.422375001907349 and perplexity is 83.29387373807582
At time: 586.4178485870361 and batch: 850, loss is 4.3888707447052 and perplexity is 80.54940675805717
At time: 587.1395792961121 and batch: 900, loss is 4.428319892883301 and perplexity is 83.79052153220975
At time: 587.8560266494751 and batch: 950, loss is 4.410035696029663 and perplexity is 82.27240024982605
At time: 588.6277458667755 and batch: 1000, loss is 4.423100605010986 and perplexity is 83.35433396378517
At time: 589.3432078361511 and batch: 1050, loss is 4.3676307487487795 and perplexity is 78.85657915100091
At time: 590.0571410655975 and batch: 1100, loss is 4.351271543502808 and perplexity is 77.5770428224327
At time: 590.7725882530212 and batch: 1150, loss is 4.377025108337403 and perplexity is 79.6008768375998
At time: 591.4892730712891 and batch: 1200, loss is 4.355308723449707 and perplexity is 77.89086836289582
At time: 592.2135548591614 and batch: 1250, loss is 4.401625337600708 and perplexity is 81.58336146801953
At time: 592.9470970630646 and batch: 1300, loss is 4.412790594100952 and perplexity is 82.49936481517133
At time: 593.6835107803345 and batch: 1350, loss is 4.374338178634644 and perplexity is 79.38728196297131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.480146077473958 and perplexity of 88.24756271520076
Finished 28 epochs...
Completing Train Step...
At time: 596.1424601078033 and batch: 50, loss is 4.459237899780273 and perplexity is 86.4216220560963
At time: 596.8980979919434 and batch: 100, loss is 4.4613889503479 and perplexity is 86.60771941593474
At time: 597.6185438632965 and batch: 150, loss is 4.422453098297119 and perplexity is 83.30037894291794
At time: 598.3344347476959 and batch: 200, loss is 4.424812383651734 and perplexity is 83.49714032384581
At time: 599.0525305271149 and batch: 250, loss is 4.426625566482544 and perplexity is 83.64867324203017
At time: 599.7757184505463 and batch: 300, loss is 4.431691007614136 and perplexity is 84.07346564422555
At time: 600.4992742538452 and batch: 350, loss is 4.435626955032348 and perplexity is 84.40502645919884
At time: 601.2199051380157 and batch: 400, loss is 4.460187444686889 and perplexity is 86.50372223987814
At time: 601.9348928928375 and batch: 450, loss is 4.412688579559326 and perplexity is 82.49094910955478
At time: 602.6516771316528 and batch: 500, loss is 4.482968254089355 and perplexity is 88.49696468573202
At time: 603.3673226833344 and batch: 550, loss is 4.474361753463745 and perplexity is 87.73858368828216
At time: 604.0829241275787 and batch: 600, loss is 4.417755422592163 and perplexity is 82.90997848187597
At time: 604.7968842983246 and batch: 650, loss is 4.445808820724487 and perplexity is 85.26881714025134
At time: 605.5140688419342 and batch: 700, loss is 4.472262802124024 and perplexity is 87.55461780576216
At time: 606.2320070266724 and batch: 750, loss is 4.427535057067871 and perplexity is 83.72478552927674
At time: 606.9791758060455 and batch: 800, loss is 4.406848831176758 and perplexity is 82.01062656932672
At time: 607.6936194896698 and batch: 850, loss is 4.372956275939941 and perplexity is 79.2776522303593
At time: 608.4095265865326 and batch: 900, loss is 4.4121286582946775 and perplexity is 82.44477360153809
At time: 609.1373267173767 and batch: 950, loss is 4.3936048603057865 and perplexity is 80.93164101791362
At time: 609.8541088104248 and batch: 1000, loss is 4.40736382484436 and perplexity is 82.05287239991995
At time: 610.5734102725983 and batch: 1050, loss is 4.351423330307007 and perplexity is 77.58881888754499
At time: 611.2946319580078 and batch: 1100, loss is 4.335535955429077 and perplexity is 76.36587662992181
At time: 612.0119519233704 and batch: 1150, loss is 4.3613694190979 and perplexity is 78.36437464915795
At time: 612.7287743091583 and batch: 1200, loss is 4.339059000015259 and perplexity is 76.63539149583184
At time: 613.4459788799286 and batch: 1250, loss is 4.3856790828704835 and perplexity is 80.29273012110214
At time: 614.1635258197784 and batch: 1300, loss is 4.396990318298339 and perplexity is 81.20609600462754
At time: 614.8817222118378 and batch: 1350, loss is 4.358673181533813 and perplexity is 78.15337026534377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.4747220865885415 and perplexity of 87.77020450297915
Finished 29 epochs...
Completing Train Step...
At time: 617.3669004440308 and batch: 50, loss is 4.4437157154083256 and perplexity is 85.09052718071537
At time: 618.1329348087311 and batch: 100, loss is 4.445652198791504 and perplexity is 85.25546321907343
At time: 618.8479943275452 and batch: 150, loss is 4.407714433670044 and perplexity is 82.08164590498376
At time: 619.5681037902832 and batch: 200, loss is 4.410076074600219 and perplexity is 82.27572235881493
At time: 620.2840673923492 and batch: 250, loss is 4.411428546905517 and perplexity is 82.38707327724686
At time: 621.0080919265747 and batch: 300, loss is 4.416594715118408 and perplexity is 82.81380007849698
At time: 621.7239282131195 and batch: 350, loss is 4.4207916259765625 and perplexity is 83.16209258030584
At time: 622.4366738796234 and batch: 400, loss is 4.444744148254395 and perplexity is 85.17808208821165
At time: 623.1534342765808 and batch: 450, loss is 4.3967424011230465 and perplexity is 81.18596611406541
At time: 623.8677461147308 and batch: 500, loss is 4.467348718643189 and perplexity is 87.12542251915741
At time: 624.5796947479248 and batch: 550, loss is 4.459699840545654 and perplexity is 86.46155294847776
At time: 625.2882161140442 and batch: 600, loss is 4.4027399063110355 and perplexity is 81.6743424228393
At time: 626.0577175617218 and batch: 650, loss is 4.430947036743164 and perplexity is 84.0109406960186
At time: 626.7712378501892 and batch: 700, loss is 4.457028827667236 and perplexity is 86.23092117440028
At time: 627.4864735603333 and batch: 750, loss is 4.412190551757813 and perplexity is 82.44987655201169
At time: 628.1982514858246 and batch: 800, loss is 4.3918578147888185 and perplexity is 80.79037319388284
At time: 628.9134483337402 and batch: 850, loss is 4.357883901596069 and perplexity is 78.09170971504491
At time: 629.6240627765656 and batch: 900, loss is 4.396144857406616 and perplexity is 81.13746844133449
At time: 630.3382380008698 and batch: 950, loss is 4.378562679290772 and perplexity is 79.72336297512057
At time: 631.0514154434204 and batch: 1000, loss is 4.392303638458252 and perplexity is 80.82639948460486
At time: 631.764874458313 and batch: 1050, loss is 4.336031904220581 and perplexity is 76.40375958737555
At time: 632.4763131141663 and batch: 1100, loss is 4.3204278564453125 and perplexity is 75.22080511429157
At time: 633.1909177303314 and batch: 1150, loss is 4.346427879333496 and perplexity is 77.20219423295593
At time: 633.9012825489044 and batch: 1200, loss is 4.323546094894409 and perplexity is 75.45572760278509
At time: 634.6182935237885 and batch: 1250, loss is 4.370311489105225 and perplexity is 79.06825676469691
At time: 635.3295404911041 and batch: 1300, loss is 4.381878461837768 and perplexity is 79.98814705127515
At time: 636.0459263324738 and batch: 1350, loss is 4.343929233551026 and perplexity is 77.00953409089914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.4694596354166665 and perplexity of 87.30953128572591
Finished 30 epochs...
Completing Train Step...
At time: 638.5819034576416 and batch: 50, loss is 4.42886269569397 and perplexity is 83.83601560884416
At time: 639.2976810932159 and batch: 100, loss is 4.430528402328491 and perplexity is 83.97577818566621
At time: 640.0186789035797 and batch: 150, loss is 4.393601589202881 and perplexity is 80.93137628262049
At time: 640.7381129264832 and batch: 200, loss is 4.395206575393677 and perplexity is 81.06137431857645
At time: 641.4595589637756 and batch: 250, loss is 4.396869525909424 and perplexity is 81.19628751870381
At time: 642.1721379756927 and batch: 300, loss is 4.402071447372436 and perplexity is 81.61976472210078
At time: 642.8882896900177 and batch: 350, loss is 4.4064825248718265 and perplexity is 81.98059106117687
At time: 643.6057810783386 and batch: 400, loss is 4.429873695373535 and perplexity is 83.92081665345216
At time: 644.3649942874908 and batch: 450, loss is 4.381393785476685 and perplexity is 79.94938808076964
At time: 645.0785510540009 and batch: 500, loss is 4.452406148910523 and perplexity is 85.8332232499945
At time: 645.794989824295 and batch: 550, loss is 4.445639801025391 and perplexity is 85.25440624833263
At time: 646.5070152282715 and batch: 600, loss is 4.388172616958618 and perplexity is 80.49319260684116
At time: 647.2226076126099 and batch: 650, loss is 4.416460046768188 and perplexity is 82.80264843156878
At time: 647.9354634284973 and batch: 700, loss is 4.442437858581543 and perplexity is 84.98186311300994
At time: 648.6517100334167 and batch: 750, loss is 4.397820167541504 and perplexity is 81.27351279095298
At time: 649.3635375499725 and batch: 800, loss is 4.377533302307129 and perplexity is 79.6413398038405
At time: 650.0781254768372 and batch: 850, loss is 4.3430352306365965 and perplexity is 76.94071810841086
At time: 650.793087720871 and batch: 900, loss is 4.380550765991211 and perplexity is 79.88201759007399
At time: 651.5103738307953 and batch: 950, loss is 4.36403938293457 and perplexity is 78.57388426263914
At time: 652.2252860069275 and batch: 1000, loss is 4.37786714553833 and perplexity is 79.66793196461722
At time: 652.9401831626892 and batch: 1050, loss is 4.321196012496948 and perplexity is 75.27860862916657
At time: 653.6558830738068 and batch: 1100, loss is 4.30583384513855 and perplexity is 74.13100346530696
At time: 654.3704571723938 and batch: 1150, loss is 4.33216160774231 and perplexity is 76.10862588159425
At time: 655.0841836929321 and batch: 1200, loss is 4.308564186096191 and perplexity is 74.33368294643306
At time: 655.7981443405151 and batch: 1250, loss is 4.355631866455078 and perplexity is 77.91604231936446
At time: 656.5130205154419 and batch: 1300, loss is 4.367404260635376 and perplexity is 78.83872109555453
At time: 657.226665019989 and batch: 1350, loss is 4.329648399353028 and perplexity is 75.91758920257647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.464535319010417 and perplexity of 86.88064837263724
Finished 31 epochs...
Completing Train Step...
At time: 659.7497577667236 and batch: 50, loss is 4.414573335647583 and perplexity is 82.64657103673444
At time: 660.4648339748383 and batch: 100, loss is 4.416039791107178 and perplexity is 82.76785746088198
At time: 661.179027557373 and batch: 150, loss is 4.379925556182862 and perplexity is 79.83209017834601
At time: 661.8925137519836 and batch: 200, loss is 4.381056900024414 and perplexity is 79.92245883129817
At time: 662.6355171203613 and batch: 250, loss is 4.382963933944702 and perplexity is 80.07501909384442
At time: 663.3500301837921 and batch: 300, loss is 4.38808500289917 and perplexity is 80.48614058041176
At time: 664.0637624263763 and batch: 350, loss is 4.392706851959229 and perplexity is 80.8589963514192
At time: 664.7767090797424 and batch: 400, loss is 4.415569171905518 and perplexity is 82.72891448225617
At time: 665.4897108078003 and batch: 450, loss is 4.366616420745849 and perplexity is 78.77663326708159
At time: 666.2054738998413 and batch: 500, loss is 4.438027210235596 and perplexity is 84.60786339596179
At time: 666.9162185192108 and batch: 550, loss is 4.431883907318115 and perplexity is 84.08968495516065
At time: 667.6249439716339 and batch: 600, loss is 4.3740982246398925 and perplexity is 79.36823495282633
At time: 668.3426263332367 and batch: 650, loss is 4.402423629760742 and perplexity is 81.64851482811689
At time: 669.0520393848419 and batch: 700, loss is 4.4284505271911625 and perplexity is 83.80146816398275
At time: 669.7733378410339 and batch: 750, loss is 4.3836167049407955 and perplexity is 80.1273068079154
At time: 670.4836843013763 and batch: 800, loss is 4.363625507354737 and perplexity is 78.54137117937987
At time: 671.2078037261963 and batch: 850, loss is 4.329805240631104 and perplexity is 75.92949714809974
At time: 671.9447379112244 and batch: 900, loss is 4.367115488052368 and perplexity is 78.81595792127125
At time: 672.6822810173035 and batch: 950, loss is 4.3508557605743405 and perplexity is 77.54479431703975
At time: 673.4192540645599 and batch: 1000, loss is 4.3642009735107425 and perplexity is 78.58658208776605
At time: 674.1466965675354 and batch: 1050, loss is 4.306881093978882 and perplexity is 74.20867773776557
At time: 674.8651721477509 and batch: 1100, loss is 4.2915716648101805 and perplexity is 73.08123749606878
At time: 675.5861840248108 and batch: 1150, loss is 4.318340835571289 and perplexity is 75.06398142793316
At time: 676.3341453075409 and batch: 1200, loss is 4.2941430473327635 and perplexity is 73.26939912698232
At time: 677.0724260807037 and batch: 1250, loss is 4.341406278610229 and perplexity is 76.81548739484181
At time: 677.7909171581268 and batch: 1300, loss is 4.353496398925781 and perplexity is 77.74983267179557
At time: 678.5097732543945 and batch: 1350, loss is 4.3157960891723635 and perplexity is 74.8732054726692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.46060546875 and perplexity of 86.53989043638457
Finished 32 epochs...
Completing Train Step...
At time: 681.0974080562592 and batch: 50, loss is 4.400721607208252 and perplexity is 81.50966541044141
At time: 681.8902912139893 and batch: 100, loss is 4.402036218643189 and perplexity is 81.61688941215519
At time: 682.6075711250305 and batch: 150, loss is 4.366830034255981 and perplexity is 78.79346281767586
At time: 683.3227024078369 and batch: 200, loss is 4.367443742752076 and perplexity is 78.84183387659046
At time: 684.0415778160095 and batch: 250, loss is 4.369411010742187 and perplexity is 78.99708955735127
At time: 684.7594349384308 and batch: 300, loss is 4.374495306015015 and perplexity is 79.3997568586691
At time: 685.4776875972748 and batch: 350, loss is 4.3793517684936525 and perplexity is 79.78629664693652
At time: 686.1954636573792 and batch: 400, loss is 4.401705980300903 and perplexity is 81.58994083586472
At time: 686.9200165271759 and batch: 450, loss is 4.352338523864746 and perplexity is 77.65986017805058
At time: 687.6433050632477 and batch: 500, loss is 4.424125728607177 and perplexity is 83.43982627098012
At time: 688.3634107112885 and batch: 550, loss is 4.418537397384643 and perplexity is 82.97483735078914
At time: 689.0795590877533 and batch: 600, loss is 4.360538473129273 and perplexity is 78.29928513463926
At time: 689.795309305191 and batch: 650, loss is 4.389297952651978 and perplexity is 80.58382545617962
At time: 690.5132527351379 and batch: 700, loss is 4.415278778076172 and perplexity is 82.70489400385021
At time: 691.2327766418457 and batch: 750, loss is 4.36954984664917 and perplexity is 79.00805795131478
At time: 691.9477112293243 and batch: 800, loss is 4.350125713348389 and perplexity is 77.4882036145177
At time: 692.6663372516632 and batch: 850, loss is 4.31612187385559 and perplexity is 74.89760198998636
At time: 693.3791501522064 and batch: 900, loss is 4.351705303192139 and perplexity is 77.61069991544399
At time: 694.0966022014618 and batch: 950, loss is 4.336432590484619 and perplexity is 76.43437965847431
At time: 694.8116118907928 and batch: 1000, loss is 4.350613918304443 and perplexity is 77.5260429754883
At time: 695.5305986404419 and batch: 1050, loss is 4.292967710494995 and perplexity is 73.18333349107111
At time: 696.2505457401276 and batch: 1100, loss is 4.2778362274169925 and perplexity is 72.08429711362992
At time: 696.9664897918701 and batch: 1150, loss is 4.3049690055847165 and perplexity is 74.06691975640217
At time: 697.6855506896973 and batch: 1200, loss is 4.280187120437622 and perplexity is 72.25395893486794
At time: 698.403742313385 and batch: 1250, loss is 4.327608861923218 and perplexity is 75.76291022821414
At time: 699.1188926696777 and batch: 1300, loss is 4.3400838088989255 and perplexity is 76.7139683821086
At time: 699.8382701873779 and batch: 1350, loss is 4.302221050262451 and perplexity is 73.86366656330209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.455710042317708 and perplexity of 86.11727605211125
Finished 33 epochs...
Completing Train Step...
At time: 702.3979380130768 and batch: 50, loss is 4.3873075580596925 and perplexity is 80.42359136320162
At time: 703.1744635105133 and batch: 100, loss is 4.38839015007019 and perplexity is 80.51070444612267
At time: 703.8913717269897 and batch: 150, loss is 4.35420639038086 and perplexity is 77.80505398963855
At time: 704.6160981655121 and batch: 200, loss is 4.354349794387818 and perplexity is 77.81621234619965
At time: 705.3394839763641 and batch: 250, loss is 4.356429090499878 and perplexity is 77.97818362875888
At time: 706.0598549842834 and batch: 300, loss is 4.361285476684571 and perplexity is 78.35779683051368
At time: 706.7840466499329 and batch: 350, loss is 4.366353530883789 and perplexity is 78.75592641075896
At time: 707.5083293914795 and batch: 400, loss is 4.38836709022522 and perplexity is 80.50884790316556
At time: 708.226630449295 and batch: 450, loss is 4.338549499511719 and perplexity is 76.5963556705065
At time: 708.9436748027802 and batch: 500, loss is 4.410731210708618 and perplexity is 82.32964181573982
At time: 709.6647627353668 and batch: 550, loss is 4.405707988739014 and perplexity is 81.91711871519647
At time: 710.3885412216187 and batch: 600, loss is 4.347494964599609 and perplexity is 77.28461952652042
At time: 711.1097946166992 and batch: 650, loss is 4.37672061920166 and perplexity is 79.57664292507594
At time: 711.8273894786835 and batch: 700, loss is 4.4021157550811765 and perplexity is 81.62338118698158
At time: 712.5514328479767 and batch: 750, loss is 4.356360425949097 and perplexity is 77.97282947563164
At time: 713.2661154270172 and batch: 800, loss is 4.337210302352905 and perplexity is 76.49384670379597
At time: 713.9828786849976 and batch: 850, loss is 4.303077487945557 and perplexity is 73.92695328745863
At time: 714.7042188644409 and batch: 900, loss is 4.3380678939819335 and perplexity is 76.55947532365653
At time: 715.4210875034332 and batch: 950, loss is 4.323505945205689 and perplexity is 75.45269813962608
At time: 716.144122838974 and batch: 1000, loss is 4.337956275939941 and perplexity is 76.55093038181866
At time: 716.8607215881348 and batch: 1050, loss is 4.279921321868897 and perplexity is 72.23475648810278
At time: 717.5781736373901 and batch: 1100, loss is 4.264536180496216 and perplexity is 71.13191994914511
At time: 718.3528473377228 and batch: 1150, loss is 4.291947803497314 and perplexity is 73.10873134722107
At time: 719.0709555149078 and batch: 1200, loss is 4.266676950454712 and perplexity is 71.28436013793257
At time: 719.7911376953125 and batch: 1250, loss is 4.314436407089233 and perplexity is 74.77147089569561
At time: 720.508570432663 and batch: 1300, loss is 4.327221212387085 and perplexity is 75.73354646300075
At time: 721.2262978553772 and batch: 1350, loss is 4.289189224243164 and perplexity is 72.90733303195258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.452322998046875 and perplexity of 85.82608644007166
Finished 34 epochs...
Completing Train Step...
At time: 723.7464063167572 and batch: 50, loss is 4.374449977874756 and perplexity is 79.39615789692141
At time: 724.4613652229309 and batch: 100, loss is 4.375398359298706 and perplexity is 79.47149145503101
At time: 725.1775593757629 and batch: 150, loss is 4.341829776763916 and perplexity is 76.84802550135578
At time: 725.8949520587921 and batch: 200, loss is 4.341668939590454 and perplexity is 76.83566647606997
At time: 726.6140239238739 and batch: 250, loss is 4.343995628356933 and perplexity is 77.0146472937113
At time: 727.3330626487732 and batch: 300, loss is 4.3486474609375 and perplexity is 77.37374111378688
At time: 728.0474405288696 and batch: 350, loss is 4.353752880096436 and perplexity is 77.76977659740881
At time: 728.762998342514 and batch: 400, loss is 4.375422630310059 and perplexity is 79.47342033191013
At time: 729.4797599315643 and batch: 450, loss is 4.325305614471436 and perplexity is 75.58861030327472
At time: 730.1978302001953 and batch: 500, loss is 4.3977776050567625 and perplexity is 81.27005366192003
At time: 730.9239103794098 and batch: 550, loss is 4.393175506591797 and perplexity is 80.89690017585181
At time: 731.6439032554626 and batch: 600, loss is 4.334829406738281 and perplexity is 76.3119394766238
At time: 732.3599383831024 and batch: 650, loss is 4.363837928771972 and perplexity is 78.5580568208885
At time: 733.0783050060272 and batch: 700, loss is 4.389550399780274 and perplexity is 80.60417117950479
At time: 733.7936878204346 and batch: 750, loss is 4.343704586029053 and perplexity is 76.99223603295299
At time: 734.5083327293396 and batch: 800, loss is 4.324631652832031 and perplexity is 75.5376836427899
At time: 735.2233414649963 and batch: 850, loss is 4.290543727874756 and perplexity is 73.0061531903371
At time: 735.9386241436005 and batch: 900, loss is 4.3250612449645995 and perplexity is 75.5701410086106
At time: 736.6984460353851 and batch: 950, loss is 4.3108546161651615 and perplexity is 74.50413417778707
At time: 737.4137120246887 and batch: 1000, loss is 4.325851602554321 and perplexity is 75.62989205234818
At time: 738.1362128257751 and batch: 1050, loss is 4.267028903961181 and perplexity is 71.3094533339997
At time: 738.8527543544769 and batch: 1100, loss is 4.251915044784546 and perplexity is 70.23979598524407
At time: 739.5677490234375 and batch: 1150, loss is 4.279565486907959 and perplexity is 72.20905740893608
At time: 740.2845718860626 and batch: 1200, loss is 4.25375759601593 and perplexity is 70.36933571297237
At time: 741.0032980442047 and batch: 1250, loss is 4.3017736911773685 and perplexity is 73.8306303710788
At time: 741.716206073761 and batch: 1300, loss is 4.31477611541748 and perplexity is 74.79687570194197
At time: 742.4353234767914 and batch: 1350, loss is 4.276659803390503 and perplexity is 71.99954527639616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.450029703776042 and perplexity of 85.62948748354366
Finished 35 epochs...
Completing Train Step...
At time: 744.9582026004791 and batch: 50, loss is 4.361884145736695 and perplexity is 78.40472126317005
At time: 745.6770884990692 and batch: 100, loss is 4.3625768947601316 and perplexity is 78.45905487486328
At time: 746.3922667503357 and batch: 150, loss is 4.329955711364746 and perplexity is 75.94092317486044
At time: 747.1058447360992 and batch: 200, loss is 4.329472522735596 and perplexity is 75.90423824787875
At time: 747.8227574825287 and batch: 250, loss is 4.332004661560059 and perplexity is 76.0966818606345
At time: 748.5504097938538 and batch: 300, loss is 4.33652283668518 and perplexity is 76.44127788209532
At time: 749.2656533718109 and batch: 350, loss is 4.341611032485962 and perplexity is 76.8312172739241
At time: 749.9899933338165 and batch: 400, loss is 4.362887830734253 and perplexity is 78.48345441066955
At time: 750.7451758384705 and batch: 450, loss is 4.3125341033935545 and perplexity is 74.62936805451237
At time: 751.4952104091644 and batch: 500, loss is 4.385337867736816 and perplexity is 80.26533770008145
At time: 752.2385485172272 and batch: 550, loss is 4.38113115310669 and perplexity is 79.92839354054196
At time: 752.9671580791473 and batch: 600, loss is 4.322583408355713 and perplexity is 75.3831223432147
At time: 753.692779302597 and batch: 650, loss is 4.351558275222779 and perplexity is 77.59928981065761
At time: 754.4108016490936 and batch: 700, loss is 4.377504425048828 and perplexity is 79.63904001350551
At time: 755.1292762756348 and batch: 750, loss is 4.33157868385315 and perplexity is 76.06427327374995
At time: 755.8734760284424 and batch: 800, loss is 4.312417879104614 and perplexity is 74.62069481330656
At time: 756.5926628112793 and batch: 850, loss is 4.278231220245361 and perplexity is 72.11277551804031
At time: 757.3152141571045 and batch: 900, loss is 4.311851301193237 and perplexity is 74.5784283506465
At time: 758.0372924804688 and batch: 950, loss is 4.298790102005005 and perplexity is 73.61067838805644
At time: 758.7540063858032 and batch: 1000, loss is 4.313495397567749 and perplexity is 74.70114332429412
At time: 759.472311258316 and batch: 1050, loss is 4.2545991802215575 and perplexity is 70.42858236149385
At time: 760.1909441947937 and batch: 1100, loss is 4.239618782997131 and perplexity is 69.38139742825757
At time: 760.9131355285645 and batch: 1150, loss is 4.267432918548584 and perplexity is 71.33826921399233
At time: 761.6308760643005 and batch: 1200, loss is 4.241204986572265 and perplexity is 69.49153777832146
At time: 762.3463244438171 and batch: 1250, loss is 4.289529275894165 and perplexity is 72.93212950672233
At time: 763.0632925033569 and batch: 1300, loss is 4.302739410400391 and perplexity is 73.90196446888683
At time: 763.7788510322571 and batch: 1350, loss is 4.264466209411621 and perplexity is 71.12694294568216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.450497639973959 and perplexity of 85.66956599670851
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 766.3259606361389 and batch: 50, loss is 4.3600232124328615 and perplexity is 78.25895098264628
At time: 767.0655219554901 and batch: 100, loss is 4.366267976760864 and perplexity is 78.74918880476886
At time: 767.7763252258301 and batch: 150, loss is 4.332303915023804 and perplexity is 76.11945746392993
At time: 768.4862706661224 and batch: 200, loss is 4.333237972259521 and perplexity is 76.19059060999614
At time: 769.2003803253174 and batch: 250, loss is 4.333019828796386 and perplexity is 76.17397194339482
At time: 769.912978887558 and batch: 300, loss is 4.3346264362335205 and perplexity is 76.29645197555509
At time: 770.6293706893921 and batch: 350, loss is 4.335341873168946 and perplexity is 76.3510568061675
At time: 771.341141462326 and batch: 400, loss is 4.3566957950592045 and perplexity is 77.99898353945288
At time: 772.0549056529999 and batch: 450, loss is 4.305744400024414 and perplexity is 74.12437310577207
At time: 772.7723588943481 and batch: 500, loss is 4.377546977996826 and perplexity is 79.64242896153819
At time: 773.4831562042236 and batch: 550, loss is 4.369250917434693 and perplexity is 78.98444366429004
At time: 774.2541480064392 and batch: 600, loss is 4.314431400299072 and perplexity is 74.77109653156798
At time: 774.9641947746277 and batch: 650, loss is 4.340459413528443 and perplexity is 76.74278791581789
At time: 775.6788554191589 and batch: 700, loss is 4.365497379302979 and perplexity is 78.6885282554997
At time: 776.3884909152985 and batch: 750, loss is 4.316092481613159 and perplexity is 74.89540061386302
At time: 777.1083126068115 and batch: 800, loss is 4.292249917984009 and perplexity is 73.1308218908335
At time: 777.8190777301788 and batch: 850, loss is 4.254733324050903 and perplexity is 70.43803055492148
At time: 778.5350456237793 and batch: 900, loss is 4.285666437149048 and perplexity is 72.65094788037825
At time: 779.2482221126556 and batch: 950, loss is 4.270687952041626 and perplexity is 71.57085600315443
At time: 779.9597828388214 and batch: 1000, loss is 4.284813413619995 and perplexity is 72.58900133711406
At time: 780.6699786186218 and batch: 1050, loss is 4.224461631774902 and perplexity is 68.33770279550767
At time: 781.3814477920532 and batch: 1100, loss is 4.205964298248291 and perplexity is 67.0852566815109
At time: 782.0937678813934 and batch: 1150, loss is 4.230304203033447 and perplexity is 68.73813934417493
At time: 782.8065166473389 and batch: 1200, loss is 4.1991530752182005 and perplexity is 66.62987664424756
At time: 783.5198042392731 and batch: 1250, loss is 4.245987453460693 and perplexity is 69.82467473003416
At time: 784.2349143028259 and batch: 1300, loss is 4.257286157608032 and perplexity is 70.61807683933898
At time: 784.9477722644806 and batch: 1350, loss is 4.21667709350586 and perplexity is 67.80779056988882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.4172265625 and perplexity of 82.86614229564128
Finished 37 epochs...
Completing Train Step...
At time: 787.466068983078 and batch: 50, loss is 4.345035715103149 and perplexity is 77.09479087854137
At time: 788.2118091583252 and batch: 100, loss is 4.351831607818603 and perplexity is 77.62050312498877
At time: 788.9287464618683 and batch: 150, loss is 4.317013387680054 and perplexity is 74.96440401061751
At time: 789.6467008590698 and batch: 200, loss is 4.3186632251739505 and perplexity is 75.0881851763887
At time: 790.366504907608 and batch: 250, loss is 4.3179151153564455 and perplexity is 75.03203197487922
At time: 791.0883822441101 and batch: 300, loss is 4.320648193359375 and perplexity is 75.23738086042107
At time: 791.8064033985138 and batch: 350, loss is 4.322336473464966 and perplexity is 75.36450991825785
At time: 792.5225477218628 and batch: 400, loss is 4.344217882156372 and perplexity is 77.03176599396241
At time: 793.2826075553894 and batch: 450, loss is 4.294089365005493 and perplexity is 73.26546596069117
At time: 793.9976887702942 and batch: 500, loss is 4.367031269073486 and perplexity is 78.809320401282
At time: 794.7139012813568 and batch: 550, loss is 4.359738874435425 and perplexity is 78.23670215248617
At time: 795.430321931839 and batch: 600, loss is 4.304409322738647 and perplexity is 74.02547737033142
At time: 796.1459510326385 and batch: 650, loss is 4.3310652923583985 and perplexity is 76.02523254524327
At time: 796.8613984584808 and batch: 700, loss is 4.35710521697998 and perplexity is 78.03092457134531
At time: 797.5812985897064 and batch: 750, loss is 4.3085175800323485 and perplexity is 74.33021862678976
At time: 798.2996709346771 and batch: 800, loss is 4.285317249298096 and perplexity is 72.62558348074613
At time: 799.014858007431 and batch: 850, loss is 4.2482516384124756 and perplexity is 69.98294982224513
At time: 799.7305066585541 and batch: 900, loss is 4.280001287460327 and perplexity is 72.24053301408576
At time: 800.4456841945648 and batch: 950, loss is 4.265923891067505 and perplexity is 71.2306989889216
At time: 801.1600866317749 and batch: 1000, loss is 4.281393938064575 and perplexity is 72.34120892295013
At time: 801.8755536079407 and batch: 1050, loss is 4.221415166854858 and perplexity is 68.12983117878932
At time: 802.5925505161285 and batch: 1100, loss is 4.201844682693482 and perplexity is 66.80945969341956
At time: 803.3062598705292 and batch: 1150, loss is 4.228637075424194 and perplexity is 68.62363956365748
At time: 804.0193462371826 and batch: 1200, loss is 4.199097051620483 and perplexity is 66.62614390340426
At time: 804.7331321239471 and batch: 1250, loss is 4.247438917160034 and perplexity is 69.92609629777924
At time: 805.4485199451447 and batch: 1300, loss is 4.259479169845581 and perplexity is 70.77311308209396
At time: 806.1640739440918 and batch: 1350, loss is 4.219871492385864 and perplexity is 68.02474203040774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.415661214192708 and perplexity of 82.73652939120333
Finished 38 epochs...
Completing Train Step...
At time: 808.6906926631927 and batch: 50, loss is 4.340601253509521 and perplexity is 76.75367388341824
At time: 809.4106259346008 and batch: 100, loss is 4.346563930511475 and perplexity is 77.2126983969596
At time: 810.1302525997162 and batch: 150, loss is 4.3114648532867434 and perplexity is 74.54961324126874
At time: 810.8460531234741 and batch: 200, loss is 4.313054637908936 and perplexity is 74.66822532884467
At time: 811.5918354988098 and batch: 250, loss is 4.312146492004395 and perplexity is 74.60044646701911
At time: 812.3092534542084 and batch: 300, loss is 4.315222892761231 and perplexity is 74.83030071760278
At time: 813.0256719589233 and batch: 350, loss is 4.317157878875732 and perplexity is 74.9752364895664
At time: 813.7420616149902 and batch: 400, loss is 4.339031314849853 and perplexity is 76.6332698617114
At time: 814.457845211029 and batch: 450, loss is 4.289303741455078 and perplexity is 72.9156826545381
At time: 815.1741187572479 and batch: 500, loss is 4.36248197555542 and perplexity is 78.4516079572055
At time: 815.889408826828 and batch: 550, loss is 4.355537710189819 and perplexity is 77.90870638118359
At time: 816.6065077781677 and batch: 600, loss is 4.300114345550537 and perplexity is 73.70822142490812
At time: 817.3243215084076 and batch: 650, loss is 4.32694709777832 and perplexity is 75.71278963654636
At time: 818.0403964519501 and batch: 700, loss is 4.353283967971802 and perplexity is 77.73331795485001
At time: 818.7567553520203 and batch: 750, loss is 4.304826717376709 and perplexity is 74.05638165685886
At time: 819.4868218898773 and batch: 800, loss is 4.281656646728516 and perplexity is 72.3602160818575
At time: 820.2030973434448 and batch: 850, loss is 4.244997215270996 and perplexity is 69.75556589328518
At time: 820.9201562404633 and batch: 900, loss is 4.277177858352661 and perplexity is 72.03685466142132
At time: 821.637793302536 and batch: 950, loss is 4.263250875473022 and perplexity is 71.0405524652464
At time: 822.352917432785 and batch: 1000, loss is 4.279235048294067 and perplexity is 72.1852006899031
At time: 823.0767951011658 and batch: 1050, loss is 4.219561796188355 and perplexity is 68.00367828831315
At time: 823.81005859375 and batch: 1100, loss is 4.200011444091797 and perplexity is 66.68709420978607
At time: 824.5330333709717 and batch: 1150, loss is 4.227505464553833 and perplexity is 68.54602822836088
At time: 825.262017250061 and batch: 1200, loss is 4.198384056091308 and perplexity is 66.5786566917715
At time: 825.9751856327057 and batch: 1250, loss is 4.247386789321899 and perplexity is 69.92245129655386
At time: 826.6962707042694 and batch: 1300, loss is 4.259637498855591 and perplexity is 70.78431940614327
At time: 827.4123244285583 and batch: 1350, loss is 4.220374364852905 and perplexity is 68.05895840276712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.414798583984375 and perplexity of 82.66518913617124
Finished 39 epochs...
Completing Train Step...
At time: 830.0346415042877 and batch: 50, loss is 4.336895475387573 and perplexity is 76.46976816865657
At time: 830.751736164093 and batch: 100, loss is 4.342268276214599 and perplexity is 76.88173070764138
At time: 831.4682378768921 and batch: 150, loss is 4.307307147979737 and perplexity is 74.24030137802472
At time: 832.1890835762024 and batch: 200, loss is 4.308835668563843 and perplexity is 74.35386597765456
At time: 832.9031846523285 and batch: 250, loss is 4.307897644042969 and perplexity is 74.28415292953169
At time: 833.6239159107208 and batch: 300, loss is 4.311184329986572 and perplexity is 74.5287032707399
At time: 834.3414196968079 and batch: 350, loss is 4.313324174880981 and perplexity is 74.68835388878152
At time: 835.0589690208435 and batch: 400, loss is 4.335224723815918 and perplexity is 76.34211285315904
At time: 835.7768452167511 and batch: 450, loss is 4.285790967941284 and perplexity is 72.65999572383038
At time: 836.4981410503387 and batch: 500, loss is 4.35914794921875 and perplexity is 78.19048376946373
At time: 837.2144498825073 and batch: 550, loss is 4.35236909866333 and perplexity is 77.66223464893291
At time: 837.9324662685394 and batch: 600, loss is 4.296921472549439 and perplexity is 73.47325574216575
At time: 838.6518559455872 and batch: 650, loss is 4.323885650634765 and perplexity is 75.4813533786788
At time: 839.3722386360168 and batch: 700, loss is 4.350372180938721 and perplexity is 77.50730429909493
At time: 840.089405298233 and batch: 750, loss is 4.301973524093628 and perplexity is 73.84538563549981
At time: 840.8094651699066 and batch: 800, loss is 4.278736352920532 and perplexity is 72.14921123891357
At time: 841.5256204605103 and batch: 850, loss is 4.242417621612549 and perplexity is 69.57585676578152
At time: 842.2420916557312 and batch: 900, loss is 4.274827184677124 and perplexity is 71.86771839346913
At time: 842.9572715759277 and batch: 950, loss is 4.261019811630249 and perplexity is 70.8822331332107
At time: 843.6740210056305 and batch: 1000, loss is 4.277267894744873 and perplexity is 72.04334089191535
At time: 844.3909878730774 and batch: 1050, loss is 4.217823028564453 and perplexity is 67.88553843284157
At time: 845.1089050769806 and batch: 1100, loss is 4.198222360610962 and perplexity is 66.56789209421373
At time: 845.8267464637756 and batch: 1150, loss is 4.22618935585022 and perplexity is 68.45587354370744
At time: 846.546245098114 and batch: 1200, loss is 4.197264394760132 and perplexity is 66.50415286170423
At time: 847.2657635211945 and batch: 1250, loss is 4.246649332046509 and perplexity is 69.8709054848851
At time: 847.9825577735901 and batch: 1300, loss is 4.259016599655151 and perplexity is 70.74038312023421
At time: 848.6994466781616 and batch: 1350, loss is 4.219925470352173 and perplexity is 68.02841396674218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.414242757161459 and perplexity of 82.61925437379702
Finished 40 epochs...
Completing Train Step...
At time: 851.2236039638519 and batch: 50, loss is 4.333627195358276 and perplexity is 76.22025151975046
At time: 851.9839792251587 and batch: 100, loss is 4.3385883235931395 and perplexity is 76.59932951138354
At time: 852.6983478069305 and batch: 150, loss is 4.3038130950927735 and perplexity is 73.9813544891721
At time: 853.4159023761749 and batch: 200, loss is 4.305275669097901 and perplexity is 74.08963686130096
At time: 854.1342673301697 and batch: 250, loss is 4.304323072433472 and perplexity is 74.01909292565158
At time: 854.8491554260254 and batch: 300, loss is 4.307802619934082 and perplexity is 74.27709447946195
At time: 855.5654573440552 and batch: 350, loss is 4.3100797557830814 and perplexity is 74.44642623659756
At time: 856.2841873168945 and batch: 400, loss is 4.3319884872436525 and perplexity is 76.09545105877834
At time: 856.999263048172 and batch: 450, loss is 4.282792134284973 and perplexity is 72.44242687263288
At time: 857.7169206142426 and batch: 500, loss is 4.35632155418396 and perplexity is 77.9697985930255
At time: 858.4322855472565 and batch: 550, loss is 4.34961422920227 and perplexity is 77.44857976120794
At time: 859.1499297618866 and batch: 600, loss is 4.294155941009522 and perplexity is 73.27034384502137
At time: 859.8656361103058 and batch: 650, loss is 4.321224527359009 and perplexity is 75.28075521891247
At time: 860.5833373069763 and batch: 700, loss is 4.347817516326904 and perplexity is 77.30955183480539
At time: 861.2990260124207 and batch: 750, loss is 4.299453945159912 and perplexity is 73.65956055628367
At time: 862.0147278308868 and batch: 800, loss is 4.276133804321289 and perplexity is 71.96168354108869
At time: 862.7317359447479 and batch: 850, loss is 4.240100402832031 and perplexity is 69.41482093349754
At time: 863.447268486023 and batch: 900, loss is 4.272786560058594 and perplexity is 71.72121289024535
At time: 864.161604642868 and batch: 950, loss is 4.258935298919678 and perplexity is 70.73463210884275
At time: 864.8783373832703 and batch: 1000, loss is 4.275378646850586 and perplexity is 71.90736165153011
At time: 865.5975878238678 and batch: 1050, loss is 4.216239280700684 and perplexity is 67.77810994863877
At time: 866.3141026496887 and batch: 1100, loss is 4.196482715606689 and perplexity is 66.4521882642621
At time: 867.0851259231567 and batch: 1150, loss is 4.224699029922485 and perplexity is 68.35392796539782
At time: 867.8030400276184 and batch: 1200, loss is 4.195943307876587 and perplexity is 66.41635310597974
At time: 868.5202667713165 and batch: 1250, loss is 4.2455507755279545 and perplexity is 69.79419049180036
At time: 869.2445633411407 and batch: 1300, loss is 4.257994222640991 and perplexity is 70.6680967368241
At time: 869.9642863273621 and batch: 1350, loss is 4.218993511199951 and perplexity is 67.96504379752598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.4138134765625 and perplexity of 82.58379514231852
Finished 41 epochs...
Completing Train Step...
At time: 872.4352943897247 and batch: 50, loss is 4.33062328338623 and perplexity is 75.99163613585125
At time: 873.1806063652039 and batch: 100, loss is 4.335263252258301 and perplexity is 76.3450542525189
At time: 873.8958563804626 and batch: 150, loss is 4.300669565200805 and perplexity is 73.74915704090715
At time: 874.6196644306183 and batch: 200, loss is 4.302052869796753 and perplexity is 73.85124518200737
At time: 875.3366916179657 and batch: 250, loss is 4.301133608818054 and perplexity is 73.78338780817971
At time: 876.0593175888062 and batch: 300, loss is 4.304732904434204 and perplexity is 74.0494345356544
At time: 876.7816915512085 and batch: 350, loss is 4.307152681350708 and perplexity is 74.22883461457171
At time: 877.5141458511353 and batch: 400, loss is 4.3290641975402835 and perplexity is 75.87325096184648
At time: 878.2323312759399 and batch: 450, loss is 4.280068101882935 and perplexity is 72.24535988483848
At time: 878.9462532997131 and batch: 500, loss is 4.353762397766113 and perplexity is 77.77051678797578
At time: 879.6618025302887 and batch: 550, loss is 4.347089595794678 and perplexity is 77.25329710165927
At time: 880.3790528774261 and batch: 600, loss is 4.29161750793457 and perplexity is 73.08458784512453
At time: 881.0981576442719 and batch: 650, loss is 4.318781890869141 and perplexity is 75.09709609678306
At time: 881.8130803108215 and batch: 700, loss is 4.345453386306763 and perplexity is 77.12699787813986
At time: 882.5289120674133 and batch: 750, loss is 4.297108116149903 and perplexity is 73.48697033498594
At time: 883.245786190033 and batch: 800, loss is 4.27369836807251 and perplexity is 71.78663869027794
At time: 883.9631173610687 and batch: 850, loss is 4.2379129695892335 and perplexity is 69.26314659586731
At time: 884.6794321537018 and batch: 900, loss is 4.2705662727355955 and perplexity is 71.5621478408763
At time: 885.4520473480225 and batch: 950, loss is 4.256914596557618 and perplexity is 70.59184278660823
At time: 886.1652700901031 and batch: 1000, loss is 4.273488101959228 and perplexity is 71.7715459795734
At time: 886.8815414905548 and batch: 1050, loss is 4.214410314559936 and perplexity is 67.65425937423694
At time: 887.6043212413788 and batch: 1100, loss is 4.194566388130188 and perplexity is 66.32496604866462
At time: 888.3196957111359 and batch: 1150, loss is 4.223189249038696 and perplexity is 68.25080637671368
At time: 889.0397262573242 and batch: 1200, loss is 4.19447527885437 and perplexity is 66.31892350430944
At time: 889.7569961547852 and batch: 1250, loss is 4.244254064559937 and perplexity is 69.70374625217768
At time: 890.473260641098 and batch: 1300, loss is 4.256770124435425 and perplexity is 70.5816449699392
At time: 891.1913120746613 and batch: 1350, loss is 4.2178324031829835 and perplexity is 67.88617483685113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.413384602864583 and perplexity of 82.54838471855027
Finished 42 epochs...
Completing Train Step...
At time: 893.7591891288757 and batch: 50, loss is 4.327829961776733 and perplexity is 75.77966324854442
At time: 894.4749059677124 and batch: 100, loss is 4.332215366363525 and perplexity is 76.11271748636274
At time: 895.1917986869812 and batch: 150, loss is 4.297810621261597 and perplexity is 73.53861344493968
At time: 895.9107098579407 and batch: 200, loss is 4.299122095108032 and perplexity is 73.63512068270258
At time: 896.6273801326752 and batch: 250, loss is 4.298207015991211 and perplexity is 73.56776954201219
At time: 897.3439836502075 and batch: 300, loss is 4.301931276321411 and perplexity is 73.8422658983696
At time: 898.0596907138824 and batch: 350, loss is 4.304444770812989 and perplexity is 74.02810147746591
At time: 898.7766616344452 and batch: 400, loss is 4.32634765625 and perplexity is 75.66741784643949
At time: 899.503547668457 and batch: 450, loss is 4.277518167495727 and perplexity is 72.0613736334789
At time: 900.2295379638672 and batch: 500, loss is 4.351374282836914 and perplexity is 77.58501344559544
At time: 900.951199054718 and batch: 550, loss is 4.344707326889038 and perplexity is 77.06947801429811
At time: 901.677182674408 and batch: 600, loss is 4.289216718673706 and perplexity is 72.90933760511379
At time: 902.4039466381073 and batch: 650, loss is 4.316462345123291 and perplexity is 74.92310681306698
At time: 903.1516506671906 and batch: 700, loss is 4.343215284347534 and perplexity is 76.95457281748692
At time: 903.9580845832825 and batch: 750, loss is 4.294876289367676 and perplexity is 73.32314303153204
At time: 904.7060859203339 and batch: 800, loss is 4.2713814544677735 and perplexity is 71.62050778025807
At time: 905.4606273174286 and batch: 850, loss is 4.235811643600464 and perplexity is 69.11775495697503
At time: 906.186761379242 and batch: 900, loss is 4.268529720306397 and perplexity is 71.41655607780237
At time: 906.9057624340057 and batch: 950, loss is 4.254919128417969 and perplexity is 70.45111946455671
At time: 907.6420924663544 and batch: 1000, loss is 4.271621799468994 and perplexity is 71.63772348005872
At time: 908.3751132488251 and batch: 1050, loss is 4.212741003036499 and perplexity is 67.54141754973668
At time: 909.0928952693939 and batch: 1100, loss is 4.192706608772278 and perplexity is 66.2017308765282
At time: 909.8143854141235 and batch: 1150, loss is 4.221579732894898 and perplexity is 68.141043957911
At time: 910.5327332019806 and batch: 1200, loss is 4.1929361486434935 and perplexity is 66.21692855747797
At time: 911.2534260749817 and batch: 1250, loss is 4.2428281879425045 and perplexity is 69.60442813476709
At time: 911.9696588516235 and batch: 1300, loss is 4.255385847091675 and perplexity is 70.4840079918422
At time: 912.6861333847046 and batch: 1350, loss is 4.216476497650146 and perplexity is 67.79418997227278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.412937418619792 and perplexity of 82.51147863399596
Finished 43 epochs...
Completing Train Step...
At time: 915.2106683254242 and batch: 50, loss is 4.325169324874878 and perplexity is 75.57830906406394
At time: 915.9283437728882 and batch: 100, loss is 4.329342670440674 and perplexity is 75.89438254825441
At time: 916.6375558376312 and batch: 150, loss is 4.295123796463013 and perplexity is 73.34129327574922
At time: 917.3508174419403 and batch: 200, loss is 4.29633731842041 and perplexity is 73.43034856987184
At time: 918.0618648529053 and batch: 250, loss is 4.29546275138855 and perplexity is 73.36615688193314
At time: 918.776421546936 and batch: 300, loss is 4.299268064498901 and perplexity is 73.64586994092743
At time: 919.4881956577301 and batch: 350, loss is 4.301888933181763 and perplexity is 73.83913925118924
At time: 920.2002868652344 and batch: 400, loss is 4.3237708473205565 and perplexity is 75.47268836654554
At time: 920.9103817939758 and batch: 450, loss is 4.275092334747314 and perplexity is 71.88677665058299
At time: 921.6234059333801 and batch: 500, loss is 4.349099035263062 and perplexity is 77.40868899893644
At time: 922.3345885276794 and batch: 550, loss is 4.342426834106445 and perplexity is 76.89392187926151
At time: 923.1264381408691 and batch: 600, loss is 4.286916513442993 and perplexity is 72.74182389719905
At time: 923.8401892185211 and batch: 650, loss is 4.314240970611572 and perplexity is 74.75685925066496
At time: 924.5575687885284 and batch: 700, loss is 4.341063461303711 and perplexity is 76.78915822965882
At time: 925.2712626457214 and batch: 750, loss is 4.292722511291504 and perplexity is 73.1653911957984
At time: 925.9858071804047 and batch: 800, loss is 4.269136838912964 and perplexity is 71.45992756229954
At time: 926.6980986595154 and batch: 850, loss is 4.2337588119506835 and perplexity is 68.97601337761402
At time: 927.4084091186523 and batch: 900, loss is 4.266376638412476 and perplexity is 71.26295580031112
At time: 928.1224935054779 and batch: 950, loss is 4.252969789505005 and perplexity is 70.31392012337542
At time: 928.8430099487305 and batch: 1000, loss is 4.269759769439697 and perplexity is 71.50445600023802
At time: 929.5556507110596 and batch: 1050, loss is 4.210997266769409 and perplexity is 67.42374575452921
At time: 930.2671618461609 and batch: 1100, loss is 4.190834574699402 and perplexity is 66.0779149106569
At time: 930.980792760849 and batch: 1150, loss is 4.2199593353271485 and perplexity is 68.03071778628795
At time: 931.6930623054504 and batch: 1200, loss is 4.191344537734985 and perplexity is 66.11162079837773
At time: 932.4086999893188 and batch: 1250, loss is 4.241321134567261 and perplexity is 69.49960954985451
At time: 933.1202614307404 and batch: 1300, loss is 4.253926954269409 and perplexity is 70.38125434999849
At time: 933.8314683437347 and batch: 1350, loss is 4.215040187835694 and perplexity is 67.69688640761319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.412510172526042 and perplexity of 82.47623345677833
Finished 44 epochs...
Completing Train Step...
At time: 936.3328716754913 and batch: 50, loss is 4.3226296329498295 and perplexity is 75.38660697798555
At time: 937.0801000595093 and batch: 100, loss is 4.326630973815918 and perplexity is 75.68885879223791
At time: 937.8021607398987 and batch: 150, loss is 4.292614612579346 and perplexity is 73.15749717019911
At time: 938.5213534832001 and batch: 200, loss is 4.293714981079102 and perplexity is 73.23804168180088
At time: 939.2370049953461 and batch: 250, loss is 4.292861747741699 and perplexity is 73.17557919439957
At time: 939.9566915035248 and batch: 300, loss is 4.296747379302978 and perplexity is 73.46046565791151
At time: 940.6753618717194 and batch: 350, loss is 4.299447231292724 and perplexity is 73.65906601743714
At time: 941.4451675415039 and batch: 400, loss is 4.321300783157349 and perplexity is 75.28649603188367
At time: 942.161402463913 and batch: 450, loss is 4.272757000923157 and perplexity is 71.71909290453245
At time: 942.8791778087616 and batch: 500, loss is 4.346905374526978 and perplexity is 77.23906671214363
At time: 943.595240354538 and batch: 550, loss is 4.340219163894654 and perplexity is 76.72435270374031
At time: 944.3110854625702 and batch: 600, loss is 4.284683790206909 and perplexity is 72.57959271281042
At time: 945.0272171497345 and batch: 650, loss is 4.3120823669433594 and perplexity is 74.59566286221227
At time: 945.74347615242 and batch: 700, loss is 4.338979635238648 and perplexity is 76.62930958645316
At time: 946.4595999717712 and batch: 750, loss is 4.290631752014161 and perplexity is 73.01257977698617
At time: 947.1763389110565 and batch: 800, loss is 4.26696216583252 and perplexity is 71.30469443333013
At time: 947.9035847187042 and batch: 850, loss is 4.231753921508789 and perplexity is 68.83786256257329
At time: 948.6217544078827 and batch: 900, loss is 4.264698915481567 and perplexity is 71.14349654302892
At time: 949.33846616745 and batch: 950, loss is 4.2510875415802 and perplexity is 70.18169637111748
At time: 950.0578625202179 and batch: 1000, loss is 4.267951917648316 and perplexity is 71.3753033209936
At time: 950.7776358127594 and batch: 1050, loss is 4.209296360015869 and perplexity is 67.30916172602541
At time: 951.4988358020782 and batch: 1100, loss is 4.188953070640564 and perplexity is 65.95370593204395
At time: 952.2168548107147 and batch: 1150, loss is 4.218277139663696 and perplexity is 67.91637300995399
At time: 952.9314568042755 and batch: 1200, loss is 4.189706182479858 and perplexity is 66.00339515725001
At time: 953.6510093212128 and batch: 1250, loss is 4.239742593765259 and perplexity is 69.3899881241663
At time: 954.3811552524567 and batch: 1300, loss is 4.252383127212524 and perplexity is 70.27268169550064
At time: 955.0977923870087 and batch: 1350, loss is 4.213505334854126 and perplexity is 67.5930613381587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.412075602213542 and perplexity of 82.4403995209775
Finished 45 epochs...
Completing Train Step...
At time: 957.6180768013 and batch: 50, loss is 4.32016450881958 and perplexity is 75.2009985019857
At time: 958.3822157382965 and batch: 100, loss is 4.324002475738525 and perplexity is 75.49017201072797
At time: 959.0976345539093 and batch: 150, loss is 4.2901974868774415 and perplexity is 72.98087984263346
At time: 959.8548085689545 and batch: 200, loss is 4.291182355880737 and perplexity is 73.05279185516709
At time: 960.571417093277 and batch: 250, loss is 4.29035210609436 and perplexity is 72.99216496155042
At time: 961.2901301383972 and batch: 300, loss is 4.294282894134522 and perplexity is 73.2796463346199
At time: 962.0055482387543 and batch: 350, loss is 4.297085018157959 and perplexity is 73.4852729531403
At time: 962.7238042354584 and batch: 400, loss is 4.318909292221069 and perplexity is 75.10666417783199
At time: 963.4401443004608 and batch: 450, loss is 4.2704806852340695 and perplexity is 71.55602327753554
At time: 964.1573331356049 and batch: 500, loss is 4.3447653102874755 and perplexity is 77.07394689410835
At time: 964.8766987323761 and batch: 550, loss is 4.3380614852905275 and perplexity is 76.55898467917717
At time: 965.5943729877472 and batch: 600, loss is 4.28250262260437 and perplexity is 72.42145697954297
At time: 966.3112077713013 and batch: 650, loss is 4.309967403411865 and perplexity is 74.43806247393438
At time: 967.0297384262085 and batch: 700, loss is 4.33693678855896 and perplexity is 76.47292744255422
At time: 967.7480275630951 and batch: 750, loss is 4.288575887680054 and perplexity is 72.86263000928572
At time: 968.4662535190582 and batch: 800, loss is 4.264828958511353 and perplexity is 71.15274886045556
At time: 969.1820728778839 and batch: 850, loss is 4.229788541793823 and perplexity is 68.70270288742012
At time: 969.9018075466156 and batch: 900, loss is 4.262703523635865 and perplexity is 71.00167892806338
At time: 970.6203963756561 and batch: 950, loss is 4.249118871688843 and perplexity is 70.0436676895814
At time: 971.3364806175232 and batch: 1000, loss is 4.266073112487793 and perplexity is 71.2413289280804
At time: 972.0539603233337 and batch: 1050, loss is 4.207552671432495 and perplexity is 67.19189777478505
At time: 972.7709407806396 and batch: 1100, loss is 4.18707685470581 and perplexity is 65.8300785501161
At time: 973.4867668151855 and batch: 1150, loss is 4.216609840393066 and perplexity is 67.8032304382446
At time: 974.2042293548584 and batch: 1200, loss is 4.188053574562073 and perplexity is 65.89440750554529
At time: 974.9183650016785 and batch: 1250, loss is 4.238145923614502 and perplexity is 69.2792836041871
At time: 975.6370351314545 and batch: 1300, loss is 4.25081690788269 and perplexity is 70.1627054090441
At time: 976.3538544178009 and batch: 1350, loss is 4.21193377494812 and perplexity is 67.48691822001584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.41166015625 and perplexity of 82.40615710319335
Finished 46 epochs...
Completing Train Step...
At time: 978.8710792064667 and batch: 50, loss is 4.317791957855224 and perplexity is 75.02279178632058
At time: 979.5862038135529 and batch: 100, loss is 4.321506690979004 and perplexity is 75.30199970639082
At time: 980.3016819953918 and batch: 150, loss is 4.287874412536621 and perplexity is 72.81153660791975
At time: 981.0238754749298 and batch: 200, loss is 4.288754768371582 and perplexity is 72.87566489273811
At time: 981.7437772750854 and batch: 250, loss is 4.287952914237976 and perplexity is 72.81725266177874
At time: 982.4957187175751 and batch: 300, loss is 4.291948509216309 and perplexity is 73.10878294145964
At time: 983.2369978427887 and batch: 350, loss is 4.294811143875122 and perplexity is 73.31836651484959
At time: 983.9789946079254 and batch: 400, loss is 4.3165902900695805 and perplexity is 74.93269345921239
At time: 984.7184994220734 and batch: 450, loss is 4.268276596069336 and perplexity is 71.39848110423515
At time: 985.435685634613 and batch: 500, loss is 4.342684211730957 and perplexity is 76.91371520128408
At time: 986.1662681102753 and batch: 550, loss is 4.335964727401733 and perplexity is 76.39862719824916
At time: 986.8883893489838 and batch: 600, loss is 4.280379514694214 and perplexity is 72.26786151892881
At time: 987.6046912670135 and batch: 650, loss is 4.307911109924317 and perplexity is 74.28515323785605
At time: 988.3143856525421 and batch: 700, loss is 4.334949150085449 and perplexity is 76.32107787080562
At time: 989.0358285903931 and batch: 750, loss is 4.286572151184082 and perplexity is 72.71677867096734
At time: 989.750322341919 and batch: 800, loss is 4.262740049362183 and perplexity is 71.0042723633193
At time: 990.4709136486053 and batch: 850, loss is 4.227838954925537 and perplexity is 68.56889148091902
At time: 991.1903412342072 and batch: 900, loss is 4.260706920623779 and perplexity is 70.86005818929594
At time: 991.9058825969696 and batch: 950, loss is 4.247216653823853 and perplexity is 69.91055601741121
At time: 992.6199960708618 and batch: 1000, loss is 4.264251937866211 and perplexity is 71.1117040983809
At time: 993.3489758968353 and batch: 1050, loss is 4.2058480358123775 and perplexity is 67.07745763953183
At time: 994.063551902771 and batch: 1100, loss is 4.185210356712341 and perplexity is 65.70732143918158
At time: 994.7816655635834 and batch: 1150, loss is 4.21491455078125 and perplexity is 67.68838170447405
At time: 995.5071034431458 and batch: 1200, loss is 4.18639066696167 and perplexity is 65.78492225164034
At time: 996.2188858985901 and batch: 1250, loss is 4.236527528762817 and perplexity is 69.16725304756429
At time: 996.9346172809601 and batch: 1300, loss is 4.249213352203369 and perplexity is 70.0502857639786
At time: 997.6590750217438 and batch: 1350, loss is 4.210326738357544 and perplexity is 67.37855137111882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.41125 and perplexity of 82.37236463338908
Finished 47 epochs...
Completing Train Step...
At time: 1000.1773178577423 and batch: 50, loss is 4.315466279983521 and perplexity is 74.84851567319137
At time: 1000.8953258991241 and batch: 100, loss is 4.319074382781983 and perplexity is 75.11906460271783
At time: 1001.6091549396515 and batch: 150, loss is 4.285624551773071 and perplexity is 72.6479049318392
At time: 1002.3249745368958 and batch: 200, loss is 4.286395654678345 and perplexity is 72.70394554615766
At time: 1003.0447995662689 and batch: 250, loss is 4.285620956420899 and perplexity is 72.64764373750592
At time: 1003.7603843212128 and batch: 300, loss is 4.289667434692383 and perplexity is 72.9422064181774
At time: 1004.4767913818359 and batch: 350, loss is 4.292603998184204 and perplexity is 73.15672065173771
At time: 1005.192556142807 and batch: 400, loss is 4.31432538986206 and perplexity is 74.76317043508071
At time: 1005.9105014801025 and batch: 450, loss is 4.266118302345276 and perplexity is 71.24454838632442
At time: 1006.6297132968903 and batch: 500, loss is 4.340645036697388 and perplexity is 76.75703447750955
At time: 1007.3475058078766 and batch: 550, loss is 4.333908948898316 and perplexity is 76.24172987109753
At time: 1008.0687704086304 and batch: 600, loss is 4.278294343948364 and perplexity is 72.11732768713816
At time: 1008.7871797084808 and batch: 650, loss is 4.305894193649292 and perplexity is 74.13547729595902
At time: 1009.5049874782562 and batch: 700, loss is 4.332994832992553 and perplexity is 76.17206793753112
At time: 1010.2233319282532 and batch: 750, loss is 4.284601736068725 and perplexity is 72.57363750120886
At time: 1010.9442529678345 and batch: 800, loss is 4.260688362121582 and perplexity is 70.85874314495301
At time: 1011.664568901062 and batch: 850, loss is 4.225913562774658 and perplexity is 68.4369964910024
At time: 1012.3804836273193 and batch: 900, loss is 4.25897439956665 and perplexity is 70.73739793279398
At time: 1013.0987160205841 and batch: 950, loss is 4.245335597991943 and perplexity is 69.77917396552992
At time: 1013.8168363571167 and batch: 1000, loss is 4.262455129623413 and perplexity is 70.9840447263494
At time: 1014.5338032245636 and batch: 1050, loss is 4.204160118103028 and perplexity is 66.96433191120258
At time: 1015.3056161403656 and batch: 1100, loss is 4.183327159881592 and perplexity is 65.58369805980355
At time: 1016.0242381095886 and batch: 1150, loss is 4.213186583518982 and perplexity is 67.57151939305494
At time: 1016.7393243312836 and batch: 1200, loss is 4.1847193241119385 and perplexity is 65.67506492241858
At time: 1017.4585614204407 and batch: 1250, loss is 4.234868783950805 and perplexity is 69.05261732739795
At time: 1018.1789772510529 and batch: 1300, loss is 4.247589092254639 and perplexity is 69.9365982444518
At time: 1018.8973331451416 and batch: 1350, loss is 4.208720512390137 and perplexity is 67.27041306278909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.410786946614583 and perplexity of 82.33423066079473
Finished 48 epochs...
Completing Train Step...
At time: 1021.4030747413635 and batch: 50, loss is 4.313197803497315 and perplexity is 74.67891601450798
At time: 1022.1502811908722 and batch: 100, loss is 4.316719036102295 and perplexity is 74.94234136726752
At time: 1022.866307258606 and batch: 150, loss is 4.283445014953613 and perplexity is 72.48973857549491
At time: 1023.5843319892883 and batch: 200, loss is 4.284098386764526 and perplexity is 72.53711680337443
At time: 1024.3017308712006 and batch: 250, loss is 4.283362083435058 and perplexity is 72.48372714066737
At time: 1025.0215468406677 and batch: 300, loss is 4.28742075920105 and perplexity is 72.77851290269284
At time: 1025.737414598465 and batch: 350, loss is 4.290429372787475 and perplexity is 72.99780504265269
At time: 1026.4551031589508 and batch: 400, loss is 4.312097539901734 and perplexity is 74.59679470768646
At time: 1027.1757111549377 and batch: 450, loss is 4.263985347747803 and perplexity is 71.09274894750531
At time: 1027.900184392929 and batch: 500, loss is 4.338628969192505 and perplexity is 76.6024430003169
At time: 1028.6156935691833 and batch: 550, loss is 4.331878147125244 and perplexity is 76.08705514090998
At time: 1029.3331871032715 and batch: 600, loss is 4.276227741241455 and perplexity is 71.9684437175206
At time: 1030.0529544353485 and batch: 650, loss is 4.303891277313232 and perplexity is 73.98713874184851
At time: 1030.7731556892395 and batch: 700, loss is 4.331060867309571 and perplexity is 76.02489613062143
At time: 1031.4901020526886 and batch: 750, loss is 4.282645664215088 and perplexity is 72.43181700233937
At time: 1032.2070536613464 and batch: 800, loss is 4.258650388717651 and perplexity is 70.71448196114599
At time: 1032.9222521781921 and batch: 850, loss is 4.2240039730072025 and perplexity is 68.30643460228599
At time: 1033.694884777069 and batch: 900, loss is 4.25693564414978 and perplexity is 70.59332859056144
At time: 1034.410606622696 and batch: 950, loss is 4.243445682525635 and perplexity is 69.64742176490292
At time: 1035.126830816269 and batch: 1000, loss is 4.260625925064087 and perplexity is 70.85431907164775
At time: 1035.843628168106 and batch: 1050, loss is 4.202405300140381 and perplexity is 66.84692474294054
At time: 1036.5619368553162 and batch: 1100, loss is 4.181474308967591 and perplexity is 65.46229375168383
At time: 1037.2781071662903 and batch: 1150, loss is 4.211513357162476 and perplexity is 67.45855148265609
At time: 1037.99356007576 and batch: 1200, loss is 4.183031034469605 and perplexity is 65.5642799354379
At time: 1038.7121856212616 and batch: 1250, loss is 4.233218984603882 and perplexity is 68.93878828778416
At time: 1039.4284555912018 and batch: 1300, loss is 4.2459679794311525 and perplexity is 69.82331497549578
At time: 1040.1440818309784 and batch: 1350, loss is 4.207160139083863 and perplexity is 67.16552795718425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.410443929036458 and perplexity of 82.30599341560931
Finished 49 epochs...
Completing Train Step...
At time: 1042.6312990188599 and batch: 50, loss is 4.31098650932312 and perplexity is 74.51396141138295
At time: 1043.3821694850922 and batch: 100, loss is 4.314387826919556 and perplexity is 74.76783857318262
At time: 1044.0981018543243 and batch: 150, loss is 4.2813168811798095 and perplexity is 72.33563474951734
At time: 1044.81596326828 and batch: 200, loss is 4.281850595474243 and perplexity is 72.37425161605022
At time: 1045.5309677124023 and batch: 250, loss is 4.281149225234985 and perplexity is 72.32350826689581
At time: 1046.2484385967255 and batch: 300, loss is 4.2852613067626955 and perplexity is 72.62152073511248
At time: 1046.9718153476715 and batch: 350, loss is 4.288318939208985 and perplexity is 72.84391047298789
At time: 1047.6901700496674 and batch: 400, loss is 4.3099361038208 and perplexity is 74.43573262948108
At time: 1048.4077439308167 and batch: 450, loss is 4.261915321350098 and perplexity is 70.94573729199792
At time: 1049.1229479312897 and batch: 500, loss is 4.336664171218872 and perplexity is 76.45208243797002
At time: 1049.8417201042175 and batch: 550, loss is 4.329892864227295 and perplexity is 75.93615065519471
At time: 1050.5596837997437 and batch: 600, loss is 4.274216861724853 and perplexity is 71.82386925783433
At time: 1051.2824347019196 and batch: 650, loss is 4.30193377494812 and perplexity is 73.84245040285793
At time: 1052.0009562969208 and batch: 700, loss is 4.329168729782104 and perplexity is 75.88118257741131
At time: 1052.7708694934845 and batch: 750, loss is 4.280731534957885 and perplexity is 72.29330574877515
At time: 1053.49289560318 and batch: 800, loss is 4.2566624450683594 and perplexity is 70.57404519225936
At time: 1054.2125577926636 and batch: 850, loss is 4.222125701904297 and perplexity is 68.17825701384335
At time: 1054.9269964694977 and batch: 900, loss is 4.254985132217407 and perplexity is 70.45576965957964
At time: 1055.6453387737274 and batch: 950, loss is 4.241585674285889 and perplexity is 69.51799738905929
At time: 1056.3613066673279 and batch: 1000, loss is 4.258828086853027 and perplexity is 70.72704890926312
At time: 1057.0787909030914 and batch: 1050, loss is 4.200694808959961 and perplexity is 66.73268140169759
At time: 1057.7964594364166 and batch: 1100, loss is 4.1796188640594485 and perplexity is 65.34094468513456
At time: 1058.5175535678864 and batch: 1150, loss is 4.209807214736938 and perplexity is 67.3435557134818
At time: 1059.2365539073944 and batch: 1200, loss is 4.181349840164184 and perplexity is 65.45414624537803
At time: 1059.957036972046 and batch: 1250, loss is 4.231558299064636 and perplexity is 68.82439764871117
At time: 1060.6745340824127 and batch: 1300, loss is 4.24431432723999 and perplexity is 69.70794691330686
At time: 1061.3909418582916 and batch: 1350, loss is 4.205482921600342 and perplexity is 67.05297117689018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.410043131510417 and perplexity of 82.27301198694894
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc67f09898>
SETTINGS FOR THIS RUN
{'seq_len': 20, 'lr': 11.24667852206237, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.11779092712664929, 'anneal': 4.93421875558552, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1375 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.3180077075958252 and batch: 50, loss is 7.389096336364746 and perplexity is 1618.2431045529327
At time: 2.030153751373291 and batch: 100, loss is 6.501271686553955 and perplexity is 665.9880227727343
At time: 2.744476795196533 and batch: 150, loss is 6.100459451675415 and perplexity is 446.0626672485469
At time: 3.459207057952881 and batch: 200, loss is 5.8701873397827145 and perplexity is 354.31535141153927
At time: 4.17265510559082 and batch: 250, loss is 5.793099575042724 and perplexity is 328.0281982760529
At time: 4.885010004043579 and batch: 300, loss is 5.737729797363281 and perplexity is 310.35903271414395
At time: 5.595600366592407 and batch: 350, loss is 5.7027113819122315 and perplexity is 299.6788442013355
At time: 6.344444274902344 and batch: 400, loss is 5.699547243118286 and perplexity is 298.7321173221641
At time: 7.058915138244629 and batch: 450, loss is 5.635624752044678 and perplexity is 280.2339393586189
At time: 7.77214241027832 and batch: 500, loss is 5.638842248916626 and perplexity is 281.13704326982463
At time: 8.484786987304688 and batch: 550, loss is 5.60091947555542 and perplexity is 270.6751722463805
At time: 9.196302890777588 and batch: 600, loss is 5.5311833763122555 and perplexity is 252.44246877334075
At time: 9.908564805984497 and batch: 650, loss is 5.562198219299316 and perplexity is 260.39461211020114
At time: 10.619662046432495 and batch: 700, loss is 5.563126668930054 and perplexity is 260.6364876589179
At time: 11.332134246826172 and batch: 750, loss is 5.53576078414917 and perplexity is 253.60064961906684
At time: 12.050424098968506 and batch: 800, loss is 5.489883155822754 and perplexity is 242.2289021675881
At time: 12.764614343643188 and batch: 850, loss is 5.4808577537536625 and perplexity is 240.05252504563015
At time: 13.483314990997314 and batch: 900, loss is 5.524317016601563 and perplexity is 250.71504531877596
At time: 14.197714567184448 and batch: 950, loss is 5.481644763946533 and perplexity is 240.2415231916438
At time: 14.908545732498169 and batch: 1000, loss is 5.469680194854736 and perplexity is 237.38426391159194
At time: 15.6239013671875 and batch: 1050, loss is 5.43573037147522 and perplexity is 229.460378403767
At time: 16.336731433868408 and batch: 1100, loss is 5.417692251205445 and perplexity is 225.3584512496699
At time: 17.05250859260559 and batch: 1150, loss is 5.404268884658814 and perplexity is 222.3535949176245
At time: 17.76661443710327 and batch: 1200, loss is 5.395888261795044 and perplexity is 220.4979200130559
At time: 18.480926513671875 and batch: 1250, loss is 5.422860956192016 and perplexity is 226.5262780766591
At time: 19.193479537963867 and batch: 1300, loss is 5.400395927429199 and perplexity is 221.4940944333255
At time: 19.90802836418152 and batch: 1350, loss is 5.372090787887573 and perplexity is 215.31257033821186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.053249104817708 and perplexity of 156.5302222598561
Finished 1 epochs...
Completing Train Step...
At time: 22.41192078590393 and batch: 50, loss is 5.318161697387695 and perplexity is 204.00850770026904
At time: 23.12813115119934 and batch: 100, loss is 5.328415699005127 and perplexity is 206.11117321235932
At time: 23.84963846206665 and batch: 150, loss is 5.252153968811035 and perplexity is 190.97718464878068
At time: 24.56538414955139 and batch: 200, loss is 5.24475212097168 and perplexity is 189.56881927038344
At time: 25.27795672416687 and batch: 250, loss is 5.232369337081909 and perplexity is 187.23590335532953
At time: 25.98862075805664 and batch: 300, loss is 5.208602752685547 and perplexity is 182.83840915667184
At time: 26.704156398773193 and batch: 350, loss is 5.214725179672241 and perplexity is 183.96125773891487
At time: 27.420982599258423 and batch: 400, loss is 5.221972131729126 and perplexity is 185.29925851201017
At time: 28.133455753326416 and batch: 450, loss is 5.164303007125855 and perplexity is 174.91550121710543
At time: 28.84657382965088 and batch: 500, loss is 5.222613258361816 and perplexity is 185.41809689281473
At time: 29.59094500541687 and batch: 550, loss is 5.198996105194092 and perplexity is 181.09035492603468
At time: 30.306527853012085 and batch: 600, loss is 5.154618005752564 and perplexity is 173.2296213996165
At time: 31.01999068260193 and batch: 650, loss is 5.176329078674317 and perplexity is 177.0317471065611
At time: 31.738718509674072 and batch: 700, loss is 5.187133502960205 and perplexity is 178.95484348118103
At time: 32.45441746711731 and batch: 750, loss is 5.140426607131958 and perplexity is 170.78861242242814
At time: 33.170953035354614 and batch: 800, loss is 5.087184324264526 and perplexity is 161.93326797131485
At time: 33.88645625114441 and batch: 850, loss is 5.086835117340088 and perplexity is 161.87672962520787
At time: 34.602309465408325 and batch: 900, loss is 5.125163593292236 and perplexity is 168.20165606387255
At time: 35.319130420684814 and batch: 950, loss is 5.075257253646851 and perplexity is 160.0133507030568
At time: 36.03564167022705 and batch: 1000, loss is 5.080916538238525 and perplexity is 160.92147904777147
At time: 36.75183057785034 and batch: 1050, loss is 5.047628564834595 and perplexity is 155.65290569306862
At time: 37.469502687454224 and batch: 1100, loss is 5.033049240112304 and perplexity is 153.40005387089084
At time: 38.186561584472656 and batch: 1150, loss is 5.04194821357727 and perplexity is 154.7712489451725
At time: 38.898770570755005 and batch: 1200, loss is 5.027355194091797 and perplexity is 152.529068971917
At time: 39.6145133972168 and batch: 1250, loss is 5.072085247039795 and perplexity is 159.50659144417008
At time: 40.330405473709106 and batch: 1300, loss is 5.059851360321045 and perplexity is 157.56709386041112
At time: 41.049322843551636 and batch: 1350, loss is 5.011539793014526 and perplexity is 150.1357362151164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.945615641276041 and perplexity of 140.55735713099685
Finished 2 epochs...
Completing Train Step...
At time: 43.581902503967285 and batch: 50, loss is 5.071536560058593 and perplexity is 159.4190962599581
At time: 44.295220136642456 and batch: 100, loss is 5.093047456741333 and perplexity is 162.88549295662006
At time: 45.01766037940979 and batch: 150, loss is 5.03355978012085 and perplexity is 153.47839073105408
At time: 45.7401020526886 and batch: 200, loss is 5.023245992660523 and perplexity is 151.9035823088718
At time: 46.45960283279419 and batch: 250, loss is 5.026909980773926 and perplexity is 152.46117611357207
At time: 47.178019762039185 and batch: 300, loss is 5.01044529914856 and perplexity is 149.9715034650302
At time: 47.94641709327698 and batch: 350, loss is 5.0205545997619625 and perplexity is 151.49529975701296
At time: 48.66244173049927 and batch: 400, loss is 5.023460330963135 and perplexity is 151.93614455440817
At time: 49.38078451156616 and batch: 450, loss is 4.979208745956421 and perplexity is 145.35931998839652
At time: 50.10073781013489 and batch: 500, loss is 5.041687278747559 and perplexity is 154.7308690041804
At time: 50.817054271698 and batch: 550, loss is 5.008549432754517 and perplexity is 149.68744688329267
At time: 51.53366041183472 and batch: 600, loss is 4.972234048843384 and perplexity is 144.34901015711017
At time: 52.26333570480347 and batch: 650, loss is 5.002727289199829 and perplexity is 148.8184771670662
At time: 52.98537588119507 and batch: 700, loss is 5.0255879878997805 and perplexity is 152.25975669202057
At time: 53.703299045562744 and batch: 750, loss is 4.977536697387695 and perplexity is 145.1164752261552
At time: 54.43173098564148 and batch: 800, loss is 4.933111486434936 and perplexity is 138.81074886085952
At time: 55.171627044677734 and batch: 850, loss is 4.934756660461426 and perplexity is 139.03930465496035
At time: 55.92280101776123 and batch: 900, loss is 4.987049713134765 and perplexity is 146.50355774847395
At time: 56.67498755455017 and batch: 950, loss is 4.941890125274658 and perplexity is 140.0346826684669
At time: 57.406946420669556 and batch: 1000, loss is 4.936957178115844 and perplexity is 139.34559998004954
At time: 58.135756492614746 and batch: 1050, loss is 4.892188348770142 and perplexity is 133.24484142260394
At time: 58.849382638931274 and batch: 1100, loss is 4.882877912521362 and perplexity is 132.01003105254355
At time: 59.58670687675476 and batch: 1150, loss is 4.8936559772491455 and perplexity is 133.44053891703754
At time: 60.32078409194946 and batch: 1200, loss is 4.8856940269470215 and perplexity is 132.38231034984204
At time: 61.04571294784546 and batch: 1250, loss is 4.930382146835327 and perplexity is 138.43240373817198
At time: 61.76137852668762 and batch: 1300, loss is 4.916904582977295 and perplexity is 136.5791886413195
At time: 62.480228662490845 and batch: 1350, loss is 4.878780155181885 and perplexity is 131.4701927973956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.912952067057292 and perplexity of 136.04042266582175
Finished 3 epochs...
Completing Train Step...
At time: 65.02984476089478 and batch: 50, loss is 4.9513938617706295 and perplexity is 141.37187951133188
At time: 65.75077795982361 and batch: 100, loss is 4.973020210266113 and perplexity is 144.46253639943387
At time: 66.52348852157593 and batch: 150, loss is 4.918176651000977 and perplexity is 136.75303721005028
At time: 67.23855519294739 and batch: 200, loss is 4.914050903320312 and perplexity is 136.1899909759755
At time: 67.95513129234314 and batch: 250, loss is 4.91512622833252 and perplexity is 136.33651824778505
At time: 68.6685733795166 and batch: 300, loss is 4.899777431488037 and perplexity is 134.2598943343915
At time: 69.38471055030823 and batch: 350, loss is 4.91114429473877 and perplexity is 135.79471471445814
At time: 70.10282158851624 and batch: 400, loss is 4.9153291130065915 and perplexity is 136.36418164399848
At time: 70.81823945045471 and batch: 450, loss is 4.874839220046997 and perplexity is 130.95309688538364
At time: 71.5368926525116 and batch: 500, loss is 4.937299365997315 and perplexity is 139.39329051479507
At time: 72.26336002349854 and batch: 550, loss is 4.912380628585815 and perplexity is 135.96270614185516
At time: 72.98190402984619 and batch: 600, loss is 4.877856941223144 and perplexity is 131.34887369061008
At time: 73.6979467868805 and batch: 650, loss is 4.906789360046386 and perplexity is 135.20462343740516
At time: 74.41472125053406 and batch: 700, loss is 4.9252145385742185 and perplexity is 137.71888448682625
At time: 75.13070821762085 and batch: 750, loss is 4.875578985214234 and perplexity is 131.05000726604365
At time: 75.84576344490051 and batch: 800, loss is 4.841951169967651 and perplexity is 126.71635582616261
At time: 76.56215786933899 and batch: 850, loss is 4.826156082153321 and perplexity is 124.73058386445517
At time: 77.2764732837677 and batch: 900, loss is 4.8828822612762455 and perplexity is 132.010605133059
At time: 77.9916090965271 and batch: 950, loss is 4.836803417205811 and perplexity is 126.06572742932048
At time: 78.70573949813843 and batch: 1000, loss is 4.830934543609619 and perplexity is 125.32803045279469
At time: 79.4202811717987 and batch: 1050, loss is 4.810375299453735 and perplexity is 122.77768727071137
At time: 80.14226150512695 and batch: 1100, loss is 4.787552433013916 and perplexity is 120.00728304882713
At time: 80.86262631416321 and batch: 1150, loss is 4.791300249099732 and perplexity is 120.4578921472237
At time: 81.58451819419861 and batch: 1200, loss is 4.78751651763916 and perplexity is 120.0029730196816
At time: 82.3043520450592 and batch: 1250, loss is 4.8381743812561036 and perplexity is 126.23867753670099
At time: 83.02187442779541 and batch: 1300, loss is 4.823139581680298 and perplexity is 124.35490090800573
At time: 83.73750948905945 and batch: 1350, loss is 4.778908128738403 and perplexity is 118.97437439911779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.907690836588542 and perplexity of 135.3265621880287
Finished 4 epochs...
Completing Train Step...
At time: 86.2402663230896 and batch: 50, loss is 4.859827871322632 and perplexity is 129.00199525339403
At time: 86.98759603500366 and batch: 100, loss is 4.87868215560913 and perplexity is 131.45730940596522
At time: 87.70347094535828 and batch: 150, loss is 4.8291545009613035 and perplexity is 125.10513964998025
At time: 88.41839504241943 and batch: 200, loss is 4.831099271774292 and perplexity is 125.3486772097378
At time: 89.135018825531 and batch: 250, loss is 4.820602769851685 and perplexity is 124.03983572394368
At time: 89.85334777832031 and batch: 300, loss is 4.821051063537598 and perplexity is 124.09545446493419
At time: 90.56826782226562 and batch: 350, loss is 4.837925567626953 and perplexity is 126.20727154048586
At time: 91.28820133209229 and batch: 400, loss is 4.834739828109742 and perplexity is 125.80584780341303
At time: 92.01045942306519 and batch: 450, loss is 4.803347024917603 and perplexity is 121.91779728873045
At time: 92.73677706718445 and batch: 500, loss is 4.8636040687561035 and perplexity is 129.49005317836261
At time: 93.45281147956848 and batch: 550, loss is 4.837599868774414 and perplexity is 126.16617267025384
At time: 94.17330026626587 and batch: 600, loss is 4.800189981460571 and perplexity is 121.5335044383036
At time: 94.89373326301575 and batch: 650, loss is 4.822212390899658 and perplexity is 124.23965362672111
At time: 95.60982203483582 and batch: 700, loss is 4.856230268478393 and perplexity is 128.53873112818428
At time: 96.32851266860962 and batch: 750, loss is 4.806761722564698 and perplexity is 122.33482130452022
At time: 97.04842782020569 and batch: 800, loss is 4.779546451568604 and perplexity is 119.05034270207715
At time: 97.76640725135803 and batch: 850, loss is 4.726790781021118 and perplexity is 112.9325548765357
At time: 98.48359942436218 and batch: 900, loss is 4.804932899475098 and perplexity is 122.11129701414794
At time: 99.20203828811646 and batch: 950, loss is 4.765607795715332 and perplexity is 117.40245231453711
At time: 99.91695785522461 and batch: 1000, loss is 4.774180145263672 and perplexity is 118.4131931961478
At time: 100.6366593837738 and batch: 1050, loss is 4.741602268218994 and perplexity is 114.61770293453276
At time: 101.3540289402008 and batch: 1100, loss is 4.719510688781738 and perplexity is 112.11338090922432
At time: 102.07018423080444 and batch: 1150, loss is 4.734420251846314 and perplexity is 113.79746571975377
At time: 102.83935976028442 and batch: 1200, loss is 4.72881700515747 and perplexity is 113.161613528723
At time: 103.55446100234985 and batch: 1250, loss is 4.7691167545318605 and perplexity is 117.81513630679389
At time: 104.26942372322083 and batch: 1300, loss is 4.747196760177612 and perplexity is 115.26072777342716
At time: 104.98770475387573 and batch: 1350, loss is 4.722831106185913 and perplexity is 112.48626284968805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.889937337239584 and perplexity of 132.94524307434924
Finished 5 epochs...
Completing Train Step...
At time: 107.48340463638306 and batch: 50, loss is 4.802799291610718 and perplexity is 121.85103713550222
At time: 108.22880148887634 and batch: 100, loss is 4.810214557647705 and perplexity is 122.75795334959483
At time: 108.94367599487305 and batch: 150, loss is 4.76340163230896 and perplexity is 117.14372881851439
At time: 109.66103172302246 and batch: 200, loss is 4.766512317657471 and perplexity is 117.50869345019215
At time: 110.37539076805115 and batch: 250, loss is 4.760325336456299 and perplexity is 116.78391378389612
At time: 111.0939633846283 and batch: 300, loss is 4.752723455429077 and perplexity is 115.89950222023168
At time: 111.81376552581787 and batch: 350, loss is 4.76607871055603 and perplexity is 117.45775189134017
At time: 112.5361840724945 and batch: 400, loss is 4.778847894668579 and perplexity is 118.96720830416662
At time: 113.25370931625366 and batch: 450, loss is 4.73412787437439 and perplexity is 113.76419876790591
At time: 113.96865892410278 and batch: 500, loss is 4.803555936813354 and perplexity is 121.94327002757433
At time: 114.68555402755737 and batch: 550, loss is 4.78636981010437 and perplexity is 119.8654435744185
At time: 115.40272688865662 and batch: 600, loss is 4.752904462814331 and perplexity is 115.92048278484205
At time: 116.11729073524475 and batch: 650, loss is 4.771086254119873 and perplexity is 118.0474018175472
At time: 116.83347153663635 and batch: 700, loss is 4.803546886444092 and perplexity is 121.94216640094568
At time: 117.55188798904419 and batch: 750, loss is 4.743144254684449 and perplexity is 114.79457821575748
At time: 118.26712250709534 and batch: 800, loss is 4.732941560745239 and perplexity is 113.629318769207
At time: 118.98360705375671 and batch: 850, loss is 4.696913747787476 and perplexity is 109.60837083043204
At time: 119.69962120056152 and batch: 900, loss is 4.759778661727905 and perplexity is 116.72008841699461
At time: 120.41674208641052 and batch: 950, loss is 4.720757427215577 and perplexity is 112.25324413846668
At time: 121.13327765464783 and batch: 1000, loss is 4.723628015518188 and perplexity is 112.57593992981087
At time: 121.87812876701355 and batch: 1050, loss is 4.688391180038452 and perplexity is 108.67819543578155
At time: 122.59530806541443 and batch: 1100, loss is 4.663177480697632 and perplexity is 105.97227260515724
At time: 123.31077218055725 and batch: 1150, loss is 4.676649284362793 and perplexity is 107.40957001208295
At time: 124.02794551849365 and batch: 1200, loss is 4.670710353851319 and perplexity is 106.7735625110689
At time: 124.74550294876099 and batch: 1250, loss is 4.7069057369232175 and perplexity is 110.70906639224597
At time: 125.46280121803284 and batch: 1300, loss is 4.704165258407593 and perplexity is 110.4060859196719
At time: 126.17940950393677 and batch: 1350, loss is 4.6682628154754635 and perplexity is 106.51254966906855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.860138346354167 and perplexity of 129.04205337013912
Finished 6 epochs...
Completing Train Step...
At time: 128.68700456619263 and batch: 50, loss is 4.7368439674377445 and perplexity is 114.07361292764753
At time: 129.40839076042175 and batch: 100, loss is 4.755572423934937 and perplexity is 116.23016705509545
At time: 130.1297528743744 and batch: 150, loss is 4.707729291915894 and perplexity is 110.80027895073144
At time: 130.84699964523315 and batch: 200, loss is 4.7143399524688725 and perplexity is 111.53516835999307
At time: 131.56768989562988 and batch: 250, loss is 4.714089450836181 and perplexity is 111.50723211739967
At time: 132.28369522094727 and batch: 300, loss is 4.705130634307861 and perplexity is 110.51272075733135
At time: 133.00945854187012 and batch: 350, loss is 4.7250065517425535 and perplexity is 112.73123695769193
At time: 133.7394678592682 and batch: 400, loss is 4.729055032730103 and perplexity is 113.18855231886664
At time: 134.4889931678772 and batch: 450, loss is 4.69248483657837 and perplexity is 109.12399850102577
At time: 135.2514204978943 and batch: 500, loss is 4.759958553314209 and perplexity is 116.74108726755509
At time: 136.002277135849 and batch: 550, loss is 4.736784019470215 and perplexity is 114.06677465137618
At time: 136.75404047966003 and batch: 600, loss is 4.694007635116577 and perplexity is 109.29029895533446
At time: 137.50218105316162 and batch: 650, loss is 4.724786720275879 and perplexity is 112.70645780824967
At time: 138.2313187122345 and batch: 700, loss is 4.755660820007324 and perplexity is 116.24044179947286
At time: 138.95332789421082 and batch: 750, loss is 4.698156957626343 and perplexity is 109.74472177432642
At time: 139.67133116722107 and batch: 800, loss is 4.676649827957153 and perplexity is 107.40962839933526
At time: 140.4473774433136 and batch: 850, loss is 4.653908987045288 and perplexity is 104.99460701171677
At time: 141.16388034820557 and batch: 900, loss is 4.720339317321777 and perplexity is 112.20631975693897
At time: 141.88329768180847 and batch: 950, loss is 4.674057655334472 and perplexity is 107.13156465150429
At time: 142.60220623016357 and batch: 1000, loss is 4.684317560195923 and perplexity is 108.23638228278516
At time: 143.3197627067566 and batch: 1050, loss is 4.643447570800781 and perplexity is 103.90194011118626
At time: 144.04241394996643 and batch: 1100, loss is 4.620228786468505 and perplexity is 101.51725524719716
At time: 144.76193594932556 and batch: 1150, loss is 4.627093496322632 and perplexity is 102.21653919425763
At time: 145.48098063468933 and batch: 1200, loss is 4.619310913085937 and perplexity is 101.42411801136815
At time: 146.19814229011536 and batch: 1250, loss is 4.669903087615967 and perplexity is 106.68740260089767
At time: 146.91387009620667 and batch: 1300, loss is 4.660017728805542 and perplexity is 105.63795497481964
At time: 147.63354778289795 and batch: 1350, loss is 4.640869150161743 and perplexity is 103.63438229090312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.885947672526042 and perplexity of 132.41589279644228
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 150.1718475818634 and batch: 50, loss is 4.673891286849976 and perplexity is 107.11374281798831
At time: 150.8878309726715 and batch: 100, loss is 4.639814710617065 and perplexity is 103.52516369235148
At time: 151.60686826705933 and batch: 150, loss is 4.589018630981445 and perplexity is 98.39781819343533
At time: 152.31804728507996 and batch: 200, loss is 4.5794878101348875 and perplexity is 97.46446111134296
At time: 153.0313606262207 and batch: 250, loss is 4.559296607971191 and perplexity is 95.51627081218278
At time: 153.74648547172546 and batch: 300, loss is 4.538058776855468 and perplexity is 93.50910178541659
At time: 154.4583878517151 and batch: 350, loss is 4.542215461730957 and perplexity is 93.89859860147948
At time: 155.1703724861145 and batch: 400, loss is 4.53947265625 and perplexity is 93.64140588661314
At time: 155.88406896591187 and batch: 450, loss is 4.488631086349487 and perplexity is 88.99952978018611
At time: 156.59450340270996 and batch: 500, loss is 4.539862155914307 and perplexity is 93.67788628686179
At time: 157.30719566345215 and batch: 550, loss is 4.530573720932007 and perplexity is 92.8117938791522
At time: 158.0174810886383 and batch: 600, loss is 4.469254989624023 and perplexity is 87.29166558562315
At time: 158.72989010810852 and batch: 650, loss is 4.48418267250061 and perplexity is 88.60450231360767
At time: 159.49471616744995 and batch: 700, loss is 4.493594188690185 and perplexity is 89.44234150575629
At time: 160.20831966400146 and batch: 750, loss is 4.434959506988525 and perplexity is 84.34870928588408
At time: 160.9208698272705 and batch: 800, loss is 4.413463411331176 and perplexity is 82.55489048652115
At time: 161.63322377204895 and batch: 850, loss is 4.382366256713867 and perplexity is 80.0271743774464
At time: 162.3456108570099 and batch: 900, loss is 4.427198143005371 and perplexity is 83.69658222296414
At time: 163.06227946281433 and batch: 950, loss is 4.383463945388794 and perplexity is 80.11506753128133
At time: 163.77845573425293 and batch: 1000, loss is 4.373827180862427 and perplexity is 79.3467256017335
At time: 164.48969531059265 and batch: 1050, loss is 4.318552389144897 and perplexity is 75.079863161313
At time: 165.20157194137573 and batch: 1100, loss is 4.269859399795532 and perplexity is 71.51158036952893
At time: 165.91511964797974 and batch: 1150, loss is 4.281211061477661 and perplexity is 72.32798061917946
At time: 166.63464212417603 and batch: 1200, loss is 4.262295112609864 and perplexity is 70.97268698024314
At time: 167.34573650360107 and batch: 1250, loss is 4.297360105514526 and perplexity is 73.50549060330609
At time: 168.05955958366394 and batch: 1300, loss is 4.282225275039673 and perplexity is 72.40137384995019
At time: 168.7760889530182 and batch: 1350, loss is 4.2690293502807615 and perplexity is 71.45224684523089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.672327473958333 and perplexity of 106.94636787175652
Finished 8 epochs...
Completing Train Step...
At time: 171.26695275306702 and batch: 50, loss is 4.503089866638184 and perplexity is 90.29570238350725
At time: 172.02564096450806 and batch: 100, loss is 4.497986688613891 and perplexity is 89.83608110149929
At time: 172.7498230934143 and batch: 150, loss is 4.461778450012207 and perplexity is 86.64145966405471
At time: 173.46588325500488 and batch: 200, loss is 4.461735363006592 and perplexity is 86.63772662341903
At time: 174.191344499588 and batch: 250, loss is 4.447541446685791 and perplexity is 85.416684168653
At time: 174.91346907615662 and batch: 300, loss is 4.437252492904663 and perplexity is 84.54234160156766
At time: 175.6324861049652 and batch: 350, loss is 4.445298051834106 and perplexity is 85.22527560191475
At time: 176.35550618171692 and batch: 400, loss is 4.444872970581055 and perplexity is 85.18905563373036
At time: 177.07218980789185 and batch: 450, loss is 4.3994762134552 and perplexity is 81.40821696706702
At time: 177.84371542930603 and batch: 500, loss is 4.458003511428833 and perplexity is 86.31501002637869
At time: 178.55890274047852 and batch: 550, loss is 4.454165019989014 and perplexity is 85.98432566982609
At time: 179.27391171455383 and batch: 600, loss is 4.397988958358765 and perplexity is 81.28723217141878
At time: 179.9910945892334 and batch: 650, loss is 4.413370428085327 and perplexity is 82.54721462171189
At time: 180.7083773612976 and batch: 700, loss is 4.435337800979614 and perplexity is 84.38062393194184
At time: 181.42577528953552 and batch: 750, loss is 4.379612760543823 and perplexity is 79.80712295370597
At time: 182.14286017417908 and batch: 800, loss is 4.361505651473999 and perplexity is 78.37505114134336
At time: 182.85681867599487 and batch: 850, loss is 4.332119445800782 and perplexity is 76.10541706180554
At time: 183.57378101348877 and batch: 900, loss is 4.384092712402344 and perplexity is 80.16545708301722
At time: 184.29325318336487 and batch: 950, loss is 4.344624452590942 and perplexity is 77.06309120005808
At time: 185.01413822174072 and batch: 1000, loss is 4.336408748626709 and perplexity is 76.43255734257883
At time: 185.73753333091736 and batch: 1050, loss is 4.287647008895874 and perplexity is 72.79498088189423
At time: 186.45416164398193 and batch: 1100, loss is 4.247928781509399 and perplexity is 69.96035899079655
At time: 187.1722993850708 and batch: 1150, loss is 4.263075942993164 and perplexity is 71.028126252139
At time: 187.88966917991638 and batch: 1200, loss is 4.250691385269165 and perplexity is 70.15389895560523
At time: 188.60889506340027 and batch: 1250, loss is 4.295457553863526 and perplexity is 73.36577556048775
At time: 189.3259711265564 and batch: 1300, loss is 4.285564098358154 and perplexity is 72.64351325064692
At time: 190.04853057861328 and batch: 1350, loss is 4.266798048019409 and perplexity is 71.29299302304644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.661234130859375 and perplexity of 105.76653138466442
Finished 9 epochs...
Completing Train Step...
At time: 192.5733802318573 and batch: 50, loss is 4.458612661361695 and perplexity is 86.36760482637432
At time: 193.3180603981018 and batch: 100, loss is 4.4519836044311525 and perplexity is 85.79696255678104
At time: 194.03599667549133 and batch: 150, loss is 4.4178433609008785 and perplexity is 82.91726976574621
At time: 194.75368332862854 and batch: 200, loss is 4.419593744277954 and perplexity is 83.06253387326734
At time: 195.46977639198303 and batch: 250, loss is 4.406764621734619 and perplexity is 82.00372079098385
At time: 196.23905611038208 and batch: 300, loss is 4.396886672973633 and perplexity is 81.19767980859619
At time: 196.95800161361694 and batch: 350, loss is 4.407058763504028 and perplexity is 82.02784505831882
At time: 197.67206239700317 and batch: 400, loss is 4.4081053543090825 and perplexity is 82.11373958708688
At time: 198.39004230499268 and batch: 450, loss is 4.362582321166992 and perplexity is 78.45948062677209
At time: 199.10973596572876 and batch: 500, loss is 4.424724855422974 and perplexity is 83.48983228688117
At time: 199.8253583908081 and batch: 550, loss is 4.422067270278931 and perplexity is 83.26824552218174
At time: 200.54162287712097 and batch: 600, loss is 4.365529327392578 and perplexity is 78.69104224380922
At time: 201.2580156326294 and batch: 650, loss is 4.379167375564575 and perplexity is 79.77158597431121
At time: 201.97542595863342 and batch: 700, loss is 4.406201524734497 and perplexity is 81.95755774016507
At time: 202.69256949424744 and batch: 750, loss is 4.354600582122803 and perplexity is 77.83573014514907
At time: 203.4072389602661 and batch: 800, loss is 4.335542688369751 and perplexity is 76.36639079856957
At time: 204.1243941783905 and batch: 850, loss is 4.308572454452515 and perplexity is 74.33429756635141
At time: 204.84417009353638 and batch: 900, loss is 4.360540189743042 and perplexity is 78.29941954438561
At time: 205.56057214736938 and batch: 950, loss is 4.323391370773315 and perplexity is 75.44405368479188
At time: 206.27703762054443 and batch: 1000, loss is 4.318193082809448 and perplexity is 75.05289133667841
At time: 206.99429178237915 and batch: 1050, loss is 4.27287446975708 and perplexity is 71.7275181575886
At time: 207.7098319530487 and batch: 1100, loss is 4.233086776733399 and perplexity is 68.92967463985221
At time: 208.43295669555664 and batch: 1150, loss is 4.251103038787842 and perplexity is 70.18278399986637
At time: 209.16674280166626 and batch: 1200, loss is 4.240094327926636 and perplexity is 69.41439924630824
At time: 209.89710521697998 and batch: 1250, loss is 4.286367616653442 and perplexity is 72.70190709969904
At time: 210.61217975616455 and batch: 1300, loss is 4.277733249664307 and perplexity is 72.07687441690209
At time: 211.33617997169495 and batch: 1350, loss is 4.256639556884766 and perplexity is 70.57242989904168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.659460856119792 and perplexity of 105.57914445962025
Finished 10 epochs...
Completing Train Step...
At time: 213.8887894153595 and batch: 50, loss is 4.426310968399048 and perplexity is 83.62236166874352
At time: 214.6119213104248 and batch: 100, loss is 4.422966451644897 and perplexity is 83.3431524493415
At time: 215.3829526901245 and batch: 150, loss is 4.391099710464477 and perplexity is 80.72914887274295
At time: 216.09962606430054 and batch: 200, loss is 4.3907490158081055 and perplexity is 80.7008425553471
At time: 216.81593656539917 and batch: 250, loss is 4.377951622009277 and perplexity is 79.67466231463138
At time: 217.53019213676453 and batch: 300, loss is 4.370878667831421 and perplexity is 79.11311531805727
At time: 218.24617671966553 and batch: 350, loss is 4.3815170001983645 and perplexity is 79.95923962928592
At time: 218.96616125106812 and batch: 400, loss is 4.384396276473999 and perplexity is 80.18979612961854
At time: 219.68013191223145 and batch: 450, loss is 4.338201351165772 and perplexity is 76.56969341745321
At time: 220.39660596847534 and batch: 500, loss is 4.403079833984375 and perplexity is 81.7021105113341
At time: 221.11394739151 and batch: 550, loss is 4.399356107711792 and perplexity is 81.39843995979781
At time: 221.8329141139984 and batch: 600, loss is 4.344598064422607 and perplexity is 77.06105767306573
At time: 222.54763746261597 and batch: 650, loss is 4.355703048706054 and perplexity is 77.92158875604551
At time: 223.2633466720581 and batch: 700, loss is 4.386359024047851 and perplexity is 80.34734301922984
At time: 223.98667979240417 and batch: 750, loss is 4.33602198600769 and perplexity is 76.40300180238023
At time: 224.70241928100586 and batch: 800, loss is 4.317424383163452 and perplexity is 74.9952203743409
At time: 225.41889357566833 and batch: 850, loss is 4.291153135299683 and perplexity is 73.05065724132892
At time: 226.13562273979187 and batch: 900, loss is 4.343451385498047 and perplexity is 76.97274402570355
At time: 226.85246777534485 and batch: 950, loss is 4.307442979812622 and perplexity is 74.25038625914331
At time: 227.57512879371643 and batch: 1000, loss is 4.304275989532471 and perplexity is 74.01560797406863
At time: 228.296968460083 and batch: 1050, loss is 4.257297887802124 and perplexity is 70.61890520794513
At time: 229.01898574829102 and batch: 1100, loss is 4.220187239646911 and perplexity is 68.04622404765287
At time: 229.74027848243713 and batch: 1150, loss is 4.240353441238403 and perplexity is 69.43238777161379
At time: 230.46211171150208 and batch: 1200, loss is 4.229580383300782 and perplexity is 68.68840332465854
At time: 231.18326687812805 and batch: 1250, loss is 4.27665078163147 and perplexity is 71.99889571677828
At time: 231.90361547470093 and batch: 1300, loss is 4.26889497756958 and perplexity is 71.44264625814522
At time: 232.62088704109192 and batch: 1350, loss is 4.24655047416687 and perplexity is 69.86399853672917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.656224772135417 and perplexity of 105.23803371034948
Finished 11 epochs...
Completing Train Step...
At time: 235.15045619010925 and batch: 50, loss is 4.401090564727784 and perplexity is 81.53974456303268
At time: 235.87068939208984 and batch: 100, loss is 4.399595804214478 and perplexity is 81.41795321971686
At time: 236.58724880218506 and batch: 150, loss is 4.366663694381714 and perplexity is 78.78035741298356
At time: 237.30193448066711 and batch: 200, loss is 4.368033294677734 and perplexity is 78.88832893585403
At time: 238.01769828796387 and batch: 250, loss is 4.356074542999267 and perplexity is 77.95054155915405
At time: 238.74036407470703 and batch: 300, loss is 4.348419647216797 and perplexity is 77.3561163216
At time: 239.4550642967224 and batch: 350, loss is 4.360015516281128 and perplexity is 78.25834869220267
At time: 240.1723494529724 and batch: 400, loss is 4.3628079700469975 and perplexity is 78.47718691832861
At time: 240.89414191246033 and batch: 450, loss is 4.31987154006958 and perplexity is 75.1789701864174
At time: 241.61118125915527 and batch: 500, loss is 4.386458044052124 and perplexity is 80.35529940739319
At time: 242.330415725708 and batch: 550, loss is 4.3806618309021 and perplexity is 79.89089017194637
At time: 243.04675030708313 and batch: 600, loss is 4.325029392242431 and perplexity is 75.56773393224101
At time: 243.7648847103119 and batch: 650, loss is 4.336544256210328 and perplexity is 76.44291523550481
At time: 244.48322653770447 and batch: 700, loss is 4.369032897949219 and perplexity is 78.96722539354937
At time: 245.2030792236328 and batch: 750, loss is 4.321234712600708 and perplexity is 75.28152197550446
At time: 245.92329287528992 and batch: 800, loss is 4.301525478363037 and perplexity is 73.8123069366802
At time: 246.64162755012512 and batch: 850, loss is 4.278097734451294 and perplexity is 72.1031501293785
At time: 247.35926604270935 and batch: 900, loss is 4.328069114685059 and perplexity is 75.79778834265035
At time: 248.0777587890625 and batch: 950, loss is 4.293646011352539 and perplexity is 73.23299064827829
At time: 248.79381585121155 and batch: 1000, loss is 4.291773500442505 and perplexity is 73.09598938252797
At time: 249.51502132415771 and batch: 1050, loss is 4.245214290618897 and perplexity is 69.77070975063906
At time: 250.23627376556396 and batch: 1100, loss is 4.207036862373352 and perplexity is 67.15724852217922
At time: 250.95927786827087 and batch: 1150, loss is 4.229613151550293 and perplexity is 68.69065416027497
At time: 251.73191499710083 and batch: 1200, loss is 4.219253234863281 and perplexity is 67.98269822021587
At time: 252.45272946357727 and batch: 1250, loss is 4.267895851135254 and perplexity is 71.37130166879807
At time: 253.1748538017273 and batch: 1300, loss is 4.2611887168884275 and perplexity is 70.89420652625437
At time: 253.89377403259277 and batch: 1350, loss is 4.234546403884888 and perplexity is 69.03035972796816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.655978190104166 and perplexity of 105.21208710134762
Finished 12 epochs...
Completing Train Step...
At time: 256.3711488246918 and batch: 50, loss is 4.380273246765137 and perplexity is 79.85985187022412
At time: 257.11695313453674 and batch: 100, loss is 4.379127311706543 and perplexity is 79.76839008083608
At time: 257.83213782310486 and batch: 150, loss is 4.349304943084717 and perplexity is 77.42462969456605
At time: 258.5480167865753 and batch: 200, loss is 4.349641494750976 and perplexity is 77.45069146801991
At time: 259.2641324996948 and batch: 250, loss is 4.337607946395874 and perplexity is 76.52427007469672
At time: 259.97930932044983 and batch: 300, loss is 4.330417499542237 and perplexity is 75.97599989375404
At time: 260.69775128364563 and batch: 350, loss is 4.343384456634522 and perplexity is 76.96759249981831
At time: 261.41327238082886 and batch: 400, loss is 4.34729606628418 and perplexity is 77.26924927449728
At time: 262.12927532196045 and batch: 450, loss is 4.303320569992065 and perplexity is 73.94492578686335
At time: 262.8463845252991 and batch: 500, loss is 4.369743165969848 and perplexity is 79.02333321186
At time: 263.56346559524536 and batch: 550, loss is 4.36470440864563 and perplexity is 78.62615529475534
At time: 264.2806224822998 and batch: 600, loss is 4.309386682510376 and perplexity is 74.39484728438582
At time: 264.996422290802 and batch: 650, loss is 4.319820461273193 and perplexity is 75.17513023317758
At time: 265.7130103111267 and batch: 700, loss is 4.3537158203125 and perplexity is 77.7668945196963
At time: 266.4323740005493 and batch: 750, loss is 4.306909599304199 and perplexity is 74.21079311041538
At time: 267.15048336982727 and batch: 800, loss is 4.288501796722412 and perplexity is 72.8572317472357
At time: 267.86762142181396 and batch: 850, loss is 4.2654836463928225 and perplexity is 71.19934695480723
At time: 268.58660864830017 and batch: 900, loss is 4.313825368881226 and perplexity is 74.72579662589251
At time: 269.3060917854309 and batch: 950, loss is 4.28136978149414 and perplexity is 72.33946142854828
At time: 270.07926392555237 and batch: 1000, loss is 4.280883150100708 and perplexity is 72.30426733960196
At time: 270.79783487319946 and batch: 1050, loss is 4.234425039291382 and perplexity is 69.02198239478626
At time: 271.51085090637207 and batch: 1100, loss is 4.197029819488526 and perplexity is 66.48855446155476
At time: 272.23598313331604 and batch: 1150, loss is 4.218867053985596 and perplexity is 67.95644967081994
At time: 272.95706367492676 and batch: 1200, loss is 4.208395819664002 and perplexity is 67.24857439460372
At time: 273.6737380027771 and batch: 1250, loss is 4.257016639709473 and perplexity is 70.5990465682835
At time: 274.3896994590759 and batch: 1300, loss is 4.249862604141235 and perplexity is 70.09578081503585
At time: 275.1117515563965 and batch: 1350, loss is 4.223352098464966 and perplexity is 68.26192188642723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.6569921875 and perplexity of 105.31882599101104
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 277.57771468162537 and batch: 50, loss is 4.369049091339111 and perplexity is 78.96850415097255
At time: 278.3257038593292 and batch: 100, loss is 4.376038341522217 and perplexity is 79.52236807516789
At time: 279.04148030281067 and batch: 150, loss is 4.349069633483887 and perplexity is 77.40641307921449
At time: 279.76035618782043 and batch: 200, loss is 4.349397144317627 and perplexity is 77.43176866998759
At time: 280.47533774375916 and batch: 250, loss is 4.33438193321228 and perplexity is 76.27779954291815
At time: 281.19308710098267 and batch: 300, loss is 4.324532480239868 and perplexity is 75.53019274634897
At time: 281.911404132843 and batch: 350, loss is 4.329842643737793 and perplexity is 75.93233720029549
At time: 282.62731647491455 and batch: 400, loss is 4.33368634223938 and perplexity is 76.22475984323015
At time: 283.346244096756 and batch: 450, loss is 4.280434408187866 and perplexity is 72.27182866321056
At time: 284.0597231388092 and batch: 500, loss is 4.3443983364105225 and perplexity is 77.04566795813729
At time: 284.7693953514099 and batch: 550, loss is 4.336960573196411 and perplexity is 76.47474634503911
At time: 285.48043394088745 and batch: 600, loss is 4.282444810867309 and perplexity is 72.41727029033558
At time: 286.1914732456207 and batch: 650, loss is 4.285058088302613 and perplexity is 72.60676420095066
At time: 286.9053156375885 and batch: 700, loss is 4.3073515033721925 and perplexity is 74.24359440875969
At time: 287.64987087249756 and batch: 750, loss is 4.2616610622406 and perplexity is 70.92770098505625
At time: 288.382479429245 and batch: 800, loss is 4.2382658672332765 and perplexity is 69.28759371053087
At time: 289.17621660232544 and batch: 850, loss is 4.203741731643677 and perplexity is 66.93632080161456
At time: 289.92244386672974 and batch: 900, loss is 4.2427818298339846 and perplexity is 69.60120147992554
At time: 290.6704490184784 and batch: 950, loss is 4.2130278205871585 and perplexity is 67.56079239207594
At time: 291.4082307815552 and batch: 1000, loss is 4.207424068450928 and perplexity is 67.18325725200448
At time: 292.1401832103729 and batch: 1050, loss is 4.157175960540772 and perplexity is 63.890837343132425
At time: 292.85666584968567 and batch: 1100, loss is 4.114723262786865 and perplexity is 61.23526584091621
At time: 293.5933744907379 and batch: 1150, loss is 4.12852071762085 and perplexity is 62.0860122266972
At time: 294.33458137512207 and batch: 1200, loss is 4.1104697990417485 and perplexity is 60.97535700599726
At time: 295.0598187446594 and batch: 1250, loss is 4.153897356987 and perplexity is 63.681707630806876
At time: 295.7759499549866 and batch: 1300, loss is 4.146820425987244 and perplexity is 63.23262750874558
At time: 296.49141335487366 and batch: 1350, loss is 4.130437197685242 and perplexity is 62.205112922007686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.6068359375 and perplexity of 100.16671396466135
Finished 14 epochs...
Completing Train Step...
At time: 299.0479393005371 and batch: 50, loss is 4.34124044418335 and perplexity is 76.80274979870939
At time: 299.7602310180664 and batch: 100, loss is 4.342534790039062 and perplexity is 76.90222348240619
At time: 300.47213792800903 and batch: 150, loss is 4.315012855529785 and perplexity is 74.8145852188895
At time: 301.1914846897125 and batch: 200, loss is 4.317342643737793 and perplexity is 74.98909055862754
At time: 301.9104392528534 and batch: 250, loss is 4.300763578414917 and perplexity is 73.75609076212417
At time: 302.62571692466736 and batch: 300, loss is 4.294151201248169 and perplexity is 73.2699965619003
At time: 303.34227108955383 and batch: 350, loss is 4.300476961135864 and perplexity is 73.73495402129863
At time: 304.05730843544006 and batch: 400, loss is 4.305326957702636 and perplexity is 74.0934369128498
At time: 304.77117013931274 and batch: 450, loss is 4.254893665313721 and perplexity is 70.44932558319637
At time: 305.48249530792236 and batch: 500, loss is 4.320278615951538 and perplexity is 75.20957996183867
At time: 306.1992154121399 and batch: 550, loss is 4.3156316947937015 and perplexity is 74.860897750265
At time: 306.91714453697205 and batch: 600, loss is 4.261529493331909 and perplexity is 70.91836971870738
At time: 307.6857924461365 and batch: 650, loss is 4.2670722007751465 and perplexity is 71.31254087297448
At time: 308.39818000793457 and batch: 700, loss is 4.292023200988769 and perplexity is 73.1142437699769
At time: 309.11220717430115 and batch: 750, loss is 4.247031364440918 and perplexity is 69.89760353364206
At time: 309.82872700691223 and batch: 800, loss is 4.2253021621704105 and perplexity is 68.3951668586292
At time: 310.54711723327637 and batch: 850, loss is 4.192591414451599 and perplexity is 66.19410525233485
At time: 311.26045989990234 and batch: 900, loss is 4.234584083557129 and perplexity is 69.03296081830119
At time: 311.97775077819824 and batch: 950, loss is 4.206033372879029 and perplexity is 67.0898907308899
At time: 312.69743847846985 and batch: 1000, loss is 4.203498201370239 and perplexity is 66.9200217658417
At time: 313.4129662513733 and batch: 1050, loss is 4.156055135726929 and perplexity is 63.819267023648536
At time: 314.12573170661926 and batch: 1100, loss is 4.116408214569092 and perplexity is 61.33853128544601
At time: 314.83832335472107 and batch: 1150, loss is 4.132635221481324 and perplexity is 62.3419916166536
At time: 315.553964138031 and batch: 1200, loss is 4.117571330070495 and perplexity is 61.40991658864605
At time: 316.2651469707489 and batch: 1250, loss is 4.163583817481995 and perplexity is 64.30155519393756
At time: 316.9808146953583 and batch: 1300, loss is 4.156790366172791 and perplexity is 63.8662061452181
At time: 317.6946268081665 and batch: 1350, loss is 4.138350052833557 and perplexity is 62.69928554977708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.60391357421875 and perplexity of 99.87441774366185
Finished 15 epochs...
Completing Train Step...
At time: 320.2477207183838 and batch: 50, loss is 4.331076450347901 and perplexity is 76.02608083872252
At time: 320.971887588501 and batch: 100, loss is 4.3303725624084475 and perplexity is 75.97258582679181
At time: 321.69131088256836 and batch: 150, loss is 4.302459850311279 and perplexity is 73.88130731670655
At time: 322.4071979522705 and batch: 200, loss is 4.304239578247071 and perplexity is 74.0129130197063
At time: 323.12334179878235 and batch: 250, loss is 4.28783166885376 and perplexity is 72.80842444120351
At time: 323.8381130695343 and batch: 300, loss is 4.28146731376648 and perplexity is 72.34651720467855
At time: 324.554651260376 and batch: 350, loss is 4.2889244270324705 and perplexity is 72.88802992934326
At time: 325.2741539478302 and batch: 400, loss is 4.294212522506714 and perplexity is 73.2744897080644
At time: 325.992556810379 and batch: 450, loss is 4.244637622833252 and perplexity is 69.73048682869059
At time: 326.7624571323395 and batch: 500, loss is 4.310460557937622 and perplexity is 74.47478099454014
At time: 327.4795038700104 and batch: 550, loss is 4.306614627838135 and perplexity is 74.18890627212889
At time: 328.19993138313293 and batch: 600, loss is 4.252639579772949 and perplexity is 70.29070561569083
At time: 328.92309641838074 and batch: 650, loss is 4.259203424453736 and perplexity is 70.75360041268299
At time: 329.6511299610138 and batch: 700, loss is 4.2858749103546145 and perplexity is 72.6660952352243
At time: 330.3743770122528 and batch: 750, loss is 4.241145009994507 and perplexity is 69.48737003868698
At time: 331.0963225364685 and batch: 800, loss is 4.220135059356689 and perplexity is 68.04267346856948
At time: 331.8172733783722 and batch: 850, loss is 4.187883443832398 and perplexity is 65.8831977955
At time: 332.53833055496216 and batch: 900, loss is 4.231091461181641 and perplexity is 68.79227531115983
At time: 333.2660768032074 and batch: 950, loss is 4.202923593521118 and perplexity is 66.8815800415876
At time: 333.98582100868225 and batch: 1000, loss is 4.2020803689956665 and perplexity is 66.82520762363849
At time: 334.7069180011749 and batch: 1050, loss is 4.155851058959961 and perplexity is 63.80624432282462
At time: 335.4235088825226 and batch: 1100, loss is 4.11686842918396 and perplexity is 61.3667666706682
At time: 336.13928627967834 and batch: 1150, loss is 4.134339723587036 and perplexity is 62.448344286063886
At time: 336.85717821121216 and batch: 1200, loss is 4.120716919898987 and perplexity is 61.60339113388501
At time: 337.57463216781616 and batch: 1250, loss is 4.167199258804321 and perplexity is 64.53445445679145
At time: 338.28948950767517 and batch: 1300, loss is 4.160408782958984 and perplexity is 64.09771930054926
At time: 339.0079936981201 and batch: 1350, loss is 4.140640983581543 and perplexity is 62.843089930964275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.602735188802083 and perplexity of 99.75679650148352
Finished 16 epochs...
Completing Train Step...
At time: 341.4979581832886 and batch: 50, loss is 4.3231411552429195 and perplexity is 75.42517877238053
At time: 342.2433125972748 and batch: 100, loss is 4.322112312316895 and perplexity is 75.34761801651554
At time: 342.9591701030731 and batch: 150, loss is 4.294427661895752 and perplexity is 73.29025563288715
At time: 343.67191314697266 and batch: 200, loss is 4.29555235862732 and perplexity is 73.37273131522447
At time: 344.39295649528503 and batch: 250, loss is 4.279274129867554 and perplexity is 72.18802185625596
At time: 345.1686341762543 and batch: 300, loss is 4.273173990249634 and perplexity is 71.7490052369065
At time: 345.8853497505188 and batch: 350, loss is 4.281465282440186 and perplexity is 72.34637024544514
At time: 346.6056101322174 and batch: 400, loss is 4.286923503875732 and perplexity is 72.74233239580366
At time: 347.3282036781311 and batch: 450, loss is 4.2377330017089845 and perplexity is 69.25068257579035
At time: 348.04405212402344 and batch: 500, loss is 4.303509531021118 and perplexity is 73.95889981636516
At time: 348.7652657032013 and batch: 550, loss is 4.300818939208984 and perplexity is 73.76017407090262
At time: 349.4862823486328 and batch: 600, loss is 4.247058525085449 and perplexity is 69.89950202338717
At time: 350.21051383018494 and batch: 650, loss is 4.254348669052124 and perplexity is 70.41094142468306
At time: 350.9267568588257 and batch: 700, loss is 4.2814825248718265 and perplexity is 72.34761768354295
At time: 351.64166283607483 and batch: 750, loss is 4.236804361343384 and perplexity is 69.18640344732114
At time: 352.35877203941345 and batch: 800, loss is 4.216271390914917 and perplexity is 67.78028635321178
At time: 353.07720708847046 and batch: 850, loss is 4.184711389541626 and perplexity is 65.67454382106553
At time: 353.79473423957825 and batch: 900, loss is 4.228135566711426 and perplexity is 68.58923283887174
At time: 354.519553899765 and batch: 950, loss is 4.200222158432007 and perplexity is 66.70114761741827
At time: 355.2389724254608 and batch: 1000, loss is 4.20067412853241 and perplexity is 66.7313013555846
At time: 355.95576763153076 and batch: 1050, loss is 4.154903755187989 and perplexity is 63.74582904722944
At time: 356.67264223098755 and batch: 1100, loss is 4.1167767000198365 and perplexity is 61.361137806626004
At time: 357.391033411026 and batch: 1150, loss is 4.134891295433045 and perplexity is 62.482798535725465
At time: 358.1093182563782 and batch: 1200, loss is 4.121743707656861 and perplexity is 61.666677226877376
At time: 358.8278863430023 and batch: 1250, loss is 4.16855384349823 and perplexity is 64.62193107489644
At time: 359.54481530189514 and batch: 1300, loss is 4.161476917266846 and perplexity is 64.16622085150537
At time: 360.269877910614 and batch: 1350, loss is 4.140696048736572 and perplexity is 62.84655049073112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.6023486328125 and perplexity of 99.71824236644025
Finished 17 epochs...
Completing Train Step...
At time: 362.7673900127411 and batch: 50, loss is 4.316517934799195 and perplexity is 74.92727188005868
At time: 363.51602029800415 and batch: 100, loss is 4.314978046417236 and perplexity is 74.81198103489726
At time: 364.23125290870667 and batch: 150, loss is 4.287761516571045 and perplexity is 72.80331694318154
At time: 364.955206155777 and batch: 200, loss is 4.288642468452454 and perplexity is 72.86748142097444
At time: 365.66888546943665 and batch: 250, loss is 4.272400274276733 and perplexity is 71.69351335576063
At time: 366.3875849246979 and batch: 300, loss is 4.2667249584197995 and perplexity is 71.28778243715352
At time: 367.12153840065 and batch: 350, loss is 4.2753815078735355 and perplexity is 71.90756738043632
At time: 367.847944021225 and batch: 400, loss is 4.281313877105713 and perplexity is 72.33541744823715
At time: 368.5886070728302 and batch: 450, loss is 4.232431087493897 and perplexity is 68.8844930080821
At time: 369.32246446609497 and batch: 500, loss is 4.298051805496216 and perplexity is 73.55635193817479
At time: 370.0857264995575 and batch: 550, loss is 4.296240968704224 and perplexity is 73.42327391745349
At time: 370.8340940475464 and batch: 600, loss is 4.2426248550415036 and perplexity is 69.5902767032465
At time: 371.5614264011383 and batch: 650, loss is 4.250488862991333 and perplexity is 70.13969266678352
At time: 372.29809379577637 and batch: 700, loss is 4.277844324111938 and perplexity is 72.08488076055683
At time: 373.0147485733032 and batch: 750, loss is 4.233211317062378 and perplexity is 68.93825969879022
At time: 373.7308430671692 and batch: 800, loss is 4.2131262874603275 and perplexity is 67.56744521958778
At time: 374.45250153541565 and batch: 850, loss is 4.181694512367248 and perplexity is 65.47671035855201
At time: 375.16943287849426 and batch: 900, loss is 4.2252431488037105 and perplexity is 68.39113074866025
At time: 375.88797521591187 and batch: 950, loss is 4.197986364364624 and perplexity is 66.55218417513075
At time: 376.6092526912689 and batch: 1000, loss is 4.198934502601624 and perplexity is 66.61531476923878
At time: 377.32554936408997 and batch: 1050, loss is 4.153489589691162 and perplexity is 63.655745606683865
At time: 378.0411157608032 and batch: 1100, loss is 4.115930829048157 and perplexity is 61.30925614705448
At time: 378.7664575576782 and batch: 1150, loss is 4.134550161361695 and perplexity is 62.461487159496556
At time: 379.482861995697 and batch: 1200, loss is 4.121815462112426 and perplexity is 61.671102244483805
At time: 380.20247769355774 and batch: 1250, loss is 4.168669333457947 and perplexity is 64.62939469009115
At time: 380.9198794364929 and batch: 1300, loss is 4.16130413532257 and perplexity is 64.15513504485102
At time: 381.6429178714752 and batch: 1350, loss is 4.139930934906006 and perplexity is 62.79848411621605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.60210693359375 and perplexity of 99.69414345762615
Finished 18 epochs...
Completing Train Step...
At time: 384.2372553348541 and batch: 50, loss is 4.3106620979309085 and perplexity is 74.48979215402545
At time: 384.9545011520386 and batch: 100, loss is 4.308713407516479 and perplexity is 74.34477595181397
At time: 385.6745979785919 and batch: 150, loss is 4.282056293487549 and perplexity is 72.38914038706497
At time: 386.3919823169708 and batch: 200, loss is 4.282849831581116 and perplexity is 72.44660672537138
At time: 387.1096398830414 and batch: 250, loss is 4.267000231742859 and perplexity is 71.30740876309659
At time: 387.82745146751404 and batch: 300, loss is 4.261320018768311 and perplexity is 70.90351567998542
At time: 388.54773354530334 and batch: 350, loss is 4.27032096862793 and perplexity is 71.54459550497562
At time: 389.2673354148865 and batch: 400, loss is 4.276590261459351 and perplexity is 71.99453846306932
At time: 389.9882354736328 and batch: 450, loss is 4.227848715782166 and perplexity is 68.5695607753044
At time: 390.70491099357605 and batch: 500, loss is 4.293762111663819 and perplexity is 73.24149351487166
At time: 391.42163491249084 and batch: 550, loss is 4.293039131164551 and perplexity is 73.18856048040466
At time: 392.14052391052246 and batch: 600, loss is 4.238955850601196 and perplexity is 69.33541749470484
At time: 392.85685682296753 and batch: 650, loss is 4.2469582271575925 and perplexity is 69.89249159974739
At time: 393.5737192630768 and batch: 700, loss is 4.2745039176940915 and perplexity is 71.84448968770573
At time: 394.29133772850037 and batch: 750, loss is 4.2302708196640015 and perplexity is 68.73584467177636
At time: 395.0103693008423 and batch: 800, loss is 4.210152778625488 and perplexity is 67.36683123582135
At time: 395.72816157341003 and batch: 850, loss is 4.178824830055237 and perplexity is 65.28908234614248
At time: 396.4440631866455 and batch: 900, loss is 4.222567958831787 and perplexity is 68.2084159888282
At time: 397.1611568927765 and batch: 950, loss is 4.195529308319092 and perplexity is 66.38886245613449
At time: 397.8769702911377 and batch: 1000, loss is 4.196804838180542 and perplexity is 66.47359746219179
At time: 398.59557247161865 and batch: 1050, loss is 4.151777591705322 and perplexity is 63.546860330699324
At time: 399.31642627716064 and batch: 1100, loss is 4.114823093414307 and perplexity is 61.2413793010769
At time: 400.0353379249573 and batch: 1150, loss is 4.133726148605347 and perplexity is 62.410039297055235
At time: 400.80842447280884 and batch: 1200, loss is 4.121294236183166 and perplexity is 61.63896604274628
At time: 401.529643535614 and batch: 1250, loss is 4.168191180229187 and perplexity is 64.59849932329563
At time: 402.24982476234436 and batch: 1300, loss is 4.160548043251038 and perplexity is 64.10664618922515
At time: 402.96464800834656 and batch: 1350, loss is 4.138801994323731 and perplexity is 62.72762836248537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.602174072265625 and perplexity of 99.7008370147074
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 405.4603579044342 and batch: 50, loss is 4.3094774436950685 and perplexity is 74.40159975528685
At time: 406.18136501312256 and batch: 100, loss is 4.311369695663452 and perplexity is 74.54251961476709
At time: 406.8983066082001 and batch: 150, loss is 4.287281885147094 and perplexity is 72.76840655733602
At time: 407.61548686027527 and batch: 200, loss is 4.28829752445221 and perplexity is 72.84235055506525
At time: 408.33349657058716 and batch: 250, loss is 4.273831748962403 and perplexity is 71.796214294616
At time: 409.05907583236694 and batch: 300, loss is 4.266783370971679 and perplexity is 71.2919466600638
At time: 409.77524733543396 and batch: 350, loss is 4.275312433242798 and perplexity is 71.90260056331475
At time: 410.4914367198944 and batch: 400, loss is 4.278058338165283 and perplexity is 72.10030958900744
At time: 411.2122142314911 and batch: 450, loss is 4.228021378517151 and perplexity is 68.5814012053757
At time: 411.9295027256012 and batch: 500, loss is 4.292012434005738 and perplexity is 73.11345655439284
At time: 412.64543175697327 and batch: 550, loss is 4.289233922958374 and perplexity is 72.91059196890309
At time: 413.36219668388367 and batch: 600, loss is 4.236769170761108 and perplexity is 69.1839687803372
At time: 414.07804226875305 and batch: 650, loss is 4.241914286613464 and perplexity is 69.54084561388822
At time: 414.79519152641296 and batch: 700, loss is 4.267825546264649 and perplexity is 71.36628409505131
At time: 415.51319551467896 and batch: 750, loss is 4.2192626523971555 and perplexity is 67.98333845259394
At time: 416.22846031188965 and batch: 800, loss is 4.199149932861328 and perplexity is 66.62966726972574
At time: 416.94209122657776 and batch: 850, loss is 4.164233021736145 and perplexity is 64.34331359051562
At time: 417.6613280773163 and batch: 900, loss is 4.201904945373535 and perplexity is 66.81348593182832
At time: 418.38005661964417 and batch: 950, loss is 4.175323376655578 and perplexity is 65.06087542790057
At time: 419.1509499549866 and batch: 1000, loss is 4.176247782707215 and perplexity is 65.12104590156109
At time: 419.8665645122528 and batch: 1050, loss is 4.130962800979614 and perplexity is 62.23781672814753
At time: 420.58562874794006 and batch: 1100, loss is 4.091247925758362 and perplexity is 59.814489190185704
At time: 421.3029088973999 and batch: 1150, loss is 4.10651349067688 and perplexity is 60.734596266926275
At time: 422.0254895687103 and batch: 1200, loss is 4.093211650848389 and perplexity is 59.932063807686035
At time: 422.7425284385681 and batch: 1250, loss is 4.137573490142822 and perplexity is 62.65061452438053
At time: 423.4601697921753 and batch: 1300, loss is 4.132602424621582 and perplexity is 62.339947028626746
At time: 424.1759285926819 and batch: 1350, loss is 4.114146876335144 and perplexity is 61.19998083319505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.591323649088542 and perplexity of 98.62488854616991
Finished 20 epochs...
Completing Train Step...
At time: 426.6690330505371 and batch: 50, loss is 4.303925609588623 and perplexity is 73.98967893227618
At time: 427.4156107902527 and batch: 100, loss is 4.303209686279297 and perplexity is 73.93672695351867
At time: 428.134884595871 and batch: 150, loss is 4.278432731628418 and perplexity is 72.12730852740522
At time: 428.86013555526733 and batch: 200, loss is 4.279359216690064 and perplexity is 72.19416436697861
At time: 429.5778203010559 and batch: 250, loss is 4.265120310783386 and perplexity is 71.17348239573212
At time: 430.3010079860687 and batch: 300, loss is 4.2581832981109615 and perplexity is 70.68145960368189
At time: 431.02213883399963 and batch: 350, loss is 4.267978782653809 and perplexity is 71.37722084466641
At time: 431.7394070625305 and batch: 400, loss is 4.27083381652832 and perplexity is 71.58129641075146
At time: 432.45801401138306 and batch: 450, loss is 4.221003828048706 and perplexity is 68.10181249834919
At time: 433.17757201194763 and batch: 500, loss is 4.285519437789917 and perplexity is 72.64026902251146
At time: 433.8985798358917 and batch: 550, loss is 4.283643789291382 and perplexity is 72.50414910744985
At time: 434.615350484848 and batch: 600, loss is 4.231358232498169 and perplexity is 68.81062956509699
At time: 435.33197116851807 and batch: 650, loss is 4.237443723678589 and perplexity is 69.23065277196206
At time: 436.051876783371 and batch: 700, loss is 4.263882455825805 and perplexity is 71.08543445423359
At time: 436.7686457633972 and batch: 750, loss is 4.216240382194519 and perplexity is 67.77818460585013
At time: 437.5394229888916 and batch: 800, loss is 4.196789655685425 and perplexity is 66.47258823478423
At time: 438.2580680847168 and batch: 850, loss is 4.162416439056397 and perplexity is 64.22653474281945
At time: 438.97623920440674 and batch: 900, loss is 4.200658745765686 and perplexity is 66.73027485143795
At time: 439.69329500198364 and batch: 950, loss is 4.174380679130554 and perplexity is 64.99957160168132
At time: 440.40991401672363 and batch: 1000, loss is 4.176404142379761 and perplexity is 65.1312290030666
At time: 441.13511395454407 and batch: 1050, loss is 4.132042407989502 and perplexity is 62.30504539510752
At time: 441.8722126483917 and batch: 1100, loss is 4.09276029586792 and perplexity is 59.905019275997816
At time: 442.6096897125244 and batch: 1150, loss is 4.108833765983581 and perplexity is 60.87568086516545
At time: 443.33074712753296 and batch: 1200, loss is 4.096176571846009 and perplexity is 60.1100213267275
At time: 444.0493540763855 and batch: 1250, loss is 4.1410798788070675 and perplexity is 62.87067751667918
At time: 444.7759349346161 and batch: 1300, loss is 4.136330132484436 and perplexity is 62.5727658099358
At time: 445.4929885864258 and batch: 1350, loss is 4.11694947719574 and perplexity is 61.37174052665408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.590279541015625 and perplexity of 98.52196724367978
Finished 21 epochs...
Completing Train Step...
At time: 448.0256390571594 and batch: 50, loss is 4.301745738983154 and perplexity is 73.82856667180229
At time: 448.79851150512695 and batch: 100, loss is 4.299925508499146 and perplexity is 73.69430389582487
At time: 449.5089795589447 and batch: 150, loss is 4.274921607971192 and perplexity is 71.87450470056463
At time: 450.22268533706665 and batch: 200, loss is 4.275665755271912 and perplexity is 71.92800982460591
At time: 450.9403760433197 and batch: 250, loss is 4.2613190746307374 and perplexity is 70.90344873734378
At time: 451.66705560684204 and batch: 300, loss is 4.254429302215576 and perplexity is 70.41661911053357
At time: 452.3920874595642 and batch: 350, loss is 4.264569549560547 and perplexity is 71.13429359436084
At time: 453.1059715747833 and batch: 400, loss is 4.267366876602173 and perplexity is 71.33355805140873
At time: 453.82152700424194 and batch: 450, loss is 4.217824964523316 and perplexity is 67.88566985657856
At time: 454.5345549583435 and batch: 500, loss is 4.282439088821411 and perplexity is 72.41685591657667
At time: 455.24863934516907 and batch: 550, loss is 4.281009874343872 and perplexity is 72.31343062375096
At time: 455.96337628364563 and batch: 600, loss is 4.228916349411011 and perplexity is 68.64280703743215
At time: 456.7268183231354 and batch: 650, loss is 4.2354175472259525 and perplexity is 69.09052126703386
At time: 457.4436249732971 and batch: 700, loss is 4.262061281204224 and perplexity is 70.95609327722455
At time: 458.1570258140564 and batch: 750, loss is 4.214887180328369 and perplexity is 67.68652906816588
At time: 458.86924600601196 and batch: 800, loss is 4.195974216461182 and perplexity is 66.41840597317365
At time: 459.5851399898529 and batch: 850, loss is 4.161870498657226 and perplexity is 64.19148045245191
At time: 460.29975986480713 and batch: 900, loss is 4.200292792320251 and perplexity is 66.70585914521963
At time: 461.01367115974426 and batch: 950, loss is 4.174164533615112 and perplexity is 64.98552375401833
At time: 461.7257673740387 and batch: 1000, loss is 4.176871900558472 and perplexity is 65.16170179451535
At time: 462.44160318374634 and batch: 1050, loss is 4.132848348617554 and perplexity is 62.35527980277996
At time: 463.15443897247314 and batch: 1100, loss is 4.093754096031189 and perplexity is 59.964582485996615
At time: 463.8655939102173 and batch: 1150, loss is 4.11019679069519 and perplexity is 60.958712496748625
At time: 464.57726979255676 and batch: 1200, loss is 4.097867674827576 and perplexity is 60.211759563686975
At time: 465.2956368923187 and batch: 1250, loss is 4.142956910133361 and perplexity is 62.98879857166276
At time: 466.00739765167236 and batch: 1300, loss is 4.13819787979126 and perplexity is 62.68974513466054
At time: 466.7203085422516 and batch: 1350, loss is 4.118182291984558 and perplexity is 61.44744717253875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.58980224609375 and perplexity of 98.47495442940254
Finished 22 epochs...
Completing Train Step...
At time: 469.2404248714447 and batch: 50, loss is 4.299822483062744 and perplexity is 73.68671189909679
At time: 469.9559483528137 and batch: 100, loss is 4.297415246963501 and perplexity is 73.50954391431726
At time: 470.67491030693054 and batch: 150, loss is 4.2723366355896 and perplexity is 71.68895101986676
At time: 471.39047384262085 and batch: 200, loss is 4.273064894676208 and perplexity is 71.74117816499466
At time: 472.1113259792328 and batch: 250, loss is 4.258629932403564 and perplexity is 70.713035418288
At time: 472.83275151252747 and batch: 300, loss is 4.2517506980896 and perplexity is 70.22825325545092
At time: 473.5513484477997 and batch: 350, loss is 4.262159271240234 and perplexity is 70.96304660803298
At time: 474.26890230178833 and batch: 400, loss is 4.264989175796509 and perplexity is 71.16414967398869
At time: 475.04376101493835 and batch: 450, loss is 4.215667300224304 and perplexity is 67.7393532781083
At time: 475.76395559310913 and batch: 500, loss is 4.280221290588379 and perplexity is 72.25642790571983
At time: 476.4796049594879 and batch: 550, loss is 4.279149436950684 and perplexity is 72.17902108242565
At time: 477.19465494155884 and batch: 600, loss is 4.227387170791626 and perplexity is 68.53792014037502
At time: 477.91131138801575 and batch: 650, loss is 4.233994722366333 and perplexity is 68.99228745713674
At time: 478.62763571739197 and batch: 700, loss is 4.260865507125854 and perplexity is 70.87129652916198
At time: 479.34550046920776 and batch: 750, loss is 4.2139350700378415 and perplexity is 67.62211469696886
At time: 480.0620276927948 and batch: 800, loss is 4.195405979156494 and perplexity is 66.38067527819122
At time: 480.777911901474 and batch: 850, loss is 4.161468410491944 and perplexity is 64.16567500622996
At time: 481.494145154953 and batch: 900, loss is 4.20001329421997 and perplexity is 66.68721758957199
At time: 482.2111279964447 and batch: 950, loss is 4.173991136550903 and perplexity is 64.97425643187181
At time: 482.928781747818 and batch: 1000, loss is 4.17718026638031 and perplexity is 65.18179853465622
At time: 483.64426374435425 and batch: 1050, loss is 4.133398079872132 and perplexity is 62.389567872723866
At time: 484.36221385002136 and batch: 1100, loss is 4.094405903816223 and perplexity is 60.00368060853337
At time: 485.07934856414795 and batch: 1150, loss is 4.111051940917969 and perplexity is 61.01086364867702
At time: 485.79485392570496 and batch: 1200, loss is 4.0989406251907345 and perplexity is 60.27639846394185
At time: 486.5114405155182 and batch: 1250, loss is 4.144107303619385 and perplexity is 63.0613021710703
At time: 487.23117566108704 and batch: 1300, loss is 4.13932728767395 and perplexity is 62.760587424369355
At time: 487.9550259113312 and batch: 1350, loss is 4.118838930130005 and perplexity is 61.48780916045277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589536946614583 and perplexity of 98.44883254049635
Finished 23 epochs...
Completing Train Step...
At time: 490.48329305648804 and batch: 50, loss is 4.298118505477905 and perplexity is 73.56125830912784
At time: 491.2043514251709 and batch: 100, loss is 4.295353384017944 and perplexity is 73.35813345702194
At time: 491.92541241645813 and batch: 150, loss is 4.270209522247314 and perplexity is 71.53662256304
At time: 492.6483759880066 and batch: 200, loss is 4.270925917625427 and perplexity is 71.58788943029074
At time: 493.4210605621338 and batch: 250, loss is 4.256461677551269 and perplexity is 70.55987763867496
At time: 494.1371006965637 and batch: 300, loss is 4.249611873626709 and perplexity is 70.07820786697518
At time: 494.8529567718506 and batch: 350, loss is 4.260248880386353 and perplexity is 70.82760886353468
At time: 495.5741560459137 and batch: 400, loss is 4.263072824478149 and perplexity is 71.02790475020616
At time: 496.29304933547974 and batch: 450, loss is 4.213931288719177 and perplexity is 67.62185899668788
At time: 497.01024436950684 and batch: 500, loss is 4.278445119857788 and perplexity is 72.12820206258176
At time: 497.7261848449707 and batch: 550, loss is 4.277679090499878 and perplexity is 72.0729708993156
At time: 498.44368386268616 and batch: 600, loss is 4.226139488220215 and perplexity is 68.45245989664984
At time: 499.16041135787964 and batch: 650, loss is 4.232956886291504 and perplexity is 68.92072191540282
At time: 499.8769838809967 and batch: 700, loss is 4.259931688308716 and perplexity is 70.80514646976017
At time: 500.59494042396545 and batch: 750, loss is 4.213120193481445 and perplexity is 67.56703346625808
At time: 501.3135838508606 and batch: 800, loss is 4.194881429672241 and perplexity is 66.34586446000628
At time: 502.03314685821533 and batch: 850, loss is 4.161107358932495 and perplexity is 64.14251207096443
At time: 502.75146222114563 and batch: 900, loss is 4.199714207649231 and perplexity is 66.66727532073206
At time: 503.47022247314453 and batch: 950, loss is 4.173702263832093 and perplexity is 64.95548985247055
At time: 504.18672800064087 and batch: 1000, loss is 4.1772839450836186 and perplexity is 65.18855684934825
At time: 504.9007706642151 and batch: 1050, loss is 4.133726873397827 and perplexity is 62.4100845313988
At time: 505.621386051178 and batch: 1100, loss is 4.094788842201233 and perplexity is 60.02666272116595
At time: 506.3399863243103 and batch: 1150, loss is 4.111569604873657 and perplexity is 61.04245494982682
At time: 507.0636637210846 and batch: 1200, loss is 4.099651045799256 and perplexity is 60.31923527389273
At time: 507.78517150878906 and batch: 1250, loss is 4.144837021827698 and perplexity is 63.10733594530338
At time: 508.503826379776 and batch: 1300, loss is 4.140085062980652 and perplexity is 62.80816387160545
At time: 509.22485184669495 and batch: 1350, loss is 4.119221382141113 and perplexity is 61.511329794194936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589369303385417 and perplexity of 98.43232964363973
Finished 24 epochs...
Completing Train Step...
At time: 511.76509523391724 and batch: 50, loss is 4.296562442779541 and perplexity is 73.44688139093492
At time: 512.523209810257 and batch: 100, loss is 4.293541049957275 and perplexity is 73.22530441478644
At time: 513.2390539646149 and batch: 150, loss is 4.268359603881836 and perplexity is 71.40440798195262
At time: 513.9672439098358 and batch: 200, loss is 4.269104509353638 and perplexity is 71.45761733167659
At time: 514.6854083538055 and batch: 250, loss is 4.254639525413513 and perplexity is 70.43142387348865
At time: 515.4012837409973 and batch: 300, loss is 4.247815847396851 and perplexity is 69.95245852586473
At time: 516.1176037788391 and batch: 350, loss is 4.258614797592163 and perplexity is 70.71196519793213
At time: 516.8338541984558 and batch: 400, loss is 4.261469688415527 and perplexity is 70.9141285783582
At time: 517.5510613918304 and batch: 450, loss is 4.2124561595916745 and perplexity is 67.5221815594464
At time: 518.2731132507324 and batch: 500, loss is 4.276931943893433 and perplexity is 72.01914193525977
At time: 518.989536523819 and batch: 550, loss is 4.276434421539307 and perplexity is 71.98331971413879
At time: 519.7034571170807 and batch: 600, loss is 4.225101709365845 and perplexity is 68.3814582296261
At time: 520.438558101654 and batch: 650, loss is 4.231963343620301 and perplexity is 68.85228024274093
At time: 521.1987082958221 and batch: 700, loss is 4.259087104797363 and perplexity is 70.74537085683468
At time: 521.9573538303375 and batch: 750, loss is 4.212386112213135 and perplexity is 67.51745197328438
At time: 522.7270588874817 and batch: 800, loss is 4.194408693313599 and perplexity is 66.31450776993816
At time: 523.4765195846558 and batch: 850, loss is 4.160760083198547 and perplexity is 64.12024080036751
At time: 524.2274265289307 and batch: 900, loss is 4.199375681877136 and perplexity is 66.64471054947441
At time: 524.9522023200989 and batch: 950, loss is 4.173400321006775 and perplexity is 64.9358799690254
At time: 525.6754412651062 and batch: 1000, loss is 4.177289652824402 and perplexity is 65.18892892979463
At time: 526.4245698451996 and batch: 1050, loss is 4.133869037628174 and perplexity is 62.418957643736746
At time: 527.1562743186951 and batch: 1100, loss is 4.0949931335449214 and perplexity is 60.038926901440696
At time: 527.8792858123779 and batch: 1150, loss is 4.111957983970642 and perplexity is 61.06616716772481
At time: 528.5957775115967 and batch: 1200, loss is 4.100100889205932 and perplexity is 60.346375588164506
At time: 529.3176784515381 and batch: 1250, loss is 4.145362010002136 and perplexity is 63.14047524850501
At time: 530.0418200492859 and batch: 1300, loss is 4.14056291103363 and perplexity is 62.83818380233413
At time: 530.7583634853363 and batch: 1350, loss is 4.11939766407013 and perplexity is 61.522174085865686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589272867838542 and perplexity of 98.42283772578686
Finished 25 epochs...
Completing Train Step...
At time: 533.251583814621 and batch: 50, loss is 4.29509729385376 and perplexity is 73.33934956586823
At time: 534.024754524231 and batch: 100, loss is 4.291872644424439 and perplexity is 73.10323676924018
At time: 534.7496771812439 and batch: 150, loss is 4.266681051254272 and perplexity is 71.28465246140466
At time: 535.4703779220581 and batch: 200, loss is 4.267424287796021 and perplexity is 71.33765351369944
At time: 536.1910691261292 and batch: 250, loss is 4.25295774936676 and perplexity is 70.31307353915312
At time: 536.9106240272522 and batch: 300, loss is 4.246168117523194 and perplexity is 69.83729067902365
At time: 537.6306807994843 and batch: 350, loss is 4.2571311378479 and perplexity is 70.6071304904785
At time: 538.3509809970856 and batch: 400, loss is 4.2600186252593994 and perplexity is 70.81130232086855
At time: 539.072939157486 and batch: 450, loss is 4.211109004020691 and perplexity is 67.43127991946653
At time: 539.7909767627716 and batch: 500, loss is 4.275551109313965 and perplexity is 71.91976404169824
At time: 540.5114223957062 and batch: 550, loss is 4.275382175445556 and perplexity is 71.9076153839324
At time: 541.2292191982269 and batch: 600, loss is 4.2241450119018555 and perplexity is 68.31606914572669
At time: 541.9479479789734 and batch: 650, loss is 4.231146111488342 and perplexity is 68.79603493283558
At time: 542.6642413139343 and batch: 700, loss is 4.258329048156738 and perplexity is 70.69176218043695
At time: 543.3812148571014 and batch: 750, loss is 4.211664667129517 and perplexity is 67.46875940611838
At time: 544.0987544059753 and batch: 800, loss is 4.193903121948242 and perplexity is 66.28098952734776
At time: 544.8169083595276 and batch: 850, loss is 4.160379185676574 and perplexity is 64.09582221032379
At time: 545.5362777709961 and batch: 900, loss is 4.198990244865417 and perplexity is 66.6190281611828
At time: 546.2585241794586 and batch: 950, loss is 4.173032264709473 and perplexity is 64.9119843072261
At time: 546.9820311069489 and batch: 1000, loss is 4.177252335548401 and perplexity is 65.18649630193136
At time: 547.7094941139221 and batch: 1050, loss is 4.134005494117737 and perplexity is 62.427475696737574
At time: 548.4295706748962 and batch: 1100, loss is 4.095336604118347 and perplexity is 60.05955204795407
At time: 549.2004125118256 and batch: 1150, loss is 4.112027974128723 and perplexity is 61.070441347991796
At time: 549.9179944992065 and batch: 1200, loss is 4.100450015068054 and perplexity is 60.36744774677137
At time: 550.6344957351685 and batch: 1250, loss is 4.145686883926391 and perplexity is 63.16099127485885
At time: 551.3508121967316 and batch: 1300, loss is 4.1409873247146605 and perplexity is 62.86485884745759
At time: 552.069109916687 and batch: 1350, loss is 4.119467105865478 and perplexity is 61.52644644442636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589151611328125 and perplexity of 98.41090403947214
Finished 26 epochs...
Completing Train Step...
At time: 554.6146218776703 and batch: 50, loss is 4.293736562728882 and perplexity is 73.23962229662311
At time: 555.3313541412354 and batch: 100, loss is 4.290388460159302 and perplexity is 72.99481857169012
At time: 556.0470590591431 and batch: 150, loss is 4.2651940441131595 and perplexity is 71.17873044705651
At time: 556.7637689113617 and batch: 200, loss is 4.265923495292664 and perplexity is 71.23067079760858
At time: 557.4834494590759 and batch: 250, loss is 4.251496720314026 and perplexity is 70.21041910474148
At time: 558.19912981987 and batch: 300, loss is 4.2447262954711915 and perplexity is 69.73667028905034
At time: 558.9152128696442 and batch: 350, loss is 4.255837812423706 and perplexity is 70.51587151997977
At time: 559.6309583187103 and batch: 400, loss is 4.258748874664307 and perplexity is 70.72144668680238
At time: 560.3463411331177 and batch: 450, loss is 4.209920120239258 and perplexity is 67.35115960071992
At time: 561.066766500473 and batch: 500, loss is 4.274321908950806 and perplexity is 71.83141455235624
At time: 561.7925815582275 and batch: 550, loss is 4.2743188095092775 and perplexity is 71.83119191543196
At time: 562.5110104084015 and batch: 600, loss is 4.22332654953003 and perplexity is 68.26017788930504
At time: 563.22975730896 and batch: 650, loss is 4.2303345680236815 and perplexity is 68.74022660879456
At time: 563.943609714508 and batch: 700, loss is 4.257624082565307 and perplexity is 70.64194448244679
At time: 564.6582410335541 and batch: 750, loss is 4.21101288318634 and perplexity is 67.42479868007541
At time: 565.379830121994 and batch: 800, loss is 4.193443126678467 and perplexity is 66.25050759699853
At time: 566.0988957881927 and batch: 850, loss is 4.160014634132385 and perplexity is 64.07246023794121
At time: 566.8147532939911 and batch: 900, loss is 4.198614301681519 and perplexity is 66.59398789878058
At time: 567.5867507457733 and batch: 950, loss is 4.172637581825256 and perplexity is 64.88636971319293
At time: 568.3089370727539 and batch: 1000, loss is 4.1771533489227295 and perplexity is 65.18004402997268
At time: 569.0263710021973 and batch: 1050, loss is 4.13393201828003 and perplexity is 62.42288895417423
At time: 569.743896484375 and batch: 1100, loss is 4.095300679206848 and perplexity is 60.05739445261802
At time: 570.4754674434662 and batch: 1150, loss is 4.112127599716186 and perplexity is 61.07652582966772
At time: 571.1945576667786 and batch: 1200, loss is 4.100614490509034 and perplexity is 60.37737752594036
At time: 571.9115681648254 and batch: 1250, loss is 4.145922718048095 and perplexity is 63.17588854833578
At time: 572.6322898864746 and batch: 1300, loss is 4.141235389709473 and perplexity is 62.88045535273498
At time: 573.342271566391 and batch: 1350, loss is 4.119419784545898 and perplexity is 61.52353500067876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589094645182292 and perplexity of 98.40529810923667
Finished 27 epochs...
Completing Train Step...
At time: 575.7963576316833 and batch: 50, loss is 4.292447080612183 and perplexity is 73.1452419773824
At time: 576.532276391983 and batch: 100, loss is 4.288972654342651 and perplexity is 72.89154520773667
At time: 577.2411980628967 and batch: 150, loss is 4.263827047348022 and perplexity is 71.08149582763562
At time: 577.9449875354767 and batch: 200, loss is 4.264543137550354 and perplexity is 71.13241481948457
At time: 578.6638765335083 and batch: 250, loss is 4.25010766506195 and perplexity is 70.11296065658854
At time: 579.3821721076965 and batch: 300, loss is 4.24336669921875 and perplexity is 69.64192099845319
At time: 580.1010026931763 and batch: 350, loss is 4.254616470336914 and perplexity is 70.42980009033458
At time: 580.8182125091553 and batch: 400, loss is 4.257561340332031 and perplexity is 70.63751238812817
At time: 581.5348949432373 and batch: 450, loss is 4.208802752494812 and perplexity is 67.27594561609665
At time: 582.2519052028656 and batch: 500, loss is 4.2731580066680905 and perplexity is 71.74785843999561
At time: 582.9700043201447 and batch: 550, loss is 4.273428831100464 and perplexity is 71.76729214447384
At time: 583.687135219574 and batch: 600, loss is 4.222536163330078 and perplexity is 68.20624730249847
At time: 584.4047713279724 and batch: 650, loss is 4.229643650054932 and perplexity is 68.69274915445646
At time: 585.1204946041107 and batch: 700, loss is 4.256967754364013 and perplexity is 70.5955953938595
At time: 585.8369519710541 and batch: 750, loss is 4.210397810935974 and perplexity is 67.38334030867468
At time: 586.6205811500549 and batch: 800, loss is 4.19296293258667 and perplexity is 66.2187021316813
At time: 587.3381102085114 and batch: 850, loss is 4.159674296379089 and perplexity is 64.05065767110445
At time: 588.0548467636108 and batch: 900, loss is 4.198175239562988 and perplexity is 66.56475541927914
At time: 588.7696783542633 and batch: 950, loss is 4.172219843864441 and perplexity is 64.85926987413406
At time: 589.4858455657959 and batch: 1000, loss is 4.176936635971069 and perplexity is 65.16592020070523
At time: 590.2023310661316 and batch: 1050, loss is 4.1338610315322875 and perplexity is 62.41845791357715
At time: 590.9173812866211 and batch: 1100, loss is 4.095204339027405 and perplexity is 60.051608791159886
At time: 591.6342256069183 and batch: 1150, loss is 4.112021899223327 and perplexity is 61.07007035196502
At time: 592.3566167354584 and batch: 1200, loss is 4.100742974281311 and perplexity is 60.38513553754374
At time: 593.0710544586182 and batch: 1250, loss is 4.146078844070434 and perplexity is 63.18575271852934
At time: 593.7863101959229 and batch: 1300, loss is 4.141455173492432 and perplexity is 62.894276975915254
At time: 594.5045635700226 and batch: 1350, loss is 4.11936282157898 and perplexity is 61.520030537403066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.589049072265625 and perplexity of 98.40081359497331
Finished 28 epochs...
Completing Train Step...
At time: 596.9853711128235 and batch: 50, loss is 4.291210041046143 and perplexity is 73.05481436178952
At time: 597.7297921180725 and batch: 100, loss is 4.287642517089844 and perplexity is 72.7946539016945
At time: 598.4453935623169 and batch: 150, loss is 4.262523899078369 and perplexity is 70.98892642826983
At time: 599.1606059074402 and batch: 200, loss is 4.263246603012085 and perplexity is 71.04024894790943
At time: 599.8730075359344 and batch: 250, loss is 4.248841910362244 and perplexity is 70.02427098865022
At time: 600.5838973522186 and batch: 300, loss is 4.242111310958863 and perplexity is 69.5545482033013
At time: 601.2925834655762 and batch: 350, loss is 4.2535178661346436 and perplexity is 70.35246810239161
At time: 602.0047626495361 and batch: 400, loss is 4.256443920135498 and perplexity is 70.55862468871561
At time: 602.7172791957855 and batch: 450, loss is 4.20775731086731 and perplexity is 67.20564929377335
At time: 603.4300963878632 and batch: 500, loss is 4.272062349319458 and perplexity is 71.66929042132053
At time: 604.1392793655396 and batch: 550, loss is 4.2724983024597165 and perplexity is 71.70054168508814
At time: 604.9049470424652 and batch: 600, loss is 4.2218128490448 and perplexity is 68.15693058737176
At time: 605.6135971546173 and batch: 650, loss is 4.228869795799255 and perplexity is 68.63961154122504
At time: 606.3289260864258 and batch: 700, loss is 4.256310901641846 and perplexity is 70.54923971094695
At time: 607.0389778614044 and batch: 750, loss is 4.209775733947754 and perplexity is 67.34143571857119
At time: 607.7513289451599 and batch: 800, loss is 4.1924804592132565 and perplexity is 66.18676107705458
At time: 608.4692368507385 and batch: 850, loss is 4.1592791509628295 and perplexity is 64.02535334709037
At time: 609.1837089061737 and batch: 900, loss is 4.197743120193482 and perplexity is 66.53599771296446
At time: 609.9012496471405 and batch: 950, loss is 4.171801490783691 and perplexity is 64.83214147378766
At time: 610.6163001060486 and batch: 1000, loss is 4.176733503341675 and perplexity is 65.1526842203632
At time: 611.326269865036 and batch: 1050, loss is 4.1337211179733275 and perplexity is 62.40972533590293
At time: 612.0416181087494 and batch: 1100, loss is 4.095072159767151 and perplexity is 60.04367173850122
At time: 612.7522101402283 and batch: 1150, loss is 4.111967144012451 and perplexity is 61.066726538931086
At time: 613.4656176567078 and batch: 1200, loss is 4.100796914100647 and perplexity is 60.388392788692165
At time: 614.1912162303925 and batch: 1250, loss is 4.146160869598389 and perplexity is 63.1909357758239
At time: 614.9087290763855 and batch: 1300, loss is 4.1415569829940795 and perplexity is 62.90068053687682
At time: 615.6418912410736 and batch: 1350, loss is 4.119221353530884 and perplexity is 61.51132803434174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.58900390625 and perplexity of 98.39636932265482
Finished 29 epochs...
Completing Train Step...
At time: 618.2484023571014 and batch: 50, loss is 4.290009317398071 and perplexity is 72.9671483604329
At time: 619.0361869335175 and batch: 100, loss is 4.286366653442383 and perplexity is 72.70183707245181
At time: 619.7601048946381 and batch: 150, loss is 4.261304187774658 and perplexity is 70.90239321576361
At time: 620.4792635440826 and batch: 200, loss is 4.2620042705535885 and perplexity is 70.95204813948936
At time: 621.2009515762329 and batch: 250, loss is 4.247614169120789 and perplexity is 69.93835205715492
At time: 621.917325258255 and batch: 300, loss is 4.240900726318359 and perplexity is 69.47039748163179
At time: 622.6353387832642 and batch: 350, loss is 4.252457914352417 and perplexity is 70.27793738490375
At time: 623.3542139530182 and batch: 400, loss is 4.25538215637207 and perplexity is 70.48374785561215
At time: 624.1251208782196 and batch: 450, loss is 4.206744828224182 and perplexity is 67.13763917568524
At time: 624.8432059288025 and batch: 500, loss is 4.271026277542115 and perplexity is 71.5950743454425
At time: 625.5677824020386 and batch: 550, loss is 4.2716959285736085 and perplexity is 71.6430341171909
At time: 626.2848613262177 and batch: 600, loss is 4.221120109558106 and perplexity is 68.1097319403328
At time: 626.9997143745422 and batch: 650, loss is 4.2282554626464846 and perplexity is 68.5974569020839
At time: 627.7155020236969 and batch: 700, loss is 4.255726251602173 and perplexity is 70.50800515021939
At time: 628.4309883117676 and batch: 750, loss is 4.209182858467102 and perplexity is 67.30152246546547
At time: 629.1511306762695 and batch: 800, loss is 4.19197021484375 and perplexity is 66.15299826925296
At time: 629.8665204048157 and batch: 850, loss is 4.158842158317566 and perplexity is 63.997380850899994
At time: 630.5819683074951 and batch: 900, loss is 4.197291212081909 and perplexity is 66.50593634888513
At time: 631.3001642227173 and batch: 950, loss is 4.171350522041321 and perplexity is 64.80291079603842
At time: 632.0172877311707 and batch: 1000, loss is 4.176485953330993 and perplexity is 65.1365576688349
At time: 632.7325539588928 and batch: 1050, loss is 4.133539252281189 and perplexity is 62.39837618005084
At time: 633.4499173164368 and batch: 1100, loss is 4.094899878501892 and perplexity is 60.033328229785326
At time: 634.1689641475677 and batch: 1150, loss is 4.111815299987793 and perplexity is 61.057454625360876
At time: 634.8857858181 and batch: 1200, loss is 4.100801992416382 and perplexity is 60.38869946079617
At time: 635.6038408279419 and batch: 1250, loss is 4.146186738014221 and perplexity is 63.192570446370475
At time: 636.3205327987671 and batch: 1300, loss is 4.141625714302063 and perplexity is 62.90500393149777
At time: 637.0381605625153 and batch: 1350, loss is 4.11906735420227 and perplexity is 61.50185606047984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.588981119791667 and perplexity of 98.3941272434297
Finished 30 epochs...
Completing Train Step...
At time: 639.62393450737 and batch: 50, loss is 4.288827810287476 and perplexity is 72.88098806532888
At time: 640.3401787281036 and batch: 100, loss is 4.285145854949951 and perplexity is 72.61313693287123
At time: 641.0588576793671 and batch: 150, loss is 4.260134620666504 and perplexity is 70.819516583108
At time: 641.7762460708618 and batch: 200, loss is 4.26083125591278 and perplexity is 70.86886914285445
At time: 642.5475234985352 and batch: 250, loss is 4.246480002403259 and perplexity is 69.85907527101699
At time: 643.2675490379333 and batch: 300, loss is 4.239747133255005 and perplexity is 69.3903031200208
At time: 643.9878740310669 and batch: 350, loss is 4.251426458358765 and perplexity is 70.20548615671679
At time: 644.7046430110931 and batch: 400, loss is 4.254371385574341 and perplexity is 70.41254093456584
At time: 645.4223289489746 and batch: 450, loss is 4.205789403915405 and perplexity is 67.07352487624017
At time: 646.1410021781921 and batch: 500, loss is 4.270058584213257 and perplexity is 71.52582578071029
At time: 646.856431722641 and batch: 550, loss is 4.270869979858398 and perplexity is 71.58388507560807
At time: 647.5730257034302 and batch: 600, loss is 4.220520706176758 and perplexity is 68.0689189696433
At time: 648.2892105579376 and batch: 650, loss is 4.227578897476196 and perplexity is 68.5510619483482
At time: 649.0055496692657 and batch: 700, loss is 4.255103702545166 and perplexity is 70.46412411856636
At time: 649.7210164070129 and batch: 750, loss is 4.208596911430359 and perplexity is 67.26209888900037
At time: 650.4400224685669 and batch: 800, loss is 4.191486988067627 and perplexity is 66.12103909155552
At time: 651.1594617366791 and batch: 850, loss is 4.15843843460083 and perplexity is 63.97154880529711
At time: 651.8781940937042 and batch: 900, loss is 4.196841154098511 and perplexity is 66.47601155573908
At time: 652.5929532051086 and batch: 950, loss is 4.170878877639771 and perplexity is 64.77235407247755
At time: 653.3118124008179 and batch: 1000, loss is 4.176242814064026 and perplexity is 65.12072233912377
At time: 654.0278120040894 and batch: 1050, loss is 4.133343658447266 and perplexity is 62.3861726359311
At time: 654.7450788021088 and batch: 1100, loss is 4.094724411964417 and perplexity is 60.02279531366188
At time: 655.461727142334 and batch: 1150, loss is 4.111661772727967 and perplexity is 61.04808136120448
At time: 656.1847243309021 and batch: 1200, loss is 4.1007494497299195 and perplexity is 60.385526559651645
At time: 656.9067332744598 and batch: 1250, loss is 4.146176462173462 and perplexity is 63.191921092915706
At time: 657.6279864311218 and batch: 1300, loss is 4.1416461515426635 and perplexity is 62.90628954933527
At time: 658.349279165268 and batch: 1350, loss is 4.118863191604614 and perplexity is 61.489300963470065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.588951416015625 and perplexity of 98.39120460971711
Finished 31 epochs...
Completing Train Step...
At time: 660.8983662128448 and batch: 50, loss is 4.287691211700439 and perplexity is 72.79819869532514
At time: 661.6129019260406 and batch: 100, loss is 4.283960332870484 and perplexity is 72.5271034631433
At time: 662.3289525508881 and batch: 150, loss is 4.259019641876221 and perplexity is 70.74059832844559
At time: 663.0466783046722 and batch: 200, loss is 4.259680066108704 and perplexity is 70.78733256431431
At time: 663.7632691860199 and batch: 250, loss is 4.245383844375611 and perplexity is 69.78254063954336
At time: 664.480902671814 and batch: 300, loss is 4.238647661209106 and perplexity is 69.3140523469619
At time: 665.1975293159485 and batch: 350, loss is 4.250435657501221 and perplexity is 70.13596094933366
At time: 665.9140071868896 and batch: 400, loss is 4.25340256690979 and perplexity is 70.3443569849646
At time: 666.631749868393 and batch: 450, loss is 4.204857535362244 and perplexity is 67.0110502812345
At time: 667.3497807979584 and batch: 500, loss is 4.26908239364624 and perplexity is 71.45603701339532
At time: 668.0698268413544 and batch: 550, loss is 4.2701200675964355 and perplexity is 71.53022356565789
At time: 668.7890610694885 and batch: 600, loss is 4.219873971939087 and perplexity is 68.02491070158521
At time: 669.5054655075073 and batch: 650, loss is 4.226861276626587 and perplexity is 68.5018859240118
At time: 670.2200920581818 and batch: 700, loss is 4.254494161605835 and perplexity is 70.42118643762858
At time: 670.9368815422058 and batch: 750, loss is 4.208018879890442 and perplexity is 67.22323050905854
At time: 671.6515386104584 and batch: 800, loss is 4.191000876426696 and perplexity is 66.0889046958274
At time: 672.3673584461212 and batch: 850, loss is 4.158035359382629 and perplexity is 63.94576865532259
At time: 673.0839233398438 and batch: 900, loss is 4.1963951110839846 and perplexity is 66.44636700701449
At time: 673.7998554706573 and batch: 950, loss is 4.170422735214234 and perplexity is 64.74281539121823
At time: 674.515721321106 and batch: 1000, loss is 4.1759698915481565 and perplexity is 65.10295185284095
At time: 675.2332520484924 and batch: 1050, loss is 4.133112020492554 and perplexity is 62.37172330407088
At time: 675.9512257575989 and batch: 1100, loss is 4.094528436660767 and perplexity is 60.0110334806764
At time: 676.6721889972687 and batch: 1150, loss is 4.111498351097107 and perplexity is 61.03810559933742
At time: 677.3883543014526 and batch: 1200, loss is 4.100665483474732 and perplexity is 60.38045642598193
At time: 678.1126878261566 and batch: 1250, loss is 4.146142325401306 and perplexity is 63.18976396152224
At time: 678.8277959823608 and batch: 1300, loss is 4.141675343513489 and perplexity is 62.90812593470826
At time: 679.5485410690308 and batch: 1350, loss is 4.118656759262085 and perplexity is 61.47660889310367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.5889359537760415 and perplexity of 98.38968327310015
Finished 32 epochs...
Completing Train Step...
At time: 682.0456619262695 and batch: 50, loss is 4.286602277755737 and perplexity is 72.71896941121008
At time: 682.7935476303101 and batch: 100, loss is 4.282835311889649 and perplexity is 72.44555483063051
At time: 683.5117094516754 and batch: 150, loss is 4.2579194450378415 and perplexity is 70.6628125435031
At time: 684.2289447784424 and batch: 200, loss is 4.258600530624389 and perplexity is 70.71095635980001
At time: 684.9475448131561 and batch: 250, loss is 4.2443674421310424 and perplexity is 69.71164954164414
At time: 685.6639699935913 and batch: 300, loss is 4.237572908401489 and perplexity is 69.23959689236604
At time: 686.3805241584778 and batch: 350, loss is 4.249463415145874 and perplexity is 70.06780493491647
At time: 687.0966417789459 and batch: 400, loss is 4.252482442855835 and perplexity is 70.2796612186726
At time: 687.8134036064148 and batch: 450, loss is 4.203966221809387 and perplexity is 66.95134903414542
At time: 688.5384051799774 and batch: 500, loss is 4.268180313110352 and perplexity is 71.39160697814336
At time: 689.2749104499817 and batch: 550, loss is 4.269299325942993 and perplexity is 71.47153981709009
At time: 690.0049381256104 and batch: 600, loss is 4.219307641983033 and perplexity is 67.98639706363979
At time: 690.7277359962463 and batch: 650, loss is 4.226255879402161 and perplexity is 68.46042762304187
At time: 691.443909406662 and batch: 700, loss is 4.253912267684936 and perplexity is 70.38022069735159
At time: 692.1582074165344 and batch: 750, loss is 4.2074677276611325 and perplexity is 67.18619048398588
At time: 692.875725030899 and batch: 800, loss is 4.19047751903534 and perplexity is 66.05432562846268
At time: 693.5941889286041 and batch: 850, loss is 4.157617769241333 and perplexity is 63.91907110745886
At time: 694.3276970386505 and batch: 900, loss is 4.195928492546082 and perplexity is 66.41536913304647
At time: 695.045557975769 and batch: 950, loss is 4.16993471622467 and perplexity is 64.711227376272
At time: 695.7696850299835 and batch: 1000, loss is 4.175649528503418 and perplexity is 65.08209861344588
At time: 696.4878945350647 and batch: 1050, loss is 4.132883052825928 and perplexity is 62.35744383095372
At time: 697.2070498466492 and batch: 1100, loss is 4.094325513839721 and perplexity is 59.9988571079424
At time: 697.9858539104462 and batch: 1150, loss is 4.111297988891602 and perplexity is 61.02587709498564
At time: 698.7078015804291 and batch: 1200, loss is 4.100531435012817 and perplexity is 60.372363061132
At time: 699.425279378891 and batch: 1250, loss is 4.146068167686463 and perplexity is 63.18507812677289
At time: 700.1430857181549 and batch: 1300, loss is 4.141638121604919 and perplexity is 62.90578441777457
At time: 700.8592853546143 and batch: 1350, loss is 4.118401069641113 and perplexity is 61.46089197168961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.588892415364583 and perplexity of 98.38539963583861
Finished 33 epochs...
Completing Train Step...
At time: 703.3567633628845 and batch: 50, loss is 4.28552924156189 and perplexity is 72.6409811746359
At time: 704.108190536499 and batch: 100, loss is 4.2816829586029055 and perplexity is 72.36212003982209
At time: 704.8266599178314 and batch: 150, loss is 4.256860685348511 and perplexity is 70.5880371975934
At time: 705.5403320789337 and batch: 200, loss is 4.25753249168396 and perplexity is 70.63547462078621
At time: 706.2579028606415 and batch: 250, loss is 4.2434297466278075 and perplexity is 69.64631187954929
At time: 706.9742035865784 and batch: 300, loss is 4.236550998687744 and perplexity is 69.16887641685084
At time: 707.6929295063019 and batch: 350, loss is 4.248547763824463 and perplexity is 70.00367662080774
At time: 708.4098217487335 and batch: 400, loss is 4.25162052154541 and perplexity is 70.21911177915347
At time: 709.1413042545319 and batch: 450, loss is 4.2030863857269285 and perplexity is 66.89246872780326
At time: 709.855316400528 and batch: 500, loss is 4.2672608375549315 and perplexity is 71.32599430990966
At time: 710.5721180438995 and batch: 550, loss is 4.26856725692749 and perplexity is 71.41923686432116
At time: 711.289199590683 and batch: 600, loss is 4.21870683670044 and perplexity is 67.94556274510494
At time: 712.0074474811554 and batch: 650, loss is 4.225573163032532 and perplexity is 68.41370451956043
At time: 712.7219667434692 and batch: 700, loss is 4.253315191268921 and perplexity is 70.33821087020584
At time: 713.4400115013123 and batch: 750, loss is 4.206910409927368 and perplexity is 67.14875686074505
At time: 714.1566507816315 and batch: 800, loss is 4.190005493164063 and perplexity is 66.02315363542866
At time: 714.8731541633606 and batch: 850, loss is 4.157192163467407 and perplexity is 63.89187257006938
At time: 715.5957677364349 and batch: 900, loss is 4.195428471565247 and perplexity is 66.3821683562643
At time: 716.3690981864929 and batch: 950, loss is 4.169450855255127 and perplexity is 64.67992371297453
At time: 717.0881037712097 and batch: 1000, loss is 4.1753781080245975 and perplexity is 65.06443639612976
At time: 717.8084454536438 and batch: 1050, loss is 4.132614560127259 and perplexity is 62.34070355999824
At time: 718.53373503685 and batch: 1100, loss is 4.0940902090072635 and perplexity is 59.98474074781196
At time: 719.2535102367401 and batch: 1150, loss is 4.11110869884491 and perplexity is 61.01432659709253
At time: 719.9762721061707 and batch: 1200, loss is 4.100498862266541 and perplexity is 60.37039659949462
At time: 720.6956577301025 and batch: 1250, loss is 4.1459395122528075 and perplexity is 63.17694954605022
At time: 721.4107582569122 and batch: 1300, loss is 4.141587724685669 and perplexity is 62.902614239921164
At time: 722.1354131698608 and batch: 1350, loss is 4.118154129981995 and perplexity is 61.44571671374065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.5889054361979165 and perplexity of 98.38668070406997
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 724.6493706703186 and batch: 50, loss is 4.285469207763672 and perplexity is 72.6366203915282
At time: 725.3675808906555 and batch: 100, loss is 4.2824040794372555 and perplexity is 72.41432069142716
At time: 726.0903515815735 and batch: 150, loss is 4.257961845397949 and perplexity is 70.6658087357205
At time: 726.8064858913422 and batch: 200, loss is 4.259296736717224 and perplexity is 70.76020289932916
At time: 727.5229203701019 and batch: 250, loss is 4.244987964630127 and perplexity is 69.7549206125811
At time: 728.2386002540588 and batch: 300, loss is 4.238471517562866 and perplexity is 69.30184419227182
At time: 728.9562954902649 and batch: 350, loss is 4.250143013000488 and perplexity is 70.11543904901532
At time: 729.6738607883453 and batch: 400, loss is 4.252895269393921 and perplexity is 70.30868051746754
At time: 730.3906300067902 and batch: 450, loss is 4.203712587356567 and perplexity is 66.93437001869026
At time: 731.1085398197174 and batch: 500, loss is 4.266855020523071 and perplexity is 71.29705487905844
At time: 731.824954032898 and batch: 550, loss is 4.26754545211792 and perplexity is 71.34629761578111
At time: 732.543538570404 and batch: 600, loss is 4.218147411346435 and perplexity is 67.90756290464135
At time: 733.2625839710236 and batch: 650, loss is 4.224875011444092 and perplexity is 68.3659580521544
At time: 733.9834272861481 and batch: 700, loss is 4.250972452163697 and perplexity is 70.17361966541348
At time: 734.7566206455231 and batch: 750, loss is 4.2039756584167485 and perplexity is 66.95198083071956
At time: 735.4728932380676 and batch: 800, loss is 4.187111101150513 and perplexity is 65.83233303486473
At time: 736.1920518875122 and batch: 850, loss is 4.153331017494201 and perplexity is 63.6456523755279
At time: 736.9120900630951 and batch: 900, loss is 4.189799222946167 and perplexity is 66.00953642960242
At time: 737.6348814964294 and batch: 950, loss is 4.162988862991333 and perplexity is 64.2633100730997
At time: 738.3505516052246 and batch: 1000, loss is 4.169087886810303 and perplexity is 64.65645120180125
At time: 739.0644397735596 and batch: 1050, loss is 4.126087651252747 and perplexity is 61.93513645822748
At time: 739.7852966785431 and batch: 1100, loss is 4.086661458015442 and perplexity is 59.54078012366265
At time: 740.5029633045197 and batch: 1150, loss is 4.103046593666076 and perplexity is 60.52440025118078
At time: 741.2191236019135 and batch: 1200, loss is 4.09234803199768 and perplexity is 59.88032769097825
At time: 741.9409358501434 and batch: 1250, loss is 4.137182111740112 and perplexity is 62.62609922463156
At time: 742.6620390415192 and batch: 1300, loss is 4.133416104316711 and perplexity is 62.39069242016697
At time: 743.385035276413 and batch: 1350, loss is 4.111650381088257 and perplexity is 61.047385927417714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586675211588542 and perplexity of 98.16750080857575
Finished 35 epochs...
Completing Train Step...
At time: 745.9012954235077 and batch: 50, loss is 4.284437170028687 and perplexity is 72.56169532773914
At time: 746.6147167682648 and batch: 100, loss is 4.281179075241089 and perplexity is 72.32566715628026
At time: 747.328287601471 and batch: 150, loss is 4.256611356735229 and perplexity is 70.57043977402643
At time: 748.04114818573 and batch: 200, loss is 4.2577053689956665 and perplexity is 70.64768694733648
At time: 748.7594919204712 and batch: 250, loss is 4.24316906452179 and perplexity is 69.62815869849967
At time: 749.4726147651672 and batch: 300, loss is 4.236850461959839 and perplexity is 69.18959305669128
At time: 750.1872715950012 and batch: 350, loss is 4.248569974899292 and perplexity is 70.00523149497512
At time: 750.9054255485535 and batch: 400, loss is 4.251515321731567 and perplexity is 70.21172513020993
At time: 751.6200082302094 and batch: 450, loss is 4.202521209716797 and perplexity is 66.85467339073477
At time: 752.3363065719604 and batch: 500, loss is 4.26572979927063 and perplexity is 71.21687503616346
At time: 753.0506300926208 and batch: 550, loss is 4.26646131515503 and perplexity is 71.26899037076332
At time: 753.8183283805847 and batch: 600, loss is 4.2172172737121585 and perplexity is 67.84442889094454
At time: 754.5331842899323 and batch: 650, loss is 4.223998985290527 and perplexity is 68.30609390999274
At time: 755.2455694675446 and batch: 700, loss is 4.250349884033203 and perplexity is 70.12994540272109
At time: 755.9598858356476 and batch: 750, loss is 4.203382124900818 and perplexity is 66.91225437679579
At time: 756.6799111366272 and batch: 800, loss is 4.186632142066956 and perplexity is 65.80080959078852
At time: 757.3952510356903 and batch: 850, loss is 4.152953329086304 and perplexity is 63.62161868932065
At time: 758.1122539043427 and batch: 900, loss is 4.189731020927429 and perplexity is 66.00503459948062
At time: 758.8257324695587 and batch: 950, loss is 4.162913827896118 and perplexity is 64.2584882504148
At time: 759.5439233779907 and batch: 1000, loss is 4.169201078414917 and perplexity is 64.6637701834773
At time: 760.2694339752197 and batch: 1050, loss is 4.126278166770935 and perplexity is 61.94693718691915
At time: 760.9833590984344 and batch: 1100, loss is 4.087002053260803 and perplexity is 59.56106288418768
At time: 761.7004745006561 and batch: 1150, loss is 4.103577704429626 and perplexity is 60.55655394944338
At time: 762.4217257499695 and batch: 1200, loss is 4.093054404258728 and perplexity is 59.922640435956914
At time: 763.143271446228 and batch: 1250, loss is 4.137961745262146 and perplexity is 62.67494366885023
At time: 763.8576986789703 and batch: 1300, loss is 4.134232416152954 and perplexity is 62.44164347400575
At time: 764.5771946907043 and batch: 1350, loss is 4.112278938293457 and perplexity is 61.085769763655705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586493326822917 and perplexity of 98.14964725939274
Finished 36 epochs...
Completing Train Step...
At time: 767.0766792297363 and batch: 50, loss is 4.283950529098511 and perplexity is 72.52639242744452
At time: 767.8310744762421 and batch: 100, loss is 4.280483074188233 and perplexity is 72.2753459296357
At time: 768.5775833129883 and batch: 150, loss is 4.255858392715454 and perplexity is 70.51732277212207
At time: 769.3171901702881 and batch: 200, loss is 4.256863412857055 and perplexity is 70.58822972733054
At time: 770.061815738678 and batch: 250, loss is 4.242222304344177 and perplexity is 69.56226872652599
At time: 770.8100748062134 and batch: 300, loss is 4.2359894752502445 and perplexity is 69.13004737432622
At time: 771.5436387062073 and batch: 350, loss is 4.247796001434327 and perplexity is 69.95107026577008
At time: 772.3561449050903 and batch: 400, loss is 4.250767068862915 and perplexity is 70.15920865571984
At time: 773.0713484287262 and batch: 450, loss is 4.201834816932678 and perplexity is 66.80880057052217
At time: 773.7974524497986 and batch: 500, loss is 4.265095319747925 and perplexity is 71.17170371893523
At time: 774.5394756793976 and batch: 550, loss is 4.265950107574463 and perplexity is 71.23256643351611
At time: 775.2722115516663 and batch: 600, loss is 4.216787433624267 and perplexity is 67.81527290232239
At time: 775.9881765842438 and batch: 650, loss is 4.223585305213928 and perplexity is 68.27784288367809
At time: 776.7156822681427 and batch: 700, loss is 4.250086135864258 and perplexity is 70.11145119704658
At time: 777.4364528656006 and batch: 750, loss is 4.203177871704102 and perplexity is 66.89858873061331
At time: 778.1569452285767 and batch: 800, loss is 4.186515355110169 and perplexity is 65.79312536319979
At time: 778.8757553100586 and batch: 850, loss is 4.152856521606445 and perplexity is 63.615459938862145
At time: 779.5961968898773 and batch: 900, loss is 4.189750084877014 and perplexity is 66.00629292812692
At time: 780.3173470497131 and batch: 950, loss is 4.16296266078949 and perplexity is 64.26162625493805
At time: 781.0333724021912 and batch: 1000, loss is 4.169329991340637 and perplexity is 64.67210671661162
At time: 781.7511973381042 and batch: 1050, loss is 4.126451740264892 and perplexity is 61.9576904664618
At time: 782.482804775238 and batch: 1100, loss is 4.087276554107666 and perplexity is 59.577414690579104
At time: 783.2035095691681 and batch: 1150, loss is 4.10394847869873 and perplexity is 60.57901092445043
At time: 783.9207980632782 and batch: 1200, loss is 4.093503632545471 and perplexity is 59.949565428338374
At time: 784.6445722579956 and batch: 1250, loss is 4.138432559967041 and perplexity is 62.70445890151554
At time: 785.3593425750732 and batch: 1300, loss is 4.134681358337402 and perplexity is 62.469682455297324
At time: 786.0785496234894 and batch: 1350, loss is 4.112597141265869 and perplexity is 61.10521053005738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586418863932292 and perplexity of 98.14233902504347
Finished 37 epochs...
Completing Train Step...
At time: 788.5841569900513 and batch: 50, loss is 4.283563423156738 and perplexity is 72.49832246337598
At time: 789.3509714603424 and batch: 100, loss is 4.279927988052368 and perplexity is 72.2352380198475
At time: 790.0790419578552 and batch: 150, loss is 4.255267448425293 and perplexity is 70.4756632733063
At time: 790.8247756958008 and batch: 200, loss is 4.256245198249817 and perplexity is 70.54460453886784
At time: 791.5450761318207 and batch: 250, loss is 4.241540422439575 and perplexity is 69.51485164250136
At time: 792.2605476379395 and batch: 300, loss is 4.235357236862183 and perplexity is 69.08635451821354
At time: 792.9769768714905 and batch: 350, loss is 4.247254648208618 and perplexity is 69.91321227643685
At time: 793.698733329773 and batch: 400, loss is 4.250221996307373 and perplexity is 70.12097721696365
At time: 794.4209489822388 and batch: 450, loss is 4.20135425567627 and perplexity is 66.77670256252813
At time: 795.1352207660675 and batch: 500, loss is 4.264638051986695 and perplexity is 71.13916663295993
At time: 795.8574011325836 and batch: 550, loss is 4.265604848861694 and perplexity is 71.20797701442191
At time: 796.5830583572388 and batch: 600, loss is 4.2165198802948 and perplexity is 67.797131127323
At time: 797.3089916706085 and batch: 650, loss is 4.223318347930908 and perplexity is 68.25961804898581
At time: 798.0281069278717 and batch: 700, loss is 4.2499132823944095 and perplexity is 70.09933323677696
At time: 798.7439568042755 and batch: 750, loss is 4.203055057525635 and perplexity is 66.8903731399036
At time: 799.4618785381317 and batch: 800, loss is 4.186469979286194 and perplexity is 65.79014001365643
At time: 800.1780045032501 and batch: 850, loss is 4.152809562683106 and perplexity is 63.61247269549511
At time: 800.8971152305603 and batch: 900, loss is 4.189766669273377 and perplexity is 66.00738761172859
At time: 801.615327835083 and batch: 950, loss is 4.163007369041443 and perplexity is 64.26449934414048
At time: 802.3326432704926 and batch: 1000, loss is 4.169429545402527 and perplexity is 64.67854540801983
At time: 803.048912525177 and batch: 1050, loss is 4.126573066711426 and perplexity is 61.96520802891188
At time: 803.7700366973877 and batch: 1100, loss is 4.087462997436523 and perplexity is 59.58852353765177
At time: 804.487785577774 and batch: 1150, loss is 4.104205474853516 and perplexity is 60.594581498021796
At time: 805.2054150104523 and batch: 1200, loss is 4.09381130695343 and perplexity is 59.96801321319906
At time: 805.9250860214233 and batch: 1250, loss is 4.138740801811219 and perplexity is 62.72379001874208
At time: 806.6427085399628 and batch: 1300, loss is 4.134979338645935 and perplexity is 62.48829996423707
At time: 807.3631246089935 and batch: 1350, loss is 4.112788286209106 and perplexity is 61.11689159840864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586374918619792 and perplexity of 98.13802622404992
Finished 38 epochs...
Completing Train Step...
At time: 809.8806648254395 and batch: 50, loss is 4.283220758438111 and perplexity is 72.4734841019662
At time: 810.5952956676483 and batch: 100, loss is 4.279438447952271 and perplexity is 72.19988462835417
At time: 811.313225030899 and batch: 150, loss is 4.254776372909546 and perplexity is 70.44106289701085
At time: 812.0298738479614 and batch: 200, loss is 4.255728759765625 and perplexity is 70.50818199604277
At time: 812.7453079223633 and batch: 250, loss is 4.240980825424194 and perplexity is 69.47596222121444
At time: 813.4619991779327 and batch: 300, loss is 4.234836130142212 and perplexity is 69.05036253326291
At time: 814.1814427375793 and batch: 350, loss is 4.246806211471558 and perplexity is 69.88186765221198
At time: 814.8983082771301 and batch: 400, loss is 4.249787101745605 and perplexity is 70.09048861545016
At time: 815.6212587356567 and batch: 450, loss is 4.200961365699768 and perplexity is 66.75047181866304
At time: 816.3412518501282 and batch: 500, loss is 4.264266700744629 and perplexity is 71.11275391957173
At time: 817.0621037483215 and batch: 550, loss is 4.2653289413452145 and perplexity is 71.18833290843317
At time: 817.7782328128815 and batch: 600, loss is 4.216320419311524 and perplexity is 67.78360959343911
At time: 818.4939918518066 and batch: 650, loss is 4.223113918304444 and perplexity is 68.24566518700324
At time: 819.2090871334076 and batch: 700, loss is 4.249773817062378 and perplexity is 70.08955749169651
At time: 819.9266829490662 and batch: 750, loss is 4.202953643798828 and perplexity is 66.88358988183955
At time: 820.6434135437012 and batch: 800, loss is 4.186440467834473 and perplexity is 65.78819847976455
At time: 821.3596661090851 and batch: 850, loss is 4.152783999443054 and perplexity is 63.610846575369834
At time: 822.0748853683472 and batch: 900, loss is 4.189766693115234 and perplexity is 66.00738918546733
At time: 822.7937388420105 and batch: 950, loss is 4.163028049468994 and perplexity is 64.26582837520569
At time: 823.5087947845459 and batch: 1000, loss is 4.169502120018006 and perplexity is 64.68323959892005
At time: 824.2290081977844 and batch: 1050, loss is 4.126651654243469 and perplexity is 61.970077913036974
At time: 824.9384913444519 and batch: 1100, loss is 4.0875916671752925 and perplexity is 59.59619127070104
At time: 825.6576998233795 and batch: 1150, loss is 4.104384956359863 and perplexity is 60.60545808082906
At time: 826.373738527298 and batch: 1200, loss is 4.094033298492431 and perplexity is 59.98132708247157
At time: 827.0930361747742 and batch: 1250, loss is 4.138954405784607 and perplexity is 62.73718950055638
At time: 827.8153474330902 and batch: 1300, loss is 4.135217747688293 and perplexity is 62.503199516013524
At time: 828.5321407318115 and batch: 1350, loss is 4.112915334701538 and perplexity is 61.12465690062284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586346028645833 and perplexity of 98.13519105998203
Finished 39 epochs...
Completing Train Step...
At time: 831.0776083469391 and batch: 50, loss is 4.282895307540894 and perplexity is 72.4499013792581
At time: 831.7930963039398 and batch: 100, loss is 4.278997421264648 and perplexity is 72.16804957296034
At time: 832.5174217224121 and batch: 150, loss is 4.254345140457153 and perplexity is 70.4106929734276
At time: 833.240713596344 and batch: 200, loss is 4.2552872133255 and perplexity is 70.47705623152372
At time: 833.9614908695221 and batch: 250, loss is 4.240504479408264 and perplexity is 69.44287550439593
At time: 834.6828734874725 and batch: 300, loss is 4.234387912750244 and perplexity is 69.01941989488617
At time: 835.4050765037537 and batch: 350, loss is 4.2464188385009765 and perplexity is 69.8548025480331
At time: 836.1244847774506 and batch: 400, loss is 4.249418678283692 and perplexity is 70.06467039129905
At time: 836.8447682857513 and batch: 450, loss is 4.200629091262817 and perplexity is 66.72829602765157
At time: 837.5663480758667 and batch: 500, loss is 4.2639532566070555 and perplexity is 71.09046753669945
At time: 838.2879865169525 and batch: 550, loss is 4.2650927066802975 and perplexity is 71.17151774270326
At time: 839.0082221031189 and batch: 600, loss is 4.216157217025756 and perplexity is 67.77254805607454
At time: 839.7261915206909 and batch: 650, loss is 4.22294310092926 and perplexity is 68.23400863720732
At time: 840.4446499347687 and batch: 700, loss is 4.249653339385986 and perplexity is 70.0811137733206
At time: 841.1613750457764 and batch: 750, loss is 4.202858786582947 and perplexity is 66.87724579161134
At time: 841.8795640468597 and batch: 800, loss is 4.186410646438599 and perplexity is 65.78623661310682
At time: 842.5952208042145 and batch: 850, loss is 4.152761721611023 and perplexity is 63.60942947939947
At time: 843.3120877742767 and batch: 900, loss is 4.189752612113953 and perplexity is 66.0064597418794
At time: 844.0298392772675 and batch: 950, loss is 4.1630291175842284 and perplexity is 64.26589701855268
At time: 844.7472026348114 and batch: 1000, loss is 4.169550843238831 and perplexity is 64.68639125146541
At time: 845.4670329093933 and batch: 1050, loss is 4.126699414253235 and perplexity is 61.973037675241855
At time: 846.2361676692963 and batch: 1100, loss is 4.087673449516297 and perplexity is 59.60106538604366
At time: 846.953958272934 and batch: 1150, loss is 4.104512343406677 and perplexity is 60.61317892291199
At time: 847.6738238334656 and batch: 1200, loss is 4.094200387001037 and perplexity is 59.991350110301056
At time: 848.3898959159851 and batch: 1250, loss is 4.139110631942749 and perplexity is 62.74699145628571
At time: 849.1164753437042 and batch: 1300, loss is 4.135421919822693 and perplexity is 62.51596223051649
At time: 849.8367393016815 and batch: 1350, loss is 4.113003792762757 and perplexity is 61.130064108416974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586324462890625 and perplexity of 98.1330747232946
Finished 40 epochs...
Completing Train Step...
At time: 852.45570063591 and batch: 50, loss is 4.2825806522369385 and perplexity is 72.4271082197004
At time: 853.2834725379944 and batch: 100, loss is 4.27859335899353 and perplexity is 72.13889507746019
At time: 854.0299751758575 and batch: 150, loss is 4.25395022392273 and perplexity is 70.38289211644253
At time: 854.7827513217926 and batch: 200, loss is 4.254890832901001 and perplexity is 70.4491260419131
At time: 855.5071654319763 and batch: 250, loss is 4.2400885009765625 and perplexity is 69.41399477324786
At time: 856.2311036586761 and batch: 300, loss is 4.233990678787231 and perplexity is 68.99200848192902
At time: 856.9539175033569 and batch: 350, loss is 4.246073989868164 and perplexity is 69.83071736798844
At time: 857.6737170219421 and batch: 400, loss is 4.24909125328064 and perplexity is 70.04173322168869
At time: 858.3935272693634 and batch: 450, loss is 4.200343613624573 and perplexity is 66.70924931013285
At time: 859.1123998165131 and batch: 500, loss is 4.263682813644409 and perplexity is 71.07124421956549
At time: 859.8270540237427 and batch: 550, loss is 4.264887161254883 and perplexity is 71.15689026616839
At time: 860.54700756073 and batch: 600, loss is 4.216012201309204 and perplexity is 67.76272068403453
At time: 861.2628655433655 and batch: 650, loss is 4.2227928352355955 and perplexity is 68.22375617688374
At time: 861.9817585945129 and batch: 700, loss is 4.2495407867431645 and perplexity is 70.07322640263403
At time: 862.702455997467 and batch: 750, loss is 4.2027685260772705 and perplexity is 66.87120969000281
At time: 863.4229907989502 and batch: 800, loss is 4.1863815879821775 and perplexity is 65.78432499439154
At time: 864.1377654075623 and batch: 850, loss is 4.1527383995056155 and perplexity is 63.60794599087933
At time: 864.9325726032257 and batch: 900, loss is 4.189727983474731 and perplexity is 66.00483411261473
At time: 865.6583580970764 and batch: 950, loss is 4.163018751144409 and perplexity is 64.2652308134519
At time: 866.3768992424011 and batch: 1000, loss is 4.169582896232605 and perplexity is 64.68846467719102
At time: 867.0987329483032 and batch: 1050, loss is 4.126727838516235 and perplexity is 61.974799238199104
At time: 867.818675994873 and batch: 1100, loss is 4.087726774215699 and perplexity is 59.60424367967943
At time: 868.536425113678 and batch: 1150, loss is 4.104605193138123 and perplexity is 60.618807101580444
At time: 869.2581870555878 and batch: 1200, loss is 4.094327831268311 and perplexity is 59.99899615117031
At time: 869.9795844554901 and batch: 1250, loss is 4.139227046966552 and perplexity is 62.75429657399415
At time: 870.6983773708344 and batch: 1300, loss is 4.135590415000916 and perplexity is 62.52649675619779
At time: 871.4181234836578 and batch: 1350, loss is 4.1130651044845585 and perplexity is 61.13381221280151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586308186848958 and perplexity of 98.13147751827967
Finished 41 epochs...
Completing Train Step...
At time: 874.0204572677612 and batch: 50, loss is 4.282283754348755 and perplexity is 72.40560795607207
At time: 874.7747993469238 and batch: 100, loss is 4.2782059669494625 and perplexity is 72.11095445577607
At time: 875.4937694072723 and batch: 150, loss is 4.253591613769531 and perplexity is 70.35765662184085
At time: 876.2160913944244 and batch: 200, loss is 4.254516062736511 and perplexity is 70.42272875812434
At time: 876.933703660965 and batch: 250, loss is 4.239698123931885 and perplexity is 69.38690243156722
At time: 877.656328201294 and batch: 300, loss is 4.233621129989624 and perplexity is 68.96651727856188
At time: 878.376935005188 and batch: 350, loss is 4.2457534694671635 and perplexity is 69.80833878504326
At time: 879.0956108570099 and batch: 400, loss is 4.248795728683472 and perplexity is 70.02103722493209
At time: 879.8154952526093 and batch: 450, loss is 4.200074291229248 and perplexity is 66.69128543446391
At time: 880.5334115028381 and batch: 500, loss is 4.263429489135742 and perplexity is 71.05324241178904
At time: 881.2558968067169 and batch: 550, loss is 4.264696111679077 and perplexity is 71.14329707099581
At time: 881.9738137722015 and batch: 600, loss is 4.215882387161255 and perplexity is 67.75392469512046
At time: 882.6898047924042 and batch: 650, loss is 4.222650294303894 and perplexity is 68.2140321921635
At time: 883.4126899242401 and batch: 700, loss is 4.249433717727661 and perplexity is 70.06572413290745
At time: 884.2259106636047 and batch: 750, loss is 4.202678017616272 and perplexity is 66.86515755361756
At time: 884.9422497749329 and batch: 800, loss is 4.18634982585907 and perplexity is 65.78223557774484
At time: 885.6578283309937 and batch: 850, loss is 4.152706627845764 and perplexity is 63.60592509295927
At time: 886.3748235702515 and batch: 900, loss is 4.189693279266358 and perplexity is 66.00254350684507
At time: 887.0931293964386 and batch: 950, loss is 4.162996015548706 and perplexity is 64.26376972175582
At time: 887.8091082572937 and batch: 1000, loss is 4.169601726531982 and perplexity is 64.68968279181587
At time: 888.5291783809662 and batch: 1050, loss is 4.1267386293411255 and perplexity is 61.97546800101355
At time: 889.2447874546051 and batch: 1100, loss is 4.087758936882019 and perplexity is 59.60616074190893
At time: 889.961941242218 and batch: 1150, loss is 4.104670433998108 and perplexity is 60.62276205369788
At time: 890.678567647934 and batch: 1200, loss is 4.094428038597107 and perplexity is 60.005008791555326
At time: 891.3956654071808 and batch: 1250, loss is 4.139315915107727 and perplexity is 62.75987367949124
At time: 892.1130344867706 and batch: 1300, loss is 4.135729427337647 and perplexity is 62.53518931479189
At time: 892.8296828269958 and batch: 1350, loss is 4.113107318878174 and perplexity is 61.136392994086144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.5862943522135415 and perplexity of 98.13011991445629
Finished 42 epochs...
Completing Train Step...
At time: 895.3471131324768 and batch: 50, loss is 4.281989583969116 and perplexity is 72.38431150343803
At time: 896.0697665214539 and batch: 100, loss is 4.277842674255371 and perplexity is 72.08476183094103
At time: 896.7890214920044 and batch: 150, loss is 4.253251829147339 and perplexity is 70.33375423312927
At time: 897.5013287067413 and batch: 200, loss is 4.254171066284179 and perplexity is 70.39843735700383
At time: 898.2163555622101 and batch: 250, loss is 4.239340043067932 and perplexity is 69.36206075752698
At time: 898.9284253120422 and batch: 300, loss is 4.233278999328613 and perplexity is 68.94292575433977
At time: 899.6399924755096 and batch: 350, loss is 4.245458326339722 and perplexity is 69.78773837379717
At time: 900.3535912036896 and batch: 400, loss is 4.2485192775726315 and perplexity is 70.00168250684897
At time: 901.0660319328308 and batch: 450, loss is 4.199828462600708 and perplexity is 66.67489282219925
At time: 901.7788670063019 and batch: 500, loss is 4.263196563720703 and perplexity is 71.0366942331309
At time: 902.5467913150787 and batch: 550, loss is 4.264517841339111 and perplexity is 71.13061546165163
At time: 903.2571151256561 and batch: 600, loss is 4.215759038925171 and perplexity is 67.74556788343115
At time: 903.9721019268036 and batch: 650, loss is 4.22251434803009 and perplexity is 68.20475937898233
At time: 904.685222864151 and batch: 700, loss is 4.249328689575195 and perplexity is 70.05836564578152
At time: 905.3990271091461 and batch: 750, loss is 4.202587084770203 and perplexity is 66.8590775909767
At time: 906.1082539558411 and batch: 800, loss is 4.186315159797669 and perplexity is 65.77995520625319
At time: 906.819776058197 and batch: 850, loss is 4.152672233581543 and perplexity is 63.603737451586994
At time: 907.5346484184265 and batch: 900, loss is 4.189651346206665 and perplexity is 65.99977587627633
At time: 908.2480418682098 and batch: 950, loss is 4.1629648017883305 and perplexity is 64.26176383915262
At time: 908.9624648094177 and batch: 1000, loss is 4.169609518051147 and perplexity is 64.6901868246827
At time: 909.6719319820404 and batch: 1050, loss is 4.126736550331116 and perplexity is 61.97533915352918
At time: 910.3836514949799 and batch: 1100, loss is 4.087775130271911 and perplexity is 59.60712597552497
At time: 911.0934224128723 and batch: 1150, loss is 4.104715476036072 and perplexity is 60.625492687943954
At time: 911.8098754882812 and batch: 1200, loss is 4.094508581161499 and perplexity is 60.009841943474385
At time: 912.5230405330658 and batch: 1250, loss is 4.1393854665756225 and perplexity is 62.7642388726316
At time: 913.238988161087 and batch: 1300, loss is 4.135858473777771 and perplexity is 62.543259779076585
At time: 913.9515864849091 and batch: 1350, loss is 4.113136625289917 and perplexity is 61.13818470864593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586285807291667 and perplexity of 98.12928140383055
Finished 43 epochs...
Completing Train Step...
At time: 916.4752750396729 and batch: 50, loss is 4.281705446243286 and perplexity is 72.36374731145142
At time: 917.1918168067932 and batch: 100, loss is 4.277493906021118 and perplexity is 72.05962533950039
At time: 917.9066359996796 and batch: 150, loss is 4.252931842803955 and perplexity is 70.31125199269256
At time: 918.6256132125854 and batch: 200, loss is 4.253843350410461 and perplexity is 70.37537045149898
At time: 919.3410589694977 and batch: 250, loss is 4.238997602462769 and perplexity is 69.33831243789237
At time: 920.0621840953827 and batch: 300, loss is 4.232954092025757 and perplexity is 68.9205293328594
At time: 920.822024345398 and batch: 350, loss is 4.245178518295288 and perplexity is 69.76821393487225
At time: 921.5399165153503 and batch: 400, loss is 4.2482571792602535 and perplexity is 69.98333758819142
At time: 922.2571215629578 and batch: 450, loss is 4.1995928812026975 and perplexity is 66.65918730777216
At time: 922.9723918437958 and batch: 500, loss is 4.262972650527954 and perplexity is 71.02078996077746
At time: 923.6910066604614 and batch: 550, loss is 4.264346113204956 and perplexity is 71.11840138255765
At time: 924.4101345539093 and batch: 600, loss is 4.215641164779663 and perplexity is 67.73758290312558
At time: 925.1465504169464 and batch: 650, loss is 4.222380905151367 and perplexity is 68.19565854678233
At time: 925.8691637516022 and batch: 700, loss is 4.249224519729614 and perplexity is 70.05106805675146
At time: 926.5919213294983 and batch: 750, loss is 4.202493433952331 and perplexity is 66.85281647686199
At time: 927.3175399303436 and batch: 800, loss is 4.186276469230652 and perplexity is 65.77741019172225
At time: 928.0320248603821 and batch: 850, loss is 4.152632493972778 and perplexity is 63.60120991416665
At time: 928.7494807243347 and batch: 900, loss is 4.18960373878479 and perplexity is 65.99663387189449
At time: 929.4753322601318 and batch: 950, loss is 4.162923607826233 and perplexity is 64.25911669701223
At time: 930.1904380321503 and batch: 1000, loss is 4.169609174728394 and perplexity is 64.69016461507346
At time: 930.9075257778168 and batch: 1050, loss is 4.126723518371582 and perplexity is 61.97453149867991
At time: 931.6259360313416 and batch: 1100, loss is 4.087780532836914 and perplexity is 59.607448007767566
At time: 932.3406417369843 and batch: 1150, loss is 4.104742269515992 and perplexity is 60.62711707762636
At time: 933.0575363636017 and batch: 1200, loss is 4.094574222564697 and perplexity is 60.01378120299311
At time: 933.7708024978638 and batch: 1250, loss is 4.139440813064575 and perplexity is 62.76771274901754
At time: 934.4897463321686 and batch: 1300, loss is 4.135972690582276 and perplexity is 62.550403678320784
At time: 935.2082753181458 and batch: 1350, loss is 4.113158001899719 and perplexity is 61.1394916497334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586278889973959 and perplexity of 98.12860261476233
Finished 44 epochs...
Completing Train Step...
At time: 937.6977660655975 and batch: 50, loss is 4.281426992416382 and perplexity is 72.34360015424008
At time: 938.4452311992645 and batch: 100, loss is 4.277158145904541 and perplexity is 72.03543465265705
At time: 939.1619863510132 and batch: 150, loss is 4.252624502182007 and perplexity is 70.28964580917422
At time: 939.9059309959412 and batch: 200, loss is 4.253532447814941 and perplexity is 70.35349396706904
At time: 940.621256351471 and batch: 250, loss is 4.238676080703735 and perplexity is 69.3160222452919
At time: 941.3394463062286 and batch: 300, loss is 4.232646493911743 and perplexity is 68.89933276819858
At time: 942.0541660785675 and batch: 350, loss is 4.244912300109863 and perplexity is 69.74964283964952
At time: 942.7759582996368 and batch: 400, loss is 4.248008441925049 and perplexity is 69.96593228405524
At time: 943.4917650222778 and batch: 450, loss is 4.199368710517883 and perplexity is 66.64424594687435
At time: 944.207364320755 and batch: 500, loss is 4.262758731842041 and perplexity is 71.00559891159915
At time: 944.9225823879242 and batch: 550, loss is 4.264181356430054 and perplexity is 71.10668510930387
At time: 945.6392171382904 and batch: 600, loss is 4.215528964996338 and perplexity is 67.72998318735206
At time: 946.3552048206329 and batch: 650, loss is 4.222252464294433 and perplexity is 68.18690000044903
At time: 947.0753355026245 and batch: 700, loss is 4.249122400283813 and perplexity is 70.04391484575093
At time: 947.798770904541 and batch: 750, loss is 4.202399616241455 and perplexity is 66.8465447928566
At time: 948.5173857212067 and batch: 800, loss is 4.186233868598938 and perplexity is 65.77460809218162
At time: 949.2378487586975 and batch: 850, loss is 4.1525888252258305 and perplexity is 63.59843258966694
At time: 949.953905582428 and batch: 900, loss is 4.189550461769104 and perplexity is 65.99311786185861
At time: 950.6714034080505 and batch: 950, loss is 4.162874388694763 and perplexity is 64.25595399693269
At time: 951.3895571231842 and batch: 1000, loss is 4.1696018743515015 and perplexity is 64.68969235421439
At time: 952.1071157455444 and batch: 1050, loss is 4.126700439453125 and perplexity is 61.97310121002585
At time: 952.8272624015808 and batch: 1100, loss is 4.087774543762207 and perplexity is 59.60709101537742
At time: 953.5443112850189 and batch: 1150, loss is 4.104767632484436 and perplexity is 60.62865478078396
At time: 954.262659072876 and batch: 1200, loss is 4.094626121520996 and perplexity is 60.016895936426096
At time: 954.9787476062775 and batch: 1250, loss is 4.139483499526977 and perplexity is 62.77039213781428
At time: 955.7007293701172 and batch: 1300, loss is 4.136075983047485 and perplexity is 62.55686499741361
At time: 956.4213001728058 and batch: 1350, loss is 4.113169999122619 and perplexity is 61.14022515824277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.58627197265625 and perplexity of 98.12792383038939
Finished 45 epochs...
Completing Train Step...
At time: 958.8809535503387 and batch: 50, loss is 4.281156063079834 and perplexity is 72.32400280551504
At time: 959.6260960102081 and batch: 100, loss is 4.276835203170776 and perplexity is 72.01217508841818
At time: 960.3405532836914 and batch: 150, loss is 4.252329435348511 and perplexity is 70.26890872552097
At time: 961.0564613342285 and batch: 200, loss is 4.253234314918518 and perplexity is 70.3325224024511
At time: 961.7723069190979 and batch: 250, loss is 4.23836651802063 and perplexity is 69.29456791236524
At time: 962.4915628433228 and batch: 300, loss is 4.23235071182251 and perplexity is 68.87895659320795
At time: 963.2072401046753 and batch: 350, loss is 4.244656839370728 and perplexity is 69.73182682007891
At time: 963.9239017963409 and batch: 400, loss is 4.247769756317139 and perplexity is 69.94923441582469
At time: 964.6388399600983 and batch: 450, loss is 4.199153280258178 and perplexity is 66.6298903060374
At time: 965.3531057834625 and batch: 500, loss is 4.262552671432495 and perplexity is 70.99096897618436
At time: 966.0674822330475 and batch: 550, loss is 4.264022274017334 and perplexity is 71.09537418598448
At time: 966.7868888378143 and batch: 600, loss is 4.21542254447937 and perplexity is 67.72277571104553
At time: 967.5043771266937 and batch: 650, loss is 4.222126407623291 and perplexity is 68.1783051285513
At time: 968.2191178798676 and batch: 700, loss is 4.24902006149292 and perplexity is 70.03674700297664
At time: 968.9357826709747 and batch: 750, loss is 4.20230498790741 and perplexity is 66.84021951496617
At time: 969.656168460846 and batch: 800, loss is 4.186189250946045 and perplexity is 65.77167344901751
At time: 970.3729572296143 and batch: 850, loss is 4.152542572021485 and perplexity is 63.5954910263972
At time: 971.0898430347443 and batch: 900, loss is 4.189493412971497 and perplexity is 65.98935314122161
At time: 971.8057548999786 and batch: 950, loss is 4.162819924354554 and perplexity is 64.25245443409527
At time: 972.5229561328888 and batch: 1000, loss is 4.16958872795105 and perplexity is 64.68884192320365
At time: 973.2372269630432 and batch: 1050, loss is 4.126670298576355 and perplexity is 61.97123331456934
At time: 973.9547121524811 and batch: 1100, loss is 4.087761235237122 and perplexity is 59.60629773819005
At time: 974.6696305274963 and batch: 1150, loss is 4.104781966209412 and perplexity is 60.62952382147552
At time: 975.3939332962036 and batch: 1200, loss is 4.094668002128601 and perplexity is 60.01940953312959
At time: 976.1639523506165 and batch: 1250, loss is 4.13951684474945 and perplexity is 62.77248526540254
At time: 976.8853039741516 and batch: 1300, loss is 4.136164264678955 and perplexity is 62.562387863295456
At time: 977.6027009487152 and batch: 1350, loss is 4.113174395561218 and perplexity is 61.14049395807948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586268717447917 and perplexity of 98.12760440407394
Finished 46 epochs...
Completing Train Step...
At time: 980.1037776470184 and batch: 50, loss is 4.2808905124664305 and perplexity is 72.30479967202102
At time: 980.8266072273254 and batch: 100, loss is 4.276522254943847 and perplexity is 71.98964253185267
At time: 981.5422711372375 and batch: 150, loss is 4.252043991088867 and perplexity is 70.24885373132155
At time: 982.2576763629913 and batch: 200, loss is 4.252946681976319 and perplexity is 70.31229536122135
At time: 982.9806656837463 and batch: 250, loss is 4.238067169189453 and perplexity is 69.27382776887747
At time: 983.6983108520508 and batch: 300, loss is 4.232065095901489 and perplexity is 68.8592864757647
At time: 984.4160265922546 and batch: 350, loss is 4.244409713745117 and perplexity is 69.71459642787447
At time: 985.135901927948 and batch: 400, loss is 4.247539463043213 and perplexity is 69.93312743235867
At time: 985.8554644584656 and batch: 450, loss is 4.198944644927979 and perplexity is 66.61599040692766
At time: 986.5758335590363 and batch: 500, loss is 4.262353610992432 and perplexity is 70.9768388890768
At time: 987.2935047149658 and batch: 550, loss is 4.263867769241333 and perplexity is 71.08439045966212
At time: 988.0129706859589 and batch: 600, loss is 4.215321607589722 and perplexity is 67.71594032968378
At time: 988.7339437007904 and batch: 650, loss is 4.222002358436584 and perplexity is 68.1698481897981
At time: 989.4509589672089 and batch: 700, loss is 4.248918313980102 and perplexity is 70.02962130068063
At time: 990.167646408081 and batch: 750, loss is 4.2022100973129275 and perplexity is 66.83387730771372
At time: 990.8835237026215 and batch: 800, loss is 4.18614251613617 and perplexity is 65.76859969418992
At time: 991.6014451980591 and batch: 850, loss is 4.152493739128113 and perplexity is 63.59238555039029
At time: 992.3216762542725 and batch: 900, loss is 4.189432487487793 and perplexity is 65.98533283043315
At time: 993.0400931835175 and batch: 950, loss is 4.162761397361756 and perplexity is 64.24869404120068
At time: 993.7627897262573 and batch: 1000, loss is 4.169571137428283 and perplexity is 64.68770402266524
At time: 994.4831147193909 and batch: 1050, loss is 4.126634850502014 and perplexity is 61.96903659261883
At time: 995.2555627822876 and batch: 1100, loss is 4.087741827964782 and perplexity is 59.605140953761754
At time: 995.9717676639557 and batch: 1150, loss is 4.104788603782654 and perplexity is 60.62992625571612
At time: 996.6883742809296 and batch: 1200, loss is 4.094700937271118 and perplexity is 60.02138631348896
At time: 997.4059221744537 and batch: 1250, loss is 4.139542303085327 and perplexity is 62.77408336875868
At time: 998.1232085227966 and batch: 1300, loss is 4.1362388324737545 and perplexity is 62.5670531765347
At time: 998.842257976532 and batch: 1350, loss is 4.113172159194947 and perplexity is 61.14035722569385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586263427734375 and perplexity of 98.12708533852894
Finished 47 epochs...
Completing Train Step...
At time: 1001.3163061141968 and batch: 50, loss is 4.280629615783692 and perplexity is 72.28593805021477
At time: 1002.0396299362183 and batch: 100, loss is 4.276218070983886 and perplexity is 71.96774776749804
At time: 1002.7683165073395 and batch: 150, loss is 4.251766834259033 and perplexity is 70.22938647958739
At time: 1003.5122561454773 and batch: 200, loss is 4.252667698860169 and perplexity is 70.29268215396186
At time: 1004.2522444725037 and batch: 250, loss is 4.237776255607605 and perplexity is 69.25367800257534
At time: 1004.9954826831818 and batch: 300, loss is 4.231787986755371 and perplexity is 68.8402075812773
At time: 1005.7330627441406 and batch: 350, loss is 4.2441690635681155 and perplexity is 69.69782161641612
At time: 1006.4835860729218 and batch: 400, loss is 4.247315816879272 and perplexity is 69.91748890548948
At time: 1007.2015693187714 and batch: 450, loss is 4.198741307258606 and perplexity is 66.60244624376134
At time: 1007.9261620044708 and batch: 500, loss is 4.262159767150879 and perplexity is 70.96308179937192
At time: 1008.659604549408 and batch: 550, loss is 4.263716945648193 and perplexity is 71.07367006494138
At time: 1009.3866682052612 and batch: 600, loss is 4.215224161148071 and perplexity is 67.70934197375398
At time: 1010.102697134018 and batch: 650, loss is 4.22187906742096 and perplexity is 68.16144397807237
At time: 1010.8242981433868 and batch: 700, loss is 4.2488176631927494 and perplexity is 70.02257311886719
At time: 1011.5425939559937 and batch: 750, loss is 4.202114596366882 and perplexity is 66.82749491397011
At time: 1012.2623221874237 and batch: 800, loss is 4.186094260215759 and perplexity is 65.76542604645182
At time: 1012.9804916381836 and batch: 850, loss is 4.152442979812622 and perplexity is 63.58915772635106
At time: 1013.7517879009247 and batch: 900, loss is 4.18936888217926 and perplexity is 65.98113594645328
At time: 1014.4711637496948 and batch: 950, loss is 4.1626998853683475 and perplexity is 64.24474209750355
At time: 1015.1945362091064 and batch: 1000, loss is 4.1695500755310055 and perplexity is 64.68634159123572
At time: 1015.9098045825958 and batch: 1050, loss is 4.126594591140747 and perplexity is 61.96654180900682
At time: 1016.6354992389679 and batch: 1100, loss is 4.087716784477234 and perplexity is 59.60364825184776
At time: 1017.3537473678589 and batch: 1150, loss is 4.104787783622742 and perplexity is 60.6298765295015
At time: 1018.075300693512 and batch: 1200, loss is 4.094725942611694 and perplexity is 60.02288718746042
At time: 1018.7908363342285 and batch: 1250, loss is 4.139561052322388 and perplexity is 62.77526034596271
At time: 1019.5070366859436 and batch: 1300, loss is 4.136298770904541 and perplexity is 62.57080345991297
At time: 1020.222748041153 and batch: 1350, loss is 4.113164372444153 and perplexity is 61.13988114282228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586261393229167 and perplexity of 98.12688569866587
Finished 48 epochs...
Completing Train Step...
At time: 1022.6972522735596 and batch: 50, loss is 4.280372819900513 and perplexity is 72.26737770212422
At time: 1023.4436542987823 and batch: 100, loss is 4.275921602249145 and perplexity is 71.94641474281889
At time: 1024.1614866256714 and batch: 150, loss is 4.251496849060058 and perplexity is 70.21042814405496
At time: 1024.8814685344696 and batch: 200, loss is 4.252395281791687 and perplexity is 70.27353583556412
At time: 1025.5974309444427 and batch: 250, loss is 4.237491598129273 and perplexity is 69.23396723077231
At time: 1026.3159165382385 and batch: 300, loss is 4.231518545150757 and perplexity is 68.82166166391768
At time: 1027.033052444458 and batch: 350, loss is 4.243930826187134 and perplexity is 69.68121896769837
At time: 1027.7497243881226 and batch: 400, loss is 4.247097988128662 and perplexity is 69.90226052488514
At time: 1028.4665307998657 and batch: 450, loss is 4.198542199134827 and perplexity is 66.58918647575818
At time: 1029.1826450824738 and batch: 500, loss is 4.261970100402832 and perplexity is 70.94962373872923
At time: 1029.8977086544037 and batch: 550, loss is 4.263568744659424 and perplexity is 71.0631376572382
At time: 1030.6172614097595 and batch: 600, loss is 4.215127859115601 and perplexity is 67.70282174046558
At time: 1031.3347098827362 and batch: 650, loss is 4.221755714416504 and perplexity is 68.1530365777193
At time: 1032.1064453125 and batch: 700, loss is 4.24871943473816 and perplexity is 70.01569524753018
At time: 1032.826399564743 and batch: 750, loss is 4.20201949596405 and perplexity is 66.8211398944708
At time: 1033.5501444339752 and batch: 800, loss is 4.186044969558716 and perplexity is 65.76218450528077
At time: 1034.2746090888977 and batch: 850, loss is 4.152390837669373 and perplexity is 63.58584213782149
At time: 1034.9948961734772 and batch: 900, loss is 4.189303250312805 and perplexity is 65.97680562345542
At time: 1035.7233390808105 and batch: 950, loss is 4.162635221481323 and perplexity is 64.2405879170728
At time: 1036.444118976593 and batch: 1000, loss is 4.169524974822998 and perplexity is 64.68471793864089
At time: 1037.1696889400482 and batch: 1050, loss is 4.126550965309143 and perplexity is 61.96383852605568
At time: 1037.888662815094 and batch: 1100, loss is 4.087686347961426 and perplexity is 59.601834152073096
At time: 1038.60515999794 and batch: 1150, loss is 4.104781923294067 and perplexity is 60.629521219538645
At time: 1039.3228695392609 and batch: 1200, loss is 4.094744181632995 and perplexity is 60.02398195616212
At time: 1040.0465037822723 and batch: 1250, loss is 4.139574460983276 and perplexity is 62.77610208378414
At time: 1040.7628464698792 and batch: 1300, loss is 4.136346487998963 and perplexity is 62.573789228085225
At time: 1041.4801540374756 and batch: 1350, loss is 4.113151535987854 and perplexity is 61.139096328447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586258544921875 and perplexity of 98.12660620353982
Finished 49 epochs...
Completing Train Step...
At time: 1044.0268576145172 and batch: 50, loss is 4.280120105743408 and perplexity is 72.24911702014566
At time: 1044.8009815216064 and batch: 100, loss is 4.275632104873657 and perplexity is 71.92558945915302
At time: 1045.5149295330048 and batch: 150, loss is 4.251233282089234 and perplexity is 70.19192543264765
At time: 1046.2311782836914 and batch: 200, loss is 4.252128067016602 and perplexity is 70.25476021716445
At time: 1046.9404475688934 and batch: 250, loss is 4.23721137046814 and perplexity is 69.21456867619703
At time: 1047.655730009079 and batch: 300, loss is 4.231256437301636 and perplexity is 68.80362533004161
At time: 1048.3670122623444 and batch: 350, loss is 4.243689994812012 and perplexity is 69.66443956449872
At time: 1049.083255290985 and batch: 400, loss is 4.246885452270508 and perplexity is 69.88740536664042
At time: 1049.8013279438019 and batch: 450, loss is 4.198347101211548 and perplexity is 66.57619633098027
At time: 1050.5146548748016 and batch: 500, loss is 4.2617836189270015 and perplexity is 70.93639418175684
At time: 1051.2805361747742 and batch: 550, loss is 4.263422374725342 and perplexity is 71.05273691166042
At time: 1051.9955582618713 and batch: 600, loss is 4.215029850006103 and perplexity is 67.69618657235509
At time: 1052.7059738636017 and batch: 650, loss is 4.22163188457489 and perplexity is 68.14459772049636
At time: 1053.4218168258667 and batch: 700, loss is 4.248623895645141 and perplexity is 70.00900633104078
At time: 1054.1362090110779 and batch: 750, loss is 4.201924571990967 and perplexity is 66.81479726742448
At time: 1054.8649036884308 and batch: 800, loss is 4.18599543094635 and perplexity is 65.75892681860553
At time: 1055.5761849880219 and batch: 850, loss is 4.152337536811829 and perplexity is 63.582453048229404
At time: 1056.2915887832642 and batch: 900, loss is 4.189236154556275 and perplexity is 65.97237900827386
At time: 1057.0082550048828 and batch: 950, loss is 4.16256808757782 and perplexity is 64.23627534040428
At time: 1057.7256286144257 and batch: 1000, loss is 4.169496531486511 and perplexity is 64.68287811560846
At time: 1058.4389839172363 and batch: 1050, loss is 4.126504454612732 and perplexity is 61.960956611793996
At time: 1059.1602635383606 and batch: 1100, loss is 4.0876509475708005 and perplexity is 59.59972426120784
At time: 1059.8732705116272 and batch: 1150, loss is 4.104771385192871 and perplexity is 60.628882302875056
At time: 1060.5852010250092 and batch: 1200, loss is 4.094756894111633 and perplexity is 60.02474501460066
At time: 1061.2986371517181 and batch: 1250, loss is 4.139583377838135 and perplexity is 62.77666185167071
At time: 1062.021636247635 and batch: 1300, loss is 4.136381673812866 and perplexity is 62.57599097652316
At time: 1062.7358429431915 and batch: 1350, loss is 4.113134336471558 and perplexity is 61.138044774606485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.586258544921875 and perplexity of 98.12660620353982
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc67f09898>
SETTINGS FOR THIS RUN
{'seq_len': 20, 'lr': 5.4106838696977135, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.676365908478051, 'anneal': 6.5125845255230495, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1375 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4115240573883057 and batch: 50, loss is 7.504141874313355 and perplexity is 1815.546628918543
At time: 2.1258773803710938 and batch: 100, loss is 6.712684926986694 and perplexity is 822.7767720321269
At time: 2.8452579975128174 and batch: 150, loss is 6.408935775756836 and perplexity is 607.2470899887957
At time: 3.596583604812622 and batch: 200, loss is 6.290854654312134 and perplexity is 539.614315766314
At time: 4.315377473831177 and batch: 250, loss is 6.250687484741211 and perplexity is 518.3690730246763
At time: 5.041964530944824 and batch: 300, loss is 6.205398073196411 and perplexity is 495.41612857486746
At time: 5.7590296268463135 and batch: 350, loss is 6.175093145370483 and perplexity is 480.6277900893684
At time: 6.488580942153931 and batch: 400, loss is 6.184001445770264 and perplexity is 484.9284943656082
At time: 7.207307577133179 and batch: 450, loss is 6.119943370819092 and perplexity is 454.8389366137824
At time: 7.92476749420166 and batch: 500, loss is 6.100050201416016 and perplexity is 445.88015333574754
At time: 8.64392352104187 and batch: 550, loss is 6.052374248504639 and perplexity is 425.12117609318847
At time: 9.361969709396362 and batch: 600, loss is 5.978997249603271 and perplexity is 395.0440389121358
At time: 10.078015327453613 and batch: 650, loss is 5.991841859817505 and perplexity is 400.1509535603668
At time: 10.794476509094238 and batch: 700, loss is 5.988108816146851 and perplexity is 398.65995728447285
At time: 11.509621858596802 and batch: 750, loss is 5.953808469772339 and perplexity is 385.21763865342405
At time: 12.225879907608032 and batch: 800, loss is 5.906370143890381 and perplexity is 367.3702311916604
At time: 12.943496227264404 and batch: 850, loss is 5.8871409034729005 and perplexity is 360.37346751373667
At time: 13.658989429473877 and batch: 900, loss is 5.925949420928955 and perplexity is 374.6339518807668
At time: 14.37565803527832 and batch: 950, loss is 5.884767284393311 and perplexity is 359.5190925569512
At time: 15.091398239135742 and batch: 1000, loss is 5.887539682388305 and perplexity is 360.51720551218915
At time: 15.808910846710205 and batch: 1050, loss is 5.847507257461547 and perplexity is 346.3698924883396
At time: 16.529344081878662 and batch: 1100, loss is 5.834242219924927 and perplexity is 341.8056223976485
At time: 17.257006645202637 and batch: 1150, loss is 5.8266748046875 and perplexity is 339.2287995636352
At time: 17.97795081138611 and batch: 1200, loss is 5.805281047821045 and perplexity is 332.04850173876684
At time: 18.700504302978516 and batch: 1250, loss is 5.821826000213623 and perplexity is 337.587926793124
At time: 19.42358136177063 and batch: 1300, loss is 5.785589628219604 and perplexity is 325.573951118147
At time: 20.141809225082397 and batch: 1350, loss is 5.770011339187622 and perplexity is 320.5413673054539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 5.16300537109375 and perplexity of 174.68867176301782
Finished 1 epochs...
Completing Train Step...
At time: 22.721309185028076 and batch: 50, loss is 5.427358989715576 and perplexity is 227.54749588358172
At time: 23.43577790260315 and batch: 100, loss is 5.340357170104981 and perplexity is 208.58719809733736
At time: 24.152810096740723 and batch: 150, loss is 5.216334295272827 and perplexity is 184.25751095756107
At time: 24.91956663131714 and batch: 200, loss is 5.1700197696685795 and perplexity is 175.91831529647806
At time: 25.63286852836609 and batch: 250, loss is 5.162642936706543 and perplexity is 174.62537005336662
At time: 26.350003242492676 and batch: 300, loss is 5.119348936080932 and perplexity is 167.22645905727632
At time: 27.06266975402832 and batch: 350, loss is 5.102639513015747 and perplexity is 164.45541713482376
At time: 27.775319576263428 and batch: 400, loss is 5.102710361480713 and perplexity is 164.46706896143525
At time: 28.48803997039795 and batch: 450, loss is 5.048749551773072 and perplexity is 155.82748840147255
At time: 29.19802737236023 and batch: 500, loss is 5.084944467544556 and perplexity is 161.57096655596342
At time: 29.908459186553955 and batch: 550, loss is 5.046181640625 and perplexity is 155.4278505936279
At time: 30.62233877182007 and batch: 600, loss is 4.977097396850586 and perplexity is 145.05273948124344
At time: 31.33563470840454 and batch: 650, loss is 4.988177042007447 and perplexity is 146.66880856758314
At time: 32.051209449768066 and batch: 700, loss is 4.993829307556152 and perplexity is 147.5001669370954
At time: 32.766141414642334 and batch: 750, loss is 4.940954265594482 and perplexity is 139.90369115952763
At time: 33.47787857055664 and batch: 800, loss is 4.891165618896484 and perplexity is 133.1086376044993
At time: 34.19218635559082 and batch: 850, loss is 4.868525629043579 and perplexity is 130.12891709581766
At time: 34.90767312049866 and batch: 900, loss is 4.914228744506836 and perplexity is 136.21421331936463
At time: 35.624510765075684 and batch: 950, loss is 4.870847024917603 and perplexity is 130.4313487230069
At time: 36.335866928100586 and batch: 1000, loss is 4.87545337677002 and perplexity is 131.033547312293
At time: 37.04836845397949 and batch: 1050, loss is 4.809780874252319 and perplexity is 122.70472680614385
At time: 37.76547026634216 and batch: 1100, loss is 4.7901795387268065 and perplexity is 120.32296935680083
At time: 38.48773264884949 and batch: 1150, loss is 4.801525840759277 and perplexity is 121.69596458810328
At time: 39.20126557350159 and batch: 1200, loss is 4.783832778930664 and perplexity is 119.56172663999232
At time: 39.91518020629883 and batch: 1250, loss is 4.824245157241822 and perplexity is 124.49246067475192
At time: 40.6310031414032 and batch: 1300, loss is 4.808891248703003 and perplexity is 122.5956140880824
At time: 41.34635591506958 and batch: 1350, loss is 4.762222738265991 and perplexity is 117.00571014511979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.621146240234375 and perplexity of 101.61043537301713
Finished 2 epochs...
Completing Train Step...
At time: 43.83905506134033 and batch: 50, loss is 4.771386489868164 and perplexity is 118.08284918858344
At time: 44.58358430862427 and batch: 100, loss is 4.771799812316894 and perplexity is 118.13166556872099
At time: 45.29657769203186 and batch: 150, loss is 4.707315282821655 and perplexity is 110.75441612207085
At time: 46.013303995132446 and batch: 200, loss is 4.697902460098266 and perplexity is 109.71679556764143
At time: 46.726860761642456 and batch: 250, loss is 4.698179426193238 and perplexity is 109.74718760865076
At time: 47.44025254249573 and batch: 300, loss is 4.679989681243897 and perplexity is 107.76896052404388
At time: 48.15470838546753 and batch: 350, loss is 4.668252058029175 and perplexity is 106.51140387219937
At time: 48.868778467178345 and batch: 400, loss is 4.685768136978149 and perplexity is 108.39350139503442
At time: 49.58437633514404 and batch: 450, loss is 4.63512957572937 and perplexity is 103.04126877779902
At time: 50.297892808914185 and batch: 500, loss is 4.700417575836181 and perplexity is 109.99309332165144
At time: 51.01551055908203 and batch: 550, loss is 4.677830076217651 and perplexity is 107.53647326590647
At time: 51.7308075428009 and batch: 600, loss is 4.619579000473022 and perplexity is 101.4513121831976
At time: 52.443121910095215 and batch: 650, loss is 4.64114405632019 and perplexity is 103.66287593718171
At time: 53.15826988220215 and batch: 700, loss is 4.660980062484741 and perplexity is 105.73966286730965
At time: 53.88247060775757 and batch: 750, loss is 4.608859949111938 and perplexity is 100.36965786798194
At time: 54.59981966018677 and batch: 800, loss is 4.57136794090271 and perplexity is 96.67626678039838
At time: 55.31526017189026 and batch: 850, loss is 4.545701293945313 and perplexity is 94.22648450698092
At time: 56.03113865852356 and batch: 900, loss is 4.58458028793335 and perplexity is 97.96206265272093
At time: 56.74503755569458 and batch: 950, loss is 4.5583154296875 and perplexity is 95.42259828376298
At time: 57.462838888168335 and batch: 1000, loss is 4.573087577819824 and perplexity is 96.84265788290037
At time: 58.181602239608765 and batch: 1050, loss is 4.509702959060669 and perplexity is 90.89481501892331
At time: 58.899773359298706 and batch: 1100, loss is 4.4947065830230715 and perplexity is 89.54189201901528
At time: 59.66773581504822 and batch: 1150, loss is 4.51649206161499 and perplexity is 91.51400874628402
At time: 60.38252639770508 and batch: 1200, loss is 4.496914157867431 and perplexity is 89.73978079412811
At time: 61.09943699836731 and batch: 1250, loss is 4.5447937107086185 and perplexity is 94.14100492497512
At time: 61.8172881603241 and batch: 1300, loss is 4.539541339874267 and perplexity is 93.64783773862983
At time: 62.53147768974304 and batch: 1350, loss is 4.500055465698242 and perplexity is 90.02212430203204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.523642171223958 and perplexity of 92.17068880911239
Finished 3 epochs...
Completing Train Step...
At time: 65.03314661979675 and batch: 50, loss is 4.5391755962371825 and perplexity is 93.61359290064772
At time: 65.75200510025024 and batch: 100, loss is 4.548344688415527 and perplexity is 94.4758917707369
At time: 66.46734547615051 and batch: 150, loss is 4.496302328109741 and perplexity is 89.68489211876437
At time: 67.18606352806091 and batch: 200, loss is 4.486969499588013 and perplexity is 88.85177212979877
At time: 67.90586400032043 and batch: 250, loss is 4.488802862167359 and perplexity is 89.01481906033095
At time: 68.62004709243774 and batch: 300, loss is 4.481815156936645 and perplexity is 88.39497789936631
At time: 69.3361976146698 and batch: 350, loss is 4.473067054748535 and perplexity is 87.62506216065209
At time: 70.05424094200134 and batch: 400, loss is 4.488838405609131 and perplexity is 89.01798300959757
At time: 70.76896691322327 and batch: 450, loss is 4.438831024169922 and perplexity is 84.6758997161208
At time: 71.48491287231445 and batch: 500, loss is 4.508225555419922 and perplexity is 90.76062583848932
At time: 72.20759224891663 and batch: 550, loss is 4.497578115463257 and perplexity is 89.7993839880411
At time: 72.92525720596313 and batch: 600, loss is 4.439392442703247 and perplexity is 84.72345168257608
At time: 73.64262652397156 and batch: 650, loss is 4.463628940582275 and perplexity is 86.80193730352542
At time: 74.36832070350647 and batch: 700, loss is 4.488503570556641 and perplexity is 88.98818165813097
At time: 75.09034061431885 and batch: 750, loss is 4.433147363662719 and perplexity is 84.19599574651278
At time: 75.8090591430664 and batch: 800, loss is 4.402318096160888 and perplexity is 81.63989862108205
At time: 76.54471611976624 and batch: 850, loss is 4.372555236816407 and perplexity is 79.24586516454748
At time: 77.28122925758362 and batch: 900, loss is 4.4148729705810545 and perplexity is 82.67133854696898
At time: 78.05800080299377 and batch: 950, loss is 4.394003992080688 and perplexity is 80.9639498547516
At time: 78.77994227409363 and batch: 1000, loss is 4.411532821655274 and perplexity is 82.39566461661812
At time: 79.50080609321594 and batch: 1050, loss is 4.346667604446411 and perplexity is 77.22070375619586
At time: 80.22680377960205 and batch: 1100, loss is 4.329053955078125 and perplexity is 75.87247383692451
At time: 80.96412754058838 and batch: 1150, loss is 4.356371469497681 and perplexity is 77.973690577117
At time: 81.6827301979065 and batch: 1200, loss is 4.337569074630737 and perplexity is 76.521295499057
At time: 82.40163516998291 and batch: 1250, loss is 4.383034582138062 and perplexity is 80.08067644911523
At time: 83.1221706867218 and batch: 1300, loss is 4.381264400482178 and perplexity is 79.93904449879857
At time: 83.83967781066895 and batch: 1350, loss is 4.349403867721557 and perplexity is 77.43228927679553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.483758138020833 and perplexity of 88.5668946307559
Finished 4 epochs...
Completing Train Step...
At time: 86.39262437820435 and batch: 50, loss is 4.401326856613159 and perplexity is 81.55901401952697
At time: 87.10781478881836 and batch: 100, loss is 4.408119955062866 and perplexity is 82.11493851833349
At time: 87.82510423660278 and batch: 150, loss is 4.365467119216919 and perplexity is 78.68614716988887
At time: 88.53996872901917 and batch: 200, loss is 4.357825040817261 and perplexity is 78.08711331146787
At time: 89.25292563438416 and batch: 250, loss is 4.358967399597168 and perplexity is 78.1763677815643
At time: 89.96834635734558 and batch: 300, loss is 4.354332942962646 and perplexity is 77.81490104316885
At time: 90.69054055213928 and batch: 350, loss is 4.3501169300079345 and perplexity is 77.48752301223317
At time: 91.407954454422 and batch: 400, loss is 4.362480726242065 and perplexity is 78.45150994662518
At time: 92.12718439102173 and batch: 450, loss is 4.3132280921936035 and perplexity is 74.68117797577008
At time: 92.84300494194031 and batch: 500, loss is 4.385184984207154 and perplexity is 80.25306738993235
At time: 93.55943417549133 and batch: 550, loss is 4.382201385498047 and perplexity is 80.01398128751842
At time: 94.27903699874878 and batch: 600, loss is 4.319662914276123 and perplexity is 75.1632875500691
At time: 94.99526691436768 and batch: 650, loss is 4.345324211120605 and perplexity is 77.11703562728285
At time: 95.71071410179138 and batch: 700, loss is 4.375153579711914 and perplexity is 79.45204083684517
At time: 96.42676210403442 and batch: 750, loss is 4.318441038131714 and perplexity is 75.07150340792246
At time: 97.19531965255737 and batch: 800, loss is 4.2897451210021975 and perplexity is 72.9478732491385
At time: 97.91485047340393 and batch: 850, loss is 4.260808687210083 and perplexity is 70.86726974246456
At time: 98.63102507591248 and batch: 900, loss is 4.303210067749023 and perplexity is 73.93675515814704
At time: 99.3477520942688 and batch: 950, loss is 4.2828856945037845 and perplexity is 72.44920491901512
At time: 100.0627338886261 and batch: 1000, loss is 4.3018276977539065 and perplexity is 73.834617818342
At time: 100.78098130226135 and batch: 1050, loss is 4.239616498947144 and perplexity is 69.38123895785863
At time: 101.50068616867065 and batch: 1100, loss is 4.220010290145874 and perplexity is 68.03418436749926
At time: 102.22029209136963 and batch: 1150, loss is 4.24718279838562 and perplexity is 69.90818920496513
At time: 102.93780732154846 and batch: 1200, loss is 4.228318214416504 and perplexity is 68.60176164898718
At time: 103.65840935707092 and batch: 1250, loss is 4.271071653366089 and perplexity is 71.59832310464046
At time: 104.3847439289093 and batch: 1300, loss is 4.271008577346802 and perplexity is 71.59380710985833
At time: 105.10305881500244 and batch: 1350, loss is 4.2464608955383305 and perplexity is 69.85774049585349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.473881022135417 and perplexity of 87.6964151390835
Finished 5 epochs...
Completing Train Step...
At time: 107.63474822044373 and batch: 50, loss is 4.3034388256073 and perplexity is 73.95367070661348
At time: 108.38262510299683 and batch: 100, loss is 4.309716386795044 and perplexity is 74.4193796282799
At time: 109.10472273826599 and batch: 150, loss is 4.272138929367065 and perplexity is 71.67477906915074
At time: 109.82018852233887 and batch: 200, loss is 4.263437414169312 and perplexity is 71.05380551335169
At time: 110.53756213188171 and batch: 250, loss is 4.265340919494629 and perplexity is 71.18918561802825
At time: 111.2521584033966 and batch: 300, loss is 4.261292600631714 and perplexity is 70.90157166435809
At time: 111.97492742538452 and batch: 350, loss is 4.261029071807862 and perplexity is 70.88288951831818
At time: 112.69234776496887 and batch: 400, loss is 4.270429916381836 and perplexity is 71.55239055257879
At time: 113.41220021247864 and batch: 450, loss is 4.221023626327515 and perplexity is 68.10316081036754
At time: 114.12957715988159 and batch: 500, loss is 4.294108934402466 and perplexity is 73.266899735708
At time: 114.84698629379272 and batch: 550, loss is 4.2955908203125 and perplexity is 73.37555340838804
At time: 115.6176245212555 and batch: 600, loss is 4.2326037788391115 and perplexity is 68.89638979105028
At time: 116.3338782787323 and batch: 650, loss is 4.256778302192688 and perplexity is 70.58222217185912
At time: 117.05061864852905 and batch: 700, loss is 4.29121696472168 and perplexity is 73.0553201713716
At time: 117.77333354949951 and batch: 750, loss is 4.233344826698303 and perplexity is 68.94746423517742
At time: 118.48990941047668 and batch: 800, loss is 4.20626651763916 and perplexity is 67.10553421089845
At time: 119.20909333229065 and batch: 850, loss is 4.1790470790863035 and perplexity is 65.30359439401771
At time: 119.92486214637756 and batch: 900, loss is 4.218585820198059 and perplexity is 67.93734070826034
At time: 120.64332747459412 and batch: 950, loss is 4.202113642692566 and perplexity is 66.82743118233498
At time: 121.36232733726501 and batch: 1000, loss is 4.217280902862549 and perplexity is 67.84874591165631
At time: 122.08161902427673 and batch: 1050, loss is 4.158622875213623 and perplexity is 63.983348845129875
At time: 122.80113387107849 and batch: 1100, loss is 4.139499154090881 and perplexity is 62.77137478862073
At time: 123.52164268493652 and batch: 1150, loss is 4.162137670516968 and perplexity is 64.2086329008877
At time: 124.23587155342102 and batch: 1200, loss is 4.144459776878357 and perplexity is 63.083533511507895
At time: 124.95228576660156 and batch: 1250, loss is 4.190576200485229 and perplexity is 66.06084428671721
At time: 125.66916108131409 and batch: 1300, loss is 4.188072257041931 and perplexity is 65.89563858798608
At time: 126.38419079780579 and batch: 1350, loss is 4.1657701587677005 and perplexity is 64.44229413440912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.46718505859375 and perplexity of 87.1111647349472
Finished 6 epochs...
Completing Train Step...
At time: 128.92051339149475 and batch: 50, loss is 4.227272262573242 and perplexity is 68.53004502254649
At time: 129.65829610824585 and batch: 100, loss is 4.234753427505493 and perplexity is 69.04465212235125
At time: 130.3682198524475 and batch: 150, loss is 4.202044172286987 and perplexity is 66.82278881484243
At time: 131.07856798171997 and batch: 200, loss is 4.193759341239929 and perplexity is 66.27146028480283
At time: 131.7869415283203 and batch: 250, loss is 4.192366905212403 and perplexity is 66.17924573223753
At time: 132.49786257743835 and batch: 300, loss is 4.189364910125732 and perplexity is 65.98087386636998
At time: 133.20936131477356 and batch: 350, loss is 4.191699228286743 and perplexity is 66.13507412472707
At time: 133.9201364517212 and batch: 400, loss is 4.198515710830688 and perplexity is 66.58742266449477
At time: 134.68009638786316 and batch: 450, loss is 4.1528662109375 and perplexity is 63.61607633309992
At time: 135.3910219669342 and batch: 500, loss is 4.221085205078125 and perplexity is 68.1073546470471
At time: 136.10052371025085 and batch: 550, loss is 4.229169754981995 and perplexity is 68.66020371125734
At time: 136.8119351863861 and batch: 600, loss is 4.164189682006836 and perplexity is 64.34052502914997
At time: 137.52587795257568 and batch: 650, loss is 4.192423219680786 and perplexity is 66.1829726862187
At time: 138.24047946929932 and batch: 700, loss is 4.226329355239868 and perplexity is 68.4654579951144
At time: 138.9513373374939 and batch: 750, loss is 4.168712887763977 and perplexity is 64.63220963982717
At time: 139.66072249412537 and batch: 800, loss is 4.143150033950806 and perplexity is 63.00096438361538
At time: 140.37303686141968 and batch: 850, loss is 4.113511037826538 and perplexity is 61.16107989732251
At time: 141.0822777748108 and batch: 900, loss is 4.155230789184571 and perplexity is 63.76667950968739
At time: 141.79277539253235 and batch: 950, loss is 4.137941808700561 and perplexity is 62.67369415843147
At time: 142.50277400016785 and batch: 1000, loss is 4.152287073135376 and perplexity is 63.57924452484833
At time: 143.21302461624146 and batch: 1050, loss is 4.09586775302887 and perplexity is 60.09146108706632
At time: 143.92450642585754 and batch: 1100, loss is 4.0774477767944335 and perplexity is 58.99470987761267
At time: 144.6340844631195 and batch: 1150, loss is 4.0964555311203 and perplexity is 60.1267919137011
At time: 145.34576559066772 and batch: 1200, loss is 4.080429081916809 and perplexity is 59.17085354690926
At time: 146.0561602115631 and batch: 1250, loss is 4.123829736709594 and perplexity is 61.79544997230588
At time: 146.76415348052979 and batch: 1300, loss is 4.125116486549377 and perplexity is 61.87501643774702
At time: 147.47189211845398 and batch: 1350, loss is 4.100484247207642 and perplexity is 60.369514289040076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.471507161458334 and perplexity of 87.48848296631311
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 149.98268818855286 and batch: 50, loss is 4.186819944381714 and perplexity is 65.81316829560299
At time: 150.6935532093048 and batch: 100, loss is 4.204110774993897 and perplexity is 66.9610277643843
At time: 151.40874910354614 and batch: 150, loss is 4.174425144195556 and perplexity is 65.00246187611536
At time: 152.13091278076172 and batch: 200, loss is 4.165740351676941 and perplexity is 64.44037332572613
At time: 152.9015998840332 and batch: 250, loss is 4.1534271144866945 and perplexity is 63.651768825187965
At time: 153.61364006996155 and batch: 300, loss is 4.143746047019959 and perplexity is 63.03852497394671
At time: 154.33914470672607 and batch: 350, loss is 4.13312524318695 and perplexity is 62.37254803175965
At time: 155.07122373580933 and batch: 400, loss is 4.134955554008484 and perplexity is 62.48681372035247
At time: 155.80942916870117 and batch: 450, loss is 4.078505802154541 and perplexity is 59.05716080828384
At time: 156.54200077056885 and batch: 500, loss is 4.132458271980286 and perplexity is 62.33096120828476
At time: 157.28521990776062 and batch: 550, loss is 4.133966293334961 and perplexity is 62.42502853878908
At time: 158.0254282951355 and batch: 600, loss is 4.0672292804718015 and perplexity is 58.3949422322652
At time: 158.76335740089417 and batch: 650, loss is 4.081928610801697 and perplexity is 59.25964850961341
At time: 159.49386548995972 and batch: 700, loss is 4.096276435852051 and perplexity is 60.11602445400366
At time: 160.2104115486145 and batch: 750, loss is 4.042740020751953 and perplexity is 56.98226167693583
At time: 160.93221831321716 and batch: 800, loss is 4.012395310401916 and perplexity is 55.27912276546196
At time: 161.68391132354736 and batch: 850, loss is 3.966134238243103 and perplexity is 52.78010066041906
At time: 162.41561317443848 and batch: 900, loss is 3.982897930145264 and perplexity is 53.672347789497195
At time: 163.12971806526184 and batch: 950, loss is 3.9597725820541383 and perplexity is 52.445397568201656
At time: 163.8458650112152 and batch: 1000, loss is 3.9596968698501587 and perplexity is 52.44142696187676
At time: 164.5593864917755 and batch: 1050, loss is 3.8959398603439332 and perplexity is 49.20227490539882
At time: 165.2732880115509 and batch: 1100, loss is 3.8631664514541626 and perplexity is 47.615886304723944
At time: 165.99198985099792 and batch: 1150, loss is 3.86850200176239 and perplexity is 47.87062223544015
At time: 166.7145426273346 and batch: 1200, loss is 3.83237087726593 and perplexity is 46.171876421419825
At time: 167.43170976638794 and batch: 1250, loss is 3.8624835443496703 and perplexity is 47.5833801782754
At time: 168.15879321098328 and batch: 1300, loss is 3.8538496494293213 and perplexity is 47.17431871181135
At time: 168.87595057487488 and batch: 1350, loss is 3.815231537818909 and perplexity is 45.38726403456174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.369604899088542 and perplexity of 79.01240765736496
Finished 8 epochs...
Completing Train Step...
At time: 171.4791078567505 and batch: 50, loss is 4.096357479095459 and perplexity is 60.120896649032765
At time: 172.19942331314087 and batch: 100, loss is 4.106853404045105 and perplexity is 60.7552442771797
At time: 172.91299295425415 and batch: 150, loss is 4.080688943862915 and perplexity is 59.186231798089324
At time: 173.6326379776001 and batch: 200, loss is 4.076017851829529 and perplexity is 58.91041215323518
At time: 174.353746175766 and batch: 250, loss is 4.065635356903076 and perplexity is 58.30193929702228
At time: 175.07305026054382 and batch: 300, loss is 4.06180703163147 and perplexity is 58.079167203525785
At time: 175.79247760772705 and batch: 350, loss is 4.054644193649292 and perplexity is 57.6646418998121
At time: 176.51002836227417 and batch: 400, loss is 4.058503799438476 and perplexity is 57.887634740931404
At time: 177.23645877838135 and batch: 450, loss is 4.00624361038208 and perplexity is 54.94010601864179
At time: 177.95481896400452 and batch: 500, loss is 4.065968618392945 and perplexity is 58.32137232613488
At time: 178.67024540901184 and batch: 550, loss is 4.07068859577179 and perplexity is 58.597298554734905
At time: 179.385639667511 and batch: 600, loss is 4.006034350395202 and perplexity is 54.92861045559988
At time: 180.10134601593018 and batch: 650, loss is 4.0255406379699705 and perplexity is 56.010582044842565
At time: 180.8186273574829 and batch: 700, loss is 4.044170279502868 and perplexity is 57.06381936574211
At time: 181.53342199325562 and batch: 750, loss is 3.992771654129028 and perplexity is 54.20491863921104
At time: 182.25191855430603 and batch: 800, loss is 3.966443667411804 and perplexity is 52.796434890103605
At time: 182.9656834602356 and batch: 850, loss is 3.9241779327392576 and perplexity is 50.61145493255939
At time: 183.6847379207611 and batch: 900, loss is 3.9452861833572386 and perplexity is 51.6911291364093
At time: 184.39958834648132 and batch: 950, loss is 3.927490382194519 and perplexity is 50.77938078829072
At time: 185.11716723442078 and batch: 1000, loss is 3.9329152345657348 and perplexity is 51.05559997933095
At time: 185.83321690559387 and batch: 1050, loss is 3.873580961227417 and perplexity is 48.114373663169424
At time: 186.55198740959167 and batch: 1100, loss is 3.8450551557540895 and perplexity is 46.761263432532736
At time: 187.26611804962158 and batch: 1150, loss is 3.8559938621520997 and perplexity is 47.275579009194004
At time: 187.98463416099548 and batch: 1200, loss is 3.8273433589935304 and perplexity is 45.940329011172786
At time: 188.70016527175903 and batch: 1250, loss is 3.8628531742095946 and perplexity is 47.600971667394965
At time: 189.41764569282532 and batch: 1300, loss is 3.8610402822494505 and perplexity is 47.51475442345158
At time: 190.13289499282837 and batch: 1350, loss is 3.829594569206238 and perplexity is 46.0438668480087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.361750895182292 and perplexity of 78.39427448662876
Finished 9 epochs...
Completing Train Step...
At time: 192.69537091255188 and batch: 50, loss is 4.065108027458191 and perplexity is 58.27120307250738
At time: 193.46568655967712 and batch: 100, loss is 4.070734314918518 and perplexity is 58.59997763446755
At time: 194.18449211120605 and batch: 150, loss is 4.044373526573181 and perplexity is 57.07541859856416
At time: 194.9031069278717 and batch: 200, loss is 4.038552851676941 and perplexity is 56.74416613403009
At time: 195.62099289894104 and batch: 250, loss is 4.029418406486511 and perplexity is 56.2281997793943
At time: 196.34032034873962 and batch: 300, loss is 4.026903147697449 and perplexity is 56.08694902128033
At time: 197.06584882736206 and batch: 350, loss is 4.021046667098999 and perplexity is 55.759436862905375
At time: 197.78278923034668 and batch: 400, loss is 4.025654168128967 and perplexity is 56.01694129610418
At time: 198.50171422958374 and batch: 450, loss is 3.974119420051575 and perplexity is 53.203246560109086
At time: 199.21770310401917 and batch: 500, loss is 4.035181393623352 and perplexity is 56.5531776937903
At time: 199.93873572349548 and batch: 550, loss is 4.040961427688599 and perplexity is 56.88100349682463
At time: 200.6585190296173 and batch: 600, loss is 3.9774590730667114 and perplexity is 53.38122396887419
At time: 201.37677216529846 and batch: 650, loss is 3.99835259437561 and perplexity is 54.50827878122677
At time: 202.0950632095337 and batch: 700, loss is 4.018677544593811 and perplexity is 55.62749228430638
At time: 202.81320214271545 and batch: 750, loss is 3.968824095726013 and perplexity is 52.92226272125499
At time: 203.53058290481567 and batch: 800, loss is 3.94417546749115 and perplexity is 51.63374685274722
At time: 204.2499098777771 and batch: 850, loss is 3.9036490440368654 and perplexity is 49.583050127987576
At time: 204.96389150619507 and batch: 900, loss is 3.9260511922836305 and perplexity is 50.706352179402764
At time: 205.68099880218506 and batch: 950, loss is 3.910547866821289 and perplexity is 49.92629744376516
At time: 206.3956480026245 and batch: 1000, loss is 3.918342933654785 and perplexity is 50.31699705554089
At time: 207.1132140159607 and batch: 1050, loss is 3.8608914613723755 and perplexity is 47.507683762168334
At time: 207.8297131061554 and batch: 1100, loss is 3.8339639282226563 and perplexity is 46.24548919220633
At time: 208.60333967208862 and batch: 1150, loss is 3.847343058586121 and perplexity is 46.86837113886044
At time: 209.3270480632782 and batch: 1200, loss is 3.821810984611511 and perplexity is 45.68687166877111
At time: 210.0464780330658 and batch: 1250, loss is 3.8593085384368897 and perplexity is 47.432542247232924
At time: 210.76331996917725 and batch: 1300, loss is 3.860355076789856 and perplexity is 47.48220820600567
At time: 211.4858362674713 and batch: 1350, loss is 3.8317479038238527 and perplexity is 46.143121526336344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.359342447916666 and perplexity of 78.2056931958052
Finished 10 epochs...
Completing Train Step...
At time: 214.01476526260376 and batch: 50, loss is 4.041345820426941 and perplexity is 56.90287234436227
At time: 214.76009130477905 and batch: 100, loss is 4.044716477394104 and perplexity is 57.094996017081066
At time: 215.4788534641266 and batch: 150, loss is 4.018998165130615 and perplexity is 55.64533046023357
At time: 216.20088958740234 and batch: 200, loss is 4.012723250389099 and perplexity is 55.29725397308465
At time: 216.92179465293884 and batch: 250, loss is 4.004553112983704 and perplexity is 54.847308371540855
At time: 217.6401243209839 and batch: 300, loss is 4.002701873779297 and perplexity is 54.74586680929651
At time: 218.35545921325684 and batch: 350, loss is 3.99796133518219 and perplexity is 54.486956087658065
At time: 219.07324743270874 and batch: 400, loss is 4.003036589622497 and perplexity is 54.76419418532777
At time: 219.79422330856323 and batch: 450, loss is 3.952065291404724 and perplexity is 52.04273934114799
At time: 220.51389813423157 and batch: 500, loss is 4.013753304481506 and perplexity is 55.35424248142282
At time: 221.22995114326477 and batch: 550, loss is 4.020088710784912 and perplexity is 55.70604733479327
At time: 221.95168662071228 and batch: 600, loss is 3.9573332548141478 and perplexity is 52.31762198788701
At time: 222.6675410270691 and batch: 650, loss is 3.9791850996017457 and perplexity is 53.47344093949206
At time: 223.38648319244385 and batch: 700, loss is 4.0003195476531985 and perplexity is 54.615599531680985
At time: 224.10715222358704 and batch: 750, loss is 3.951467227935791 and perplexity is 52.01162378539091
At time: 224.8263885974884 and batch: 800, loss is 3.927965965270996 and perplexity is 50.80353634595954
At time: 225.54525446891785 and batch: 850, loss is 3.888638696670532 and perplexity is 48.84434927002768
At time: 226.26648211479187 and batch: 900, loss is 3.9117102909088133 and perplexity is 49.98436671853781
At time: 227.034677028656 and batch: 950, loss is 3.897524275779724 and perplexity is 49.28029353987802
At time: 227.75028586387634 and batch: 1000, loss is 3.906533613204956 and perplexity is 49.72628234795298
At time: 228.46572160720825 and batch: 1050, loss is 3.8502749490737913 and perplexity is 47.00598570721937
At time: 229.18309140205383 and batch: 1100, loss is 3.8242778968811035 and perplexity is 45.7997163047308
At time: 229.89775896072388 and batch: 1150, loss is 3.839097189903259 and perplexity is 46.48348972667245
At time: 230.61385345458984 and batch: 1200, loss is 3.815115089416504 and perplexity is 45.38197906789433
At time: 231.32989382743835 and batch: 1250, loss is 3.853751263618469 and perplexity is 47.169677656524236
At time: 232.05194306373596 and batch: 1300, loss is 3.8564394426345827 and perplexity is 47.296648778288926
At time: 232.76786041259766 and batch: 1350, loss is 3.8293627977371214 and perplexity is 46.033196429942535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.35870849609375 and perplexity of 78.15613026595608
Finished 11 epochs...
Completing Train Step...
At time: 235.2839479446411 and batch: 50, loss is 4.02174418926239 and perplexity is 55.798343873609085
At time: 235.99785733222961 and batch: 100, loss is 4.023688373565673 and perplexity is 55.906931661049505
At time: 236.72207140922546 and batch: 150, loss is 3.99893283367157 and perplexity is 54.5399158041651
At time: 237.4623408317566 and batch: 200, loss is 3.992505521774292 and perplexity is 54.19049487597548
At time: 238.2070689201355 and batch: 250, loss is 3.9847505569458006 and perplexity is 53.77187478418202
At time: 238.95527696609497 and batch: 300, loss is 3.9835662269592285 and perplexity is 53.708228836782276
At time: 239.70561337471008 and batch: 350, loss is 3.979673366546631 and perplexity is 53.499556628327035
At time: 240.44939279556274 and batch: 400, loss is 3.98506311416626 and perplexity is 53.78868419871854
At time: 241.187397480011 and batch: 450, loss is 3.9345150661468504 and perplexity is 51.13734571283832
At time: 241.92561793327332 and batch: 500, loss is 3.996658053398132 and perplexity is 54.41599048445408
At time: 242.64300203323364 and batch: 550, loss is 4.003374743461609 and perplexity is 54.782716039278895
At time: 243.3611283302307 and batch: 600, loss is 3.941040048599243 and perplexity is 51.47210696420675
At time: 244.0771746635437 and batch: 650, loss is 3.9636485290527346 and perplexity is 52.64906760164648
At time: 244.80228400230408 and batch: 700, loss is 3.9851206588745116 and perplexity is 53.791779541917464
At time: 245.57739901542664 and batch: 750, loss is 3.9372489929199217 and perplexity is 51.27734275501837
At time: 246.29520773887634 and batch: 800, loss is 3.91466392993927 and perplexity is 50.13222074120677
At time: 247.01120448112488 and batch: 850, loss is 3.8761583518981935 and perplexity is 48.23854314887861
At time: 247.73184466362 and batch: 900, loss is 3.899389705657959 and perplexity is 49.37230826868471
At time: 248.44969391822815 and batch: 950, loss is 3.886021695137024 and perplexity is 48.716690647348386
At time: 249.16771459579468 and batch: 1000, loss is 3.8960178661346436 and perplexity is 49.20611311745701
At time: 249.89219880104065 and batch: 1050, loss is 3.8405098152160644 and perplexity is 46.54919988187504
At time: 250.61234331130981 and batch: 1100, loss is 3.8151606559753417 and perplexity is 45.38404701562797
At time: 251.33053541183472 and batch: 1150, loss is 3.830910439491272 and perplexity is 46.10449448450659
At time: 252.04736590385437 and batch: 1200, loss is 3.8079227113723757 and perplexity is 45.05674572067392
At time: 252.7705385684967 and batch: 1250, loss is 3.8472024488449095 and perplexity is 46.86178145262151
At time: 253.48593091964722 and batch: 1300, loss is 3.8510063695907593 and perplexity is 47.040379426188466
At time: 254.2024643421173 and batch: 1350, loss is 3.824867959022522 and perplexity is 45.826748958098264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.358807779947917 and perplexity of 78.16389029301195
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 256.73266410827637 and batch: 50, loss is 4.019758062362671 and perplexity is 55.68763126292306
At time: 257.45311403274536 and batch: 100, loss is 4.0314142560958866 and perplexity is 56.340534874648114
At time: 258.1680898666382 and batch: 150, loss is 4.009646024703979 and perplexity is 55.127353388134246
At time: 258.8839361667633 and batch: 200, loss is 4.0047957229614255 and perplexity is 54.86061649007886
At time: 259.602459192276 and batch: 250, loss is 3.997135910987854 and perplexity is 54.44199979238698
At time: 260.3231189250946 and batch: 300, loss is 3.994440641403198 and perplexity is 54.295461495036974
At time: 261.040118932724 and batch: 350, loss is 3.98740553855896 and perplexity is 53.91482780789176
At time: 261.75901985168457 and batch: 400, loss is 3.9902864360809325 and perplexity is 54.07037485168648
At time: 262.4749388694763 and batch: 450, loss is 3.9357822227478025 and perplexity is 51.20218581061639
At time: 263.19401836395264 and batch: 500, loss is 3.9973081159591675 and perplexity is 54.45137578267237
At time: 263.913134098053 and batch: 550, loss is 4.001967077255249 and perplexity is 54.705654512396336
At time: 264.70942306518555 and batch: 600, loss is 3.9399276113510133 and perplexity is 51.41487931216247
At time: 265.43229699134827 and batch: 650, loss is 3.954937334060669 and perplexity is 52.19242315472624
At time: 266.15363931655884 and batch: 700, loss is 3.973355269432068 and perplexity is 53.16260679571852
At time: 266.8702600002289 and batch: 750, loss is 3.9219091939926147 and perplexity is 50.49676091830285
At time: 267.58694648742676 and batch: 800, loss is 3.896976351737976 and perplexity is 49.2532990783957
At time: 268.30485820770264 and batch: 850, loss is 3.855896372795105 and perplexity is 47.27097036804523
At time: 269.0246524810791 and batch: 900, loss is 3.8684979486465454 and perplexity is 47.870428210655874
At time: 269.7447557449341 and batch: 950, loss is 3.8531443786621096 and perplexity is 47.14105977351785
At time: 270.464316368103 and batch: 1000, loss is 3.8638061237335206 and perplexity is 47.64635461107671
At time: 271.18652176856995 and batch: 1050, loss is 3.804141240119934 and perplexity is 44.88668667135266
At time: 271.9030303955078 and batch: 1100, loss is 3.7746635150909422 and perplexity is 43.58284087992652
At time: 272.62364745140076 and batch: 1150, loss is 3.7849510955810546 and perplexity is 44.0335170725263
At time: 273.3412890434265 and batch: 1200, loss is 3.7558808946609497 and perplexity is 42.77188074184469
At time: 274.0573790073395 and batch: 1250, loss is 3.7923876667022705 and perplexity is 44.36219606156053
At time: 274.78046679496765 and batch: 1300, loss is 3.799308876991272 and perplexity is 44.670301149504134
At time: 275.4983546733856 and batch: 1350, loss is 3.769560203552246 and perplexity is 43.36099063212444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.339490966796875 and perplexity of 76.66850259018578
Finished 13 epochs...
Completing Train Step...
At time: 278.00382375717163 and batch: 50, loss is 4.006699247360229 and perplexity is 54.96514446631545
At time: 278.7510507106781 and batch: 100, loss is 4.013801355361938 and perplexity is 55.35690236541407
At time: 279.46165108680725 and batch: 150, loss is 3.9912083339691162 and perplexity is 54.120245200228005
At time: 280.17996883392334 and batch: 200, loss is 3.9865923261642457 and perplexity is 53.87100142415399
At time: 280.89518308639526 and batch: 250, loss is 3.978662223815918 and perplexity is 53.44548828056102
At time: 281.61203694343567 and batch: 300, loss is 3.9757729673385622 and perplexity is 53.291293418878084
At time: 282.3226342201233 and batch: 350, loss is 3.9702012157440185 and perplexity is 52.99519323415538
At time: 283.08418107032776 and batch: 400, loss is 3.9736041450500488 and perplexity is 53.175839318896166
At time: 283.80250000953674 and batch: 450, loss is 3.9203691196441652 and perplexity is 50.41905200624319
At time: 284.5136048793793 and batch: 500, loss is 3.9836709976196287 and perplexity is 53.71385617817142
At time: 285.22570395469666 and batch: 550, loss is 3.988972587585449 and perplexity is 53.99938121868875
At time: 285.94189715385437 and batch: 600, loss is 3.9271178150177004 and perplexity is 50.76046558156122
At time: 286.6587243080139 and batch: 650, loss is 3.943743257522583 and perplexity is 51.6114350546811
At time: 287.37019634246826 and batch: 700, loss is 3.9634136056900022 and perplexity is 52.63670055835136
At time: 288.08577966690063 and batch: 750, loss is 3.9132015609741213 and perplexity is 50.05896251577387
At time: 288.8033471107483 and batch: 800, loss is 3.8895986700057983 and perplexity is 48.89126105633469
At time: 289.5193030834198 and batch: 850, loss is 3.8494855690002443 and perplexity is 46.96889476011794
At time: 290.2409167289734 and batch: 900, loss is 3.863592758178711 and perplexity is 47.63618960466014
At time: 290.95714354515076 and batch: 950, loss is 3.849609079360962 and perplexity is 46.97469626351778
At time: 291.67149448394775 and batch: 1000, loss is 3.8613050842285155 and perplexity is 47.52733809047413
At time: 292.3890013694763 and batch: 1050, loss is 3.8026325368881224 and perplexity is 44.81901704163651
At time: 293.1014721393585 and batch: 1100, loss is 3.774808430671692 and perplexity is 43.58915717027674
At time: 293.81748032569885 and batch: 1150, loss is 3.786746034622192 and perplexity is 44.11262552762631
At time: 294.5307285785675 and batch: 1200, loss is 3.7595530414581297 and perplexity is 42.92923410221801
At time: 295.24951791763306 and batch: 1250, loss is 3.7976891803741455 and perplexity is 44.598007376640666
At time: 295.96419191360474 and batch: 1300, loss is 3.8048055458068846 and perplexity is 44.91651505906365
At time: 296.68127489089966 and batch: 1350, loss is 3.7759146595001223 and perplexity is 43.637403433328934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.337718098958334 and perplexity of 76.5326998834098
Finished 14 epochs...
Completing Train Step...
At time: 299.23530077934265 and batch: 50, loss is 4.001870303153992 and perplexity is 54.700360678004394
At time: 300.0066487789154 and batch: 100, loss is 4.007177429199219 and perplexity is 54.9914340852849
At time: 300.72330117225647 and batch: 150, loss is 3.983882474899292 and perplexity is 53.725216639553594
At time: 301.4723663330078 and batch: 200, loss is 3.9788230419158936 and perplexity is 53.454083973591544
At time: 302.19368386268616 and batch: 250, loss is 3.9705279588699343 and perplexity is 53.01251187847113
At time: 302.9123239517212 and batch: 300, loss is 3.9676870107650757 and perplexity is 52.86211981249461
At time: 303.6328549385071 and batch: 350, loss is 3.962525086402893 and perplexity is 52.58995260599962
At time: 304.35583877563477 and batch: 400, loss is 3.9657847595214846 and perplexity is 52.76165836109667
At time: 305.07386469841003 and batch: 450, loss is 3.9131541061401367 and perplexity is 50.05658703238277
At time: 305.7938244342804 and batch: 500, loss is 3.9771387338638307 and perplexity is 53.36412660876276
At time: 306.51077795028687 and batch: 550, loss is 3.9824665641784667 and perplexity is 53.649200358168684
At time: 307.2286148071289 and batch: 600, loss is 3.9208838319778443 and perplexity is 50.445009994038365
At time: 307.9474332332611 and batch: 650, loss is 3.9380347776412963 and perplexity is 51.317651542448516
At time: 308.66330122947693 and batch: 700, loss is 3.958512396812439 and perplexity is 52.37934827810236
At time: 309.38001585006714 and batch: 750, loss is 3.908743815422058 and perplexity is 49.83630903326861
At time: 310.0975115299225 and batch: 800, loss is 3.885872178077698 and perplexity is 48.709407215534924
At time: 310.81299233436584 and batch: 850, loss is 3.846342992782593 and perplexity is 46.8215231130762
At time: 311.5445110797882 and batch: 900, loss is 3.861248517036438 and perplexity is 47.52464967845011
At time: 312.28159165382385 and batch: 950, loss is 3.847993941307068 and perplexity is 46.89888688180124
At time: 313.0181107521057 and batch: 1000, loss is 3.8601924133300782 and perplexity is 47.47448521388233
At time: 313.7640640735626 and batch: 1050, loss is 3.801993522644043 and perplexity is 44.79038620007332
At time: 314.5124583244324 and batch: 1100, loss is 3.77502103805542 and perplexity is 43.5984255321678
At time: 315.2390179634094 and batch: 1150, loss is 3.787756338119507 and perplexity is 44.15721518822485
At time: 315.97548055648804 and batch: 1200, loss is 3.7614278554916383 and perplexity is 43.00979392650835
At time: 316.7043778896332 and batch: 1250, loss is 3.8002278852462767 and perplexity is 44.711372394530045
At time: 317.42037081718445 and batch: 1300, loss is 3.807365050315857 and perplexity is 45.03162633295518
At time: 318.1415934562683 and batch: 1350, loss is 3.7787311267852783 and perplexity is 43.76047999169957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.336956380208333 and perplexity of 76.4744256880117
Finished 15 epochs...
Completing Train Step...
At time: 320.69419145584106 and batch: 50, loss is 3.9976004123687745 and perplexity is 54.46729405062476
At time: 321.40941166877747 and batch: 100, loss is 4.001866645812989 and perplexity is 54.700160620498224
At time: 322.1266257762909 and batch: 150, loss is 3.9782713413238526 and perplexity is 53.424601457324954
At time: 322.84569025039673 and batch: 200, loss is 3.9730149507522583 and perplexity is 53.14451764577096
At time: 323.564786195755 and batch: 250, loss is 3.964518179893494 and perplexity is 52.69487382237372
At time: 324.28004455566406 and batch: 300, loss is 3.961854729652405 and perplexity is 52.55471039001078
At time: 324.9964120388031 and batch: 350, loss is 3.956956443786621 and perplexity is 52.29791184472087
At time: 325.7128093242645 and batch: 400, loss is 3.9602021980285644 and perplexity is 52.46793378939578
At time: 326.4283263683319 and batch: 450, loss is 3.9079671335220336 and perplexity is 49.797617101684935
At time: 327.1465907096863 and batch: 500, loss is 3.972461953163147 and perplexity is 53.115136980105746
At time: 327.86528849601746 and batch: 550, loss is 3.977819209098816 and perplexity is 53.400451933196784
At time: 328.5803370475769 and batch: 600, loss is 3.9164692401885985 and perplexity is 50.22280669640643
At time: 329.2971968650818 and batch: 650, loss is 3.9339292573928835 and perplexity is 51.10739778080192
At time: 330.0124409198761 and batch: 700, loss is 3.955013270378113 and perplexity is 52.19638660562211
At time: 330.7297465801239 and batch: 750, loss is 3.9054646348953246 and perplexity is 49.67315443205844
At time: 331.44706988334656 and batch: 800, loss is 3.8830911207199095 and perplexity is 48.5741317518047
At time: 332.1667892932892 and batch: 850, loss is 3.844002437591553 and perplexity is 46.712062902903455
At time: 332.888108253479 and batch: 900, loss is 3.8593708086013794 and perplexity is 47.435495971404315
At time: 333.60849118232727 and batch: 950, loss is 3.8466502809524536 and perplexity is 46.835913024035186
At time: 334.3242392539978 and batch: 1000, loss is 3.8591807317733764 and perplexity is 47.4264804396443
At time: 335.04123973846436 and batch: 1050, loss is 3.8012752294540406 and perplexity is 44.758225122612465
At time: 335.7589604854584 and batch: 1100, loss is 3.774845323562622 and perplexity is 43.590765329962615
At time: 336.4827971458435 and batch: 1150, loss is 3.788060073852539 and perplexity is 44.17062934942562
At time: 337.20003271102905 and batch: 1200, loss is 3.762267894744873 and perplexity is 43.0459390212038
At time: 337.91521859169006 and batch: 1250, loss is 3.80143901348114 and perplexity is 44.76555640532598
At time: 338.63431906700134 and batch: 1300, loss is 3.8086109066009524 and perplexity is 45.08776423026501
At time: 339.3496940135956 and batch: 1350, loss is 3.780134735107422 and perplexity is 43.82194569239021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.336543782552083 and perplexity of 76.44287902769638
Finished 16 epochs...
Completing Train Step...
At time: 341.9015808105469 and batch: 50, loss is 3.993692011833191 and perplexity is 54.254829518091995
At time: 342.6178526878357 and batch: 100, loss is 3.997268099784851 and perplexity is 54.44919689052304
At time: 343.33530616760254 and batch: 150, loss is 3.97354519367218 and perplexity is 53.172704622297246
At time: 344.05106353759766 and batch: 200, loss is 3.968181471824646 and perplexity is 52.88826453550927
At time: 344.76799988746643 and batch: 250, loss is 3.9595641469955445 and perplexity is 52.43446724785711
At time: 345.4880473613739 and batch: 300, loss is 3.9571002674102784 and perplexity is 52.30543406083531
At time: 346.2111508846283 and batch: 350, loss is 3.9524272727966308 and perplexity is 52.06158125437859
At time: 346.9324367046356 and batch: 400, loss is 3.955686664581299 and perplexity is 52.23154718692675
At time: 347.6519069671631 and batch: 450, loss is 3.903736114501953 and perplexity is 49.58736753517929
At time: 348.3701457977295 and batch: 500, loss is 3.968644642829895 and perplexity is 52.91276652002643
At time: 349.0899679660797 and batch: 550, loss is 3.9740421676635744 and perplexity is 53.19913664101552
At time: 349.811115026474 and batch: 600, loss is 3.9128897619247436 and perplexity is 50.04335661192804
At time: 350.5288636684418 and batch: 650, loss is 3.9305534887313844 and perplexity is 50.93516190679961
At time: 351.25024366378784 and batch: 700, loss is 3.9521116733551027 and perplexity is 52.04515324088194
At time: 351.96908688545227 and batch: 750, loss is 3.902701148986816 and perplexity is 49.536072868478236
At time: 352.6897110939026 and batch: 800, loss is 3.8807003355026244 and perplexity is 48.45814014642701
At time: 353.40791606903076 and batch: 850, loss is 3.841971549987793 and perplexity is 46.61729222028367
At time: 354.1253864765167 and batch: 900, loss is 3.8576164627075196 and perplexity is 47.35235065796818
At time: 354.84414315223694 and batch: 950, loss is 3.8453357458114623 and perplexity is 46.77438601906965
At time: 355.566330909729 and batch: 1000, loss is 3.8581098413467405 and perplexity is 47.375719060561025
At time: 356.28163480758667 and batch: 1050, loss is 3.8004099130630493 and perplexity is 44.71951184881299
At time: 357.050904750824 and batch: 1100, loss is 3.7743694019317626 and perplexity is 43.57002447773796
At time: 357.76851439476013 and batch: 1150, loss is 3.7878975296020507 and perplexity is 44.16345025106081
At time: 358.48474073410034 and batch: 1200, loss is 3.7624987363815308 and perplexity is 43.0558769632202
At time: 359.2026252746582 and batch: 1250, loss is 3.801921181678772 and perplexity is 44.78714613749679
At time: 359.9210524559021 and batch: 1300, loss is 3.8091452169418334 and perplexity is 45.111861526084326
At time: 360.63664078712463 and batch: 1350, loss is 3.7808012056350706 and perplexity is 43.851161462301775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3363045247395835 and perplexity of 76.42459165946411
Finished 17 epochs...
Completing Train Step...
At time: 363.152556180954 and batch: 50, loss is 3.9900593852996824 and perplexity is 54.058099524447975
At time: 363.8971481323242 and batch: 100, loss is 3.9931381702423097 and perplexity is 54.2247892565397
At time: 364.6153268814087 and batch: 150, loss is 3.9693799018859863 and perplexity is 52.951685416775454
At time: 365.33316683769226 and batch: 200, loss is 3.963943395614624 and perplexity is 52.66459434024239
At time: 366.05047249794006 and batch: 250, loss is 3.9552536010742188 and perplexity is 52.208932507071474
At time: 366.76664328575134 and batch: 300, loss is 3.952982044219971 and perplexity is 52.09047154492525
At time: 367.48282980918884 and batch: 350, loss is 3.9485197257995606 and perplexity is 51.8585451238416
At time: 368.201425075531 and batch: 400, loss is 3.951793327331543 and perplexity is 52.02858751025929
At time: 368.92341208457947 and batch: 450, loss is 3.90005916595459 and perplexity is 49.40537213506164
At time: 369.6422121524811 and batch: 500, loss is 3.965314259529114 and perplexity is 52.736839840255094
At time: 370.357932806015 and batch: 550, loss is 3.970756983757019 and perplexity is 53.02465445344104
At time: 371.076584815979 and batch: 600, loss is 3.909772367477417 and perplexity is 49.88759464179398
At time: 371.79357266426086 and batch: 650, loss is 3.9275803852081297 and perplexity is 50.78395129126739
At time: 372.5120053291321 and batch: 700, loss is 3.949522547721863 and perplexity is 51.91057609428898
At time: 373.2288615703583 and batch: 750, loss is 3.9002148294448853 and perplexity is 49.41306334633239
At time: 373.94997930526733 and batch: 800, loss is 3.8785118770599367 and perplexity is 48.35220747740761
At time: 374.667768239975 and batch: 850, loss is 3.840090217590332 and perplexity is 46.5296720453283
At time: 375.4367415904999 and batch: 900, loss is 3.8559020709991456 and perplexity is 47.271239728447014
At time: 376.15133023262024 and batch: 950, loss is 3.844002389907837 and perplexity is 46.71206067549876
At time: 376.86721634864807 and batch: 1000, loss is 3.8569657897949217 and perplexity is 47.32154978777986
At time: 377.5870349407196 and batch: 1050, loss is 3.799421534538269 and perplexity is 44.67533387953744
At time: 378.3076705932617 and batch: 1100, loss is 3.7736851024627684 and perplexity is 43.54021973197268
At time: 379.02465534210205 and batch: 1150, loss is 3.7874367380142213 and perplexity is 44.143104792563655
At time: 379.7417838573456 and batch: 1200, loss is 3.762350172996521 and perplexity is 43.049480911515296
At time: 380.4603292942047 and batch: 1250, loss is 3.8019640016555787 and perplexity is 44.78906396311596
At time: 381.1763744354248 and batch: 1300, loss is 3.809253807067871 and perplexity is 45.11676049479824
At time: 381.89409160614014 and batch: 1350, loss is 3.781029472351074 and perplexity is 43.861172365456234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3361588541666665 and perplexity of 76.41345965623447
Finished 18 epochs...
Completing Train Step...
At time: 384.39068245887756 and batch: 50, loss is 3.986629490852356 and perplexity is 53.87300356032425
At time: 385.1368262767792 and batch: 100, loss is 3.989335551261902 and perplexity is 54.018984590062715
At time: 385.8546504974365 and batch: 150, loss is 3.9655978059768677 and perplexity is 52.75179530404167
At time: 386.57915139198303 and batch: 200, loss is 3.96011221408844 and perplexity is 52.463212730396144
At time: 387.297593832016 and batch: 250, loss is 3.951373510360718 and perplexity is 52.006749610535614
At time: 388.0127580165863 and batch: 300, loss is 3.9492842864990236 and perplexity is 51.89820929027388
At time: 388.7297685146332 and batch: 350, loss is 3.945023012161255 and perplexity is 51.6775273100157
At time: 389.4672918319702 and batch: 400, loss is 3.9483052587509153 and perplexity is 51.8474243672824
At time: 390.213903427124 and batch: 450, loss is 3.8967469835281374 and perplexity is 49.242003232860895
At time: 390.95869612693787 and batch: 500, loss is 3.962295069694519 and perplexity is 52.577857429307336
At time: 391.71258187294006 and batch: 550, loss is 3.967781720161438 and perplexity is 52.86712658904314
At time: 392.45450258255005 and batch: 600, loss is 3.906940894126892 and perplexity is 49.74653903887397
At time: 393.20270323753357 and batch: 650, loss is 3.9248578357696533 and perplexity is 50.64587751482373
At time: 393.93590474128723 and batch: 700, loss is 3.9471237802505494 and perplexity is 51.786203922534156
At time: 394.724036693573 and batch: 750, loss is 3.8979006576538087 and perplexity is 49.29884524015885
At time: 395.46477031707764 and batch: 800, loss is 3.8764457511901855 and perplexity is 48.252408864429306
At time: 396.19567704200745 and batch: 850, loss is 3.8382957935333253 and perplexity is 46.446252949445594
At time: 396.9164161682129 and batch: 900, loss is 3.85420814037323 and perplexity is 47.1912333095385
At time: 397.63208866119385 and batch: 950, loss is 3.842646794319153 and perplexity is 46.64878091268283
At time: 398.35110116004944 and batch: 1000, loss is 3.855764226913452 and perplexity is 47.264724116706645
At time: 399.07041454315186 and batch: 1050, loss is 3.7983477210998533 and perplexity is 44.627386653446976
At time: 399.7863574028015 and batch: 1100, loss is 3.7728592491149904 and perplexity is 43.504276739607185
At time: 400.502206325531 and batch: 1150, loss is 3.7867820501327514 and perplexity is 44.11421429496675
At time: 401.21976947784424 and batch: 1200, loss is 3.7619533824920652 and perplexity is 43.032402674733014
At time: 401.9369647502899 and batch: 1250, loss is 3.8017231607437134 and perplexity is 44.77827822298537
At time: 402.65315437316895 and batch: 1300, loss is 3.809084162712097 and perplexity is 45.10910734020508
At time: 403.3718681335449 and batch: 1350, loss is 3.7809714698791503 and perplexity is 43.858628382816924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.336072998046875 and perplexity of 76.40689937471272
Finished 19 epochs...
Completing Train Step...
At time: 405.925475358963 and batch: 50, loss is 3.9833840799331663 and perplexity is 53.69844693352393
At time: 406.64328265190125 and batch: 100, loss is 3.9857880449295044 and perplexity is 53.827691407663245
At time: 407.35837149620056 and batch: 150, loss is 3.962102007865906 and perplexity is 52.567707631808226
At time: 408.07953357696533 and batch: 200, loss is 3.9565795707702636 and perplexity is 52.27820588648995
At time: 408.79452085494995 and batch: 250, loss is 3.947807765007019 and perplexity is 51.8216370130781
At time: 409.51387548446655 and batch: 300, loss is 3.9458853960037232 and perplexity is 51.722112396552504
At time: 410.23016023635864 and batch: 350, loss is 3.9418170261383056 and perplexity is 51.51211517593439
At time: 410.9453525543213 and batch: 400, loss is 3.9451036787033082 and perplexity is 51.68169612558536
At time: 411.66500329971313 and batch: 450, loss is 3.893691716194153 and perplexity is 49.091785343681174
At time: 412.385737657547 and batch: 500, loss is 3.959491701126099 and perplexity is 52.43066872488373
At time: 413.1343722343445 and batch: 550, loss is 3.9650206756591797 and perplexity is 52.72135942723766
At time: 413.85697507858276 and batch: 600, loss is 3.9043033361434936 and perplexity is 49.615502541830246
At time: 414.57619881629944 and batch: 650, loss is 3.9223062086105345 and perplexity is 50.51681285073678
At time: 415.2938895225525 and batch: 700, loss is 3.9448507261276244 and perplexity is 51.66862476072209
At time: 416.013710975647 and batch: 750, loss is 3.8957028198242187 and perplexity is 49.19061335476873
At time: 416.73379397392273 and batch: 800, loss is 3.874463634490967 and perplexity is 48.15686168315541
At time: 417.45293974876404 and batch: 850, loss is 3.836557321548462 and perplexity is 46.36557758613926
At time: 418.1710731983185 and batch: 900, loss is 3.852524170875549 and perplexity is 47.11183158589128
At time: 418.88715863227844 and batch: 950, loss is 3.8412716722488405 and perplexity is 46.58467722979266
At time: 419.608446598053 and batch: 1000, loss is 3.854517617225647 and perplexity is 47.205840164009906
At time: 420.3248429298401 and batch: 1050, loss is 3.7972072839736937 and perplexity is 44.5765209349538
At time: 421.04367780685425 and batch: 1100, loss is 3.7719326496124266 and perplexity is 43.46398436874914
At time: 421.7575681209564 and batch: 1150, loss is 3.785994458198547 and perplexity is 44.07948397405986
At time: 422.4757990837097 and batch: 1200, loss is 3.76138475894928 and perplexity is 43.007940393043306
At time: 423.19187903404236 and batch: 1250, loss is 3.8012884140014647 and perplexity is 44.758815243444445
At time: 423.9151487350464 and batch: 1300, loss is 3.808725175857544 and perplexity is 45.0929166699421
At time: 424.6307442188263 and batch: 1350, loss is 3.7807187461853027 and perplexity is 43.84754566873613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.336029052734375 and perplexity of 76.40354172341961
Finished 20 epochs...
Completing Train Step...
At time: 427.14758372306824 and batch: 50, loss is 3.9802888441085815 and perplexity is 53.53249454023916
At time: 427.860808134079 and batch: 100, loss is 3.98244010925293 and perplexity is 53.64778109134148
At time: 428.57360649108887 and batch: 150, loss is 3.958827247619629 and perplexity is 52.39584255466969
At time: 429.2865116596222 and batch: 200, loss is 3.95327543258667 and perplexity is 52.1057565254005
At time: 429.99976682662964 and batch: 250, loss is 3.944479379653931 and perplexity is 51.649441361180926
At time: 430.7126181125641 and batch: 300, loss is 3.9427123355865477 and perplexity is 51.55825511102818
At time: 431.47051429748535 and batch: 350, loss is 3.938828053474426 and perplexity is 51.358376746254464
At time: 432.1851930618286 and batch: 400, loss is 3.9421154260635376 and perplexity is 51.52748868086371
At time: 432.9004485607147 and batch: 450, loss is 3.8908297920227053 and perplexity is 48.95148923076415
At time: 433.614093542099 and batch: 500, loss is 3.956849927902222 and perplexity is 52.29234158305443
At time: 434.3286397457123 and batch: 550, loss is 3.9624121618270873 and perplexity is 52.5840142432098
At time: 435.0396065711975 and batch: 600, loss is 3.9018108224868775 and perplexity is 49.491989217433925
At time: 435.7547161579132 and batch: 650, loss is 3.9198847913742068 and perplexity is 50.39463854655364
At time: 436.4711158275604 and batch: 700, loss is 3.942673101425171 and perplexity is 51.55623230580865
At time: 437.184219121933 and batch: 750, loss is 3.8935926580429077 and perplexity is 49.08692264303275
At time: 437.89452171325684 and batch: 800, loss is 3.872545266151428 and perplexity is 48.06456763966094
At time: 438.610116481781 and batch: 850, loss is 3.834860258102417 and perplexity is 46.286958988505795
At time: 439.3206670284271 and batch: 900, loss is 3.850849680900574 and perplexity is 47.0330093081725
At time: 440.03361415863037 and batch: 950, loss is 3.839884057044983 and perplexity is 46.52008045150261
At time: 440.74560832977295 and batch: 1000, loss is 3.853238749504089 and perplexity is 47.145508724942836
At time: 441.46125984191895 and batch: 1050, loss is 3.7960205364227293 and perplexity is 44.52365123560185
At time: 442.17129945755005 and batch: 1100, loss is 3.7709337139129637 and perplexity is 43.42058832166127
At time: 442.8853130340576 and batch: 1150, loss is 3.785113797187805 and perplexity is 44.040681979359995
At time: 443.59661865234375 and batch: 1200, loss is 3.7606926918029786 and perplexity is 42.978186307567285
At time: 444.30864667892456 and batch: 1250, loss is 3.8007160425186157 and perplexity is 44.73320390429228
At time: 445.0204977989197 and batch: 1300, loss is 3.808232140541077 and perplexity is 45.070689749279985
At time: 445.73371386528015 and batch: 1350, loss is 3.7803248500823976 and perplexity is 43.83027769249281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.336017252604167 and perplexity of 76.40264015699822
Finished 21 epochs...
Completing Train Step...
At time: 448.2723798751831 and batch: 50, loss is 3.97732223033905 and perplexity is 53.37391963636418
At time: 449.03369784355164 and batch: 100, loss is 3.9792541074752807 and perplexity is 53.4771311552674
At time: 449.75290513038635 and batch: 150, loss is 3.955728726387024 and perplexity is 52.2337441863218
At time: 450.52251172065735 and batch: 200, loss is 3.950151505470276 and perplexity is 51.943235923097596
At time: 451.2419972419739 and batch: 250, loss is 3.941338953971863 and perplexity is 51.48749455312035
At time: 451.9603531360626 and batch: 300, loss is 3.9397159385681153 and perplexity is 51.403997333326096
At time: 452.68376445770264 and batch: 350, loss is 3.936007866859436 and perplexity is 51.213740585931795
At time: 453.40313696861267 and batch: 400, loss is 3.9392930459976196 and perplexity is 51.382263560609466
At time: 454.12482738494873 and batch: 450, loss is 3.888117260932922 and perplexity is 48.81888671985673
At time: 454.8468904495239 and batch: 500, loss is 3.9543322324752808 and perplexity is 52.1608509898769
At time: 455.56929063796997 and batch: 550, loss is 3.959929189682007 and perplexity is 52.45361156067759
At time: 456.2872123718262 and batch: 600, loss is 3.8994352769851686 and perplexity is 49.374558281567566
At time: 457.0082850456238 and batch: 650, loss is 3.9175706481933594 and perplexity is 50.2781529715436
At time: 457.72520327568054 and batch: 700, loss is 3.940573182106018 and perplexity is 51.44808197079806
At time: 458.4466640949249 and batch: 750, loss is 3.8915531730651853 and perplexity is 48.98691262083457
At time: 459.1668336391449 and batch: 800, loss is 3.8706771087646485 and perplexity is 47.974859283337075
At time: 459.88517332077026 and batch: 850, loss is 3.8331969499588014 and perplexity is 46.210033505803054
At time: 460.6009237766266 and batch: 900, loss is 3.8491840887069704 and perplexity is 46.954736698246464
At time: 461.31868982315063 and batch: 950, loss is 3.838490562438965 and perplexity is 46.45530011632842
At time: 462.0356242656708 and batch: 1000, loss is 3.8519361877441405 and perplexity is 47.084138765882365
At time: 462.75457859039307 and batch: 1050, loss is 3.794801959991455 and perplexity is 44.46942880736811
At time: 463.46910548210144 and batch: 1100, loss is 3.7698805952072143 and perplexity is 43.37488535743229
At time: 464.18798446655273 and batch: 1150, loss is 3.7841643142700194 and perplexity is 43.99888594958111
At time: 464.90201020240784 and batch: 1200, loss is 3.7599090003967284 and perplexity is 42.94451786685883
At time: 465.6201069355011 and batch: 1250, loss is 3.8000416421890257 and perplexity is 44.70304598723324
At time: 466.3399796485901 and batch: 1300, loss is 3.8076419734954836 and perplexity is 45.04409836092017
At time: 467.0599989891052 and batch: 1350, loss is 3.779826340675354 and perplexity is 43.808433332011745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.336031901041666 and perplexity of 76.4037593444945
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 469.567170381546 and batch: 50, loss is 3.9781287145614623 and perplexity is 53.4169822227534
At time: 470.3436734676361 and batch: 100, loss is 3.9842260599136354 and perplexity is 53.743678990395225
At time: 471.0928268432617 and batch: 150, loss is 3.9628084087371827 and perplexity is 52.604854625069855
At time: 471.84139108657837 and batch: 200, loss is 3.9589704370498655 and perplexity is 52.40334562267905
At time: 472.58355808258057 and batch: 250, loss is 3.949691038131714 and perplexity is 51.91932326541723
At time: 473.3112270832062 and batch: 300, loss is 3.9473509311676027 and perplexity is 51.79796854236738
At time: 474.0493974685669 and batch: 350, loss is 3.944805302619934 and perplexity is 51.66627784385093
At time: 474.7749881744385 and batch: 400, loss is 3.9461366367340087 and perplexity is 51.735108730378755
At time: 475.4909052848816 and batch: 450, loss is 3.89331636428833 and perplexity is 49.07336210630712
At time: 476.21786165237427 and batch: 500, loss is 3.962490129470825 and perplexity is 52.588114254730655
At time: 476.93460750579834 and batch: 550, loss is 3.9661956071853637 and perplexity is 52.783339818759856
At time: 477.6508047580719 and batch: 600, loss is 3.9060390758514405 and perplexity is 49.701696923586866
At time: 478.37066984176636 and batch: 650, loss is 3.922466812133789 and perplexity is 50.52492668040158
At time: 479.0937259197235 and batch: 700, loss is 3.9456331443786623 and perplexity is 51.70906705507235
At time: 479.8162989616394 and batch: 750, loss is 3.8933420085906985 and perplexity is 49.074620574579406
At time: 480.5334153175354 and batch: 800, loss is 3.871856050491333 and perplexity is 48.031452200098634
At time: 481.2509970664978 and batch: 850, loss is 3.8313895797729494 and perplexity is 46.12659029805493
At time: 481.9747505187988 and batch: 900, loss is 3.8462219858169555 and perplexity is 46.815857725420365
At time: 482.69353890419006 and batch: 950, loss is 3.834855670928955 and perplexity is 46.28674666268288
At time: 483.4107904434204 and batch: 1000, loss is 3.846996159553528 and perplexity is 46.85211536597213
At time: 484.1312358379364 and batch: 1050, loss is 3.787930965423584 and perplexity is 44.16492691698833
At time: 484.8488314151764 and batch: 1100, loss is 3.760669136047363 and perplexity is 42.97717393583747
At time: 485.5668046474457 and batch: 1150, loss is 3.773739743232727 and perplexity is 43.542598868101315
At time: 486.2837748527527 and batch: 1200, loss is 3.7483361101150514 and perplexity is 42.450390429629614
At time: 487.05746245384216 and batch: 1250, loss is 3.787203688621521 and perplexity is 44.13281846745731
At time: 487.77730679512024 and batch: 1300, loss is 3.7952113819122313 and perplexity is 44.48763929395807
At time: 488.4953806400299 and batch: 1350, loss is 3.7677067947387695 and perplexity is 43.280699419130876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.331263427734375 and perplexity of 76.04029732566099
Finished 23 epochs...
Completing Train Step...
At time: 491.01769518852234 and batch: 50, loss is 3.9759387445449828 and perplexity is 53.30012863294592
At time: 491.73356652259827 and batch: 100, loss is 3.9806100797653197 and perplexity is 53.549693848647294
At time: 492.4496011734009 and batch: 150, loss is 3.9587970399856567 and perplexity is 52.39425982414152
At time: 493.1648361682892 and batch: 200, loss is 3.953889837265015 and perplexity is 52.13778038277489
At time: 493.8828821182251 and batch: 250, loss is 3.9444703531265257 and perplexity is 51.64897514818717
At time: 494.5988266468048 and batch: 300, loss is 3.9421145153045654 and perplexity is 51.52744175176245
At time: 495.31837916374207 and batch: 350, loss is 3.9391517210006715 and perplexity is 51.37500247546707
At time: 496.0402307510376 and batch: 400, loss is 3.9411021184921267 and perplexity is 51.47530193152713
At time: 496.7593002319336 and batch: 450, loss is 3.888471746444702 and perplexity is 48.83619537555273
At time: 497.4780468940735 and batch: 500, loss is 3.9570060682296755 and perplexity is 52.30050716386417
At time: 498.1951916217804 and batch: 550, loss is 3.961576962471008 and perplexity is 52.54011444346794
At time: 498.91160130500793 and batch: 600, loss is 3.9011787080764773 and perplexity is 49.460714503490266
At time: 499.6298336982727 and batch: 650, loss is 3.9180837774276736 and perplexity is 50.30395878197226
At time: 500.347252368927 and batch: 700, loss is 3.941424722671509 and perplexity is 51.491910757959566
At time: 501.06503891944885 and batch: 750, loss is 3.890075516700745 and perplexity is 48.91458025197999
At time: 501.78309893608093 and batch: 800, loss is 3.868780927658081 and perplexity is 47.883976453956556
At time: 502.49870467185974 and batch: 850, loss is 3.8292743873596193 and perplexity is 46.02912679757006
At time: 503.21477341651917 and batch: 900, loss is 3.84442542552948 and perplexity is 46.73182572148934
At time: 503.929988861084 and batch: 950, loss is 3.8337138080596924 and perplexity is 46.23392370935375
At time: 504.6486098766327 and batch: 1000, loss is 3.8464469432830812 and perplexity is 46.82639048681602
At time: 505.370493888855 and batch: 1050, loss is 3.7879268026351927 and perplexity is 44.164743068125915
At time: 506.1429605484009 and batch: 1100, loss is 3.7614304924011233 and perplexity is 43.009907339591436
At time: 506.86060786247253 and batch: 1150, loss is 3.7748086214065553 and perplexity is 43.58916548424948
At time: 507.57724118232727 and batch: 1200, loss is 3.7498744535446167 and perplexity is 42.51574396403137
At time: 508.2939591407776 and batch: 1250, loss is 3.7892280626296997 and perplexity is 44.222250289256635
At time: 509.01088190078735 and batch: 1300, loss is 3.797828207015991 and perplexity is 44.60420811886343
At time: 509.73631978034973 and batch: 1350, loss is 3.7703476667404177 and perplexity is 43.39514926361718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330491129557291 and perplexity of 75.98159421372243
Finished 24 epochs...
Completing Train Step...
At time: 512.2833573818207 and batch: 50, loss is 3.9753690195083617 and perplexity is 53.26977086382032
At time: 513.0019590854645 and batch: 100, loss is 3.9794015502929687 and perplexity is 53.485016555475326
At time: 513.7235479354858 and batch: 150, loss is 3.9572814893722534 and perplexity is 52.31491381316148
At time: 514.4450082778931 and batch: 200, loss is 3.952027745246887 and perplexity is 52.040785372924645
At time: 515.1627225875854 and batch: 250, loss is 3.942420244216919 and perplexity is 51.54319758887033
At time: 515.8817889690399 and batch: 300, loss is 3.940155701637268 and perplexity is 51.426607884238365
At time: 516.5985565185547 and batch: 350, loss is 3.9370255994796755 and perplexity is 51.26588901240675
At time: 517.317214012146 and batch: 400, loss is 3.9391546392440797 and perplexity is 51.37515240044816
At time: 518.0337488651276 and batch: 450, loss is 3.8866046905517577 and perplexity is 48.74510053522931
At time: 518.7536005973816 and batch: 500, loss is 3.954950451850891 and perplexity is 52.19310780847493
At time: 519.4746196269989 and batch: 550, loss is 3.9597416639328005 and perplexity is 52.44377608010285
At time: 520.1949849128723 and batch: 600, loss is 3.8993786859512327 and perplexity is 49.37176420332492
At time: 520.9197187423706 and batch: 650, loss is 3.9164849805831907 and perplexity is 50.223597229422985
At time: 521.635516166687 and batch: 700, loss is 3.9399440908432006 and perplexity is 51.415726610245905
At time: 522.3561315536499 and batch: 750, loss is 3.8888446426391603 and perplexity is 48.85440960275703
At time: 523.0754859447479 and batch: 800, loss is 3.867698369026184 and perplexity is 47.832167290205234
At time: 523.7907977104187 and batch: 850, loss is 3.8285241413116453 and perplexity is 45.99460657804682
At time: 524.5770649909973 and batch: 900, loss is 3.843895516395569 and perplexity is 46.70706866027095
At time: 525.2922909259796 and batch: 950, loss is 3.8334675359725954 and perplexity is 46.22253898639445
At time: 526.0106472969055 and batch: 1000, loss is 3.8464501190185545 and perplexity is 46.826539195281505
At time: 526.7280082702637 and batch: 1050, loss is 3.7881594562530516 and perplexity is 44.17501935074334
At time: 527.4459426403046 and batch: 1100, loss is 3.7619228553771973 and perplexity is 43.03108903968437
At time: 528.1613578796387 and batch: 1150, loss is 3.775425605773926 and perplexity is 43.61606761618402
At time: 528.8832068443298 and batch: 1200, loss is 3.7507290410995484 and perplexity is 42.55209291918185
At time: 529.5995199680328 and batch: 1250, loss is 3.79028413772583 and perplexity is 44.26897697558401
At time: 530.3159520626068 and batch: 1300, loss is 3.7990041255950926 and perplexity is 44.65668988699204
At time: 531.0345964431763 and batch: 1350, loss is 3.7715117168426513 and perplexity is 43.445692803453035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330246175130208 and perplexity of 75.9629844652061
Finished 25 epochs...
Completing Train Step...
At time: 533.5377511978149 and batch: 50, loss is 3.9747555112838744 and perplexity is 53.237099444395845
At time: 534.2817554473877 and batch: 100, loss is 3.9784611892700195 and perplexity is 53.43474497101857
At time: 534.9970021247864 and batch: 150, loss is 3.956169018745422 and perplexity is 52.2567473684286
At time: 535.7151508331299 and batch: 200, loss is 3.950782718658447 and perplexity is 51.97603352870019
At time: 536.4287705421448 and batch: 250, loss is 3.9410806941986083 and perplexity is 51.47419912136311
At time: 537.1432781219482 and batch: 300, loss is 3.9388919830322267 and perplexity is 51.36166016952198
At time: 537.860013961792 and batch: 350, loss is 3.935737648010254 and perplexity is 51.19990353748821
At time: 538.5763461589813 and batch: 400, loss is 3.9379321479797365 and perplexity is 51.31238509948992
At time: 539.2972402572632 and batch: 450, loss is 3.885455174446106 and perplexity is 48.68909945033355
At time: 540.0143618583679 and batch: 500, loss is 3.9537621259689333 and perplexity is 52.13112222443746
At time: 540.7288839817047 and batch: 550, loss is 3.958648433685303 and perplexity is 52.38647428553365
At time: 541.4453632831573 and batch: 600, loss is 3.8983318185806275 and perplexity is 49.32010555894371
At time: 542.1655569076538 and batch: 650, loss is 3.9155695629119873 and perplexity is 50.17764269801195
At time: 542.9387776851654 and batch: 700, loss is 3.9391264867782594 and perplexity is 51.37370608358498
At time: 543.6793665885925 and batch: 750, loss is 3.888129334449768 and perplexity is 48.81947613906611
At time: 544.4123029708862 and batch: 800, loss is 3.8670974826812743 and perplexity is 47.80343422755014
At time: 545.1440074443817 and batch: 850, loss is 3.8280875205993654 and perplexity is 45.97452876367545
At time: 545.8592925071716 and batch: 900, loss is 3.843598175048828 and perplexity is 46.69318278209835
At time: 546.5739328861237 and batch: 950, loss is 3.8333397340774535 and perplexity is 46.2166320357815
At time: 547.3080062866211 and batch: 1000, loss is 3.8464559745788574 and perplexity is 46.82681339170832
At time: 548.0428459644318 and batch: 1050, loss is 3.788281869888306 and perplexity is 44.1804273064465
At time: 548.7561366558075 and batch: 1100, loss is 3.7621952962875365 and perplexity is 43.042814065871106
At time: 549.4729497432709 and batch: 1150, loss is 3.775769453048706 and perplexity is 43.63106746085053
At time: 550.1928977966309 and batch: 1200, loss is 3.7512300205230713 and perplexity is 42.57341598292432
At time: 550.9094159603119 and batch: 1250, loss is 3.790893120765686 and perplexity is 44.29594424222449
At time: 551.6278386116028 and batch: 1300, loss is 3.799621877670288 and perplexity is 44.684285172485964
At time: 552.342854976654 and batch: 1350, loss is 3.772129092216492 and perplexity is 43.47252338570749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330144449869792 and perplexity of 75.95525750384965
Finished 26 epochs...
Completing Train Step...
At time: 554.8838806152344 and batch: 50, loss is 3.9741492557525633 and perplexity is 53.204833939945246
At time: 555.6448516845703 and batch: 100, loss is 3.9776481437683104 and perplexity is 53.39131774853113
At time: 556.3624188899994 and batch: 150, loss is 3.955250134468079 and perplexity is 52.208751519579195
At time: 557.0770716667175 and batch: 200, loss is 3.9498021697998045 and perplexity is 51.92509346703798
At time: 557.7953109741211 and batch: 250, loss is 3.940044569969177 and perplexity is 51.420893077073835
At time: 558.5112166404724 and batch: 300, loss is 3.937911043167114 and perplexity is 51.311302172644716
At time: 559.2308983802795 and batch: 350, loss is 3.9347802352905275 and perplexity is 51.15090755702274
At time: 559.9495086669922 and batch: 400, loss is 3.936998748779297 and perplexity is 51.264512505861404
At time: 560.6650400161743 and batch: 450, loss is 3.884589657783508 and perplexity is 48.6469764551791
At time: 561.3825235366821 and batch: 500, loss is 3.952909173965454 and perplexity is 52.08667583730464
At time: 562.1505191326141 and batch: 550, loss is 3.957852592468262 and perplexity is 52.344799555515294
At time: 562.8676209449768 and batch: 600, loss is 3.897571725845337 and perplexity is 49.282631948518286
At time: 563.5857727527618 and batch: 650, loss is 3.9149041223526 and perplexity is 50.14426356653193
At time: 564.3010075092316 and batch: 700, loss is 3.9385444831848146 and perplexity is 51.343815101209074
At time: 565.0216481685638 and batch: 750, loss is 3.8876079511642456 and perplexity is 48.79402911460231
At time: 565.7368636131287 and batch: 800, loss is 3.8666617250442505 and perplexity is 47.78260805392118
At time: 566.4541981220245 and batch: 850, loss is 3.827753686904907 and perplexity is 45.95918347841644
At time: 567.1721832752228 and batch: 900, loss is 3.843361096382141 and perplexity is 46.682114136702694
At time: 567.8867335319519 and batch: 950, loss is 3.8332232666015624 and perplexity is 46.21124961474868
At time: 568.6025550365448 and batch: 1000, loss is 3.846423282623291 and perplexity is 46.82528255662874
At time: 569.3190529346466 and batch: 1050, loss is 3.788314642906189 and perplexity is 44.1818752561074
At time: 570.035257101059 and batch: 1100, loss is 3.762337079048157 and perplexity is 43.048917227525564
At time: 570.7506103515625 and batch: 1150, loss is 3.775965690612793 and perplexity is 43.63963035540087
At time: 571.4677691459656 and batch: 1200, loss is 3.751539635658264 and perplexity is 42.586599397656535
At time: 572.1823811531067 and batch: 1250, loss is 3.791273665428162 and perplexity is 44.31280403512384
At time: 572.8988034725189 and batch: 1300, loss is 3.799982304573059 and perplexity is 44.700393493755016
At time: 573.615891456604 and batch: 1350, loss is 3.7724985551834105 and perplexity is 43.48858784060486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3300927734375 and perplexity of 75.95133250854381
Finished 27 epochs...
Completing Train Step...
At time: 576.1238560676575 and batch: 50, loss is 3.9735625791549682 and perplexity is 53.17362906347416
At time: 576.8334472179413 and batch: 100, loss is 3.976911268234253 and perplexity is 53.35198948454846
At time: 577.5465743541718 and batch: 150, loss is 3.954444489479065 and perplexity is 52.166706739392154
At time: 578.2603290081024 and batch: 200, loss is 3.948965606689453 and perplexity is 51.881673013848875
At time: 578.9764263629913 and batch: 250, loss is 3.9391674041748046 and perplexity is 51.375808204895165
At time: 579.6895315647125 and batch: 300, loss is 3.9370782566070557 and perplexity is 51.268588597930325
At time: 580.4506211280823 and batch: 350, loss is 3.9339859867095948 and perplexity is 51.11029715079577
At time: 581.1668469905853 and batch: 400, loss is 3.936211247444153 and perplexity is 51.22415752570345
At time: 581.8831071853638 and batch: 450, loss is 3.8838643074035644 and perplexity is 48.611703146624635
At time: 582.5971610546112 and batch: 500, loss is 3.9522169399261475 and perplexity is 52.050632144070725
At time: 583.3113963603973 and batch: 550, loss is 3.9572008657455444 and perplexity is 52.31069616510222
At time: 584.0234839916229 and batch: 600, loss is 3.8969475984573365 and perplexity is 49.25188290482479
At time: 584.7396855354309 and batch: 650, loss is 3.9143531417846678 and perplexity is 50.11664266170318
At time: 585.4526450634003 and batch: 700, loss is 3.9380664730072024 and perplexity is 51.31927809996863
At time: 586.164005279541 and batch: 750, loss is 3.8871732425689696 and perplexity is 48.772822540422034
At time: 586.8762516975403 and batch: 800, loss is 3.866292567253113 and perplexity is 47.76497198732318
At time: 587.5893504619598 and batch: 850, loss is 3.8274601221084597 and perplexity is 45.945693460267805
At time: 588.3016471862793 and batch: 900, loss is 3.8431407690048216 and perplexity is 46.671829921915865
At time: 589.011148929596 and batch: 950, loss is 3.8330980348587036 and perplexity is 46.20546286176989
At time: 589.7232844829559 and batch: 1000, loss is 3.8463568544387816 and perplexity is 46.82217214143012
At time: 590.4358515739441 and batch: 1050, loss is 3.7882896852493286 and perplexity is 44.180772593785306
At time: 591.150393486023 and batch: 1100, loss is 3.7623986291885374 and perplexity is 43.05156697596951
At time: 591.8617975711823 and batch: 1150, loss is 3.776074810028076 and perplexity is 43.64439254616743
At time: 592.5796558856964 and batch: 1200, loss is 3.7517355632781983 and perplexity is 42.59494410617037
At time: 593.2905633449554 and batch: 1250, loss is 3.7915231132507325 and perplexity is 44.32385914638179
At time: 594.003870010376 and batch: 1300, loss is 3.8002057790756227 and perplexity is 44.71038400822647
At time: 594.716118812561 and batch: 1350, loss is 3.772736597061157 and perplexity is 43.49894117793003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330064697265625 and perplexity of 75.94920011581299
Finished 28 epochs...
Completing Train Step...
At time: 597.2634768486023 and batch: 50, loss is 3.9729942178726194 and perplexity is 53.143415818305215
At time: 597.9810848236084 and batch: 100, loss is 3.9762238693237304 and perplexity is 53.31532798708288
At time: 598.7281124591827 and batch: 150, loss is 3.9537107133865357 and perplexity is 52.128442097717326
At time: 599.4442994594574 and batch: 200, loss is 3.94821665763855 and perplexity is 51.84283083130937
At time: 600.16295337677 and batch: 250, loss is 3.938385124206543 and perplexity is 51.33563365520474
At time: 600.878410577774 and batch: 300, loss is 3.936334843635559 and perplexity is 51.230489027748334
At time: 601.5957791805267 and batch: 350, loss is 3.933284797668457 and perplexity is 51.07447173220765
At time: 602.3107681274414 and batch: 400, loss is 3.9355091094970702 and perplexity is 51.18820372463844
At time: 603.0234606266022 and batch: 450, loss is 3.8832190990448 and perplexity is 48.580348585620804
At time: 603.7395389080048 and batch: 500, loss is 3.9516135454177856 and perplexity is 52.01923455199816
At time: 604.4547598361969 and batch: 550, loss is 3.956628885269165 and perplexity is 52.280784023585944
At time: 605.1791212558746 and batch: 600, loss is 3.896398415565491 and perplexity is 49.2248420392123
At time: 605.9068984985352 and batch: 650, loss is 3.9138627910614012 and perplexity is 50.092073953860435
At time: 606.6255242824554 and batch: 700, loss is 3.9376422452926634 and perplexity is 51.29751165719907
At time: 607.3473711013794 and batch: 750, loss is 3.886782321929932 and perplexity is 48.75375996368695
At time: 608.0705282688141 and batch: 800, loss is 3.8659537124633787 and perplexity is 47.74878933972242
At time: 608.7881960868835 and batch: 850, loss is 3.8271840715408327 and perplexity is 45.93301187596789
At time: 609.505716085434 and batch: 900, loss is 3.8429239892959597 and perplexity is 46.66171351276907
At time: 610.2205533981323 and batch: 950, loss is 3.832960858345032 and perplexity is 46.199124992175285
At time: 610.9364552497864 and batch: 1000, loss is 3.846264977455139 and perplexity is 46.817870459102004
At time: 611.652440071106 and batch: 1050, loss is 3.7882272243499755 and perplexity is 44.17801310917583
At time: 612.3667767047882 and batch: 1100, loss is 3.762408285140991 and perplexity is 43.051982681860295
At time: 613.0902726650238 and batch: 1150, loss is 3.77612765789032 and perplexity is 43.64669911996063
At time: 613.8120212554932 and batch: 1200, loss is 3.751857957839966 and perplexity is 42.60015781474603
At time: 614.5223648548126 and batch: 1250, loss is 3.791690106391907 and perplexity is 44.331261544907306
At time: 615.2364301681519 and batch: 1300, loss is 3.800347981452942 and perplexity is 44.716742383200604
At time: 615.9533534049988 and batch: 1350, loss is 3.772896523475647 and perplexity is 43.505898363930804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330050048828125 and perplexity of 75.94808758685032
Finished 29 epochs...
Completing Train Step...
At time: 618.4396693706512 and batch: 50, loss is 3.9724415874481203 and perplexity is 53.1140552633774
At time: 619.2061471939087 and batch: 100, loss is 3.975571026802063 and perplexity is 53.28053283303016
At time: 619.9174997806549 and batch: 150, loss is 3.95302574634552 and perplexity is 52.092748058996506
At time: 620.6434350013733 and batch: 200, loss is 3.9475256109237673 and perplexity is 51.807017389184345
At time: 621.3625857830048 and batch: 250, loss is 3.9376645421981813 and perplexity is 51.29865544572122
At time: 622.0833015441895 and batch: 300, loss is 3.935650305747986 and perplexity is 51.19543181737326
At time: 622.8087401390076 and batch: 350, loss is 3.9326424980163575 and perplexity is 51.041677149884734
At time: 623.5464951992035 and batch: 400, loss is 3.9348623847961424 and perplexity is 51.15510975139204
At time: 624.2883114814758 and batch: 450, loss is 3.8826249742507937 and perplexity is 48.55149436837632
At time: 625.0441555976868 and batch: 500, loss is 3.9510644674301147 and perplexity is 51.99067977548719
At time: 625.80539727211 and batch: 550, loss is 3.9561052894592286 and perplexity is 52.25341718933611
At time: 626.5402805805206 and batch: 600, loss is 3.895895314216614 and perplexity is 49.20008318341321
At time: 627.2649564743042 and batch: 650, loss is 3.9134094524383545 and perplexity is 50.0693704286099
At time: 627.9925813674927 and batch: 700, loss is 3.9372494983673096 and perplexity is 51.27736867302387
At time: 628.70876288414 and batch: 750, loss is 3.886415858268738 and perplexity is 48.735896755621916
At time: 629.4454843997955 and batch: 800, loss is 3.865629906654358 and perplexity is 47.73333050732545
At time: 630.1814184188843 and batch: 850, loss is 3.8269160985946655 and perplexity is 45.920704720515154
At time: 630.8908927440643 and batch: 900, loss is 3.842706351280212 and perplexity is 46.65155925504511
At time: 631.5956313610077 and batch: 950, loss is 3.832812395095825 and perplexity is 46.192266629088465
At time: 632.3103744983673 and batch: 1000, loss is 3.8461538791656493 and perplexity is 46.81266936269825
At time: 633.0318820476532 and batch: 1050, loss is 3.788138780593872 and perplexity is 44.17410601254097
At time: 633.7459712028503 and batch: 1100, loss is 3.7623823785781862 and perplexity is 43.05086736741412
At time: 634.464569568634 and batch: 1150, loss is 3.776141114234924 and perplexity is 43.64728644893648
At time: 635.1809570789337 and batch: 1200, loss is 3.7519296312332155 and perplexity is 42.60321122203231
At time: 635.9516940116882 and batch: 1250, loss is 3.791801481246948 and perplexity is 44.336199207696296
At time: 636.6670277118683 and batch: 1300, loss is 3.800437164306641 and perplexity is 44.72073052772886
At time: 637.3879375457764 and batch: 1350, loss is 3.7730053949356077 and perplexity is 43.51063517244953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330042317708333 and perplexity of 75.94750042535696
Finished 30 epochs...
Completing Train Step...
At time: 639.8970263004303 and batch: 50, loss is 3.9719020414352415 and perplexity is 53.085405516255655
At time: 640.646772146225 and batch: 100, loss is 3.9749444627761843 and perplexity is 53.24715962419517
At time: 641.3624050617218 and batch: 150, loss is 3.952376642227173 and perplexity is 52.058945413600426
At time: 642.0794775485992 and batch: 200, loss is 3.9468754720687866 and perplexity is 51.7733465807554
At time: 642.7940394878387 and batch: 250, loss is 3.936987771987915 and perplexity is 51.26394978909076
At time: 643.5162267684937 and batch: 300, loss is 3.935007758140564 and perplexity is 51.16254688134797
At time: 644.2351999282837 and batch: 350, loss is 3.9320407819747927 and perplexity is 51.01097379223307
At time: 644.9505116939545 and batch: 400, loss is 3.934254698753357 and perplexity is 51.12403294860496
At time: 645.6689331531525 and batch: 450, loss is 3.8820662307739258 and perplexity is 48.52437411494371
At time: 646.3876287937164 and batch: 500, loss is 3.9505515193939207 and perplexity is 51.964018097008356
At time: 647.1043791770935 and batch: 550, loss is 3.9556136178970336 and perplexity is 52.22773198493632
At time: 647.8235700130463 and batch: 600, loss is 3.895422863960266 and perplexity is 49.17684408159294
At time: 648.538861989975 and batch: 650, loss is 3.9129800033569335 and perplexity is 50.04787279987086
At time: 649.2586133480072 and batch: 700, loss is 3.9368767881393434 and perplexity is 51.25826063435816
At time: 649.9738731384277 and batch: 750, loss is 3.886064176559448 and perplexity is 48.71876024562232
At time: 650.6924085617065 and batch: 800, loss is 3.8653146839141845 and perplexity is 47.718286247356325
At time: 651.4080438613892 and batch: 850, loss is 3.8266525936126707 and perplexity is 45.90860598015377
At time: 652.1252460479736 and batch: 900, loss is 3.8424858951568606 and perplexity is 46.64127576671406
At time: 652.84015417099 and batch: 950, loss is 3.832654185295105 and perplexity is 46.18495913786387
At time: 653.5589668750763 and batch: 1000, loss is 3.846027898788452 and perplexity is 46.80677225642196
At time: 654.3272755146027 and batch: 1050, loss is 3.7880318355560303 and perplexity is 44.16938206370777
At time: 655.044294834137 and batch: 1100, loss is 3.762330565452576 and perplexity is 43.048636825201754
At time: 655.7612266540527 and batch: 1150, loss is 3.7761256742477416 and perplexity is 43.64661254059573
At time: 656.479133605957 and batch: 1200, loss is 3.7519644832611085 and perplexity is 42.60469605621274
At time: 657.1947469711304 and batch: 1250, loss is 3.791872959136963 and perplexity is 44.33936837892843
At time: 657.9122266769409 and batch: 1300, loss is 3.8004895401000978 and perplexity is 44.72307287281478
At time: 658.6275095939636 and batch: 1350, loss is 3.773078589439392 and perplexity is 43.513820028355866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330036214192709 and perplexity of 75.94703688001611
Finished 31 epochs...
Completing Train Step...
At time: 661.1363959312439 and batch: 50, loss is 3.9713739061355593 and perplexity is 53.05737664187507
At time: 661.8533847332001 and batch: 100, loss is 3.9743385553359984 and perplexity is 53.21490654618682
At time: 662.5705237388611 and batch: 150, loss is 3.9517549514770507 and perplexity is 52.02659090706648
At time: 663.2888822555542 and batch: 200, loss is 3.946256370544434 and perplexity is 51.741303542936656
At time: 664.0085175037384 and batch: 250, loss is 3.9363440322875975 and perplexity is 51.2309597690485
At time: 664.7288219928741 and batch: 300, loss is 3.934396753311157 and perplexity is 51.1312958663515
At time: 665.4449672698975 and batch: 350, loss is 3.9314691686630248 and perplexity is 50.98182357268767
At time: 666.1662797927856 and batch: 400, loss is 3.9336761713027952 and perplexity is 51.09446484596498
At time: 666.8833289146423 and batch: 450, loss is 3.8815335750579836 and perplexity is 48.49853421220576
At time: 667.6085751056671 and batch: 500, loss is 3.9500644540786745 and perplexity is 51.93871438893199
At time: 668.3297092914581 and batch: 550, loss is 3.955145058631897 and perplexity is 52.20326592956379
At time: 669.0553224086761 and batch: 600, loss is 3.8949725580215455 and perplexity is 49.154704441835214
At time: 669.7786338329315 and batch: 650, loss is 3.9125676345825195 and perplexity is 50.02723887458799
At time: 670.493239402771 and batch: 700, loss is 3.9365179347991943 and perplexity is 51.23986973633424
At time: 671.209347486496 and batch: 750, loss is 3.8857222604751587 and perplexity is 48.70210536533568
At time: 671.9213871955872 and batch: 800, loss is 3.8650048065185545 and perplexity is 47.7035017199038
At time: 672.6645398139954 and batch: 850, loss is 3.826391382217407 and perplexity is 45.8966156951988
At time: 673.377683877945 and batch: 900, loss is 3.8422625160217283 and perplexity is 46.63085824244393
At time: 674.0974581241608 and batch: 950, loss is 3.832487530708313 and perplexity is 46.177262843911905
At time: 674.8127975463867 and batch: 1000, loss is 3.8458902168273927 and perplexity is 46.80032825184861
At time: 675.5237159729004 and batch: 1050, loss is 3.787910761833191 and perplexity is 44.16403463590883
At time: 676.2338380813599 and batch: 1100, loss is 3.7622594118118284 and perplexity is 43.04557386693404
At time: 676.9495322704315 and batch: 1150, loss is 3.77608784198761 and perplexity is 43.64496132183109
At time: 677.6728262901306 and batch: 1200, loss is 3.7519715309143065 and perplexity is 42.60499632039323
At time: 678.3863022327423 and batch: 1250, loss is 3.791914439201355 and perplexity is 44.34120761692949
At time: 679.0965402126312 and batch: 1300, loss is 3.800514807701111 and perplexity is 44.7242029318531
At time: 679.8149437904358 and batch: 1350, loss is 3.77312566280365 and perplexity is 43.51586841846825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.330056559244792 and perplexity of 75.94858204215514
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 682.3107724189758 and batch: 50, loss is 3.9714870119094847 and perplexity is 53.063378076914674
At time: 683.0254874229431 and batch: 100, loss is 3.97525842666626 and perplexity is 53.2638799342157
At time: 683.7405917644501 and batch: 150, loss is 3.9531711196899413 and perplexity is 52.1003215064773
At time: 684.4512338638306 and batch: 200, loss is 3.948091883659363 and perplexity is 51.83636259855619
At time: 685.1740255355835 and batch: 250, loss is 3.9381277656555174 and perplexity is 51.3224236908328
At time: 685.8888621330261 and batch: 300, loss is 3.9361831951141357 and perplexity is 51.222720588886496
At time: 686.6060967445374 and batch: 350, loss is 3.933396430015564 and perplexity is 51.08017361361565
At time: 687.3261661529541 and batch: 400, loss is 3.9354266500473023 and perplexity is 51.18398294754857
At time: 688.0430417060852 and batch: 450, loss is 3.8826461601257325 and perplexity is 48.55252298516014
At time: 688.7608578205109 and batch: 500, loss is 3.9514742136001586 and perplexity is 52.01198712240696
At time: 689.4780554771423 and batch: 550, loss is 3.956595106124878 and perplexity is 52.279018053265624
At time: 690.1927561759949 and batch: 600, loss is 3.89626540184021 and perplexity is 49.21829489503597
At time: 690.9090974330902 and batch: 650, loss is 3.913661952018738 and perplexity is 50.08201451987988
At time: 691.6544024944305 and batch: 700, loss is 3.937533860206604 and perplexity is 51.291952073276896
At time: 692.3740079402924 and batch: 750, loss is 3.8860328340530397 and perplexity is 48.717233301496364
At time: 693.0936028957367 and batch: 800, loss is 3.8649698877334595 and perplexity is 47.70183600066158
At time: 693.814698934555 and batch: 850, loss is 3.8247820997238158 and perplexity is 45.82281447447915
At time: 694.5326988697052 and batch: 900, loss is 3.840545573234558 and perplexity is 46.55086441878538
At time: 695.2492306232452 and batch: 950, loss is 3.8302984189987184 and perplexity is 46.0762862219811
At time: 695.9661302566528 and batch: 1000, loss is 3.843837580680847 and perplexity is 46.70436273125128
At time: 696.6811711788177 and batch: 1050, loss is 3.785679578781128 and perplexity is 44.065606436816054
At time: 697.3973395824432 and batch: 1100, loss is 3.7585990905761717 and perplexity is 42.8883012485545
At time: 698.11563539505 and batch: 1150, loss is 3.7727210092544556 and perplexity is 43.49826313012788
At time: 698.8305168151855 and batch: 1200, loss is 3.7484778785705566 and perplexity is 42.45640898252681
At time: 699.5471422672272 and batch: 1250, loss is 3.7885092163085936 and perplexity is 44.19047271029144
At time: 700.2705767154694 and batch: 1300, loss is 3.796897587776184 and perplexity is 44.5627178934071
At time: 700.9885668754578 and batch: 1350, loss is 3.7700773429870607 and perplexity is 43.383420109396674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329920247395833 and perplexity of 75.93823005607639
Finished 33 epochs...
Completing Train Step...
At time: 703.4972171783447 and batch: 50, loss is 3.9712381076812746 and perplexity is 53.050172021337936
At time: 704.2450239658356 and batch: 100, loss is 3.97485466003418 and perplexity is 53.24237809795727
At time: 704.9618546962738 and batch: 150, loss is 3.9526650094985962 and perplexity is 52.07395967434918
At time: 705.6805744171143 and batch: 200, loss is 3.9475495338439943 and perplexity is 51.8082567791534
At time: 706.3997869491577 and batch: 250, loss is 3.9375741147994994 and perplexity is 51.29401685148454
At time: 707.1237437725067 and batch: 300, loss is 3.9355434942245484 and perplexity is 51.18996384733411
At time: 707.8420910835266 and batch: 350, loss is 3.9327342557907103 and perplexity is 51.04636083545824
At time: 708.5605642795563 and batch: 400, loss is 3.934855132102966 and perplexity is 51.15473874042203
At time: 709.2758696079254 and batch: 450, loss is 3.8821271800994874 and perplexity is 48.52733173295082
At time: 710.0243704319 and batch: 500, loss is 3.9508815670013426 and perplexity is 51.98117152742193
At time: 710.7411124706268 and batch: 550, loss is 3.9559852075576782 and perplexity is 52.24714287636077
At time: 711.4580018520355 and batch: 600, loss is 3.8957123804092406 and perplexity is 49.19108364805812
At time: 712.1756918430328 and batch: 650, loss is 3.913162922859192 and perplexity is 50.05702836919307
At time: 712.8953521251678 and batch: 700, loss is 3.937079071998596 and perplexity is 51.26863040192081
At time: 713.6142172813416 and batch: 750, loss is 3.8856672382354738 and perplexity is 48.69942574014127
At time: 714.332522392273 and batch: 800, loss is 3.8646952962875365 and perplexity is 47.688739282746624
At time: 715.0462930202484 and batch: 850, loss is 3.8247363424301146 and perplexity is 45.82071779446859
At time: 715.7645404338837 and batch: 900, loss is 3.8405565071105956 and perplexity is 46.551373402948954
At time: 716.4799735546112 and batch: 950, loss is 3.8304003620147706 and perplexity is 46.0809836169963
At time: 717.195319890976 and batch: 1000, loss is 3.8439044523239136 and perplexity is 46.707486033154495
At time: 717.9107191562653 and batch: 1050, loss is 3.7858241987228394 and perplexity is 44.07197966308712
At time: 718.6291587352753 and batch: 1100, loss is 3.7589131450653075 and perplexity is 42.90177262735578
At time: 719.3468291759491 and batch: 1150, loss is 3.773036184310913 and perplexity is 43.51197485834957
At time: 720.0637798309326 and batch: 1200, loss is 3.748819909095764 and perplexity is 42.47093285405164
At time: 720.7867925167084 and batch: 1250, loss is 3.788797655105591 and perplexity is 44.203220795511534
At time: 721.5020127296448 and batch: 1300, loss is 3.797240123748779 and perplexity is 44.57798484191245
At time: 722.2391133308411 and batch: 1350, loss is 3.7703704023361206 and perplexity is 43.396135889402025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329791666666667 and perplexity of 75.92846649080082
Finished 34 epochs...
Completing Train Step...
At time: 724.9345891475677 and batch: 50, loss is 3.971082091331482 and perplexity is 53.04189597275953
At time: 725.7220542430878 and batch: 100, loss is 3.97456326007843 and perplexity is 53.22686553162586
At time: 726.455596446991 and batch: 150, loss is 3.952298674583435 and perplexity is 52.054886658518896
At time: 727.1980497837067 and batch: 200, loss is 3.947145266532898 and perplexity is 51.78731662748729
At time: 727.911169052124 and batch: 250, loss is 3.9371712160110475 and perplexity is 51.27335471689425
At time: 728.6577186584473 and batch: 300, loss is 3.9351092529296876 and perplexity is 51.16773987678126
At time: 729.3695914745331 and batch: 350, loss is 3.932273120880127 and perplexity is 51.02282700297168
At time: 730.0838203430176 and batch: 400, loss is 3.9344565582275393 and perplexity is 51.13435386066594
At time: 730.7957479953766 and batch: 450, loss is 3.881768069267273 and perplexity is 48.509908171149185
At time: 731.5088045597076 and batch: 500, loss is 3.9504754304885865 and perplexity is 51.96006436217418
At time: 732.221114397049 and batch: 550, loss is 3.955593810081482 and perplexity is 52.22669747790017
At time: 732.9304995536804 and batch: 600, loss is 3.8953451824188234 and perplexity is 49.173024096914475
At time: 733.6442024707794 and batch: 650, loss is 3.9128337001800535 and perplexity is 50.04055117268587
At time: 734.3554263114929 and batch: 700, loss is 3.9367770195007323 and perplexity is 51.253146922575375
At time: 735.0661652088165 and batch: 750, loss is 3.8854451990127563 and perplexity is 48.688613757889634
At time: 735.777081489563 and batch: 800, loss is 3.8645222997665405 and perplexity is 47.680490010328306
At time: 736.4871606826782 and batch: 850, loss is 3.824714779853821 and perplexity is 45.81972979239729
At time: 737.1971476078033 and batch: 900, loss is 3.84057993888855 and perplexity is 46.55246419717358
At time: 737.9094722270966 and batch: 950, loss is 3.830502400398254 and perplexity is 46.08568588597579
At time: 738.6202578544617 and batch: 1000, loss is 3.8439976263046263 and perplexity is 46.71183815830655
At time: 739.3297939300537 and batch: 1050, loss is 3.7859561252593994 and perplexity is 44.07779431026812
At time: 740.0415122509003 and batch: 1100, loss is 3.7591537380218507 and perplexity is 42.91209573345665
At time: 740.7552402019501 and batch: 1150, loss is 3.7732827281951904 and perplexity is 43.52270379216589
At time: 741.465781211853 and batch: 1200, loss is 3.74908127784729 and perplexity is 42.482034879545914
At time: 742.1762902736664 and batch: 1250, loss is 3.789031777381897 and perplexity is 44.21357096573968
At time: 742.8879973888397 and batch: 1300, loss is 3.7974940443038943 and perplexity is 44.5893055457883
At time: 743.6010973453522 and batch: 1350, loss is 3.7706042528152466 and perplexity is 43.406285283246035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329696451822917 and perplexity of 75.92123731789547
Finished 35 epochs...
Completing Train Step...
At time: 746.1047875881195 and batch: 50, loss is 3.970949287414551 and perplexity is 53.03485226893898
At time: 746.8199622631073 and batch: 100, loss is 3.9743213987350465 and perplexity is 53.21399356710234
At time: 747.5788805484772 and batch: 150, loss is 3.9520086574554445 and perplexity is 52.03979203874725
At time: 748.3034272193909 and batch: 200, loss is 3.946826376914978 and perplexity is 51.770804822736345
At time: 749.019540309906 and batch: 250, loss is 3.9368478536605833 and perplexity is 51.25677752476116
At time: 749.7355222702026 and batch: 300, loss is 3.934776759147644 and perplexity is 51.1507297494685
At time: 750.4520192146301 and batch: 350, loss is 3.931920189857483 and perplexity is 51.004822641795016
At time: 751.1676354408264 and batch: 400, loss is 3.9341467380523683 and perplexity is 51.11851386009815
At time: 751.8831088542938 and batch: 450, loss is 3.881489338874817 and perplexity is 48.49638886961427
At time: 752.6023993492126 and batch: 500, loss is 3.950165939331055 and perplexity is 51.94398566994418
At time: 753.3193736076355 and batch: 550, loss is 3.9553110885620115 and perplexity is 52.21193395371364
At time: 754.0368876457214 and batch: 600, loss is 3.895076398849487 and perplexity is 49.15980897206642
At time: 754.7516195774078 and batch: 650, loss is 3.9125933504104613 and perplexity is 50.02852538299703
At time: 755.4681725502014 and batch: 700, loss is 3.9365573644638063 and perplexity is 51.2418901470445
At time: 756.1840858459473 and batch: 750, loss is 3.8852910470962523 and perplexity is 48.68110889322643
At time: 756.898686170578 and batch: 800, loss is 3.864398899078369 and perplexity is 47.674606568066565
At time: 757.6180238723755 and batch: 850, loss is 3.8246991443634033 and perplexity is 45.8190133840519
At time: 758.3382749557495 and batch: 900, loss is 3.8406007051467896 and perplexity is 46.55343092770444
At time: 759.0580506324768 and batch: 950, loss is 3.830588865280151 and perplexity is 46.08967085164037
At time: 759.7727084159851 and batch: 1000, loss is 3.844086127281189 and perplexity is 46.715972384539434
At time: 760.4895067214966 and batch: 1050, loss is 3.7860673904418944 and perplexity is 44.08269890694639
At time: 761.2058670520782 and batch: 1100, loss is 3.759336857795715 and perplexity is 42.9199545062498
At time: 761.9224026203156 and batch: 1150, loss is 3.773472537994385 and perplexity is 43.53096561189545
At time: 762.6379647254944 and batch: 1200, loss is 3.749282999038696 and perplexity is 42.49060527062086
At time: 763.3559646606445 and batch: 1250, loss is 3.7892197275161745 and perplexity is 44.221881693316284
At time: 764.0726537704468 and batch: 1300, loss is 3.797686581611633 and perplexity is 44.59789147716157
At time: 764.7894523143768 and batch: 1350, loss is 3.770790486335754 and perplexity is 43.41436974134167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329622395833334 and perplexity of 75.9156151037174
Finished 36 epochs...
Completing Train Step...
At time: 767.3322014808655 and batch: 50, loss is 3.9708263063430786 and perplexity is 53.0283303870238
At time: 768.0519888401031 and batch: 100, loss is 3.9741108512878416 and perplexity is 53.202790676012654
At time: 768.7692112922668 and batch: 150, loss is 3.9517659521102906 and perplexity is 52.02716323565975
At time: 769.4857332706451 and batch: 200, loss is 3.9465621280670167 and perplexity is 51.75712625455644
At time: 770.2002680301666 and batch: 250, loss is 3.936575093269348 and perplexity is 51.2427986126035
At time: 770.9181711673737 and batch: 300, loss is 3.9345055866241454 and perplexity is 51.13686095750619
At time: 771.6373682022095 and batch: 350, loss is 3.9316347551345827 and perplexity is 50.99026617193748
At time: 772.3577327728271 and batch: 400, loss is 3.9338913106918336 and perplexity is 51.10545846045253
At time: 773.0750951766968 and batch: 450, loss is 3.8812597465515135 and perplexity is 48.48525574911028
At time: 773.7935166358948 and batch: 500, loss is 3.9499162006378175 and perplexity is 51.93101486656459
At time: 774.5131828784943 and batch: 550, loss is 3.9550909900665285 and perplexity is 52.200443450171825
At time: 775.2281982898712 and batch: 600, loss is 3.89486581325531 and perplexity is 49.14945771443552
At time: 775.9466822147369 and batch: 650, loss is 3.9124054193496702 and perplexity is 50.019124352552474
At time: 776.676061630249 and batch: 700, loss is 3.936387438774109 and perplexity is 51.233183573276094
At time: 777.3969504833221 and batch: 750, loss is 3.8851736068725584 and perplexity is 48.675392108605145
At time: 778.1183035373688 and batch: 800, loss is 3.864302606582642 and perplexity is 47.67001608223552
At time: 778.8360662460327 and batch: 850, loss is 3.8246826124191284 and perplexity is 45.818255912937154
At time: 779.5534887313843 and batch: 900, loss is 3.840614523887634 and perplexity is 46.554074241946736
At time: 780.2721660137177 and batch: 950, loss is 3.8306582736968995 and perplexity is 46.09286997374435
At time: 780.9900054931641 and batch: 1000, loss is 3.844162368774414 and perplexity is 46.719534215809425
At time: 781.7033624649048 and batch: 1050, loss is 3.786158757209778 and perplexity is 44.08672678466938
At time: 782.4197642803192 and batch: 1100, loss is 3.759476923942566 and perplexity is 42.925966559933265
At time: 783.1356294155121 and batch: 1150, loss is 3.773619303703308 and perplexity is 43.53735493377879
At time: 783.9051904678345 and batch: 1200, loss is 3.7494413900375365 and perplexity is 42.497335933055105
At time: 784.6249101161957 and batch: 1250, loss is 3.7893716430664064 and perplexity is 44.22860019511554
At time: 785.3398959636688 and batch: 1300, loss is 3.797836170196533 and perplexity is 44.60456331163985
At time: 786.0557713508606 and batch: 1350, loss is 3.7709397220611574 and perplexity is 43.42084919977425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329571940104167 and perplexity of 75.91178482263278
Finished 37 epochs...
Completing Train Step...
At time: 788.5586128234863 and batch: 50, loss is 3.9707086992263796 and perplexity is 53.02209424469817
At time: 789.3073935508728 and batch: 100, loss is 3.9739225912094116 and perplexity is 53.19277565721099
At time: 790.0259652137756 and batch: 150, loss is 3.9515549612045286 and perplexity is 52.01618713533382
At time: 790.7501015663147 and batch: 200, loss is 3.9463351917266847 and perplexity is 51.745382014385974
At time: 791.4662652015686 and batch: 250, loss is 3.936337504386902 and perplexity is 51.230625339522156
At time: 792.1861538887024 and batch: 300, loss is 3.9342754220962526 and perplexity is 51.12509242044782
At time: 792.9044697284698 and batch: 350, loss is 3.9313951301574708 and perplexity is 50.978049094390016
At time: 793.6204166412354 and batch: 400, loss is 3.933672776222229 and perplexity is 51.09429137643482
At time: 794.3359317779541 and batch: 450, loss is 3.8810633039474487 and perplexity is 48.47573211466658
At time: 795.0521223545074 and batch: 500, loss is 3.949706292152405 and perplexity is 51.920115249889044
At time: 795.770441532135 and batch: 550, loss is 3.9549096965789796 and perplexity is 52.19098070751987
At time: 796.4871399402618 and batch: 600, loss is 3.8946919679641723 and perplexity is 49.14091405530887
At time: 797.2252230644226 and batch: 650, loss is 3.912250642776489 and perplexity is 50.011383162984515
At time: 797.9649524688721 and batch: 700, loss is 3.9362496185302733 and perplexity is 51.22612308997349
At time: 798.6860935688019 and batch: 750, loss is 3.885077242851257 and perplexity is 48.670701778076335
At time: 799.4018383026123 and batch: 800, loss is 3.8642223596572878 and perplexity is 47.66619086349642
At time: 800.1139931678772 and batch: 850, loss is 3.824663519859314 and perplexity is 45.81738113349646
At time: 800.8475971221924 and batch: 900, loss is 3.8406213760375976 and perplexity is 46.554393238537756
At time: 801.5819139480591 and batch: 950, loss is 3.8307119512557986 and perplexity is 46.09534419289162
At time: 802.3013169765472 and batch: 1000, loss is 3.8442248821258547 and perplexity is 46.72245490176099
At time: 803.0725648403168 and batch: 1050, loss is 3.7862327671051026 and perplexity is 44.08998975944864
At time: 803.7898187637329 and batch: 1100, loss is 3.759585452079773 and perplexity is 42.93062548792965
At time: 804.506422996521 and batch: 1150, loss is 3.773734049797058 and perplexity is 43.54235096182153
At time: 805.222668170929 and batch: 1200, loss is 3.7495675802230837 and perplexity is 42.502699018139
At time: 805.94224858284 and batch: 1250, loss is 3.7894959020614625 and perplexity is 44.234096337994046
At time: 806.6603243350983 and batch: 1300, loss is 3.797955050468445 and perplexity is 44.60986622945476
At time: 807.3800227642059 and batch: 1350, loss is 3.771060438156128 and perplexity is 43.426091111515156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329539388020834 and perplexity of 75.90931377610637
Finished 38 epochs...
Completing Train Step...
At time: 809.9204075336456 and batch: 50, loss is 3.9705954599380493 and perplexity is 53.01609040042206
At time: 810.689350605011 and batch: 100, loss is 3.9737509632110597 and perplexity is 53.18364707098105
At time: 811.410174369812 and batch: 150, loss is 3.9513663387298585 and perplexity is 52.00637663866262
At time: 812.1264505386353 and batch: 200, loss is 3.9461348390579225 and perplexity is 51.73501572749457
At time: 812.8455018997192 and batch: 250, loss is 3.936125440597534 and perplexity is 51.21976233084707
At time: 813.5614476203918 and batch: 300, loss is 3.934073848724365 and perplexity is 51.11478800176368
At time: 814.2798109054565 and batch: 350, loss is 3.93118736743927 and perplexity is 50.967458856507996
At time: 814.9948208332062 and batch: 400, loss is 3.9334805536270143 and perplexity is 51.08447084304022
At time: 815.7119750976562 and batch: 450, loss is 3.880890369415283 and perplexity is 48.46734971143633
At time: 816.4300320148468 and batch: 500, loss is 3.9495250368118286 and perplexity is 51.91070530454385
At time: 817.1492743492126 and batch: 550, loss is 3.954754228591919 and perplexity is 52.18286731150957
At time: 817.8658556938171 and batch: 600, loss is 3.8945428323745728 and perplexity is 49.13358594257256
At time: 818.5884084701538 and batch: 650, loss is 3.9121183490753175 and perplexity is 50.00476740962607
At time: 819.3082494735718 and batch: 700, loss is 3.9361330890655517 and perplexity is 51.220154085059285
At time: 820.0235958099365 and batch: 750, loss is 3.8849937295913697 and perplexity is 48.66663729883115
At time: 820.7391405105591 and batch: 800, loss is 3.8641520071029665 and perplexity is 47.66283754317312
At time: 821.5112574100494 and batch: 850, loss is 3.8246413373947146 and perplexity is 45.81636480233384
At time: 822.228752374649 and batch: 900, loss is 3.840621633529663 and perplexity is 46.554405225926175
At time: 822.9523210525513 and batch: 950, loss is 3.8307522201538085 and perplexity is 46.0972004389799
At time: 823.6735880374908 and batch: 1000, loss is 3.8442749547958375 and perplexity is 46.72479447840001
At time: 824.3930723667145 and batch: 1050, loss is 3.7862919664382932 and perplexity is 44.092599934702335
At time: 825.112307548523 and batch: 1100, loss is 3.759670457839966 and perplexity is 42.934274993497105
At time: 825.8340485095978 and batch: 1150, loss is 3.773824772834778 and perplexity is 43.54630143536701
At time: 826.5496776103973 and batch: 1200, loss is 3.749669599533081 and perplexity is 42.50703533535613
At time: 827.2682464122772 and batch: 1250, loss is 3.7895987701416014 and perplexity is 44.23864684860815
At time: 827.9929118156433 and batch: 1300, loss is 3.798051199913025 and perplexity is 44.61415564952488
At time: 828.7107346057892 and batch: 1350, loss is 3.77115912437439 and perplexity is 43.43037687969049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329513753255208 and perplexity of 75.90736788358026
Finished 39 epochs...
Completing Train Step...
At time: 831.2398462295532 and batch: 50, loss is 3.9704853200912478 and perplexity is 53.01025153789895
At time: 831.9604957103729 and batch: 100, loss is 3.9735923480987547 and perplexity is 53.175212009809876
At time: 832.6785123348236 and batch: 150, loss is 3.951194305419922 and perplexity is 51.99743057908385
At time: 833.3961887359619 and batch: 200, loss is 3.9459541845321655 and perplexity is 51.725670406925936
At time: 834.1125535964966 and batch: 250, loss is 3.9359328746795654 and perplexity is 51.209900099890945
At time: 834.8284285068512 and batch: 300, loss is 3.9338933277130126 and perplexity is 51.105561541348564
At time: 835.5447473526001 and batch: 350, loss is 3.9310029792785643 and perplexity is 50.95806192688151
At time: 836.25852394104 and batch: 400, loss is 3.9333081340789793 and perplexity is 51.07566364095457
At time: 836.97238945961 and batch: 450, loss is 3.8807349014282226 and perplexity is 48.45981517584325
At time: 837.6926610469818 and batch: 500, loss is 3.9493646240234375 and perplexity is 51.90237883141284
At time: 838.4079020023346 and batch: 550, loss is 3.954616551399231 and perplexity is 52.175683415372376
At time: 839.1220371723175 and batch: 600, loss is 3.8944112491607665 and perplexity is 49.1271212127627
At time: 839.8940703868866 and batch: 650, loss is 3.9120017957687376 and perplexity is 49.99893952827574
At time: 840.6091077327728 and batch: 700, loss is 3.936031584739685 and perplexity is 51.21495528170307
At time: 841.32861495018 and batch: 750, loss is 3.8849183893203736 and perplexity is 48.662970879304844
At time: 842.0485067367554 and batch: 800, loss is 3.8640881109237672 and perplexity is 47.65979216725929
At time: 842.763973236084 and batch: 850, loss is 3.824616289138794 and perplexity is 45.81521719667574
At time: 843.4820954799652 and batch: 900, loss is 3.840616455078125 and perplexity is 46.55416414681905
At time: 844.1964409351349 and batch: 950, loss is 3.8307813119888308 and perplexity is 46.098541510637084
At time: 844.9110014438629 and batch: 1000, loss is 3.84431405544281 and perplexity is 46.72662148381209
At time: 845.629682302475 and batch: 1050, loss is 3.786338996887207 and perplexity is 44.09467367823521
At time: 846.3450932502747 and batch: 1100, loss is 3.759737477302551 and perplexity is 42.937152521957785
At time: 847.0651330947876 and batch: 1150, loss is 3.773897047042847 and perplexity is 43.54944882355371
At time: 847.7837197780609 and batch: 1200, loss is 3.7497532510757448 and perplexity is 42.51059126316335
At time: 848.5035905838013 and batch: 1250, loss is 3.7896848106384278 and perplexity is 44.24245332751525
At time: 849.225373506546 and batch: 1300, loss is 3.7981300830841063 and perplexity is 44.6176750944083
At time: 849.9537544250488 and batch: 1350, loss is 3.7712404918670654 and perplexity is 43.43391084433612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329498291015625 and perplexity of 75.90619419474588
Finished 40 epochs...
Completing Train Step...
At time: 852.5366117954254 and batch: 50, loss is 3.970378360748291 and perplexity is 53.00458189944029
At time: 853.2557027339935 and batch: 100, loss is 3.9734439134597777 and perplexity is 53.16731955218421
At time: 853.9749088287354 and batch: 150, loss is 3.9510348510742186 and perplexity is 51.9891400238127
At time: 854.6913557052612 and batch: 200, loss is 3.9457887506484983 and perplexity is 51.71711393616983
At time: 855.4082450866699 and batch: 250, loss is 3.935755434036255 and perplexity is 51.20081418840218
At time: 856.1236307621002 and batch: 300, loss is 3.9337286043167112 and perplexity is 51.09714395298749
At time: 856.8427982330322 and batch: 350, loss is 3.9308362197875977 and perplexity is 50.94956489491399
At time: 857.5629765987396 and batch: 400, loss is 3.9331507444381715 and perplexity is 51.067625493177296
At time: 858.2857489585876 and batch: 450, loss is 3.8805929231643677 and perplexity is 48.45293542381703
At time: 859.0596776008606 and batch: 500, loss is 3.949220051765442 and perplexity is 51.8948757296931
At time: 859.77707695961 and batch: 550, loss is 3.954491972923279 and perplexity is 52.16918385311192
At time: 860.493022441864 and batch: 600, loss is 3.8942922496795656 and perplexity is 49.12127545865324
At time: 861.2120587825775 and batch: 650, loss is 3.91189667224884 and perplexity is 49.99368374001971
At time: 861.928670167923 and batch: 700, loss is 3.9359405708312987 and perplexity is 51.21029422056896
At time: 862.6451086997986 and batch: 750, loss is 3.884848704338074 and perplexity is 48.65957991919134
At time: 863.3609185218811 and batch: 800, loss is 3.864028525352478 and perplexity is 47.65695241592043
At time: 864.0776517391205 and batch: 850, loss is 3.8245890760421752 and perplexity is 45.81397043970769
At time: 864.7935853004456 and batch: 900, loss is 3.840606384277344 and perplexity is 46.553695311467166
At time: 865.5103855133057 and batch: 950, loss is 3.8308011531829833 and perplexity is 46.09945616982328
At time: 866.2257063388824 and batch: 1000, loss is 3.8443437957763673 and perplexity is 46.72801116978578
At time: 866.9421253204346 and batch: 1050, loss is 3.786375722885132 and perplexity is 44.096293128866996
At time: 867.6617650985718 and batch: 1100, loss is 3.7597904300689695 and perplexity is 42.93942622316481
At time: 868.3795027732849 and batch: 1150, loss is 3.7739550971984865 and perplexity is 43.55197694921428
At time: 869.0959436893463 and batch: 1200, loss is 3.7498222303390505 and perplexity is 42.51352371356934
At time: 869.8119869232178 and batch: 1250, loss is 3.7897576665878296 and perplexity is 44.24567677087838
At time: 870.5277132987976 and batch: 1300, loss is 3.7981957149505616 and perplexity is 44.62060353179999
At time: 871.2467362880707 and batch: 1350, loss is 3.7713083124160764 and perplexity is 43.43685665590743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329484456380208 and perplexity of 75.9051440674874
Finished 41 epochs...
Completing Train Step...
At time: 873.7065505981445 and batch: 50, loss is 3.9702740287780762 and perplexity is 52.99905211545196
At time: 874.4402704238892 and batch: 100, loss is 3.973303861618042 and perplexity is 53.15987389256211
At time: 875.1601099967957 and batch: 150, loss is 3.950885548591614 and perplexity is 51.9813784955608
At time: 875.8994333744049 and batch: 200, loss is 3.9456350088119505 and perplexity is 51.70916346326815
At time: 876.63427567482 and batch: 250, loss is 3.935590181350708 and perplexity is 51.1923538154243
At time: 877.4132363796234 and batch: 300, loss is 3.933576407432556 and perplexity is 51.08936771866295
At time: 878.1557800769806 and batch: 350, loss is 3.9306835031509397 and perplexity is 50.941784642826136
At time: 878.8965473175049 and batch: 400, loss is 3.933005204200745 and perplexity is 51.060193639668185
At time: 879.6205847263336 and batch: 450, loss is 3.8804613161087036 and perplexity is 48.44655909524174
At time: 880.3448917865753 and batch: 500, loss is 3.949087743759155 and perplexity is 51.88801007634927
At time: 881.0599834918976 and batch: 550, loss is 3.9543771171569824 and perplexity is 52.16319226561418
At time: 881.7866942882538 and batch: 600, loss is 3.8941828918457033 and perplexity is 49.11590395608582
At time: 882.5135135650635 and batch: 650, loss is 3.911800332069397 and perplexity is 49.988867571556156
At time: 883.2401678562164 and batch: 700, loss is 3.935857515335083 and perplexity is 51.20604110079606
At time: 883.9510943889618 and batch: 750, loss is 3.8847827911376953 and perplexity is 48.65637271624945
At time: 884.6661548614502 and batch: 800, loss is 3.8639718961715697 and perplexity is 47.65425371815376
At time: 885.3781871795654 and batch: 850, loss is 3.824559712409973 and perplexity is 45.81262519488071
At time: 886.0943102836609 and batch: 900, loss is 3.8405926847457885 and perplexity is 46.553057552017734
At time: 886.8102853298187 and batch: 950, loss is 3.8308135509490966 and perplexity is 46.100027703641686
At time: 887.5274319648743 and batch: 1000, loss is 3.844365749359131 and perplexity is 46.72903702830697
At time: 888.2416682243347 and batch: 1050, loss is 3.786403956413269 and perplexity is 44.09753814037524
At time: 888.9532477855682 and batch: 1100, loss is 3.7598326110839846 and perplexity is 42.94123748994733
At time: 889.6649000644684 and batch: 1150, loss is 3.774002022743225 and perplexity is 43.554020697408674
At time: 890.3844885826111 and batch: 1200, loss is 3.7498799180984497 and perplexity is 42.5159762942378
At time: 891.0970773696899 and batch: 1250, loss is 3.789819874763489 and perplexity is 44.2484292993251
At time: 891.8139398097992 and batch: 1300, loss is 3.798251094818115 and perplexity is 44.62307468333914
At time: 892.5269913673401 and batch: 1350, loss is 3.771365580558777 and perplexity is 43.439344275242846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3294775390625 and perplexity of 75.90461900930622
Finished 42 epochs...
Completing Train Step...
At time: 895.0384294986725 and batch: 50, loss is 3.9701723051071167 and perplexity is 52.993661131513385
At time: 895.8063488006592 and batch: 100, loss is 3.9731707191467285 and perplexity is 53.15279652673679
At time: 896.5280869007111 and batch: 150, loss is 3.950744142532349 and perplexity is 51.97402853334942
At time: 897.2458174228668 and batch: 200, loss is 3.9454907083511355 and perplexity is 51.70170234548635
At time: 897.9646182060242 and batch: 250, loss is 3.9354349613189696 and perplexity is 51.184408353303695
At time: 898.6801054477692 and batch: 300, loss is 3.933433871269226 and perplexity is 51.08208615515676
At time: 899.396986246109 and batch: 350, loss is 3.9305414295196535 and perplexity is 50.93454767260122
At time: 900.1146242618561 and batch: 400, loss is 3.932869062423706 and perplexity is 51.05324268733837
At time: 900.8327507972717 and batch: 450, loss is 3.880338025093079 and perplexity is 48.440586437962416
At time: 901.5462911128998 and batch: 500, loss is 3.9489650344848632 and perplexity is 51.88164332692593
At time: 902.2682361602783 and batch: 550, loss is 3.9542695045471192 and perplexity is 52.15757915038209
At time: 902.983624458313 and batch: 600, loss is 3.894080762863159 and perplexity is 49.110888054926846
At time: 903.7028381824493 and batch: 650, loss is 3.911710500717163 and perplexity is 49.98437720567641
At time: 904.4232406616211 and batch: 700, loss is 3.9357802724838256 and perplexity is 51.20208595293524
At time: 905.1416356563568 and batch: 750, loss is 3.8847195148468017 and perplexity is 48.65329401886093
At time: 905.8613891601562 and batch: 800, loss is 3.8639174795150755 and perplexity is 47.65166060355365
At time: 906.585506439209 and batch: 850, loss is 3.824528594017029 and perplexity is 45.81119960178929
At time: 907.3009312152863 and batch: 900, loss is 3.8405756425857542 and perplexity is 46.552264194121136
At time: 908.0198848247528 and batch: 950, loss is 3.830819773674011 and perplexity is 46.10031457232519
At time: 908.7355809211731 and batch: 1000, loss is 3.844381036758423 and perplexity is 46.72975139921496
At time: 909.4527390003204 and batch: 1050, loss is 3.7864250469207765 and perplexity is 44.09846818964203
At time: 910.1675827503204 and batch: 1100, loss is 3.759865984916687 and perplexity is 42.942670627537865
At time: 910.8876950740814 and batch: 1150, loss is 3.7740401601791382 and perplexity is 43.55568176775607
At time: 911.607875585556 and batch: 1200, loss is 3.7499282503128053 and perplexity is 42.51803123517712
At time: 912.3248035907745 and batch: 1250, loss is 3.789873309135437 and perplexity is 44.25079374952529
At time: 913.0422601699829 and batch: 1300, loss is 3.798297963142395 and perplexity is 44.62516614108493
At time: 913.758225440979 and batch: 1350, loss is 3.7714141035079956 and perplexity is 43.44145213147849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329467366536458 and perplexity of 75.90384687151995
Finished 43 epochs...
Completing Train Step...
At time: 916.2826502323151 and batch: 50, loss is 3.970072793960571 and perplexity is 52.98838793390986
At time: 917.0005404949188 and batch: 100, loss is 3.973043327331543 and perplexity is 53.14602572678646
At time: 917.7161610126495 and batch: 150, loss is 3.9506093645095826 and perplexity is 51.967024048584406
At time: 918.4341123104095 and batch: 200, loss is 3.945354356765747 and perplexity is 51.694653216995114
At time: 919.1514320373535 and batch: 250, loss is 3.93528799533844 and perplexity is 51.17688653928114
At time: 919.868106842041 and batch: 300, loss is 3.933299198150635 and perplexity is 51.07520723452335
At time: 920.5851852893829 and batch: 350, loss is 3.9304080247879027 and perplexity is 50.92775321614851
At time: 921.3042161464691 and batch: 400, loss is 3.932740559577942 and perplexity is 51.04668262187014
At time: 922.0206782817841 and batch: 450, loss is 3.880221333503723 and perplexity is 48.43493415873478
At time: 922.7389948368073 and batch: 500, loss is 3.9488503694534303 and perplexity is 51.875694657721745
At time: 923.4652428627014 and batch: 550, loss is 3.9541681528091432 and perplexity is 52.15229315696342
At time: 924.1961123943329 and batch: 600, loss is 3.893984546661377 and perplexity is 49.10616301912821
At time: 924.9125349521637 and batch: 650, loss is 3.911625804901123 and perplexity is 49.98014391733315
At time: 925.6328113079071 and batch: 700, loss is 3.935707631111145 and perplexity is 51.198366698215025
At time: 926.3504137992859 and batch: 750, loss is 3.88465847492218 and perplexity is 48.650324316097574
At time: 927.0671825408936 and batch: 800, loss is 3.8638646459579467 and perplexity is 47.649143063326754
At time: 927.7851936817169 and batch: 850, loss is 3.8244961595535276 and perplexity is 45.80971376420416
At time: 928.5003907680511 and batch: 900, loss is 3.840556054115295 and perplexity is 46.55135231540035
At time: 929.2167150974274 and batch: 950, loss is 3.8308208703994753 and perplexity is 46.10036513174181
At time: 929.9326403141022 and batch: 1000, loss is 3.8443907785415647 and perplexity is 46.73020663253676
At time: 930.6503150463104 and batch: 1050, loss is 3.786440305709839 and perplexity is 44.099141083999875
At time: 931.3658797740936 and batch: 1100, loss is 3.759892420768738 and perplexity is 42.94380586863071
At time: 932.0812649726868 and batch: 1150, loss is 3.7740710878372195 and perplexity is 43.557028863820435
At time: 932.849954366684 and batch: 1200, loss is 3.7499692010879517 and perplexity is 42.519772417165036
At time: 933.5643548965454 and batch: 1250, loss is 3.789919877052307 and perplexity is 44.2528544647913
At time: 934.2812647819519 and batch: 1300, loss is 3.798338212966919 and perplexity is 44.62696233233942
At time: 934.9966199398041 and batch: 1350, loss is 3.771455726623535 and perplexity is 43.44326033769109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329462483723958 and perplexity of 75.90347624817248
Finished 44 epochs...
Completing Train Step...
At time: 937.486020565033 and batch: 50, loss is 3.969975414276123 and perplexity is 52.98322819264451
At time: 938.2006707191467 and batch: 100, loss is 3.9729208040237425 and perplexity is 53.13951449881471
At time: 938.9192242622375 and batch: 150, loss is 3.950480089187622 and perplexity is 51.96030642903976
At time: 939.6606397628784 and batch: 200, loss is 3.9452240467071533 and perplexity is 51.68791732259235
At time: 940.3881845474243 and batch: 250, loss is 3.9351476573944093 and perplexity is 51.169704984176455
At time: 941.1140818595886 and batch: 300, loss is 3.9331709861755373 and perplexity is 51.068659201102406
At time: 941.8418774604797 and batch: 350, loss is 3.9302817678451536 and perplexity is 50.921323639624234
At time: 942.5766136646271 and batch: 400, loss is 3.932618427276611 and perplexity is 51.04044855374451
At time: 943.3205411434174 and batch: 450, loss is 3.880110206604004 and perplexity is 48.429552033718586
At time: 944.0518062114716 and batch: 500, loss is 3.9487419939041137 and perplexity is 51.87007290545279
At time: 944.7675380706787 and batch: 550, loss is 3.9540713262557983 and perplexity is 52.14724367463392
At time: 945.4857354164124 and batch: 600, loss is 3.8938929510116576 and perplexity is 49.101665314209484
At time: 946.203779220581 and batch: 650, loss is 3.9115453815460204 and perplexity is 49.97612450810016
At time: 946.9193382263184 and batch: 700, loss is 3.9356385612487794 and perplexity is 51.19483055619567
At time: 947.6390120983124 and batch: 750, loss is 3.8845992279052735 and perplexity is 48.64744201489501
At time: 948.3544435501099 and batch: 800, loss is 3.863813009262085 and perplexity is 47.64668268254183
At time: 949.0718419551849 and batch: 850, loss is 3.82446252822876 and perplexity is 45.80817314874965
At time: 949.7943775653839 and batch: 900, loss is 3.84053412437439 and perplexity is 46.550331467498786
At time: 950.5127060413361 and batch: 950, loss is 3.830817608833313 and perplexity is 46.100214772596026
At time: 951.2875957489014 and batch: 1000, loss is 3.8443960046768186 and perplexity is 46.730450851555226
At time: 952.0036256313324 and batch: 1050, loss is 3.786450562477112 and perplexity is 44.09959340094656
At time: 952.720146894455 and batch: 1100, loss is 3.759912896156311 and perplexity is 42.94468516870171
At time: 953.4373302459717 and batch: 1150, loss is 3.7740959119796753 and perplexity is 43.55811014313076
At time: 954.1570823192596 and batch: 1200, loss is 3.7500041055679323 and perplexity is 42.52125657361185
At time: 954.8805124759674 and batch: 1250, loss is 3.789960470199585 and perplexity is 44.25465086389055
At time: 955.5991806983948 and batch: 1300, loss is 3.7983726406097413 and perplexity is 44.62849875990648
At time: 956.3203830718994 and batch: 1350, loss is 3.77149169921875 and perplexity is 43.44482313261877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329457194010416 and perplexity of 75.90307474158823
Finished 45 epochs...
Completing Train Step...
At time: 959.072692155838 and batch: 50, loss is 3.969880051612854 and perplexity is 52.97817581180354
At time: 959.8798472881317 and batch: 100, loss is 3.9728023767471314 and perplexity is 53.13322170345885
At time: 960.6240947246552 and batch: 150, loss is 3.9503554010391237 and perplexity is 51.95382799853587
At time: 961.3609285354614 and batch: 200, loss is 3.9450992059707644 and perplexity is 51.68146496769813
At time: 962.1028432846069 and batch: 250, loss is 3.935013384819031 and perplexity is 51.16283475735873
At time: 962.8207056522369 and batch: 300, loss is 3.933047957420349 and perplexity is 51.06237667400543
At time: 963.5436270236969 and batch: 350, loss is 3.9301615619659422 and perplexity is 50.91520296502343
At time: 964.2619800567627 and batch: 400, loss is 3.9325013303756715 and perplexity is 51.034472225307894
At time: 964.9817199707031 and batch: 450, loss is 3.880003504753113 and perplexity is 48.42438478656109
At time: 965.6980073451996 and batch: 500, loss is 3.94863893032074 and perplexity is 51.864727265344435
At time: 966.4143285751343 and batch: 550, loss is 3.9539786911010744 and perplexity is 52.14241323038563
At time: 967.1297461986542 and batch: 600, loss is 3.893805365562439 and perplexity is 49.09736491112467
At time: 967.8453600406647 and batch: 650, loss is 3.9114683055877686 and perplexity is 49.97227269885684
At time: 968.5679521560669 and batch: 700, loss is 3.9355723428726197 and perplexity is 51.19144062988741
At time: 969.2865443229675 and batch: 750, loss is 3.884541211128235 and perplexity is 48.6446197289689
At time: 970.0590195655823 and batch: 800, loss is 3.8637624168395996 and perplexity is 47.644272182418575
At time: 970.7766554355621 and batch: 850, loss is 3.824427900314331 and perplexity is 45.80658693471352
At time: 971.4956421852112 and batch: 900, loss is 3.8405102682113648 and perplexity is 46.5492209684486
At time: 972.2136828899384 and batch: 950, loss is 3.830811023712158 and perplexity is 46.09991119809603
At time: 972.9363529682159 and batch: 1000, loss is 3.844397196769714 and perplexity is 46.73050655862689
At time: 973.6538999080658 and batch: 1050, loss is 3.7864567184448243 and perplexity is 44.09986487745527
At time: 974.3742568492889 and batch: 1100, loss is 3.759928574562073 and perplexity is 42.94535847817931
At time: 975.101407289505 and batch: 1150, loss is 3.7741159868240355 and perplexity is 43.558984574189516
At time: 975.8188166618347 and batch: 1200, loss is 3.7500337028503417 and perplexity is 42.522515105875534
At time: 976.5340805053711 and batch: 1250, loss is 3.789996123313904 and perplexity is 44.25622870814431
At time: 977.2502093315125 and batch: 1300, loss is 3.7984024906158447 and perplexity is 44.62983094074955
At time: 977.9714152812958 and batch: 1350, loss is 3.7715228605270386 and perplexity is 43.44617695123921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329451497395834 and perplexity of 75.90264235225736
Finished 46 epochs...
Completing Train Step...
At time: 980.4789762496948 and batch: 50, loss is 3.969786067008972 and perplexity is 52.973196912909074
At time: 981.2261712551117 and batch: 100, loss is 3.972687478065491 and perplexity is 53.12711711704499
At time: 981.9420344829559 and batch: 150, loss is 3.9502347135543823 and perplexity is 51.94755820006275
At time: 982.6589524745941 and batch: 200, loss is 3.9449787855148317 and perplexity is 51.675241836827105
At time: 983.3762528896332 and batch: 250, loss is 3.9348840284347535 and perplexity is 51.15621694608243
At time: 984.0917763710022 and batch: 300, loss is 3.9329295492172243 and perplexity is 51.056330827682146
At time: 984.8072288036346 and batch: 350, loss is 3.9300460052490234 and perplexity is 50.90931971125883
At time: 985.5230047702789 and batch: 400, loss is 3.932388672828674 and perplexity is 51.028723130700214
At time: 986.2390429973602 and batch: 450, loss is 3.879900860786438 and perplexity is 48.4194145707085
At time: 986.9542927742004 and batch: 500, loss is 3.9485402297973633 and perplexity is 51.85960844223798
At time: 987.671138048172 and batch: 550, loss is 3.953889331817627 and perplexity is 52.137754029876646
At time: 988.387130022049 and batch: 600, loss is 3.8937209129333494 and perplexity is 49.093218684658886
At time: 989.1613571643829 and batch: 650, loss is 3.9113939714431765 and perplexity is 49.968558190771546
At time: 989.8751022815704 and batch: 700, loss is 3.935508542060852 and perplexity is 51.188174678605954
At time: 990.5923275947571 and batch: 750, loss is 3.8844843339920043 and perplexity is 48.64185304098708
At time: 991.3111114501953 and batch: 800, loss is 3.8637127113342284 and perplexity is 47.64190405864659
At time: 992.0267033576965 and batch: 850, loss is 3.824392514228821 and perplexity is 45.804966047589915
At time: 992.7437064647675 and batch: 900, loss is 3.8404848337173463 and perplexity is 46.54803702762285
At time: 993.4619402885437 and batch: 950, loss is 3.8308013582229616 and perplexity is 46.099465622055746
At time: 994.1777210235596 and batch: 1000, loss is 3.8443951654434203 and perplexity is 46.73041163381661
At time: 994.8933398723602 and batch: 1050, loss is 3.7864591121673583 and perplexity is 44.09997044042191
At time: 995.6089377403259 and batch: 1100, loss is 3.759940104484558 and perplexity is 42.94585363768823
At time: 996.3287606239319 and batch: 1150, loss is 3.7741318798065184 and perplexity is 43.55967686186957
At time: 997.0492272377014 and batch: 1200, loss is 3.750058879852295 and perplexity is 42.523585708798656
At time: 997.7662591934204 and batch: 1250, loss is 3.7900274801254272 and perplexity is 44.25761646412433
At time: 998.4896533489227 and batch: 1300, loss is 3.798428483009338 and perplexity is 44.6309909919531
At time: 999.2086696624756 and batch: 1350, loss is 3.7715501165390015 and perplexity is 43.44736113689596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329449055989583 and perplexity of 75.9024570432981
Finished 47 epochs...
Completing Train Step...
At time: 1001.7550871372223 and batch: 50, loss is 3.9696940088272097 and perplexity is 52.96832052117845
At time: 1002.4773464202881 and batch: 100, loss is 3.9725755786895753 and perplexity is 53.1211725583978
At time: 1003.1995813846588 and batch: 150, loss is 3.95011736869812 and perplexity is 51.94146277895275
At time: 1003.9155297279358 and batch: 200, loss is 3.9448623609542848 and perplexity is 51.66922591971214
At time: 1004.631306886673 and batch: 250, loss is 3.934758839607239 and perplexity is 51.14981316011245
At time: 1005.3477327823639 and batch: 300, loss is 3.932814908027649 and perplexity is 51.050478004674424
At time: 1006.0653686523438 and batch: 350, loss is 3.92993492603302 and perplexity is 50.903665058001074
At time: 1006.7835988998413 and batch: 400, loss is 3.932279667854309 and perplexity is 51.023161049196254
At time: 1007.5609397888184 and batch: 450, loss is 3.8798012113571168 and perplexity is 48.414589844073085
At time: 1008.2789032459259 and batch: 500, loss is 3.948445324897766 and perplexity is 51.854686944846414
At time: 1008.9973902702332 and batch: 550, loss is 3.9538028621673584 and perplexity is 52.13324589143129
At time: 1009.7199161052704 and batch: 600, loss is 3.893638949394226 and perplexity is 49.089194995608686
At time: 1010.4353799819946 and batch: 650, loss is 3.9113218641281127 and perplexity is 49.964955222104564
At time: 1011.153256893158 and batch: 700, loss is 3.935446662902832 and perplexity is 51.18500729545476
At time: 1011.8692741394043 and batch: 750, loss is 3.8844282484054564 and perplexity is 48.639125010630806
At time: 1012.5877890586853 and batch: 800, loss is 3.8636635637283323 and perplexity is 47.63956263066003
At time: 1013.305385351181 and batch: 850, loss is 3.82435649394989 and perplexity is 45.803316169651175
At time: 1014.0213389396667 and batch: 900, loss is 3.840458159446716 and perplexity is 46.54679540924558
At time: 1014.7396030426025 and batch: 950, loss is 3.830789213180542 and perplexity is 46.09890574549011
At time: 1015.455733537674 and batch: 1000, loss is 3.844390254020691 and perplexity is 46.73018212157437
At time: 1016.1738159656525 and batch: 1050, loss is 3.7864584016799925 and perplexity is 44.09993910796121
At time: 1016.8896102905273 and batch: 1100, loss is 3.759948492050171 and perplexity is 42.94621385036406
At time: 1017.6059641838074 and batch: 1150, loss is 3.774144320487976 and perplexity is 43.56021877730469
At time: 1018.3213155269623 and batch: 1200, loss is 3.7500805377960207 and perplexity is 42.52450669219821
At time: 1019.0366728305817 and batch: 1250, loss is 3.7900552129745484 and perplexity is 44.25884387094386
At time: 1019.753767490387 and batch: 1300, loss is 3.7984509325027465 and perplexity is 44.63199294633782
At time: 1020.469306230545 and batch: 1350, loss is 3.771574020385742 and perplexity is 43.44839970837074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.3294482421875 and perplexity of 75.90239527374554
Finished 48 epochs...
Completing Train Step...
At time: 1023.0007379055023 and batch: 50, loss is 3.9696031045913696 and perplexity is 52.96350569532508
At time: 1023.713122844696 and batch: 100, loss is 3.972466478347778 and perplexity is 53.11537733645111
At time: 1024.4306499958038 and batch: 150, loss is 3.9500030851364136 and perplexity is 51.93552706276997
At time: 1025.1420879364014 and batch: 200, loss is 3.944749116897583 and perplexity is 51.66337501825851
At time: 1025.9098672866821 and batch: 250, loss is 3.9346374559402464 and perplexity is 51.14360478503039
At time: 1026.6239693164825 and batch: 300, loss is 3.932703523635864 and perplexity is 51.04479209489822
At time: 1027.336490392685 and batch: 350, loss is 3.9298271560668945 and perplexity is 50.89817946733838
At time: 1028.0483474731445 and batch: 400, loss is 3.932173795700073 and perplexity is 51.01775940316704
At time: 1028.7582213878632 and batch: 450, loss is 3.879704489707947 and perplexity is 48.409907331553306
At time: 1029.47385764122 and batch: 500, loss is 3.948353495597839 and perplexity is 51.849925383875096
At time: 1030.193745136261 and batch: 550, loss is 3.953718690872192 and perplexity is 52.128857953275194
At time: 1030.9066581726074 and batch: 600, loss is 3.893559422492981 and perplexity is 49.085291239274945
At time: 1031.6213765144348 and batch: 650, loss is 3.9112517070770263 and perplexity is 49.96144995114969
At time: 1032.3578493595123 and batch: 700, loss is 3.9353861904144285 and perplexity is 51.18191210428256
At time: 1033.0828294754028 and batch: 750, loss is 3.884373278617859 and perplexity is 48.63645140174458
At time: 1033.7996573448181 and batch: 800, loss is 3.8636148262023924 and perplexity is 47.637240852819865
At time: 1034.505654335022 and batch: 850, loss is 3.824319839477539 and perplexity is 45.8016373040342
At time: 1035.215159893036 and batch: 900, loss is 3.8404301404953003 and perplexity is 46.545491235117325
At time: 1035.9328546524048 and batch: 950, loss is 3.8307750177383424 and perplexity is 46.09825135578282
At time: 1036.6551425457 and batch: 1000, loss is 3.8443829679489134 and perplexity is 46.72984164335364
At time: 1037.3667380809784 and batch: 1050, loss is 3.786455135345459 and perplexity is 44.09979506304243
At time: 1038.077550649643 and batch: 1100, loss is 3.759953637123108 and perplexity is 42.94643481233512
At time: 1038.7881672382355 and batch: 1150, loss is 3.774153571128845 and perplexity is 43.56062173910861
At time: 1039.5017085075378 and batch: 1200, loss is 3.7500988483428954 and perplexity is 42.525285346300095
At time: 1040.216941356659 and batch: 1250, loss is 3.790079789161682 and perplexity is 44.2599315979392
At time: 1040.9287943840027 and batch: 1300, loss is 3.798470616340637 and perplexity is 44.632871483898185
At time: 1041.6398854255676 and batch: 1350, loss is 3.771595206260681 and perplexity is 43.44932021048403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329443766276042 and perplexity of 75.90205554210519
Finished 49 epochs...
Completing Train Step...
At time: 1044.15864443779 and batch: 50, loss is 3.969513521194458 and perplexity is 52.95876125708717
At time: 1044.905106306076 and batch: 100, loss is 3.9723596000671386 and perplexity is 53.10970075960266
At time: 1045.6212990283966 and batch: 150, loss is 3.9498915576934817 and perplexity is 51.929735149223994
At time: 1046.337577342987 and batch: 200, loss is 3.9446388149261473 and perplexity is 51.657676760413295
At time: 1047.0546026229858 and batch: 250, loss is 3.934519352912903 and perplexity is 51.13756492714578
At time: 1047.7760603427887 and batch: 300, loss is 3.932594909667969 and perplexity is 51.03924821856506
At time: 1048.4924485683441 and batch: 350, loss is 3.9297226953506468 and perplexity is 50.892862884747366
At time: 1049.2083168029785 and batch: 400, loss is 3.9320706939697265 and perplexity is 51.0124996550434
At time: 1049.9312744140625 and batch: 450, loss is 3.879610023498535 and perplexity is 48.40533444710465
At time: 1050.6488862037659 and batch: 500, loss is 3.9482645177841187 and perplexity is 51.84531209611604
At time: 1051.3640995025635 and batch: 550, loss is 3.9536366748809812 and perplexity is 52.12458272864025
At time: 1052.083335161209 and batch: 600, loss is 3.8934818840026857 and perplexity is 49.08148538744846
At time: 1052.7976729869843 and batch: 650, loss is 3.911183114051819 and perplexity is 49.958023061685495
At time: 1053.5129253864288 and batch: 700, loss is 3.9353270435333254 and perplexity is 51.178884943337145
At time: 1054.2295293807983 and batch: 750, loss is 3.8843189764022825 and perplexity is 48.63381040638228
At time: 1054.9462132453918 and batch: 800, loss is 3.8635665655136107 and perplexity is 47.63494190223957
At time: 1055.6610262393951 and batch: 850, loss is 3.824282693862915 and perplexity is 45.79993600566383
At time: 1056.3768796920776 and batch: 900, loss is 3.840401306152344 and perplexity is 46.5441491458092
At time: 1057.093018770218 and batch: 950, loss is 3.8307589626312257 and perplexity is 46.09751124936067
At time: 1057.8092296123505 and batch: 1000, loss is 3.8443735313415526 and perplexity is 46.72940067426664
At time: 1058.5287384986877 and batch: 1050, loss is 3.786449432373047 and perplexity is 44.099543563844954
At time: 1059.244564294815 and batch: 1100, loss is 3.759956374168396 and perplexity is 42.94655235883303
At time: 1059.9589817523956 and batch: 1150, loss is 3.774160451889038 and perplexity is 43.560921470331834
At time: 1060.6784234046936 and batch: 1200, loss is 3.750114531517029 and perplexity is 42.52595228298509
At time: 1061.3937540054321 and batch: 1250, loss is 3.7901017761230467 and perplexity is 44.26090475004352
At time: 1062.111872434616 and batch: 1300, loss is 3.798487734794617 and perplexity is 44.63363553619434
At time: 1062.828741312027 and batch: 1350, loss is 3.771613807678223 and perplexity is 43.45012843694823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 151 batches
Done Evaluating: Achieved loss of 4.329442952473959 and perplexity of 75.90199377287938
Finished Training.
Improved accuracyfrom -78.02705800717966 to -75.90199377287938
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efc565c8f98>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -393.90350062627937, 'params': {'seq_len': 20, 'lr': 24.508440653647003, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.8051793355431327, 'anneal': 7.835234642237461, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -161.06340312676477, 'params': {'seq_len': 20, 'lr': 25.35419659816711, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.025124451429033634, 'anneal': 2.101066051940741, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -78.02705800717966, 'params': {'seq_len': 20, 'lr': 7.586349786777084, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.1501156749828374, 'anneal': 2.9661893316284225, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -82.27301198694894, 'params': {'seq_len': 20, 'lr': 0.7211867437783614, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.9818069729096831, 'anneal': 6.186620571858408, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -98.12660620353982, 'params': {'seq_len': 20, 'lr': 11.24667852206237, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.11779092712664929, 'anneal': 4.93421875558552, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -75.90199377287938, 'params': {'seq_len': 20, 'lr': 5.4106838696977135, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'dropout': 0.676365908478051, 'anneal': 6.5125845255230495, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'num_layers': 1}}]
