Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'num_layers': 1, 'anneal': 7.964455081285388, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.34985756432001314, 'tune_wordvecs': True, 'lr': 2.824426131503266, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7028172016143799 and batch: 50, loss is 7.40599479675293 and perplexity is 1645.8212796425005
At time: 2.571237802505493 and batch: 100, loss is 6.538463125228882 and perplexity is 691.2234374265744
At time: 3.4394962787628174 and batch: 150, loss is 6.265924501419067 and perplexity is 526.3279520401742
At time: 4.3404998779296875 and batch: 200, loss is 6.139637365341186 and perplexity is 463.88531945967463
At time: 5.212413787841797 and batch: 250, loss is 6.093957605361939 and perplexity is 443.17184436684704
At time: 6.0817224979400635 and batch: 300, loss is 6.028491020202637 and perplexity is 415.0881968589376
At time: 6.950055122375488 and batch: 350, loss is 6.005515871047973 and perplexity is 405.66020312104365
At time: 7.817111968994141 and batch: 400, loss is 5.912904415130615 and perplexity is 369.7785877864036
At time: 8.685819864273071 and batch: 450, loss is 5.8717629337310795 and perplexity is 354.8740485593784
At time: 9.553391218185425 and batch: 500, loss is 5.840628042221069 and perplexity is 343.9953164194826
At time: 10.423221111297607 and batch: 550, loss is 5.827464256286621 and perplexity is 339.4967100191732
At time: 11.29025149345398 and batch: 600, loss is 5.829551343917847 and perplexity is 340.2060093309343
At time: 12.158522367477417 and batch: 650, loss is 5.789326572418213 and perplexity is 326.79287892008347
At time: 13.027036190032959 and batch: 700, loss is 5.7809899711608885 and perplexity is 324.0798613801419
At time: 13.89692759513855 and batch: 750, loss is 5.718969192504883 and perplexity is 304.59078661189176
At time: 14.764987468719482 and batch: 800, loss is 5.711023607254028 and perplexity is 302.1802238980373
At time: 15.632370471954346 and batch: 850, loss is 5.757310485839843 and perplexity is 316.49596283265
At time: 16.5005521774292 and batch: 900, loss is 5.732123708724975 and perplexity is 308.6240003836477
At time: 17.367981672286987 and batch: 950, loss is 5.690363550186158 and perplexity is 296.00121237766365
At time: 18.236300945281982 and batch: 1000, loss is 5.672024517059326 and perplexity is 290.62230903354146
At time: 19.104881048202515 and batch: 1050, loss is 5.650548028945923 and perplexity is 284.4473084755127
At time: 19.973487615585327 and batch: 1100, loss is 5.630218410491944 and perplexity is 278.72298701025164
At time: 20.841315746307373 and batch: 1150, loss is 5.6629885387420655 and perplexity is 288.0080809919326
At time: 21.717237949371338 and batch: 1200, loss is 5.65360460281372 and perplexity is 285.3180727851447
At time: 22.6023907661438 and batch: 1250, loss is 5.623247680664062 and perplexity is 276.7868403884343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.154726794166287 and perplexity of 173.2484678004548
Finished 1 epochs...
Completing Train Step...
At time: 25.430221557617188 and batch: 50, loss is 5.407054882049561 and perplexity is 222.97393518482573
At time: 26.353574991226196 and batch: 100, loss is 5.411845865249634 and perplexity is 224.04476267233096
At time: 27.231998920440674 and batch: 150, loss is 5.2754425621032714 and perplexity is 195.47696807032665
At time: 28.103564262390137 and batch: 200, loss is 5.28220383644104 and perplexity is 196.80311966347026
At time: 28.98230504989624 and batch: 250, loss is 5.291354207992554 and perplexity is 198.6122056124148
At time: 29.910284519195557 and batch: 300, loss is 5.283329668045044 and perplexity is 197.02481160583542
At time: 30.792196035385132 and batch: 350, loss is 5.283783102035523 and perplexity is 197.1141696098323
At time: 31.655261754989624 and batch: 400, loss is 5.248298034667969 and perplexity is 190.2422071250618
At time: 32.51816439628601 and batch: 450, loss is 5.204604988098144 and perplexity is 182.1089233671172
At time: 33.381070137023926 and batch: 500, loss is 5.190546607971191 and perplexity is 179.56667868916045
At time: 34.24577045440674 and batch: 550, loss is 5.201512899398804 and perplexity is 181.54669609937022
At time: 35.113518714904785 and batch: 600, loss is 5.199367618560791 and perplexity is 181.15764491226062
At time: 35.977741956710815 and batch: 650, loss is 5.170578966140747 and perplexity is 176.01671570779877
At time: 36.842811822891235 and batch: 700, loss is 5.163493518829346 and perplexity is 174.77396645908584
At time: 37.705015659332275 and batch: 750, loss is 5.142436113357544 and perplexity is 171.13215826549896
At time: 38.56719493865967 and batch: 800, loss is 5.142713718414306 and perplexity is 171.1796720127313
At time: 39.43157649040222 and batch: 850, loss is 5.1871312141418455 and perplexity is 178.95443388651844
At time: 40.296818256378174 and batch: 900, loss is 5.15485520362854 and perplexity is 173.27071597144857
At time: 41.16228199005127 and batch: 950, loss is 5.134128274917603 and perplexity is 169.7163094080349
At time: 42.02858304977417 and batch: 1000, loss is 5.109930419921875 and perplexity is 165.6588279185225
At time: 42.89424395561218 and batch: 1050, loss is 5.088941240310669 and perplexity is 162.21802119872837
At time: 43.76328372955322 and batch: 1100, loss is 5.063529672622681 and perplexity is 158.14774208817062
At time: 44.62926888465881 and batch: 1150, loss is 5.091587114334106 and perplexity is 162.64779796401467
At time: 45.49814176559448 and batch: 1200, loss is 5.082332439422608 and perplexity is 161.14948934243017
At time: 46.364081144332886 and batch: 1250, loss is 5.0668150997161865 and perplexity is 158.66817942613446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.847997512260493 and perplexity of 127.48484722739562
Finished 2 epochs...
Completing Train Step...
At time: 48.80737853050232 and batch: 50, loss is 4.999622325897217 and perplexity is 148.3571178791969
At time: 49.69993305206299 and batch: 100, loss is 5.030993146896362 and perplexity is 153.0849730895311
At time: 50.56232213973999 and batch: 150, loss is 4.914772787094116 and perplexity is 136.28833981456
At time: 51.424582958221436 and batch: 200, loss is 4.951571083068847 and perplexity is 141.39693583954232
At time: 52.28739905357361 and batch: 250, loss is 4.963431873321533 and perplexity is 143.08400041973036
At time: 53.1490204334259 and batch: 300, loss is 4.971885871887207 and perplexity is 144.29875990662384
At time: 54.01264691352844 and batch: 350, loss is 4.967065935134888 and perplexity is 143.60492247994887
At time: 54.87584710121155 and batch: 400, loss is 4.958755321502686 and perplexity is 142.41642287447849
At time: 55.73813796043396 and batch: 450, loss is 4.906481504440308 and perplexity is 135.16300634246886
At time: 56.6019926071167 and batch: 500, loss is 4.910532913208008 and perplexity is 135.7117177079104
At time: 57.463675022125244 and batch: 550, loss is 4.919298887252808 and perplexity is 136.90659257253682
At time: 58.32527470588684 and batch: 600, loss is 4.924467573165893 and perplexity is 137.6160516551069
At time: 59.18887710571289 and batch: 650, loss is 4.911595735549927 and perplexity is 135.85603183010258
At time: 60.05049657821655 and batch: 700, loss is 4.907712068557739 and perplexity is 135.3294354679312
At time: 60.91334414482117 and batch: 750, loss is 4.897128000259399 and perplexity is 133.90465277942417
At time: 61.77615237236023 and batch: 800, loss is 4.907486505508423 and perplexity is 135.29891359023816
At time: 62.6395161151886 and batch: 850, loss is 4.95047737121582 and perplexity is 141.24237287391406
At time: 63.503581047058105 and batch: 900, loss is 4.9215659904479985 and perplexity is 137.21732604533403
At time: 64.36575865745544 and batch: 950, loss is 4.911868848800659 and perplexity is 135.89314097985687
At time: 65.2320966720581 and batch: 1000, loss is 4.890329360961914 and perplexity is 132.99737098042567
At time: 66.09637141227722 and batch: 1050, loss is 4.8697050666809085 and perplexity is 130.28248658336
At time: 66.96183061599731 and batch: 1100, loss is 4.840454320907593 and perplexity is 126.52682245486291
At time: 67.83030867576599 and batch: 1150, loss is 4.871647119522095 and perplexity is 130.5357479004168
At time: 68.69612956047058 and batch: 1200, loss is 4.871474800109863 and perplexity is 130.513255995015
At time: 69.56145405769348 and batch: 1250, loss is 4.8666802024841305 and perplexity is 129.8889951832814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.745139017592382 and perplexity of 115.0237947227012
Finished 3 epochs...
Completing Train Step...
At time: 72.08434700965881 and batch: 50, loss is 4.807436838150024 and perplexity is 122.41743933423278
At time: 72.94577121734619 and batch: 100, loss is 4.839413700103759 and perplexity is 126.3952244948364
At time: 73.80638480186462 and batch: 150, loss is 4.728209848403931 and perplexity is 113.09292754451722
At time: 74.66830849647522 and batch: 200, loss is 4.780199241638184 and perplexity is 119.12808295484501
At time: 75.53137230873108 and batch: 250, loss is 4.786674871444702 and perplexity is 119.90201546534621
At time: 76.39280772209167 and batch: 300, loss is 4.799788599014282 and perplexity is 121.48473281167828
At time: 77.25316095352173 and batch: 350, loss is 4.793987855911255 and perplexity is 120.78207103622059
At time: 78.1140832901001 and batch: 400, loss is 4.793286981582642 and perplexity is 120.69744764192099
At time: 78.97498917579651 and batch: 450, loss is 4.731634216308594 and perplexity is 113.480863173917
At time: 79.83447980880737 and batch: 500, loss is 4.747251815795899 and perplexity is 115.26707369874671
At time: 80.6944990158081 and batch: 550, loss is 4.752551879882812 and perplexity is 115.87961840566338
At time: 81.55259370803833 and batch: 600, loss is 4.7645639801025395 and perplexity is 117.27996973756505
At time: 82.41277885437012 and batch: 650, loss is 4.759612140655517 and perplexity is 116.70065368089365
At time: 83.27322387695312 and batch: 700, loss is 4.754452762603759 and perplexity is 116.1001014598243
At time: 84.13363647460938 and batch: 750, loss is 4.747905197143555 and perplexity is 115.34241166423092
At time: 84.99739408493042 and batch: 800, loss is 4.762222833633423 and perplexity is 117.00572130365444
At time: 85.86141419410706 and batch: 850, loss is 4.805732765197754 and perplexity is 122.20900872789434
At time: 86.72553658485413 and batch: 900, loss is 4.775154790878296 and perplexity is 118.52866035625017
At time: 87.59048843383789 and batch: 950, loss is 4.770220012664795 and perplexity is 117.94518854150853
At time: 88.45477628707886 and batch: 1000, loss is 4.751980276107788 and perplexity is 115.81340010545244
At time: 89.31917929649353 and batch: 1050, loss is 4.730496606826782 and perplexity is 113.351839671061
At time: 90.1817581653595 and batch: 1100, loss is 4.697319898605347 and perplexity is 109.65289740152987
At time: 91.04716324806213 and batch: 1150, loss is 4.730309543609619 and perplexity is 113.33063769437754
At time: 91.91218876838684 and batch: 1200, loss is 4.736699876785278 and perplexity is 114.05717717047949
At time: 92.77519631385803 and batch: 1250, loss is 4.734741201400757 and perplexity is 113.83399482736016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.690507652115648 and perplexity of 108.908453383127
Finished 4 epochs...
Completing Train Step...
At time: 95.24932217597961 and batch: 50, loss is 4.6799093532562255 and perplexity is 107.76030400799647
At time: 96.14764952659607 and batch: 100, loss is 4.708356475830078 and perplexity is 110.86979290011321
At time: 97.0149519443512 and batch: 150, loss is 4.60018177986145 and perplexity is 99.50240153082305
At time: 97.88315010070801 and batch: 200, loss is 4.65880012512207 and perplexity is 105.50940808719135
At time: 98.75228238105774 and batch: 250, loss is 4.664405879974365 and perplexity is 106.10252885514144
At time: 99.62118434906006 and batch: 300, loss is 4.6792506504058835 and perplexity is 107.68934536148973
At time: 100.48962903022766 and batch: 350, loss is 4.672018527984619 and perplexity is 106.9133323253615
At time: 101.36926937103271 and batch: 400, loss is 4.674813499450684 and perplexity is 107.21257002416728
At time: 102.26025438308716 and batch: 450, loss is 4.608695774078369 and perplexity is 100.35318102861201
At time: 103.14541983604431 and batch: 500, loss is 4.63073148727417 and perplexity is 102.58907927678864
At time: 104.02657675743103 and batch: 550, loss is 4.636928634643555 and perplexity is 103.22681294332055
At time: 104.9219605922699 and batch: 600, loss is 4.649318323135376 and perplexity is 104.51371670598937
At time: 105.80868816375732 and batch: 650, loss is 4.649537725448608 and perplexity is 104.53664977289111
At time: 106.69024014472961 and batch: 700, loss is 4.643114194869995 and perplexity is 103.86730747835513
At time: 107.571524143219 and batch: 750, loss is 4.638200130462646 and perplexity is 103.35814888325113
At time: 108.44800043106079 and batch: 800, loss is 4.655280990600586 and perplexity is 105.13875885162987
At time: 109.31673097610474 and batch: 850, loss is 4.699174175262451 and perplexity is 109.8564128382142
At time: 110.18637347221375 and batch: 900, loss is 4.666183319091797 and perplexity is 106.29128734405276
At time: 111.05843138694763 and batch: 950, loss is 4.664181518554687 and perplexity is 106.07872621143345
At time: 111.92801833152771 and batch: 1000, loss is 4.647779283523559 and perplexity is 104.35298967036329
At time: 112.79574275016785 and batch: 1050, loss is 4.62707636833191 and perplexity is 102.21478844531606
At time: 113.66454434394836 and batch: 1100, loss is 4.589597682952881 and perplexity is 98.45481214368237
At time: 114.53351712226868 and batch: 1150, loss is 4.622130355834961 and perplexity is 101.71048100781064
At time: 115.40475177764893 and batch: 1200, loss is 4.634344863891601 and perplexity is 102.96044279111219
At time: 116.30179190635681 and batch: 1250, loss is 4.632593641281128 and perplexity is 102.78029392213807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.661231716183851 and perplexity of 105.76627599311813
Finished 5 epochs...
Completing Train Step...
At time: 118.77322578430176 and batch: 50, loss is 4.578956480026245 and perplexity is 97.4126890638737
At time: 119.64064621925354 and batch: 100, loss is 4.608006448745727 and perplexity is 100.28402887562146
At time: 120.51031732559204 and batch: 150, loss is 4.5031850624084475 and perplexity is 90.30429856160058
At time: 121.3770079612732 and batch: 200, loss is 4.566749153137207 and perplexity is 96.23076924345555
At time: 122.24434757232666 and batch: 250, loss is 4.569222574234009 and perplexity is 96.46908306198041
At time: 123.11269450187683 and batch: 300, loss is 4.585400714874267 and perplexity is 98.04246634627725
At time: 123.98094630241394 and batch: 350, loss is 4.574509563446045 and perplexity is 96.9804647068554
At time: 124.84652805328369 and batch: 400, loss is 4.580272655487061 and perplexity is 97.54098566669094
At time: 125.71311640739441 and batch: 450, loss is 4.51225884437561 and perplexity is 91.1274288823832
At time: 126.58141422271729 and batch: 500, loss is 4.540073051452636 and perplexity is 93.69764461851862
At time: 127.44924426078796 and batch: 550, loss is 4.546111888885498 and perplexity is 94.2651813685751
At time: 128.31357407569885 and batch: 600, loss is 4.559600868225098 and perplexity is 95.54533703861675
At time: 129.17977833747864 and batch: 650, loss is 4.562587080001831 and perplexity is 95.83108208456075
At time: 130.04541945457458 and batch: 700, loss is 4.556404800415039 and perplexity is 95.24045513365442
At time: 130.91646361351013 and batch: 750, loss is 4.550680522918701 and perplexity is 94.69682975528137
At time: 131.78050899505615 and batch: 800, loss is 4.573786211013794 and perplexity is 96.91033901768047
At time: 132.64728999137878 and batch: 850, loss is 4.613641166687012 and perplexity is 100.85069609802524
At time: 133.51311206817627 and batch: 900, loss is 4.579534940719604 and perplexity is 97.46905477663445
At time: 134.3814115524292 and batch: 950, loss is 4.580666561126709 and perplexity is 97.57941517934582
At time: 135.25106978416443 and batch: 1000, loss is 4.563860311508178 and perplexity is 95.95317494730406
At time: 136.1160490512848 and batch: 1050, loss is 4.545088205337525 and perplexity is 94.16873302797366
At time: 136.9813904762268 and batch: 1100, loss is 4.505094442367554 and perplexity is 90.47688849694238
At time: 137.85027885437012 and batch: 1150, loss is 4.535549478530884 and perplexity is 93.27475370056729
At time: 138.7703151702881 and batch: 1200, loss is 4.551283893585205 and perplexity is 94.75398428551615
At time: 139.6371555328369 and batch: 1250, loss is 4.551883144378662 and perplexity is 94.81078270233095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.642415958599453 and perplexity of 103.7948088704744
Finished 6 epochs...
Completing Train Step...
At time: 142.0907723903656 and batch: 50, loss is 4.495349426269531 and perplexity is 89.59947192501885
At time: 142.98955416679382 and batch: 100, loss is 4.5235362720489505 and perplexity is 92.16092852601969
At time: 143.85878586769104 and batch: 150, loss is 4.4255470943450925 and perplexity is 83.5585091070967
At time: 144.72778749465942 and batch: 200, loss is 4.49160758972168 and perplexity is 89.26483182103813
At time: 145.5998797416687 and batch: 250, loss is 4.4891203498840335 and perplexity is 89.04308465873737
At time: 146.46981048583984 and batch: 300, loss is 4.5079234600067135 and perplexity is 90.73321161078829
At time: 147.33907270431519 and batch: 350, loss is 4.494019975662232 and perplexity is 89.48043299837394
At time: 148.2080056667328 and batch: 400, loss is 4.503946075439453 and perplexity is 90.37304746564973
At time: 149.07645416259766 and batch: 450, loss is 4.431652479171753 and perplexity is 84.07022648694875
At time: 149.94845461845398 and batch: 500, loss is 4.462917165756226 and perplexity is 86.740175852431
At time: 150.8176510334015 and batch: 550, loss is 4.47007399559021 and perplexity is 87.36318726487926
At time: 151.68584299087524 and batch: 600, loss is 4.4842473411560055 and perplexity is 88.6102324329118
At time: 152.5547091960907 and batch: 650, loss is 4.490442981719971 and perplexity is 89.16093379560722
At time: 153.4256453514099 and batch: 700, loss is 4.4869346427917485 and perplexity is 88.84867509565656
At time: 154.2946252822876 and batch: 750, loss is 4.477613372802734 and perplexity is 88.024340498187
At time: 155.16325402259827 and batch: 800, loss is 4.505032634735107 and perplexity is 90.47129650748883
At time: 156.03108859062195 and batch: 850, loss is 4.543850154876709 and perplexity is 94.05221952433472
At time: 156.90284657478333 and batch: 900, loss is 4.506704578399658 and perplexity is 90.62268594054386
At time: 157.77155137062073 and batch: 950, loss is 4.508318605422974 and perplexity is 90.76907150792928
At time: 158.64064049720764 and batch: 1000, loss is 4.494350242614746 and perplexity is 89.50999030892274
At time: 159.50950813293457 and batch: 1050, loss is 4.47312406539917 and perplexity is 87.63005786486062
At time: 160.43314123153687 and batch: 1100, loss is 4.4325053501129155 and perplexity is 84.1419581247031
At time: 161.30093502998352 and batch: 1150, loss is 4.462686309814453 and perplexity is 86.72015367865286
At time: 162.1684765815735 and batch: 1200, loss is 4.481010704040528 and perplexity is 88.32389689788496
At time: 163.03666257858276 and batch: 1250, loss is 4.482969570159912 and perplexity is 88.49708115405822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6278321203524175 and perplexity of 102.29206667612074
Finished 7 epochs...
Completing Train Step...
At time: 165.4966790676117 and batch: 50, loss is 4.424932079315186 and perplexity is 83.50713516761101
At time: 166.4175922870636 and batch: 100, loss is 4.4553006076812744 and perplexity is 86.08202387372407
At time: 167.28344798088074 and batch: 150, loss is 4.359341802597046 and perplexity is 78.20564272815326
At time: 168.15101385116577 and batch: 200, loss is 4.42485857963562 and perplexity is 83.50099764549046
At time: 169.01736092567444 and batch: 250, loss is 4.420689086914063 and perplexity is 83.15356565447611
At time: 169.88469696044922 and batch: 300, loss is 4.44113709449768 and perplexity is 84.87139362064292
At time: 170.75228238105774 and batch: 350, loss is 4.426680746078492 and perplexity is 83.65328906936979
At time: 171.61917734146118 and batch: 400, loss is 4.436298723220825 and perplexity is 84.46174612002058
At time: 172.4854736328125 and batch: 450, loss is 4.362702465057373 and perplexity is 78.46890762029831
At time: 173.35196805000305 and batch: 500, loss is 4.399768209457397 and perplexity is 81.43199131180528
At time: 174.21864533424377 and batch: 550, loss is 4.402858877182007 and perplexity is 81.68405986852842
At time: 175.08684086799622 and batch: 600, loss is 4.420621433258057 and perplexity is 83.14794020204299
At time: 175.95243406295776 and batch: 650, loss is 4.426834144592285 and perplexity is 83.66612234386443
At time: 176.81918144226074 and batch: 700, loss is 4.422696113586426 and perplexity is 83.32062466851693
At time: 177.69515299797058 and batch: 750, loss is 4.414111118316651 and perplexity is 82.60837918640664
At time: 178.57337188720703 and batch: 800, loss is 4.442867069244385 and perplexity is 85.01834606368189
At time: 179.44304060935974 and batch: 850, loss is 4.482573251724244 and perplexity is 88.46201507841914
At time: 180.3214671611786 and batch: 900, loss is 4.441334943771363 and perplexity is 84.88818702545387
At time: 181.2042601108551 and batch: 950, loss is 4.4465933036804195 and perplexity is 85.33573531862372
At time: 182.0707676410675 and batch: 1000, loss is 4.43200756072998 and perplexity is 84.10008357450718
At time: 182.9692268371582 and batch: 1050, loss is 4.4120914745330815 and perplexity is 82.44170805172635
At time: 183.8326976299286 and batch: 1100, loss is 4.369424962997437 and perplexity is 78.9981917525978
At time: 184.69758367538452 and batch: 1150, loss is 4.401391153335571 and perplexity is 81.56425816540047
At time: 185.56503415107727 and batch: 1200, loss is 4.418772506713867 and perplexity is 82.9943478025959
At time: 186.42961812019348 and batch: 1250, loss is 4.423465251922607 and perplexity is 83.38473440661225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.620718183308623 and perplexity of 101.56694963027766
Finished 8 epochs...
Completing Train Step...
At time: 188.91580986976624 and batch: 50, loss is 4.364362545013428 and perplexity is 78.59928046574527
At time: 189.7812123298645 and batch: 100, loss is 4.392626094818115 and perplexity is 80.85246667370312
At time: 190.64819240570068 and batch: 150, loss is 4.302377634048462 and perplexity is 73.87523332142094
At time: 191.5163927078247 and batch: 200, loss is 4.366486282348633 and perplexity is 78.76638206934177
At time: 192.38426446914673 and batch: 250, loss is 4.362671499252319 and perplexity is 78.46647780502298
At time: 193.25059533119202 and batch: 300, loss is 4.379974822998047 and perplexity is 79.83602334806528
At time: 194.11920404434204 and batch: 350, loss is 4.363238573074341 and perplexity is 78.51098670921691
At time: 194.9903450012207 and batch: 400, loss is 4.377111015319824 and perplexity is 79.60771540246301
At time: 195.8557641506195 and batch: 450, loss is 4.3010019683837895 and perplexity is 73.77367557024277
At time: 196.72364258766174 and batch: 500, loss is 4.342954683303833 and perplexity is 76.93452098837041
At time: 197.59007143974304 and batch: 550, loss is 4.3455788516998295 and perplexity is 77.13667525431993
At time: 198.4583215713501 and batch: 600, loss is 4.364530162811279 and perplexity is 78.61245620826325
At time: 199.32439160346985 and batch: 650, loss is 4.369855842590332 and perplexity is 79.03223779564546
At time: 200.19032168388367 and batch: 700, loss is 4.365453939437867 and perplexity is 78.68511011068883
At time: 201.05691409111023 and batch: 750, loss is 4.359590368270874 and perplexity is 78.22508438259901
At time: 201.92336201667786 and batch: 800, loss is 4.389865055084228 and perplexity is 80.62953770013313
At time: 202.789963722229 and batch: 850, loss is 4.427857961654663 and perplexity is 83.75182501190385
At time: 203.65699362754822 and batch: 900, loss is 4.385224943161011 and perplexity is 80.25627428262068
At time: 204.52550554275513 and batch: 950, loss is 4.391282901763916 and perplexity is 80.74393910510716
At time: 205.4468879699707 and batch: 1000, loss is 4.375087003707886 and perplexity is 79.44675141353068
At time: 206.3125615119934 and batch: 1050, loss is 4.3577817440032955 and perplexity is 78.0837324614403
At time: 207.17984175682068 and batch: 1100, loss is 4.3156154918670655 and perplexity is 74.85968479445759
At time: 208.04803490638733 and batch: 1150, loss is 4.346626291275024 and perplexity is 77.21751358992535
At time: 208.91645646095276 and batch: 1200, loss is 4.365009069442749 and perplexity is 78.65011325124159
At time: 209.78484630584717 and batch: 1250, loss is 4.37039155960083 and perplexity is 79.07458805267402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6183917191776915 and perplexity of 101.33093241432067
Finished 9 epochs...
Completing Train Step...
At time: 212.2376651763916 and batch: 50, loss is 4.310645608901978 and perplexity is 74.48856389981397
At time: 213.13240933418274 and batch: 100, loss is 4.3356281280517575 and perplexity is 76.37291579745835
At time: 213.9981529712677 and batch: 150, loss is 4.249354147911072 and perplexity is 70.06014923788865
At time: 214.86455821990967 and batch: 200, loss is 4.314112100601196 and perplexity is 74.7472259541725
At time: 215.73024988174438 and batch: 250, loss is 4.311648273468018 and perplexity is 74.56328839895501
At time: 216.59569358825684 and batch: 300, loss is 4.3276315116882325 and perplexity is 75.76462625976141
At time: 217.46054077148438 and batch: 350, loss is 4.309902038574219 and perplexity is 74.4331970010832
At time: 218.3265106678009 and batch: 400, loss is 4.323889999389649 and perplexity is 75.48168162929665
At time: 219.1896595954895 and batch: 450, loss is 4.247677268981934 and perplexity is 69.94276529669438
At time: 220.05845856666565 and batch: 500, loss is 4.291774682998657 and perplexity is 73.09607582269099
At time: 220.92250180244446 and batch: 550, loss is 4.29358325958252 and perplexity is 73.22839529267615
At time: 221.7884819507599 and batch: 600, loss is 4.3148813152313235 and perplexity is 74.80474473324493
At time: 222.65430283546448 and batch: 650, loss is 4.319028673171997 and perplexity is 75.11563101804683
At time: 223.51974940299988 and batch: 700, loss is 4.313236351013184 and perplexity is 74.68179475669196
At time: 224.38510155677795 and batch: 750, loss is 4.309162702560425 and perplexity is 74.37818619615892
At time: 225.24809384346008 and batch: 800, loss is 4.3402743434906 and perplexity is 76.72858643932861
At time: 226.1133165359497 and batch: 850, loss is 4.380058174133301 and perplexity is 79.84267804858003
At time: 226.97861576080322 and batch: 900, loss is 4.335519723892212 and perplexity is 76.36463710443974
At time: 227.87402200698853 and batch: 950, loss is 4.341049089431762 and perplexity is 76.7880546336401
At time: 228.7417049407959 and batch: 1000, loss is 4.326020584106446 and perplexity is 75.64267318875191
At time: 229.60702896118164 and batch: 1050, loss is 4.310027322769165 and perplexity is 74.44252288842772
At time: 230.47153973579407 and batch: 1100, loss is 4.264559135437012 and perplexity is 71.13355279689716
At time: 231.34067797660828 and batch: 1150, loss is 4.297397871017456 and perplexity is 73.50826662754544
At time: 232.20482397079468 and batch: 1200, loss is 4.3164972305297855 and perplexity is 74.92572058169489
At time: 233.06947255134583 and batch: 1250, loss is 4.321229629516601 and perplexity is 75.28113931416914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.618144459968065 and perplexity of 101.30588050534647
Finished 10 epochs...
Completing Train Step...
At time: 235.53345823287964 and batch: 50, loss is 4.261091556549072 and perplexity is 70.88731875570443
At time: 236.39628863334656 and batch: 100, loss is 4.284809789657593 and perplexity is 72.58873827777906
At time: 237.2589831352234 and batch: 150, loss is 4.2018220281600955 and perplexity is 66.80794617342853
At time: 238.12302613258362 and batch: 200, loss is 4.26700945854187 and perplexity is 71.3080667052606
At time: 238.9861319065094 and batch: 250, loss is 4.261784806251526 and perplexity is 70.9364784063273
At time: 239.85077214241028 and batch: 300, loss is 4.2805084705352785 and perplexity is 72.27718148271187
At time: 240.71401810646057 and batch: 350, loss is 4.260690717697144 and perplexity is 70.85891005827331
At time: 241.5794916152954 and batch: 400, loss is 4.276568174362183 and perplexity is 71.99294833026347
At time: 242.44377779960632 and batch: 450, loss is 4.19909517288208 and perplexity is 66.62601873042664
At time: 243.31066751480103 and batch: 500, loss is 4.244403676986694 and perplexity is 69.71417557896756
At time: 244.1755828857422 and batch: 550, loss is 4.24725640296936 and perplexity is 69.91333495750474
At time: 245.04248523712158 and batch: 600, loss is 4.269179310798645 and perplexity is 71.46296266462657
At time: 245.9058861732483 and batch: 650, loss is 4.2736559581756595 and perplexity is 71.78359429089265
At time: 246.7685923576355 and batch: 700, loss is 4.268805599212646 and perplexity is 71.4362611171586
At time: 247.63235425949097 and batch: 750, loss is 4.2633252573013305 and perplexity is 71.04583678794916
At time: 248.49703454971313 and batch: 800, loss is 4.295139985084534 and perplexity is 73.34248057979829
At time: 249.4155650138855 and batch: 850, loss is 4.335055780410767 and perplexity is 76.32921644606935
At time: 250.28074097633362 and batch: 900, loss is 4.289938125610352 and perplexity is 72.96195388360059
At time: 251.14764022827148 and batch: 950, loss is 4.295579996109009 and perplexity is 73.37475918076508
At time: 252.0123884677887 and batch: 1000, loss is 4.281522026062012 and perplexity is 72.35047555699286
At time: 252.88291001319885 and batch: 1050, loss is 4.265421252250672 and perplexity is 71.19490467121996
At time: 253.76385235786438 and batch: 1100, loss is 4.21976450920105 and perplexity is 68.01746491613054
At time: 254.64139342308044 and batch: 1150, loss is 4.253102035522461 and perplexity is 70.32321947417097
At time: 255.53232431411743 and batch: 1200, loss is 4.271380891799927 and perplexity is 71.62046748171252
At time: 256.4231014251709 and batch: 1250, loss is 4.2775938510894775 and perplexity is 72.06682770359527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.619508172473768 and perplexity of 101.44412684419856
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 258.9677560329437 and batch: 50, loss is 4.254894533157349 and perplexity is 70.4493867222212
At time: 259.8854835033417 and batch: 100, loss is 4.296781673431396 and perplexity is 73.46298496375287
At time: 260.7548134326935 and batch: 150, loss is 4.215578818321228 and perplexity is 67.73335983637661
At time: 261.6475636959076 and batch: 200, loss is 4.2881701374053955 and perplexity is 72.83307197414311
At time: 262.5420928001404 and batch: 250, loss is 4.284609966278076 and perplexity is 72.57423479989677
At time: 263.441769361496 and batch: 300, loss is 4.290588541030884 and perplexity is 73.0094248997856
At time: 264.3120141029358 and batch: 350, loss is 4.2721779155731205 and perplexity is 71.67757345132733
At time: 265.1805913448334 and batch: 400, loss is 4.277426347732544 and perplexity is 72.05475727897793
At time: 266.04785799980164 and batch: 450, loss is 4.196007342338562 and perplexity is 66.42060617758733
At time: 266.91789746284485 and batch: 500, loss is 4.219443206787109 and perplexity is 67.99561425098656
At time: 267.7879948616028 and batch: 550, loss is 4.215204362869263 and perplexity is 67.70800145859363
At time: 268.6572005748749 and batch: 600, loss is 4.231330819129944 and perplexity is 68.80874325982602
At time: 269.52872920036316 and batch: 650, loss is 4.234045734405518 and perplexity is 68.99580698417631
At time: 270.3984525203705 and batch: 700, loss is 4.223066644668579 and perplexity is 68.24243904253426
At time: 271.2680969238281 and batch: 750, loss is 4.2080918884277345 and perplexity is 67.2281385579526
At time: 272.1678349971771 and batch: 800, loss is 4.221980457305908 and perplexity is 68.16835520939377
At time: 273.03823041915894 and batch: 850, loss is 4.244529275894165 and perplexity is 69.72293215315207
At time: 273.9069449901581 and batch: 900, loss is 4.194146952629089 and perplexity is 66.2971528366267
At time: 274.7736625671387 and batch: 950, loss is 4.182627964019775 and perplexity is 65.53785823689225
At time: 275.64377188682556 and batch: 1000, loss is 4.152887921333313 and perplexity is 63.617457478289765
At time: 276.51410007476807 and batch: 1050, loss is 4.132180075645447 and perplexity is 62.313623375102274
At time: 277.38279962539673 and batch: 1100, loss is 4.067802519798279 and perplexity is 58.42842610583975
At time: 278.2516314983368 and batch: 1150, loss is 4.085531206130981 and perplexity is 59.47352206116157
At time: 279.1204483509064 and batch: 1200, loss is 4.094296445846558 and perplexity is 59.997113086921885
At time: 279.98920702934265 and batch: 1250, loss is 4.085871067047119 and perplexity is 59.4937382220021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.543060971002509 and perplexity of 93.97802431003757
Finished 12 epochs...
Completing Train Step...
At time: 282.42741346359253 and batch: 50, loss is 4.207470254898071 and perplexity is 67.18636027962282
At time: 283.32399797439575 and batch: 100, loss is 4.239157552719116 and perplexity is 69.34940400576743
At time: 284.1891028881073 and batch: 150, loss is 4.153977103233338 and perplexity is 63.68678621044697
At time: 285.0558190345764 and batch: 200, loss is 4.223746194839477 and perplexity is 68.28882896400245
At time: 285.9231402873993 and batch: 250, loss is 4.2225191688537596 and perplexity is 68.20508818289325
At time: 286.7902657985687 and batch: 300, loss is 4.231607284545898 and perplexity is 68.82776912752867
At time: 287.6566846370697 and batch: 350, loss is 4.2139039230346675 and perplexity is 67.62000850354875
At time: 288.5214841365814 and batch: 400, loss is 4.222909193038941 and perplexity is 68.2316950051518
At time: 289.3872449398041 and batch: 450, loss is 4.14556158542633 and perplexity is 63.1530777931738
At time: 290.2535903453827 and batch: 500, loss is 4.172283701896667 and perplexity is 64.86341179172581
At time: 291.1208498477936 and batch: 550, loss is 4.169088191986084 and perplexity is 64.65647093338728
At time: 291.9869728088379 and batch: 600, loss is 4.187988858222962 and perplexity is 65.8901431987103
At time: 292.8552689552307 and batch: 650, loss is 4.193383779525757 and perplexity is 66.24657593467467
At time: 293.72198271751404 and batch: 700, loss is 4.183846411705017 and perplexity is 65.61776135755744
At time: 294.6177513599396 and batch: 750, loss is 4.1727287340164185 and perplexity is 64.89228451753844
At time: 295.48658061027527 and batch: 800, loss is 4.192400569915772 and perplexity is 66.18147367441559
At time: 296.3552830219269 and batch: 850, loss is 4.219884042739868 and perplexity is 68.0255957703586
At time: 297.2221374511719 and batch: 900, loss is 4.172510490417481 and perplexity is 64.87812373712809
At time: 298.08955121040344 and batch: 950, loss is 4.165937004089355 and perplexity is 64.45304692670373
At time: 298.95686650276184 and batch: 1000, loss is 4.14103440284729 and perplexity is 62.86781847728653
At time: 299.8208854198456 and batch: 1050, loss is 4.125817089080811 and perplexity is 61.91838141993018
At time: 300.68664264678955 and batch: 1100, loss is 4.066798405647278 and perplexity is 58.3697867416008
At time: 301.5531644821167 and batch: 1150, loss is 4.0915377950668335 and perplexity is 59.83183008797947
At time: 302.41882371902466 and batch: 1200, loss is 4.10727165222168 and perplexity is 60.78066036206108
At time: 303.2847535610199 and batch: 1250, loss is 4.103082966804505 and perplexity is 60.52660175360694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.539646566349225 and perplexity of 93.65769248896369
Finished 13 epochs...
Completing Train Step...
At time: 305.8160116672516 and batch: 50, loss is 4.187685565948486 and perplexity is 65.87016225749977
At time: 306.69418263435364 and batch: 100, loss is 4.216864862442017 and perplexity is 67.82052396201762
At time: 307.5785217285156 and batch: 150, loss is 4.13007836818695 and perplexity is 62.18279589678965
At time: 308.4627637863159 and batch: 200, loss is 4.198058118820191 and perplexity is 66.55695976220578
At time: 309.34846568107605 and batch: 250, loss is 4.19724217414856 and perplexity is 66.50267511517384
At time: 310.22825145721436 and batch: 300, loss is 4.208241806030274 and perplexity is 67.23821799483203
At time: 311.09991931915283 and batch: 350, loss is 4.190047073364258 and perplexity is 66.02589894844925
At time: 311.9729537963867 and batch: 400, loss is 4.199857139587403 and perplexity is 66.67680488462955
At time: 312.84067702293396 and batch: 450, loss is 4.1236346292495725 and perplexity is 61.78339439512564
At time: 313.7043857574463 and batch: 500, loss is 4.151883788108826 and perplexity is 63.553609137063546
At time: 314.5700454711914 and batch: 550, loss is 4.148805184364319 and perplexity is 63.3582536234002
At time: 315.43335604667664 and batch: 600, loss is 4.168919639587402 and perplexity is 64.64557384851186
At time: 316.30222821235657 and batch: 650, loss is 4.17538724899292 and perplexity is 65.06503115080011
At time: 317.2220423221588 and batch: 700, loss is 4.166102814674377 and perplexity is 64.46373481017874
At time: 318.0881769657135 and batch: 750, loss is 4.156979336738586 and perplexity is 63.878276118727506
At time: 318.9587948322296 and batch: 800, loss is 4.17870325088501 and perplexity is 65.28114503620154
At time: 319.83122730255127 and batch: 850, loss is 4.20821907043457 and perplexity is 67.23668931126967
At time: 320.69958233833313 and batch: 900, loss is 4.162183036804199 and perplexity is 64.21154587424553
At time: 321.56813859939575 and batch: 950, loss is 4.157743430137634 and perplexity is 63.927103739908276
At time: 322.43635749816895 and batch: 1000, loss is 4.135216670036316 and perplexity is 62.503132159353264
At time: 323.3058753013611 and batch: 1050, loss is 4.122387070655822 and perplexity is 61.70636405042205
At time: 324.175488948822 and batch: 1100, loss is 4.0655176639556885 and perplexity is 58.29507797372059
At time: 325.0436704158783 and batch: 1150, loss is 4.09295702457428 and perplexity is 59.91680547224801
At time: 325.9121062755585 and batch: 1200, loss is 4.11169919013977 and perplexity is 61.0503656651407
At time: 326.7822048664093 and batch: 1250, loss is 4.108749995231628 and perplexity is 60.870581477196495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.538668221800867 and perplexity of 93.566107804099
Finished 14 epochs...
Completing Train Step...
At time: 329.2646656036377 and batch: 50, loss is 4.172446994781494 and perplexity is 64.87400439018147
At time: 330.16541385650635 and batch: 100, loss is 4.200618681907653 and perplexity is 66.7276014327339
At time: 331.03008794784546 and batch: 150, loss is 4.1129332447052 and perplexity is 61.12575165325527
At time: 331.89651370048523 and batch: 200, loss is 4.180658016204834 and perplexity is 65.4088791589996
At time: 332.76564049720764 and batch: 250, loss is 4.180073204040528 and perplexity is 65.3706384337243
At time: 333.6321613788605 and batch: 300, loss is 4.192910766601562 and perplexity is 66.21524785795063
At time: 334.4977767467499 and batch: 350, loss is 4.173369698524475 and perplexity is 64.93389150163645
At time: 335.36374402046204 and batch: 400, loss is 4.183994407653809 and perplexity is 65.62747323904898
At time: 336.2317011356354 and batch: 450, loss is 4.108819885253906 and perplexity is 60.874835872160155
At time: 337.0971591472626 and batch: 500, loss is 4.138382000923157 and perplexity is 62.701288704167965
At time: 337.96240854263306 and batch: 550, loss is 4.135186476707458 and perplexity is 62.50124501021915
At time: 338.8573007583618 and batch: 600, loss is 4.156022019386292 and perplexity is 63.81715359805724
At time: 339.726443529129 and batch: 650, loss is 4.1632419872283934 and perplexity is 64.27957873334218
At time: 340.592853307724 and batch: 700, loss is 4.154194946289063 and perplexity is 63.70066144582132
At time: 341.45707964897156 and batch: 750, loss is 4.146065073013306 and perplexity is 63.184882589910266
At time: 342.32195019721985 and batch: 800, loss is 4.169087572097778 and perplexity is 64.65643085360946
At time: 343.18957328796387 and batch: 850, loss is 4.199922933578491 and perplexity is 66.681191962056
At time: 344.0562090873718 and batch: 900, loss is 4.154570817947388 and perplexity is 63.72460921943811
At time: 344.92346143722534 and batch: 950, loss is 4.15151581287384 and perplexity is 63.530227285042216
At time: 345.78914308547974 and batch: 1000, loss is 4.130054063796997 and perplexity is 62.18128460023544
At time: 346.6564927101135 and batch: 1050, loss is 4.118948798179627 and perplexity is 61.494565077243635
At time: 347.5222415924072 and batch: 1100, loss is 4.063085899353028 and perplexity is 58.15349029032604
At time: 348.39024233818054 and batch: 1150, loss is 4.091983394622803 and perplexity is 59.858497065855005
At time: 349.2551028728485 and batch: 1200, loss is 4.112387528419495 and perplexity is 61.092403435262554
At time: 350.1208140850067 and batch: 1250, loss is 4.109957919120789 and perplexity is 60.94415293212333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.538415170934078 and perplexity of 93.54243381490518
Finished 15 epochs...
Completing Train Step...
At time: 352.6288893222809 and batch: 50, loss is 4.159676647186279 and perplexity is 64.05080824202798
At time: 353.50880789756775 and batch: 100, loss is 4.187339606285096 and perplexity is 65.84737777982004
At time: 354.3843204975128 and batch: 150, loss is 4.099338459968567 and perplexity is 60.30038328221436
At time: 355.2784125804901 and batch: 200, loss is 4.1672414112091065 and perplexity is 64.53717479657234
At time: 356.16450214385986 and batch: 250, loss is 4.166518959999085 and perplexity is 64.49056667462342
At time: 357.0627727508545 and batch: 300, loss is 4.180218558311463 and perplexity is 65.38014102581883
At time: 357.95342230796814 and batch: 350, loss is 4.16046863079071 and perplexity is 64.10155552486168
At time: 358.8482871055603 and batch: 400, loss is 4.171591320037842 and perplexity is 64.81851708603311
At time: 359.73407077789307 and batch: 450, loss is 4.097121834754944 and perplexity is 60.16686796361883
At time: 360.59933829307556 and batch: 500, loss is 4.127251658439636 and perplexity is 62.00727137682296
At time: 361.49569869041443 and batch: 550, loss is 4.124407892227173 and perplexity is 61.83118768265033
At time: 362.36111092567444 and batch: 600, loss is 4.145823702812195 and perplexity is 63.169633482506484
At time: 363.22853684425354 and batch: 650, loss is 4.153599209785462 and perplexity is 63.662723937995665
At time: 364.09603238105774 and batch: 700, loss is 4.144638957977295 and perplexity is 63.09483790110365
At time: 364.96192145347595 and batch: 750, loss is 4.1371977376937865 and perplexity is 62.62707782480259
At time: 365.82651829719543 and batch: 800, loss is 4.161061015129089 and perplexity is 64.139539531875
At time: 366.69477677345276 and batch: 850, loss is 4.192817158699036 and perplexity is 66.2090498775779
At time: 367.56260919570923 and batch: 900, loss is 4.147894115447998 and perplexity is 63.30055617509836
At time: 368.4293053150177 and batch: 950, loss is 4.1457079935073855 and perplexity is 63.162324590992675
At time: 369.2971041202545 and batch: 1000, loss is 4.124985299110413 and perplexity is 61.866899745220366
At time: 370.1647720336914 and batch: 1050, loss is 4.115100336074829 and perplexity is 61.25836037783907
At time: 371.0329291820526 and batch: 1100, loss is 4.059907999038696 and perplexity is 57.96897763195398
At time: 371.90006852149963 and batch: 1150, loss is 4.089638586044312 and perplexity is 59.71830477471577
At time: 372.7652587890625 and batch: 1200, loss is 4.111031551361084 and perplexity is 61.00961967688408
At time: 373.63236951828003 and batch: 1250, loss is 4.108852806091309 and perplexity is 60.8768399557217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.538508728472856 and perplexity of 93.551185824186
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 376.1051290035248 and batch: 50, loss is 4.159635329246521 and perplexity is 64.04816184926375
At time: 377.0263867378235 and batch: 100, loss is 4.198557081222535 and perplexity is 66.59017746924573
At time: 377.89181876182556 and batch: 150, loss is 4.114791069030762 and perplexity is 61.23941811506039
At time: 378.7566497325897 and batch: 200, loss is 4.186363668441772 and perplexity is 65.7831461800837
At time: 379.62378215789795 and batch: 250, loss is 4.188588581085205 and perplexity is 65.92967087562408
At time: 380.48754835128784 and batch: 300, loss is 4.203315010070801 and perplexity is 66.90776372291366
At time: 381.3523736000061 and batch: 350, loss is 4.183184661865234 and perplexity is 65.57435317875397
At time: 382.21823287010193 and batch: 400, loss is 4.192934141159058 and perplexity is 66.21679562815794
At time: 383.0825171470642 and batch: 450, loss is 4.121886110305786 and perplexity is 61.67545935034888
At time: 383.9761474132538 and batch: 500, loss is 4.146357583999634 and perplexity is 63.20336756563509
At time: 384.8420569896698 and batch: 550, loss is 4.141545829772949 and perplexity is 62.899978995589755
At time: 385.7081546783447 and batch: 600, loss is 4.160879364013672 and perplexity is 64.1278895711229
At time: 386.5741925239563 and batch: 650, loss is 4.169148373603821 and perplexity is 64.6603621814948
At time: 387.4404332637787 and batch: 700, loss is 4.159101338386535 and perplexity is 64.01396984615839
At time: 388.306054353714 and batch: 750, loss is 4.145750093460083 and perplexity is 63.164983777845634
At time: 389.1841323375702 and batch: 800, loss is 4.161856598854065 and perplexity is 64.19058820971003
At time: 390.0486557483673 and batch: 850, loss is 4.185652847290039 and perplexity is 65.73640274343126
At time: 390.9143030643463 and batch: 900, loss is 4.137265467643738 and perplexity is 62.631319697298565
At time: 391.78033900260925 and batch: 950, loss is 4.129877309799195 and perplexity is 62.17029478086972
At time: 392.6466851234436 and batch: 1000, loss is 4.107556838989257 and perplexity is 60.79799667404683
At time: 393.51441502571106 and batch: 1050, loss is 4.087158608436584 and perplexity is 59.57038820680195
At time: 394.3849587440491 and batch: 1100, loss is 4.0251343011856076 and perplexity is 55.98782750835984
At time: 395.2560622692108 and batch: 1150, loss is 4.050204386711121 and perplexity is 57.40918952185871
At time: 396.1226577758789 and batch: 1200, loss is 4.074304938316345 and perplexity is 58.80959008658249
At time: 396.98868584632874 and batch: 1250, loss is 4.077971577644348 and perplexity is 59.0256194513108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.530042216725593 and perplexity of 92.76247712749206
Finished 17 epochs...
Completing Train Step...
At time: 399.4539840221405 and batch: 50, loss is 4.156503095626831 and perplexity is 63.84786190031715
At time: 400.35148882865906 and batch: 100, loss is 4.189444346427917 and perplexity is 65.98611535118418
At time: 401.2187647819519 and batch: 150, loss is 4.103180527687073 and perplexity is 60.532507070352146
At time: 402.0879530906677 and batch: 200, loss is 4.1723755264282225 and perplexity is 64.8693681175929
At time: 402.9554569721222 and batch: 250, loss is 4.172309737205506 and perplexity is 64.86510055266776
At time: 403.82460284233093 and batch: 300, loss is 4.186707038879394 and perplexity is 65.80573804624322
At time: 404.69166898727417 and batch: 350, loss is 4.166802773475647 and perplexity is 64.508872564163
At time: 405.5879981517792 and batch: 400, loss is 4.177010536193848 and perplexity is 65.17073615467348
At time: 406.4524645805359 and batch: 450, loss is 4.1062421798706055 and perplexity is 60.71812054976545
At time: 407.31731700897217 and batch: 500, loss is 4.13170702457428 and perplexity is 62.2841528199048
At time: 408.18414878845215 and batch: 550, loss is 4.127425165176391 and perplexity is 62.01803098954029
At time: 409.0481312274933 and batch: 600, loss is 4.147754273414612 and perplexity is 63.29170471552584
At time: 409.9137966632843 and batch: 650, loss is 4.156220283508301 and perplexity is 63.829807504350825
At time: 410.77912974357605 and batch: 700, loss is 4.147211575508118 and perplexity is 63.25736575856113
At time: 411.6446373462677 and batch: 750, loss is 4.136004176139831 and perplexity is 62.55237314366063
At time: 412.50814509391785 and batch: 800, loss is 4.154392809867859 and perplexity is 63.713266733689224
At time: 413.37329483032227 and batch: 850, loss is 4.179897646903992 and perplexity is 65.35916315894004
At time: 414.2369740009308 and batch: 900, loss is 4.133532114028931 and perplexity is 62.39793076629093
At time: 415.1011199951172 and batch: 950, loss is 4.127950320243835 and perplexity is 62.05060862619412
At time: 415.9649727344513 and batch: 1000, loss is 4.1073358583450315 and perplexity is 60.784562977922235
At time: 416.830406665802 and batch: 1050, loss is 4.089617052078247 and perplexity is 59.717018816613255
At time: 417.697448015213 and batch: 1100, loss is 4.030032868385315 and perplexity is 56.26276048282741
At time: 418.56359672546387 and batch: 1150, loss is 4.0571165990829465 and perplexity is 57.80738866499883
At time: 419.4329643249512 and batch: 1200, loss is 4.082140254974365 and perplexity is 59.27219179620478
At time: 420.29974389076233 and batch: 1250, loss is 4.086379146575927 and perplexity is 59.52397345278547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528977442831889 and perplexity of 92.66375862929802
Finished 18 epochs...
Completing Train Step...
At time: 422.81295824050903 and batch: 50, loss is 4.155721135139466 and perplexity is 63.79795491029849
At time: 423.68289709091187 and batch: 100, loss is 4.185959525108338 and perplexity is 65.75656573161999
At time: 424.55225443840027 and batch: 150, loss is 4.098923988342285 and perplexity is 60.2753956629773
At time: 425.42322301864624 and batch: 200, loss is 4.167016396522522 and perplexity is 64.52265461810039
At time: 426.2920846939087 and batch: 250, loss is 4.16597469329834 and perplexity is 64.45547615683667
At time: 427.1617558002472 and batch: 300, loss is 4.180126914978027 and perplexity is 65.37414964629393
At time: 428.06037735939026 and batch: 350, loss is 4.16016414642334 and perplexity is 64.0820405744297
At time: 428.93149065971375 and batch: 400, loss is 4.170519618988037 and perplexity is 64.74908822336292
At time: 429.81464862823486 and batch: 450, loss is 4.099831519126892 and perplexity is 60.33012226937967
At time: 430.7011227607727 and batch: 500, loss is 4.125746021270752 and perplexity is 61.91398117252008
At time: 431.5881881713867 and batch: 550, loss is 4.121608037948608 and perplexity is 61.65831149427018
At time: 432.47633814811707 and batch: 600, loss is 4.142441329956054 and perplexity is 62.95633116622136
At time: 433.3695831298828 and batch: 650, loss is 4.151052007675171 and perplexity is 63.500768467459224
At time: 434.24139881134033 and batch: 700, loss is 4.142320594787598 and perplexity is 62.94873058181075
At time: 435.1151371002197 and batch: 750, loss is 4.132126221656799 and perplexity is 62.310267628297396
At time: 435.9886703491211 and batch: 800, loss is 4.151530003547668 and perplexity is 63.53112882817258
At time: 436.8574924468994 and batch: 850, loss is 4.177909345626831 and perplexity is 65.2293385593356
At time: 437.72730469703674 and batch: 900, loss is 4.132405042648315 and perplexity is 62.32764346116007
At time: 438.598206281662 and batch: 950, loss is 4.127750372886657 and perplexity is 62.03820301126432
At time: 439.467928647995 and batch: 1000, loss is 4.107986254692078 and perplexity is 60.82410989483093
At time: 440.3396725654602 and batch: 1050, loss is 4.091399488449096 and perplexity is 59.82355552215377
At time: 441.2109045982361 and batch: 1100, loss is 4.032739319801331 and perplexity is 56.41523915559103
At time: 442.08285188674927 and batch: 1150, loss is 4.060666913986206 and perplexity is 58.01298785346591
At time: 442.9559168815613 and batch: 1200, loss is 4.086073608398437 and perplexity is 59.50578938452473
At time: 443.8277213573456 and batch: 1250, loss is 4.090194487571717 and perplexity is 59.751511500533724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5286780587078015 and perplexity of 92.63602072343765
Finished 19 epochs...
Completing Train Step...
At time: 446.3219804763794 and batch: 50, loss is 4.154038567543029 and perplexity is 63.69070079510026
At time: 447.21849966049194 and batch: 100, loss is 4.183095469474792 and perplexity is 65.56850470626576
At time: 448.08869767189026 and batch: 150, loss is 4.095627636909485 and perplexity is 60.07703389079243
At time: 448.95828080177307 and batch: 200, loss is 4.163529758453369 and perplexity is 64.29807920827969
At time: 449.827605009079 and batch: 250, loss is 4.16184175491333 and perplexity is 64.1896353754948
At time: 450.7518482208252 and batch: 300, loss is 4.175950021743774 and perplexity is 65.10165828277445
At time: 451.6197941303253 and batch: 350, loss is 4.155924372673034 and perplexity is 63.810922366993516
At time: 452.4871187210083 and batch: 400, loss is 4.1662917947769165 and perplexity is 64.47591832457786
At time: 453.3561725616455 and batch: 450, loss is 4.095757970809936 and perplexity is 60.0848644752312
At time: 454.2217662334442 and batch: 500, loss is 4.122027530670166 and perplexity is 61.68418213305852
At time: 455.08991503715515 and batch: 550, loss is 4.118064956665039 and perplexity is 61.44023763966582
At time: 455.95916533470154 and batch: 600, loss is 4.139201951026917 and perplexity is 62.7527217157168
At time: 456.82776737213135 and batch: 650, loss is 4.147990574836731 and perplexity is 63.30666240255024
At time: 457.69628167152405 and batch: 700, loss is 4.139376721382141 and perplexity is 62.763689989618996
At time: 458.56340980529785 and batch: 750, loss is 4.1297442960739135 and perplexity is 62.16202582831336
At time: 459.4290978908539 and batch: 800, loss is 4.149753108024597 and perplexity is 63.41834088565318
At time: 460.2954885959625 and batch: 850, loss is 4.176700000762939 and perplexity is 65.15050147399432
At time: 461.16311526298523 and batch: 900, loss is 4.131661281585694 and perplexity is 62.281303821774586
At time: 462.02926778793335 and batch: 950, loss is 4.127594408988952 and perplexity is 62.028528045808336
At time: 462.896865606308 and batch: 1000, loss is 4.108318529129028 and perplexity is 60.844323549753696
At time: 463.7636752128601 and batch: 1050, loss is 4.092406749725342 and perplexity is 59.88384383098075
At time: 464.63101983070374 and batch: 1100, loss is 4.034240951538086 and perplexity is 56.50001770629431
At time: 465.5037250518799 and batch: 1150, loss is 4.062617344856262 and perplexity is 58.126248593556056
At time: 466.3769996166229 and batch: 1200, loss is 4.088297729492187 and perplexity is 59.63828475415025
At time: 467.2470951080322 and batch: 1250, loss is 4.09226010799408 and perplexity is 59.87506300428031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528600094092154 and perplexity of 92.62879867322265
Finished 20 epochs...
Completing Train Step...
At time: 469.76665234565735 and batch: 50, loss is 4.152174625396729 and perplexity is 63.57209558452521
At time: 470.63575410842896 and batch: 100, loss is 4.1805828046798705 and perplexity is 65.40395984244886
At time: 471.5068702697754 and batch: 150, loss is 4.093082885742188 and perplexity is 59.92434714595405
At time: 472.38123631477356 and batch: 200, loss is 4.160314316749573 and perplexity is 64.0916645179657
At time: 473.3077480792999 and batch: 250, loss is 4.158516058921814 and perplexity is 63.97651474607178
At time: 474.18148469924927 and batch: 300, loss is 4.172724962234497 and perplexity is 64.89203975845444
At time: 475.0537340641022 and batch: 350, loss is 4.152713208198548 and perplexity is 63.60634364376261
At time: 475.92491579055786 and batch: 400, loss is 4.163226318359375 and perplexity is 64.27857155293317
At time: 476.7982220649719 and batch: 450, loss is 4.09285002708435 and perplexity is 59.910394867423314
At time: 477.6685631275177 and batch: 500, loss is 4.119308071136475 and perplexity is 61.5166623807131
At time: 478.54496145248413 and batch: 550, loss is 4.115503449440002 and perplexity is 61.28305941955839
At time: 479.41978454589844 and batch: 600, loss is 4.136870846748352 and perplexity is 62.60660894585053
At time: 480.2950985431671 and batch: 650, loss is 4.145834403038025 and perplexity is 63.17030941546664
At time: 481.164999961853 and batch: 700, loss is 4.1372923135757445 and perplexity is 62.63300111601815
At time: 482.0334618091583 and batch: 750, loss is 4.128007793426514 and perplexity is 62.05417497464274
At time: 482.9012978076935 and batch: 800, loss is 4.14846706867218 and perplexity is 63.33683482484363
At time: 483.769727230072 and batch: 850, loss is 4.175782113075257 and perplexity is 65.09072806767851
At time: 484.6369824409485 and batch: 900, loss is 4.131031398773193 and perplexity is 62.242086251490505
At time: 485.50436329841614 and batch: 950, loss is 4.127394666671753 and perplexity is 62.01613956117752
At time: 486.3749005794525 and batch: 1000, loss is 4.108448514938354 and perplexity is 60.852232962437775
At time: 487.24350905418396 and batch: 1050, loss is 4.092996907234192 and perplexity is 59.919195161476964
At time: 488.1138560771942 and batch: 1100, loss is 4.035123205184936 and perplexity is 56.549887048432524
At time: 488.98651599884033 and batch: 1150, loss is 4.063801226615905 and perplexity is 58.19510394928013
At time: 489.8564534187317 and batch: 1200, loss is 4.089691243171692 and perplexity is 59.721449451891324
At time: 490.72467374801636 and batch: 1250, loss is 4.093502569198608 and perplexity is 59.94950168118993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528596084483349 and perplexity of 92.62842726872049
Finished 21 epochs...
Completing Train Step...
At time: 493.207017660141 and batch: 50, loss is 4.150255451202392 and perplexity is 63.45020665965118
At time: 494.10402059555054 and batch: 100, loss is 4.178228883743286 and perplexity is 65.25018514977924
At time: 494.97616386413574 and batch: 150, loss is 4.090551023483276 and perplexity is 59.77281885834704
At time: 495.8701467514038 and batch: 200, loss is 4.157642650604248 and perplexity is 63.92066152085002
At time: 496.73680567741394 and batch: 250, loss is 4.155754265785217 and perplexity is 63.80006861275625
At time: 497.60230231285095 and batch: 300, loss is 4.170076608657837 and perplexity is 64.72041006123897
At time: 498.46901392936707 and batch: 350, loss is 4.1500710678100585 and perplexity is 63.4385085738025
At time: 499.3326756954193 and batch: 400, loss is 4.160733609199524 and perplexity is 64.11854330364505
At time: 500.1994457244873 and batch: 450, loss is 4.090440831184387 and perplexity is 59.76623271690366
At time: 501.0662534236908 and batch: 500, loss is 4.117338871955871 and perplexity is 61.395643014285
At time: 501.9351997375488 and batch: 550, loss is 4.113382759094239 and perplexity is 61.15323473472172
At time: 502.80668020248413 and batch: 600, loss is 4.134939017295838 and perplexity is 62.48578040241366
At time: 503.6743874549866 and batch: 650, loss is 4.144059429168701 and perplexity is 63.058283218135394
At time: 504.5435047149658 and batch: 700, loss is 4.1355948781967165 and perplexity is 62.526775824818294
At time: 505.40881609916687 and batch: 750, loss is 4.126551065444946 and perplexity is 61.96384473085476
At time: 506.2863783836365 and batch: 800, loss is 4.147346801757813 and perplexity is 63.26592039329078
At time: 507.17029881477356 and batch: 850, loss is 4.1749412727355955 and perplexity is 65.03602016131185
At time: 508.0490839481354 and batch: 900, loss is 4.130400810241699 and perplexity is 62.20284947815389
At time: 508.9473762512207 and batch: 950, loss is 4.12709596157074 and perplexity is 61.99761779035689
At time: 509.84007143974304 and batch: 1000, loss is 4.1084260082244874 and perplexity is 60.85086339405463
At time: 510.72540044784546 and batch: 1050, loss is 4.093293251991272 and perplexity is 59.93695453212976
At time: 511.6265799999237 and batch: 1100, loss is 4.035615134239197 and perplexity is 56.5777124243811
At time: 512.515243768692 and batch: 1150, loss is 4.06453384399414 and perplexity is 58.2377543150535
At time: 513.4138422012329 and batch: 1200, loss is 4.090598134994507 and perplexity is 59.77563491250775
At time: 514.2845730781555 and batch: 1250, loss is 4.094294176101685 and perplexity is 59.99697690893661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528631279938413 and perplexity of 92.6316874257411
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 516.8356504440308 and batch: 50, loss is 4.15059202671051 and perplexity is 63.47156603951824
At time: 517.7648723125458 and batch: 100, loss is 4.180802459716797 and perplexity is 65.41832772959481
At time: 518.6332612037659 and batch: 150, loss is 4.093990607261658 and perplexity is 59.97876646045343
At time: 519.5070807933807 and batch: 200, loss is 4.161681613922119 and perplexity is 64.17935680669237
At time: 520.3806314468384 and batch: 250, loss is 4.160194473266602 and perplexity is 64.08398400989905
At time: 521.251859664917 and batch: 300, loss is 4.17472439289093 and perplexity is 65.02191668879634
At time: 522.1220090389252 and batch: 350, loss is 4.153516030311584 and perplexity is 63.657428726342395
At time: 522.9955070018768 and batch: 400, loss is 4.163733186721802 and perplexity is 64.31116058571247
At time: 523.8621141910553 and batch: 450, loss is 4.093411064147949 and perplexity is 59.94401624997774
At time: 524.7308502197266 and batch: 500, loss is 4.120253257751465 and perplexity is 61.5748345940621
At time: 525.5995290279388 and batch: 550, loss is 4.11507661819458 and perplexity is 61.25690747661331
At time: 526.4701058864594 and batch: 600, loss is 4.136630492210388 and perplexity is 62.59156297154039
At time: 527.3424279689789 and batch: 650, loss is 4.146179418563843 and perplexity is 63.19210791317956
At time: 528.2156314849854 and batch: 700, loss is 4.137161703109741 and perplexity is 62.62482112476305
At time: 529.08456158638 and batch: 750, loss is 4.126036825180054 and perplexity is 61.93198861847646
At time: 529.9527473449707 and batch: 800, loss is 4.146328673362732 and perplexity is 63.201540342437674
At time: 530.8237173557281 and batch: 850, loss is 4.170983748435974 and perplexity is 64.77914715701299
At time: 531.6930215358734 and batch: 900, loss is 4.1243861865997316 and perplexity is 61.82984561249154
At time: 532.5650935173035 and batch: 950, loss is 4.120907487869263 and perplexity is 61.61513188576562
At time: 533.4395246505737 and batch: 1000, loss is 4.102137899398803 and perplexity is 60.46942705633863
At time: 534.3071298599243 and batch: 1050, loss is 4.086836133003235 and perplexity is 59.55118131709166
At time: 535.1800074577332 and batch: 1100, loss is 4.026936483383179 and perplexity is 56.08881874935062
At time: 536.048136472702 and batch: 1150, loss is 4.053948073387146 and perplexity is 57.624514342599525
At time: 536.9159309864044 and batch: 1200, loss is 4.080861110687255 and perplexity is 59.19642258089485
At time: 537.7856223583221 and batch: 1250, loss is 4.086998424530029 and perplexity is 59.560846753518014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528452629590556 and perplexity of 92.61514022068594
Finished 23 epochs...
Completing Train Step...
At time: 540.3098917007446 and batch: 50, loss is 4.150030908584594 and perplexity is 63.435960983588544
At time: 541.1807057857513 and batch: 100, loss is 4.179608726501465 and perplexity is 65.34028229087667
At time: 542.047367811203 and batch: 150, loss is 4.092543973922729 and perplexity is 59.89206190723349
At time: 542.9151937961578 and batch: 200, loss is 4.159981479644776 and perplexity is 64.07033598356662
At time: 543.7854771614075 and batch: 250, loss is 4.158457345962525 and perplexity is 63.972758605834166
At time: 544.6526546478271 and batch: 300, loss is 4.173021693229675 and perplexity is 64.91129809512249
At time: 545.5264265537262 and batch: 350, loss is 4.151907391548157 and perplexity is 63.555109238524786
At time: 546.396312713623 and batch: 400, loss is 4.1621431016922 and perplexity is 64.20898163017141
At time: 547.2648355960846 and batch: 450, loss is 4.091913962364197 and perplexity is 59.85434109948772
At time: 548.1341879367828 and batch: 500, loss is 4.118832187652588 and perplexity is 61.48739458168571
At time: 549.0082004070282 and batch: 550, loss is 4.1136436986923215 and perplexity is 61.169194117341746
At time: 549.8727037906647 and batch: 600, loss is 4.135376782417297 and perplexity is 62.513140485869194
At time: 550.738438129425 and batch: 650, loss is 4.145083889961243 and perplexity is 63.12291705870228
At time: 551.6073067188263 and batch: 700, loss is 4.136294856071472 and perplexity is 62.57055850614189
At time: 552.4731795787811 and batch: 750, loss is 4.125403547286988 and perplexity is 61.892780875219586
At time: 553.3416178226471 and batch: 800, loss is 4.145898790359497 and perplexity is 63.174376913432695
At time: 554.2080008983612 and batch: 850, loss is 4.170761051177979 and perplexity is 64.76472262477735
At time: 555.0750806331635 and batch: 900, loss is 4.124370760917664 and perplexity is 61.828891852307045
At time: 555.9411582946777 and batch: 950, loss is 4.121066670417786 and perplexity is 61.62494072016369
At time: 556.808497428894 and batch: 1000, loss is 4.102518029212952 and perplexity is 60.492417657836924
At time: 557.6738278865814 and batch: 1050, loss is 4.087257084846496 and perplexity is 59.57625477362409
At time: 558.5434119701385 and batch: 1100, loss is 4.027570538520813 and perplexity is 56.124393430005156
At time: 559.4140636920929 and batch: 1150, loss is 4.05473023891449 and perplexity is 57.66960388269144
At time: 560.2864344120026 and batch: 1200, loss is 4.081755037307739 and perplexity is 59.24936349799944
At time: 561.1539928913116 and batch: 1250, loss is 4.088066158294677 and perplexity is 59.624475844066914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528438373203695 and perplexity of 92.61381987282947
Finished 24 epochs...
Completing Train Step...
At time: 563.695760011673 and batch: 50, loss is 4.149631547927856 and perplexity is 63.410632214542886
At time: 564.6231656074524 and batch: 100, loss is 4.17896451473236 and perplexity is 65.29820286751342
At time: 565.4962556362152 and batch: 150, loss is 4.091743569374085 and perplexity is 59.84414320818585
At time: 566.3629598617554 and batch: 200, loss is 4.158850975036621 and perplexity is 63.99794510031464
At time: 567.2260255813599 and batch: 250, loss is 4.157313370704651 and perplexity is 63.89961719676889
At time: 568.0961363315582 and batch: 300, loss is 4.1718845415115355 and perplexity is 64.83752605391831
At time: 568.9590408802032 and batch: 350, loss is 4.150801939964294 and perplexity is 63.48489096095827
At time: 569.83021068573 and batch: 400, loss is 4.161010026931763 and perplexity is 64.13626925575036
At time: 570.7007629871368 and batch: 450, loss is 4.0908604431152344 and perplexity is 59.791316603594474
At time: 571.5672459602356 and batch: 500, loss is 4.117857308387756 and perplexity is 61.427481004656556
At time: 572.4343905448914 and batch: 550, loss is 4.11270721912384 and perplexity is 61.111937230967015
At time: 573.3008213043213 and batch: 600, loss is 4.134558463096619 and perplexity is 62.46200570035833
At time: 574.1685013771057 and batch: 650, loss is 4.144376888275146 and perplexity is 63.078304822231914
At time: 575.0350203514099 and batch: 700, loss is 4.135650897026062 and perplexity is 62.53027859971254
At time: 575.9003286361694 and batch: 750, loss is 4.125023241043091 and perplexity is 61.869247139497574
At time: 576.7646317481995 and batch: 800, loss is 4.145676321983338 and perplexity is 63.16032417558874
At time: 577.6330630779266 and batch: 850, loss is 4.170702757835389 and perplexity is 64.76094738265041
At time: 578.497305393219 and batch: 900, loss is 4.124493365287781 and perplexity is 61.83647280936735
At time: 579.3667149543762 and batch: 950, loss is 4.121326465606689 and perplexity is 61.64095266310203
At time: 580.2347745895386 and batch: 1000, loss is 4.102912964820862 and perplexity is 60.51631298582594
At time: 581.0984909534454 and batch: 1050, loss is 4.087689232826233 and perplexity is 59.60200609555492
At time: 581.9620850086212 and batch: 1100, loss is 4.028129239082336 and perplexity is 56.15575892127256
At time: 582.8286232948303 and batch: 1150, loss is 4.055374684333802 and perplexity is 57.70678077269872
At time: 583.6926550865173 and batch: 1200, loss is 4.082456502914429 and perplexity is 59.29093946906361
At time: 584.6146507263184 and batch: 1250, loss is 4.08883677482605 and perplexity is 59.670441159368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.52845753022354 and perplexity of 92.61559409460905
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 587.1440210342407 and batch: 50, loss is 4.149594354629516 and perplexity is 63.40827380783975
At time: 588.0134718418121 and batch: 100, loss is 4.179117922782898 and perplexity is 65.30822090592315
At time: 588.8975827693939 and batch: 150, loss is 4.091945972442627 and perplexity is 59.85625707230572
At time: 589.7839119434357 and batch: 200, loss is 4.158908386230468 and perplexity is 64.00161940421867
At time: 590.6701726913452 and batch: 250, loss is 4.157378497123719 and perplexity is 63.903778885533164
At time: 591.5699374675751 and batch: 300, loss is 4.171955466270447 and perplexity is 64.84212480290275
At time: 592.4592573642731 and batch: 350, loss is 4.150780324935913 and perplexity is 63.483518748068626
At time: 593.3502175807953 and batch: 400, loss is 4.16082181930542 and perplexity is 64.12419945660103
At time: 594.2310228347778 and batch: 450, loss is 4.090638227462769 and perplexity is 59.7780315132958
At time: 595.1244473457336 and batch: 500, loss is 4.117845611572266 and perplexity is 61.426762502947305
At time: 595.9969975948334 and batch: 550, loss is 4.11226254940033 and perplexity is 61.08476864370711
At time: 596.8655898571014 and batch: 600, loss is 4.134233908653259 and perplexity is 62.44173666824726
At time: 597.7350401878357 and batch: 650, loss is 4.144203672409057 and perplexity is 63.0673796052685
At time: 598.6056227684021 and batch: 700, loss is 4.135672960281372 and perplexity is 62.53165823643346
At time: 599.474794626236 and batch: 750, loss is 4.124737520217895 and perplexity is 61.85157233230112
At time: 600.3421862125397 and batch: 800, loss is 4.145259127616883 and perplexity is 63.13397953995811
At time: 601.2108881473541 and batch: 850, loss is 4.170189952850341 and perplexity is 64.72774615960014
At time: 602.0796277523041 and batch: 900, loss is 4.123586225509643 and perplexity is 61.78040392014707
At time: 602.9484579563141 and batch: 950, loss is 4.120402054786682 and perplexity is 61.58399742857811
At time: 603.8174986839294 and batch: 1000, loss is 4.101913046836853 and perplexity is 60.455831879257836
At time: 604.685106754303 and batch: 1050, loss is 4.086734819412231 and perplexity is 59.54514827868346
At time: 605.5552673339844 and batch: 1100, loss is 4.026958794593811 and perplexity is 56.09007017276017
At time: 606.4252378940582 and batch: 1150, loss is 4.054068212509155 and perplexity is 57.63143771703457
At time: 607.3571553230286 and batch: 1200, loss is 4.081177773475647 and perplexity is 59.2151708534156
At time: 608.2291326522827 and batch: 1250, loss is 4.08776921749115 and perplexity is 59.60677353269925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528460648808166 and perplexity of 92.6158829246273
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 610.7963812351227 and batch: 50, loss is 4.149553184509277 and perplexity is 63.40566333532002
At time: 611.697014093399 and batch: 100, loss is 4.179080567359924 and perplexity is 65.30578133527344
At time: 612.5621764659882 and batch: 150, loss is 4.091885280609131 and perplexity is 59.852624396555804
At time: 613.4311144351959 and batch: 200, loss is 4.158842301368713 and perplexity is 63.997390005799375
At time: 614.2986345291138 and batch: 250, loss is 4.1573331880569455 and perplexity is 63.900883530541996
At time: 615.1631727218628 and batch: 300, loss is 4.171959791183472 and perplexity is 64.8424052400593
At time: 616.0288918018341 and batch: 350, loss is 4.1507451581954955 and perplexity is 63.48128627889858
At time: 616.8975501060486 and batch: 400, loss is 4.160778365135193 and perplexity is 64.12141305326301
At time: 617.7687609195709 and batch: 450, loss is 4.0905896759033205 and perplexity is 59.775129267099935
At time: 618.6367366313934 and batch: 500, loss is 4.1178458786010745 and perplexity is 61.42677890566472
At time: 619.5039944648743 and batch: 550, loss is 4.11218144416809 and perplexity is 61.079814550264125
At time: 620.3772127628326 and batch: 600, loss is 4.134154047966003 and perplexity is 62.43675022735642
At time: 621.2412705421448 and batch: 650, loss is 4.14411892414093 and perplexity is 63.062034980548646
At time: 622.1112055778503 and batch: 700, loss is 4.135616707801819 and perplexity is 62.52814077454097
At time: 622.9770753383636 and batch: 750, loss is 4.124671854972839 and perplexity is 61.847510966993575
At time: 623.8422601222992 and batch: 800, loss is 4.145195226669312 and perplexity is 63.12994534773732
At time: 624.7106156349182 and batch: 850, loss is 4.1701224374771115 and perplexity is 64.7233761891816
At time: 625.5799560546875 and batch: 900, loss is 4.123474378585815 and perplexity is 61.773494358429545
At time: 626.4481585025787 and batch: 950, loss is 4.120289139747619 and perplexity is 61.57704406168005
At time: 627.3147177696228 and batch: 1000, loss is 4.101787700653076 and perplexity is 60.448254446355755
At time: 628.1840245723724 and batch: 1050, loss is 4.086620230674743 and perplexity is 59.53832546623487
At time: 629.1093611717224 and batch: 1100, loss is 4.026800074577332 and perplexity is 56.0811682623724
At time: 629.9749755859375 and batch: 1150, loss is 4.0538964223861695 and perplexity is 57.62153805561769
At time: 630.8416652679443 and batch: 1200, loss is 4.081016674041748 and perplexity is 59.20563209127807
At time: 631.7113358974457 and batch: 1250, loss is 4.08763575553894 and perplexity is 59.59881882717572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528462876368613 and perplexity of 92.61608923233466
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 634.1828136444092 and batch: 50, loss is 4.149549832344055 and perplexity is 63.40545078941677
At time: 635.0825152397156 and batch: 100, loss is 4.179077968597412 and perplexity is 65.30561162127762
At time: 635.9574384689331 and batch: 150, loss is 4.091880569458008 and perplexity is 59.85234242246135
At time: 636.8258311748505 and batch: 200, loss is 4.15883593082428 and perplexity is 63.99698230888136
At time: 637.6978530883789 and batch: 250, loss is 4.157328743934631 and perplexity is 63.9005995478306
At time: 638.5713965892792 and batch: 300, loss is 4.1719605207443236 and perplexity is 64.84245254655696
At time: 639.4435596466064 and batch: 350, loss is 4.150741481781006 and perplexity is 63.48105289580692
At time: 640.3162026405334 and batch: 400, loss is 4.1607742547988895 and perplexity is 64.12114949323278
At time: 641.1851241588593 and batch: 450, loss is 4.0905842781066895 and perplexity is 59.77480661397937
At time: 642.0544366836548 and batch: 500, loss is 4.11784649848938 and perplexity is 61.426816983418384
At time: 642.9216103553772 and batch: 550, loss is 4.112172021865844 and perplexity is 61.07923904050162
At time: 643.7942442893982 and batch: 600, loss is 4.1341446685791015 and perplexity is 62.43616461166551
At time: 644.6629734039307 and batch: 650, loss is 4.144108242988587 and perplexity is 63.06136140894322
At time: 645.5284113883972 and batch: 700, loss is 4.1356090688705445 and perplexity is 62.52766312819524
At time: 646.3876082897186 and batch: 750, loss is 4.124663066864014 and perplexity is 61.84696744672487
At time: 647.2585687637329 and batch: 800, loss is 4.145187230110168 and perplexity is 63.129440527414026
At time: 648.1288707256317 and batch: 850, loss is 4.170113782882691 and perplexity is 64.72281603703509
At time: 649.0012392997742 and batch: 900, loss is 4.123459529876709 and perplexity is 61.77257710859137
At time: 649.868932723999 and batch: 950, loss is 4.120274209976197 and perplexity is 61.57612473734999
At time: 650.7392642498016 and batch: 1000, loss is 4.101770949363709 and perplexity is 60.4472418686348
At time: 651.6640145778656 and batch: 1050, loss is 4.086604714393616 and perplexity is 59.53740166000611
At time: 652.5324821472168 and batch: 1100, loss is 4.02677882194519 and perplexity is 56.07997640259839
At time: 653.4086883068085 and batch: 1150, loss is 4.053873410224915 and perplexity is 57.62021207474911
At time: 654.2772309780121 and batch: 1200, loss is 4.080995354652405 and perplexity is 59.20436987681109
At time: 655.1465365886688 and batch: 1250, loss is 4.087618083953857 and perplexity is 59.597765630883806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528462876368613 and perplexity of 92.61608923233466
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 657.6506178379059 and batch: 50, loss is 4.149549770355224 and perplexity is 63.40544685898712
At time: 658.5185222625732 and batch: 100, loss is 4.179077849388123 and perplexity is 65.30560383624251
At time: 659.3849251270294 and batch: 150, loss is 4.091880116462708 and perplexity is 59.85231530963768
At time: 660.2571816444397 and batch: 200, loss is 4.15883542060852 and perplexity is 63.99694965662076
At time: 661.1259133815765 and batch: 250, loss is 4.1573286485672 and perplexity is 63.900593453794855
At time: 661.991836309433 and batch: 300, loss is 4.171960835456848 and perplexity is 64.8424729532921
At time: 662.8618030548096 and batch: 350, loss is 4.150741395950317 and perplexity is 63.48104744718463
At time: 663.7307345867157 and batch: 400, loss is 4.160773987770081 and perplexity is 64.12113237104089
At time: 664.6005656719208 and batch: 450, loss is 4.090584030151367 and perplexity is 59.774791792499784
At time: 665.4674918651581 and batch: 500, loss is 4.117846813201904 and perplexity is 61.42683631521007
At time: 666.3478329181671 and batch: 550, loss is 4.112171387672424 and perplexity is 61.079200304462375
At time: 667.2319662570953 and batch: 600, loss is 4.134143991470337 and perplexity is 62.43612233560553
At time: 668.103529214859 and batch: 650, loss is 4.144107480049133 and perplexity is 63.06131329696095
At time: 668.9899680614471 and batch: 700, loss is 4.1356085634231565 and perplexity is 62.52763152375923
At time: 669.8693149089813 and batch: 750, loss is 4.1246624851226805 and perplexity is 61.84693146779803
At time: 670.7342867851257 and batch: 800, loss is 4.145186576843262 and perplexity is 63.12939928705318
At time: 671.6060690879822 and batch: 850, loss is 4.170113215446472 and perplexity is 64.7227793109755
At time: 672.4722003936768 and batch: 900, loss is 4.123458576202393 and perplexity is 61.77251819769921
At time: 673.3398776054382 and batch: 950, loss is 4.120273270606995 and perplexity is 61.576066894662006
At time: 674.2632403373718 and batch: 1000, loss is 4.10176995754242 and perplexity is 60.44718191580318
At time: 675.1281504631042 and batch: 1050, loss is 4.086603832244873 and perplexity is 59.537349139185274
At time: 675.9973475933075 and batch: 1100, loss is 4.026777124404907 and perplexity is 56.079881204660154
At time: 676.8644621372223 and batch: 1150, loss is 4.053871760368347 and perplexity is 57.6201170097422
At time: 677.7305383682251 and batch: 1200, loss is 4.080993728637695 and perplexity is 59.20427360971307
At time: 678.5974245071411 and batch: 1250, loss is 4.087616767883301 and perplexity is 59.59768719607084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.528462876368613 and perplexity of 92.61608923233466
Annealing...
Model not improving. Stopping early with 92.61381987282947loss at 28 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -92.61381987282947
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe70d2b860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'num_layers': 1, 'anneal': 4.847042992056301, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.1305334884213566, 'tune_wordvecs': True, 'lr': 23.373514309577473, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4637494087219238 and batch: 50, loss is 6.79183783531189 and perplexity is 890.5487406982656
At time: 2.311220407485962 and batch: 100, loss is 6.012553119659424 and perplexity is 408.52500315445695
At time: 3.158383846282959 and batch: 150, loss is 5.82715425491333 and perplexity is 339.39148188411514
At time: 4.00774359703064 and batch: 200, loss is 5.79831449508667 and perplexity is 329.74330729349833
At time: 4.8555989265441895 and batch: 250, loss is 5.795037279129028 and perplexity is 328.66443607749574
At time: 5.704299211502075 and batch: 300, loss is 5.79002947807312 and perplexity is 327.0226642318964
At time: 6.554030179977417 and batch: 350, loss is 5.8126059341430665 and perplexity is 334.4896489154252
At time: 7.402480363845825 and batch: 400, loss is 5.746708726882934 and perplexity is 313.1582728782657
At time: 8.254250049591064 and batch: 450, loss is 5.737667417526245 and perplexity is 310.3396731720893
At time: 9.10356855392456 and batch: 500, loss is 5.733615074157715 and perplexity is 309.08461493647087
At time: 9.953138589859009 and batch: 550, loss is 5.735227241516113 and perplexity is 309.5833129479692
At time: 10.857595443725586 and batch: 600, loss is 5.753389978408814 and perplexity is 315.25756721430287
At time: 11.704099893569946 and batch: 650, loss is 5.732289590835571 and perplexity is 308.6751998306307
At time: 12.55472993850708 and batch: 700, loss is 5.765063409805298 and perplexity is 318.95926854014067
At time: 13.409579038619995 and batch: 750, loss is 5.716846113204956 and perplexity is 303.9448021986016
At time: 14.289226293563843 and batch: 800, loss is 5.740634365081787 and perplexity is 311.26180198426647
At time: 15.156651735305786 and batch: 850, loss is 5.77456337928772 and perplexity is 322.0038104878925
At time: 16.011739253997803 and batch: 900, loss is 5.755077390670777 and perplexity is 315.7899857773705
At time: 16.86552095413208 and batch: 950, loss is 5.742027750015259 and perplexity is 311.6958117906593
At time: 17.732282161712646 and batch: 1000, loss is 5.732808771133423 and perplexity is 308.8354995214381
At time: 18.580440998077393 and batch: 1050, loss is 5.72854772567749 and perplexity is 307.5223371287644
At time: 19.42790913581848 and batch: 1100, loss is 5.723876686096191 and perplexity is 306.0892377575313
At time: 20.27906894683838 and batch: 1150, loss is 5.764936876296997 and perplexity is 318.9189120581648
At time: 21.1301167011261 and batch: 1200, loss is 5.752739753723144 and perplexity is 315.0526455915249
At time: 21.97920799255371 and batch: 1250, loss is 5.72726113319397 and perplexity is 307.12693561615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.384358426950274 and perplexity of 217.97021542200505
Finished 1 epochs...
Completing Train Step...
At time: 24.468660593032837 and batch: 50, loss is 5.63287205696106 and perplexity is 279.4636015105123
At time: 25.307202577590942 and batch: 100, loss is 5.662515916824341 and perplexity is 287.8719942217023
At time: 26.149881839752197 and batch: 150, loss is 5.576052808761597 and perplexity is 264.02737970212405
At time: 26.990171194076538 and batch: 200, loss is 5.623804321289063 and perplexity is 276.94095407725075
At time: 27.830255031585693 and batch: 250, loss is 5.645170745849609 and perplexity is 282.9218598321734
At time: 28.670128107070923 and batch: 300, loss is 5.633869142532348 and perplexity is 279.7423895999724
At time: 29.50914716720581 and batch: 350, loss is 5.702369842529297 and perplexity is 299.57650955046
At time: 30.349022388458252 and batch: 400, loss is 5.6637295055389405 and perplexity is 288.2215644995073
At time: 31.194072484970093 and batch: 450, loss is 5.62528205871582 and perplexity is 277.3505026181682
At time: 32.03471636772156 and batch: 500, loss is 5.645384073257446 and perplexity is 282.9822212572962
At time: 32.87606954574585 and batch: 550, loss is 5.604272317886353 and perplexity is 271.58422652806075
At time: 33.71617889404297 and batch: 600, loss is 5.651033325195312 and perplexity is 284.5853831883252
At time: 34.56439256668091 and batch: 650, loss is 5.6476044082641605 and perplexity is 283.6112346412358
At time: 35.420379400253296 and batch: 700, loss is 5.700010204315186 and perplexity is 298.8704507197788
At time: 36.277961015701294 and batch: 750, loss is 5.647293119430542 and perplexity is 283.5229633704466
At time: 37.15040469169617 and batch: 800, loss is 5.683427200317383 and perplexity is 293.95514871723856
At time: 38.02508234977722 and batch: 850, loss is 5.709783124923706 and perplexity is 301.8056070705347
At time: 38.897534132003784 and batch: 900, loss is 5.705828437805176 and perplexity is 300.61441726790906
At time: 39.758185148239136 and batch: 950, loss is 5.679859714508057 and perplexity is 292.9083362501497
At time: 40.62075114250183 and batch: 1000, loss is 5.698649034500122 and perplexity is 298.4639140291493
At time: 41.47829580307007 and batch: 1050, loss is 5.664077548980713 and perplexity is 288.32189558358385
At time: 42.32624793052673 and batch: 1100, loss is 5.6768965244293215 and perplexity is 292.0416778451289
At time: 43.1830677986145 and batch: 1150, loss is 5.672630233764648 and perplexity is 290.79839714543397
At time: 44.0462806224823 and batch: 1200, loss is 5.688655271530151 and perplexity is 295.49599147630994
At time: 44.89436411857605 and batch: 1250, loss is 5.697164096832275 and perplexity is 298.02104262031196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.479144242558166 and perplexity of 239.64154456684045
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 47.36431407928467 and batch: 50, loss is 5.62914270401001 and perplexity is 278.4233240900298
At time: 48.20216965675354 and batch: 100, loss is 5.614053411483765 and perplexity is 274.2536509303717
At time: 49.04355573654175 and batch: 150, loss is 5.506938676834107 and perplexity is 246.395674529023
At time: 49.885009765625 and batch: 200, loss is 5.541566867828369 and perplexity is 255.07735901261336
At time: 50.723620891571045 and batch: 250, loss is 5.544054441452026 and perplexity is 255.71267258992296
At time: 51.56529355049133 and batch: 300, loss is 5.510967826843261 and perplexity is 247.39044235193984
At time: 52.40648889541626 and batch: 350, loss is 5.543485898971557 and perplexity is 255.5673303932829
At time: 53.25032830238342 and batch: 400, loss is 5.493776149749756 and perplexity is 243.17373573506373
At time: 54.090903997421265 and batch: 450, loss is 5.4696228885650635 and perplexity is 237.3706606899793
At time: 54.931090354919434 and batch: 500, loss is 5.47477159500122 and perplexity is 238.59596419563698
At time: 55.77395272254944 and batch: 550, loss is 5.459948511123657 and perplexity is 235.08531977465543
At time: 56.61052131652832 and batch: 600, loss is 5.475801105499268 and perplexity is 238.84172773193154
At time: 57.453654527664185 and batch: 650, loss is 5.451597490310669 and perplexity is 233.1302919771858
At time: 58.29163455963135 and batch: 700, loss is 5.4645607471466064 and perplexity is 236.1720930558735
At time: 59.138641357421875 and batch: 750, loss is 5.4331886577606205 and perplexity is 228.87789637788228
At time: 59.97924852371216 and batch: 800, loss is 5.442686777114869 and perplexity is 231.0621627455844
At time: 60.820157289505005 and batch: 850, loss is 5.453142147064209 and perplexity is 233.49067652069732
At time: 61.658807039260864 and batch: 900, loss is 5.449877529144287 and perplexity is 232.7296615614207
At time: 62.49964785575867 and batch: 950, loss is 5.42359619140625 and perplexity is 226.69288941500645
At time: 63.39624619483948 and batch: 1000, loss is 5.398282775878906 and perplexity is 221.0265380270513
At time: 64.23803782463074 and batch: 1050, loss is 5.388378419876099 and perplexity is 218.8482177452048
At time: 65.08042550086975 and batch: 1100, loss is 5.383508081436157 and perplexity is 217.78494421054563
At time: 65.92574954032898 and batch: 1150, loss is 5.369243106842041 and perplexity is 214.7003010004445
At time: 66.77068066596985 and batch: 1200, loss is 5.344109220504761 and perplexity is 209.37129784836782
At time: 67.61333918571472 and batch: 1250, loss is 5.376662845611572 and perplexity is 216.29924568806385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.217057945084398 and perplexity of 184.3908971272634
Finished 3 epochs...
Completing Train Step...
At time: 70.05837512016296 and batch: 50, loss is 5.428563680648804 and perplexity is 227.82178547266923
At time: 70.92995524406433 and batch: 100, loss is 5.444975147247314 and perplexity is 231.59152395373263
At time: 71.77635192871094 and batch: 150, loss is 5.356826572418213 and perplexity is 212.05094924344357
At time: 72.62163543701172 and batch: 200, loss is 5.40012056350708 and perplexity is 221.43311134741248
At time: 73.46776485443115 and batch: 250, loss is 5.410125732421875 and perplexity is 223.65970718940278
At time: 74.31216049194336 and batch: 300, loss is 5.38647539138794 and perplexity is 218.43213938231142
At time: 75.15712690353394 and batch: 350, loss is 5.417162837982178 and perplexity is 225.23917508157965
At time: 76.00795388221741 and batch: 400, loss is 5.3843840312957765 and perplexity is 217.9757964781593
At time: 76.85523223876953 and batch: 450, loss is 5.364891014099121 and perplexity is 213.76793572028052
At time: 77.70220017433167 and batch: 500, loss is 5.368063058853149 and perplexity is 214.44709376975902
At time: 78.54640173912048 and batch: 550, loss is 5.354530220031738 and perplexity is 211.56456420954595
At time: 79.39334774017334 and batch: 600, loss is 5.378971328735352 and perplexity is 216.79914562970924
At time: 80.24152255058289 and batch: 650, loss is 5.363645496368409 and perplexity is 213.50184970790485
At time: 81.08878922462463 and batch: 700, loss is 5.379153881072998 and perplexity is 216.8387264332179
At time: 81.93495273590088 and batch: 750, loss is 5.352188396453857 and perplexity is 211.0696969968492
At time: 82.77940917015076 and batch: 800, loss is 5.365380897521972 and perplexity is 213.87268274314727
At time: 83.6283106803894 and batch: 850, loss is 5.377114152908325 and perplexity is 216.3968851469678
At time: 84.534099817276 and batch: 900, loss is 5.3739291858673095 and perplexity is 215.7087646023158
At time: 85.38070774078369 and batch: 950, loss is 5.355626993179321 and perplexity is 211.79672983579047
At time: 86.22860169410706 and batch: 1000, loss is 5.338307409286499 and perplexity is 208.16008212377903
At time: 87.07435441017151 and batch: 1050, loss is 5.331252355575561 and perplexity is 206.69666985994243
At time: 87.9233832359314 and batch: 1100, loss is 5.320260858535766 and perplexity is 204.43720422763658
At time: 88.77097916603088 and batch: 1150, loss is 5.318513822555542 and perplexity is 204.08035687949499
At time: 89.61767530441284 and batch: 1200, loss is 5.310526037216187 and perplexity is 202.45670014058058
At time: 90.46690773963928 and batch: 1250, loss is 5.3389181137084964 and perplexity is 208.28724523199935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.206524702754334 and perplexity of 182.45885631456443
Finished 4 epochs...
Completing Train Step...
At time: 92.95620894432068 and batch: 50, loss is 5.359631814956665 and perplexity is 212.64663872267133
At time: 93.8046145439148 and batch: 100, loss is 5.3748119354248045 and perplexity is 215.89926548873228
At time: 94.6521246433258 and batch: 150, loss is 5.291205272674561 and perplexity is 198.58262744308632
At time: 95.49452924728394 and batch: 200, loss is 5.3385889530181885 and perplexity is 208.21869654096258
At time: 96.33890199661255 and batch: 250, loss is 5.34843599319458 and perplexity is 210.2791625075958
At time: 97.19246459007263 and batch: 300, loss is 5.326923885345459 and perplexity is 205.80392298577567
At time: 98.0397400856018 and batch: 350, loss is 5.352047672271729 and perplexity is 211.03999647620697
At time: 98.88380742073059 and batch: 400, loss is 5.325330438613892 and perplexity is 205.47624653416858
At time: 99.73415493965149 and batch: 450, loss is 5.300009469985962 and perplexity is 200.33870717055308
At time: 100.5767092704773 and batch: 500, loss is 5.309242057800293 and perplexity is 202.1969167189632
At time: 101.4208927154541 and batch: 550, loss is 5.293958616256714 and perplexity is 199.13014705479742
At time: 102.27044749259949 and batch: 600, loss is 5.314620990753173 and perplexity is 203.2874507025041
At time: 103.1161437034607 and batch: 650, loss is 5.304000625610351 and perplexity is 201.13988788339182
At time: 103.96319961547852 and batch: 700, loss is 5.324115324020386 and perplexity is 205.22672098015917
At time: 104.80873274803162 and batch: 750, loss is 5.297564830780029 and perplexity is 199.84954946320377
At time: 105.65546584129333 and batch: 800, loss is 5.317064476013184 and perplexity is 203.78478796252182
At time: 106.55374217033386 and batch: 850, loss is 5.324850931167602 and perplexity is 205.37774276245318
At time: 107.40008926391602 and batch: 900, loss is 5.323027181625366 and perplexity is 205.0035265401589
At time: 108.2434504032135 and batch: 950, loss is 5.305554428100586 and perplexity is 201.45266247411183
At time: 109.08545255661011 and batch: 1000, loss is 5.2912801742553714 and perplexity is 198.597502152866
At time: 109.92708325386047 and batch: 1050, loss is 5.291206521987915 and perplexity is 198.58287553516965
At time: 110.77030658721924 and batch: 1100, loss is 5.273309183120728 and perplexity is 195.06038613664427
At time: 111.62057638168335 and batch: 1150, loss is 5.275370016098022 and perplexity is 195.46278751155256
At time: 112.46989917755127 and batch: 1200, loss is 5.27303526878357 and perplexity is 195.00696361720088
At time: 113.3136203289032 and batch: 1250, loss is 5.299978666305542 and perplexity is 200.33253609608792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.202802003735173 and perplexity of 181.78087964364641
Finished 5 epochs...
Completing Train Step...
At time: 115.80477905273438 and batch: 50, loss is 5.30519666671753 and perplexity is 201.38060338171357
At time: 116.65566277503967 and batch: 100, loss is 5.322378826141358 and perplexity is 204.87065445831155
At time: 117.51978921890259 and batch: 150, loss is 5.244292879104615 and perplexity is 189.4817813191497
At time: 118.38506555557251 and batch: 200, loss is 5.294126815795899 and perplexity is 199.1636434707317
At time: 119.25781059265137 and batch: 250, loss is 5.302636613845825 and perplexity is 200.86571773816368
At time: 120.1255395412445 and batch: 300, loss is 5.281237392425537 and perplexity is 196.61301234507624
At time: 120.99331378936768 and batch: 350, loss is 5.302674741744995 and perplexity is 200.87337647200098
At time: 121.86203241348267 and batch: 400, loss is 5.279590215682983 and perplexity is 196.28942254182036
At time: 122.72336840629578 and batch: 450, loss is 5.251591625213623 and perplexity is 190.8698200424706
At time: 123.57617354393005 and batch: 500, loss is 5.261315002441406 and perplexity is 192.73477142372099
At time: 124.41945695877075 and batch: 550, loss is 5.247698717117309 and perplexity is 190.12822579037436
At time: 125.26490187644958 and batch: 600, loss is 5.2698232936859135 and perplexity is 194.38161095242174
At time: 126.10611891746521 and batch: 650, loss is 5.25963020324707 and perplexity is 192.4103254260299
At time: 126.9511489868164 and batch: 700, loss is 5.281638097763062 and perplexity is 196.69181201521948
At time: 127.798269033432 and batch: 750, loss is 5.252107276916504 and perplexity is 190.96826777039178
At time: 128.7023663520813 and batch: 800, loss is 5.275897102355957 and perplexity is 195.56584041728695
At time: 129.5494396686554 and batch: 850, loss is 5.283729972839356 and perplexity is 197.10369737064107
At time: 130.38764023780823 and batch: 900, loss is 5.283611984252929 and perplexity is 197.08044275592545
At time: 131.23553895950317 and batch: 950, loss is 5.265512266159058 and perplexity is 193.5454301711058
At time: 132.0855576992035 and batch: 1000, loss is 5.254156341552735 and perplexity is 191.35997527446548
At time: 132.94027638435364 and batch: 1050, loss is 5.25634729385376 and perplexity is 191.7796954781927
At time: 133.78800082206726 and batch: 1100, loss is 5.236230907440185 and perplexity is 187.96032577360128
At time: 134.63525676727295 and batch: 1150, loss is 5.25541501045227 and perplexity is 191.60098576834758
At time: 135.47523379325867 and batch: 1200, loss is 5.250755243301391 and perplexity is 190.71024671881912
At time: 136.32984280586243 and batch: 1250, loss is 5.267494897842408 and perplexity is 193.92954012158697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.197920973283531 and perplexity of 180.8957635312121
Finished 6 epochs...
Completing Train Step...
At time: 138.81910276412964 and batch: 50, loss is 5.262427339553833 and perplexity is 192.9492767418266
At time: 139.72258496284485 and batch: 100, loss is 5.277457485198974 and perplexity is 195.87123620456308
At time: 140.56710839271545 and batch: 150, loss is 5.204021406173706 and perplexity is 182.0026788953532
At time: 141.413024187088 and batch: 200, loss is 5.257806615829468 and perplexity is 192.05976811066827
At time: 142.25902104377747 and batch: 250, loss is 5.264880361557007 and perplexity is 193.42316655661352
At time: 143.1152606010437 and batch: 300, loss is 5.241990346908569 and perplexity is 189.0459953152827
At time: 143.96579933166504 and batch: 350, loss is 5.260766906738281 and perplexity is 192.6291632679929
At time: 144.81234216690063 and batch: 400, loss is 5.2403347778320315 and perplexity is 188.73327554741218
At time: 145.6541678905487 and batch: 450, loss is 5.211077537536621 and perplexity is 183.29145524642152
At time: 146.49870038032532 and batch: 500, loss is 5.221426658630371 and perplexity is 185.19821031331506
At time: 147.3453071117401 and batch: 550, loss is 5.212839412689209 and perplexity is 183.6146765612915
At time: 148.18866896629333 and batch: 600, loss is 5.235158529281616 and perplexity is 187.7588692636509
At time: 149.02999663352966 and batch: 650, loss is 5.227296543121338 and perplexity is 186.28849922022107
At time: 149.87795519828796 and batch: 700, loss is 5.253851957321167 and perplexity is 191.3017371792674
At time: 150.7921175956726 and batch: 750, loss is 5.221766595840454 and perplexity is 185.26117677795716
At time: 151.6368329524994 and batch: 800, loss is 5.241125144958496 and perplexity is 188.882503088578
At time: 152.4827446937561 and batch: 850, loss is 5.251509189605713 and perplexity is 190.85408622134608
At time: 153.33154773712158 and batch: 900, loss is 5.2475509929656985 and perplexity is 190.10014133394952
At time: 154.17573595046997 and batch: 950, loss is 5.2308358955383305 and perplexity is 186.94900806746963
At time: 155.01946115493774 and batch: 1000, loss is 5.224045505523682 and perplexity is 185.68385170370553
At time: 155.86288022994995 and batch: 1050, loss is 5.227492599487305 and perplexity is 186.32502584692148
At time: 156.70671486854553 and batch: 1100, loss is 5.2053573703765865 and perplexity is 182.24599045076124
At time: 157.55162525177002 and batch: 1150, loss is 5.216868047714233 and perplexity is 184.3558851052673
At time: 158.39505243301392 and batch: 1200, loss is 5.215810813903809 and perplexity is 184.1610808253639
At time: 159.23536896705627 and batch: 1250, loss is 5.2388458347320555 and perplexity is 188.4524715415209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.203002038663321 and perplexity of 181.81724580597466
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 161.7179822921753 and batch: 50, loss is 5.233167448043823 and perplexity is 187.38539803099553
At time: 162.56514644622803 and batch: 100, loss is 5.2427326107025145 and perplexity is 189.18636940385244
At time: 163.40854334831238 and batch: 150, loss is 5.164529314041138 and perplexity is 174.9550902840919
At time: 164.25617170333862 and batch: 200, loss is 5.209384317398071 and perplexity is 182.9813650627317
At time: 165.10342693328857 and batch: 250, loss is 5.213146276473999 and perplexity is 183.67102990184168
At time: 165.94728136062622 and batch: 300, loss is 5.18061975479126 and perplexity is 177.7929648974812
At time: 166.79131817817688 and batch: 350, loss is 5.193006725311279 and perplexity is 180.0089776196673
At time: 167.63487696647644 and batch: 400, loss is 5.162654151916504 and perplexity is 174.62732852453857
At time: 168.4825472831726 and batch: 450, loss is 5.1297478771209715 and perplexity is 168.9745103338957
At time: 169.3311095237732 and batch: 500, loss is 5.123710632324219 and perplexity is 167.95744308178433
At time: 170.17450189590454 and batch: 550, loss is 5.120654964447022 and perplexity is 167.44500423840418
At time: 171.0193886756897 and batch: 600, loss is 5.138332252502441 and perplexity is 170.4312948068429
At time: 171.8668451309204 and batch: 650, loss is 5.1354918289184575 and perplexity is 169.9478846075031
At time: 172.78222036361694 and batch: 700, loss is 5.15122353553772 and perplexity is 172.64259549408615
At time: 173.62902522087097 and batch: 750, loss is 5.119446392059326 and perplexity is 167.24275706961356
At time: 174.47203826904297 and batch: 800, loss is 5.121987533569336 and perplexity is 167.6682850162324
At time: 175.31671977043152 and batch: 850, loss is 5.136555938720703 and perplexity is 170.12882407003917
At time: 176.16478824615479 and batch: 900, loss is 5.125506744384766 and perplexity is 168.25938455014625
At time: 177.00916171073914 and batch: 950, loss is 5.094694356918335 and perplexity is 163.15397012066381
At time: 177.85258150100708 and batch: 1000, loss is 5.087171154022217 and perplexity is 161.9311352849818
At time: 178.69540071487427 and batch: 1050, loss is 5.074102458953857 and perplexity is 159.82867478676368
At time: 179.54710054397583 and batch: 1100, loss is 5.040959634780884 and perplexity is 154.61832097330478
At time: 180.40192890167236 and batch: 1150, loss is 5.052009706497192 and perplexity is 156.336339139303
At time: 181.24624943733215 and batch: 1200, loss is 5.062248735427857 and perplexity is 157.94529445208724
At time: 182.0882053375244 and batch: 1250, loss is 5.1173185443878175 and perplexity is 166.88726830550567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.096619988880018 and perplexity of 163.46844730668897
Finished 8 epochs...
Completing Train Step...
At time: 184.5684700012207 and batch: 50, loss is 5.160194625854492 and perplexity is 174.19835580992628
At time: 185.47325658798218 and batch: 100, loss is 5.179679641723633 and perplexity is 177.6258979510633
At time: 186.319726228714 and batch: 150, loss is 5.107707061767578 and perplexity is 165.2909181617754
At time: 187.16266870498657 and batch: 200, loss is 5.152846088409424 and perplexity is 172.92294461231126
At time: 188.0078341960907 and batch: 250, loss is 5.1633094310760494 and perplexity is 174.74179567348042
At time: 188.8500370979309 and batch: 300, loss is 5.1360501956939695 and perplexity is 170.0428043573877
At time: 189.6920130252838 and batch: 350, loss is 5.15034857749939 and perplexity is 172.49160653153618
At time: 190.53404688835144 and batch: 400, loss is 5.124566478729248 and perplexity is 168.10125038534775
At time: 191.37614727020264 and batch: 450, loss is 5.091488771438598 and perplexity is 162.63180349509898
At time: 192.21904063224792 and batch: 500, loss is 5.090203714370728 and perplexity is 162.42294657183146
At time: 193.06027841567993 and batch: 550, loss is 5.0886579036712645 and perplexity is 162.17206540053917
At time: 193.93293023109436 and batch: 600, loss is 5.109994478225708 and perplexity is 165.6694400819489
At time: 194.7822368144989 and batch: 650, loss is 5.109241352081299 and perplexity is 165.54471706727034
At time: 195.625182390213 and batch: 700, loss is 5.124967737197876 and perplexity is 168.16871597031528
At time: 196.46890783309937 and batch: 750, loss is 5.094060401916504 and perplexity is 163.05057062401428
At time: 197.31108140945435 and batch: 800, loss is 5.106738185882568 and perplexity is 165.13084933309295
At time: 198.1522011756897 and batch: 850, loss is 5.123949813842773 and perplexity is 167.99762020269404
At time: 198.99405312538147 and batch: 900, loss is 5.112433443069458 and perplexity is 166.0739951692671
At time: 199.83814358711243 and batch: 950, loss is 5.084842176437378 and perplexity is 161.55444012817424
At time: 200.68151760101318 and batch: 1000, loss is 5.079846868515014 and perplexity is 160.74943824359522
At time: 201.5278661251068 and batch: 1050, loss is 5.072633562088012 and perplexity is 159.59407529071166
At time: 202.3711006641388 and batch: 1100, loss is 5.044858531951904 and perplexity is 155.22233864376958
At time: 203.21988201141357 and batch: 1150, loss is 5.062020311355591 and perplexity is 157.9092200650193
At time: 204.0622956752777 and batch: 1200, loss is 5.07425009727478 and perplexity is 159.85227336592905
At time: 204.90444421768188 and batch: 1250, loss is 5.120533103942871 and perplexity is 167.42460054899735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.09443653412979 and perplexity of 163.1119107312926
Finished 9 epochs...
Completing Train Step...
At time: 207.382554769516 and batch: 50, loss is 5.1417844867706295 and perplexity is 171.02068032623632
At time: 208.22489213943481 and batch: 100, loss is 5.160879249572754 and perplexity is 174.31765696953786
At time: 209.06734681129456 and batch: 150, loss is 5.090521812438965 and perplexity is 162.47462121573983
At time: 209.9113667011261 and batch: 200, loss is 5.134382038116455 and perplexity is 169.75938262657542
At time: 210.7512505054474 and batch: 250, loss is 5.146378974914551 and perplexity is 171.80824064723453
At time: 211.59392499923706 and batch: 300, loss is 5.119336767196655 and perplexity is 167.2244241102295
At time: 212.43625783920288 and batch: 350, loss is 5.133689603805542 and perplexity is 169.64187609291818
At time: 213.27799344062805 and batch: 400, loss is 5.1094620895385745 and perplexity is 165.58126302056317
At time: 214.12209820747375 and batch: 450, loss is 5.075969371795654 and perplexity is 160.1273396961416
At time: 214.9862575531006 and batch: 500, loss is 5.076472206115723 and perplexity is 160.2078774650022
At time: 215.90049409866333 and batch: 550, loss is 5.07674617767334 and perplexity is 160.25177587991786
At time: 216.74060344696045 and batch: 600, loss is 5.099125852584839 and perplexity is 163.87859062268333
At time: 217.58627653121948 and batch: 650, loss is 5.09943678855896 and perplexity is 163.9295542947051
At time: 218.44037532806396 and batch: 700, loss is 5.114851493835449 and perplexity is 166.47605642685997
At time: 219.30154180526733 and batch: 750, loss is 5.083489923477173 and perplexity is 161.33612529998038
At time: 220.1780333518982 and batch: 800, loss is 5.101270713806152 and perplexity is 164.23046468232843
At time: 221.04716849327087 and batch: 850, loss is 5.119749822616577 and perplexity is 167.29351133239507
At time: 221.91154289245605 and batch: 900, loss is 5.107605657577515 and perplexity is 165.27415781989316
At time: 222.7714400291443 and batch: 950, loss is 5.080639533996582 and perplexity is 160.8769092887466
At time: 223.6393585205078 and batch: 1000, loss is 5.07676218032837 and perplexity is 160.25434035432428
At time: 224.49985074996948 and batch: 1050, loss is 5.07150333404541 and perplexity is 159.41379948695987
At time: 225.36129760742188 and batch: 1100, loss is 5.0459762382507325 and perplexity is 155.39592862262523
At time: 226.2198576927185 and batch: 1150, loss is 5.064360761642456 and perplexity is 158.27923157228716
At time: 227.07985043525696 and batch: 1200, loss is 5.076671152114868 and perplexity is 160.23975335194046
At time: 227.94491124153137 and batch: 1250, loss is 5.118602657318116 and perplexity is 167.10170805760148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.093717032105383 and perplexity of 162.99459359133746
Finished 10 epochs...
Completing Train Step...
At time: 230.4899504184723 and batch: 50, loss is 5.128767929077148 and perplexity is 168.8090051994989
At time: 231.38475632667542 and batch: 100, loss is 5.148519372940063 and perplexity is 172.17637250008437
At time: 232.23215556144714 and batch: 150, loss is 5.078782949447632 and perplexity is 160.57850479697703
At time: 233.08344292640686 and batch: 200, loss is 5.121827154159546 and perplexity is 167.64139663187024
At time: 233.93314743041992 and batch: 250, loss is 5.13440577507019 and perplexity is 169.76341224501218
At time: 234.79897928237915 and batch: 300, loss is 5.107593641281128 and perplexity is 165.27217184855982
At time: 235.648583650589 and batch: 350, loss is 5.121692323684693 and perplexity is 167.6187949864868
At time: 236.50780606269836 and batch: 400, loss is 5.098859872817993 and perplexity is 163.83500802965935
At time: 237.35736989974976 and batch: 450, loss is 5.065701560974121 and perplexity is 158.49159459648166
At time: 238.26413011550903 and batch: 500, loss is 5.06703182220459 and perplexity is 158.70257011529336
At time: 239.11897802352905 and batch: 550, loss is 5.068192462921143 and perplexity is 158.88687371443208
At time: 239.96841645240784 and batch: 600, loss is 5.091222429275513 and perplexity is 162.588493556657
At time: 240.84492349624634 and batch: 650, loss is 5.091975746154785 and perplexity is 162.7110203581881
At time: 241.69467115402222 and batch: 700, loss is 5.1079073333740235 and perplexity is 165.32402455451106
At time: 242.55150151252747 and batch: 750, loss is 5.0762647438049315 and perplexity is 160.1746438160204
At time: 243.4048273563385 and batch: 800, loss is 5.095637273788452 and perplexity is 163.307883303782
At time: 244.26837062835693 and batch: 850, loss is 5.114864749908447 and perplexity is 166.4782632602434
At time: 245.1071617603302 and batch: 900, loss is 5.102172698974609 and perplexity is 164.37866495285596
At time: 245.96060633659363 and batch: 950, loss is 5.075972232818604 and perplexity is 160.12779782479063
At time: 246.81805419921875 and batch: 1000, loss is 5.072558517456055 and perplexity is 159.58209906145035
At time: 247.66026163101196 and batch: 1050, loss is 5.06837459564209 and perplexity is 158.91581484854817
At time: 248.5078363418579 and batch: 1100, loss is 5.04378496170044 and perplexity is 155.05578597762943
At time: 249.36092805862427 and batch: 1150, loss is 5.063552484512329 and perplexity is 158.1513497781602
At time: 250.20321011543274 and batch: 1200, loss is 5.0751859569549564 and perplexity is 160.00194268717448
At time: 251.0551745891571 and batch: 1250, loss is 5.114540100097656 and perplexity is 166.42422489582216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.093092869668111 and perplexity of 162.89289023154961
Finished 11 epochs...
Completing Train Step...
At time: 253.43160438537598 and batch: 50, loss is 5.117936601638794 and perplexity is 166.99044607337692
At time: 254.34124898910522 and batch: 100, loss is 5.137845258712769 and perplexity is 170.34831603142027
At time: 255.19976449012756 and batch: 150, loss is 5.069059429168701 and perplexity is 159.02468300049048
At time: 256.04730701446533 and batch: 200, loss is 5.111732816696167 and perplexity is 165.95768009979437
At time: 256.8941366672516 and batch: 250, loss is 5.124863080978393 and perplexity is 168.15111698920404
At time: 257.7454266548157 and batch: 300, loss is 5.098003635406494 and perplexity is 163.6947864065378
At time: 258.5894181728363 and batch: 350, loss is 5.11254096031189 and perplexity is 166.0918519472059
At time: 259.48422145843506 and batch: 400, loss is 5.0900167465209964 and perplexity is 162.3925815414946
At time: 260.32980728149414 and batch: 450, loss is 5.056376962661743 and perplexity is 157.0205930486443
At time: 261.17349910736084 and batch: 500, loss is 5.058506088256836 and perplexity is 157.35526576595748
At time: 262.02401328086853 and batch: 550, loss is 5.060475559234619 and perplexity is 157.6654777715846
At time: 262.8678801059723 and batch: 600, loss is 5.084400234222412 and perplexity is 161.48305817557736
At time: 263.71426272392273 and batch: 650, loss is 5.085683641433715 and perplexity is 161.6904397459793
At time: 264.5527627468109 and batch: 700, loss is 5.101719045639038 and perplexity is 164.30411093532138
At time: 265.40075850486755 and batch: 750, loss is 5.069509353637695 and perplexity is 159.09624819480524
At time: 266.25254130363464 and batch: 800, loss is 5.090791788101196 and perplexity is 162.51849133086355
At time: 267.09343004226685 and batch: 850, loss is 5.110777425765991 and perplexity is 165.79920135406675
At time: 267.93615102767944 and batch: 900, loss is 5.097320165634155 and perplexity is 163.58294419287935
At time: 268.7794990539551 and batch: 950, loss is 5.0715565013885495 and perplexity is 159.42227532045519
At time: 269.6227300167084 and batch: 1000, loss is 5.068459968566895 and perplexity is 158.9293825356079
At time: 270.4648756980896 and batch: 1050, loss is 5.065138311386108 and perplexity is 158.40234940713916
At time: 271.30815172195435 and batch: 1100, loss is 5.041081228256226 and perplexity is 154.63712269536364
At time: 272.15091156959534 and batch: 1150, loss is 5.061275005340576 and perplexity is 157.79157322037497
At time: 272.9913158416748 and batch: 1200, loss is 5.072632474899292 and perplexity is 159.59390178192749
At time: 273.8398857116699 and batch: 1250, loss is 5.110327968597412 and perplexity is 165.7246984586579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.092930257755475 and perplexity of 162.86640406065598
Finished 12 epochs...
Completing Train Step...
At time: 276.31381845474243 and batch: 50, loss is 5.108656139373779 and perplexity is 165.44786653702434
At time: 277.16129302978516 and batch: 100, loss is 5.129042091369629 and perplexity is 168.85529260819692
At time: 278.0106234550476 and batch: 150, loss is 5.0610613441467285 and perplexity is 157.757862885885
At time: 278.8543269634247 and batch: 200, loss is 5.103551387786865 and perplexity is 164.60544827509247
At time: 279.69999647140503 and batch: 250, loss is 5.117147436141968 and perplexity is 166.85871496069734
At time: 280.54353880882263 and batch: 300, loss is 5.090230331420899 and perplexity is 162.4272698490855
At time: 281.4184784889221 and batch: 350, loss is 5.104618139266968 and perplexity is 164.78113507108736
At time: 282.2632830142975 and batch: 400, loss is 5.0826802444458 and perplexity is 161.20554769243114
At time: 283.1067762374878 and batch: 450, loss is 5.048872585296631 and perplexity is 155.846661585886
At time: 283.95414328575134 and batch: 500, loss is 5.051063318252563 and perplexity is 156.18845425502897
At time: 284.8027935028076 and batch: 550, loss is 5.053887729644775 and perplexity is 156.63021827254715
At time: 285.6515407562256 and batch: 600, loss is 5.077960214614868 and perplexity is 160.44644559998727
At time: 286.49612760543823 and batch: 650, loss is 5.0794246387481685 and perplexity is 160.68157937279076
At time: 287.3417809009552 and batch: 700, loss is 5.095896759033203 and perplexity is 163.35026478829798
At time: 288.18977189064026 and batch: 750, loss is 5.063979883193969 and perplexity is 158.21895790333758
At time: 289.0339307785034 and batch: 800, loss is 5.086342506408691 and perplexity is 161.79700701638222
At time: 289.87817430496216 and batch: 850, loss is 5.107183389663696 and perplexity is 165.2043825790219
At time: 290.72389101982117 and batch: 900, loss is 5.092831840515137 and perplexity is 162.8503759873569
At time: 291.5718536376953 and batch: 950, loss is 5.067128257751465 and perplexity is 158.7178754224089
At time: 292.4170868396759 and batch: 1000, loss is 5.064095211029053 and perplexity is 158.2372060054586
At time: 293.2665219306946 and batch: 1050, loss is 5.061616334915161 and perplexity is 157.84544134380423
At time: 294.1142752170563 and batch: 1100, loss is 5.037979230880738 and perplexity is 154.1581819670967
At time: 294.96086072921753 and batch: 1150, loss is 5.058837356567383 and perplexity is 157.4074012139376
At time: 295.8055896759033 and batch: 1200, loss is 5.06958963394165 and perplexity is 159.10902100266398
At time: 296.64986300468445 and batch: 1250, loss is 5.1063461589813235 and perplexity is 165.06612628534586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.092807296418796 and perplexity of 162.84637902109063
Finished 13 epochs...
Completing Train Step...
At time: 299.1013021469116 and batch: 50, loss is 5.1003968620300295 and perplexity is 164.0870142853928
At time: 299.9749746322632 and batch: 100, loss is 5.120922155380249 and perplexity is 167.489750002916
At time: 300.81877422332764 and batch: 150, loss is 5.053443870544434 and perplexity is 156.56071195142607
At time: 301.6617314815521 and batch: 200, loss is 5.096176319122314 and perplexity is 163.39593738669888
At time: 302.5038056373596 and batch: 250, loss is 5.109624948501587 and perplexity is 165.60823160933154
At time: 303.3762047290802 and batch: 300, loss is 5.083175420761108 and perplexity is 161.28539262857385
At time: 304.22063064575195 and batch: 350, loss is 5.097460851669312 and perplexity is 163.6059596476557
At time: 305.06065225601196 and batch: 400, loss is 5.075753507614135 and perplexity is 160.09277766950527
At time: 305.901974439621 and batch: 450, loss is 5.042042121887207 and perplexity is 154.78578393405522
At time: 306.74587655067444 and batch: 500, loss is 5.044409151077271 and perplexity is 155.15260036416527
At time: 307.5857644081116 and batch: 550, loss is 5.048005704879761 and perplexity is 155.7116197079591
At time: 308.4274232387543 and batch: 600, loss is 5.072219820022583 and perplexity is 159.52805816634302
At time: 309.2746539115906 and batch: 650, loss is 5.073468704223632 and perplexity is 159.7274146985112
At time: 310.12184953689575 and batch: 700, loss is 5.090319719314575 and perplexity is 162.44178952954505
At time: 310.96955728530884 and batch: 750, loss is 5.058800497055054 and perplexity is 157.40159936081918
At time: 311.81844568252563 and batch: 800, loss is 5.0815825843811036 and perplexity is 161.0286958798644
At time: 312.66787576675415 and batch: 850, loss is 5.103445253372192 and perplexity is 164.5879788992552
At time: 313.5121660232544 and batch: 900, loss is 5.087724666595459 and perplexity is 162.0207910148593
At time: 314.3554813861847 and batch: 950, loss is 5.062179784774781 and perplexity is 157.93440439632704
At time: 315.1946702003479 and batch: 1000, loss is 5.0598858451843265 and perplexity is 157.57252763379145
At time: 316.0371148586273 and batch: 1050, loss is 5.057997760772705 and perplexity is 157.27529808620253
At time: 316.87915802001953 and batch: 1100, loss is 5.034370098114014 and perplexity is 153.6028074343641
At time: 317.72676253318787 and batch: 1150, loss is 5.056075658798218 and perplexity is 156.97328926407002
At time: 318.5713438987732 and batch: 1200, loss is 5.066507225036621 and perplexity is 158.61933703029067
At time: 319.41661524772644 and batch: 1250, loss is 5.102423620223999 and perplexity is 164.4199162280333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.092997530080976 and perplexity of 162.87736083094273
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 321.9301989078522 and batch: 50, loss is 5.098405408859253 and perplexity is 163.76056783980286
At time: 322.78011870384216 and batch: 100, loss is 5.12540906906128 and perplexity is 168.2429505629413
At time: 323.6247591972351 and batch: 150, loss is 5.054790019989014 and perplexity is 156.7716079838036
At time: 324.4700620174408 and batch: 200, loss is 5.096608858108521 and perplexity is 163.4666277868813
At time: 325.36657071113586 and batch: 250, loss is 5.107181911468506 and perplexity is 165.20413837487868
At time: 326.2162742614746 and batch: 300, loss is 5.079240589141846 and perplexity is 160.65200871268158
At time: 327.058495759964 and batch: 350, loss is 5.090506114959717 and perplexity is 162.4720707937626
At time: 327.90187311172485 and batch: 400, loss is 5.062437601089478 and perplexity is 157.97512771177438
At time: 328.74550104141235 and batch: 450, loss is 5.031043062210083 and perplexity is 153.09261456470048
At time: 329.58862352371216 and batch: 500, loss is 5.028093252182007 and perplexity is 152.64168583904674
At time: 330.43311858177185 and batch: 550, loss is 5.030149364471436 and perplexity is 152.9558571602543
At time: 331.2858738899231 and batch: 600, loss is 5.053584489822388 and perplexity is 156.5827289536679
At time: 332.1421654224396 and batch: 650, loss is 5.0560668849945065 and perplexity is 156.97191201728398
At time: 332.99299216270447 and batch: 700, loss is 5.071194229125976 and perplexity is 159.3645315121918
At time: 333.84353613853455 and batch: 750, loss is 5.032771224975586 and perplexity is 153.3574122617224
At time: 334.6933693885803 and batch: 800, loss is 5.051833877563476 and perplexity is 156.30885310398205
At time: 335.542870759964 and batch: 850, loss is 5.06448187828064 and perplexity is 158.29840298167437
At time: 336.39650559425354 and batch: 900, loss is 5.047801160812378 and perplexity is 155.6797730770561
At time: 337.24583435058594 and batch: 950, loss is 5.020614528656006 and perplexity is 151.50437897483124
At time: 338.0921335220337 and batch: 1000, loss is 5.017289247512817 and perplexity is 151.00142102206823
At time: 338.9384295940399 and batch: 1050, loss is 5.011416578292847 and perplexity is 150.11723842178822
At time: 339.7876121997833 and batch: 1100, loss is 4.9814169979095455 and perplexity is 145.6806646650876
At time: 340.6355414390564 and batch: 1150, loss is 4.997633171081543 and perplexity is 148.06230591425563
At time: 341.4894275665283 and batch: 1200, loss is 5.019256467819214 and perplexity is 151.29876645989646
At time: 342.3353340625763 and batch: 1250, loss is 5.067302131652832 and perplexity is 158.7454747179544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.072388586336679 and perplexity of 159.55498340068704
Finished 15 epochs...
Completing Train Step...
At time: 344.81256580352783 and batch: 50, loss is 5.083387289047241 and perplexity is 161.3195675084471
At time: 345.6866805553436 and batch: 100, loss is 5.109555358886719 and perplexity is 165.5967073972623
At time: 346.5276527404785 and batch: 150, loss is 5.040352249145508 and perplexity is 154.52443654109373
At time: 347.4035234451294 and batch: 200, loss is 5.081038398742676 and perplexity is 160.941090215226
At time: 348.2512836456299 and batch: 250, loss is 5.093606252670288 and perplexity is 162.9765381424405
At time: 349.09446835517883 and batch: 300, loss is 5.066539134979248 and perplexity is 158.6243986449923
At time: 349.9361340999603 and batch: 350, loss is 5.078933401107788 and perplexity is 160.60266591710385
At time: 350.77755427360535 and batch: 400, loss is 5.05246901512146 and perplexity is 156.40816226138315
At time: 351.61705017089844 and batch: 450, loss is 5.021135663986206 and perplexity is 151.58335383590276
At time: 352.460542678833 and batch: 500, loss is 5.018814897537231 and perplexity is 151.2319721691993
At time: 353.3039126396179 and batch: 550, loss is 5.021356773376465 and perplexity is 151.61687404452275
At time: 354.1460542678833 and batch: 600, loss is 5.045831117630005 and perplexity is 155.37337910524457
At time: 354.9894199371338 and batch: 650, loss is 5.049811315536499 and perplexity is 155.993028248557
At time: 355.82870507240295 and batch: 700, loss is 5.064577302932739 and perplexity is 158.31350927245157
At time: 356.6694595813751 and batch: 750, loss is 5.02764741897583 and perplexity is 152.57364827468257
At time: 357.51113772392273 and batch: 800, loss is 5.048260488510132 and perplexity is 155.75129753413495
At time: 358.35752749443054 and batch: 850, loss is 5.063030481338501 and perplexity is 158.06881581500025
At time: 359.2016670703888 and batch: 900, loss is 5.047220335006714 and perplexity is 155.589376502306
At time: 360.049423456192 and batch: 950, loss is 5.020961313247681 and perplexity is 151.55692747000853
At time: 360.8938548564911 and batch: 1000, loss is 5.019147043228149 and perplexity is 151.2822115600211
At time: 361.7369740009308 and batch: 1050, loss is 5.014305562973022 and perplexity is 150.55155188421523
At time: 362.57998156547546 and batch: 1100, loss is 4.9858535957336425 and perplexity is 146.32842705304066
At time: 363.42015838623047 and batch: 1150, loss is 5.0035607051849365 and perplexity is 148.94255656251048
At time: 364.26269817352295 and batch: 1200, loss is 5.0255426406860355 and perplexity is 152.2528522928379
At time: 365.1053946018219 and batch: 1250, loss is 5.06974531173706 and perplexity is 159.133792672433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071605821595575 and perplexity of 159.43013825397756
Finished 16 epochs...
Completing Train Step...
At time: 367.55141043663025 and batch: 50, loss is 5.078318614959716 and perplexity is 160.50395996738888
At time: 368.4212443828583 and batch: 100, loss is 5.104035472869873 and perplexity is 164.68515060698272
At time: 369.2584807872772 and batch: 150, loss is 5.034667797088623 and perplexity is 153.64854163980974
At time: 370.1019854545593 and batch: 200, loss is 5.074612369537354 and perplexity is 159.9101939015471
At time: 370.943284034729 and batch: 250, loss is 5.087825517654419 and perplexity is 162.03713180718594
At time: 371.7835235595703 and batch: 300, loss is 5.061011867523193 and perplexity is 157.75005775258072
At time: 372.6231601238251 and batch: 350, loss is 5.073648118972779 and perplexity is 159.75607472348986
At time: 373.46234226226807 and batch: 400, loss is 5.0475677967071535 and perplexity is 155.6434472448479
At time: 374.29902815818787 and batch: 450, loss is 5.016257400512695 and perplexity is 150.8456910173588
At time: 375.14093589782715 and batch: 500, loss is 5.014416179656982 and perplexity is 150.56820631876192
At time: 375.98002552986145 and batch: 550, loss is 5.017364501953125 and perplexity is 151.0127849770816
At time: 376.8196074962616 and batch: 600, loss is 5.042625045776367 and perplexity is 154.8760385684482
At time: 377.662490606308 and batch: 650, loss is 5.0469136142730715 and perplexity is 155.54166133258823
At time: 378.5000801086426 and batch: 700, loss is 5.061839084625245 and perplexity is 157.88060528633514
At time: 379.338698387146 and batch: 750, loss is 5.025450706481934 and perplexity is 152.23885569143317
At time: 380.177791595459 and batch: 800, loss is 5.04677529335022 and perplexity is 155.5201481543465
At time: 381.01639890670776 and batch: 850, loss is 5.062539262771606 and perplexity is 157.99118854536388
At time: 381.8561737537384 and batch: 900, loss is 5.04740478515625 and perplexity is 155.61807763295053
At time: 382.6998267173767 and batch: 950, loss is 5.02140887260437 and perplexity is 151.62477337237053
At time: 383.54054164886475 and batch: 1000, loss is 5.020507287979126 and perplexity is 151.48813241384136
At time: 384.38675355911255 and batch: 1050, loss is 5.01598051071167 and perplexity is 150.8039291659697
At time: 385.2309296131134 and batch: 1100, loss is 4.988157043457031 and perplexity is 146.6658754333499
At time: 386.0731191635132 and batch: 1150, loss is 5.006214113235473 and perplexity is 149.3382867257809
At time: 386.91604375839233 and batch: 1200, loss is 5.028395376205444 and perplexity is 152.68780952650286
At time: 387.75809478759766 and batch: 1250, loss is 5.070371265411377 and perplexity is 159.23343423691253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071446773779654 and perplexity of 159.4047832550783
Finished 17 epochs...
Completing Train Step...
At time: 390.262099981308 and batch: 50, loss is 5.074583129882813 and perplexity is 159.90551825107735
At time: 391.1070075035095 and batch: 100, loss is 5.100055208206177 and perplexity is 164.03096290514705
At time: 391.9507575035095 and batch: 150, loss is 5.030883846282959 and perplexity is 153.06824172246053
At time: 392.7960021495819 and batch: 200, loss is 5.07046989440918 and perplexity is 159.24914004545934
At time: 393.6427159309387 and batch: 250, loss is 5.083795518875122 and perplexity is 161.3854364116356
At time: 394.4902272224426 and batch: 300, loss is 5.05715708732605 and perplexity is 157.143136479447
At time: 395.3357660770416 and batch: 350, loss is 5.07007435798645 and perplexity is 159.18616366583
At time: 396.18468165397644 and batch: 400, loss is 5.044138040542602 and perplexity is 155.11054256113974
At time: 397.03053736686707 and batch: 450, loss is 5.012953462600708 and perplexity is 150.34812863018004
At time: 397.87584614753723 and batch: 500, loss is 5.0115079498291015 and perplexity is 150.13095549114638
At time: 398.72413206100464 and batch: 550, loss is 5.0149197769165035 and perplexity is 150.6440511508569
At time: 399.56929540634155 and batch: 600, loss is 5.04066954612732 and perplexity is 154.57347445780442
At time: 400.4137752056122 and batch: 650, loss is 5.04525390625 and perplexity is 155.28372170080155
At time: 401.25954842567444 and batch: 700, loss is 5.060139999389649 and perplexity is 157.61258044391232
At time: 402.10196685791016 and batch: 750, loss is 5.024110984802246 and perplexity is 152.0350345582538
At time: 402.9469702243805 and batch: 800, loss is 5.045553283691406 and perplexity is 155.33021710358648
At time: 403.79761266708374 and batch: 850, loss is 5.062029857635498 and perplexity is 157.9107275178292
At time: 404.64073491096497 and batch: 900, loss is 5.047290916442871 and perplexity is 155.6003586115123
At time: 405.48893785476685 and batch: 950, loss is 5.021618852615356 and perplexity is 151.6566148868728
At time: 406.3334436416626 and batch: 1000, loss is 5.0211678409576415 and perplexity is 151.58823140762158
At time: 407.182879447937 and batch: 1050, loss is 5.016811590194703 and perplexity is 150.92931131150783
At time: 408.0294380187988 and batch: 1100, loss is 4.989284238815308 and perplexity is 146.83128973695935
At time: 408.8790605068207 and batch: 1150, loss is 5.007602663040161 and perplexity is 149.54579440868187
At time: 409.72432470321655 and batch: 1200, loss is 5.02980393409729 and perplexity is 152.9030306857474
At time: 410.56892585754395 and batch: 1250, loss is 5.070171127319336 and perplexity is 159.2015687500504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071393312328923 and perplexity of 159.39626147190762
Finished 18 epochs...
Completing Train Step...
At time: 413.0472548007965 and batch: 50, loss is 5.071608171463013 and perplexity is 159.43051289410832
At time: 413.9188234806061 and batch: 100, loss is 5.0969056797027585 and perplexity is 163.51515541361314
At time: 414.75966119766235 and batch: 150, loss is 5.027933378219604 and perplexity is 152.61728435853647
At time: 415.6019186973572 and batch: 200, loss is 5.067204265594483 and perplexity is 158.72993968425155
At time: 416.44299817085266 and batch: 250, loss is 5.080767259597779 and perplexity is 160.89745870101927
At time: 417.2870283126831 and batch: 300, loss is 5.05425705909729 and perplexity is 156.68807710913762
At time: 418.1288421154022 and batch: 350, loss is 5.067092227935791 and perplexity is 158.7121569496316
At time: 418.9718425273895 and batch: 400, loss is 5.0413113212585445 and perplexity is 154.6727077089689
At time: 419.8100299835205 and batch: 450, loss is 5.0101359176635745 and perplexity is 149.92511223524647
At time: 420.64991307258606 and batch: 500, loss is 5.008958797454834 and perplexity is 149.74873618410956
At time: 421.49597334861755 and batch: 550, loss is 5.012941932678222 and perplexity is 150.34639513790455
At time: 422.3383195400238 and batch: 600, loss is 5.039114980697632 and perplexity is 154.33336655814094
At time: 423.1832230091095 and batch: 650, loss is 5.043719749450684 and perplexity is 155.04567477067903
At time: 424.0252001285553 and batch: 700, loss is 5.058916015625 and perplexity is 157.419783218751
At time: 424.8692853450775 and batch: 750, loss is 5.022828369140625 and perplexity is 151.84015704500572
At time: 425.710408449173 and batch: 800, loss is 5.044539089202881 and perplexity is 155.17276191208867
At time: 426.55003118515015 and batch: 850, loss is 5.061525154113769 and perplexity is 157.83104952610515
At time: 427.39118123054504 and batch: 900, loss is 5.046962509155273 and perplexity is 155.54926670972705
At time: 428.2355008125305 and batch: 950, loss is 5.021529283523559 and perplexity is 151.64303174993609
At time: 429.07662558555603 and batch: 1000, loss is 5.021293573379516 and perplexity is 151.60729216133646
At time: 429.91677832603455 and batch: 1050, loss is 5.01701623916626 and perplexity is 150.96020200061167
At time: 430.7598853111267 and batch: 1100, loss is 4.989811868667602 and perplexity is 146.90878275069142
At time: 431.60143280029297 and batch: 1150, loss is 5.008011455535889 and perplexity is 149.60694010430305
At time: 432.44581604003906 and batch: 1200, loss is 5.030540838241577 and perplexity is 153.01574708822508
At time: 433.34331154823303 and batch: 1250, loss is 5.069663038253784 and perplexity is 159.1207007195706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071288616987911 and perplexity of 159.37957429950652
Finished 19 epochs...
Completing Train Step...
At time: 435.80589842796326 and batch: 50, loss is 5.068952388763428 and perplexity is 159.00766184496567
At time: 436.65219736099243 and batch: 100, loss is 5.094264869689941 and perplexity is 163.08391261971306
At time: 437.4970541000366 and batch: 150, loss is 5.025450944900513 and perplexity is 152.23889198800916
At time: 438.34234046936035 and batch: 200, loss is 5.064645614624023 and perplexity is 158.32432430541547
At time: 439.1873297691345 and batch: 250, loss is 5.07840371131897 and perplexity is 160.51761885118026
At time: 440.03251218795776 and batch: 300, loss is 5.051802597045898 and perplexity is 156.30396375862597
At time: 440.8802809715271 and batch: 350, loss is 5.064669046401978 and perplexity is 158.32803416929158
At time: 441.725102186203 and batch: 400, loss is 5.038946256637574 and perplexity is 154.30732900257556
At time: 442.56892442703247 and batch: 450, loss is 5.007960548400879 and perplexity is 149.59932423745738
At time: 443.41259002685547 and batch: 500, loss is 5.006895666122436 and perplexity is 149.44010335900185
At time: 444.257178068161 and batch: 550, loss is 5.011353960037232 and perplexity is 150.1078386364826
At time: 445.10145592689514 and batch: 600, loss is 5.037801017761231 and perplexity is 154.13071140447087
At time: 445.9492313861847 and batch: 650, loss is 5.042523307800293 and perplexity is 154.8602825952459
At time: 446.7934708595276 and batch: 700, loss is 5.05776005744934 and perplexity is 157.23791766806517
At time: 447.6388535499573 and batch: 750, loss is 5.021831455230713 and perplexity is 151.68886090750664
At time: 448.4880621433258 and batch: 800, loss is 5.043391427993774 and perplexity is 154.99477830450894
At time: 449.33151412010193 and batch: 850, loss is 5.060776586532593 and perplexity is 157.71294652867934
At time: 450.1775770187378 and batch: 900, loss is 5.046528358459472 and perplexity is 155.48174954472628
At time: 451.02695083618164 and batch: 950, loss is 5.021219034194946 and perplexity is 151.59599189856522
At time: 451.869469165802 and batch: 1000, loss is 5.021056880950928 and perplexity is 151.57141210959887
At time: 452.71732878685 and batch: 1050, loss is 5.016971616744995 and perplexity is 150.95346594117456
At time: 453.56396198272705 and batch: 1100, loss is 4.989776515960694 and perplexity is 146.90358921935578
At time: 454.4083664417267 and batch: 1150, loss is 5.00825756072998 and perplexity is 149.64376368038563
At time: 455.31954860687256 and batch: 1200, loss is 5.0309319591522215 and perplexity is 153.07560645193053
At time: 456.16680932044983 and batch: 1250, loss is 5.069187164306641 and perplexity is 159.0449973377091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071192831888686 and perplexity of 159.3643088422812
Finished 20 epochs...
Completing Train Step...
At time: 458.59456634521484 and batch: 50, loss is 5.066779460906982 and perplexity is 158.6625247819241
At time: 459.4654076099396 and batch: 100, loss is 5.091986541748047 and perplexity is 162.71277692966476
At time: 460.309597492218 and batch: 150, loss is 5.0231271076202395 and perplexity is 151.88552431880353
At time: 461.15601801872253 and batch: 200, loss is 5.062498245239258 and perplexity is 157.98470826958032
At time: 462.00064992904663 and batch: 250, loss is 5.076177015304565 and perplexity is 160.1605925510789
At time: 462.8433575630188 and batch: 300, loss is 5.049821367263794 and perplexity is 155.99459625581747
At time: 463.68595719337463 and batch: 350, loss is 5.0626913356781005 and perplexity is 158.0152165515645
At time: 464.52711367607117 and batch: 400, loss is 5.03677978515625 and perplexity is 153.97338844189815
At time: 465.36981225013733 and batch: 450, loss is 5.005953598022461 and perplexity is 149.29938689742215
At time: 466.21016550064087 and batch: 500, loss is 5.004973907470703 and perplexity is 149.1531913235998
At time: 467.05153703689575 and batch: 550, loss is 5.009799966812134 and perplexity is 149.87475322568838
At time: 467.891592502594 and batch: 600, loss is 5.036575756072998 and perplexity is 153.941976597183
At time: 468.7348439693451 and batch: 650, loss is 5.041268815994263 and perplexity is 154.66613344437198
At time: 469.57359170913696 and batch: 700, loss is 5.056480770111084 and perplexity is 157.03689380195777
At time: 470.41788148880005 and batch: 750, loss is 5.020596475601196 and perplexity is 151.50164388266208
At time: 471.26081347465515 and batch: 800, loss is 5.042367696762085 and perplexity is 154.83618650075184
At time: 472.10486483573914 and batch: 850, loss is 5.060067501068115 and perplexity is 157.60115421057392
At time: 472.9460999965668 and batch: 900, loss is 5.046138591766358 and perplexity is 155.42115974607634
At time: 473.785781621933 and batch: 950, loss is 5.020517272949219 and perplexity is 151.48964502586455
At time: 474.63073348999023 and batch: 1000, loss is 5.020570898056031 and perplexity is 151.49776889207962
At time: 475.47158765792847 and batch: 1050, loss is 5.016616773605347 and perplexity is 150.8999106418064
At time: 476.3477363586426 and batch: 1100, loss is 4.98943603515625 and perplexity is 146.85357988121564
At time: 477.1920382976532 and batch: 1150, loss is 5.008125076293945 and perplexity is 149.62393952397338
At time: 478.03721714019775 and batch: 1200, loss is 5.030866813659668 and perplexity is 153.06563459096475
At time: 478.8872835636139 and batch: 1250, loss is 5.068408870697022 and perplexity is 158.92126179017825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.070960720090101 and perplexity of 159.32732279854017
Finished 21 epochs...
Completing Train Step...
At time: 481.3373305797577 and batch: 50, loss is 5.0646410655975345 and perplexity is 158.32360408550855
At time: 482.2153799533844 and batch: 100, loss is 5.089822673797608 and perplexity is 162.361068628934
At time: 483.06501841545105 and batch: 150, loss is 5.021094570159912 and perplexity is 151.57712482387902
At time: 483.9056041240692 and batch: 200, loss is 5.060470361709594 and perplexity is 157.66465830344796
At time: 484.751238822937 and batch: 250, loss is 5.074144401550293 and perplexity is 159.83537855695496
At time: 485.5972535610199 and batch: 300, loss is 5.047704219818115 and perplexity is 155.66468205654843
At time: 486.4411542415619 and batch: 350, loss is 5.060675716400146 and perplexity is 157.697038805195
At time: 487.2850112915039 and batch: 400, loss is 5.034804725646973 and perplexity is 153.66958195358606
At time: 488.12959122657776 and batch: 450, loss is 5.004002275466919 and perplexity is 149.0083396920797
At time: 488.97183418273926 and batch: 500, loss is 5.003154268264771 and perplexity is 148.8820331088558
At time: 489.8138659000397 and batch: 550, loss is 5.008369808197021 and perplexity is 149.6605617565704
At time: 490.657506942749 and batch: 600, loss is 5.035446710586548 and perplexity is 153.76826718469775
At time: 491.50304317474365 and batch: 650, loss is 5.040194320678711 and perplexity is 154.50003466067457
At time: 492.3565995693207 and batch: 700, loss is 5.055386409759522 and perplexity is 156.8651328529768
At time: 493.1987102031708 and batch: 750, loss is 5.019662828445434 and perplexity is 151.36026081497758
At time: 494.0432085990906 and batch: 800, loss is 5.041345167160034 and perplexity is 154.67794283479037
At time: 494.8862233161926 and batch: 850, loss is 5.05924277305603 and perplexity is 157.47122970751238
At time: 495.7297601699829 and batch: 900, loss is 5.0456204605102535 and perplexity is 155.34065204393144
At time: 496.5772397518158 and batch: 950, loss is 5.020097723007202 and perplexity is 151.4261008849803
At time: 497.4225673675537 and batch: 1000, loss is 5.02011926651001 and perplexity is 151.4293631687502
At time: 498.3002061843872 and batch: 1050, loss is 5.016266889572144 and perplexity is 150.84712240787968
At time: 499.14534878730774 and batch: 1100, loss is 4.989140892028809 and perplexity is 146.8102434519128
At time: 499.9875295162201 and batch: 1150, loss is 5.007969150543213 and perplexity is 149.60061111767246
At time: 500.83675622940063 and batch: 1200, loss is 5.0306937122344975 and perplexity is 153.03914100457735
At time: 501.68178725242615 and batch: 1250, loss is 5.067729978561402 and perplexity is 158.81340801005007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0709393355098085 and perplexity of 159.32391568704298
Finished 22 epochs...
Completing Train Step...
At time: 504.1520037651062 and batch: 50, loss is 5.062811155319213 and perplexity is 158.0341510124396
At time: 504.99380230903625 and batch: 100, loss is 5.0878989696502686 and perplexity is 162.04903419504058
At time: 505.8431673049927 and batch: 150, loss is 5.019326400756836 and perplexity is 151.3093475970743
At time: 506.6850140094757 and batch: 200, loss is 5.058685693740845 and perplexity is 157.38353017277197
At time: 507.52910327911377 and batch: 250, loss is 5.07235933303833 and perplexity is 159.5503159594239
At time: 508.3724353313446 and batch: 300, loss is 5.046108722686768 and perplexity is 155.41651752841562
At time: 509.2170696258545 and batch: 350, loss is 5.059033145904541 and perplexity is 157.43822292186746
At time: 510.0631010532379 and batch: 400, loss is 5.033124027252197 and perplexity is 153.41152665118207
At time: 510.90504598617554 and batch: 450, loss is 5.002315149307251 and perplexity is 148.75715577323737
At time: 511.74469232559204 and batch: 500, loss is 5.001619958877564 and perplexity is 148.65377716027095
At time: 512.5869336128235 and batch: 550, loss is 5.007098016738891 and perplexity is 149.4703457157162
At time: 513.4292163848877 and batch: 600, loss is 5.0344260597229 and perplexity is 153.61140353512118
At time: 514.2739448547363 and batch: 650, loss is 5.0392117691040035 and perplexity is 154.3483049616605
At time: 515.1166853904724 and batch: 700, loss is 5.054373054504395 and perplexity is 156.70625326058496
At time: 515.9602134227753 and batch: 750, loss is 5.01874303817749 and perplexity is 151.2211051269609
At time: 516.8034057617188 and batch: 800, loss is 5.040337591171265 and perplexity is 154.52217154248316
At time: 517.6486909389496 and batch: 850, loss is 5.058576564788819 and perplexity is 157.3663560101743
At time: 518.4903566837311 and batch: 900, loss is 5.045128831863403 and perplexity is 155.2643008991091
At time: 519.3342840671539 and batch: 950, loss is 5.01950927734375 and perplexity is 151.3370210644685
At time: 520.212518453598 and batch: 1000, loss is 5.019503784179688 and perplexity is 151.3361897476664
At time: 521.053564786911 and batch: 1050, loss is 5.015829734802246 and perplexity is 150.7811932804599
At time: 521.8964288234711 and batch: 1100, loss is 4.988872299194336 and perplexity is 146.7708165676207
At time: 522.7381937503815 and batch: 1150, loss is 5.007611341476441 and perplexity is 149.5470922379611
At time: 523.5824129581451 and batch: 1200, loss is 5.030425500869751 and perplexity is 152.99809967183086
At time: 524.425892829895 and batch: 1250, loss is 5.066844930648804 and perplexity is 158.6729127165023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.070897457373404 and perplexity of 159.31724363807675
Finished 23 epochs...
Completing Train Step...
At time: 526.828106880188 and batch: 50, loss is 5.060988321304321 and perplexity is 157.74634337892363
At time: 527.7249186038971 and batch: 100, loss is 5.086121425628662 and perplexity is 161.76124076162617
At time: 528.5644364356995 and batch: 150, loss is 5.017824878692627 and perplexity is 151.08232375644252
At time: 529.4007925987244 and batch: 200, loss is 5.057169942855835 and perplexity is 157.14515665070377
At time: 530.2492110729218 and batch: 250, loss is 5.0709224319458 and perplexity is 159.32122256779786
At time: 531.0891423225403 and batch: 300, loss is 5.044317979812622 and perplexity is 155.1384555501866
At time: 531.9304208755493 and batch: 350, loss is 5.057366008758545 and perplexity is 157.17597047837071
At time: 532.7709217071533 and batch: 400, loss is 5.03144570350647 and perplexity is 153.15426838485018
At time: 533.6108331680298 and batch: 450, loss is 5.000689325332641 and perplexity is 148.5154993216437
At time: 534.4500234127045 and batch: 500, loss is 5.000264863967896 and perplexity is 148.45247360705247
At time: 535.2879712581635 and batch: 550, loss is 5.005835208892822 and perplexity is 149.2817125191996
At time: 536.1275107860565 and batch: 600, loss is 5.033436737060547 and perplexity is 153.45950744192078
At time: 536.9679870605469 and batch: 650, loss is 5.038223714828491 and perplexity is 154.1958757755803
At time: 537.807772397995 and batch: 700, loss is 5.053483848571777 and perplexity is 156.5669710649621
At time: 538.6470649242401 and batch: 750, loss is 5.017542314529419 and perplexity is 151.0396393368824
At time: 539.487636089325 and batch: 800, loss is 5.039459476470947 and perplexity is 154.38654290958993
At time: 540.3273553848267 and batch: 850, loss is 5.057858991622925 and perplexity is 157.25347464105118
At time: 541.1656308174133 and batch: 900, loss is 5.044513072967529 and perplexity is 155.16872495350805
At time: 542.0391728878021 and batch: 950, loss is 5.018944616317749 and perplexity is 151.25159106864677
At time: 542.8840353488922 and batch: 1000, loss is 5.018985815048218 and perplexity is 151.2578225705443
At time: 543.7243866920471 and batch: 1050, loss is 5.015246210098266 and perplexity is 150.6932343948724
At time: 544.5709276199341 and batch: 1100, loss is 4.9882394981384275 and perplexity is 146.6779692199683
At time: 545.4117550849915 and batch: 1150, loss is 5.007152996063232 and perplexity is 149.4785637202408
At time: 546.25310587883 and batch: 1200, loss is 5.030094499588013 and perplexity is 152.9474654851886
At time: 547.0906093120575 and batch: 1250, loss is 5.0661543846130375 and perplexity is 158.56337958882858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0708693901117705 and perplexity of 159.31277210206906
Finished 24 epochs...
Completing Train Step...
At time: 549.5582456588745 and batch: 50, loss is 5.059435033798218 and perplexity is 157.50150815359422
At time: 550.4089789390564 and batch: 100, loss is 5.084436883926392 and perplexity is 161.4889765903107
At time: 551.2546153068542 and batch: 150, loss is 5.01617280960083 and perplexity is 150.83293138248686
At time: 552.1042776107788 and batch: 200, loss is 5.055561199188232 and perplexity is 156.8925536162927
At time: 552.9427330493927 and batch: 250, loss is 5.069156751632691 and perplexity is 159.04016042761376
At time: 553.7911007404327 and batch: 300, loss is 5.042825660705566 and perplexity is 154.90711213076634
At time: 554.6413748264313 and batch: 350, loss is 5.055875959396363 and perplexity is 156.94194492192415
At time: 555.4866554737091 and batch: 400, loss is 5.0299425792694095 and perplexity is 152.92423142241032
At time: 556.334810256958 and batch: 450, loss is 4.999296731948853 and perplexity is 148.30882156233955
At time: 557.179440498352 and batch: 500, loss is 4.9990272712707515 and perplexity is 148.2688635505112
At time: 558.0229518413544 and batch: 550, loss is 5.004680395126343 and perplexity is 149.10941944485324
At time: 558.8663568496704 and batch: 600, loss is 5.03238284111023 and perplexity is 153.2978622820413
At time: 559.7099025249481 and batch: 650, loss is 5.037446870803833 and perplexity is 154.07613614638657
At time: 560.5539667606354 and batch: 700, loss is 5.052559289932251 and perplexity is 156.42228261598407
At time: 561.3996706008911 and batch: 750, loss is 5.016695117950439 and perplexity is 150.91173325959167
At time: 562.2456686496735 and batch: 800, loss is 5.038450498580932 and perplexity is 154.23084886041346
At time: 563.1470663547516 and batch: 850, loss is 5.057058572769165 and perplexity is 157.12765635551057
At time: 563.9944274425507 and batch: 900, loss is 5.043892850875855 and perplexity is 155.07251572098474
At time: 564.8427188396454 and batch: 950, loss is 5.018305311203003 and perplexity is 151.1549260553741
At time: 565.6895842552185 and batch: 1000, loss is 5.018357934951783 and perplexity is 151.1628806035269
At time: 566.5347721576691 and batch: 1050, loss is 5.014706983566284 and perplexity is 150.61199850893735
At time: 567.3786060810089 and batch: 1100, loss is 4.987629661560058 and perplexity is 146.58854689836974
At time: 568.2234313488007 and batch: 1150, loss is 5.0066538906097415 and perplexity is 149.40397676883347
At time: 569.0709187984467 and batch: 1200, loss is 5.029682455062866 and perplexity is 152.88445730138037
At time: 569.9171559810638 and batch: 1250, loss is 5.065458354949951 and perplexity is 158.4530531728386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.070814146612682 and perplexity of 159.30397135018313
Finished 25 epochs...
Completing Train Step...
At time: 572.342077255249 and batch: 50, loss is 5.057789859771728 and perplexity is 157.24260379300742
At time: 573.2216572761536 and batch: 100, loss is 5.08294605255127 and perplexity is 161.24840312906332
At time: 574.0645492076874 and batch: 150, loss is 5.01480767250061 and perplexity is 150.6271642340615
At time: 574.9050681591034 and batch: 200, loss is 5.054549236297607 and perplexity is 156.73386448151777
At time: 575.747047662735 and batch: 250, loss is 5.06790135383606 and perplexity is 158.84062703373442
At time: 576.5885634422302 and batch: 300, loss is 5.041192712783814 and perplexity is 154.6543633029476
At time: 577.4357671737671 and batch: 350, loss is 5.054280471801758 and perplexity is 156.69174564372557
At time: 578.276121377945 and batch: 400, loss is 5.028462600708008 and perplexity is 152.69807423356187
At time: 579.1206459999084 and batch: 450, loss is 4.997762203216553 and perplexity is 148.08141194232147
At time: 579.9654304981232 and batch: 500, loss is 4.998015928268432 and perplexity is 148.11898867312635
At time: 580.810476064682 and batch: 550, loss is 5.003503074645996 and perplexity is 148.9339731700398
At time: 581.651305437088 and batch: 600, loss is 5.031528778076172 and perplexity is 153.1669921382972
At time: 582.4940528869629 and batch: 650, loss is 5.036672601699829 and perplexity is 153.95688592634212
At time: 583.3415727615356 and batch: 700, loss is 5.051750707626343 and perplexity is 156.29585344709375
At time: 584.1818742752075 and batch: 750, loss is 5.015674428939819 and perplexity is 150.75777789552012
At time: 585.0530705451965 and batch: 800, loss is 5.0375986003875735 and perplexity is 154.09951582803828
At time: 585.8957169055939 and batch: 850, loss is 5.056611785888672 and perplexity is 157.05746946054774
At time: 586.7367134094238 and batch: 900, loss is 5.043473148345948 and perplexity is 155.0074450499354
At time: 587.5767524242401 and batch: 950, loss is 5.0176065731048585 and perplexity is 151.04934524078152
At time: 588.4217948913574 and batch: 1000, loss is 5.017837800979614 and perplexity is 151.08427609820313
At time: 589.2619099617004 and batch: 1050, loss is 5.014355640411377 and perplexity is 150.55909130904996
At time: 590.1036779880524 and batch: 1100, loss is 4.9872230052947994 and perplexity is 146.52894786633954
At time: 590.9519114494324 and batch: 1150, loss is 5.006509704589844 and perplexity is 149.3824363570167
At time: 591.7966201305389 and batch: 1200, loss is 5.029319591522217 and perplexity is 152.82899116982085
At time: 592.6412816047668 and batch: 1250, loss is 5.064894971847534 and perplexity is 158.36380854197367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.070839986313868 and perplexity of 159.30808777038385
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 595.0499536991119 and batch: 50, loss is 5.058503246307373 and perplexity is 157.35481857087984
At time: 595.9239177703857 and batch: 100, loss is 5.084411354064941 and perplexity is 161.48485385173925
At time: 596.7706401348114 and batch: 150, loss is 5.0162125587463375 and perplexity is 150.83892698178292
At time: 597.6142241954803 and batch: 200, loss is 5.055056171417236 and perplexity is 156.8133385242484
At time: 598.4590182304382 and batch: 250, loss is 5.06881965637207 and perplexity is 158.98655777839232
At time: 599.3051915168762 and batch: 300, loss is 5.041411075592041 and perplexity is 154.6881377514305
At time: 600.1519548892975 and batch: 350, loss is 5.052631740570068 and perplexity is 156.43361592067603
At time: 600.9996964931488 and batch: 400, loss is 5.025524959564209 and perplexity is 152.2501603154068
At time: 601.8486444950104 and batch: 450, loss is 4.994315547943115 and perplexity is 147.57190491488166
At time: 602.6942656040192 and batch: 500, loss is 4.994375 and perplexity is 147.5806786289723
At time: 603.5401473045349 and batch: 550, loss is 4.9989268016815185 and perplexity is 148.2539677869924
At time: 604.3872175216675 and batch: 600, loss is 5.02582615852356 and perplexity is 152.29602481207735
At time: 605.2319362163544 and batch: 650, loss is 5.030297966003418 and perplexity is 152.97858832385498
At time: 606.0787003040314 and batch: 700, loss is 5.046076412200928 and perplexity is 155.4114960263508
At time: 606.9771070480347 and batch: 750, loss is 5.008448677062988 and perplexity is 149.672365780835
At time: 607.8200030326843 and batch: 800, loss is 5.028920364379883 and perplexity is 152.76798986590867
At time: 608.668755531311 and batch: 850, loss is 5.044906311035156 and perplexity is 155.2297552019869
At time: 609.5120842456818 and batch: 900, loss is 5.031077966690064 and perplexity is 153.09795827605998
At time: 610.3598198890686 and batch: 950, loss is 5.005686597824097 and perplexity is 149.2595292527412
At time: 611.2037122249603 and batch: 1000, loss is 5.005279760360718 and perplexity is 149.19881723527394
At time: 612.0460226535797 and batch: 1050, loss is 5.000034284591675 and perplexity is 148.4182474743615
At time: 612.8902428150177 and batch: 1100, loss is 4.971313743591309 and perplexity is 144.21622611522827
At time: 613.7361562252045 and batch: 1150, loss is 4.989005470275879 and perplexity is 146.79036349751604
At time: 614.58136677742 and batch: 1200, loss is 5.015073928833008 and perplexity is 150.66727501000705
At time: 615.4252400398254 and batch: 1250, loss is 5.056162509918213 and perplexity is 156.98692316210307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068791967238823 and perplexity of 158.98215563935557
Finished 27 epochs...
Completing Train Step...
At time: 617.8734273910522 and batch: 50, loss is 5.056217994689941 and perplexity is 156.99563378735033
At time: 618.7150537967682 and batch: 100, loss is 5.081485328674316 and perplexity is 161.01303568176766
At time: 619.5550379753113 and batch: 150, loss is 5.013238086700439 and perplexity is 150.3909274214324
At time: 620.398458480835 and batch: 200, loss is 5.051951742172241 and perplexity is 156.3272774715687
At time: 621.2394545078278 and batch: 250, loss is 5.066221342086792 and perplexity is 158.57399694760767
At time: 622.0801997184753 and batch: 300, loss is 5.039047784805298 and perplexity is 154.32299633828015
At time: 622.9251940250397 and batch: 350, loss is 5.050447778701782 and perplexity is 156.0923436670284
At time: 623.7648818492889 and batch: 400, loss is 5.023319368362427 and perplexity is 151.91472874977939
At time: 624.6089088916779 and batch: 450, loss is 4.992503843307495 and perplexity is 147.30479025016018
At time: 625.45472407341 and batch: 500, loss is 4.992834405899048 and perplexity is 147.35349175237732
At time: 626.3011512756348 and batch: 550, loss is 4.997283658981323 and perplexity is 148.01056538924863
At time: 627.1432111263275 and batch: 600, loss is 5.024272518157959 and perplexity is 152.0595952712057
At time: 627.991941690445 and batch: 650, loss is 5.029332447052002 and perplexity is 152.8309558800975
At time: 628.8940055370331 and batch: 700, loss is 5.045158891677857 and perplexity is 155.2689681853339
At time: 629.7339065074921 and batch: 750, loss is 5.007764854431152 and perplexity is 149.57005141617964
At time: 630.5781235694885 and batch: 800, loss is 5.028457746505738 and perplexity is 152.69733300802227
At time: 631.4209189414978 and batch: 850, loss is 5.044780578613281 and perplexity is 155.2102390158527
At time: 632.2654159069061 and batch: 900, loss is 5.03102367401123 and perplexity is 153.0896464034202
At time: 633.1083700656891 and batch: 950, loss is 5.005951461791992 and perplexity is 149.29906795986352
At time: 633.9534471035004 and batch: 1000, loss is 5.005884447097778 and perplexity is 149.2890630637191
At time: 634.7948100566864 and batch: 1050, loss is 5.0010426902770995 and perplexity is 148.5679887662313
At time: 635.6430456638336 and batch: 1100, loss is 4.972310247421265 and perplexity is 144.36000976547464
At time: 636.4904117584229 and batch: 1150, loss is 4.990085878372192 and perplexity is 146.94904269841092
At time: 637.3311519622803 and batch: 1200, loss is 5.016417045593261 and perplexity is 150.86977471222485
At time: 638.1706686019897 and batch: 1250, loss is 5.056851358413696 and perplexity is 157.09510062259662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068705537893476 and perplexity of 158.96841550950552
Finished 28 epochs...
Completing Train Step...
At time: 640.6171233654022 and batch: 50, loss is 5.055056858062744 and perplexity is 156.81344619945978
At time: 641.5210766792297 and batch: 100, loss is 5.079981126785278 and perplexity is 160.77102163396162
At time: 642.3633396625519 and batch: 150, loss is 5.011780481338501 and perplexity is 150.1718764829306
At time: 643.2091362476349 and batch: 200, loss is 5.050388250350952 and perplexity is 156.08305202379353
At time: 644.0511198043823 and batch: 250, loss is 5.0649849224090575 and perplexity is 158.37805409616524
At time: 644.8930945396423 and batch: 300, loss is 5.0378534126281735 and perplexity is 154.13878727415192
At time: 645.7373571395874 and batch: 350, loss is 5.049190416336059 and perplexity is 155.89620236471086
At time: 646.5814414024353 and batch: 400, loss is 5.022079315185547 and perplexity is 151.7264631615739
At time: 647.4269366264343 and batch: 450, loss is 4.9913571834564205 and perplexity is 147.13597856459702
At time: 648.2715866565704 and batch: 500, loss is 4.992053518295288 and perplexity is 147.23847015261273
At time: 649.1152067184448 and batch: 550, loss is 4.996454229354859 and perplexity is 147.88785193940552
At time: 649.9897260665894 and batch: 600, loss is 5.023483924865722 and perplexity is 151.93972936329192
At time: 650.8309197425842 and batch: 650, loss is 5.028775634765625 and perplexity is 152.74588141357694
At time: 651.6790838241577 and batch: 700, loss is 5.044702577590942 and perplexity is 155.19813293068165
At time: 652.5238797664642 and batch: 750, loss is 5.007567663192749 and perplexity is 149.54056042029134
At time: 653.3677413463593 and batch: 800, loss is 5.028300752639771 and perplexity is 152.67336234506325
At time: 654.2129156589508 and batch: 850, loss is 5.044923849105835 and perplexity is 155.2324776562784
At time: 655.0579831600189 and batch: 900, loss is 5.03116024017334 and perplexity is 153.1105546965384
At time: 655.9057502746582 and batch: 950, loss is 5.006334886550904 and perplexity is 149.3563238949719
At time: 656.7512717247009 and batch: 1000, loss is 5.0064707851409915 and perplexity is 149.37662258806057
At time: 657.5941443443298 and batch: 1050, loss is 5.001791591644287 and perplexity is 148.67929320896124
At time: 658.4436640739441 and batch: 1100, loss is 4.973007011413574 and perplexity is 144.46062967230185
At time: 659.2931878566742 and batch: 1150, loss is 4.990746021270752 and perplexity is 147.04608209180756
At time: 660.1367359161377 and batch: 1200, loss is 5.017179279327393 and perplexity is 150.9848165827987
At time: 660.9855332374573 and batch: 1250, loss is 5.057217035293579 and perplexity is 157.1525571734631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068685935361542 and perplexity of 158.9652993566064
Finished 29 epochs...
Completing Train Step...
At time: 663.4921402931213 and batch: 50, loss is 5.054154243469238 and perplexity is 156.67196795423197
At time: 664.3338572978973 and batch: 100, loss is 5.07888503074646 and perplexity is 160.59489769600057
At time: 665.17928647995 and batch: 150, loss is 5.010693292617798 and perplexity is 150.00870003052339
At time: 666.0200791358948 and batch: 200, loss is 5.049247331619263 and perplexity is 155.90507549372478
At time: 666.8623144626617 and batch: 250, loss is 5.064097852706909 and perplexity is 158.23762401773382
At time: 667.7030847072601 and batch: 300, loss is 5.0369935131073 and perplexity is 154.00630037570073
At time: 668.5463011264801 and batch: 350, loss is 5.048226194381714 and perplexity is 155.74595627072355
At time: 669.39319896698 and batch: 400, loss is 5.021152973175049 and perplexity is 151.5859776435077
At time: 670.2343418598175 and batch: 450, loss is 4.990482931137085 and perplexity is 147.0074008069701
At time: 671.0769889354706 and batch: 500, loss is 4.991442222595214 and perplexity is 147.14849141353315
At time: 671.9756898880005 and batch: 550, loss is 4.9958366012573245 and perplexity is 147.79654044793784
At time: 672.8201296329498 and batch: 600, loss is 5.022910003662109 and perplexity is 151.85255294952813
At time: 673.664244890213 and batch: 650, loss is 5.028350591659546 and perplexity is 152.68097162540627
At time: 674.506331205368 and batch: 700, loss is 5.04436318397522 and perplexity is 155.1454686126651
At time: 675.3491234779358 and batch: 750, loss is 5.007439861297607 and perplexity is 149.52145007446433
At time: 676.2026438713074 and batch: 800, loss is 5.028156270980835 and perplexity is 152.65130543784403
At time: 677.0500776767731 and batch: 850, loss is 5.0450117301940915 and perplexity is 155.24612025480198
At time: 677.8924541473389 and batch: 900, loss is 5.0312579727172855 and perplexity is 153.12551931180687
At time: 678.7343792915344 and batch: 950, loss is 5.006616258621216 and perplexity is 149.3983545058832
At time: 679.574490070343 and batch: 1000, loss is 5.006937236785888 and perplexity is 149.44631581237184
At time: 680.4192175865173 and batch: 1050, loss is 5.002333364486694 and perplexity is 148.75986543620164
At time: 681.2631058692932 and batch: 1100, loss is 4.973505430221557 and perplexity is 144.53264951367444
At time: 682.1092276573181 and batch: 1150, loss is 4.991200914382935 and perplexity is 147.11298755798873
At time: 682.9529314041138 and batch: 1200, loss is 5.017693147659302 and perplexity is 151.06242283663008
At time: 683.7937536239624 and batch: 1250, loss is 5.057433004379273 and perplexity is 157.1865009328196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068691281506615 and perplexity of 158.96614921042996
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 686.2704977989197 and batch: 50, loss is 5.05403920173645 and perplexity is 156.6539451762649
At time: 687.143239736557 and batch: 100, loss is 5.078892726898193 and perplexity is 160.59613366345684
At time: 687.9907329082489 and batch: 150, loss is 5.010652799606323 and perplexity is 150.0026258494934
At time: 688.8298444747925 and batch: 200, loss is 5.049129295349121 and perplexity is 155.88667412615303
At time: 689.6689825057983 and batch: 250, loss is 5.064052410125733 and perplexity is 158.23043345503936
At time: 690.5085296630859 and batch: 300, loss is 5.03717643737793 and perplexity is 154.0344744426512
At time: 691.3470830917358 and batch: 350, loss is 5.047439956665039 and perplexity is 155.6235510517895
At time: 692.1876950263977 and batch: 400, loss is 5.020169315338134 and perplexity is 151.4369422205802
At time: 693.0302238464355 and batch: 450, loss is 4.989534606933594 and perplexity is 146.86805621306104
At time: 693.9265072345734 and batch: 500, loss is 4.990605401992798 and perplexity is 147.0254060316782
At time: 694.7683162689209 and batch: 550, loss is 4.994156093597412 and perplexity is 147.54837580929816
At time: 695.6072099208832 and batch: 600, loss is 5.021011886596679 and perplexity is 151.5645924052137
At time: 696.4468750953674 and batch: 650, loss is 5.026762504577636 and perplexity is 152.43869337711197
At time: 697.2846250534058 and batch: 700, loss is 5.042943782806397 and perplexity is 154.92541116502372
At time: 698.1275949478149 and batch: 750, loss is 5.00574857711792 and perplexity is 149.26878053965189
At time: 698.975843667984 and batch: 800, loss is 5.026643676757812 and perplexity is 152.4205804956996
At time: 699.8385064601898 and batch: 850, loss is 5.042384786605835 and perplexity is 154.83883264959704
At time: 700.698442697525 and batch: 900, loss is 5.028266897201538 and perplexity is 152.66819360897006
At time: 701.53919672966 and batch: 950, loss is 5.004055662155151 and perplexity is 149.01629496620578
At time: 702.3825101852417 and batch: 1000, loss is 5.0038371086120605 and perplexity is 148.98373048563357
At time: 703.2246577739716 and batch: 1050, loss is 4.9993173789978025 and perplexity is 148.31188373345032
At time: 704.0661959648132 and batch: 1100, loss is 4.9702424049377445 and perplexity is 144.06180443144922
At time: 704.9083561897278 and batch: 1150, loss is 4.987324695587159 and perplexity is 146.54384919553456
At time: 705.752911567688 and batch: 1200, loss is 5.014386587142944 and perplexity is 150.5637506929296
At time: 706.5983784198761 and batch: 1250, loss is 5.0558864116668705 and perplexity is 156.94358533015946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.06924460752167 and perplexity of 159.0541336560977
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 709.0576326847076 and batch: 50, loss is 5.054102220535278 and perplexity is 156.66381763079343
At time: 709.962769985199 and batch: 100, loss is 5.078460092544556 and perplexity is 160.52666928639414
At time: 710.8084638118744 and batch: 150, loss is 5.010224981307983 and perplexity is 149.93846570677616
At time: 711.6547701358795 and batch: 200, loss is 5.048996391296387 and perplexity is 155.86595753208462
At time: 712.5000932216644 and batch: 250, loss is 5.0636491870880125 and perplexity is 158.16664416052268
At time: 713.347957611084 and batch: 300, loss is 5.036676530838013 and perplexity is 153.95749084540967
At time: 714.1951503753662 and batch: 350, loss is 5.046952104568481 and perplexity is 155.5476482923006
At time: 715.0686600208282 and batch: 400, loss is 5.0196780014038085 and perplexity is 151.36255741533748
At time: 715.9165122509003 and batch: 450, loss is 4.98925048828125 and perplexity is 146.82633418614114
At time: 716.7603054046631 and batch: 500, loss is 4.990438022613525 and perplexity is 147.0007990698858
At time: 717.6069386005402 and batch: 550, loss is 4.9937428283691405 and perplexity is 147.48741179410976
At time: 718.4515314102173 and batch: 600, loss is 5.020555238723755 and perplexity is 151.4953965567522
At time: 719.2956652641296 and batch: 650, loss is 5.026601400375366 and perplexity is 152.414136841154
At time: 720.1404709815979 and batch: 700, loss is 5.04294487953186 and perplexity is 154.92558107576022
At time: 720.9840369224548 and batch: 750, loss is 5.005642204284668 and perplexity is 149.2529032410214
At time: 721.8315813541412 and batch: 800, loss is 5.02626051902771 and perplexity is 152.36219055904814
At time: 722.6747736930847 and batch: 850, loss is 5.041754865646363 and perplexity is 154.741327137178
At time: 723.5264811515808 and batch: 900, loss is 5.02757453918457 and perplexity is 152.56252914422944
At time: 724.3801143169403 and batch: 950, loss is 5.003459930419922 and perplexity is 148.92754766764384
At time: 725.2274203300476 and batch: 1000, loss is 5.00318190574646 and perplexity is 148.8861478901805
At time: 726.0753467082977 and batch: 1050, loss is 4.998628435134887 and perplexity is 148.2097403609197
At time: 726.9225378036499 and batch: 1100, loss is 4.969622020721435 and perplexity is 143.97245847908187
At time: 727.7701709270477 and batch: 1150, loss is 4.986685762405395 and perplexity is 146.45024737353947
At time: 728.6184403896332 and batch: 1200, loss is 5.013667907714844 and perplexity is 150.45558249647348
At time: 729.4636225700378 and batch: 1250, loss is 5.055333271026611 and perplexity is 156.85679746004718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068930521498632 and perplexity of 159.00418482033612
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 731.9441089630127 and batch: 50, loss is 5.053970527648926 and perplexity is 156.6431874789146
At time: 732.7854342460632 and batch: 100, loss is 5.078355216979981 and perplexity is 160.50983484409966
At time: 733.626136302948 and batch: 150, loss is 5.010040273666382 and perplexity is 149.91077348395146
At time: 734.4666242599487 and batch: 200, loss is 5.048849887847901 and perplexity is 155.84312430441867
At time: 735.3062028884888 and batch: 250, loss is 5.063540563583374 and perplexity is 158.14946447839264
At time: 736.148500919342 and batch: 300, loss is 5.036585464477539 and perplexity is 153.94347113542239
At time: 737.0457482337952 and batch: 350, loss is 5.0468385887146 and perplexity is 155.52999217032934
At time: 737.8899321556091 and batch: 400, loss is 5.0195972347259525 and perplexity is 151.35033285809837
At time: 738.739423751831 and batch: 450, loss is 4.989216756820679 and perplexity is 146.82138160296807
At time: 739.580025434494 and batch: 500, loss is 4.990416984558106 and perplexity is 146.9977064914593
At time: 740.4224030971527 and batch: 550, loss is 4.993648099899292 and perplexity is 147.47344119898557
At time: 741.2673473358154 and batch: 600, loss is 5.020441856384277 and perplexity is 151.47822062801234
At time: 742.1077346801758 and batch: 650, loss is 5.026538496017456 and perplexity is 152.40454962928146
At time: 742.9486038684845 and batch: 700, loss is 5.042962303161621 and perplexity is 154.92828046524193
At time: 743.7896127700806 and batch: 750, loss is 5.005667381286621 and perplexity is 149.25666102896244
At time: 744.6333339214325 and batch: 800, loss is 5.026187362670899 and perplexity is 152.35104470397107
At time: 745.4771888256073 and batch: 850, loss is 5.041614894866943 and perplexity is 154.71966938877154
At time: 746.3278803825378 and batch: 900, loss is 5.02737340927124 and perplexity is 152.5318473415828
At time: 747.1690533161163 and batch: 950, loss is 5.003303413391113 and perplexity is 148.90423979446226
At time: 748.0112266540527 and batch: 1000, loss is 5.003017635345459 and perplexity is 148.86169231168194
At time: 748.8538579940796 and batch: 1050, loss is 4.9985324478149415 and perplexity is 148.19551478790086
At time: 749.7018647193909 and batch: 1100, loss is 4.969438724517822 and perplexity is 143.9460712924272
At time: 750.5524210929871 and batch: 1150, loss is 4.98650164604187 and perplexity is 146.4232859686498
At time: 751.3942558765411 and batch: 1200, loss is 5.013511219024658 and perplexity is 150.43200965516806
At time: 752.2421381473541 and batch: 1250, loss is 5.055224761962891 and perplexity is 156.83977799921624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068888197850137 and perplexity of 158.9974553255179
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 754.7486026287079 and batch: 50, loss is 5.053949022293091 and perplexity is 156.6398188476507
At time: 755.6224656105042 and batch: 100, loss is 5.078338918685913 and perplexity is 160.5072188289289
At time: 756.4703538417816 and batch: 150, loss is 5.009999732971192 and perplexity is 149.90469612016904
At time: 757.3153500556946 and batch: 200, loss is 5.048818120956421 and perplexity is 155.83817373143356
At time: 758.1659471988678 and batch: 250, loss is 5.063521919250488 and perplexity is 158.1465159146182
At time: 759.0661654472351 and batch: 300, loss is 5.036579160690308 and perplexity is 153.94250071159342
At time: 759.9105184078217 and batch: 350, loss is 5.046811618804932 and perplexity is 155.5257975970537
At time: 760.7554519176483 and batch: 400, loss is 5.019582777023316 and perplexity is 151.34814469580985
At time: 761.6001882553101 and batch: 450, loss is 4.989212551116943 and perplexity is 146.82076411703352
At time: 762.4486262798309 and batch: 500, loss is 4.990423269271851 and perplexity is 146.9986303328688
At time: 763.2952797412872 and batch: 550, loss is 4.993628206253052 and perplexity is 147.47050744369824
At time: 764.1421437263489 and batch: 600, loss is 5.020407438278198 and perplexity is 151.47300712426608
At time: 764.9849233627319 and batch: 650, loss is 5.0265045261383055 and perplexity is 152.3993725530814
At time: 765.829690694809 and batch: 700, loss is 5.042932186126709 and perplexity is 154.92361455507233
At time: 766.6739647388458 and batch: 750, loss is 5.005643339157104 and perplexity is 149.2530726241234
At time: 767.519668340683 and batch: 800, loss is 5.026165800094605 and perplexity is 152.3477596583633
At time: 768.3652112483978 and batch: 850, loss is 5.041589937210083 and perplexity is 154.71580799653924
At time: 769.2142863273621 and batch: 900, loss is 5.027335786819458 and perplexity is 152.52610882746018
At time: 770.0592014789581 and batch: 950, loss is 5.0032724761962895 and perplexity is 148.8996331862438
At time: 770.9049210548401 and batch: 1000, loss is 5.002987155914306 and perplexity is 148.85715516112515
At time: 771.7571585178375 and batch: 1050, loss is 4.998511133193969 and perplexity is 148.19235609033672
At time: 772.6049537658691 and batch: 1100, loss is 4.969399881362915 and perplexity is 143.94048008147263
At time: 773.4563195705414 and batch: 1150, loss is 4.986461658477783 and perplexity is 146.41743097518247
At time: 774.3065674304962 and batch: 1200, loss is 5.013477611541748 and perplexity is 150.42695409892715
At time: 775.1505165100098 and batch: 1250, loss is 5.055202226638794 and perplexity is 156.8362436038123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.06887839658417 and perplexity of 158.99589695680723
Annealing...
Model not improving. Stopping early with 158.9652993566064loss at 33 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe70d2b860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'num_layers': 1, 'anneal': 4.377602789040021, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.34815859453726294, 'tune_wordvecs': True, 'lr': 22.262110235502455, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4655156135559082 and batch: 50, loss is 6.982629365921021 and perplexity is 1077.7484397357473
At time: 2.3089475631713867 and batch: 100, loss is 6.213829879760742 and perplexity is 499.6110420360784
At time: 3.1540181636810303 and batch: 150, loss is 6.027009735107422 and perplexity is 414.47378806936683
At time: 3.9962754249572754 and batch: 200, loss is 5.995952415466308 and perplexity is 401.7991815685771
At time: 4.871576309204102 and batch: 250, loss is 5.998483667373657 and perplexity is 402.8175248112009
At time: 5.715836524963379 and batch: 300, loss is 5.996777801513672 and perplexity is 402.1309579098319
At time: 6.559617280960083 and batch: 350, loss is 6.012444219589233 and perplexity is 408.48051717524584
At time: 7.404208660125732 and batch: 400, loss is 5.942533407211304 and perplexity is 380.8986796988056
At time: 8.250252962112427 and batch: 450, loss is 5.926587724685669 and perplexity is 374.87315847475713
At time: 9.10041880607605 and batch: 500, loss is 5.916088266372681 and perplexity is 370.9577839989626
At time: 9.949505090713501 and batch: 550, loss is 5.919760961532592 and perplexity is 372.3227037896938
At time: 10.797534465789795 and batch: 600, loss is 5.943291177749634 and perplexity is 381.18742288309255
At time: 11.647730350494385 and batch: 650, loss is 5.915479898452759 and perplexity is 370.7321738174892
At time: 12.4958016872406 and batch: 700, loss is 5.948803825378418 and perplexity is 383.2945774901548
At time: 13.341756582260132 and batch: 750, loss is 5.901588792800903 and perplexity is 365.61789773563646
At time: 14.192585945129395 and batch: 800, loss is 5.896269788742066 and perplexity is 363.67833749080694
At time: 15.04249119758606 and batch: 850, loss is 5.9397601222991945 and perplexity is 379.8438025508916
At time: 15.892776727676392 and batch: 900, loss is 5.940773115158081 and perplexity is 380.2287765654117
At time: 16.742748022079468 and batch: 950, loss is 5.904259462356567 and perplexity is 366.59564736606234
At time: 17.587738513946533 and batch: 1000, loss is 5.901661176681518 and perplexity is 365.6443635357339
At time: 18.433759689331055 and batch: 1050, loss is 5.8919327926635745 and perplexity is 362.1044813382158
At time: 19.282609939575195 and batch: 1100, loss is 5.878284330368042 and perplexity is 357.19588556752376
At time: 20.12935996055603 and batch: 1150, loss is 5.921369476318359 and perplexity is 372.9220722811267
At time: 20.977973699569702 and batch: 1200, loss is 5.905041360855103 and perplexity is 366.88240004344243
At time: 21.82438898086548 and batch: 1250, loss is 5.874003601074219 and perplexity is 355.6700947552186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.393302527657391 and perplexity of 219.9285075059344
Finished 1 epochs...
Completing Train Step...
At time: 24.30327343940735 and batch: 50, loss is 5.687204275131226 and perplexity is 295.067538773617
At time: 25.149126052856445 and batch: 100, loss is 5.75724799156189 and perplexity is 316.47618426400766
At time: 25.98971152305603 and batch: 150, loss is 5.674064998626709 and perplexity is 291.21592392235107
At time: 26.829288244247437 and batch: 200, loss is 5.694155158996582 and perplexity is 297.12566357482416
At time: 27.669482469558716 and batch: 250, loss is 5.74011347770691 and perplexity is 311.09971186034574
At time: 28.510964155197144 and batch: 300, loss is 5.7202795791625975 and perplexity is 304.990179937423
At time: 29.350001335144043 and batch: 350, loss is 5.755617141723633 and perplexity is 315.9604797626901
At time: 30.245446920394897 and batch: 400, loss is 5.7345223903656 and perplexity is 309.36517967843236
At time: 31.086952447891235 and batch: 450, loss is 5.672957725524903 and perplexity is 290.8936468202848
At time: 31.92556881904602 and batch: 500, loss is 5.657572603225708 and perplexity is 286.45246415980637
At time: 32.76364326477051 and batch: 550, loss is 5.67920786857605 and perplexity is 292.7174673580281
At time: 33.60168981552124 and batch: 600, loss is 5.6878219318389895 and perplexity is 295.24984551387854
At time: 34.442267417907715 and batch: 650, loss is 5.70735595703125 and perplexity is 301.07396246262584
At time: 35.28264832496643 and batch: 700, loss is 5.773834323883056 and perplexity is 321.76913742476097
At time: 36.12267708778381 and batch: 750, loss is 5.708922119140625 and perplexity is 301.54586253430097
At time: 36.9618444442749 and batch: 800, loss is 5.7159534931182865 and perplexity is 303.6736160140442
At time: 37.80062747001648 and batch: 850, loss is 5.725184440612793 and perplexity is 306.4897891950817
At time: 38.64333462715149 and batch: 900, loss is 5.724471969604492 and perplexity is 306.27150187686664
At time: 39.48268795013428 and batch: 950, loss is 5.707483654022217 and perplexity is 301.1124111565296
At time: 40.32363820075989 and batch: 1000, loss is 5.74307936668396 and perplexity is 312.0237687141626
At time: 41.16533970832825 and batch: 1050, loss is 5.6943004322052 and perplexity is 297.1688311088014
At time: 42.00451183319092 and batch: 1100, loss is 5.742214279174805 and perplexity is 311.75395757123977
At time: 42.844971895217896 and batch: 1150, loss is 5.7621425437927245 and perplexity is 318.02899052337284
At time: 43.687171459198 and batch: 1200, loss is 5.735317287445068 and perplexity is 309.61119092010273
At time: 44.52538514137268 and batch: 1250, loss is 5.692671108245849 and perplexity is 296.6850410447014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.520555927805657 and perplexity of 249.77385483085192
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 46.966551780700684 and batch: 50, loss is 5.642976608276367 and perplexity is 282.3017708782318
At time: 47.80574154853821 and batch: 100, loss is 5.623778076171875 and perplexity is 276.9336858248353
At time: 48.64353275299072 and batch: 150, loss is 5.506323709487915 and perplexity is 246.24419581694937
At time: 49.48132133483887 and batch: 200, loss is 5.533773565292359 and perplexity is 253.09719003391658
At time: 50.350380420684814 and batch: 250, loss is 5.544863376617432 and perplexity is 255.91961125171647
At time: 51.187483072280884 and batch: 300, loss is 5.5385603427886965 and perplexity is 254.3116142379175
At time: 52.02428221702576 and batch: 350, loss is 5.562479238510132 and perplexity is 260.46779828147555
At time: 52.862046003341675 and batch: 400, loss is 5.5281358909606935 and perplexity is 251.67432509543477
At time: 53.70363712310791 and batch: 450, loss is 5.506646022796631 and perplexity is 246.3235763904758
At time: 54.545214891433716 and batch: 500, loss is 5.4889697265625 and perplexity is 242.00774422211535
At time: 55.40460824966431 and batch: 550, loss is 5.49787015914917 and perplexity is 244.17133198568217
At time: 56.24726986885071 and batch: 600, loss is 5.5030049705505375 and perplexity is 245.42833018753498
At time: 57.0860161781311 and batch: 650, loss is 5.498156652450562 and perplexity is 244.24129545824255
At time: 57.92638421058655 and batch: 700, loss is 5.515916957855224 and perplexity is 248.61784484479625
At time: 58.768433570861816 and batch: 750, loss is 5.473383378982544 and perplexity is 238.2649712541567
At time: 59.613009214401245 and batch: 800, loss is 5.4969071197509765 and perplexity is 243.93629856445784
At time: 60.454556703567505 and batch: 850, loss is 5.502013788223267 and perplexity is 245.18518648381885
At time: 61.297616958618164 and batch: 900, loss is 5.507912063598633 and perplexity is 246.63562958303683
At time: 62.13840174674988 and batch: 950, loss is 5.44580771446228 and perplexity is 231.78441975205678
At time: 62.98524212837219 and batch: 1000, loss is 5.462444763183594 and perplexity is 235.67288503879772
At time: 63.832640171051025 and batch: 1050, loss is 5.406590213775635 and perplexity is 222.8703503393924
At time: 64.67710328102112 and batch: 1100, loss is 5.3746678829193115 and perplexity is 215.86816689857324
At time: 65.52024054527283 and batch: 1150, loss is 5.401399965286255 and perplexity is 221.71659456983934
At time: 66.36060047149658 and batch: 1200, loss is 5.418070878982544 and perplexity is 225.4437943747364
At time: 67.20087170600891 and batch: 1250, loss is 5.39603895187378 and perplexity is 220.5311493655878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.243556113138686 and perplexity of 189.34222900652009
Finished 3 epochs...
Completing Train Step...
At time: 69.63743305206299 and batch: 50, loss is 5.442594976425171 and perplexity is 231.04095205327422
At time: 70.53055810928345 and batch: 100, loss is 5.466150026321412 and perplexity is 236.5477348658236
At time: 71.37052130699158 and batch: 150, loss is 5.368123741149902 and perplexity is 214.46010730678262
At time: 72.24361681938171 and batch: 200, loss is 5.400445280075073 and perplexity is 221.50502602268048
At time: 73.0874376296997 and batch: 250, loss is 5.420489320755005 and perplexity is 225.9896768908684
At time: 73.92568945884705 and batch: 300, loss is 5.410494403839111 and perplexity is 223.74217933225773
At time: 74.76506543159485 and batch: 350, loss is 5.446413545608521 and perplexity is 231.9248845174246
At time: 75.60513687133789 and batch: 400, loss is 5.41870569229126 and perplexity is 225.58695453095152
At time: 76.45158290863037 and batch: 450, loss is 5.391995277404785 and perplexity is 219.64119374535102
At time: 77.29405546188354 and batch: 500, loss is 5.382271223068237 and perplexity is 217.51574159696366
At time: 78.13975048065186 and batch: 550, loss is 5.387153768539429 and perplexity is 218.58036902693678
At time: 78.97977900505066 and batch: 600, loss is 5.399523401260376 and perplexity is 221.30091932711585
At time: 79.81953620910645 and batch: 650, loss is 5.3992418384552 and perplexity is 221.23861799076317
At time: 80.66096377372742 and batch: 700, loss is 5.421257562637329 and perplexity is 226.16335833176538
At time: 81.50066590309143 and batch: 750, loss is 5.3813338947296145 and perplexity is 217.31195345139074
At time: 82.34185194969177 and batch: 800, loss is 5.406624240875244 and perplexity is 222.87793410002925
At time: 83.18397116661072 and batch: 850, loss is 5.421719732284546 and perplexity is 226.2679083293557
At time: 84.02447390556335 and batch: 900, loss is 5.419065465927124 and perplexity is 225.6681293711948
At time: 84.86355519294739 and batch: 950, loss is 5.364177856445313 and perplexity is 213.6155398283901
At time: 85.70767045021057 and batch: 1000, loss is 5.379139938354492 and perplexity is 216.8357031329706
At time: 86.54620361328125 and batch: 1050, loss is 5.3387088966369625 and perplexity is 208.2436725427482
At time: 87.39430642127991 and batch: 1100, loss is 5.312905788421631 and perplexity is 202.9390704498055
At time: 88.2381420135498 and batch: 1150, loss is 5.344172315597534 and perplexity is 209.38450856659105
At time: 89.08243370056152 and batch: 1200, loss is 5.367572011947632 and perplexity is 214.34181603823384
At time: 89.92236614227295 and batch: 1250, loss is 5.356367626190186 and perplexity is 211.9536515890422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.227606334825502 and perplexity of 186.3462187919213
Finished 4 epochs...
Completing Train Step...
At time: 92.39067816734314 and batch: 50, loss is 5.3707612609863284 and perplexity is 215.02649669721146
At time: 93.23737287521362 and batch: 100, loss is 5.390732250213623 and perplexity is 219.36395606157285
At time: 94.11388492584229 and batch: 150, loss is 5.297711114883423 and perplexity is 199.87878641375912
At time: 94.95888447761536 and batch: 200, loss is 5.334394159317017 and perplexity is 207.3470914441161
At time: 95.80715560913086 and batch: 250, loss is 5.364218225479126 and perplexity is 213.6241634554031
At time: 96.6527464389801 and batch: 300, loss is 5.345865840911865 and perplexity is 209.73940696216425
At time: 97.497478723526 and batch: 350, loss is 5.3785249042510985 and perplexity is 216.7023827831939
At time: 98.34206485748291 and batch: 400, loss is 5.355358943939209 and perplexity is 211.7399654914594
At time: 99.18780946731567 and batch: 450, loss is 5.327362794876098 and perplexity is 205.8942721151136
At time: 100.03434705734253 and batch: 500, loss is 5.321633396148681 and perplexity is 204.71799463353412
At time: 100.8785309791565 and batch: 550, loss is 5.322027406692505 and perplexity is 204.79867157466916
At time: 101.72048449516296 and batch: 600, loss is 5.347694158554077 and perplexity is 210.1232279866375
At time: 102.56716227531433 and batch: 650, loss is 5.355366649627686 and perplexity is 211.74159709995786
At time: 103.41124653816223 and batch: 700, loss is 5.377818460464478 and perplexity is 216.54934879265113
At time: 104.2547116279602 and batch: 750, loss is 5.3421016025543215 and perplexity is 208.95138192890977
At time: 105.09928941726685 and batch: 800, loss is 5.359030714035034 and perplexity is 212.51885504144533
At time: 105.95072031021118 and batch: 850, loss is 5.371320304870605 and perplexity is 215.14673955253227
At time: 106.79601645469666 and batch: 900, loss is 5.369812488555908 and perplexity is 214.82258223484178
At time: 107.64144206047058 and batch: 950, loss is 5.322116994857788 and perplexity is 204.8170199337937
At time: 108.48586750030518 and batch: 1000, loss is 5.335660991668701 and perplexity is 207.6099318998294
At time: 109.33225440979004 and batch: 1050, loss is 5.306081275939942 and perplexity is 201.55882533745077
At time: 110.17376136779785 and batch: 1100, loss is 5.277926588058472 and perplexity is 195.9631415163969
At time: 111.02646923065186 and batch: 1150, loss is 5.3135185718536375 and perplexity is 203.0634662598371
At time: 111.87567210197449 and batch: 1200, loss is 5.332019987106324 and perplexity is 206.85539765546656
At time: 112.72308611869812 and batch: 1250, loss is 5.329235420227051 and perplexity is 206.28019618151004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.228772685475593 and perplexity of 186.56369062490452
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 115.23085427284241 and batch: 50, loss is 5.34118836402893 and perplexity is 208.76064658368747
At time: 116.08527445793152 and batch: 100, loss is 5.352311010360718 and perplexity is 211.0955786637116
At time: 116.93380355834961 and batch: 150, loss is 5.248451938629151 and perplexity is 190.27148840751718
At time: 117.77917981147766 and batch: 200, loss is 5.280086221694947 and perplexity is 196.3868074252332
At time: 118.61897444725037 and batch: 250, loss is 5.297560710906982 and perplexity is 199.8487261101275
At time: 119.4630880355835 and batch: 300, loss is 5.277226819992065 and perplexity is 195.8260607357411
At time: 120.30738019943237 and batch: 350, loss is 5.299954385757446 and perplexity is 200.32767197136215
At time: 121.15424489974976 and batch: 400, loss is 5.267946977615356 and perplexity is 194.01723156432337
At time: 121.99792051315308 and batch: 450, loss is 5.2339130306243895 and perplexity is 187.52516141571797
At time: 122.84332180023193 and batch: 500, loss is 5.225304746627808 and perplexity is 185.91781972229964
At time: 123.6859917640686 and batch: 550, loss is 5.23057264328003 and perplexity is 186.89979979628723
At time: 124.52843832969666 and batch: 600, loss is 5.242053260803223 and perplexity is 189.05788930926155
At time: 125.370276927948 and batch: 650, loss is 5.248276796340942 and perplexity is 190.23816674175825
At time: 126.21156072616577 and batch: 700, loss is 5.263917798995972 and perplexity is 193.23707423516825
At time: 127.05349230766296 and batch: 750, loss is 5.232605752944946 and perplexity is 187.28017412596273
At time: 127.89839696884155 and batch: 800, loss is 5.235837182998657 and perplexity is 187.88633576609968
At time: 128.7404749393463 and batch: 850, loss is 5.2468210792541505 and perplexity is 189.96143526211276
At time: 129.59273648262024 and batch: 900, loss is 5.239994792938233 and perplexity is 188.66911999134683
At time: 130.43583011627197 and batch: 950, loss is 5.179469375610352 and perplexity is 177.5885531701917
At time: 131.27871370315552 and batch: 1000, loss is 5.192741937637329 and perplexity is 179.9613197710772
At time: 132.11855506896973 and batch: 1050, loss is 5.148737831115723 and perplexity is 172.213989945082
At time: 132.96668004989624 and batch: 1100, loss is 5.110921649932862 and perplexity is 165.82311533019504
At time: 133.80876541137695 and batch: 1150, loss is 5.149839677810669 and perplexity is 172.40384793871957
At time: 134.65350818634033 and batch: 1200, loss is 5.174895029067994 and perplexity is 176.77805674507388
At time: 135.49669075012207 and batch: 1250, loss is 5.204461288452149 and perplexity is 182.08275625944376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.112582241531706 and perplexity of 166.09870856297965
Finished 6 epochs...
Completing Train Step...
At time: 137.94291949272156 and batch: 50, loss is 5.260442523956299 and perplexity is 192.56668781764918
At time: 138.84155201911926 and batch: 100, loss is 5.281616048812866 and perplexity is 196.68747521506364
At time: 139.68673086166382 and batch: 150, loss is 5.188992195129394 and perplexity is 179.28777475992914
At time: 140.53140711784363 and batch: 200, loss is 5.220634784698486 and perplexity is 185.0516147286077
At time: 141.37750601768494 and batch: 250, loss is 5.245814905166626 and perplexity is 189.7703971132577
At time: 142.22284483909607 and batch: 300, loss is 5.2293680477142335 and perplexity is 186.67479667229472
At time: 143.069109916687 and batch: 350, loss is 5.25367413520813 and perplexity is 191.26772252450306
At time: 143.91511917114258 and batch: 400, loss is 5.227531805038452 and perplexity is 186.33233096545192
At time: 144.76195526123047 and batch: 450, loss is 5.192396593093872 and perplexity is 179.89918184137596
At time: 145.6086835861206 and batch: 500, loss is 5.188184690475464 and perplexity is 179.14305748521787
At time: 146.4560055732727 and batch: 550, loss is 5.194459724426269 and perplexity is 180.27072061495977
At time: 147.30244541168213 and batch: 600, loss is 5.2096956348419186 and perplexity is 183.0383392216393
At time: 148.14509630203247 and batch: 650, loss is 5.21810980796814 and perplexity is 184.58495311030217
At time: 148.9943196773529 and batch: 700, loss is 5.232114191055298 and perplexity is 187.18813695251248
At time: 149.836843252182 and batch: 750, loss is 5.2013490200042725 and perplexity is 181.5169467744516
At time: 150.6810004711151 and batch: 800, loss is 5.215212469100952 and perplexity is 184.0509219595426
At time: 151.52713465690613 and batch: 850, loss is 5.229570589065552 and perplexity is 186.71260986710828
At time: 152.3784749507904 and batch: 900, loss is 5.222517356872559 and perplexity is 185.40031587381418
At time: 153.2245786190033 and batch: 950, loss is 5.166158847808838 and perplexity is 175.24041792391498
At time: 154.0711190700531 and batch: 1000, loss is 5.180937623977661 and perplexity is 177.84948878570623
At time: 154.91551089286804 and batch: 1050, loss is 5.143695468902588 and perplexity is 171.3478102607124
At time: 155.75906229019165 and batch: 1100, loss is 5.111611328125 and perplexity is 165.93751936303926
At time: 156.6029224395752 and batch: 1150, loss is 5.1548350811004635 and perplexity is 173.2672293616814
At time: 157.44903373718262 and batch: 1200, loss is 5.1804001426696775 and perplexity is 177.7539236943753
At time: 158.35501646995544 and batch: 1250, loss is 5.203295297622681 and perplexity is 181.87057316125356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108796725307938 and perplexity of 165.471127814962
Finished 7 epochs...
Completing Train Step...
At time: 160.80295634269714 and batch: 50, loss is 5.2366416454315186 and perplexity is 188.03754407741982
At time: 161.6444103717804 and batch: 100, loss is 5.25752028465271 and perplexity is 192.00478328356806
At time: 162.48799681663513 and batch: 150, loss is 5.166527519226074 and perplexity is 175.3050359678291
At time: 163.33050990104675 and batch: 200, loss is 5.198368549346924 and perplexity is 180.97674626655288
At time: 164.17421650886536 and batch: 250, loss is 5.22563006401062 and perplexity is 185.9783118598682
At time: 165.0215413570404 and batch: 300, loss is 5.208913192749024 and perplexity is 182.89517833527427
At time: 165.8625009059906 and batch: 350, loss is 5.232442455291748 and perplexity is 187.2495942099197
At time: 166.70444297790527 and batch: 400, loss is 5.207880411148071 and perplexity is 182.70638506807424
At time: 167.54625868797302 and batch: 450, loss is 5.17257134437561 and perplexity is 176.3677571687066
At time: 168.38759398460388 and batch: 500, loss is 5.170734252929687 and perplexity is 176.04405090072314
At time: 169.22790956497192 and batch: 550, loss is 5.1783418941497805 and perplexity is 177.3884382030394
At time: 170.0698161125183 and batch: 600, loss is 5.194982891082764 and perplexity is 180.3650569197806
At time: 170.91001892089844 and batch: 650, loss is 5.2041561794281 and perplexity is 182.02720964170345
At time: 171.75580668449402 and batch: 700, loss is 5.2172764301300045 and perplexity is 184.43118818216792
At time: 172.60808515548706 and batch: 750, loss is 5.185221347808838 and perplexity is 178.61298100656603
At time: 173.44913172721863 and batch: 800, loss is 5.205126886367798 and perplexity is 182.2039905046404
At time: 174.29108691215515 and batch: 850, loss is 5.219999589920044 and perplexity is 184.9341082328969
At time: 175.13655757904053 and batch: 900, loss is 5.211397762298584 and perplexity is 183.35015910776104
At time: 175.9759624004364 and batch: 950, loss is 5.157360439300537 and perplexity is 173.7053441456978
At time: 176.81800031661987 and batch: 1000, loss is 5.172173023223877 and perplexity is 176.2975201499163
At time: 177.65872168540955 and batch: 1050, loss is 5.138446340560913 and perplexity is 170.4507400915865
At time: 178.50590991973877 and batch: 1100, loss is 5.107727375030517 and perplexity is 165.2942757937596
At time: 179.3532223701477 and batch: 1150, loss is 5.1524559593200685 and perplexity is 172.855495499187
At time: 180.25931787490845 and batch: 1200, loss is 5.177192888259888 and perplexity is 177.18473489330756
At time: 181.10413885116577 and batch: 1250, loss is 5.196738691329956 and perplexity is 180.682020111902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.106722866531706 and perplexity of 165.1283196550505
Finished 8 epochs...
Completing Train Step...
At time: 183.52243518829346 and batch: 50, loss is 5.218254346847534 and perplexity is 184.6116347407978
At time: 184.42882084846497 and batch: 100, loss is 5.2405668640136716 and perplexity is 188.77708301603982
At time: 185.27330350875854 and batch: 150, loss is 5.1505774402618405 and perplexity is 172.531087954848
At time: 186.11880254745483 and batch: 200, loss is 5.182796926498413 and perplexity is 178.18047239249617
At time: 186.96324396133423 and batch: 250, loss is 5.210291976928711 and perplexity is 183.1475252396869
At time: 187.8063566684723 and batch: 300, loss is 5.193272199630737 and perplexity is 180.05677172427022
At time: 188.65392303466797 and batch: 350, loss is 5.216316556930542 and perplexity is 184.25424256375103
At time: 189.50032353401184 and batch: 400, loss is 5.193144998550415 and perplexity is 180.03386976499544
At time: 190.34634518623352 and batch: 450, loss is 5.1573263359069825 and perplexity is 173.69942030499598
At time: 191.1955976486206 and batch: 500, loss is 5.15692777633667 and perplexity is 173.63020453289616
At time: 192.05267214775085 and batch: 550, loss is 5.165791091918945 and perplexity is 175.1759840767658
At time: 192.8933665752411 and batch: 600, loss is 5.183066234588623 and perplexity is 178.22846429724305
At time: 193.7382938861847 and batch: 650, loss is 5.192688465118408 and perplexity is 179.9516970432787
At time: 194.5824954509735 and batch: 700, loss is 5.206933631896972 and perplexity is 182.53348431598633
At time: 195.4265730381012 and batch: 750, loss is 5.176329526901245 and perplexity is 177.03182645697512
At time: 196.2725431919098 and batch: 800, loss is 5.196488599777222 and perplexity is 180.63683871491952
At time: 197.1192009449005 and batch: 850, loss is 5.21229769706726 and perplexity is 183.51523655911234
At time: 197.9652910232544 and batch: 900, loss is 5.2021107006072995 and perplexity is 181.6552573794961
At time: 198.8069884777069 and batch: 950, loss is 5.149819192886352 and perplexity is 172.40031629511546
At time: 199.6510832309723 and batch: 1000, loss is 5.165274410247803 and perplexity is 175.0854972350284
At time: 200.4949028491974 and batch: 1050, loss is 5.132822294235229 and perplexity is 169.49480785632053
At time: 201.33842015266418 and batch: 1100, loss is 5.103111915588379 and perplexity is 164.53312465013138
At time: 202.2365598678589 and batch: 1150, loss is 5.147950515747071 and perplexity is 172.07845658485115
At time: 203.08050084114075 and batch: 1200, loss is 5.171989440917969 and perplexity is 176.2651580152898
At time: 203.92395853996277 and batch: 1250, loss is 5.189863595962525 and perplexity is 179.44407436613818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104968885435675 and perplexity of 164.83894155999843
Finished 9 epochs...
Completing Train Step...
At time: 206.36384415626526 and batch: 50, loss is 5.203383121490479 and perplexity is 181.88654643983446
At time: 207.20851683616638 and batch: 100, loss is 5.2263393974304195 and perplexity is 186.11027929074956
At time: 208.0530879497528 and batch: 150, loss is 5.1372091007232665 and perplexity is 170.23998205159552
At time: 208.89939880371094 and batch: 200, loss is 5.170255823135376 and perplexity is 175.95984632625573
At time: 209.74236273765564 and batch: 250, loss is 5.196953792572021 and perplexity is 180.7208892190937
At time: 210.58360123634338 and batch: 300, loss is 5.180392179489136 and perplexity is 177.75250821342482
At time: 211.42495393753052 and batch: 350, loss is 5.203516035079956 and perplexity is 181.91072324027655
At time: 212.26922798156738 and batch: 400, loss is 5.180722246170044 and perplexity is 177.8111880774351
At time: 213.10943698883057 and batch: 450, loss is 5.145180616378784 and perplexity is 171.6024760900208
At time: 213.9502875804901 and batch: 500, loss is 5.145223960876465 and perplexity is 171.60991427434874
At time: 214.79269289970398 and batch: 550, loss is 5.154469604492188 and perplexity is 173.2039158128842
At time: 215.63482451438904 and batch: 600, loss is 5.172253522872925 and perplexity is 176.31171260965274
At time: 216.48088264465332 and batch: 650, loss is 5.1824331378936765 and perplexity is 178.11566415601695
At time: 217.3223865032196 and batch: 700, loss is 5.1966594886779784 and perplexity is 180.66771018344414
At time: 218.165518283844 and batch: 750, loss is 5.167112712860107 and perplexity is 175.4076533814916
At time: 219.00997757911682 and batch: 800, loss is 5.188404455184936 and perplexity is 179.18243113351076
At time: 219.8512625694275 and batch: 850, loss is 5.204821329116822 and perplexity is 182.14832525907772
At time: 220.6917586326599 and batch: 900, loss is 5.193537788391113 and perplexity is 180.10459912999937
At time: 221.53286290168762 and batch: 950, loss is 5.14172698020935 and perplexity is 171.0108457977808
At time: 222.37468123435974 and batch: 1000, loss is 5.157186212539673 and perplexity is 173.67508266249922
At time: 223.22196531295776 and batch: 1050, loss is 5.126676712036133 and perplexity is 168.45635779080024
At time: 224.12227988243103 and batch: 1100, loss is 5.097750177383423 and perplexity is 163.65330190709986
At time: 224.96487021446228 and batch: 1150, loss is 5.143116140365601 and perplexity is 171.24857233293542
At time: 225.80639505386353 and batch: 1200, loss is 5.166018733978271 and perplexity is 175.21586603775918
At time: 226.65248656272888 and batch: 1250, loss is 5.183114643096924 and perplexity is 178.23709228016858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.103850650091241 and perplexity of 164.6547158524527
Finished 10 epochs...
Completing Train Step...
At time: 229.16496348381042 and batch: 50, loss is 5.190198812484741 and perplexity is 179.50423706788206
At time: 230.03920435905457 and batch: 100, loss is 5.213718729019165 and perplexity is 183.77620295079987
At time: 230.8780379295349 and batch: 150, loss is 5.126282596588135 and perplexity is 168.38997961907208
At time: 231.72335004806519 and batch: 200, loss is 5.1589587211608885 and perplexity is 173.9831962301173
At time: 232.57270002365112 and batch: 250, loss is 5.185677576065063 and perplexity is 178.69448788688703
At time: 233.4161684513092 and batch: 300, loss is 5.169217309951782 and perplexity is 175.7772045604187
At time: 234.25600671768188 and batch: 350, loss is 5.192485332489014 and perplexity is 179.91514669430424
At time: 235.0937683582306 and batch: 400, loss is 5.1699723052978515 and perplexity is 175.90996564250008
At time: 235.93044662475586 and batch: 450, loss is 5.133947372436523 and perplexity is 169.68561008345904
At time: 236.77240371704102 and batch: 500, loss is 5.134910163879394 and perplexity is 169.84906060858304
At time: 237.6137571334839 and batch: 550, loss is 5.144997053146362 and perplexity is 171.57097907575337
At time: 238.45709013938904 and batch: 600, loss is 5.162733144760132 and perplexity is 174.64112337863406
At time: 239.2954876422882 and batch: 650, loss is 5.172964916229248 and perplexity is 176.437184215187
At time: 240.13870930671692 and batch: 700, loss is 5.1873305225372315 and perplexity is 178.99010456219807
At time: 240.97919034957886 and batch: 750, loss is 5.158400907516479 and perplexity is 173.88617309229303
At time: 241.81785011291504 and batch: 800, loss is 5.180899438858032 and perplexity is 177.8426977113608
At time: 242.65787386894226 and batch: 850, loss is 5.197096815109253 and perplexity is 180.74673822765197
At time: 243.49855303764343 and batch: 900, loss is 5.184865083694458 and perplexity is 178.54935894487258
At time: 244.33702898025513 and batch: 950, loss is 5.1345570945739745 and perplexity is 169.7891027039985
At time: 245.23351550102234 and batch: 1000, loss is 5.150007629394532 and perplexity is 172.43280586974203
At time: 246.07837462425232 and batch: 1050, loss is 5.120995798110962 and perplexity is 167.50208485965328
At time: 246.92164731025696 and batch: 1100, loss is 5.091735439300537 and perplexity is 162.67192448242602
At time: 247.7614872455597 and batch: 1150, loss is 5.137391748428345 and perplexity is 170.27107883342134
At time: 248.60642886161804 and batch: 1200, loss is 5.159393033981323 and perplexity is 174.0587757741747
At time: 249.4465720653534 and batch: 1250, loss is 5.176080303192139 and perplexity is 176.9877114260398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.10295071567062 and perplexity of 164.50660406158505
Finished 11 epochs...
Completing Train Step...
At time: 251.85925126075745 and batch: 50, loss is 5.178978843688965 and perplexity is 177.50146167831352
At time: 252.73332381248474 and batch: 100, loss is 5.2022316360473635 and perplexity is 181.6772272664294
At time: 253.58276796340942 and batch: 150, loss is 5.116094522476196 and perplexity is 166.6831195990625
At time: 254.43218564987183 and batch: 200, loss is 5.148793506622314 and perplexity is 172.22357831313053
At time: 255.2770848274231 and batch: 250, loss is 5.1752009677886965 and perplexity is 176.83214827152713
At time: 256.1288421154022 and batch: 300, loss is 5.158558168411255 and perplexity is 173.9135207377639
At time: 256.98251962661743 and batch: 350, loss is 5.181484441757203 and perplexity is 177.9467666424636
At time: 257.8291802406311 and batch: 400, loss is 5.160113630294799 and perplexity is 174.1842470879795
At time: 258.67969274520874 and batch: 450, loss is 5.124494142532349 and perplexity is 168.08909101998734
At time: 259.5244688987732 and batch: 500, loss is 5.125326919555664 and perplexity is 168.2291300554109
At time: 260.3747100830078 and batch: 550, loss is 5.135690908432007 and perplexity is 169.9817211176658
At time: 261.222042798996 and batch: 600, loss is 5.153458461761475 and perplexity is 173.02887044531852
At time: 262.066871881485 and batch: 650, loss is 5.164843254089355 and perplexity is 175.0100243161167
At time: 262.91711950302124 and batch: 700, loss is 5.179203243255615 and perplexity is 177.54129739878596
At time: 263.7677254676819 and batch: 750, loss is 5.1508525848388675 and perplexity is 172.57856547935955
At time: 264.62502908706665 and batch: 800, loss is 5.173207740783692 and perplexity is 176.48003269795262
At time: 265.47232961654663 and batch: 850, loss is 5.189990139007568 and perplexity is 179.4667832025158
At time: 266.31675386428833 and batch: 900, loss is 5.176804695129395 and perplexity is 177.11596634499637
At time: 267.21756315231323 and batch: 950, loss is 5.1272959136962895 and perplexity is 168.56069854785943
At time: 268.0631775856018 and batch: 1000, loss is 5.142294769287109 and perplexity is 171.1079714590386
At time: 268.90951561927795 and batch: 1050, loss is 5.114927930831909 and perplexity is 166.4887818429359
At time: 269.7546763420105 and batch: 1100, loss is 5.085922737121582 and perplexity is 161.72910385491843
At time: 270.6030321121216 and batch: 1150, loss is 5.132126121520996 and perplexity is 169.3768512597329
At time: 271.4471871852875 and batch: 1200, loss is 5.1532422542572025 and perplexity is 172.99146434895755
At time: 272.29464864730835 and batch: 1250, loss is 5.169777526855468 and perplexity is 175.8757055100685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.102526142649407 and perplexity of 164.43677382075623
Finished 12 epochs...
Completing Train Step...
At time: 274.76442074775696 and batch: 50, loss is 5.167671566009521 and perplexity is 175.5057078975007
At time: 275.60488772392273 and batch: 100, loss is 5.19155595779419 and perplexity is 179.74801578539953
At time: 276.4489116668701 and batch: 150, loss is 5.107030172348022 and perplexity is 165.17907234601807
At time: 277.2895133495331 and batch: 200, loss is 5.139083461761475 and perplexity is 170.55937247407257
At time: 278.1325273513794 and batch: 250, loss is 5.166051540374756 and perplexity is 175.22161433322074
At time: 278.97987151145935 and batch: 300, loss is 5.149123706817627 and perplexity is 172.2804559623156
At time: 279.8195426464081 and batch: 350, loss is 5.171817188262939 and perplexity is 176.2347984886621
At time: 280.66655254364014 and batch: 400, loss is 5.1509015750885006 and perplexity is 172.5870203534652
At time: 281.515967130661 and batch: 450, loss is 5.114623594284057 and perplexity is 166.4381209311883
At time: 282.35727047920227 and batch: 500, loss is 5.116542234420776 and perplexity is 166.7577623306413
At time: 283.20368099212646 and batch: 550, loss is 5.127284288406372 and perplexity is 168.55873899226037
At time: 284.0495991706848 and batch: 600, loss is 5.145396823883057 and perplexity is 171.6395818442299
At time: 284.89295053482056 and batch: 650, loss is 5.1570157718658445 and perplexity is 173.64548388687223
At time: 285.73482298851013 and batch: 700, loss is 5.171622142791748 and perplexity is 176.20042804135963
At time: 286.57461857795715 and batch: 750, loss is 5.1437899684906006 and perplexity is 171.36400332329572
At time: 287.41372179985046 and batch: 800, loss is 5.166478967666626 and perplexity is 175.2965248415696
At time: 288.25658655166626 and batch: 850, loss is 5.183618249893189 and perplexity is 178.3268762972104
At time: 289.1558847427368 and batch: 900, loss is 5.169446840286255 and perplexity is 175.81755539166682
At time: 289.99873661994934 and batch: 950, loss is 5.120137538909912 and perplexity is 167.3583863282416
At time: 290.8401629924774 and batch: 1000, loss is 5.135206146240234 and perplexity is 169.8993403751234
At time: 291.6822226047516 and batch: 1050, loss is 5.109240093231201 and perplexity is 165.54450867141824
At time: 292.5274701118469 and batch: 1100, loss is 5.080167770385742 and perplexity is 160.80103131676015
At time: 293.3676378726959 and batch: 1150, loss is 5.126683578491211 and perplexity is 168.4575144927849
At time: 294.208726644516 and batch: 1200, loss is 5.146920108795166 and perplexity is 171.90123706670497
At time: 295.04806661605835 and batch: 1250, loss is 5.163285455703735 and perplexity is 174.7376062240923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.102010239649863 and perplexity of 164.35196227505332
Finished 13 epochs...
Completing Train Step...
At time: 297.515230178833 and batch: 50, loss is 5.1583429718017575 and perplexity is 173.87609916439763
At time: 298.3892858028412 and batch: 100, loss is 5.1820263671875 and perplexity is 178.04322665525265
At time: 299.23547887802124 and batch: 150, loss is 5.098487586975097 and perplexity is 163.77402592769016
At time: 300.0802969932556 and batch: 200, loss is 5.1308912181854245 and perplexity is 169.1678163166824
At time: 300.926531791687 and batch: 250, loss is 5.157431344985962 and perplexity is 173.7176612788587
At time: 301.77140522003174 and batch: 300, loss is 5.1402131366729735 and perplexity is 170.7521579900557
At time: 302.61728405952454 and batch: 350, loss is 5.162868337631226 and perplexity is 174.66473520955427
At time: 303.46020913124084 and batch: 400, loss is 5.142188348770142 and perplexity is 171.08976302915107
At time: 304.3052146434784 and batch: 450, loss is 5.1058980751037595 and perplexity is 164.99217938386988
At time: 305.14861726760864 and batch: 500, loss is 5.107887163162231 and perplexity is 165.3206899675512
At time: 306.0013885498047 and batch: 550, loss is 5.118623247146607 and perplexity is 167.10514868853184
At time: 306.84830927848816 and batch: 600, loss is 5.137492609024048 and perplexity is 170.2882533419645
At time: 307.69472217559814 and batch: 650, loss is 5.148328104019165 and perplexity is 172.14344366034436
At time: 308.5380828380585 and batch: 700, loss is 5.163627328872681 and perplexity is 174.79735453585653
At time: 309.3822388648987 and batch: 750, loss is 5.1356956577301025 and perplexity is 169.9825284134472
At time: 310.28191781044006 and batch: 800, loss is 5.159089527130127 and perplexity is 174.00595575924007
At time: 311.12748169898987 and batch: 850, loss is 5.176867094039917 and perplexity is 177.12701853315104
At time: 311.97244000434875 and batch: 900, loss is 5.161804685592651 and perplexity is 174.47905147682818
At time: 312.8158059120178 and batch: 950, loss is 5.113462381362915 and perplexity is 166.2449630050369
At time: 313.65921568870544 and batch: 1000, loss is 5.127966194152832 and perplexity is 168.67371936342775
At time: 314.5027422904968 and batch: 1050, loss is 5.103153104782105 and perplexity is 164.53990177644818
At time: 315.34965801239014 and batch: 1100, loss is 5.074813413619995 and perplexity is 159.94234613168453
At time: 316.19440746307373 and batch: 1150, loss is 5.121529245376587 and perplexity is 167.5914622257445
At time: 317.04175329208374 and batch: 1200, loss is 5.140879173278808 and perplexity is 170.8659230594471
At time: 317.8930380344391 and batch: 1250, loss is 5.156277666091919 and perplexity is 173.51736244200504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.101591903797901 and perplexity of 164.2832223360863
Finished 14 epochs...
Completing Train Step...
At time: 320.32842445373535 and batch: 50, loss is 5.149179162979126 and perplexity is 172.29001024002397
At time: 321.1712923049927 and batch: 100, loss is 5.172709522247314 and perplexity is 176.39212897381265
At time: 322.01989340782166 and batch: 150, loss is 5.090453004837036 and perplexity is 162.46344211128772
At time: 322.86150193214417 and batch: 200, loss is 5.122751655578614 and perplexity is 167.79645300482534
At time: 323.70241832733154 and batch: 250, loss is 5.149517707824707 and perplexity is 172.3483480093541
At time: 324.54480051994324 and batch: 300, loss is 5.132039613723755 and perplexity is 169.3621994751827
At time: 325.3848466873169 and batch: 350, loss is 5.154471168518066 and perplexity is 173.20418670850265
At time: 326.22390627861023 and batch: 400, loss is 5.133655643463134 and perplexity is 169.63611509454267
At time: 327.06455993652344 and batch: 450, loss is 5.097521486282349 and perplexity is 163.61588013246885
At time: 327.9091877937317 and batch: 500, loss is 5.099680614471436 and perplexity is 163.96952944115293
At time: 328.75233936309814 and batch: 550, loss is 5.110774927139282 and perplexity is 165.79878708427145
At time: 329.591228723526 and batch: 600, loss is 5.130086393356323 and perplexity is 169.0317206317653
At time: 330.4414749145508 and batch: 650, loss is 5.1409252262115475 and perplexity is 170.87379211750442
At time: 331.2866563796997 and batch: 700, loss is 5.156685943603516 and perplexity is 173.58822014278033
At time: 332.1829664707184 and batch: 750, loss is 5.128422336578369 and perplexity is 168.75067615319657
At time: 333.02420234680176 and batch: 800, loss is 5.152674808502197 and perplexity is 172.8933289227593
At time: 333.8650588989258 and batch: 850, loss is 5.170601692199707 and perplexity is 176.02071591951224
At time: 334.7054636478424 and batch: 900, loss is 5.154488306045533 and perplexity is 173.20715502544442
At time: 335.55346751213074 and batch: 950, loss is 5.106817016601562 and perplexity is 165.14386722977235
At time: 336.3951156139374 and batch: 1000, loss is 5.121044454574585 and perplexity is 167.5102351170317
At time: 337.23560428619385 and batch: 1050, loss is 5.097356929779052 and perplexity is 163.58895829049328
At time: 338.07493567466736 and batch: 1100, loss is 5.069383153915405 and perplexity is 159.07617155932522
At time: 338.927613735199 and batch: 1150, loss is 5.115802145004272 and perplexity is 166.6343923336662
At time: 339.76809191703796 and batch: 1200, loss is 5.134811019897461 and perplexity is 169.8322219311272
At time: 340.6131112575531 and batch: 1250, loss is 5.150497941970825 and perplexity is 172.51737257339056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.101439984175411 and perplexity of 164.25826638666464
Finished 15 epochs...
Completing Train Step...
At time: 343.02275252342224 and batch: 50, loss is 5.14016092300415 and perplexity is 170.7432426261815
At time: 343.8964011669159 and batch: 100, loss is 5.163770399093628 and perplexity is 174.82236462104746
At time: 344.7386357784271 and batch: 150, loss is 5.082125654220581 and perplexity is 161.1161694578683
At time: 345.5846018791199 and batch: 200, loss is 5.1146456813812256 and perplexity is 166.44179710673566
At time: 346.42872500419617 and batch: 250, loss is 5.1419540119171145 and perplexity is 171.04967508972192
At time: 347.27250576019287 and batch: 300, loss is 5.123472337722778 and perplexity is 167.9174244981218
At time: 348.11540150642395 and batch: 350, loss is 5.146007776260376 and perplexity is 171.7444774946604
At time: 348.9577558040619 and batch: 400, loss is 5.1259566593170165 and perplexity is 168.33510399212318
At time: 349.8008351325989 and batch: 450, loss is 5.089396686553955 and perplexity is 162.29191961417698
At time: 350.64451718330383 and batch: 500, loss is 5.091367635726929 and perplexity is 162.61210416901253
At time: 351.48896837234497 and batch: 550, loss is 5.102958431243897 and perplexity is 164.5078733292397
At time: 352.3395719528198 and batch: 600, loss is 5.122465744018554 and perplexity is 167.74848491681652
At time: 353.1875331401825 and batch: 650, loss is 5.133740339279175 and perplexity is 169.65048317219
At time: 354.10488533973694 and batch: 700, loss is 5.14979434967041 and perplexity is 172.3960333700303
At time: 354.9488060474396 and batch: 750, loss is 5.12154263496399 and perplexity is 167.5937062212989
At time: 355.798889875412 and batch: 800, loss is 5.146355257034302 and perplexity is 171.80416576828097
At time: 356.6433804035187 and batch: 850, loss is 5.164798431396484 and perplexity is 175.00218007134882
At time: 357.48845195770264 and batch: 900, loss is 5.147455453872681 and perplexity is 171.9932881851454
At time: 358.33767557144165 and batch: 950, loss is 5.099952392578125 and perplexity is 164.01409882563627
At time: 359.18077540397644 and batch: 1000, loss is 5.113917188644409 and perplexity is 166.32058962118762
At time: 360.0241904258728 and batch: 1050, loss is 5.091868543624878 and perplexity is 162.69357826009727
At time: 360.8702208995819 and batch: 1100, loss is 5.064499807357788 and perplexity is 158.30124115139668
At time: 361.71284198760986 and batch: 1150, loss is 5.1099049949646 and perplexity is 165.6546161034432
At time: 362.5627529621124 and batch: 1200, loss is 5.128507347106933 and perplexity is 168.765022347152
At time: 363.40574979782104 and batch: 1250, loss is 5.144167098999024 and perplexity is 171.4286421048566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.101103177035812 and perplexity of 164.20295234541243
Finished 16 epochs...
Completing Train Step...
At time: 365.8162770271301 and batch: 50, loss is 5.131856632232666 and perplexity is 169.33121216252744
At time: 366.68954372406006 and batch: 100, loss is 5.155418291091919 and perplexity is 173.3683100137949
At time: 367.5313413143158 and batch: 150, loss is 5.074612884521485 and perplexity is 159.91027625278053
At time: 368.37463092803955 and batch: 200, loss is 5.107052927017212 and perplexity is 165.1828309839295
At time: 369.2176146507263 and batch: 250, loss is 5.134752321243286 and perplexity is 169.8222533008398
At time: 370.0584533214569 and batch: 300, loss is 5.115901794433594 and perplexity is 166.65099818313556
At time: 370.8978605270386 and batch: 350, loss is 5.138452024459839 and perplexity is 170.45170891911837
At time: 371.74214267730713 and batch: 400, loss is 5.118412494659424 and perplexity is 167.06993457368614
At time: 372.5864222049713 and batch: 450, loss is 5.081062574386596 and perplexity is 160.94498111674756
At time: 373.4287061691284 and batch: 500, loss is 5.083748636245727 and perplexity is 161.37787041538888
At time: 374.2729012966156 and batch: 550, loss is 5.095647439956665 and perplexity is 163.30954352763322
At time: 375.11304664611816 and batch: 600, loss is 5.115963830947876 and perplexity is 166.6613369508517
At time: 376.0123755931854 and batch: 650, loss is 5.126360559463501 and perplexity is 168.40310829783417
At time: 376.853551864624 and batch: 700, loss is 5.14369270324707 and perplexity is 171.34733637235078
At time: 377.69599175453186 and batch: 750, loss is 5.115131435394287 and perplexity is 166.5226665173516
At time: 378.54481744766235 and batch: 800, loss is 5.140237684249878 and perplexity is 170.75634959323222
At time: 379.3910074234009 and batch: 850, loss is 5.158766164779663 and perplexity is 173.94969788072112
At time: 380.2347288131714 and batch: 900, loss is 5.140608139038086 and perplexity is 170.81961881902623
At time: 381.0759174823761 and batch: 950, loss is 5.093676519393921 and perplexity is 162.98799037215548
At time: 381.9225883483887 and batch: 1000, loss is 5.10808388710022 and perplexity is 165.35321570390317
At time: 382.7662761211395 and batch: 1050, loss is 5.087026834487915 and perplexity is 161.90776714522858
At time: 383.6107485294342 and batch: 1100, loss is 5.058593635559082 and perplexity is 157.36904239801416
At time: 384.46089458465576 and batch: 1150, loss is 5.104590549468994 and perplexity is 164.77658885557568
At time: 385.30303478240967 and batch: 1200, loss is 5.122991266250611 and perplexity is 167.83666364294518
At time: 386.1459095478058 and batch: 1250, loss is 5.138847246170044 and perplexity is 170.51908844907422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.101047488024635 and perplexity of 164.1938082999778
Finished 17 epochs...
Completing Train Step...
At time: 388.6199986934662 and batch: 50, loss is 5.124100437164307 and perplexity is 168.02292646806774
At time: 389.46375942230225 and batch: 100, loss is 5.1475867080688475 and perplexity is 172.01586450751833
At time: 390.30426931381226 and batch: 150, loss is 5.068070621490478 and perplexity is 158.86751588974136
At time: 391.14493775367737 and batch: 200, loss is 5.100115261077881 and perplexity is 164.0408137313001
At time: 391.98584055900574 and batch: 250, loss is 5.127508935928344 and perplexity is 168.59660954888582
At time: 392.8251705169678 and batch: 300, loss is 5.109186353683472 and perplexity is 165.53561262343013
At time: 393.6690442562103 and batch: 350, loss is 5.13099440574646 and perplexity is 169.18527323170656
At time: 394.5130090713501 and batch: 400, loss is 5.111296815872192 and perplexity is 165.88533818623895
At time: 395.3524217605591 and batch: 450, loss is 5.073409929275512 and perplexity is 159.7180270038823
At time: 396.19497752189636 and batch: 500, loss is 5.076735458374023 and perplexity is 160.2500581023729
At time: 397.0816879272461 and batch: 550, loss is 5.0890293788909915 and perplexity is 162.232319494926
At time: 397.9278426170349 and batch: 600, loss is 5.109163265228272 and perplexity is 165.53179070597528
At time: 398.76802110671997 and batch: 650, loss is 5.119253692626953 and perplexity is 167.21053259017683
At time: 399.6119921207428 and batch: 700, loss is 5.137218437194824 and perplexity is 170.24157149976583
At time: 400.4556701183319 and batch: 750, loss is 5.108632802963257 and perplexity is 165.44400562274092
At time: 401.2960045337677 and batch: 800, loss is 5.134078912734985 and perplexity is 169.7079320473408
At time: 402.14136958122253 and batch: 850, loss is 5.1530134391784665 and perplexity is 172.95188582167648
At time: 402.9861149787903 and batch: 900, loss is 5.134030055999756 and perplexity is 169.69964087437995
At time: 403.83503699302673 and batch: 950, loss is 5.087189435958862 and perplexity is 161.93409572679923
At time: 404.67792868614197 and batch: 1000, loss is 5.1016341495513915 and perplexity is 164.29016275120017
At time: 405.51597929000854 and batch: 1050, loss is 5.081264839172364 and perplexity is 160.977537911309
At time: 406.35766100883484 and batch: 1100, loss is 5.05329478263855 and perplexity is 156.5373723826045
At time: 407.1965880393982 and batch: 1150, loss is 5.098910131454468 and perplexity is 163.84324236069116
At time: 408.04766154289246 and batch: 1200, loss is 5.117388973236084 and perplexity is 166.89902239751146
At time: 408.89246916770935 and batch: 1250, loss is 5.133063316345215 and perplexity is 169.5356647760544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1003172937100825 and perplexity of 164.07395867675427
Finished 18 epochs...
Completing Train Step...
At time: 411.37752985954285 and batch: 50, loss is 5.116014909744263 and perplexity is 166.66985002876447
At time: 412.2535934448242 and batch: 100, loss is 5.140261039733887 and perplexity is 170.76033773699692
At time: 413.0976686477661 and batch: 150, loss is 5.060481262207031 and perplexity is 157.66637693601868
At time: 413.9463002681732 and batch: 200, loss is 5.093331623077392 and perplexity is 162.93178610751679
At time: 414.7970850467682 and batch: 250, loss is 5.12084997177124 and perplexity is 167.47766042462868
At time: 415.63670778274536 and batch: 300, loss is 5.101928415298462 and perplexity is 164.3385148325103
At time: 416.48292112350464 and batch: 350, loss is 5.123845138549805 and perplexity is 167.98003592291718
At time: 417.32926654815674 and batch: 400, loss is 5.104100122451782 and perplexity is 164.69579777727998
At time: 418.1759579181671 and batch: 450, loss is 5.066083097457886 and perplexity is 158.55207645947527
At time: 419.08197808265686 and batch: 500, loss is 5.069733533859253 and perplexity is 159.13191842510525
At time: 419.93274545669556 and batch: 550, loss is 5.082395792007446 and perplexity is 161.15969890252302
At time: 420.7771337032318 and batch: 600, loss is 5.102505073547364 and perplexity is 164.43330932208596
At time: 421.62362837791443 and batch: 650, loss is 5.112862501144409 and perplexity is 166.14526584647035
At time: 422.4684171676636 and batch: 700, loss is 5.130540809631348 and perplexity is 169.1085488512904
At time: 423.31410694122314 and batch: 750, loss is 5.102796688079834 and perplexity is 164.4812674570111
At time: 424.1596622467041 and batch: 800, loss is 5.128932647705078 and perplexity is 168.83681347742532
At time: 425.0107436180115 and batch: 850, loss is 5.148338460922242 and perplexity is 172.14522654253815
At time: 425.8571922779083 and batch: 900, loss is 5.127331848144531 and perplexity is 168.56675579238825
At time: 426.7019407749176 and batch: 950, loss is 5.081375026702881 and perplexity is 160.9952766059538
At time: 427.5476188659668 and batch: 1000, loss is 5.0952535820007325 and perplexity is 163.24523542959443
At time: 428.3926885128021 and batch: 1050, loss is 5.075988883972168 and perplexity is 160.13046415954085
At time: 429.2400107383728 and batch: 1100, loss is 5.047802886962891 and perplexity is 155.68004180400808
At time: 430.0835471153259 and batch: 1150, loss is 5.093901700973511 and perplexity is 163.024696397887
At time: 430.9311854839325 and batch: 1200, loss is 5.111879968643189 and perplexity is 165.98210289243102
At time: 431.77452993392944 and batch: 1250, loss is 5.126889057159424 and perplexity is 168.49213247501996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.100606431056113 and perplexity of 164.1214054447046
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 434.25881600379944 and batch: 50, loss is 5.113616981506348 and perplexity is 166.2706664869923
At time: 435.0979459285736 and batch: 100, loss is 5.144452991485596 and perplexity is 171.47765927210375
At time: 435.9387400150299 and batch: 150, loss is 5.060572013854981 and perplexity is 157.6806860688307
At time: 436.77919006347656 and batch: 200, loss is 5.093976840972901 and perplexity is 163.03694653370667
At time: 437.6182599067688 and batch: 250, loss is 5.118071632385254 and perplexity is 167.01299644042405
At time: 438.458936214447 and batch: 300, loss is 5.098093576431275 and perplexity is 163.70950994549364
At time: 439.3035945892334 and batch: 350, loss is 5.116656246185303 and perplexity is 166.77677576123
At time: 440.14460587501526 and batch: 400, loss is 5.089170198440552 and perplexity is 162.25516658570223
At time: 441.01484274864197 and batch: 450, loss is 5.052221097946167 and perplexity is 156.369390797857
At time: 441.85448837280273 and batch: 500, loss is 5.0505908107757564 and perplexity is 156.11467147543289
At time: 442.69596767425537 and batch: 550, loss is 5.062796573638916 and perplexity is 158.03184662577448
At time: 443.5355384349823 and batch: 600, loss is 5.080646982192993 and perplexity is 160.87810753602744
At time: 444.3738296031952 and batch: 650, loss is 5.092781934738159 and perplexity is 162.84224901560526
At time: 445.21671056747437 and batch: 700, loss is 5.109626836776734 and perplexity is 165.60854432353466
At time: 446.05881357192993 and batch: 750, loss is 5.0746364402771 and perplexity is 159.91404310453368
At time: 446.89743161201477 and batch: 800, loss is 5.093926038742065 and perplexity is 163.02866410349893
At time: 447.75120520591736 and batch: 850, loss is 5.104887495040893 and perplexity is 164.82552579943385
At time: 448.5963137149811 and batch: 900, loss is 5.084439811706543 and perplexity is 161.48944939522306
At time: 449.438946723938 and batch: 950, loss is 5.037460660934448 and perplexity is 154.07826089107894
At time: 450.283486366272 and batch: 1000, loss is 5.049048433303833 and perplexity is 155.87406932048208
At time: 451.1251132488251 and batch: 1050, loss is 5.024361724853516 and perplexity is 152.07316061027805
At time: 451.96971011161804 and batch: 1100, loss is 4.9906208705902095 and perplexity is 147.02768032608344
At time: 452.80791187286377 and batch: 1150, loss is 5.031401224136353 and perplexity is 153.14745633096078
At time: 453.65298771858215 and batch: 1200, loss is 5.060369882583618 and perplexity is 157.64881709225318
At time: 454.4962866306305 and batch: 1250, loss is 5.0880210399627686 and perplexity is 162.06881677869362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.081239129505018 and perplexity of 160.97339928556084
Finished 20 epochs...
Completing Train Step...
At time: 456.9355297088623 and batch: 50, loss is 5.096675796508789 and perplexity is 163.47757034767724
At time: 457.8189573287964 and batch: 100, loss is 5.126687278747559 and perplexity is 168.45813782992545
At time: 458.6660945415497 and batch: 150, loss is 5.04356390953064 and perplexity is 155.02151434774768
At time: 459.5091700553894 and batch: 200, loss is 5.0750425338745115 and perplexity is 159.97899636123282
At time: 460.35280752182007 and batch: 250, loss is 5.1021351146697995 and perplexity is 164.3724870111057
At time: 461.20650839805603 and batch: 300, loss is 5.083247632980346 and perplexity is 161.29703982523614
At time: 462.05269289016724 and batch: 350, loss is 5.1026702499389645 and perplexity is 164.46047206603905
At time: 462.9539792537689 and batch: 400, loss is 5.077258434295654 and perplexity is 160.33388694252218
At time: 463.8019061088562 and batch: 450, loss is 5.040561714172363 and perplexity is 154.5568073965063
At time: 464.65010499954224 and batch: 500, loss is 5.039406394958496 and perplexity is 154.37834805588972
At time: 465.4929642677307 and batch: 550, loss is 5.052152643203735 and perplexity is 156.35868693785486
At time: 466.338214635849 and batch: 600, loss is 5.071242427825927 and perplexity is 159.37221286054304
At time: 467.18090534210205 and batch: 650, loss is 5.085236072540283 and perplexity is 161.61808832711364
At time: 468.0258574485779 and batch: 700, loss is 5.101937627792358 and perplexity is 164.34002880704875
At time: 468.8691487312317 and batch: 750, loss is 5.068186817169189 and perplexity is 158.88597668108656
At time: 469.712694644928 and batch: 800, loss is 5.0899278259277345 and perplexity is 162.3781421387922
At time: 470.5606298446655 and batch: 850, loss is 5.102850637435913 and perplexity is 164.49014135484592
At time: 471.4061095714569 and batch: 900, loss is 5.0829283714294435 and perplexity is 161.24555210160807
At time: 472.250928401947 and batch: 950, loss is 5.037013301849365 and perplexity is 154.0093479968095
At time: 473.0949788093567 and batch: 1000, loss is 5.050215187072754 and perplexity is 156.05604211641813
At time: 473.94201469421387 and batch: 1050, loss is 5.027373428344727 and perplexity is 152.53185025089698
At time: 474.7839150428772 and batch: 1100, loss is 4.995396909713745 and perplexity is 147.73156984349194
At time: 475.62852239608765 and batch: 1150, loss is 5.037925853729248 and perplexity is 154.14995366206813
At time: 476.4727714061737 and batch: 1200, loss is 5.06687481880188 and perplexity is 158.6776552276787
At time: 477.317120552063 and batch: 1250, loss is 5.0904216289520265 and perplexity is 162.45834475697725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.080456364763914 and perplexity of 160.84744428734146
Finished 21 epochs...
Completing Train Step...
At time: 479.7529733181 and batch: 50, loss is 5.090545997619629 and perplexity is 162.47855074132525
At time: 480.6243190765381 and batch: 100, loss is 5.120741767883301 and perplexity is 167.4595396710134
At time: 481.46570920944214 and batch: 150, loss is 5.037312602996826 and perplexity is 154.05545007024224
At time: 482.3064181804657 and batch: 200, loss is 5.067861442565918 and perplexity is 158.83428762906703
At time: 483.1501851081848 and batch: 250, loss is 5.0953919887542725 and perplexity is 163.26783123633118
At time: 484.02205181121826 and batch: 300, loss is 5.076982860565185 and perplexity is 160.28970922256602
At time: 484.86553263664246 and batch: 350, loss is 5.096723423004151 and perplexity is 163.4853563968328
At time: 485.7082381248474 and batch: 400, loss is 5.071883506774903 and perplexity is 159.47441578783184
At time: 486.5500671863556 and batch: 450, loss is 5.035163793563843 and perplexity is 153.72476967774202
At time: 487.3950366973877 and batch: 500, loss is 5.0349218368530275 and perplexity is 153.6875794374955
At time: 488.24395990371704 and batch: 550, loss is 5.047964448928833 and perplexity is 155.70519580953052
At time: 489.0881402492523 and batch: 600, loss is 5.067251424789429 and perplexity is 158.7374254369306
At time: 489.92807173728943 and batch: 650, loss is 5.082015542984009 and perplexity is 161.0984297339069
At time: 490.7664361000061 and batch: 700, loss is 5.0987107467651365 and perplexity is 163.81057778323054
At time: 491.609801530838 and batch: 750, loss is 5.065521450042724 and perplexity is 158.46305109833548
At time: 492.4524178504944 and batch: 800, loss is 5.08775993347168 and perplexity is 162.02650508279922
At time: 493.2960958480835 and batch: 850, loss is 5.1018216419219975 and perplexity is 164.32096879114013
At time: 494.1412055492401 and batch: 900, loss is 5.082273015975952 and perplexity is 161.13991356886217
At time: 494.9855854511261 and batch: 950, loss is 5.03686710357666 and perplexity is 153.98683374196466
At time: 495.82718896865845 and batch: 1000, loss is 5.050795850753784 and perplexity is 156.14668450610534
At time: 496.6685309410095 and batch: 1050, loss is 5.028645467758179 and perplexity is 152.7260002332585
At time: 497.51098108291626 and batch: 1100, loss is 4.997217626571655 and perplexity is 148.00079221763622
At time: 498.3532679080963 and batch: 1150, loss is 5.040229539871216 and perplexity is 154.50547612295878
At time: 499.19789481163025 and batch: 1200, loss is 5.069169454574585 and perplexity is 159.04218071836226
At time: 500.0391626358032 and batch: 1250, loss is 5.090826053619384 and perplexity is 162.52406020659382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.080066096173586 and perplexity of 160.78468282971096
Finished 22 epochs...
Completing Train Step...
At time: 502.4986550807953 and batch: 50, loss is 5.086610097885131 and perplexity is 161.84030830964346
At time: 503.34218740463257 and batch: 100, loss is 5.116435594558716 and perplexity is 166.73998025402489
At time: 504.1850745677948 and batch: 150, loss is 5.032945718765259 and perplexity is 153.3841745126177
At time: 505.02783608436584 and batch: 200, loss is 5.063679113388061 and perplexity is 158.17137757379996
At time: 505.9095573425293 and batch: 250, loss is 5.09125846862793 and perplexity is 162.59435324626432
At time: 506.7541341781616 and batch: 300, loss is 5.072895154953003 and perplexity is 159.635829423156
At time: 507.60069704055786 and batch: 350, loss is 5.092761087417602 and perplexity is 162.83885422642606
At time: 508.4433777332306 and batch: 400, loss is 5.06812783241272 and perplexity is 158.87660510683835
At time: 509.2878248691559 and batch: 450, loss is 5.031410989761352 and perplexity is 153.14895191889156
At time: 510.13122153282166 and batch: 500, loss is 5.031619071960449 and perplexity is 153.18082280536333
At time: 510.9754636287689 and batch: 550, loss is 5.045146980285645 and perplexity is 155.2671187267703
At time: 511.8217701911926 and batch: 600, loss is 5.064720411300659 and perplexity is 158.33616688159123
At time: 512.6647152900696 and batch: 650, loss is 5.080044422149658 and perplexity is 160.7811980164132
At time: 513.5076971054077 and batch: 700, loss is 5.096803903579712 and perplexity is 163.49851432188302
At time: 514.3494226932526 and batch: 750, loss is 5.063570709228515 and perplexity is 158.15423206788873
At time: 515.1930632591248 and batch: 800, loss is 5.086079053878784 and perplexity is 161.75438680000076
At time: 516.035475730896 and batch: 850, loss is 5.100809555053711 and perplexity is 164.15474582667113
At time: 516.8807430267334 and batch: 900, loss is 5.081568212509155 and perplexity is 161.02638161269738
At time: 517.7295508384705 and batch: 950, loss is 5.0364345264434816 and perplexity is 153.92023696403743
At time: 518.5730483531952 and batch: 1000, loss is 5.050564880371094 and perplexity is 156.11062341131193
At time: 519.4169056415558 and batch: 1050, loss is 5.028942880630493 and perplexity is 152.7714296669792
At time: 520.2618610858917 and batch: 1100, loss is 4.99774808883667 and perplexity is 148.07932187976976
At time: 521.1089475154877 and batch: 1150, loss is 5.040935258865357 and perplexity is 154.6145520561094
At time: 521.9567365646362 and batch: 1200, loss is 5.070000438690186 and perplexity is 159.17439717152766
At time: 522.7998626232147 and batch: 1250, loss is 5.090404081344604 and perplexity is 162.45549402673282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079772058194571 and perplexity of 160.7374129764218
Finished 23 epochs...
Completing Train Step...
At time: 525.2594153881073 and batch: 50, loss is 5.083378562927246 and perplexity is 161.31815982068525
At time: 526.1351175308228 and batch: 100, loss is 5.11296085357666 and perplexity is 166.16160744107796
At time: 526.9731221199036 and batch: 150, loss is 5.029581842422485 and perplexity is 152.86907596624943
At time: 527.8503177165985 and batch: 200, loss is 5.060239953994751 and perplexity is 157.62833533452354
At time: 528.6931283473969 and batch: 250, loss is 5.088032560348511 and perplexity is 162.07068388473456
At time: 529.5349633693695 and batch: 300, loss is 5.069916267395019 and perplexity is 159.16099982020447
At time: 530.3789563179016 and batch: 350, loss is 5.089634094238281 and perplexity is 162.3304535369412
At time: 531.2233235836029 and batch: 400, loss is 5.064974193572998 and perplexity is 158.37635489310267
At time: 532.0687758922577 and batch: 450, loss is 5.028361616134643 and perplexity is 152.68265486225417
At time: 532.9144222736359 and batch: 500, loss is 5.028850393295288 and perplexity is 152.7573008979301
At time: 533.7595703601837 and batch: 550, loss is 5.042874393463134 and perplexity is 154.91466136545338
At time: 534.6009657382965 and batch: 600, loss is 5.06262541770935 and perplexity is 158.00480085275242
At time: 535.441180229187 and batch: 650, loss is 5.078397665023804 and perplexity is 160.51664831721143
At time: 536.2834656238556 and batch: 700, loss is 5.095100364685059 and perplexity is 163.220225348863
At time: 537.1254937648773 and batch: 750, loss is 5.061839199066162 and perplexity is 157.88062335433744
At time: 537.9655058383942 and batch: 800, loss is 5.084508399963379 and perplexity is 161.50052605491464
At time: 538.8084900379181 and batch: 850, loss is 5.099751024246216 and perplexity is 163.98107490524373
At time: 539.6514117717743 and batch: 900, loss is 5.080833120346069 and perplexity is 160.90805587701252
At time: 540.4911124706268 and batch: 950, loss is 5.035905447006225 and perplexity is 153.83882247094843
At time: 541.3353340625763 and batch: 1000, loss is 5.050154829025269 and perplexity is 156.04662316267536
At time: 542.1758899688721 and batch: 1050, loss is 5.028841848373413 and perplexity is 152.7559956043049
At time: 543.0161521434784 and batch: 1100, loss is 4.997683525085449 and perplexity is 148.06976163189702
At time: 543.8568391799927 and batch: 1150, loss is 5.041203660964966 and perplexity is 154.65605649620161
At time: 544.6987018585205 and batch: 1200, loss is 5.070158443450928 and perplexity is 159.19954947110818
At time: 545.5433568954468 and batch: 1250, loss is 5.0896021842956545 and perplexity is 162.3252736641274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079668253877737 and perplexity of 160.720728605048
Finished 24 epochs...
Completing Train Step...
At time: 547.9996469020844 and batch: 50, loss is 5.080659103393555 and perplexity is 160.8800575836533
At time: 548.8374297618866 and batch: 100, loss is 5.110164041519165 and perplexity is 165.69753391960884
At time: 549.706333398819 and batch: 150, loss is 5.027076387405396 and perplexity is 152.48654877535012
At time: 550.5437932014465 and batch: 200, loss is 5.05774266242981 and perplexity is 157.2351825352053
At time: 551.3818867206573 and batch: 250, loss is 5.085497541427612 and perplexity is 161.66035195391171
At time: 552.2241926193237 and batch: 300, loss is 5.067061128616333 and perplexity is 158.7072211863107
At time: 553.0650856494904 and batch: 350, loss is 5.087145528793335 and perplexity is 161.92698581574277
At time: 553.9060726165771 and batch: 400, loss is 5.062273550033569 and perplexity is 157.94921385092218
At time: 554.7490148544312 and batch: 450, loss is 5.025908784866333 and perplexity is 152.30860899551183
At time: 555.5930912494659 and batch: 500, loss is 5.026693305969238 and perplexity is 152.42814519662818
At time: 556.4350745677948 and batch: 550, loss is 5.0407885074615475 and perplexity is 154.5918638183519
At time: 557.2740437984467 and batch: 600, loss is 5.060776395797729 and perplexity is 157.7129164473249
At time: 558.1168584823608 and batch: 650, loss is 5.076672887802124 and perplexity is 160.24003147827952
At time: 558.9547650814056 and batch: 700, loss is 5.093557987213135 and perplexity is 162.96867219515016
At time: 559.7963943481445 and batch: 750, loss is 5.060263576507569 and perplexity is 157.6320589558759
At time: 560.6381316184998 and batch: 800, loss is 5.082982931137085 and perplexity is 161.2543498517884
At time: 561.4727947711945 and batch: 850, loss is 5.098562526702881 and perplexity is 163.78629956849707
At time: 562.3158354759216 and batch: 900, loss is 5.079926128387451 and perplexity is 160.76217972850228
At time: 563.1574811935425 and batch: 950, loss is 5.035124473571777 and perplexity is 153.7187253398504
At time: 563.9992439746857 and batch: 1000, loss is 5.049434337615967 and perplexity is 155.93423340402575
At time: 564.8418774604797 and batch: 1050, loss is 5.028310117721557 and perplexity is 152.67479215028337
At time: 565.681517124176 and batch: 1100, loss is 4.997203063964844 and perplexity is 147.99863695598455
At time: 566.5266366004944 and batch: 1150, loss is 5.040832195281983 and perplexity is 154.59861774747026
At time: 567.3665041923523 and batch: 1200, loss is 5.069602813720703 and perplexity is 159.11111803822533
At time: 568.2047252655029 and batch: 1250, loss is 5.088607225418091 and perplexity is 162.16384701179967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079457972171533 and perplexity of 160.68693552917193
Finished 25 epochs...
Completing Train Step...
At time: 570.6457192897797 and batch: 50, loss is 5.078088321685791 and perplexity is 160.46700124080644
At time: 571.5211982727051 and batch: 100, loss is 5.107663602828979 and perplexity is 165.28373495000056
At time: 572.3669509887695 and batch: 150, loss is 5.0248682498931885 and perplexity is 152.15020898580772
At time: 573.213972568512 and batch: 200, loss is 5.055403327941894 and perplexity is 156.86778674835176
At time: 574.059072971344 and batch: 250, loss is 5.08322883605957 and perplexity is 161.29400796605196
At time: 574.9073576927185 and batch: 300, loss is 5.064876546859741 and perplexity is 158.36089071761506
At time: 575.751503944397 and batch: 350, loss is 5.084766073226929 and perplexity is 161.54214578444365
At time: 576.595415353775 and batch: 400, loss is 5.0599838066101075 and perplexity is 157.5879644193548
At time: 577.4414553642273 and batch: 450, loss is 5.023777122497559 and perplexity is 151.98428426349952
At time: 578.2862672805786 and batch: 500, loss is 5.024724655151367 and perplexity is 152.12836258438205
At time: 579.130576133728 and batch: 550, loss is 5.038887672424316 and perplexity is 154.29828929390078
At time: 579.9767045974731 and batch: 600, loss is 5.059068584442139 and perplexity is 157.44380240111352
At time: 580.8231616020203 and batch: 650, loss is 5.07525601387024 and perplexity is 160.0131523223701
At time: 581.6719226837158 and batch: 700, loss is 5.09231746673584 and perplexity is 162.76663156381454
At time: 582.5158500671387 and batch: 750, loss is 5.059000377655029 and perplexity is 157.43306403142037
At time: 583.3619673252106 and batch: 800, loss is 5.081719408035278 and perplexity is 161.05072992181633
At time: 584.2083411216736 and batch: 850, loss is 5.097272853851319 and perplexity is 163.5752049752275
At time: 585.054924249649 and batch: 900, loss is 5.07889651298523 and perplexity is 160.5967416955477
At time: 585.9066548347473 and batch: 950, loss is 5.034311218261719 and perplexity is 153.593763590003
At time: 586.7531712055206 and batch: 1000, loss is 5.048586854934692 and perplexity is 155.80213782406233
At time: 587.6042201519012 and batch: 1050, loss is 5.027709140777588 and perplexity is 152.5830656857816
At time: 588.4507098197937 and batch: 1100, loss is 4.996544609069824 and perplexity is 147.9012186053393
At time: 589.2997324466705 and batch: 1150, loss is 5.040469360351563 and perplexity is 154.54253414391732
At time: 590.1447503566742 and batch: 1200, loss is 5.069249677658081 and perplexity is 159.05494008429645
At time: 590.9881625175476 and batch: 1250, loss is 5.087595024108887 and perplexity is 161.99978759813268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0794143119867705 and perplexity of 160.67992006102722
Finished 26 epochs...
Completing Train Step...
At time: 593.442654132843 and batch: 50, loss is 5.075907793045044 and perplexity is 160.1174795582151
At time: 594.3426520824432 and batch: 100, loss is 5.105514745712281 and perplexity is 164.92894515269188
At time: 595.1825122833252 and batch: 150, loss is 5.0227148151397705 and perplexity is 151.8229159665981
At time: 596.0237443447113 and batch: 200, loss is 5.053292503356934 and perplexity is 156.53701559025606
At time: 596.8671822547913 and batch: 250, loss is 5.081118354797363 and perplexity is 160.95395894429623
At time: 597.7155678272247 and batch: 300, loss is 5.062822046279908 and perplexity is 158.03587216553908
At time: 598.5576837062836 and batch: 350, loss is 5.082671985626221 and perplexity is 161.20421633039533
At time: 599.4005925655365 and batch: 400, loss is 5.057838726043701 and perplexity is 157.25028784059396
At time: 600.2423460483551 and batch: 450, loss is 5.02185170173645 and perplexity is 151.69193210798966
At time: 601.0864684581757 and batch: 500, loss is 5.022910623550415 and perplexity is 151.8526470811791
At time: 601.929361820221 and batch: 550, loss is 5.037141008377075 and perplexity is 154.0290172517965
At time: 602.774619102478 and batch: 600, loss is 5.05757061958313 and perplexity is 157.20813367365196
At time: 603.6176438331604 and batch: 650, loss is 5.073887128829956 and perplexity is 159.79426256354597
At time: 604.4659955501556 and batch: 700, loss is 5.091078853607177 and perplexity is 162.56515148074809
At time: 605.3084659576416 and batch: 750, loss is 5.057697982788086 and perplexity is 157.2281574805228
At time: 606.1507413387299 and batch: 800, loss is 5.080366983413696 and perplexity is 160.83306816807996
At time: 606.9960005283356 and batch: 850, loss is 5.0961915969848635 and perplexity is 163.39843374644084
At time: 607.8395090103149 and batch: 900, loss is 5.077802143096924 and perplexity is 160.42108559118284
At time: 608.6863505840302 and batch: 950, loss is 5.033371419906616 and perplexity is 153.44948423099842
At time: 609.5269029140472 and batch: 1000, loss is 5.047611646652221 and perplexity is 155.65027235109892
At time: 610.3706085681915 and batch: 1050, loss is 5.026896171569824 and perplexity is 152.45907076061013
At time: 611.2127692699432 and batch: 1100, loss is 4.995742998123169 and perplexity is 147.7827068759775
At time: 612.0564353466034 and batch: 1150, loss is 5.0398118114471435 and perplexity is 154.4409482724043
At time: 612.8990323543549 and batch: 1200, loss is 5.068629961013794 and perplexity is 158.956401626685
At time: 613.8010120391846 and batch: 1250, loss is 5.086632652282715 and perplexity is 161.84395856146656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0792721936302465 and perplexity of 160.65708611746143
Finished 27 epochs...
Completing Train Step...
At time: 616.3002600669861 and batch: 50, loss is 5.073879699707032 and perplexity is 159.7930754367365
At time: 617.1436083316803 and batch: 100, loss is 5.103475942611694 and perplexity is 164.59303005666644
At time: 617.9897301197052 and batch: 150, loss is 5.020781059265136 and perplexity is 151.52961119225773
At time: 618.8342726230621 and batch: 200, loss is 5.0515244102478025 and perplexity is 156.2604881068748
At time: 619.6814422607422 and batch: 250, loss is 5.079140329360962 and perplexity is 160.63590258490143
At time: 620.5291147232056 and batch: 300, loss is 5.060775194168091 and perplexity is 157.71272693492398
At time: 621.3751096725464 and batch: 350, loss is 5.080423583984375 and perplexity is 160.8421716691516
At time: 622.2210712432861 and batch: 400, loss is 5.055810585021972 and perplexity is 156.9316852758219
At time: 623.0684802532196 and batch: 450, loss is 5.019754238128662 and perplexity is 151.374097240854
At time: 623.9153468608856 and batch: 500, loss is 5.021029415130616 and perplexity is 151.56724913359943
At time: 624.7630994319916 and batch: 550, loss is 5.035509767532349 and perplexity is 153.77796364769895
At time: 625.6083030700684 and batch: 600, loss is 5.05598967552185 and perplexity is 156.9597927666018
At time: 626.4541888237 and batch: 650, loss is 5.072704381942749 and perplexity is 159.6053781201683
At time: 627.306049823761 and batch: 700, loss is 5.08968186378479 and perplexity is 162.33820817430748
At time: 628.1537554264069 and batch: 750, loss is 5.056257638931275 and perplexity is 157.00185788351843
At time: 629.0062267780304 and batch: 800, loss is 5.078773384094238 and perplexity is 160.57696881417732
At time: 629.862401008606 and batch: 850, loss is 5.094979124069214 and perplexity is 163.2004376277854
At time: 630.7079076766968 and batch: 900, loss is 5.076620273590088 and perplexity is 160.23160079707543
At time: 631.5542984008789 and batch: 950, loss is 5.0324994087219235 and perplexity is 153.31573288927208
At time: 632.4045920372009 and batch: 1000, loss is 5.046556301116944 and perplexity is 155.48609417869687
At time: 633.2503111362457 and batch: 1050, loss is 5.026034536361695 and perplexity is 152.32776323516137
At time: 634.0953485965729 and batch: 1100, loss is 4.99495641708374 and perplexity is 147.66650950610844
At time: 634.9396932125092 and batch: 1150, loss is 5.039124441146851 and perplexity is 154.33482662802444
At time: 635.8430850505829 and batch: 1200, loss is 5.067914514541626 and perplexity is 158.84271750221475
At time: 636.6901545524597 and batch: 1250, loss is 5.085578165054321 and perplexity is 161.67338612320526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079192001454151 and perplexity of 160.64420319268234
Finished 28 epochs...
Completing Train Step...
At time: 639.1893110275269 and batch: 50, loss is 5.071876144409179 and perplexity is 159.4732416831814
At time: 640.0874192714691 and batch: 100, loss is 5.1015878105163575 and perplexity is 164.28254987998062
At time: 640.9300625324249 and batch: 150, loss is 5.019108791351318 and perplexity is 151.27642484217483
At time: 641.7711412906647 and batch: 200, loss is 5.049809808731079 and perplexity is 155.99279319759367
At time: 642.6103734970093 and batch: 250, loss is 5.077324390411377 and perplexity is 160.3444622916743
At time: 643.4557750225067 and batch: 300, loss is 5.058950538635254 and perplexity is 157.4252179173518
At time: 644.2953245639801 and batch: 350, loss is 5.07865569114685 and perplexity is 160.55807114951773
At time: 645.1375203132629 and batch: 400, loss is 5.053890800476074 and perplexity is 156.63069925826224
At time: 645.9837634563446 and batch: 450, loss is 5.017880630493164 and perplexity is 151.09074710282746
At time: 646.8263125419617 and batch: 500, loss is 5.019203310012817 and perplexity is 151.29072396312355
At time: 647.6670179367065 and batch: 550, loss is 5.033776407241821 and perplexity is 153.51164191439346
At time: 648.5088291168213 and batch: 600, loss is 5.054424676895142 and perplexity is 156.71434302082784
At time: 649.350191116333 and batch: 650, loss is 5.071436166763306 and perplexity is 159.4030924549036
At time: 650.1929562091827 and batch: 700, loss is 5.0884206676483155 and perplexity is 162.13359690794664
At time: 651.0349354743958 and batch: 750, loss is 5.054985466003418 and perplexity is 156.8022513642216
At time: 651.8814871311188 and batch: 800, loss is 5.077001447677612 and perplexity is 160.2926885731009
At time: 652.7221722602844 and batch: 850, loss is 5.093706092834473 and perplexity is 162.99281055907377
At time: 653.566440820694 and batch: 900, loss is 5.075441732406616 and perplexity is 160.04287249052803
At time: 654.4120750427246 and batch: 950, loss is 5.0314186668396 and perplexity is 153.15012765989212
At time: 655.2541086673737 and batch: 1000, loss is 5.0455224609375 and perplexity is 155.3254294723148
At time: 656.0943088531494 and batch: 1050, loss is 5.024858741760254 and perplexity is 152.14876232827223
At time: 656.9352781772614 and batch: 1100, loss is 4.9938729763031 and perplexity is 147.50660822520067
At time: 657.8355708122253 and batch: 1150, loss is 5.038285369873047 and perplexity is 154.20538302225344
At time: 658.6793160438538 and batch: 1200, loss is 5.067138290405273 and perplexity is 158.719467791894
At time: 659.522134065628 and batch: 1250, loss is 5.084590101242066 and perplexity is 161.51372139343255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079108245181342 and perplexity of 160.63074879642735
Finished 29 epochs...
Completing Train Step...
At time: 661.9796831607819 and batch: 50, loss is 5.069894876480102 and perplexity is 159.15759525721273
At time: 662.8271942138672 and batch: 100, loss is 5.099805660247803 and perplexity is 163.99003442026628
At time: 663.6706073284149 and batch: 150, loss is 5.017099008560181 and perplexity is 150.97269740214873
At time: 664.5156261920929 and batch: 200, loss is 5.047971897125244 and perplexity is 155.70635553673
At time: 665.3583772182465 and batch: 250, loss is 5.0756692218780515 and perplexity is 160.0792847005381
At time: 666.204850435257 and batch: 300, loss is 5.057163801193237 and perplexity is 157.14419152113646
At time: 667.0533428192139 and batch: 350, loss is 5.076784257888794 and perplexity is 160.25787841826246
At time: 667.9002947807312 and batch: 400, loss is 5.0522263240814205 and perplexity is 156.3702080075782
At time: 668.7471995353699 and batch: 450, loss is 5.016187210083007 and perplexity is 150.83510346506648
At time: 669.595890045166 and batch: 500, loss is 5.017672653198242 and perplexity is 151.05932692541228
At time: 670.4424524307251 and batch: 550, loss is 5.0323869991302494 and perplexity is 153.2984996989469
At time: 671.2849369049072 and batch: 600, loss is 5.0531079292297365 and perplexity is 156.50812557348624
At time: 672.1251225471497 and batch: 650, loss is 5.070196599960327 and perplexity is 159.20562408610624
At time: 672.9690186977386 and batch: 700, loss is 5.087150688171387 and perplexity is 161.9278212604346
At time: 673.8117337226868 and batch: 750, loss is 5.053728847503662 and perplexity is 156.60533450495052
At time: 674.6557004451752 and batch: 800, loss is 5.075626544952392 and perplexity is 160.07245315458107
At time: 675.499589920044 and batch: 850, loss is 5.0927323150634765 and perplexity is 162.8341690366491
At time: 676.3459846973419 and batch: 900, loss is 5.074258842468262 and perplexity is 159.85367131110073
At time: 677.1980168819427 and batch: 950, loss is 5.030329437255859 and perplexity is 152.98340282738508
At time: 678.0402903556824 and batch: 1000, loss is 5.044393901824951 and perplexity is 155.15023442105377
At time: 678.886670589447 and batch: 1050, loss is 5.023803844451904 and perplexity is 151.98834563486847
At time: 679.7963218688965 and batch: 1100, loss is 4.99279070854187 and perplexity is 147.34705293489748
At time: 680.6388165950775 and batch: 1150, loss is 5.037280702590943 and perplexity is 154.05053571724184
At time: 681.4839885234833 and batch: 1200, loss is 5.066266860961914 and perplexity is 158.58121522185488
At time: 682.328962802887 and batch: 1250, loss is 5.083419322967529 and perplexity is 161.32473528938527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078941178147811 and perplexity of 160.60391493532126
Finished 30 epochs...
Completing Train Step...
At time: 684.7616446018219 and batch: 50, loss is 5.067994585037232 and perplexity is 158.85543662653498
At time: 685.6355187892914 and batch: 100, loss is 5.097985591888428 and perplexity is 163.69183280334866
At time: 686.477801322937 and batch: 150, loss is 5.015231523513794 and perplexity is 150.69102124220794
At time: 687.3253607749939 and batch: 200, loss is 5.046347570419312 and perplexity is 155.45364284469952
At time: 688.1705770492554 and batch: 250, loss is 5.074010095596313 and perplexity is 159.81391315545605
At time: 689.0152044296265 and batch: 300, loss is 5.055303649902344 and perplexity is 156.85215125417074
At time: 689.8575992584229 and batch: 350, loss is 5.074990854263306 and perplexity is 159.97072892253067
At time: 690.7000670433044 and batch: 400, loss is 5.050336866378784 and perplexity is 156.07503206264298
At time: 691.5406517982483 and batch: 450, loss is 5.01419732093811 and perplexity is 150.53525675980475
At time: 692.3831746578217 and batch: 500, loss is 5.015840740203857 and perplexity is 150.78285269717864
At time: 693.2304100990295 and batch: 550, loss is 5.03083607673645 and perplexity is 153.06092989661127
At time: 694.0716450214386 and batch: 600, loss is 5.0515537929534915 and perplexity is 156.26507953026154
At time: 694.9151978492737 and batch: 650, loss is 5.068721590042114 and perplexity is 158.97096731462094
At time: 695.7603344917297 and batch: 700, loss is 5.085690135955811 and perplexity is 161.6914898515228
At time: 696.6032295227051 and batch: 750, loss is 5.052162418365478 and perplexity is 156.36021537678
At time: 697.4512820243835 and batch: 800, loss is 5.074070959091187 and perplexity is 159.8236402847506
At time: 698.298567533493 and batch: 850, loss is 5.091236705780029 and perplexity is 162.59081476858893
At time: 699.1419129371643 and batch: 900, loss is 5.072582311630249 and perplexity is 159.58589623088884
At time: 699.9843273162842 and batch: 950, loss is 5.029212074279785 and perplexity is 152.8125603014433
At time: 700.8847875595093 and batch: 1000, loss is 5.043281888961792 and perplexity is 154.97780125637277
At time: 701.7328922748566 and batch: 1050, loss is 5.022669239044189 and perplexity is 151.815996628548
At time: 702.574378490448 and batch: 1100, loss is 4.991908531188965 and perplexity is 147.2171240203803
At time: 703.4202103614807 and batch: 1150, loss is 5.036175680160523 and perplexity is 153.88040043881782
At time: 704.2637071609497 and batch: 1200, loss is 5.065237283706665 and perplexity is 158.41802763108393
At time: 705.1037776470184 and batch: 1250, loss is 5.08213357925415 and perplexity is 161.11744631397931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078697928546989 and perplexity of 160.56485284823395
Finished 31 epochs...
Completing Train Step...
At time: 707.5594441890717 and batch: 50, loss is 5.066269721984863 and perplexity is 158.58166892699998
At time: 708.399840593338 and batch: 100, loss is 5.096193771362305 and perplexity is 163.39878903669546
At time: 709.2409563064575 and batch: 150, loss is 5.0132972240448 and perplexity is 150.39982140447677
At time: 710.0883462429047 and batch: 200, loss is 5.0446998882293705 and perplexity is 155.1977155473482
At time: 710.9430456161499 and batch: 250, loss is 5.07228048324585 and perplexity is 159.53773594609257
At time: 711.7825059890747 and batch: 300, loss is 5.053575000762939 and perplexity is 156.58124313789378
At time: 712.6293110847473 and batch: 350, loss is 5.073072643280029 and perplexity is 159.66416543403489
At time: 713.4720525741577 and batch: 400, loss is 5.048556842803955 and perplexity is 155.79746194009982
At time: 714.3182713985443 and batch: 450, loss is 5.0125524616241455 and perplexity is 150.2878509702829
At time: 715.1636214256287 and batch: 500, loss is 5.014291934967041 and perplexity is 150.54950018074598
At time: 716.0087175369263 and batch: 550, loss is 5.0294648456573485 and perplexity is 152.85119182508572
At time: 716.8483228683472 and batch: 600, loss is 5.050260581970215 and perplexity is 156.06312642524261
At time: 717.6885046958923 and batch: 650, loss is 5.067529392242432 and perplexity is 158.7815554078392
At time: 718.5328404903412 and batch: 700, loss is 5.084480438232422 and perplexity is 161.49601028379033
At time: 719.3807101249695 and batch: 750, loss is 5.050811548233032 and perplexity is 156.14913563468323
At time: 720.2222023010254 and batch: 800, loss is 5.072657480239868 and perplexity is 159.5978925316902
At time: 721.06569480896 and batch: 850, loss is 5.0901452159881595 and perplexity is 162.41344537007024
At time: 721.9099531173706 and batch: 900, loss is 5.071392087936402 and perplexity is 159.3960663084366
At time: 722.8241736888885 and batch: 950, loss is 5.028149499893188 and perplexity is 152.6502718259748
At time: 723.6709089279175 and batch: 1000, loss is 5.042197227478027 and perplexity is 154.8097939365175
At time: 724.5159285068512 and batch: 1050, loss is 5.021445999145508 and perplexity is 151.63040278025616
At time: 725.3578913211823 and batch: 1100, loss is 4.990547304153442 and perplexity is 147.01686442138435
At time: 726.2020058631897 and batch: 1150, loss is 5.0352334213256835 and perplexity is 153.73547356203358
At time: 727.0433809757233 and batch: 1200, loss is 5.064211406707764 and perplexity is 158.25559355326382
At time: 727.8862040042877 and batch: 1250, loss is 5.081216869354248 and perplexity is 160.96981603330468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078687236256843 and perplexity of 160.56313605141835
Finished 32 epochs...
Completing Train Step...
At time: 730.382598400116 and batch: 50, loss is 5.0643753242492675 and perplexity is 158.2815365472861
At time: 731.2272176742554 and batch: 100, loss is 5.094638175964356 and perplexity is 163.14480423245345
At time: 732.0723986625671 and batch: 150, loss is 5.011535120010376 and perplexity is 150.13503463183727
At time: 732.9182572364807 and batch: 200, loss is 5.042869672775269 and perplexity is 154.9139300634174
At time: 733.7620482444763 and batch: 250, loss is 5.070582332611084 and perplexity is 159.26704673910027
At time: 734.609210729599 and batch: 300, loss is 5.052194623947144 and perplexity is 156.36525112955476
At time: 735.4554264545441 and batch: 350, loss is 5.071527328491211 and perplexity is 159.41762457862188
At time: 736.3085858821869 and batch: 400, loss is 5.046922559738159 and perplexity is 155.5430527313127
At time: 737.1546792984009 and batch: 450, loss is 5.010746068954468 and perplexity is 150.01661714909602
At time: 738.0017819404602 and batch: 500, loss is 5.012802810668945 and perplexity is 150.32548010023015
At time: 738.8486158847809 and batch: 550, loss is 5.028169975280762 and perplexity is 152.6533974314526
At time: 739.6970860958099 and batch: 600, loss is 5.048884048461914 and perplexity is 155.84844809216602
At time: 740.543892621994 and batch: 650, loss is 5.066233940124512 and perplexity is 158.57599468138633
At time: 741.3892660140991 and batch: 700, loss is 5.0834694099426265 and perplexity is 161.33281575974576
At time: 742.2375957965851 and batch: 750, loss is 5.049361190795898 and perplexity is 155.92282772786197
At time: 743.0817437171936 and batch: 800, loss is 5.07128493309021 and perplexity is 159.3789871625407
At time: 743.9251608848572 and batch: 850, loss is 5.088887252807617 and perplexity is 162.20926368921397
At time: 744.8246600627899 and batch: 900, loss is 5.0701833343505855 and perplexity is 159.20351214043663
At time: 745.6701772212982 and batch: 950, loss is 5.02700963973999 and perplexity is 152.47637099388896
At time: 746.5154321193695 and batch: 1000, loss is 5.041005878448487 and perplexity is 154.62547125687155
At time: 747.358921289444 and batch: 1050, loss is 5.020232162475586 and perplexity is 151.44645989797942
At time: 748.2085485458374 and batch: 1100, loss is 4.989443082809448 and perplexity is 146.85461485796458
At time: 749.0555484294891 and batch: 1150, loss is 5.0342363834381105 and perplexity is 153.58226985786848
At time: 749.9029226303101 and batch: 1200, loss is 5.063070592880249 and perplexity is 158.0751563260678
At time: 750.7560479640961 and batch: 1250, loss is 5.080091419219971 and perplexity is 160.78875443924483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078634220318203 and perplexity of 160.5546238716922
Finished 33 epochs...
Completing Train Step...
At time: 753.2181239128113 and batch: 50, loss is 5.062631645202637 and perplexity is 158.0057848296528
At time: 754.0934822559357 and batch: 100, loss is 5.09296441078186 and perplexity is 162.87196653623835
At time: 754.9355201721191 and batch: 150, loss is 5.009845695495605 and perplexity is 149.88160695754394
At time: 755.7764136791229 and batch: 200, loss is 5.041563968658448 and perplexity is 154.71179030325757
At time: 756.6236791610718 and batch: 250, loss is 5.0692695617675785 and perplexity is 159.05810278158484
At time: 757.4664380550385 and batch: 300, loss is 5.050584897994995 and perplexity is 156.11374840633582
At time: 758.3089015483856 and batch: 350, loss is 5.069985408782959 and perplexity is 159.1720048130837
At time: 759.148185968399 and batch: 400, loss is 5.045439701080323 and perplexity is 155.31257529386804
At time: 759.9875628948212 and batch: 450, loss is 5.009209766387939 and perplexity is 149.78632318105042
At time: 760.8299300670624 and batch: 500, loss is 5.01135479927063 and perplexity is 150.10796461204697
At time: 761.6725618839264 and batch: 550, loss is 5.02685772895813 and perplexity is 152.45320994840685
At time: 762.5137960910797 and batch: 600, loss is 5.0474826431274415 and perplexity is 155.63019421243587
At time: 763.3551590442657 and batch: 650, loss is 5.064847707748413 and perplexity is 158.3563237961109
At time: 764.1992318630219 and batch: 700, loss is 5.082330665588379 and perplexity is 161.14920349020355
At time: 765.0411102771759 and batch: 750, loss is 5.04809084892273 and perplexity is 155.72487818923062
At time: 765.8822777271271 and batch: 800, loss is 5.070110397338867 and perplexity is 159.1919007354618
At time: 766.7839105129242 and batch: 850, loss is 5.08773964881897 and perplexity is 162.02321846474794
At time: 767.6290581226349 and batch: 900, loss is 5.068886346817017 and perplexity is 158.99716101623454
At time: 768.4703783988953 and batch: 950, loss is 5.025995283126831 and perplexity is 152.3217839950479
At time: 769.3144934177399 and batch: 1000, loss is 5.039811325073242 and perplexity is 154.44087315637609
At time: 770.1532025337219 and batch: 1050, loss is 5.019262657165528 and perplexity is 151.2997029032569
At time: 771.0012099742889 and batch: 1100, loss is 4.988259372711181 and perplexity is 146.68088441090794
At time: 771.8446364402771 and batch: 1150, loss is 5.033156747817993 and perplexity is 153.4165464452586
At time: 772.6866655349731 and batch: 1200, loss is 5.0619277667999265 and perplexity is 157.8946071025986
At time: 773.5297677516937 and batch: 1250, loss is 5.078995733261109 and perplexity is 160.61267693910054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078529524977189 and perplexity of 160.53781543049274
Finished 34 epochs...
Completing Train Step...
At time: 776.019793510437 and batch: 50, loss is 5.060993671417236 and perplexity is 157.74718734193033
At time: 776.8688642978668 and batch: 100, loss is 5.091239557266236 and perplexity is 162.59127839471552
At time: 777.7120735645294 and batch: 150, loss is 5.0082246208190915 and perplexity is 149.63883450932863
At time: 778.5592896938324 and batch: 200, loss is 5.040182933807373 and perplexity is 154.49827539867454
At time: 779.4020183086395 and batch: 250, loss is 5.067800722122192 and perplexity is 158.82464343344625
At time: 780.2506241798401 and batch: 300, loss is 5.0489706325531 and perplexity is 155.8619426726065
At time: 781.0963215827942 and batch: 350, loss is 5.068377342224121 and perplexity is 158.9162513244691
At time: 781.9414629936218 and batch: 400, loss is 5.043779287338257 and perplexity is 155.0549061374375
At time: 782.7899441719055 and batch: 450, loss is 5.007457618713379 and perplexity is 149.5241052125942
At time: 783.6414563655853 and batch: 500, loss is 5.00975076675415 and perplexity is 149.86737956053332
At time: 784.5027418136597 and batch: 550, loss is 5.025427894592285 and perplexity is 152.23538287506787
At time: 785.345419883728 and batch: 600, loss is 5.046019830703735 and perplexity is 155.4027028599923
At time: 786.1960914134979 and batch: 650, loss is 5.063502569198608 and perplexity is 158.14345580093737
At time: 787.039222240448 and batch: 700, loss is 5.080886173248291 and perplexity is 160.91659274281838
At time: 787.885641336441 and batch: 750, loss is 5.046919288635254 and perplexity is 155.54254393481324
At time: 788.7868602275848 and batch: 800, loss is 5.068574619293213 and perplexity is 158.94760494933544
At time: 789.6328074932098 and batch: 850, loss is 5.08642318725586 and perplexity is 161.81006146259244
At time: 790.4782245159149 and batch: 900, loss is 5.067583913803101 and perplexity is 158.7902126620468
At time: 791.3224363327026 and batch: 950, loss is 5.024779176712036 and perplexity is 152.13665708624467
At time: 792.16832447052 and batch: 1000, loss is 5.038619546890259 and perplexity is 154.25692352853653
At time: 793.0116722583771 and batch: 1050, loss is 5.017994394302368 and perplexity is 151.10793673951412
At time: 793.8573107719421 and batch: 1100, loss is 4.987000970840454 and perplexity is 146.49641700297414
At time: 794.7056767940521 and batch: 1150, loss is 5.032119188308716 and perplexity is 153.25745019879818
At time: 795.5479457378387 and batch: 1200, loss is 5.060926198959351 and perplexity is 157.73654411054233
At time: 796.3936140537262 and batch: 1250, loss is 5.077994651794434 and perplexity is 160.45197101818445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078488537864963 and perplexity of 160.53123558388054
Finished 35 epochs...
Completing Train Step...
At time: 798.8105683326721 and batch: 50, loss is 5.059271221160889 and perplexity is 157.47570952928822
At time: 799.6837508678436 and batch: 100, loss is 5.089753332138062 and perplexity is 162.34981063331819
At time: 800.5276834964752 and batch: 150, loss is 5.006702394485473 and perplexity is 149.4112236165052
At time: 801.3691730499268 and batch: 200, loss is 5.038704338073731 and perplexity is 154.27000371017544
At time: 802.2142214775085 and batch: 250, loss is 5.066322069168091 and perplexity is 158.58997044795865
At time: 803.0555546283722 and batch: 300, loss is 5.047636699676514 and perplexity is 155.65417191000094
At time: 803.8996841907501 and batch: 350, loss is 5.066886882781983 and perplexity is 158.67956952330113
At time: 804.7403054237366 and batch: 400, loss is 5.042364435195923 and perplexity is 154.83568149310884
At time: 805.5822551250458 and batch: 450, loss is 5.0059687042236325 and perplexity is 149.3016422610304
At time: 806.4276158809662 and batch: 500, loss is 5.008254041671753 and perplexity is 149.6432370761945
At time: 807.2695534229279 and batch: 550, loss is 5.024031858444214 and perplexity is 152.02300505560757
At time: 808.1098132133484 and batch: 600, loss is 5.044701013565064 and perplexity is 155.19789019697527
At time: 808.9489958286285 and batch: 650, loss is 5.062205867767334 and perplexity is 157.93852385194447
At time: 809.7950413227081 and batch: 700, loss is 5.079537696838379 and perplexity is 160.69974675225194
At time: 810.6962282657623 and batch: 750, loss is 5.045757722854614 and perplexity is 155.36197592945533
At time: 811.5412318706512 and batch: 800, loss is 5.067186861038208 and perplexity is 158.72717708412486
At time: 812.3819401264191 and batch: 850, loss is 5.085060729980468 and perplexity is 161.58975228212392
At time: 813.2219262123108 and batch: 900, loss is 5.06643012046814 and perplexity is 158.60710722624913
At time: 814.0641770362854 and batch: 950, loss is 5.023513717651367 and perplexity is 151.94425613851203
At time: 814.9040913581848 and batch: 1000, loss is 5.037393350601196 and perplexity is 154.06789018102336
At time: 815.7452001571655 and batch: 1050, loss is 5.016915369033813 and perplexity is 150.94497539300954
At time: 816.5905344486237 and batch: 1100, loss is 4.985829448699951 and perplexity is 146.32489369824276
At time: 817.4366476535797 and batch: 1150, loss is 5.031045303344727 and perplexity is 153.09295766624723
At time: 818.2783668041229 and batch: 1200, loss is 5.059945764541626 and perplexity is 157.58196956124968
At time: 819.1176586151123 and batch: 1250, loss is 5.07684886932373 and perplexity is 160.26823324426343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078432403341697 and perplexity of 160.52222449242075
Finished 36 epochs...
Completing Train Step...
At time: 821.557196855545 and batch: 50, loss is 5.057663316726685 and perplexity is 157.2227070940339
At time: 822.4009728431702 and batch: 100, loss is 5.088216075897217 and perplexity is 162.10042910448848
At time: 823.2531723976135 and batch: 150, loss is 5.0051916980743405 and perplexity is 149.18567902480228
At time: 824.1023969650269 and batch: 200, loss is 5.037495670318603 and perplexity is 154.0836551705291
At time: 824.9482169151306 and batch: 250, loss is 5.0649893188476565 and perplexity is 158.37875039708612
At time: 825.7931008338928 and batch: 300, loss is 5.046088933944702 and perplexity is 155.41344206146746
At time: 826.6366093158722 and batch: 350, loss is 5.065244121551514 and perplexity is 158.4191108726816
At time: 827.4787685871124 and batch: 400, loss is 5.040813512802124 and perplexity is 154.59572948888808
At time: 828.3231012821198 and batch: 450, loss is 5.004474534988403 and perplexity is 149.07872691849047
At time: 829.1646785736084 and batch: 500, loss is 5.006707792282104 and perplexity is 149.4120301100813
At time: 830.0114696025848 and batch: 550, loss is 5.0226350688934325 and perplexity is 151.81080914168498
At time: 830.8538925647736 and batch: 600, loss is 5.043362560272217 and perplexity is 154.99030402298752
At time: 831.7302112579346 and batch: 650, loss is 5.060903310775757 and perplexity is 157.73293384887762
At time: 832.5720987319946 and batch: 700, loss is 5.078473463058471 and perplexity is 160.52881562480846
At time: 833.4161636829376 and batch: 750, loss is 5.044248914718628 and perplexity is 155.12774126816763
At time: 834.2598176002502 and batch: 800, loss is 5.065838460922241 and perplexity is 158.51329357281983
At time: 835.1047968864441 and batch: 850, loss is 5.084125118255615 and perplexity is 161.4386377185748
At time: 835.9508707523346 and batch: 900, loss is 5.065341758728027 and perplexity is 158.43457922250371
At time: 836.7949154376984 and batch: 950, loss is 5.02256332397461 and perplexity is 151.79991787820776
At time: 837.6368036270142 and batch: 1000, loss is 5.036253337860107 and perplexity is 153.89235090074374
At time: 838.4803898334503 and batch: 1050, loss is 5.015806484222412 and perplexity is 150.77768757104312
At time: 839.3254988193512 and batch: 1100, loss is 4.98458456993103 and perplexity is 146.14285027921667
At time: 840.171510219574 and batch: 1150, loss is 5.0299646854400635 and perplexity is 152.92761202893325
At time: 841.0154795646667 and batch: 1200, loss is 5.058901252746582 and perplexity is 157.417459266785
At time: 841.8606028556824 and batch: 1250, loss is 5.075813608169556 and perplexity is 160.1023996235015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078391416229471 and perplexity of 160.515645284823
Finished 37 epochs...
Completing Train Step...
At time: 844.2907028198242 and batch: 50, loss is 5.056038026809692 and perplexity is 156.96738215819843
At time: 845.1670105457306 and batch: 100, loss is 5.086681222915649 and perplexity is 161.85181961587713
At time: 846.0132117271423 and batch: 150, loss is 5.003952465057373 and perplexity is 149.0009177105013
At time: 846.8542182445526 and batch: 200, loss is 5.035987434387207 and perplexity is 153.85143583015713
At time: 847.7006280422211 and batch: 250, loss is 5.063539838790893 and perplexity is 158.14934985289145
At time: 848.5455894470215 and batch: 300, loss is 5.0446981430053714 and perplexity is 155.19744469280678
At time: 849.392783164978 and batch: 350, loss is 5.063721885681153 and perplexity is 158.17814307100707
At time: 850.2379541397095 and batch: 400, loss is 5.039387378692627 and perplexity is 154.37541238409156
At time: 851.0810544490814 and batch: 450, loss is 5.003109760284424 and perplexity is 148.87540681771475
At time: 851.9228706359863 and batch: 500, loss is 5.005322952270507 and perplexity is 149.2052615562989
At time: 852.766473531723 and batch: 550, loss is 5.0213322353363035 and perplexity is 151.61315370922333
At time: 853.6412074565887 and batch: 600, loss is 5.04199649810791 and perplexity is 154.77872218270372
At time: 854.4831788539886 and batch: 650, loss is 5.0596712207794186 and perplexity is 157.5387123527403
At time: 855.3246960639954 and batch: 700, loss is 5.077249298095703 and perplexity is 160.33242210676366
At time: 856.165242433548 and batch: 750, loss is 5.043114824295044 and perplexity is 154.95191210429408
At time: 857.0076239109039 and batch: 800, loss is 5.064453973770141 and perplexity is 158.29398580385632
At time: 857.8455677032471 and batch: 850, loss is 5.082890033721924 and perplexity is 161.23937043528895
At time: 858.6837601661682 and batch: 900, loss is 5.064266099929809 and perplexity is 158.2642492982851
At time: 859.5525505542755 and batch: 950, loss is 5.021424674987793 and perplexity is 151.62716942410742
At time: 860.4050459861755 and batch: 1000, loss is 5.0350358772277835 and perplexity is 153.70510702605725
At time: 861.2491784095764 and batch: 1050, loss is 5.014669008255005 and perplexity is 150.6062790800109
At time: 862.0932581424713 and batch: 1100, loss is 4.983497972488403 and perplexity is 145.98413807559825
At time: 862.936952829361 and batch: 1150, loss is 5.028975915908814 and perplexity is 152.77647659704064
At time: 863.776219367981 and batch: 1200, loss is 5.057927045822144 and perplexity is 157.2641767645004
At time: 864.61993932724 and batch: 1250, loss is 5.074731693267823 and perplexity is 159.92927612088158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078373150233805 and perplexity of 160.51271333351946
Finished 38 epochs...
Completing Train Step...
At time: 867.0783052444458 and batch: 50, loss is 5.054539165496826 and perplexity is 156.73228605394098
At time: 867.9199979305267 and batch: 100, loss is 5.085196409225464 and perplexity is 161.61167814512123
At time: 868.7667183876038 and batch: 150, loss is 5.0027693748474125 and perplexity is 148.8247404208458
At time: 869.604594707489 and batch: 200, loss is 5.034682884216308 and perplexity is 153.650859772463
At time: 870.447342634201 and batch: 250, loss is 5.0622563076019285 and perplexity is 157.9464904458788
At time: 871.2895159721375 and batch: 300, loss is 5.0432876873016355 and perplexity is 154.97869987293797
At time: 872.137529373169 and batch: 350, loss is 5.062305002212525 and perplexity is 157.95418177598773
At time: 872.9775531291962 and batch: 400, loss is 5.037992897033692 and perplexity is 154.16028873078625
At time: 873.8199412822723 and batch: 450, loss is 5.001712493896484 and perplexity is 148.66753347681373
At time: 874.660756111145 and batch: 500, loss is 5.003933877944946 and perplexity is 148.9981482394304
At time: 875.5628283023834 and batch: 550, loss is 5.019870290756225 and perplexity is 151.39166562199247
At time: 876.40314412117 and batch: 600, loss is 5.0407784557342525 and perplexity is 154.59030991090452
At time: 877.2463707923889 and batch: 650, loss is 5.058528747558594 and perplexity is 157.35883136680448
At time: 878.0897727012634 and batch: 700, loss is 5.076202516555786 and perplexity is 160.164676898663
At time: 878.9306497573853 and batch: 750, loss is 5.042094011306762 and perplexity is 154.7938158869238
At time: 879.769368648529 and batch: 800, loss is 5.063176212310791 and perplexity is 158.0918530157937
At time: 880.6084883213043 and batch: 850, loss is 5.081807126998902 and perplexity is 161.06485774456578
At time: 881.4519610404968 and batch: 900, loss is 5.063157835006714 and perplexity is 158.08894774043443
At time: 882.2943286895752 and batch: 950, loss is 5.020483379364014 and perplexity is 151.48451058568597
At time: 883.1349699497223 and batch: 1000, loss is 5.033978652954102 and perplexity is 153.54269212553598
At time: 883.979843378067 and batch: 1050, loss is 5.013725023269654 and perplexity is 150.4641760959538
At time: 884.8264715671539 and batch: 1100, loss is 4.982417364120483 and perplexity is 145.82647159761027
At time: 885.6638669967651 and batch: 1150, loss is 5.02797308921814 and perplexity is 152.62334506362967
At time: 886.503591299057 and batch: 1200, loss is 5.056875991821289 and perplexity is 157.0989704579045
At time: 887.3442993164062 and batch: 1250, loss is 5.073761367797852 and perplexity is 159.77416793574878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078301868299498 and perplexity of 160.50127208461416
Finished 39 epochs...
Completing Train Step...
At time: 889.8492097854614 and batch: 50, loss is 5.053021764755249 and perplexity is 156.4946407140594
At time: 890.6947424411774 and batch: 100, loss is 5.0838390064239505 and perplexity is 161.39245482128737
At time: 891.540524482727 and batch: 150, loss is 5.001353597640991 and perplexity is 148.61418682926413
At time: 892.3853938579559 and batch: 200, loss is 5.033307056427002 and perplexity is 153.4396080060859
At time: 893.2312426567078 and batch: 250, loss is 5.060782508850098 and perplexity is 157.7138805575891
At time: 894.0759515762329 and batch: 300, loss is 5.041939325332642 and perplexity is 154.76987330656394
At time: 894.9234511852264 and batch: 350, loss is 5.060835809707641 and perplexity is 157.72228706670444
At time: 895.7775237560272 and batch: 400, loss is 5.036437025070191 and perplexity is 153.92062155373307
At time: 896.6250159740448 and batch: 450, loss is 5.000285243988037 and perplexity is 148.45549910228425
At time: 897.500726222992 and batch: 500, loss is 5.002395696640015 and perplexity is 148.76913824793613
At time: 898.3449051380157 and batch: 550, loss is 5.018604726791382 and perplexity is 151.20019097266808
At time: 899.1905426979065 and batch: 600, loss is 5.0394762134552 and perplexity is 154.38912689635146
At time: 900.0351915359497 and batch: 650, loss is 5.057280235290527 and perplexity is 157.1624895284559
At time: 900.8846991062164 and batch: 700, loss is 5.0749938106536865 and perplexity is 159.97120185915392
At time: 901.7290258407593 and batch: 750, loss is 5.040716009140015 and perplexity is 154.58065657396057
At time: 902.5761795043945 and batch: 800, loss is 5.061798982620239 and perplexity is 157.87427408445944
At time: 903.4206891059875 and batch: 850, loss is 5.080562601089477 and perplexity is 160.86453303650566
At time: 904.2663390636444 and batch: 900, loss is 5.061894216537476 and perplexity is 157.8893097859544
At time: 905.1094119548798 and batch: 950, loss is 5.019220199584961 and perplexity is 151.29327922029913
At time: 905.9526278972626 and batch: 1000, loss is 5.032590055465699 and perplexity is 153.32963109113186
At time: 906.8006303310394 and batch: 1050, loss is 5.012626256942749 and perplexity is 150.2989419193525
At time: 907.6515452861786 and batch: 1100, loss is 4.981203603744507 and perplexity is 145.64958057798842
At time: 908.4975500106812 and batch: 1150, loss is 5.027057867050171 and perplexity is 152.4837246964514
At time: 909.343540430069 and batch: 1200, loss is 5.055866689682007 and perplexity is 156.94049012166704
At time: 910.188773393631 and batch: 1250, loss is 5.072740850448608 and perplexity is 159.61119879597038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078268009380703 and perplexity of 160.4958377770766
Finished 40 epochs...
Completing Train Step...
At time: 912.6244359016418 and batch: 50, loss is 5.05136134147644 and perplexity is 156.23500897854825
At time: 913.5214014053345 and batch: 100, loss is 5.0823545742034915 and perplexity is 161.15305639054407
At time: 914.3637628555298 and batch: 150, loss is 4.999979887008667 and perplexity is 148.41017410001265
At time: 915.2052237987518 and batch: 200, loss is 5.032063703536988 and perplexity is 153.24894698006
At time: 916.0488610267639 and batch: 250, loss is 5.0594760704040525 and perplexity is 157.50797161352094
At time: 916.8906042575836 and batch: 300, loss is 5.040461750030517 and perplexity is 154.54135803009265
At time: 917.7321064472198 and batch: 350, loss is 5.059476175308228 and perplexity is 157.50798813676565
At time: 918.5762577056885 and batch: 400, loss is 5.034971714019775 and perplexity is 153.69524512969207
At time: 919.4545335769653 and batch: 450, loss is 4.9988336181640625 and perplexity is 148.24015360443425
At time: 920.2989552021027 and batch: 500, loss is 5.0010731983184815 and perplexity is 148.57252135372048
At time: 921.1490459442139 and batch: 550, loss is 5.017309494018555 and perplexity is 151.0044783041549
At time: 921.9954810142517 and batch: 600, loss is 5.0382154846191405 and perplexity is 154.194606716464
At time: 922.8395276069641 and batch: 650, loss is 5.0560922527313235 and perplexity is 156.97589408994358
At time: 923.6805875301361 and batch: 700, loss is 5.07382984161377 and perplexity is 159.78510865728464
At time: 924.5224459171295 and batch: 750, loss is 5.039596252441406 and perplexity is 154.40766072299397
At time: 925.3660342693329 and batch: 800, loss is 5.060582304000855 and perplexity is 157.6823086344401
At time: 926.2117545604706 and batch: 850, loss is 5.079407835006714 and perplexity is 160.6788793437599
At time: 927.0586886405945 and batch: 900, loss is 5.0606880950927735 and perplexity is 157.6989909004488
At time: 927.9101777076721 and batch: 950, loss is 5.018204336166382 and perplexity is 151.13966395173892
At time: 928.7601799964905 and batch: 1000, loss is 5.031433181762695 and perplexity is 153.1523506383503
At time: 929.6063148975372 and batch: 1050, loss is 5.011518182754517 and perplexity is 150.13249177787682
At time: 930.4506368637085 and batch: 1100, loss is 4.980190753936768 and perplexity is 145.50213411147038
At time: 931.2949249744415 and batch: 1150, loss is 5.025947504043579 and perplexity is 152.31450637370966
At time: 932.1400120258331 and batch: 1200, loss is 5.0547332763671875 and perplexity is 156.76271244735196
At time: 932.9822256565094 and batch: 1250, loss is 5.071648817062378 and perplexity is 159.43699317455858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078319243270985 and perplexity of 160.50406081386728
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 935.4625804424286 and batch: 50, loss is 5.051906614303589 and perplexity is 156.32022291390402
At time: 936.3101892471313 and batch: 100, loss is 5.084218921661377 and perplexity is 161.45378192289226
At time: 937.1577703952789 and batch: 150, loss is 5.0015749263763425 and perplexity is 148.6470830595966
At time: 938.0052886009216 and batch: 200, loss is 5.033713636398315 and perplexity is 153.5020061615678
At time: 938.851035118103 and batch: 250, loss is 5.060771265029907 and perplexity is 157.7121072610439
At time: 939.7005889415741 and batch: 300, loss is 5.040992755889892 and perplexity is 154.6234421883781
At time: 940.5999135971069 and batch: 350, loss is 5.058061504364014 and perplexity is 157.28532369805765
At time: 941.4449014663696 and batch: 400, loss is 5.0319078540802 and perplexity is 153.22506507595907
At time: 942.2934885025024 and batch: 450, loss is 4.994933004379273 and perplexity is 147.66305227423334
At time: 943.1426160335541 and batch: 500, loss is 4.996201725006103 and perplexity is 147.85051432781577
At time: 943.9903807640076 and batch: 550, loss is 5.011639928817749 and perplexity is 150.1507709303988
At time: 944.8422267436981 and batch: 600, loss is 5.031479692459106 and perplexity is 153.159474026491
At time: 945.689493894577 and batch: 650, loss is 5.049061212539673 and perplexity is 155.87606128470316
At time: 946.5353319644928 and batch: 700, loss is 5.067520217895508 and perplexity is 158.78009869744693
At time: 947.3830142021179 and batch: 750, loss is 5.030469207763672 and perplexity is 153.00478688968101
At time: 948.2301330566406 and batch: 800, loss is 5.04941987991333 and perplexity is 155.9319789695453
At time: 949.0723929405212 and batch: 850, loss is 5.0647689056396485 and perplexity is 158.343845475525
At time: 949.9177317619324 and batch: 900, loss is 5.045775699615478 and perplexity is 155.3647688596478
At time: 950.7642960548401 and batch: 950, loss is 5.003961925506592 and perplexity is 149.00232733278466
At time: 951.6090304851532 and batch: 1000, loss is 5.0165316772460935 and perplexity is 150.8870701551462
At time: 952.4527153968811 and batch: 1050, loss is 4.994466972351074 and perplexity is 147.59425259516422
At time: 953.3007082939148 and batch: 1100, loss is 4.960633926391601 and perplexity is 142.68421852510386
At time: 954.1470685005188 and batch: 1150, loss is 5.004865493774414 and perplexity is 149.13702195132169
At time: 954.9955995082855 and batch: 1200, loss is 5.038107442855835 and perplexity is 154.17794815918828
At time: 955.8388803005219 and batch: 1250, loss is 5.060173072814941 and perplexity is 157.61779331802015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.074904838617701 and perplexity of 159.9569695287745
Finished 42 epochs...
Completing Train Step...
At time: 958.2756633758545 and batch: 50, loss is 5.048944540023804 and perplexity is 155.85787589335763
At time: 959.146956205368 and batch: 100, loss is 5.079861783981324 and perplexity is 160.75183591430672
At time: 959.9861850738525 and batch: 150, loss is 4.997616739273071 and perplexity is 148.05987300279153
At time: 960.8269507884979 and batch: 200, loss is 5.029590892791748 and perplexity is 152.8704594940965
At time: 961.6671950817108 and batch: 250, loss is 5.057171049118042 and perplexity is 157.14533049454778
At time: 962.5656776428223 and batch: 300, loss is 5.037663707733154 and perplexity is 154.10954916508754
At time: 963.4037535190582 and batch: 350, loss is 5.054847393035889 and perplexity is 156.78060270664201
At time: 964.2456066608429 and batch: 400, loss is 5.029034595489502 and perplexity is 152.7854417196587
At time: 965.0909268856049 and batch: 450, loss is 4.992335071563721 and perplexity is 147.2799314616329
At time: 965.9344782829285 and batch: 500, loss is 4.994322919845581 and perplexity is 147.57299280458133
At time: 966.7754008769989 and batch: 550, loss is 5.009510259628296 and perplexity is 149.83133972190868
At time: 967.6173832416534 and batch: 600, loss is 5.029589691162109 and perplexity is 152.87027580053189
At time: 968.4596831798553 and batch: 650, loss is 5.047540836334228 and perplexity is 155.6392510960319
At time: 969.3068811893463 and batch: 700, loss is 5.06600323677063 and perplexity is 158.539414887271
At time: 970.1478126049042 and batch: 750, loss is 5.0293758106231685 and perplexity is 152.83758331982295
At time: 970.9876034259796 and batch: 800, loss is 5.048660583496094 and perplexity is 155.81362531502032
At time: 971.8306863307953 and batch: 850, loss is 5.064773731231689 and perplexity is 158.34460958016902
At time: 972.6781039237976 and batch: 900, loss is 5.045774631500244 and perplexity is 155.36460291225993
At time: 973.5184755325317 and batch: 950, loss is 5.004204988479614 and perplexity is 149.0385486833097
At time: 974.3596432209015 and batch: 1000, loss is 5.0174088954925535 and perplexity is 151.01948911791484
At time: 975.2007455825806 and batch: 1050, loss is 4.995584793090821 and perplexity is 147.75932875737243
At time: 976.0411548614502 and batch: 1100, loss is 4.961785202026367 and perplexity is 142.8485819850263
At time: 976.8802306652069 and batch: 1150, loss is 5.006224508285523 and perplexity is 149.33983911281425
At time: 977.7227623462677 and batch: 1200, loss is 5.039876728057862 and perplexity is 154.45097438074922
At time: 978.5646729469299 and batch: 1250, loss is 5.060742740631103 and perplexity is 157.70760868216013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.074772521527144 and perplexity of 159.93580588813882
Finished 43 epochs...
Completing Train Step...
At time: 981.0123181343079 and batch: 50, loss is 5.047386741638183 and perplexity is 155.61526976068734
At time: 981.8561108112335 and batch: 100, loss is 5.077944164276123 and perplexity is 160.44387040085138
At time: 982.7009475231171 and batch: 150, loss is 4.995715932846069 and perplexity is 147.77870715019247
At time: 983.5461001396179 and batch: 200, loss is 5.027554445266723 and perplexity is 152.55946359610184
At time: 984.4461314678192 and batch: 250, loss is 5.055459041595459 and perplexity is 156.87652666934227
At time: 985.2912123203278 and batch: 300, loss is 5.036054534912109 and perplexity is 153.86175968862267
At time: 986.1391656398773 and batch: 350, loss is 5.053259735107422 and perplexity is 156.53188623031184
At time: 986.9804141521454 and batch: 400, loss is 5.0274820804595945 and perplexity is 152.5484240593848
At time: 987.8242449760437 and batch: 450, loss is 4.990900573730468 and perplexity is 147.06881018178237
At time: 988.6667678356171 and batch: 500, loss is 4.993294267654419 and perplexity is 147.42126957077352
At time: 989.5083148479462 and batch: 550, loss is 5.008447742462158 and perplexity is 149.672225896983
At time: 990.3490400314331 and batch: 600, loss is 5.02862135887146 and perplexity is 152.7223182238046
At time: 991.1924614906311 and batch: 650, loss is 5.046747732162475 and perplexity is 155.51586189341262
At time: 992.0358591079712 and batch: 700, loss is 5.065255279541016 and perplexity is 158.42087852131934
At time: 992.8824238777161 and batch: 750, loss is 5.028962087631226 and perplexity is 152.77436397612036
At time: 993.7279858589172 and batch: 800, loss is 5.048308868408203 and perplexity is 155.75883294831385
At time: 994.5715389251709 and batch: 850, loss is 5.064808034896851 and perplexity is 158.35004147380263
At time: 995.4137246608734 and batch: 900, loss is 5.045904121398926 and perplexity is 155.3847223615544
At time: 996.2608075141907 and batch: 950, loss is 5.004578971862793 and perplexity is 149.0942970478016
At time: 997.1075448989868 and batch: 1000, loss is 5.018131055831909 and perplexity is 151.12858879241313
At time: 997.949898481369 and batch: 1050, loss is 4.996384649276734 and perplexity is 147.87756224909674
At time: 998.7921035289764 and batch: 1100, loss is 4.962529315948486 and perplexity is 142.95491716147237
At time: 999.6359951496124 and batch: 1150, loss is 5.00704779624939 and perplexity is 149.46283943027467
At time: 1000.4820272922516 and batch: 1200, loss is 5.0409008979797365 and perplexity is 154.60923945444432
At time: 1001.3249249458313 and batch: 1250, loss is 5.0610338306427005 and perplexity is 157.75352247399928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.074757819628194 and perplexity of 159.9334545453667
Finished 44 epochs...
Completing Train Step...
At time: 1003.7306439876556 and batch: 50, loss is 5.046216173171997 and perplexity is 155.43321800585787
At time: 1004.6046752929688 and batch: 100, loss is 5.076603899002075 and perplexity is 160.22897709210687
At time: 1005.4435343742371 and batch: 150, loss is 4.994358758926392 and perplexity is 147.57828177977137
At time: 1006.329756975174 and batch: 200, loss is 5.02613471031189 and perplexity is 152.34302327324545
At time: 1007.1800110340118 and batch: 250, loss is 5.054307022094727 and perplexity is 156.69590591070607
At time: 1008.023250579834 and batch: 300, loss is 5.034928884506225 and perplexity is 153.68866257807298
At time: 1008.8687562942505 and batch: 350, loss is 5.052086334228516 and perplexity is 156.348319297295
At time: 1009.712156534195 and batch: 400, loss is 5.026385326385498 and perplexity is 152.3812076681914
At time: 1010.5554847717285 and batch: 450, loss is 4.989902019500732 and perplexity is 146.922027296845
At time: 1011.4024744033813 and batch: 500, loss is 4.992544994354248 and perplexity is 147.31085212119606
At time: 1012.2442016601562 and batch: 550, loss is 5.007677154541016 and perplexity is 149.5569347142778
At time: 1013.0856931209564 and batch: 600, loss is 5.027903842926025 and perplexity is 152.61277682880365
At time: 1013.9265050888062 and batch: 650, loss is 5.046148872375488 and perplexity is 155.42275757848358
At time: 1014.7682459354401 and batch: 700, loss is 5.06473741531372 and perplexity is 158.33885925473152
At time: 1015.6101982593536 and batch: 750, loss is 5.0287179756164555 and perplexity is 152.73707446991833
At time: 1016.4544305801392 and batch: 800, loss is 5.048040113449097 and perplexity is 155.71697761420066
At time: 1017.2997827529907 and batch: 850, loss is 5.0648055934906 and perplexity is 158.34965487749355
At time: 1018.1494972705841 and batch: 900, loss is 5.046000051498413 and perplexity is 155.39962914842266
At time: 1018.9977424144745 and batch: 950, loss is 5.0049052429199214 and perplexity is 149.1429501383272
At time: 1019.8387038707733 and batch: 1000, loss is 5.01863278388977 and perplexity is 151.2044332708156
At time: 1020.6804904937744 and batch: 1050, loss is 4.9969704055786135 and perplexity is 147.964207837213
At time: 1021.521767616272 and batch: 1100, loss is 4.9629975700378415 and perplexity is 143.021872060756
At time: 1022.3626618385315 and batch: 1150, loss is 5.007586050033569 and perplexity is 149.5433100240502
At time: 1023.2062740325928 and batch: 1200, loss is 5.041584663391113 and perplexity is 154.71499205552763
At time: 1024.0479898452759 and batch: 1250, loss is 5.061170167922974 and perplexity is 157.7750316264233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0747515824589415 and perplexity of 159.93245701645247
Finished 45 epochs...
Completing Train Step...
At time: 1026.514291524887 and batch: 50, loss is 5.045261220932007 and perplexity is 155.28485755600084
At time: 1027.355167388916 and batch: 100, loss is 5.075524101257324 and perplexity is 160.05605558092964
At time: 1028.2253835201263 and batch: 150, loss is 4.9932629680633545 and perplexity is 147.41665541753264
At time: 1029.0683035850525 and batch: 200, loss is 5.025005674362182 and perplexity is 152.17111958426594
At time: 1029.90988945961 and batch: 250, loss is 5.053392400741577 and perplexity is 156.55265400981904
At time: 1030.7504880428314 and batch: 300, loss is 5.03402156829834 and perplexity is 153.5492816044177
At time: 1031.592235326767 and batch: 350, loss is 5.051132421493531 and perplexity is 156.1992477563484
At time: 1032.4312798976898 and batch: 400, loss is 5.025480527877807 and perplexity is 152.24339573431098
At time: 1033.2731790542603 and batch: 450, loss is 4.98910228729248 and perplexity is 146.80457599057016
At time: 1034.1111333370209 and batch: 500, loss is 4.9919578552246096 and perplexity is 147.22438554213522
At time: 1034.957087278366 and batch: 550, loss is 5.0071041297912595 and perplexity is 149.47125943855988
At time: 1035.7985560894012 and batch: 600, loss is 5.027337255477906 and perplexity is 152.52633283638284
At time: 1036.6426949501038 and batch: 650, loss is 5.045669021606446 and perplexity is 155.3481957394415
At time: 1037.4843664169312 and batch: 700, loss is 5.0643402862548825 and perplexity is 158.27599077685423
At time: 1038.322881937027 and batch: 750, loss is 5.028494119644165 and perplexity is 152.70288719026416
At time: 1039.163592338562 and batch: 800, loss is 5.047787618637085 and perplexity is 155.6776648485545
At time: 1040.0026488304138 and batch: 850, loss is 5.064719533920288 and perplexity is 158.33602796060734
At time: 1040.8448526859283 and batch: 900, loss is 5.045982675552368 and perplexity is 155.3969289563105
At time: 1041.6868057250977 and batch: 950, loss is 5.00510087966919 and perplexity is 149.17213083458373
At time: 1042.5301840305328 and batch: 1000, loss is 5.01896113395691 and perplexity is 151.25408940848376
At time: 1043.3668808937073 and batch: 1050, loss is 4.997323369979858 and perplexity is 148.0164431532991
At time: 1044.2073605060577 and batch: 1100, loss is 4.963259191513061 and perplexity is 143.0592945489631
At time: 1045.0483090877533 and batch: 1150, loss is 5.007892932891846 and perplexity is 149.58920934496365
At time: 1045.8857595920563 and batch: 1200, loss is 5.041983385086059 and perplexity is 154.77669257924487
At time: 1046.726054430008 and batch: 1250, loss is 5.061208753585816 and perplexity is 157.78111959805193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.074751136946852 and perplexity of 159.93238576462525
Finished 46 epochs...
Completing Train Step...
At time: 1049.1424474716187 and batch: 50, loss is 5.044421243667602 and perplexity is 155.15447657234446
At time: 1049.9876832962036 and batch: 100, loss is 5.074571418762207 and perplexity is 159.9036455892331
At time: 1050.8368272781372 and batch: 150, loss is 4.992282199859619 and perplexity is 147.2721447265273
At time: 1051.6850209236145 and batch: 200, loss is 5.02409236907959 and perplexity is 152.0322043425597
At time: 1052.5304760932922 and batch: 250, loss is 5.0525792121887205 and perplexity is 156.42539893185779
At time: 1053.3743813037872 and batch: 300, loss is 5.033256978988647 and perplexity is 153.43192433596562
At time: 1054.221516609192 and batch: 350, loss is 5.050299577713012 and perplexity is 156.06921234144252
At time: 1055.0704929828644 and batch: 400, loss is 5.024707155227661 and perplexity is 152.12570037293767
At time: 1055.915763616562 and batch: 450, loss is 4.988368673324585 and perplexity is 146.69691759775125
At time: 1056.7601234912872 and batch: 500, loss is 4.99143762588501 and perplexity is 147.1478150161157
At time: 1057.6053378582 and batch: 550, loss is 5.0066091823577885 and perplexity is 149.3972973275115
At time: 1058.4509308338165 and batch: 600, loss is 5.026805486679077 and perplexity is 152.4452456533085
At time: 1059.2998950481415 and batch: 650, loss is 5.045195798873902 and perplexity is 155.27469883333285
At time: 1060.1464562416077 and batch: 700, loss is 5.063965253829956 and perplexity is 158.2166432775394
At time: 1060.9919092655182 and batch: 750, loss is 5.028261194229126 and perplexity is 152.66732294895633
At time: 1061.8368113040924 and batch: 800, loss is 5.047463912963867 and perplexity is 155.62727926073993
At time: 1062.681750535965 and batch: 850, loss is 5.0646178245544435 and perplexity is 158.31992452256227
At time: 1063.5268907546997 and batch: 900, loss is 5.045948123931884 and perplexity is 155.3915598333533
At time: 1064.3735511302948 and batch: 950, loss is 5.0052557468414305 and perplexity is 149.1952344896159
At time: 1065.2219128608704 and batch: 1000, loss is 5.019204206466675 and perplexity is 151.2908595883375
At time: 1066.0712184906006 and batch: 1050, loss is 4.997550983428955 and perplexity is 148.0501375209484
At time: 1066.9171586036682 and batch: 1100, loss is 4.963418092727661 and perplexity is 143.08202865081705
At time: 1067.7646877765656 and batch: 1150, loss is 5.00814208984375 and perplexity is 149.62648517997567
At time: 1068.610939502716 and batch: 1200, loss is 5.042272577285766 and perplexity is 154.82145926421134
At time: 1069.456648349762 and batch: 1250, loss is 5.061182622909546 and perplexity is 157.77699672456117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.074744454265511 and perplexity of 159.93131699102628
Finished 47 epochs...
Completing Train Step...
At time: 1071.8838694095612 and batch: 50, loss is 5.043688955307007 and perplexity is 155.04090034540624
At time: 1072.759403705597 and batch: 100, loss is 5.0737756156921385 and perplexity is 159.77644439742062
At time: 1073.602150440216 and batch: 150, loss is 4.991585454940796 and perplexity is 147.16956934659194
At time: 1074.4437091350555 and batch: 200, loss is 5.023243827819824 and perplexity is 151.90325346217048
At time: 1075.2844593524933 and batch: 250, loss is 5.051920413970947 and perplexity is 156.32238009586575
At time: 1076.1262454986572 and batch: 300, loss is 5.032625732421875 and perplexity is 153.33510152324436
At time: 1076.9686875343323 and batch: 350, loss is 5.049566793441772 and perplexity is 155.95488916964064
At time: 1077.8096809387207 and batch: 400, loss is 5.024030714035034 and perplexity is 152.02283107918458
At time: 1078.6509838104248 and batch: 450, loss is 4.987713232040405 and perplexity is 146.60079788555066
At time: 1079.492190361023 and batch: 500, loss is 4.990991935729981 and perplexity is 147.08224729615796
At time: 1080.3338491916656 and batch: 550, loss is 5.006141242980957 and perplexity is 149.32740480330716
At time: 1081.174418926239 and batch: 600, loss is 5.026359453201294 and perplexity is 152.37726513213937
At time: 1082.0164377689362 and batch: 650, loss is 5.044828281402588 and perplexity is 155.2176431537804
At time: 1082.8567559719086 and batch: 700, loss is 5.063623580932617 and perplexity is 158.16259417270652
At time: 1083.6949608325958 and batch: 750, loss is 5.028067407608032 and perplexity is 152.63774093068284
At time: 1084.5539162158966 and batch: 800, loss is 5.047199773788452 and perplexity is 155.586177428065
At time: 1085.4022569656372 and batch: 850, loss is 5.06447322845459 and perplexity is 158.29703373394648
At time: 1086.25021982193 and batch: 900, loss is 5.045896892547607 and perplexity is 155.38359911255918
At time: 1087.0907895565033 and batch: 950, loss is 5.005358514785766 and perplexity is 149.2105677650402
At time: 1087.9299094676971 and batch: 1000, loss is 5.019397382736206 and perplexity is 151.32008821525454
At time: 1088.7716584205627 and batch: 1050, loss is 4.997724981307983 and perplexity is 148.07590017212524
At time: 1089.6131331920624 and batch: 1100, loss is 4.963492555618286 and perplexity is 143.09268334895177
At time: 1090.4562830924988 and batch: 1150, loss is 5.008286476135254 and perplexity is 149.64809075301838
At time: 1091.297639131546 and batch: 1200, loss is 5.042485456466675 and perplexity is 154.85442103795984
At time: 1092.1666581630707 and batch: 1250, loss is 5.061116533279419 and perplexity is 157.76656964576983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.074748463874315 and perplexity of 159.93195825432855
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1094.6034622192383 and batch: 50, loss is 5.043809595108033 and perplexity is 155.05960557704972
At time: 1095.4463200569153 and batch: 100, loss is 5.0741816425323485 and perplexity is 159.8413310942581
At time: 1096.291484117508 and batch: 150, loss is 4.991937093734741 and perplexity is 147.22132897627594
At time: 1097.13449883461 and batch: 200, loss is 5.023357019424439 and perplexity is 151.9204486083308
At time: 1097.9772400856018 and batch: 250, loss is 5.052132272720337 and perplexity is 156.3555018682593
At time: 1098.819195985794 and batch: 300, loss is 5.032930603027344 and perplexity is 153.38185601515846
At time: 1099.663100719452 and batch: 350, loss is 5.048811626434326 and perplexity is 155.83716164025765
At time: 1100.5055527687073 and batch: 400, loss is 5.023198003768921 and perplexity is 151.8962927992356
At time: 1101.3534920215607 and batch: 450, loss is 4.986447343826294 and perplexity is 146.4153350756872
At time: 1102.1974303722382 and batch: 500, loss is 4.989769191741943 and perplexity is 146.90251326927333
At time: 1103.0417244434357 and batch: 550, loss is 5.00418176651001 and perplexity is 149.03508775484727
At time: 1103.8845031261444 and batch: 600, loss is 5.023981189727783 and perplexity is 152.01530244021583
At time: 1104.7253329753876 and batch: 650, loss is 5.042852058410644 and perplexity is 154.91120137699994
At time: 1105.5681166648865 and batch: 700, loss is 5.061762161254883 and perplexity is 157.86846104515575
At time: 1106.4095907211304 and batch: 750, loss is 5.025634708404541 and perplexity is 152.26687051088732
At time: 1107.2504856586456 and batch: 800, loss is 5.044764556884766 and perplexity is 155.20775229946116
At time: 1108.0937247276306 and batch: 850, loss is 5.0606325340271 and perplexity is 157.6902292198651
At time: 1108.942393064499 and batch: 900, loss is 5.041702499389649 and perplexity is 154.7332241252809
At time: 1109.7846224308014 and batch: 950, loss is 5.001669702529907 and perplexity is 148.6611719260013
At time: 1110.6277318000793 and batch: 1000, loss is 5.015135536193847 and perplexity is 150.6765575091175
At time: 1111.4706609249115 and batch: 1050, loss is 4.993402013778686 and perplexity is 147.43715449695856
At time: 1112.312040090561 and batch: 1100, loss is 4.958836574554443 and perplexity is 142.42799511359087
At time: 1113.1580247879028 and batch: 1150, loss is 5.002718849182129 and perplexity is 148.8172211417853
At time: 1114.0344676971436 and batch: 1200, loss is 5.037849807739258 and perplexity is 154.13823162194777
At time: 1114.8809010982513 and batch: 1250, loss is 5.0586723041534425 and perplexity is 157.3814228863469
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.075690276431342 and perplexity of 160.0826551338498
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1117.2800753116608 and batch: 50, loss is 5.043812303543091 and perplexity is 155.06002554649035
At time: 1118.1573190689087 and batch: 100, loss is 5.073625144958496 and perplexity is 159.7524045273071
At time: 1119.0079789161682 and batch: 150, loss is 4.991450471878052 and perplexity is 147.1497052880647
At time: 1119.846407175064 and batch: 200, loss is 5.023023900985717 and perplexity is 151.86984953389077
At time: 1120.6873972415924 and batch: 250, loss is 5.051558628082275 and perplexity is 156.26583509387208
At time: 1121.527530670166 and batch: 300, loss is 5.032329721450806 and perplexity is 153.28971936808279
At time: 1122.3711841106415 and batch: 350, loss is 5.048172578811646 and perplexity is 155.73760608634424
At time: 1123.2096536159515 and batch: 400, loss is 5.022552547454834 and perplexity is 151.79828201220525
At time: 1124.0470740795135 and batch: 450, loss is 4.985993900299072 and perplexity is 146.3489590397404
At time: 1124.886958360672 and batch: 500, loss is 4.989478006362915 and perplexity is 146.85974363251543
At time: 1125.7281122207642 and batch: 550, loss is 5.003508024215698 and perplexity is 148.93471033094525
At time: 1126.575891494751 and batch: 600, loss is 5.02321307182312 and perplexity is 151.89858159805195
At time: 1127.4173498153687 and batch: 650, loss is 5.042562847137451 and perplexity is 154.86640578922456
At time: 1128.2555196285248 and batch: 700, loss is 5.06159140586853 and perplexity is 157.8415064564831
At time: 1129.0973978042603 and batch: 750, loss is 5.025249662399292 and perplexity is 152.20825204679227
At time: 1129.9359774589539 and batch: 800, loss is 5.044220170974731 and perplexity is 155.1232823801843
At time: 1130.7749242782593 and batch: 850, loss is 5.05968505859375 and perplexity is 157.54089235927515
At time: 1131.6138906478882 and batch: 900, loss is 5.040744876861572 and perplexity is 154.58511902972282
At time: 1132.4557754993439 and batch: 950, loss is 5.000760498046875 and perplexity is 148.5260699490019
At time: 1133.2938296794891 and batch: 1000, loss is 5.014099445343017 and perplexity is 150.5205237529796
At time: 1134.1367564201355 and batch: 1050, loss is 4.992381811141968 and perplexity is 147.28681542438918
At time: 1134.9781701564789 and batch: 1100, loss is 4.957912187576294 and perplexity is 142.29639736257852
At time: 1135.874305486679 and batch: 1150, loss is 5.001648721694946 and perplexity is 148.65805292320778
At time: 1136.7156138420105 and batch: 1200, loss is 5.036725664138794 and perplexity is 153.96505547095097
At time: 1137.5596814155579 and batch: 1250, loss is 5.057838888168335 and perplexity is 157.25031333474132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.075281741845346 and perplexity of 160.01726918972892
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe70d2b860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'num_layers': 1, 'anneal': 4.0019315899158165, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.42331717900717813, 'tune_wordvecs': True, 'lr': 0.04237964674543293, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5434823036193848 and batch: 50, loss is 9.89222583770752 and perplexity is 19776.0292180748
At time: 2.3850622177124023 and batch: 100, loss is 9.80373249053955 and perplexity is 18101.181485029247
At time: 3.227292776107788 and batch: 150, loss is 9.702378025054932 and perplexity is 16356.45705142324
At time: 4.075036525726318 and batch: 200, loss is 9.597579555511475 and perplexity is 14729.087446655618
At time: 4.921149015426636 and batch: 250, loss is 9.50199161529541 and perplexity is 13386.360779445504
At time: 5.77067494392395 and batch: 300, loss is 9.371652755737305 and perplexity is 11750.519786435343
At time: 6.618095636367798 and batch: 350, loss is 9.2318829536438 and perplexity is 10217.762983497683
At time: 7.462921619415283 and batch: 400, loss is 9.056020183563232 and perplexity is 8569.975763829227
At time: 8.307003021240234 and batch: 450, loss is 8.853438568115234 and perplexity is 6998.412160659847
At time: 9.14736533164978 and batch: 500, loss is 8.651184883117676 and perplexity is 5716.916600175954
At time: 9.988518238067627 and batch: 550, loss is 8.422596225738525 and perplexity is 4548.697584201292
At time: 10.835344314575195 and batch: 600, loss is 8.24606294631958 and perplexity is 3812.5858790696425
At time: 11.677613735198975 and batch: 650, loss is 8.022574501037598 and perplexity is 3049.0169343434736
At time: 12.520944356918335 and batch: 700, loss is 7.841042242050171 and perplexity is 2542.8537222723685
At time: 13.363398790359497 and batch: 750, loss is 7.586607446670532 and perplexity is 1971.6133514035898
At time: 14.204188585281372 and batch: 800, loss is 7.45403133392334 and perplexity is 1726.8104821257307
At time: 15.047415733337402 and batch: 850, loss is 7.3955620002746585 and perplexity is 1628.7400187828227
At time: 15.889991760253906 and batch: 900, loss is 7.293129138946533 and perplexity is 1470.1638541167665
At time: 16.735458374023438 and batch: 950, loss is 7.200436925888061 and perplexity is 1340.0161242407703
At time: 17.58265519142151 and batch: 1000, loss is 7.158283491134643 and perplexity is 1284.7038336752375
At time: 18.42579174041748 and batch: 1050, loss is 7.083124704360962 and perplexity is 1191.6863744949446
At time: 19.267670392990112 and batch: 1100, loss is 7.032057619094848 and perplexity is 1132.3581765489457
At time: 20.111844778060913 and batch: 1150, loss is 7.044487495422363 and perplexity is 1146.5210879110646
At time: 20.956572771072388 and batch: 1200, loss is 7.010499753952026 and perplexity is 1108.208197960631
At time: 21.799715280532837 and batch: 1250, loss is 6.942374629974365 and perplexity is 1035.2255762028894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.370365337733805 and perplexity of 584.271246237244
Finished 1 epochs...
Completing Train Step...
At time: 24.29875659942627 and batch: 50, loss is 6.7805699634552 and perplexity is 880.5704740946036
At time: 25.165698766708374 and batch: 100, loss is 6.764459028244018 and perplexity is 866.4973303941741
At time: 26.00194501876831 and batch: 150, loss is 6.674493618011475 and perplexity is 791.946324966964
At time: 26.83753490447998 and batch: 200, loss is 6.641973676681519 and perplexity is 766.6065343231486
At time: 27.67448902130127 and batch: 250, loss is 6.688164348602295 and perplexity is 802.8471511774143
At time: 28.509578704833984 and batch: 300, loss is 6.65262879371643 and perplexity is 774.818488640434
At time: 29.34386682510376 and batch: 350, loss is 6.653831844329834 and perplexity is 775.7511954327139
At time: 30.18253207206726 and batch: 400, loss is 6.590827760696411 and perplexity is 728.3835477006165
At time: 31.017703771591187 and batch: 450, loss is 6.563999147415161 and perplexity is 709.1018345186774
At time: 31.85912013053894 and batch: 500, loss is 6.5495271968841555 and perplexity is 698.9136471349486
At time: 32.694863080978394 and batch: 550, loss is 6.546330080032349 and perplexity is 696.682706724633
At time: 33.53211450576782 and batch: 600, loss is 6.560337619781494 and perplexity is 706.5101861480384
At time: 34.36756682395935 and batch: 650, loss is 6.536942195892334 and perplexity is 690.1729344953504
At time: 35.20547151565552 and batch: 700, loss is 6.523921279907227 and perplexity is 681.2445050118193
At time: 36.0440878868103 and batch: 750, loss is 6.443139734268189 and perplexity is 628.3766414781621
At time: 36.880329608917236 and batch: 800, loss is 6.424029245376587 and perplexity is 616.4820741496719
At time: 37.717958211898804 and batch: 850, loss is 6.465736799240112 and perplexity is 642.7377577178304
At time: 38.5544695854187 and batch: 900, loss is 6.448070306777954 and perplexity is 631.4825487306822
At time: 39.395715951919556 and batch: 950, loss is 6.412942867279053 and perplexity is 609.6852663905371
At time: 40.23448586463928 and batch: 1000, loss is 6.409301519393921 and perplexity is 607.4692273682876
At time: 41.07535934448242 and batch: 1050, loss is 6.3808058166503905 and perplexity is 590.4032728112522
At time: 41.91353130340576 and batch: 1100, loss is 6.360588912963867 and perplexity is 578.5869934622635
At time: 42.75163292884827 and batch: 1150, loss is 6.3979397392272945 and perplexity is 600.6063565906836
At time: 43.59056258201599 and batch: 1200, loss is 6.3934887027740475 and perplexity is 597.9389765102151
At time: 44.42829418182373 and batch: 1250, loss is 6.356024103164673 and perplexity is 575.9518728727382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.923277472057482 and perplexity of 373.6342852396444
Finished 2 epochs...
Completing Train Step...
At time: 46.87676787376404 and batch: 50, loss is 6.335503301620483 and perplexity is 564.2533211417355
At time: 47.717875957489014 and batch: 100, loss is 6.357671318054199 and perplexity is 576.9013711725622
At time: 48.55227017402649 and batch: 150, loss is 6.273086977005005 and perplexity is 530.1112960272139
At time: 49.388670444488525 and batch: 200, loss is 6.27593768119812 and perplexity is 531.6246425484005
At time: 50.2260000705719 and batch: 250, loss is 6.328965301513672 and perplexity is 560.5762662585547
At time: 51.06400108337402 and batch: 300, loss is 6.317072772979737 and perplexity is 553.9490821128421
At time: 51.9001829624176 and batch: 350, loss is 6.336085577011108 and perplexity is 564.5819676368122
At time: 52.73437976837158 and batch: 400, loss is 6.286157102584839 and perplexity is 537.0853941257959
At time: 53.56900358200073 and batch: 450, loss is 6.273577003479004 and perplexity is 530.3711282535687
At time: 54.4061222076416 and batch: 500, loss is 6.26903886795044 and perplexity is 527.9696853512037
At time: 55.24202513694763 and batch: 550, loss is 6.275379915237426 and perplexity is 531.3282030985237
At time: 56.07899332046509 and batch: 600, loss is 6.298600397109985 and perplexity is 543.8102588357019
At time: 56.91480827331543 and batch: 650, loss is 6.2851919841766355 and perplexity is 536.5672931797438
At time: 57.75375723838806 and batch: 700, loss is 6.282672119140625 and perplexity is 535.2169181141919
At time: 58.594313621520996 and batch: 750, loss is 6.217286748886108 and perplexity is 501.3411206267557
At time: 59.43097710609436 and batch: 800, loss is 6.209885587692261 and perplexity is 497.644311395457
At time: 60.26775789260864 and batch: 850, loss is 6.2512386035919185 and perplexity is 518.6548347295424
At time: 61.104814529418945 and batch: 900, loss is 6.246493616104126 and perplexity is 516.1996535368003
At time: 61.94978356361389 and batch: 950, loss is 6.217536344528198 and perplexity is 501.46626880323447
At time: 62.79932928085327 and batch: 1000, loss is 6.215383129119873 and perplexity is 500.3876655557888
At time: 63.63699412345886 and batch: 1050, loss is 6.193108549118042 and perplexity is 489.3649592892806
At time: 64.53564095497131 and batch: 1100, loss is 6.177429447174072 and perplexity is 481.7519943911238
At time: 65.37820243835449 and batch: 1150, loss is 6.221682481765747 and perplexity is 503.54973295320156
At time: 66.22081279754639 and batch: 1200, loss is 6.224943552017212 and perplexity is 505.1945244401239
At time: 67.0612359046936 and batch: 1250, loss is 6.195184936523438 and perplexity is 490.38212617819966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.767934980183623 and perplexity of 319.8764988429729
Finished 3 epochs...
Completing Train Step...
At time: 69.46767354011536 and batch: 50, loss is 6.177049875259399 and perplexity is 481.5691695639953
At time: 70.3408420085907 and batch: 100, loss is 6.205111246109009 and perplexity is 495.2740501865955
At time: 71.18511605262756 and batch: 150, loss is 6.119422960281372 and perplexity is 454.6022952188817
At time: 72.02755355834961 and batch: 200, loss is 6.129159421920776 and perplexity is 459.05013098124505
At time: 72.87066793441772 and batch: 250, loss is 6.1814565753936765 and perplexity is 483.69598316134307
At time: 73.71318006515503 and batch: 300, loss is 6.176660871505737 and perplexity is 481.3818737811299
At time: 74.55572152137756 and batch: 350, loss is 6.201958341598511 and perplexity is 493.7149575265224
At time: 75.4008400440216 and batch: 400, loss is 6.155428504943847 and perplexity is 471.268740185702
At time: 76.24566531181335 and batch: 450, loss is 6.145579519271851 and perplexity is 466.65000338792345
At time: 77.09184646606445 and batch: 500, loss is 6.1401211643219 and perplexity is 464.1098010019894
At time: 77.93505358695984 and batch: 550, loss is 6.149032440185547 and perplexity is 468.26409400393226
At time: 78.78191304206848 and batch: 600, loss is 6.175926752090454 and perplexity is 481.02861168556893
At time: 79.62504386901855 and batch: 650, loss is 6.164104528427124 and perplexity is 475.3752672457749
At time: 80.4675464630127 and batch: 700, loss is 6.165889501571655 and perplexity is 476.2245570856653
At time: 81.31003737449646 and batch: 750, loss is 6.105490093231201 and perplexity is 448.31230244869613
At time: 82.1569983959198 and batch: 800, loss is 6.100566215515137 and perplexity is 446.110293153976
At time: 82.99899816513062 and batch: 850, loss is 6.143437776565552 and perplexity is 465.65162865930336
At time: 83.84392881393433 and batch: 900, loss is 6.14129189491272 and perplexity is 464.65346672447185
At time: 84.68749451637268 and batch: 950, loss is 6.1110608005523686 and perplexity is 450.8166881976289
At time: 85.5300452709198 and batch: 1000, loss is 6.108634414672852 and perplexity is 449.7241589359308
At time: 86.40203213691711 and batch: 1050, loss is 6.091132736206054 and perplexity is 441.9217084594498
At time: 87.24813604354858 and batch: 1100, loss is 6.076688957214356 and perplexity is 435.58456529081485
At time: 88.09354662895203 and batch: 1150, loss is 6.12238751411438 and perplexity is 455.95198782537227
At time: 88.93458795547485 and batch: 1200, loss is 6.128897180557251 and perplexity is 458.92976483215153
At time: 89.7796881198883 and batch: 1250, loss is 6.1006779384613035 and perplexity is 446.16013669452406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.677455484432025 and perplexity of 292.20496309287677
Finished 4 epochs...
Completing Train Step...
At time: 92.20995330810547 and batch: 50, loss is 6.0830894947052006 and perplexity is 438.38148196801166
At time: 93.05532789230347 and batch: 100, loss is 6.114370813369751 and perplexity is 452.31136955641387
At time: 93.8955135345459 and batch: 150, loss is 6.027425765991211 and perplexity is 414.6462578396127
At time: 94.73886346817017 and batch: 200, loss is 6.039622745513916 and perplexity is 419.7346582316185
At time: 95.58147144317627 and batch: 250, loss is 6.090445547103882 and perplexity is 441.6181289975786
At time: 96.42060685157776 and batch: 300, loss is 6.089174327850341 and perplexity is 441.05709220522334
At time: 97.26428890228271 and batch: 350, loss is 6.117218780517578 and perplexity is 453.6013735500702
At time: 98.10707569122314 and batch: 400, loss is 6.073191318511963 and perplexity is 434.06370911025397
At time: 98.94671368598938 and batch: 450, loss is 6.063483438491821 and perplexity is 429.87025839287134
At time: 99.7869029045105 and batch: 500, loss is 6.056672639846802 and perplexity is 426.95244621412394
At time: 100.62572002410889 and batch: 550, loss is 6.067560844421386 and perplexity is 431.62659214292114
At time: 101.46674132347107 and batch: 600, loss is 6.095958642959594 and perplexity is 444.059535745355
At time: 102.30805206298828 and batch: 650, loss is 6.085406684875489 and perplexity is 439.3984730546888
At time: 103.14861941337585 and batch: 700, loss is 6.089633474349975 and perplexity is 441.2596485232018
At time: 103.9947338104248 and batch: 750, loss is 6.031451988220215 and perplexity is 416.3190811395076
At time: 104.8358564376831 and batch: 800, loss is 6.0277675533294675 and perplexity is 414.7880029023517
At time: 105.67510318756104 and batch: 850, loss is 6.07213583946228 and perplexity is 433.6058056553224
At time: 106.5138909816742 and batch: 900, loss is 6.06992883682251 and perplexity is 432.6498917379011
At time: 107.4105954170227 and batch: 950, loss is 6.0392446613311765 and perplexity is 419.57599319265444
At time: 108.25116801261902 and batch: 1000, loss is 6.036478071212769 and perplexity is 418.4168026379059
At time: 109.09313082695007 and batch: 1050, loss is 6.02186619758606 and perplexity is 412.34739985556365
At time: 109.93467450141907 and batch: 1100, loss is 6.00765061378479 and perplexity is 406.52710827379474
At time: 110.7727210521698 and batch: 1150, loss is 6.054064168930053 and perplexity is 425.8402044311635
At time: 111.61403250694275 and batch: 1200, loss is 6.062262868881225 and perplexity is 429.34589189698244
At time: 112.45502352714539 and batch: 1250, loss is 6.034626388549805 and perplexity is 417.6427443748711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.615367137602646 and perplexity of 274.61418188240896
Finished 5 epochs...
Completing Train Step...
At time: 114.89149761199951 and batch: 50, loss is 6.017827157974243 and perplexity is 410.68527133482615
At time: 115.73493099212646 and batch: 100, loss is 6.050854177474975 and perplexity is 424.47545260643045
At time: 116.57657480239868 and batch: 150, loss is 5.962982025146484 and perplexity is 388.76771249612165
At time: 117.42190217971802 and batch: 200, loss is 5.976945056915283 and perplexity is 394.23416371825465
At time: 118.26556825637817 and batch: 250, loss is 6.025828704833985 and perplexity is 413.9845709251043
At time: 119.10978055000305 and batch: 300, loss is 6.026382274627686 and perplexity is 414.2138037209487
At time: 119.95593643188477 and batch: 350, loss is 6.0564608669281 and perplexity is 426.8620388216989
At time: 120.79896426200867 and batch: 400, loss is 6.013938398361206 and perplexity is 409.09131630067935
At time: 121.64130878448486 and batch: 450, loss is 6.003888359069824 and perplexity is 405.00052324546857
At time: 122.48662948608398 and batch: 500, loss is 5.9961716747283935 and perplexity is 401.88728941951246
At time: 123.33455181121826 and batch: 550, loss is 6.008646612167358 and perplexity is 406.93221032310964
At time: 124.17781281471252 and batch: 600, loss is 6.037839298248291 and perplexity is 418.9867507281414
At time: 125.02089667320251 and batch: 650, loss is 6.028145523071289 and perplexity is 414.9448098489931
At time: 125.86637496948242 and batch: 700, loss is 6.0339650058746335 and perplexity is 417.3666140233113
At time: 126.71116065979004 and batch: 750, loss is 5.977077579498291 and perplexity is 394.2864121099099
At time: 127.55624389648438 and batch: 800, loss is 5.973944997787475 and perplexity is 393.0532102677373
At time: 128.39756417274475 and batch: 850, loss is 6.020007743835449 and perplexity is 411.5817829359259
At time: 129.30120706558228 and batch: 900, loss is 6.0175501823425295 and perplexity is 410.5715372738715
At time: 130.1474847793579 and batch: 950, loss is 5.986153030395508 and perplexity is 397.88102578024433
At time: 130.99079537391663 and batch: 1000, loss is 5.9831757068634035 and perplexity is 396.6981669896502
At time: 131.83486604690552 and batch: 1050, loss is 5.970727882385254 and perplexity is 391.79074456887344
At time: 132.67730283737183 and batch: 1100, loss is 5.956389284133911 and perplexity is 386.21309786277374
At time: 133.52546739578247 and batch: 1150, loss is 6.002885322570801 and perplexity is 404.59449660234253
At time: 134.36981439590454 and batch: 1200, loss is 6.011952829360962 and perplexity is 408.2798431493324
At time: 135.21554708480835 and batch: 1250, loss is 5.9843612575531 and perplexity is 397.1687516710046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.569567158274407 and perplexity of 262.3205313921341
Finished 6 epochs...
Completing Train Step...
At time: 137.593364238739 and batch: 50, loss is 5.968521757125854 and perplexity is 390.92735783070145
At time: 138.49266934394836 and batch: 100, loss is 6.002527780532837 and perplexity is 404.44986291933
At time: 139.337340593338 and batch: 150, loss is 5.913790435791015 and perplexity is 370.10636444195734
At time: 140.1815469264984 and batch: 200, loss is 5.928951349258423 and perplexity is 375.7602658652054
At time: 141.02309370040894 and batch: 250, loss is 5.976744508743286 and perplexity is 394.1551087048163
At time: 141.86373734474182 and batch: 300, loss is 5.978221654891968 and perplexity is 394.7377636329203
At time: 142.70755791664124 and batch: 350, loss is 6.009479198455811 and perplexity is 407.27115758359525
At time: 143.54596734046936 and batch: 400, loss is 5.968230590820313 and perplexity is 390.8135495255626
At time: 144.38415098190308 and batch: 450, loss is 5.958015861511231 and perplexity is 386.841814540125
At time: 145.22196698188782 and batch: 500, loss is 5.949372005462647 and perplexity is 383.5124197164243
At time: 146.06070852279663 and batch: 550, loss is 5.963253965377808 and perplexity is 388.87344845406784
At time: 146.90071058273315 and batch: 600, loss is 5.992720279693604 and perplexity is 400.5026085391634
At time: 147.74265789985657 and batch: 650, loss is 5.983821573257447 and perplexity is 396.9544637621141
At time: 148.5818588733673 and batch: 700, loss is 5.990351152420044 and perplexity is 399.5548899623822
At time: 149.420729637146 and batch: 750, loss is 5.93464919090271 and perplexity is 377.90739956125554
At time: 150.26021146774292 and batch: 800, loss is 5.932067365646362 and perplexity is 376.93296714090695
At time: 151.160311460495 and batch: 850, loss is 5.979098968505859 and perplexity is 395.0842244020252
At time: 151.9968388080597 and batch: 900, loss is 5.976341485977173 and perplexity is 393.99628722911353
At time: 152.83902716636658 and batch: 950, loss is 5.944258413314819 and perplexity is 381.5562992819341
At time: 153.68187355995178 and batch: 1000, loss is 5.940733728408813 and perplexity is 380.2138008848481
At time: 154.52316784858704 and batch: 1050, loss is 5.9301678085327145 and perplexity is 376.2176410582742
At time: 155.36706471443176 and batch: 1100, loss is 5.915610828399658 and perplexity is 370.78071693912585
At time: 156.21174883842468 and batch: 1150, loss is 5.962316541671753 and perplexity is 388.50908007529256
At time: 157.0492649078369 and batch: 1200, loss is 5.971640586853027 and perplexity is 392.1484969681455
At time: 157.8871099948883 and batch: 1250, loss is 5.944036149978638 and perplexity is 381.4715027298461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.533652201186132 and perplexity of 253.06647498354957
Finished 7 epochs...
Completing Train Step...
At time: 160.3177101612091 and batch: 50, loss is 5.929061193466186 and perplexity is 375.801543220916
At time: 161.16253805160522 and batch: 100, loss is 5.963368740081787 and perplexity is 388.9180838504577
At time: 162.00960659980774 and batch: 150, loss is 5.873926610946655 and perplexity is 355.64271272333934
At time: 162.85920238494873 and batch: 200, loss is 5.889985618591308 and perplexity is 361.4000868925617
At time: 163.70429158210754 and batch: 250, loss is 5.937126741409302 and perplexity is 378.8448450352146
At time: 164.54895734786987 and batch: 300, loss is 5.939294452667236 and perplexity is 379.6669620049878
At time: 165.39660954475403 and batch: 350, loss is 5.971215038299561 and perplexity is 391.9816542448742
At time: 166.24156999588013 and batch: 400, loss is 5.931022701263427 and perplexity is 376.53940430171326
At time: 167.08620142936707 and batch: 450, loss is 5.920153503417969 and perplexity is 372.46888473500263
At time: 167.92797756195068 and batch: 500, loss is 5.9112240505218505 and perplexity is 369.15774670018794
At time: 168.77003383636475 and batch: 550, loss is 5.926112155914307 and perplexity is 374.6949228923623
At time: 169.61130356788635 and batch: 600, loss is 5.955548429489136 and perplexity is 385.8884852806878
At time: 170.45204830169678 and batch: 650, loss is 5.947191410064697 and perplexity is 382.67704543660597
At time: 171.29684734344482 and batch: 700, loss is 5.954238786697387 and perplexity is 385.3834399941669
At time: 172.13974571228027 and batch: 750, loss is 5.899260578155517 and perplexity is 364.76765095408695
At time: 173.03965997695923 and batch: 800, loss is 5.8974174213409425 and perplexity is 364.09594619126113
At time: 173.88060426712036 and batch: 850, loss is 5.94525444984436 and perplexity is 381.93653262581006
At time: 174.7240481376648 and batch: 900, loss is 5.942242660522461 and perplexity is 380.7879507667138
At time: 175.56627225875854 and batch: 950, loss is 5.909566602706909 and perplexity is 368.5463937822857
At time: 176.40911030769348 and batch: 1000, loss is 5.9052496433258055 and perplexity is 366.9588231747038
At time: 177.25151562690735 and batch: 1050, loss is 5.896192665100098 and perplexity is 363.6502903744766
At time: 178.0934226512909 and batch: 1100, loss is 5.881592664718628 and perplexity is 358.37956591118285
At time: 178.9350962638855 and batch: 1150, loss is 5.928401689529419 and perplexity is 375.5537823323397
At time: 179.77778315544128 and batch: 1200, loss is 5.937855386734009 and perplexity is 379.12098915368165
At time: 180.6226942539215 and batch: 1250, loss is 5.910282049179077 and perplexity is 368.81016334479784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.502817863965556 and perplexity of 245.38241322663276
Finished 8 epochs...
Completing Train Step...
At time: 183.04831910133362 and batch: 50, loss is 5.8957521438598635 and perplexity is 363.4901299771677
At time: 183.95337057113647 and batch: 100, loss is 5.9300809001922605 and perplexity is 376.1849460281963
At time: 184.7940330505371 and batch: 150, loss is 5.84029706954956 and perplexity is 343.8814822096662
At time: 185.63571524620056 and batch: 200, loss is 5.85708664894104 and perplexity is 349.7038484248127
At time: 186.4784710407257 and batch: 250, loss is 5.903707532882691 and perplexity is 366.3933682503242
At time: 187.31967186927795 and batch: 300, loss is 5.906407432556152 and perplexity is 367.38393019283274
At time: 188.1633141040802 and batch: 350, loss is 5.9387186336517335 and perplexity is 379.4484054792477
At time: 189.00639247894287 and batch: 400, loss is 5.899232549667358 and perplexity is 364.75742721158014
At time: 189.8472547531128 and batch: 450, loss is 5.887920446395874 and perplexity is 360.65450362558875
At time: 190.68489122390747 and batch: 500, loss is 5.878806085586548 and perplexity is 357.3823030127557
At time: 191.527019739151 and batch: 550, loss is 5.894354944229126 and perplexity is 362.9826163332874
At time: 192.36742901802063 and batch: 600, loss is 5.9236642932891845 and perplexity is 373.7788428712452
At time: 193.20842027664185 and batch: 650, loss is 5.916005210876465 and perplexity is 370.92697519557544
At time: 194.0814836025238 and batch: 700, loss is 5.923416996002198 and perplexity is 373.68641980592776
At time: 194.92088341712952 and batch: 750, loss is 5.869142303466797 and perplexity is 353.9452724087062
At time: 195.7584092617035 and batch: 800, loss is 5.867786674499512 and perplexity is 353.465779025587
At time: 196.59967637062073 and batch: 850, loss is 5.9162386035919186 and perplexity is 371.01355695293404
At time: 197.43920254707336 and batch: 900, loss is 5.913095197677612 and perplexity is 369.84914181723127
At time: 198.28075408935547 and batch: 950, loss is 5.879862184524536 and perplexity is 357.7599334558794
At time: 199.12251591682434 and batch: 1000, loss is 5.874741020202637 and perplexity is 355.9324694146541
At time: 199.9642322063446 and batch: 1050, loss is 5.866966381072998 and perplexity is 353.1759522583022
At time: 200.8044788837433 and batch: 1100, loss is 5.85270212173462 and perplexity is 348.1739188470892
At time: 201.64726400375366 and batch: 1150, loss is 5.899369277954102 and perplexity is 364.8073032793366
At time: 202.48542499542236 and batch: 1200, loss is 5.9088703060150145 and perplexity is 368.28986546775815
At time: 203.32495546340942 and batch: 1250, loss is 5.881383972167969 and perplexity is 358.3047825691033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.47587552035812 and perplexity of 238.85950176670679
Finished 9 epochs...
Completing Train Step...
At time: 205.77939987182617 and batch: 50, loss is 5.867076196670532 and perplexity is 353.2147386161688
At time: 206.61773300170898 and batch: 100, loss is 5.901328735351562 and perplexity is 365.5228284399929
At time: 207.45513534545898 and batch: 150, loss is 5.81131501197815 and perplexity is 334.05812740397374
At time: 208.2910611629486 and batch: 200, loss is 5.828772239685058 and perplexity is 339.9410566153379
At time: 209.13048696517944 and batch: 250, loss is 5.8749424076080325 and perplexity is 356.00415694940654
At time: 209.97423362731934 and batch: 300, loss is 5.878219165802002 and perplexity is 357.17260981103493
At time: 210.82470154762268 and batch: 350, loss is 5.910603580474853 and perplexity is 368.92876642079193
At time: 211.67556738853455 and batch: 400, loss is 5.871872758865356 and perplexity is 354.9130247896584
At time: 212.5252845287323 and batch: 450, loss is 5.859922981262207 and perplexity is 350.6971327292285
At time: 213.36207222938538 and batch: 500, loss is 5.850689468383789 and perplexity is 347.4738701566326
At time: 214.19984698295593 and batch: 550, loss is 5.866711292266846 and perplexity is 353.08587251593445
At time: 215.0413384437561 and batch: 600, loss is 5.895983581542969 and perplexity is 363.57426502631614
At time: 215.9087483882904 and batch: 650, loss is 5.888903789520263 and perplexity is 361.0093251790848
At time: 216.75061702728271 and batch: 700, loss is 5.89679744720459 and perplexity is 363.870286080416
At time: 217.58948230743408 and batch: 750, loss is 5.843002252578735 and perplexity is 344.8130039597279
At time: 218.42753911018372 and batch: 800, loss is 5.842245712280273 and perplexity is 344.552237679336
At time: 219.26685690879822 and batch: 850, loss is 5.890992507934571 and perplexity is 361.76416004863955
At time: 220.10346364974976 and batch: 900, loss is 5.887514610290527 and perplexity is 360.5081667028732
At time: 220.94238090515137 and batch: 950, loss is 5.85376088142395 and perplexity is 348.5427465727511
At time: 221.7794075012207 and batch: 1000, loss is 5.847941331863403 and perplexity is 346.52027542857337
At time: 222.61588215827942 and batch: 1050, loss is 5.841260471343994 and perplexity is 344.21293788355854
At time: 223.4553165435791 and batch: 1100, loss is 5.827320365905762 and perplexity is 339.4478632226444
At time: 224.29498267173767 and batch: 1150, loss is 5.874054164886474 and perplexity is 355.68807924579306
At time: 225.14184141159058 and batch: 1200, loss is 5.883631677627563 and perplexity is 359.1110519737025
At time: 225.98155236244202 and batch: 1250, loss is 5.855942630767823 and perplexity is 349.3040096220073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.452700872490876 and perplexity of 233.38766575172644
Finished 10 epochs...
Completing Train Step...
At time: 228.41496467590332 and batch: 50, loss is 5.841887140274048 and perplexity is 344.42871303978444
At time: 229.29294657707214 and batch: 100, loss is 5.8760370349884035 and perplexity is 356.3940622086832
At time: 230.13509035110474 and batch: 150, loss is 5.785826005935669 and perplexity is 325.65091864146785
At time: 230.98054265975952 and batch: 200, loss is 5.803674373626709 and perplexity is 331.5154363256483
At time: 231.82368326187134 and batch: 250, loss is 5.84981861114502 and perplexity is 347.1714017440428
At time: 232.67092514038086 and batch: 300, loss is 5.853345718383789 and perplexity is 348.39807453977636
At time: 233.5135462284088 and batch: 350, loss is 5.885960702896118 and perplexity is 359.9484054181232
At time: 234.35953450202942 and batch: 400, loss is 5.847522354125976 and perplexity is 346.3751215578454
At time: 235.2046558856964 and batch: 450, loss is 5.8351365756988525 and perplexity is 342.1114549707783
At time: 236.04769158363342 and batch: 500, loss is 5.825812139511108 and perplexity is 338.93628488067833
At time: 236.89458584785461 and batch: 550, loss is 5.8422199058532716 and perplexity is 344.5433461318961
At time: 237.76711463928223 and batch: 600, loss is 5.871437311172485 and perplexity is 354.7585123753226
At time: 238.61147332191467 and batch: 650, loss is 5.864842004776001 and perplexity is 352.42647000968367
At time: 239.45741248130798 and batch: 700, loss is 5.873163709640503 and perplexity is 355.3714959022671
At time: 240.30082511901855 and batch: 750, loss is 5.819750728607178 and perplexity is 336.88806660513706
At time: 241.14592456817627 and batch: 800, loss is 5.819524583816528 and perplexity is 336.8118897376676
At time: 241.9905915260315 and batch: 850, loss is 5.8684321212768555 and perplexity is 353.693996016598
At time: 242.8336672782898 and batch: 900, loss is 5.864639339447021 and perplexity is 352.3550526203566
At time: 243.68057942390442 and batch: 950, loss is 5.830571784973144 and perplexity is 340.55334669862935
At time: 244.52900528907776 and batch: 1000, loss is 5.8240756416320805 and perplexity is 338.3482334636454
At time: 245.37203788757324 and batch: 1050, loss is 5.8181994438171385 and perplexity is 336.3658624198946
At time: 246.21684050559998 and batch: 1100, loss is 5.804751663208008 and perplexity is 331.8727668909308
At time: 247.05952882766724 and batch: 1150, loss is 5.8515104484558105 and perplexity is 347.7592564117642
At time: 247.9048366546631 and batch: 1200, loss is 5.861063213348388 and perplexity is 351.0972369149646
At time: 248.74952745437622 and batch: 1250, loss is 5.833322114944458 and perplexity is 341.4912699829169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.432251422074589 and perplexity of 228.6634843386557
Finished 11 epochs...
Completing Train Step...
At time: 251.14607667922974 and batch: 50, loss is 5.81944206237793 and perplexity is 336.7840966827668
At time: 252.0138053894043 and batch: 100, loss is 5.853481969833374 and perplexity is 348.44554751652254
At time: 252.8559808731079 and batch: 150, loss is 5.76293251991272 and perplexity is 318.28032509243576
At time: 253.69735097885132 and batch: 200, loss is 5.781299610137939 and perplexity is 324.18022467429427
At time: 254.53992319107056 and batch: 250, loss is 5.827263069152832 and perplexity is 339.42841451947186
At time: 255.38175320625305 and batch: 300, loss is 5.831107397079467 and perplexity is 340.73580005172056
At time: 256.2225458621979 and batch: 350, loss is 5.863737335205078 and perplexity is 352.0373701652233
At time: 257.06191062927246 and batch: 400, loss is 5.825666971206665 and perplexity is 338.8870856460591
At time: 257.90729308128357 and batch: 450, loss is 5.812887935638428 and perplexity is 334.5839887979711
At time: 258.7497034072876 and batch: 500, loss is 5.803735294342041 and perplexity is 331.5356330983675
At time: 259.62344574928284 and batch: 550, loss is 5.820131435394287 and perplexity is 337.0163465955717
At time: 260.46488976478577 and batch: 600, loss is 5.84935583114624 and perplexity is 347.01077493347157
At time: 261.3097894191742 and batch: 650, loss is 5.843145217895508 and perplexity is 344.86230378406486
At time: 262.15039706230164 and batch: 700, loss is 5.851813087463379 and perplexity is 347.86451785530335
At time: 262.9890465736389 and batch: 750, loss is 5.798867473602295 and perplexity is 329.9256986827149
At time: 263.827805519104 and batch: 800, loss is 5.79910343170166 and perplexity is 330.0035565087388
At time: 264.66840720176697 and batch: 850, loss is 5.847997817993164 and perplexity is 346.53984957064466
At time: 265.50954842567444 and batch: 900, loss is 5.844002094268799 and perplexity is 345.1579347857378
At time: 266.3514952659607 and batch: 950, loss is 5.809591341018677 and perplexity is 333.4828170762831
At time: 267.18988156318665 and batch: 1000, loss is 5.802588624954224 and perplexity is 331.1556892135646
At time: 268.0296380519867 and batch: 1050, loss is 5.79733868598938 and perplexity is 329.42169771476705
At time: 268.8689241409302 and batch: 1100, loss is 5.7844271469116215 and perplexity is 325.19569738465003
At time: 269.7107877731323 and batch: 1150, loss is 5.831037092208862 and perplexity is 340.7118455074578
At time: 270.5502607822418 and batch: 1200, loss is 5.840673837661743 and perplexity is 344.0110701973108
At time: 271.39094734191895 and batch: 1250, loss is 5.81300721168518 and perplexity is 334.6238990335818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.413912362425867 and perplexity of 224.50822925300048
Finished 12 epochs...
Completing Train Step...
At time: 273.8826880455017 and batch: 50, loss is 5.799174461364746 and perplexity is 330.0269973826638
At time: 274.72811341285706 and batch: 100, loss is 5.833028402328491 and perplexity is 341.3909844169691
At time: 275.56933665275574 and batch: 150, loss is 5.742102012634278 and perplexity is 311.7189599974927
At time: 276.4170162677765 and batch: 200, loss is 5.760950565338135 and perplexity is 317.6501326587972
At time: 277.25952553749084 and batch: 250, loss is 5.8068086051940915 and perplexity is 332.5561124793655
At time: 278.10345554351807 and batch: 300, loss is 5.811005001068115 and perplexity is 333.954581790857
At time: 278.95218324661255 and batch: 350, loss is 5.843421335220337 and perplexity is 344.95753938831524
At time: 279.79918575286865 and batch: 400, loss is 5.805740337371827 and perplexity is 332.20104317363024
At time: 280.67884135246277 and batch: 450, loss is 5.7926382637023925 and perplexity is 327.87691004641044
At time: 281.5260055065155 and batch: 500, loss is 5.783320646286011 and perplexity is 324.8360671442837
At time: 282.36878728866577 and batch: 550, loss is 5.800013685226441 and perplexity is 330.3040801648496
At time: 283.2171730995178 and batch: 600, loss is 5.829232683181763 and perplexity is 340.0976163047988
At time: 284.06396985054016 and batch: 650, loss is 5.823379678726196 and perplexity is 338.11283756679086
At time: 284.91409397125244 and batch: 700, loss is 5.832246351242065 and perplexity is 341.12410359752937
At time: 285.75402998924255 and batch: 750, loss is 5.779818696975708 and perplexity is 323.7004972176658
At time: 286.61397552490234 and batch: 800, loss is 5.780476493835449 and perplexity is 323.9134964356603
At time: 287.4596736431122 and batch: 850, loss is 5.829342403411865 and perplexity is 340.1349339407294
At time: 288.3078935146332 and batch: 900, loss is 5.8250924015045165 and perplexity is 338.6924273219402
At time: 289.1536943912506 and batch: 950, loss is 5.790447387695313 and perplexity is 327.1593587109334
At time: 289.9979393482208 and batch: 1000, loss is 5.783127784729004 and perplexity is 324.7734247954468
At time: 290.8442385196686 and batch: 1050, loss is 5.778284683227539 and perplexity is 323.20431687567867
At time: 291.69179701805115 and batch: 1100, loss is 5.765764102935791 and perplexity is 319.1828394266589
At time: 292.53605914115906 and batch: 1150, loss is 5.812292776107788 and perplexity is 334.3849171937959
At time: 293.3812551498413 and batch: 1200, loss is 5.822078132629395 and perplexity is 337.6730543839035
At time: 294.22490429878235 and batch: 1250, loss is 5.7944355392456055 and perplexity is 328.4667250693106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.397349113965556 and perplexity of 220.82027027381253
Finished 13 epochs...
Completing Train Step...
At time: 296.62804532051086 and batch: 50, loss is 5.780624780654907 and perplexity is 323.9615320992662
At time: 297.50203371047974 and batch: 100, loss is 5.814243259429932 and perplexity is 335.0377658762184
At time: 298.34308528900146 and batch: 150, loss is 5.722994661331176 and perplexity is 305.81937849829467
At time: 299.18241691589355 and batch: 200, loss is 5.742221698760987 and perplexity is 311.7562706651766
At time: 300.0232894420624 and batch: 250, loss is 5.787948589324952 and perplexity is 326.342873978741
At time: 300.86205983161926 and batch: 300, loss is 5.792499732971192 and perplexity is 327.83149216427665
At time: 301.70272397994995 and batch: 350, loss is 5.8246842384338375 and perplexity is 338.5542137895375
At time: 302.591920375824 and batch: 400, loss is 5.787372961044311 and perplexity is 326.15507584743665
At time: 303.4358947277069 and batch: 450, loss is 5.774049615859985 and perplexity is 321.83841919610893
At time: 304.27986550331116 and batch: 500, loss is 5.764672203063965 and perplexity is 318.8345139280908
At time: 305.12174820899963 and batch: 550, loss is 5.781529397964477 and perplexity is 324.25472590293873
At time: 305.9668471813202 and batch: 600, loss is 5.810716180801392 and perplexity is 333.85814286689896
At time: 306.8088438510895 and batch: 650, loss is 5.805226612091064 and perplexity is 332.030426928148
At time: 307.6548113822937 and batch: 700, loss is 5.814176921844482 and perplexity is 335.0155410169756
At time: 308.49411964416504 and batch: 750, loss is 5.762286205291748 and perplexity is 318.0746823268821
At time: 309.33863854408264 and batch: 800, loss is 5.763367986679077 and perplexity is 318.4189557787363
At time: 310.182874917984 and batch: 850, loss is 5.812225046157837 and perplexity is 334.3622700870424
At time: 311.02480840682983 and batch: 900, loss is 5.80765193939209 and perplexity is 332.83668671406747
At time: 311.8676669597626 and batch: 950, loss is 5.772823762893677 and perplexity is 321.44413433227527
At time: 312.70896220207214 and batch: 1000, loss is 5.765308380126953 and perplexity is 319.0374136659774
At time: 313.54995250701904 and batch: 1050, loss is 5.760618991851807 and perplexity is 317.54482575633216
At time: 314.3901822566986 and batch: 1100, loss is 5.748577804565429 and perplexity is 313.74413735974747
At time: 315.2381401062012 and batch: 1150, loss is 5.79497709274292 and perplexity is 328.64465554811136
At time: 316.08007884025574 and batch: 1200, loss is 5.804955778121948 and perplexity is 331.9405139860447
At time: 316.9246644973755 and batch: 1250, loss is 5.7772972106933596 and perplexity is 322.88531901656927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3820676037864965 and perplexity of 217.47145570677603
Finished 14 epochs...
Completing Train Step...
At time: 319.4133839607239 and batch: 50, loss is 5.763459224700927 and perplexity is 318.4480090197412
At time: 320.25622820854187 and batch: 100, loss is 5.796813821792602 and perplexity is 329.24884142700137
At time: 321.10093092918396 and batch: 150, loss is 5.705182180404663 and perplexity is 300.42020573812755
At time: 321.9461431503296 and batch: 200, loss is 5.725039176940918 and perplexity is 306.4452705964568
At time: 322.7899649143219 and batch: 250, loss is 5.770564413070678 and perplexity is 320.718699398504
At time: 323.632474899292 and batch: 300, loss is 5.7754660987854 and perplexity is 322.29462084628904
At time: 324.5311977863312 and batch: 350, loss is 5.807334613800049 and perplexity is 332.731085871214
At time: 325.37918186187744 and batch: 400, loss is 5.770377426147461 and perplexity is 320.6587348021576
At time: 326.2245509624481 and batch: 450, loss is 5.7567972946167 and perplexity is 316.33358155225153
At time: 327.072261095047 and batch: 500, loss is 5.747404251098633 and perplexity is 313.3761578037363
At time: 327.91655254364014 and batch: 550, loss is 5.764424028396607 and perplexity is 318.7553970964572
At time: 328.75857162475586 and batch: 600, loss is 5.793566217422486 and perplexity is 328.18130585558947
At time: 329.6014356613159 and batch: 650, loss is 5.788432941436768 and perplexity is 326.50097712463423
At time: 330.44165444374084 and batch: 700, loss is 5.797432584762573 and perplexity is 329.45263146034335
At time: 331.28504705429077 and batch: 750, loss is 5.7460379314422605 and perplexity is 312.94827817622826
At time: 332.1290638446808 and batch: 800, loss is 5.747403154373169 and perplexity is 313.37581411631265
At time: 332.97182297706604 and batch: 850, loss is 5.796223192214966 and perplexity is 329.05443473967347
At time: 333.8212368488312 and batch: 900, loss is 5.791429901123047 and perplexity is 327.4809551344075
At time: 334.6639449596405 and batch: 950, loss is 5.756505985260009 and perplexity is 316.24144404102117
At time: 335.51210141181946 and batch: 1000, loss is 5.748656845092773 and perplexity is 313.7689368418845
At time: 336.35370326042175 and batch: 1050, loss is 5.744361791610718 and perplexity is 312.4241724619001
At time: 337.20372104644775 and batch: 1100, loss is 5.732601432800293 and perplexity is 308.7714727215903
At time: 338.044593334198 and batch: 1150, loss is 5.7789061069488525 and perplexity is 323.4052261234371
At time: 338.88681077957153 and batch: 1200, loss is 5.789052753448487 and perplexity is 326.70340908049224
At time: 339.73197984695435 and batch: 1250, loss is 5.7614114952087405 and perplexity is 317.7965808419197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.368001005075274 and perplexity of 214.43378693031036
Finished 15 epochs...
Completing Train Step...
At time: 342.16572403907776 and batch: 50, loss is 5.747504825592041 and perplexity is 313.4076770370425
At time: 343.04264664649963 and batch: 100, loss is 5.780648288726806 and perplexity is 323.9691478997713
At time: 343.8817148208618 and batch: 150, loss is 5.688819427490234 and perplexity is 295.544502886101
At time: 344.7272741794586 and batch: 200, loss is 5.7090697097778325 and perplexity is 301.5903711647475
At time: 345.5714201927185 and batch: 250, loss is 5.754456396102905 and perplexity is 315.5939427887302
At time: 346.44423365592957 and batch: 300, loss is 5.759572896957398 and perplexity is 317.212817421751
At time: 347.2840096950531 and batch: 350, loss is 5.7911005783081055 and perplexity is 327.3731259407042
At time: 348.1245951652527 and batch: 400, loss is 5.754544858932495 and perplexity is 315.6218623568144
At time: 348.967253446579 and batch: 450, loss is 5.740706119537354 and perplexity is 311.28413720672285
At time: 349.8112006187439 and batch: 500, loss is 5.731345062255859 and perplexity is 308.38378492905895
At time: 350.65087938308716 and batch: 550, loss is 5.748503665924073 and perplexity is 313.72087765790195
At time: 351.49311780929565 and batch: 600, loss is 5.777566289901733 and perplexity is 322.9722124326872
At time: 352.330561876297 and batch: 650, loss is 5.772824659347534 and perplexity is 321.44442249223846
At time: 353.1689956188202 and batch: 700, loss is 5.781819467544556 and perplexity is 324.3487959778938
At time: 354.00845861434937 and batch: 750, loss is 5.730972023010254 and perplexity is 308.2687671289825
At time: 354.8498270511627 and batch: 800, loss is 5.732661170959473 and perplexity is 308.7899187119371
At time: 355.6949315071106 and batch: 850, loss is 5.781220903396607 and perplexity is 324.1547105092873
At time: 356.53879141807556 and batch: 900, loss is 5.776252317428589 and perplexity is 322.54811452346735
At time: 357.3861184120178 and batch: 950, loss is 5.74127610206604 and perplexity is 311.4616143010045
At time: 358.2228662967682 and batch: 1000, loss is 5.7332143878936765 and perplexity is 308.9607937852141
At time: 359.06825947761536 and batch: 1050, loss is 5.729176616668701 and perplexity is 307.7157959820695
At time: 359.9095253944397 and batch: 1100, loss is 5.7176747894287105 and perplexity is 304.1967784184491
At time: 360.75813579559326 and batch: 1150, loss is 5.763908615112305 and perplexity is 318.59114866189935
At time: 361.6087951660156 and batch: 1200, loss is 5.774189233779907 and perplexity is 321.88335674371865
At time: 362.47913932800293 and batch: 1250, loss is 5.746601104736328 and perplexity is 313.124571926228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.354938145101506 and perplexity of 211.65088430402974
Finished 16 epochs...
Completing Train Step...
At time: 364.9154372215271 and batch: 50, loss is 5.732626152038574 and perplexity is 308.77910541153574
At time: 365.8164265155792 and batch: 100, loss is 5.765541439056396 and perplexity is 319.1117768492241
At time: 366.6594099998474 and batch: 150, loss is 5.673514986038208 and perplexity is 291.0557955385721
At time: 367.5537619590759 and batch: 200, loss is 5.6942008590698245 and perplexity is 297.13924254968936
At time: 368.39291763305664 and batch: 250, loss is 5.739346628189087 and perplexity is 310.86123664529913
At time: 369.2338078022003 and batch: 300, loss is 5.744812440872193 and perplexity is 312.5649979135465
At time: 370.0720374584198 and batch: 350, loss is 5.7758666610717775 and perplexity is 322.4237457760644
At time: 370.91832995414734 and batch: 400, loss is 5.739729661941528 and perplexity is 310.9803297981524
At time: 371.764949798584 and batch: 450, loss is 5.72558708190918 and perplexity is 306.6132194885317
At time: 372.6092839241028 and batch: 500, loss is 5.716348667144775 and perplexity is 303.7936436539637
At time: 373.447496175766 and batch: 550, loss is 5.733615865707398 and perplexity is 309.0848595923966
At time: 374.28595876693726 and batch: 600, loss is 5.762607412338257 and perplexity is 318.17686656643207
At time: 375.1239604949951 and batch: 650, loss is 5.758210191726684 and perplexity is 316.78084424909866
At time: 375.9593803882599 and batch: 700, loss is 5.767109775543213 and perplexity is 319.6126441537997
At time: 376.79918360710144 and batch: 750, loss is 5.716718921661377 and perplexity is 303.9061454485051
At time: 377.6421363353729 and batch: 800, loss is 5.718815851211548 and perplexity is 304.5440838475574
At time: 378.4822874069214 and batch: 850, loss is 5.767144622802735 and perplexity is 319.623781972617
At time: 379.3224205970764 and batch: 900, loss is 5.761992540359497 and perplexity is 317.98128866073944
At time: 380.1636874675751 and batch: 950, loss is 5.7270364093780515 and perplexity is 307.057924633703
At time: 381.01197957992554 and batch: 1000, loss is 5.718828229904175 and perplexity is 304.54785372849585
At time: 381.85673427581787 and batch: 1050, loss is 5.714925346374511 and perplexity is 303.36155542409824
At time: 382.69308519363403 and batch: 1100, loss is 5.70367922782898 and perplexity is 299.96902755076474
At time: 383.5368962287903 and batch: 1150, loss is 5.74974383354187 and perplexity is 314.1101854849836
At time: 384.3788013458252 and batch: 1200, loss is 5.760271873474121 and perplexity is 317.4346192400332
At time: 385.22149419784546 and batch: 1250, loss is 5.732738933563232 and perplexity is 308.8139319536849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.342780565693431 and perplexity of 209.09330038835495
Finished 17 epochs...
Completing Train Step...
At time: 387.80391120910645 and batch: 50, loss is 5.718717679977417 and perplexity is 304.51418784648666
At time: 388.6474883556366 and batch: 100, loss is 5.751398134231567 and perplexity is 314.63024823294865
At time: 389.55557584762573 and batch: 150, loss is 5.659190454483032 and perplexity is 286.9162767278173
At time: 390.39981174468994 and batch: 200, loss is 5.680324611663818 and perplexity is 293.04454016052335
At time: 391.243456363678 and batch: 250, loss is 5.725147294998169 and perplexity is 306.4784046549302
At time: 392.0887553691864 and batch: 300, loss is 5.730862283706665 and perplexity is 308.2349397852883
At time: 392.9333291053772 and batch: 350, loss is 5.761574935913086 and perplexity is 317.8485259837908
At time: 393.7775571346283 and batch: 400, loss is 5.725797986984253 and perplexity is 306.6778925923094
At time: 394.62173986434937 and batch: 450, loss is 5.711399230957031 and perplexity is 302.2937512731638
At time: 395.4686737060547 and batch: 500, loss is 5.702251968383789 and perplexity is 299.54119930650563
At time: 396.3155174255371 and batch: 550, loss is 5.719587650299072 and perplexity is 304.77922142139226
At time: 397.1586275100708 and batch: 600, loss is 5.748506193161011 and perplexity is 313.72167050589405
At time: 397.9992125034332 and batch: 650, loss is 5.744421529769897 and perplexity is 312.4428366643231
At time: 398.8469979763031 and batch: 700, loss is 5.753210077285766 and perplexity is 315.20085712516914
At time: 399.69166564941406 and batch: 750, loss is 5.703294553756714 and perplexity is 299.8536594343673
At time: 400.53625655174255 and batch: 800, loss is 5.705751152038574 and perplexity is 300.59118494999433
At time: 401.38197326660156 and batch: 850, loss is 5.75386715888977 and perplexity is 315.40803786982605
At time: 402.22516107559204 and batch: 900, loss is 5.748498640060425 and perplexity is 313.7193009435095
At time: 403.0682199001312 and batch: 950, loss is 5.713572998046875 and perplexity is 302.9515822077295
At time: 403.9133539199829 and batch: 1000, loss is 5.705304899215698 and perplexity is 300.45707521082926
At time: 404.7683799266815 and batch: 1050, loss is 5.7014994812011714 and perplexity is 299.31588317774305
At time: 405.6109220981598 and batch: 1100, loss is 5.690461444854736 and perplexity is 296.0301907366385
At time: 406.45250844955444 and batch: 1150, loss is 5.736403160095215 and perplexity is 309.9475718448725
At time: 407.29484724998474 and batch: 1200, loss is 5.7471914386749265 and perplexity is 313.30947455982533
At time: 408.1426327228546 and batch: 1250, loss is 5.719680509567261 and perplexity is 304.80752431092503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.331312638999772 and perplexity of 206.70913061855936
Finished 18 epochs...
Completing Train Step...
At time: 410.6113336086273 and batch: 50, loss is 5.705617189407349 and perplexity is 300.55091966101753
At time: 411.482302904129 and batch: 100, loss is 5.738044662475586 and perplexity is 310.4567693319524
At time: 412.32828283309937 and batch: 150, loss is 5.645597190856933 and perplexity is 283.0425361758451
At time: 413.1689615249634 and batch: 200, loss is 5.667227954864502 and perplexity is 289.23165888923666
At time: 414.0072646141052 and batch: 250, loss is 5.711737203598022 and perplexity is 302.3959355574102
At time: 414.8505017757416 and batch: 300, loss is 5.717709436416626 and perplexity is 304.20731810313794
At time: 415.6932165622711 and batch: 350, loss is 5.748087635040283 and perplexity is 313.59038722981296
At time: 416.534601688385 and batch: 400, loss is 5.712631168365479 and perplexity is 302.6663877389699
At time: 417.3767657279968 and batch: 450, loss is 5.698033990859986 and perplexity is 298.2804021367114
At time: 418.21618151664734 and batch: 500, loss is 5.688923444747925 and perplexity is 295.5752462137074
At time: 419.05659556388855 and batch: 550, loss is 5.706317777633667 and perplexity is 300.76155587275076
At time: 419.89541149139404 and batch: 600, loss is 5.735182514190674 and perplexity is 309.5694664240418
At time: 420.7365155220032 and batch: 650, loss is 5.731347236633301 and perplexity is 308.3844554725333
At time: 421.57518458366394 and batch: 700, loss is 5.740010452270508 and perplexity is 311.0676623277538
At time: 422.4165198802948 and batch: 750, loss is 5.690546379089356 and perplexity is 296.05533490209814
At time: 423.2546887397766 and batch: 800, loss is 5.693450193405152 and perplexity is 296.9162740205602
At time: 424.10215282440186 and batch: 850, loss is 5.7412543296813965 and perplexity is 311.4548331127578
At time: 424.94211745262146 and batch: 900, loss is 5.735697317123413 and perplexity is 309.72887472165235
At time: 425.78405928611755 and batch: 950, loss is 5.700843772888184 and perplexity is 299.1196835968119
At time: 426.62766695022583 and batch: 1000, loss is 5.6926198387146 and perplexity is 296.6698305316395
At time: 427.4699544906616 and batch: 1050, loss is 5.688789377212524 and perplexity is 295.5356218251535
At time: 428.311372756958 and batch: 1100, loss is 5.677936277389526 and perplexity is 292.3454869600145
At time: 429.1568329334259 and batch: 1150, loss is 5.723734827041626 and perplexity is 306.0458193073733
At time: 429.99988317489624 and batch: 1200, loss is 5.734790391921997 and perplexity is 309.44810113912513
At time: 430.8400580883026 and batch: 1250, loss is 5.707334299087524 and perplexity is 301.0674418903007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.320513871464416 and perplexity of 204.48893602753435
Finished 19 epochs...
Completing Train Step...
At time: 433.32218384742737 and batch: 50, loss is 5.693184509277343 and perplexity is 296.8373985577082
At time: 434.16396474838257 and batch: 100, loss is 5.725391445159912 and perplexity is 306.5532405422227
At time: 435.00771141052246 and batch: 150, loss is 5.632742300033569 and perplexity is 279.4273415247779
At time: 435.8540394306183 and batch: 200, loss is 5.654821214675903 and perplexity is 285.665405378625
At time: 436.69908928871155 and batch: 250, loss is 5.69898567199707 and perplexity is 298.5644050876774
At time: 437.54935693740845 and batch: 300, loss is 5.705307540893554 and perplexity is 300.4578689226799
At time: 438.398309469223 and batch: 350, loss is 5.735283069610595 and perplexity is 309.6005968768746
At time: 439.24127173423767 and batch: 400, loss is 5.700129547119141 and perplexity is 298.9061208858334
At time: 440.0888292789459 and batch: 450, loss is 5.685399885177612 and perplexity is 294.5356019260927
At time: 440.9328017234802 and batch: 500, loss is 5.676292238235473 and perplexity is 291.865254401673
At time: 441.78008222579956 and batch: 550, loss is 5.693666601181031 and perplexity is 296.9805359641846
At time: 442.62363743782043 and batch: 600, loss is 5.7225370025634765 and perplexity is 305.6794496006782
At time: 443.46559166908264 and batch: 650, loss is 5.718957710266113 and perplexity is 304.5872892478317
At time: 444.306378364563 and batch: 700, loss is 5.727405185699463 and perplexity is 307.1711812074975
At time: 445.15195751190186 and batch: 750, loss is 5.678413000106811 and perplexity is 292.4848879201063
At time: 445.9945409297943 and batch: 800, loss is 5.6817385196685795 and perplexity is 293.4591712376321
At time: 446.8413345813751 and batch: 850, loss is 5.729273185729981 and perplexity is 307.7455132424894
At time: 447.68675446510315 and batch: 900, loss is 5.723467674255371 and perplexity is 305.9640692343898
At time: 448.534405708313 and batch: 950, loss is 5.688801603317261 and perplexity is 295.53923509670744
At time: 449.3782162666321 and batch: 1000, loss is 5.680533332824707 and perplexity is 293.10571114075475
At time: 450.21971321105957 and batch: 1050, loss is 5.676737937927246 and perplexity is 291.99536764916235
At time: 451.0640609264374 and batch: 1100, loss is 5.666042909622193 and perplexity is 288.88910929641526
At time: 451.9195373058319 and batch: 1150, loss is 5.711660985946655 and perplexity is 302.3728885277254
At time: 452.7666823863983 and batch: 1200, loss is 5.722996597290039 and perplexity is 305.8199705526039
At time: 453.61212825775146 and batch: 1250, loss is 5.695620517730713 and perplexity is 297.56137842239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.310405202155565 and perplexity of 202.4322377509328
Finished 20 epochs...
Completing Train Step...
At time: 456.0458390712738 and batch: 50, loss is 5.681397752761841 and perplexity is 293.35918710020553
At time: 456.9185848236084 and batch: 100, loss is 5.713379516601562 and perplexity is 302.8929723678853
At time: 457.75765681266785 and batch: 150, loss is 5.620543222427369 and perplexity is 276.03929324878277
At time: 458.597229719162 and batch: 200, loss is 5.643050575256348 and perplexity is 282.3226526599386
At time: 459.44008135795593 and batch: 250, loss is 5.686882514953613 and perplexity is 294.97261306240927
At time: 460.28479647636414 and batch: 300, loss is 5.693395509719848 and perplexity is 296.9000379883973
At time: 461.1268472671509 and batch: 350, loss is 5.723061170578003 and perplexity is 305.83971899123145
At time: 461.96970891952515 and batch: 400, loss is 5.688237886428833 and perplexity is 295.372681587614
At time: 462.81024837493896 and batch: 450, loss is 5.673396234512329 and perplexity is 291.02123427087815
At time: 463.6545195579529 and batch: 500, loss is 5.664280939102173 and perplexity is 288.38054337293437
At time: 464.4935677051544 and batch: 550, loss is 5.681602087020874 and perplexity is 293.4191365569875
At time: 465.3347735404968 and batch: 600, loss is 5.710511751174927 and perplexity is 302.0255906918003
At time: 466.1745777130127 and batch: 650, loss is 5.707120199203491 and perplexity is 301.00299028569253
At time: 467.0188205242157 and batch: 700, loss is 5.715343799591064 and perplexity is 303.4885246062226
At time: 467.8617789745331 and batch: 750, loss is 5.666791715621948 and perplexity is 289.10551220650757
At time: 468.7011079788208 and batch: 800, loss is 5.67056809425354 and perplexity is 290.1993481544941
At time: 469.54726815223694 and batch: 850, loss is 5.717847566604615 and perplexity is 304.2493412194388
At time: 470.3864734172821 and batch: 900, loss is 5.711800022125244 and perplexity is 302.41493222138473
At time: 471.2259500026703 and batch: 950, loss is 5.677327632904053 and perplexity is 292.1676066299555
At time: 472.06657814979553 and batch: 1000, loss is 5.669064903259278 and perplexity is 289.7634508083688
At time: 472.9051089286804 and batch: 1050, loss is 5.665249433517456 and perplexity is 288.6599736101378
At time: 473.7453384399414 and batch: 1100, loss is 5.654694700241089 and perplexity is 285.6292668673875
At time: 474.5852801799774 and batch: 1150, loss is 5.70007999420166 and perplexity is 298.8913095824654
At time: 475.4240734577179 and batch: 1200, loss is 5.7118075084686275 and perplexity is 302.41719621188616
At time: 476.3207383155823 and batch: 1250, loss is 5.684418087005615 and perplexity is 294.24656931959305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.300800852531935 and perplexity of 200.49731447794005
Finished 21 epochs...
Completing Train Step...
At time: 478.78833198547363 and batch: 50, loss is 5.670173854827881 and perplexity is 290.08496267926444
At time: 479.69123673439026 and batch: 100, loss is 5.70194257736206 and perplexity is 299.44853828378587
At time: 480.5349454879761 and batch: 150, loss is 5.608885707855225 and perplexity is 272.84004503734957
At time: 481.38156175613403 and batch: 200, loss is 5.631777114868164 and perplexity is 279.1577725128625
At time: 482.2215766906738 and batch: 250, loss is 5.675277442932129 and perplexity is 291.56922114428113
At time: 483.06695675849915 and batch: 300, loss is 5.682048063278199 and perplexity is 293.55002370944845
At time: 483.91201877593994 and batch: 350, loss is 5.711478748321533 and perplexity is 302.3177898312992
At time: 484.76056432724 and batch: 400, loss is 5.676859893798828 and perplexity is 292.03098037026797
At time: 485.6015224456787 and batch: 450, loss is 5.661938152313232 and perplexity is 287.7057200379856
At time: 486.44859766960144 and batch: 500, loss is 5.6527421760559085 and perplexity is 285.0721129211165
At time: 487.29350090026855 and batch: 550, loss is 5.670014209747315 and perplexity is 290.0386557384625
At time: 488.1334762573242 and batch: 600, loss is 5.699020109176636 and perplexity is 298.57468698074604
At time: 488.97557640075684 and batch: 650, loss is 5.695738525390625 and perplexity is 297.5964950163105
At time: 489.8149461746216 and batch: 700, loss is 5.703797054290772 and perplexity is 300.0043739222562
At time: 490.6570875644684 and batch: 750, loss is 5.655697011947632 and perplexity is 285.91569994893547
At time: 491.49622654914856 and batch: 800, loss is 5.6598606300354 and perplexity is 287.1086254485625
At time: 492.3392906188965 and batch: 850, loss is 5.7069141292572025 and perplexity is 300.9409690062321
At time: 493.18246841430664 and batch: 900, loss is 5.700708980560303 and perplexity is 299.0793672755714
At time: 494.0270199775696 and batch: 950, loss is 5.666422500610351 and perplexity is 288.9987898144324
At time: 494.86938309669495 and batch: 1000, loss is 5.6581581115722654 and perplexity is 286.62023357886864
At time: 495.7138750553131 and batch: 1050, loss is 5.654292850494385 and perplexity is 285.5145098779234
At time: 496.55663323402405 and batch: 1100, loss is 5.643844070434571 and perplexity is 282.54676322709554
At time: 497.45780849456787 and batch: 1150, loss is 5.688956317901611 and perplexity is 295.5849628639096
At time: 498.29878425598145 and batch: 1200, loss is 5.701104679107666 and perplexity is 299.1977359643633
At time: 499.14180040359497 and batch: 1250, loss is 5.673663988113403 and perplexity is 291.09916668722013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.291518608148951 and perplexity of 198.6448601742138
Finished 22 epochs...
Completing Train Step...
At time: 501.68153953552246 and batch: 50, loss is 5.659369926452637 and perplexity is 286.96777477820336
At time: 502.52777314186096 and batch: 100, loss is 5.690936212539673 and perplexity is 296.17076967349345
At time: 503.36783623695374 and batch: 150, loss is 5.597734756469727 and perplexity is 269.8145190560513
At time: 504.2086751461029 and batch: 200, loss is 5.620997371673584 and perplexity is 276.1646847568119
At time: 505.04892659187317 and batch: 250, loss is 5.664015054702759 and perplexity is 288.30387767791484
At time: 505.88843154907227 and batch: 300, loss is 5.671240005493164 and perplexity is 290.39440188030767
At time: 506.7315444946289 and batch: 350, loss is 5.700353145599365 and perplexity is 298.97296331284736
At time: 507.571649312973 and batch: 400, loss is 5.665941619873047 and perplexity is 288.859849272899
At time: 508.4123284816742 and batch: 450, loss is 5.650839052200317 and perplexity is 284.5301013036639
At time: 509.25916934013367 and batch: 500, loss is 5.641696100234985 and perplexity is 281.9405125369944
At time: 510.09914994239807 and batch: 550, loss is 5.658843202590942 and perplexity is 286.8166618045884
At time: 510.9399833679199 and batch: 600, loss is 5.688030548095703 and perplexity is 295.31144585663804
At time: 511.78381967544556 and batch: 650, loss is 5.684786863327027 and perplexity is 294.355100497649
At time: 512.637478351593 and batch: 700, loss is 5.69269585609436 and perplexity is 296.69238345200665
At time: 513.4881072044373 and batch: 750, loss is 5.645046138763428 and perplexity is 282.88660795996503
At time: 514.3348286151886 and batch: 800, loss is 5.649740695953369 and perplexity is 284.2177574532625
At time: 515.1890127658844 and batch: 850, loss is 5.6964354324340825 and perplexity is 297.8039643948343
At time: 516.0305650234222 and batch: 900, loss is 5.690003700256348 and perplexity is 295.8947155247577
At time: 516.8728637695312 and batch: 950, loss is 5.655930242538452 and perplexity is 285.98239201357177
At time: 517.7167744636536 and batch: 1000, loss is 5.6476610660552975 and perplexity is 283.62730388255176
At time: 518.5601124763489 and batch: 1050, loss is 5.643776483535767 and perplexity is 282.5276674129227
At time: 519.4583330154419 and batch: 1100, loss is 5.633436126708984 and perplexity is 279.62128294125205
At time: 520.2990760803223 and batch: 1150, loss is 5.678273601531982 and perplexity is 292.44411878521686
At time: 521.1386513710022 and batch: 1200, loss is 5.690849857330322 and perplexity is 296.14519488894865
At time: 521.9822325706482 and batch: 1250, loss is 5.663349542617798 and perplexity is 288.112071794825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.282757167398494 and perplexity of 196.91204705577354
Finished 23 epochs...
Completing Train Step...
At time: 524.4078254699707 and batch: 50, loss is 5.649050483703613 and perplexity is 284.0216545595649
At time: 525.2800471782684 and batch: 100, loss is 5.6804365539550785 and perplexity is 293.0773460739406
At time: 526.1204588413239 and batch: 150, loss is 5.58707088470459 and perplexity is 266.95253864207444
At time: 526.9615819454193 and batch: 200, loss is 5.610636930465699 and perplexity is 273.3182673078712
At time: 527.8009762763977 and batch: 250, loss is 5.653347787857055 and perplexity is 285.24480824476416
At time: 528.6427927017212 and batch: 300, loss is 5.66082389831543 and perplexity is 287.3853213251655
At time: 529.4793350696564 and batch: 350, loss is 5.689576749801636 and perplexity is 295.76841010641397
At time: 530.3274374008179 and batch: 400, loss is 5.655520315170288 and perplexity is 285.86518402929397
At time: 531.1715741157532 and batch: 450, loss is 5.640318069458008 and perplexity is 281.5522574086401
At time: 532.0170683860779 and batch: 500, loss is 5.631085777282715 and perplexity is 278.96484694848505
At time: 532.854864358902 and batch: 550, loss is 5.648085746765137 and perplexity is 283.74778050753304
At time: 533.6912188529968 and batch: 600, loss is 5.677489137649536 and perplexity is 292.214796895526
At time: 534.5338129997253 and batch: 650, loss is 5.674335832595825 and perplexity is 291.2948057683564
At time: 535.3750686645508 and batch: 700, loss is 5.6820038890838624 and perplexity is 293.5370566600603
At time: 536.2146601676941 and batch: 750, loss is 5.634826555252075 and perplexity is 280.01034677435734
At time: 537.0537068843842 and batch: 800, loss is 5.639926509857178 and perplexity is 281.4420344999437
At time: 537.8920834064484 and batch: 850, loss is 5.686364870071412 and perplexity is 294.8199615119235
At time: 538.7297947406769 and batch: 900, loss is 5.679685831069946 and perplexity is 292.85740876944243
At time: 539.5658347606659 and batch: 950, loss is 5.645855541229248 and perplexity is 283.11566976708554
At time: 540.4040625095367 and batch: 1000, loss is 5.637521104812622 and perplexity is 280.7658659660332
At time: 541.3034086227417 and batch: 1050, loss is 5.633676805496216 and perplexity is 279.6885899518744
At time: 542.1453359127045 and batch: 1100, loss is 5.623413963317871 and perplexity is 276.8328690655727
At time: 542.986426115036 and batch: 1150, loss is 5.66796983718872 and perplexity is 289.4463143592752
At time: 543.8258454799652 and batch: 1200, loss is 5.680991001129151 and perplexity is 293.23988703628055
At time: 544.666802406311 and batch: 1250, loss is 5.653416929244995 and perplexity is 285.26453114853535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.274343671589873 and perplexity of 195.26227826642486
Finished 24 epochs...
Completing Train Step...
At time: 547.151686668396 and batch: 50, loss is 5.639167556762695 and perplexity is 281.2285142331252
At time: 548.001856803894 and batch: 100, loss is 5.670384864807129 and perplexity is 290.1461799597211
At time: 548.8543663024902 and batch: 150, loss is 5.57681827545166 and perplexity is 264.2295612382941
At time: 549.6961615085602 and batch: 200, loss is 5.600634279251099 and perplexity is 270.59798769448196
At time: 550.5454275608063 and batch: 250, loss is 5.643062486648559 and perplexity is 282.3260155358128
At time: 551.3935556411743 and batch: 300, loss is 5.650817470550537 and perplexity is 284.52396074092763
At time: 552.2419033050537 and batch: 350, loss is 5.679248695373535 and perplexity is 292.7294183187463
At time: 553.0852909088135 and batch: 400, loss is 5.645534009933471 and perplexity is 283.02465385194733
At time: 553.9280004501343 and batch: 450, loss is 5.630121164321899 and perplexity is 278.695883585135
At time: 554.7738072872162 and batch: 500, loss is 5.620882453918457 and perplexity is 276.13295035465234
At time: 555.6172914505005 and batch: 550, loss is 5.637690668106079 and perplexity is 280.8134775874342
At time: 556.4694793224335 and batch: 600, loss is 5.667336196899414 and perplexity is 289.26296760698494
At time: 557.314050912857 and batch: 650, loss is 5.6642516040802 and perplexity is 288.3720838474389
At time: 558.1568946838379 and batch: 700, loss is 5.671700563430786 and perplexity is 290.5281761301663
At time: 559.0009245872498 and batch: 750, loss is 5.625074071884155 and perplexity is 277.2928233643386
At time: 559.8424112796783 and batch: 800, loss is 5.630355129241943 and perplexity is 278.76109627369783
At time: 560.6875929832458 and batch: 850, loss is 5.676622304916382 and perplexity is 291.96160529770145
At time: 561.5290265083313 and batch: 900, loss is 5.669705572128296 and perplexity is 289.9491527110432
At time: 562.4315011501312 and batch: 950, loss is 5.636118659973144 and perplexity is 280.37238330956154
At time: 563.2760193347931 and batch: 1000, loss is 5.627773113250733 and perplexity is 278.0422590893067
At time: 564.1218273639679 and batch: 1050, loss is 5.623999662399292 and perplexity is 276.99505731480946
At time: 564.9685380458832 and batch: 1100, loss is 5.613788318634033 and perplexity is 274.1809578841265
At time: 565.8144643306732 and batch: 1150, loss is 5.6580053901672365 and perplexity is 286.57646387645707
At time: 566.6607115268707 and batch: 1200, loss is 5.67148006439209 and perplexity is 290.464122008811
At time: 567.5027067661285 and batch: 1250, loss is 5.643846025466919 and perplexity is 282.5473156156976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.266271438041743 and perplexity of 193.69242020800576
Finished 25 epochs...
Completing Train Step...
At time: 569.9140391349792 and batch: 50, loss is 5.629644899368286 and perplexity is 278.5631821061075
At time: 570.7864725589752 and batch: 100, loss is 5.660661563873291 and perplexity is 287.3386725758012
At time: 571.6261281967163 and batch: 150, loss is 5.566902160644531 and perplexity is 261.62237849881177
At time: 572.4688222408295 and batch: 200, loss is 5.590959177017212 and perplexity is 267.9925487672889
At time: 573.3070220947266 and batch: 250, loss is 5.633174839019776 and perplexity is 279.54823088659566
At time: 574.149656534195 and batch: 300, loss is 5.641162271499634 and perplexity is 281.79004475532855
At time: 574.9916524887085 and batch: 350, loss is 5.669327983856201 and perplexity is 289.8396919783205
At time: 575.8335616588593 and batch: 400, loss is 5.635861787796021 and perplexity is 280.3003726942132
At time: 576.6763586997986 and batch: 450, loss is 5.620310220718384 and perplexity is 275.9749831141855
At time: 577.5182108879089 and batch: 500, loss is 5.611051349639893 and perplexity is 273.4315591120277
At time: 578.3587439060211 and batch: 550, loss is 5.627659206390381 and perplexity is 278.01058997222776
At time: 579.2004923820496 and batch: 600, loss is 5.657553005218506 and perplexity is 286.446850317361
At time: 580.0424077510834 and batch: 650, loss is 5.654471712112427 and perplexity is 285.5655820324225
At time: 580.8864583969116 and batch: 700, loss is 5.661740007400513 and perplexity is 287.6487182606924
At time: 581.724805355072 and batch: 750, loss is 5.6156394481658936 and perplexity is 274.68897240761606
At time: 582.5667705535889 and batch: 800, loss is 5.621053066253662 and perplexity is 276.18006606128534
At time: 583.4122302532196 and batch: 850, loss is 5.667204885482788 and perplexity is 289.2249865706576
At time: 584.3112306594849 and batch: 900, loss is 5.660071783065796 and perplexity is 287.16925570578474
At time: 585.15789103508 and batch: 950, loss is 5.626708059310913 and perplexity is 277.74628672707036
At time: 586.0053803920746 and batch: 1000, loss is 5.618333253860474 and perplexity is 275.4299286737628
At time: 586.8477642536163 and batch: 1050, loss is 5.614616985321045 and perplexity is 274.4082566745989
At time: 587.6916027069092 and batch: 1100, loss is 5.604430141448975 and perplexity is 271.62709230076894
At time: 588.5382335186005 and batch: 1150, loss is 5.648336038589478 and perplexity is 283.8188091457419
At time: 589.39439868927 and batch: 1200, loss is 5.662256422042847 and perplexity is 287.79730263292146
At time: 590.2376880645752 and batch: 1250, loss is 5.634539670944214 and perplexity is 279.93002772151794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.258477204037408 and perplexity of 192.18860431957629
Finished 26 epochs...
Completing Train Step...
At time: 592.7007811069489 and batch: 50, loss is 5.6203919506073 and perplexity is 275.9975394406495
At time: 593.5736422538757 and batch: 100, loss is 5.65127013206482 and perplexity is 284.652782942073
At time: 594.4174911975861 and batch: 150, loss is 5.55730037689209 and perplexity is 259.12235852849227
At time: 595.2599830627441 and batch: 200, loss is 5.581553220748901 and perplexity is 265.4836404118076
At time: 596.1023387908936 and batch: 250, loss is 5.623656253814698 and perplexity is 276.899951165306
At time: 596.9479525089264 and batch: 300, loss is 5.631874895095825 and perplexity is 279.1850699579655
At time: 597.7917191982269 and batch: 350, loss is 5.659734487533569 and perplexity is 287.07241113238155
At time: 598.6387794017792 and batch: 400, loss is 5.6265160274505615 and perplexity is 277.6929557117153
At time: 599.4821014404297 and batch: 450, loss is 5.610771522521973 and perplexity is 273.35505625117975
At time: 600.3249027729034 and batch: 500, loss is 5.601563415527344 and perplexity is 270.8495269401507
At time: 601.1703383922577 and batch: 550, loss is 5.6179359436035154 and perplexity is 275.32051927420497
At time: 602.0145542621613 and batch: 600, loss is 5.648072757720947 and perplexity is 283.7440949190095
At time: 602.8562862873077 and batch: 650, loss is 5.644993295669556 and perplexity is 282.87165975134366
At time: 603.700740814209 and batch: 700, loss is 5.652072772979737 and perplexity is 284.88134862802207
At time: 604.5481085777283 and batch: 750, loss is 5.606399431228637 and perplexity is 272.1625318022444
At time: 605.3955883979797 and batch: 800, loss is 5.612033576965332 and perplexity is 273.7002630035741
At time: 606.2960460186005 and batch: 850, loss is 5.658290405273437 and perplexity is 286.6581541386698
At time: 607.1391930580139 and batch: 900, loss is 5.65073881149292 and perplexity is 284.5015812344931
At time: 607.9828252792358 and batch: 950, loss is 5.617538938522339 and perplexity is 275.2112373232811
At time: 608.8270180225372 and batch: 1000, loss is 5.609283981323242 and perplexity is 272.948731630357
At time: 609.6701951026917 and batch: 1050, loss is 5.605584955215454 and perplexity is 271.9409521962903
At time: 610.5207366943359 and batch: 1100, loss is 5.595388040542603 and perplexity is 269.18208339047703
At time: 611.3653962612152 and batch: 1150, loss is 5.638920183181763 and perplexity is 281.1589543324947
At time: 612.2073197364807 and batch: 1200, loss is 5.653252153396607 and perplexity is 285.2175303158077
At time: 613.0503926277161 and batch: 1250, loss is 5.625575084686279 and perplexity is 277.4317854267605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.250999283616561 and perplexity of 190.75679338693516
Finished 27 epochs...
Completing Train Step...
At time: 615.5110473632812 and batch: 50, loss is 5.611481819152832 and perplexity is 273.54928839971745
At time: 616.3524749279022 and batch: 100, loss is 5.642208271026611 and perplexity is 282.08495121797125
At time: 617.1930069923401 and batch: 150, loss is 5.547992458343506 and perplexity is 256.721658812397
At time: 618.0349106788635 and batch: 200, loss is 5.572427206039428 and perplexity is 263.0718545395618
At time: 618.8773274421692 and batch: 250, loss is 5.6143933773040775 and perplexity is 274.3469036482542
At time: 619.7207868099213 and batch: 300, loss is 5.622805061340332 and perplexity is 276.66435629323763
At time: 620.5593082904816 and batch: 350, loss is 5.6504225730896 and perplexity is 284.4116251332381
At time: 621.4002845287323 and batch: 400, loss is 5.617452144622803 and perplexity is 275.1873517033758
At time: 622.2390713691711 and batch: 450, loss is 5.601497402191162 and perplexity is 270.83164784940993
At time: 623.0772686004639 and batch: 500, loss is 5.592368450164795 and perplexity is 268.370489718485
At time: 623.9194421768188 and batch: 550, loss is 5.6085167980194095 and perplexity is 272.7394102248584
At time: 624.759459733963 and batch: 600, loss is 5.638865833282471 and perplexity is 281.14367378689343
At time: 625.6039698123932 and batch: 650, loss is 5.635746574401855 and perplexity is 280.2680801971891
At time: 626.4447338581085 and batch: 700, loss is 5.642675132751465 and perplexity is 282.21667663123634
At time: 627.2874970436096 and batch: 750, loss is 5.597402868270874 and perplexity is 269.72498565965805
At time: 628.1880939006805 and batch: 800, loss is 5.603338718414307 and perplexity is 271.3307939582226
At time: 629.0335781574249 and batch: 850, loss is 5.649346628189087 and perplexity is 284.105778462118
At time: 629.8767290115356 and batch: 900, loss is 5.641587362289429 and perplexity is 281.9098565716556
At time: 630.7184088230133 and batch: 950, loss is 5.608662509918213 and perplexity is 272.77915449773724
At time: 631.5601074695587 and batch: 1000, loss is 5.600467987060547 and perplexity is 270.55299310357753
At time: 632.4010193347931 and batch: 1050, loss is 5.596742000579834 and perplexity is 269.54679201886483
At time: 633.2416763305664 and batch: 1100, loss is 5.5865458965301515 and perplexity is 266.81242849745354
At time: 634.0797407627106 and batch: 1150, loss is 5.629718494415283 and perplexity is 278.583683730986
At time: 634.9218122959137 and batch: 1200, loss is 5.644523200988769 and perplexity is 282.73871453961596
At time: 635.7622129917145 and batch: 1250, loss is 5.616792621612549 and perplexity is 275.0059191488454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.243613584198221 and perplexity of 189.35311101773323
Finished 28 epochs...
Completing Train Step...
At time: 638.1642625331879 and batch: 50, loss is 5.602745265960693 and perplexity is 271.16981980278536
At time: 639.0352239608765 and batch: 100, loss is 5.63340726852417 and perplexity is 279.6132136950234
At time: 639.8789184093475 and batch: 150, loss is 5.538938674926758 and perplexity is 254.40784669743422
At time: 640.7253952026367 and batch: 200, loss is 5.563578023910522 and perplexity is 260.7541537883136
At time: 641.5705902576447 and batch: 250, loss is 5.6054143142700195 and perplexity is 271.89455189411353
At time: 642.4115929603577 and batch: 300, loss is 5.613966159820556 and perplexity is 274.229722887083
At time: 643.2562689781189 and batch: 350, loss is 5.641366558074951 and perplexity is 281.8476165589032
At time: 644.0985000133514 and batch: 400, loss is 5.608656396865845 and perplexity is 272.7774869895776
At time: 644.9411051273346 and batch: 450, loss is 5.592450370788574 and perplexity is 268.39247569694703
At time: 645.7824468612671 and batch: 500, loss is 5.583497486114502 and perplexity is 266.0003131707099
At time: 646.6250185966492 and batch: 550, loss is 5.599377708435059 and perplexity is 270.25817570389376
At time: 647.4729783535004 and batch: 600, loss is 5.629871864318847 and perplexity is 278.6264133603301
At time: 648.3135502338409 and batch: 650, loss is 5.62675742149353 and perplexity is 277.75999722838435
At time: 649.1845464706421 and batch: 700, loss is 5.633481998443603 and perplexity is 279.63410994873317
At time: 650.0309674739838 and batch: 750, loss is 5.588638916015625 and perplexity is 267.3714569338909
At time: 650.8797011375427 and batch: 800, loss is 5.594804172515869 and perplexity is 269.0249624520255
At time: 651.7231550216675 and batch: 850, loss is 5.640703144073487 and perplexity is 281.6606969132092
At time: 652.563481092453 and batch: 900, loss is 5.632708683013916 and perplexity is 279.41794816823534
At time: 653.4092974662781 and batch: 950, loss is 5.599990615844726 and perplexity is 270.42386971466243
At time: 654.2538244724274 and batch: 1000, loss is 5.591889209747315 and perplexity is 268.24190654654046
At time: 655.0967881679535 and batch: 1050, loss is 5.588176937103271 and perplexity is 267.2479654864409
At time: 655.9384694099426 and batch: 1100, loss is 5.577951526641845 and perplexity is 264.5291694366587
At time: 656.7807364463806 and batch: 1150, loss is 5.620704545974731 and perplexity is 276.0838284789752
At time: 657.6235039234161 and batch: 1200, loss is 5.635921907424927 and perplexity is 280.31722475516665
At time: 658.4654259681702 and batch: 1250, loss is 5.608274354934692 and perplexity is 272.67329445589877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.236470688868613 and perplexity of 188.00540057283703
Finished 29 epochs...
Completing Train Step...
At time: 660.9338109493256 and batch: 50, loss is 5.594270715713501 and perplexity is 268.8814875280381
At time: 661.7849977016449 and batch: 100, loss is 5.624893636703491 and perplexity is 277.2427944972775
At time: 662.6304414272308 and batch: 150, loss is 5.5301479244232175 and perplexity is 252.18121202491523
At time: 663.4759130477905 and batch: 200, loss is 5.554939651489258 and perplexity is 258.5113632740391
At time: 664.317453622818 and batch: 250, loss is 5.596620960235596 and perplexity is 269.5141679568242
At time: 665.1637427806854 and batch: 300, loss is 5.605385942459106 and perplexity is 271.88683786273
At time: 666.0067083835602 and batch: 350, loss is 5.632570600509643 and perplexity is 279.37936810189103
At time: 666.8471231460571 and batch: 400, loss is 5.6000815200805665 and perplexity is 270.44845350726024
At time: 667.687068939209 and batch: 450, loss is 5.583629016876221 and perplexity is 266.0353026955676
At time: 668.5259642601013 and batch: 500, loss is 5.574815454483033 and perplexity is 263.7008863298334
At time: 669.367782831192 and batch: 550, loss is 5.590482892990113 and perplexity is 267.8649385886866
At time: 670.205634355545 and batch: 600, loss is 5.621064767837525 and perplexity is 276.1832978243979
At time: 671.1064488887787 and batch: 650, loss is 5.617919340133667 and perplexity is 275.31594803621374
At time: 671.9543759822845 and batch: 700, loss is 5.6245361328125 and perplexity is 277.1436968344509
At time: 672.7989165782928 and batch: 750, loss is 5.580080184936524 and perplexity is 265.09286138828924
At time: 673.6428821086884 and batch: 800, loss is 5.586596698760986 and perplexity is 266.8259835083451
At time: 674.4821753501892 and batch: 850, loss is 5.632125835418702 and perplexity is 279.2551375405863
At time: 675.3256404399872 and batch: 900, loss is 5.624077396392822 and perplexity is 277.0165900837058
At time: 676.1690099239349 and batch: 950, loss is 5.591559400558472 and perplexity is 268.1534524882111
At time: 677.012363910675 and batch: 1000, loss is 5.583526630401611 and perplexity is 266.00806567317807
At time: 677.8555569648743 and batch: 1050, loss is 5.579819116592407 and perplexity is 265.0236630670681
At time: 678.6979382038116 and batch: 1100, loss is 5.56953143119812 and perplexity is 262.3111596139116
At time: 679.5364828109741 and batch: 1150, loss is 5.611870708465577 and perplexity is 273.6556894822659
At time: 680.3776075839996 and batch: 1200, loss is 5.627444324493408 and perplexity is 277.9508569472787
At time: 681.2192344665527 and batch: 1250, loss is 5.5999036693573 and perplexity is 270.40035833120527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.229401303033759 and perplexity of 186.68100470552994
Finished 30 epochs...
Completing Train Step...
At time: 683.6497786045074 and batch: 50, loss is 5.5859403228759765 and perplexity is 266.6509028329287
At time: 684.5183744430542 and batch: 100, loss is 5.616497373580932 and perplexity is 274.9247361776797
At time: 685.3568160533905 and batch: 150, loss is 5.52163631439209 and perplexity is 250.0438529781786
At time: 686.1953401565552 and batch: 200, loss is 5.546486692428589 and perplexity is 256.33538697940014
At time: 687.0331597328186 and batch: 250, loss is 5.588066215515137 and perplexity is 267.21837700534854
At time: 687.8729383945465 and batch: 300, loss is 5.597035579681396 and perplexity is 269.6259369409739
At time: 688.7119109630585 and batch: 350, loss is 5.624007577896118 and perplexity is 276.99724987698403
At time: 689.5494611263275 and batch: 400, loss is 5.591735944747925 and perplexity is 268.20079760125907
At time: 690.3894407749176 and batch: 450, loss is 5.575038223266602 and perplexity is 263.75963719919406
At time: 691.2266635894775 and batch: 500, loss is 5.566371831893921 and perplexity is 261.48366941364975
At time: 692.0622224807739 and batch: 550, loss is 5.581821908950806 and perplexity is 265.5549823177199
At time: 692.9570982456207 and batch: 600, loss is 5.612450532913208 and perplexity is 273.8144077511807
At time: 693.7962114810944 and batch: 650, loss is 5.609301013946533 and perplexity is 272.9533807028735
At time: 694.6367130279541 and batch: 700, loss is 5.615870351791382 and perplexity is 274.75240641051545
At time: 695.4782152175903 and batch: 750, loss is 5.571964054107666 and perplexity is 262.95004051331784
At time: 696.3198111057281 and batch: 800, loss is 5.578485078811646 and perplexity is 264.6703472084912
At time: 697.1574213504791 and batch: 850, loss is 5.6238006973266605 and perplexity is 276.9399504554641
At time: 697.9969182014465 and batch: 900, loss is 5.615663938522339 and perplexity is 274.69569972083866
At time: 698.8322157859802 and batch: 950, loss is 5.5834215641021725 and perplexity is 265.98011865826743
At time: 699.6705360412598 and batch: 1000, loss is 5.575344505310059 and perplexity is 263.84043441259314
At time: 700.5129137039185 and batch: 1050, loss is 5.571660242080688 and perplexity is 262.87016526266086
At time: 701.3568031787872 and batch: 1100, loss is 5.561302452087403 and perplexity is 260.1614635935021
At time: 702.1987874507904 and batch: 1150, loss is 5.603237781524658 and perplexity is 271.3034080539619
At time: 703.0365800857544 and batch: 1200, loss is 5.6191400623321535 and perplexity is 275.6522375418508
At time: 703.8764450550079 and batch: 1250, loss is 5.591781568527222 and perplexity is 268.21303421439416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.222637538492244 and perplexity of 185.4225989230457
Finished 31 epochs...
Completing Train Step...
At time: 706.3310663700104 and batch: 50, loss is 5.577876539230346 and perplexity is 264.50933382269665
At time: 707.2096571922302 and batch: 100, loss is 5.608428907394409 and perplexity is 272.7154400410242
At time: 708.0534725189209 and batch: 150, loss is 5.513346195220947 and perplexity is 247.97952821087372
At time: 708.8969144821167 and batch: 200, loss is 5.538251686096191 and perplexity is 254.2331313689556
At time: 709.7473661899567 and batch: 250, loss is 5.579712867736816 and perplexity is 264.9955061020119
At time: 710.589433670044 and batch: 300, loss is 5.588877477645874 and perplexity is 267.43524911341984
At time: 711.4322710037231 and batch: 350, loss is 5.615709381103516 and perplexity is 274.7081828861037
At time: 712.2749402523041 and batch: 400, loss is 5.583602428436279 and perplexity is 266.028229325935
At time: 713.1186258792877 and batch: 450, loss is 5.56669282913208 and perplexity is 261.56761842233436
At time: 713.9679727554321 and batch: 500, loss is 5.55812216758728 and perplexity is 259.3353903934599
At time: 714.8439626693726 and batch: 550, loss is 5.573411169052124 and perplexity is 263.3308349067662
At time: 715.6866881847382 and batch: 600, loss is 5.604055414199829 and perplexity is 271.52532529630315
At time: 716.5281076431274 and batch: 650, loss is 5.600927791595459 and perplexity is 270.67742320130986
At time: 717.371298789978 and batch: 700, loss is 5.607430534362793 and perplexity is 272.4433041695436
At time: 718.2153189182281 and batch: 750, loss is 5.563927268981933 and perplexity is 260.845236795594
At time: 719.0585827827454 and batch: 800, loss is 5.570529975891113 and perplexity is 262.57321984791
At time: 719.905473947525 and batch: 850, loss is 5.615760498046875 and perplexity is 274.7222254876329
At time: 720.7490789890289 and batch: 900, loss is 5.607465839385986 and perplexity is 272.4529229565108
At time: 721.5941700935364 and batch: 950, loss is 5.575450048446656 and perplexity is 263.86828242915976
At time: 722.4356017112732 and batch: 1000, loss is 5.567356567382813 and perplexity is 261.74128848519035
At time: 723.2813308238983 and batch: 1050, loss is 5.563746566772461 and perplexity is 260.79810574344566
At time: 724.1282033920288 and batch: 1100, loss is 5.553274669647217 and perplexity is 258.08130466746144
At time: 724.9765655994415 and batch: 1150, loss is 5.594822044372559 and perplexity is 269.02977047056436
At time: 725.8251461982727 and batch: 1200, loss is 5.611015796661377 and perplexity is 273.42183797848963
At time: 726.672208070755 and batch: 1250, loss is 5.583813791275024 and perplexity is 266.0844637503961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.216101430628422 and perplexity of 184.21460889317376
Finished 32 epochs...
Completing Train Step...
At time: 729.1291840076447 and batch: 50, loss is 5.570047187805176 and perplexity is 262.44648322160947
At time: 729.9731059074402 and batch: 100, loss is 5.600541086196899 and perplexity is 270.57277101657627
At time: 730.8163249492645 and batch: 150, loss is 5.505257549285889 and perplexity is 245.98179995825578
At time: 731.6563704013824 and batch: 200, loss is 5.530222024917602 and perplexity is 252.19989946976676
At time: 732.4987516403198 and batch: 250, loss is 5.5715716934204105 and perplexity is 262.8468894922342
At time: 733.3408098220825 and batch: 300, loss is 5.580918769836426 and perplexity is 265.31525749489697
At time: 734.1827855110168 and batch: 350, loss is 5.607651491165161 and perplexity is 272.50350902195026
At time: 735.0201241970062 and batch: 400, loss is 5.5757080173492435 and perplexity is 263.93636102110804
At time: 735.9098052978516 and batch: 450, loss is 5.558556118011475 and perplexity is 259.44795351777117
At time: 736.7623851299286 and batch: 500, loss is 5.550122423171997 and perplexity is 257.2690496710695
At time: 737.6040394306183 and batch: 550, loss is 5.565204715728759 and perplexity is 261.17866561820335
At time: 738.4532742500305 and batch: 600, loss is 5.595870027542114 and perplexity is 269.3118569272391
At time: 739.2935781478882 and batch: 650, loss is 5.592760877609253 and perplexity is 268.47582633107027
At time: 740.1344995498657 and batch: 700, loss is 5.599192638397216 and perplexity is 270.20816364109896
At time: 740.9762141704559 and batch: 750, loss is 5.556057643890381 and perplexity is 258.8005386315894
At time: 741.8141305446625 and batch: 800, loss is 5.56270923614502 and perplexity is 260.5277121488045
At time: 742.6569573879242 and batch: 850, loss is 5.607919673919678 and perplexity is 272.57659956398885
At time: 743.4992332458496 and batch: 900, loss is 5.599578609466553 and perplexity is 270.3124763045031
At time: 744.341076374054 and batch: 950, loss is 5.567668390274048 and perplexity is 261.82291813685697
At time: 745.18403673172 and batch: 1000, loss is 5.559597539901733 and perplexity is 259.71828903817874
At time: 746.0226290225983 and batch: 1050, loss is 5.556065311431885 and perplexity is 258.8025230030682
At time: 746.8637137413025 and batch: 1100, loss is 5.545450010299683 and perplexity is 256.069786360207
At time: 747.7046887874603 and batch: 1150, loss is 5.586625061035156 and perplexity is 266.8335514073659
At time: 748.5496180057526 and batch: 1200, loss is 5.60308352470398 and perplexity is 271.26156088048515
At time: 749.3940072059631 and batch: 1250, loss is 5.576004104614258 and perplexity is 264.01452078686526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.209716351362911 and perplexity of 183.04213117851404
Finished 33 epochs...
Completing Train Step...
At time: 751.8329319953918 and batch: 50, loss is 5.562382459640503 and perplexity is 260.4425917221339
At time: 752.7353758811951 and batch: 100, loss is 5.592838592529297 and perplexity is 268.49669171921283
At time: 753.5802364349365 and batch: 150, loss is 5.49740701675415 and perplexity is 244.05827207363126
At time: 754.4268147945404 and batch: 200, loss is 5.522437353134155 and perplexity is 250.24422803500656
At time: 755.2756719589233 and batch: 250, loss is 5.563709373474121 and perplexity is 260.7884059820764
At time: 756.1196591854095 and batch: 300, loss is 5.57317289352417 and perplexity is 263.26809708779234
At time: 756.9631745815277 and batch: 350, loss is 5.599836740493775 and perplexity is 270.38226134813743
At time: 757.8347380161285 and batch: 400, loss is 5.568054552078247 and perplexity is 261.92404367145815
At time: 758.6798820495605 and batch: 450, loss is 5.550698652267456 and perplexity is 257.41733830285995
At time: 759.5245418548584 and batch: 500, loss is 5.542385549545288 and perplexity is 255.2862716876623
At time: 760.3674957752228 and batch: 550, loss is 5.557241888046264 and perplexity is 259.1072032040274
At time: 761.212760925293 and batch: 600, loss is 5.587911758422852 and perplexity is 267.1771064191806
At time: 762.0547244548798 and batch: 650, loss is 5.584835643768311 and perplexity is 266.35650179080767
At time: 762.9043064117432 and batch: 700, loss is 5.591136827468872 and perplexity is 268.04016199374917
At time: 763.7471942901611 and batch: 750, loss is 5.548448324203491 and perplexity is 256.8387161313588
At time: 764.5902984142303 and batch: 800, loss is 5.5551894092559815 and perplexity is 258.57593655830703
At time: 765.430881023407 and batch: 850, loss is 5.600306262969971 and perplexity is 270.50924170473814
At time: 766.2753763198853 and batch: 900, loss is 5.591930418014527 and perplexity is 268.2529605584596
At time: 767.1193678379059 and batch: 950, loss is 5.560131273269653 and perplexity is 259.85694635502426
At time: 767.9648001194 and batch: 1000, loss is 5.552107095718384 and perplexity is 257.7801515081129
At time: 768.8135786056519 and batch: 1050, loss is 5.548599033355713 and perplexity is 256.8774269934921
At time: 769.6572701931 and batch: 1100, loss is 5.537814397811889 and perplexity is 254.1219825029462
At time: 770.4990479946136 and batch: 1150, loss is 5.578638353347778 and perplexity is 264.71091754230775
At time: 771.3414754867554 and batch: 1200, loss is 5.595307788848877 and perplexity is 269.1604819391524
At time: 772.187185049057 and batch: 1250, loss is 5.568453941345215 and perplexity is 262.0286742160288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.203566502480611 and perplexity of 181.91990403329316
Finished 34 epochs...
Completing Train Step...
At time: 774.6516590118408 and batch: 50, loss is 5.554969482421875 and perplexity is 258.5190750241213
At time: 775.4961559772491 and batch: 100, loss is 5.585313587188721 and perplexity is 266.4838355550656
At time: 776.3373990058899 and batch: 150, loss is 5.489770870208741 and perplexity is 242.20170487353752
At time: 777.1796028614044 and batch: 200, loss is 5.51485951423645 and perplexity is 248.3550844428915
At time: 778.0193445682526 and batch: 250, loss is 5.555988664627075 and perplexity is 258.7826873767817
At time: 778.86434674263 and batch: 300, loss is 5.5655888748168945 and perplexity is 261.27901905083496
At time: 779.7341167926788 and batch: 350, loss is 5.592226228713989 and perplexity is 268.3323243921122
At time: 780.5770719051361 and batch: 400, loss is 5.560600357055664 and perplexity is 259.97886962912156
At time: 781.4208011627197 and batch: 450, loss is 5.543062429428101 and perplexity is 255.45912832431628
At time: 782.2658824920654 and batch: 500, loss is 5.53484540939331 and perplexity is 253.36861620130432
At time: 783.1042704582214 and batch: 550, loss is 5.549538307189941 and perplexity is 257.1188185879281
At time: 783.9478445053101 and batch: 600, loss is 5.5801939868927 and perplexity is 265.12303119114245
At time: 784.7860441207886 and batch: 650, loss is 5.577113389968872 and perplexity is 264.3075507250409
At time: 785.6259377002716 and batch: 700, loss is 5.583292675018311 and perplexity is 265.94583893363676
At time: 786.4683156013489 and batch: 750, loss is 5.541000890731811 and perplexity is 254.93303191633004
At time: 787.3115563392639 and batch: 800, loss is 5.547916946411132 and perplexity is 256.7022739957596
At time: 788.1538310050964 and batch: 850, loss is 5.592871112823486 and perplexity is 268.50542345259487
At time: 788.9954569339752 and batch: 900, loss is 5.584414892196655 and perplexity is 266.244455447549
At time: 789.8371670246124 and batch: 950, loss is 5.552789173126221 and perplexity is 257.9560375027581
At time: 790.6746809482574 and batch: 1000, loss is 5.544753999710083 and perplexity is 255.89162108687455
At time: 791.5170106887817 and batch: 1050, loss is 5.541330614089966 and perplexity is 255.01710315108124
At time: 792.3580749034882 and batch: 1100, loss is 5.530358047485351 and perplexity is 252.23420668090324
At time: 793.1998207569122 and batch: 1150, loss is 5.570874195098877 and perplexity is 262.6636181511497
At time: 794.0467457771301 and batch: 1200, loss is 5.587732210159301 and perplexity is 267.1291395399764
At time: 794.8895528316498 and batch: 1250, loss is 5.561060543060303 and perplexity is 260.0985357986631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.197577928974681 and perplexity of 180.83371891167292
Finished 35 epochs...
Completing Train Step...
At time: 797.3039147853851 and batch: 50, loss is 5.547734985351562 and perplexity is 256.65556842741546
At time: 798.2143442630768 and batch: 100, loss is 5.577981729507446 and perplexity is 264.5371590962653
At time: 799.0575349330902 and batch: 150, loss is 5.482316455841064 and perplexity is 240.40294568252182
At time: 799.9033553600311 and batch: 200, loss is 5.507493724822998 and perplexity is 246.53247391424165
At time: 800.7446398735046 and batch: 250, loss is 5.548562526702881 and perplexity is 256.86804942961714
At time: 801.6161813735962 and batch: 300, loss is 5.558168306350708 and perplexity is 259.3473560837246
At time: 802.458315372467 and batch: 350, loss is 5.58477933883667 and perplexity is 266.3415050283821
At time: 803.302549123764 and batch: 400, loss is 5.553280754089355 and perplexity is 258.08287495300385
At time: 804.1464145183563 and batch: 450, loss is 5.535735654830932 and perplexity is 253.59427688770876
At time: 804.9882230758667 and batch: 500, loss is 5.5275076961517335 and perplexity is 251.5162742394226
At time: 805.8324553966522 and batch: 550, loss is 5.542025461196899 and perplexity is 255.19436262440846
At time: 806.6733431816101 and batch: 600, loss is 5.5726645565032955 and perplexity is 263.1343021769618
At time: 807.5163419246674 and batch: 650, loss is 5.569620885848999 and perplexity is 262.3346256166724
At time: 808.3593311309814 and batch: 700, loss is 5.575664253234863 and perplexity is 263.9248103327698
At time: 809.2017061710358 and batch: 750, loss is 5.533682460784912 and perplexity is 253.07413278940783
At time: 810.0474953651428 and batch: 800, loss is 5.540789451599121 and perplexity is 254.87913479534856
At time: 810.8901116847992 and batch: 850, loss is 5.585627813339233 and perplexity is 266.56758490231414
At time: 811.7392425537109 and batch: 900, loss is 5.577126340866089 and perplexity is 264.31097376712967
At time: 812.6136341094971 and batch: 950, loss is 5.545645360946655 and perplexity is 256.11981464501235
At time: 813.4534523487091 and batch: 1000, loss is 5.537592716217041 and perplexity is 254.06565458024156
At time: 814.2945492267609 and batch: 1050, loss is 5.534280071258545 and perplexity is 253.22541774200187
At time: 815.1370470523834 and batch: 1100, loss is 5.523102474212647 and perplexity is 250.41072611039525
At time: 815.9779176712036 and batch: 1150, loss is 5.5633250331878665 and perplexity is 260.68819375050185
At time: 816.8199219703674 and batch: 1200, loss is 5.58035795211792 and perplexity is 265.16650571272004
At time: 817.662278175354 and batch: 1250, loss is 5.553830671310425 and perplexity is 258.22483820084335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.191765332744069 and perplexity of 179.7856544573555
Finished 36 epochs...
Completing Train Step...
At time: 820.0860662460327 and batch: 50, loss is 5.540726556777954 and perplexity is 254.86310472185596
At time: 820.9558930397034 and batch: 100, loss is 5.570847787857056 and perplexity is 262.6566820210499
At time: 821.8003244400024 and batch: 150, loss is 5.475019054412842 and perplexity is 238.6550143185051
At time: 822.6724565029144 and batch: 200, loss is 5.500348987579346 and perplexity is 244.7773416118807
At time: 823.5139892101288 and batch: 250, loss is 5.541238069534302 and perplexity is 254.99350379859604
At time: 824.3574590682983 and batch: 300, loss is 5.551083793640137 and perplexity is 257.516499464193
At time: 825.196263551712 and batch: 350, loss is 5.577524175643921 and perplexity is 264.4161467840117
At time: 826.0392756462097 and batch: 400, loss is 5.546195373535157 and perplexity is 256.2607225142305
At time: 826.8809340000153 and batch: 450, loss is 5.528513059616089 and perplexity is 251.76926666559532
At time: 827.7221684455872 and batch: 500, loss is 5.520298013687134 and perplexity is 249.70944293397886
At time: 828.5662007331848 and batch: 550, loss is 5.5347019290924075 and perplexity is 253.33226540388654
At time: 829.4084777832031 and batch: 600, loss is 5.565317258834839 and perplexity is 261.20806113057387
At time: 830.2520051002502 and batch: 650, loss is 5.562286005020142 and perplexity is 260.41747204229756
At time: 831.0933065414429 and batch: 700, loss is 5.568210401535034 and perplexity is 261.9648675724926
At time: 831.9336588382721 and batch: 750, loss is 5.526553955078125 and perplexity is 251.27650719401387
At time: 832.7709853649139 and batch: 800, loss is 5.533796195983887 and perplexity is 253.10291786316293
At time: 833.6102662086487 and batch: 850, loss is 5.5785641098022465 and perplexity is 264.69126519478743
At time: 834.4491114616394 and batch: 900, loss is 5.570033941268921 and perplexity is 262.4430067377803
At time: 835.2894620895386 and batch: 950, loss is 5.538687963485717 and perplexity is 254.34407173446718
At time: 836.1367461681366 and batch: 1000, loss is 5.530605697631836 and perplexity is 252.29668025461183
At time: 836.9807806015015 and batch: 1050, loss is 5.527426061630249 and perplexity is 251.49574266678334
At time: 837.8194065093994 and batch: 1100, loss is 5.516013431549072 and perplexity is 248.64183108364682
At time: 838.6622288227081 and batch: 1150, loss is 5.555955934524536 and perplexity is 258.7742175314987
At time: 839.5041360855103 and batch: 1200, loss is 5.573172969818115 and perplexity is 263.2681171735549
At time: 840.3452260494232 and batch: 1250, loss is 5.54685417175293 and perplexity is 256.4296022442327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.186152325929516 and perplexity of 178.779343211723
Finished 37 epochs...
Completing Train Step...
At time: 842.8373692035675 and batch: 50, loss is 5.533867807388305 and perplexity is 253.12104356756944
At time: 843.6733598709106 and batch: 100, loss is 5.563883657455444 and perplexity is 260.83386118469576
At time: 844.5737302303314 and batch: 150, loss is 5.467884550094604 and perplexity is 236.9583885767986
At time: 845.4137976169586 and batch: 200, loss is 5.493374824523926 and perplexity is 243.0761635610508
At time: 846.2519118785858 and batch: 250, loss is 5.534098291397095 and perplexity is 253.17939064417567
At time: 847.094779253006 and batch: 300, loss is 5.544050893783569 and perplexity is 255.7117654077496
At time: 847.9341576099396 and batch: 350, loss is 5.570440921783447 and perplexity is 262.5498376652747
At time: 848.7777895927429 and batch: 400, loss is 5.539260177612305 and perplexity is 254.48965265308425
At time: 849.6156647205353 and batch: 450, loss is 5.521413850784302 and perplexity is 249.98823350742316
At time: 850.4538218975067 and batch: 500, loss is 5.513281345367432 and perplexity is 247.96344729622248
At time: 851.2919306755066 and batch: 550, loss is 5.527574434280395 and perplexity is 251.5330605250297
At time: 852.1306114196777 and batch: 600, loss is 5.558138284683228 and perplexity is 259.3395701605122
At time: 852.9657919406891 and batch: 650, loss is 5.555133504867554 and perplexity is 258.56148143276783
At time: 853.804591178894 and batch: 700, loss is 5.560912866592407 and perplexity is 260.0601282016103
At time: 854.6416721343994 and batch: 750, loss is 5.519586906433106 and perplexity is 249.53193585847384
At time: 855.4851448535919 and batch: 800, loss is 5.527031469345093 and perplexity is 251.39652396373222
At time: 856.3266749382019 and batch: 850, loss is 5.571672677993774 and perplexity is 262.87343431351576
At time: 857.1648089885712 and batch: 900, loss is 5.5631325054168705 and perplexity is 260.6380088647812
At time: 858.0030596256256 and batch: 950, loss is 5.531913480758667 and perplexity is 252.62684544117752
At time: 858.8470747470856 and batch: 1000, loss is 5.5238145732879635 and perplexity is 250.5891068617541
At time: 859.6845276355743 and batch: 1050, loss is 5.520763788223267 and perplexity is 249.82577832485765
At time: 860.5268657207489 and batch: 1100, loss is 5.509101514816284 and perplexity is 246.92916517148433
At time: 861.3640878200531 and batch: 1150, loss is 5.5487559795379635 and perplexity is 256.9177460888463
At time: 862.2069461345673 and batch: 1200, loss is 5.566166095733642 and perplexity is 261.4298783011072
At time: 863.0424036979675 and batch: 1250, loss is 5.5399882602691655 and perplexity is 254.67500962494148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.180644459968065 and perplexity of 177.79735735837184
Finished 38 epochs...
Completing Train Step...
At time: 865.4719324111938 and batch: 50, loss is 5.527165822982788 and perplexity is 251.43030227029905
At time: 866.3467044830322 and batch: 100, loss is 5.557025508880615 and perplexity is 259.0511438688393
At time: 867.1933124065399 and batch: 150, loss is 5.460993242263794 and perplexity is 235.3310490669846
At time: 868.0382144451141 and batch: 200, loss is 5.486525201797486 and perplexity is 241.41687279220008
At time: 868.8805267810822 and batch: 250, loss is 5.527138738632202 and perplexity is 251.42349253606358
At time: 869.7255504131317 and batch: 300, loss is 5.537278327941895 and perplexity is 253.98579187193144
At time: 870.5679230690002 and batch: 350, loss is 5.56351821899414 and perplexity is 260.7385598742511
At time: 871.4150865077972 and batch: 400, loss is 5.532480354309082 and perplexity is 252.7700935159193
At time: 872.2600092887878 and batch: 450, loss is 5.514423341751098 and perplexity is 248.24678240936083
At time: 873.103592634201 and batch: 500, loss is 5.50642873764038 and perplexity is 246.27005974808813
At time: 873.9489259719849 and batch: 550, loss is 5.520607013702392 and perplexity is 249.78661507813848
At time: 874.7930481433868 and batch: 600, loss is 5.551111640930176 and perplexity is 257.5236707006927
At time: 875.6361985206604 and batch: 650, loss is 5.548129272460938 and perplexity is 256.75678436235376
At time: 876.4829804897308 and batch: 700, loss is 5.553800277709961 and perplexity is 258.2169899375503
At time: 877.329847574234 and batch: 750, loss is 5.512789392471314 and perplexity is 247.84149096103954
At time: 878.1749823093414 and batch: 800, loss is 5.520435190200805 and perplexity is 249.7436995543396
At time: 879.0189356803894 and batch: 850, loss is 5.564953804016113 and perplexity is 261.1131410526907
At time: 879.8619616031647 and batch: 900, loss is 5.556388683319092 and perplexity is 258.8862259962137
At time: 880.7045657634735 and batch: 950, loss is 5.525332593917847 and perplexity is 250.96979516884173
At time: 881.5475106239319 and batch: 1000, loss is 5.517199115753174 and perplexity is 248.9368166205287
At time: 882.3964152336121 and batch: 1050, loss is 5.5142255306243895 and perplexity is 248.19768129016455
At time: 883.2366571426392 and batch: 1100, loss is 5.5023501586914065 and perplexity is 245.2676734120829
At time: 884.0886330604553 and batch: 1150, loss is 5.541735963821411 and perplexity is 255.1204952189153
At time: 884.9365866184235 and batch: 1200, loss is 5.559323043823242 and perplexity is 259.6470071700695
At time: 885.7835674285889 and batch: 1250, loss is 5.533293151855469 and perplexity is 252.97562794535366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.175295641822537 and perplexity of 176.84889047683015
Finished 39 epochs...
Completing Train Step...
At time: 888.2399575710297 and batch: 50, loss is 5.5206138610839846 and perplexity is 249.78832546826445
At time: 889.0785892009735 and batch: 100, loss is 5.55038628578186 and perplexity is 257.3369423107233
At time: 889.9195261001587 and batch: 150, loss is 5.454260301589966 and perplexity is 233.75190119492999
At time: 890.7610635757446 and batch: 200, loss is 5.479860172271729 and perplexity is 239.8131724986372
At time: 891.5995559692383 and batch: 250, loss is 5.520373067855835 and perplexity is 249.7281853719754
At time: 892.4359872341156 and batch: 300, loss is 5.530634536743164 and perplexity is 252.30395637157912
At time: 893.2777066230774 and batch: 350, loss is 5.556770658493042 and perplexity is 258.98513299622846
At time: 894.1189801692963 and batch: 400, loss is 5.525854892730713 and perplexity is 251.1009106326686
At time: 894.9607591629028 and batch: 450, loss is 5.507609910964966 and perplexity is 246.56111923531796
At time: 895.8048083782196 and batch: 500, loss is 5.499751224517822 and perplexity is 244.63106648205735
At time: 896.6515655517578 and batch: 550, loss is 5.513826637268067 and perplexity is 248.09869662751782
At time: 897.4906613826752 and batch: 600, loss is 5.544260759353637 and perplexity is 255.76543613479186
At time: 898.3299243450165 and batch: 650, loss is 5.541287059783936 and perplexity is 255.00599630000502
At time: 899.1675312519073 and batch: 700, loss is 5.546840410232544 and perplexity is 256.42607340731513
At time: 900.0161280632019 and batch: 750, loss is 5.506176338195801 and perplexity is 246.20790916550737
At time: 900.8562536239624 and batch: 800, loss is 5.51402419090271 and perplexity is 248.14771426843393
At time: 901.693540096283 and batch: 850, loss is 5.55840612411499 and perplexity is 259.4090408266941
At time: 902.5329399108887 and batch: 900, loss is 5.549806184768677 and perplexity is 257.1877041805398
At time: 903.3720324039459 and batch: 950, loss is 5.518905973434448 and perplexity is 249.36207916620532
At time: 904.2149176597595 and batch: 1000, loss is 5.510731639862061 and perplexity is 247.33201884988978
At time: 905.0545432567596 and batch: 1050, loss is 5.507848892211914 and perplexity is 246.62004976040646
At time: 905.8991231918335 and batch: 1100, loss is 5.495757827758789 and perplexity is 243.65610557253635
At time: 906.739823102951 and batch: 1150, loss is 5.5348890209198 and perplexity is 253.3796662343741
At time: 907.5804224014282 and batch: 1200, loss is 5.552653284072876 and perplexity is 257.9209864825961
At time: 908.4222385883331 and batch: 1250, loss is 5.526775598526001 and perplexity is 251.33220715797663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.170099188811587 and perplexity of 175.93228713312658
Finished 40 epochs...
Completing Train Step...
At time: 910.8157870769501 and batch: 50, loss is 5.51421332359314 and perplexity is 248.194651551805
At time: 911.904712677002 and batch: 100, loss is 5.543980598449707 and perplexity is 255.69379069560435
At time: 912.746551990509 and batch: 150, loss is 5.4476955795288085 and perplexity is 232.22241076495405
At time: 913.5880119800568 and batch: 200, loss is 5.473370084762573 and perplexity is 238.26180372827238
At time: 914.4290151596069 and batch: 250, loss is 5.513794040679931 and perplexity is 248.0906095882926
At time: 915.270533323288 and batch: 300, loss is 5.524131135940552 and perplexity is 250.66844657146387
At time: 916.1122028827667 and batch: 350, loss is 5.550181684494018 and perplexity is 257.2842962268292
At time: 916.956710100174 and batch: 400, loss is 5.519388923645019 and perplexity is 249.48253772024788
At time: 917.8005838394165 and batch: 450, loss is 5.500969915390015 and perplexity is 244.92937786759398
At time: 918.6450009346008 and batch: 500, loss is 5.493217163085937 and perplexity is 243.03784284449205
At time: 919.4933500289917 and batch: 550, loss is 5.507215299606323 and perplexity is 246.46384261156066
At time: 920.3389065265656 and batch: 600, loss is 5.537568674087525 and perplexity is 254.05954637429588
At time: 921.1806616783142 and batch: 650, loss is 5.534597339630127 and perplexity is 253.3057709040166
At time: 922.0229573249817 and batch: 700, loss is 5.540054988861084 and perplexity is 254.69200429673958
At time: 922.8668320178986 and batch: 750, loss is 5.499740362167358 and perplexity is 244.62840922811097
At time: 923.709445476532 and batch: 800, loss is 5.507845306396485 and perplexity is 246.61916542801234
At time: 924.5554320812225 and batch: 850, loss is 5.551990013122559 and perplexity is 257.74997170562153
At time: 925.396969795227 and batch: 900, loss is 5.543382196426392 and perplexity is 255.54082878482586
At time: 926.2391736507416 and batch: 950, loss is 5.512600545883179 and perplexity is 247.79469136018477
At time: 927.0820519924164 and batch: 1000, loss is 5.5043908214569095 and perplexity is 245.76869305299294
At time: 927.9258329868317 and batch: 1050, loss is 5.501614580154419 and perplexity is 245.0873261136368
At time: 928.7709546089172 and batch: 1100, loss is 5.489310188293457 and perplexity is 242.09015262527598
At time: 929.6136100292206 and batch: 1150, loss is 5.528198394775391 and perplexity is 251.69005619243626
At time: 930.454286813736 and batch: 1200, loss is 5.5461298274993895 and perplexity is 256.2439261902191
At time: 931.2968411445618 and batch: 1250, loss is 5.520432786941528 and perplexity is 249.743099356198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.16505420991104 and perplexity of 175.0469475940527
Finished 41 epochs...
Completing Train Step...
At time: 933.7353391647339 and batch: 50, loss is 5.507941675186157 and perplexity is 246.64293296370064
At time: 934.5743267536163 and batch: 100, loss is 5.537703046798706 and perplexity is 254.0936873380992
At time: 935.4124314785004 and batch: 150, loss is 5.441281824111939 and perplexity is 230.73775920539697
At time: 936.2504277229309 and batch: 200, loss is 5.467056245803833 and perplexity is 236.7621961915205
At time: 937.0863351821899 and batch: 250, loss is 5.507352352142334 and perplexity is 246.49762342104546
At time: 937.9259295463562 and batch: 300, loss is 5.517811479568482 and perplexity is 249.08930320319277
At time: 938.7662632465363 and batch: 350, loss is 5.543742771148682 and perplexity is 255.6329869621548
At time: 939.6043617725372 and batch: 400, loss is 5.513071956634522 and perplexity is 247.91153197961606
At time: 940.444910287857 and batch: 450, loss is 5.4944842910766605 and perplexity is 243.3459980928041
At time: 941.2844631671906 and batch: 500, loss is 5.48683518409729 and perplexity is 241.49171934959452
At time: 942.1272614002228 and batch: 550, loss is 5.500752096176147 and perplexity is 244.87603335299482
At time: 942.9726536273956 and batch: 600, loss is 5.531026792526245 and perplexity is 252.40294347042197
At time: 943.8175628185272 and batch: 650, loss is 5.528051795959473 and perplexity is 251.6531614326503
At time: 944.658989906311 and batch: 700, loss is 5.53342646598816 and perplexity is 253.00935541990958
At time: 945.5009081363678 and batch: 750, loss is 5.493454427719116 and perplexity is 243.09551397051183
At time: 946.3459222316742 and batch: 800, loss is 5.501689929962158 and perplexity is 245.1057940923093
At time: 947.2163245677948 and batch: 850, loss is 5.545720634460449 and perplexity is 256.13909440903154
At time: 948.0572512149811 and batch: 900, loss is 5.537089290618897 and perplexity is 253.9377836155756
At time: 948.901743888855 and batch: 950, loss is 5.506433849334717 and perplexity is 246.27131860857517
At time: 949.7412824630737 and batch: 1000, loss is 5.4982108306884765 and perplexity is 244.2545283797214
At time: 950.589864730835 and batch: 1050, loss is 5.495529479980469 and perplexity is 243.6004735941159
At time: 951.4278399944305 and batch: 1100, loss is 5.482996320724487 and perplexity is 240.5664427748309
At time: 952.2694916725159 and batch: 1150, loss is 5.521659078598023 and perplexity is 250.04954509272798
At time: 953.1088175773621 and batch: 1200, loss is 5.539747114181519 and perplexity is 254.61360314701253
At time: 953.9488325119019 and batch: 1250, loss is 5.514220714569092 and perplexity is 248.1964859592851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.160118381472399 and perplexity of 174.1850746702385
Finished 42 epochs...
Completing Train Step...
At time: 956.3842945098877 and batch: 50, loss is 5.50182071685791 and perplexity is 245.1378528146343
At time: 957.2259809970856 and batch: 100, loss is 5.531576375961304 and perplexity is 252.5416980722363
At time: 958.0690851211548 and batch: 150, loss is 5.4349972438812255 and perplexity is 229.29221631829532
At time: 958.9076542854309 and batch: 200, loss is 5.460894260406494 and perplexity is 235.30775671544674
At time: 959.749597787857 and batch: 250, loss is 5.501040678024292 and perplexity is 244.94671032882192
At time: 960.5894706249237 and batch: 300, loss is 5.51162371635437 and perplexity is 247.5527563724399
At time: 961.4263002872467 and batch: 350, loss is 5.53744384765625 and perplexity is 254.0278350070402
At time: 962.2739446163177 and batch: 400, loss is 5.506895141601563 and perplexity is 246.38494786952998
At time: 963.1380333900452 and batch: 450, loss is 5.488137454986572 and perplexity is 241.8064118486722
At time: 963.9794898033142 and batch: 500, loss is 5.4805992984771725 and perplexity is 239.9904902208566
At time: 964.8333549499512 and batch: 550, loss is 5.494442949295044 and perplexity is 243.33593794364734
At time: 965.6724920272827 and batch: 600, loss is 5.524623775482178 and perplexity is 250.7919661829073
At time: 966.5151047706604 and batch: 650, loss is 5.521636924743652 and perplexity is 250.04400559288146
At time: 967.360200881958 and batch: 700, loss is 5.526945018768311 and perplexity is 251.37479152863895
At time: 968.2045207023621 and batch: 750, loss is 5.487310132980347 and perplexity is 241.60644281370094
At time: 969.0814261436462 and batch: 800, loss is 5.495662145614624 and perplexity is 243.63279314922536
At time: 969.926552772522 and batch: 850, loss is 5.539574136734009 and perplexity is 254.56956454479123
At time: 970.7685165405273 and batch: 900, loss is 5.530930166244507 and perplexity is 252.37855589075411
At time: 971.613032579422 and batch: 950, loss is 5.500389623641968 and perplexity is 244.78728860136462
At time: 972.4580385684967 and batch: 1000, loss is 5.492154989242554 and perplexity is 242.77983145560225
At time: 973.2994000911713 and batch: 1050, loss is 5.4895836448669435 and perplexity is 242.1563628212818
At time: 974.1411418914795 and batch: 1100, loss is 5.476805362701416 and perplexity is 239.0817067373333
At time: 974.9852395057678 and batch: 1150, loss is 5.515255365371704 and perplexity is 248.45341554596968
At time: 975.8273596763611 and batch: 1200, loss is 5.53350043296814 and perplexity is 253.0280704499753
At time: 976.6709451675415 and batch: 1250, loss is 5.508135757446289 and perplexity is 246.69080662713986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.155286357350593 and perplexity of 173.34543839282992
Finished 43 epochs...
Completing Train Step...
At time: 979.0977799892426 and batch: 50, loss is 5.495817127227784 and perplexity is 243.67055467862207
At time: 979.976945400238 and batch: 100, loss is 5.525544109344483 and perplexity is 251.02288476657608
At time: 980.8181412220001 and batch: 150, loss is 5.4288503932952885 and perplexity is 227.88711422455117
At time: 981.6583712100983 and batch: 200, loss is 5.454856042861938 and perplexity is 233.8911983382827
At time: 982.4980585575104 and batch: 250, loss is 5.494847650527954 and perplexity is 243.43443622758866
At time: 983.3386771678925 and batch: 300, loss is 5.5055597877502445 and perplexity is 246.05615635585025
At time: 984.1803073883057 and batch: 350, loss is 5.53127950668335 and perplexity is 252.4667373279978
At time: 985.0238149166107 and batch: 400, loss is 5.500852899551392 and perplexity is 244.90071892784698
At time: 985.8652656078339 and batch: 450, loss is 5.481918973922729 and perplexity is 240.30740884683874
At time: 986.7054436206818 and batch: 500, loss is 5.4744930171966555 and perplexity is 238.52950591308434
At time: 987.5486452579498 and batch: 550, loss is 5.488267297744751 and perplexity is 241.83781069854456
At time: 988.3875291347504 and batch: 600, loss is 5.518348360061646 and perplexity is 249.22307029639677
At time: 989.2295160293579 and batch: 650, loss is 5.515347757339478 and perplexity is 248.47637170639808
At time: 990.0709888935089 and batch: 700, loss is 5.520598497390747 and perplexity is 249.78448782653786
At time: 990.9405255317688 and batch: 750, loss is 5.481306848526001 and perplexity is 240.1603555909695
At time: 991.7833421230316 and batch: 800, loss is 5.4897427749633785 and perplexity is 242.19490025280115
At time: 992.6261043548584 and batch: 850, loss is 5.533542881011963 and perplexity is 253.03881122455903
At time: 993.4737884998322 and batch: 900, loss is 5.52489930152893 and perplexity is 250.8610754221674
At time: 994.3170528411865 and batch: 950, loss is 5.494474277496338 and perplexity is 243.34356134030642
At time: 995.1600403785706 and batch: 1000, loss is 5.486220932006836 and perplexity is 241.34342810492262
At time: 996.0020990371704 and batch: 1050, loss is 5.483762016296387 and perplexity is 240.7507139736288
At time: 996.8456301689148 and batch: 1100, loss is 5.470727834701538 and perplexity is 237.63308744143384
At time: 997.6879696846008 and batch: 1150, loss is 5.5089795207977295 and perplexity is 246.89904312771867
At time: 998.5304129123688 and batch: 1200, loss is 5.5274496746063235 and perplexity is 251.501681299852
At time: 999.374890089035 and batch: 1250, loss is 5.502165584564209 and perplexity is 245.22240752291765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.15054299013458 and perplexity of 172.52514433929136
Finished 44 epochs...
Completing Train Step...
At time: 1001.8016424179077 and batch: 50, loss is 5.489916515350342 and perplexity is 242.23698294411736
At time: 1002.640504360199 and batch: 100, loss is 5.519603385925293 and perplexity is 249.53604805194453
At time: 1003.4781129360199 and batch: 150, loss is 5.422846698760987 and perplexity is 226.52304841689644
At time: 1004.314309835434 and batch: 200, loss is 5.448936462402344 and perplexity is 232.51075043820597
At time: 1005.150463104248 and batch: 250, loss is 5.488763666152954 and perplexity is 241.9578811448071
At time: 1005.9885015487671 and batch: 300, loss is 5.499617605209351 and perplexity is 244.59838123186236
At time: 1006.822500705719 and batch: 350, loss is 5.5252446269989015 and perplexity is 250.94771910020845
At time: 1007.663809299469 and batch: 400, loss is 5.494934492111206 and perplexity is 243.4555773774008
At time: 1008.5053513050079 and batch: 450, loss is 5.47580551147461 and perplexity is 238.84278006501282
At time: 1009.348730802536 and batch: 500, loss is 5.468508358001709 and perplexity is 237.10625120739178
At time: 1010.187075138092 and batch: 550, loss is 5.482217321395874 and perplexity is 240.37911465113694
At time: 1011.0241661071777 and batch: 600, loss is 5.512196636199951 and perplexity is 247.69462489515945
At time: 1011.8617994785309 and batch: 650, loss is 5.509183197021485 and perplexity is 246.94933571399986
At time: 1012.7288892269135 and batch: 700, loss is 5.514381971359253 and perplexity is 248.23651255513357
At time: 1013.5670535564423 and batch: 750, loss is 5.475435829162597 and perplexity is 238.75450043258564
At time: 1014.4025228023529 and batch: 800, loss is 5.483909711837769 and perplexity is 240.78627440666128
At time: 1015.2439107894897 and batch: 850, loss is 5.527641019821167 and perplexity is 251.5498095475019
At time: 1016.0845193862915 and batch: 900, loss is 5.518986930847168 and perplexity is 249.38226769215956
At time: 1016.9291033744812 and batch: 950, loss is 5.488676099777222 and perplexity is 241.9366946976992
At time: 1017.7686982154846 and batch: 1000, loss is 5.48039361000061 and perplexity is 239.94113201891435
At time: 1018.6090080738068 and batch: 1050, loss is 5.4780552959442135 and perplexity is 239.38072975080033
At time: 1019.4475409984589 and batch: 1100, loss is 5.464750757217407 and perplexity is 236.21697239562448
At time: 1020.2914733886719 and batch: 1150, loss is 5.502820138931274 and perplexity is 245.38297146384565
At time: 1021.1294255256653 and batch: 1200, loss is 5.521467752456665 and perplexity is 250.0017086544437
At time: 1021.9716064929962 and batch: 1250, loss is 5.4963217449188235 and perplexity is 243.7935461805324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.145900754162865 and perplexity of 171.72609802396525
Finished 45 epochs...
Completing Train Step...
At time: 1024.3745391368866 and batch: 50, loss is 5.484104223251343 and perplexity is 240.83311464059773
At time: 1025.2443706989288 and batch: 100, loss is 5.513765163421631 and perplexity is 248.08344551511757
At time: 1026.086175918579 and batch: 150, loss is 5.416984910964966 and perplexity is 225.19910251210015
At time: 1026.9283065795898 and batch: 200, loss is 5.443137512207032 and perplexity is 231.16633404587157
At time: 1027.7724058628082 and batch: 250, loss is 5.4828057384490965 and perplexity is 240.52059944338626
At time: 1028.6167163848877 and batch: 300, loss is 5.493807411193847 and perplexity is 243.18133781603325
At time: 1029.4616160392761 and batch: 350, loss is 5.519335556030273 and perplexity is 249.46922378755852
At time: 1030.3148891925812 and batch: 400, loss is 5.489133415222168 and perplexity is 242.04736138774817
At time: 1031.1579468250275 and batch: 450, loss is 5.469769020080566 and perplexity is 237.40535055894156
At time: 1032.000811100006 and batch: 500, loss is 5.462634963989258 and perplexity is 235.7177144745674
At time: 1032.8440363407135 and batch: 550, loss is 5.476289319992065 and perplexity is 238.95836219389574
At time: 1033.730696439743 and batch: 600, loss is 5.506161193847657 and perplexity is 246.20418053544898
At time: 1034.572331905365 and batch: 650, loss is 5.503135528564453 and perplexity is 245.4603749146853
At time: 1035.4158749580383 and batch: 700, loss is 5.508282737731934 and perplexity is 246.72706797715034
At time: 1036.2596118450165 and batch: 750, loss is 5.469677076339722 and perplexity is 237.3835236263551
At time: 1037.1059863567352 and batch: 800, loss is 5.4781632995605465 and perplexity is 239.40658513150603
At time: 1037.9521338939667 and batch: 850, loss is 5.521852302551269 and perplexity is 250.09786532250087
At time: 1038.7995703220367 and batch: 900, loss is 5.513185987472534 and perplexity is 247.9398031512226
At time: 1039.641709804535 and batch: 950, loss is 5.482965698242188 and perplexity is 240.55907614598806
At time: 1040.4861688613892 and batch: 1000, loss is 5.474681482315064 and perplexity is 238.57446464110305
At time: 1041.3358752727509 and batch: 1050, loss is 5.472460565567016 and perplexity is 238.04519856242342
At time: 1042.179817199707 and batch: 1100, loss is 5.458876037597657 and perplexity is 234.8333321420312
At time: 1043.0237231254578 and batch: 1150, loss is 5.4967734813690186 and perplexity is 243.9037014903755
At time: 1043.869001865387 and batch: 1200, loss is 5.515600891113281 and perplexity is 248.53927742951473
At time: 1044.721593618393 and batch: 1250, loss is 5.490580215454101 and perplexity is 242.39780901906937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141335146270529 and perplexity of 170.9438510707517
Finished 46 epochs...
Completing Train Step...
At time: 1047.206017255783 and batch: 50, loss is 5.478401966094971 and perplexity is 239.46373029054504
At time: 1048.0441596508026 and batch: 100, loss is 5.508077363967896 and perplexity is 246.67640191342807
At time: 1048.8884162902832 and batch: 150, loss is 5.411258640289307 and perplexity is 223.91323661693008
At time: 1049.7241876125336 and batch: 200, loss is 5.437448902130127 and perplexity is 229.8550521304554
At time: 1050.5626337528229 and batch: 250, loss is 5.476958227157593 and perplexity is 239.11825662593375
At time: 1051.4062311649323 and batch: 300, loss is 5.488117370605469 and perplexity is 241.80155536531336
At time: 1052.2457616329193 and batch: 350, loss is 5.513543004989624 and perplexity is 248.02833780740266
At time: 1053.0853986740112 and batch: 400, loss is 5.48344500541687 and perplexity is 240.67440547399906
At time: 1053.9271922111511 and batch: 450, loss is 5.463866958618164 and perplexity is 236.00829639366586
At time: 1054.7740437984467 and batch: 500, loss is 5.456864042282104 and perplexity is 234.36132357656638
At time: 1055.6417047977448 and batch: 550, loss is 5.470472660064697 and perplexity is 237.57245724061826
At time: 1056.4824678897858 and batch: 600, loss is 5.5002367973327635 and perplexity is 244.74988152197335
At time: 1057.3227624893188 and batch: 650, loss is 5.497196683883667 and perplexity is 244.0069439948811
At time: 1058.1650698184967 and batch: 700, loss is 5.502300386428833 and perplexity is 245.25546618883408
At time: 1059.0112698078156 and batch: 750, loss is 5.464028978347779 and perplexity is 236.04653749185695
At time: 1059.851377248764 and batch: 800, loss is 5.47255723953247 and perplexity is 238.06821244812966
At time: 1060.6940126419067 and batch: 850, loss is 5.51615951538086 and perplexity is 248.6781562882722
At time: 1061.534882068634 and batch: 900, loss is 5.50748236656189 and perplexity is 246.52967374993372
At time: 1062.3734412193298 and batch: 950, loss is 5.477379188537598 and perplexity is 239.21893736709248
At time: 1063.2137563228607 and batch: 1000, loss is 5.469057521820068 and perplexity is 237.23649714156562
At time: 1064.0590901374817 and batch: 1050, loss is 5.466974096298218 and perplexity is 236.7427470930329
At time: 1064.9010243415833 and batch: 1100, loss is 5.453090705871582 and perplexity is 233.4786657907555
At time: 1065.74862241745 and batch: 1150, loss is 5.490834321975708 and perplexity is 242.4594117096555
At time: 1066.5902028083801 and batch: 1200, loss is 5.509846544265747 and perplexity is 247.11320321983843
At time: 1067.4404482841492 and batch: 1250, loss is 5.484960212707519 and perplexity is 241.0393535039815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.136908983662181 and perplexity of 170.18889779451496
Finished 47 epochs...
Completing Train Step...
At time: 1069.8721709251404 and batch: 50, loss is 5.472790393829346 and perplexity is 238.12372554611858
At time: 1070.7478413581848 and batch: 100, loss is 5.502516269683838 and perplexity is 245.3084184527306
At time: 1071.5912296772003 and batch: 150, loss is 5.405761451721191 and perplexity is 222.68572036769552
At time: 1072.4347665309906 and batch: 200, loss is 5.431862421035767 and perplexity is 228.57455130435
At time: 1073.277261018753 and batch: 250, loss is 5.471222352981568 and perplexity is 237.75063040829093
At time: 1074.1193063259125 and batch: 300, loss is 5.482551393508911 and perplexity is 240.4594320250488
At time: 1074.962969303131 and batch: 350, loss is 5.507850637435913 and perplexity is 246.62048016801154
At time: 1075.8050858974457 and batch: 400, loss is 5.4778533554077145 and perplexity is 239.3323939584491
At time: 1076.6496069431305 and batch: 450, loss is 5.458087930679321 and perplexity is 234.6483312781202
At time: 1077.525475025177 and batch: 500, loss is 5.451214179992676 and perplexity is 233.04094785519007
At time: 1078.3675019741058 and batch: 550, loss is 5.464779815673828 and perplexity is 236.2238365959537
At time: 1079.2094922065735 and batch: 600, loss is 5.494408864974975 and perplexity is 243.3276441449993
At time: 1080.0540676116943 and batch: 650, loss is 5.49136344909668 and perplexity is 242.58773750746192
At time: 1080.8972296714783 and batch: 700, loss is 5.496433296203613 and perplexity is 243.82074318073424
At time: 1081.7420628070831 and batch: 750, loss is 5.458472957611084 and perplexity is 234.73869460019554
At time: 1082.5836758613586 and batch: 800, loss is 5.4670307826995845 and perplexity is 236.7561675677909
At time: 1083.424971818924 and batch: 850, loss is 5.510617351531982 and perplexity is 247.3037533017223
At time: 1084.2678496837616 and batch: 900, loss is 5.501785812377929 and perplexity is 245.1292965546849
At time: 1085.1123459339142 and batch: 950, loss is 5.471936998367309 and perplexity is 237.9205985255009
At time: 1085.9532272815704 and batch: 1000, loss is 5.463468961715698 and perplexity is 235.9143845123035
At time: 1086.7962973117828 and batch: 1050, loss is 5.461605024337769 and perplexity is 235.4750644327712
At time: 1087.640807390213 and batch: 1100, loss is 5.447411003112793 and perplexity is 232.15633514580756
At time: 1088.4878981113434 and batch: 1150, loss is 5.484993410110474 and perplexity is 241.04735551734998
At time: 1089.335860967636 and batch: 1200, loss is 5.504199523925781 and perplexity is 245.72168260541784
At time: 1090.1836295127869 and batch: 1250, loss is 5.479437522888183 and perplexity is 239.71183702531067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1326048913663325 and perplexity of 169.45796320404116
Finished 48 epochs...
Completing Train Step...
At time: 1092.5899715423584 and batch: 50, loss is 5.467283105850219 and perplexity is 236.81591416733195
At time: 1093.4607291221619 and batch: 100, loss is 5.497068729400635 and perplexity is 243.9757242099043
At time: 1094.2985439300537 and batch: 150, loss is 5.400273590087891 and perplexity is 221.4669990921169
At time: 1095.138810634613 and batch: 200, loss is 5.4263845062255855 and perplexity is 227.3258626121753
At time: 1095.9835932254791 and batch: 250, loss is 5.46558840751648 and perplexity is 236.41492250810862
At time: 1096.823230266571 and batch: 300, loss is 5.477079305648804 and perplexity is 239.1472104564747
At time: 1097.6660022735596 and batch: 350, loss is 5.502268438339233 and perplexity is 245.2476308703879
At time: 1098.5384068489075 and batch: 400, loss is 5.472365198135376 and perplexity is 238.02249788569196
At time: 1099.3828058242798 and batch: 450, loss is 5.452418355941773 and perplexity is 233.32173918690626
At time: 1100.2315373420715 and batch: 500, loss is 5.445675630569458 and perplexity is 231.7538067853848
At time: 1101.0729112625122 and batch: 550, loss is 5.45920581817627 and perplexity is 234.91078838526303
At time: 1101.912150144577 and batch: 600, loss is 5.488692588806153 and perplexity is 241.9406840317475
At time: 1102.7581012248993 and batch: 650, loss is 5.485632829666137 and perplexity is 241.20153519780703
At time: 1103.598525762558 and batch: 700, loss is 5.490675544738769 and perplexity is 242.4209177302592
At time: 1104.4404277801514 and batch: 750, loss is 5.453023242950439 and perplexity is 233.46291516923418
At time: 1105.2822144031525 and batch: 800, loss is 5.461574373245239 and perplexity is 235.46784697539482
At time: 1106.1234118938446 and batch: 850, loss is 5.505162448883056 and perplexity is 245.95840810229535
At time: 1106.9624404907227 and batch: 900, loss is 5.496295862197876 and perplexity is 243.78723622186752
At time: 1107.803069114685 and batch: 950, loss is 5.466577568054199 and perplexity is 236.64889051686544
At time: 1108.6406254768372 and batch: 1000, loss is 5.458046073913574 and perplexity is 234.63850986343277
At time: 1109.4812898635864 and batch: 1050, loss is 5.456311740875244 and perplexity is 234.231921225681
At time: 1110.3225300312042 and batch: 1100, loss is 5.441836462020874 and perplexity is 230.86577061038253
At time: 1111.1593263149261 and batch: 1150, loss is 5.479266538619995 and perplexity is 239.67085357614317
At time: 1112.002119064331 and batch: 1200, loss is 5.498657445907593 and perplexity is 244.36364053319446
At time: 1112.8437368869781 and batch: 1250, loss is 5.474000177383423 and perplexity is 238.41197803952613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.128344904767336 and perplexity of 168.73760998860385
Finished 49 epochs...
Completing Train Step...
At time: 1115.275164604187 and batch: 50, loss is 5.461880559921265 and perplexity is 235.53995513148618
At time: 1116.1172099113464 and batch: 100, loss is 5.491697864532471 and perplexity is 242.66887615767044
At time: 1116.958729982376 and batch: 150, loss is 5.394820690155029 and perplexity is 220.26264829402038
At time: 1117.814593553543 and batch: 200, loss is 5.421027040481567 and perplexity is 226.11122867559948
At time: 1118.667130947113 and batch: 250, loss is 5.460047607421875 and perplexity is 235.10861701392648
At time: 1119.5177290439606 and batch: 300, loss is 5.471679954528809 and perplexity is 237.85945036081375
At time: 1120.3907659053802 and batch: 350, loss is 5.496797428131104 and perplexity is 243.90954226422036
At time: 1121.23477602005 and batch: 400, loss is 5.4669731330871585 and perplexity is 236.7425190599105
At time: 1122.0791084766388 and batch: 450, loss is 5.446856870651245 and perplexity is 232.0277254210517
At time: 1122.921474456787 and batch: 500, loss is 5.440227518081665 and perplexity is 230.49461918896966
At time: 1123.7709176540375 and batch: 550, loss is 5.453712320327758 and perplexity is 233.6238446225753
At time: 1124.610783815384 and batch: 600, loss is 5.483083734512329 and perplexity is 240.58747251795288
At time: 1125.4556431770325 and batch: 650, loss is 5.480016117095947 and perplexity is 239.85057303780218
At time: 1126.2985377311707 and batch: 700, loss is 5.485017223358154 and perplexity is 241.05309570607562
At time: 1127.1387813091278 and batch: 750, loss is 5.447679624557495 and perplexity is 232.21870569260932
At time: 1127.9815635681152 and batch: 800, loss is 5.456234397888184 and perplexity is 234.21380572979086
At time: 1128.8224551677704 and batch: 850, loss is 5.499795961380005 and perplexity is 244.6420107531685
At time: 1129.6645584106445 and batch: 900, loss is 5.490888910293579 and perplexity is 242.4726475223499
At time: 1130.5069835186005 and batch: 950, loss is 5.461299772262573 and perplexity is 235.40319615022557
At time: 1131.3503770828247 and batch: 1000, loss is 5.452721853256225 and perplexity is 233.39256245494508
At time: 1132.1922783851624 and batch: 1050, loss is 5.451094455718994 and perplexity is 233.0130488670964
At time: 1133.0332264900208 and batch: 1100, loss is 5.436347894668579 and perplexity is 229.6021192690199
At time: 1133.8758022785187 and batch: 1150, loss is 5.473636436462402 and perplexity is 238.32527361698092
At time: 1134.717562675476 and batch: 1200, loss is 5.493203163146973 and perplexity is 243.03444035334354
At time: 1135.5596587657928 and batch: 1250, loss is 5.468650770187378 and perplexity is 237.14002043137944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.124165110344435 and perplexity of 168.0337933964524
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe70d2b860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'num_layers': 1, 'anneal': 5.677451280969865, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.9784253954823844, 'tune_wordvecs': True, 'lr': 9.067242169773948, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4268429279327393 and batch: 50, loss is 10.741443767547608 and perplexity is 46232.753104609175
At time: 2.2671029567718506 and batch: 100, loss is 9.791539440155029 and perplexity is 17881.812970632414
At time: 3.108450174331665 and batch: 150, loss is 9.020460052490234 and perplexity is 8270.581107602924
At time: 3.9495980739593506 and batch: 200, loss is 8.536597385406495 and perplexity is 5097.968394896241
At time: 4.8467116355896 and batch: 250, loss is 8.293309936523437 and perplexity is 3997.042279738632
At time: 5.68652606010437 and batch: 300, loss is 8.088069763183594 and perplexity is 3255.397813108292
At time: 6.527777671813965 and batch: 350, loss is 7.967967939376831 and perplexity is 2886.9848713336573
At time: 7.3676252365112305 and batch: 400, loss is 7.833737449645996 and perplexity is 2524.3463821516407
At time: 8.205206632614136 and batch: 450, loss is 7.74328384399414 and perplexity is 2306.0326143694383
At time: 9.043911933898926 and batch: 500, loss is 7.685126447677613 and perplexity is 2175.7450741249595
At time: 9.885284900665283 and batch: 550, loss is 7.648342714309693 and perplexity is 2097.16710276367
At time: 10.724228858947754 and batch: 600, loss is 7.621077632904052 and perplexity is 2040.7601355581764
At time: 11.562703847885132 and batch: 650, loss is 7.5820456218719485 and perplexity is 1962.6396804433175
At time: 12.401767492294312 and batch: 700, loss is 7.530638647079468 and perplexity is 1864.2957504785059
At time: 13.2415189743042 and batch: 750, loss is 7.452876195907593 and perplexity is 1724.8169293278222
At time: 14.07821798324585 and batch: 800, loss is 7.41265827178955 and perplexity is 1656.8247887435464
At time: 14.915795087814331 and batch: 850, loss is 7.423413791656494 and perplexity is 1674.7409769203364
At time: 15.754544496536255 and batch: 900, loss is 7.407401275634766 and perplexity is 1648.1377211467598
At time: 16.592559099197388 and batch: 950, loss is 7.355123653411865 and perplexity is 1564.190400916272
At time: 17.430498838424683 and batch: 1000, loss is 7.339870386123657 and perplexity is 1540.5124289380612
At time: 18.269482851028442 and batch: 1050, loss is 7.299980850219726 and perplexity is 1480.2715804376146
At time: 19.109442472457886 and batch: 1100, loss is 7.283620843887329 and perplexity is 1456.2513493230197
At time: 19.95503520965576 and batch: 1150, loss is 7.29335654258728 and perplexity is 1470.4982127454316
At time: 20.7984299659729 and batch: 1200, loss is 7.280884323120117 and perplexity is 1452.27173489554
At time: 21.640265226364136 and batch: 1250, loss is 7.246530046463013 and perplexity is 1403.227257207369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.7146241125399175 and perplexity of 824.3738368640321
Finished 1 epochs...
Completing Train Step...
At time: 24.08686399459839 and batch: 50, loss is 6.460451326370239 and perplexity is 639.3495467732968
At time: 24.92020034790039 and batch: 100, loss is 5.96753399848938 and perplexity is 390.5414066014798
At time: 25.76181936264038 and batch: 150, loss is 5.715347566604614 and perplexity is 303.48966785376035
At time: 26.61585807800293 and batch: 200, loss is 5.651183118820191 and perplexity is 284.62801545740166
At time: 27.45457434654236 and batch: 250, loss is 5.607631607055664 and perplexity is 272.49809058620923
At time: 28.294995307922363 and batch: 300, loss is 5.566374111175537 and perplexity is 261.48426540924953
At time: 29.134838581085205 and batch: 350, loss is 5.565778236389161 and perplexity is 261.32849994142435
At time: 30.031078338623047 and batch: 400, loss is 5.501360330581665 and perplexity is 245.02502068658484
At time: 30.865398645401 and batch: 450, loss is 5.453713970184326 and perplexity is 233.6242300687277
At time: 31.701143980026245 and batch: 500, loss is 5.427518072128296 and perplexity is 227.5836975676846
At time: 32.540313482284546 and batch: 550, loss is 5.424922733306885 and perplexity is 226.9938065768761
At time: 33.38044714927673 and batch: 600, loss is 5.432081174850464 and perplexity is 228.6245583288048
At time: 34.21890926361084 and batch: 650, loss is 5.402105722427368 and perplexity is 221.8731278705104
At time: 35.055259466171265 and batch: 700, loss is 5.399947662353515 and perplexity is 221.3948286166825
At time: 35.89028239250183 and batch: 750, loss is 5.359979553222656 and perplexity is 212.72059695439796
At time: 36.725289821624756 and batch: 800, loss is 5.380151033401489 and perplexity is 217.0550555127856
At time: 37.56063461303711 and batch: 850, loss is 5.419928274154663 and perplexity is 225.8629217120425
At time: 38.39538216590881 and batch: 900, loss is 5.387345190048218 and perplexity is 218.62221401585523
At time: 39.23669695854187 and batch: 950, loss is 5.355905570983887 and perplexity is 211.85573992287095
At time: 40.076012134552 and batch: 1000, loss is 5.347431831359863 and perplexity is 210.06811417904186
At time: 40.910850048065186 and batch: 1050, loss is 5.33457166671753 and perplexity is 207.3839003541525
At time: 41.74661564826965 and batch: 1100, loss is 5.315528516769409 and perplexity is 203.47202309224573
At time: 42.58529281616211 and batch: 1150, loss is 5.339838104248047 and perplexity is 208.47895569950833
At time: 43.424851179122925 and batch: 1200, loss is 5.332951335906983 and perplexity is 207.04814192412218
At time: 44.26714324951172 and batch: 1250, loss is 5.322086591720581 and perplexity is 204.81079294849457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.063511312442975 and perplexity of 158.14483849386104
Finished 2 epochs...
Completing Train Step...
At time: 46.69969153404236 and batch: 50, loss is 5.272507371902466 and perplexity is 194.90404721632743
At time: 47.5420286655426 and batch: 100, loss is 5.25749870300293 and perplexity is 192.0006395482936
At time: 48.38413763046265 and batch: 150, loss is 5.14297155380249 and perplexity is 171.2238138803371
At time: 49.22572994232178 and batch: 200, loss is 5.1799859619140625 and perplexity is 177.6803166843018
At time: 50.0967652797699 and batch: 250, loss is 5.186906957626343 and perplexity is 178.91430668830236
At time: 50.94320559501648 and batch: 300, loss is 5.187413873672486 and perplexity is 179.00502421238886
At time: 51.78605318069458 and batch: 350, loss is 5.191922302246094 and perplexity is 179.81387753703032
At time: 52.62894320487976 and batch: 400, loss is 5.1587116909027095 and perplexity is 173.94022242436748
At time: 53.47271370887756 and batch: 450, loss is 5.11277208328247 and perplexity is 166.13024402589193
At time: 54.311914682388306 and batch: 500, loss is 5.126412076950073 and perplexity is 168.41178422618364
At time: 55.158442735672 and batch: 550, loss is 5.125109310150147 and perplexity is 168.19252579728874
At time: 56.003767013549805 and batch: 600, loss is 5.136614542007447 and perplexity is 170.1387944704457
At time: 56.84866738319397 and batch: 650, loss is 5.138827562332153 and perplexity is 170.51573201201387
At time: 57.69224739074707 and batch: 700, loss is 5.143461589813232 and perplexity is 171.30774027683387
At time: 58.535327434539795 and batch: 750, loss is 5.108711471557617 and perplexity is 165.4570213820677
At time: 59.37991213798523 and batch: 800, loss is 5.1343463516235355 and perplexity is 169.75332461766442
At time: 60.22403001785278 and batch: 850, loss is 5.169850797653198 and perplexity is 175.88859253542805
At time: 61.06898880004883 and batch: 900, loss is 5.131208000183105 and perplexity is 169.22141412444057
At time: 61.91250514984131 and batch: 950, loss is 5.123163509368896 and perplexity is 167.86557484305985
At time: 62.75908422470093 and batch: 1000, loss is 5.107180795669556 and perplexity is 165.20395404037737
At time: 63.60062837600708 and batch: 1050, loss is 5.104088144302368 and perplexity is 164.6938250382213
At time: 64.44598984718323 and batch: 1100, loss is 5.072640399932862 and perplexity is 159.5951665739684
At time: 65.28987073898315 and batch: 1150, loss is 5.103713693618775 and perplexity is 164.63216686754464
At time: 66.13340163230896 and batch: 1200, loss is 5.111708993911743 and perplexity is 165.95372657285014
At time: 66.97775363922119 and batch: 1250, loss is 5.1124372482299805 and perplexity is 166.0746271086797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.977874087591241 and perplexity of 145.1654443636587
Finished 3 epochs...
Completing Train Step...
At time: 69.43165159225464 and batch: 50, loss is 5.071026477813721 and perplexity is 159.33780014507732
At time: 70.30102753639221 and batch: 100, loss is 5.068023328781128 and perplexity is 158.8600027921459
At time: 71.14081025123596 and batch: 150, loss is 4.962526187896729 and perplexity is 142.95446999179194
At time: 72.01320099830627 and batch: 200, loss is 5.014673442840576 and perplexity is 150.60694695792387
At time: 72.85452914237976 and batch: 250, loss is 5.019885540008545 and perplexity is 151.39397424930297
At time: 73.69397950172424 and batch: 300, loss is 5.020797243118286 and perplexity is 151.53206354507736
At time: 74.53435587882996 and batch: 350, loss is 5.017544384002686 and perplexity is 151.03995190970164
At time: 75.37634658813477 and batch: 400, loss is 5.004063415527344 and perplexity is 149.0174503494824
At time: 76.21714043617249 and batch: 450, loss is 4.94826003074646 and perplexity is 140.92953740409988
At time: 77.06321024894714 and batch: 500, loss is 4.966956367492676 and perplexity is 143.5891888891447
At time: 77.90476489067078 and batch: 550, loss is 4.972086610794068 and perplexity is 144.32772918948265
At time: 78.7460105419159 and batch: 600, loss is 4.980814218521118 and perplexity is 145.59287782384237
At time: 79.58940052986145 and batch: 650, loss is 4.988651523590088 and perplexity is 146.73841672858563
At time: 80.4297149181366 and batch: 700, loss is 4.993609504699707 and perplexity is 147.46774954192506
At time: 81.27468085289001 and batch: 750, loss is 4.960498790740967 and perplexity is 142.66493810316376
At time: 82.11680293083191 and batch: 800, loss is 4.989093837738037 and perplexity is 146.8033355625533
At time: 82.95452737808228 and batch: 850, loss is 5.021424016952515 and perplexity is 151.62706964811355
At time: 83.79690170288086 and batch: 900, loss is 4.974847373962402 and perplexity is 144.72673439440658
At time: 84.64025807380676 and batch: 950, loss is 4.968433380126953 and perplexity is 143.8014286370896
At time: 85.47981667518616 and batch: 1000, loss is 4.964690542221069 and perplexity is 143.26420918887513
At time: 86.32329392433167 and batch: 1050, loss is 4.954575710296631 and perplexity is 141.8224198131589
At time: 87.17113184928894 and batch: 1100, loss is 4.923642272949219 and perplexity is 137.50252395152108
At time: 88.01235914230347 and batch: 1150, loss is 4.956603145599365 and perplexity is 142.1102472709891
At time: 88.85898733139038 and batch: 1200, loss is 4.9764836978912355 and perplexity is 144.9637480757324
At time: 89.70066380500793 and batch: 1250, loss is 4.978013286590576 and perplexity is 145.1856526547065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.937779781592154 and perplexity of 139.46027331387666
Finished 4 epochs...
Completing Train Step...
At time: 92.21369957923889 and batch: 50, loss is 4.93295802116394 and perplexity is 138.7894478661917
At time: 93.06812262535095 and batch: 100, loss is 4.927196359634399 and perplexity is 137.99208930459736
At time: 93.96986961364746 and batch: 150, loss is 4.834082870483399 and perplexity is 125.72322583479783
At time: 94.81662487983704 and batch: 200, loss is 4.8842983818054195 and perplexity is 132.19768049042028
At time: 95.66206574440002 and batch: 250, loss is 4.890111598968506 and perplexity is 132.9684123609607
At time: 96.51190710067749 and batch: 300, loss is 4.891163635253906 and perplexity is 133.10837356480013
At time: 97.35614252090454 and batch: 350, loss is 4.888715019226074 and perplexity is 132.78284098265004
At time: 98.20223474502563 and batch: 400, loss is 4.874760208129882 and perplexity is 130.9427504388983
At time: 99.04827380180359 and batch: 450, loss is 4.827038831710816 and perplexity is 124.84073834442371
At time: 99.89625930786133 and batch: 500, loss is 4.850104141235351 and perplexity is 127.75369358075335
At time: 100.73873138427734 and batch: 550, loss is 4.857128038406372 and perplexity is 128.65418115159696
At time: 101.58281779289246 and batch: 600, loss is 4.8665979290008545 and perplexity is 129.8783092028009
At time: 102.42629790306091 and batch: 650, loss is 4.87625560760498 and perplexity is 131.13870864055127
At time: 103.28974652290344 and batch: 700, loss is 4.879831714630127 and perplexity is 131.60851423465493
At time: 104.13506031036377 and batch: 750, loss is 4.849870309829712 and perplexity is 127.72382424733797
At time: 104.97735357284546 and batch: 800, loss is 4.877401819229126 and perplexity is 131.28910753078253
At time: 105.82029747962952 and batch: 850, loss is 4.911217746734619 and perplexity is 135.8046894736082
At time: 106.66424942016602 and batch: 900, loss is 4.863919258117676 and perplexity is 129.5308734982812
At time: 107.50480675697327 and batch: 950, loss is 4.861500778198242 and perplexity is 129.21798419253903
At time: 108.35050702095032 and batch: 1000, loss is 4.860130882263183 and perplexity is 129.0410901921067
At time: 109.19805145263672 and batch: 1050, loss is 4.850454502105713 and perplexity is 127.79846131800191
At time: 110.04209399223328 and batch: 1100, loss is 4.819249906539917 and perplexity is 123.87214024110511
At time: 110.88741850852966 and batch: 1150, loss is 4.857438201904297 and perplexity is 128.69409117144136
At time: 111.73499608039856 and batch: 1200, loss is 4.869598846435547 and perplexity is 130.26864868061455
At time: 112.57743740081787 and batch: 1250, loss is 4.882566633224488 and perplexity is 131.9689454577764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.932368146242016 and perplexity of 138.7076035927831
Finished 5 epochs...
Completing Train Step...
At time: 115.03305912017822 and batch: 50, loss is 4.839283647537232 and perplexity is 126.37878754035124
At time: 115.87733888626099 and batch: 100, loss is 4.829459323883056 and perplexity is 125.14328037696325
At time: 116.72065234184265 and batch: 150, loss is 4.738879127502441 and perplexity is 114.3060073888645
At time: 117.56771993637085 and batch: 200, loss is 4.796801166534424 and perplexity is 121.12234694623908
At time: 118.41177725791931 and batch: 250, loss is 4.80255838394165 and perplexity is 121.82168582179267
At time: 119.26130747795105 and batch: 300, loss is 4.809644689559937 and perplexity is 122.68801743847574
At time: 120.1010491847992 and batch: 350, loss is 4.796461200714111 and perplexity is 121.08117648687231
At time: 120.94129252433777 and batch: 400, loss is 4.791700239181519 and perplexity is 120.50608374679398
At time: 121.78224563598633 and batch: 450, loss is 4.743837413787841 and perplexity is 114.87417670669011
At time: 122.6239595413208 and batch: 500, loss is 4.7679234790802 and perplexity is 117.67463424230901
At time: 123.46712756156921 and batch: 550, loss is 4.774685287475586 and perplexity is 118.47302380869053
At time: 124.30818438529968 and batch: 600, loss is 4.789599657058716 and perplexity is 120.25321649877702
At time: 125.14855313301086 and batch: 650, loss is 4.799076509475708 and perplexity is 121.39825559776048
At time: 125.99203085899353 and batch: 700, loss is 4.798867473602295 and perplexity is 121.37288165949481
At time: 126.83378028869629 and batch: 750, loss is 4.770923624038696 and perplexity is 118.02820532001316
At time: 127.67883324623108 and batch: 800, loss is 4.800464305877686 and perplexity is 121.56684861942095
At time: 128.51942992210388 and batch: 850, loss is 4.834916849136352 and perplexity is 125.82812005501506
At time: 129.36243152618408 and batch: 900, loss is 4.789372482299805 and perplexity is 120.22590110611198
At time: 130.20437216758728 and batch: 950, loss is 4.782673807144165 and perplexity is 119.42323823964742
At time: 131.04523277282715 and batch: 1000, loss is 4.77703296661377 and perplexity is 118.75148719857111
At time: 131.8877329826355 and batch: 1050, loss is 4.77029055595398 and perplexity is 117.95350907652733
At time: 132.72733640670776 and batch: 1100, loss is 4.732834215164185 and perplexity is 113.617121818615
At time: 133.57230925559998 and batch: 1150, loss is 4.772268180847168 and perplexity is 118.18700768254274
At time: 134.41367888450623 and batch: 1200, loss is 4.780903282165528 and perplexity is 119.21198348441095
At time: 135.25117754936218 and batch: 1250, loss is 4.793876533508301 and perplexity is 120.76862603421786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.927759769189096 and perplexity of 138.06985727173495
Finished 6 epochs...
Completing Train Step...
At time: 137.67961144447327 and batch: 50, loss is 4.758357629776001 and perplexity is 116.5543432344164
At time: 138.5509021282196 and batch: 100, loss is 4.734032201766968 and perplexity is 113.75331517101785
At time: 139.39283061027527 and batch: 150, loss is 4.656592750549317 and perplexity is 105.27676616099198
At time: 140.23704886436462 and batch: 200, loss is 4.71164381980896 and perplexity is 111.23485976766594
At time: 141.07928800582886 and batch: 250, loss is 4.7208591747283934 and perplexity is 112.26466620793713
At time: 141.92496728897095 and batch: 300, loss is 4.729898281097412 and perplexity is 113.28403863448943
At time: 142.77165269851685 and batch: 350, loss is 4.716081771850586 and perplexity is 111.72961177147586
At time: 143.61569237709045 and batch: 400, loss is 4.710108680725098 and perplexity is 111.06422979083143
At time: 144.4579577445984 and batch: 450, loss is 4.657055196762085 and perplexity is 105.3254622615914
At time: 145.30836081504822 and batch: 500, loss is 4.689099969863892 and perplexity is 108.75525274044435
At time: 146.15013790130615 and batch: 550, loss is 4.692018098831177 and perplexity is 109.07307809596271
At time: 146.99594521522522 and batch: 600, loss is 4.708593692779541 and perplexity is 110.89609621384517
At time: 147.83798241615295 and batch: 650, loss is 4.728624935150147 and perplexity is 113.13988066396632
At time: 148.67877388000488 and batch: 700, loss is 4.722898368835449 and perplexity is 112.49382922822824
At time: 149.5220868587494 and batch: 750, loss is 4.6937679672241215 and perplexity is 109.2641087183228
At time: 150.36778283119202 and batch: 800, loss is 4.722752456665039 and perplexity is 112.4774162069067
At time: 151.21439576148987 and batch: 850, loss is 4.762648324966431 and perplexity is 117.05551681700992
At time: 152.05765080451965 and batch: 900, loss is 4.7259563636779784 and perplexity is 112.83836129798462
At time: 152.90114307403564 and batch: 950, loss is 4.723198986053466 and perplexity is 112.52765189379804
At time: 153.74294066429138 and batch: 1000, loss is 4.720591869354248 and perplexity is 112.23466126975237
At time: 154.58549761772156 and batch: 1050, loss is 4.715260677337646 and perplexity is 111.63790885385079
At time: 155.42685103416443 and batch: 1100, loss is 4.678717546463012 and perplexity is 107.63195104679932
At time: 156.27672028541565 and batch: 1150, loss is 4.713058185577393 and perplexity is 111.39229785685443
At time: 157.1202449798584 and batch: 1200, loss is 4.722732982635498 and perplexity is 112.4752258397085
At time: 158.01642107963562 and batch: 1250, loss is 4.735159645080566 and perplexity is 113.8816379103214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.927003289661268 and perplexity of 137.96544974735514
Finished 7 epochs...
Completing Train Step...
At time: 160.46199655532837 and batch: 50, loss is 4.689069290161132 and perplexity is 108.75191621279883
At time: 161.3018114566803 and batch: 100, loss is 4.668758668899536 and perplexity is 106.5653773778464
At time: 162.14044904708862 and batch: 150, loss is 4.5992105770111085 and perplexity is 99.40581142672494
At time: 162.98414301872253 and batch: 200, loss is 4.659706430435181 and perplexity is 105.6050751695681
At time: 163.82813167572021 and batch: 250, loss is 4.665173149108886 and perplexity is 106.18396928999731
At time: 164.66952991485596 and batch: 300, loss is 4.671862392425537 and perplexity is 106.89664065556109
At time: 165.51907396316528 and batch: 350, loss is 4.6591972064971925 and perplexity is 105.55131222716909
At time: 166.36146426200867 and batch: 400, loss is 4.659920272827148 and perplexity is 105.62766042620284
At time: 167.20311975479126 and batch: 450, loss is 4.6068355751037595 and perplexity is 100.1666776646274
At time: 168.0475013256073 and batch: 500, loss is 4.636480989456177 and perplexity is 103.18061429837014
At time: 168.89112615585327 and batch: 550, loss is 4.635588445663452 and perplexity is 103.08856216793839
At time: 169.73624277114868 and batch: 600, loss is 4.661114931106567 and perplexity is 105.75392479163433
At time: 170.58780455589294 and batch: 650, loss is 4.675018558502197 and perplexity is 107.234557186343
At time: 171.43658018112183 and batch: 700, loss is 4.6724964427948 and perplexity is 106.96444000187054
At time: 172.28128576278687 and batch: 750, loss is 4.646513471603393 and perplexity is 104.22098197821484
At time: 173.124657869339 and batch: 800, loss is 4.664350070953369 and perplexity is 106.09660754211374
At time: 173.9729220867157 and batch: 850, loss is 4.707600078582764 and perplexity is 110.7859630023021
At time: 174.82071042060852 and batch: 900, loss is 4.654998426437378 and perplexity is 105.10905460308993
At time: 175.6613893508911 and batch: 950, loss is 4.6565527248382566 and perplexity is 105.27255246789686
At time: 176.50808358192444 and batch: 1000, loss is 4.65615291595459 and perplexity is 105.23047197885116
At time: 177.35043454170227 and batch: 1050, loss is 4.656449508666992 and perplexity is 105.2616871988373
At time: 178.1977255344391 and batch: 1100, loss is 4.618353271484375 and perplexity is 101.32703654860177
At time: 179.0423641204834 and batch: 1150, loss is 4.648352823257446 and perplexity is 104.41285742291547
At time: 179.94348978996277 and batch: 1200, loss is 4.656707010269165 and perplexity is 105.28879574203526
At time: 180.78494668006897 and batch: 1250, loss is 4.677058191299438 and perplexity is 107.45349951124932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.93910250698563 and perplexity of 139.64486301259774
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 183.1884250640869 and batch: 50, loss is 4.635540609359741 and perplexity is 103.0836309101169
At time: 184.0604453086853 and batch: 100, loss is 4.617250423431397 and perplexity is 101.21534982169486
At time: 184.90143156051636 and batch: 150, loss is 4.522654390335083 and perplexity is 92.07968931537175
At time: 185.74232697486877 and batch: 200, loss is 4.5792766952514645 and perplexity is 97.44388708481557
At time: 186.58104848861694 and batch: 250, loss is 4.576788415908814 and perplexity is 97.20172088703688
At time: 187.42750930786133 and batch: 300, loss is 4.567052497863769 and perplexity is 96.2599647677689
At time: 188.27091646194458 and batch: 350, loss is 4.540377607345581 and perplexity is 93.72618513421276
At time: 189.10975337028503 and batch: 400, loss is 4.527944736480713 and perplexity is 92.56811357236688
At time: 189.94831490516663 and batch: 450, loss is 4.470172491073608 and perplexity is 87.37179256802496
At time: 190.7870659828186 and batch: 500, loss is 4.482014932632446 and perplexity is 88.41263883163526
At time: 191.63184189796448 and batch: 550, loss is 4.472355890274048 and perplexity is 87.56276848251957
At time: 192.47294902801514 and batch: 600, loss is 4.482709846496582 and perplexity is 88.47409935253492
At time: 193.31064915657043 and batch: 650, loss is 4.488260908126831 and perplexity is 88.96659018957052
At time: 194.15143823623657 and batch: 700, loss is 4.470486841201782 and perplexity is 87.39926221953553
At time: 194.9937448501587 and batch: 750, loss is 4.431758737564087 and perplexity is 84.0791601286872
At time: 195.8412959575653 and batch: 800, loss is 4.436983823776245 and perplexity is 84.5196307353261
At time: 196.6886327266693 and batch: 850, loss is 4.458187351226806 and perplexity is 86.33087961907066
At time: 197.53810572624207 and batch: 900, loss is 4.39845157623291 and perplexity is 81.32484579765816
At time: 198.37639093399048 and batch: 950, loss is 4.38053484916687 and perplexity is 79.8807461321508
At time: 199.21191215515137 and batch: 1000, loss is 4.359263496398926 and perplexity is 78.19951898136645
At time: 200.05397582054138 and batch: 1050, loss is 4.346941003799438 and perplexity is 77.24181873292206
At time: 200.8962438106537 and batch: 1100, loss is 4.279015560150146 and perplexity is 72.16935863282053
At time: 201.76627254486084 and batch: 1150, loss is 4.291365871429443 and perplexity is 73.0661994085585
At time: 202.6067831516266 and batch: 1200, loss is 4.2850604915618895 and perplexity is 72.60693869403998
At time: 203.44578766822815 and batch: 1250, loss is 4.325801191329956 and perplexity is 75.62607955298839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.790223950017108 and perplexity of 120.32831317378486
Finished 9 epochs...
Completing Train Step...
At time: 205.87402749061584 and batch: 50, loss is 4.48966272354126 and perplexity is 89.09139238144834
At time: 206.72168922424316 and batch: 100, loss is 4.476970291137695 and perplexity is 87.96775185624469
At time: 207.563823223114 and batch: 150, loss is 4.3931847667694095 and perplexity is 80.89764929898422
At time: 208.4066038131714 and batch: 200, loss is 4.454386873245239 and perplexity is 86.00340368864234
At time: 209.2505099773407 and batch: 250, loss is 4.4561283397674565 and perplexity is 86.1533062242023
At time: 210.09468579292297 and batch: 300, loss is 4.456559925079346 and perplexity is 86.19049675060457
At time: 210.93799662590027 and batch: 350, loss is 4.434069080352783 and perplexity is 84.27363637686457
At time: 211.78262066841125 and batch: 400, loss is 4.427918815612793 and perplexity is 83.75692179703466
At time: 212.62783217430115 and batch: 450, loss is 4.374800338745117 and perplexity is 79.42398007752261
At time: 213.47242259979248 and batch: 500, loss is 4.390798263549804 and perplexity is 80.70481698746124
At time: 214.32009553909302 and batch: 550, loss is 4.382651357650757 and perplexity is 80.04999345255334
At time: 215.16594076156616 and batch: 600, loss is 4.398498239517212 and perplexity is 81.32864077060073
At time: 216.0112636089325 and batch: 650, loss is 4.4121018981933595 and perplexity is 82.4425674005626
At time: 216.8536992073059 and batch: 700, loss is 4.396636295318603 and perplexity is 81.17735226881895
At time: 217.6970133781433 and batch: 750, loss is 4.362252578735352 and perplexity is 78.43361347182864
At time: 218.53990769386292 and batch: 800, loss is 4.378363513946534 and perplexity is 79.70748642517155
At time: 219.3888611793518 and batch: 850, loss is 4.403297595977783 and perplexity is 81.71990406309402
At time: 220.236829996109 and batch: 900, loss is 4.349582262039185 and perplexity is 77.44610398939987
At time: 221.0807204246521 and batch: 950, loss is 4.337120914459229 and perplexity is 76.4870093855512
At time: 221.92344427108765 and batch: 1000, loss is 4.3224641132354735 and perplexity is 75.3741300409503
At time: 222.76960134506226 and batch: 1050, loss is 4.317855758666992 and perplexity is 75.02757845403266
At time: 223.67028403282166 and batch: 1100, loss is 4.256715259552002 and perplexity is 70.57777262244504
At time: 224.51934909820557 and batch: 1150, loss is 4.2779566669464115 and perplexity is 72.09297943528966
At time: 225.36326479911804 and batch: 1200, loss is 4.280418243408203 and perplexity is 72.27066041446662
At time: 226.20504927635193 and batch: 1250, loss is 4.325882778167725 and perplexity is 75.63224989737802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.782784789148038 and perplexity of 119.43649280543181
Finished 10 epochs...
Completing Train Step...
At time: 228.63905882835388 and batch: 50, loss is 4.434993505477905 and perplexity is 84.35157706333075
At time: 229.51225090026855 and batch: 100, loss is 4.420016813278198 and perplexity is 83.09768249105188
At time: 230.35333395004272 and batch: 150, loss is 4.339125261306763 and perplexity is 76.6404696240871
At time: 231.2002260684967 and batch: 200, loss is 4.400916376113892 and perplexity is 81.52554250490446
At time: 232.0411353111267 and batch: 250, loss is 4.402312021255494 and perplexity is 81.63940266792792
At time: 232.8850588798523 and batch: 300, loss is 4.40548110961914 and perplexity is 81.89853553954678
At time: 233.72519612312317 and batch: 350, loss is 4.38332498550415 and perplexity is 80.1039355242081
At time: 234.56539011001587 and batch: 400, loss is 4.379300842285156 and perplexity is 79.7822335368186
At time: 235.40602946281433 and batch: 450, loss is 4.3289045715332035 and perplexity is 75.86114058434258
At time: 236.2464964389801 and batch: 500, loss is 4.344739732742309 and perplexity is 77.0719755569616
At time: 237.08695578575134 and batch: 550, loss is 4.337892646789551 and perplexity is 76.54605966611827
At time: 237.92916917800903 and batch: 600, loss is 4.355465774536133 and perplexity is 77.90310216903596
At time: 238.7717366218567 and batch: 650, loss is 4.370832366943359 and perplexity is 79.10945239535968
At time: 239.61562657356262 and batch: 700, loss is 4.355799522399902 and perplexity is 77.92910650216983
At time: 240.45677065849304 and batch: 750, loss is 4.3238427734375 and perplexity is 75.47811701918363
At time: 241.29973912239075 and batch: 800, loss is 4.343000478744507 and perplexity is 76.93804431933776
At time: 242.1440348625183 and batch: 850, loss is 4.372362279891968 and perplexity is 79.23057560129185
At time: 242.98539471626282 and batch: 900, loss is 4.318310012817383 and perplexity is 75.06166778496303
At time: 243.82982277870178 and batch: 950, loss is 4.309457397460937 and perplexity is 74.4001082983475
At time: 244.70393109321594 and batch: 1000, loss is 4.296523828506469 and perplexity is 73.44404534775686
At time: 245.54690980911255 and batch: 1050, loss is 4.2953855991363525 and perplexity is 73.36049673604393
At time: 246.3898630142212 and batch: 1100, loss is 4.239029760360718 and perplexity is 69.34054224812051
At time: 247.23415756225586 and batch: 1150, loss is 4.2622599697113035 and perplexity is 70.97019283813
At time: 248.07968997955322 and batch: 1200, loss is 4.270298309326172 and perplexity is 71.54297437276388
At time: 248.9294261932373 and batch: 1250, loss is 4.315532169342041 and perplexity is 74.8534475563528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.780681526573905 and perplexity of 119.18555049142516
Finished 11 epochs...
Completing Train Step...
At time: 251.41794061660767 and batch: 50, loss is 4.394482440948487 and perplexity is 81.00269623323341
At time: 252.29964804649353 and batch: 100, loss is 4.378794240951538 and perplexity is 79.74182598703331
At time: 253.1457908153534 and batch: 150, loss is 4.299905061721802 and perplexity is 73.69279710020619
At time: 253.9872715473175 and batch: 200, loss is 4.362507610321045 and perplexity is 78.45361907156548
At time: 254.83061718940735 and batch: 250, loss is 4.36614499092102 and perplexity is 78.73950436518336
At time: 255.6786665916443 and batch: 300, loss is 4.369057760238648 and perplexity is 78.96918872396886
At time: 256.524209022522 and batch: 350, loss is 4.345954475402832 and perplexity is 77.16565506032651
At time: 257.36957573890686 and batch: 400, loss is 4.345459461212158 and perplexity is 77.12746641877858
At time: 258.21652269363403 and batch: 450, loss is 4.293922166824341 and perplexity is 73.25321713206078
At time: 259.06101632118225 and batch: 500, loss is 4.311434545516968 and perplexity is 74.54735384299259
At time: 259.9048840999603 and batch: 550, loss is 4.305834650993347 and perplexity is 74.1310632041558
At time: 260.750417470932 and batch: 600, loss is 4.321682825088501 and perplexity is 75.31526412517641
At time: 261.5944540500641 and batch: 650, loss is 4.339568881988526 and perplexity is 76.6744764639834
At time: 262.4382965564728 and batch: 700, loss is 4.325000410079956 and perplexity is 75.56554384763514
At time: 263.2834231853485 and batch: 750, loss is 4.29486252784729 and perplexity is 73.32213400054734
At time: 264.1274425983429 and batch: 800, loss is 4.317146282196045 and perplexity is 74.97436703080582
At time: 264.97299575805664 and batch: 850, loss is 4.34858847618103 and perplexity is 77.36917737710687
At time: 265.8218502998352 and batch: 900, loss is 4.292735080718995 and perplexity is 73.16631084865762
At time: 266.6996121406555 and batch: 950, loss is 4.2890528869628906 and perplexity is 72.89739372201964
At time: 267.54593086242676 and batch: 1000, loss is 4.276716108322144 and perplexity is 72.00359932000137
At time: 268.3903865814209 and batch: 1050, loss is 4.27649772644043 and perplexity is 71.9878767553158
At time: 269.2379493713379 and batch: 1100, loss is 4.222507972717285 and perplexity is 68.20432455369253
At time: 270.08405447006226 and batch: 1150, loss is 4.245592203140259 and perplexity is 69.797081958358
At time: 270.92846155166626 and batch: 1200, loss is 4.257113027572632 and perplexity is 70.60585178748825
At time: 271.7722179889679 and batch: 1250, loss is 4.301650447845459 and perplexity is 73.82153179887557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.780347838018932 and perplexity of 119.14578627210949
Finished 12 epochs...
Completing Train Step...
At time: 274.21231055259705 and batch: 50, loss is 4.36199631690979 and perplexity is 78.41351650600751
At time: 275.0530083179474 and batch: 100, loss is 4.345504941940308 and perplexity is 77.13097431188183
At time: 275.89351415634155 and batch: 150, loss is 4.268766260147094 and perplexity is 71.43345093667503
At time: 276.73408937454224 and batch: 200, loss is 4.331768016815186 and perplexity is 76.07867611133693
At time: 277.57746267318726 and batch: 250, loss is 4.335752925872803 and perplexity is 76.38244756569632
At time: 278.4215667247772 and batch: 300, loss is 4.337978057861328 and perplexity is 76.55259782632632
At time: 279.26956129074097 and batch: 350, loss is 4.315229864120483 and perplexity is 74.83082238833043
At time: 280.11088609695435 and batch: 400, loss is 4.317712726593018 and perplexity is 75.01684787130817
At time: 280.95400881767273 and batch: 450, loss is 4.265356092453003 and perplexity is 71.1902657767729
At time: 281.79261922836304 and batch: 500, loss is 4.283329744338989 and perplexity is 72.48138312035385
At time: 282.639835357666 and batch: 550, loss is 4.279519929885864 and perplexity is 72.20576785424394
At time: 283.48167729377747 and batch: 600, loss is 4.297080907821655 and perplexity is 73.48497090457587
At time: 284.32034611701965 and batch: 650, loss is 4.3157300281524655 and perplexity is 74.86825943572457
At time: 285.1623649597168 and batch: 700, loss is 4.3006591796875 and perplexity is 73.74839112203267
At time: 286.0022852420807 and batch: 750, loss is 4.270875568389893 and perplexity is 71.58428512552216
At time: 286.8447811603546 and batch: 800, loss is 4.29368040561676 and perplexity is 73.23550948642486
At time: 287.6886057853699 and batch: 850, loss is 4.327959632873535 and perplexity is 75.78949031772275
At time: 288.56340193748474 and batch: 900, loss is 4.271374931335449 and perplexity is 71.62004059173246
At time: 289.40418910980225 and batch: 950, loss is 4.270380334854126 and perplexity is 71.54884296369204
At time: 290.24946665763855 and batch: 1000, loss is 4.256753735542297 and perplexity is 70.58048822438191
At time: 291.09823203086853 and batch: 1050, loss is 4.259447813034058 and perplexity is 70.77089389771957
At time: 291.9450182914734 and batch: 1100, loss is 4.206875905990601 and perplexity is 67.14644000425488
At time: 292.78881072998047 and batch: 1150, loss is 4.229768257141114 and perplexity is 68.70130929108619
At time: 293.63333559036255 and batch: 1200, loss is 4.243504886627197 and perplexity is 69.65154529999855
At time: 294.47757482528687 and batch: 1250, loss is 4.288147706985473 and perplexity is 72.83143831607637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.779934402799954 and perplexity of 119.0965373891846
Finished 13 epochs...
Completing Train Step...
At time: 296.91632771492004 and batch: 50, loss is 4.334511423110962 and perplexity is 76.28767738697935
At time: 297.78702449798584 and batch: 100, loss is 4.31648551940918 and perplexity is 74.92484312268272
At time: 298.63283491134644 and batch: 150, loss is 4.2433824634552 and perplexity is 69.64301885881605
At time: 299.4764926433563 and batch: 200, loss is 4.306679849624634 and perplexity is 74.19374516293318
At time: 300.32072401046753 and batch: 250, loss is 4.309785451889038 and perplexity is 74.42451958722273
At time: 301.166490316391 and batch: 300, loss is 4.312451095581054 and perplexity is 74.62317349102405
At time: 302.00955629348755 and batch: 350, loss is 4.289875841140747 and perplexity is 72.95740962852133
At time: 302.85438680648804 and batch: 400, loss is 4.292826108932495 and perplexity is 73.17297135036476
At time: 303.69968485832214 and batch: 450, loss is 4.241129598617554 and perplexity is 69.48629915088577
At time: 304.5438439846039 and batch: 500, loss is 4.260520725250244 and perplexity is 70.84686560253014
At time: 305.3903398513794 and batch: 550, loss is 4.257566533088684 and perplexity is 70.63787919249295
At time: 306.237806558609 and batch: 600, loss is 4.275354452133179 and perplexity is 71.90562189428199
At time: 307.08293199539185 and batch: 650, loss is 4.295521335601807 and perplexity is 73.37045510641654
At time: 307.92462038993835 and batch: 700, loss is 4.281129512786865 and perplexity is 72.32208260754291
At time: 308.76658272743225 and batch: 750, loss is 4.251240873336792 and perplexity is 70.1924582789527
At time: 309.66594195365906 and batch: 800, loss is 4.274509515762329 and perplexity is 71.84489187918723
At time: 310.50942039489746 and batch: 850, loss is 4.310668649673462 and perplexity is 74.49028019356528
At time: 311.3547179698944 and batch: 900, loss is 4.254461488723755 and perplexity is 70.41888561209568
At time: 312.1978929042816 and batch: 950, loss is 4.253112277984619 and perplexity is 70.32393976077404
At time: 313.0413272380829 and batch: 1000, loss is 4.240734543800354 and perplexity is 69.45885367526745
At time: 313.8882613182068 and batch: 1050, loss is 4.24393967628479 and perplexity is 69.68183565601993
At time: 314.73368072509766 and batch: 1100, loss is 4.193056974411011 and perplexity is 66.22492975205907
At time: 315.5747420787811 and batch: 1150, loss is 4.21517463684082 and perplexity is 67.70598879853085
At time: 316.424546957016 and batch: 1200, loss is 4.229494924545288 and perplexity is 68.68253355000897
At time: 317.27456188201904 and batch: 1250, loss is 4.273325972557068 and perplexity is 71.75991064497201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.782235918253877 and perplexity of 119.37095557821385
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 319.72800278663635 and batch: 50, loss is 4.322467212677002 and perplexity is 75.37436365902116
At time: 320.56997895240784 and batch: 100, loss is 4.31420557975769 and perplexity is 74.75421358839893
At time: 321.41885209083557 and batch: 150, loss is 4.240593385696411 and perplexity is 69.44904968715308
At time: 322.2678339481354 and batch: 200, loss is 4.301224346160889 and perplexity is 73.79008302048307
At time: 323.1104402542114 and batch: 250, loss is 4.305701704025268 and perplexity is 74.12120835916274
At time: 323.9737253189087 and batch: 300, loss is 4.301541471481324 and perplexity is 73.81348743507594
At time: 324.82059478759766 and batch: 350, loss is 4.274227724075318 and perplexity is 71.82464943811121
At time: 325.70441722869873 and batch: 400, loss is 4.273083457946777 and perplexity is 71.74250992825677
At time: 326.5526354312897 and batch: 450, loss is 4.2199138832092284 and perplexity is 68.02762571635203
At time: 327.3937051296234 and batch: 500, loss is 4.231845998764038 and perplexity is 68.84420125583557
At time: 328.23679757118225 and batch: 550, loss is 4.225006518363952 and perplexity is 68.3749492399099
At time: 329.0762298107147 and batch: 600, loss is 4.239833555221558 and perplexity is 69.39630022561323
At time: 329.91905331611633 and batch: 650, loss is 4.257633962631226 and perplexity is 70.64264243296276
At time: 330.7639362812042 and batch: 700, loss is 4.240088443756104 and perplexity is 69.41399080134737
At time: 331.6708827018738 and batch: 750, loss is 4.203045854568481 and perplexity is 66.88975755349821
At time: 332.51263189315796 and batch: 800, loss is 4.221038608551026 and perplexity is 68.10418115478808
At time: 333.35336899757385 and batch: 850, loss is 4.24692403793335 and perplexity is 69.8901020705275
At time: 334.1956903934479 and batch: 900, loss is 4.185869479179383 and perplexity is 65.75064488715176
At time: 335.0371677875519 and batch: 950, loss is 4.1803332042694095 and perplexity is 65.38763702440218
At time: 335.87930393218994 and batch: 1000, loss is 4.160174069404602 and perplexity is 64.08267646247246
At time: 336.7208788394928 and batch: 1050, loss is 4.156114091873169 and perplexity is 63.82302967260269
At time: 337.5634322166443 and batch: 1100, loss is 4.099995222091675 and perplexity is 60.33999929768824
At time: 338.4051432609558 and batch: 1150, loss is 4.1144488382339475 and perplexity is 61.21846368603241
At time: 339.2495324611664 and batch: 1200, loss is 4.1263887357711795 and perplexity is 61.95378697651233
At time: 340.0956914424896 and batch: 1250, loss is 4.178335137367249 and perplexity is 65.2571185867605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7579991695654655 and perplexity of 116.51257062735777
Finished 15 epochs...
Completing Train Step...
At time: 342.5834798812866 and batch: 50, loss is 4.29502724647522 and perplexity is 73.33421251660782
At time: 343.4534556865692 and batch: 100, loss is 4.282567720413208 and perplexity is 72.42617161115966
At time: 344.2950437068939 and batch: 150, loss is 4.21133249759674 and perplexity is 67.44635206155407
At time: 345.1325104236603 and batch: 200, loss is 4.271512231826782 and perplexity is 71.62987473359578
At time: 345.97207522392273 and batch: 250, loss is 4.278685111999511 and perplexity is 72.14551434159577
At time: 346.81522250175476 and batch: 300, loss is 4.275720834732056 and perplexity is 71.93197168966397
At time: 347.66583228111267 and batch: 350, loss is 4.250224666595459 and perplexity is 70.12116446042367
At time: 348.5126414299011 and batch: 400, loss is 4.251046380996704 and perplexity is 70.17880771099396
At time: 349.35423254966736 and batch: 450, loss is 4.198436393737793 and perplexity is 66.58214135315751
At time: 350.1930248737335 and batch: 500, loss is 4.2124491739273076 and perplexity is 67.52170987379621
At time: 351.03421211242676 and batch: 550, loss is 4.2061479377746585 and perplexity is 67.09757731751739
At time: 351.8738477230072 and batch: 600, loss is 4.222245597839356 and perplexity is 68.18643179977074
At time: 352.71204590797424 and batch: 650, loss is 4.242175579071045 and perplexity is 69.5590184864548
At time: 353.6114115715027 and batch: 700, loss is 4.226132822036743 and perplexity is 68.45200358151402
At time: 354.45417499542236 and batch: 750, loss is 4.190880017280579 and perplexity is 66.08091772989665
At time: 355.2942831516266 and batch: 800, loss is 4.21086582660675 and perplexity is 67.41488414881836
At time: 356.13497710227966 and batch: 850, loss is 4.2389743041992185 and perplexity is 69.33669699443364
At time: 356.974232673645 and batch: 900, loss is 4.179679207801819 and perplexity is 65.34488772123444
At time: 357.812349319458 and batch: 950, loss is 4.176082057952881 and perplexity is 65.1102546264425
At time: 358.6578998565674 and batch: 1000, loss is 4.158391389846802 and perplexity is 63.96853935030884
At time: 359.4959478378296 and batch: 1050, loss is 4.1573491334915165 and perplexity is 63.90190246602302
At time: 360.33724308013916 and batch: 1100, loss is 4.103520956039429 and perplexity is 60.55311755999654
At time: 361.17381525039673 and batch: 1150, loss is 4.120614972114563 and perplexity is 61.59711112476784
At time: 362.0113170146942 and batch: 1200, loss is 4.1347336053848265 and perplexity is 62.472946397024074
At time: 362.8497579097748 and batch: 1250, loss is 4.1853985023498534 and perplexity is 65.71968514811546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.756937514256387 and perplexity of 116.38894007630437
Finished 16 epochs...
Completing Train Step...
At time: 365.2827353477478 and batch: 50, loss is 4.284312763214111 and perplexity is 72.55266871986063
At time: 366.1630516052246 and batch: 100, loss is 4.2699431133270265 and perplexity is 71.51756710704637
At time: 367.01070976257324 and batch: 150, loss is 4.199310626983642 and perplexity is 66.64037512594949
At time: 367.85758566856384 and batch: 200, loss is 4.258332605361939 and perplexity is 70.69201364598827
At time: 368.7013418674469 and batch: 250, loss is 4.266203842163086 and perplexity is 71.25064289267114
At time: 369.5483591556549 and batch: 300, loss is 4.263562507629395 and perplexity is 71.06269443569686
At time: 370.3923645019531 and batch: 350, loss is 4.238728485107422 and perplexity is 69.31965480528383
At time: 371.23999094963074 and batch: 400, loss is 4.240368099212646 and perplexity is 69.43340551722443
At time: 372.0844044685364 and batch: 450, loss is 4.187887334823609 and perplexity is 65.8834541469423
At time: 372.93602871894836 and batch: 500, loss is 4.202411198616028 and perplexity is 66.84731903906112
At time: 373.7814016342163 and batch: 550, loss is 4.196472082138062 and perplexity is 66.45148165075986
At time: 374.6297256946564 and batch: 600, loss is 4.21339075088501 and perplexity is 67.58531670061996
At time: 375.5294723510742 and batch: 650, loss is 4.234350290298462 and perplexity is 69.01682326393524
At time: 376.38023710250854 and batch: 700, loss is 4.219094095230102 and perplexity is 67.97188033935822
At time: 377.2267973423004 and batch: 750, loss is 4.1848035383224484 and perplexity is 65.68059592905263
At time: 378.0743975639343 and batch: 800, loss is 4.205832476615906 and perplexity is 67.0764139763089
At time: 378.9187240600586 and batch: 850, loss is 4.2350806140899655 and perplexity is 69.06724630231
At time: 379.76274132728577 and batch: 900, loss is 4.176442422866821 and perplexity is 65.13372230595779
At time: 380.6070008277893 and batch: 950, loss is 4.17358018875122 and perplexity is 64.94756088976794
At time: 381.45556020736694 and batch: 1000, loss is 4.1573091173172 and perplexity is 63.89934540751698
At time: 382.30258226394653 and batch: 1050, loss is 4.157826552391052 and perplexity is 63.93241772567781
At time: 383.1458282470703 and batch: 1100, loss is 4.104834198951721 and perplexity is 60.63269075047759
At time: 383.99098443984985 and batch: 1150, loss is 4.123081936836242 and perplexity is 61.7492566164883
At time: 384.8366675376892 and batch: 1200, loss is 4.137888469696045 and perplexity is 62.670351295128995
At time: 385.6812686920166 and batch: 1250, loss is 4.187792129516602 and perplexity is 65.87718199103956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.756913902115648 and perplexity of 116.38619191671596
Finished 17 epochs...
Completing Train Step...
At time: 388.17374634742737 and batch: 50, loss is 4.27598313331604 and perplexity is 71.95084181867725
At time: 389.0161044597626 and batch: 100, loss is 4.260769491195679 and perplexity is 70.86449208237586
At time: 389.8569166660309 and batch: 150, loss is 4.19070864200592 and perplexity is 66.06959406479685
At time: 390.70419216156006 and batch: 200, loss is 4.249142456054687 and perplexity is 70.04531964454533
At time: 391.5483922958374 and batch: 250, loss is 4.257427163124085 and perplexity is 70.62803507977328
At time: 392.38963198661804 and batch: 300, loss is 4.254890375137329 and perplexity is 70.44909379286983
At time: 393.2320046424866 and batch: 350, loss is 4.230384788513184 and perplexity is 68.74367886330957
At time: 394.0731966495514 and batch: 400, loss is 4.232666616439819 and perplexity is 68.90071921090592
At time: 394.91544556617737 and batch: 450, loss is 4.180208683013916 and perplexity is 65.37949538066053
At time: 395.75626730918884 and batch: 500, loss is 4.19532386302948 and perplexity is 66.37522457802709
At time: 396.66129326820374 and batch: 550, loss is 4.189472646713257 and perplexity is 65.98798280350164
At time: 397.5007019042969 and batch: 600, loss is 4.2072671699523925 and perplexity is 67.17271712670144
At time: 398.34010100364685 and batch: 650, loss is 4.228667869567871 and perplexity is 68.62575280241134
At time: 399.18312430381775 and batch: 700, loss is 4.213767557144165 and perplexity is 67.61078806956402
At time: 400.0307602882385 and batch: 750, loss is 4.180182585716247 and perplexity is 65.37778917477193
At time: 400.87433528900146 and batch: 800, loss is 4.201728501319885 and perplexity is 66.80169812950645
At time: 401.7149715423584 and batch: 850, loss is 4.231700172424317 and perplexity is 68.83416268991681
At time: 402.56168484687805 and batch: 900, loss is 4.173479013442993 and perplexity is 64.9409901326811
At time: 403.40984082221985 and batch: 950, loss is 4.171323919296265 and perplexity is 64.80118688365414
At time: 404.25687551498413 and batch: 1000, loss is 4.155776290893555 and perplexity is 63.80147383165441
At time: 405.09764647483826 and batch: 1050, loss is 4.157175288200379 and perplexity is 63.89079438675617
At time: 405.93939185142517 and batch: 1100, loss is 4.104560480117798 and perplexity is 60.61609671222176
At time: 406.78453826904297 and batch: 1150, loss is 4.123498644828796 and perplexity is 61.77499338724127
At time: 407.6259889602661 and batch: 1200, loss is 4.1392922115325925 and perplexity is 62.75838606374101
At time: 408.46739196777344 and batch: 1250, loss is 4.188339476585388 and perplexity is 65.91324954333632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.757146904938413 and perplexity of 116.41331338753032
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 410.93890929222107 and batch: 50, loss is 4.2718479347229 and perplexity is 71.65392512665986
At time: 411.8162899017334 and batch: 100, loss is 4.259541425704956 and perplexity is 70.77751926022331
At time: 412.6627423763275 and batch: 150, loss is 4.190460739135742 and perplexity is 66.05321725280987
At time: 413.5104262828827 and batch: 200, loss is 4.2475150680541995 and perplexity is 69.93142143529224
At time: 414.36240005493164 and batch: 250, loss is 4.256289358139038 and perplexity is 70.54771984957479
At time: 415.2180519104004 and batch: 300, loss is 4.253496952056885 and perplexity is 70.35099676078914
At time: 416.0687153339386 and batch: 350, loss is 4.227570238113404 and perplexity is 68.55046834240314
At time: 416.913330078125 and batch: 400, loss is 4.228009643554688 and perplexity is 68.58059640992902
At time: 417.7577259540558 and batch: 450, loss is 4.175309510231018 and perplexity is 65.05997327243445
At time: 418.6595530509949 and batch: 500, loss is 4.189544129371643 and perplexity is 65.99269996852972
At time: 419.5048668384552 and batch: 550, loss is 4.182213644981385 and perplexity is 65.51071027883566
At time: 420.3475675582886 and batch: 600, loss is 4.198693256378174 and perplexity is 66.59924601446794
At time: 421.19315934181213 and batch: 650, loss is 4.219913654327392 and perplexity is 68.02761014606594
At time: 422.03739070892334 and batch: 700, loss is 4.20465669631958 and perplexity is 66.99759319744724
At time: 422.88485193252563 and batch: 750, loss is 4.1698583889007566 and perplexity is 64.7062883299537
At time: 423.72883105278015 and batch: 800, loss is 4.190052962303161 and perplexity is 66.0262877720791
At time: 424.5704803466797 and batch: 850, loss is 4.215922508239746 and perplexity is 67.75664311018377
At time: 425.4171464443207 and batch: 900, loss is 4.156451983451843 and perplexity is 63.84459858062551
At time: 426.26273107528687 and batch: 950, loss is 4.153188252449036 and perplexity is 63.6365666496694
At time: 427.10610151290894 and batch: 1000, loss is 4.13589225769043 and perplexity is 62.54537277079476
At time: 427.9511318206787 and batch: 1050, loss is 4.135228071212769 and perplexity is 62.50384477265417
At time: 428.7949845790863 and batch: 1100, loss is 4.081947417259216 and perplexity is 59.26076298415535
At time: 429.64564180374146 and batch: 1150, loss is 4.097782888412476 and perplexity is 60.206654640863796
At time: 430.48794627189636 and batch: 1200, loss is 4.114719753265381 and perplexity is 61.23505093481227
At time: 431.335435628891 and batch: 1250, loss is 4.167399291992187 and perplexity is 64.54736478064721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.754811085053604 and perplexity of 116.14171018683193
Finished 19 epochs...
Completing Train Step...
At time: 433.7932639122009 and batch: 50, loss is 4.268031921386719 and perplexity is 71.38101384051733
At time: 434.6318316459656 and batch: 100, loss is 4.254296998977662 and perplexity is 70.40730338008649
At time: 435.47102451324463 and batch: 150, loss is 4.185368747711181 and perplexity is 65.71772971172203
At time: 436.31232213974 and batch: 200, loss is 4.24243348121643 and perplexity is 69.5769602200597
At time: 437.15112113952637 and batch: 250, loss is 4.251491026878357 and perplexity is 70.21001936737497
At time: 437.98967933654785 and batch: 300, loss is 4.248688697814941 and perplexity is 70.01354321355491
At time: 438.83290123939514 and batch: 350, loss is 4.223433046340943 and perplexity is 68.26744776766517
At time: 439.6736192703247 and batch: 400, loss is 4.224162645339966 and perplexity is 68.31727380352503
At time: 440.57546520233154 and batch: 450, loss is 4.171515574455261 and perplexity is 64.8136075556344
At time: 441.4178762435913 and batch: 500, loss is 4.186081743240356 and perplexity is 65.7646028673835
At time: 442.25912404060364 and batch: 550, loss is 4.178942551612854 and perplexity is 65.29676873102942
At time: 443.09946179389954 and batch: 600, loss is 4.195914611816407 and perplexity is 66.41444724565952
At time: 443.9403409957886 and batch: 650, loss is 4.2175288581848145 and perplexity is 67.8655714552195
At time: 444.7845721244812 and batch: 700, loss is 4.202473287582397 and perplexity is 66.851469648857
At time: 445.627721786499 and batch: 750, loss is 4.167928857803345 and perplexity is 64.58155591066306
At time: 446.46751618385315 and batch: 800, loss is 4.188714027404785 and perplexity is 65.93794202896859
At time: 447.3079717159271 and batch: 850, loss is 4.2150763988494875 and perplexity is 67.69933782488458
At time: 448.1487638950348 and batch: 900, loss is 4.155805621147156 and perplexity is 63.80334517250533
At time: 448.9889187812805 and batch: 950, loss is 4.153089327812195 and perplexity is 63.63027173678988
At time: 449.82841300964355 and batch: 1000, loss is 4.136207313537597 and perplexity is 62.565081160658174
At time: 450.6678340435028 and batch: 1050, loss is 4.136351275444031 and perplexity is 62.57408879738093
At time: 451.50943517684937 and batch: 1100, loss is 4.083375325202942 and perplexity is 59.34544234114707
At time: 452.3501784801483 and batch: 1150, loss is 4.0997298192977905 and perplexity is 60.3239870182375
At time: 453.19170236587524 and batch: 1200, loss is 4.117065234184265 and perplexity is 61.37884514571472
At time: 454.0312430858612 and batch: 1250, loss is 4.1692881011962895 and perplexity is 64.66939764946798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.754483633667883 and perplexity of 116.10368564882481
Finished 20 epochs...
Completing Train Step...
At time: 456.43466353416443 and batch: 50, loss is 4.265688676834106 and perplexity is 71.21394648495537
At time: 457.3061089515686 and batch: 100, loss is 4.2514919185638425 and perplexity is 70.21008197265809
At time: 458.14876317977905 and batch: 150, loss is 4.182747349739075 and perplexity is 65.54568298831173
At time: 458.9917447566986 and batch: 200, loss is 4.239500331878662 and perplexity is 69.37317961084568
At time: 459.8344306945801 and batch: 250, loss is 4.248715772628784 and perplexity is 70.01543884286565
At time: 460.676949262619 and batch: 300, loss is 4.24591061592102 and perplexity is 69.81930977993686
At time: 461.5200915336609 and batch: 350, loss is 4.220864934921265 and perplexity is 68.09235428148276
At time: 462.3943557739258 and batch: 400, loss is 4.221750001907349 and perplexity is 68.15264725398595
At time: 463.2427763938904 and batch: 450, loss is 4.169127149581909 and perplexity is 64.65898984311438
At time: 464.0856158733368 and batch: 500, loss is 4.183929476737976 and perplexity is 65.6232121254483
At time: 464.93115305900574 and batch: 550, loss is 4.176994304656983 and perplexity is 65.16967834205207
At time: 465.7723774909973 and batch: 600, loss is 4.1942045116424564 and perplexity is 66.30096894515769
At time: 466.61127829551697 and batch: 650, loss is 4.216034297943115 and perplexity is 67.76421802860938
At time: 467.4550025463104 and batch: 700, loss is 4.2011024951934814 and perplexity is 66.75989294373963
At time: 468.2978813648224 and batch: 750, loss is 4.166800723075867 and perplexity is 64.50874029532051
At time: 469.14237332344055 and batch: 800, loss is 4.187898707389832 and perplexity is 65.88420341514812
At time: 469.98392510414124 and batch: 850, loss is 4.214599990844727 and perplexity is 67.66709299988405
At time: 470.82674074172974 and batch: 900, loss is 4.15548321723938 and perplexity is 63.78277804033256
At time: 471.67354583740234 and batch: 950, loss is 4.15308123588562 and perplexity is 63.62975684738626
At time: 472.5159411430359 and batch: 1000, loss is 4.136512308120728 and perplexity is 62.58416608175612
At time: 473.3568515777588 and batch: 1050, loss is 4.137138843536377 and perplexity is 62.623389564432635
At time: 474.2017035484314 and batch: 1100, loss is 4.084375891685486 and perplexity is 59.404851117904414
At time: 475.0483224391937 and batch: 1150, loss is 4.100963478088379 and perplexity is 60.39845215794886
At time: 475.8879599571228 and batch: 1200, loss is 4.118526935577393 and perplexity is 61.46862829126156
At time: 476.7352592945099 and batch: 1250, loss is 4.17036880493164 and perplexity is 64.73932388703803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.754372255645529 and perplexity of 116.09075497003928
Finished 21 epochs...
Completing Train Step...
At time: 479.2653272151947 and batch: 50, loss is 4.263730926513672 and perplexity is 71.07466374330785
At time: 480.1205904483795 and batch: 100, loss is 4.249246511459351 and perplexity is 70.05260861784768
At time: 480.9658200740814 and batch: 150, loss is 4.180683808326721 and perplexity is 65.41056621453964
At time: 481.81077575683594 and batch: 200, loss is 4.23712703704834 and perplexity is 69.20873182104499
At time: 482.6527078151703 and batch: 250, loss is 4.246531991958618 and perplexity is 69.86270730769132
At time: 483.49928426742554 and batch: 300, loss is 4.243758974075317 and perplexity is 69.66924513195877
At time: 484.37328147888184 and batch: 350, loss is 4.2188288021087645 and perplexity is 67.95385025879374
At time: 485.2145142555237 and batch: 400, loss is 4.219816770553589 and perplexity is 68.02101969373125
At time: 486.06681871414185 and batch: 450, loss is 4.167216448783875 and perplexity is 64.53556381227894
At time: 486.91359066963196 and batch: 500, loss is 4.182233772277832 and perplexity is 65.51202884559146
At time: 487.76153540611267 and batch: 550, loss is 4.175477066040039 and perplexity is 65.07087536221974
At time: 488.6026711463928 and batch: 600, loss is 4.192824354171753 and perplexity is 66.20952628470391
At time: 489.4431734085083 and batch: 650, loss is 4.214834623336792 and perplexity is 67.68297176130923
At time: 490.28595900535583 and batch: 700, loss is 4.199994802474976 and perplexity is 66.6859844379515
At time: 491.13311743736267 and batch: 750, loss is 4.165894083976745 and perplexity is 64.45028065403639
At time: 491.9720456600189 and batch: 800, loss is 4.187182912826538 and perplexity is 65.8370607347866
At time: 492.8109972476959 and batch: 850, loss is 4.21415373802185 and perplexity is 67.63690310528085
At time: 493.6539857387543 and batch: 900, loss is 4.155142631530762 and perplexity is 63.761058236613394
At time: 494.4950752258301 and batch: 950, loss is 4.153018651008606 and perplexity is 63.625774711491566
At time: 495.3367998600006 and batch: 1000, loss is 4.136692485809326 and perplexity is 62.595443368070775
At time: 496.18063139915466 and batch: 1050, loss is 4.137646541595459 and perplexity is 62.65519140995218
At time: 497.0258116722107 and batch: 1100, loss is 4.085032534599304 and perplexity is 59.443871702330476
At time: 497.8718719482422 and batch: 1150, loss is 4.101781039237976 and perplexity is 60.447851776782024
At time: 498.714289188385 and batch: 1200, loss is 4.119530215263366 and perplexity is 61.53032946394188
At time: 499.55570697784424 and batch: 1250, loss is 4.171026329994202 and perplexity is 64.78190561276885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.754253749429744 and perplexity of 116.07699830912124
Finished 22 epochs...
Completing Train Step...
At time: 502.0169475078583 and batch: 50, loss is 4.261986503601074 and perplexity is 70.95078754901772
At time: 502.85441994667053 and batch: 100, loss is 4.247305459976197 and perplexity is 69.91676478058343
At time: 503.6942150592804 and batch: 150, loss is 4.178887982368469 and perplexity is 65.29320563291762
At time: 504.5333766937256 and batch: 200, loss is 4.235026187896729 and perplexity is 69.06348733731042
At time: 505.40654373168945 and batch: 250, loss is 4.244644050598144 and perplexity is 69.73093504130628
At time: 506.24803590774536 and batch: 300, loss is 4.241928024291992 and perplexity is 69.54180095023187
At time: 507.08687353134155 and batch: 350, loss is 4.217066011428833 and perplexity is 67.83416736383016
At time: 507.9294834136963 and batch: 400, loss is 4.218159255981445 and perplexity is 67.90836724970194
At time: 508.7705113887787 and batch: 450, loss is 4.165579166412353 and perplexity is 64.42998732415917
At time: 509.6127188205719 and batch: 500, loss is 4.180762987136841 and perplexity is 65.4157455503859
At time: 510.4525339603424 and batch: 550, loss is 4.174208464622498 and perplexity is 64.98837869625211
At time: 511.2949950695038 and batch: 600, loss is 4.191632232666016 and perplexity is 66.13064351280167
At time: 512.1401567459106 and batch: 650, loss is 4.213789863586426 and perplexity is 67.61229624252519
At time: 512.9822881221771 and batch: 700, loss is 4.198995475769043 and perplexity is 66.61937663981018
At time: 513.8267939090729 and batch: 750, loss is 4.16509328365326 and perplexity is 64.39868950830004
At time: 514.665301322937 and batch: 800, loss is 4.186461873054505 and perplexity is 65.78960670570017
At time: 515.5050451755524 and batch: 850, loss is 4.21371015548706 and perplexity is 67.60690720967561
At time: 516.3436543941498 and batch: 900, loss is 4.154836678504944 and perplexity is 63.74155333186105
At time: 517.182538986206 and batch: 950, loss is 4.152823085784912 and perplexity is 63.61333293925609
At time: 518.0181937217712 and batch: 1000, loss is 4.13676233291626 and perplexity is 62.59981563139046
At time: 518.8562822341919 and batch: 1050, loss is 4.137969150543213 and perplexity is 62.67540779614243
At time: 519.6973447799683 and batch: 1100, loss is 4.085424346923828 and perplexity is 59.46716710729622
At time: 520.5372071266174 and batch: 1150, loss is 4.102344689369201 and perplexity is 60.48193282035831
At time: 521.3777372837067 and batch: 1200, loss is 4.120319228172303 and perplexity is 61.578896845806064
At time: 522.2132432460785 and batch: 1250, loss is 4.171482253074646 and perplexity is 64.8114479127293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7541628649635035 and perplexity of 116.06644919246975
Finished 23 epochs...
Completing Train Step...
At time: 524.6630289554596 and batch: 50, loss is 4.260381536483765 and perplexity is 70.83700520094246
At time: 525.55859541893 and batch: 100, loss is 4.245542421340942 and perplexity is 69.79360742051617
At time: 526.4049870967865 and batch: 150, loss is 4.177198510169983 and perplexity is 65.18298770852672
At time: 527.3119773864746 and batch: 200, loss is 4.233164339065552 and perplexity is 68.93502119351474
At time: 528.1579179763794 and batch: 250, loss is 4.242949676513672 and perplexity is 69.61288479097125
At time: 529.0085985660553 and batch: 300, loss is 4.24029390335083 and perplexity is 69.42825403697486
At time: 529.8552799224854 and batch: 350, loss is 4.2154850673675535 and perplexity is 67.72701006694946
At time: 530.700040102005 and batch: 400, loss is 4.216686992645264 and perplexity is 67.80846181198272
At time: 531.5469381809235 and batch: 450, loss is 4.16412691116333 and perplexity is 64.3364864468762
At time: 532.3900997638702 and batch: 500, loss is 4.1794093799591066 and perplexity is 65.32725822971915
At time: 533.234085559845 and batch: 550, loss is 4.173088178634644 and perplexity is 64.91561389253059
At time: 534.0786080360413 and batch: 600, loss is 4.190558576583863 and perplexity is 66.05968004717256
At time: 534.9285752773285 and batch: 650, loss is 4.212827758789063 and perplexity is 67.54727741043003
At time: 535.7719967365265 and batch: 700, loss is 4.198067827224731 and perplexity is 66.55760592723276
At time: 536.6159303188324 and batch: 750, loss is 4.164378380775451 and perplexity is 64.35266715256085
At time: 537.4615662097931 and batch: 800, loss is 4.185791635513306 and perplexity is 65.74552681511418
At time: 538.3113913536072 and batch: 850, loss is 4.213359785079956 and perplexity is 67.58322389928134
At time: 539.1578142642975 and batch: 900, loss is 4.154535975456238 and perplexity is 63.722388933985705
At time: 540.0004527568817 and batch: 950, loss is 4.152630128860474 and perplexity is 63.601059490340404
At time: 540.8524146080017 and batch: 1000, loss is 4.136749587059021 and perplexity is 62.599017748162105
At time: 541.6991946697235 and batch: 1050, loss is 4.138177156448364 and perplexity is 62.688446007037236
At time: 542.5436644554138 and batch: 1100, loss is 4.0857295274734495 and perplexity is 59.485318099562484
At time: 543.3908138275146 and batch: 1150, loss is 4.102743706703186 and perplexity is 60.50607097539741
At time: 544.2380619049072 and batch: 1200, loss is 4.120924987792969 and perplexity is 61.61621015530753
At time: 545.0855247974396 and batch: 1250, loss is 4.171861352920533 and perplexity is 64.83602258047678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.754175784814096 and perplexity of 116.06794876333926
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 547.5709607601166 and batch: 50, loss is 4.259610633850098 and perplexity is 70.78241781055691
At time: 548.4116818904877 and batch: 100, loss is 4.245278720855713 and perplexity is 69.77520523881223
At time: 549.3129150867462 and batch: 150, loss is 4.177271113395691 and perplexity is 65.18772037549701
At time: 550.1539175510406 and batch: 200, loss is 4.232857503890991 and perplexity is 68.91387274896275
At time: 550.9965469837189 and batch: 250, loss is 4.242538871765137 and perplexity is 69.58429336048971
At time: 551.8395674228668 and batch: 300, loss is 4.240181970596313 and perplexity is 69.42048317617426
At time: 552.6807832717896 and batch: 350, loss is 4.214659347534179 and perplexity is 67.67110961371466
At time: 553.5259926319122 and batch: 400, loss is 4.215585489273071 and perplexity is 67.73381168386534
At time: 554.3830678462982 and batch: 450, loss is 4.163092260360718 and perplexity is 64.26995507384109
At time: 555.2383987903595 and batch: 500, loss is 4.178348536491394 and perplexity is 65.25799298085185
At time: 556.0814702510834 and batch: 550, loss is 4.171277813911438 and perplexity is 64.7981992688687
At time: 556.9210343360901 and batch: 600, loss is 4.188532085418701 and perplexity is 65.92594624013944
At time: 557.7636950016022 and batch: 650, loss is 4.211148400306701 and perplexity is 67.43393651378592
At time: 558.6136724948883 and batch: 700, loss is 4.195910701751709 and perplexity is 66.41418756138161
At time: 559.4563834667206 and batch: 750, loss is 4.161747851371765 and perplexity is 64.18360802440053
At time: 560.3041684627533 and batch: 800, loss is 4.183028240203857 and perplexity is 65.56409673167217
At time: 561.1486701965332 and batch: 850, loss is 4.209907560348511 and perplexity is 67.35031368282598
At time: 561.9899139404297 and batch: 900, loss is 4.150799489021301 and perplexity is 63.48473536330026
At time: 562.8351812362671 and batch: 950, loss is 4.148821897506714 and perplexity is 63.359312547763885
At time: 563.6826386451721 and batch: 1000, loss is 4.132617821693421 and perplexity is 62.34090688865907
At time: 564.5219857692719 and batch: 1050, loss is 4.133479204177856 and perplexity is 62.3946293884052
At time: 565.3651337623596 and batch: 1100, loss is 4.080912618637085 and perplexity is 59.19947174578669
At time: 566.2058398723602 and batch: 1150, loss is 4.097148728370667 and perplexity is 60.168486090003626
At time: 567.0486390590668 and batch: 1200, loss is 4.115429906845093 and perplexity is 61.27855267006577
At time: 567.8958730697632 and batch: 1250, loss is 4.167567210197449 and perplexity is 64.55820436835212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.754020301094891 and perplexity of 116.04990348989446
Finished 25 epochs...
Completing Train Step...
At time: 570.330094575882 and batch: 50, loss is 4.259285659790039 and perplexity is 70.75941909805509
At time: 571.2050619125366 and batch: 100, loss is 4.244511766433716 and perplexity is 69.72171135291693
At time: 572.0476541519165 and batch: 150, loss is 4.176431741714477 and perplexity is 65.13302660646255
At time: 572.8921642303467 and batch: 200, loss is 4.232200374603272 and perplexity is 68.86860230074697
At time: 573.7360935211182 and batch: 250, loss is 4.241868505477905 and perplexity is 69.53766202788293
At time: 574.5785377025604 and batch: 300, loss is 4.239488143920898 and perplexity is 69.37233409861517
At time: 575.4245643615723 and batch: 350, loss is 4.214079532623291 and perplexity is 67.63188426814332
At time: 576.2679390907288 and batch: 400, loss is 4.215122585296631 and perplexity is 67.70246468898148
At time: 577.1128165721893 and batch: 450, loss is 4.162566337585449 and perplexity is 64.23616292751088
At time: 577.9572961330414 and batch: 500, loss is 4.177882008552551 and perplexity is 65.2275554044354
At time: 578.8057930469513 and batch: 550, loss is 4.170846934318543 and perplexity is 64.77028506141043
At time: 579.6551585197449 and batch: 600, loss is 4.188164596557617 and perplexity is 65.9017236402802
At time: 580.4982450008392 and batch: 650, loss is 4.210821557044983 and perplexity is 67.41189978749917
At time: 581.3415288925171 and batch: 700, loss is 4.195688638687134 and perplexity is 66.39944106074763
At time: 582.1872200965881 and batch: 750, loss is 4.161609616279602 and perplexity is 64.17473621064202
At time: 583.031943321228 and batch: 800, loss is 4.182961549758911 and perplexity is 65.55972437868735
At time: 583.8792824745178 and batch: 850, loss is 4.209849376678466 and perplexity is 67.34639510839689
At time: 584.723984003067 and batch: 900, loss is 4.150696830749512 and perplexity is 63.47821846459537
At time: 585.5706841945648 and batch: 950, loss is 4.148820657730102 and perplexity is 63.35923399641873
At time: 586.4152402877808 and batch: 1000, loss is 4.132664828300476 and perplexity is 62.343837392048655
At time: 587.2606203556061 and batch: 1050, loss is 4.133647394180298 and perplexity is 62.40512442382966
At time: 588.1074676513672 and batch: 1100, loss is 4.081187291145325 and perplexity is 59.21573444653371
At time: 588.9531979560852 and batch: 1150, loss is 4.0974393606185915 and perplexity is 60.185975533745264
At time: 589.7996180057526 and batch: 1200, loss is 4.115740118026733 and perplexity is 61.29756491105115
At time: 590.64439702034 and batch: 1250, loss is 4.16788827419281 and perplexity is 64.57893501113341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7539548108177465 and perplexity of 116.04230359841453
Finished 26 epochs...
Completing Train Step...
At time: 593.2052221298218 and batch: 50, loss is 4.258895702362061 and perplexity is 70.73183131635818
At time: 594.047860622406 and batch: 100, loss is 4.244032039642334 and perplexity is 69.68827200155759
At time: 594.8859386444092 and batch: 150, loss is 4.175936880111695 and perplexity is 65.10080274635513
At time: 595.7279286384583 and batch: 200, loss is 4.231711540222168 and perplexity is 68.83494518721118
At time: 596.5682249069214 and batch: 250, loss is 4.241405344009399 and perplexity is 69.50546231962933
At time: 597.4087557792664 and batch: 300, loss is 4.239019832611084 and perplexity is 69.33985385599469
At time: 598.2540657520294 and batch: 350, loss is 4.213647060394287 and perplexity is 67.6026416801615
At time: 599.0935370922089 and batch: 400, loss is 4.21473879814148 and perplexity is 67.67648633805933
At time: 599.9347870349884 and batch: 450, loss is 4.16216203212738 and perplexity is 64.21019714564123
At time: 600.7727355957031 and batch: 500, loss is 4.177527365684509 and perplexity is 65.20442701851238
At time: 601.613285779953 and batch: 550, loss is 4.170548205375671 and perplexity is 64.75093919235394
At time: 602.4566743373871 and batch: 600, loss is 4.187926340103149 and perplexity is 65.88602399960698
At time: 603.301676273346 and batch: 650, loss is 4.210630116462707 and perplexity is 67.3989956493788
At time: 604.1397221088409 and batch: 700, loss is 4.195545358657837 and perplexity is 66.3899280284172
At time: 604.9808187484741 and batch: 750, loss is 4.161510820388794 and perplexity is 64.16839632359329
At time: 605.8202292919159 and batch: 800, loss is 4.182913050651551 and perplexity is 65.55654486767853
At time: 606.6621553897858 and batch: 850, loss is 4.209824810028076 and perplexity is 67.3447406533755
At time: 607.5016641616821 and batch: 900, loss is 4.150660653114318 and perplexity is 63.47592201430536
At time: 608.3464081287384 and batch: 950, loss is 4.1488545656204225 and perplexity is 63.361382410799756
At time: 609.1911749839783 and batch: 1000, loss is 4.132718710899353 and perplexity is 62.34719673053543
At time: 610.0339508056641 and batch: 1050, loss is 4.133784384727478 and perplexity is 62.41367392155812
At time: 610.8770899772644 and batch: 1100, loss is 4.081398816108703 and perplexity is 59.22826137742655
At time: 611.7200314998627 and batch: 1150, loss is 4.0976661014556885 and perplexity is 60.199623699459075
At time: 612.5617747306824 and batch: 1200, loss is 4.11596978187561 and perplexity is 61.311644362444035
At time: 613.4028010368347 and batch: 1250, loss is 4.168090262413025 and perplexity is 64.59198051275413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753931198677007 and perplexity of 116.03956362355869
Finished 27 epochs...
Completing Train Step...
At time: 615.8547720909119 and batch: 50, loss is 4.258519229888916 and perplexity is 70.70520774072855
At time: 616.7286789417267 and batch: 100, loss is 4.243601388931275 and perplexity is 69.65826715893478
At time: 617.5707559585571 and batch: 150, loss is 4.175523185729981 and perplexity is 65.07387648002046
At time: 618.4137170314789 and batch: 200, loss is 4.23127215385437 and perplexity is 68.80470669434503
At time: 619.2557108402252 and batch: 250, loss is 4.240994138717651 and perplexity is 69.47688718124482
At time: 620.0974066257477 and batch: 300, loss is 4.2386124801635745 and perplexity is 69.311613849025
At time: 620.9393956661224 and batch: 350, loss is 4.21326265335083 and perplexity is 67.57665974268316
At time: 621.7825403213501 and batch: 400, loss is 4.214388246536255 and perplexity is 67.65276639491255
At time: 622.6250252723694 and batch: 450, loss is 4.161805396080017 and perplexity is 64.18730155766947
At time: 623.4653735160828 and batch: 500, loss is 4.177212252616882 and perplexity is 65.1838834884291
At time: 624.3070108890533 and batch: 550, loss is 4.170292782783508 and perplexity is 64.73440245163893
At time: 625.1540677547455 and batch: 600, loss is 4.187721014022827 and perplexity is 65.87249726929392
At time: 626.0056278705597 and batch: 650, loss is 4.210471301078797 and perplexity is 67.3882925019443
At time: 626.8566339015961 and batch: 700, loss is 4.195422353744507 and perplexity is 66.38176224330024
At time: 627.7017920017242 and batch: 750, loss is 4.161417202949524 and perplexity is 64.16238932383176
At time: 628.5474443435669 and batch: 800, loss is 4.182860064506531 and perplexity is 65.55307137110957
At time: 629.3878357410431 and batch: 850, loss is 4.209801111221314 and perplexity is 67.34314468229167
At time: 630.229104757309 and batch: 900, loss is 4.150632219314575 and perplexity is 63.474117178309584
At time: 631.0750305652618 and batch: 950, loss is 4.148886866569519 and perplexity is 63.363429076642156
At time: 631.9171528816223 and batch: 1000, loss is 4.132767176628112 and perplexity is 62.350218506086755
At time: 632.7627103328705 and batch: 1050, loss is 4.133898830413818 and perplexity is 62.42081730606406
At time: 633.6120235919952 and batch: 1100, loss is 4.081573944091797 and perplexity is 59.238634811696535
At time: 634.460063457489 and batch: 1150, loss is 4.097851991653442 and perplexity is 60.21081525958171
At time: 635.3045845031738 and batch: 1200, loss is 4.11615716457367 and perplexity is 61.32313418025004
At time: 636.2086737155914 and batch: 1250, loss is 4.168242521286011 and perplexity is 64.60181596365929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.75392139741104 and perplexity of 116.03842629450659
Finished 28 epochs...
Completing Train Step...
At time: 638.7287282943726 and batch: 50, loss is 4.2581634521484375 and perplexity is 70.68005687600274
At time: 639.6003539562225 and batch: 100, loss is 4.243199739456177 and perplexity is 69.63029457045796
At time: 640.4420638084412 and batch: 150, loss is 4.175149154663086 and perplexity is 65.0495413798991
At time: 641.2829706668854 and batch: 200, loss is 4.230865936279297 and perplexity is 68.77676268929532
At time: 642.1225898265839 and batch: 250, loss is 4.240612993240356 and perplexity is 69.45041142579689
At time: 642.9670450687408 and batch: 300, loss is 4.238238706588745 and perplexity is 69.28571184038417
At time: 643.8080477714539 and batch: 350, loss is 4.212905960083008 and perplexity is 67.55255990147214
At time: 644.6523010730743 and batch: 400, loss is 4.214059810638428 and perplexity is 67.63055044629839
At time: 645.4933519363403 and batch: 450, loss is 4.161476888656616 and perplexity is 64.16621901569508
At time: 646.3361525535583 and batch: 500, loss is 4.176921935081482 and perplexity is 65.16496221074917
At time: 647.1824114322662 and batch: 550, loss is 4.170058627128601 and perplexity is 64.7192462997564
At time: 648.0234980583191 and batch: 600, loss is 4.187530040740967 and perplexity is 65.85991858344075
At time: 648.8672337532043 and batch: 650, loss is 4.210326547622681 and perplexity is 67.37853851968126
At time: 649.7110800743103 and batch: 700, loss is 4.195306749343872 and perplexity is 66.37408866302135
At time: 650.5512821674347 and batch: 750, loss is 4.161323499679566 and perplexity is 64.15637737981758
At time: 651.3907120227814 and batch: 800, loss is 4.182800397872925 and perplexity is 65.54916015670402
At time: 652.2304632663727 and batch: 850, loss is 4.209772081375122 and perplexity is 67.3411897495353
At time: 653.0728855133057 and batch: 900, loss is 4.150601749420166 and perplexity is 63.47218315812628
At time: 653.9145574569702 and batch: 950, loss is 4.1489121627807615 and perplexity is 63.365031951602354
At time: 654.7555232048035 and batch: 1000, loss is 4.132807474136353 and perplexity is 62.3527311151564
At time: 655.5963678359985 and batch: 1050, loss is 4.1339970970153805 and perplexity is 62.426951489035204
At time: 656.4381775856018 and batch: 1100, loss is 4.0817247581481935 and perplexity is 59.24756950422954
At time: 657.3405981063843 and batch: 1150, loss is 4.098010187149048 and perplexity is 60.22034109279459
At time: 658.1863269805908 and batch: 1200, loss is 4.116318073272705 and perplexity is 61.33300239991165
At time: 659.0272789001465 and batch: 1250, loss is 4.168368158340454 and perplexity is 64.60993285540911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753915160241788 and perplexity of 116.03770254545911
Finished 29 epochs...
Completing Train Step...
At time: 661.5000960826874 and batch: 50, loss is 4.257826166152954 and perplexity is 70.6562215025533
At time: 662.3427906036377 and batch: 100, loss is 4.2428193855285645 and perplexity is 69.60381545047515
At time: 663.1879296302795 and batch: 150, loss is 4.174801521301269 and perplexity is 65.02693191927366
At time: 664.0267305374146 and batch: 200, loss is 4.230484189987183 and perplexity is 68.7505124259442
At time: 664.8684449195862 and batch: 250, loss is 4.24025239944458 and perplexity is 69.42537255302507
At time: 665.7072598934174 and batch: 300, loss is 4.237887802124024 and perplexity is 69.26140343997055
At time: 666.5454292297363 and batch: 350, loss is 4.212568082809448 and perplexity is 67.52973928221206
At time: 667.3835182189941 and batch: 400, loss is 4.2137478256225585 and perplexity is 67.60945401899984
At time: 668.2247560024261 and batch: 450, loss is 4.161167373657227 and perplexity is 64.1463616816855
At time: 669.0664114952087 and batch: 500, loss is 4.176649317741394 and perplexity is 65.14719953340132
At time: 669.9067063331604 and batch: 550, loss is 4.169837112426758 and perplexity is 64.70491162293827
At time: 670.7447311878204 and batch: 600, loss is 4.187347211837769 and perplexity is 65.84787858742459
At time: 671.5866804122925 and batch: 650, loss is 4.2101893854141235 and perplexity is 67.36929736431165
At time: 672.4252967834473 and batch: 700, loss is 4.195194101333618 and perplexity is 66.3666121751147
At time: 673.2620875835419 and batch: 750, loss is 4.161228451728821 and perplexity is 64.15027973740924
At time: 674.1009316444397 and batch: 800, loss is 4.182735080718994 and perplexity is 65.54487881194417
At time: 674.9423849582672 and batch: 850, loss is 4.209736375808716 and perplexity is 67.33878533713857
At time: 675.7827911376953 and batch: 900, loss is 4.150566849708557 and perplexity is 63.46996803589268
At time: 676.6218721866608 and batch: 950, loss is 4.148929724693298 and perplexity is 63.36614477252297
At time: 677.4650542736053 and batch: 1000, loss is 4.132839856147766 and perplexity is 62.354750254698715
At time: 678.3017151355743 and batch: 1050, loss is 4.134082722663879 and perplexity is 62.43229706609623
At time: 679.2043619155884 and batch: 1100, loss is 4.081857552528382 and perplexity is 59.25543777091874
At time: 680.0435826778412 and batch: 1150, loss is 4.098148341178894 and perplexity is 60.22866135032061
At time: 680.8878552913666 and batch: 1200, loss is 4.116459217071533 and perplexity is 61.34165978381756
At time: 681.7255659103394 and batch: 1250, loss is 4.168476476669311 and perplexity is 64.61693167440696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7539138237055205 and perplexity of 116.0375474569649
Finished 30 epochs...
Completing Train Step...
At time: 684.1419394016266 and batch: 50, loss is 4.257504196166992 and perplexity is 70.63347598179196
At time: 685.0182192325592 and batch: 100, loss is 4.242456007003784 and perplexity is 69.57852751352253
At time: 685.8622727394104 and batch: 150, loss is 4.174473247528076 and perplexity is 65.0055887863604
At time: 686.7076790332794 and batch: 200, loss is 4.230121297836304 and perplexity is 68.72556793097036
At time: 687.5587196350098 and batch: 250, loss is 4.239907445907593 and perplexity is 69.40142815529603
At time: 688.4065628051758 and batch: 300, loss is 4.237553749084473 and perplexity is 69.23827032168722
At time: 689.2511217594147 and batch: 350, loss is 4.212244701385498 and perplexity is 67.50790494956307
At time: 690.0957264900208 and batch: 400, loss is 4.213448696136474 and perplexity is 67.58923306225768
At time: 690.9410214424133 and batch: 450, loss is 4.160871963500977 and perplexity is 64.12741499361809
At time: 691.7876932621002 and batch: 500, loss is 4.176389980316162 and perplexity is 65.13030661699062
At time: 692.6311912536621 and batch: 550, loss is 4.1696239805221555 and perplexity is 64.6911224113996
At time: 693.4759790897369 and batch: 600, loss is 4.1871698951721195 and perplexity is 65.83620369626037
At time: 694.319474697113 and batch: 650, loss is 4.210057277679443 and perplexity is 67.36039794690372
At time: 695.167772769928 and batch: 700, loss is 4.195083265304565 and perplexity is 66.35925677098896
At time: 696.0098276138306 and batch: 750, loss is 4.161132216453552 and perplexity is 64.1441065146258
At time: 696.851071357727 and batch: 800, loss is 4.182665095329285 and perplexity is 65.54029178857125
At time: 697.6975743770599 and batch: 850, loss is 4.209694786071777 and perplexity is 67.33598479300797
At time: 698.541291475296 and batch: 900, loss is 4.150527877807617 and perplexity is 63.467494538784464
At time: 699.3891084194183 and batch: 950, loss is 4.148940386772156 and perplexity is 63.36682039095721
At time: 700.2391290664673 and batch: 1000, loss is 4.132865152359009 and perplexity is 62.35632761358368
At time: 701.1581716537476 and batch: 1050, loss is 4.134158186912536 and perplexity is 62.43700865026211
At time: 702.0258684158325 and batch: 1100, loss is 4.081976447105408 and perplexity is 59.262483339960696
At time: 702.8735902309418 and batch: 1150, loss is 4.0982710647583005 and perplexity is 60.23605328079717
At time: 703.7220356464386 and batch: 1200, loss is 4.116585559844971 and perplexity is 61.349410348845566
At time: 704.5703711509705 and batch: 1250, loss is 4.168572125434875 and perplexity is 64.62311249974559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753912041657163 and perplexity of 116.03734067262828
Finished 31 epochs...
Completing Train Step...
At time: 707.034613609314 and batch: 50, loss is 4.257194776535034 and perplexity is 70.61162397854349
At time: 707.8758208751678 and batch: 100, loss is 4.242106456756591 and perplexity is 69.5542105722749
At time: 708.7164738178253 and batch: 150, loss is 4.174160103797913 and perplexity is 64.98523588066512
At time: 709.561038017273 and batch: 200, loss is 4.229773292541504 and perplexity is 68.70165523055675
At time: 710.405166387558 and batch: 250, loss is 4.239575214385987 and perplexity is 69.37837464298211
At time: 711.2446253299713 and batch: 300, loss is 4.237233018875122 and perplexity is 69.21606707756774
At time: 712.0904564857483 and batch: 350, loss is 4.211932754516601 and perplexity is 67.48684935427252
At time: 712.9333357810974 and batch: 400, loss is 4.213160104751587 and perplexity is 67.56973020619826
At time: 713.7765643596649 and batch: 450, loss is 4.160587501525879 and perplexity is 64.1091757767959
At time: 714.6182734966278 and batch: 500, loss is 4.176141052246094 and perplexity is 65.11409587319982
At time: 715.464438199997 and batch: 550, loss is 4.16941725730896 and perplexity is 64.67775063688522
At time: 716.3086512088776 and batch: 600, loss is 4.186996927261353 and perplexity is 65.82481713043785
At time: 717.1474821567535 and batch: 650, loss is 4.209928731918335 and perplexity is 67.35173960978932
At time: 717.9896295070648 and batch: 700, loss is 4.194973554611206 and perplexity is 66.35197685026768
At time: 718.8306758403778 and batch: 750, loss is 4.161035356521606 and perplexity is 64.1378938217194
At time: 719.6690940856934 and batch: 800, loss is 4.182591214179992 and perplexity is 65.53544977535789
At time: 720.5151219367981 and batch: 850, loss is 4.209648399353028 and perplexity is 67.33286137006287
At time: 721.3580520153046 and batch: 900, loss is 4.150484647750854 and perplexity is 63.46475089469734
At time: 722.2547178268433 and batch: 950, loss is 4.148944716453553 and perplexity is 63.36709474969456
At time: 723.0963265895844 and batch: 1000, loss is 4.132884068489075 and perplexity is 62.35750716514349
At time: 723.9403402805328 and batch: 1050, loss is 4.134224705696106 and perplexity is 62.44116202226433
At time: 724.7806406021118 and batch: 1100, loss is 4.0820837450027465 and perplexity is 59.26884242096605
At time: 725.6196894645691 and batch: 1150, loss is 4.098381781578064 and perplexity is 60.2427227942584
At time: 726.4660882949829 and batch: 1200, loss is 4.1166997194290165 and perplexity is 61.356414371792994
At time: 727.3111901283264 and batch: 1250, loss is 4.16865825176239 and perplexity is 64.62867849078353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753912932681342 and perplexity of 116.03744406475053
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 729.7542095184326 and batch: 50, loss is 4.25703761100769 and perplexity is 70.6005271374676
At time: 730.6315622329712 and batch: 100, loss is 4.2419930267333985 and perplexity is 69.5463214839947
At time: 731.4798605442047 and batch: 150, loss is 4.174157667160034 and perplexity is 64.9850775353707
At time: 732.3279461860657 and batch: 200, loss is 4.229755668640137 and perplexity is 68.70044445003062
At time: 733.172919511795 and batch: 250, loss is 4.239486289024353 and perplexity is 69.3722054202317
At time: 734.0224752426147 and batch: 300, loss is 4.237145195007324 and perplexity is 69.20998852176828
At time: 734.8668479919434 and batch: 350, loss is 4.211840677261352 and perplexity is 67.48063563649401
At time: 735.7131567001343 and batch: 400, loss is 4.212917594909668 and perplexity is 67.55334586836933
At time: 736.5588490962982 and batch: 450, loss is 4.160445866584777 and perplexity is 64.10009632045998
At time: 737.4045579433441 and batch: 500, loss is 4.176033864021301 and perplexity is 65.107116782899
At time: 738.2531712055206 and batch: 550, loss is 4.168945569992065 and perplexity is 64.64725015613496
At time: 739.0934875011444 and batch: 600, loss is 4.186468830108643 and perplexity is 65.79006440914787
At time: 739.9374895095825 and batch: 650, loss is 4.209670186042786 and perplexity is 67.33432834620429
At time: 740.7856047153473 and batch: 700, loss is 4.1947200489044185 and perplexity is 66.33515837735901
At time: 741.6292617321014 and batch: 750, loss is 4.160644264221191 and perplexity is 64.11281488968918
At time: 742.47598695755 and batch: 800, loss is 4.1819807100296025 and perplexity is 65.49545232182037
At time: 743.3214089870453 and batch: 850, loss is 4.208966712951661 and perplexity is 67.28697711521491
At time: 744.2266726493835 and batch: 900, loss is 4.1496599817276 and perplexity is 63.412435245394306
At time: 745.0700647830963 and batch: 950, loss is 4.148140926361084 and perplexity is 63.316181371321925
At time: 746.1338667869568 and batch: 1000, loss is 4.132090911865235 and perplexity is 62.308067504578304
At time: 746.9795513153076 and batch: 1050, loss is 4.133329944610596 and perplexity is 62.385317088015555
At time: 747.8225295543671 and batch: 1100, loss is 4.081120042800904 and perplexity is 59.21175242032234
At time: 748.6675651073456 and batch: 1150, loss is 4.097355670928955 and perplexity is 60.180938798896726
At time: 749.5105957984924 and batch: 1200, loss is 4.115655813217163 and perplexity is 61.292397449338225
At time: 750.3580451011658 and batch: 1250, loss is 4.16774742603302 and perplexity is 64.56983982751083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753863480839416 and perplexity of 116.03170594129077
Finished 33 epochs...
Completing Train Step...
At time: 752.7938239574432 and batch: 50, loss is 4.256962766647339 and perplexity is 70.59524328390933
At time: 753.634473323822 and batch: 100, loss is 4.241896467208862 and perplexity is 69.53960644846451
At time: 754.5073087215424 and batch: 150, loss is 4.174009890556335 and perplexity is 64.97547497085608
At time: 755.353794336319 and batch: 200, loss is 4.229620780944824 and perplexity is 68.69117823037533
At time: 756.1950843334198 and batch: 250, loss is 4.239367980957031 and perplexity is 69.3639986141578
At time: 757.0361275672913 and batch: 300, loss is 4.237074251174927 and perplexity is 69.20507867410616
At time: 757.8794968128204 and batch: 350, loss is 4.211728639602661 and perplexity is 67.47307568757772
At time: 758.7223916053772 and batch: 400, loss is 4.212851791381836 and perplexity is 67.54890076614765
At time: 759.5632030963898 and batch: 450, loss is 4.160365877151489 and perplexity is 64.09496919514223
At time: 760.4106314182281 and batch: 500, loss is 4.175983848571778 and perplexity is 65.10386050261877
At time: 761.2519948482513 and batch: 550, loss is 4.1688876676559445 and perplexity is 64.64350703769585
At time: 762.0941288471222 and batch: 600, loss is 4.186426467895508 and perplexity is 65.7872774554484
At time: 762.9349102973938 and batch: 650, loss is 4.209614448547363 and perplexity is 67.33057540397708
At time: 763.7750885486603 and batch: 700, loss is 4.194652891159057 and perplexity is 66.33070360727203
At time: 764.6149249076843 and batch: 750, loss is 4.160596442222595 and perplexity is 64.10974896005557
At time: 765.4575881958008 and batch: 800, loss is 4.181980404853821 and perplexity is 65.49543233419757
At time: 766.2979776859283 and batch: 850, loss is 4.208961658477783 and perplexity is 67.28663701580629
At time: 767.1383793354034 and batch: 900, loss is 4.1496546268463135 and perplexity is 63.412095680240625
At time: 767.9759199619293 and batch: 950, loss is 4.148132076263428 and perplexity is 63.31562101941319
At time: 768.820556640625 and batch: 1000, loss is 4.132117791175842 and perplexity is 62.30974232498702
At time: 769.662791967392 and batch: 1050, loss is 4.133355965614319 and perplexity is 62.38694043770428
At time: 770.5069313049316 and batch: 1100, loss is 4.081192603111267 and perplexity is 59.21604899933381
At time: 771.346715927124 and batch: 1150, loss is 4.097408199310303 and perplexity is 60.184100089227826
At time: 772.195100069046 and batch: 1200, loss is 4.1156918859481815 and perplexity is 61.29460847338348
At time: 773.033588886261 and batch: 1250, loss is 4.167774271965027 and perplexity is 64.57157328830864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753826948848084 and perplexity of 116.02746714944125
Finished 34 epochs...
Completing Train Step...
At time: 775.5111021995544 and batch: 50, loss is 4.256896352767944 and perplexity is 70.59055493562343
At time: 776.3565721511841 and batch: 100, loss is 4.24181487083435 and perplexity is 69.53393250018327
At time: 777.2050821781158 and batch: 150, loss is 4.173914303779602 and perplexity is 64.96926447146247
At time: 778.0673627853394 and batch: 200, loss is 4.229527883529663 and perplexity is 68.6847972938642
At time: 778.9233567714691 and batch: 250, loss is 4.239281144142151 and perplexity is 69.35797552696742
At time: 779.7705352306366 and batch: 300, loss is 4.237000551223755 and perplexity is 69.19997845113241
At time: 780.6136739253998 and batch: 350, loss is 4.211653985977173 and perplexity is 67.46803876586935
At time: 781.4632201194763 and batch: 400, loss is 4.2127931118011475 and perplexity is 67.54493714126777
At time: 782.3043937683105 and batch: 450, loss is 4.160302829742432 and perplexity is 64.09092830078617
At time: 783.1759080886841 and batch: 500, loss is 4.175939311981201 and perplexity is 65.10096106320468
At time: 784.020578622818 and batch: 550, loss is 4.16883912563324 and perplexity is 64.64036918726894
At time: 784.866001367569 and batch: 600, loss is 4.186394720077515 and perplexity is 65.78518888609142
At time: 785.7060523033142 and batch: 650, loss is 4.2095866918563845 and perplexity is 67.32870655593882
At time: 786.5475854873657 and batch: 700, loss is 4.194626016616821 and perplexity is 66.32892102392955
At time: 787.3893752098083 and batch: 750, loss is 4.160578460693359 and perplexity is 64.10859617909473
At time: 788.238171339035 and batch: 800, loss is 4.181975498199463 and perplexity is 65.49511097153746
At time: 789.0882542133331 and batch: 850, loss is 4.208954734802246 and perplexity is 67.2861711465764
At time: 789.9304978847504 and batch: 900, loss is 4.149647049903869 and perplexity is 63.41161521226164
At time: 790.7754328250885 and batch: 950, loss is 4.148127961158752 and perplexity is 63.31536046954119
At time: 791.621390581131 and batch: 1000, loss is 4.132134623527527 and perplexity is 62.31079115331033
At time: 792.4622690677643 and batch: 1050, loss is 4.1333819055557255 and perplexity is 62.38855877227343
At time: 793.3063144683838 and batch: 1100, loss is 4.081237015724182 and perplexity is 59.218678997198445
At time: 794.1478309631348 and batch: 1150, loss is 4.097445912361145 and perplexity is 60.186369858054064
At time: 794.9937832355499 and batch: 1200, loss is 4.115722327232361 and perplexity is 61.296474388378975
At time: 795.8399727344513 and batch: 1250, loss is 4.167796130180359 and perplexity is 64.57298472308757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753798881586451 and perplexity of 116.02421062186527
Finished 35 epochs...
Completing Train Step...
At time: 798.2899115085602 and batch: 50, loss is 4.256830778121948 and perplexity is 70.58592613674045
At time: 799.1872944831848 and batch: 100, loss is 4.241737937927246 and perplexity is 69.52858325838262
At time: 800.0318458080292 and batch: 150, loss is 4.173834924697876 and perplexity is 64.96410747558978
At time: 800.8738555908203 and batch: 200, loss is 4.229446887969971 and perplexity is 68.67923435555473
At time: 801.716956615448 and batch: 250, loss is 4.2392046976089475 and perplexity is 69.35267355284975
At time: 802.5609929561615 and batch: 300, loss is 4.23692850112915 and perplexity is 69.1949927657501
At time: 803.4022176265717 and batch: 350, loss is 4.211588544845581 and perplexity is 67.46362372553043
At time: 804.3085803985596 and batch: 400, loss is 4.212735681533814 and perplexity is 67.54105812885813
At time: 805.1493699550629 and batch: 450, loss is 4.160245070457458 and perplexity is 64.08722656150017
At time: 805.9955770969391 and batch: 500, loss is 4.175895295143127 and perplexity is 65.09809558780815
At time: 806.8394107818604 and batch: 550, loss is 4.168793611526489 and perplexity is 64.63742720555668
At time: 807.6856627464294 and batch: 600, loss is 4.186364498138428 and perplexity is 65.78320076016271
At time: 808.5261209011078 and batch: 650, loss is 4.209565472602844 and perplexity is 67.32727790620137
At time: 809.3696572780609 and batch: 700, loss is 4.194608869552613 and perplexity is 66.32778368741289
At time: 810.2099304199219 and batch: 750, loss is 4.160567507743836 and perplexity is 64.10789400472221
At time: 811.0533516407013 and batch: 800, loss is 4.181968235969544 and perplexity is 65.49463533271013
At time: 811.8966970443726 and batch: 850, loss is 4.208946838378906 and perplexity is 67.28563982858184
At time: 812.7384114265442 and batch: 900, loss is 4.149638161659241 and perplexity is 63.41105159681813
At time: 813.5795409679413 and batch: 950, loss is 4.148125400543213 and perplexity is 63.31519834345288
At time: 814.4233958721161 and batch: 1000, loss is 4.13214714050293 and perplexity is 62.31157110083181
At time: 815.2696931362152 and batch: 1050, loss is 4.13340539932251 and perplexity is 62.390024531741275
At time: 816.1133012771606 and batch: 1100, loss is 4.081270756721497 and perplexity is 59.220677128196776
At time: 816.954053401947 and batch: 1150, loss is 4.097477564811706 and perplexity is 60.18827493430045
At time: 817.7973253726959 and batch: 1200, loss is 4.1157498979568485 and perplexity is 61.2981643998837
At time: 818.6415641307831 and batch: 1250, loss is 4.167815275192261 and perplexity is 64.57422098548274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753783734175411 and perplexity of 116.02245316876683
Finished 36 epochs...
Completing Train Step...
At time: 821.1316158771515 and batch: 50, loss is 4.256766109466553 and perplexity is 70.5813615874006
At time: 821.9721026420593 and batch: 100, loss is 4.241663722991944 and perplexity is 69.52342339054645
At time: 822.8124821186066 and batch: 150, loss is 4.173762645721435 and perplexity is 64.95941210608638
At time: 823.6551525592804 and batch: 200, loss is 4.229371242523193 and perplexity is 68.67403928068191
At time: 824.4945516586304 and batch: 250, loss is 4.239132947921753 and perplexity is 69.34769769872636
At time: 825.335373878479 and batch: 300, loss is 4.236859388351441 and perplexity is 69.19021067285021
At time: 826.2049083709717 and batch: 350, loss is 4.211526546478272 and perplexity is 67.45944122066224
At time: 827.04625415802 and batch: 400, loss is 4.212679224014282 and perplexity is 67.53724503588954
At time: 827.8855812549591 and batch: 450, loss is 4.160189509391785 and perplexity is 64.08366590581417
At time: 828.7300362586975 and batch: 500, loss is 4.175851483345031 and perplexity is 65.09524358566395
At time: 829.5719501972198 and batch: 550, loss is 4.1687495899200435 and perplexity is 64.63458182480416
At time: 830.4130897521973 and batch: 600, loss is 4.186334438323975 and perplexity is 65.78122335907406
At time: 831.2513957023621 and batch: 650, loss is 4.209546136856079 and perplexity is 67.32597609559116
At time: 832.0890159606934 and batch: 700, loss is 4.194594230651855 and perplexity is 66.32681272867694
At time: 832.9265694618225 and batch: 750, loss is 4.1605583810806275 and perplexity is 64.1073089162346
At time: 833.7645528316498 and batch: 800, loss is 4.181959791183472 and perplexity is 65.49408224686123
At time: 834.6039316654205 and batch: 850, loss is 4.20893856048584 and perplexity is 67.28508284755577
At time: 835.4451124668121 and batch: 900, loss is 4.1496287441253665 and perplexity is 63.41045442390365
At time: 836.2847471237183 and batch: 950, loss is 4.148123488426209 and perplexity is 63.31507727750124
At time: 837.1241488456726 and batch: 1000, loss is 4.132157349586487 and perplexity is 62.312207248114994
At time: 837.9672605991364 and batch: 1050, loss is 4.133426723480224 and perplexity is 62.39135496064928
At time: 838.8054509162903 and batch: 1100, loss is 4.081299648284912 and perplexity is 59.222388130862186
At time: 839.6484699249268 and batch: 1150, loss is 4.09750606060028 and perplexity is 60.18999007109459
At time: 840.4889078140259 and batch: 1200, loss is 4.115775227546692 and perplexity is 61.299717076910355
At time: 841.3310213088989 and batch: 1250, loss is 4.167832593917847 and perplexity is 64.57533933838012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7537757149578015 and perplexity of 116.02152276319791
Finished 37 epochs...
Completing Train Step...
At time: 843.7648916244507 and batch: 50, loss is 4.2567023658752445 and perplexity is 70.57686262132522
At time: 844.6652359962463 and batch: 100, loss is 4.241591472625732 and perplexity is 69.51840047920197
At time: 845.5152235031128 and batch: 150, loss is 4.173694262504577 and perplexity is 64.9549701244016
At time: 846.3583152294159 and batch: 200, loss is 4.229298505783081 and perplexity is 68.6690443365945
At time: 847.2033321857452 and batch: 250, loss is 4.239064311981201 and perplexity is 69.3429381176107
At time: 848.0776791572571 and batch: 300, loss is 4.236792583465576 and perplexity is 69.18558858311405
At time: 848.92076420784 and batch: 350, loss is 4.211466093063354 and perplexity is 67.4553631903387
At time: 849.7724361419678 and batch: 400, loss is 4.212623233795166 and perplexity is 67.53346371660092
At time: 850.6238226890564 and batch: 450, loss is 4.160135531425476 and perplexity is 64.08020689321107
At time: 851.4728722572327 and batch: 500, loss is 4.175808272361755 and perplexity is 65.09243081695371
At time: 852.3274476528168 and batch: 550, loss is 4.168706784248352 and perplexity is 64.6318151573296
At time: 853.1720342636108 and batch: 600, loss is 4.1863046169281 and perplexity is 65.779261700421
At time: 854.0164215564728 and batch: 650, loss is 4.2095274209976195 and perplexity is 67.32471604394341
At time: 854.860270023346 and batch: 700, loss is 4.19458046913147 and perplexity is 66.32589997717191
At time: 855.710896730423 and batch: 750, loss is 4.160549492835998 and perplexity is 64.10673911732269
At time: 856.5563733577728 and batch: 800, loss is 4.181950631141663 and perplexity is 65.4934823210773
At time: 857.4005582332611 and batch: 850, loss is 4.2089305591583255 and perplexity is 67.28454447972491
At time: 858.2444133758545 and batch: 900, loss is 4.14961886882782 and perplexity is 63.40982822989057
At time: 859.0905532836914 and batch: 950, loss is 4.148121809959411 and perplexity is 63.31497100533543
At time: 859.936003446579 and batch: 1000, loss is 4.132166447639466 and perplexity is 62.31277417045669
At time: 860.7799758911133 and batch: 1050, loss is 4.133446297645569 and perplexity is 62.39257623129999
At time: 861.6232187747955 and batch: 1100, loss is 4.081326146125793 and perplexity is 59.2239574170707
At time: 862.4664235115051 and batch: 1150, loss is 4.09753258228302 and perplexity is 60.191586432084364
At time: 863.3100464344025 and batch: 1200, loss is 4.11579909324646 and perplexity is 61.30118005501143
At time: 864.1563904285431 and batch: 1250, loss is 4.1678486251831055 and perplexity is 64.57637457107222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753769477788549 and perplexity of 116.02079911958027
Finished 38 epochs...
Completing Train Step...
At time: 866.5920338630676 and batch: 50, loss is 4.256639785766602 and perplexity is 70.57244605179086
At time: 867.4627301692963 and batch: 100, loss is 4.241521005630493 and perplexity is 69.51350189900253
At time: 868.3032839298248 and batch: 150, loss is 4.1736283779144285 and perplexity is 64.9506907337913
At time: 869.175952911377 and batch: 200, loss is 4.229227657318115 and perplexity is 68.66417941255081
At time: 870.0172481536865 and batch: 250, loss is 4.238997569084168 and perplexity is 69.33831012347653
At time: 870.8552203178406 and batch: 300, loss is 4.236727752685547 and perplexity is 69.18110337283086
At time: 871.6959528923035 and batch: 350, loss is 4.2114067554473875 and perplexity is 67.45136066865405
At time: 872.5389699935913 and batch: 400, loss is 4.212568159103394 and perplexity is 67.5297444343225
At time: 873.3906919956207 and batch: 450, loss is 4.160082669258117 and perplexity is 64.07681956412145
At time: 874.2295348644257 and batch: 500, loss is 4.1757652759552 and perplexity is 65.08963213650168
At time: 875.072952747345 and batch: 550, loss is 4.168664574623108 and perplexity is 64.62908713020788
At time: 875.9151608943939 and batch: 600, loss is 4.186274652481079 and perplexity is 65.77729069074893
At time: 876.7617483139038 and batch: 650, loss is 4.209509062767029 and perplexity is 67.3234800926268
At time: 877.6048600673676 and batch: 700, loss is 4.194567108154297 and perplexity is 66.32501380425644
At time: 878.4512922763824 and batch: 750, loss is 4.160540561676026 and perplexity is 64.10616657233707
At time: 879.2919232845306 and batch: 800, loss is 4.181940822601319 and perplexity is 65.49283992876413
At time: 880.1372802257538 and batch: 850, loss is 4.208922548294067 and perplexity is 67.28400547453136
At time: 880.9765515327454 and batch: 900, loss is 4.149608631134033 and perplexity is 63.40917906280906
At time: 881.8174345493317 and batch: 950, loss is 4.14812017917633 and perplexity is 63.314867752436136
At time: 882.6586129665375 and batch: 1000, loss is 4.132174439430237 and perplexity is 62.313272163100166
At time: 883.4968428611755 and batch: 1050, loss is 4.133464317321778 and perplexity is 62.39370053545129
At time: 884.3396947383881 and batch: 1100, loss is 4.081350865364075 and perplexity is 59.225421406280375
At time: 885.1801936626434 and batch: 1150, loss is 4.097557868957519 and perplexity is 60.19310849638195
At time: 886.0196673870087 and batch: 1200, loss is 4.115822019577027 and perplexity is 61.30258548224007
At time: 886.8650727272034 and batch: 1250, loss is 4.167863869667054 and perplexity is 64.57735901208142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753767250228102 and perplexity of 116.02054067652496
Finished 39 epochs...
Completing Train Step...
At time: 889.3618297576904 and batch: 50, loss is 4.256578102111816 and perplexity is 70.56809301964806
At time: 890.2051649093628 and batch: 100, loss is 4.241451435089111 and perplexity is 69.50866597526293
At time: 891.1101987361908 and batch: 150, loss is 4.173564100265503 and perplexity is 64.94651599026766
At time: 891.9550731182098 and batch: 200, loss is 4.229158267974854 and perplexity is 68.6594150155369
At time: 892.801164150238 and batch: 250, loss is 4.238932375907898 and perplexity is 69.33378988614831
At time: 893.6471221446991 and batch: 300, loss is 4.236664619445801 and perplexity is 69.17673588351404
At time: 894.4953966140747 and batch: 350, loss is 4.211348390579223 and perplexity is 67.44742399376399
At time: 895.3438529968262 and batch: 400, loss is 4.212513599395752 and perplexity is 67.5260601317172
At time: 896.1905658245087 and batch: 450, loss is 4.160030589103699 and perplexity is 64.07348252036155
At time: 897.0351734161377 and batch: 500, loss is 4.175722637176514 and perplexity is 65.08685685324998
At time: 897.8791065216064 and batch: 550, loss is 4.1686228036880495 and perplexity is 64.62638756918847
At time: 898.7228121757507 and batch: 600, loss is 4.186244964599609 and perplexity is 65.7753379313263
At time: 899.5666291713715 and batch: 650, loss is 4.209490985870361 and perplexity is 67.32226310403358
At time: 900.4163074493408 and batch: 700, loss is 4.1945537757873534 and perplexity is 66.32412954072953
At time: 901.2682769298553 and batch: 750, loss is 4.160531072616577 and perplexity is 64.10555826799758
At time: 902.1120321750641 and batch: 800, loss is 4.1819306039810185 and perplexity is 65.4921706857199
At time: 902.9585547447205 and batch: 850, loss is 4.208914785385132 and perplexity is 67.28348315695142
At time: 903.8053660392761 and batch: 900, loss is 4.149598288536072 and perplexity is 63.408523250554374
At time: 904.651683807373 and batch: 950, loss is 4.1481183958053585 and perplexity is 63.31475483863959
At time: 905.4949939250946 and batch: 1000, loss is 4.132181715965271 and perplexity is 62.31372558945783
At time: 906.3390235900879 and batch: 1050, loss is 4.133481101989746 and perplexity is 62.39474780178708
At time: 907.1881530284882 and batch: 1100, loss is 4.081374416351318 and perplexity is 59.226816239849164
At time: 908.0308423042297 and batch: 1150, loss is 4.09758189201355 and perplexity is 60.1945545361691
At time: 908.8787565231323 and batch: 1200, loss is 4.115844149589538 and perplexity is 61.303942124234965
At time: 909.7267262935638 and batch: 1250, loss is 4.167878413200379 and perplexity is 64.57829820188383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753765022667655 and perplexity of 116.02028223404533
Finished 40 epochs...
Completing Train Step...
At time: 912.1559870243073 and batch: 50, loss is 4.256517362594605 and perplexity is 70.56380687791793
At time: 913.0525109767914 and batch: 100, loss is 4.24138295173645 and perplexity is 69.503905951771
At time: 913.8928649425507 and batch: 150, loss is 4.173501086235047 and perplexity is 64.94242357747206
At time: 914.7388706207275 and batch: 200, loss is 4.229090414047241 and perplexity is 68.65475636261624
At time: 915.5780396461487 and batch: 250, loss is 4.238868389129639 and perplexity is 69.32935358224292
At time: 916.4237768650055 and batch: 300, loss is 4.2366025924682615 and perplexity is 69.17244519274183
At time: 917.2657680511475 and batch: 350, loss is 4.211290626525879 and perplexity is 67.44352806968976
At time: 918.1078412532806 and batch: 400, loss is 4.212459630966187 and perplexity is 67.5224159546333
At time: 918.9519665241241 and batch: 450, loss is 4.159979295730591 and perplexity is 64.07019605960384
At time: 919.7945914268494 and batch: 500, loss is 4.1756804132461545 and perplexity is 65.08410868835848
At time: 920.6328206062317 and batch: 550, loss is 4.168581571578979 and perplexity is 64.62372294186189
At time: 921.4729807376862 and batch: 600, loss is 4.186215410232544 and perplexity is 65.77339401157101
At time: 922.3135671615601 and batch: 650, loss is 4.2094729614257815 and perplexity is 67.32104966856907
At time: 923.1555078029633 and batch: 700, loss is 4.194540519714355 and perplexity is 66.3232503490541
At time: 923.9976644515991 and batch: 750, loss is 4.160521574020386 and perplexity is 64.10494935807786
At time: 924.8350281715393 and batch: 800, loss is 4.181919832229614 and perplexity is 65.49146522413785
At time: 925.6988196372986 and batch: 850, loss is 4.208906917572022 and perplexity is 67.28295378516303
At time: 926.5695979595184 and batch: 900, loss is 4.149587697982788 and perplexity is 63.40785172276621
At time: 927.4214060306549 and batch: 950, loss is 4.148116517066955 and perplexity is 63.314635886889924
At time: 928.2673132419586 and batch: 1000, loss is 4.1321884059906 and perplexity is 62.31414247125486
At time: 929.120608329773 and batch: 1050, loss is 4.133496990203858 and perplexity is 62.39573915077494
At time: 929.9630908966064 and batch: 1100, loss is 4.081397070884704 and perplexity is 59.22815801093357
At time: 930.8091847896576 and batch: 1150, loss is 4.0976051902771 and perplexity is 60.19595698110213
At time: 931.650591135025 and batch: 1200, loss is 4.115865168571472 and perplexity is 61.305230684229
At time: 932.4942307472229 and batch: 1250, loss is 4.1678924036026 and perplexity is 64.57920168457048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753765022667655 and perplexity of 116.02028223404533
Finished 41 epochs...
Completing Train Step...
At time: 934.9596178531647 and batch: 50, loss is 4.256457395553589 and perplexity is 70.55957550208949
At time: 935.8002135753632 and batch: 100, loss is 4.241315307617188 and perplexity is 69.49920458027945
At time: 936.6409523487091 and batch: 150, loss is 4.173439412117005 and perplexity is 64.93841843428251
At time: 937.4829390048981 and batch: 200, loss is 4.229023408889771 and perplexity is 68.65015629397094
At time: 938.3279206752777 and batch: 250, loss is 4.238805365562439 and perplexity is 69.32498433675268
At time: 939.1692814826965 and batch: 300, loss is 4.23654167175293 and perplexity is 69.16823128625784
At time: 940.0143427848816 and batch: 350, loss is 4.2112335777282714 and perplexity is 67.43968060725456
At time: 940.8600056171417 and batch: 400, loss is 4.212406349182129 and perplexity is 67.51881833569199
At time: 941.7060341835022 and batch: 450, loss is 4.1599285364151 and perplexity is 64.06694398284576
At time: 942.5503282546997 and batch: 500, loss is 4.175638589859009 and perplexity is 65.08138670740541
At time: 943.3918511867523 and batch: 550, loss is 4.168540601730347 and perplexity is 64.62107537195051
At time: 944.2347238063812 and batch: 600, loss is 4.186185636520386 and perplexity is 65.77143572262297
At time: 945.0786082744598 and batch: 650, loss is 4.209455070495605 and perplexity is 67.31984524314423
At time: 945.9213011264801 and batch: 700, loss is 4.194527206420898 and perplexity is 66.32236737403686
At time: 946.7687811851501 and batch: 750, loss is 4.160511589050293 and perplexity is 64.10430927527129
At time: 947.608850479126 and batch: 800, loss is 4.181908783912658 and perplexity is 65.49074165766928
At time: 948.4534649848938 and batch: 850, loss is 4.208898801803588 and perplexity is 67.28240773450642
At time: 949.3054497241974 and batch: 900, loss is 4.149576864242554 and perplexity is 63.40716478229288
At time: 950.1529965400696 and batch: 950, loss is 4.1481143617630005 and perplexity is 63.31449942475186
At time: 950.9950711727142 and batch: 1000, loss is 4.1321945476531985 and perplexity is 62.31452518486825
At time: 951.8387424945831 and batch: 1050, loss is 4.13351185798645 and perplexity is 62.3966668439557
At time: 952.6849353313446 and batch: 1100, loss is 4.081418671607971 and perplexity is 59.229437395802144
At time: 953.5284507274628 and batch: 1150, loss is 4.097627682685852 and perplexity is 60.19731094839873
At time: 954.3741006851196 and batch: 1200, loss is 4.115885710716247 and perplexity is 61.30649003808808
At time: 955.21617436409 and batch: 1250, loss is 4.1679057693481445 and perplexity is 64.58006483951598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762349595118 and perplexity of 116.01997210382969
Finished 42 epochs...
Completing Train Step...
At time: 957.6545069217682 and batch: 50, loss is 4.25639838218689 and perplexity is 70.55541166684841
At time: 958.5269017219543 and batch: 100, loss is 4.241248588562012 and perplexity is 69.49456781369646
At time: 959.3690495491028 and batch: 150, loss is 4.173378610610962 and perplexity is 64.9344702006722
At time: 960.218451499939 and batch: 200, loss is 4.228957195281982 and perplexity is 68.64561086993358
At time: 961.0607373714447 and batch: 250, loss is 4.238743052482605 and perplexity is 69.32066461805812
At time: 961.9052271842957 and batch: 300, loss is 4.2364816474914555 and perplexity is 69.16407963885844
At time: 962.7473287582397 and batch: 350, loss is 4.211177024841309 and perplexity is 67.43586680646212
At time: 963.5915582180023 and batch: 400, loss is 4.212353467941284 and perplexity is 67.51524795120203
At time: 964.4340677261353 and batch: 450, loss is 4.15987841129303 and perplexity is 64.06373269994162
At time: 965.2756962776184 and batch: 500, loss is 4.17559705734253 and perplexity is 65.07868376976978
At time: 966.1156625747681 and batch: 550, loss is 4.168499975204468 and perplexity is 64.6184500954879
At time: 966.9598405361176 and batch: 600, loss is 4.186156063079834 and perplexity is 65.76949066373982
At time: 967.8057687282562 and batch: 650, loss is 4.209437155723572 and perplexity is 67.31863923426613
At time: 968.6508305072784 and batch: 700, loss is 4.194513988494873 and perplexity is 66.3214907356848
At time: 969.5010576248169 and batch: 750, loss is 4.160501432418823 and perplexity is 64.1036581947328
At time: 970.3490133285522 and batch: 800, loss is 4.1818973827362065 and perplexity is 65.48999499042412
At time: 971.2072365283966 and batch: 850, loss is 4.2088908767700195 and perplexity is 67.28187452127939
At time: 972.052686214447 and batch: 900, loss is 4.149565782546997 and perplexity is 63.406462127289956
At time: 972.8963098526001 and batch: 950, loss is 4.148112125396729 and perplexity is 63.31435783049916
At time: 973.7387628555298 and batch: 1000, loss is 4.132200245857239 and perplexity is 62.31488026675909
At time: 974.5842924118042 and batch: 1050, loss is 4.133525943756103 and perplexity is 62.397545755222026
At time: 975.4325428009033 and batch: 1100, loss is 4.081439771652222 and perplexity is 59.23068715273704
At time: 976.2755117416382 and batch: 1150, loss is 4.097649602890015 and perplexity is 60.198630500207145
At time: 977.1225681304932 and batch: 1200, loss is 4.11590579032898 and perplexity is 61.30772106102523
At time: 978.0274479389191 and batch: 1250, loss is 4.167918634414673 and perplexity is 64.5808956716909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753761904083029 and perplexity of 116.01992041554102
Finished 43 epochs...
Completing Train Step...
At time: 980.5038666725159 and batch: 50, loss is 4.256339921951294 and perplexity is 70.55128710142249
At time: 981.400652885437 and batch: 100, loss is 4.2411823558807376 and perplexity is 69.48996515456105
At time: 982.2438049316406 and batch: 150, loss is 4.17331859588623 and perplexity is 64.93057329325455
At time: 983.0818140506744 and batch: 200, loss is 4.228892002105713 and perplexity is 68.64113579039795
At time: 983.920156955719 and batch: 250, loss is 4.238681573867797 and perplexity is 69.31640301062004
At time: 984.7604410648346 and batch: 300, loss is 4.236422452926636 and perplexity is 69.15998562243603
At time: 985.6022856235504 and batch: 350, loss is 4.211120891571045 and perplexity is 67.43208151696652
At time: 986.444760799408 and batch: 400, loss is 4.212301006317139 and perplexity is 67.51170608454679
At time: 987.2892279624939 and batch: 450, loss is 4.159828691482544 and perplexity is 64.06054754247614
At time: 988.1278529167175 and batch: 500, loss is 4.175555982589722 and perplexity is 65.07601073381839
At time: 988.9715015888214 and batch: 550, loss is 4.168459525108338 and perplexity is 64.61583632583371
At time: 989.8114743232727 and batch: 600, loss is 4.18612663269043 and perplexity is 65.76755507050142
At time: 990.652093410492 and batch: 650, loss is 4.209419393539429 and perplexity is 67.31744351881908
At time: 991.4906630516052 and batch: 700, loss is 4.1945005226135255 and perplexity is 66.32059766437276
At time: 992.3294265270233 and batch: 750, loss is 4.1604909372329715 and perplexity is 64.10298541845671
At time: 993.1710515022278 and batch: 800, loss is 4.18188554763794 and perplexity is 65.4892199144845
At time: 994.0097899436951 and batch: 850, loss is 4.208882656097412 and perplexity is 67.28132142128996
At time: 994.8509728908539 and batch: 900, loss is 4.149554705619812 and perplexity is 63.405759782415814
At time: 995.6879200935364 and batch: 950, loss is 4.148109674453735 and perplexity is 63.31420265080762
At time: 996.5240919589996 and batch: 1000, loss is 4.132205505371093 and perplexity is 62.31520801359709
At time: 997.3689498901367 and batch: 1050, loss is 4.1335394048690794 and perplexity is 62.39838570128818
At time: 998.2068748474121 and batch: 1100, loss is 4.08146026134491 and perplexity is 59.23190078374794
At time: 999.1119956970215 and batch: 1150, loss is 4.097670965194702 and perplexity is 60.19991649542949
At time: 999.9519987106323 and batch: 1200, loss is 4.115925126075744 and perplexity is 61.30890650305504
At time: 1000.7925806045532 and batch: 1250, loss is 4.167931270599365 and perplexity is 64.58171173297215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762349595118 and perplexity of 116.01997210382969
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1003.2593133449554 and batch: 50, loss is 4.256307601928711 and perplexity is 70.5490069190781
At time: 1004.1040735244751 and batch: 100, loss is 4.241155920028686 and perplexity is 69.48812815240463
At time: 1004.9481887817383 and batch: 150, loss is 4.173305568695068 and perplexity is 64.92972743577359
At time: 1005.7926115989685 and batch: 200, loss is 4.228876581192017 and perplexity is 68.64007728952848
At time: 1006.6385562419891 and batch: 250, loss is 4.238653788566589 and perplexity is 69.31447706024039
At time: 1007.482988357544 and batch: 300, loss is 4.236398429870605 and perplexity is 69.15832420818253
At time: 1008.3416180610657 and batch: 350, loss is 4.211098794937134 and perplexity is 67.43059151140949
At time: 1009.1856474876404 and batch: 400, loss is 4.212261362075806 and perplexity is 67.50902968723017
At time: 1010.0454154014587 and batch: 450, loss is 4.159814128875732 and perplexity is 64.05961466070274
At time: 1010.9026930332184 and batch: 500, loss is 4.175573091506958 and perplexity is 65.07712412342455
At time: 1011.7523186206818 and batch: 550, loss is 4.168364090919495 and perplexity is 64.6096700601485
At time: 1012.5998568534851 and batch: 600, loss is 4.186003894805908 and perplexity is 65.7594833952822
At time: 1013.4489500522614 and batch: 650, loss is 4.209330716133118 and perplexity is 67.31147424720199
At time: 1014.2965476512909 and batch: 700, loss is 4.194431219100952 and perplexity is 66.31600157326314
At time: 1015.1367931365967 and batch: 750, loss is 4.160421681404114 and perplexity is 64.09854606679659
At time: 1015.986624956131 and batch: 800, loss is 4.1817654037475585 and perplexity is 65.48135225746086
At time: 1016.8329205513 and batch: 850, loss is 4.208748292922974 and perplexity is 67.27228189666661
At time: 1017.6796600818634 and batch: 900, loss is 4.149390983581543 and perplexity is 63.39537971193259
At time: 1018.5271773338318 and batch: 950, loss is 4.147961359024048 and perplexity is 63.30481287397778
At time: 1019.3713099956512 and batch: 1000, loss is 4.132037782669068 and perplexity is 62.30475721497436
At time: 1020.2188520431519 and batch: 1050, loss is 4.133384261131287 and perplexity is 62.38870573341086
At time: 1021.1205286979675 and batch: 1100, loss is 4.081245179176331 and perplexity is 59.21916242802399
At time: 1021.9663829803467 and batch: 1150, loss is 4.09746482372284 and perplexity is 60.18750807502611
At time: 1022.8169987201691 and batch: 1200, loss is 4.115734071731567 and perplexity is 61.29719428900119
At time: 1023.6630182266235 and batch: 1250, loss is 4.167769660949707 and perplexity is 64.57127554848144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753761458570939 and perplexity of 116.01986872727537
Finished 45 epochs...
Completing Train Step...
At time: 1026.0738699436188 and batch: 50, loss is 4.256298770904541 and perplexity is 70.54838390184379
At time: 1026.9466087818146 and batch: 100, loss is 4.241144275665283 and perplexity is 69.48731901209918
At time: 1027.7874264717102 and batch: 150, loss is 4.173291764259338 and perplexity is 64.92883112371081
At time: 1028.6296796798706 and batch: 200, loss is 4.228862371444702 and perplexity is 68.63910193830428
At time: 1029.4746751785278 and batch: 250, loss is 4.238640866279602 and perplexity is 69.31358136446269
At time: 1030.3131699562073 and batch: 300, loss is 4.236388530731201 and perplexity is 69.15763960367875
At time: 1031.1561298370361 and batch: 350, loss is 4.211085090637207 and perplexity is 67.42966742869113
At time: 1031.9973187446594 and batch: 400, loss is 4.21225248336792 and perplexity is 67.50843029693687
At time: 1032.8362128734589 and batch: 450, loss is 4.159803814888001 and perplexity is 64.05895395403032
At time: 1033.6767134666443 and batch: 500, loss is 4.175565285682678 and perplexity is 65.07661614481158
At time: 1034.5184903144836 and batch: 550, loss is 4.168356595039367 and perplexity is 64.60918575562181
At time: 1035.3574192523956 and batch: 600, loss is 4.185998649597168 and perplexity is 65.75913847396974
At time: 1036.2025401592255 and batch: 650, loss is 4.2093252658844 and perplexity is 67.3111073839255
At time: 1037.0440707206726 and batch: 700, loss is 4.194425230026245 and perplexity is 66.31560440296477
At time: 1037.8881080150604 and batch: 750, loss is 4.160417098999023 and perplexity is 64.09825234196575
At time: 1038.729086637497 and batch: 800, loss is 4.181765422821045 and perplexity is 65.48135350641857
At time: 1039.5685107707977 and batch: 850, loss is 4.208747816085816 and perplexity is 67.27224981875054
At time: 1040.409634590149 and batch: 900, loss is 4.149389839172363 and perplexity is 63.3953071617196
At time: 1041.2530691623688 and batch: 950, loss is 4.147959432601929 and perplexity is 63.30469092230344
At time: 1042.0940940380096 and batch: 1000, loss is 4.132039785385132 and perplexity is 62.304881993837455
At time: 1042.9917068481445 and batch: 1050, loss is 4.133387455940246 and perplexity is 62.38890505372531
At time: 1043.8333568572998 and batch: 1100, loss is 4.081253576278686 and perplexity is 59.21965969948013
At time: 1044.6724934577942 and batch: 1150, loss is 4.097470736503601 and perplexity is 60.18786395161801
At time: 1045.5171701908112 and batch: 1200, loss is 4.115738110542297 and perplexity is 61.29744185726717
At time: 1046.3613533973694 and batch: 1250, loss is 4.167773170471191 and perplexity is 64.57150216315787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762795107208 and perplexity of 116.0200237921414
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1048.8100471496582 and batch: 50, loss is 4.256294507980346 and perplexity is 70.54808316007215
At time: 1049.6551299095154 and batch: 100, loss is 4.241139717102051 and perplexity is 69.48700225048364
At time: 1050.4983189105988 and batch: 150, loss is 4.173287019729615 and perplexity is 64.9285230676724
At time: 1051.3398237228394 and batch: 200, loss is 4.228857717514038 and perplexity is 68.63878249742635
At time: 1052.1835222244263 and batch: 250, loss is 4.238634357452392 and perplexity is 69.31313021580654
At time: 1053.0290353298187 and batch: 300, loss is 4.236384916305542 and perplexity is 69.15738963898337
At time: 1053.8753035068512 and batch: 350, loss is 4.2110785961151125 and perplexity is 67.42922950664826
At time: 1054.7180697917938 and batch: 400, loss is 4.212246761322022 and perplexity is 67.50804401170534
At time: 1055.562194108963 and batch: 450, loss is 4.159801549911499 and perplexity is 64.05880886216922
At time: 1056.4081189632416 and batch: 500, loss is 4.175570940971374 and perplexity is 65.07698417290389
At time: 1057.2543861865997 and batch: 550, loss is 4.168340191841126 and perplexity is 64.60812596703163
At time: 1058.0982475280762 and batch: 600, loss is 4.185975952148437 and perplexity is 65.75764592623426
At time: 1058.9429910182953 and batch: 650, loss is 4.2093034362792965 and perplexity is 67.30963802507009
At time: 1059.7881169319153 and batch: 700, loss is 4.194407472610473 and perplexity is 66.31442681966068
At time: 1060.6323463916779 and batch: 750, loss is 4.160401525497437 and perplexity is 64.0972541155042
At time: 1061.4751946926117 and batch: 800, loss is 4.18174400806427 and perplexity is 65.47995125417444
At time: 1062.318062543869 and batch: 850, loss is 4.208725061416626 and perplexity is 67.27071907837608
At time: 1063.1613702774048 and batch: 900, loss is 4.149361968040466 and perplexity is 63.39354028737458
At time: 1064.0673727989197 and batch: 950, loss is 4.147934002876282 and perplexity is 63.3030811218496
At time: 1064.9161689281464 and batch: 1000, loss is 4.132010798454285 and perplexity is 62.30307599270702
At time: 1065.7581124305725 and batch: 1050, loss is 4.133360047340393 and perplexity is 62.38719508462542
At time: 1066.6024057865143 and batch: 1100, loss is 4.0812158203125 and perplexity is 59.21742384621962
At time: 1067.4473361968994 and batch: 1150, loss is 4.097434153556824 and perplexity is 60.18566214246905
At time: 1068.2920293807983 and batch: 1200, loss is 4.115704207420349 and perplexity is 61.29536371784868
At time: 1069.1362545490265 and batch: 1250, loss is 4.167744436264038 and perplexity is 64.56964677889515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762349595118 and perplexity of 116.01997210382969
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1071.5719456672668 and batch: 50, loss is 4.256294260025024 and perplexity is 70.54806566730164
At time: 1072.4424965381622 and batch: 100, loss is 4.24113974571228 and perplexity is 69.48700423852272
At time: 1073.2831654548645 and batch: 150, loss is 4.173286695480346 and perplexity is 64.92850201464972
At time: 1074.123074054718 and batch: 200, loss is 4.228857440948486 and perplexity is 68.63876351430622
At time: 1074.963570356369 and batch: 250, loss is 4.238633742332459 and perplexity is 69.31308757993159
At time: 1075.8085532188416 and batch: 300, loss is 4.236385192871094 and perplexity is 69.15740876553764
At time: 1076.6497201919556 and batch: 350, loss is 4.211078062057495 and perplexity is 67.42919349556419
At time: 1077.4888949394226 and batch: 400, loss is 4.212246627807617 and perplexity is 67.50803499840963
At time: 1078.3276941776276 and batch: 450, loss is 4.159801898002624 and perplexity is 64.05883116047593
At time: 1079.1722087860107 and batch: 500, loss is 4.175573072433472 and perplexity is 65.0771228821769
At time: 1080.0155203342438 and batch: 550, loss is 4.16833797454834 and perplexity is 64.60798271205884
At time: 1080.8567349910736 and batch: 600, loss is 4.185972785949707 and perplexity is 65.75743772478879
At time: 1081.7023832798004 and batch: 650, loss is 4.209300122261047 and perplexity is 67.30941496007094
At time: 1082.5494792461395 and batch: 700, loss is 4.194404773712158 and perplexity is 66.31424784400737
At time: 1083.3962564468384 and batch: 750, loss is 4.1603994989395146 and perplexity is 64.09712421883769
At time: 1084.2402396202087 and batch: 800, loss is 4.181741080284119 and perplexity is 65.47975954355347
At time: 1085.0789694786072 and batch: 850, loss is 4.208722190856934 and perplexity is 67.27052597403858
At time: 1085.9449920654297 and batch: 900, loss is 4.149358167648315 and perplexity is 63.393299367519425
At time: 1086.7860088348389 and batch: 950, loss is 4.1479308843612674 and perplexity is 63.30288371054849
At time: 1087.621949672699 and batch: 1000, loss is 4.132006778717041 and perplexity is 62.30282555121537
At time: 1088.4657022953033 and batch: 1050, loss is 4.1333565998077395 and perplexity is 62.38698000310398
At time: 1089.3055810928345 and batch: 1100, loss is 4.081210403442383 and perplexity is 59.21710307399477
At time: 1090.145626783371 and batch: 1150, loss is 4.097429118156433 and perplexity is 60.18535908432538
At time: 1090.9850838184357 and batch: 1200, loss is 4.115699591636658 and perplexity is 61.29508079236144
At time: 1091.8248093128204 and batch: 1250, loss is 4.167740411758423 and perplexity is 64.56938691851204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762349595118 and perplexity of 116.01997210382969
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1094.2229578495026 and batch: 50, loss is 4.256294240951538 and perplexity is 70.54806432170412
At time: 1095.0975251197815 and batch: 100, loss is 4.241139812469482 and perplexity is 69.48700887728087
At time: 1095.9471850395203 and batch: 150, loss is 4.173286590576172 and perplexity is 64.92849520337919
At time: 1096.7913568019867 and batch: 200, loss is 4.228857421875 and perplexity is 68.63876220512574
At time: 1097.6364693641663 and batch: 250, loss is 4.238633594512939 and perplexity is 69.31307733410506
At time: 1098.4813923835754 and batch: 300, loss is 4.236385345458984 and perplexity is 69.15741931812158
At time: 1099.3266298770905 and batch: 350, loss is 4.211078176498413 and perplexity is 67.42920121222345
At time: 1100.1677174568176 and batch: 400, loss is 4.212246704101562 and perplexity is 67.50804014886415
At time: 1101.007542848587 and batch: 450, loss is 4.159802002906799 and perplexity is 64.05883788051513
At time: 1101.8517875671387 and batch: 500, loss is 4.175573348999023 and perplexity is 65.07714088026978
At time: 1102.6931307315826 and batch: 550, loss is 4.16833788394928 and perplexity is 64.6079768586366
At time: 1103.5358788967133 and batch: 600, loss is 4.1859724235534665 and perplexity is 65.75741389454491
At time: 1104.38130569458 and batch: 650, loss is 4.209299697875976 and perplexity is 67.30938639496615
At time: 1105.2252793312073 and batch: 700, loss is 4.194404411315918 and perplexity is 66.31422381197764
At time: 1106.0710294246674 and batch: 750, loss is 4.160399160385132 and perplexity is 64.09710251847903
At time: 1106.9123015403748 and batch: 800, loss is 4.181740927696228 and perplexity is 65.47974955213583
At time: 1107.811725616455 and batch: 850, loss is 4.208721990585327 and perplexity is 67.27051250166363
At time: 1108.6563761234283 and batch: 900, loss is 4.149357891082763 and perplexity is 63.39328183511903
At time: 1109.4991545677185 and batch: 950, loss is 4.147930607795716 and perplexity is 63.30286620315395
At time: 1110.3408737182617 and batch: 1000, loss is 4.132006554603577 and perplexity is 62.302811588314896
At time: 1111.1853172779083 and batch: 1050, loss is 4.133356366157532 and perplexity is 62.38696542637485
At time: 1112.0303752422333 and batch: 1100, loss is 4.081209907531738 and perplexity is 59.217073707610275
At time: 1112.8764503002167 and batch: 1150, loss is 4.097428655624389 and perplexity is 60.18533124667467
At time: 1113.7182157039642 and batch: 1200, loss is 4.115699281692505 and perplexity is 61.29506179431249
At time: 1114.5645768642426 and batch: 1250, loss is 4.167740092277527 and perplexity is 64.56936628982974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762349595118 and perplexity of 116.01997210382969
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1117.0052301883698 and batch: 50, loss is 4.256294298171997 and perplexity is 70.54806835849683
At time: 1117.8451766967773 and batch: 100, loss is 4.24113977432251 and perplexity is 69.4870062265619
At time: 1118.6908984184265 and batch: 150, loss is 4.173286600112915 and perplexity is 64.92849582258556
At time: 1119.5372834205627 and batch: 200, loss is 4.2288574123382565 and perplexity is 68.63876155053546
At time: 1120.3846638202667 and batch: 250, loss is 4.238633661270142 and perplexity is 69.31308196125235
At time: 1121.2237474918365 and batch: 300, loss is 4.236385374069214 and perplexity is 69.15742129673127
At time: 1122.066551208496 and batch: 350, loss is 4.211078147888184 and perplexity is 67.4291992830586
At time: 1122.905262708664 and batch: 400, loss is 4.212246694564819 and perplexity is 67.50803950505733
At time: 1123.7468552589417 and batch: 450, loss is 4.159801988601685 and perplexity is 64.05883696414612
At time: 1124.5965530872345 and batch: 500, loss is 4.175573372840882 and perplexity is 65.07714243182977
At time: 1125.443273305893 and batch: 550, loss is 4.168337869644165 and perplexity is 64.60797593441205
At time: 1126.2845244407654 and batch: 600, loss is 4.185972356796265 and perplexity is 65.75740950476413
At time: 1127.126915216446 and batch: 650, loss is 4.209299612045288 and perplexity is 67.30938061775544
At time: 1127.9703345298767 and batch: 700, loss is 4.194404382705688 and perplexity is 66.31422191471248
At time: 1128.8105607032776 and batch: 750, loss is 4.160399160385132 and perplexity is 64.09710251847903
At time: 1129.7116587162018 and batch: 800, loss is 4.181740922927856 and perplexity is 65.47974923990407
At time: 1130.5521266460419 and batch: 850, loss is 4.208722009658813 and perplexity is 67.27051378474681
At time: 1131.390694618225 and batch: 900, loss is 4.1493579339981075 and perplexity is 63.3932845556636
At time: 1132.2331929206848 and batch: 950, loss is 4.14793062210083 and perplexity is 63.302867108708696
At time: 1133.0728776454926 and batch: 1000, loss is 4.13200647354126 and perplexity is 62.30280653790485
At time: 1133.9131789207458 and batch: 1050, loss is 4.133356404304505 and perplexity is 62.38696780624876
At time: 1134.7520368099213 and batch: 1100, loss is 4.081209869384765 and perplexity is 59.217071448658224
At time: 1135.5969724655151 and batch: 1150, loss is 4.097428669929505 and perplexity is 60.18533210763278
At time: 1136.4372696876526 and batch: 1200, loss is 4.115699214935303 and perplexity is 61.29505770242577
At time: 1137.2816650867462 and batch: 1250, loss is 4.16774007320404 and perplexity is 64.56936505826681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.753762349595118 and perplexity of 116.01997210382969
Annealing...
Model not improving. Stopping early with 116.01986872727537loss at 49 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe70d2b860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'num_layers': 1, 'anneal': 8.0, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.4245306207484014, 'tune_wordvecs': True, 'lr': 4.453365840901657, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4638962745666504 and batch: 50, loss is 7.321816701889038 and perplexity is 1512.950053820745
At time: 2.3016161918640137 and batch: 100, loss is 6.453088836669922 and perplexity is 634.6596282439338
At time: 3.139352321624756 and batch: 150, loss is 6.1569561195373534 and perplexity is 471.9892073486418
At time: 3.980738878250122 and batch: 200, loss is 6.054516191482544 and perplexity is 426.03273731864897
At time: 4.823016166687012 and batch: 250, loss is 6.005525178909302 and perplexity is 405.6639789675334
At time: 5.66290020942688 and batch: 300, loss is 5.961948356628418 and perplexity is 388.36606317276676
At time: 6.534141778945923 and batch: 350, loss is 5.95638710975647 and perplexity is 386.2122580906394
At time: 7.375771760940552 and batch: 400, loss is 5.874098424911499 and perplexity is 355.7038223574741
At time: 8.216484308242798 and batch: 450, loss is 5.824537773132324 and perplexity is 338.5046309757056
At time: 9.061376094818115 and batch: 500, loss is 5.806756715774537 and perplexity is 332.5388567834174
At time: 9.9083571434021 and batch: 550, loss is 5.800788202285767 and perplexity is 330.5600054061412
At time: 10.75399661064148 and batch: 600, loss is 5.809697065353394 and perplexity is 333.51807618909834
At time: 11.5955069065094 and batch: 650, loss is 5.774866418838501 and perplexity is 322.10140516474917
At time: 12.44456148147583 and batch: 700, loss is 5.7705082321167 and perplexity is 320.7006816221456
At time: 13.285817384719849 and batch: 750, loss is 5.709542875289917 and perplexity is 301.7331070933557
At time: 14.130045175552368 and batch: 800, loss is 5.713175163269043 and perplexity is 302.83108150360295
At time: 14.977097988128662 and batch: 850, loss is 5.758450126647949 and perplexity is 316.8568601550922
At time: 15.822140455245972 and batch: 900, loss is 5.731404886245728 and perplexity is 308.40223422933406
At time: 16.66575026512146 and batch: 950, loss is 5.694196434020996 and perplexity is 297.1379276969413
At time: 17.5086510181427 and batch: 1000, loss is 5.672759466171264 and perplexity is 290.83598015055594
At time: 18.350064039230347 and batch: 1050, loss is 5.657117280960083 and perplexity is 286.32206566375356
At time: 19.191296100616455 and batch: 1100, loss is 5.640994453430176 and perplexity is 281.7427592618164
At time: 20.0362446308136 and batch: 1150, loss is 5.677785606384277 and perplexity is 292.3014422898249
At time: 20.8809654712677 and batch: 1200, loss is 5.66926986694336 and perplexity is 289.8228478796721
At time: 21.722159147262573 and batch: 1250, loss is 5.638027811050415 and perplexity is 280.9081678312543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.137915395472172 and perplexity of 170.360264129272
Finished 1 epochs...
Completing Train Step...
At time: 24.21048092842102 and batch: 50, loss is 5.3767622947692875 and perplexity is 216.32075753551177
At time: 25.055458784103394 and batch: 100, loss is 5.358374547958374 and perplexity is 212.37945311852457
At time: 25.895857095718384 and batch: 150, loss is 5.212512941360473 and perplexity is 183.5547414179445
At time: 26.73516082763672 and batch: 200, loss is 5.2223709869384765 and perplexity is 185.37318082772674
At time: 27.57632827758789 and batch: 250, loss is 5.224137372970581 and perplexity is 185.7009107886673
At time: 28.41829252243042 and batch: 300, loss is 5.214195680618286 and perplexity is 183.86387621096648
At time: 29.257317304611206 and batch: 350, loss is 5.216777086257935 and perplexity is 184.33911658813682
At time: 30.099643230438232 and batch: 400, loss is 5.184724760055542 and perplexity is 178.5243060069002
At time: 30.94002938270569 and batch: 450, loss is 5.130341720581055 and perplexity is 169.07488454211207
At time: 31.780604124069214 and batch: 500, loss is 5.119864749908447 and perplexity is 167.31273902747222
At time: 32.688830852508545 and batch: 550, loss is 5.127426843643189 and perplexity is 168.5827696360213
At time: 33.52757930755615 and batch: 600, loss is 5.1265538311004635 and perplexity is 168.43565898770564
At time: 34.369513511657715 and batch: 650, loss is 5.103263626098633 and perplexity is 164.5580879479752
At time: 35.2104172706604 and batch: 700, loss is 5.0964734172821045 and perplexity is 163.44448923098832
At time: 36.052666425704956 and batch: 750, loss is 5.071035099029541 and perplexity is 159.33917383656217
At time: 36.89398431777954 and batch: 800, loss is 5.082529487609864 and perplexity is 161.18124668594305
At time: 37.73664212226868 and batch: 850, loss is 5.125436019897461 and perplexity is 168.24748491224258
At time: 38.5745484828949 and batch: 900, loss is 5.088028535842896 and perplexity is 162.07003163166974
At time: 39.414464712142944 and batch: 950, loss is 5.063971672058106 and perplexity is 158.2176587513118
At time: 40.25846076011658 and batch: 1000, loss is 5.043464059829712 and perplexity is 155.00603626865632
At time: 41.09538745880127 and batch: 1050, loss is 5.026708345413208 and perplexity is 152.4304376484159
At time: 41.93412923812866 and batch: 1100, loss is 4.992739114761353 and perplexity is 147.33945093949802
At time: 42.77412295341492 and batch: 1150, loss is 5.0256248950958256 and perplexity is 152.26537627641147
At time: 43.61446976661682 and batch: 1200, loss is 5.013138761520386 and perplexity is 150.37599055730354
At time: 44.458030700683594 and batch: 1250, loss is 4.998972730636597 and perplexity is 148.26077709319003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.825055421703923 and perplexity of 124.59337336890016
Finished 2 epochs...
Completing Train Step...
At time: 46.94356036186218 and batch: 50, loss is 4.917300958633422 and perplexity is 136.63333603747648
At time: 47.78368043899536 and batch: 100, loss is 4.941523418426514 and perplexity is 139.98334040570973
At time: 48.62503528594971 and batch: 150, loss is 4.817501106262207 and perplexity is 123.655701916713
At time: 49.46603727340698 and batch: 200, loss is 4.863040971755981 and perplexity is 129.41715824325797
At time: 50.301652669906616 and batch: 250, loss is 4.868543815612793 and perplexity is 130.13128371589548
At time: 51.13913321495056 and batch: 300, loss is 4.874575262069702 and perplexity is 130.91853533240874
At time: 51.98143815994263 and batch: 350, loss is 4.871103239059448 and perplexity is 130.46477136058311
At time: 52.818002462387085 and batch: 400, loss is 4.862754411697388 and perplexity is 129.38007776795413
At time: 53.6890435218811 and batch: 450, loss is 4.805521678924561 and perplexity is 122.18321480615852
At time: 54.53164529800415 and batch: 500, loss is 4.805206899642944 and perplexity is 122.14476011426392
At time: 55.37030339241028 and batch: 550, loss is 4.8292138957977295 and perplexity is 125.1125704699598
At time: 56.21312069892883 and batch: 600, loss is 4.824576053619385 and perplexity is 124.53366159526487
At time: 57.055070638656616 and batch: 650, loss is 4.821723918914795 and perplexity is 124.17898085619615
At time: 57.900880336761475 and batch: 700, loss is 4.814901704788208 and perplexity is 123.3346885050044
At time: 58.740869760513306 and batch: 750, loss is 4.800815916061401 and perplexity is 121.60960027691634
At time: 59.58317565917969 and batch: 800, loss is 4.82201491355896 and perplexity is 124.21512153265978
At time: 60.42329740524292 and batch: 850, loss is 4.867192764282226 and perplexity is 129.95558838531312
At time: 61.2630512714386 and batch: 900, loss is 4.834478521347046 and perplexity is 125.77297817930994
At time: 62.10372495651245 and batch: 950, loss is 4.816215486526489 and perplexity is 123.49682985230774
At time: 62.94496393203735 and batch: 1000, loss is 4.801273908615112 and perplexity is 121.66530932449366
At time: 63.7886164188385 and batch: 1050, loss is 4.777204704284668 and perplexity is 118.77188305371652
At time: 64.63227987289429 and batch: 1100, loss is 4.7472686672210695 and perplexity is 115.26901612958012
At time: 65.48772835731506 and batch: 1150, loss is 4.774534034729004 and perplexity is 118.45510579355218
At time: 66.35489320755005 and batch: 1200, loss is 4.772148466110229 and perplexity is 118.1728598028802
At time: 67.20301342010498 and batch: 1250, loss is 4.770176773071289 and perplexity is 117.9400887497573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.723975411296761 and perplexity of 112.61505512994572
Finished 3 epochs...
Completing Train Step...
At time: 69.648366689682 and batch: 50, loss is 4.703147897720337 and perplexity is 110.29382022525854
At time: 70.52235865592957 and batch: 100, loss is 4.729256362915039 and perplexity is 113.21134288517511
At time: 71.36989426612854 and batch: 150, loss is 4.617502164840698 and perplexity is 101.24083312396854
At time: 72.21454930305481 and batch: 200, loss is 4.669972791671753 and perplexity is 106.69483940474491
At time: 73.05719685554504 and batch: 250, loss is 4.673458137512207 and perplexity is 107.06735661802189
At time: 73.9024293422699 and batch: 300, loss is 4.686514520645142 and perplexity is 108.47443473398774
At time: 74.74517512321472 and batch: 350, loss is 4.67927508354187 and perplexity is 107.69197658205358
At time: 75.64933824539185 and batch: 400, loss is 4.68121563911438 and perplexity is 107.90116174940059
At time: 76.49277949333191 and batch: 450, loss is 4.6131057453155515 and perplexity is 100.7967129331674
At time: 77.33680963516235 and batch: 500, loss is 4.623269529342651 and perplexity is 101.82641291397005
At time: 78.18290638923645 and batch: 550, loss is 4.647194290161133 and perplexity is 104.29196171627125
At time: 79.02917385101318 and batch: 600, loss is 4.644532299041748 and perplexity is 104.01470662937247
At time: 79.87657356262207 and batch: 650, loss is 4.652006568908692 and perplexity is 104.7950532446311
At time: 80.72134780883789 and batch: 700, loss is 4.642332944869995 and perplexity is 103.78619283392133
At time: 81.56500720977783 and batch: 750, loss is 4.637664737701416 and perplexity is 103.302826489451
At time: 82.41123080253601 and batch: 800, loss is 4.663521223068237 and perplexity is 106.00870602685757
At time: 83.25540971755981 and batch: 850, loss is 4.708909025192261 and perplexity is 110.93107086145571
At time: 84.1010901927948 and batch: 900, loss is 4.667591648101807 and perplexity is 106.44108590560155
At time: 84.94505643844604 and batch: 950, loss is 4.660546855926514 and perplexity is 105.69386567242957
At time: 85.78736782073975 and batch: 1000, loss is 4.6402575874328615 and perplexity is 103.5710227414076
At time: 86.6306095123291 and batch: 1050, loss is 4.619483604431152 and perplexity is 101.4416345911821
At time: 87.47398281097412 and batch: 1100, loss is 4.5802366065979 and perplexity is 97.53746948588758
At time: 88.31574654579163 and batch: 1150, loss is 4.610702705383301 and perplexity is 100.55478520436557
At time: 89.15863609313965 and batch: 1200, loss is 4.615164194107056 and perplexity is 101.00441150018587
At time: 90.0053539276123 and batch: 1250, loss is 4.621196231842041 and perplexity is 101.6155151690096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.673594676665146 and perplexity of 107.0819765022725
Finished 4 epochs...
Completing Train Step...
At time: 92.47177910804749 and batch: 50, loss is 4.552779197692871 and perplexity is 94.89577629210117
At time: 93.3126528263092 and batch: 100, loss is 4.581710157394409 and perplexity is 97.68130184785784
At time: 94.16381812095642 and batch: 150, loss is 4.47755805015564 and perplexity is 88.01947089336285
At time: 95.00236916542053 and batch: 200, loss is 4.53202618598938 and perplexity is 92.9466977145105
At time: 95.8548800945282 and batch: 250, loss is 4.5317467594146725 and perplexity is 92.92072956540225
At time: 96.70028710365295 and batch: 300, loss is 4.549899425506592 and perplexity is 94.62289118699597
At time: 97.5762083530426 and batch: 350, loss is 4.539566907882691 and perplexity is 93.6502321579441
At time: 98.42199969291687 and batch: 400, loss is 4.548061141967773 and perplexity is 94.44910726473212
At time: 99.26762318611145 and batch: 450, loss is 4.474810066223145 and perplexity is 87.7779268331993
At time: 100.10981702804565 and batch: 500, loss is 4.498417053222656 and perplexity is 89.87475169202544
At time: 100.95066118240356 and batch: 550, loss is 4.520005207061768 and perplexity is 91.83607617303295
At time: 101.78885579109192 and batch: 600, loss is 4.516168699264527 and perplexity is 91.48442134529908
At time: 102.62813758850098 and batch: 650, loss is 4.528611545562744 and perplexity is 92.62985941526
At time: 103.4669771194458 and batch: 700, loss is 4.519409160614014 and perplexity is 91.78135391617786
At time: 104.3062117099762 and batch: 750, loss is 4.5164447784423825 and perplexity is 91.50968177590971
At time: 105.15245223045349 and batch: 800, loss is 4.543674917221069 and perplexity is 94.03573947788209
At time: 105.99425792694092 and batch: 850, loss is 4.586380853652954 and perplexity is 98.13860867821843
At time: 106.83022785186768 and batch: 900, loss is 4.544418478012085 and perplexity is 94.10568676852073
At time: 107.67200064659119 and batch: 950, loss is 4.540074605941772 and perplexity is 93.69779027060243
At time: 108.51894068717957 and batch: 1000, loss is 4.520695514678955 and perplexity is 91.89949320205858
At time: 109.35906982421875 and batch: 1050, loss is 4.4970087051391605 and perplexity is 89.74826584668072
At time: 110.197993516922 and batch: 1100, loss is 4.459735975265503 and perplexity is 86.46467726891916
At time: 111.04114174842834 and batch: 1150, loss is 4.486284551620483 and perplexity is 88.7909341268788
At time: 111.88210844993591 and batch: 1200, loss is 4.492422332763672 and perplexity is 89.33758935698228
At time: 112.72386574745178 and batch: 1250, loss is 4.504661502838135 and perplexity is 90.43772595353151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.653357735515511 and perplexity of 104.93674452384104
Finished 5 epochs...
Completing Train Step...
At time: 115.18886995315552 and batch: 50, loss is 4.4388895511627195 and perplexity is 84.68085568692126
At time: 116.03034234046936 and batch: 100, loss is 4.466225280761718 and perplexity is 87.02759747952676
At time: 116.8728563785553 and batch: 150, loss is 4.3695792293548585 and perplexity is 79.01037945593451
At time: 117.71612000465393 and batch: 200, loss is 4.425376558303833 and perplexity is 83.54426058471789
At time: 118.61026763916016 and batch: 250, loss is 4.421880950927735 and perplexity is 83.25273248193545
At time: 119.45859050750732 and batch: 300, loss is 4.441709032058716 and perplexity is 84.9199486424088
At time: 120.29927229881287 and batch: 350, loss is 4.427972259521485 and perplexity is 83.761398213933
At time: 121.14476275444031 and batch: 400, loss is 4.440690898895264 and perplexity is 84.83353282532678
At time: 121.99160408973694 and batch: 450, loss is 4.365646467208863 and perplexity is 78.70026063795083
At time: 122.8384599685669 and batch: 500, loss is 4.394806051254273 and perplexity is 81.0289137824367
At time: 123.68566608428955 and batch: 550, loss is 4.410491313934326 and perplexity is 82.30989356910466
At time: 124.52990293502808 and batch: 600, loss is 4.411431617736817 and perplexity is 82.3873262744386
At time: 125.37069368362427 and batch: 650, loss is 4.428551797866821 and perplexity is 83.80995522502285
At time: 126.2127161026001 and batch: 700, loss is 4.4191070079803465 and perplexity is 83.02211416072859
At time: 127.0529215335846 and batch: 750, loss is 4.417377643585205 and perplexity is 82.87866274812401
At time: 127.89831733703613 and batch: 800, loss is 4.443787260055542 and perplexity is 85.0966151702429
At time: 128.7415874004364 and batch: 850, loss is 4.490868034362793 and perplexity is 89.19883994163663
At time: 129.583261013031 and batch: 900, loss is 4.44533709526062 and perplexity is 85.22860315365894
At time: 130.42244148254395 and batch: 950, loss is 4.441160058975219 and perplexity is 84.87334267023479
At time: 131.26530170440674 and batch: 1000, loss is 4.423405628204346 and perplexity is 83.37976284691362
At time: 132.10428524017334 and batch: 1050, loss is 4.402829790115357 and perplexity is 81.68168395338911
At time: 132.94608235359192 and batch: 1100, loss is 4.360768127441406 and perplexity is 78.3172689680608
At time: 133.78631258010864 and batch: 1150, loss is 4.387338809967041 and perplexity is 80.42610479310206
At time: 134.63214707374573 and batch: 1200, loss is 4.3966358947753905 and perplexity is 81.17731975378798
At time: 135.47540760040283 and batch: 1250, loss is 4.409480628967285 and perplexity is 82.22674622200029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.642488131557938 and perplexity of 103.80230031924275
Finished 6 epochs...
Completing Train Step...
At time: 137.90014266967773 and batch: 50, loss is 4.347249336242676 and perplexity is 77.2656385636367
At time: 138.77530884742737 and batch: 100, loss is 4.372237291336059 and perplexity is 79.22067330491338
At time: 139.61581420898438 and batch: 150, loss is 4.281380004882813 and perplexity is 72.3402009867592
At time: 140.48677134513855 and batch: 200, loss is 4.335103960037231 and perplexity is 76.33289404779808
At time: 141.32509446144104 and batch: 250, loss is 4.331477966308594 and perplexity is 76.05661265270332
At time: 142.16670060157776 and batch: 300, loss is 4.353213081359863 and perplexity is 77.72780789860236
At time: 143.0151629447937 and batch: 350, loss is 4.337121067047119 and perplexity is 76.48702105654345
At time: 143.85550475120544 and batch: 400, loss is 4.349881038665772 and perplexity is 77.4692465321454
At time: 144.69809794425964 and batch: 450, loss is 4.275167713165283 and perplexity is 71.89219556631187
At time: 145.54330611228943 and batch: 500, loss is 4.3104890441894534 and perplexity is 74.4769025321238
At time: 146.38977527618408 and batch: 550, loss is 4.322821860313415 and perplexity is 75.40109973960267
At time: 147.232115983963 and batch: 600, loss is 4.327448062896728 and perplexity is 75.75072860542821
At time: 148.07367253303528 and batch: 650, loss is 4.34582631111145 and perplexity is 77.15576581256447
At time: 148.9120044708252 and batch: 700, loss is 4.3364269733428955 and perplexity is 76.43395031693709
At time: 149.7538938522339 and batch: 750, loss is 4.331468906402588 and perplexity is 76.05592359006299
At time: 150.5939028263092 and batch: 800, loss is 4.362082915306091 and perplexity is 78.42030728481629
At time: 151.43566346168518 and batch: 850, loss is 4.405009708404541 and perplexity is 81.85993756869946
At time: 152.2785656452179 and batch: 900, loss is 4.359228038787842 and perplexity is 78.19674626239276
At time: 153.12141108512878 and batch: 950, loss is 4.357812280654907 and perplexity is 78.08611691358144
At time: 153.96480870246887 and batch: 1000, loss is 4.340546617507934 and perplexity is 76.74948048412648
At time: 154.80612993240356 and batch: 1050, loss is 4.320247488021851 and perplexity is 75.20723887975852
At time: 155.64866280555725 and batch: 1100, loss is 4.27805157661438 and perplexity is 72.09982208074217
At time: 156.4900884628296 and batch: 1150, loss is 4.30612732887268 and perplexity is 74.15276290188295
At time: 157.33252716064453 and batch: 1200, loss is 4.313813076019287 and perplexity is 74.72487803763742
At time: 158.17384672164917 and batch: 1250, loss is 4.328455572128296 and perplexity is 75.82708662304088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.637110800638686 and perplexity of 103.24561907129109
Finished 7 epochs...
Completing Train Step...
At time: 160.6323938369751 and batch: 50, loss is 4.26598219871521 and perplexity is 71.23485240451078
At time: 161.4722716808319 and batch: 100, loss is 4.292176942825318 and perplexity is 73.12548535222057
At time: 162.36541032791138 and batch: 150, loss is 4.207673563957214 and perplexity is 67.20002126397237
At time: 163.19991731643677 and batch: 200, loss is 4.259708023071289 and perplexity is 70.78931159078596
At time: 164.0425865650177 and batch: 250, loss is 4.253005743026733 and perplexity is 70.31644820187681
At time: 164.88498449325562 and batch: 300, loss is 4.273471784591675 and perplexity is 71.77037486643256
At time: 165.7273440361023 and batch: 350, loss is 4.25855673789978 and perplexity is 70.70785980216513
At time: 166.56682682037354 and batch: 400, loss is 4.278054685592651 and perplexity is 72.10004623787084
At time: 167.4072937965393 and batch: 450, loss is 4.199466104507446 and perplexity is 66.65073701195875
At time: 168.24679446220398 and batch: 500, loss is 4.235246458053589 and perplexity is 69.0787016380662
At time: 169.0887908935547 and batch: 550, loss is 4.246256761550903 and perplexity is 69.8434816121443
At time: 169.9351568222046 and batch: 600, loss is 4.255389432907105 and perplexity is 70.4842607349388
At time: 170.77829384803772 and batch: 650, loss is 4.275685520172119 and perplexity is 71.92943148859167
At time: 171.6153266429901 and batch: 700, loss is 4.266482162475586 and perplexity is 71.27047615373853
At time: 172.4584276676178 and batch: 750, loss is 4.256935729980468 and perplexity is 70.59333464963568
At time: 173.2976689338684 and batch: 800, loss is 4.291647453308105 and perplexity is 73.08677642317592
At time: 174.1360845565796 and batch: 850, loss is 4.334816284179688 and perplexity is 76.31093807529713
At time: 174.97790217399597 and batch: 900, loss is 4.288475732803345 and perplexity is 72.85533282699079
At time: 175.8250172138214 and batch: 950, loss is 4.286142606735229 and perplexity is 72.68555028981983
At time: 176.66731572151184 and batch: 1000, loss is 4.270540962219238 and perplexity is 71.56033658888477
At time: 177.50619745254517 and batch: 1050, loss is 4.250216369628906 and perplexity is 70.12058266988105
At time: 178.3452184200287 and batch: 1100, loss is 4.209371790885926 and perplexity is 67.31423910615038
At time: 179.18263030052185 and batch: 1150, loss is 4.235295791625976 and perplexity is 69.08210962125719
At time: 180.0207757949829 and batch: 1200, loss is 4.24188265800476 and perplexity is 69.53864616847626
At time: 180.86075282096863 and batch: 1250, loss is 4.25860032081604 and perplexity is 70.71094152405253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.636000138999772 and perplexity of 103.13101177954525
Finished 8 epochs...
Completing Train Step...
At time: 183.32083654403687 and batch: 50, loss is 4.199530696868896 and perplexity is 66.65504227949693
At time: 184.21517753601074 and batch: 100, loss is 4.222468099594116 and perplexity is 68.20160508847613
At time: 185.06016182899475 and batch: 150, loss is 4.140607013702392 and perplexity is 62.840955195052445
At time: 185.9037036895752 and batch: 200, loss is 4.195146532058716 and perplexity is 66.36345523858299
At time: 186.75180339813232 and batch: 250, loss is 4.1888428926467896 and perplexity is 65.94643968533994
At time: 187.59531927108765 and batch: 300, loss is 4.208556108474731 and perplexity is 67.25935445255492
At time: 188.44209241867065 and batch: 350, loss is 4.192113499641419 and perplexity is 66.16247766733574
At time: 189.2842311859131 and batch: 400, loss is 4.210133380889893 and perplexity is 67.36552448451518
At time: 190.13128685951233 and batch: 450, loss is 4.132542495727539 and perplexity is 62.336211176490494
At time: 190.97854685783386 and batch: 500, loss is 4.170039315223693 and perplexity is 64.71799645989464
At time: 191.82478523254395 and batch: 550, loss is 4.18583815574646 and perplexity is 65.74858538349253
At time: 192.66971278190613 and batch: 600, loss is 4.1918989944458005 and perplexity is 66.14828699416208
At time: 193.51435327529907 and batch: 650, loss is 4.214403982162476 and perplexity is 67.65383096193312
At time: 194.36051154136658 and batch: 700, loss is 4.203009605407715 and perplexity is 66.88733289986912
At time: 195.20632481575012 and batch: 750, loss is 4.194060792922974 and perplexity is 66.29144093949323
At time: 196.05513048171997 and batch: 800, loss is 4.228617935180664 and perplexity is 68.62232610305436
At time: 196.8987534046173 and batch: 850, loss is 4.274755487442016 and perplexity is 71.86256586148016
At time: 197.74322080612183 and batch: 900, loss is 4.223565330505371 and perplexity is 68.27647906728659
At time: 198.58890962600708 and batch: 950, loss is 4.2221336174011235 and perplexity is 68.17879668075628
At time: 199.43494367599487 and batch: 1000, loss is 4.211498713493347 and perplexity is 67.45756364918104
At time: 200.2841773033142 and batch: 1050, loss is 4.19101553440094 and perplexity is 66.08987343239002
At time: 201.13029742240906 and batch: 1100, loss is 4.1474854326248165 and perplexity is 63.27469161065338
At time: 201.97818303108215 and batch: 1150, loss is 4.171937246322631 and perplexity is 64.84094339353523
At time: 202.82338333129883 and batch: 1200, loss is 4.17994975566864 and perplexity is 65.36256903292785
At time: 203.66902947425842 and batch: 1250, loss is 4.1968135213851925 and perplexity is 66.4741746685484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.642452936102874 and perplexity of 103.79864701433654
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 206.13737034797668 and batch: 50, loss is 4.177606115341186 and perplexity is 65.20956204694214
At time: 206.97821855545044 and batch: 100, loss is 4.221152448654175 and perplexity is 68.11193458311281
At time: 207.81673455238342 and batch: 150, loss is 4.1402703428268435 and perplexity is 62.819802036672584
At time: 208.66205620765686 and batch: 200, loss is 4.205503511428833 and perplexity is 67.05435180027989
At time: 209.50028729438782 and batch: 250, loss is 4.197454805374146 and perplexity is 66.51681716395562
At time: 210.34220814704895 and batch: 300, loss is 4.201858749389649 and perplexity is 66.81039948840004
At time: 211.18777084350586 and batch: 350, loss is 4.181163635253906 and perplexity is 65.4419594966005
At time: 212.02949357032776 and batch: 400, loss is 4.184027605056762 and perplexity is 65.62965193688629
At time: 212.87135529518127 and batch: 450, loss is 4.101196484565735 and perplexity is 60.41252702822824
At time: 213.72401976585388 and batch: 500, loss is 4.121673760414123 and perplexity is 61.66236396368877
At time: 214.58555245399475 and batch: 550, loss is 4.119947152137756 and perplexity is 61.55598907603629
At time: 215.44887614250183 and batch: 600, loss is 4.114564566612244 and perplexity is 61.22554880952361
At time: 216.29779529571533 and batch: 650, loss is 4.132084012031555 and perplexity is 62.3076375907588
At time: 217.14730834960938 and batch: 700, loss is 4.113324694633484 and perplexity is 61.149684008209924
At time: 217.99140644073486 and batch: 750, loss is 4.093408269882202 and perplexity is 59.94384875070043
At time: 218.83304929733276 and batch: 800, loss is 4.1056349658966065 and perplexity is 60.68126284988037
At time: 219.67347049713135 and batch: 850, loss is 4.134492521286011 and perplexity is 62.4578869784077
At time: 220.51136183738708 and batch: 900, loss is 4.0698224067687985 and perplexity is 58.546564195066104
At time: 221.35241222381592 and batch: 950, loss is 4.052571058273315 and perplexity is 57.54521912331008
At time: 222.19438362121582 and batch: 1000, loss is 4.023313751220703 and perplexity is 55.88599159776326
At time: 223.03713250160217 and batch: 1050, loss is 3.9939754676818846 and perplexity is 54.27021054665703
At time: 223.87511897087097 and batch: 1100, loss is 3.9227528524398805 and perplexity is 50.53938091304249
At time: 224.71435117721558 and batch: 1150, loss is 3.921826915740967 and perplexity is 50.49260630402015
At time: 225.5579788684845 and batch: 1200, loss is 3.9180276870727537 and perplexity is 50.301137294200174
At time: 226.40029001235962 and batch: 1250, loss is 3.9130930852890016 and perplexity is 50.0535326300292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.540572340471031 and perplexity of 93.74443850439066
Finished 10 epochs...
Completing Train Step...
At time: 228.871018409729 and batch: 50, loss is 4.100498166084289 and perplexity is 60.370354570710624
At time: 229.74206018447876 and batch: 100, loss is 4.13450930595398 and perplexity is 62.45893532210072
At time: 230.5856215953827 and batch: 150, loss is 4.052145438194275 and perplexity is 57.520731934087785
At time: 231.43097591400146 and batch: 200, loss is 4.113446583747864 and perplexity is 61.15713794330587
At time: 232.2727234363556 and batch: 250, loss is 4.108861389160157 and perplexity is 60.87736246807264
At time: 233.11853528022766 and batch: 300, loss is 4.118656520843506 and perplexity is 61.47659423593968
At time: 233.95988631248474 and batch: 350, loss is 4.100662794113159 and perplexity is 60.380294041321044
At time: 234.80675983428955 and batch: 400, loss is 4.110480427742004 and perplexity is 60.97600509823406
At time: 235.65675616264343 and batch: 450, loss is 4.0294038105010985 and perplexity is 56.22737907940004
At time: 236.4991226196289 and batch: 500, loss is 4.053715481758117 and perplexity is 57.61111292152618
At time: 237.34259009361267 and batch: 550, loss is 4.053542804718018 and perplexity is 57.60116566392626
At time: 238.18360567092896 and batch: 600, loss is 4.053666753768921 and perplexity is 57.608305716233424
At time: 239.03134846687317 and batch: 650, loss is 4.076745977401734 and perplexity is 58.953321950757726
At time: 239.87596011161804 and batch: 700, loss is 4.059536285400391 and perplexity is 57.94743377669005
At time: 240.72100520133972 and batch: 750, loss is 4.045923132896423 and perplexity is 57.16393159056019
At time: 241.5687255859375 and batch: 800, loss is 4.064037971496582 and perplexity is 58.20888297321289
At time: 242.41314816474915 and batch: 850, loss is 4.098310146331787 and perplexity is 60.23840744654197
At time: 243.26003670692444 and batch: 900, loss is 4.037932896614075 and perplexity is 56.708998203341935
At time: 244.10374450683594 and batch: 950, loss is 4.026832256317139 and perplexity is 56.08297308097845
At time: 244.9513761997223 and batch: 1000, loss is 4.004463815689087 and perplexity is 54.842410873956176
At time: 245.80184984207153 and batch: 1050, loss is 3.9815326929092407 and perplexity is 53.59912229820744
At time: 246.64539909362793 and batch: 1100, loss is 3.9169149875640867 and perplexity is 50.2451983708253
At time: 247.49116134643555 and batch: 1150, loss is 3.9241289710998535 and perplexity is 50.60897697341623
At time: 248.3429081439972 and batch: 1200, loss is 3.930421996116638 and perplexity is 50.92846474950096
At time: 249.24273419380188 and batch: 1250, loss is 3.9345267248153686 and perplexity is 51.13794190967631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536682574418339 and perplexity of 93.38050284103258
Finished 11 epochs...
Completing Train Step...
At time: 251.68915939331055 and batch: 50, loss is 4.071355156898498 and perplexity is 58.63637025647375
At time: 252.58482193946838 and batch: 100, loss is 4.100298914909363 and perplexity is 60.35832690493474
At time: 253.42500758171082 and batch: 150, loss is 4.017823171615601 and perplexity is 55.579985955004865
At time: 254.26870560646057 and batch: 200, loss is 4.077276711463928 and perplexity is 58.98461879121151
At time: 255.10890007019043 and batch: 250, loss is 4.073547644615173 and perplexity is 58.765070813651526
At time: 255.9516363143921 and batch: 300, loss is 4.085036916732788 and perplexity is 59.44413219388181
At time: 256.7934715747833 and batch: 350, loss is 4.066752886772155 and perplexity is 58.367129875036426
At time: 257.63895869255066 and batch: 400, loss is 4.077706823348999 and perplexity is 59.00999423353844
At time: 258.48123145103455 and batch: 450, loss is 3.9973809146881103 and perplexity is 54.45533991790879
At time: 259.3228032588959 and batch: 500, loss is 4.0233499670028685 and perplexity is 55.888015589311046
At time: 260.16104459762573 and batch: 550, loss is 4.024133820533752 and perplexity is 55.931840781686745
At time: 260.9995958805084 and batch: 600, loss is 4.025719265937806 and perplexity is 56.020587994935156
At time: 261.83951354026794 and batch: 650, loss is 4.050734062194824 and perplexity is 57.439605816759425
At time: 262.6786036491394 and batch: 700, loss is 4.034613094329834 and perplexity is 56.521047693453696
At time: 263.5200951099396 and batch: 750, loss is 4.0235078191757205 and perplexity is 55.896838330334305
At time: 264.36520504951477 and batch: 800, loss is 4.044281392097473 and perplexity is 57.07016022703817
At time: 265.2073037624359 and batch: 850, loss is 4.080331707000733 and perplexity is 59.16509207052716
At time: 266.04579734802246 and batch: 900, loss is 4.021888432502746 and perplexity is 55.80639298803716
At time: 266.89134764671326 and batch: 950, loss is 4.013893599510193 and perplexity is 55.36200895124551
At time: 267.7370619773865 and batch: 1000, loss is 3.9948117876052858 and perplexity is 54.31561678941057
At time: 268.58160495758057 and batch: 1050, loss is 3.974721703529358 and perplexity is 53.23529964803037
At time: 269.42327976226807 and batch: 1100, loss is 3.9126543283462523 and perplexity is 50.0315761122182
At time: 270.32208228111267 and batch: 1150, loss is 3.9235751533508303 and perplexity is 50.58095658351927
At time: 271.17492938041687 and batch: 1200, loss is 3.933878140449524 and perplexity is 51.1047853936135
At time: 272.01265835762024 and batch: 1250, loss is 3.9414029836654665 and perplexity is 51.49079138716751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5361902835595345 and perplexity of 93.3345437866327
Finished 12 epochs...
Completing Train Step...
At time: 274.46087551116943 and batch: 50, loss is 4.049687404632568 and perplexity is 57.379517670300274
At time: 275.30636262893677 and batch: 100, loss is 4.076233758926391 and perplexity is 58.92313270247717
At time: 276.14665937423706 and batch: 150, loss is 3.9940483713150026 and perplexity is 54.274167186400916
At time: 276.98828744888306 and batch: 200, loss is 4.052920656204224 and perplexity is 57.565340329810404
At time: 277.83377170562744 and batch: 250, loss is 4.049726357460022 and perplexity is 57.38175280828368
At time: 278.6787736415863 and batch: 300, loss is 4.062339820861816 and perplexity is 58.11011940308643
At time: 279.5229365825653 and batch: 350, loss is 4.043727674484253 and perplexity is 57.038568221463294
At time: 280.3656451702118 and batch: 400, loss is 4.055237002372742 and perplexity is 57.698836136884566
At time: 281.21799516677856 and batch: 450, loss is 3.976019792556763 and perplexity is 53.30444867746239
At time: 282.06659173965454 and batch: 500, loss is 4.002606730461121 and perplexity is 54.74065835365066
At time: 282.9108338356018 and batch: 550, loss is 4.0043819665908815 and perplexity is 54.837922255779894
At time: 283.7607114315033 and batch: 600, loss is 4.00674946308136 and perplexity is 54.9679046499836
At time: 284.6053738594055 and batch: 650, loss is 4.032998452186584 and perplexity is 56.42986006537188
At time: 285.4467692375183 and batch: 700, loss is 4.01744243144989 and perplexity is 55.55882844995374
At time: 286.2933452129364 and batch: 750, loss is 4.00766450881958 and perplexity is 55.018225816445735
At time: 287.1360967159271 and batch: 800, loss is 4.030043129920959 and perplexity is 56.26333782811177
At time: 287.97999238967896 and batch: 850, loss is 4.06730749130249 and perplexity is 58.39950952780884
At time: 288.8272159099579 and batch: 900, loss is 4.009991364479065 and perplexity is 55.14639434356477
At time: 289.67288303375244 and batch: 950, loss is 4.003738484382629 and perplexity is 54.802646379387426
At time: 290.5348083972931 and batch: 1000, loss is 3.986763801574707 and perplexity is 53.88023976828694
At time: 291.4046766757965 and batch: 1050, loss is 3.9684586477279664 and perplexity is 52.902925919804304
At time: 292.320508480072 and batch: 1100, loss is 3.907508330345154 and perplexity is 49.77477503716422
At time: 293.1616847515106 and batch: 1150, loss is 3.9205867719650267 and perplexity is 50.430027024253725
At time: 294.00442576408386 and batch: 1200, loss is 3.9329967641830446 and perplexity is 51.05976269254869
At time: 294.8491792678833 and batch: 1250, loss is 3.942050452232361 and perplexity is 51.52414085127504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536634013600593 and perplexity of 93.37596831755401
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 297.27320623397827 and batch: 50, loss is 4.046357707977295 and perplexity is 57.18877900939761
At time: 298.1329083442688 and batch: 100, loss is 4.085204501152038 and perplexity is 59.45409493903047
At time: 298.96860456466675 and batch: 150, loss is 4.0088717555999756 and perplexity is 55.08468650157787
At time: 299.8083806037903 and batch: 200, loss is 4.0705844688415525 and perplexity is 58.59119731557337
At time: 300.6495831012726 and batch: 250, loss is 4.071173415184021 and perplexity is 58.62571455033511
At time: 301.49159502983093 and batch: 300, loss is 4.088321309089661 and perplexity is 59.63969101747827
At time: 302.33534145355225 and batch: 350, loss is 4.066184797286987 and perplexity is 58.33398153877711
At time: 303.17966055870056 and batch: 400, loss is 4.076568107604981 and perplexity is 58.94283686788187
At time: 304.02226543426514 and batch: 450, loss is 3.9994880676269533 and perplexity is 54.570206625811494
At time: 304.86569595336914 and batch: 500, loss is 4.018764524459839 and perplexity is 55.63233096656366
At time: 305.71096873283386 and batch: 550, loss is 4.016716876029968 and perplexity is 55.51853206124675
At time: 306.54962372779846 and batch: 600, loss is 4.015908832550049 and perplexity is 55.47368879349501
At time: 307.39204478263855 and batch: 650, loss is 4.041953248977661 and perplexity is 56.93744727347866
At time: 308.2351167201996 and batch: 700, loss is 4.02349479675293 and perplexity is 55.89611042281247
At time: 309.0779039859772 and batch: 750, loss is 4.006974573135376 and perplexity is 54.98027987080957
At time: 309.9226281642914 and batch: 800, loss is 4.022649183273315 and perplexity is 55.848863897345666
At time: 310.7622621059418 and batch: 850, loss is 4.0518562746047975 and perplexity is 57.504101437355295
At time: 311.6043870449066 and batch: 900, loss is 3.9902677488327027 and perplexity is 54.06936443461074
At time: 312.4484646320343 and batch: 950, loss is 3.9766501235961913 and perplexity is 53.33805871761637
At time: 313.289678812027 and batch: 1000, loss is 3.9546194219589235 and perplexity is 52.17583318900109
At time: 314.18215560913086 and batch: 1050, loss is 3.9275272989273073 and perplexity is 50.781255431725086
At time: 315.02729988098145 and batch: 1100, loss is 3.8624437475204467 and perplexity is 47.58148654830105
At time: 315.868106842041 and batch: 1150, loss is 3.8676383447647096 and perplexity is 47.82929628585473
At time: 316.7106239795685 and batch: 1200, loss is 3.8813909530639648 and perplexity is 48.49161774778113
At time: 317.5530514717102 and batch: 1250, loss is 3.8942490100860594 and perplexity is 49.11915152058934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.524340998517336 and perplexity of 92.2351227135649
Finished 14 epochs...
Completing Train Step...
At time: 320.0412030220032 and batch: 50, loss is 4.041178803443909 and perplexity is 56.893369391896776
At time: 320.8838801383972 and batch: 100, loss is 4.071960778236389 and perplexity is 58.671892448890645
At time: 321.7258803844452 and batch: 150, loss is 3.9927944946289062 and perplexity is 54.20615672078776
At time: 322.566059589386 and batch: 200, loss is 4.050886511802673 and perplexity is 57.44836312964872
At time: 323.4127869606018 and batch: 250, loss is 4.048000454902649 and perplexity is 57.282802907870554
At time: 324.25610756874084 and batch: 300, loss is 4.065484499931335 and perplexity is 58.293144706392624
At time: 325.1010191440582 and batch: 350, loss is 4.044364676475525 and perplexity is 57.07491347777102
At time: 325.9435453414917 and batch: 400, loss is 4.055786490440369 and perplexity is 57.730549671164106
At time: 326.78277468681335 and batch: 450, loss is 3.9796919584274293 and perplexity is 53.50055129495297
At time: 327.62389636039734 and batch: 500, loss is 4.000724081993103 and perplexity is 54.637697886656376
At time: 328.468789100647 and batch: 550, loss is 3.9992358827590944 and perplexity is 54.55644658057505
At time: 329.3073344230652 and batch: 600, loss is 3.999982442855835 and perplexity is 54.59719145396794
At time: 330.145649433136 and batch: 650, loss is 4.026866106987 and perplexity is 56.084871559317236
At time: 330.98657512664795 and batch: 700, loss is 4.0089884805679326 and perplexity is 55.09111663511595
At time: 331.8242943286896 and batch: 750, loss is 3.9947918844223023 and perplexity is 54.31453574650888
At time: 332.6643307209015 and batch: 800, loss is 4.013025026321412 and perplexity is 55.31394387163488
At time: 333.5035398006439 and batch: 850, loss is 4.044127416610718 and perplexity is 57.06137349782582
At time: 334.3438024520874 and batch: 900, loss is 3.98507474899292 and perplexity is 53.78931002437616
At time: 335.23733592033386 and batch: 950, loss is 3.9740493202209475 and perplexity is 53.19951715225335
At time: 336.07746505737305 and batch: 1000, loss is 3.954745111465454 and perplexity is 52.18239155587776
At time: 336.9167265892029 and batch: 1050, loss is 3.9306415796279905 and perplexity is 50.93964902851514
At time: 337.75508189201355 and batch: 1100, loss is 3.8682077884674073 and perplexity is 47.85654013361176
At time: 338.5988931655884 and batch: 1150, loss is 3.8758784437179568 and perplexity is 48.22504267558314
At time: 339.43979835510254 and batch: 1200, loss is 3.8916204881668093 and perplexity is 48.99021029082614
At time: 340.28154587745667 and batch: 1250, loss is 3.9045924091339113 and perplexity is 49.62984711673578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.523133215242929 and perplexity of 92.12378992151444
Finished 15 epochs...
Completing Train Step...
At time: 342.71601152420044 and batch: 50, loss is 4.038770174980163 and perplexity is 56.75649930374699
At time: 343.5882649421692 and batch: 100, loss is 4.066974358558655 and perplexity is 58.38005797911501
At time: 344.4389555454254 and batch: 150, loss is 3.9865374326705934 and perplexity is 53.868044337842406
At time: 345.2815296649933 and batch: 200, loss is 4.042822833061218 and perplexity is 56.98698070500655
At time: 346.12704968452454 and batch: 250, loss is 4.039017338752746 and perplexity is 56.770529188001646
At time: 346.9741291999817 and batch: 300, loss is 4.0563663434982296 and perplexity is 57.764034614158
At time: 347.8245003223419 and batch: 350, loss is 4.035172343254089 and perplexity is 56.55266586896529
At time: 348.6709473133087 and batch: 400, loss is 4.046721720695496 and perplexity is 57.20960024166217
At time: 349.51462411880493 and batch: 450, loss is 3.9711054801940917 and perplexity is 53.0431365768851
At time: 350.36089849472046 and batch: 500, loss is 3.992550926208496 and perplexity is 54.192955420593954
At time: 351.2038667201996 and batch: 550, loss is 3.991546087265015 and perplexity is 54.138527578712214
At time: 352.049174785614 and batch: 600, loss is 3.9928650426864625 and perplexity is 54.209980994747966
At time: 352.89577436447144 and batch: 650, loss is 4.020282740592957 and perplexity is 55.71685701713094
At time: 353.74005579948425 and batch: 700, loss is 4.002826585769653 and perplexity is 54.7526947010612
At time: 354.5886883735657 and batch: 750, loss is 3.9897423362731934 and perplexity is 54.040963173295175
At time: 355.4300229549408 and batch: 800, loss is 4.009363608360291 and perplexity is 55.111786720797234
At time: 356.2727451324463 and batch: 850, loss is 4.041329770088196 and perplexity is 56.901959041315
At time: 357.17065048217773 and batch: 900, loss is 3.9831513738632203 and perplexity is 53.68595243280529
At time: 358.01260566711426 and batch: 950, loss is 3.9733463191986083 and perplexity is 53.16213098010571
At time: 358.8548004627228 and batch: 1000, loss is 3.9554234313964844 and perplexity is 52.217799919860774
At time: 359.69885063171387 and batch: 1050, loss is 3.9327716255187988 and perplexity is 51.048268459726046
At time: 360.54424834251404 and batch: 1100, loss is 3.8714263343811037 and perplexity is 48.01081674530176
At time: 361.3911728858948 and batch: 1150, loss is 3.8801657724380494 and perplexity is 48.4322431369358
At time: 362.23287439346313 and batch: 1200, loss is 3.8967341232299804 and perplexity is 49.24136997008945
At time: 363.0781047344208 and batch: 1250, loss is 3.909465160369873 and perplexity is 49.87227117200166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522878382327783 and perplexity of 92.10031673857132
Finished 16 epochs...
Completing Train Step...
At time: 365.517213344574 and batch: 50, loss is 4.035617914199829 and perplexity is 56.577869708412926
At time: 366.3900442123413 and batch: 100, loss is 4.062688794136047 and perplexity is 58.13040182052544
At time: 367.2286741733551 and batch: 150, loss is 3.9817079496383667 and perplexity is 53.60851672825994
At time: 368.0692081451416 and batch: 200, loss is 4.037166419029236 and perplexity is 56.66554868103004
At time: 368.9106910228729 and batch: 250, loss is 4.033122606277466 and perplexity is 56.43686649827654
At time: 369.7553086280823 and batch: 300, loss is 4.050512938499451 and perplexity is 57.4269059630315
At time: 370.6043140888214 and batch: 350, loss is 4.029331660270691 and perplexity is 56.223322407390896
At time: 371.45264625549316 and batch: 400, loss is 4.040935726165771 and perplexity is 56.87954158720155
At time: 372.30014872550964 and batch: 450, loss is 3.965619568824768 and perplexity is 52.75294334583162
At time: 373.1425337791443 and batch: 500, loss is 3.987287120819092 and perplexity is 53.90844371383983
At time: 373.98745822906494 and batch: 550, loss is 3.9866032886505125 and perplexity is 53.871591987504296
At time: 374.8331232070923 and batch: 600, loss is 3.988270344734192 and perplexity is 53.96147385090244
At time: 375.6762318611145 and batch: 650, loss is 4.016152925491333 and perplexity is 55.48723118209
At time: 376.5179691314697 and batch: 700, loss is 3.998945565223694 and perplexity is 54.54061018636628
At time: 377.3593509197235 and batch: 750, loss is 3.9865306043624877 and perplexity is 53.86767651149444
At time: 378.19917035102844 and batch: 800, loss is 4.006969141960144 and perplexity is 54.97998126408616
At time: 379.1074423789978 and batch: 850, loss is 4.039566078186035 and perplexity is 56.80168996480737
At time: 379.94510865211487 and batch: 900, loss is 3.9818446588516236 and perplexity is 53.61584600738431
At time: 380.7902572154999 and batch: 950, loss is 3.9728180980682373 and perplexity is 53.134057034464874
At time: 381.6345684528351 and batch: 1000, loss is 3.9557664680480955 and perplexity is 52.23571561179358
At time: 382.47586131095886 and batch: 1050, loss is 3.9339443397521974 and perplexity is 51.10816860675177
At time: 383.31945276260376 and batch: 1100, loss is 3.8731974840164183 and perplexity is 48.09592643462007
At time: 384.1619679927826 and batch: 1150, loss is 3.88257688999176 and perplexity is 48.5491598618716
At time: 385.003466129303 and batch: 1200, loss is 3.8996500158309937 and perplexity is 49.38516205570644
At time: 385.8426818847656 and batch: 1250, loss is 3.912173671722412 and perplexity is 50.00753388225011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522860561844206 and perplexity of 92.0986754810135
Finished 17 epochs...
Completing Train Step...
At time: 388.34719347953796 and batch: 50, loss is 4.032389421463012 and perplexity is 56.39550301017826
At time: 389.19201135635376 and batch: 100, loss is 4.058829231262207 and perplexity is 57.90647628512936
At time: 390.0384395122528 and batch: 150, loss is 3.977576265335083 and perplexity is 53.38748020218347
At time: 390.88304805755615 and batch: 200, loss is 4.032585577964783 and perplexity is 56.40656643981068
At time: 391.72500944137573 and batch: 250, loss is 4.028506531715393 and perplexity is 56.17695007279856
At time: 392.5708909034729 and batch: 300, loss is 4.046007924079895 and perplexity is 57.16877879346879
At time: 393.4156951904297 and batch: 350, loss is 4.024857907295227 and perplexity is 55.97235495325921
At time: 394.2605993747711 and batch: 400, loss is 4.036519808769226 and perplexity is 56.62891999938068
At time: 395.1089675426483 and batch: 450, loss is 3.9614263772964478 and perplexity is 52.5322032768302
At time: 395.9574213027954 and batch: 500, loss is 3.983263750076294 and perplexity is 53.69198579583183
At time: 396.8015124797821 and batch: 550, loss is 3.9828299999237062 and perplexity is 53.66870193885346
At time: 397.65202045440674 and batch: 600, loss is 3.9847698783874512 and perplexity is 53.77291374436018
At time: 398.49508476257324 and batch: 650, loss is 4.013036217689514 and perplexity is 55.31456291380591
At time: 399.3385443687439 and batch: 700, loss is 3.9960249042510987 and perplexity is 54.38154795127472
At time: 400.180171251297 and batch: 750, loss is 3.98404746055603 and perplexity is 53.73408126095186
At time: 401.0797653198242 and batch: 800, loss is 4.005021915435791 and perplexity is 54.873026952191644
At time: 401.92289090156555 and batch: 850, loss is 4.038115944862366 and perplexity is 56.71937963624793
At time: 402.98790311813354 and batch: 900, loss is 3.980679817199707 and perplexity is 53.55342839712592
At time: 403.8311834335327 and batch: 950, loss is 3.972216019630432 and perplexity is 53.10207579298535
At time: 404.67402720451355 and batch: 1000, loss is 3.955770492553711 and perplexity is 52.235925835147405
At time: 405.52517080307007 and batch: 1050, loss is 3.934537100791931 and perplexity is 51.1384725185158
At time: 406.3707208633423 and batch: 1100, loss is 3.8741784572601317 and perplexity is 48.14313040071251
At time: 407.21548318862915 and batch: 1150, loss is 3.8839955043792727 and perplexity is 48.61808127344783
At time: 408.0578804016113 and batch: 1200, loss is 3.9014392566680907 and perplexity is 49.47360310197452
At time: 408.90225052833557 and batch: 1250, loss is 3.9138242912292482 and perplexity is 50.090145454544704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522935407875228 and perplexity of 92.10556895930715
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 411.4037022590637 and batch: 50, loss is 4.0320098066329955 and perplexity is 56.3740985038804
At time: 412.24517703056335 and batch: 100, loss is 4.0613580989837645 and perplexity is 58.05309942099372
At time: 413.0845413208008 and batch: 150, loss is 3.9820197725296023 and perplexity is 53.62523569748419
At time: 413.92580008506775 and batch: 200, loss is 4.03659264087677 and perplexity is 56.63304455317032
At time: 414.7697205543518 and batch: 250, loss is 4.032480840682983 and perplexity is 56.400658878742476
At time: 415.60804629325867 and batch: 300, loss is 4.05013985157013 and perplexity is 57.4054847312646
At time: 416.45345759391785 and batch: 350, loss is 4.0271932935714725 and perplexity is 56.10322477918325
At time: 417.2914283275604 and batch: 400, loss is 4.0382373952865604 and perplexity is 56.72626864729301
At time: 418.13112211227417 and batch: 450, loss is 3.9638512086868287 and perplexity is 52.65973957686286
At time: 418.9739272594452 and batch: 500, loss is 3.985883102416992 and perplexity is 53.832808375964945
At time: 419.81739258766174 and batch: 550, loss is 3.984763140678406 and perplexity is 53.772551439333405
At time: 420.6599988937378 and batch: 600, loss is 3.985522918701172 and perplexity is 53.81342216651912
At time: 421.5048842430115 and batch: 650, loss is 4.0150550889968875 and perplexity is 55.42634870034257
At time: 422.4030783176422 and batch: 700, loss is 3.9977192640304566 and perplexity is 54.47376796374005
At time: 423.24565863609314 and batch: 750, loss is 3.9828684902191163 and perplexity is 53.67076770280104
At time: 424.08797121047974 and batch: 800, loss is 4.003678336143493 and perplexity is 54.79935019583852
At time: 424.93100237846375 and batch: 850, loss is 4.033601722717285 and perplexity is 56.46391280747604
At time: 425.77142000198364 and batch: 900, loss is 3.9720903491973876 and perplexity is 53.095402851429405
At time: 426.61802101135254 and batch: 950, loss is 3.962657985687256 and perplexity is 52.59694223751386
At time: 427.46152448654175 and batch: 1000, loss is 3.944782547950745 and perplexity is 51.665102208166
At time: 428.30016565322876 and batch: 1050, loss is 3.923285846710205 and perplexity is 50.56632529345721
At time: 429.1433742046356 and batch: 1100, loss is 3.8613857984542848 and perplexity is 47.53117437759037
At time: 429.98174262046814 and batch: 1150, loss is 3.870498332977295 and perplexity is 47.96628330670685
At time: 430.8228578567505 and batch: 1200, loss is 3.8890517234802244 and perplexity is 48.86452746255856
At time: 431.66209077835083 and batch: 1250, loss is 3.904216365814209 and perplexity is 49.61118765287294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522574097570712 and perplexity of 92.07229627938186
Finished 19 epochs...
Completing Train Step...
At time: 434.09814167022705 and batch: 50, loss is 4.030992269515991 and perplexity is 56.316764940676194
At time: 434.9741494655609 and batch: 100, loss is 4.059407978057862 and perplexity is 57.93999917242313
At time: 435.81771516799927 and batch: 150, loss is 3.9800437927246093 and perplexity is 53.5193779355513
At time: 436.6648144721985 and batch: 200, loss is 4.034546217918396 and perplexity is 56.51726789500432
At time: 437.5106954574585 and batch: 250, loss is 4.030353260040283 and perplexity is 56.28078948978906
At time: 438.35386395454407 and batch: 300, loss is 4.04792067527771 and perplexity is 57.27823308963072
At time: 439.25141644477844 and batch: 350, loss is 4.025303473472595 and perplexity is 55.997299898393265
At time: 440.09400510787964 and batch: 400, loss is 4.036450452804566 and perplexity is 56.62499258220293
At time: 440.9400999546051 and batch: 450, loss is 3.9619651317596434 and perplexity is 52.56051286107977
At time: 441.78216314315796 and batch: 500, loss is 3.984151782989502 and perplexity is 53.739687223478114
At time: 442.62443470954895 and batch: 550, loss is 3.982895965576172 and perplexity is 53.67224234656522
At time: 443.4678301811218 and batch: 600, loss is 3.9839900493621827 and perplexity is 53.73099641174966
At time: 444.3147873878479 and batch: 650, loss is 4.013634305000306 and perplexity is 55.34765574720204
At time: 445.1614212989807 and batch: 700, loss is 3.9964762830734255 and perplexity is 54.40610017110445
At time: 446.0050733089447 and batch: 750, loss is 3.9820907974243163 and perplexity is 53.62904455946403
At time: 446.8597848415375 and batch: 800, loss is 4.003049712181092 and perplexity is 54.7649128363901
At time: 447.72688722610474 and batch: 850, loss is 4.033429570198059 and perplexity is 56.45419323928897
At time: 448.5794425010681 and batch: 900, loss is 3.972194504737854 and perplexity is 53.10093331981915
At time: 449.42161321640015 and batch: 950, loss is 3.9631123113632203 and perplexity is 52.62084380798702
At time: 450.2716066837311 and batch: 1000, loss is 3.945415873527527 and perplexity is 51.6978334024787
At time: 451.11646723747253 and batch: 1050, loss is 3.924095640182495 and perplexity is 50.60729015789884
At time: 451.9615786075592 and batch: 1100, loss is 3.8624841356277466 and perplexity is 47.58340831329321
At time: 452.80500054359436 and batch: 1150, loss is 3.8717521619796753 and perplexity is 48.026462543205305
At time: 453.64834332466125 and batch: 1200, loss is 3.8905096817016602 and perplexity is 48.935821861607934
At time: 454.493595123291 and batch: 1250, loss is 3.9056044721603396 and perplexity is 49.68010107580739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5225295463617705 and perplexity of 92.06819443864426
Finished 20 epochs...
Completing Train Step...
At time: 456.958176612854 and batch: 50, loss is 4.0303147459030155 and perplexity is 56.278621925478305
At time: 457.8023591041565 and batch: 100, loss is 4.058304042816162 and perplexity is 57.876072457401584
At time: 458.6423683166504 and batch: 150, loss is 3.978805193901062 and perplexity is 53.45312993282186
At time: 459.4822940826416 and batch: 200, loss is 4.033111243247986 and perplexity is 56.43622520814226
At time: 460.3558437824249 and batch: 250, loss is 4.028886680603027 and perplexity is 56.198309737548605
At time: 461.20021295547485 and batch: 300, loss is 4.046379518508911 and perplexity is 57.190026340672716
At time: 462.0440583229065 and batch: 350, loss is 4.023960771560669 and perplexity is 55.92216267149452
At time: 462.88417649269104 and batch: 400, loss is 4.035136551856994 and perplexity is 56.550641806266626
At time: 463.72627353668213 and batch: 450, loss is 3.960612163543701 and perplexity is 52.489448242690536
At time: 464.5695502758026 and batch: 500, loss is 3.982955002784729 and perplexity is 53.675411099466594
At time: 465.4124310016632 and batch: 550, loss is 3.9816022443771364 and perplexity is 53.60285032548458
At time: 466.2544560432434 and batch: 600, loss is 3.9828851079940795 and perplexity is 53.67165959895147
At time: 467.0975601673126 and batch: 650, loss is 4.01263500213623 and perplexity is 55.292374302347454
At time: 467.94293689727783 and batch: 700, loss is 3.995594415664673 and perplexity is 54.35814235385227
At time: 468.79055666923523 and batch: 750, loss is 3.98155592918396 and perplexity is 53.60036775660773
At time: 469.63295578956604 and batch: 800, loss is 4.002659702301026 and perplexity is 54.74355814384418
At time: 470.4774537086487 and batch: 850, loss is 4.033349151611328 and perplexity is 56.449653455397566
At time: 471.31945180892944 and batch: 900, loss is 3.9723599338531494 and perplexity is 53.10971848688077
At time: 472.1664066314697 and batch: 950, loss is 3.9635130977630615 and perplexity is 52.64193775333428
At time: 473.01069808006287 and batch: 1000, loss is 3.9459444761276243 and perplexity is 51.7251682356301
At time: 473.8547143936157 and batch: 1050, loss is 3.9247779512405394 and perplexity is 50.641831854341895
At time: 474.6994740962982 and batch: 1100, loss is 3.863384122848511 and perplexity is 47.626252049211125
At time: 475.5423970222473 and batch: 1150, loss is 3.8727809047698973 and perplexity is 48.07589484248768
At time: 476.38147473335266 and batch: 1200, loss is 3.8916339302062988 and perplexity is 48.99086882359347
At time: 477.22783041000366 and batch: 1250, loss is 3.9066476917266844 and perplexity is 49.73195537231324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522526427777144 and perplexity of 92.06790731663622
Finished 21 epochs...
Completing Train Step...
At time: 479.68789625167847 and batch: 50, loss is 4.029760789871216 and perplexity is 56.24745467684103
At time: 480.5576288700104 and batch: 100, loss is 4.057438941001892 and perplexity is 57.82602541313153
At time: 481.4029772281647 and batch: 150, loss is 3.977812190055847 and perplexity is 53.40007711444556
At time: 482.2747139930725 and batch: 200, loss is 4.031934914588928 and perplexity is 56.36987669050306
At time: 483.1205575466156 and batch: 250, loss is 4.0276874780654905 and perplexity is 56.130956974779664
At time: 483.96387910842896 and batch: 300, loss is 4.045151662826538 and perplexity is 57.11984833490371
At time: 484.800767660141 and batch: 350, loss is 4.0228622531890865 and perplexity is 55.86076487789764
At time: 485.64098954200745 and batch: 400, loss is 4.034060039520264 and perplexity is 56.489797098627314
At time: 486.48166155815125 and batch: 450, loss is 3.9595273876190187 and perplexity is 52.43253982495826
At time: 487.32396483421326 and batch: 500, loss is 3.9819889497756957 and perplexity is 53.623582845513965
At time: 488.1678590774536 and batch: 550, loss is 3.980594124794006 and perplexity is 53.54883947163389
At time: 489.00753712654114 and batch: 600, loss is 3.98200412273407 and perplexity is 53.62439648007696
At time: 489.85119223594666 and batch: 650, loss is 4.011846795082092 and perplexity is 55.24880963412714
At time: 490.689945936203 and batch: 700, loss is 3.994899253845215 and perplexity is 54.32036777995317
At time: 491.5324544906616 and batch: 750, loss is 3.981132078170776 and perplexity is 53.5776540003914
At time: 492.37437081336975 and batch: 800, loss is 4.002376108169556 and perplexity is 54.728035393203335
At time: 493.21879267692566 and batch: 850, loss is 4.033288621902466 and perplexity is 56.44623667771787
At time: 494.0542449951172 and batch: 900, loss is 3.972485370635986 and perplexity is 53.11638081694702
At time: 494.8958034515381 and batch: 950, loss is 3.96382164478302 and perplexity is 52.65818277240015
At time: 495.73586773872375 and batch: 1000, loss is 3.9463634157180785 and perplexity is 51.74684249621283
At time: 496.5716857910156 and batch: 1050, loss is 3.9253417158126833 and perplexity is 50.67038997428159
At time: 497.41408705711365 and batch: 1100, loss is 3.864119358062744 and perplexity is 47.66128142267611
At time: 498.25384187698364 and batch: 1150, loss is 3.873616394996643 and perplexity is 48.116078566987674
At time: 499.0951590538025 and batch: 1200, loss is 3.8925259637832643 and perplexity is 49.034589820948504
At time: 499.9371647834778 and batch: 1250, loss is 3.9074645376205446 and perplexity is 49.77259531187693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522535783531022 and perplexity of 92.06876868534646
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 502.4410080909729 and batch: 50, loss is 4.029636998176574 and perplexity is 56.240492140068305
At time: 503.2910966873169 and batch: 100, loss is 4.057475991249085 and perplexity is 57.828167921357256
At time: 504.16768431663513 and batch: 150, loss is 3.978133645057678 and perplexity is 53.417245595631385
At time: 505.0110716819763 and batch: 200, loss is 4.032133793830871 and perplexity is 56.38108860371916
At time: 505.85745763778687 and batch: 250, loss is 4.027861499786377 and perplexity is 56.140725830479326
At time: 506.70221161842346 and batch: 300, loss is 4.045293574333191 and perplexity is 57.12795487383265
At time: 507.54974460601807 and batch: 350, loss is 4.022909440994263 and perplexity is 55.86340088698094
At time: 508.3972923755646 and batch: 400, loss is 4.033775696754455 and perplexity is 56.47373691688679
At time: 509.2428014278412 and batch: 450, loss is 3.959348692893982 and perplexity is 52.42317124375422
At time: 510.08833479881287 and batch: 500, loss is 3.9820589637756347 and perplexity is 53.62733737847344
At time: 510.93803238868713 and batch: 550, loss is 3.980277805328369 and perplexity is 53.53190361005929
At time: 511.7830672264099 and batch: 600, loss is 3.98162727355957 and perplexity is 53.60419197779451
At time: 512.6304478645325 and batch: 650, loss is 4.011653237342834 and perplexity is 55.238116834308144
At time: 513.4760625362396 and batch: 700, loss is 3.9948008823394776 and perplexity is 54.31502446640167
At time: 514.3198752403259 and batch: 750, loss is 3.980727882385254 and perplexity is 53.55600251446074
At time: 515.1624021530151 and batch: 800, loss is 4.001753811836243 and perplexity is 54.69398893204139
At time: 516.0087172985077 and batch: 850, loss is 4.032378630638123 and perplexity is 56.394894459464105
At time: 516.856616973877 and batch: 900, loss is 3.971169948577881 and perplexity is 53.046556292401895
At time: 517.7049388885498 and batch: 950, loss is 3.96238516330719 and perplexity is 52.58259457181956
At time: 518.550137758255 and batch: 1000, loss is 3.944731092453003 and perplexity is 51.66244382301085
At time: 519.3975365161896 and batch: 1050, loss is 3.92372088432312 and perplexity is 50.58832833263456
At time: 520.2422256469727 and batch: 1100, loss is 3.8623115253448486 and perplexity is 47.575195636539625
At time: 521.083694934845 and batch: 1150, loss is 3.871797966957092 and perplexity is 48.02866244462034
At time: 521.9314186573029 and batch: 1200, loss is 3.890782814025879 and perplexity is 48.949189641873886
At time: 522.7888031005859 and batch: 1250, loss is 3.9060575437545775 and perplexity is 49.702614818187165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522496132955064 and perplexity of 92.06511817801321
Finished 23 epochs...
Completing Train Step...
At time: 525.2730576992035 and batch: 50, loss is 4.0295541906356815 and perplexity is 56.23583519603329
At time: 526.1784517765045 and batch: 100, loss is 4.057303757667541 and perplexity is 57.81820882655197
At time: 527.0228192806244 and batch: 150, loss is 3.977902874946594 and perplexity is 53.40491991418562
At time: 527.862035036087 and batch: 200, loss is 4.031901030540467 and perplexity is 56.36796668322909
At time: 528.7044560909271 and batch: 250, loss is 4.02765495300293 and perplexity is 56.12913134158205
At time: 529.5463147163391 and batch: 300, loss is 4.045137004852295 and perplexity is 57.11901107977428
At time: 530.3872308731079 and batch: 350, loss is 4.022718243598938 and perplexity is 55.85272097125592
At time: 531.2309634685516 and batch: 400, loss is 4.033618965148926 and perplexity is 56.46488639102624
At time: 532.0711665153503 and batch: 450, loss is 3.9591787338256834 and perplexity is 52.41426220751957
At time: 532.9184353351593 and batch: 500, loss is 3.9819063568115234 and perplexity is 53.619154097751434
At time: 533.7608797550201 and batch: 550, loss is 3.9801327276229856 and perplexity is 53.524137887648955
At time: 534.6039001941681 and batch: 600, loss is 3.9814914512634276 and perplexity is 53.596911827771756
At time: 535.4461498260498 and batch: 650, loss is 4.011498126983643 and perplexity is 55.22954949462366
At time: 536.2893435955048 and batch: 700, loss is 3.9946524810791018 and perplexity is 54.30696464637203
At time: 537.129011631012 and batch: 750, loss is 3.9806318044662476 and perplexity is 53.55085721236776
At time: 537.9700133800507 and batch: 800, loss is 4.001742882728577 and perplexity is 54.69339117881413
At time: 538.8100166320801 and batch: 850, loss is 4.032390484809875 and perplexity is 56.39556297819136
At time: 539.6485590934753 and batch: 900, loss is 3.971186065673828 and perplexity is 53.047411255729074
At time: 540.4935646057129 and batch: 950, loss is 3.9624161863327028 and perplexity is 52.584225868296244
At time: 541.3362121582031 and batch: 1000, loss is 3.9448075914382934 and perplexity is 51.666396098711544
At time: 542.1813480854034 and batch: 1050, loss is 3.923808288574219 and perplexity is 50.59275016082729
At time: 543.0210475921631 and batch: 1100, loss is 3.862460145950317 and perplexity is 47.58226681636891
At time: 543.8614776134491 and batch: 1150, loss is 3.871927556991577 and perplexity is 48.03488688394675
At time: 544.7034180164337 and batch: 1200, loss is 3.8909183168411254 and perplexity is 48.955822844273044
At time: 545.5460405349731 and batch: 1250, loss is 3.906192173957825 and perplexity is 49.70930674177948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522472520814325 and perplexity of 92.06294434915006
Finished 24 epochs...
Completing Train Step...
At time: 548.0301594734192 and batch: 50, loss is 4.029477682113647 and perplexity is 56.231532839982634
At time: 548.8804442882538 and batch: 100, loss is 4.057169795036316 and perplexity is 57.81046386594406
At time: 549.72381067276 and batch: 150, loss is 3.9777389192581176 and perplexity is 53.39616459153515
At time: 550.5680613517761 and batch: 200, loss is 4.031727614402771 and perplexity is 56.358192415689516
At time: 551.4117681980133 and batch: 250, loss is 4.0274854135513305 and perplexity is 56.119616046067435
At time: 552.2584080696106 and batch: 300, loss is 4.0449742364883425 and perplexity is 57.109714668391604
At time: 553.106942653656 and batch: 350, loss is 4.022564172744751 and perplexity is 55.84411635770433
At time: 553.9524400234222 and batch: 400, loss is 4.033481168746948 and perplexity is 56.457106268892204
At time: 554.7962694168091 and batch: 450, loss is 3.9590365648269654 and perplexity is 52.40681105401705
At time: 555.6436381340027 and batch: 500, loss is 3.9817755651474 and perplexity is 53.61214161795504
At time: 556.48752617836 and batch: 550, loss is 3.9800035429000853 and perplexity is 53.51722383333217
At time: 557.3308138847351 and batch: 600, loss is 3.9813796615600587 and perplexity is 53.5909205797832
At time: 558.1799895763397 and batch: 650, loss is 4.011390662193298 and perplexity is 55.22361458156919
At time: 559.027704000473 and batch: 700, loss is 3.9945546102523806 and perplexity is 54.30164983893191
At time: 559.8719394207001 and batch: 750, loss is 3.9805724954605104 and perplexity is 53.54768125845238
At time: 560.7125632762909 and batch: 800, loss is 4.001723699569702 and perplexity is 54.692341996865096
At time: 561.555294752121 and batch: 850, loss is 4.032393746376037 and perplexity is 56.39574691635122
At time: 562.4009654521942 and batch: 900, loss is 3.9712035131454466 and perplexity is 53.04833680700564
At time: 563.2463717460632 and batch: 950, loss is 3.962454013824463 and perplexity is 52.586215035289364
At time: 564.0907578468323 and batch: 1000, loss is 3.944872612953186 and perplexity is 51.669755635274825
At time: 564.9357259273529 and batch: 1050, loss is 3.9238928174972534 and perplexity is 50.597026892262946
At time: 565.780369758606 and batch: 1100, loss is 3.862577214241028 and perplexity is 47.58783751708309
At time: 566.6245789527893 and batch: 1150, loss is 3.8720426321029664 and perplexity is 48.04041482196343
At time: 567.4706797599792 and batch: 1200, loss is 3.8910435819625855 and perplexity is 48.96195568547534
At time: 568.3156399726868 and batch: 1250, loss is 3.9063119888305664 and perplexity is 49.715263012858614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522460046475821 and perplexity of 92.06179593198151
Finished 25 epochs...
Completing Train Step...
At time: 570.7531886100769 and batch: 50, loss is 4.0293990421295165 and perplexity is 56.22711096700274
At time: 571.636435508728 and batch: 100, loss is 4.057047615051269 and perplexity is 57.80340101581163
At time: 572.4787130355835 and batch: 150, loss is 3.977595510482788 and perplexity is 53.388507662012344
At time: 573.3269140720367 and batch: 200, loss is 4.031570706367493 and perplexity is 56.34935005618249
At time: 574.1681871414185 and batch: 250, loss is 4.027326869964599 and perplexity is 56.110719346129514
At time: 575.009836435318 and batch: 300, loss is 4.044812707901001 and perplexity is 57.10049056185626
At time: 575.8517427444458 and batch: 350, loss is 4.022421813011169 and perplexity is 55.836166970027314
At time: 576.6949195861816 and batch: 400, loss is 4.033347992897034 and perplexity is 56.449588046415094
At time: 577.5352728366852 and batch: 450, loss is 3.958903226852417 and perplexity is 52.399823701828616
At time: 578.3774950504303 and batch: 500, loss is 3.9816528367996216 and perplexity is 53.60556229213656
At time: 579.220155954361 and batch: 550, loss is 3.9798803997039793 and perplexity is 53.510633957100154
At time: 580.0620160102844 and batch: 600, loss is 3.9812753915786745 and perplexity is 53.585332946808236
At time: 580.906414270401 and batch: 650, loss is 4.011296944618225 and perplexity is 55.218439400830334
At time: 581.7449541091919 and batch: 700, loss is 3.9944713258743287 and perplexity is 54.29712754811859
At time: 582.5855994224548 and batch: 750, loss is 3.9805232763290403 and perplexity is 53.545045752947786
At time: 583.4250507354736 and batch: 800, loss is 4.001700625419617 and perplexity is 54.69108003211681
At time: 584.26344871521 and batch: 850, loss is 4.032392888069153 and perplexity is 56.39569851151417
At time: 585.1049430370331 and batch: 900, loss is 3.9712200021743773 and perplexity is 53.04921152977762
At time: 585.9441103935242 and batch: 950, loss is 3.9624930477142333 and perplexity is 52.588267719872356
At time: 586.7839303016663 and batch: 1000, loss is 3.9449317979812624 and perplexity is 51.67281380171073
At time: 587.623064994812 and batch: 1050, loss is 3.9239729404449464 and perplexity is 50.601081037614925
At time: 588.4639980792999 and batch: 1100, loss is 3.8626810121536255 and perplexity is 47.5927772916471
At time: 589.3083863258362 and batch: 1150, loss is 3.8721513175964355 and perplexity is 48.04563640190467
At time: 590.1554701328278 and batch: 1200, loss is 3.8911623048782347 and perplexity is 48.96776893668645
At time: 591.0579888820648 and batch: 1250, loss is 3.906422734260559 and perplexity is 49.720769055917096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522449799697765 and perplexity of 92.0608526000242
Finished 26 epochs...
Completing Train Step...
At time: 593.4924771785736 and batch: 50, loss is 4.029320592880249 and perplexity is 56.2227001653732
At time: 594.4093019962311 and batch: 100, loss is 4.056932044029236 and perplexity is 57.79672100369458
At time: 595.2548544406891 and batch: 150, loss is 3.977461533546448 and perplexity is 53.38135531245565
At time: 596.1039426326752 and batch: 200, loss is 4.0314208602905275 and perplexity is 56.340906959735264
At time: 596.9609282016754 and batch: 250, loss is 4.027174353599548 and perplexity is 56.102162195743766
At time: 597.8077414035797 and batch: 300, loss is 4.044655823707581 and perplexity is 57.091533100111015
At time: 598.6593492031097 and batch: 350, loss is 4.022285099029541 and perplexity is 55.828533907107094
At time: 599.5137693881989 and batch: 400, loss is 4.033217906951904 and perplexity is 56.44224522601112
At time: 600.3549001216888 and batch: 450, loss is 3.958774542808533 and perplexity is 52.39308111445685
At time: 601.2010865211487 and batch: 500, loss is 3.9815350866317747 and perplexity is 53.599250599787844
At time: 602.0439245700836 and batch: 550, loss is 3.97976158618927 and perplexity is 53.5042765482859
At time: 602.886785030365 and batch: 600, loss is 3.9811749124526976 and perplexity is 53.57994900987973
At time: 603.732852935791 and batch: 650, loss is 4.011208901405334 and perplexity is 55.21357800602421
At time: 604.5768845081329 and batch: 700, loss is 3.9943934440612794 and perplexity is 54.29289895404919
At time: 605.4186668395996 and batch: 750, loss is 3.9804780149459837 and perplexity is 53.54262228496635
At time: 606.2636423110962 and batch: 800, loss is 4.001676244735718 and perplexity is 54.68974664243697
At time: 607.107255935669 and batch: 850, loss is 4.032390499114991 and perplexity is 56.39556378493639
At time: 607.9571447372437 and batch: 900, loss is 3.97123553276062 and perplexity is 53.05003542153014
At time: 608.7987515926361 and batch: 950, loss is 3.962531776428223 and perplexity is 52.590304435291515
At time: 609.6443135738373 and batch: 1000, loss is 3.9449876499176026 and perplexity is 51.6756999090143
At time: 610.4896583557129 and batch: 1050, loss is 3.924049582481384 and perplexity is 50.60495935613082
At time: 611.3318331241608 and batch: 1100, loss is 3.862778134346008 and perplexity is 47.59739983099119
At time: 612.2333014011383 and batch: 1150, loss is 3.872256259918213 and perplexity is 48.05067868710989
At time: 613.0759603977203 and batch: 1200, loss is 3.891276521682739 and perplexity is 48.97336219819426
At time: 613.9224228858948 and batch: 1250, loss is 3.9065275239944457 and perplexity is 49.72597955507375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522444008040602 and perplexity of 92.06031941667185
Finished 27 epochs...
Completing Train Step...
At time: 616.3919043540955 and batch: 50, loss is 4.029243535995484 and perplexity is 56.21836798615961
At time: 617.2337403297424 and batch: 100, loss is 4.056820693016053 and perplexity is 57.790285638551005
At time: 618.0786249637604 and batch: 150, loss is 3.977333273887634 and perplexity is 53.374509077093556
At time: 618.9207396507263 and batch: 200, loss is 4.031275715827942 and perplexity is 56.33272998250884
At time: 619.7611298561096 and batch: 250, loss is 4.027025914192199 and perplexity is 56.093835042090596
At time: 620.6011185646057 and batch: 300, loss is 4.04450376033783 and perplexity is 57.08285222924152
At time: 621.4399921894073 and batch: 350, loss is 4.022152290344239 and perplexity is 55.821119885250674
At time: 622.2829117774963 and batch: 400, loss is 4.033090496063233 and perplexity is 56.43505432749737
At time: 623.1273486614227 and batch: 450, loss is 3.958649311065674 and perplexity is 52.38652024841806
At time: 623.9694967269897 and batch: 500, loss is 3.9814208507537843 and perplexity is 53.59312799205335
At time: 624.8102731704712 and batch: 550, loss is 3.979646372795105 and perplexity is 53.49811249408005
At time: 625.65234541893 and batch: 600, loss is 3.981077136993408 and perplexity is 53.57471046186148
At time: 626.4949750900269 and batch: 650, loss is 4.011123695373535 and perplexity is 55.208873676562355
At time: 627.3355977535248 and batch: 700, loss is 3.9943188333511355 and perplexity is 54.28884827341642
At time: 628.1741597652435 and batch: 750, loss is 3.980434913635254 and perplexity is 53.54031457749892
At time: 629.014402627945 and batch: 800, loss is 4.001651706695557 and perplexity is 54.68840467970212
At time: 629.8527202606201 and batch: 850, loss is 4.032387757301331 and perplexity is 56.39540915902124
At time: 630.6944992542267 and batch: 900, loss is 3.9712503576278686 and perplexity is 53.0508218870924
At time: 631.5366110801697 and batch: 950, loss is 3.9625698471069337 and perplexity is 52.59230662198704
At time: 632.3769145011902 and batch: 1000, loss is 3.945041079521179 and perplexity is 51.67846099493618
At time: 633.2172524929047 and batch: 1050, loss is 3.924123148918152 and perplexity is 50.608682319614324
At time: 634.1123335361481 and batch: 1100, loss is 3.8628709936141967 and perplexity is 47.601819895926
At time: 634.9542820453644 and batch: 1150, loss is 3.8723583316802976 and perplexity is 48.05558355487284
At time: 635.7955577373505 and batch: 1200, loss is 3.891386637687683 and perplexity is 48.97875524611318
At time: 636.6378514766693 and batch: 1250, loss is 3.9066278028488157 and perplexity is 49.730966269362774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522441780480155 and perplexity of 92.06011434697395
Finished 28 epochs...
Completing Train Step...
At time: 639.0568957328796 and batch: 50, loss is 4.029168171882629 and perplexity is 56.21413129837933
At time: 639.9235656261444 and batch: 100, loss is 4.056712946891785 and perplexity is 57.78405929469134
At time: 640.763072013855 and batch: 150, loss is 3.9772094106674194 and perplexity is 53.36789834794344
At time: 641.604229927063 and batch: 200, loss is 4.031134095191955 and perplexity is 56.32475267035121
At time: 642.4422721862793 and batch: 250, loss is 4.026881537437439 and perplexity is 56.085736980825125
At time: 643.2811088562012 and batch: 300, loss is 4.044356260299683 and perplexity is 57.07443312728436
At time: 644.1231455802917 and batch: 350, loss is 4.02202269077301 and perplexity is 55.81388596081493
At time: 644.9713046550751 and batch: 400, loss is 4.032965621948242 and perplexity is 56.42800749002673
At time: 645.8156719207764 and batch: 450, loss is 3.958526849746704 and perplexity is 52.38010531885058
At time: 646.6579413414001 and batch: 500, loss is 3.9813094663619997 and perplexity is 53.58715888652688
At time: 647.497073173523 and batch: 550, loss is 3.979534344673157 and perplexity is 53.49211953670572
At time: 648.3379373550415 and batch: 600, loss is 3.9809815788269045 and perplexity is 53.569591205356055
At time: 649.1787557601929 and batch: 650, loss is 4.011040735244751 and perplexity is 55.20429373127116
At time: 650.0219819545746 and batch: 700, loss is 3.9942462730407713 and perplexity is 54.284909200648286
At time: 650.8613431453705 and batch: 750, loss is 3.9803927993774413 and perplexity is 53.538059814366605
At time: 651.7005157470703 and batch: 800, loss is 4.001627202033997 and perplexity is 54.68706457527364
At time: 652.5397443771362 and batch: 850, loss is 4.032384643554687 and perplexity is 56.39523355827865
At time: 653.3775169849396 and batch: 900, loss is 3.9712644910812376 and perplexity is 53.051571683708325
At time: 654.2180504798889 and batch: 950, loss is 3.962606554031372 and perplexity is 52.594237159244074
At time: 655.0553483963013 and batch: 1000, loss is 3.9450924682617186 and perplexity is 51.6811167541972
At time: 655.9478883743286 and batch: 1050, loss is 3.9241943359375 and perplexity is 50.612285129096904
At time: 656.7913103103638 and batch: 1100, loss is 3.8629605197906494 and perplexity is 47.60608169562194
At time: 657.6333267688751 and batch: 1150, loss is 3.8724576091766356 and perplexity is 48.06035462971949
At time: 658.4779698848724 and batch: 1200, loss is 3.8914934158325196 and perplexity is 48.983985385962114
At time: 659.3178243637085 and batch: 1250, loss is 3.906724510192871 and perplexity is 49.73577585158521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522440443943887 and perplexity of 92.05999130537448
Finished 29 epochs...
Completing Train Step...
At time: 661.7841529846191 and batch: 50, loss is 4.029094443321228 and perplexity is 56.209986864131764
At time: 662.6259922981262 and batch: 100, loss is 4.056608080863953 and perplexity is 57.77800002763229
At time: 663.4752595424652 and batch: 150, loss is 3.977089123725891 and perplexity is 53.361479272748575
At time: 664.3209671974182 and batch: 200, loss is 4.030995559692383 and perplexity is 56.31695023307148
At time: 665.1682486534119 and batch: 250, loss is 4.026740374565125 and perplexity is 56.07782031587979
At time: 666.0127544403076 and batch: 300, loss is 4.044212856292725 and perplexity is 57.066249011710546
At time: 666.860187292099 and batch: 350, loss is 4.021895613670349 and perplexity is 55.80679374453746
At time: 667.7047162055969 and batch: 400, loss is 4.032843346595764 and perplexity is 56.42110815733903
At time: 668.5497448444366 and batch: 450, loss is 3.9584073543548586 and perplexity is 52.37384651159736
At time: 669.3965978622437 and batch: 500, loss is 3.9812007665634157 and perplexity is 53.58133428972121
At time: 670.2408027648926 and batch: 550, loss is 3.979425253868103 and perplexity is 53.48628435660946
At time: 671.0881712436676 and batch: 600, loss is 3.980888104438782 and perplexity is 53.56458405461997
At time: 671.9358122348785 and batch: 650, loss is 4.0109597063064575 and perplexity is 55.1998207671831
At time: 672.7795386314392 and batch: 700, loss is 3.99417555809021 and perplexity is 54.2810705817034
At time: 673.6338834762573 and batch: 750, loss is 3.980351753234863 and perplexity is 53.53586232862957
At time: 674.4988534450531 and batch: 800, loss is 4.0016028022766115 and perplexity is 54.68573024044468
At time: 675.3797762393951 and batch: 850, loss is 4.032381315231323 and perplexity is 56.395045857017514
At time: 676.2224848270416 and batch: 900, loss is 3.9712778329849243 and perplexity is 53.05227949738994
At time: 677.1259865760803 and batch: 950, loss is 3.962642107009888 and perplexity is 52.59610707426818
At time: 677.9707689285278 and batch: 1000, loss is 3.9451420974731444 and perplexity is 51.68368171091518
At time: 678.8182973861694 and batch: 1050, loss is 3.924263048171997 and perplexity is 50.61576293178355
At time: 679.66987657547 and batch: 1100, loss is 3.863047232627869 and perplexity is 47.61020993301767
At time: 680.5165400505066 and batch: 1150, loss is 3.872554349899292 and perplexity is 48.065004248057576
At time: 681.3585803508759 and batch: 1200, loss is 3.891596670150757 and perplexity is 48.98904345510694
At time: 682.2042179107666 and batch: 1250, loss is 3.906817774772644 and perplexity is 49.74041465413431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522440889455976 and perplexity of 92.0600323192227
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 684.6246061325073 and batch: 50, loss is 4.029070115089416 and perplexity is 56.20861939117534
At time: 685.4990129470825 and batch: 100, loss is 4.056599717140198 and perplexity is 57.77751679042181
At time: 686.3413140773773 and batch: 150, loss is 3.9771097469329835 and perplexity is 53.36257976893421
At time: 687.1802864074707 and batch: 200, loss is 4.031004462242127 and perplexity is 56.31745159975406
At time: 688.0285265445709 and batch: 250, loss is 4.026751337051391 and perplexity is 56.078435071584465
At time: 688.8701453208923 and batch: 300, loss is 4.044225349426269 and perplexity is 57.066961952433765
At time: 689.7119135856628 and batch: 350, loss is 4.021900897026062 and perplexity is 55.8070885924589
At time: 690.5546345710754 and batch: 400, loss is 4.0328041934967045 and perplexity is 56.41889913934751
At time: 691.3956341743469 and batch: 450, loss is 3.9583847618103025 and perplexity is 52.372663266502784
At time: 692.2394979000092 and batch: 500, loss is 3.9812215900421144 and perplexity is 53.58245005111142
At time: 693.0783762931824 and batch: 550, loss is 3.9793825817108153 and perplexity is 53.4840020301669
At time: 693.9236102104187 and batch: 600, loss is 3.980834698677063 and perplexity is 53.56172347359382
At time: 694.7653088569641 and batch: 650, loss is 4.0109200143814085 and perplexity is 55.197629823516166
At time: 695.6103208065033 and batch: 700, loss is 3.9941547393798826 and perplexity is 54.27994053158184
At time: 696.4535810947418 and batch: 750, loss is 3.9802974224090577 and perplexity is 53.53295376003223
At time: 697.29674077034 and batch: 800, loss is 4.0014917755126955 and perplexity is 54.67965899782515
At time: 698.1409275531769 and batch: 850, loss is 4.03224534034729 and perplexity is 56.387378068522246
At time: 699.0368099212646 and batch: 900, loss is 3.9711006546020506 and perplexity is 53.042880612964986
At time: 699.8799073696136 and batch: 950, loss is 3.962457184791565 and perplexity is 52.58638178471163
At time: 700.7185280323029 and batch: 1000, loss is 3.944916481971741 and perplexity is 51.67202238646321
At time: 701.5596103668213 and batch: 1050, loss is 3.9240566062927247 and perplexity is 50.605314797066505
At time: 702.4086334705353 and batch: 1100, loss is 3.8627789402008057 and perplexity is 47.59743818759965
At time: 703.2462787628174 and batch: 1150, loss is 3.872301001548767 and perplexity is 48.05282860091855
At time: 704.0866270065308 and batch: 1200, loss is 3.8913674688339235 and perplexity is 48.97781638851499
At time: 704.9300954341888 and batch: 1250, loss is 3.9066347599029543 and perplexity is 49.731312251590985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.52244534457687 and perplexity of 92.06044245870984
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 707.3499693870544 and batch: 50, loss is 4.029068946838379 and perplexity is 56.20855372543579
At time: 708.2270421981812 and batch: 100, loss is 4.05659836769104 and perplexity is 57.77743882265304
At time: 709.0696053504944 and batch: 150, loss is 3.977110733985901 and perplexity is 53.36263244065026
At time: 709.9136924743652 and batch: 200, loss is 4.031003885269165 and perplexity is 56.31741910611658
At time: 710.7584688663483 and batch: 250, loss is 4.026752161979675 and perplexity is 56.07848133229076
At time: 711.601884841919 and batch: 300, loss is 4.044228439331055 and perplexity is 57.067138284185006
At time: 712.4491529464722 and batch: 350, loss is 4.021900277137757 and perplexity is 55.807053998308064
At time: 713.2948129177094 and batch: 400, loss is 4.032799849510193 and perplexity is 56.418654056942955
At time: 714.1413977146149 and batch: 450, loss is 3.9583813095092775 and perplexity is 52.3724824606158
At time: 714.9847612380981 and batch: 500, loss is 3.9812240552902223 and perplexity is 53.582582145307846
At time: 715.8299098014832 and batch: 550, loss is 3.979378113746643 and perplexity is 53.483763066095875
At time: 716.6725783348083 and batch: 600, loss is 3.9808278846740723 and perplexity is 53.56135850509334
At time: 717.5163691043854 and batch: 650, loss is 4.010913133621216 and perplexity is 55.1972500231688
At time: 718.3620648384094 and batch: 700, loss is 3.994149718284607 and perplexity is 54.27966798751312
At time: 719.2065510749817 and batch: 750, loss is 3.9802892065048217 and perplexity is 53.53251394021743
At time: 720.0522840023041 and batch: 800, loss is 4.001478891372681 and perplexity is 54.6789545019811
At time: 720.950305223465 and batch: 850, loss is 4.032229042053222 and perplexity is 56.38645905794193
At time: 721.7941114902496 and batch: 900, loss is 3.971078405380249 and perplexity is 53.04170046327798
At time: 722.6378931999207 and batch: 950, loss is 3.962433738708496 and perplexity is 52.58514885448976
At time: 723.4835348129272 and batch: 1000, loss is 3.944888029098511 and perplexity is 51.670552189876474
At time: 724.3313982486725 and batch: 1050, loss is 3.924029998779297 and perplexity is 50.60396833338664
At time: 725.1794283390045 and batch: 1100, loss is 3.8627446603775026 and perplexity is 47.595806583794605
At time: 726.0278196334839 and batch: 1150, loss is 3.872268223762512 and perplexity is 48.05125356138702
At time: 726.8717257976532 and batch: 1200, loss is 3.891337833404541 and perplexity is 48.97636493140348
At time: 727.7192361354828 and batch: 1250, loss is 3.906611108779907 and perplexity is 49.73013606411476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.52244579008896 and perplexity of 92.06048347275905
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 730.2018716335297 and batch: 50, loss is 4.029069142341614 and perplexity is 56.208564714390945
At time: 731.0430717468262 and batch: 100, loss is 4.056598315238952 and perplexity is 57.77743579210581
At time: 731.8852438926697 and batch: 150, loss is 3.9771109008789063 and perplexity is 53.36264134650109
At time: 732.7282869815826 and batch: 200, loss is 4.031003952026367 and perplexity is 56.31742286571005
At time: 733.5697979927063 and batch: 250, loss is 4.0267524147033695 and perplexity is 56.078495504653525
At time: 734.4133603572845 and batch: 300, loss is 4.044228987693787 and perplexity is 57.067169577685455
At time: 735.2534070014954 and batch: 350, loss is 4.021900448799133 and perplexity is 55.8070635782246
At time: 736.0934116840363 and batch: 400, loss is 4.032799658775329 and perplexity is 56.4186432959397
At time: 736.9417765140533 and batch: 450, loss is 3.9583812713623048 and perplexity is 52.37248046276418
At time: 737.7834448814392 and batch: 500, loss is 3.9812244272232054 and perplexity is 53.58260207444117
At time: 738.6249606609344 and batch: 550, loss is 3.9793779706954955 and perplexity is 53.48375541518275
At time: 739.4658133983612 and batch: 600, loss is 3.9808275079727173 and perplexity is 53.56133832846081
At time: 740.3071858882904 and batch: 650, loss is 4.010912594795227 and perplexity is 55.19722028146397
At time: 741.1464521884918 and batch: 700, loss is 3.9941494417190553 and perplexity is 54.27965297562887
At time: 741.985182762146 and batch: 750, loss is 3.9802886486053466 and perplexity is 53.532484074464335
At time: 742.8846163749695 and batch: 800, loss is 4.001477942466736 and perplexity is 54.67890261682073
At time: 743.7272510528564 and batch: 850, loss is 4.032227854728699 and perplexity is 56.38639210895606
At time: 744.5659837722778 and batch: 900, loss is 3.971076488494873 and perplexity is 53.0415987885155
At time: 745.4042534828186 and batch: 950, loss is 3.962431802749634 and perplexity is 52.585047051903366
At time: 746.2486624717712 and batch: 1000, loss is 3.9448855829238894 and perplexity is 51.67042579483762
At time: 747.0891063213348 and batch: 1050, loss is 3.9240278673171995 and perplexity is 50.603860473061104
At time: 747.9308717250824 and batch: 1100, loss is 3.8627416563034056 and perplexity is 47.595663602679686
At time: 748.7729470729828 and batch: 1150, loss is 3.872265491485596 and perplexity is 48.051122272235474
At time: 749.6212103366852 and batch: 1200, loss is 3.8913353872299195 and perplexity is 48.97624512680906
At time: 750.4610073566437 and batch: 1250, loss is 3.9066090965270996 and perplexity is 49.73003599460953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.52244579008896 and perplexity of 92.06048347275905
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 752.9701821804047 and batch: 50, loss is 4.029069175720215 and perplexity is 56.20856659055425
At time: 753.863810300827 and batch: 100, loss is 4.056598324775695 and perplexity is 57.77743634311437
At time: 754.7068889141083 and batch: 150, loss is 3.977110896110535 and perplexity is 53.36264109204819
At time: 755.5487139225006 and batch: 200, loss is 4.031003909111023 and perplexity is 56.31742044882852
At time: 756.3958647251129 and batch: 250, loss is 4.026752429008484 and perplexity is 56.078496306862824
At time: 757.2398347854614 and batch: 300, loss is 4.044229030609131 and perplexity is 57.06717202674273
At time: 758.0825834274292 and batch: 350, loss is 4.021900486946106 and perplexity is 55.80706570709518
At time: 758.9272062778473 and batch: 400, loss is 4.032799677848816 and perplexity is 56.418644372039964
At time: 759.7732758522034 and batch: 450, loss is 3.9583812856674196 and perplexity is 52.37248121195854
At time: 760.6159522533417 and batch: 500, loss is 3.981224455833435 and perplexity is 53.582603607451745
At time: 761.4591159820557 and batch: 550, loss is 3.979377951622009 and perplexity is 53.48375439506108
At time: 762.3042593002319 and batch: 600, loss is 3.9808274507522583 and perplexity is 53.56133526365653
At time: 763.1446063518524 and batch: 650, loss is 4.0109125423431395 and perplexity is 55.19721738625464
At time: 764.0223717689514 and batch: 700, loss is 3.9941494846343994 and perplexity is 54.27965530505891
At time: 764.8626365661621 and batch: 750, loss is 3.9802887344360354 and perplexity is 53.53248866919451
At time: 765.7068274021149 and batch: 800, loss is 4.001477880477905 and perplexity is 54.678899227339585
At time: 766.5502216815948 and batch: 850, loss is 4.032227759361267 and perplexity is 56.38638673153089
At time: 767.3978900909424 and batch: 900, loss is 3.9710764503479004 and perplexity is 53.04159676513912
At time: 768.2405707836151 and batch: 950, loss is 3.962431774139404 and perplexity is 52.58504554743311
At time: 769.0862336158752 and batch: 1000, loss is 3.9448854780197142 and perplexity is 51.67042037439451
At time: 769.9331855773926 and batch: 1050, loss is 3.924027876853943 and perplexity is 50.60386095565713
At time: 770.7770807743073 and batch: 1100, loss is 3.8627415227890016 and perplexity is 47.59565724797345
At time: 771.6210010051727 and batch: 1150, loss is 3.8722654294967653 and perplexity is 48.051119293602696
At time: 772.4652409553528 and batch: 1200, loss is 3.8913353300094604 and perplexity is 48.97624232436591
At time: 773.314519405365 and batch: 1250, loss is 3.9066089534759523 and perplexity is 49.73002888067133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.52244579008896 and perplexity of 92.06048347275905
Annealing...
Model not improving. Stopping early with 92.05999130537448loss at 33 epochs.
Finished Training.
Improved accuracyfrom -92.61381987282947 to -92.05999130537448
<pretraining.langmodel.trainer.TrainLangModel object at 0x7efe5f0df908>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -92.61381987282947, 'params': {'wordvec_source': '', 'num_layers': 1, 'anneal': 7.964455081285388, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.34985756432001314, 'tune_wordvecs': True, 'lr': 2.824426131503266, 'seq_len': 35}}, {'best_accuracy': -158.9652993566064, 'params': {'wordvec_source': '', 'num_layers': 1, 'anneal': 4.847042992056301, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.1305334884213566, 'tune_wordvecs': True, 'lr': 23.373514309577473, 'seq_len': 35}}, {'best_accuracy': -159.93131699102628, 'params': {'wordvec_source': '', 'num_layers': 1, 'anneal': 4.377602789040021, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.34815859453726294, 'tune_wordvecs': True, 'lr': 22.262110235502455, 'seq_len': 35}}, {'best_accuracy': -168.0337933964524, 'params': {'wordvec_source': '', 'num_layers': 1, 'anneal': 4.0019315899158165, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.42331717900717813, 'tune_wordvecs': True, 'lr': 0.04237964674543293, 'seq_len': 35}}, {'best_accuracy': -116.01986872727537, 'params': {'wordvec_source': '', 'num_layers': 1, 'anneal': 5.677451280969865, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.9784253954823844, 'tune_wordvecs': True, 'lr': 9.067242169773948, 'seq_len': 35}}, {'best_accuracy': -92.05999130537448, 'params': {'wordvec_source': '', 'num_layers': 1, 'anneal': 8.0, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 50, 'dropout': 0.4245306207484014, 'tune_wordvecs': True, 'lr': 4.453365840901657, 'seq_len': 35}}]
