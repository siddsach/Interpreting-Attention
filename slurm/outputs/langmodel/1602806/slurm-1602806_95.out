Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'batch_size': 50, 'data': 'gigasmall', 'num_layers': 1, 'seq_len': 50, 'wordvec_source': '', 'dropout': 0.8862032697787757, 'anneal': 6.999888548859264, 'lr': 21.507733036036004, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 8576 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 9.987932920455933 and batch: 50, loss is 9.031182804107665 and perplexity is 8359.741663513327
At time: 13.818919897079468 and batch: 100, loss is 8.19032675743103 and perplexity is 3605.900308704027
At time: 17.658390283584595 and batch: 150, loss is 7.929311866760254 and perplexity is 2777.5148464617464
At time: 21.49543809890747 and batch: 200, loss is 7.786273984909058 and perplexity is 2407.3310987025634
At time: 25.337613821029663 and batch: 250, loss is 7.680406131744385 and perplexity is 2165.499071194952
At time: 29.184645652770996 and batch: 300, loss is 7.644632053375244 and perplexity is 2089.3996468325854
At time: 33.03135085105896 and batch: 350, loss is 7.608549604415893 and perplexity is 2015.3529179231086
At time: 37.34180665016174 and batch: 400, loss is 7.551123743057251 and perplexity is 1902.8798783647092
At time: 41.187984228134155 and batch: 450, loss is 7.532192125320434 and perplexity is 1867.1941440744029
At time: 45.03429841995239 and batch: 500, loss is 7.477551345825195 and perplexity is 1767.9064799191133
At time: 48.87611532211304 and batch: 550, loss is 7.436710433959961 and perplexity is 1697.1581146931296
At time: 52.71980905532837 and batch: 600, loss is 7.40710147857666 and perplexity is 1647.6436883650945
At time: 56.551443338394165 and batch: 650, loss is 7.392866477966309 and perplexity is 1624.3556254973382
At time: 60.39320254325867 and batch: 700, loss is 7.368956031799317 and perplexity is 1585.9772087165345
At time: 64.23826932907104 and batch: 750, loss is 7.360835103988648 and perplexity is 1573.1497581919184
At time: 68.08884930610657 and batch: 800, loss is 7.317309551239013 and perplexity is 1506.1463042821313
At time: 71.938223361969 and batch: 850, loss is 7.293120832443237 and perplexity is 1470.1516422465859
At time: 75.78579998016357 and batch: 900, loss is 7.286015243530273 and perplexity is 1459.742374821098
At time: 79.63422083854675 and batch: 950, loss is 7.314650068283081 and perplexity is 1502.146055511228
At time: 83.48773837089539 and batch: 1000, loss is 7.29241985321045 and perplexity is 1469.1214575875322
At time: 87.33584117889404 and batch: 1050, loss is 7.2469868564605715 and perplexity is 1403.8684118790907
At time: 91.1843855381012 and batch: 1100, loss is 7.255928297042846 and perplexity is 1416.4773046962428
At time: 95.03573346138 and batch: 1150, loss is 7.197690000534058 and perplexity is 1336.3402509604646
At time: 98.88403081893921 and batch: 1200, loss is 7.215848970413208 and perplexity is 1360.8284809324236
At time: 102.74001669883728 and batch: 1250, loss is 7.203331327438354 and perplexity is 1343.9002874405332
At time: 106.59870648384094 and batch: 1300, loss is 7.172399263381958 and perplexity is 1302.967016610487
At time: 110.45049715042114 and batch: 1350, loss is 7.155679206848145 and perplexity is 1281.3624525101445
At time: 114.30043983459473 and batch: 1400, loss is 7.1556025409698485 and perplexity is 1281.264219497915
At time: 118.14762949943542 and batch: 1450, loss is 7.11573561668396 and perplexity is 1231.1889613064038
At time: 121.99460530281067 and batch: 1500, loss is 7.134352111816407 and perplexity is 1254.3240637076285
At time: 125.84956884384155 and batch: 1550, loss is 7.143310651779175 and perplexity is 1265.611459763461
At time: 129.69933557510376 and batch: 1600, loss is 7.145906085968018 and perplexity is 1268.9005374619312
At time: 133.54978013038635 and batch: 1650, loss is 7.115655345916748 and perplexity is 1231.09013679031
At time: 137.4049515724182 and batch: 1700, loss is 7.129828567504883 and perplexity is 1248.66288716936
At time: 141.260014295578 and batch: 1750, loss is 7.141594858169555 and perplexity is 1263.4417935904967
At time: 145.11711049079895 and batch: 1800, loss is 7.095709304809571 and perplexity is 1206.7780334526888
At time: 148.9706027507782 and batch: 1850, loss is 7.1110733604431156 and perplexity is 1225.4622030684286
At time: 152.8126471042633 and batch: 1900, loss is 7.0890857601165775 and perplexity is 1198.811298352516
At time: 156.66508793830872 and batch: 1950, loss is 7.067969131469726 and perplexity is 1173.7618560467176
At time: 160.51625752449036 and batch: 2000, loss is 7.085412120819091 and perplexity is 1194.4153775141954
At time: 164.3662326335907 and batch: 2050, loss is 7.071253681182862 and perplexity is 1177.6234735799862
At time: 168.21605110168457 and batch: 2100, loss is 7.07774169921875 and perplexity is 1185.2887552702337
At time: 172.06932759284973 and batch: 2150, loss is 7.0830329895019535 and perplexity is 1191.5770841589756
At time: 175.92070245742798 and batch: 2200, loss is 7.06687858581543 and perplexity is 1172.4825128732884
At time: 179.7743842601776 and batch: 2250, loss is 7.049358253479004 and perplexity is 1152.1191370410957
At time: 183.6256890296936 and batch: 2300, loss is 7.082344980239868 and perplexity is 1190.7575500444436
At time: 187.47733759880066 and batch: 2350, loss is 7.083603858947754 and perplexity is 1192.2575133082648
At time: 191.33270454406738 and batch: 2400, loss is 7.064586391448975 and perplexity is 1169.7980329128236
At time: 195.1839280128479 and batch: 2450, loss is 7.055899000167846 and perplexity is 1159.6795549071794
At time: 199.03773260116577 and batch: 2500, loss is 7.045713310241699 and perplexity is 1147.9273721972738
At time: 202.89104866981506 and batch: 2550, loss is 7.0129164123535155 and perplexity is 1110.889597320977
At time: 206.74793791770935 and batch: 2600, loss is 7.076213340759278 and perplexity is 1183.4785928151098
At time: 210.59881281852722 and batch: 2650, loss is 7.090505895614624 and perplexity is 1200.5149822774445
At time: 214.4562304019928 and batch: 2700, loss is 7.080166635513305 and perplexity is 1188.1664927472768
At time: 218.31059050559998 and batch: 2750, loss is 7.083545904159546 and perplexity is 1192.1884182788053
At time: 222.16375160217285 and batch: 2800, loss is 7.07542573928833 and perplexity is 1182.5468503036245
At time: 226.02229833602905 and batch: 2850, loss is 7.043303365707398 and perplexity is 1145.1642617096873
At time: 229.8757951259613 and batch: 2900, loss is 7.047472629547119 and perplexity is 1149.948720561627
At time: 233.72747468948364 and batch: 2950, loss is 7.068733644485474 and perplexity is 1174.659555370738
At time: 237.57661890983582 and batch: 3000, loss is 7.05945158958435 and perplexity is 1163.8067469891978
At time: 241.42362213134766 and batch: 3050, loss is 7.046792907714844 and perplexity is 1149.1673409007558
At time: 245.27951502799988 and batch: 3100, loss is 7.066674728393554 and perplexity is 1172.2435179722897
At time: 249.14053320884705 and batch: 3150, loss is 7.032416343688965 and perplexity is 1132.7644541427485
At time: 252.99305033683777 and batch: 3200, loss is 7.014624652862548 and perplexity is 1112.7888856909929
At time: 256.8512852191925 and batch: 3250, loss is 7.032703723907471 and perplexity is 1133.0900350196066
At time: 260.70699071884155 and batch: 3300, loss is 7.044140253067017 and perplexity is 1146.1230363422262
At time: 264.5640664100647 and batch: 3350, loss is 7.021915159225464 and perplexity is 1120.9313253315124
At time: 268.4193842411041 and batch: 3400, loss is 7.037586641311646 and perplexity is 1138.636350151437
At time: 272.27391147613525 and batch: 3450, loss is 7.03766881942749 and perplexity is 1138.729924986174
At time: 276.1312336921692 and batch: 3500, loss is 7.038271713256836 and perplexity is 1139.4166652261642
At time: 279.98652958869934 and batch: 3550, loss is 7.066004362106323 and perplexity is 1171.4579487763874
At time: 283.8390474319458 and batch: 3600, loss is 7.066415023803711 and perplexity is 1171.9391204786823
At time: 287.6951804161072 and batch: 3650, loss is 7.09013801574707 and perplexity is 1200.073418211013
At time: 291.5511498451233 and batch: 3700, loss is 7.050379085540771 and perplexity is 1153.2958577100683
At time: 295.40407705307007 and batch: 3750, loss is 7.064873714447021 and perplexity is 1170.1341910814226
At time: 299.25243854522705 and batch: 3800, loss is 7.032256717681885 and perplexity is 1132.5836499068846
At time: 303.1043574810028 and batch: 3850, loss is 7.018162126541138 and perplexity is 1116.7323178654622
At time: 306.95713996887207 and batch: 3900, loss is 7.0630943393707275 and perplexity is 1168.053934792861
At time: 310.81216049194336 and batch: 3950, loss is 7.056706647872925 and perplexity is 1160.616545766653
At time: 314.6649646759033 and batch: 4000, loss is 7.034616422653198 and perplexity is 1135.2593688881095
At time: 318.51318192481995 and batch: 4050, loss is 7.036034345626831 and perplexity is 1136.8702209905036
At time: 322.3555808067322 and batch: 4100, loss is 7.03168963432312 and perplexity is 1131.941562642322
At time: 326.2019817829132 and batch: 4150, loss is 7.026230726242066 and perplexity is 1125.779232797771
At time: 330.0548915863037 and batch: 4200, loss is 7.0389962482452395 and perplexity is 1140.2425116076627
At time: 333.90877318382263 and batch: 4250, loss is 7.038205881118774 and perplexity is 1139.341657459937
At time: 337.7593071460724 and batch: 4300, loss is 7.034304637908935 and perplexity is 1134.9054675095006
At time: 341.6121838092804 and batch: 4350, loss is 7.031360769271851 and perplexity is 1131.5693678265777
At time: 345.4617221355438 and batch: 4400, loss is 7.0104639434814455 and perplexity is 1108.1685132141295
At time: 349.3146450519562 and batch: 4450, loss is 6.9936819267272945 and perplexity is 1089.7263914854145
At time: 353.1654510498047 and batch: 4500, loss is 7.019449367523193 and perplexity is 1118.1707470748963
At time: 357.0177390575409 and batch: 4550, loss is 7.050543880462646 and perplexity is 1153.4859306719375
At time: 360.866090297699 and batch: 4600, loss is 7.028565273284912 and perplexity is 1128.4104875755756
At time: 364.71523094177246 and batch: 4650, loss is 7.020809068679809 and perplexity is 1119.6921592315257
At time: 368.5680048465729 and batch: 4700, loss is 7.043064155578613 and perplexity is 1144.8903595805525
At time: 372.4180028438568 and batch: 4750, loss is 7.036371631622314 and perplexity is 1137.2537360682222
At time: 376.26837134361267 and batch: 4800, loss is 7.032504005432129 and perplexity is 1132.8637586019302
At time: 380.1204216480255 and batch: 4850, loss is 7.047248086929321 and perplexity is 1149.6905370532638
At time: 383.9729974269867 and batch: 4900, loss is 7.037064161300659 and perplexity is 1138.0415908071645
At time: 387.8230504989624 and batch: 4950, loss is 7.05543306350708 and perplexity is 1159.1393435497184
At time: 391.6727132797241 and batch: 5000, loss is 7.038298568725586 and perplexity is 1139.447265205697
At time: 395.5246272087097 and batch: 5050, loss is 7.0391965675354005 and perplexity is 1140.470947057449
At time: 399.3713116645813 and batch: 5100, loss is 7.024697465896606 and perplexity is 1124.054442756367
At time: 403.2152352333069 and batch: 5150, loss is 7.049276733398438 and perplexity is 1152.0252200243358
At time: 407.06371235847473 and batch: 5200, loss is 7.001413259506226 and perplexity is 1098.1840813345816
At time: 410.91413140296936 and batch: 5250, loss is 7.0154445934295655 and perplexity is 1113.7016806085483
At time: 414.7624502182007 and batch: 5300, loss is 7.014148750305176 and perplexity is 1112.2594326085073
At time: 418.611935377121 and batch: 5350, loss is 7.026328067779541 and perplexity is 1125.8888232129134
At time: 422.4714877605438 and batch: 5400, loss is 7.053208694458008 and perplexity is 1156.5638553504034
At time: 426.32389187812805 and batch: 5450, loss is 6.994350709915161 and perplexity is 1090.455425931232
At time: 430.1752812862396 and batch: 5500, loss is 7.0005870532989505 and perplexity is 1097.2771295460427
At time: 434.0400593280792 and batch: 5550, loss is 7.02830813407898 and perplexity is 1128.1203663012102
At time: 437.89136362075806 and batch: 5600, loss is 7.040166015625 and perplexity is 1141.5771105357194
At time: 441.74532985687256 and batch: 5650, loss is 7.046006422042847 and perplexity is 1148.263892573563
At time: 445.593284368515 and batch: 5700, loss is 7.017611780166626 and perplexity is 1116.1178973705785
At time: 449.44056129455566 and batch: 5750, loss is 7.051975975036621 and perplexity is 1155.1390150184452
At time: 453.2930166721344 and batch: 5800, loss is 7.035346755981445 and perplexity is 1136.088789481297
At time: 457.14261078834534 and batch: 5850, loss is 7.021811141967773 and perplexity is 1120.81473519279
At time: 460.9911844730377 and batch: 5900, loss is 7.027536125183105 and perplexity is 1127.2497834349058
At time: 464.8463513851166 and batch: 5950, loss is 7.010141668319702 and perplexity is 1107.8114355690125
At time: 468.7007477283478 and batch: 6000, loss is 7.054179859161377 and perplexity is 1157.6876149334093
At time: 472.55079340934753 and batch: 6050, loss is 7.0354663848876955 and perplexity is 1136.2247066702362
At time: 476.4050860404968 and batch: 6100, loss is 7.018897495269775 and perplexity is 1117.5538299103928
At time: 480.25175642967224 and batch: 6150, loss is 7.0197529602050786 and perplexity is 1118.5102670660913
At time: 484.1033716201782 and batch: 6200, loss is 7.005411014556885 and perplexity is 1102.5831396121869
At time: 487.9514570236206 and batch: 6250, loss is 7.0210949802398686 and perplexity is 1120.0123379327727
At time: 491.80245089530945 and batch: 6300, loss is 7.037020177841186 and perplexity is 1137.991536901757
At time: 495.6537206172943 and batch: 6350, loss is 6.9983755970001225 and perplexity is 1094.853230287837
At time: 499.50043511390686 and batch: 6400, loss is 7.027615785598755 and perplexity is 1127.3395841979307
At time: 503.3508496284485 and batch: 6450, loss is 7.01969874382019 and perplexity is 1118.4496271268047
At time: 507.20535612106323 and batch: 6500, loss is 7.014097099304199 and perplexity is 1112.201984779099
At time: 511.05848598480225 and batch: 6550, loss is 7.008866224288941 and perplexity is 1106.399384773608
At time: 514.9092464447021 and batch: 6600, loss is 7.040711660385131 and perplexity is 1142.2001760751336
At time: 518.7627449035645 and batch: 6650, loss is 7.038873138427735 and perplexity is 1140.1021452005662
At time: 522.6154451370239 and batch: 6700, loss is 7.0379891872406 and perplexity is 1139.0947958452837
At time: 526.4700405597687 and batch: 6750, loss is 6.988570852279663 and perplexity is 1084.170928058381
At time: 530.318674325943 and batch: 6800, loss is 7.038065176010132 and perplexity is 1139.181357546017
At time: 534.1712048053741 and batch: 6850, loss is 7.0251587295532225 and perplexity is 1124.5730478165067
At time: 538.0260484218597 and batch: 6900, loss is 7.041686725616455 and perplexity is 1143.3144389052263
At time: 541.8799846172333 and batch: 6950, loss is 7.033874664306641 and perplexity is 1134.417593011466
At time: 545.7313079833984 and batch: 7000, loss is 7.045092678070068 and perplexity is 1147.215152575459
At time: 549.5825624465942 and batch: 7050, loss is 7.013329133987427 and perplexity is 1111.3481801176897
At time: 553.4322230815887 and batch: 7100, loss is 6.9738557910919186 and perplexity is 1068.3340922832201
At time: 557.2794630527496 and batch: 7150, loss is 7.052474374771118 and perplexity is 1155.7148794902673
At time: 561.1337792873383 and batch: 7200, loss is 7.04284852027893 and perplexity is 1144.6435074207363
At time: 564.9886078834534 and batch: 7250, loss is 7.003094463348389 and perplexity is 1100.0319054808197
At time: 568.8406729698181 and batch: 7300, loss is 6.992682828903198 and perplexity is 1088.6381919182506
At time: 572.6923961639404 and batch: 7350, loss is 6.990108995437622 and perplexity is 1085.839821322776
At time: 576.5407853126526 and batch: 7400, loss is 7.019059295654297 and perplexity is 1117.7346651790092
At time: 580.3884363174438 and batch: 7450, loss is 7.020280647277832 and perplexity is 1119.1006462287905
At time: 584.2387280464172 and batch: 7500, loss is 7.0245788955688475 and perplexity is 1123.9211711538555
At time: 588.088958978653 and batch: 7550, loss is 7.038310079574585 and perplexity is 1139.460381286598
At time: 591.942768573761 and batch: 7600, loss is 7.068408222198486 and perplexity is 1174.2773571630776
At time: 595.7992618083954 and batch: 7650, loss is 7.060652627944946 and perplexity is 1165.2053632644265
At time: 599.6506695747375 and batch: 7700, loss is 7.006627969741821 and perplexity is 1103.9257506638028
At time: 603.4976391792297 and batch: 7750, loss is 7.025905447006226 and perplexity is 1125.4130997401712
At time: 607.3442194461823 and batch: 7800, loss is 7.041304979324341 and perplexity is 1142.8780661545927
At time: 611.1975333690643 and batch: 7850, loss is 6.990578670501709 and perplexity is 1086.349932994432
At time: 615.0483076572418 and batch: 7900, loss is 7.012736263275147 and perplexity is 1110.6894896090014
At time: 618.8961913585663 and batch: 7950, loss is 7.048044881820679 and perplexity is 1150.6069696557522
At time: 622.7436361312866 and batch: 8000, loss is 7.027173175811767 and perplexity is 1126.8407230732582
At time: 626.594441652298 and batch: 8050, loss is 7.020526113510132 and perplexity is 1119.3753813657052
At time: 630.4460823535919 and batch: 8100, loss is 7.023136081695557 and perplexity is 1122.3007313732355
At time: 634.2965819835663 and batch: 8150, loss is 7.040126123428345 and perplexity is 1141.5315714254623
At time: 638.1467578411102 and batch: 8200, loss is 7.027496919631958 and perplexity is 1127.2055898521885
At time: 641.9941041469574 and batch: 8250, loss is 7.0278853416442875 and perplexity is 1127.643506358407
At time: 645.8447661399841 and batch: 8300, loss is 7.056851644515991 and perplexity is 1160.7848434706816
At time: 649.6894493103027 and batch: 8350, loss is 7.040547485351563 and perplexity is 1142.0126707151549
At time: 653.5348143577576 and batch: 8400, loss is 7.044358406066895 and perplexity is 1146.3730937952334
At time: 657.3846688270569 and batch: 8450, loss is 7.042235851287842 and perplexity is 1143.9424346226006
At time: 661.2341294288635 and batch: 8500, loss is 7.031588439941406 and perplexity is 1131.8270223112704
At time: 665.0809831619263 and batch: 8550, loss is 7.0525823497772215 and perplexity is 1155.8396745486864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 569 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
