Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'wordvec_source': 'glove', 'wordvec_dim': 200, 'anneal': 6.553025622356705, 'dropout': 0.9706642817607135, 'batch_size': 50, 'num_layers': 1, 'seq_len': 50, 'data': 'gigasmall', 'lr': 0.07392275779714796}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 8576 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 9.136380434036255 and batch: 50, loss is 11.411933975219727 and perplexity is 90394.07256142372
At time: 12.87273621559143 and batch: 100, loss is 11.36830997467041 and perplexity is 86535.49665272949
At time: 16.61042594909668 and batch: 150, loss is 11.328326110839845 and perplexity is 83143.73290628103
At time: 20.353034734725952 and batch: 200, loss is 11.29097173690796 and perplexity is 80095.24250979556
At time: 24.111226797103882 and batch: 250, loss is 11.247206687927246 and perplexity is 76665.46980891074
At time: 27.875019550323486 and batch: 300, loss is 11.21752857208252 and perplexity is 74423.6146807196
At time: 31.641732692718506 and batch: 350, loss is 11.18208854675293 and perplexity is 71832.23047709747
At time: 35.87345767021179 and batch: 400, loss is 11.146009902954102 and perplexity is 69286.81469638848
At time: 39.64306163787842 and batch: 450, loss is 11.104486751556397 and perplexity is 66468.72076530535
At time: 43.40889048576355 and batch: 500, loss is 11.068313484191895 and perplexity is 64107.29764162745
At time: 47.23118329048157 and batch: 550, loss is 11.029791355133057 and perplexity is 61684.70925630201
At time: 51.40219211578369 and batch: 600, loss is 10.994977989196776 and perplexity is 59574.20689699655
At time: 55.62322163581848 and batch: 650, loss is 10.952631206512452 and perplexity is 57104.10054781299
At time: 59.847827196121216 and batch: 700, loss is 10.908959197998048 and perplexity is 54663.92136793881
At time: 64.06947660446167 and batch: 750, loss is 10.862709674835205 and perplexity is 52193.31365861169
At time: 68.28654980659485 and batch: 800, loss is 10.82171579360962 and perplexity is 50096.96936915446
At time: 72.48298978805542 and batch: 850, loss is 10.760519180297852 and perplexity is 47123.12707989715
At time: 76.67429685592651 and batch: 900, loss is 10.731141891479492 and perplexity is 45758.91391877407
At time: 80.8670289516449 and batch: 950, loss is 10.681816291809081 and perplexity is 43556.59002851771
At time: 85.05888247489929 and batch: 1000, loss is 10.644791278839111 and perplexity is 41973.396455229595
At time: 89.21838355064392 and batch: 1050, loss is 10.591815357208253 and perplexity is 39807.688746819884
At time: 93.4094500541687 and batch: 1100, loss is 10.540962581634522 and perplexity is 37833.96721340704
At time: 97.59987735748291 and batch: 1150, loss is 10.489987964630126 and perplexity is 35953.72466617669
At time: 101.81587934494019 and batch: 1200, loss is 10.452656269073486 and perplexity is 34636.25583995958
At time: 106.03241300582886 and batch: 1250, loss is 10.406000804901122 and perplexity is 33057.40269440501
At time: 110.24793267250061 and batch: 1300, loss is 10.357731819152832 and perplexity is 31499.653323589595
At time: 114.46285963058472 and batch: 1350, loss is 10.310412502288818 and perplexity is 30043.82723196147
At time: 118.67807149887085 and batch: 1400, loss is 10.26649751663208 and perplexity is 28753.003681084447
At time: 122.89549136161804 and batch: 1450, loss is 10.2135595703125 and perplexity is 27270.466170259493
At time: 127.11461806297302 and batch: 1500, loss is 10.183766479492187 and perplexity is 26469.978407663228
At time: 131.33628368377686 and batch: 1550, loss is 10.159528694152833 and perplexity is 25836.117476571213
At time: 135.5569519996643 and batch: 1600, loss is 10.128196487426758 and perplexity is 25039.16523572125
At time: 139.77274990081787 and batch: 1650, loss is 10.075150375366212 and perplexity is 23745.54885736731
At time: 143.99671745300293 and batch: 1700, loss is 10.056321773529053 and perplexity is 23302.636170741007
At time: 148.21453833580017 and batch: 1750, loss is 10.037164573669434 and perplexity is 22860.471751157253
At time: 152.4129855632782 and batch: 1800, loss is 9.997554817199706 and perplexity is 21972.672853083903
At time: 156.61761045455933 and batch: 1850, loss is 9.978474178314208 and perplexity is 21557.394702383874
At time: 160.83606696128845 and batch: 1900, loss is 9.936434803009034 and perplexity is 20669.920427540816
At time: 165.05815625190735 and batch: 1950, loss is 9.890650653839112 and perplexity is 19744.90285717602
At time: 169.2810447216034 and batch: 2000, loss is 9.868187427520752 and perplexity is 19306.313148025612
At time: 173.49984526634216 and batch: 2050, loss is 9.852553157806396 and perplexity is 19006.820317330115
At time: 177.72326517105103 and batch: 2100, loss is 9.81400493621826 and perplexity is 18288.08321394826
At time: 181.94083857536316 and batch: 2150, loss is 9.81072681427002 and perplexity is 18228.230802323687
At time: 186.1604835987091 and batch: 2200, loss is 9.780344009399414 and perplexity is 17682.73483381085
At time: 190.38103008270264 and batch: 2250, loss is 9.745162906646728 and perplexity is 17071.452562749215
At time: 194.6024510860443 and batch: 2300, loss is 9.737516269683837 and perplexity is 16941.41118580991
At time: 198.83251976966858 and batch: 2350, loss is 9.699905738830566 and perplexity is 16316.069153768643
At time: 203.0496895313263 and batch: 2400, loss is 9.69086223602295 and perplexity is 16169.17993421119
At time: 207.2732448577881 and batch: 2450, loss is 9.679547424316405 and perplexity is 15987.259844442313
At time: 211.4957149028778 and batch: 2500, loss is 9.633554039001465 and perplexity is 15268.60500856841
At time: 215.71474194526672 and batch: 2550, loss is 9.609516105651856 and perplexity is 14905.955434260119
At time: 219.93491053581238 and batch: 2600, loss is 9.593777770996093 and perplexity is 14673.196939226145
At time: 224.14648699760437 and batch: 2650, loss is 9.609474601745605 and perplexity is 14905.336791721318
At time: 228.3568091392517 and batch: 2700, loss is 9.573260231018066 and perplexity is 14375.206503425648
At time: 232.57566165924072 and batch: 2750, loss is 9.53642276763916 and perplexity is 13855.295257935539
At time: 236.79840993881226 and batch: 2800, loss is 9.548755912780761 and perplexity is 14027.232710717242
At time: 241.02214741706848 and batch: 2850, loss is 9.503777275085449 and perplexity is 13410.285620080083
At time: 245.24098181724548 and batch: 2900, loss is 9.480815811157226 and perplexity is 13105.874072361306
At time: 249.45907354354858 and batch: 2950, loss is 9.478122291564942 and perplexity is 13070.620642988515
At time: 253.67890071868896 and batch: 3000, loss is 9.468027439117431 and perplexity is 12939.338408462647
At time: 257.856938123703 and batch: 3050, loss is 9.429330825805664 and perplexity is 12448.193933726172
At time: 262.0450282096863 and batch: 3100, loss is 9.420724010467529 and perplexity is 12341.514371435209
At time: 266.2566783428192 and batch: 3150, loss is 9.389604377746583 and perplexity is 11963.365422302832
At time: 270.4834032058716 and batch: 3200, loss is 9.376029567718506 and perplexity is 11802.062315921145
At time: 274.7024476528168 and batch: 3250, loss is 9.348840923309327 and perplexity is 11485.503148038602
At time: 278.9262490272522 and batch: 3300, loss is 9.356941432952881 and perplexity is 11578.91942596325
At time: 283.15053153038025 and batch: 3350, loss is 9.335678749084472 and perplexity is 11335.319494431762
At time: 287.34757804870605 and batch: 3400, loss is 9.33176124572754 and perplexity is 11291.000209414378
At time: 291.51422691345215 and batch: 3450, loss is 9.308143787384033 and perplexity is 11027.459809301195
At time: 295.6951231956482 and batch: 3500, loss is 9.296849155426026 and perplexity is 10903.60944806389
At time: 299.90597200393677 and batch: 3550, loss is 9.293293209075928 and perplexity is 10864.90565293699
At time: 304.1253116130829 and batch: 3600, loss is 9.296033439636231 and perplexity is 10894.718828273799
At time: 308.34177684783936 and batch: 3650, loss is 9.287054061889648 and perplexity is 10797.328937154192
At time: 312.5470495223999 and batch: 3700, loss is 9.253419342041015 and perplexity is 10440.203379490755
At time: 316.718035697937 and batch: 3750, loss is 9.257826671600341 and perplexity is 10486.318343726201
At time: 320.9135389328003 and batch: 3800, loss is 9.215932865142822 and perplexity is 10056.081603491359
At time: 325.10847759246826 and batch: 3850, loss is 9.209826927185059 and perplexity is 9994.866869990965
At time: 329.29819893836975 and batch: 3900, loss is 9.217493915557862 and perplexity is 10071.791912954375
At time: 333.4941041469574 and batch: 3950, loss is 9.179205436706543 and perplexity is 9693.447654536427
At time: 337.6852424144745 and batch: 4000, loss is 9.185276832580566 and perplexity is 9752.479433947292
At time: 341.89150500297546 and batch: 4050, loss is 9.15451940536499 and perplexity is 9457.084341207104
At time: 346.0943510532379 and batch: 4100, loss is 9.143329467773437 and perplexity is 9351.850038342925
At time: 350.28948974609375 and batch: 4150, loss is 9.144843082427979 and perplexity is 9366.015853696368
At time: 354.4866874217987 and batch: 4200, loss is 9.133656768798827 and perplexity is 9261.828485631879
At time: 358.68081283569336 and batch: 4250, loss is 9.11643268585205 and perplexity is 9103.667978494515
At time: 362.9041669368744 and batch: 4300, loss is 9.102366123199463 and perplexity is 8976.507117367155
At time: 367.09431767463684 and batch: 4350, loss is 9.092122516632081 and perplexity is 8885.024665179217
At time: 371.2773633003235 and batch: 4400, loss is 9.055451011657714 and perplexity is 8565.099362280513
At time: 375.4653856754303 and batch: 4450, loss is 9.058730659484864 and perplexity is 8593.235985681235
At time: 379.654066324234 and batch: 4500, loss is 9.050165939331055 and perplexity is 8519.951602343013
At time: 383.8473312854767 and batch: 4550, loss is 9.044017391204834 and perplexity is 8467.726987190395
At time: 388.0390577316284 and batch: 4600, loss is 9.030111923217774 and perplexity is 8350.794167627693
At time: 392.20020627975464 and batch: 4650, loss is 9.03896074295044 and perplexity is 8425.016746640864
At time: 396.35804510116577 and batch: 4700, loss is 9.008795566558838 and perplexity is 8174.669497916928
At time: 400.5200848579407 and batch: 4750, loss is 9.000798168182373 and perplexity is 8109.554133158084
At time: 404.70504570007324 and batch: 4800, loss is 9.005300483703614 and perplexity is 8146.148221852838
At time: 408.900141954422 and batch: 4850, loss is 8.983974094390868 and perplexity is 7974.259689097408
At time: 413.09591579437256 and batch: 4900, loss is 8.972069625854493 and perplexity is 7879.893171584375
At time: 417.289267539978 and batch: 4950, loss is 8.9802095413208 and perplexity is 7944.296599509793
At time: 421.4847037792206 and batch: 5000, loss is 8.940179386138915 and perplexity is 7632.56610941313
At time: 425.6794784069061 and batch: 5050, loss is 8.928456611633301 and perplexity is 7543.613661434939
At time: 429.83502888679504 and batch: 5100, loss is 8.928664073944091 and perplexity is 7545.178839308944
At time: 433.96844959259033 and batch: 5150, loss is 8.911958961486816 and perplexity is 7420.182722164178
At time: 438.1244330406189 and batch: 5200, loss is 8.897003421783447 and perplexity is 7310.035592766294
At time: 442.28540086746216 and batch: 5250, loss is 8.898146324157715 and perplexity is 7318.395025899293
At time: 446.47918701171875 and batch: 5300, loss is 8.884181594848632 and perplexity is 7216.90590375341
At time: 450.6731309890747 and batch: 5350, loss is 8.882945346832276 and perplexity is 7207.989530705492
At time: 454.86199474334717 and batch: 5400, loss is 8.885330276489258 and perplexity is 7225.200594134401
At time: 459.0475535392761 and batch: 5450, loss is 8.854236297607422 and perplexity is 7003.997227829109
At time: 463.2081506252289 and batch: 5500, loss is 8.852040405273437 and perplexity is 6988.6340780939845
At time: 467.3702564239502 and batch: 5550, loss is 8.844357109069824 and perplexity is 6935.1440852821615
At time: 471.54181122779846 and batch: 5600, loss is 8.861494274139405 and perplexity is 7055.017001529806
At time: 475.7328143119812 and batch: 5650, loss is 8.853016719818115 and perplexity is 6995.460515024451
At time: 479.906818151474 and batch: 5700, loss is 8.82939926147461 and perplexity is 6832.181228078001
At time: 484.0741744041443 and batch: 5750, loss is 8.828328571319581 and perplexity is 6824.869993631552
At time: 488.23571729660034 and batch: 5800, loss is 8.827338657379151 and perplexity is 6818.117302526196
At time: 492.3642692565918 and batch: 5850, loss is 8.807646389007568 and perplexity is 6685.166455039451
At time: 496.5212411880493 and batch: 5900, loss is 8.8097829246521 and perplexity is 6699.464820504927
At time: 500.6743907928467 and batch: 5950, loss is 8.789678401947022 and perplexity is 6566.120184412004
At time: 504.83054423332214 and batch: 6000, loss is 8.796178741455078 and perplexity is 6608.941219713785
At time: 508.9828562736511 and batch: 6050, loss is 8.777752113342284 and perplexity is 6488.275859709564
At time: 513.1399405002594 and batch: 6100, loss is 8.763182621002198 and perplexity is 6394.430275575893
At time: 517.2973234653473 and batch: 6150, loss is 8.751995449066163 and perplexity is 6323.293337144787
At time: 521.450443983078 and batch: 6200, loss is 8.738039379119874 and perplexity is 6235.657958184364
At time: 525.6084206104279 and batch: 6250, loss is 8.748583240509033 and perplexity is 6301.753711257543
At time: 529.7597253322601 and batch: 6300, loss is 8.751565341949464 and perplexity is 6320.574228475417
At time: 533.9151725769043 and batch: 6350, loss is 8.720784492492676 and perplexity is 6128.985345621081
At time: 538.0718522071838 and batch: 6400, loss is 8.73290433883667 and perplexity is 6203.719675740992
At time: 542.2252449989319 and batch: 6450, loss is 8.716753540039063 and perplexity is 6104.329423913339
At time: 546.3855957984924 and batch: 6500, loss is 8.713281784057617 and perplexity is 6083.173427201806
At time: 550.5378801822662 and batch: 6550, loss is 8.687903366088868 and perplexity is 5930.734618894971
At time: 554.6937117576599 and batch: 6600, loss is 8.716360759735107 and perplexity is 6101.932234362004
At time: 558.8473224639893 and batch: 6650, loss is 8.696221141815185 and perplexity is 5980.270869460087
At time: 562.9824368953705 and batch: 6700, loss is 8.695342693328858 and perplexity is 5975.019816294509
At time: 567.1157667636871 and batch: 6750, loss is 8.662217502593995 and perplexity is 5780.338376465705
At time: 571.2508850097656 and batch: 6800, loss is 8.677644653320312 and perplexity is 5870.203930266559
At time: 575.3816905021667 and batch: 6850, loss is 8.667544612884521 and perplexity is 5811.213039868598
At time: 579.5624308586121 and batch: 6900, loss is 8.66924446105957 and perplexity is 5821.099620211422
At time: 583.744720697403 and batch: 6950, loss is 8.670973815917968 and perplexity is 5831.175076629577
At time: 587.9045302867889 and batch: 7000, loss is 8.653889770507812 and perplexity is 5732.401148336812
At time: 592.0674939155579 and batch: 7050, loss is 8.646043815612792 and perplexity is 5687.600967407224
At time: 596.2250530719757 and batch: 7100, loss is 8.597141208648681 and perplexity is 5416.153784393656
At time: 600.3578586578369 and batch: 7150, loss is 8.666596660614013 and perplexity is 5805.70689746547
At time: 604.4795370101929 and batch: 7200, loss is 8.664468555450439 and perplexity is 5793.364879852389
At time: 608.6142907142639 and batch: 7250, loss is 8.60754545211792 and perplexity is 5472.798931004274
At time: 612.7432124614716 and batch: 7300, loss is 8.591793231964111 and perplexity is 5387.265635659729
At time: 616.8689601421356 and batch: 7350, loss is 8.590825233459473 and perplexity is 5382.053293756911
At time: 621.0015025138855 and batch: 7400, loss is 8.59613431930542 and perplexity is 5410.703061464611
At time: 625.1471121311188 and batch: 7450, loss is 8.60756248474121 and perplexity is 5472.892147920673
At time: 629.2954869270325 and batch: 7500, loss is 8.587016925811767 and perplexity is 5361.595758057749
At time: 633.4261591434479 and batch: 7550, loss is 8.6096236038208 and perplexity is 5484.184063340912
At time: 637.5608849525452 and batch: 7600, loss is 8.62060480117798 and perplexity is 5544.738844377104
At time: 641.6983046531677 and batch: 7650, loss is 8.602829208374024 and perplexity is 5447.048647321666
At time: 645.8342266082764 and batch: 7700, loss is 8.562009735107422 and perplexity is 5229.179885501069
At time: 649.9815213680267 and batch: 7750, loss is 8.570016021728515 and perplexity is 5271.214243481127
At time: 654.1360237598419 and batch: 7800, loss is 8.577329425811767 and perplexity is 5309.906075298097
At time: 658.2862622737885 and batch: 7850, loss is 8.532724781036377 and perplexity is 5078.264158193495
At time: 662.421736240387 and batch: 7900, loss is 8.544658317565919 and perplexity is 5139.22884771351
At time: 666.5649628639221 and batch: 7950, loss is 8.587230739593506 and perplexity is 5362.742263687921
At time: 670.7036271095276 and batch: 8000, loss is 8.549639549255371 and perplexity is 5164.892402299687
At time: 674.8428988456726 and batch: 8050, loss is 8.533033084869384 and perplexity is 5079.830047870981
At time: 678.9863407611847 and batch: 8100, loss is 8.536820011138916 and perplexity is 5099.103460186699
At time: 683.1209576129913 and batch: 8150, loss is 8.55190357208252 and perplexity is 5176.599083694706
At time: 687.2737197875977 and batch: 8200, loss is 8.523827266693115 and perplexity is 5033.280647485393
At time: 691.4285986423492 and batch: 8250, loss is 8.518584327697754 and perplexity is 5006.960521802256
At time: 695.5800247192383 and batch: 8300, loss is 8.541349201202392 and perplexity is 5122.250648349262
At time: 699.738044500351 and batch: 8350, loss is 8.525189208984376 and perplexity is 5040.14035546586
At time: 703.8859302997589 and batch: 8400, loss is 8.498108501434325 and perplexity is 4905.481348494839
At time: 708.0188479423523 and batch: 8450, loss is 8.519944229125976 and perplexity is 5013.774126432188
At time: 712.1491687297821 and batch: 8500, loss is 8.502590255737305 and perplexity is 4927.515850366134
At time: 716.2600712776184 and batch: 8550, loss is 8.519903869628907 and perplexity is 5013.571777113414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 569 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
