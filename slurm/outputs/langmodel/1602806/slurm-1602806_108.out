Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'wordvec_source': 'glove', 'lr': 4.857434427133022, 'wordvec_dim': 200, 'anneal': 6.489112728821843, 'tune_wordvecs': True, 'seq_len': 50, 'data': 'gigasmall', 'dropout': 0.48068792282763206, 'batch_size': 80}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 5360 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 11.926636695861816 and batch: 50, loss is 8.795599489212035 and perplexity is 6605.114084233546
At time: 15.854384422302246 and batch: 100, loss is 7.821061048507691 and perplexity is 2492.548720555366
At time: 19.803006887435913 and batch: 150, loss is 7.548169498443603 and perplexity is 1897.2666013128974
At time: 23.74668025970459 and batch: 200, loss is 7.295041360855103 and perplexity is 1472.9778232570543
At time: 27.683459281921387 and batch: 250, loss is 7.104337348937988 and perplexity is 1217.2352152193491
At time: 31.616389274597168 and batch: 300, loss is 6.953501710891723 and perplexity is 1046.8089399569396
At time: 35.56221580505371 and batch: 350, loss is 6.809296560287476 and perplexity is 906.2331023275098
At time: 39.92154884338379 and batch: 400, loss is 6.680667819976807 and perplexity is 796.8510874389593
At time: 43.87523436546326 and batch: 450, loss is 6.591873712539673 and perplexity is 729.1458003852432
At time: 47.82309126853943 and batch: 500, loss is 6.454775924682617 and perplexity is 635.7312586080777
At time: 51.77430987358093 and batch: 550, loss is 6.374513940811157 and perplexity is 586.7001906067416
At time: 55.740001916885376 and batch: 600, loss is 6.255185804367065 and perplexity is 520.7061152399182
At time: 59.70245575904846 and batch: 650, loss is 6.205199785232544 and perplexity is 495.31790325823624
At time: 63.65721130371094 and batch: 700, loss is 6.171135082244873 and perplexity is 478.72919481490237
At time: 67.62135195732117 and batch: 750, loss is 6.107854948043824 and perplexity is 449.37375054533726
At time: 71.56774878501892 and batch: 800, loss is 6.010212259292603 and perplexity is 407.5698215752379
At time: 75.51807427406311 and batch: 850, loss is 5.982258787155152 and perplexity is 396.33459333150887
At time: 79.47132873535156 and batch: 900, loss is 5.913084983825684 and perplexity is 369.8453642521526
At time: 83.4257071018219 and batch: 950, loss is 5.912687425613403 and perplexity is 369.69835841394564
At time: 87.39101099967957 and batch: 1000, loss is 5.914880056381225 and perplexity is 370.50985974569494
At time: 91.36288785934448 and batch: 1050, loss is 5.868976469039917 and perplexity is 353.8865809639751
At time: 95.32075595855713 and batch: 1100, loss is 5.83251311302185 and perplexity is 341.21511460912257
At time: 99.27638864517212 and batch: 1150, loss is 5.806525440216064 and perplexity is 332.4619575663985
At time: 103.23197484016418 and batch: 1200, loss is 5.789023218154907 and perplexity is 326.6937599418873
At time: 107.19147825241089 and batch: 1250, loss is 5.709897117614746 and perplexity is 301.8400126648116
At time: 111.1463634967804 and batch: 1300, loss is 5.729788093566895 and perplexity is 307.90401462233245
At time: 115.10279703140259 and batch: 1350, loss is 5.707790441513062 and perplexity is 301.2048028491938
At time: 119.07867431640625 and batch: 1400, loss is 5.6644501209259035 and perplexity is 288.429336246508
At time: 123.04700970649719 and batch: 1450, loss is 5.691791725158692 and perplexity is 296.42425591921403
At time: 127.01813793182373 and batch: 1500, loss is 5.662437181472779 and perplexity is 287.849329411305
At time: 130.98737406730652 and batch: 1550, loss is 5.66678204536438 and perplexity is 289.1027164952578
At time: 134.96037316322327 and batch: 1600, loss is 5.648820867538452 and perplexity is 283.956446083169
At time: 138.93090558052063 and batch: 1650, loss is 5.656515979766846 and perplexity is 286.149951615397
At time: 142.90060830116272 and batch: 1700, loss is 5.632838525772095 and perplexity is 279.4542309207856
At time: 146.8706238269806 and batch: 1750, loss is 5.641382627487182 and perplexity is 281.85214572083044
At time: 150.84038090705872 and batch: 1800, loss is 5.6074799633026124 and perplexity is 272.4567710860546
At time: 154.7886905670166 and batch: 1850, loss is 5.619930582046509 and perplexity is 275.87023222312797
At time: 158.74664902687073 and batch: 1900, loss is 5.5860796070098875 and perplexity is 266.6880456596295
At time: 162.7182970046997 and batch: 1950, loss is 5.55271357536316 and perplexity is 257.93653734044835
At time: 166.68473100662231 and batch: 2000, loss is 5.555005159378052 and perplexity is 258.5282983623612
At time: 170.65696144104004 and batch: 2050, loss is 5.575030946731568 and perplexity is 263.7577179499362
At time: 174.62735652923584 and batch: 2100, loss is 5.547651290893555 and perplexity is 256.6340886776019
At time: 178.61201238632202 and batch: 2150, loss is 5.55163164138794 and perplexity is 257.6576179506394
At time: 182.59392380714417 and batch: 2200, loss is 5.534070596694947 and perplexity is 253.17237901345462
At time: 186.56419205665588 and batch: 2250, loss is 5.515910120010376 and perplexity is 248.6161448403588
At time: 190.54795598983765 and batch: 2300, loss is 5.518436403274536 and perplexity is 249.24501366220002
At time: 194.50408673286438 and batch: 2350, loss is 5.505286436080933 and perplexity is 245.9889056867261
At time: 198.48874473571777 and batch: 2400, loss is 5.520352554321289 and perplexity is 249.7230626167608
At time: 202.45984077453613 and batch: 2450, loss is 5.505187101364136 and perplexity is 245.96447166203805
At time: 206.42800211906433 and batch: 2500, loss is 5.496138544082641 and perplexity is 243.7488870899326
At time: 210.41333484649658 and batch: 2550, loss is 5.520576496124267 and perplexity is 249.77899231191304
At time: 214.38635277748108 and batch: 2600, loss is 5.530701198577881 and perplexity is 252.32077597682388
At time: 218.36045789718628 and batch: 2650, loss is 5.519894542694092 and perplexity is 249.6087127392888
At time: 222.33827757835388 and batch: 2700, loss is 5.518073568344116 and perplexity is 249.15459526947797
At time: 226.30437064170837 and batch: 2750, loss is 5.439994487762451 and perplexity is 230.44091321208657
At time: 230.2758617401123 and batch: 2800, loss is 5.444971265792847 and perplexity is 231.59062504352195
At time: 234.24772191047668 and batch: 2850, loss is 5.464320583343506 and perplexity is 236.11537987831775
At time: 238.22009706497192 and batch: 2900, loss is 5.454951791763306 and perplexity is 233.91359423573732
At time: 242.18464255332947 and batch: 2950, loss is 5.472770376205444 and perplexity is 238.11895892264693
At time: 246.14374780654907 and batch: 3000, loss is 5.472813158035279 and perplexity is 238.12914630534377
At time: 250.1130015850067 and batch: 3050, loss is 5.417150716781617 and perplexity is 225.2364449289106
At time: 254.089421749115 and batch: 3100, loss is 5.441776361465454 and perplexity is 230.8518958662854
At time: 258.05670952796936 and batch: 3150, loss is 5.426859884262085 and perplexity is 227.43395402449556
At time: 262.036824464798 and batch: 3200, loss is 5.433526344299317 and perplexity is 228.95519841368807
At time: 266.0174298286438 and batch: 3250, loss is 5.432768869400024 and perplexity is 228.78183626485443
At time: 269.99651312828064 and batch: 3300, loss is 5.416098680496216 and perplexity is 224.99961261598747
At time: 273.96632194519043 and batch: 3350, loss is 5.4245862865448 and perplexity is 226.9174480916428
At time: 277.9360466003418 and batch: 3400, loss is 5.424685745239258 and perplexity is 226.94001812715427
At time: 281.924293756485 and batch: 3450, loss is 5.4101212787628175 and perplexity is 223.65871108754018
At time: 285.8956322669983 and batch: 3500, loss is 5.426284132003784 and perplexity is 227.30304610073415
At time: 289.86836290359497 and batch: 3550, loss is 5.388585233688355 and perplexity is 218.89348326002766
At time: 293.8534288406372 and batch: 3600, loss is 5.396951341629029 and perplexity is 220.73245154604123
At time: 297.8251600265503 and batch: 3650, loss is 5.4175385475158695 and perplexity is 225.32381548612642
At time: 301.7922704219818 and batch: 3700, loss is 5.407517747879028 and perplexity is 223.07716608947078
At time: 305.76142477989197 and batch: 3750, loss is 5.3949507713317875 and perplexity is 220.2913021821317
At time: 309.74010610580444 and batch: 3800, loss is 5.349880771636963 and perplexity is 210.58318888095917
At time: 313.7137875556946 and batch: 3850, loss is 5.411417045593262 and perplexity is 223.94870847062657
At time: 317.68578243255615 and batch: 3900, loss is 5.41343397140503 and perplexity is 224.40085221817426
At time: 321.65823125839233 and batch: 3950, loss is 5.3867527103424075 and perplexity is 218.49272315495304
At time: 325.63028740882874 and batch: 4000, loss is 5.366363439559937 and perplexity is 214.08292491368618
At time: 329.6110610961914 and batch: 4050, loss is 5.3988101768493655 and perplexity is 221.14313838257166
At time: 333.5874662399292 and batch: 4100, loss is 5.3578352355957035 and perplexity is 212.2649451344417
At time: 337.5543563365936 and batch: 4150, loss is 5.402688007354737 and perplexity is 222.00235886963756
At time: 341.5217032432556 and batch: 4200, loss is 5.337376518249512 and perplexity is 207.9663979324968
At time: 345.4932816028595 and batch: 4250, loss is 5.400431232452393 and perplexity is 221.50191442550846
At time: 349.45810413360596 and batch: 4300, loss is 5.391465702056885 and perplexity is 219.52490797751577
At time: 353.4458544254303 and batch: 4350, loss is 5.412389183044434 and perplexity is 224.16652325307237
At time: 357.42306685447693 and batch: 4400, loss is 5.38157244682312 and perplexity is 217.36379985661873
At time: 361.38392877578735 and batch: 4450, loss is 5.411692657470703 and perplexity is 224.0104399011857
At time: 365.3541929721832 and batch: 4500, loss is 5.362243719100952 and perplexity is 213.2027773338247
At time: 369.3227343559265 and batch: 4550, loss is 5.384121513366699 and perplexity is 217.91858143379488
At time: 373.2848858833313 and batch: 4600, loss is 5.374110088348389 and perplexity is 215.7477903828468
At time: 377.25890922546387 and batch: 4650, loss is 5.322879533767701 and perplexity is 204.97326044306857
At time: 381.2250201702118 and batch: 4700, loss is 5.4188025856018065 and perplexity is 225.60881345676688
At time: 385.1909120082855 and batch: 4750, loss is 5.362887926101685 and perplexity is 213.34016830492
At time: 389.157288312912 and batch: 4800, loss is 5.3671536636352535 and perplexity is 214.25216525515648
At time: 393.1247055530548 and batch: 4850, loss is 5.335649538040161 and perplexity is 207.60755402640592
At time: 397.09348607063293 and batch: 4900, loss is 5.358255681991577 and perplexity is 212.35420992980946
At time: 401.0719120502472 and batch: 4950, loss is 5.369492692947388 and perplexity is 214.75389390013132
At time: 405.04677653312683 and batch: 5000, loss is 5.332131452560425 and perplexity is 206.87845617138964
At time: 409.0174379348755 and batch: 5050, loss is 5.34593469619751 and perplexity is 209.75384912614555
At time: 412.9869599342346 and batch: 5100, loss is 5.327497072219849 and perplexity is 205.92192090732837
At time: 416.9642117023468 and batch: 5150, loss is 5.3499322032928465 and perplexity is 210.59401980158825
At time: 420.94831013679504 and batch: 5200, loss is 5.384402561187744 and perplexity is 217.97983558354156
At time: 424.92102551460266 and batch: 5250, loss is 5.3708850765228275 and perplexity is 215.05312196653853
At time: 428.89491605758667 and batch: 5300, loss is 5.33972900390625 and perplexity is 208.45621181488934
At time: 432.86589336395264 and batch: 5350, loss is 5.318827028274536 and perplexity is 204.14428602536844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 356 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
