Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'lr': 23.539469069257617, 'tune_wordvecs': True, 'anneal': 6.806922257196906, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.5657637377857802}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.156714677810669 and batch: 50, loss is 7.550477046966552 and perplexity is 1901.649691207788
At time: 5.58575439453125 and batch: 100, loss is 6.472808198928833 and perplexity is 647.2989211960696
At time: 8.017988920211792 and batch: 150, loss is 6.3898015975952145 and perplexity is 595.7383720364281
At time: 10.454860925674438 and batch: 200, loss is 6.425464134216309 and perplexity is 617.367292340728
At time: 12.891542434692383 and batch: 250, loss is 6.44410249710083 and perplexity is 628.98191047197
At time: 15.323492050170898 and batch: 300, loss is 6.522143220901489 and perplexity is 680.034288321757
At time: 17.746225833892822 and batch: 350, loss is 6.553045930862427 and perplexity is 701.3772702081659
At time: 20.163452863693237 and batch: 400, loss is 6.642541446685791 and perplexity is 767.0419141045106
At time: 22.575319528579712 and batch: 450, loss is 6.717532291412353 and perplexity is 826.7747528938788
At time: 24.99253559112549 and batch: 500, loss is 6.775354700088501 and perplexity is 875.9900216740789
At time: 27.40911865234375 and batch: 550, loss is 6.941128787994384 and perplexity is 1033.936651786094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.7923940022786455 and perplexity of 891.0441722486884
Finished 1 epochs...
Completing Train Step...
At time: 31.46208953857422 and batch: 50, loss is 6.886630487442017 and perplexity is 979.0967739510922
At time: 33.88493490219116 and batch: 100, loss is 6.83553840637207 and perplexity is 930.3291110286701
At time: 36.30041742324829 and batch: 150, loss is 6.846334495544434 and perplexity is 940.4274402176949
At time: 38.70185661315918 and batch: 200, loss is 6.998271284103393 and perplexity is 1094.7390289323328
At time: 41.10572552680969 and batch: 250, loss is 6.93980375289917 and perplexity is 1032.5675566860891
At time: 43.51492524147034 and batch: 300, loss is 7.104359035491943 and perplexity is 1217.2616131427592
At time: 45.92309331893921 and batch: 350, loss is 7.153930835723877 and perplexity is 1279.1241126929856
At time: 48.33143711090088 and batch: 400, loss is 6.945112037658691 and perplexity is 1038.063292871894
At time: 50.73841428756714 and batch: 450, loss is 6.756413974761963 and perplexity is 859.5542790864637
At time: 53.14558124542236 and batch: 500, loss is 6.676012372970581 and perplexity is 793.1500111961979
At time: 55.54878640174866 and batch: 550, loss is 6.7777328777313235 and perplexity is 878.0757607048946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.052082824707031 and perplexity of 424.99730371616397
Finished 2 epochs...
Completing Train Step...
At time: 59.45070695877075 and batch: 50, loss is 6.722668685913086 and perplexity is 831.0323190973548
At time: 61.88766145706177 and batch: 100, loss is 6.91631049156189 and perplexity is 1008.5919129961919
At time: 64.31682395935059 and batch: 150, loss is 6.790534753799438 and perplexity is 889.3890388554399
At time: 66.74050068855286 and batch: 200, loss is 6.689507970809936 and perplexity is 803.9265994620255
At time: 69.20782685279846 and batch: 250, loss is 6.749839992523193 and perplexity is 853.9221176709057
At time: 71.63789510726929 and batch: 300, loss is 6.799379606246948 and perplexity is 897.2904455492645
At time: 74.05051922798157 and batch: 350, loss is 6.811888027191162 and perplexity is 908.584621044673
At time: 76.45612740516663 and batch: 400, loss is 6.879421834945679 and perplexity is 972.0641837506716
At time: 78.86133408546448 and batch: 450, loss is 6.843457040786743 and perplexity is 937.7252923241581
At time: 81.26834321022034 and batch: 500, loss is 6.762886543273925 and perplexity is 865.1358471020567
At time: 83.67796564102173 and batch: 550, loss is 6.713800458908081 and perplexity is 823.6951179123436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.241777547200521 and perplexity of 513.7709518663722
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 87.59591388702393 and batch: 50, loss is 6.323138227462769 and perplexity is 557.3192455103797
At time: 90.0095739364624 and batch: 100, loss is 6.292202281951904 and perplexity is 540.3420051500901
At time: 92.41784071922302 and batch: 150, loss is 6.312100019454956 and perplexity is 551.2012676266932
At time: 94.82722163200378 and batch: 200, loss is 6.301124591827392 and perplexity is 545.1846757365022
At time: 97.23687624931335 and batch: 250, loss is 6.2346306324005125 and perplexity is 510.1121647403279
At time: 99.65976524353027 and batch: 300, loss is 6.21200629234314 and perplexity is 498.7007878424253
At time: 102.07601165771484 and batch: 350, loss is 6.186976766586303 and perplexity is 486.37346076251686
At time: 104.48742341995239 and batch: 400, loss is 6.213749647140503 and perplexity is 499.57095854109883
At time: 106.89858341217041 and batch: 450, loss is 6.209216985702515 and perplexity is 497.3116966245117
At time: 109.31034255027771 and batch: 500, loss is 6.208150644302368 and perplexity is 496.78167521573243
At time: 111.72359800338745 and batch: 550, loss is 6.183879432678222 and perplexity is 484.8693303500573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.859442138671875 and perplexity of 350.52854314727915
Finished 4 epochs...
Completing Train Step...
At time: 115.64025688171387 and batch: 50, loss is 6.226119661331177 and perplexity is 505.789037963599
At time: 118.05156898498535 and batch: 100, loss is 6.2009732532501225 and perplexity is 493.228844146062
At time: 120.4891414642334 and batch: 150, loss is 6.226838531494141 and perplexity is 506.15276533245793
At time: 122.91321849822998 and batch: 200, loss is 6.233560924530029 and perplexity is 509.5667854931177
At time: 125.3501284122467 and batch: 250, loss is 6.183005285263062 and perplexity is 484.44566827677215
At time: 127.78867173194885 and batch: 300, loss is 6.176900491714478 and perplexity is 481.4972364272681
At time: 130.22487497329712 and batch: 350, loss is 6.150324573516846 and perplexity is 468.8695447249351
At time: 132.6630458831787 and batch: 400, loss is 6.166520233154297 and perplexity is 476.5250217005489
At time: 135.1021535396576 and batch: 450, loss is 6.142456178665161 and perplexity is 465.19477026060355
At time: 137.54150915145874 and batch: 500, loss is 6.134474935531617 and perplexity is 461.49671486488205
At time: 139.98176741600037 and batch: 550, loss is 6.106618947982788 and perplexity is 448.81866767413345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.804216512044271 and perplexity of 331.6952123070251
Finished 5 epochs...
Completing Train Step...
At time: 143.8897898197174 and batch: 50, loss is 6.1580242252349855 and perplexity is 472.49361104050627
At time: 146.36995601654053 and batch: 100, loss is 6.13034538269043 and perplexity is 459.5948693832324
At time: 148.8093945980072 and batch: 150, loss is 6.152472381591797 and perplexity is 469.87766876011915
At time: 151.24756503105164 and batch: 200, loss is 6.154027662277222 and perplexity is 470.6090290106663
At time: 153.68646717071533 and batch: 250, loss is 6.1086205387115475 and perplexity is 449.717918624199
At time: 156.12813210487366 and batch: 300, loss is 6.100967426300048 and perplexity is 446.28931332485956
At time: 158.5693621635437 and batch: 350, loss is 6.078095502853394 and perplexity is 436.1976659374991
At time: 161.00961089134216 and batch: 400, loss is 6.094897565841674 and perplexity is 443.5886042244964
At time: 163.45275378227234 and batch: 450, loss is 6.076506700515747 and perplexity is 435.50518432005657
At time: 165.89203310012817 and batch: 500, loss is 6.0706892013549805 and perplexity is 432.9789884714421
At time: 168.3289737701416 and batch: 550, loss is 6.040556612014771 and perplexity is 420.1268174518678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.7666371663411455 and perplexity of 319.4616279657064
Finished 6 epochs...
Completing Train Step...
At time: 172.2351257801056 and batch: 50, loss is 6.100008211135864 and perplexity is 445.8614310962737
At time: 174.703027009964 and batch: 100, loss is 6.081755790710449 and perplexity is 437.79720055012484
At time: 177.15483617782593 and batch: 150, loss is 6.112221899032593 and perplexity is 451.3404347709895
At time: 179.64446759223938 and batch: 200, loss is 6.118681392669678 and perplexity is 454.2653018476736
At time: 182.0868787765503 and batch: 250, loss is 6.07290675163269 and perplexity is 433.9402065283524
At time: 184.52949690818787 and batch: 300, loss is 6.067213563919068 and perplexity is 431.47672266806705
At time: 186.96946358680725 and batch: 350, loss is 6.04637487411499 and perplexity is 422.5783503056734
At time: 189.40995287895203 and batch: 400, loss is 6.065675401687622 and perplexity is 430.81355163258326
At time: 191.8512680530548 and batch: 450, loss is 6.044522533416748 and perplexity is 421.79631575003094
At time: 194.29372572898865 and batch: 500, loss is 6.038202209472656 and perplexity is 419.1388333174111
At time: 196.73729705810547 and batch: 550, loss is 6.011852760314941 and perplexity is 408.23898901906983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.746050516764323 and perplexity of 312.95221675588215
Finished 7 epochs...
Completing Train Step...
At time: 200.67074704170227 and batch: 50, loss is 6.07110595703125 and perplexity is 433.15947252885843
At time: 203.10920548439026 and batch: 100, loss is 6.051764316558838 and perplexity is 424.86196016712614
At time: 205.5457901954651 and batch: 150, loss is 6.083544397354126 and perplexity is 438.58094823082484
At time: 207.98684096336365 and batch: 200, loss is 6.086673793792724 and perplexity is 439.9555916685571
At time: 210.4265570640564 and batch: 250, loss is 6.039584274291992 and perplexity is 419.7185108370396
At time: 212.86982321739197 and batch: 300, loss is 6.036108770370483 and perplexity is 418.26230948924615
At time: 215.3176188468933 and batch: 350, loss is 6.011236972808838 and perplexity is 407.9876779351819
At time: 217.75879454612732 and batch: 400, loss is 6.028479776382446 and perplexity is 415.0835297081273
At time: 220.1983745098114 and batch: 450, loss is 6.005978412628174 and perplexity is 405.84788123353917
At time: 222.64883065223694 and batch: 500, loss is 5.998830804824829 and perplexity is 402.957382133504
At time: 225.08813071250916 and batch: 550, loss is 5.949911985397339 and perplexity is 383.719564649797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.666365559895834 and perplexity of 288.9823344853392
Finished 8 epochs...
Completing Train Step...
At time: 229.01408505439758 and batch: 50, loss is 5.980803956985474 and perplexity is 395.7584130317102
At time: 231.48219561576843 and batch: 100, loss is 5.945362844467163 and perplexity is 381.9779347360412
At time: 233.92470169067383 and batch: 150, loss is 5.969044704437255 and perplexity is 391.13184570496395
At time: 236.36713314056396 and batch: 200, loss is 5.968907375335693 and perplexity is 391.0781356080649
At time: 238.81082320213318 and batch: 250, loss is 5.9222369956970216 and perplexity is 373.24572977483695
At time: 241.28470396995544 and batch: 300, loss is 5.917936067581177 and perplexity is 371.6438739242199
At time: 243.72746419906616 and batch: 350, loss is 5.886511278152466 and perplexity is 360.1466386698624
At time: 246.1717345714569 and batch: 400, loss is 5.903382959365845 and perplexity is 366.2744659635528
At time: 248.61629223823547 and batch: 450, loss is 5.882125434875488 and perplexity is 358.5705507197144
At time: 251.06115245819092 and batch: 500, loss is 5.878920297622681 and perplexity is 357.4231227042665
At time: 253.50670337677002 and batch: 550, loss is 5.8488711166381835 and perplexity is 346.8426145345943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5958094278971355 and perplexity of 269.29553721881007
Finished 9 epochs...
Completing Train Step...
At time: 257.46446442604065 and batch: 50, loss is 5.9091653060913085 and perplexity is 368.3985270329664
At time: 259.9065947532654 and batch: 100, loss is 5.888230237960816 and perplexity is 360.76624865661034
At time: 262.3499720096588 and batch: 150, loss is 5.923798913955689 and perplexity is 373.82916461529504
At time: 264.7905809879303 and batch: 200, loss is 5.92871057510376 and perplexity is 375.6698033957851
At time: 267.23127365112305 and batch: 250, loss is 5.8892221260070805 and perplexity is 361.1242659133146
At time: 269.6798403263092 and batch: 300, loss is 5.887161693572998 and perplexity is 360.38095979208106
At time: 272.1233072280884 and batch: 350, loss is 5.859339447021484 and perplexity is 350.4925486408734
At time: 274.56462025642395 and batch: 400, loss is 5.87556755065918 and perplexity is 356.22678005269586
At time: 277.0056848526001 and batch: 450, loss is 5.853471851348877 and perplexity is 348.44202179348935
At time: 279.44636249542236 and batch: 500, loss is 5.850686702728272 and perplexity is 347.4729091649354
At time: 281.8878080844879 and batch: 550, loss is 5.822246198654175 and perplexity is 337.72981052108463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5743352254231775 and perplexity of 263.57427990358394
Finished 10 epochs...
Completing Train Step...
At time: 285.81889247894287 and batch: 50, loss is 5.883667488098144 and perplexity is 359.1239121397274
At time: 288.3008391857147 and batch: 100, loss is 5.865045776367188 and perplexity is 352.4982918296285
At time: 290.74205565452576 and batch: 150, loss is 5.899102048873901 and perplexity is 364.7098291837675
At time: 293.1876451969147 and batch: 200, loss is 5.90393835067749 and perplexity is 366.4779481205068
At time: 295.63144850730896 and batch: 250, loss is 5.862987966537475 and perplexity is 351.77366320940166
At time: 298.0877637863159 and batch: 300, loss is 5.862070732116699 and perplexity is 351.4511522288808
At time: 300.53562474250793 and batch: 350, loss is 5.833695888519287 and perplexity is 341.6189342529502
At time: 303.00675559043884 and batch: 400, loss is 5.848601894378662 and perplexity is 346.74924935076785
At time: 305.44798588752747 and batch: 450, loss is 5.828955307006836 and perplexity is 340.0032944108141
At time: 307.89158368110657 and batch: 500, loss is 5.821964321136474 and perplexity is 337.63462549633095
At time: 310.334135055542 and batch: 550, loss is 5.7957737922668455 and perplexity is 328.906590916764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.556255594889323 and perplexity of 258.85177352757034
Finished 11 epochs...
Completing Train Step...
At time: 314.2640383243561 and batch: 50, loss is 5.856074199676514 and perplexity is 349.34997019277955
At time: 316.70459246635437 and batch: 100, loss is 5.840720825195312 and perplexity is 344.0272348087844
At time: 319.14413022994995 and batch: 150, loss is 5.875656032562256 and perplexity is 356.258301070621
At time: 321.58348178863525 and batch: 200, loss is 5.882310085296631 and perplexity is 358.63676703616136
At time: 324.0246284008026 and batch: 250, loss is 5.84152006149292 and perplexity is 344.302303770116
At time: 326.4682950973511 and batch: 300, loss is 5.840595817565918 and perplexity is 343.98423146764264
At time: 328.92072892189026 and batch: 350, loss is 5.81332013130188 and perplexity is 334.7286258004811
At time: 331.3620111942291 and batch: 400, loss is 5.831746454238892 and perplexity is 340.9536192962766
At time: 333.8020040988922 and batch: 450, loss is 5.81125467300415 and perplexity is 334.0379712874158
At time: 336.2417175769806 and batch: 500, loss is 5.804317035675049 and perplexity is 331.728557190024
At time: 338.6821303367615 and batch: 550, loss is 5.778892240524292 and perplexity is 323.4007416803582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.54146474202474 and perplexity of 255.05131036247897
Finished 12 epochs...
Completing Train Step...
At time: 342.6050398349762 and batch: 50, loss is 5.837518987655639 and perplexity is 342.92747705607024
At time: 345.07165122032166 and batch: 100, loss is 5.816669511795044 and perplexity is 335.85163897959603
At time: 347.51113772392273 and batch: 150, loss is 5.851792230606079 and perplexity is 347.85726257035617
At time: 349.95113134384155 and batch: 200, loss is 5.859284181594848 and perplexity is 350.47317905587914
At time: 352.38982796669006 and batch: 250, loss is 5.820098524093628 and perplexity is 337.00525513178013
At time: 354.8330490589142 and batch: 300, loss is 5.820760717391968 and perplexity is 337.2284916579422
At time: 357.28202509880066 and batch: 350, loss is 5.788577518463135 and perplexity is 326.54818507752327
At time: 359.72768354415894 and batch: 400, loss is 5.804040851593018 and perplexity is 331.6369516935453
At time: 362.1831154823303 and batch: 450, loss is 5.785001306533814 and perplexity is 325.3824652355475
At time: 364.65531826019287 and batch: 500, loss is 5.775284929275513 and perplexity is 322.2362361767116
At time: 367.0940635204315 and batch: 550, loss is 5.747592763900757 and perplexity is 313.4352387899489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.519059244791666 and perplexity of 249.40030215968818
Finished 13 epochs...
Completing Train Step...
At time: 371.0520737171173 and batch: 50, loss is 5.811594152450562 and perplexity is 334.1513895634865
At time: 373.51066756248474 and batch: 100, loss is 5.792723941802978 and perplexity is 327.9050031207526
At time: 375.9762017726898 and batch: 150, loss is 5.829706487655639 and perplexity is 340.25879425736093
At time: 378.4358775615692 and batch: 200, loss is 5.836792793273926 and perplexity is 342.6785354498349
At time: 380.8978500366211 and batch: 250, loss is 5.797350721359253 and perplexity is 329.42566245060175
At time: 383.355162858963 and batch: 300, loss is 5.798445587158203 and perplexity is 329.7865368601905
At time: 385.81174874305725 and batch: 350, loss is 5.767900667190552 and perplexity is 319.8655231111908
At time: 388.2679135799408 and batch: 400, loss is 5.783540925979614 and perplexity is 324.9076298152377
At time: 390.7241590023041 and batch: 450, loss is 5.767624273300171 and perplexity is 319.7771264515703
At time: 393.1844162940979 and batch: 500, loss is 5.759467439651489 and perplexity is 317.17936677646463
At time: 395.6410355567932 and batch: 550, loss is 5.736862487792969 and perplexity is 310.0899720511484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5147959391276045 and perplexity of 248.3392957432551
Finished 14 epochs...
Completing Train Step...
At time: 399.750070810318 and batch: 50, loss is 5.797965469360352 and perplexity is 329.6282384783283
At time: 402.21947622299194 and batch: 100, loss is 5.780642900466919 and perplexity is 323.96740227450994
At time: 404.65669417381287 and batch: 150, loss is 5.813871917724609 and perplexity is 334.913375477991
At time: 407.11061358451843 and batch: 200, loss is 5.825044898986817 and perplexity is 338.6763389610384
At time: 409.5667996406555 and batch: 250, loss is 5.787385931015015 and perplexity is 326.15930609664827
At time: 412.02180886268616 and batch: 300, loss is 5.787979106903077 and perplexity is 326.35283332486
At time: 414.4784851074219 and batch: 350, loss is 5.755829982757568 and perplexity is 316.0277362751044
At time: 416.93627643585205 and batch: 400, loss is 5.77613600730896 and perplexity is 322.51060109531903
At time: 419.39904618263245 and batch: 450, loss is 5.759689893722534 and perplexity is 317.24993246637695
At time: 421.8556475639343 and batch: 500, loss is 5.749776067733765 and perplexity is 314.1203107361676
At time: 424.31278824806213 and batch: 550, loss is 5.724513597488404 and perplexity is 306.28425157676185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.509326171875 and perplexity of 246.98464578326517
Finished 15 epochs...
Completing Train Step...
At time: 428.53522872924805 and batch: 50, loss is 5.78633243560791 and perplexity is 325.81587969652145
At time: 430.9870309829712 and batch: 100, loss is 5.768278532028198 and perplexity is 319.98641188351957
At time: 433.4398159980774 and batch: 150, loss is 5.803828191757202 and perplexity is 331.56643333232506
At time: 435.88924193382263 and batch: 200, loss is 5.8138742351531985 and perplexity is 334.9141516167216
At time: 438.3436551094055 and batch: 250, loss is 5.777500238418579 and perplexity is 322.9508803435543
At time: 440.8020992279053 and batch: 300, loss is 5.778740100860595 and perplexity is 323.3515433428839
At time: 443.26023745536804 and batch: 350, loss is 5.746697874069214 and perplexity is 313.15487424830746
At time: 445.71968722343445 and batch: 400, loss is 5.763609933853149 and perplexity is 318.49600566588344
At time: 448.17731642723083 and batch: 450, loss is 5.747182931900024 and perplexity is 313.30680931798685
At time: 450.6360068321228 and batch: 500, loss is 5.740248613357544 and perplexity is 311.1417553630441
At time: 453.09168887138367 and batch: 550, loss is 5.713562183380127 and perplexity is 302.94830590504324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.50332285563151 and perplexity of 245.5063605938436
Finished 16 epochs...
Completing Train Step...
At time: 457.0988416671753 and batch: 50, loss is 5.780322198867798 and perplexity is 323.86352206870174
At time: 459.5855026245117 and batch: 100, loss is 5.76247311592102 and perplexity is 318.13413942232
At time: 462.0264301300049 and batch: 150, loss is 5.797099580764771 and perplexity is 329.34294068172807
At time: 464.46652936935425 and batch: 200, loss is 5.808547430038452 and perplexity is 333.1348723450813
At time: 466.9111247062683 and batch: 250, loss is 5.767095565795898 and perplexity is 319.6081025711551
At time: 469.35356402397156 and batch: 300, loss is 5.767599554061889 and perplexity is 319.7692219022823
At time: 471.79490303993225 and batch: 350, loss is 5.7363916015625 and perplexity is 309.9439893264278
At time: 474.2366855144501 and batch: 400, loss is 5.752858324050903 and perplexity is 315.09000370171185
At time: 476.6765480041504 and batch: 450, loss is 5.736074199676514 and perplexity is 309.845628130506
At time: 479.1323275566101 and batch: 500, loss is 5.72649037361145 and perplexity is 306.89030579154087
At time: 481.5883252620697 and batch: 550, loss is 5.702155542373657 and perplexity is 299.51231713630517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.502896626790364 and perplexity of 245.40174099980172
Finished 17 epochs...
Completing Train Step...
At time: 485.6448304653168 and batch: 50, loss is 5.767352495193482 and perplexity is 319.69022983841427
At time: 488.1031496524811 and batch: 100, loss is 5.746640748977661 and perplexity is 313.1369857583917
At time: 490.5988812446594 and batch: 150, loss is 5.780982036590576 and perplexity is 324.0772899558964
At time: 493.05590057373047 and batch: 200, loss is 5.7927226543426515 and perplexity is 327.9045809563419
At time: 495.5110058784485 and batch: 250, loss is 5.752995119094849 and perplexity is 315.13310940087086
At time: 497.96124720573425 and batch: 300, loss is 5.755605726242066 and perplexity is 315.9568729422443
At time: 500.40644812583923 and batch: 350, loss is 5.72180136680603 and perplexity is 305.454663557698
At time: 502.8549575805664 and batch: 400, loss is 5.742205982208252 and perplexity is 311.75137096981155
At time: 505.29742646217346 and batch: 450, loss is 5.724932489395141 and perplexity is 306.41257844657565
At time: 507.7490978240967 and batch: 500, loss is 5.717552919387817 and perplexity is 304.15970820354374
At time: 510.2138683795929 and batch: 550, loss is 5.689379177093506 and perplexity is 295.7099801129261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.503949483235677 and perplexity of 245.66024986698787
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 514.228217124939 and batch: 50, loss is 5.748635101318359 and perplexity is 313.7621143950769
At time: 516.7965252399445 and batch: 100, loss is 5.7191002368927 and perplexity is 304.6307041404851
At time: 519.2452473640442 and batch: 150, loss is 5.743417377471924 and perplexity is 312.12925394065695
At time: 521.6910111904144 and batch: 200, loss is 5.737348461151123 and perplexity is 310.24070413911903
At time: 524.1348531246185 and batch: 250, loss is 5.68333607673645 and perplexity is 293.9283636918459
At time: 526.5785038471222 and batch: 300, loss is 5.6780682563781735 and perplexity is 292.38407296793423
At time: 529.0213527679443 and batch: 350, loss is 5.618940896987915 and perplexity is 275.5973426358337
At time: 531.4653463363647 and batch: 400, loss is 5.614401721954346 and perplexity is 274.3491929867693
At time: 533.9087398052216 and batch: 450, loss is 5.590283451080322 and perplexity is 267.8115204208562
At time: 536.3519310951233 and batch: 500, loss is 5.5958194828033445 and perplexity is 269.29824497379235
At time: 538.7960288524628 and batch: 550, loss is 5.609417982101441 and perplexity is 272.9853094234755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.41127675374349 and perplexity of 223.91729249581533
Finished 19 epochs...
Completing Train Step...
At time: 542.7624020576477 and batch: 50, loss is 5.693577156066895 and perplexity is 296.95397369419953
At time: 545.2483520507812 and batch: 100, loss is 5.672505369186402 and perplexity is 290.76208899306783
At time: 547.690817117691 and batch: 150, loss is 5.7012106132507325 and perplexity is 299.2294328989942
At time: 550.1645045280457 and batch: 200, loss is 5.699107446670532 and perplexity is 298.6007648844207
At time: 552.6295874118805 and batch: 250, loss is 5.6540728950500485 and perplexity is 285.45171631318505
At time: 555.0911448001862 and batch: 300, loss is 5.652567138671875 and perplexity is 285.0222190109871
At time: 557.5520348548889 and batch: 350, loss is 5.6023243808746335 and perplexity is 271.0557124845596
At time: 560.0127158164978 and batch: 400, loss is 5.6112253284454345 and perplexity is 273.479134546518
At time: 562.4728243350983 and batch: 450, loss is 5.597158069610596 and perplexity is 269.6589654256872
At time: 564.9293372631073 and batch: 500, loss is 5.600485935211181 and perplexity is 270.55784907303
At time: 567.373692035675 and batch: 550, loss is 5.601441612243653 and perplexity is 270.8165385874688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.403639221191407 and perplexity of 222.2136310517491
Finished 20 epochs...
Completing Train Step...
At time: 571.3596217632294 and batch: 50, loss is 5.676295824050904 and perplexity is 291.86630097848223
At time: 573.8034360408783 and batch: 100, loss is 5.654043111801148 and perplexity is 285.4432147602715
At time: 576.2448828220367 and batch: 150, loss is 5.682926864624023 and perplexity is 293.8081092517023
At time: 578.6894373893738 and batch: 200, loss is 5.683944044113159 and perplexity is 294.10711688060763
At time: 581.133727312088 and batch: 250, loss is 5.643298244476318 and perplexity is 282.3925839506594
At time: 583.5787796974182 and batch: 300, loss is 5.643987512588501 and perplexity is 282.58729525032965
At time: 586.0234622955322 and batch: 350, loss is 5.60004508972168 and perplexity is 270.4386011525021
At time: 588.4648571014404 and batch: 400, loss is 5.614442014694214 and perplexity is 274.360247490142
At time: 590.9089057445526 and batch: 450, loss is 5.602017040252686 and perplexity is 270.97241885371585
At time: 593.3495380878448 and batch: 500, loss is 5.602948770523072 and perplexity is 271.22500971380344
At time: 595.7985467910767 and batch: 550, loss is 5.597438650131226 and perplexity is 269.7346370941005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.401978047688802 and perplexity of 221.8448020851967
Finished 21 epochs...
Completing Train Step...
At time: 599.8925046920776 and batch: 50, loss is 5.666660842895507 and perplexity is 289.06767865564
At time: 602.387330532074 and batch: 100, loss is 5.643425788879394 and perplexity is 282.4286038412315
At time: 604.8345062732697 and batch: 150, loss is 5.673720760345459 and perplexity is 291.1156935057903
At time: 607.2828323841095 and batch: 200, loss is 5.67577672958374 and perplexity is 291.7148341127
At time: 609.7290160655975 and batch: 250, loss is 5.63872784614563 and perplexity is 281.10488225273474
At time: 612.2192223072052 and batch: 300, loss is 5.641793222427368 and perplexity is 281.96789654746954
At time: 614.6597082614899 and batch: 350, loss is 5.6015181064605715 and perplexity is 270.8372552788602
At time: 617.100234746933 and batch: 400, loss is 5.616992073059082 and perplexity is 275.0607749475771
At time: 619.5419337749481 and batch: 450, loss is 5.604356889724731 and perplexity is 271.6071958766392
At time: 621.9913032054901 and batch: 500, loss is 5.603029088973999 and perplexity is 271.2467949613034
At time: 624.4425632953644 and batch: 550, loss is 5.5935307884216305 and perplexity is 268.68260836427174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.401858011881511 and perplexity of 221.81817436345705
Finished 22 epochs...
Completing Train Step...
At time: 628.4271986484528 and batch: 50, loss is 5.65890266418457 and perplexity is 286.83371688743443
At time: 630.881674528122 and batch: 100, loss is 5.636759529113769 and perplexity is 280.5521229065369
At time: 633.3287267684937 and batch: 150, loss is 5.668349161148071 and perplexity is 289.55612910762324
At time: 635.7767007350922 and batch: 200, loss is 5.6719680023193355 and perplexity is 290.6058850534137
At time: 638.2223768234253 and batch: 250, loss is 5.63786283493042 and perplexity is 280.86182851419596
At time: 640.6675868034363 and batch: 300, loss is 5.64233811378479 and perplexity is 282.1215802840362
At time: 643.1122515201569 and batch: 350, loss is 5.601969118118286 and perplexity is 270.9594335881843
At time: 645.5555636882782 and batch: 400, loss is 5.617723598480224 and perplexity is 275.262062511319
At time: 647.9986038208008 and batch: 450, loss is 5.604399070739746 and perplexity is 271.61865278547697
At time: 650.4403381347656 and batch: 500, loss is 5.602286605834961 and perplexity is 271.0454735376566
At time: 652.8835859298706 and batch: 550, loss is 5.590761709213257 and perplexity is 267.9396340918517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.40132802327474 and perplexity of 221.70064440579003
Finished 23 epochs...
Completing Train Step...
At time: 656.8140771389008 and batch: 50, loss is 5.654849748611451 and perplexity is 285.67355665318365
At time: 659.2817571163177 and batch: 100, loss is 5.632943086624145 and perplexity is 279.48345242096565
At time: 661.7239241600037 and batch: 150, loss is 5.665491256713867 and perplexity is 288.72978672850775
At time: 664.1687936782837 and batch: 200, loss is 5.669604396820068 and perplexity is 289.9198185001211
At time: 666.6104004383087 and batch: 250, loss is 5.6359828662872316 and perplexity is 280.3343130951097
At time: 669.0512590408325 and batch: 300, loss is 5.640976467132568 and perplexity is 281.73769179827207
At time: 671.4912621974945 and batch: 350, loss is 5.60158145904541 and perplexity is 270.85441406257365
At time: 673.9729318618774 and batch: 400, loss is 5.617699527740479 and perplexity is 275.25543682959335
At time: 676.416610956192 and batch: 450, loss is 5.604042682647705 and perplexity is 271.52186837947727
At time: 678.8589794635773 and batch: 500, loss is 5.6011816596984865 and perplexity is 270.746148288455
At time: 681.3001141548157 and batch: 550, loss is 5.5883394145965575 and perplexity is 267.29139079368093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.401269022623698 and perplexity of 221.68756430930452
Finished 24 epochs...
Completing Train Step...
At time: 685.251134634018 and batch: 50, loss is 5.651340827941895 and perplexity is 284.67290743157577
At time: 687.6960542201996 and batch: 100, loss is 5.6296151256561275 and perplexity is 278.5548883695737
At time: 690.1431868076324 and batch: 150, loss is 5.662714309692383 and perplexity is 287.92911163792166
At time: 692.5972077846527 and batch: 200, loss is 5.667676200866699 and perplexity is 289.36133488524496
At time: 695.0475862026215 and batch: 250, loss is 5.63404896736145 and perplexity is 279.7926987506565
At time: 697.4917252063751 and batch: 300, loss is 5.639827632904053 and perplexity is 281.4142077448262
At time: 699.9417245388031 and batch: 350, loss is 5.600595731735229 and perplexity is 270.5875570152966
At time: 702.4014415740967 and batch: 400, loss is 5.617029647827149 and perplexity is 275.0711104865766
At time: 704.8422017097473 and batch: 450, loss is 5.603528251647949 and perplexity is 271.3822250347836
At time: 707.2885265350342 and batch: 500, loss is 5.600103425979614 and perplexity is 270.4543779886707
At time: 709.7399089336395 and batch: 550, loss is 5.586452560424805 and perplexity is 266.7875264266687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.40016835530599 and perplexity of 221.44369428702873
Finished 25 epochs...
Completing Train Step...
At time: 713.6731405258179 and batch: 50, loss is 5.648054113388062 and perplexity is 283.73880474896555
At time: 716.1564753055573 and batch: 100, loss is 5.626709270477295 and perplexity is 277.74662312423914
At time: 718.6130321025848 and batch: 150, loss is 5.659574565887451 and perplexity is 287.02650571057114
At time: 721.066520690918 and batch: 200, loss is 5.665846462249756 and perplexity is 288.83236336394555
At time: 723.513290643692 and batch: 250, loss is 5.633233041763305 and perplexity is 279.56450183408447
At time: 725.9555716514587 and batch: 300, loss is 5.6394831085205075 and perplexity is 281.3172703879823
At time: 728.3976848125458 and batch: 350, loss is 5.600188913345337 and perplexity is 270.47749940927383
At time: 730.8417899608612 and batch: 400, loss is 5.616917495727539 and perplexity is 275.04026241386384
At time: 733.2890341281891 and batch: 450, loss is 5.602990407943725 and perplexity is 271.2363030587359
At time: 735.779242515564 and batch: 500, loss is 5.598839492797851 and perplexity is 270.11275766428855
At time: 738.2448139190674 and batch: 550, loss is 5.584317455291748 and perplexity is 266.21851467567626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.401309712727865 and perplexity of 221.6965849829136
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 742.5032405853271 and batch: 50, loss is 5.646402769088745 and perplexity is 283.27064094744856
At time: 744.9608836174011 and batch: 100, loss is 5.621590442657471 and perplexity is 276.3285185958613
At time: 747.4128770828247 and batch: 150, loss is 5.65246280670166 and perplexity is 284.99248363252485
At time: 749.8657886981964 and batch: 200, loss is 5.656211280822754 and perplexity is 286.06277530922654
At time: 752.3125793933868 and batch: 250, loss is 5.617856340408325 and perplexity is 275.2986037534536
At time: 754.7625522613525 and batch: 300, loss is 5.620939922332764 and perplexity is 276.14881973343665
At time: 757.217113494873 and batch: 350, loss is 5.578058404922485 and perplexity is 264.557443370375
At time: 759.6717672348022 and batch: 400, loss is 5.588905115127563 and perplexity is 267.4426404523587
At time: 762.1270542144775 and batch: 450, loss is 5.575815753936768 and perplexity is 263.9647981558004
At time: 764.5849177837372 and batch: 500, loss is 5.57789023399353 and perplexity is 264.5129562401873
At time: 767.039183139801 and batch: 550, loss is 5.57073145866394 and perplexity is 262.62612915829334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.379933166503906 and perplexity of 217.00777155222823
Finished 27 epochs...
Completing Train Step...
At time: 771.0754132270813 and batch: 50, loss is 5.639100532531739 and perplexity is 281.2096657398957
At time: 773.562171459198 and batch: 100, loss is 5.617626724243164 and perplexity is 275.23539800059444
At time: 776.0183246135712 and batch: 150, loss is 5.647921323776245 and perplexity is 283.70112968470954
At time: 778.4807257652283 and batch: 200, loss is 5.652510299682617 and perplexity is 285.00601909654006
At time: 780.9389681816101 and batch: 250, loss is 5.614974775314331 and perplexity is 274.5064547690123
At time: 783.3989696502686 and batch: 300, loss is 5.619234037399292 and perplexity is 275.67814319660454
At time: 785.8575301170349 and batch: 350, loss is 5.576935806274414 and perplexity is 264.26061818106064
At time: 788.3165118694305 and batch: 400, loss is 5.589714879989624 and perplexity is 267.65929381251436
At time: 790.7759261131287 and batch: 450, loss is 5.577185764312744 and perplexity is 264.32668050284923
At time: 793.2337880134583 and batch: 500, loss is 5.578762092590332 and perplexity is 264.7436746973711
At time: 795.691150188446 and batch: 550, loss is 5.569972677230835 and perplexity is 262.4269289119214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.378781127929687 and perplexity of 216.757914178795
Finished 28 epochs...
Completing Train Step...
At time: 799.7347354888916 and batch: 50, loss is 5.636942777633667 and perplexity is 280.6035383785744
At time: 802.1923611164093 and batch: 100, loss is 5.615730619430542 and perplexity is 274.7140172902848
At time: 804.6530253887177 and batch: 150, loss is 5.646219758987427 and perplexity is 283.2188043022079
At time: 807.1128556728363 and batch: 200, loss is 5.651242179870605 and perplexity is 284.64482638340263
At time: 809.573609828949 and batch: 250, loss is 5.6137918186187745 and perplexity is 274.1819175149747
At time: 812.0298433303833 and batch: 300, loss is 5.618821086883545 and perplexity is 275.5643252673951
At time: 814.4842863082886 and batch: 350, loss is 5.577125558853149 and perplexity is 264.3107670726089
At time: 816.9383838176727 and batch: 400, loss is 5.590466957092286 and perplexity is 267.8606699544061
At time: 819.3918018341064 and batch: 450, loss is 5.578027029037475 and perplexity is 264.54914277667314
At time: 821.8520925045013 and batch: 500, loss is 5.579032697677612 and perplexity is 264.8153253766514
At time: 824.3082804679871 and batch: 550, loss is 5.5692738342285155 and perplexity is 262.24359775634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.378340657552084 and perplexity of 216.66245976245276
Finished 29 epochs...
Completing Train Step...
At time: 828.3358192443848 and batch: 50, loss is 5.6354687213897705 and perplexity is 280.1902176845829
At time: 830.8317244052887 and batch: 100, loss is 5.614514760971069 and perplexity is 274.38020690264153
At time: 833.2854511737823 and batch: 150, loss is 5.645178241729736 and perplexity is 282.92398058846857
At time: 835.7453441619873 and batch: 200, loss is 5.650538492202759 and perplexity is 284.4445957875232
At time: 838.205174446106 and batch: 250, loss is 5.613150224685669 and perplexity is 274.0060604805656
At time: 840.6663870811462 and batch: 300, loss is 5.618712730407715 and perplexity is 275.5344677059038
At time: 843.1262204647064 and batch: 350, loss is 5.577380418777466 and perplexity is 264.3781378793958
At time: 845.5872712135315 and batch: 400, loss is 5.5909738349914555 and perplexity is 267.99647702395623
At time: 848.043121099472 and batch: 450, loss is 5.57853367805481 and perplexity is 264.6832102996192
At time: 850.5015723705292 and batch: 500, loss is 5.579084424972534 and perplexity is 264.8290239113781
At time: 852.9611165523529 and batch: 550, loss is 5.568656234741211 and perplexity is 262.0816862481956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.378099568684896 and perplexity of 216.6102311515879
Finished 30 epochs...
Completing Train Step...
At time: 856.988224029541 and batch: 50, loss is 5.634300909042358 and perplexity is 279.86319907409757
At time: 859.4433786869049 and batch: 100, loss is 5.613623962402344 and perplexity is 274.1358982381059
At time: 861.9279246330261 and batch: 150, loss is 5.644455795288086 and perplexity is 282.71965698075337
At time: 864.3891625404358 and batch: 200, loss is 5.650060262680054 and perplexity is 284.3085985057932
At time: 866.8442125320435 and batch: 250, loss is 5.612719430923462 and perplexity is 273.88804580072264
At time: 869.3049612045288 and batch: 300, loss is 5.618677625656128 and perplexity is 275.52479530663584
At time: 871.7612535953522 and batch: 350, loss is 5.577589464187622 and perplexity is 264.4334106927279
At time: 874.2156839370728 and batch: 400, loss is 5.59130368232727 and perplexity is 268.0848895284232
At time: 876.6735591888428 and batch: 450, loss is 5.578844356536865 and perplexity is 264.7654544527034
At time: 879.1298005580902 and batch: 500, loss is 5.579026975631714 and perplexity is 264.81381009554025
At time: 881.5875945091248 and batch: 550, loss is 5.568091878890991 and perplexity is 261.9338206436599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.37793935139974 and perplexity of 216.57552922841393
Finished 31 epochs...
Completing Train Step...
At time: 885.6149725914001 and batch: 50, loss is 5.633348646163941 and perplexity is 279.59682258893264
At time: 888.0927572250366 and batch: 100, loss is 5.612911148071289 and perplexity is 273.94055986945045
At time: 890.5517041683197 and batch: 150, loss is 5.643906221389771 and perplexity is 282.56432432403267
At time: 893.0075283050537 and batch: 200, loss is 5.6496940612792965 and perplexity is 284.20450335983054
At time: 895.4619035720825 and batch: 250, loss is 5.612396078109741 and perplexity is 273.7994976473874
At time: 897.9266226291656 and batch: 300, loss is 5.61865683555603 and perplexity is 275.51906717810647
At time: 900.3854908943176 and batch: 350, loss is 5.5777465438842775 and perplexity is 264.47495107515493
At time: 902.8378329277039 and batch: 400, loss is 5.591514186859131 and perplexity is 268.14132855271913
At time: 905.288158416748 and batch: 450, loss is 5.579037294387818 and perplexity is 264.8165426587579
At time: 907.7422959804535 and batch: 500, loss is 5.578923110961914 and perplexity is 264.78630672493523
At time: 910.1857628822327 and batch: 550, loss is 5.5675865745544435 and perplexity is 261.80149778267344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377825927734375 and perplexity of 216.5509658311221
Finished 32 epochs...
Completing Train Step...
At time: 914.1702091693878 and batch: 50, loss is 5.632535381317139 and perplexity is 279.3695287594119
At time: 916.6260795593262 and batch: 100, loss is 5.612315082550049 and perplexity is 273.77732200190854
At time: 919.0800032615662 and batch: 150, loss is 5.643464670181275 and perplexity is 282.4395852465214
At time: 921.5369398593903 and batch: 200, loss is 5.649392194747925 and perplexity is 284.11872447973906
At time: 924.015462398529 and batch: 250, loss is 5.612125387191773 and perplexity is 273.7253926402586
At time: 926.471718788147 and batch: 300, loss is 5.618633260726929 and perplexity is 275.51257193974595
At time: 928.9241321086884 and batch: 350, loss is 5.577855911254883 and perplexity is 264.5038775869244
At time: 931.3796889781952 and batch: 400, loss is 5.591639223098755 and perplexity is 268.17485803228556
At time: 933.8342227935791 and batch: 450, loss is 5.579146194458008 and perplexity is 264.84538276915737
At time: 936.2884833812714 and batch: 500, loss is 5.578782577514648 and perplexity is 264.74909800705825
At time: 938.7454526424408 and batch: 550, loss is 5.56712176322937 and perplexity is 261.6798377582517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377738952636719 and perplexity of 216.53213210876567
Finished 33 epochs...
Completing Train Step...
At time: 942.7071731090546 and batch: 50, loss is 5.631833724975586 and perplexity is 279.17357611166943
At time: 945.1928434371948 and batch: 100, loss is 5.611800279617309 and perplexity is 273.6364169059146
At time: 947.6454193592072 and batch: 150, loss is 5.64309326171875 and perplexity is 282.3347042724552
At time: 950.0987467765808 and batch: 200, loss is 5.649128341674805 and perplexity is 284.0437687702361
At time: 952.5567560195923 and batch: 250, loss is 5.611885957717895 and perplexity is 273.65986255874134
At time: 955.0195486545563 and batch: 300, loss is 5.618604202270507 and perplexity is 275.50456608600007
At time: 957.4788637161255 and batch: 350, loss is 5.577927989959717 and perplexity is 264.5229433709547
At time: 959.9395377635956 and batch: 400, loss is 5.591696195602417 and perplexity is 268.1901370606048
At time: 962.3948171138763 and batch: 450, loss is 5.579199848175048 and perplexity is 264.8595930895989
At time: 964.8499052524567 and batch: 500, loss is 5.578613367080688 and perplexity is 264.704303487251
At time: 967.3073194026947 and batch: 550, loss is 5.566702508926392 and perplexity is 261.57015035533357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.37767333984375 and perplexity of 216.51792529688996
Finished 34 epochs...
Completing Train Step...
At time: 971.3245823383331 and batch: 50, loss is 5.631206855773926 and perplexity is 278.9986256361495
At time: 973.7819938659668 and batch: 100, loss is 5.611346416473388 and perplexity is 273.5122516006057
At time: 976.2437744140625 and batch: 150, loss is 5.642779226303101 and perplexity is 282.2460550964657
At time: 978.7075984477997 and batch: 200, loss is 5.648890295028687 and perplexity is 283.9761611509323
At time: 981.1647036075592 and batch: 250, loss is 5.611651010513306 and perplexity is 273.59557449147144
At time: 983.6216084957123 and batch: 300, loss is 5.61857385635376 and perplexity is 275.4962057742254
At time: 986.0994579792023 and batch: 350, loss is 5.577971029281616 and perplexity is 264.5343285040666
At time: 988.5507137775421 and batch: 400, loss is 5.591692991256714 and perplexity is 268.1892776880685
At time: 991.0114674568176 and batch: 450, loss is 5.5792074871063235 and perplexity is 264.86161634155576
At time: 993.4689950942993 and batch: 500, loss is 5.578407487869263 and perplexity is 264.64981198351444
At time: 995.9281959533691 and batch: 550, loss is 5.566327333450317 and perplexity is 261.47203405621264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377617899576823 and perplexity of 216.50592181805803
Finished 35 epochs...
Completing Train Step...
At time: 999.9143531322479 and batch: 50, loss is 5.63063003540039 and perplexity is 278.83773995022557
At time: 1002.4003088474274 and batch: 100, loss is 5.610915594100952 and perplexity is 273.39444178285544
At time: 1004.8615617752075 and batch: 150, loss is 5.642535572052002 and perplexity is 282.1772930227115
At time: 1007.3224213123322 and batch: 200, loss is 5.648665180206299 and perplexity is 283.9122411027976
At time: 1009.7788934707642 and batch: 250, loss is 5.611401557922363 and perplexity is 273.5273338782969
At time: 1012.2332162857056 and batch: 300, loss is 5.618541250228882 and perplexity is 275.487223056983
At time: 1014.6861662864685 and batch: 350, loss is 5.577997159957886 and perplexity is 264.54124105528143
At time: 1017.1453721523285 and batch: 400, loss is 5.59162260055542 and perplexity is 268.17040032113573
At time: 1019.6030900478363 and batch: 450, loss is 5.579190340042114 and perplexity is 264.8570747813511
At time: 1022.0596151351929 and batch: 500, loss is 5.578160009384155 and perplexity is 264.58432495261064
At time: 1024.5200824737549 and batch: 550, loss is 5.5660094165802 and perplexity is 261.3889208977598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377570088704427 and perplexity of 216.49557072850644
Finished 36 epochs...
Completing Train Step...
At time: 1028.5394327640533 and batch: 50, loss is 5.630081453323364 and perplexity is 278.684816513054
At time: 1031.0031554698944 and batch: 100, loss is 5.610534563064575 and perplexity is 273.29028985918103
At time: 1033.462432384491 and batch: 150, loss is 5.642313289642334 and perplexity is 282.11457694466355
At time: 1035.92005610466 and batch: 200, loss is 5.648457250595093 and perplexity is 283.8532134778966
At time: 1038.3766026496887 and batch: 250, loss is 5.611168451309204 and perplexity is 273.46358027887163
At time: 1040.8372149467468 and batch: 300, loss is 5.618458795547485 and perplexity is 275.4645087822391
At time: 1043.295273065567 and batch: 350, loss is 5.577977132797241 and perplexity is 264.53594309840145
At time: 1045.7583146095276 and batch: 400, loss is 5.591552476882935 and perplexity is 268.15159588713925
At time: 1048.2435705661774 and batch: 450, loss is 5.579130792617798 and perplexity is 264.8413036943045
At time: 1050.7031424045563 and batch: 500, loss is 5.577928085327148 and perplexity is 264.5229685978295
At time: 1053.1610329151154 and batch: 550, loss is 5.565711898803711 and perplexity is 261.31116461472726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377438354492187 and perplexity of 216.46705273348212
Finished 37 epochs...
Completing Train Step...
At time: 1057.124748468399 and batch: 50, loss is 5.62957013130188 and perplexity is 278.5423552542108
At time: 1059.600924730301 and batch: 100, loss is 5.61023118019104 and perplexity is 273.2073908414412
At time: 1062.0443947315216 and batch: 150, loss is 5.64209137916565 and perplexity is 282.0519797101622
At time: 1064.5023953914642 and batch: 200, loss is 5.64830249786377 and perplexity is 283.80928981655705
At time: 1066.9613287448883 and batch: 250, loss is 5.6109747791290285 and perplexity is 273.4106231194101
At time: 1069.4202935695648 and batch: 300, loss is 5.618362150192261 and perplexity is 275.4378877033586
At time: 1071.8816332817078 and batch: 350, loss is 5.577926445007324 and perplexity is 264.5225346959161
At time: 1074.340056180954 and batch: 400, loss is 5.591474924087525 and perplexity is 268.13080078765387
At time: 1076.798846244812 and batch: 450, loss is 5.579030351638794 and perplexity is 264.8147041103472
At time: 1079.2631673812866 and batch: 500, loss is 5.577716503143311 and perplexity is 264.46700617099344
At time: 1081.71901845932 and batch: 550, loss is 5.565415067672729 and perplexity is 261.2336108369538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377329508463542 and perplexity of 216.44349243670513
Finished 38 epochs...
Completing Train Step...
At time: 1085.7588963508606 and batch: 50, loss is 5.629113626480103 and perplexity is 278.4152283451993
At time: 1088.2147986888885 and batch: 100, loss is 5.609962520599365 and perplexity is 273.1340009142706
At time: 1090.6800167560577 and batch: 150, loss is 5.641890115737915 and perplexity is 281.9952186740784
At time: 1093.1418223381042 and batch: 200, loss is 5.648159494400025 and perplexity is 283.76870700688056
At time: 1095.5968825817108 and batch: 250, loss is 5.610800380706787 and perplexity is 273.3629448957384
At time: 1098.0556817054749 and batch: 300, loss is 5.618270206451416 and perplexity is 275.412564077785
At time: 1100.5158414840698 and batch: 350, loss is 5.577855348587036 and perplexity is 264.50372875913905
At time: 1102.9760115146637 and batch: 400, loss is 5.591396265029907 and perplexity is 268.1097107010197
At time: 1105.434681892395 and batch: 450, loss is 5.578910875320434 and perplexity is 264.78306691443805
At time: 1107.8945693969727 and batch: 500, loss is 5.577516269683838 and perplexity is 264.4140563287734
At time: 1110.3715217113495 and batch: 550, loss is 5.5651225566864015 and perplexity is 261.1572083106198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3772430419921875 and perplexity of 216.42477814075772
Finished 39 epochs...
Completing Train Step...
At time: 1114.3079640865326 and batch: 50, loss is 5.628695964813232 and perplexity is 278.2989692570189
At time: 1116.7894384860992 and batch: 100, loss is 5.609706783294678 and perplexity is 273.06415929199324
At time: 1119.2480895519257 and batch: 150, loss is 5.641697015762329 and perplexity is 281.9407706613611
At time: 1121.7106177806854 and batch: 200, loss is 5.64801552772522 and perplexity is 283.7278567103304
At time: 1124.1680264472961 and batch: 250, loss is 5.6106323146820065 and perplexity is 273.3170057327818
At time: 1126.626606464386 and batch: 300, loss is 5.618176164627076 and perplexity is 275.3866649956305
At time: 1129.080852508545 and batch: 350, loss is 5.577765789031982 and perplexity is 264.48004098363043
At time: 1131.5315685272217 and batch: 400, loss is 5.591307687759399 and perplexity is 268.08596332640354
At time: 1133.9870245456696 and batch: 450, loss is 5.578781404495239 and perplexity is 264.74878745141
At time: 1136.4334003925323 and batch: 500, loss is 5.577322082519531 and perplexity is 264.3627154979986
At time: 1138.885132074356 and batch: 550, loss is 5.564835996627807 and perplexity is 261.08238180735964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377170817057292 and perplexity of 216.40914743971663
Finished 40 epochs...
Completing Train Step...
At time: 1142.907122850418 and batch: 50, loss is 5.6283041286468505 and perplexity is 278.189943017404
At time: 1145.3639290332794 and batch: 100, loss is 5.609459562301636 and perplexity is 272.99666044327876
At time: 1147.821623802185 and batch: 150, loss is 5.641509428024292 and perplexity is 281.88788699025145
At time: 1150.2797656059265 and batch: 200, loss is 5.6478705310821535 and perplexity is 283.6867201059698
At time: 1152.7400124073029 and batch: 250, loss is 5.61046555519104 and perplexity is 273.27143132812125
At time: 1155.1976652145386 and batch: 300, loss is 5.618078517913818 and perplexity is 275.3597757057655
At time: 1157.6543900966644 and batch: 350, loss is 5.577660284042358 and perplexity is 264.4521384916027
At time: 1160.113620519638 and batch: 400, loss is 5.591209487915039 and perplexity is 268.059638619092
At time: 1162.5753259658813 and batch: 450, loss is 5.578646278381347 and perplexity is 264.7130153935281
At time: 1165.033400774002 and batch: 500, loss is 5.577131500244141 and perplexity is 264.3123374508845
At time: 1167.490628004074 and batch: 550, loss is 5.564559154510498 and perplexity is 261.01011321194494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3771006266276045 and perplexity of 216.39395812174823
Finished 41 epochs...
Completing Train Step...
At time: 1171.552551984787 and batch: 50, loss is 5.6279317569732665 and perplexity is 278.08637224735565
At time: 1174.042908191681 and batch: 100, loss is 5.6092218494415285 and perplexity is 272.93177333888025
At time: 1176.5060532093048 and batch: 150, loss is 5.641326055526734 and perplexity is 281.8362012433998
At time: 1178.9675238132477 and batch: 200, loss is 5.647724275588989 and perplexity is 283.645232398793
At time: 1181.4282371997833 and batch: 250, loss is 5.610298566818237 and perplexity is 273.225801986361
At time: 1183.8901116847992 and batch: 300, loss is 5.617976655960083 and perplexity is 275.3317284495305
At time: 1186.3510010242462 and batch: 350, loss is 5.577542400360107 and perplexity is 264.4209657371537
At time: 1188.8130149841309 and batch: 400, loss is 5.591102094650268 and perplexity is 268.0308523651
At time: 1191.2746803760529 and batch: 450, loss is 5.5785073471069335 and perplexity is 264.6762410315592
At time: 1193.7361381053925 and batch: 500, loss is 5.5769437885284425 and perplexity is 264.26272758486357
At time: 1196.2025809288025 and batch: 550, loss is 5.564293336868286 and perplexity is 260.9407413395998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.377031453450521 and perplexity of 216.3789899818662
Finished 42 epochs...
Completing Train Step...
At time: 1200.2580044269562 and batch: 50, loss is 5.627574996948242 and perplexity is 277.9871798412285
At time: 1202.717039823532 and batch: 100, loss is 5.608993015289307 and perplexity is 272.8693243734144
At time: 1205.177149772644 and batch: 150, loss is 5.6411462497711184 and perplexity is 281.78553002790005
At time: 1207.6393678188324 and batch: 200, loss is 5.647576990127564 and perplexity is 283.60345865626607
At time: 1210.101187467575 and batch: 250, loss is 5.6101314735412595 and perplexity is 273.1801516057943
At time: 1212.5619657039642 and batch: 300, loss is 5.617871723175049 and perplexity is 275.3028386402265
At time: 1215.0245387554169 and batch: 350, loss is 5.577415685653687 and perplexity is 264.3874618348724
At time: 1217.4825575351715 and batch: 400, loss is 5.59098593711853 and perplexity is 267.9997203710024
At time: 1219.9417338371277 and batch: 450, loss is 5.578366022109986 and perplexity is 264.6388383056356
At time: 1222.401657819748 and batch: 500, loss is 5.576758813858032 and perplexity is 264.2138501946058
At time: 1224.8556656837463 and batch: 550, loss is 5.564039402008056 and perplexity is 260.874487801317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.376960245768229 and perplexity of 216.3635826840584
Finished 43 epochs...
Completing Train Step...
At time: 1228.8262062072754 and batch: 50, loss is 5.627231044769287 and perplexity is 277.8915819864882
At time: 1231.2997765541077 and batch: 100, loss is 5.608773584365845 and perplexity is 272.80945497442167
At time: 1233.759225845337 and batch: 150, loss is 5.6409696674346925 and perplexity is 281.7357760736008
At time: 1236.25092959404 and batch: 200, loss is 5.64742901802063 and perplexity is 283.5614963596563
At time: 1238.7133462429047 and batch: 250, loss is 5.6099650669097905 and perplexity is 273.13469639911017
At time: 1241.1728022098541 and batch: 300, loss is 5.617763528823852 and perplexity is 275.27305403950913
At time: 1243.630606174469 and batch: 350, loss is 5.577282199859619 and perplexity is 264.35217221997124
At time: 1246.092095375061 and batch: 400, loss is 5.590861730575561 and perplexity is 267.96643511938424
At time: 1248.553318977356 and batch: 450, loss is 5.578222579956055 and perplexity is 264.6008806630834
At time: 1251.0138099193573 and batch: 500, loss is 5.576576251983642 and perplexity is 264.1656192215768
At time: 1253.4753367900848 and batch: 550, loss is 5.563796510696411 and perplexity is 260.81113134947753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.376887512207031 and perplexity of 216.34784636246255
Finished 44 epochs...
Completing Train Step...
At time: 1257.5035514831543 and batch: 50, loss is 5.626899108886719 and perplexity is 277.79935510651546
At time: 1259.9927139282227 and batch: 100, loss is 5.608566055297851 and perplexity is 272.7528449568063
At time: 1262.4503514766693 and batch: 150, loss is 5.640794582366944 and perplexity is 281.6864526641819
At time: 1264.9100551605225 and batch: 200, loss is 5.647280569076538 and perplexity is 283.51940507921694
At time: 1267.3680889606476 and batch: 250, loss is 5.609799394607544 and perplexity is 273.08944929333387
At time: 1269.8295333385468 and batch: 300, loss is 5.617653064727783 and perplexity is 275.242647929845
At time: 1272.291613817215 and batch: 350, loss is 5.577144565582276 and perplexity is 264.3157908035061
At time: 1274.7527258396149 and batch: 400, loss is 5.590731163024902 and perplexity is 267.9314496823229
At time: 1277.2128386497498 and batch: 450, loss is 5.578075313568116 and perplexity is 264.5619167162528
At time: 1279.670788526535 and batch: 500, loss is 5.576395139694214 and perplexity is 264.11777991376545
At time: 1282.1348912715912 and batch: 550, loss is 5.563561782836914 and perplexity is 260.7499188952981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.376818339029948 and perplexity of 216.3328814121671
Finished 45 epochs...
Completing Train Step...
At time: 1286.1821928024292 and batch: 50, loss is 5.626579904556275 and perplexity is 277.71069450054836
At time: 1288.6477982997894 and batch: 100, loss is 5.608369760513305 and perplexity is 272.699310250335
At time: 1291.1123487949371 and batch: 150, loss is 5.640620937347412 and perplexity is 281.63754346114996
At time: 1293.5734386444092 and batch: 200, loss is 5.647131280899048 and perplexity is 283.4770821431851
At time: 1296.0840952396393 and batch: 250, loss is 5.609634590148926 and perplexity is 273.0444466429094
At time: 1298.5445330142975 and batch: 300, loss is 5.617539930343628 and perplexity is 275.2115102837807
At time: 1301.0074446201324 and batch: 350, loss is 5.577003231048584 and perplexity is 264.27843649425466
At time: 1303.466794013977 and batch: 400, loss is 5.590595922470093 and perplexity is 267.89521693454077
At time: 1305.925724029541 and batch: 450, loss is 5.577925300598144 and perplexity is 264.5222319740723
At time: 1308.3839161396027 and batch: 500, loss is 5.576214923858642 and perplexity is 264.07018599608654
At time: 1310.8448016643524 and batch: 550, loss is 5.563333797454834 and perplexity is 260.6904785014392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.376753743489584 and perplexity of 216.31890772411745
Finished 46 epochs...
Completing Train Step...
At time: 1314.8543264865875 and batch: 50, loss is 5.626272840499878 and perplexity is 277.6254326192897
At time: 1317.3347713947296 and batch: 100, loss is 5.6081822490692135 and perplexity is 272.64818080269515
At time: 1319.7841882705688 and batch: 150, loss is 5.640449438095093 and perplexity is 281.58924697454535
At time: 1322.2266154289246 and batch: 200, loss is 5.646980657577514 and perplexity is 283.4343870990095
At time: 1324.6681816577911 and batch: 250, loss is 5.609470462799072 and perplexity is 272.99963625889495
At time: 1327.1123507022858 and batch: 300, loss is 5.617425231933594 and perplexity is 275.17994577136244
At time: 1329.5664916038513 and batch: 350, loss is 5.576858940124512 and perplexity is 264.24030626542975
At time: 1332.0253944396973 and batch: 400, loss is 5.590456523895264 and perplexity is 267.85787532584055
At time: 1334.4854619503021 and batch: 450, loss is 5.577773208618164 and perplexity is 264.4820033233678
At time: 1336.944776058197 and batch: 500, loss is 5.5760354328155515 and perplexity is 264.0227920164776
At time: 1339.4054481983185 and batch: 550, loss is 5.56311074256897 and perplexity is 260.63233670115864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.376692199707032 and perplexity of 216.30559504995898
Finished 47 epochs...
Completing Train Step...
At time: 1343.4527900218964 and batch: 50, loss is 5.625976343154907 and perplexity is 277.5431296175353
At time: 1345.9124693870544 and batch: 100, loss is 5.607999992370606 and perplexity is 272.59849337345156
At time: 1348.3720774650574 and batch: 150, loss is 5.640282382965088 and perplexity is 281.54220997527943
At time: 1350.830607175827 and batch: 200, loss is 5.646828269958496 and perplexity is 283.39119849839904
At time: 1353.291608095169 and batch: 250, loss is 5.6093064975738525 and perplexity is 272.9548774815927
At time: 1355.7485361099243 and batch: 300, loss is 5.617307929992676 and perplexity is 275.14766852275073
At time: 1358.2553269863129 and batch: 350, loss is 5.576710777282715 and perplexity is 264.2011585709245
At time: 1360.7165639400482 and batch: 400, loss is 5.590313539505005 and perplexity is 267.8195785688456
At time: 1363.1774983406067 and batch: 450, loss is 5.577620067596436 and perplexity is 264.44150338033046
At time: 1365.6267309188843 and batch: 500, loss is 5.575858716964722 and perplexity is 263.9761391264219
At time: 1368.0777049064636 and batch: 550, loss is 5.562890787124633 and perplexity is 260.57501550401685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.376633707682291 and perplexity of 216.29294326775965
Finished 48 epochs...
Completing Train Step...
At time: 1372.0525224208832 and batch: 50, loss is 5.625688800811767 and perplexity is 277.4633356883393
At time: 1374.5522711277008 and batch: 100, loss is 5.607820358276367 and perplexity is 272.5495297878999
At time: 1377.015657901764 and batch: 150, loss is 5.640119371414184 and perplexity is 281.4963190834608
At time: 1379.4771196842194 and batch: 200, loss is 5.64667293548584 and perplexity is 283.34718149479244
At time: 1381.9360694885254 and batch: 250, loss is 5.609142398834228 and perplexity is 272.9100896051412
At time: 1384.3915884494781 and batch: 300, loss is 5.617188291549683 and perplexity is 275.1147522531603
At time: 1386.8481938838959 and batch: 350, loss is 5.576557750701904 and perplexity is 264.1607318642415
At time: 1389.306220293045 and batch: 400, loss is 5.5901678276062015 and perplexity is 267.7805569125444
At time: 1391.7691757678986 and batch: 450, loss is 5.5774666118621825 and perplexity is 264.40092642872486
At time: 1394.232115983963 and batch: 500, loss is 5.575687885284424 and perplexity is 263.93104749066606
At time: 1396.6885960102081 and batch: 550, loss is 5.562672557830811 and perplexity is 260.5181566067598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.37657725016276 and perplexity of 216.28073224939575
Finished 49 epochs...
Completing Train Step...
At time: 1400.7266988754272 and batch: 50, loss is 5.625408439636231 and perplexity is 277.38555664499887
At time: 1403.1766338348389 and batch: 100, loss is 5.607642183303833 and perplexity is 272.5009726088812
At time: 1405.6291003227234 and batch: 150, loss is 5.639959030151367 and perplexity is 281.4511872265267
At time: 1408.0882778167725 and batch: 200, loss is 5.646515426635742 and perplexity is 283.3025553206577
At time: 1410.5494756698608 and batch: 250, loss is 5.608978385925293 and perplexity is 272.86533249793933
At time: 1413.0093994140625 and batch: 300, loss is 5.617066249847412 and perplexity is 275.08117882919623
At time: 1415.4670760631561 and batch: 350, loss is 5.576399488449097 and perplexity is 264.1189284997479
At time: 1417.9245002269745 and batch: 400, loss is 5.590019979476929 and perplexity is 267.7409689847219
At time: 1420.4228467941284 and batch: 450, loss is 5.577313156127929 and perplexity is 264.3603557034074
At time: 1422.8808336257935 and batch: 500, loss is 5.575524559020996 and perplexity is 263.88794413892737
At time: 1425.3407626152039 and batch: 550, loss is 5.562457571029663 and perplexity is 260.46215466168513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3765202840169275 and perplexity of 216.26841192058583
Finished Training.
Improved accuracyfrom -10000000 to -216.26841192058583
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe582f07898>
SETTINGS FOR THIS RUN
{'lr': 19.036308676398342, 'tune_wordvecs': True, 'anneal': 2.612398573606364, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.3547617270083059}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9779295921325684 and batch: 50, loss is 7.548709297180176 and perplexity is 1898.2910198922805
At time: 5.419942140579224 and batch: 100, loss is 6.4846134281158445 and perplexity is 654.9857162305356
At time: 7.870798587799072 and batch: 150, loss is 6.2521640491485595 and perplexity is 519.1350437111344
At time: 10.315586566925049 and batch: 200, loss is 6.203279609680176 and perplexity is 494.3677184824879
At time: 12.761193752288818 and batch: 250, loss is 6.152845888137818 and perplexity is 470.05320392495594
At time: 15.210358619689941 and batch: 300, loss is 6.208636903762818 and perplexity is 497.0232987461871
At time: 17.659230709075928 and batch: 350, loss is 6.22447078704834 and perplexity is 504.955742614791
At time: 20.10602641105652 and batch: 400, loss is 6.259586477279663 and perplexity is 523.0026219178815
At time: 22.55829882621765 and batch: 450, loss is 6.272934036254883 and perplexity is 530.0302266075182
At time: 25.00190305709839 and batch: 500, loss is 6.279943265914917 and perplexity is 533.758380673143
At time: 27.444229125976562 and batch: 550, loss is 6.284893264770508 and perplexity is 536.4070340540248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.780384826660156 and perplexity of 323.8838055612628
Finished 1 epochs...
Completing Train Step...
At time: 31.438085794448853 and batch: 50, loss is 6.106483020782471 and perplexity is 448.75766515523094
At time: 33.890602350234985 and batch: 100, loss is 6.105871047973633 and perplexity is 448.48312168153694
At time: 36.32416319847107 and batch: 150, loss is 6.138752174377442 and perplexity is 463.4748740547578
At time: 38.758875131607056 and batch: 200, loss is 6.152005014419555 and perplexity is 469.65811467295424
At time: 41.18885040283203 and batch: 250, loss is 6.1139594841003415 and perplexity is 452.1253589096629
At time: 43.62248396873474 and batch: 300, loss is 6.139148111343384 and perplexity is 463.6584172235464
At time: 46.05391597747803 and batch: 350, loss is 6.132222213745117 and perplexity is 460.45826127358083
At time: 48.48046398162842 and batch: 400, loss is 6.120697822570801 and perplexity is 455.18222012539627
At time: 50.90664553642273 and batch: 450, loss is 6.085320281982422 and perplexity is 439.3605093955163
At time: 53.33662939071655 and batch: 500, loss is 6.092433061599731 and perplexity is 442.49672425177306
At time: 55.76698112487793 and batch: 550, loss is 6.0426498126983645 and perplexity is 421.0071482264137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.727896118164063 and perplexity of 307.3220185349827
Finished 2 epochs...
Completing Train Step...
At time: 59.7670042514801 and batch: 50, loss is 6.088052206039428 and perplexity is 440.56244999850907
At time: 62.19761085510254 and batch: 100, loss is 6.09176775932312 and perplexity is 442.2024280825462
At time: 64.6240770816803 and batch: 150, loss is 6.105348424911499 and perplexity is 448.2487952966885
At time: 67.05158519744873 and batch: 200, loss is 6.09566071510315 and perplexity is 443.92725774536547
At time: 69.4862973690033 and batch: 250, loss is 6.059248704910278 and perplexity is 428.05372136378554
At time: 71.94115924835205 and batch: 300, loss is 6.060304012298584 and perplexity is 428.50568805848343
At time: 74.37819647789001 and batch: 350, loss is 6.036026706695557 and perplexity is 418.2279867553903
At time: 76.83575248718262 and batch: 400, loss is 6.0606015682220455 and perplexity is 428.6332114359295
At time: 79.29252910614014 and batch: 450, loss is 6.018938913345337 and perplexity is 411.1421067886423
At time: 81.74952220916748 and batch: 500, loss is 5.9968212127685545 and perplexity is 402.14841529826265
At time: 84.20275592803955 and batch: 550, loss is 5.986139230728149 and perplexity is 397.8755351923243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.760870869954427 and perplexity of 317.62481841831766
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 88.20421743392944 and batch: 50, loss is 5.931637773513794 and perplexity is 376.77107448011407
At time: 90.66368675231934 and batch: 100, loss is 5.825423336029052 and perplexity is 338.80453088778694
At time: 93.11811447143555 and batch: 150, loss is 5.798216037750244 and perplexity is 329.7108432439461
At time: 95.56271624565125 and batch: 200, loss is 5.760738115310669 and perplexity is 317.5826550474534
At time: 98.01767539978027 and batch: 250, loss is 5.690396547317505 and perplexity is 296.0109797296938
At time: 100.47531247138977 and batch: 300, loss is 5.682248516082764 and perplexity is 293.60887253298995
At time: 102.92878842353821 and batch: 350, loss is 5.6244901752471925 and perplexity is 277.130960277577
At time: 105.39157319068909 and batch: 400, loss is 5.603230066299439 and perplexity is 271.30131489514054
At time: 107.85007429122925 and batch: 450, loss is 5.575777044296265 and perplexity is 263.9545803711231
At time: 110.30129790306091 and batch: 500, loss is 5.587428102493286 and perplexity is 267.0479158718222
At time: 112.75442361831665 and batch: 550, loss is 5.594646472930908 and perplexity is 268.982540672139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.414873758951823 and perplexity of 224.72417447251195
Finished 4 epochs...
Completing Train Step...
At time: 116.69578742980957 and batch: 50, loss is 5.627336015701294 and perplexity is 277.9207540559291
At time: 119.17033743858337 and batch: 100, loss is 5.600690269470215 and perplexity is 270.61313895926276
At time: 121.62699747085571 and batch: 150, loss is 5.626194105148316 and perplexity is 277.60357454376276
At time: 124.10214972496033 and batch: 200, loss is 5.6290492343902585 and perplexity is 278.3973011839916
At time: 126.5606541633606 and batch: 250, loss is 5.584411144256592 and perplexity is 266.24345758115777
At time: 129.01402926445007 and batch: 300, loss is 5.597461671829223 and perplexity is 269.7408469149352
At time: 131.46665716171265 and batch: 350, loss is 5.567887783050537 and perplexity is 261.8803664954625
At time: 133.9248170852661 and batch: 400, loss is 5.596118974685669 and perplexity is 269.3789096906972
At time: 136.38103675842285 and batch: 450, loss is 5.554259119033813 and perplexity is 258.33549774911586
At time: 138.8381586074829 and batch: 500, loss is 5.55182412147522 and perplexity is 257.70721668466217
At time: 141.29347586631775 and batch: 550, loss is 5.54474061012268 and perplexity is 255.8881948265866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.374144999186198 and perplexity of 215.75532245043914
Finished 5 epochs...
Completing Train Step...
At time: 145.32328081130981 and batch: 50, loss is 5.595984945297241 and perplexity is 269.3428074196127
At time: 147.78230953216553 and batch: 100, loss is 5.56428466796875 and perplexity is 260.9384792803331
At time: 150.24022459983826 and batch: 150, loss is 5.5871281337738035 and perplexity is 266.96782186390675
At time: 152.70379853248596 and batch: 200, loss is 5.597174949645996 and perplexity is 269.6635173169876
At time: 155.1605682373047 and batch: 250, loss is 5.553282089233399 and perplexity is 258.08321953104706
At time: 157.62270164489746 and batch: 300, loss is 5.568298568725586 and perplexity is 261.9879652971056
At time: 160.0742084980011 and batch: 350, loss is 5.534420909881592 and perplexity is 253.26108417270382
At time: 162.5305802822113 and batch: 400, loss is 5.552295541763305 and perplexity is 257.8287337355444
At time: 164.99085354804993 and batch: 450, loss is 5.529016847610474 and perplexity is 251.89613695467736
At time: 167.44510769844055 and batch: 500, loss is 5.531248140335083 and perplexity is 252.45881849258203
At time: 169.8958728313446 and batch: 550, loss is 5.512171277999878 and perplexity is 247.68834388494224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.364786783854167 and perplexity of 213.74565579711816
Finished 6 epochs...
Completing Train Step...
At time: 173.88872694969177 and batch: 50, loss is 5.5554429626464845 and perplexity is 258.6415076762622
At time: 176.37429213523865 and batch: 100, loss is 5.531635675430298 and perplexity is 252.5566741048618
At time: 178.83830380439758 and batch: 150, loss is 5.5691586780548095 and perplexity is 262.2134005257756
At time: 181.29889273643494 and batch: 200, loss is 5.582368049621582 and perplexity is 265.70005230459503
At time: 183.7620985507965 and batch: 250, loss is 5.534082698822021 and perplexity is 253.1754429562973
At time: 186.24368286132812 and batch: 300, loss is 5.55445294380188 and perplexity is 258.3855744199432
At time: 188.6961748600006 and batch: 350, loss is 5.519321718215942 and perplexity is 249.46577170264322
At time: 191.14411449432373 and batch: 400, loss is 5.545901374816895 and perplexity is 256.1853932641645
At time: 193.60095834732056 and batch: 450, loss is 5.5055242538452145 and perplexity is 246.04741317509885
At time: 196.06601738929749 and batch: 500, loss is 5.506989984512329 and perplexity is 246.4083168433283
At time: 198.52541780471802 and batch: 550, loss is 5.49544316291809 and perplexity is 243.57944762430424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3472849527994795 and perplexity of 210.03726194267026
Finished 7 epochs...
Completing Train Step...
At time: 202.5687997341156 and batch: 50, loss is 5.545519332885743 and perplexity is 256.0875383953107
At time: 205.02846789360046 and batch: 100, loss is 5.521195983886718 and perplexity is 249.93377527909465
At time: 207.4845359325409 and batch: 150, loss is 5.557029714584351 and perplexity is 259.05223336349377
At time: 209.94255828857422 and batch: 200, loss is 5.563934078216553 and perplexity is 260.847012958058
At time: 212.40390610694885 and batch: 250, loss is 5.521694107055664 and perplexity is 250.05830409603274
At time: 214.86345624923706 and batch: 300, loss is 5.539235057830811 and perplexity is 254.48326000890836
At time: 217.31735396385193 and batch: 350, loss is 5.503365573883056 and perplexity is 245.51684842032077
At time: 219.77188539505005 and batch: 400, loss is 5.517633790969849 and perplexity is 249.04504680601715
At time: 222.23041415214539 and batch: 450, loss is 5.483885765075684 and perplexity is 240.78050842407322
At time: 224.69216227531433 and batch: 500, loss is 5.482745637893677 and perplexity is 240.50614445615062
At time: 227.14901900291443 and batch: 550, loss is 5.466729412078857 and perplexity is 236.68482696513078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.337515767415365 and perplexity of 207.9953590962965
Finished 8 epochs...
Completing Train Step...
At time: 231.13956832885742 and batch: 50, loss is 5.5152549552917485 and perplexity is 248.45331366022492
At time: 233.62471055984497 and batch: 100, loss is 5.495325832366944 and perplexity is 243.550869990014
At time: 236.07197165489197 and batch: 150, loss is 5.5371679592132566 and perplexity is 253.95776132986649
At time: 238.51641702651978 and batch: 200, loss is 5.54671082496643 and perplexity is 256.3928465192581
At time: 240.95962524414062 and batch: 250, loss is 5.50161208152771 and perplexity is 245.08671373266276
At time: 243.4014527797699 and batch: 300, loss is 5.523352384567261 and perplexity is 250.47331416413226
At time: 245.84654426574707 and batch: 350, loss is 5.485955638885498 and perplexity is 241.27940984574371
At time: 248.33547449111938 and batch: 400, loss is 5.507612066268921 and perplexity is 246.56165065004606
At time: 250.79007720947266 and batch: 450, loss is 5.465237722396851 and perplexity is 236.33202984818755
At time: 253.2410593032837 and batch: 500, loss is 5.472106027603149 and perplexity is 237.9608174614525
At time: 255.6963930130005 and batch: 550, loss is 5.454053382873536 and perplexity is 233.70353855531178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.33797353108724 and perplexity of 208.09059361139705
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 259.6993896961212 and batch: 50, loss is 5.483318977355957 and perplexity is 240.64407565660986
At time: 262.1535999774933 and batch: 100, loss is 5.4288056945800784 and perplexity is 227.8769281909854
At time: 264.6156256198883 and batch: 150, loss is 5.437128849029541 and perplexity is 229.78149807956467
At time: 267.0768668651581 and batch: 200, loss is 5.428383769989014 and perplexity is 227.78080159175948
At time: 269.53720355033875 and batch: 250, loss is 5.375683746337891 and perplexity is 216.08757089594027
At time: 271.99265480041504 and batch: 300, loss is 5.35371584892273 and perplexity is 211.39234227656513
At time: 274.4509663581848 and batch: 350, loss is 5.295421533584594 and perplexity is 199.42167118328524
At time: 276.90421772003174 and batch: 400, loss is 5.308003587722778 and perplexity is 201.9466568894838
At time: 279.36021089553833 and batch: 450, loss is 5.27536229133606 and perplexity is 195.4612776138783
At time: 281.81864738464355 and batch: 500, loss is 5.286547918319702 and perplexity is 197.6599081615844
At time: 284.274866104126 and batch: 550, loss is 5.293265733718872 and perplexity is 198.99222104192623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.205173746744792 and perplexity of 182.21252885237874
Finished 10 epochs...
Completing Train Step...
At time: 288.29342794418335 and batch: 50, loss is 5.357018308639526 and perplexity is 212.09161098921757
At time: 290.7766366004944 and batch: 100, loss is 5.331757984161377 and perplexity is 206.80120803133323
At time: 293.22637271881104 and batch: 150, loss is 5.355608415603638 and perplexity is 211.7927952025606
At time: 295.6776211261749 and batch: 200, loss is 5.3587777805328365 and perplexity is 212.46510870056827
At time: 298.1199634075165 and batch: 250, loss is 5.324095516204834 and perplexity is 205.2226559273838
At time: 300.58676624298096 and batch: 300, loss is 5.316923828125 and perplexity is 203.7561280779742
At time: 303.0292613506317 and batch: 350, loss is 5.273501815795899 and perplexity is 195.09796475996484
At time: 305.485675573349 and batch: 400, loss is 5.302708034515381 and perplexity is 200.8800642145266
At time: 307.93406891822815 and batch: 450, loss is 5.273508377075196 and perplexity is 195.09924485640138
At time: 310.4486117362976 and batch: 500, loss is 5.2810275268554685 and perplexity is 196.57175437262288
At time: 312.8907001018524 and batch: 550, loss is 5.273881235122681 and perplexity is 195.17200274324327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1905364990234375 and perplexity of 179.56486346816232
Finished 11 epochs...
Completing Train Step...
At time: 317.00299620628357 and batch: 50, loss is 5.329948968887329 and perplexity is 206.42743966558524
At time: 319.46459007263184 and batch: 100, loss is 5.31203950881958 and perplexity is 202.76334459746568
At time: 321.916508436203 and batch: 150, loss is 5.3458006477355955 and perplexity is 209.72573382973775
At time: 324.35654401779175 and batch: 200, loss is 5.354424743652344 and perplexity is 211.5422503221197
At time: 326.7977616786957 and batch: 250, loss is 5.321011505126953 and perplexity is 204.59072192966312
At time: 329.23524236679077 and batch: 300, loss is 5.312611131668091 and perplexity is 202.87928189112426
At time: 331.67745661735535 and batch: 350, loss is 5.268224601745605 and perplexity is 194.07110290713615
At time: 334.1202025413513 and batch: 400, loss is 5.294624729156494 and perplexity is 199.26283440195633
At time: 336.5633063316345 and batch: 450, loss is 5.2637819576263425 and perplexity is 193.21082642915033
At time: 339.0067286491394 and batch: 500, loss is 5.2690039730072025 and perplexity is 194.22241530405609
At time: 341.4479079246521 and batch: 550, loss is 5.259693536758423 and perplexity is 192.42251183345957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.186519877115885 and perplexity of 178.8450658488955
Finished 12 epochs...
Completing Train Step...
At time: 345.4714210033417 and batch: 50, loss is 5.31486068725586 and perplexity is 203.33618383382472
At time: 347.93289256095886 and batch: 100, loss is 5.300446376800537 and perplexity is 200.42625564080802
At time: 350.3770749568939 and batch: 150, loss is 5.332803726196289 and perplexity is 207.01758186332947
At time: 352.82139015197754 and batch: 200, loss is 5.342833395004273 and perplexity is 209.1043469351042
At time: 355.2667019367218 and batch: 250, loss is 5.308320636749268 and perplexity is 202.01069403137373
At time: 357.70886421203613 and batch: 300, loss is 5.3010322380065915 and perplexity is 200.54371201186868
At time: 360.1527409553528 and batch: 350, loss is 5.258224983215332 and perplexity is 192.1401364643591
At time: 362.5958716869354 and batch: 400, loss is 5.285524682998657 and perplexity is 197.45775900271653
At time: 365.0401587486267 and batch: 450, loss is 5.253601303100586 and perplexity is 191.25379260044545
At time: 367.48674488067627 and batch: 500, loss is 5.258625078201294 and perplexity is 192.21702615012376
At time: 369.9316051006317 and batch: 550, loss is 5.250319366455078 and perplexity is 190.6271386516766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.184886169433594 and perplexity of 178.55312382977075
Finished 13 epochs...
Completing Train Step...
At time: 373.90926909446716 and batch: 50, loss is 5.307083568572998 and perplexity is 201.76094753908995
At time: 376.3526849746704 and batch: 100, loss is 5.290527849197388 and perplexity is 198.4481484638493
At time: 378.7953155040741 and batch: 150, loss is 5.324654150009155 and perplexity is 205.33733226845544
At time: 381.2390561103821 and batch: 200, loss is 5.334820117950439 and perplexity is 207.4354315411095
At time: 383.6850781440735 and batch: 250, loss is 5.299703321456909 and perplexity is 200.27738315764753
At time: 386.1279721260071 and batch: 300, loss is 5.288763046264648 and perplexity is 198.0982354440232
At time: 388.5820186138153 and batch: 350, loss is 5.245304212570191 and perplexity is 189.67350751893437
At time: 391.03447103500366 and batch: 400, loss is 5.2711237525939945 and perplexity is 194.63456068967258
At time: 393.4811797142029 and batch: 450, loss is 5.238136625289917 and perplexity is 188.31886665183393
At time: 395.92713236808777 and batch: 500, loss is 5.244833707809448 and perplexity is 189.58428622182845
At time: 398.3693950176239 and batch: 550, loss is 5.237160406112671 and perplexity is 188.1351158678837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.176374308268229 and perplexity of 177.03975436167335
Finished 14 epochs...
Completing Train Step...
At time: 402.3176238536835 and batch: 50, loss is 5.293949728012085 and perplexity is 199.12837714520305
At time: 404.7814197540283 and batch: 100, loss is 5.279839696884156 and perplexity is 196.33839917185375
At time: 407.22647070884705 and batch: 150, loss is 5.310942192077636 and perplexity is 202.54097101422775
At time: 409.66932821273804 and batch: 200, loss is 5.322682065963745 and perplexity is 204.9327888195121
At time: 412.11213636398315 and batch: 250, loss is 5.284707689285279 and perplexity is 197.29650313643754
At time: 414.55569076538086 and batch: 300, loss is 5.272591524124145 and perplexity is 194.92044951505017
At time: 416.99647641181946 and batch: 350, loss is 5.2315256690979 and perplexity is 187.0780050344563
At time: 419.4396767616272 and batch: 400, loss is 5.257291507720947 and perplexity is 191.9608620426397
At time: 421.8892443180084 and batch: 450, loss is 5.22134768486023 and perplexity is 185.18358508993532
At time: 424.34101963043213 and batch: 500, loss is 5.227705612182617 and perplexity is 186.36471967037525
At time: 426.7870864868164 and batch: 550, loss is 5.220795097351075 and perplexity is 185.0812832218845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.168596394856771 and perplexity of 175.66809571791742
Finished 15 epochs...
Completing Train Step...
At time: 430.7876625061035 and batch: 50, loss is 5.277179183959961 and perplexity is 195.81673258140503
At time: 433.23330783843994 and batch: 100, loss is 5.263109750747681 and perplexity is 193.08099242513697
At time: 435.7098915576935 and batch: 150, loss is 5.292875928878784 and perplexity is 198.91466802727643
At time: 438.1513135433197 and batch: 200, loss is 5.308340148925781 and perplexity is 202.01463573814883
At time: 440.59321761131287 and batch: 250, loss is 5.2679363632202145 and perplexity is 194.01517219969276
At time: 443.0361988544464 and batch: 300, loss is 5.251886758804321 and perplexity is 190.92616045138024
At time: 445.48167157173157 and batch: 350, loss is 5.2166898059844975 and perplexity is 184.32302812174896
At time: 447.935489654541 and batch: 400, loss is 5.241770753860473 and perplexity is 189.00448668661113
At time: 450.3792631626129 and batch: 450, loss is 5.202674598693847 and perplexity is 181.7577213184382
At time: 452.82016682624817 and batch: 500, loss is 5.208902292251587 and perplexity is 182.8931846977174
At time: 455.2570741176605 and batch: 550, loss is 5.203932380676269 and perplexity is 181.98647673754294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.160848999023438 and perplexity of 174.3123838444169
Finished 16 epochs...
Completing Train Step...
At time: 459.1465697288513 and batch: 50, loss is 5.263728952407837 and perplexity is 193.20058551849098
At time: 461.6145579814911 and batch: 100, loss is 5.2497947406768795 and perplexity is 190.52715696949278
At time: 464.05906224250793 and batch: 150, loss is 5.279977598190308 and perplexity is 196.36547636049428
At time: 466.50790548324585 and batch: 200, loss is 5.294266414642334 and perplexity is 199.19144842633682
At time: 468.9556634426117 and batch: 250, loss is 5.2561421298980715 and perplexity is 191.740353233191
At time: 471.39875316619873 and batch: 300, loss is 5.238539209365845 and perplexity is 188.3946960915831
At time: 473.8457705974579 and batch: 350, loss is 5.202487735748291 and perplexity is 181.7237607083437
At time: 476.29543828964233 and batch: 400, loss is 5.227095041275025 and perplexity is 186.25096552536263
At time: 478.7394781112671 and batch: 450, loss is 5.183326683044434 and perplexity is 178.27488967099669
At time: 481.1834604740143 and batch: 500, loss is 5.189407529830933 and perplexity is 179.36225466032153
At time: 483.62953782081604 and batch: 550, loss is 5.184527406692505 and perplexity is 178.48907711111067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.150640869140625 and perplexity of 172.54203175538535
Finished 17 epochs...
Completing Train Step...
At time: 487.58319568634033 and batch: 50, loss is 5.244421424865723 and perplexity is 189.5061399645123
At time: 490.02361607551575 and batch: 100, loss is 5.2303822803497315 and perplexity is 186.8642243889527
At time: 492.46275758743286 and batch: 150, loss is 5.261194791793823 and perplexity is 192.71160404454716
At time: 494.9003894329071 and batch: 200, loss is 5.278777475357056 and perplexity is 196.12995502418673
At time: 497.367787361145 and batch: 250, loss is 5.24237681388855 and perplexity is 189.119069469634
At time: 499.807391166687 and batch: 300, loss is 5.221437273025512 and perplexity is 185.20017609073156
At time: 502.24925565719604 and batch: 350, loss is 5.186517009735107 and perplexity is 178.84455303272665
At time: 504.70273303985596 and batch: 400, loss is 5.2081708812713625 and perplexity is 182.75946352275048
At time: 507.1452605724335 and batch: 450, loss is 5.165972347259522 and perplexity is 175.2077385371663
At time: 509.58664083480835 and batch: 500, loss is 5.17188666343689 and perplexity is 176.24704285727816
At time: 512.0260102748871 and batch: 550, loss is 5.165315275192261 and perplexity is 175.09265224034172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.141353352864583 and perplexity of 170.94696340438668
Finished 18 epochs...
Completing Train Step...
At time: 515.9629273414612 and batch: 50, loss is 5.227333145141602 and perplexity is 186.295317880432
At time: 518.4332096576691 and batch: 100, loss is 5.214381361007691 and perplexity is 183.8980192968514
At time: 520.8789165019989 and batch: 150, loss is 5.243777847290039 and perplexity is 189.38421729993138
At time: 523.3195123672485 and batch: 200, loss is 5.262961950302124 and perplexity is 193.05245707724845
At time: 525.7659368515015 and batch: 250, loss is 5.2249259185791015 and perplexity is 185.84740217634737
At time: 528.2251508235931 and batch: 300, loss is 5.2036208820343015 and perplexity is 181.92979702546768
At time: 530.6648626327515 and batch: 350, loss is 5.171385765075684 and perplexity is 176.1587831087705
At time: 533.1081624031067 and batch: 400, loss is 5.195972957611084 and perplexity is 180.54371875447254
At time: 535.550413608551 and batch: 450, loss is 5.154092082977295 and perplexity is 173.13853994937512
At time: 537.9928250312805 and batch: 500, loss is 5.159988946914673 and perplexity is 174.16253056116093
At time: 540.4350354671478 and batch: 550, loss is 5.151384601593017 and perplexity is 172.67040459540974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.137423706054688 and perplexity of 170.27652037988463
Finished 19 epochs...
Completing Train Step...
At time: 544.3945243358612 and batch: 50, loss is 5.213880376815796 and perplexity is 183.80591237024646
At time: 546.8381872177124 and batch: 100, loss is 5.203649225234986 and perplexity is 181.93495357129137
At time: 549.2806558609009 and batch: 150, loss is 5.232311134338379 and perplexity is 187.22500602919703
At time: 551.7220225334167 and batch: 200, loss is 5.250706424713135 and perplexity is 190.70093674106013
At time: 554.1630728244781 and batch: 250, loss is 5.2101481246948245 and perplexity is 183.121180953938
At time: 556.6068873405457 and batch: 300, loss is 5.1916436672210695 and perplexity is 179.7637820722627
At time: 559.0779430866241 and batch: 350, loss is 5.159354419708252 and perplexity is 174.052054750841
At time: 561.5214235782623 and batch: 400, loss is 5.185451040267944 and perplexity is 178.65401177345018
At time: 563.9652862548828 and batch: 450, loss is 5.144981050491333 and perplexity is 171.56823350653053
At time: 566.4068632125854 and batch: 500, loss is 5.148982667922974 and perplexity is 172.25615943065463
At time: 568.8482127189636 and batch: 550, loss is 5.1398571109771725 and perplexity is 170.6913766546984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.132404581705729 and perplexity of 169.42402253638053
Finished 20 epochs...
Completing Train Step...
At time: 572.7515332698822 and batch: 50, loss is 5.202343435287475 and perplexity is 181.69753977782395
At time: 575.220014333725 and batch: 100, loss is 5.192534561157227 and perplexity is 179.92400389537966
At time: 577.6603798866272 and batch: 150, loss is 5.221121940612793 and perplexity is 185.1417856790474
At time: 580.1004281044006 and batch: 200, loss is 5.23957480430603 and perplexity is 188.58989774308913
At time: 582.5399043560028 and batch: 250, loss is 5.1999666786193846 and perplexity is 181.2662017344181
At time: 584.9813013076782 and batch: 300, loss is 5.181817560195923 and perplexity is 178.00605386582828
At time: 587.4321899414062 and batch: 350, loss is 5.148895387649536 and perplexity is 172.24112552204951
At time: 589.891220331192 and batch: 400, loss is 5.175987186431885 and perplexity is 176.9712316710297
At time: 592.3384232521057 and batch: 450, loss is 5.133798656463623 and perplexity is 169.66037699919607
At time: 594.7781701087952 and batch: 500, loss is 5.137695264816284 and perplexity is 170.32276673990592
At time: 597.2201502323151 and batch: 550, loss is 5.129706211090088 and perplexity is 168.9674699834024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.129081726074219 and perplexity of 168.86198527046778
Finished 21 epochs...
Completing Train Step...
At time: 601.1639490127563 and batch: 50, loss is 5.191063985824585 and perplexity is 179.65960654925726
At time: 603.6139507293701 and batch: 100, loss is 5.181052179336548 and perplexity is 177.8698635647197
At time: 606.0559983253479 and batch: 150, loss is 5.210081357955932 and perplexity is 183.10895495801307
At time: 608.5117485523224 and batch: 200, loss is 5.229950313568115 and perplexity is 186.78352268282717
At time: 610.9585280418396 and batch: 250, loss is 5.190951251983643 and perplexity is 179.63935397334558
At time: 613.4061434268951 and batch: 300, loss is 5.175465269088745 and perplexity is 176.87889141507034
At time: 615.8486084938049 and batch: 350, loss is 5.142670154571533 and perplexity is 171.17221493084452
At time: 618.289154291153 and batch: 400, loss is 5.171155500411987 and perplexity is 176.1182246355916
At time: 620.7627468109131 and batch: 450, loss is 5.124839668273926 and perplexity is 168.14718016288214
At time: 623.2115750312805 and batch: 500, loss is 5.128214693069458 and perplexity is 168.71563980832957
At time: 625.654559135437 and batch: 550, loss is 5.120013399124145 and perplexity is 167.33761178351702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.12410634358724 and perplexity of 168.0239188854647
Finished 22 epochs...
Completing Train Step...
At time: 629.5619711875916 and batch: 50, loss is 5.179626302719116 and perplexity is 177.61642381516302
At time: 632.0313019752502 and batch: 100, loss is 5.1711438941955565 and perplexity is 176.116180581221
At time: 634.4724016189575 and batch: 150, loss is 5.200157127380371 and perplexity is 181.30072694548466
At time: 636.9135963916779 and batch: 200, loss is 5.221964588165283 and perplexity is 185.2978607004958
At time: 639.353443145752 and batch: 250, loss is 5.184918479919434 and perplexity is 178.55889306115273
At time: 641.7966177463531 and batch: 300, loss is 5.1697265148162845 and perplexity is 175.86673396051802
At time: 644.2479250431061 and batch: 350, loss is 5.137468490600586 and perplexity is 170.28414630728759
At time: 646.6932246685028 and batch: 400, loss is 5.160521574020386 and perplexity is 174.2553189543504
At time: 649.1367161273956 and batch: 450, loss is 5.116814212799072 and perplexity is 166.80312300468998
At time: 651.5797791481018 and batch: 500, loss is 5.121831703186035 and perplexity is 167.6421592387587
At time: 654.0202136039734 and batch: 550, loss is 5.112003335952759 and perplexity is 166.00258092103525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123978678385416 and perplexity of 168.00246944715158
Finished 23 epochs...
Completing Train Step...
At time: 657.962958574295 and batch: 50, loss is 5.172769784927368 and perplexity is 176.40275915654348
At time: 660.4060368537903 and batch: 100, loss is 5.163472719192505 and perplexity is 174.77033126185987
At time: 662.8500745296478 and batch: 150, loss is 5.191585550308227 and perplexity is 179.75333505978483
At time: 665.2960834503174 and batch: 200, loss is 5.213890647888183 and perplexity is 183.80780026377295
At time: 667.7405812740326 and batch: 250, loss is 5.176058044433594 and perplexity is 176.98377194314992
At time: 670.1871373653412 and batch: 300, loss is 5.161270799636841 and perplexity is 174.38592442348664
At time: 672.6337764263153 and batch: 350, loss is 5.127125282287597 and perplexity is 168.53193925210542
At time: 675.0873625278473 and batch: 400, loss is 5.15269660949707 and perplexity is 172.89709821042374
At time: 677.5321679115295 and batch: 450, loss is 5.108379459381103 and perplexity is 165.40209675460616
At time: 679.9729659557343 and batch: 500, loss is 5.113031301498413 and perplexity is 166.1733135933295
At time: 682.4391074180603 and batch: 550, loss is 5.103566398620606 and perplexity is 164.60791915865425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.119835408528646 and perplexity of 167.30782991287126
Finished 24 epochs...
Completing Train Step...
At time: 686.3448481559753 and batch: 50, loss is 5.162511157989502 and perplexity is 174.60235966231127
At time: 688.8168668746948 and batch: 100, loss is 5.153106393814087 and perplexity is 172.96796324842705
At time: 691.2662093639374 and batch: 150, loss is 5.181775417327881 and perplexity is 177.99855233825852
At time: 693.7057774066925 and batch: 200, loss is 5.204451875686646 and perplexity is 182.0810423652232
At time: 696.1466450691223 and batch: 250, loss is 5.166845645904541 and perplexity is 175.3608140484174
At time: 698.5871906280518 and batch: 300, loss is 5.152739124298096 and perplexity is 172.9044490524107
At time: 701.0310146808624 and batch: 350, loss is 5.1187902927398685 and perplexity is 167.13306519883173
At time: 703.4730155467987 and batch: 400, loss is 5.143833875656128 and perplexity is 171.3715275961386
At time: 705.9167935848236 and batch: 450, loss is 5.102838134765625 and perplexity is 164.4880848016991
At time: 708.3589413166046 and batch: 500, loss is 5.106829385757447 and perplexity is 165.1459099326427
At time: 710.7998583316803 and batch: 550, loss is 5.094639739990234 and perplexity is 163.14505939534877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.121390787760417 and perplexity of 167.56825951772709
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 714.7380557060242 and batch: 50, loss is 5.14936336517334 and perplexity is 172.32174936106935
At time: 717.1821944713593 and batch: 100, loss is 5.1311440181732175 and perplexity is 169.21058734461212
At time: 719.6344995498657 and batch: 150, loss is 5.149390497207642 and perplexity is 172.32642486411166
At time: 722.077602148056 and batch: 200, loss is 5.153736534118653 and perplexity is 173.07699168146576
At time: 724.5202584266663 and batch: 250, loss is 5.116087074279785 and perplexity is 166.68187811507278
At time: 726.9622147083282 and batch: 300, loss is 5.093709268569946 and perplexity is 162.99332818194614
At time: 729.4042453765869 and batch: 350, loss is 5.0451881694793705 and perplexity is 155.27351418591383
At time: 731.8456842899323 and batch: 400, loss is 5.065607481002807 and perplexity is 158.47668441319433
At time: 734.2868022918701 and batch: 450, loss is 5.028923683166504 and perplexity is 152.7684968711108
At time: 736.7284264564514 and batch: 500, loss is 5.036235418319702 and perplexity is 153.88959324525175
At time: 739.1689343452454 and batch: 550, loss is 5.04584397315979 and perplexity is 155.3753765251863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.070525614420573 and perplexity of 159.2580136565736
Finished 26 epochs...
Completing Train Step...
At time: 743.0736827850342 and batch: 50, loss is 5.111208667755127 and perplexity is 165.8707163504822
At time: 745.5411567687988 and batch: 100, loss is 5.095886030197144 and perplexity is 163.34851223948817
At time: 747.9888081550598 and batch: 150, loss is 5.121167335510254 and perplexity is 167.5308201961881
At time: 750.432638168335 and batch: 200, loss is 5.132014694213868 and perplexity is 169.3579791047633
At time: 752.8771500587463 and batch: 250, loss is 5.10185998916626 and perplexity is 164.3272701682873
At time: 755.3301451206207 and batch: 300, loss is 5.084837942123413 and perplexity is 161.5537560574007
At time: 757.7716162204742 and batch: 350, loss is 5.045011930465698 and perplexity is 155.24615134619498
At time: 760.2235713005066 and batch: 400, loss is 5.06957405090332 and perplexity is 159.10654162000927
At time: 762.672611951828 and batch: 450, loss is 5.033408317565918 and perplexity is 153.45514626224485
At time: 765.1180222034454 and batch: 500, loss is 5.036545972824097 and perplexity is 153.93739177325352
At time: 767.560444355011 and batch: 550, loss is 5.039323253631592 and perplexity is 154.36551336874095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.06668446858724 and perplexity of 158.64745377646506
Finished 27 epochs...
Completing Train Step...
At time: 771.4939963817596 and batch: 50, loss is 5.101488904953003 and perplexity is 164.26630222534038
At time: 773.935967206955 and batch: 100, loss is 5.086253118515015 and perplexity is 161.7825449690975
At time: 776.3794088363647 and batch: 150, loss is 5.11292802810669 and perplexity is 166.15615319774216
At time: 778.8239572048187 and batch: 200, loss is 5.125564489364624 and perplexity is 168.26910096545245
At time: 781.2673346996307 and batch: 250, loss is 5.098172035217285 and perplexity is 163.72235489879543
At time: 783.710645198822 and batch: 300, loss is 5.082603931427002 and perplexity is 161.19324607983225
At time: 786.1529443264008 and batch: 350, loss is 5.043208894729614 and perplexity is 154.96648918362854
At time: 788.5942752361298 and batch: 400, loss is 5.069200019836426 and perplexity is 159.04704195855192
At time: 791.0332355499268 and batch: 450, loss is 5.031039514541626 and perplexity is 153.09207144382424
At time: 793.4743733406067 and batch: 500, loss is 5.033059539794922 and perplexity is 153.4016338508959
At time: 795.9178230762482 and batch: 550, loss is 5.034337854385376 and perplexity is 153.59785478696946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.064023335774739 and perplexity of 158.2258330747561
Finished 28 epochs...
Completing Train Step...
At time: 799.8521573543549 and batch: 50, loss is 5.094073429107666 and perplexity is 163.0526947288023
At time: 802.3201627731323 and batch: 100, loss is 5.079084405899048 and perplexity is 160.62691952030548
At time: 804.7642843723297 and batch: 150, loss is 5.106817770004272 and perplexity is 165.14399164965633
At time: 807.2327876091003 and batch: 200, loss is 5.120643854141235 and perplexity is 167.44314388353922
At time: 809.680867433548 and batch: 250, loss is 5.0946786499023435 and perplexity is 163.15140747877177
At time: 812.125714302063 and batch: 300, loss is 5.079224309921265 and perplexity is 160.64939344448223
At time: 814.5701739788055 and batch: 350, loss is 5.040076313018798 and perplexity is 154.4818035488545
At time: 817.0152595043182 and batch: 400, loss is 5.065768737792968 and perplexity is 158.50224191524316
At time: 819.4595909118652 and batch: 450, loss is 5.0281439208984375 and perplexity is 152.64942019328524
At time: 821.9041657447815 and batch: 500, loss is 5.029248943328858 and perplexity is 152.8181944590903
At time: 824.3457660675049 and batch: 550, loss is 5.029156866073609 and perplexity is 152.80412402698576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.062572224934896 and perplexity of 157.996396362549
Finished 29 epochs...
Completing Train Step...
At time: 828.2864990234375 and batch: 50, loss is 5.089123334884643 and perplexity is 162.24756290980048
At time: 830.7286841869354 and batch: 100, loss is 5.07414231300354 and perplexity is 159.8350447336426
At time: 833.1707012653351 and batch: 150, loss is 5.102065467834473 and perplexity is 164.36103938622068
At time: 835.6113066673279 and batch: 200, loss is 5.115921869277954 and perplexity is 166.6543437095669
At time: 838.0599751472473 and batch: 250, loss is 5.091777629852295 and perplexity is 162.678787845459
At time: 840.50186419487 and batch: 300, loss is 5.076532030105591 and perplexity is 160.21746202613105
At time: 842.9435882568359 and batch: 350, loss is 5.037641201019287 and perplexity is 154.1060807045923
At time: 845.3842880725861 and batch: 400, loss is 5.06293514251709 and perplexity is 158.05374643875834
At time: 847.8252966403961 and batch: 450, loss is 5.024982252120972 and perplexity is 152.16755543733808
At time: 850.2671191692352 and batch: 500, loss is 5.026637954711914 and perplexity is 152.41970834063738
At time: 852.7076823711395 and batch: 550, loss is 5.0260536479949955 and perplexity is 152.33067449533323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.061307271321614 and perplexity of 157.79666460242325
Finished 30 epochs...
Completing Train Step...
At time: 856.6149504184723 and batch: 50, loss is 5.084586133956909 and perplexity is 161.5130806237142
At time: 859.1138346195221 and batch: 100, loss is 5.070164880752563 and perplexity is 159.20057428992692
At time: 861.5768375396729 and batch: 150, loss is 5.098327207565307 and perplexity is 163.74776205222167
At time: 864.0336580276489 and batch: 200, loss is 5.112470159530639 and perplexity is 166.08009293060744
At time: 866.4750232696533 and batch: 250, loss is 5.089180345535278 and perplexity is 162.25681301260062
At time: 868.9516310691833 and batch: 300, loss is 5.07396520614624 and perplexity is 159.80673935779552
At time: 871.398291349411 and batch: 350, loss is 5.035521001815796 and perplexity is 153.77969124263456
At time: 873.8421652317047 and batch: 400, loss is 5.060328197479248 and perplexity is 157.64224562182605
At time: 876.2815701961517 and batch: 450, loss is 5.0218596935272215 and perplexity is 151.69314440301696
At time: 878.7366473674774 and batch: 500, loss is 5.023119077682495 and perplexity is 151.8843046923958
At time: 881.1855945587158 and batch: 550, loss is 5.021345882415772 and perplexity is 151.61522280009893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.060154724121094 and perplexity of 157.61490126391837
Finished 31 epochs...
Completing Train Step...
At time: 885.1082630157471 and batch: 50, loss is 5.080511674880982 and perplexity is 160.85634102435253
At time: 887.5513021945953 and batch: 100, loss is 5.0662607574462895 and perplexity is 158.58024732188383
At time: 890.0048506259918 and batch: 150, loss is 5.0944179344177245 and perplexity is 163.10887692493256
At time: 892.4574818611145 and batch: 200, loss is 5.10854733467102 and perplexity is 165.42986601037248
At time: 894.9087522029877 and batch: 250, loss is 5.086144990921021 and perplexity is 161.76505275747218
At time: 897.3522589206696 and batch: 300, loss is 5.070748949050904 and perplexity is 159.29358545824053
At time: 899.7922203540802 and batch: 350, loss is 5.032884483337402 and perplexity is 153.37478225463744
At time: 902.2318255901337 and batch: 400, loss is 5.05730978012085 and perplexity is 157.1671329361361
At time: 904.6719706058502 and batch: 450, loss is 5.018417949676514 and perplexity is 151.17195287442811
At time: 907.1233701705933 and batch: 500, loss is 5.020211591720581 and perplexity is 151.443344561999
At time: 909.5728182792664 and batch: 550, loss is 5.017926588058471 and perplexity is 151.09769102526639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.059102376302083 and perplexity of 157.44912280992605
Finished 32 epochs...
Completing Train Step...
At time: 913.4800164699554 and batch: 50, loss is 5.076947479248047 and perplexity is 160.28403806185716
At time: 915.9483699798584 and batch: 100, loss is 5.062486562728882 and perplexity is 157.98286262236763
At time: 918.3957073688507 and batch: 150, loss is 5.09093674659729 and perplexity is 162.54205147453453
At time: 920.836859703064 and batch: 200, loss is 5.104882116317749 and perplexity is 164.8246392509477
At time: 923.2769939899445 and batch: 250, loss is 5.083195295333862 and perplexity is 161.28859813869772
At time: 925.719313621521 and batch: 300, loss is 5.067990369796753 and perplexity is 158.85476701407958
At time: 928.1719489097595 and batch: 350, loss is 5.0304530811309816 and perplexity is 153.0023194575787
At time: 930.6519515514374 and batch: 400, loss is 5.0545297050476075 and perplexity is 156.7308033031216
At time: 933.0918786525726 and batch: 450, loss is 5.015193786621094 and perplexity is 150.68533473860458
At time: 935.5325214862823 and batch: 500, loss is 5.016994495391845 and perplexity is 150.95691959171987
At time: 937.9722356796265 and batch: 550, loss is 5.013631811141968 and perplexity is 150.45015166359318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.057863362630209 and perplexity of 157.25416199863653
Finished 33 epochs...
Completing Train Step...
At time: 941.9036273956299 and batch: 50, loss is 5.073047800064087 and perplexity is 159.66019891196555
At time: 944.3467674255371 and batch: 100, loss is 5.058662929534912 and perplexity is 157.37994750245917
At time: 946.7934141159058 and batch: 150, loss is 5.086764688491821 and perplexity is 161.86532923504214
At time: 949.2391247749329 and batch: 200, loss is 5.101184377670288 and perplexity is 164.21628627067994
At time: 951.6840786933899 and batch: 250, loss is 5.079538459777832 and perplexity is 160.6998693564756
At time: 954.1267879009247 and batch: 300, loss is 5.064244575500489 and perplexity is 158.26084278729883
At time: 956.5704729557037 and batch: 350, loss is 5.027305574417114 and perplexity is 152.52150071690366
At time: 959.0141890048981 and batch: 400, loss is 5.051646032333374 and perplexity is 156.27949398907487
At time: 961.4561522006989 and batch: 450, loss is 5.011932096481323 and perplexity is 150.19464653953045
At time: 963.8987200260162 and batch: 500, loss is 5.014043979644775 and perplexity is 150.51217525855952
At time: 966.342022895813 and batch: 550, loss is 5.010627813339234 and perplexity is 149.9988778906438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.056852213541666 and perplexity of 157.09523495906856
Finished 34 epochs...
Completing Train Step...
At time: 970.2391316890717 and batch: 50, loss is 5.070202226638794 and perplexity is 159.20651988748332
At time: 972.7050116062164 and batch: 100, loss is 5.055528421401977 and perplexity is 156.88741110998572
At time: 975.1477370262146 and batch: 150, loss is 5.082914619445801 and perplexity is 161.2433346706602
At time: 977.5907003879547 and batch: 200, loss is 5.097605428695679 and perplexity is 163.629615020768
At time: 980.0334348678589 and batch: 250, loss is 5.0764982128143314 and perplexity is 160.21204399716484
At time: 982.4797487258911 and batch: 300, loss is 5.061292104721069 and perplexity is 157.7942713815924
At time: 984.9251654148102 and batch: 350, loss is 5.023360557556153 and perplexity is 151.9209861238389
At time: 987.3692328929901 and batch: 400, loss is 5.048494443893433 and perplexity is 155.7877406515147
At time: 989.8100972175598 and batch: 450, loss is 5.008322429656983 and perplexity is 149.6534712256238
At time: 992.2785761356354 and batch: 500, loss is 5.010545244216919 and perplexity is 149.98649312625483
At time: 994.7200980186462 and batch: 550, loss is 5.0067079067230225 and perplexity is 149.4120472089322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.054914347330729 and perplexity of 156.79110019276277
Finished 35 epochs...
Completing Train Step...
At time: 998.6626851558685 and batch: 50, loss is 5.066534366607666 and perplexity is 158.62364226672094
At time: 1001.1082034111023 and batch: 100, loss is 5.051441049575805 and perplexity is 156.2474626704917
At time: 1003.5542712211609 and batch: 150, loss is 5.078170576095581 and perplexity is 160.480200902142
At time: 1005.9994201660156 and batch: 200, loss is 5.093471155166626 and perplexity is 162.95452190618482
At time: 1008.4471051692963 and batch: 250, loss is 5.072601709365845 and perplexity is 159.58899186593274
At time: 1010.9080376625061 and batch: 300, loss is 5.057043771743775 and perplexity is 157.12533072229027
At time: 1013.36172914505 and batch: 350, loss is 5.018897771835327 and perplexity is 151.24450593209875
At time: 1015.8073890209198 and batch: 400, loss is 5.044118337631225 and perplexity is 155.1074864619731
At time: 1018.2499732971191 and batch: 450, loss is 5.004465751647949 and perplexity is 149.07741751502797
At time: 1020.692302942276 and batch: 500, loss is 5.006849737167358 and perplexity is 149.433239888825
At time: 1023.1354396343231 and batch: 550, loss is 5.002927103042603 and perplexity is 148.84821612989398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.053442891438802 and perplexity of 156.5605586620264
Finished 36 epochs...
Completing Train Step...
At time: 1027.0327734947205 and batch: 50, loss is 5.062331943511963 and perplexity is 157.95843732422125
At time: 1029.5018825531006 and batch: 100, loss is 5.046252565383911 and perplexity is 155.43887466739463
At time: 1031.9461386203766 and batch: 150, loss is 5.073076753616333 and perplexity is 159.6648217087993
At time: 1034.3915300369263 and batch: 200, loss is 5.088440837860108 and perplexity is 162.13686720991615
At time: 1036.8380029201508 and batch: 250, loss is 5.068693056106567 and perplexity is 158.96643131200108
At time: 1039.2838952541351 and batch: 300, loss is 5.053153791427612 and perplexity is 156.515303544708
At time: 1041.729504585266 and batch: 350, loss is 5.0148670196533205 and perplexity is 150.63610379264568
At time: 1044.1728501319885 and batch: 400, loss is 5.0413339233398435 and perplexity is 154.67620367359115
At time: 1046.618690252304 and batch: 450, loss is 5.001407775878906 and perplexity is 148.62223870215198
At time: 1049.0638251304626 and batch: 500, loss is 5.003932342529297 and perplexity is 148.99791946551758
At time: 1051.5073902606964 and batch: 550, loss is 4.999039850234985 and perplexity is 148.27072863097317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.051468912760416 and perplexity of 156.2518162830416
Finished 37 epochs...
Completing Train Step...
At time: 1055.4531071186066 and batch: 50, loss is 5.057852535247803 and perplexity is 157.25245935690725
At time: 1057.8985192775726 and batch: 100, loss is 5.041195793151855 and perplexity is 154.65483969603952
At time: 1060.3431911468506 and batch: 150, loss is 5.068283433914185 and perplexity is 158.90132846858688
At time: 1062.7891681194305 and batch: 200, loss is 5.08420428276062 and perplexity is 161.45141843430127
At time: 1065.2358798980713 and batch: 250, loss is 5.06549674987793 and perplexity is 158.45913708319816
At time: 1067.6822628974915 and batch: 300, loss is 5.049812736511231 and perplexity is 155.99324991086596
At time: 1070.1275329589844 and batch: 350, loss is 5.011566028594971 and perplexity is 150.13967516497164
At time: 1072.5734565258026 and batch: 400, loss is 5.038167495727539 and perplexity is 154.18720726574384
At time: 1075.017431974411 and batch: 450, loss is 4.998220481872559 and perplexity is 148.14929004512075
At time: 1077.4590845108032 and batch: 500, loss is 5.000632572174072 and perplexity is 148.50707083713453
At time: 1079.9020812511444 and batch: 550, loss is 4.99562819480896 and perplexity is 147.76574190528146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.050306701660157 and perplexity of 156.07032417422238
Finished 38 epochs...
Completing Train Step...
At time: 1083.8018560409546 and batch: 50, loss is 5.053667144775391 and perplexity is 156.59567182665953
At time: 1086.269403219223 and batch: 100, loss is 5.037059059143067 and perplexity is 154.01639520900804
At time: 1088.7114267349243 and batch: 150, loss is 5.063717765808105 and perplexity is 158.1774913984812
At time: 1091.1535551548004 and batch: 200, loss is 5.080084991455078 and perplexity is 160.78772093025546
At time: 1093.5940473079681 and batch: 250, loss is 5.061622009277344 and perplexity is 157.8463370185485
At time: 1096.039936542511 and batch: 300, loss is 5.044818773269653 and perplexity is 155.21616733081157
At time: 1098.4822146892548 and batch: 350, loss is 5.007303009033203 and perplexity is 149.50098912554017
At time: 1100.925416469574 and batch: 400, loss is 5.03389253616333 and perplexity is 153.5294700909715
At time: 1103.368854045868 and batch: 450, loss is 4.9934355163574216 and perplexity is 147.44209410457992
At time: 1105.812524318695 and batch: 500, loss is 4.996071624755859 and perplexity is 147.8312801901132
At time: 1108.25546336174 and batch: 550, loss is 4.9906157207489015 and perplexity is 147.0269231588115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.047839864095052 and perplexity of 155.6857985119214
Finished 39 epochs...
Completing Train Step...
At time: 1112.2046570777893 and batch: 50, loss is 5.048391637802124 and perplexity is 155.77172554606372
At time: 1114.647142648697 and batch: 100, loss is 5.031409664154053 and perplexity is 153.14874890365755
At time: 1117.1121933460236 and batch: 150, loss is 5.05889386177063 and perplexity is 157.41629580242866
At time: 1119.5559346675873 and batch: 200, loss is 5.075289344787597 and perplexity is 160.0184857964106
At time: 1122.0022368431091 and batch: 250, loss is 5.056876440048217 and perplexity is 157.09904087390927
At time: 1124.4512045383453 and batch: 300, loss is 5.040193309783936 and perplexity is 154.49987847747573
At time: 1126.8969821929932 and batch: 350, loss is 5.002131586074829 and perplexity is 148.72985193494475
At time: 1129.3409414291382 and batch: 400, loss is 5.0294491577148435 and perplexity is 152.84879392318564
At time: 1131.784895658493 and batch: 450, loss is 4.989193630218506 and perplexity is 146.81798616254792
At time: 1134.224762916565 and batch: 500, loss is 4.991558284759521 and perplexity is 147.16557077703587
At time: 1136.6752264499664 and batch: 550, loss is 4.986291313171387 and perplexity is 146.39249157725632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.045597839355469 and perplexity of 155.3371380987421
Finished 40 epochs...
Completing Train Step...
At time: 1140.6124081611633 and batch: 50, loss is 5.044642200469971 and perplexity is 155.18876279710886
At time: 1143.084651708603 and batch: 100, loss is 5.027316331863403 and perplexity is 152.52314146758073
At time: 1145.531248807907 and batch: 150, loss is 5.054466285705566 and perplexity is 156.72086385387854
At time: 1147.9777233600616 and batch: 200, loss is 5.069800500869751 and perplexity is 159.14257537078487
At time: 1150.424031496048 and batch: 250, loss is 5.05212061882019 and perplexity is 156.3536797274706
At time: 1152.868637084961 and batch: 300, loss is 5.035905351638794 and perplexity is 153.8388077997358
At time: 1155.3148901462555 and batch: 350, loss is 4.997775115966797 and perplexity is 148.08332409295522
At time: 1157.7624776363373 and batch: 400, loss is 5.025781497955323 and perplexity is 152.28922333694896
At time: 1160.2069325447083 and batch: 450, loss is 4.985324611663819 and perplexity is 146.25104211567128
At time: 1162.6664249897003 and batch: 500, loss is 4.987798490524292 and perplexity is 146.6132973801514
At time: 1165.1101293563843 and batch: 550, loss is 4.9820039844512936 and perplexity is 145.76620235691982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.043729146321614 and perplexity of 155.04713172171853
Finished 41 epochs...
Completing Train Step...
At time: 1169.0241315364838 and batch: 50, loss is 5.040763702392578 and perplexity is 154.58802920406688
At time: 1171.4642159938812 and batch: 100, loss is 5.023302888870239 and perplexity is 151.91222529282163
At time: 1173.9207093715668 and batch: 150, loss is 5.049351720809937 and perplexity is 155.9213511478639
At time: 1176.3757133483887 and batch: 200, loss is 5.064866142272949 and perplexity is 158.35924304655478
At time: 1178.849988937378 and batch: 250, loss is 5.047840986251831 and perplexity is 155.68597321589363
At time: 1181.3057641983032 and batch: 300, loss is 5.031304807662964 and perplexity is 153.13269110512974
At time: 1183.7742009162903 and batch: 350, loss is 4.993602638244629 and perplexity is 147.46673696472376
At time: 1186.240421295166 and batch: 400, loss is 5.020589399337768 and perplexity is 151.50057182091334
At time: 1188.6931283473969 and batch: 450, loss is 4.978989973068237 and perplexity is 145.32752278844765
At time: 1191.1355471611023 and batch: 500, loss is 4.981198787689209 and perplexity is 145.6488791232434
At time: 1193.5765278339386 and batch: 550, loss is 4.975465793609619 and perplexity is 144.81626393099248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.036678568522135 and perplexity of 153.957804562464
Finished 42 epochs...
Completing Train Step...
At time: 1197.4716131687164 and batch: 50, loss is 5.033334283828736 and perplexity is 153.4437858248102
At time: 1199.9423100948334 and batch: 100, loss is 5.0137941360473635 and perplexity is 150.47457545246968
At time: 1202.384848356247 and batch: 150, loss is 5.0406804943084715 and perplexity is 154.57516676546788
At time: 1204.8313627243042 and batch: 200, loss is 5.0563012218475345 and perplexity is 157.00870063145558
At time: 1207.2757699489594 and batch: 250, loss is 5.040492725372315 and perplexity is 154.54614507561925
At time: 1209.7205657958984 and batch: 300, loss is 5.023656091690063 and perplexity is 151.9658905959712
At time: 1212.1644253730774 and batch: 350, loss is 4.986909990310669 and perplexity is 146.48308928763404
At time: 1214.6098845005035 and batch: 400, loss is 5.015075845718384 and perplexity is 150.66756382217991
At time: 1217.0562295913696 and batch: 450, loss is 4.973769636154175 and perplexity is 144.57084094211166
At time: 1219.4986357688904 and batch: 500, loss is 4.975488557815551 and perplexity is 144.81956059576987
At time: 1221.9418194293976 and batch: 550, loss is 4.969907293319702 and perplexity is 144.01353573522024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.032154846191406 and perplexity of 153.26291513241216
Finished 43 epochs...
Completing Train Step...
At time: 1225.877194404602 and batch: 50, loss is 5.028004293441772 and perplexity is 152.6281076309263
At time: 1228.3201489448547 and batch: 100, loss is 5.008663511276245 and perplexity is 149.70452398000077
At time: 1230.7622015476227 and batch: 150, loss is 5.034472064971924 and perplexity is 153.61847062855415
At time: 1233.2075850963593 and batch: 200, loss is 5.0505939483642575 and perplexity is 156.11516129979935
At time: 1235.651995897293 and batch: 250, loss is 5.035794372558594 and perplexity is 153.82173585767882
At time: 1238.0970153808594 and batch: 300, loss is 5.018000888824463 and perplexity is 151.10891811653468
At time: 1240.5684673786163 and batch: 350, loss is 4.980615005493164 and perplexity is 145.56387671459788
At time: 1243.013420343399 and batch: 400, loss is 5.008268690109253 and perplexity is 149.64542913185508
At time: 1245.4593880176544 and batch: 450, loss is 4.967732629776001 and perplexity is 143.70069503418065
At time: 1247.9008708000183 and batch: 500, loss is 4.968591594696045 and perplexity is 143.82418191805908
At time: 1250.344221830368 and batch: 550, loss is 4.963656377792359 and perplexity is 143.1161270236763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.027367146809896 and perplexity of 152.53089211977607
Finished 44 epochs...
Completing Train Step...
At time: 1254.2549662590027 and batch: 50, loss is 5.0209982967376705 and perplexity is 151.5625326777679
At time: 1256.7274250984192 and batch: 100, loss is 5.000905408859253 and perplexity is 148.54759454199288
At time: 1259.1718156337738 and batch: 150, loss is 5.027447767257691 and perplexity is 152.54318972431386
At time: 1261.6176812648773 and batch: 200, loss is 5.0430770587921145 and perplexity is 154.94606037790098
At time: 1264.0609645843506 and batch: 250, loss is 5.0292554950714115 and perplexity is 152.81919568783783
At time: 1266.5055389404297 and batch: 300, loss is 5.011204442977905 and perplexity is 150.08539663164092
At time: 1268.948199748993 and batch: 350, loss is 4.974257459640503 and perplexity is 144.6413831984473
At time: 1271.3924298286438 and batch: 400, loss is 5.001851673126221 and perplexity is 148.68822634958553
At time: 1273.8358421325684 and batch: 450, loss is 4.9615367412567135 and perplexity is 142.81309412525542
At time: 1276.2806215286255 and batch: 500, loss is 4.962569656372071 and perplexity is 142.96068413970457
At time: 1278.7249116897583 and batch: 550, loss is 4.9582413482666015 and perplexity is 142.34324345248527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.024530029296875 and perplexity of 152.09875735288674
Finished 45 epochs...
Completing Train Step...
At time: 1282.6569962501526 and batch: 50, loss is 5.016545333862305 and perplexity is 150.88913077602507
At time: 1285.1011474132538 and batch: 100, loss is 4.99617567062378 and perplexity is 147.84666222417104
At time: 1287.5451152324677 and batch: 150, loss is 5.023635787963867 and perplexity is 151.96280515346044
At time: 1289.9890551567078 and batch: 200, loss is 5.039036674499512 and perplexity is 154.32128177212564
At time: 1292.433354139328 and batch: 250, loss is 5.025471029281616 and perplexity is 152.24194964264004
At time: 1294.8776848316193 and batch: 300, loss is 5.007088603973389 and perplexity is 149.46893879302397
At time: 1297.3206849098206 and batch: 350, loss is 4.9695722770690915 and perplexity is 143.9652969412822
At time: 1299.7635834217072 and batch: 400, loss is 4.997927913665771 and perplexity is 148.10595261288196
At time: 1302.232782125473 and batch: 450, loss is 4.957159643173218 and perplexity is 142.18935328792674
At time: 1304.6745791435242 and batch: 500, loss is 4.957315368652344 and perplexity is 142.21149751725974
At time: 1307.1211650371552 and batch: 550, loss is 4.9536058044433595 and perplexity is 141.6849321039021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022853088378906 and perplexity of 151.843910464419
Finished 46 epochs...
Completing Train Step...
At time: 1311.0262467861176 and batch: 50, loss is 5.012528991699218 and perplexity is 150.28432376709495
At time: 1313.4966435432434 and batch: 100, loss is 4.99143310546875 and perplexity is 147.1471498482435
At time: 1315.9394187927246 and batch: 150, loss is 5.01940426826477 and perplexity is 151.32113013763137
At time: 1318.3829457759857 and batch: 200, loss is 5.034016199111939 and perplexity is 153.54845717191625
At time: 1320.8323831558228 and batch: 250, loss is 5.021174144744873 and perplexity is 151.58918699059112
At time: 1323.2817850112915 and batch: 300, loss is 5.002091417312622 and perplexity is 148.72387776087768
At time: 1325.721878528595 and batch: 350, loss is 4.965649929046631 and perplexity is 143.40172093667763
At time: 1328.1623318195343 and batch: 400, loss is 4.9943352222442625 and perplexity is 147.57480831754094
At time: 1330.601345539093 and batch: 450, loss is 4.952193403244019 and perplexity is 141.48495739137573
At time: 1333.0413074493408 and batch: 500, loss is 4.952466411590576 and perplexity is 141.52358923882431
At time: 1335.4984686374664 and batch: 550, loss is 4.949755210876464 and perplexity is 141.1404100551712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.019878133138021 and perplexity of 151.39285289789046
Finished 47 epochs...
Completing Train Step...
At time: 1339.4238953590393 and batch: 50, loss is 5.009522085189819 and perplexity is 149.83311157211116
At time: 1341.8721799850464 and batch: 100, loss is 4.988269720077515 and perplexity is 146.6824021796055
At time: 1344.3136973381042 and batch: 150, loss is 5.015635929107666 and perplexity is 150.75197385810327
At time: 1346.7591688632965 and batch: 200, loss is 5.0301462745666505 and perplexity is 152.9553845419495
At time: 1349.1996397972107 and batch: 250, loss is 5.018083848953247 and perplexity is 151.1214546518512
At time: 1351.642912864685 and batch: 300, loss is 4.998979902267456 and perplexity is 148.2618403685669
At time: 1354.0990331172943 and batch: 350, loss is 4.9627210140228275 and perplexity is 142.98232397064223
At time: 1356.5574505329132 and batch: 400, loss is 4.990818729400635 and perplexity is 147.05677392613
At time: 1359.0216596126556 and batch: 450, loss is 4.9488139152526855 and perplexity is 141.00761771307808
At time: 1361.4901225566864 and batch: 500, loss is 4.94886342048645 and perplexity is 141.01459850094693
At time: 1364.0540645122528 and batch: 550, loss is 4.946580629348755 and perplexity is 140.693058768861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.019275919596354 and perplexity of 151.30170951840697
Finished 48 epochs...
Completing Train Step...
At time: 1368.1340000629425 and batch: 50, loss is 5.005734024047851 and perplexity is 149.26660823643644
At time: 1370.6601841449738 and batch: 100, loss is 4.984754791259766 and perplexity is 146.16772902676306
At time: 1373.1164186000824 and batch: 150, loss is 5.0115659809112545 and perplexity is 150.1396680057541
At time: 1375.5621972084045 and batch: 200, loss is 5.025837287902832 and perplexity is 152.29771978173113
At time: 1378.005954504013 and batch: 250, loss is 5.014306125640869 and perplexity is 150.55163659475656
At time: 1380.450614452362 and batch: 300, loss is 4.995816926956177 and perplexity is 147.79363268289674
At time: 1382.8942322731018 and batch: 350, loss is 4.959885082244873 and perplexity is 142.5774102795868
At time: 1385.338135957718 and batch: 400, loss is 4.986961584091187 and perplexity is 146.4906470989587
At time: 1387.7788724899292 and batch: 450, loss is 4.9449442100524905 and perplexity is 140.46301420865552
At time: 1390.2233486175537 and batch: 500, loss is 4.944800109863281 and perplexity is 140.4427749200088
At time: 1392.667079925537 and batch: 550, loss is 4.942980117797852 and perplexity is 140.18740264225778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.018207295735677 and perplexity of 151.14011126070957
Finished 49 epochs...
Completing Train Step...
At time: 1396.6553885936737 and batch: 50, loss is 5.00305365562439 and perplexity is 148.8670544479334
At time: 1399.1003637313843 and batch: 100, loss is 4.982030067443848 and perplexity is 145.770004425275
At time: 1401.5438876152039 and batch: 150, loss is 5.007829351425171 and perplexity is 149.57969854599327
At time: 1403.9886360168457 and batch: 200, loss is 5.022386226654053 and perplexity is 151.7730368998357
At time: 1406.4332611560822 and batch: 250, loss is 5.0120413208007815 and perplexity is 150.21105234352507
At time: 1408.8773908615112 and batch: 300, loss is 4.992680788040161 and perplexity is 147.33085736304255
At time: 1411.3225338459015 and batch: 350, loss is 4.9568125343322755 and perplexity is 142.1400066711316
At time: 1413.766898393631 and batch: 400, loss is 4.9832259464263915 and perplexity is 145.94443198619174
At time: 1416.217852115631 and batch: 450, loss is 4.941628561019898 and perplexity is 139.9980593909331
At time: 1418.663180589676 and batch: 500, loss is 4.940792741775513 and perplexity is 139.88109520598516
At time: 1421.1063921451569 and batch: 550, loss is 4.939202365875244 and perplexity is 139.65880848983684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.016680399576823 and perplexity of 150.9095121006645
Finished Training.
Improved accuracyfrom -216.26841192058583 to -150.9095121006645
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe57aa7feb8>
SETTINGS FOR THIS RUN
{'lr': 27.62545047311539, 'tune_wordvecs': True, 'anneal': 7.04416115551752, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.7579908248075645}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.900618076324463 and batch: 50, loss is 7.547633867263794 and perplexity is 1896.2506382798529
At time: 5.323408126831055 and batch: 100, loss is 6.571260795593262 and perplexity is 714.2698239141396
At time: 7.781339406967163 and batch: 150, loss is 6.5903909969329835 and perplexity is 728.065485625155
At time: 10.220874786376953 and batch: 200, loss is 6.651028776168824 and perplexity is 773.57975672307
At time: 12.653508424758911 and batch: 250, loss is 6.6429143905639645 and perplexity is 767.3280310401418
At time: 15.079151391983032 and batch: 300, loss is 6.691644411087037 and perplexity is 805.6459766482941
At time: 17.504050970077515 and batch: 350, loss is 6.6796090793609615 and perplexity is 796.0078752781535
At time: 19.935059309005737 and batch: 400, loss is 6.758871183395386 and perplexity is 861.6689803490056
At time: 22.360783576965332 and batch: 450, loss is 6.772045917510987 and perplexity is 873.0963510557526
At time: 24.78790783882141 and batch: 500, loss is 6.7886526489257815 and perplexity is 887.716689672773
At time: 27.205316066741943 and batch: 550, loss is 6.847568073272705 and perplexity is 941.5882463881209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.603879292805989 and perplexity of 737.9523770722844
Finished 1 epochs...
Completing Train Step...
At time: 31.10570740699768 and batch: 50, loss is 7.1303332233428955 and perplexity is 1249.2931912150107
At time: 33.5301775932312 and batch: 100, loss is 7.376433210372925 and perplexity is 1597.88028879083
At time: 35.93946123123169 and batch: 150, loss is 7.315483932495117 and perplexity is 1503.3991637366114
At time: 38.36473870277405 and batch: 200, loss is 7.445459051132202 and perplexity is 1712.0710399389118
At time: 40.77160835266113 and batch: 250, loss is 7.488224897384644 and perplexity is 1786.8773842433186
At time: 43.192885637283325 and batch: 300, loss is 7.47039041519165 and perplexity is 1755.2918444200745
At time: 45.63004112243652 and batch: 350, loss is 7.514336853027344 and perplexity is 1834.1507613830765
At time: 48.067505836486816 and batch: 400, loss is 7.485105257034302 and perplexity is 1781.311655504175
At time: 50.50293803215027 and batch: 450, loss is 7.540782642364502 and perplexity is 1883.3034015513576
At time: 52.93687891960144 and batch: 500, loss is 7.318082571029663 and perplexity is 1507.3110353050215
At time: 55.36943459510803 and batch: 550, loss is 7.239082622528076 and perplexity is 1392.815646906291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.684487915039062 and perplexity of 799.9009560284817
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.26690435409546 and batch: 50, loss is 6.530261678695679 and perplexity is 685.5775890704344
At time: 61.67349123954773 and batch: 100, loss is 6.485522747039795 and perplexity is 655.5815780103595
At time: 64.084805727005 and batch: 150, loss is 6.54248685836792 and perplexity is 694.01033919263
At time: 66.49613928794861 and batch: 200, loss is 6.557947731018066 and perplexity is 704.8237214271553
At time: 68.90652084350586 and batch: 250, loss is 6.500278015136718 and perplexity is 665.326578193994
At time: 71.31760573387146 and batch: 300, loss is 6.513843221664429 and perplexity is 674.4133633527392
At time: 73.72449994087219 and batch: 350, loss is 6.49326416015625 and perplexity is 660.676400957723
At time: 76.13334012031555 and batch: 400, loss is 6.514604940414428 and perplexity is 674.9272723590702
At time: 78.58793377876282 and batch: 450, loss is 6.507457933425903 and perplexity is 670.1207589787249
At time: 81.0010359287262 and batch: 500, loss is 6.491685256958008 and perplexity is 659.6340799537679
At time: 83.41047525405884 and batch: 550, loss is 6.468028545379639 and perplexity is 644.2124386445377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.124905395507812 and perplexity of 457.1014673711142
Finished 3 epochs...
Completing Train Step...
At time: 87.2974648475647 and batch: 50, loss is 6.52946967124939 and perplexity is 685.0348214812403
At time: 89.73524975776672 and batch: 100, loss is 6.534892635345459 and perplexity is 688.7598318926275
At time: 92.14412093162537 and batch: 150, loss is 6.571917743682861 and perplexity is 714.7392162767444
At time: 94.55336141586304 and batch: 200, loss is 6.577880268096924 and perplexity is 719.0135966888145
At time: 96.96123909950256 and batch: 250, loss is 6.513779611587524 and perplexity is 674.3704652312218
At time: 99.3725574016571 and batch: 300, loss is 6.517213659286499 and perplexity is 676.6902664499013
At time: 101.78378534317017 and batch: 350, loss is 6.493214225769043 and perplexity is 660.6434113101647
At time: 104.19613909721375 and batch: 400, loss is 6.542584428787231 and perplexity is 694.0780573760233
At time: 106.60722303390503 and batch: 450, loss is 6.542436227798462 and perplexity is 693.975201943464
At time: 109.04372572898865 and batch: 500, loss is 6.506961574554444 and perplexity is 669.7882211309393
At time: 111.48054337501526 and batch: 550, loss is 6.474202861785889 and perplexity is 648.2023147771791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.117697143554688 and perplexity of 453.8184115881076
Finished 4 epochs...
Completing Train Step...
At time: 115.43139362335205 and batch: 50, loss is 6.516749105453491 and perplexity is 676.3759803999096
At time: 117.87058305740356 and batch: 100, loss is 6.509472255706787 and perplexity is 671.4719585732694
At time: 120.3113124370575 and batch: 150, loss is 6.562208337783813 and perplexity is 707.8331044898607
At time: 122.75087451934814 and batch: 200, loss is 6.576341924667358 and perplexity is 717.9083571835077
At time: 125.19013333320618 and batch: 250, loss is 6.518486394882202 and perplexity is 677.552062542227
At time: 127.6303288936615 and batch: 300, loss is 6.5081517696380615 and perplexity is 670.5858743661134
At time: 130.11035013198853 and batch: 350, loss is 6.486570167541504 and perplexity is 656.2686073372001
At time: 132.55225944519043 and batch: 400, loss is 6.531042375564575 and perplexity is 686.1130263275073
At time: 134.99343633651733 and batch: 450, loss is 6.528390054702759 and perplexity is 684.2956456379831
At time: 137.43383526802063 and batch: 500, loss is 6.502872772216797 and perplexity is 667.0551807252999
At time: 139.8729543685913 and batch: 550, loss is 6.469255056381225 and perplexity is 645.0030570398617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.120282999674479 and perplexity of 454.99343927650597
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 143.81347155570984 and batch: 50, loss is 6.513560571670532 and perplexity is 674.2227673569514
At time: 146.27927899360657 and batch: 100, loss is 6.501040182113647 and perplexity is 665.8338614334766
At time: 148.72040057182312 and batch: 150, loss is 6.536425647735595 and perplexity is 689.8165189990146
At time: 151.16122579574585 and batch: 200, loss is 6.520574913024903 and perplexity is 678.9686210566415
At time: 153.60282707214355 and batch: 250, loss is 6.437020416259766 and perplexity is 624.5431461308809
At time: 156.04474878311157 and batch: 300, loss is 6.43327488899231 and perplexity is 622.208278132871
At time: 158.4876389503479 and batch: 350, loss is 6.374414720535278 and perplexity is 586.6419809398087
At time: 160.93168091773987 and batch: 400, loss is 6.385725698471069 and perplexity is 593.3151442982795
At time: 163.37303733825684 and batch: 450, loss is 6.38397364616394 and perplexity is 592.2765352452346
At time: 165.80827522277832 and batch: 500, loss is 6.376943693161011 and perplexity is 588.1274600269613
At time: 168.2440197467804 and batch: 550, loss is 6.383811902999878 and perplexity is 592.1807463112358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.029723612467448 and perplexity of 415.6001468075573
Finished 6 epochs...
Completing Train Step...
At time: 172.19899535179138 and batch: 50, loss is 6.449416437149048 and perplexity is 632.3331789694942
At time: 174.6619210243225 and batch: 100, loss is 6.443783712387085 and perplexity is 628.7814326100262
At time: 177.11976385116577 and batch: 150, loss is 6.484121160507202 and perplexity is 654.6633673257836
At time: 179.582905292511 and batch: 200, loss is 6.474402704238892 and perplexity is 648.3318660622984
At time: 182.0313365459442 and batch: 250, loss is 6.402441673278808 and perplexity is 603.3163423104392
At time: 184.47399306297302 and batch: 300, loss is 6.402890148162842 and perplexity is 603.5869752185876
At time: 186.91577339172363 and batch: 350, loss is 6.356461915969849 and perplexity is 576.204087185146
At time: 189.35633301734924 and batch: 400, loss is 6.38626727104187 and perplexity is 593.6365545318996
At time: 191.8486967086792 and batch: 450, loss is 6.393634214401245 and perplexity is 598.0259899142292
At time: 194.28922033309937 and batch: 500, loss is 6.378628025054931 and perplexity is 589.1188965852932
At time: 196.72996282577515 and batch: 550, loss is 6.371570739746094 and perplexity is 584.975952615096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.02279052734375 and perplexity of 412.7287210338628
Finished 7 epochs...
Completing Train Step...
At time: 200.65657424926758 and batch: 50, loss is 6.426577854156494 and perplexity is 618.0552496293908
At time: 203.12866282463074 and batch: 100, loss is 6.418230524063111 and perplexity is 612.9176110589392
At time: 205.5706913471222 and batch: 150, loss is 6.458987236022949 and perplexity is 638.4141661812567
At time: 208.01123690605164 and batch: 200, loss is 6.453933811187744 and perplexity is 635.1961260888733
At time: 210.45145416259766 and batch: 250, loss is 6.389847440719604 and perplexity is 595.7656831707303
At time: 212.89422130584717 and batch: 300, loss is 6.394552745819092 and perplexity is 598.5755479291673
At time: 215.33561253547668 and batch: 350, loss is 6.358155593872071 and perplexity is 577.1808182151793
At time: 217.77426719665527 and batch: 400, loss is 6.394667348861694 and perplexity is 598.6441504391405
At time: 220.2153205871582 and batch: 450, loss is 6.401827087402344 and perplexity is 602.9456665251337
At time: 222.65468907356262 and batch: 500, loss is 6.3813959407806395 and perplexity is 590.7517868522748
At time: 225.09350109100342 and batch: 550, loss is 6.366256513595581 and perplexity is 581.8755036504157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.019561258951823 and perplexity of 411.3980589095216
Finished 8 epochs...
Completing Train Step...
At time: 229.03948283195496 and batch: 50, loss is 6.413755950927734 and perplexity is 610.1811930991081
At time: 231.48073840141296 and batch: 100, loss is 6.402946510314941 and perplexity is 603.620995638213
At time: 233.92378401756287 and batch: 150, loss is 6.446230115890503 and perplexity is 630.3215688388519
At time: 236.3656234741211 and batch: 200, loss is 6.446675481796265 and perplexity is 630.6023550969509
At time: 238.80624175071716 and batch: 250, loss is 6.389787178039551 and perplexity is 595.7297818157456
At time: 241.24813675880432 and batch: 300, loss is 6.397482442855835 and perplexity is 600.3317642729595
At time: 243.6903109550476 and batch: 350, loss is 6.36304235458374 and perplexity is 580.0082656639967
At time: 246.1312654018402 and batch: 400, loss is 6.399406871795654 and perplexity is 601.4881724494104
At time: 248.57631254196167 and batch: 450, loss is 6.4050768375396725 and perplexity is 604.9082765565953
At time: 251.02264785766602 and batch: 500, loss is 6.382073602676392 and perplexity is 591.1522525029026
At time: 253.49933433532715 and batch: 550, loss is 6.363351697921753 and perplexity is 580.1877151112867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018761189778646 and perplexity of 411.0690436397211
Finished 9 epochs...
Completing Train Step...
At time: 257.42713928222656 and batch: 50, loss is 6.407388515472412 and perplexity is 606.3082471880139
At time: 259.8953802585602 and batch: 100, loss is 6.395755605697632 and perplexity is 599.2959836447903
At time: 262.3360183238983 and batch: 150, loss is 6.441741018295288 and perplexity is 627.4983354262746
At time: 264.7744517326355 and batch: 200, loss is 6.446130208969116 and perplexity is 630.258598497064
At time: 267.21234488487244 and batch: 250, loss is 6.392186660766601 and perplexity is 597.1609414719785
At time: 269.6538755893707 and batch: 300, loss is 6.4005351257324214 and perplexity is 602.1671868261875
At time: 272.0953645706177 and batch: 350, loss is 6.365710163116455 and perplexity is 581.5576825189743
At time: 274.5346133708954 and batch: 400, loss is 6.401160135269165 and perplexity is 602.5436646995429
At time: 276.9744381904602 and batch: 450, loss is 6.405880393981934 and perplexity is 605.3945498470515
At time: 279.41645646095276 and batch: 500, loss is 6.381490621566773 and perplexity is 590.8077223438298
At time: 281.8558850288391 and batch: 550, loss is 6.361195373535156 and perplexity is 578.9379900829066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.0184076944986975 and perplexity of 410.92375835340596
Finished 10 epochs...
Completing Train Step...
At time: 285.8023293018341 and batch: 50, loss is 6.404475450515747 and perplexity is 604.5446019339693
At time: 288.250125169754 and batch: 100, loss is 6.393449783325195 and perplexity is 597.9157055076523
At time: 290.69569873809814 and batch: 150, loss is 6.440729560852051 and perplexity is 626.863968435978
At time: 293.13507604599 and batch: 200, loss is 6.4464600563049315 and perplexity is 630.4665219062592
At time: 295.5707561969757 and batch: 250, loss is 6.393350210189819 and perplexity is 597.856172130176
At time: 298.0068521499634 and batch: 300, loss is 6.401773233413696 and perplexity is 602.9131963703848
At time: 300.4566240310669 and batch: 350, loss is 6.366832914352417 and perplexity is 582.2109938102068
At time: 302.9084703922272 and batch: 400, loss is 6.401825304031372 and perplexity is 602.9445912502932
At time: 305.3444719314575 and batch: 450, loss is 6.406144618988037 and perplexity is 605.5545313603071
At time: 307.78028750419617 and batch: 500, loss is 6.381094636917115 and perplexity is 590.5738178692294
At time: 310.216331243515 and batch: 550, loss is 6.3599391746521 and perplexity is 578.2111854275806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.01836191813151 and perplexity of 410.90494818709135
Finished 11 epochs...
Completing Train Step...
At time: 314.1398277282715 and batch: 50, loss is 6.402922601699829 and perplexity is 603.6065640686745
At time: 316.59739995002747 and batch: 100, loss is 6.39227725982666 and perplexity is 597.2150461428588
At time: 319.039705991745 and batch: 150, loss is 6.440208282470703 and perplexity is 626.5372829556229
At time: 321.4777374267578 and batch: 200, loss is 6.446596994400024 and perplexity is 630.5528627023259
At time: 323.9170482158661 and batch: 250, loss is 6.393998479843139 and perplexity is 598.2438697963563
At time: 326.35658407211304 and batch: 300, loss is 6.402423591613769 and perplexity is 603.3054334450504
At time: 328.7953474521637 and batch: 350, loss is 6.367405681610108 and perplexity is 582.5445607235124
At time: 331.2333860397339 and batch: 400, loss is 6.4021986293792725 and perplexity is 603.1697277715567
At time: 333.6717574596405 and batch: 450, loss is 6.406328887939453 and perplexity is 605.6661265402736
At time: 336.10984683036804 and batch: 500, loss is 6.380820655822754 and perplexity is 590.4120339721857
At time: 338.5504741668701 and batch: 550, loss is 6.3591227531433105 and perplexity is 577.7393140283652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018333943684896 and perplexity of 410.8934535093341
Finished 12 epochs...
Completing Train Step...
At time: 342.4870021343231 and batch: 50, loss is 6.40193772315979 and perplexity is 603.0123775658938
At time: 344.92471051216125 and batch: 100, loss is 6.3915423393249515 and perplexity is 596.7763018023727
At time: 347.36360239982605 and batch: 150, loss is 6.439867420196533 and perplexity is 626.3237564261391
At time: 349.8011567592621 and batch: 200, loss is 6.446675748825073 and perplexity is 630.6025234859685
At time: 352.2386441230774 and batch: 250, loss is 6.394406452178955 and perplexity is 598.4879865383572
At time: 354.67742562294006 and batch: 300, loss is 6.402791700363159 and perplexity is 603.5275563338372
At time: 357.11575078964233 and batch: 350, loss is 6.36776138305664 and perplexity is 582.7518095235954
At time: 359.55244421958923 and batch: 400, loss is 6.402426271438599 and perplexity is 603.3070502000971
At time: 361.9898090362549 and batch: 450, loss is 6.406454458236694 and perplexity is 605.7421849910533
At time: 364.4292297363281 and batch: 500, loss is 6.380651140213013 and perplexity is 590.3119583986738
At time: 366.86772084236145 and batch: 550, loss is 6.358550910949707 and perplexity is 577.4090327550844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018318684895833 and perplexity of 410.8871838206339
Finished 13 epochs...
Completing Train Step...
At time: 370.7937066555023 and batch: 50, loss is 6.401230220794678 and perplexity is 602.5858957688039
At time: 373.24860978126526 and batch: 100, loss is 6.3910136413574214 and perplexity is 596.4608707757584
At time: 375.6878182888031 and batch: 150, loss is 6.439593610763549 and perplexity is 626.1522865496339
At time: 378.1687812805176 and batch: 200, loss is 6.446690950393677 and perplexity is 630.6121097063536
At time: 380.6086337566376 and batch: 250, loss is 6.39468638420105 and perplexity is 598.6555459421556
At time: 383.04865193367004 and batch: 300, loss is 6.40302433013916 and perplexity is 603.6679711457773
At time: 385.4868450164795 and batch: 350, loss is 6.367981252670288 and perplexity is 582.8799530257226
At time: 387.9233295917511 and batch: 400, loss is 6.402575626373291 and perplexity is 603.3971638144678
At time: 390.361634016037 and batch: 450, loss is 6.406542406082154 and perplexity is 605.7954610538504
At time: 392.7992343902588 and batch: 500, loss is 6.380515623092651 and perplexity is 590.2319664422187
At time: 395.2355766296387 and batch: 550, loss is 6.358106594085694 and perplexity is 577.1525371712743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.01830088297526 and perplexity of 410.87986930472937
Finished 14 epochs...
Completing Train Step...
At time: 399.16569781303406 and batch: 50, loss is 6.4006919670104985 and perplexity is 602.2616389041841
At time: 401.6028287410736 and batch: 100, loss is 6.3906089782714846 and perplexity is 596.2195539084569
At time: 404.04069924354553 and batch: 150, loss is 6.439377737045288 and perplexity is 626.0171313160951
At time: 406.4765350818634 and batch: 200, loss is 6.446683139801025 and perplexity is 630.6071842712788
At time: 408.91363048553467 and batch: 250, loss is 6.394872007369995 and perplexity is 598.7666805959641
At time: 411.35160398483276 and batch: 300, loss is 6.403168725967407 and perplexity is 603.7551445760455
At time: 413.78889417648315 and batch: 350, loss is 6.3681200504302975 and perplexity is 582.9608610723551
At time: 416.2244141101837 and batch: 400, loss is 6.402643671035767 and perplexity is 603.4382231677378
At time: 418.6620066165924 and batch: 450, loss is 6.406601524353027 and perplexity is 605.8312756926501
At time: 421.0976014137268 and batch: 500, loss is 6.380381507873535 and perplexity is 590.152812660692
At time: 423.5353720188141 and batch: 550, loss is 6.357762727737427 and perplexity is 576.9541079544446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018290710449219 and perplexity of 410.875689639818
Finished 15 epochs...
Completing Train Step...
At time: 427.44627928733826 and batch: 50, loss is 6.4002515411376955 and perplexity is 601.996445699475
At time: 429.9020109176636 and batch: 100, loss is 6.39027359008789 and perplexity is 596.0196224444475
At time: 432.3433437347412 and batch: 150, loss is 6.439186496734619 and perplexity is 625.8974230523055
At time: 434.787837266922 and batch: 200, loss is 6.446654109954834 and perplexity is 630.5888781074264
At time: 437.22875809669495 and batch: 250, loss is 6.3949972343444825 and perplexity is 598.8416670308634
At time: 439.69507908821106 and batch: 300, loss is 6.403251972198486 and perplexity is 603.8054070083762
At time: 442.135760307312 and batch: 350, loss is 6.368205862045288 and perplexity is 583.0108880317364
At time: 444.5748219490051 and batch: 400, loss is 6.402685947418213 and perplexity is 603.463734892111
At time: 447.0156397819519 and batch: 450, loss is 6.406641817092895 and perplexity is 605.8556867864372
At time: 449.4554967880249 and batch: 500, loss is 6.380269241333008 and perplexity is 590.0865619649701
At time: 451.89326453208923 and batch: 550, loss is 6.357471914291382 and perplexity is 576.7863463369497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018277486165364 and perplexity of 410.8702561389964
Finished 16 epochs...
Completing Train Step...
At time: 455.85578298568726 and batch: 50, loss is 6.399882869720459 and perplexity is 601.7745477229018
At time: 458.29399156570435 and batch: 100, loss is 6.389984588623047 and perplexity is 595.847396788418
At time: 460.73324966430664 and batch: 150, loss is 6.439019336700439 and perplexity is 625.79280676175
At time: 463.1711919307709 and batch: 200, loss is 6.446613378524781 and perplexity is 630.563193843727
At time: 465.60995841026306 and batch: 250, loss is 6.395088357925415 and perplexity is 598.8962381142933
At time: 468.0495448112488 and batch: 300, loss is 6.403308115005493 and perplexity is 603.8393072904312
At time: 470.4892489910126 and batch: 350, loss is 6.368256635665894 and perplexity is 583.0404903568766
At time: 472.9273226261139 and batch: 400, loss is 6.40269778251648 and perplexity is 603.4708769869775
At time: 475.36713194847107 and batch: 450, loss is 6.406664190292358 and perplexity is 605.8692418681982
At time: 477.80753898620605 and batch: 500, loss is 6.380163202285766 and perplexity is 590.0239930655811
At time: 480.2467300891876 and batch: 550, loss is 6.357208452224731 and perplexity is 576.6344050303924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018260701497396 and perplexity of 410.86335987604497
Finished 17 epochs...
Completing Train Step...
At time: 484.1611592769623 and batch: 50, loss is 6.399548034667969 and perplexity is 601.5730862406931
At time: 486.6186616420746 and batch: 100, loss is 6.38970308303833 and perplexity is 595.6796860254485
At time: 489.06397223472595 and batch: 150, loss is 6.438850030899048 and perplexity is 625.6868653775949
At time: 491.5119812488556 and batch: 200, loss is 6.4465491771698 and perplexity is 630.5227121317856
At time: 493.9518084526062 and batch: 250, loss is 6.395152826309204 and perplexity is 598.9348492314065
At time: 496.3919577598572 and batch: 300, loss is 6.4033318042755125 and perplexity is 603.8536119722629
At time: 498.83080101013184 and batch: 350, loss is 6.368228702545166 and perplexity is 583.0242044439296
At time: 501.30944442749023 and batch: 400, loss is 6.4026648139953615 and perplexity is 603.4509817725849
At time: 503.7462270259857 and batch: 450, loss is 6.4066462898254395 and perplexity is 605.858396622945
At time: 506.1828887462616 and batch: 500, loss is 6.379980535507202 and perplexity is 589.9162251266029
At time: 508.6283142566681 and batch: 550, loss is 6.356875953674316 and perplexity is 576.4427067980583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018230183919271 and perplexity of 410.85082151268244
Finished 18 epochs...
Completing Train Step...
At time: 512.6122963428497 and batch: 50, loss is 6.399200296401977 and perplexity is 601.3639326262816
At time: 515.0501277446747 and batch: 100, loss is 6.389337863922119 and perplexity is 595.4621721395058
At time: 517.4902889728546 and batch: 150, loss is 6.438684720993042 and perplexity is 625.5834416894054
At time: 519.9323258399963 and batch: 200, loss is 6.446456537246704 and perplexity is 630.4643032617622
At time: 522.3802394866943 and batch: 250, loss is 6.395114145278931 and perplexity is 598.9116822624358
At time: 524.8288416862488 and batch: 300, loss is 6.403250789642334 and perplexity is 603.8046929749993
At time: 527.2775757312775 and batch: 350, loss is 6.368122100830078 and perplexity is 582.9620563764021
At time: 529.7172801494598 and batch: 400, loss is 6.4026071739196775 and perplexity is 603.4161998147509
At time: 532.1560897827148 and batch: 450, loss is 6.406624240875244 and perplexity is 605.8450382286021
At time: 534.5951607227325 and batch: 500, loss is 6.3798040008544925 and perplexity is 589.8120936623488
At time: 537.032280921936 and batch: 550, loss is 6.356568460464477 and perplexity is 576.2654818289907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018216959635416 and perplexity of 410.84538834072185
Finished 19 epochs...
Completing Train Step...
At time: 541.0246648788452 and batch: 50, loss is 6.398785371780395 and perplexity is 601.114463683086
At time: 543.4904589653015 and batch: 100, loss is 6.3890355110168455 and perplexity is 595.2821596368019
At time: 545.9274051189423 and batch: 150, loss is 6.43853009223938 and perplexity is 625.4867159799862
At time: 548.3649859428406 and batch: 200, loss is 6.4463354682922365 and perplexity is 630.3879782281261
At time: 550.8283395767212 and batch: 250, loss is 6.395072326660157 and perplexity is 598.886637126796
At time: 553.2945303916931 and batch: 300, loss is 6.403246746063233 and perplexity is 603.802251447898
At time: 555.747062921524 and batch: 350, loss is 6.368030204772949 and perplexity is 582.9084869234136
At time: 558.1916074752808 and batch: 400, loss is 6.402562265396118 and perplexity is 603.3891018925938
At time: 560.6304366588593 and batch: 450, loss is 6.406516752243042 and perplexity is 605.7799202738992
At time: 563.1144905090332 and batch: 500, loss is 6.379649085998535 and perplexity is 589.7207300837877
At time: 565.5510542392731 and batch: 550, loss is 6.356285696029663 and perplexity is 576.1025574814093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018166097005208 and perplexity of 410.82449219508294
Finished 20 epochs...
Completing Train Step...
At time: 569.4764831066132 and batch: 50, loss is 6.398370714187622 and perplexity is 600.8652586775386
At time: 571.9155166149139 and batch: 100, loss is 6.388720865249634 and perplexity is 595.0948860889353
At time: 574.3550493717194 and batch: 150, loss is 6.438344841003418 and perplexity is 625.3708545248434
At time: 576.7933356761932 and batch: 200, loss is 6.44617205619812 and perplexity is 630.2849736248261
At time: 579.2368950843811 and batch: 250, loss is 6.394956769943238 and perplexity is 598.8174357516219
At time: 581.678064584732 and batch: 300, loss is 6.403200283050537 and perplexity is 603.7741976279607
At time: 584.1172897815704 and batch: 350, loss is 6.367826089859009 and perplexity is 582.7895187497728
At time: 586.5567448139191 and batch: 400, loss is 6.402407903671264 and perplexity is 603.2959688983374
At time: 588.9980926513672 and batch: 450, loss is 6.406341047286987 and perplexity is 605.6734910899698
At time: 591.4389042854309 and batch: 500, loss is 6.379409837722778 and perplexity is 589.579657292321
At time: 593.8778772354126 and batch: 550, loss is 6.355979480743408 and perplexity is 575.9261730790365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.01811777750651 and perplexity of 410.80464184115056
Finished 21 epochs...
Completing Train Step...
At time: 597.7732980251312 and batch: 50, loss is 6.397981519699097 and perplexity is 600.6314507318483
At time: 600.2374174594879 and batch: 100, loss is 6.388302822113037 and perplexity is 594.8461627483466
At time: 602.6756393909454 and batch: 150, loss is 6.438134288787841 and perplexity is 625.2391951669374
At time: 605.1217861175537 and batch: 200, loss is 6.445933542251587 and perplexity is 630.1346597950114
At time: 607.5597455501556 and batch: 250, loss is 6.394726848602295 and perplexity is 598.6797706704903
At time: 610.0021817684174 and batch: 300, loss is 6.403055200576782 and perplexity is 603.6866069278709
At time: 612.4414043426514 and batch: 350, loss is 6.367539596557617 and perplexity is 582.6225773714649
At time: 614.8780601024628 and batch: 400, loss is 6.402041091918945 and perplexity is 603.0747134288528
At time: 617.3159573078156 and batch: 450, loss is 6.40606572151184 and perplexity is 605.5067565207652
At time: 619.7548065185547 and batch: 500, loss is 6.379106292724609 and perplexity is 589.4007204953865
At time: 622.1925008296967 and batch: 550, loss is 6.355580682754517 and perplexity is 575.6965406709772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.018049621582032 and perplexity of 410.7766440251252
Finished 22 epochs...
Completing Train Step...
At time: 626.1402068138123 and batch: 50, loss is 6.397517395019531 and perplexity is 600.3527475337597
At time: 628.5787825584412 and batch: 100, loss is 6.387707805633545 and perplexity is 594.4923247589056
At time: 631.0184261798859 and batch: 150, loss is 6.437789478302002 and perplexity is 625.0236433006936
At time: 633.4572143554688 and batch: 200, loss is 6.445401458740235 and perplexity is 629.7994647163983
At time: 635.8964464664459 and batch: 250, loss is 6.3945355796813965 and perplexity is 598.565272787083
At time: 638.3393068313599 and batch: 300, loss is 6.40268388748169 and perplexity is 603.4624917964032
At time: 640.7768838405609 and batch: 350, loss is 6.366757822036743 and perplexity is 582.1672758799315
At time: 643.2232542037964 and batch: 400, loss is 6.401117010116577 and perplexity is 602.5176804723529
At time: 645.6618056297302 and batch: 450, loss is 6.405489978790283 and perplexity is 605.1582407501808
At time: 648.0991201400757 and batch: 500, loss is 6.378566932678223 and perplexity is 589.0829070110924
At time: 650.5367004871368 and batch: 550, loss is 6.354927396774292 and perplexity is 575.3205690139705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.017979431152344 and perplexity of 410.74781244783753
Finished 23 epochs...
Completing Train Step...
At time: 654.4289546012878 and batch: 50, loss is 6.396915845870971 and perplexity is 599.9917144501042
At time: 656.9026443958282 and batch: 100, loss is 6.386622161865234 and perplexity is 593.8472680854582
At time: 659.3438506126404 and batch: 150, loss is 6.436699714660644 and perplexity is 624.3428862587434
At time: 661.7826609611511 and batch: 200, loss is 6.4448283004760745 and perplexity is 629.438593376507
At time: 664.2185153961182 and batch: 250, loss is 6.393919496536255 and perplexity is 598.1966203831786
At time: 666.6553785800934 and batch: 300, loss is 6.40209361076355 and perplexity is 603.106387047736
At time: 669.093555688858 and batch: 350, loss is 6.36603084564209 and perplexity is 581.7442078115091
At time: 671.5421998500824 and batch: 400, loss is 6.400635614395141 and perplexity is 602.2277008419608
At time: 673.9891905784607 and batch: 450, loss is 6.405418968200683 and perplexity is 605.1152696324243
At time: 676.4371914863586 and batch: 500, loss is 6.37836841583252 and perplexity is 588.9659757373327
At time: 678.8734014034271 and batch: 550, loss is 6.354224224090576 and perplexity is 574.9161615062796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.017903645833333 and perplexity of 410.716684973356
Finished 24 epochs...
Completing Train Step...
At time: 682.7966256141663 and batch: 50, loss is 6.396260347366333 and perplexity is 599.5985496520331
At time: 685.2317180633545 and batch: 100, loss is 6.385877017974853 and perplexity is 593.4049312445891
At time: 687.6942915916443 and batch: 150, loss is 6.435659894943237 and perplexity is 623.6940196258123
At time: 690.1309163570404 and batch: 200, loss is 6.444207668304443 and perplexity is 629.0480647352462
At time: 692.5690932273865 and batch: 250, loss is 6.393546934127808 and perplexity is 597.9737963200757
At time: 695.0083210468292 and batch: 300, loss is 6.401608076095581 and perplexity is 602.8136290661769
At time: 697.4469494819641 and batch: 350, loss is 6.365420284271241 and perplexity is 581.3891256812509
At time: 699.8852336406708 and batch: 400, loss is 6.4002522850036625 and perplexity is 601.9968935043098
At time: 702.3235695362091 and batch: 450, loss is 6.405192031860351 and perplexity is 604.9779625682255
At time: 704.7612705230713 and batch: 500, loss is 6.377918386459351 and perplexity is 588.7009833800985
At time: 707.1986989974976 and batch: 550, loss is 6.353719720840454 and perplexity is 574.626187586795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.017790730794271 and perplexity of 410.6703115010093
Finished 25 epochs...
Completing Train Step...
At time: 711.0964312553406 and batch: 50, loss is 6.3956865787506105 and perplexity is 599.2546175003829
At time: 713.561886548996 and batch: 100, loss is 6.385376491546631 and perplexity is 593.1079907133546
At time: 716.003032207489 and batch: 150, loss is 6.434843339920044 and perplexity is 623.184947012337
At time: 718.4433336257935 and batch: 200, loss is 6.443548679351807 and perplexity is 628.6336655671416
At time: 720.8834044933319 and batch: 250, loss is 6.392956838607788 and perplexity is 597.6210387523447
At time: 723.3236463069916 and batch: 300, loss is 6.401136302947998 and perplexity is 602.5293048565239
At time: 725.7635862827301 and batch: 350, loss is 6.365064611434937 and perplexity is 581.1823781313469
At time: 728.2002727985382 and batch: 400, loss is 6.400013961791992 and perplexity is 601.853440765972
At time: 730.6381461620331 and batch: 450, loss is 6.404868726730347 and perplexity is 604.7824017040061
At time: 733.0753176212311 and batch: 500, loss is 6.377154617309571 and perplexity is 588.2515233942321
At time: 735.5107517242432 and batch: 550, loss is 6.352946672439575 and perplexity is 574.182145386548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.017647298177083 and perplexity of 410.61141220757025
Finished 26 epochs...
Completing Train Step...
At time: 739.453147649765 and batch: 50, loss is 6.39474871635437 and perplexity is 598.6928625944331
At time: 741.8905742168427 and batch: 100, loss is 6.384672470092774 and perplexity is 592.6905769148137
At time: 744.3293514251709 and batch: 150, loss is 6.434263916015625 and perplexity is 622.8239633485454
At time: 746.7673456668854 and batch: 200, loss is 6.442923316955566 and perplexity is 628.2406646085036
At time: 749.2411959171295 and batch: 250, loss is 6.392105436325073 and perplexity is 597.1124393778094
At time: 751.6800556182861 and batch: 300, loss is 6.400417709350586 and perplexity is 602.0964866846092
At time: 754.1172473430634 and batch: 350, loss is 6.364620971679687 and perplexity is 580.9245997079637
At time: 756.5535068511963 and batch: 400, loss is 6.399622020721435 and perplexity is 601.6175959057424
At time: 758.9930510520935 and batch: 450, loss is 6.404068050384521 and perplexity is 604.298360546602
At time: 761.4304311275482 and batch: 500, loss is 6.375702533721924 and perplexity is 587.3979528895464
At time: 763.867374420166 and batch: 550, loss is 6.351792688369751 and perplexity is 573.5199305037952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.017418924967448 and perplexity of 410.51765026821647
Finished 27 epochs...
Completing Train Step...
At time: 767.7616701126099 and batch: 50, loss is 6.393772268295288 and perplexity is 598.1085554299905
At time: 770.2236168384552 and batch: 100, loss is 6.383710451126099 and perplexity is 592.1206715123085
At time: 772.6589403152466 and batch: 150, loss is 6.433130693435669 and perplexity is 622.1185649321369
At time: 775.0956938266754 and batch: 200, loss is 6.442137928009033 and perplexity is 627.7474450447072
At time: 777.5345435142517 and batch: 250, loss is 6.391395359039307 and perplexity is 596.6885938968921
At time: 779.9743368625641 and batch: 300, loss is 6.399677248001098 and perplexity is 601.6508225264613
At time: 782.41441822052 and batch: 350, loss is 6.363594799041748 and perplexity is 580.3287765400387
At time: 784.8526036739349 and batch: 400, loss is 6.398226194381714 and perplexity is 600.7784280215034
At time: 787.2919352054596 and batch: 450, loss is 6.402498588562012 and perplexity is 603.350681208118
At time: 789.7391476631165 and batch: 500, loss is 6.374374418258667 and perplexity is 586.6183384088482
At time: 792.1859519481659 and batch: 550, loss is 6.3505443096160885 and perplexity is 572.8044071227712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.017239888509114 and perplexity of 410.44415922101376
Finished 28 epochs...
Completing Train Step...
At time: 796.1205761432648 and batch: 50, loss is 6.392235488891601 and perplexity is 597.1901004329569
At time: 798.557434797287 and batch: 100, loss is 6.382026815414429 and perplexity is 591.1245947546253
At time: 800.9958026409149 and batch: 150, loss is 6.431840286254883 and perplexity is 621.3162964065062
At time: 803.4330248832703 and batch: 200, loss is 6.4405604839324955 and perplexity is 626.757989166791
At time: 805.8705053329468 and batch: 250, loss is 6.3900057888031006 and perplexity is 595.8600289944162
At time: 808.3092784881592 and batch: 300, loss is 6.39796178817749 and perplexity is 600.6195994763225
At time: 810.7903010845184 and batch: 350, loss is 6.361802940368652 and perplexity is 579.2898404798243
At time: 813.2251274585724 and batch: 400, loss is 6.39635760307312 and perplexity is 599.6568668685634
At time: 815.6785087585449 and batch: 450, loss is 6.400914897918701 and perplexity is 602.3959166050896
At time: 818.1248068809509 and batch: 500, loss is 6.372705192565918 and perplexity is 585.6399568035679
At time: 820.5656378269196 and batch: 550, loss is 6.348010444641114 and perplexity is 571.3548353831592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.016754659016927 and perplexity of 410.24504792130784
Finished 29 epochs...
Completing Train Step...
At time: 824.4523282051086 and batch: 50, loss is 6.389395456314087 and perplexity is 595.4964672178843
At time: 826.9151866436005 and batch: 100, loss is 6.379473686218262 and perplexity is 589.6173022681816
At time: 829.3597571849823 and batch: 150, loss is 6.42877537727356 and perplexity is 619.4149337495465
At time: 831.8037343025208 and batch: 200, loss is 6.437824096679687 and perplexity is 625.0452809797681
At time: 834.2510712146759 and batch: 250, loss is 6.3870133018493656 and perplexity is 594.0795909288883
At time: 836.6940550804138 and batch: 300, loss is 6.395425052642822 and perplexity is 599.0979172641883
At time: 839.1316742897034 and batch: 350, loss is 6.359709959030152 and perplexity is 578.0786655794856
At time: 841.5680327415466 and batch: 400, loss is 6.394500951766968 and perplexity is 598.5445460789006
At time: 844.0100381374359 and batch: 450, loss is 6.398644895553589 and perplexity is 601.0300273221386
At time: 846.4661998748779 and batch: 500, loss is 6.3697825622558595 and perplexity is 583.9308464804178
At time: 848.9153172969818 and batch: 550, loss is 6.344977111816406 and perplexity is 569.6243518994764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.016524251302084 and perplexity of 410.15053518594027
Finished 30 epochs...
Completing Train Step...
At time: 852.8608100414276 and batch: 50, loss is 6.386502485275269 and perplexity is 593.7762027219685
At time: 855.2987604141235 and batch: 100, loss is 6.3766540431976315 and perplexity is 587.9571335984148
At time: 857.7385678291321 and batch: 150, loss is 6.426087684631348 and perplexity is 617.7523720179178
At time: 860.1784865856171 and batch: 200, loss is 6.434828758239746 and perplexity is 623.1758599949254
At time: 862.6165752410889 and batch: 250, loss is 6.384506587982178 and perplexity is 592.59226830503
At time: 865.0565123558044 and batch: 300, loss is 6.392576828002929 and perplexity is 597.3939795651139
At time: 867.4960820674896 and batch: 350, loss is 6.356646499633789 and perplexity is 576.3104548633019
At time: 869.934296131134 and batch: 400, loss is 6.390923023223877 and perplexity is 596.4068230538053
At time: 872.408417224884 and batch: 450, loss is 6.3949113368988035 and perplexity is 598.7902302704741
At time: 874.8461847305298 and batch: 500, loss is 6.366012334823608 and perplexity is 581.7334393497424
At time: 877.2838854789734 and batch: 550, loss is 6.341036128997803 and perplexity is 567.3838898266316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.0152135213216145 and perplexity of 409.61329075107653
Finished 31 epochs...
Completing Train Step...
At time: 881.2036807537079 and batch: 50, loss is 6.3815514087677006 and perplexity is 590.8436369831219
At time: 883.6695718765259 and batch: 100, loss is 6.3701357078552245 and perplexity is 584.1370955050102
At time: 886.1113557815552 and batch: 150, loss is 6.417617263793946 and perplexity is 612.541848271776
At time: 888.5501153469086 and batch: 200, loss is 6.424175786972046 and perplexity is 616.5724210360165
At time: 890.9884839057922 and batch: 250, loss is 6.3728103351593015 and perplexity is 585.7015357446434
At time: 893.4260063171387 and batch: 300, loss is 6.379905452728272 and perplexity is 589.8719342398465
At time: 895.8637902736664 and batch: 350, loss is 6.343633556365967 and perplexity is 568.859543892811
At time: 898.299439907074 and batch: 400, loss is 6.378630886077881 and perplexity is 589.1205820703873
At time: 900.7374618053436 and batch: 450, loss is 6.382613000869751 and perplexity is 591.4712049733627
At time: 903.1741564273834 and batch: 500, loss is 6.355516052246093 and perplexity is 575.659334313202
At time: 905.6096827983856 and batch: 550, loss is 6.331214447021484 and perplexity is 561.8385027943116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.004731241861979 and perplexity of 405.34203512434976
Finished 32 epochs...
Completing Train Step...
At time: 909.5361866950989 and batch: 50, loss is 6.37335503578186 and perplexity is 586.0206546400374
At time: 911.9747796058655 and batch: 100, loss is 6.362539949417115 and perplexity is 579.7169397026034
At time: 914.4145848751068 and batch: 150, loss is 6.411623373031616 and perplexity is 608.8813207062839
At time: 916.8545517921448 and batch: 200, loss is 6.419238157272339 and perplexity is 613.5355184580897
At time: 919.2980272769928 and batch: 250, loss is 6.369844789505005 and perplexity is 583.9671840212663
At time: 921.7380146980286 and batch: 300, loss is 6.377643356323242 and perplexity is 588.5390951316059
At time: 924.1771409511566 and batch: 350, loss is 6.341402578353882 and perplexity is 567.591845387972
At time: 926.6140491962433 and batch: 400, loss is 6.376009359359741 and perplexity is 587.578209293323
At time: 929.0527482032776 and batch: 450, loss is 6.380229597091675 and perplexity is 590.0631688946033
At time: 931.4928247928619 and batch: 500, loss is 6.352430181503296 and perplexity is 573.8856620847423
At time: 933.9698040485382 and batch: 550, loss is 6.3276716995239255 and perplexity is 559.8515725188088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.001059977213542 and perplexity of 403.85664553777383
Finished 33 epochs...
Completing Train Step...
At time: 937.8710000514984 and batch: 50, loss is 6.369246253967285 and perplexity is 583.6177634895018
At time: 940.3368492126465 and batch: 100, loss is 6.358099784851074 and perplexity is 577.1486072176173
At time: 942.7760756015778 and batch: 150, loss is 6.407746152877808 and perplexity is 606.5251244758116
At time: 945.2159421443939 and batch: 200, loss is 6.415587892532349 and perplexity is 611.3000339257261
At time: 947.6546154022217 and batch: 250, loss is 6.366103773117065 and perplexity is 581.786634494683
At time: 950.0951654911041 and batch: 300, loss is 6.3738258934021 and perplexity is 586.2966519035007
At time: 952.5353116989136 and batch: 350, loss is 6.337470989227295 and perplexity is 565.3646884621179
At time: 954.9732673168182 and batch: 400, loss is 6.3722650337219235 and perplexity is 585.3822389197481
At time: 957.4110412597656 and batch: 450, loss is 6.376546821594238 and perplexity is 587.8940952714195
At time: 959.8493785858154 and batch: 500, loss is 6.347990608215332 and perplexity is 571.3435018577806
At time: 962.2993459701538 and batch: 550, loss is 6.32312240600586 and perplexity is 557.3104279777056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.997508748372396 and perplexity of 402.42500172242035
Finished 34 epochs...
Completing Train Step...
At time: 966.2406530380249 and batch: 50, loss is 6.36374870300293 and perplexity is 580.4180983108466
At time: 968.6805775165558 and batch: 100, loss is 6.353275241851807 and perplexity is 574.3708350737038
At time: 971.121622800827 and batch: 150, loss is 6.402882347106933 and perplexity is 603.5822666212141
At time: 973.5680174827576 and batch: 200, loss is 6.411008081436157 and perplexity is 608.5067963796724
At time: 976.025554895401 and batch: 250, loss is 6.361303472518921 and perplexity is 579.0005760739778
At time: 978.4883477687836 and batch: 300, loss is 6.369268379211426 and perplexity is 583.6306763178532
At time: 980.951922416687 and batch: 350, loss is 6.333020086288452 and perplexity is 562.8538968985079
At time: 983.420294046402 and batch: 400, loss is 6.368718528747559 and perplexity is 583.3098549297304
At time: 985.8860425949097 and batch: 450, loss is 6.372769889831543 and perplexity is 585.6778473331073
At time: 988.3554184436798 and batch: 500, loss is 6.343438520431518 and perplexity is 568.7486066588228
At time: 990.8205547332764 and batch: 550, loss is 6.3195268917083744 and perplexity is 555.3102084296223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.993886311848958 and perplexity of 400.96987983275716
Finished 35 epochs...
Completing Train Step...
At time: 994.7346642017365 and batch: 50, loss is 6.359677333831787 and perplexity is 578.0598059560016
At time: 997.2166275978088 and batch: 100, loss is 6.349495859146118 and perplexity is 572.2041647900845
At time: 999.660454750061 and batch: 150, loss is 6.399214496612549 and perplexity is 601.3724721813868
At time: 1002.0991287231445 and batch: 200, loss is 6.407727308273316 and perplexity is 606.51369485742
At time: 1004.5374095439911 and batch: 250, loss is 6.357952518463135 and perplexity is 577.0636188850441
At time: 1006.977089881897 and batch: 300, loss is 6.365711326599121 and perplexity is 581.5583591516506
At time: 1009.4162721633911 and batch: 350, loss is 6.32988597869873 and perplexity is 561.092613695563
At time: 1011.853542804718 and batch: 400, loss is 6.365468444824219 and perplexity is 581.4171263772986
At time: 1014.2921271324158 and batch: 450, loss is 6.369113998413086 and perplexity is 583.5405819027104
At time: 1016.7296650409698 and batch: 500, loss is 6.339878845214844 and perplexity is 566.7276454559496
At time: 1019.1664583683014 and batch: 550, loss is 6.316435461044311 and perplexity is 553.5961562250797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.991455586751302 and perplexity of 399.99641587338556
Finished 36 epochs...
Completing Train Step...
At time: 1023.1117227077484 and batch: 50, loss is 6.356610383987427 and perplexity is 576.289641414567
At time: 1025.5527617931366 and batch: 100, loss is 6.34589750289917 and perplexity is 570.1488704175459
At time: 1027.9936964511871 and batch: 150, loss is 6.396059350967407 and perplexity is 599.4780446136971
At time: 1030.438159942627 and batch: 200, loss is 6.404186906814576 and perplexity is 604.3701895610099
At time: 1032.8785421848297 and batch: 250, loss is 6.354354915618896 and perplexity is 574.9913030881601
At time: 1035.3194289207458 and batch: 300, loss is 6.36200852394104 and perplexity is 579.4089451972453
At time: 1037.759666442871 and batch: 350, loss is 6.326299638748169 and perplexity is 559.0839488694841
At time: 1040.2009942531586 and batch: 400, loss is 6.361066799163819 and perplexity is 578.8635582799021
At time: 1042.6406564712524 and batch: 450, loss is 6.364412908554077 and perplexity is 580.8037432933222
At time: 1045.0811004638672 and batch: 500, loss is 6.336143550872802 and perplexity is 564.6146995825086
At time: 1047.520815372467 and batch: 550, loss is 6.312400693893433 and perplexity is 551.3670246765297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.987714640299479 and perplexity of 398.50284612474337
Finished 37 epochs...
Completing Train Step...
At time: 1051.4314546585083 and batch: 50, loss is 6.352138385772705 and perplexity is 573.7182291279978
At time: 1053.8954796791077 and batch: 100, loss is 6.341314668655396 and perplexity is 567.5419507531241
At time: 1056.3335337638855 and batch: 150, loss is 6.3886119556427 and perplexity is 595.0300780679751
At time: 1058.8098244667053 and batch: 200, loss is 6.395746412277222 and perplexity is 599.2904740901886
At time: 1061.2467596530914 and batch: 250, loss is 6.345456981658936 and perplexity is 569.8977630431582
At time: 1063.6844890117645 and batch: 300, loss is 6.352057094573975 and perplexity is 573.6715927810063
At time: 1066.1228229999542 and batch: 350, loss is 6.316485595703125 and perplexity is 553.6239112752318
At time: 1068.561273574829 and batch: 400, loss is 6.351951351165772 and perplexity is 573.6109339987861
At time: 1071.0003473758698 and batch: 450, loss is 6.354306354522705 and perplexity is 574.9633815581371
At time: 1073.438784122467 and batch: 500, loss is 6.328136930465698 and perplexity is 560.1120933896306
At time: 1075.877184867859 and batch: 550, loss is 6.304406089782715 and perplexity is 546.9766366852792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.978953043619792 and perplexity of 395.02657598786385
Finished 38 epochs...
Completing Train Step...
At time: 1079.8077442646027 and batch: 50, loss is 6.343241891860962 and perplexity is 568.6367854272744
At time: 1082.2441823482513 and batch: 100, loss is 6.332294893264771 and perplexity is 562.4458671471716
At time: 1084.683294057846 and batch: 150, loss is 6.3805211162567135 and perplexity is 590.2352086921503
At time: 1087.1223666667938 and batch: 200, loss is 6.388617248535156 and perplexity is 595.0332275065211
At time: 1089.5615260601044 and batch: 250, loss is 6.339695196151734 and perplexity is 566.6235760112438
At time: 1092.000424861908 and batch: 300, loss is 6.346415710449219 and perplexity is 570.4444024336956
At time: 1094.4483251571655 and batch: 350, loss is 6.3102343654632564 and perplexity is 550.1738754586659
At time: 1096.8950510025024 and batch: 400, loss is 6.343002996444702 and perplexity is 568.5009569307678
At time: 1099.336899280548 and batch: 450, loss is 6.343611726760864 and perplexity is 568.8471260491475
At time: 1101.773236989975 and batch: 500, loss is 6.316083183288574 and perplexity is 553.4011709600823
At time: 1104.2210330963135 and batch: 550, loss is 6.28982982635498 and perplexity is 539.0615872036545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.966250101725261 and perplexity of 390.0403134980299
Finished 39 epochs...
Completing Train Step...
At time: 1108.1812467575073 and batch: 50, loss is 6.327471399307251 and perplexity is 559.7394453574501
At time: 1110.6614637374878 and batch: 100, loss is 6.313879623413086 and perplexity is 552.1830609267984
At time: 1113.103955745697 and batch: 150, loss is 6.360472736358642 and perplexity is 578.5197790939799
At time: 1115.5466270446777 and batch: 200, loss is 6.3692843532562256 and perplexity is 583.6399993348862
At time: 1117.9913046360016 and batch: 250, loss is 6.3232464027404784 and perplexity is 557.3795369354978
At time: 1120.4627933502197 and batch: 300, loss is 6.3281590938568115 and perplexity is 560.1245075105928
At time: 1122.9081408977509 and batch: 350, loss is 6.293043155670166 and perplexity is 540.7965556240839
At time: 1125.3559772968292 and batch: 400, loss is 6.320424976348877 and perplexity is 555.809148009979
At time: 1127.8019194602966 and batch: 450, loss is 6.315709276199341 and perplexity is 553.1942890187926
At time: 1130.2445878982544 and batch: 500, loss is 6.2906356620788575 and perplexity is 539.4961573605553
At time: 1132.6830425262451 and batch: 550, loss is 6.2652779483795165 and perplexity is 525.9877630899327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.938787841796875 and perplexity of 379.4746673083237
Finished 40 epochs...
Completing Train Step...
At time: 1136.6002082824707 and batch: 50, loss is 6.30602391242981 and perplexity is 547.8622640763991
At time: 1139.035211801529 and batch: 100, loss is 6.294468421936035 and perplexity is 541.5678842552801
At time: 1141.4719288349152 and batch: 150, loss is 6.34413896560669 and perplexity is 569.1471234295237
At time: 1143.909420967102 and batch: 200, loss is 6.35645167350769 and perplexity is 576.1981854668117
At time: 1146.346039056778 and batch: 250, loss is 6.310929317474365 and perplexity is 550.5563527861757
At time: 1148.7846040725708 and batch: 300, loss is 6.316585817337036 and perplexity is 553.6793991486882
At time: 1151.223724603653 and batch: 350, loss is 6.278620176315307 and perplexity is 533.0526374946537
At time: 1153.6656789779663 and batch: 400, loss is 6.309279747009278 and perplexity is 549.6489199302393
At time: 1156.1041903495789 and batch: 450, loss is 6.3074159336090085 and perplexity is 548.6254310003042
At time: 1158.5412278175354 and batch: 500, loss is 6.2849746799469 and perplexity is 536.4507075051368
At time: 1160.9776129722595 and batch: 550, loss is 6.258631658554077 and perplexity is 522.5034875502848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.932373046875 and perplexity of 377.0482060857105
Finished 41 epochs...
Completing Train Step...
At time: 1164.8869531154633 and batch: 50, loss is 6.299925651550293 and perplexity is 544.5314235537327
At time: 1167.3519883155823 and batch: 100, loss is 6.289286689758301 and perplexity is 538.7688826242603
At time: 1169.791580915451 and batch: 150, loss is 6.339534568786621 and perplexity is 566.532568068597
At time: 1172.2321751117706 and batch: 200, loss is 6.35166449546814 and perplexity is 573.4464140320176
At time: 1174.6712529659271 and batch: 250, loss is 6.305105667114258 and perplexity is 547.3594230198978
At time: 1177.1108357906342 and batch: 300, loss is 6.310638074874878 and perplexity is 550.3960306702744
At time: 1179.55251455307 and batch: 350, loss is 6.273505620956421 and perplexity is 530.33327037574
At time: 1182.0199508666992 and batch: 400, loss is 6.303472814559936 and perplexity is 546.4663950777822
At time: 1184.460330247879 and batch: 450, loss is 6.302482395172119 and perplexity is 545.9254320996382
At time: 1186.9001870155334 and batch: 500, loss is 6.2802894592285154 and perplexity is 533.9431962447209
At time: 1189.337835073471 and batch: 550, loss is 6.253632497787476 and perplexity is 519.8979268497526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.928246561686198 and perplexity of 375.4955280026238
Finished 42 epochs...
Completing Train Step...
At time: 1193.2560336589813 and batch: 50, loss is 6.295389699935913 and perplexity is 542.0670487318047
At time: 1195.692863702774 and batch: 100, loss is 6.28419638633728 and perplexity is 536.0333537805454
At time: 1198.1313858032227 and batch: 150, loss is 6.3339559268951415 and perplexity is 563.3808849808208
At time: 1200.570547580719 and batch: 200, loss is 6.346113815307617 and perplexity is 570.2722140327754
At time: 1203.008246421814 and batch: 250, loss is 6.300805616378784 and perplexity is 545.0108029420353
At time: 1205.44859957695 and batch: 300, loss is 6.304993410110473 and perplexity is 547.2979815397589
At time: 1207.8883945941925 and batch: 350, loss is 6.2685567665100095 and perplexity is 527.7152117513657
At time: 1210.329312324524 and batch: 400, loss is 6.298511915206909 and perplexity is 543.7621435977829
At time: 1212.7695226669312 and batch: 450, loss is 6.296950426101684 and perplexity is 542.913727503353
At time: 1215.2077033519745 and batch: 500, loss is 6.274972095489502 and perplexity is 531.1115611431072
At time: 1217.644217967987 and batch: 550, loss is 6.248587284088135 and perplexity is 517.2815363813058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.924287414550781 and perplexity of 374.01182499589237
Finished 43 epochs...
Completing Train Step...
At time: 1221.554626941681 and batch: 50, loss is 6.290274209976197 and perplexity is 539.3011905777993
At time: 1224.018890619278 and batch: 100, loss is 6.279636688232422 and perplexity is 533.5947673471627
At time: 1226.457495689392 and batch: 150, loss is 6.329980401992798 and perplexity is 561.1455964097872
At time: 1228.8967771530151 and batch: 200, loss is 6.342596435546875 and perplexity is 568.2698736491988
At time: 1231.3411602973938 and batch: 250, loss is 6.29680100440979 and perplexity is 542.8326104761095
At time: 1233.788861989975 and batch: 300, loss is 6.300386829376221 and perplexity is 544.7826072875288
At time: 1236.2317655086517 and batch: 350, loss is 6.263200473785401 and perplexity is 524.896171145164
At time: 1238.6688208580017 and batch: 400, loss is 6.294080963134766 and perplexity is 541.358089658047
At time: 1241.1162717342377 and batch: 450, loss is 6.292686223983765 and perplexity is 540.6035626421894
At time: 1243.600126504898 and batch: 500, loss is 6.269990358352661 and perplexity is 528.4722825097474
At time: 1246.0507881641388 and batch: 550, loss is 6.2438969230651855 and perplexity is 514.8609803036927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.9212697347005205 and perplexity of 372.88487828523506
Finished 44 epochs...
Completing Train Step...
At time: 1249.9946575164795 and batch: 50, loss is 6.28635046005249 and perplexity is 537.1892536381991
At time: 1252.4561562538147 and batch: 100, loss is 6.2756679916381835 and perplexity is 531.4812882639461
At time: 1254.902673482895 and batch: 150, loss is 6.326484308242798 and perplexity is 559.1872041535343
At time: 1257.350216627121 and batch: 200, loss is 6.339105587005616 and perplexity is 566.2895880392784
At time: 1259.7852277755737 and batch: 250, loss is 6.292980279922485 and perplexity is 540.7625537052653
At time: 1262.2390270233154 and batch: 300, loss is 6.296698274612427 and perplexity is 542.7768482563031
At time: 1264.6855309009552 and batch: 350, loss is 6.25979585647583 and perplexity is 523.1121392513772
At time: 1267.1226465702057 and batch: 400, loss is 6.29073546409607 and perplexity is 539.5500028522378
At time: 1269.5642383098602 and batch: 450, loss is 6.289434881210327 and perplexity is 538.8487294834478
At time: 1272.0027348995209 and batch: 500, loss is 6.267493963241577 and perplexity is 527.1546522345232
At time: 1274.4458994865417 and batch: 550, loss is 6.240742626190186 and perplexity is 513.2395145590367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.918437703450521 and perplexity of 371.8303505897839
Finished 45 epochs...
Completing Train Step...
At time: 1278.3886108398438 and batch: 50, loss is 6.2833891868591305 and perplexity is 535.6008425220184
At time: 1280.8535776138306 and batch: 100, loss is 6.272143278121948 and perplexity is 529.611266564989
At time: 1283.2944118976593 and batch: 150, loss is 6.323029050827026 and perplexity is 557.2584025914933
At time: 1285.7357206344604 and batch: 200, loss is 6.335855722427368 and perplexity is 564.4522107968276
At time: 1288.1770195960999 and batch: 250, loss is 6.290020475387573 and perplexity is 539.1643685710358
At time: 1290.618484735489 and batch: 300, loss is 6.293130521774292 and perplexity is 540.8438049762395
At time: 1293.0592575073242 and batch: 350, loss is 6.25661334991455 and perplexity is 521.4499777587649
At time: 1295.4990255832672 and batch: 400, loss is 6.286581678390503 and perplexity is 537.313476005315
At time: 1297.9404246807098 and batch: 450, loss is 6.284655952453614 and perplexity is 536.2797531612342
At time: 1300.381652355194 and batch: 500, loss is 6.263822870254517 and perplexity is 525.2229663562806
At time: 1302.8257946968079 and batch: 550, loss is 6.237101392745972 and perplexity is 511.37408796166454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.915902709960937 and perplexity of 370.8889567895179
Finished 46 epochs...
Completing Train Step...
At time: 1306.7667288780212 and batch: 50, loss is 6.28016526222229 and perplexity is 533.8768862160917
At time: 1309.2054286003113 and batch: 100, loss is 6.26917688369751 and perplexity is 528.0425585104638
At time: 1311.6486155986786 and batch: 150, loss is 6.320659093856811 and perplexity is 555.9392878960164
At time: 1314.0944600105286 and batch: 200, loss is 6.333141345977783 and perplexity is 562.9221525254286
At time: 1316.5371401309967 and batch: 250, loss is 6.287128295898437 and perplexity is 537.6072612453115
At time: 1318.9786067008972 and batch: 300, loss is 6.290444841384888 and perplexity is 539.3932201510011
At time: 1321.4214262962341 and batch: 350, loss is 6.25346791267395 and perplexity is 519.8123664316191
At time: 1323.861471414566 and batch: 400, loss is 6.283519983291626 and perplexity is 535.6709017831126
At time: 1326.3029448986053 and batch: 450, loss is 6.281840209960937 and perplexity is 534.7718514000993
At time: 1328.7469744682312 and batch: 500, loss is 6.260014276504517 and perplexity is 523.2264098988837
At time: 1331.1913039684296 and batch: 550, loss is 6.233316764831543 and perplexity is 509.4423850078238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.9126642862955725 and perplexity of 369.68980394510146
Finished 47 epochs...
Completing Train Step...
At time: 1335.1301898956299 and batch: 50, loss is 6.2763667488098145 and perplexity is 531.852794406883
At time: 1337.59574508667 and batch: 100, loss is 6.265187301635742 and perplexity is 525.9400861728549
At time: 1340.0355303287506 and batch: 150, loss is 6.316980962753296 and perplexity is 553.8982262567562
At time: 1342.475746870041 and batch: 200, loss is 6.328910617828369 and perplexity is 560.5456127205543
At time: 1344.914291858673 and batch: 250, loss is 6.282513408660889 and perplexity is 535.1319803207923
At time: 1347.3544528484344 and batch: 300, loss is 6.286040544509888 and perplexity is 537.0227961343938
At time: 1349.7934584617615 and batch: 350, loss is 6.2489411163330075 and perplexity is 517.4645996534891
At time: 1352.230667591095 and batch: 400, loss is 6.279430522918701 and perplexity is 533.4847699537656
At time: 1354.6690258979797 and batch: 450, loss is 6.276924200057984 and perplexity is 532.1493590634566
At time: 1357.110149383545 and batch: 500, loss is 6.254898738861084 and perplexity is 520.556659928238
At time: 1359.5509071350098 and batch: 550, loss is 6.226990613937378 and perplexity is 506.2297481353789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.906251525878906 and perplexity of 367.32665704975136
Finished 48 epochs...
Completing Train Step...
At time: 1363.5005097389221 and batch: 50, loss is 6.268540134429932 and perplexity is 527.7064348226951
At time: 1365.942589521408 and batch: 100, loss is 6.256091775894165 and perplexity is 521.1780739125908
At time: 1368.4097430706024 and batch: 150, loss is 6.305248098373413 and perplexity is 547.4373896640407
At time: 1370.851066350937 and batch: 200, loss is 6.316263198852539 and perplexity is 553.5008007511589
At time: 1373.2979397773743 and batch: 250, loss is 6.267588968276978 and perplexity is 527.2047369600332
At time: 1375.7513735294342 and batch: 300, loss is 6.270058164596557 and perplexity is 528.50811744513
At time: 1378.2093386650085 and batch: 350, loss is 6.2302045822143555 and perplexity is 507.8593718634522
At time: 1380.6547048091888 and batch: 400, loss is 6.254656076431274 and perplexity is 520.4303557095503
At time: 1383.0947725772858 and batch: 450, loss is 6.246352081298828 and perplexity is 516.12659848938
At time: 1385.5381734371185 and batch: 500, loss is 6.215838479995727 and perplexity is 500.6155694017363
At time: 1387.981038093567 and batch: 550, loss is 6.1773883247375485 and perplexity is 481.7321839826433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.8588918050130205 and perplexity of 350.33568856364764
Finished 49 epochs...
Completing Train Step...
At time: 1391.940898656845 and batch: 50, loss is 6.216170701980591 and perplexity is 500.7819125297487
At time: 1394.4020547866821 and batch: 100, loss is 6.203684158325196 and perplexity is 494.56775473261024
At time: 1396.838275194168 and batch: 150, loss is 6.2520888233184815 and perplexity is 519.095992815385
At time: 1399.2795741558075 and batch: 200, loss is 6.259758253097534 and perplexity is 523.0924688375528
At time: 1401.7226510047913 and batch: 250, loss is 6.211371374130249 and perplexity is 498.3842541265854
At time: 1404.1654121875763 and batch: 300, loss is 6.214794092178344 and perplexity is 500.0930055270319
At time: 1406.6054937839508 and batch: 350, loss is 6.176170921325683 and perplexity is 481.1460784141044
At time: 1409.0478584766388 and batch: 400, loss is 6.203231410980225 and perplexity is 494.3438911753864
At time: 1411.4895265102386 and batch: 450, loss is 6.2022398853302 and perplexity is 493.85397944751213
At time: 1413.9305074214935 and batch: 500, loss is 6.175942840576172 and perplexity is 481.0363507697728
At time: 1416.3688666820526 and batch: 550, loss is 6.142801761627197 and perplexity is 465.3555614289778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.826892598470052 and perplexity of 339.3026895331386
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe57aa7feb8>
SETTINGS FOR THIS RUN
{'lr': 20.1206014913586, 'tune_wordvecs': True, 'anneal': 6.1617772000621, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.09874894968015013}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9373366832733154 and batch: 50, loss is 7.585238218307495 and perplexity is 1968.9156098155283
At time: 5.363831281661987 and batch: 100, loss is 6.457519702911377 and perplexity is 637.4779593787518
At time: 7.797626495361328 and batch: 150, loss is 6.122809476852417 and perplexity is 456.14442317198603
At time: 10.228231191635132 and batch: 200, loss is 6.059316911697388 and perplexity is 428.08291852854126
At time: 12.693793058395386 and batch: 250, loss is 5.999964580535889 and perplexity is 403.41450451411816
At time: 15.124687433242798 and batch: 300, loss is 6.013157682418823 and perplexity is 408.7720568298487
At time: 17.554275274276733 and batch: 350, loss is 6.012324829101562 and perplexity is 408.43175139823893
At time: 19.983561038970947 and batch: 400, loss is 6.038060836791992 and perplexity is 419.0795827252807
At time: 22.413272857666016 and batch: 450, loss is 6.056551094055176 and perplexity is 426.9005550946996
At time: 24.844629049301147 and batch: 500, loss is 6.071195287704468 and perplexity is 433.1981686845011
At time: 27.272934675216675 and batch: 550, loss is 6.069519939422608 and perplexity is 432.47301848607236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.681709798177083 and perplexity of 293.4507427735805
Finished 1 epochs...
Completing Train Step...
At time: 31.19652771949768 and batch: 50, loss is 6.027196531295776 and perplexity is 414.5512174246802
At time: 33.60824251174927 and batch: 100, loss is 6.033161458969116 and perplexity is 417.03137508029243
At time: 36.0216851234436 and batch: 150, loss is 6.067377157211304 and perplexity is 431.5473151397217
At time: 38.46664214134216 and batch: 200, loss is 6.092203397750854 and perplexity is 442.3951104199243
At time: 40.91098427772522 and batch: 250, loss is 6.057017669677735 and perplexity is 427.0997829607822
At time: 43.36968231201172 and batch: 300, loss is 6.0688779830932615 and perplexity is 432.1954787882979
At time: 45.81266760826111 and batch: 350, loss is 6.047611923217773 and perplexity is 423.10142394195617
At time: 48.25689744949341 and batch: 400, loss is 6.0267401123046875 and perplexity is 414.36205154900154
At time: 50.698142766952515 and batch: 450, loss is 6.015856065750122 and perplexity is 409.876570064161
At time: 53.139728307724 and batch: 500, loss is 6.041119251251221 and perplexity is 420.36326379440834
At time: 55.57817196846008 and batch: 550, loss is 6.021560115814209 and perplexity is 412.22120714642705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.7036488850911455 and perplexity of 299.95992580728995
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.53513026237488 and batch: 50, loss is 5.954480838775635 and perplexity is 385.476734147306
At time: 61.98035001754761 and batch: 100, loss is 5.870836143493652 and perplexity is 354.54530711636806
At time: 64.43219137191772 and batch: 150, loss is 5.835090780258179 and perplexity is 342.0957881846749
At time: 66.88850426673889 and batch: 200, loss is 5.7822756099700925 and perplexity is 324.4967789725063
At time: 69.33288502693176 and batch: 250, loss is 5.6784343338012695 and perplexity is 292.49112776989807
At time: 71.77263617515564 and batch: 300, loss is 5.638590650558472 and perplexity is 281.0663185488064
At time: 74.21216678619385 and batch: 350, loss is 5.544420490264892 and perplexity is 255.80629304394202
At time: 76.65444493293762 and batch: 400, loss is 5.489697208404541 and perplexity is 242.18386451606088
At time: 79.10181736946106 and batch: 450, loss is 5.425763540267944 and perplexity is 227.184744809412
At time: 81.54538369178772 and batch: 500, loss is 5.441107931137085 and perplexity is 230.69763901844817
At time: 83.99803590774536 and batch: 550, loss is 5.50718846321106 and perplexity is 246.45722849921208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.285808308919271 and perplexity of 197.51377108425956
Finished 3 epochs...
Completing Train Step...
At time: 87.97332191467285 and batch: 50, loss is 5.614772958755493 and perplexity is 274.4510604108694
At time: 90.44339680671692 and batch: 100, loss is 5.603414678573609 and perplexity is 271.3514050713571
At time: 92.88670873641968 and batch: 150, loss is 5.60389705657959 and perplexity is 271.48233059631366
At time: 95.34084582328796 and batch: 200, loss is 5.572656269073486 and perplexity is 263.13212147893836
At time: 97.79926419258118 and batch: 250, loss is 5.500864677429199 and perplexity is 244.9036033555756
At time: 100.24669337272644 and batch: 300, loss is 5.4765435981750485 and perplexity is 239.01913181791042
At time: 102.69289016723633 and batch: 350, loss is 5.4118434429168705 and perplexity is 224.0442199620191
At time: 105.13944244384766 and batch: 400, loss is 5.41233187675476 and perplexity is 224.15367746943133
At time: 107.6003680229187 and batch: 450, loss is 5.3846417331695555 and perplexity is 218.0319764878862
At time: 110.05208611488342 and batch: 500, loss is 5.400484418869018 and perplexity is 221.51369563190974
At time: 112.49511575698853 and batch: 550, loss is 5.434521684646606 and perplexity is 229.18320021126158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.241344706217448 and perplexity of 188.92397892182655
Finished 4 epochs...
Completing Train Step...
At time: 116.43990397453308 and batch: 50, loss is 5.519798259735108 and perplexity is 249.5846808307882
At time: 118.88404440879822 and batch: 100, loss is 5.5031490135192875 and perplexity is 245.46368495907254
At time: 121.3277530670166 and batch: 150, loss is 5.5088264274597165 and perplexity is 246.86124742226323
At time: 123.77295780181885 and batch: 200, loss is 5.485635423660279 and perplexity is 241.2021608739877
At time: 126.22135281562805 and batch: 250, loss is 5.427024059295654 and perplexity is 227.47129606676387
At time: 128.66651439666748 and batch: 300, loss is 5.4181593227386475 and perplexity is 225.46373435247145
At time: 131.10669255256653 and batch: 350, loss is 5.3732162189483645 and perplexity is 215.5550262007375
At time: 133.54769778251648 and batch: 400, loss is 5.385208187103271 and perplexity is 218.1555165452159
At time: 135.99063634872437 and batch: 450, loss is 5.3628819561004635 and perplexity is 213.33889466765646
At time: 138.43680715560913 and batch: 500, loss is 5.373150978088379 and perplexity is 215.54096366418494
At time: 140.88695406913757 and batch: 550, loss is 5.386047325134277 and perplexity is 218.33865596470224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.216651916503906 and perplexity of 184.31604435025898
Finished 5 epochs...
Completing Train Step...
At time: 144.8435616493225 and batch: 50, loss is 5.453397979736328 and perplexity is 233.5504187060621
At time: 147.31587100028992 and batch: 100, loss is 5.437304697036743 and perplexity is 229.82190825101375
At time: 149.76102876663208 and batch: 150, loss is 5.448743562698365 and perplexity is 232.46590350889323
At time: 152.2051384449005 and batch: 200, loss is 5.437640924453735 and perplexity is 229.89919366960336
At time: 154.6515371799469 and batch: 250, loss is 5.386063413619995 and perplexity is 218.34216873130785
At time: 157.0980339050293 and batch: 300, loss is 5.3926132297515865 and perplexity is 219.77696348177338
At time: 159.5416271686554 and batch: 350, loss is 5.345815153121948 and perplexity is 209.72877600459893
At time: 161.98394393920898 and batch: 400, loss is 5.360147485733032 and perplexity is 212.75632265792274
At time: 164.42832207679749 and batch: 450, loss is 5.3347531604766845 and perplexity is 207.42154265363388
At time: 166.87161827087402 and batch: 500, loss is 5.344301900863647 and perplexity is 209.41164347195763
At time: 169.31538200378418 and batch: 550, loss is 5.3478467082977295 and perplexity is 210.15528467625998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1959996541341145 and perplexity of 180.548538708356
Finished 6 epochs...
Completing Train Step...
At time: 173.26176381111145 and batch: 50, loss is 5.409631824493408 and perplexity is 223.54926716259084
At time: 175.7053689956665 and batch: 100, loss is 5.391194667816162 and perplexity is 219.46541727313547
At time: 178.14728140830994 and batch: 150, loss is 5.410416793823242 and perplexity is 223.72481537198652
At time: 180.5884861946106 and batch: 200, loss is 5.409366884231567 and perplexity is 223.4900478063568
At time: 183.03290796279907 and batch: 250, loss is 5.358987741470337 and perplexity is 212.50972275741782
At time: 185.47545528411865 and batch: 300, loss is 5.36137993812561 and perplexity is 213.01869634514716
At time: 187.91746068000793 and batch: 350, loss is 5.322790775299072 and perplexity is 204.95506813773443
At time: 190.3582739830017 and batch: 400, loss is 5.338224172592163 and perplexity is 208.14275628773296
At time: 192.79797434806824 and batch: 450, loss is 5.309273195266724 and perplexity is 202.2032127166902
At time: 195.24020957946777 and batch: 500, loss is 5.317116117477417 and perplexity is 203.79531197909606
At time: 197.68279123306274 and batch: 550, loss is 5.313606519699096 and perplexity is 203.0813260395391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.178825378417969 and perplexity of 177.47422345850873
Finished 7 epochs...
Completing Train Step...
At time: 201.65621495246887 and batch: 50, loss is 5.374028720855713 and perplexity is 215.7302362402709
At time: 204.13016748428345 and batch: 100, loss is 5.36016508102417 and perplexity is 212.7600662002956
At time: 206.58451890945435 and batch: 150, loss is 5.377438249588013 and perplexity is 216.4670300251859
At time: 209.03522276878357 and batch: 200, loss is 5.382759218215942 and perplexity is 217.6219141271453
At time: 211.48215293884277 and batch: 250, loss is 5.336981239318848 and perplexity is 207.88420944186754
At time: 213.92933130264282 and batch: 300, loss is 5.338839263916015 and perplexity is 208.2708224734104
At time: 216.37596702575684 and batch: 350, loss is 5.290814962387085 and perplexity is 198.50513372496277
At time: 218.82848739624023 and batch: 400, loss is 5.314201602935791 and perplexity is 203.20221229747636
At time: 221.27574825286865 and batch: 450, loss is 5.28464596748352 and perplexity is 197.28432601658406
At time: 223.72386240959167 and batch: 500, loss is 5.288012418746948 and perplexity is 197.94959325173028
At time: 226.16874408721924 and batch: 550, loss is 5.2841026401519775 and perplexity is 197.17716516452037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.153385925292969 and perplexity of 173.01631999729486
Finished 8 epochs...
Completing Train Step...
At time: 230.16757893562317 and batch: 50, loss is 5.339385261535645 and perplexity is 208.3845688965315
At time: 232.61546969413757 and batch: 100, loss is 5.323947496414185 and perplexity is 205.19228116090568
At time: 235.06095147132874 and batch: 150, loss is 5.34669828414917 and perplexity is 209.91407580394767
At time: 237.50541806221008 and batch: 200, loss is 5.3557743740081785 and perplexity is 211.82794691372587
At time: 239.95085287094116 and batch: 250, loss is 5.306528635025025 and perplexity is 201.6490146811511
At time: 242.39441180229187 and batch: 300, loss is 5.303940801620484 and perplexity is 201.12785525270067
At time: 244.83768677711487 and batch: 350, loss is 5.253727235794067 and perplexity is 191.2778792223009
At time: 247.28385424613953 and batch: 400, loss is 5.2710340881347655 and perplexity is 194.61710966942093
At time: 249.72978496551514 and batch: 450, loss is 5.24557469367981 and perplexity is 189.7248175585989
At time: 252.1752519607544 and batch: 500, loss is 5.2537765789031985 and perplexity is 191.28731770042972
At time: 254.61916327476501 and batch: 550, loss is 5.248706970214844 and perplexity is 190.32001983517839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.146506754557292 and perplexity of 171.83019564551535
Finished 9 epochs...
Completing Train Step...
At time: 258.58863854408264 and batch: 50, loss is 5.306941175460816 and perplexity is 201.73222021518808
At time: 261.0627501010895 and batch: 100, loss is 5.291784105300903 and perplexity is 198.69760682057233
At time: 263.50905752182007 and batch: 150, loss is 5.31201205253601 and perplexity is 202.75777754600455
At time: 265.9856207370758 and batch: 200, loss is 5.324845924377441 and perplexity is 205.3767144817656
At time: 268.43155884742737 and batch: 250, loss is 5.280913667678833 and perplexity is 196.54937414864133
At time: 270.8776206970215 and batch: 300, loss is 5.277138652801514 and perplexity is 195.80879606322938
At time: 273.32326579093933 and batch: 350, loss is 5.2341539001464845 and perplexity is 187.57033595209504
At time: 275.7693910598755 and batch: 400, loss is 5.252263326644897 and perplexity is 190.99807064201366
At time: 278.21678829193115 and batch: 450, loss is 5.225991277694702 and perplexity is 186.04550190530594
At time: 280.6625735759735 and batch: 500, loss is 5.233189458847046 and perplexity is 187.38952257951055
At time: 283.10914874076843 and batch: 550, loss is 5.227795276641846 and perplexity is 186.38143071136588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.133477783203125 and perplexity of 169.60594625400336
Finished 10 epochs...
Completing Train Step...
At time: 287.105322599411 and batch: 50, loss is 5.285688514709473 and perplexity is 197.49011149529747
At time: 289.55170822143555 and batch: 100, loss is 5.271744689941406 and perplexity is 194.75545408723036
At time: 291.9978873729706 and batch: 150, loss is 5.291249122619629 and perplexity is 198.59133547131302
At time: 294.44238662719727 and batch: 200, loss is 5.300991735458374 and perplexity is 200.5355896449926
At time: 296.88723063468933 and batch: 250, loss is 5.2583709144592286 and perplexity is 192.16817775947666
At time: 299.3321692943573 and batch: 300, loss is 5.257346467971802 and perplexity is 191.97141254969836
At time: 301.77796459198 and batch: 350, loss is 5.213402481079101 and perplexity is 183.71809329419784
At time: 304.2217330932617 and batch: 400, loss is 5.2296990966796875 and perplexity is 186.73660540090276
At time: 306.6639897823334 and batch: 450, loss is 5.207810430526734 and perplexity is 182.6935996090972
At time: 309.1074752807617 and batch: 500, loss is 5.215144529342651 and perplexity is 184.03841800915208
At time: 311.55032539367676 and batch: 550, loss is 5.207390222549439 and perplexity is 182.6168464284195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124651082356771 and perplexity of 168.1154729625539
Finished 11 epochs...
Completing Train Step...
At time: 315.499618768692 and batch: 50, loss is 5.266077699661255 and perplexity is 193.65489818704705
At time: 317.97865176200867 and batch: 100, loss is 5.250037031173706 and perplexity is 190.57332548188393
At time: 320.42068338394165 and batch: 150, loss is 5.267889976501465 and perplexity is 194.00617268119746
At time: 322.8630554676056 and batch: 200, loss is 5.282006530761719 and perplexity is 196.76429312072722
At time: 325.30564522743225 and batch: 250, loss is 5.242822322845459 and perplexity is 189.2033424798017
At time: 327.8015503883362 and batch: 300, loss is 5.238725004196167 and perplexity is 188.42970210404434
At time: 330.24664878845215 and batch: 350, loss is 5.196638202667236 and perplexity is 180.66386452955382
At time: 332.6916296482086 and batch: 400, loss is 5.214318313598633 and perplexity is 183.88642536869125
At time: 335.137574672699 and batch: 450, loss is 5.190279712677002 and perplexity is 179.51875958260166
At time: 337.585720539093 and batch: 500, loss is 5.19483229637146 and perplexity is 180.33789694123038
At time: 340.0254337787628 and batch: 550, loss is 5.186878595352173 and perplexity is 178.90923234364342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.108221944173177 and perplexity of 165.37604546073584
Finished 12 epochs...
Completing Train Step...
At time: 343.9631681442261 and batch: 50, loss is 5.242978801727295 and perplexity is 189.2329511237759
At time: 346.4060182571411 and batch: 100, loss is 5.229492568969727 and perplexity is 186.69804309965247
At time: 348.85875487327576 and batch: 150, loss is 5.2504541015625 and perplexity is 190.65282455003768
At time: 351.31488394737244 and batch: 200, loss is 5.261141901016235 and perplexity is 192.7014116475033
At time: 353.75818395614624 and batch: 250, loss is 5.224472379684448 and perplexity is 185.76313226227643
At time: 356.2160084247589 and batch: 300, loss is 5.220767650604248 and perplexity is 185.07620341247403
At time: 358.66676354408264 and batch: 350, loss is 5.178229131698608 and perplexity is 177.3684365756755
At time: 361.1101007461548 and batch: 400, loss is 5.198204879760742 and perplexity is 180.9471283012291
At time: 363.55177307128906 and batch: 450, loss is 5.17607813835144 and perplexity is 176.98732827625363
At time: 366.00411105155945 and batch: 500, loss is 5.180203227996826 and perplexity is 177.71892478465554
At time: 368.4593336582184 and batch: 550, loss is 5.171171979904175 and perplexity is 176.12112699841327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.100054931640625 and perplexity of 164.03091753983955
Finished 13 epochs...
Completing Train Step...
At time: 372.3937141895294 and batch: 50, loss is 5.225453987121582 and perplexity is 185.94556826006757
At time: 374.8667013645172 and batch: 100, loss is 5.211204128265381 and perplexity is 183.31465971402088
At time: 377.3112733364105 and batch: 150, loss is 5.23175235748291 and perplexity is 187.12041825239876
At time: 379.75631403923035 and batch: 200, loss is 5.24416069984436 and perplexity is 189.4567374126422
At time: 382.19960832595825 and batch: 250, loss is 5.207112150192261 and perplexity is 182.56607279117245
At time: 384.6427233219147 and batch: 300, loss is 5.202128009796143 and perplexity is 181.65840171186326
At time: 387.0854368209839 and batch: 350, loss is 5.163387365341187 and perplexity is 174.75541457759775
At time: 389.5573890209198 and batch: 400, loss is 5.182458162307739 and perplexity is 178.1201214519182
At time: 392.0026319026947 and batch: 450, loss is 5.15711275100708 and perplexity is 173.66232469336933
At time: 394.4466688632965 and batch: 500, loss is 5.160021409988404 and perplexity is 174.1681845040033
At time: 396.89424538612366 and batch: 550, loss is 5.148682947158814 and perplexity is 172.20453841924967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.092771402994791 and perplexity of 162.84053401186014
Finished 14 epochs...
Completing Train Step...
At time: 400.89590311050415 and batch: 50, loss is 5.2042311096191405 and perplexity is 182.04084948630816
At time: 403.3413314819336 and batch: 100, loss is 5.193245792388916 and perplexity is 180.05201698433788
At time: 405.7835946083069 and batch: 150, loss is 5.210733699798584 and perplexity is 183.2284435605666
At time: 408.22640562057495 and batch: 200, loss is 5.225213479995728 and perplexity is 185.90085240333667
At time: 410.6716995239258 and batch: 250, loss is 5.187799854278564 and perplexity is 179.0741300160175
At time: 413.1171007156372 and batch: 300, loss is 5.184007959365845 and perplexity is 178.39638551345283
At time: 415.5629005432129 and batch: 350, loss is 5.147031307220459 and perplexity is 171.92035327639917
At time: 418.0085542201996 and batch: 400, loss is 5.163741235733032 and perplexity is 174.81726628773043
At time: 420.45358204841614 and batch: 450, loss is 5.14027232170105 and perplexity is 170.7622642603875
At time: 422.89780044555664 and batch: 500, loss is 5.142320919036865 and perplexity is 171.1124459481766
At time: 425.3407874107361 and batch: 550, loss is 5.134885549545288 and perplexity is 169.84487993851005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.089303080240885 and perplexity of 162.27672877693112
Finished 15 epochs...
Completing Train Step...
At time: 429.27326369285583 and batch: 50, loss is 5.188623876571655 and perplexity is 179.22175190477705
At time: 431.7456271648407 and batch: 100, loss is 5.178145771026611 and perplexity is 177.3536516398611
At time: 434.191130399704 and batch: 150, loss is 5.196866226196289 and perplexity is 180.70506483865768
At time: 436.64675307273865 and batch: 200, loss is 5.209421329498291 and perplexity is 182.9881377126879
At time: 439.09625005722046 and batch: 250, loss is 5.170475740432739 and perplexity is 175.9985471954435
At time: 441.54350328445435 and batch: 300, loss is 5.169265832901001 and perplexity is 175.7857339957244
At time: 443.9953408241272 and batch: 350, loss is 5.130759391784668 and perplexity is 169.14551700218504
At time: 446.43959498405457 and batch: 400, loss is 5.148226957321167 and perplexity is 172.1260327999765
At time: 448.88484358787537 and batch: 450, loss is 5.124920663833618 and perplexity is 168.16079988941132
At time: 451.376882314682 and batch: 500, loss is 5.128508672714234 and perplexity is 168.76524606344597
At time: 453.8194806575775 and batch: 550, loss is 5.117901134490967 and perplexity is 166.9845235035692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.082185363769531 and perplexity of 161.12578991888896
Finished 16 epochs...
Completing Train Step...
At time: 457.7815098762512 and batch: 50, loss is 5.173829488754272 and perplexity is 176.5897929181918
At time: 460.2281460762024 and batch: 100, loss is 5.1631934928894045 and perplexity is 174.72153760092374
At time: 462.67345809936523 and batch: 150, loss is 5.180835332870483 and perplexity is 177.83129729501712
At time: 465.1169385910034 and batch: 200, loss is 5.1967740249633785 and perplexity is 180.68840437695567
At time: 467.56175565719604 and batch: 250, loss is 5.158980941772461 and perplexity is 173.9870622860937
At time: 470.00625109672546 and batch: 300, loss is 5.156719064712524 and perplexity is 173.5939696723571
At time: 472.45289063453674 and batch: 350, loss is 5.11880898475647 and perplexity is 167.13618928205864
At time: 474.89751625061035 and batch: 400, loss is 5.1366952419281 and perplexity is 170.15252521168696
At time: 477.34228587150574 and batch: 450, loss is 5.1114474868774415 and perplexity is 165.91033417993805
At time: 479.78518557548523 and batch: 500, loss is 5.11702262878418 and perplexity is 166.8378910648739
At time: 482.226993560791 and batch: 550, loss is 5.108657474517822 and perplexity is 165.4480874339053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.082574462890625 and perplexity of 161.18849602078535
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 486.16986536979675 and batch: 50, loss is 5.162123289108276 and perplexity is 174.5346499724994
At time: 488.6438879966736 and batch: 100, loss is 5.141662073135376 and perplexity is 170.99974634438263
At time: 491.0922095775604 and batch: 150, loss is 5.148232345581055 and perplexity is 172.1269602622733
At time: 493.5398108959198 and batch: 200, loss is 5.1429172801971434 and perplexity is 171.21452119881255
At time: 495.98514223098755 and batch: 250, loss is 5.09450587272644 and perplexity is 163.12322107439675
At time: 498.4306626319885 and batch: 300, loss is 5.079881868362427 and perplexity is 160.7550645478647
At time: 500.8761713504791 and batch: 350, loss is 5.011495952606201 and perplexity is 150.12915434741348
At time: 503.3221085071564 and batch: 400, loss is 5.012340297698975 and perplexity is 150.25596869216983
At time: 505.7682726383209 and batch: 450, loss is 4.981759014129639 and perplexity is 145.7304983368333
At time: 508.21373414993286 and batch: 500, loss is 4.994785566329956 and perplexity is 147.64128272671908
At time: 510.66139912605286 and batch: 550, loss is 5.026335401535034 and perplexity is 152.37360024908634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.009011840820312 and perplexity of 149.75667957172294
Finished 18 epochs...
Completing Train Step...
At time: 514.599809885025 and batch: 50, loss is 5.105508327484131 and perplexity is 164.9278866044904
At time: 517.0714111328125 and batch: 100, loss is 5.094911651611328 and perplexity is 163.1894264646099
At time: 519.5150232315063 and batch: 150, loss is 5.105331087112427 and perplexity is 164.8986573149459
At time: 521.9591131210327 and batch: 200, loss is 5.104880504608154 and perplexity is 164.8243736017092
At time: 524.4033386707306 and batch: 250, loss is 5.064496469497681 and perplexity is 158.30071276488073
At time: 526.8472130298615 and batch: 300, loss is 5.053702230453491 and perplexity is 156.60116618837932
At time: 529.2922837734222 and batch: 350, loss is 4.9947576904296875 and perplexity is 147.6371671504093
At time: 531.7364251613617 and batch: 400, loss is 5.009303236007691 and perplexity is 149.8003243060519
At time: 534.1821866035461 and batch: 450, loss is 4.988395185470581 and perplexity is 146.7008068994044
At time: 536.6272673606873 and batch: 500, loss is 5.001552248001099 and perplexity is 148.6437120234931
At time: 539.0714554786682 and batch: 550, loss is 5.021769399642944 and perplexity is 151.679448058148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.002610778808593 and perplexity of 148.80113927811016
Finished 19 epochs...
Completing Train Step...
At time: 543.0378873348236 and batch: 50, loss is 5.088990993499756 and perplexity is 162.2260922633893
At time: 545.4825837612152 and batch: 100, loss is 5.076930389404297 and perplexity is 160.28129885609744
At time: 547.9299774169922 and batch: 150, loss is 5.087466135025024 and perplexity is 161.97890893946771
At time: 550.3760185241699 and batch: 200, loss is 5.0899182796478275 and perplexity is 162.37659203899543
At time: 552.8220732212067 and batch: 250, loss is 5.053000593185425 and perplexity is 156.49132751191948
At time: 555.273268699646 and batch: 300, loss is 5.045020980834961 and perplexity is 155.24755638754937
At time: 557.7191278934479 and batch: 350, loss is 4.991774091720581 and perplexity is 147.19733355882929
At time: 560.1660356521606 and batch: 400, loss is 5.010721063613891 and perplexity is 150.01286597939185
At time: 562.6120858192444 and batch: 450, loss is 4.991286029815674 and perplexity is 147.1255096764915
At time: 565.0585930347443 and batch: 500, loss is 5.002385969161987 and perplexity is 148.76769110645117
At time: 567.5033626556396 and batch: 550, loss is 5.017475538253784 and perplexity is 151.02955380903495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.9986572265625 and perplexity of 148.21400759236036
Finished 20 epochs...
Completing Train Step...
At time: 571.4201076030731 and batch: 50, loss is 5.077477931976318 and perplexity is 160.3690837215023
At time: 573.892911195755 and batch: 100, loss is 5.063797111511231 and perplexity is 158.1900426006902
At time: 576.3397459983826 and batch: 150, loss is 5.075653610229492 and perplexity is 160.0767856185111
At time: 578.7870559692383 and batch: 200, loss is 5.0804724025726316 and perplexity is 160.85002394857162
At time: 581.2319984436035 and batch: 250, loss is 5.046383152008056 and perplexity is 155.45917423069525
At time: 583.6750781536102 and batch: 300, loss is 5.040543823242188 and perplexity is 154.55404225619253
At time: 586.118813753128 and batch: 350, loss is 4.9887409687042235 and perplexity is 146.75154235002063
At time: 588.5632417201996 and batch: 400, loss is 5.010190601348877 and perplexity is 149.93331091706793
At time: 591.0077407360077 and batch: 450, loss is 4.990776195526123 and perplexity is 147.0505191647822
At time: 593.452513217926 and batch: 500, loss is 5.00075517654419 and perplexity is 148.5252795692248
At time: 595.8959572315216 and batch: 550, loss is 5.012934579849243 and perplexity is 150.34528967063758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.994539388020834 and perplexity of 147.60494111882215
Finished 21 epochs...
Completing Train Step...
At time: 599.8649868965149 and batch: 50, loss is 5.06913556098938 and perplexity is 159.0367903000097
At time: 602.3178498744965 and batch: 100, loss is 5.054861059188843 and perplexity is 156.78274530897937
At time: 604.7629909515381 and batch: 150, loss is 5.067897634506226 and perplexity is 158.84003625415005
At time: 607.2079310417175 and batch: 200, loss is 5.073725166320801 and perplexity is 159.76838397956945
At time: 609.6562013626099 and batch: 250, loss is 5.040998649597168 and perplexity is 154.62435349636988
At time: 612.1217756271362 and batch: 300, loss is 5.036389560699463 and perplexity is 153.91331598166752
At time: 614.5818719863892 and batch: 350, loss is 4.985651197433472 and perplexity is 146.2988134251136
At time: 617.0360455513 and batch: 400, loss is 5.008413848876953 and perplexity is 149.66715305461062
At time: 619.4822790622711 and batch: 450, loss is 4.9890031909942625 and perplexity is 146.7900289213203
At time: 621.9302246570587 and batch: 500, loss is 4.998192625045776 and perplexity is 148.14516313349165
At time: 624.3751473426819 and batch: 550, loss is 5.008683567047119 and perplexity is 149.70752644974087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.991045633951823 and perplexity of 147.09014556336192
Finished 22 epochs...
Completing Train Step...
At time: 628.3223030567169 and batch: 50, loss is 5.0622576713562015 and perplexity is 157.94670584622693
At time: 630.8123240470886 and batch: 100, loss is 5.047788143157959 and perplexity is 155.67774650476073
At time: 633.257087469101 and batch: 150, loss is 5.061035995483398 and perplexity is 157.75386398561463
At time: 635.7410945892334 and batch: 200, loss is 5.067778463363648 and perplexity is 158.82110823340182
At time: 638.1887009143829 and batch: 250, loss is 5.036102800369263 and perplexity is 153.86918607601004
At time: 640.6336290836334 and batch: 300, loss is 5.032599020004272 and perplexity is 153.33100562668528
At time: 643.0786411762238 and batch: 350, loss is 4.982297878265381 and perplexity is 145.80904843788628
At time: 645.5218703746796 and batch: 400, loss is 5.005980043411255 and perplexity is 149.30333522996278
At time: 647.9679787158966 and batch: 450, loss is 4.986152696609497 and perplexity is 146.37220055975538
At time: 650.413108587265 and batch: 500, loss is 4.994557189941406 and perplexity is 147.6075687936488
At time: 652.859001159668 and batch: 550, loss is 5.003904571533203 and perplexity is 148.99378170233328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.987941487630208 and perplexity of 146.63426415642408
Finished 23 epochs...
Completing Train Step...
At time: 656.8444309234619 and batch: 50, loss is 5.05557635307312 and perplexity is 156.89493116600454
At time: 659.2921273708344 and batch: 100, loss is 5.040823631286621 and perplexity is 154.5972937712942
At time: 661.7379603385925 and batch: 150, loss is 5.054432010650634 and perplexity is 156.71549232971614
At time: 664.1826577186584 and batch: 200, loss is 5.061979551315307 and perplexity is 157.90278381002022
At time: 666.6321868896484 and batch: 250, loss is 5.031120948791504 and perplexity is 153.10453888945614
At time: 669.0802094936371 and batch: 300, loss is 5.028446254730224 and perplexity is 152.6955782546325
At time: 671.5260629653931 and batch: 350, loss is 4.977934894561767 and perplexity is 145.17427170293473
At time: 673.9744696617126 and batch: 400, loss is 5.002572965621948 and perplexity is 148.79551273923698
At time: 676.4201066493988 and batch: 450, loss is 4.982028169631958 and perplexity is 145.76972778148993
At time: 678.8661272525787 and batch: 500, loss is 4.9898496437072755 and perplexity is 146.91433234060548
At time: 681.3166782855988 and batch: 550, loss is 4.997664880752564 and perplexity is 148.06700099570605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.98508555094401 and perplexity of 146.21608341506294
Finished 24 epochs...
Completing Train Step...
At time: 685.3170223236084 and batch: 50, loss is 5.047864379882813 and perplexity is 155.6896153187009
At time: 687.7999894618988 and batch: 100, loss is 5.033486614227295 and perplexity is 153.46716175824872
At time: 690.2433838844299 and batch: 150, loss is 5.047041559219361 and perplexity is 155.5615633752489
At time: 692.6956388950348 and batch: 200, loss is 5.054823713302612 and perplexity is 156.77689022774243
At time: 695.1572616100311 and batch: 250, loss is 5.025233640670776 and perplexity is 152.20581342703554
At time: 697.6610143184662 and batch: 300, loss is 5.023147869110107 and perplexity is 151.8886777213123
At time: 700.1079294681549 and batch: 350, loss is 4.972106170654297 and perplexity is 144.33055224730202
At time: 702.5532250404358 and batch: 400, loss is 4.9968924236297605 and perplexity is 147.95266974981246
At time: 704.9990813732147 and batch: 450, loss is 4.975412197113037 and perplexity is 144.80850249459255
At time: 707.4428470134735 and batch: 500, loss is 4.982437782287597 and perplexity is 145.8294491372749
At time: 709.8869054317474 and batch: 550, loss is 4.989976892471313 and perplexity is 146.93302819730238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.981917317708334 and perplexity of 145.75356982234612
Finished 25 epochs...
Completing Train Step...
At time: 713.8459937572479 and batch: 50, loss is 5.040320148468018 and perplexity is 154.51947628160622
At time: 716.2909560203552 and batch: 100, loss is 5.0263451480865475 and perplexity is 152.37508537346784
At time: 718.733401298523 and batch: 150, loss is 5.040568819046021 and perplexity is 154.5579055069967
At time: 721.175107717514 and batch: 200, loss is 5.048608741760254 and perplexity is 155.8055478755925
At time: 723.6166627407074 and batch: 250, loss is 5.020054454803467 and perplexity is 151.41954909133952
At time: 726.0590732097626 and batch: 300, loss is 5.018621854782104 and perplexity is 151.2027807503151
At time: 728.5098252296448 and batch: 350, loss is 4.966360321044922 and perplexity is 143.5036285646683
At time: 730.9612500667572 and batch: 400, loss is 4.991788778305054 and perplexity is 147.1994954007778
At time: 733.4026160240173 and batch: 450, loss is 4.969885320663452 and perplexity is 144.01037141006861
At time: 735.845207452774 and batch: 500, loss is 4.976436157226562 and perplexity is 144.9568565666101
At time: 738.2868814468384 and batch: 550, loss is 4.984177417755127 and perplexity is 146.08336001135063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.981238301595052 and perplexity of 145.65463439304511
Finished 26 epochs...
Completing Train Step...
At time: 742.2045035362244 and batch: 50, loss is 5.0348404502868656 and perplexity is 153.67507184212533
At time: 744.6837329864502 and batch: 100, loss is 5.020665454864502 and perplexity is 151.51209471488792
At time: 747.1282284259796 and batch: 150, loss is 5.034437665939331 and perplexity is 153.61318639266293
At time: 749.5717403888702 and batch: 200, loss is 5.043150749206543 and perplexity is 154.95747883801462
At time: 752.0153954029083 and batch: 250, loss is 5.014900398254395 and perplexity is 150.64113189897665
At time: 754.4585084915161 and batch: 300, loss is 5.013670110702515 and perplexity is 150.45591394863192
At time: 756.9018938541412 and batch: 350, loss is 4.960927619934082 and perplexity is 142.72613011297864
At time: 759.37171626091 and batch: 400, loss is 4.986393079757691 and perplexity is 146.40739019946272
At time: 761.8132555484772 and batch: 450, loss is 4.964036989212036 and perplexity is 143.17060902353947
At time: 764.2551074028015 and batch: 500, loss is 4.9705790996551515 and perplexity is 144.11031744656108
At time: 766.6960456371307 and batch: 550, loss is 4.978833875656128 and perplexity is 145.30483930869445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.9787846883138025 and perplexity of 145.29769232559383
Finished 27 epochs...
Completing Train Step...
At time: 770.6246256828308 and batch: 50, loss is 5.029886884689331 and perplexity is 152.9157146087298
At time: 773.0658402442932 and batch: 100, loss is 5.0152931404113765 and perplexity is 150.70030664149624
At time: 775.5091161727905 and batch: 150, loss is 5.028267669677734 and perplexity is 152.66831154156105
At time: 777.9525763988495 and batch: 200, loss is 5.037571048736572 and perplexity is 154.09527019044612
At time: 780.3940870761871 and batch: 250, loss is 5.0094934177398684 and perplexity is 149.82881630045173
At time: 782.8371315002441 and batch: 300, loss is 5.008032951354981 and perplexity is 149.61015606258712
At time: 785.2801795005798 and batch: 350, loss is 4.95530948638916 and perplexity is 141.92652390407792
At time: 787.7235658168793 and batch: 400, loss is 4.98153603553772 and perplexity is 145.69800717806547
At time: 790.1649420261383 and batch: 450, loss is 4.9589419937133785 and perplexity is 142.44301054448778
At time: 792.6052489280701 and batch: 500, loss is 4.965579566955566 and perplexity is 143.39163124670026
At time: 795.0495595932007 and batch: 550, loss is 4.973584756851197 and perplexity is 144.54411525639657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.974324035644531 and perplexity of 144.65101316431648
Finished 28 epochs...
Completing Train Step...
At time: 799.0051140785217 and batch: 50, loss is 5.0241563034057615 and perplexity is 152.04192472983067
At time: 801.4823231697083 and batch: 100, loss is 5.00896842956543 and perplexity is 149.75017858744468
At time: 803.9273581504822 and batch: 150, loss is 5.021992073059082 and perplexity is 151.71322679967935
At time: 806.3730659484863 and batch: 200, loss is 5.03157974243164 and perplexity is 153.17479839424925
At time: 808.8186318874359 and batch: 250, loss is 5.003931970596313 and perplexity is 148.9978640482871
At time: 811.2650783061981 and batch: 300, loss is 5.003172731399536 and perplexity is 148.88478196327335
At time: 813.7127387523651 and batch: 350, loss is 4.950591888427734 and perplexity is 141.25854848283487
At time: 816.1609103679657 and batch: 400, loss is 4.9769731044769285 and perplexity is 145.03471165233185
At time: 818.6196603775024 and batch: 450, loss is 4.9538710403442385 and perplexity is 141.7225170187237
At time: 821.0953261852264 and batch: 500, loss is 4.9603434085845945 and perplexity is 142.64277223957697
At time: 823.5380864143372 and batch: 550, loss is 4.968259582519531 and perplexity is 143.77643846452762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.971590677897136 and perplexity of 144.25617006636637
Finished 29 epochs...
Completing Train Step...
At time: 827.4621961116791 and batch: 50, loss is 5.019045934677124 and perplexity is 151.26691640806274
At time: 829.9276099205017 and batch: 100, loss is 5.0039290428161625 and perplexity is 148.99742781593685
At time: 832.4001240730286 and batch: 150, loss is 5.015827407836914 and perplexity is 150.78084241825871
At time: 834.8539960384369 and batch: 200, loss is 5.025976448059082 and perplexity is 152.31891503094536
At time: 837.3239228725433 and batch: 250, loss is 4.999334897994995 and perplexity is 148.31448203168478
At time: 839.7828810214996 and batch: 300, loss is 4.997953548431396 and perplexity is 148.10974932292854
At time: 842.2308514118195 and batch: 350, loss is 4.945456504821777 and perplexity is 140.53499111123406
At time: 844.6887488365173 and batch: 400, loss is 4.971748332977295 and perplexity is 144.27891457726844
At time: 847.1363229751587 and batch: 450, loss is 4.9481729316711425 and perplexity is 140.91726310625484
At time: 849.5848939418793 and batch: 500, loss is 4.954426250457764 and perplexity is 141.80122464109886
At time: 852.0401344299316 and batch: 550, loss is 4.9629264831542965 and perplexity is 143.0117054429533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.970273335774739 and perplexity of 144.06626045255314
Finished 30 epochs...
Completing Train Step...
At time: 856.0783982276917 and batch: 50, loss is 5.013375368118286 and perplexity is 150.41157471839279
At time: 858.544075012207 and batch: 100, loss is 4.998293361663818 and perplexity is 148.16008752790887
At time: 860.9992606639862 and batch: 150, loss is 5.009290742874145 and perplexity is 149.79845284228543
At time: 863.4572010040283 and batch: 200, loss is 5.019691505432129 and perplexity is 151.36460143340068
At time: 865.8976378440857 and batch: 250, loss is 4.99257758140564 and perplexity is 147.31565262572138
At time: 868.339225769043 and batch: 300, loss is 4.991201772689819 and perplexity is 147.11311382613292
At time: 870.7800004482269 and batch: 350, loss is 4.938573274612427 and perplexity is 139.57097798323926
At time: 873.2212262153625 and batch: 400, loss is 4.96546332359314 and perplexity is 143.37496389009388
At time: 875.6641073226929 and batch: 450, loss is 4.94307954788208 and perplexity is 140.2013421805035
At time: 878.1076536178589 and batch: 500, loss is 4.947061576843262 and perplexity is 140.76074101746963
At time: 880.5519812107086 and batch: 550, loss is 4.954431791305542 and perplexity is 141.80201034227602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.960662333170573 and perplexity of 142.68827178173197
Finished 31 epochs...
Completing Train Step...
At time: 884.5031816959381 and batch: 50, loss is 5.005265493392944 and perplexity is 149.19668863574086
At time: 886.945556640625 and batch: 100, loss is 4.98941143989563 and perplexity is 146.84996802356292
At time: 889.3874471187592 and batch: 150, loss is 5.0012241458892825 and perplexity is 148.5949497076168
At time: 891.8300180435181 and batch: 200, loss is 5.012735195159912 and perplexity is 150.31531610999815
At time: 894.2744765281677 and batch: 250, loss is 4.986393051147461 and perplexity is 146.40738601071374
At time: 896.7177872657776 and batch: 300, loss is 4.986369600296021 and perplexity is 146.40395267311212
At time: 899.1610667705536 and batch: 350, loss is 4.9344303417205815 and perplexity is 138.99394092605743
At time: 901.6041376590729 and batch: 400, loss is 4.958417129516602 and perplexity is 142.36826692501265
At time: 904.0497093200684 and batch: 450, loss is 4.934077091217041 and perplexity is 138.94484991765847
At time: 906.5039212703705 and batch: 500, loss is 4.940099277496338 and perplexity is 139.78412628925386
At time: 908.9525983333588 and batch: 550, loss is 4.9471446228027345 and perplexity is 140.77243111366442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.958311462402344 and perplexity of 142.35322407586605
Finished 32 epochs...
Completing Train Step...
At time: 912.8790080547333 and batch: 50, loss is 4.999812936782837 and perplexity is 148.38539905608823
At time: 915.3495101928711 and batch: 100, loss is 4.983278293609619 and perplexity is 145.95207196607788
At time: 917.7895624637604 and batch: 150, loss is 4.994393463134766 and perplexity is 147.58340345608502
At time: 920.2296772003174 and batch: 200, loss is 5.006149625778198 and perplexity is 149.3286565899109
At time: 922.6686766147614 and batch: 250, loss is 4.983439521789551 and perplexity is 145.97560545007772
At time: 925.111200094223 and batch: 300, loss is 4.980468091964721 and perplexity is 145.5424929826741
At time: 927.5533258914948 and batch: 350, loss is 4.930075731277466 and perplexity is 138.38999239403415
At time: 929.9944705963135 and batch: 400, loss is 4.956115989685059 and perplexity is 142.0410342836744
At time: 932.4371168613434 and batch: 450, loss is 4.932902755737305 and perplexity is 138.78177782008908
At time: 934.8804745674133 and batch: 500, loss is 4.940830373764038 and perplexity is 139.88635930880358
At time: 937.3219919204712 and batch: 550, loss is 4.950539655685425 and perplexity is 141.25117035416463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.959272257486979 and perplexity of 142.49006207995433
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 941.2785181999207 and batch: 50, loss is 4.998881998062134 and perplexity is 148.2473256214451
At time: 943.7210495471954 and batch: 100, loss is 4.977026472091675 and perplexity is 145.04245201548864
At time: 946.1912708282471 and batch: 150, loss is 4.986589574813843 and perplexity is 146.43616135442338
At time: 948.6342799663544 and batch: 200, loss is 4.996045017242432 and perplexity is 147.82734681966943
At time: 951.0795133113861 and batch: 250, loss is 4.966491918563843 and perplexity is 143.52251452878934
At time: 953.5235176086426 and batch: 300, loss is 4.961436052322387 and perplexity is 142.79871515091406
At time: 955.9755461215973 and batch: 350, loss is 4.904637203216553 and perplexity is 134.9139547781122
At time: 958.4190852642059 and batch: 400, loss is 4.923525285720825 and perplexity is 137.48643885324077
At time: 960.8623168468475 and batch: 450, loss is 4.901032419204712 and perplexity is 134.4284946261421
At time: 963.3071873188019 and batch: 500, loss is 4.912583456039429 and perplexity is 135.99028590820072
At time: 965.753214597702 and batch: 550, loss is 4.933230781555176 and perplexity is 138.82730929360525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.945665486653646 and perplexity of 140.56436344015282
Finished 34 epochs...
Completing Train Step...
At time: 969.7099480628967 and batch: 50, loss is 4.990669107437133 and perplexity is 147.03477264884717
At time: 972.1827259063721 and batch: 100, loss is 4.970447149276733 and perplexity is 144.09130329013018
At time: 974.6271257400513 and batch: 150, loss is 4.980907926559448 and perplexity is 145.60652168607783
At time: 977.0680766105652 and batch: 200, loss is 4.991044006347656 and perplexity is 147.0899061590229
At time: 979.5087375640869 and batch: 250, loss is 4.96275782585144 and perplexity is 142.98758750832653
At time: 981.949954032898 and batch: 300, loss is 4.958196516036987 and perplexity is 142.33686203055856
At time: 984.3925521373749 and batch: 350, loss is 4.902634248733521 and perplexity is 134.64399871267292
At time: 986.8358161449432 and batch: 400, loss is 4.923009586334229 and perplexity is 137.41555545989095
At time: 989.2788465023041 and batch: 450, loss is 4.90159704208374 and perplexity is 134.50441746166933
At time: 991.7225155830383 and batch: 500, loss is 4.913138799667358 and perplexity is 136.06582822096962
At time: 994.1656913757324 and batch: 550, loss is 4.931171712875366 and perplexity is 138.5417484247496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.943656921386719 and perplexity of 140.28231409406266
Finished 35 epochs...
Completing Train Step...
At time: 998.1530523300171 and batch: 50, loss is 4.986791286468506 and perplexity is 146.46570221410033
At time: 1000.60205245018 and batch: 100, loss is 4.9670467281341555 and perplexity is 143.60216428658606
At time: 1003.0487937927246 and batch: 150, loss is 4.977861337661743 and perplexity is 145.16359352627657
At time: 1005.4895403385162 and batch: 200, loss is 4.988489255905152 and perplexity is 146.71460775717748
At time: 1007.9701411724091 and batch: 250, loss is 4.960828971862793 and perplexity is 142.7120511499632
At time: 1010.4289720058441 and batch: 300, loss is 4.956502437591553 and perplexity is 142.09593635172092
At time: 1012.8758893013 and batch: 350, loss is 4.901831483840942 and perplexity is 134.53595461031833
At time: 1015.3195130825043 and batch: 400, loss is 4.9230202293396 and perplexity is 137.4170179821686
At time: 1017.7651309967041 and batch: 450, loss is 4.901910104751587 and perplexity is 134.5465323653952
At time: 1020.2111461162567 and batch: 500, loss is 4.912920913696289 and perplexity is 136.03618461543854
At time: 1022.657469034195 and batch: 550, loss is 4.929211282730103 and perplexity is 138.2704130586775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.942278035481771 and perplexity of 140.08901408839876
Finished 36 epochs...
Completing Train Step...
At time: 1026.5743854045868 and batch: 50, loss is 4.983901329040528 and perplexity is 146.04303361134885
At time: 1029.054127216339 and batch: 100, loss is 4.964519805908203 and perplexity is 143.23975087405327
At time: 1031.496469259262 and batch: 150, loss is 4.975613355636597 and perplexity is 144.8376348891697
At time: 1033.9382758140564 and batch: 200, loss is 4.986587162017822 and perplexity is 146.4358080342622
At time: 1036.3816123008728 and batch: 250, loss is 4.959352741241455 and perplexity is 142.5015306766377
At time: 1038.825398683548 and batch: 300, loss is 4.955236358642578 and perplexity is 141.9161455166833
At time: 1041.2704622745514 and batch: 350, loss is 4.901245994567871 and perplexity is 134.45720830685912
At time: 1043.7146756649017 and batch: 400, loss is 4.92291579246521 and perplexity is 137.4026673277043
At time: 1046.1590504646301 and batch: 450, loss is 4.901724481582642 and perplexity is 134.5215597295086
At time: 1048.6025927066803 and batch: 500, loss is 4.912407932281494 and perplexity is 135.96641847688733
At time: 1051.0451402664185 and batch: 550, loss is 4.927683477401733 and perplexity is 138.05932407734593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.941734822591146 and perplexity of 140.01293659511916
Finished 37 epochs...
Completing Train Step...
At time: 1054.9832780361176 and batch: 50, loss is 4.981845378875732 and perplexity is 145.74308485782822
At time: 1057.4559137821198 and batch: 100, loss is 4.962487239837646 and perplexity is 142.94890230107464
At time: 1059.906679391861 and batch: 150, loss is 4.973705797195435 and perplexity is 144.56161198474845
At time: 1062.3516221046448 and batch: 200, loss is 4.984880104064941 and perplexity is 146.18604686261924
At time: 1064.7963497638702 and batch: 250, loss is 4.95798550605774 and perplexity is 142.30683070082185
At time: 1067.2679891586304 and batch: 300, loss is 4.954112548828125 and perplexity is 141.75674834235255
At time: 1069.7111616134644 and batch: 350, loss is 4.900563659667969 and perplexity is 134.36549475432514
At time: 1072.1555533409119 and batch: 400, loss is 4.922442646026611 and perplexity is 137.33767112257829
At time: 1074.5989036560059 and batch: 450, loss is 4.901318206787109 and perplexity is 134.46691811084253
At time: 1077.0439493656158 and batch: 500, loss is 4.911815633773804 and perplexity is 135.8859096151205
At time: 1079.4872105121613 and batch: 550, loss is 4.926197156906128 and perplexity is 137.85427609561776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.940532938639323 and perplexity of 139.8447583791721
Finished 38 epochs...
Completing Train Step...
At time: 1083.4501757621765 and batch: 50, loss is 4.979835834503174 and perplexity is 145.4505017396529
At time: 1085.9012739658356 and batch: 100, loss is 4.960431280136109 and perplexity is 142.65530703200372
At time: 1088.3464076519012 and batch: 150, loss is 4.971751098632812 and perplexity is 144.2793136035963
At time: 1090.7975945472717 and batch: 200, loss is 4.983310222625732 and perplexity is 145.95673214653263
At time: 1093.2429192066193 and batch: 250, loss is 4.956811599731445 and perplexity is 142.1398738270254
At time: 1095.6878633499146 and batch: 300, loss is 4.953013200759887 and perplexity is 141.60099396472648
At time: 1098.132863521576 and batch: 350, loss is 4.8998118019104 and perplexity is 134.2645089829695
At time: 1100.5769522190094 and batch: 400, loss is 4.92176589012146 and perplexity is 137.24475848578032
At time: 1103.0339391231537 and batch: 450, loss is 4.900785026550293 and perplexity is 134.3952421174098
At time: 1105.4876883029938 and batch: 500, loss is 4.911026554107666 and perplexity is 135.7787271002615
At time: 1107.9330921173096 and batch: 550, loss is 4.924497146606445 and perplexity is 137.62012149540877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.9388280232747395 and perplexity of 139.60653803242332
Finished 39 epochs...
Completing Train Step...
At time: 1111.8901014328003 and batch: 50, loss is 4.977598390579224 and perplexity is 145.12542820082123
At time: 1114.377515554428 and batch: 100, loss is 4.958469944000244 and perplexity is 142.3757862300797
At time: 1116.820411682129 and batch: 150, loss is 4.970070037841797 and perplexity is 144.03697505652752
At time: 1119.2626795768738 and batch: 200, loss is 4.98194658279419 and perplexity is 145.75783537549637
At time: 1121.7037270069122 and batch: 250, loss is 4.955775842666626 and perplexity is 141.99272766550956
At time: 1124.1540007591248 and batch: 300, loss is 4.9521516418457034 and perplexity is 141.47904890508863
At time: 1126.604120016098 and batch: 350, loss is 4.8992649173736575 and perplexity is 134.19110187360943
At time: 1129.0879364013672 and batch: 400, loss is 4.921121110916138 and perplexity is 137.15629444242188
At time: 1131.5309536457062 and batch: 450, loss is 4.900223836898804 and perplexity is 134.3198420571698
At time: 1133.9753785133362 and batch: 500, loss is 4.910183343887329 and perplexity is 135.6642853458754
At time: 1136.4364576339722 and batch: 550, loss is 4.922766008377075 and perplexity is 137.3820881357276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.937367248535156 and perplexity of 139.40275320628544
Finished 40 epochs...
Completing Train Step...
At time: 1140.4140226840973 and batch: 50, loss is 4.975386266708374 and perplexity is 144.80474760020732
At time: 1142.8573818206787 and batch: 100, loss is 4.956705265045166 and perplexity is 142.12476023169816
At time: 1145.301879644394 and batch: 150, loss is 4.968653316497803 and perplexity is 143.83305927966393
At time: 1147.7472965717316 and batch: 200, loss is 4.980939340591431 and perplexity is 145.61109584585301
At time: 1150.1901037693024 and batch: 250, loss is 4.955069208145142 and perplexity is 141.8924261447736
At time: 1152.633936882019 and batch: 300, loss is 4.951614904403686 and perplexity is 141.40313217777788
At time: 1155.0776529312134 and batch: 350, loss is 4.898831787109375 and perplexity is 134.13299223162136
At time: 1157.5204629898071 and batch: 400, loss is 4.920521287918091 and perplexity is 137.0740496113249
At time: 1159.9639775753021 and batch: 450, loss is 4.899615020751953 and perplexity is 134.23809085673346
At time: 1162.4062576293945 and batch: 500, loss is 4.909255084991455 and perplexity is 135.53841219662428
At time: 1164.848391532898 and batch: 550, loss is 4.921045227050781 and perplexity is 137.14588688752934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.9362238566080725 and perplexity of 139.2434523126891
Finished 41 epochs...
Completing Train Step...
At time: 1168.7831506729126 and batch: 50, loss is 4.973417129516601 and perplexity is 144.51988774227826
At time: 1171.2617013454437 and batch: 100, loss is 4.9552391910552975 and perplexity is 141.91654748234822
At time: 1173.7052023410797 and batch: 150, loss is 4.967469720840454 and perplexity is 143.66291980335237
At time: 1176.1475162506104 and batch: 200, loss is 4.980154705047608 and perplexity is 145.49688901570576
At time: 1178.5935506820679 and batch: 250, loss is 4.954513921737671 and perplexity is 141.8136570809315
At time: 1181.0376155376434 and batch: 300, loss is 4.951185541152954 and perplexity is 141.3424319014488
At time: 1183.479091644287 and batch: 350, loss is 4.898417825698853 and perplexity is 134.07747784016018
At time: 1185.9214959144592 and batch: 400, loss is 4.9200186252594 and perplexity is 137.00516491944128
At time: 1188.3651814460754 and batch: 450, loss is 4.899017066955566 and perplexity is 134.1578466742337
At time: 1190.8535633087158 and batch: 500, loss is 4.90833797454834 and perplexity is 135.41416548604207
At time: 1193.294317483902 and batch: 550, loss is 4.919364643096924 and perplexity is 136.9155952770835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.935350545247396 and perplexity of 139.12190250702295
Finished 42 epochs...
Completing Train Step...
At time: 1197.2111825942993 and batch: 50, loss is 4.971649150848389 and perplexity is 144.26460539698357
At time: 1199.6528232097626 and batch: 100, loss is 4.953861865997315 and perplexity is 141.7212168131499
At time: 1202.0940177440643 and batch: 150, loss is 4.9664195537567135 and perplexity is 143.51212892548733
At time: 1204.535584449768 and batch: 200, loss is 4.979414339065552 and perplexity is 145.38920793520677
At time: 1206.9826972484589 and batch: 250, loss is 4.953950881958008 and perplexity is 141.73383282492003
At time: 1209.4399087429047 and batch: 300, loss is 4.9507311248779295 and perplexity is 141.27821819102377
At time: 1211.8892030715942 and batch: 350, loss is 4.8980749988555905 and perplexity is 134.03152035985875
At time: 1214.332358598709 and batch: 400, loss is 4.91958945274353 and perplexity is 136.94637868374633
At time: 1216.7758903503418 and batch: 450, loss is 4.898502950668335 and perplexity is 134.08889166716352
At time: 1219.219050168991 and batch: 500, loss is 4.907490310668945 and perplexity is 135.29942842530232
At time: 1221.6614451408386 and batch: 550, loss is 4.917834482192993 and perplexity is 136.70625259089516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.934587097167968 and perplexity of 139.01573069123893
Finished 43 epochs...
Completing Train Step...
At time: 1225.6031112670898 and batch: 50, loss is 4.970134162902832 and perplexity is 144.04621173249242
At time: 1228.074623823166 and batch: 100, loss is 4.952621374130249 and perplexity is 141.54552179295464
At time: 1230.5170531272888 and batch: 150, loss is 4.965406255722046 and perplexity is 143.36678201959975
At time: 1232.959100484848 and batch: 200, loss is 4.978655710220337 and perplexity is 145.2789533147396
At time: 1235.403208732605 and batch: 250, loss is 4.953394927978516 and perplexity is 141.65505723636232
At time: 1237.8665800094604 and batch: 300, loss is 4.950267467498779 and perplexity is 141.21272868616563
At time: 1240.31405377388 and batch: 350, loss is 4.897681760787964 and perplexity is 133.9788244254941
At time: 1242.7614595890045 and batch: 400, loss is 4.919188470840454 and perplexity is 136.8914766722945
At time: 1245.2088825702667 and batch: 450, loss is 4.89801965713501 and perplexity is 134.02410303015583
At time: 1247.6588232517242 and batch: 500, loss is 4.906725072860718 and perplexity is 135.19593179205873
At time: 1250.1040260791779 and batch: 550, loss is 4.916443872451782 and perplexity is 136.51627966407315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.933973693847657 and perplexity of 138.93048412839264
Finished 44 epochs...
Completing Train Step...
At time: 1254.0825335979462 and batch: 50, loss is 4.968730363845825 and perplexity is 143.844141662368
At time: 1256.5279984474182 and batch: 100, loss is 4.951471176147461 and perplexity is 141.3828100126348
At time: 1258.9738323688507 and batch: 150, loss is 4.964488477706909 and perplexity is 143.23526350059555
At time: 1261.4189438819885 and batch: 200, loss is 4.9779260635375975 and perplexity is 145.17298967109335
At time: 1263.865361213684 and batch: 250, loss is 4.952805280685425 and perplexity is 141.57155533606436
At time: 1266.3118331432343 and batch: 300, loss is 4.9497426319122315 and perplexity is 141.1386346661676
At time: 1268.7567455768585 and batch: 350, loss is 4.897240362167358 and perplexity is 133.91969940701316
At time: 1271.202919960022 and batch: 400, loss is 4.918793268203736 and perplexity is 136.83738748856734
At time: 1273.648472070694 and batch: 450, loss is 4.89749324798584 and perplexity is 133.9535700823347
At time: 1276.0962250232697 and batch: 500, loss is 4.906004476547241 and perplexity is 135.09854519443797
At time: 1278.5395712852478 and batch: 550, loss is 4.915170211791992 and perplexity is 136.3425149313864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.933348592122396 and perplexity of 138.8436655811116
Finished 45 epochs...
Completing Train Step...
At time: 1282.4948859214783 and batch: 50, loss is 4.967432689666748 and perplexity is 143.65759989531583
At time: 1284.9898800849915 and batch: 100, loss is 4.950387296676635 and perplexity is 141.22965110522642
At time: 1287.4347169399261 and batch: 150, loss is 4.9636546325683595 and perplexity is 143.11587725419474
At time: 1289.8780567646027 and batch: 200, loss is 4.97725998878479 and perplexity is 145.0763258041383
At time: 1292.320958852768 and batch: 250, loss is 4.952204437255859 and perplexity is 141.48651854668367
At time: 1294.7674584388733 and batch: 300, loss is 4.9491746139526365 and perplexity is 141.05848815136932
At time: 1297.215082168579 and batch: 350, loss is 4.8967420196533205 and perplexity is 133.85297815373082
At time: 1299.6592519283295 and batch: 400, loss is 4.918392839431763 and perplexity is 136.7826048305436
At time: 1302.102861881256 and batch: 450, loss is 4.897037715911865 and perplexity is 133.89256383094582
At time: 1304.5488879680634 and batch: 500, loss is 4.905350265502929 and perplexity is 135.0101911383522
At time: 1306.9944002628326 and batch: 550, loss is 4.913997783660888 and perplexity is 136.18275680217803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.932802327473959 and perplexity of 138.7678409070002
Finished 46 epochs...
Completing Train Step...
At time: 1310.9482247829437 and batch: 50, loss is 4.966193037033081 and perplexity is 143.47962470975483
At time: 1313.3916630744934 and batch: 100, loss is 4.94942268371582 and perplexity is 141.09348483775045
At time: 1315.8612143993378 and batch: 150, loss is 4.962857294082641 and perplexity is 143.00181093811767
At time: 1318.3052089214325 and batch: 200, loss is 4.976656332015991 and perplexity is 144.98877592577105
At time: 1320.7485213279724 and batch: 250, loss is 4.951633405685425 and perplexity is 141.40574834116612
At time: 1323.2016179561615 and batch: 300, loss is 4.948575038909912 and perplexity is 140.973938351817
At time: 1325.6471509933472 and batch: 350, loss is 4.89623420715332 and perplexity is 133.7850231939114
At time: 1328.093823671341 and batch: 400, loss is 4.9180128860473635 and perplexity is 136.73064368894217
At time: 1330.5379927158356 and batch: 450, loss is 4.896558589935303 and perplexity is 133.82842779139415
At time: 1332.9819707870483 and batch: 500, loss is 4.904751424789429 and perplexity is 134.92936574234534
At time: 1335.4323093891144 and batch: 550, loss is 4.912878694534302 and perplexity is 136.03044140296186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.9321955362955725 and perplexity of 138.68366334697734
Finished 47 epochs...
Completing Train Step...
At time: 1339.4004492759705 and batch: 50, loss is 4.965016288757324 and perplexity is 143.31088461055347
At time: 1341.8918826580048 and batch: 100, loss is 4.94852053642273 and perplexity is 140.9662551309282
At time: 1344.3363060951233 and batch: 150, loss is 4.962142724990844 and perplexity is 142.89966276426065
At time: 1346.7808232307434 and batch: 200, loss is 4.976068105697632 and perplexity is 144.90351479078504
At time: 1349.2242512702942 and batch: 250, loss is 4.95104043006897 and perplexity is 141.32192303601042
At time: 1351.6731078624725 and batch: 300, loss is 4.947901792526245 and perplexity is 140.8790600994343
At time: 1354.1170048713684 and batch: 350, loss is 4.895636014938354 and perplexity is 133.7050179661957
At time: 1356.5621316432953 and batch: 400, loss is 4.917570762634277 and perplexity is 136.6702052316939
At time: 1359.0094079971313 and batch: 450, loss is 4.89610405921936 and perplexity is 133.76761248255661
At time: 1361.4547157287598 and batch: 500, loss is 4.9041861724853515 and perplexity is 134.85311815906306
At time: 1363.8991799354553 and batch: 550, loss is 4.911827430725098 and perplexity is 135.88751266403332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.931744893391927 and perplexity of 138.62118061799507
Finished 48 epochs...
Completing Train Step...
At time: 1367.8718135356903 and batch: 50, loss is 4.963961801528931 and perplexity is 143.15984476183326
At time: 1370.323088645935 and batch: 100, loss is 4.947623596191407 and perplexity is 140.8398735122943
At time: 1372.7688074111938 and batch: 150, loss is 4.961628494262695 and perplexity is 142.82619825709747
At time: 1375.21360039711 and batch: 200, loss is 4.975467844009399 and perplexity is 144.8165608625327
At time: 1377.6848459243774 and batch: 250, loss is 4.950427513122559 and perplexity is 141.23533097406423
At time: 1380.1276433467865 and batch: 300, loss is 4.947172546386719 and perplexity is 140.77636203934986
At time: 1382.5716638565063 and batch: 350, loss is 4.894972658157348 and perplexity is 133.6163532472959
At time: 1385.015480518341 and batch: 400, loss is 4.917077178955078 and perplexity is 136.60276369434905
At time: 1387.4667463302612 and batch: 450, loss is 4.895674104690552 and perplexity is 133.71011085419042
At time: 1389.9121401309967 and batch: 500, loss is 4.9037122631073 and perplexity is 134.78922514265363
At time: 1392.3560996055603 and batch: 550, loss is 4.9109068775177 and perplexity is 135.76247853751792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.931166076660157 and perplexity of 138.54096757584054
Finished 49 epochs...
Completing Train Step...
At time: 1396.342839717865 and batch: 50, loss is 4.9629807472229 and perplexity is 143.0194660505078
At time: 1398.8341097831726 and batch: 100, loss is 4.9468919467926025 and perplexity is 140.7368657908752
At time: 1401.2811012268066 and batch: 150, loss is 4.961162509918213 and perplexity is 142.75965898906045
At time: 1403.7491188049316 and batch: 200, loss is 4.974897594451904 and perplexity is 144.73400282436242
At time: 1406.2199873924255 and batch: 250, loss is 4.949804267883301 and perplexity is 141.1473341510685
At time: 1408.6747999191284 and batch: 300, loss is 4.946400461196899 and perplexity is 140.66771264383442
At time: 1411.1537306308746 and batch: 350, loss is 4.894308738708496 and perplexity is 133.52767219348593
At time: 1413.64586019516 and batch: 400, loss is 4.916565971374512 and perplexity is 136.53294917239901
At time: 1416.0925896167755 and batch: 450, loss is 4.895251712799072 and perplexity is 133.65364471381622
At time: 1418.5423383712769 and batch: 500, loss is 4.903238306045532 and perplexity is 134.7253559743643
At time: 1420.995305776596 and batch: 550, loss is 4.910028800964356 and perplexity is 135.6433210106615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.930724080403646 and perplexity of 138.47974651753466
Finished Training.
Improved accuracyfrom -150.9095121006645 to -138.47974651753466
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe57da976a0>
SETTINGS FOR THIS RUN
{'lr': 26.31165848231822, 'tune_wordvecs': True, 'anneal': 4.982039223058078, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.11989636450487262}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9337949752807617 and batch: 50, loss is 7.608650827407837 and perplexity is 2015.55692830038
At time: 5.376276254653931 and batch: 100, loss is 6.341332902908325 and perplexity is 567.5522995509533
At time: 7.805254936218262 and batch: 150, loss is 6.166447267532349 and perplexity is 476.4902530244412
At time: 10.238150596618652 and batch: 200, loss is 6.155609397888184 and perplexity is 471.35399708664136
At time: 12.670592308044434 and batch: 250, loss is 6.133229303359985 and perplexity is 460.92221759017053
At time: 15.22940993309021 and batch: 300, loss is 6.1770737361907955 and perplexity is 481.58066039000306
At time: 17.672243118286133 and batch: 350, loss is 6.221531972885132 and perplexity is 503.47394994971154
At time: 20.122019052505493 and batch: 400, loss is 6.304468812942505 and perplexity is 547.0109458642426
At time: 22.578300714492798 and batch: 450, loss is 6.386331329345703 and perplexity is 593.6745831006859
At time: 25.008551359176636 and batch: 500, loss is 6.3912349319458 and perplexity is 596.5928765580774
At time: 27.451598405838013 and batch: 550, loss is 6.393148288726807 and perplexity is 597.7354643243871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.991152445475261 and perplexity of 399.87517882637076
Finished 1 epochs...
Completing Train Step...
At time: 31.52084469795227 and batch: 50, loss is 6.353848419189453 and perplexity is 574.7001457874762
At time: 33.95870327949524 and batch: 100, loss is 6.361157531738281 and perplexity is 578.9160824435977
At time: 36.406652212142944 and batch: 150, loss is 6.419352807998657 and perplexity is 613.6058647834544
At time: 38.84817957878113 and batch: 200, loss is 6.464104032516479 and perplexity is 641.6891731749347
At time: 41.28812074661255 and batch: 250, loss is 6.407384195327759 and perplexity is 606.3056278543393
At time: 43.728076219558716 and batch: 300, loss is 6.445642566680908 and perplexity is 629.951332675923
At time: 46.20763921737671 and batch: 350, loss is 6.441238088607788 and perplexity is 627.1828272304011
At time: 48.64635157585144 and batch: 400, loss is 6.438649702072143 and perplexity is 625.5615348159255
At time: 51.08630728721619 and batch: 450, loss is 6.413737878799439 and perplexity is 610.1701659259456
At time: 53.527405738830566 and batch: 500, loss is 6.428634948730469 and perplexity is 619.3279563200313
At time: 55.96814775466919 and batch: 550, loss is 6.434868879318238 and perplexity is 623.2008629840884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.049295043945312 and perplexity of 423.81415435585296
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.988306283950806 and batch: 50, loss is 6.342255239486694 and perplexity is 568.0760152809264
At time: 62.436357498168945 and batch: 100, loss is 6.21390061378479 and perplexity is 499.6463827854223
At time: 64.88318657875061 and batch: 150, loss is 6.150105791091919 and perplexity is 468.7669755295436
At time: 67.3302206993103 and batch: 200, loss is 6.102424230575561 and perplexity is 446.9399433098307
At time: 69.77516031265259 and batch: 250, loss is 5.9816660308837895 and perplexity is 396.09973313005924
At time: 72.22171449661255 and batch: 300, loss is 5.928723039627076 and perplexity is 375.6744859699915
At time: 74.68183040618896 and batch: 350, loss is 5.834382638931275 and perplexity is 341.85362177344314
At time: 77.12561297416687 and batch: 400, loss is 5.775167083740234 and perplexity is 322.1982643124245
At time: 79.56831574440002 and batch: 450, loss is 5.71773250579834 and perplexity is 304.21433605882913
At time: 82.01030588150024 and batch: 500, loss is 5.722401895523071 and perplexity is 305.6381529447272
At time: 84.45680069923401 and batch: 550, loss is 5.765985488891602 and perplexity is 319.25350984707427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4888554890950525 and perplexity of 241.9800994493772
Finished 3 epochs...
Completing Train Step...
At time: 88.41535019874573 and batch: 50, loss is 5.877643957138061 and perplexity is 356.96722010798067
At time: 90.89252543449402 and batch: 100, loss is 5.857617311477661 and perplexity is 349.8894724035735
At time: 93.33333897590637 and batch: 150, loss is 5.842258396148682 and perplexity is 344.55660796229466
At time: 95.77748513221741 and batch: 200, loss is 5.828359317779541 and perplexity is 339.80071648322826
At time: 98.22219944000244 and batch: 250, loss is 5.749634637832641 and perplexity is 314.0758878731138
At time: 100.6874406337738 and batch: 300, loss is 5.72899172782898 and perplexity is 307.65890802472285
At time: 103.13741683959961 and batch: 350, loss is 5.676769714355469 and perplexity is 292.00464636642533
At time: 105.59141755104065 and batch: 400, loss is 5.676606788635254 and perplexity is 291.95707517450035
At time: 108.03776097297668 and batch: 450, loss is 5.664707384109497 and perplexity is 288.5035480413645
At time: 110.47399353981018 and batch: 500, loss is 5.660581159591675 and perplexity is 287.31557024502763
At time: 112.90993905067444 and batch: 550, loss is 5.671670818328858 and perplexity is 290.51953446847847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4366200764973955 and perplexity of 229.66462129931531
Finished 4 epochs...
Completing Train Step...
At time: 116.86474061012268 and batch: 50, loss is 5.763674974441528 and perplexity is 318.51672150716627
At time: 119.31977558135986 and batch: 100, loss is 5.7312535572052 and perplexity is 308.3555675462327
At time: 121.77734279632568 and batch: 150, loss is 5.735213384628296 and perplexity is 309.5790231164537
At time: 124.2173478603363 and batch: 200, loss is 5.728720855712891 and perplexity is 307.5755830909826
At time: 126.65878224372864 and batch: 250, loss is 5.675278673171997 and perplexity is 291.569579844582
At time: 129.11045265197754 and batch: 300, loss is 5.669441728591919 and perplexity is 289.87266159250976
At time: 131.5694808959961 and batch: 350, loss is 5.636979885101319 and perplexity is 280.61395105849056
At time: 134.03437328338623 and batch: 400, loss is 5.653259115219116 and perplexity is 285.21951595654224
At time: 136.483628988266 and batch: 450, loss is 5.6304432392120365 and perplexity is 278.7856589876478
At time: 138.944571018219 and batch: 500, loss is 5.616365299224854 and perplexity is 274.88842806791683
At time: 141.38684916496277 and batch: 550, loss is 5.608467683792115 and perplexity is 272.7260151684189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.407368977864583 and perplexity of 223.0439813647564
Finished 5 epochs...
Completing Train Step...
At time: 145.35010290145874 and batch: 50, loss is 5.6802756881713865 and perplexity is 293.0302037488772
At time: 147.85211491584778 and batch: 100, loss is 5.647783660888672 and perplexity is 283.6620772560862
At time: 150.3200705051422 and batch: 150, loss is 5.6642239379882815 and perplexity is 288.3641058292212
At time: 152.7733509540558 and batch: 200, loss is 5.6818301963806155 and perplexity is 293.486075842812
At time: 155.21863055229187 and batch: 250, loss is 5.6349476623535155 and perplexity is 280.04426006935756
At time: 157.663996219635 and batch: 300, loss is 5.630670280456543 and perplexity is 278.84896201654215
At time: 160.10739493370056 and batch: 350, loss is 5.601447639465332 and perplexity is 270.8181708637003
At time: 162.5483362674713 and batch: 400, loss is 5.620786361694336 and perplexity is 276.1064174001252
At time: 164.99156260490417 and batch: 450, loss is 5.59937313079834 and perplexity is 270.25693856297676
At time: 167.43358540534973 and batch: 500, loss is 5.587021188735962 and perplexity is 266.93927250672834
At time: 169.87514185905457 and batch: 550, loss is 5.570795221328735 and perplexity is 262.6428754340212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.378592936197917 and perplexity of 216.71712596967296
Finished 6 epochs...
Completing Train Step...
At time: 173.85252571105957 and batch: 50, loss is 5.6256513595581055 and perplexity is 277.45294730768416
At time: 176.29979157447815 and batch: 100, loss is 5.602000074386597 and perplexity is 270.967821610942
At time: 178.74566841125488 and batch: 150, loss is 5.629014453887939 and perplexity is 278.38761855439645
At time: 181.1943485736847 and batch: 200, loss is 5.641490516662597 and perplexity is 281.8825561568701
At time: 183.63639783859253 and batch: 250, loss is 5.593838272094726 and perplexity is 268.76523658235146
At time: 186.08175802230835 and batch: 300, loss is 5.586054391860962 and perplexity is 266.6813211656215
At time: 188.52565622329712 and batch: 350, loss is 5.558404111862183 and perplexity is 259.4085188306487
At time: 190.96950221061707 and batch: 400, loss is 5.57339825630188 and perplexity is 263.32743460341715
At time: 193.41162371635437 and batch: 450, loss is 5.554005393981933 and perplexity is 258.26995987619875
At time: 195.85235333442688 and batch: 500, loss is 5.551859340667725 and perplexity is 257.7162930845672
At time: 198.29275965690613 and batch: 550, loss is 5.54133939743042 and perplexity is 255.01934306295678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.367939249674479 and perplexity of 214.4205448947423
Finished 7 epochs...
Completing Train Step...
At time: 202.234139919281 and batch: 50, loss is 5.595278625488281 and perplexity is 269.1526324294191
At time: 204.70391941070557 and batch: 100, loss is 5.567953872680664 and perplexity is 261.89767464395993
At time: 207.14523696899414 and batch: 150, loss is 5.595139884948731 and perplexity is 269.11529263830573
At time: 209.61684012413025 and batch: 200, loss is 5.6125891971588135 and perplexity is 273.8523786520098
At time: 212.05590772628784 and batch: 250, loss is 5.564000616073608 and perplexity is 260.8643697367545
At time: 214.50043725967407 and batch: 300, loss is 5.561735219955445 and perplexity is 260.2740774815267
At time: 216.94947624206543 and batch: 350, loss is 5.533742532730103 and perplexity is 253.08933590147763
At time: 219.39387369155884 and batch: 400, loss is 5.555278511047363 and perplexity is 258.5989771639269
At time: 221.8369665145874 and batch: 450, loss is 5.526739721298218 and perplexity is 251.32319021688352
At time: 224.28369784355164 and batch: 500, loss is 5.522881927490235 and perplexity is 250.35550493514324
At time: 226.7265021800995 and batch: 550, loss is 5.513944826126099 and perplexity is 248.1280208620162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.348076883951823 and perplexity of 210.20366287389953
Finished 8 epochs...
Completing Train Step...
At time: 230.70891880989075 and batch: 50, loss is 5.576091470718384 and perplexity is 264.0375877145983
At time: 233.15727257728577 and batch: 100, loss is 5.546772298812866 and perplexity is 256.4086084582007
At time: 235.6033182144165 and batch: 150, loss is 5.574807090759277 and perplexity is 263.69868081768925
At time: 238.04568195343018 and batch: 200, loss is 5.591434307098389 and perplexity is 268.1199103430014
At time: 240.4922285079956 and batch: 250, loss is 5.547528457641602 and perplexity is 256.6025674138969
At time: 242.93703985214233 and batch: 300, loss is 5.540715208053589 and perplexity is 254.86021236714203
At time: 245.38386464118958 and batch: 350, loss is 5.504936723709107 and perplexity is 245.9028953633891
At time: 247.82837438583374 and batch: 400, loss is 5.525702686309814 and perplexity is 251.06269437023076
At time: 250.2740695476532 and batch: 450, loss is 5.5003955078125 and perplexity is 244.78872897575258
At time: 252.71659421920776 and batch: 500, loss is 5.4998602771759035 and perplexity is 244.65774560479474
At time: 255.15833282470703 and batch: 550, loss is 5.487289638519287 and perplexity is 241.6014912706067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.343013509114583 and perplexity of 209.14201297050494
Finished 9 epochs...
Completing Train Step...
At time: 259.0986113548279 and batch: 50, loss is 5.548091564178467 and perplexity is 256.74710268754353
At time: 261.58700370788574 and batch: 100, loss is 5.522412090301514 and perplexity is 250.23790623680793
At time: 264.02945041656494 and batch: 150, loss is 5.548229169845581 and perplexity is 256.7824349747943
At time: 266.4755427837372 and batch: 200, loss is 5.566867208480835 and perplexity is 261.6132343904159
At time: 268.9188742637634 and batch: 250, loss is 5.52820746421814 and perplexity is 251.69233889134273
At time: 271.4173917770386 and batch: 300, loss is 5.5220309829711915 and perplexity is 250.14255690673497
At time: 273.8597867488861 and batch: 350, loss is 5.493129920959473 and perplexity is 243.0166406311475
At time: 276.31068873405457 and batch: 400, loss is 5.511227684020996 and perplexity is 247.45473688742445
At time: 278.7638449668884 and batch: 450, loss is 5.486006507873535 and perplexity is 241.29168379733582
At time: 281.2151207923889 and batch: 500, loss is 5.485551338195801 and perplexity is 241.18188013092745
At time: 283.6563489437103 and batch: 550, loss is 5.470143918991089 and perplexity is 237.4943702518687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.321658325195313 and perplexity of 204.7230981215809
Finished 10 epochs...
Completing Train Step...
At time: 287.5983440876007 and batch: 50, loss is 5.530043725967407 and perplexity is 252.15493650099614
At time: 290.04089307785034 and batch: 100, loss is 5.504171314239502 and perplexity is 245.7147509716098
At time: 292.49397230148315 and batch: 150, loss is 5.531012840270996 and perplexity is 252.39942190469606
At time: 294.93289017677307 and batch: 200, loss is 5.5522219181060795 and perplexity is 257.8097521399847
At time: 297.3721239566803 and batch: 250, loss is 5.5090102863311765 and perplexity is 246.90663922533645
At time: 299.8148579597473 and batch: 300, loss is 5.507254018783569 and perplexity is 246.47338567351596
At time: 302.2522671222687 and batch: 350, loss is 5.473663282394409 and perplexity is 238.3316717669538
At time: 304.6907567977905 and batch: 400, loss is 5.494395408630371 and perplexity is 243.324369866398
At time: 307.1296327114105 and batch: 450, loss is 5.464837675094604 and perplexity is 236.2375047657251
At time: 309.56844425201416 and batch: 500, loss is 5.464203205108642 and perplexity is 236.08766669829743
At time: 312.0049431324005 and batch: 550, loss is 5.446741638183593 and perplexity is 232.00098983412133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.311715698242187 and perplexity of 202.6976983108321
Finished 11 epochs...
Completing Train Step...
At time: 315.91110372543335 and batch: 50, loss is 5.508612861633301 and perplexity is 246.80853192526223
At time: 318.37738728523254 and batch: 100, loss is 5.4830651950836184 and perplexity is 240.58301220500346
At time: 320.8177354335785 and batch: 150, loss is 5.512254457473755 and perplexity is 247.7089473279521
At time: 323.2586188316345 and batch: 200, loss is 5.530425004959106 and perplexity is 252.25109621161099
At time: 325.69863057136536 and batch: 250, loss is 5.4902537822723385 and perplexity is 242.31869524440208
At time: 328.1384205818176 and batch: 300, loss is 5.488074855804443 and perplexity is 241.7912754388254
At time: 330.5940947532654 and batch: 350, loss is 5.457505807876587 and perplexity is 234.51177688342938
At time: 333.0775947570801 and batch: 400, loss is 5.475433588027954 and perplexity is 238.7539653522031
At time: 335.5222110748291 and batch: 450, loss is 5.447683715820313 and perplexity is 232.21965576230892
At time: 337.96342253685 and batch: 500, loss is 5.4511196994781494 and perplexity is 233.01893106662607
At time: 340.4135491847992 and batch: 550, loss is 5.431604309082031 and perplexity is 228.51556109370534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.313352966308594 and perplexity of 203.02984060820276
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 344.48211550712585 and batch: 50, loss is 5.479041175842285 and perplexity is 239.61684677263543
At time: 346.93529510498047 and batch: 100, loss is 5.432315874099731 and perplexity is 228.67822263824883
At time: 349.37745571136475 and batch: 150, loss is 5.440625944137573 and perplexity is 230.58647254817197
At time: 351.8193316459656 and batch: 200, loss is 5.432414216995239 and perplexity is 228.70071262264966
At time: 354.2620851993561 and batch: 250, loss is 5.374147872924805 and perplexity is 215.75594247572985
At time: 356.7141604423523 and batch: 300, loss is 5.356060810089112 and perplexity is 211.88863077128022
At time: 359.16649532318115 and batch: 350, loss is 5.291265850067139 and perplexity is 198.59465742523685
At time: 361.61225271224976 and batch: 400, loss is 5.291965847015381 and perplexity is 198.7337217459934
At time: 364.0579068660736 and batch: 450, loss is 5.254906644821167 and perplexity is 191.5036071663619
At time: 366.50276374816895 and batch: 500, loss is 5.27384536743164 and perplexity is 195.16500249969118
At time: 368.94610118865967 and batch: 550, loss is 5.300960683822632 and perplexity is 200.5293627835873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.209474690755209 and perplexity of 182.99790245024434
Finished 13 epochs...
Completing Train Step...
At time: 372.8917751312256 and batch: 50, loss is 5.379933938980103 and perplexity is 217.00793918563102
At time: 375.3698625564575 and batch: 100, loss is 5.353105630874634 and perplexity is 211.26338620374176
At time: 377.8145430088043 and batch: 150, loss is 5.372678527832031 and perplexity is 215.4391553322283
At time: 380.25926971435547 and batch: 200, loss is 5.365091857910156 and perplexity is 213.81087399896705
At time: 382.70371866226196 and batch: 250, loss is 5.3181984329223635 and perplexity is 204.0160021995327
At time: 385.1482512950897 and batch: 300, loss is 5.304318561553955 and perplexity is 201.203847650458
At time: 387.59245800971985 and batch: 350, loss is 5.2555575275421145 and perplexity is 191.6282941291584
At time: 390.0375339984894 and batch: 400, loss is 5.276604785919189 and perplexity is 195.70428813088748
At time: 392.48066759109497 and batch: 450, loss is 5.251039190292358 and perplexity is 190.76440600834127
At time: 394.9687592983246 and batch: 500, loss is 5.2658538818359375 and perplexity is 193.61155961902563
At time: 397.4113187789917 and batch: 550, loss is 5.276517419815064 and perplexity is 195.6871909565405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.185007731119792 and perplexity of 178.57483036789438
Finished 14 epochs...
Completing Train Step...
At time: 401.33986735343933 and batch: 50, loss is 5.3432247352600095 and perplexity is 209.18619389767056
At time: 403.78357434272766 and batch: 100, loss is 5.313953790664673 and perplexity is 203.15186253465296
At time: 406.2329194545746 and batch: 150, loss is 5.336842184066772 and perplexity is 207.85530406049017
At time: 408.68714118003845 and batch: 200, loss is 5.338110284805298 and perplexity is 208.1190527196664
At time: 411.1325304508209 and batch: 250, loss is 5.298336973190308 and perplexity is 200.00392136689652
At time: 413.57762360572815 and batch: 300, loss is 5.290345315933227 and perplexity is 198.4119283813291
At time: 416.031494140625 and batch: 350, loss is 5.247909336090088 and perplexity is 190.16827461936015
At time: 418.486793756485 and batch: 400, loss is 5.267484664916992 and perplexity is 193.92755566522047
At time: 420.9392170906067 and batch: 450, loss is 5.239287424087524 and perplexity is 188.53570852389552
At time: 423.3834936618805 and batch: 500, loss is 5.248793964385986 and perplexity is 190.33657728774622
At time: 425.8308119773865 and batch: 550, loss is 5.255858688354492 and perplexity is 191.68601375290106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.169566853841146 and perplexity of 175.83865714772472
Finished 15 epochs...
Completing Train Step...
At time: 429.83408522605896 and batch: 50, loss is 5.318325347900391 and perplexity is 204.04189652912348
At time: 432.3334891796112 and batch: 100, loss is 5.289256162643433 and perplexity is 198.19594501764456
At time: 434.780730009079 and batch: 150, loss is 5.315121641159058 and perplexity is 203.38925212854573
At time: 437.2294497489929 and batch: 200, loss is 5.3218321609497075 and perplexity is 204.75868940921444
At time: 439.67620277404785 and batch: 250, loss is 5.2854804420471195 and perplexity is 197.44902347680514
At time: 442.14007544517517 and batch: 300, loss is 5.279539651870728 and perplexity is 196.2794976512335
At time: 444.59076046943665 and batch: 350, loss is 5.237224931716919 and perplexity is 188.14725579157914
At time: 447.04330921173096 and batch: 400, loss is 5.255509605407715 and perplexity is 191.61911111232905
At time: 449.4963929653168 and batch: 450, loss is 5.2253039264678955 and perplexity is 185.91766724001943
At time: 451.94566583633423 and batch: 500, loss is 5.233969688415527 and perplexity is 187.53578647813998
At time: 454.3929796218872 and batch: 550, loss is 5.240451869964599 and perplexity is 188.75537602300318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.163534037272135 and perplexity of 174.78104817151592
Finished 16 epochs...
Completing Train Step...
At time: 458.4020562171936 and batch: 50, loss is 5.302875328063965 and perplexity is 200.91367296449397
At time: 460.8486819267273 and batch: 100, loss is 5.273307638168335 and perplexity is 195.06008477786676
At time: 463.2947452068329 and batch: 150, loss is 5.300116424560547 and perplexity is 200.36013545765755
At time: 465.74239134788513 and batch: 200, loss is 5.309893655776977 and perplexity is 202.32871075448946
At time: 468.189151763916 and batch: 250, loss is 5.27529580116272 and perplexity is 195.44828179170062
At time: 470.63863229751587 and batch: 300, loss is 5.271487255096435 and perplexity is 194.70532370003122
At time: 473.0954964160919 and batch: 350, loss is 5.2283388614654545 and perplexity is 186.48277236990143
At time: 475.5455765724182 and batch: 400, loss is 5.245047569274902 and perplexity is 189.62483533089832
At time: 477.9895746707916 and batch: 450, loss is 5.213448104858398 and perplexity is 183.7264753991493
At time: 480.4372386932373 and batch: 500, loss is 5.221207151412964 and perplexity is 185.1575624309154
At time: 482.8823730945587 and batch: 550, loss is 5.2290403175354 and perplexity is 186.61362773179903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.164398193359375 and perplexity of 174.93215155727958
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 486.84093856811523 and batch: 50, loss is 5.29166018486023 and perplexity is 198.672985651139
At time: 489.323668718338 and batch: 100, loss is 5.260240631103516 and perplexity is 192.52781390400932
At time: 491.76769638061523 and batch: 150, loss is 5.279526453018189 and perplexity is 196.2769070041844
At time: 494.21236538887024 and batch: 200, loss is 5.283224754333496 and perplexity is 197.00414208585968
At time: 496.65543603897095 and batch: 250, loss is 5.242379207611084 and perplexity is 189.11952216875403
At time: 499.1040368080139 and batch: 300, loss is 5.23316237449646 and perplexity is 187.3844473247151
At time: 501.5485739707947 and batch: 350, loss is 5.182301301956176 and perplexity is 178.0921836582702
At time: 504.00191044807434 and batch: 400, loss is 5.191342887878418 and perplexity is 179.70972097069725
At time: 506.4546482563019 and batch: 450, loss is 5.16209228515625 and perplexity is 174.52923879246916
At time: 508.90182185173035 and batch: 500, loss is 5.175804405212403 and perplexity is 176.9388876095251
At time: 511.3461010456085 and batch: 550, loss is 5.203758363723755 and perplexity is 181.95481076075055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.130919392903646 and perplexity of 169.17258263938388
Finished 18 epochs...
Completing Train Step...
At time: 515.2953128814697 and batch: 50, loss is 5.275652875900269 and perplexity is 195.5180838971683
At time: 517.7702081203461 and batch: 100, loss is 5.246737537384033 and perplexity is 189.9455661914359
At time: 520.2171082496643 and batch: 150, loss is 5.267351121902466 and perplexity is 193.90165972398668
At time: 522.6653821468353 and batch: 200, loss is 5.27360499382019 and perplexity is 195.11809562102545
At time: 525.1121561527252 and batch: 250, loss is 5.234153232574463 and perplexity is 187.57021073542847
At time: 527.5591259002686 and batch: 300, loss is 5.227744226455688 and perplexity is 186.37191614749398
At time: 530.0035433769226 and batch: 350, loss is 5.180545377731323 and perplexity is 177.77974167123313
At time: 532.448634147644 and batch: 400, loss is 5.1921320724487305 and perplexity is 179.85160108705819
At time: 534.8930268287659 and batch: 450, loss is 5.16482292175293 and perplexity is 175.0064659895992
At time: 537.3376598358154 and batch: 500, loss is 5.17684266090393 and perplexity is 177.1226908174904
At time: 539.7857129573822 and batch: 550, loss is 5.200325498580932 and perplexity is 181.3312553365219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.127276611328125 and perplexity of 168.55744495859915
Finished 19 epochs...
Completing Train Step...
At time: 543.7588655948639 and batch: 50, loss is 5.26841368675232 and perplexity is 194.1078023124767
At time: 546.2072558403015 and batch: 100, loss is 5.239821395874023 and perplexity is 188.63640815598345
At time: 548.6503722667694 and batch: 150, loss is 5.260484600067139 and perplexity is 192.57479044541222
At time: 551.0931870937347 and batch: 200, loss is 5.267709665298462 and perplexity is 193.97119434839865
At time: 553.5357129573822 and batch: 250, loss is 5.230313129425049 and perplexity is 186.85130300181422
At time: 555.9874219894409 and batch: 300, loss is 5.2257389640808105 and perplexity is 185.9985660139031
At time: 558.4362254142761 and batch: 350, loss is 5.179787731170654 and perplexity is 177.64509847381774
At time: 560.8891987800598 and batch: 400, loss is 5.192088632583618 and perplexity is 179.84378852745624
At time: 563.3310894966125 and batch: 450, loss is 5.16585708618164 and perplexity is 175.18754506815225
At time: 565.7727148532867 and batch: 500, loss is 5.1766997241973876 and perplexity is 177.0973752927129
At time: 568.2134709358215 and batch: 550, loss is 5.197809953689575 and perplexity is 180.87568167175704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.125791422526041 and perplexity of 168.30729113764772
Finished 20 epochs...
Completing Train Step...
At time: 572.2394096851349 and batch: 50, loss is 5.264166793823242 and perplexity is 193.28519525778393
At time: 574.7273805141449 and batch: 100, loss is 5.234929189682007 and perplexity is 187.71581365711745
At time: 577.1790323257446 and batch: 150, loss is 5.255970363616943 and perplexity is 191.70742153413264
At time: 579.6568315029144 and batch: 200, loss is 5.26406497001648 and perplexity is 193.2655152253769
At time: 582.1020448207855 and batch: 250, loss is 5.227871379852295 and perplexity is 186.39561547635734
At time: 584.5502893924713 and batch: 300, loss is 5.224190835952759 and perplexity is 185.7108391785494
At time: 587.0034921169281 and batch: 350, loss is 5.179272689819336 and perplexity is 177.5536274599441
At time: 589.462208032608 and batch: 400, loss is 5.191898765563965 and perplexity is 179.80964536475824
At time: 591.9169833660126 and batch: 450, loss is 5.165998373031616 and perplexity is 175.2122985131769
At time: 594.3650703430176 and batch: 500, loss is 5.176312494277954 and perplexity is 177.02881116624374
At time: 596.8100969791412 and batch: 550, loss is 5.195726585388184 and perplexity is 180.49924327613854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.124105834960938 and perplexity of 168.02383342410192
Finished 21 epochs...
Completing Train Step...
At time: 600.7601835727692 and batch: 50, loss is 5.260235195159912 and perplexity is 192.52676733651535
At time: 603.2216341495514 and batch: 100, loss is 5.231716938018799 and perplexity is 187.11379066483352
At time: 605.6688494682312 and batch: 150, loss is 5.252538776397705 and perplexity is 191.05068825978097
At time: 608.1217107772827 and batch: 200, loss is 5.261437187194824 and perplexity is 192.75832211298078
At time: 610.5669379234314 and batch: 250, loss is 5.226012449264527 and perplexity is 186.04944082233644
At time: 613.0075430870056 and batch: 300, loss is 5.222794809341431 and perplexity is 185.45176278588931
At time: 615.4497127532959 and batch: 350, loss is 5.178827228546143 and perplexity is 177.47455180887349
At time: 617.89808177948 and batch: 400, loss is 5.191321229934692 and perplexity is 179.70582886982112
At time: 620.349835395813 and batch: 450, loss is 5.165657415390014 and perplexity is 175.1525687243388
At time: 622.7977163791656 and batch: 500, loss is 5.175278635025024 and perplexity is 176.84588286913447
At time: 625.2421016693115 and batch: 550, loss is 5.193580150604248 and perplexity is 180.11222892102055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.122130330403646 and perplexity of 167.6922292260849
Finished 22 epochs...
Completing Train Step...
At time: 629.1672081947327 and batch: 50, loss is 5.257162160873413 and perplexity is 191.93603411602618
At time: 631.6469962596893 and batch: 100, loss is 5.22849814414978 and perplexity is 186.51247821221537
At time: 634.0913701057434 and batch: 150, loss is 5.249361867904663 and perplexity is 190.44470079867608
At time: 636.546407699585 and batch: 200, loss is 5.25913290977478 and perplexity is 192.31466481486447
At time: 638.9903297424316 and batch: 250, loss is 5.2244272041320805 and perplexity is 185.75474049971965
At time: 641.4861607551575 and batch: 300, loss is 5.221984462738037 and perplexity is 185.30154345290583
At time: 643.9373984336853 and batch: 350, loss is 5.178126096725464 and perplexity is 177.3501623650338
At time: 646.3926305770874 and batch: 400, loss is 5.190794410705567 and perplexity is 179.6111813168511
At time: 648.8404972553253 and batch: 450, loss is 5.1649009895324705 and perplexity is 175.0201288891134
At time: 651.3027322292328 and batch: 500, loss is 5.1739206218719485 and perplexity is 176.60588682990326
At time: 653.7475960254669 and batch: 550, loss is 5.191215190887451 and perplexity is 179.68677404523976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.123023986816406 and perplexity of 167.84215544338772
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 657.7445757389069 and batch: 50, loss is 5.2545215511322025 and perplexity is 191.4298745337251
At time: 660.1911196708679 and batch: 100, loss is 5.224858865737915 and perplexity is 185.83494099778778
At time: 662.6421372890472 and batch: 150, loss is 5.245544519424438 and perplexity is 189.71909283987353
At time: 665.0884916782379 and batch: 200, loss is 5.2543521499633785 and perplexity is 191.39744883578436
At time: 667.5524237155914 and batch: 250, loss is 5.217211446762085 and perplexity is 184.41920361181351
At time: 670.0035750865936 and batch: 300, loss is 5.2122898864746094 and perplexity is 183.51380320195204
At time: 672.4522593021393 and batch: 350, loss is 5.167606353759766 and perplexity is 175.4942631486167
At time: 674.9000990390778 and batch: 400, loss is 5.1780983543396 and perplexity is 177.34524231664366
At time: 677.3467564582825 and batch: 450, loss is 5.152469701766968 and perplexity is 172.85787097297757
At time: 679.8116819858551 and batch: 500, loss is 5.164386796951294 and perplexity is 174.9301579704538
At time: 682.2650299072266 and batch: 550, loss is 5.184512004852295 and perplexity is 178.48632807203606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.119681294759115 and perplexity of 167.28204745929793
Finished 24 epochs...
Completing Train Step...
At time: 686.2192938327789 and batch: 50, loss is 5.252479639053345 and perplexity is 191.0393903635062
At time: 688.6924042701721 and batch: 100, loss is 5.222965240478516 and perplexity is 185.48337223423593
At time: 691.1383686065674 and batch: 150, loss is 5.2441410446167 and perplexity is 189.45301363393253
At time: 693.5832235813141 and batch: 200, loss is 5.2530480480194095 and perplexity is 191.14800973304767
At time: 696.0278542041779 and batch: 250, loss is 5.216111717224121 and perplexity is 184.21650384412354
At time: 698.4732875823975 and batch: 300, loss is 5.21191237449646 and perplexity is 183.44453761820398
At time: 700.9181840419769 and batch: 350, loss is 5.167348146438599 and perplexity is 175.44895509473628
At time: 703.3906428813934 and batch: 400, loss is 5.178143501281738 and perplexity is 177.35324909277645
At time: 705.8370134830475 and batch: 450, loss is 5.152879428863526 and perplexity is 172.92871003791961
At time: 708.2833926677704 and batch: 500, loss is 5.1647043132781985 and perplexity is 174.98570997054424
At time: 710.7297976016998 and batch: 550, loss is 5.183871374130249 and perplexity is 178.3720208650716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.118976338704427 and perplexity of 167.1641625238325
Finished 25 epochs...
Completing Train Step...
At time: 714.6770980358124 and batch: 50, loss is 5.251322164535522 and perplexity is 190.81839506014933
At time: 717.122184753418 and batch: 100, loss is 5.222047500610351 and perplexity is 185.31322483612252
At time: 719.5703256130219 and batch: 150, loss is 5.243450136184692 and perplexity is 189.32216415705219
At time: 722.0136108398438 and batch: 200, loss is 5.252259159088135 and perplexity is 190.9972746483715
At time: 724.4585886001587 and batch: 250, loss is 5.215431165695191 and perplexity is 184.0911776710747
At time: 726.9085175991058 and batch: 300, loss is 5.211708993911743 and perplexity is 183.40723235459174
At time: 729.3546273708344 and batch: 350, loss is 5.167251491546631 and perplexity is 175.4319979144462
At time: 731.8000993728638 and batch: 400, loss is 5.178231143951416 and perplexity is 177.36879348616915
At time: 734.2486259937286 and batch: 450, loss is 5.153175172805786 and perplexity is 172.9798602196616
At time: 736.6974375247955 and batch: 500, loss is 5.164828977584839 and perplexity is 175.0075258025492
At time: 739.139395236969 and batch: 550, loss is 5.183290624618531 and perplexity is 178.26846147499316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.118716430664063 and perplexity of 167.12072085959522
Finished 26 epochs...
Completing Train Step...
At time: 743.031093120575 and batch: 50, loss is 5.250442228317261 and perplexity is 190.65056089573469
At time: 745.5128934383392 and batch: 100, loss is 5.221276664733887 and perplexity is 185.1704337953345
At time: 747.9642822742462 and batch: 150, loss is 5.242868175506592 and perplexity is 189.21201815544953
At time: 750.4104800224304 and batch: 200, loss is 5.2516998863220214 and perplexity is 190.89048493933026
At time: 752.8595871925354 and batch: 250, loss is 5.214960956573487 and perplexity is 184.00463666788755
At time: 755.3055400848389 and batch: 300, loss is 5.211492691040039 and perplexity is 183.36756513376375
At time: 757.7504777908325 and batch: 350, loss is 5.167091274261475 and perplexity is 175.4038929275233
At time: 760.1952369213104 and batch: 400, loss is 5.178240642547608 and perplexity is 177.37047824871684
At time: 762.6395816802979 and batch: 450, loss is 5.153203277587891 and perplexity is 172.98472184925882
At time: 765.1093349456787 and batch: 500, loss is 5.1645433139801025 and perplexity is 174.95753966182295
At time: 767.553117275238 and batch: 550, loss is 5.1827916431427 and perplexity is 178.17953100416625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.118406168619791 and perplexity of 167.0688776860016
Finished 27 epochs...
Completing Train Step...
At time: 771.4943354129791 and batch: 50, loss is 5.249668045043945 and perplexity is 190.50301953983512
At time: 773.9397029876709 and batch: 100, loss is 5.2207423686981205 and perplexity is 185.0715243924204
At time: 776.3864245414734 and batch: 150, loss is 5.242504472732544 and perplexity is 189.14321373250138
At time: 778.830518245697 and batch: 200, loss is 5.2513101863861085 and perplexity is 190.81610942259132
At time: 781.281328201294 and batch: 250, loss is 5.2144704246521 and perplexity is 183.91439865404132
At time: 783.7279596328735 and batch: 300, loss is 5.211188135147094 and perplexity is 183.3117279644284
At time: 786.1740818023682 and batch: 350, loss is 5.1669290828704835 and perplexity is 175.3754462331109
At time: 788.624484539032 and batch: 400, loss is 5.178175945281982 and perplexity is 177.35900323497643
At time: 791.0780997276306 and batch: 450, loss is 5.153160190582275 and perplexity is 172.97726861614694
At time: 793.5257532596588 and batch: 500, loss is 5.164625396728516 and perplexity is 174.97190124694515
At time: 795.9733545780182 and batch: 550, loss is 5.182312049865723 and perplexity is 178.09409778723747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.118199157714844 and perplexity of 167.03429618594052
Finished 28 epochs...
Completing Train Step...
At time: 799.9048788547516 and batch: 50, loss is 5.249148588180542 and perplexity is 190.40408713661876
At time: 802.375449180603 and batch: 100, loss is 5.220222663879395 and perplexity is 184.97536681832634
At time: 804.8194451332092 and batch: 150, loss is 5.242242383956909 and perplexity is 189.0936479148005
At time: 807.2621469497681 and batch: 200, loss is 5.2508453941345214 and perplexity is 190.7274401814381
At time: 809.708741903305 and batch: 250, loss is 5.2140376758575435 and perplexity is 183.83482713820285
At time: 812.1516091823578 and batch: 300, loss is 5.210912275314331 and perplexity is 183.2611665960545
At time: 814.5964071750641 and batch: 350, loss is 5.166680564880371 and perplexity is 175.33186769494765
At time: 817.0409836769104 and batch: 400, loss is 5.178059816360474 and perplexity is 177.3384079210899
At time: 819.4854655265808 and batch: 450, loss is 5.153092107772827 and perplexity is 172.9654922386179
At time: 821.934160232544 and batch: 500, loss is 5.164338645935058 and perplexity is 174.92173510836358
At time: 824.3783121109009 and batch: 550, loss is 5.181835746765136 and perplexity is 178.00929121468533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1178126017252605 and perplexity of 166.96974055627058
Finished 29 epochs...
Completing Train Step...
At time: 828.3275201320648 and batch: 50, loss is 5.248503141403198 and perplexity is 190.28123108496968
At time: 830.77308177948 and batch: 100, loss is 5.2197683238983155 and perplexity is 184.89134420252816
At time: 833.2165324687958 and batch: 150, loss is 5.24187068939209 and perplexity is 189.0233758943018
At time: 835.6590921878815 and batch: 200, loss is 5.250419359207154 and perplexity is 190.64620093692
At time: 838.100460767746 and batch: 250, loss is 5.213563671112061 and perplexity is 183.747709206542
At time: 840.5463380813599 and batch: 300, loss is 5.210625410079956 and perplexity is 183.20860287825917
At time: 842.9936785697937 and batch: 350, loss is 5.166529312133789 and perplexity is 175.30535027386227
At time: 845.4419496059418 and batch: 400, loss is 5.177972507476807 and perplexity is 177.32292537855466
At time: 847.8867716789246 and batch: 450, loss is 5.152975463867188 and perplexity is 172.94531804468366
At time: 850.3322992324829 and batch: 500, loss is 5.16420280456543 and perplexity is 174.8979751141203
At time: 852.7766311168671 and batch: 550, loss is 5.181514120101928 and perplexity is 177.95204788631574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.118021138509115 and perplexity of 167.0045635197754
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 856.7298781871796 and batch: 50, loss is 5.248239669799805 and perplexity is 190.2311039877434
At time: 859.2216618061066 and batch: 100, loss is 5.219377975463868 and perplexity is 184.81918624006582
At time: 861.7120983600616 and batch: 150, loss is 5.241622514724732 and perplexity is 188.9764709014226
At time: 864.1828541755676 and batch: 200, loss is 5.249555768966675 and perplexity is 190.48163180878083
At time: 866.6844623088837 and batch: 250, loss is 5.2119996547698975 and perplexity is 183.46054940635432
At time: 869.134236574173 and batch: 300, loss is 5.208691596984863 and perplexity is 182.85465402864298
At time: 871.5851895809174 and batch: 350, loss is 5.163560028076172 and perplexity is 174.78559093052303
At time: 874.0310707092285 and batch: 400, loss is 5.175631017684936 and perplexity is 176.90821127281376
At time: 876.4790554046631 and batch: 450, loss is 5.150312757492065 and perplexity is 172.4854279915851
At time: 878.9252955913544 and batch: 500, loss is 5.161882696151733 and perplexity is 174.49266321610475
At time: 881.3778607845306 and batch: 550, loss is 5.18052490234375 and perplexity is 177.77610159938587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117812093098959 and perplexity of 166.9696556310905
Finished 31 epochs...
Completing Train Step...
At time: 885.3297667503357 and batch: 50, loss is 5.247578105926514 and perplexity is 190.1052955815056
At time: 887.7709536552429 and batch: 100, loss is 5.218913164138794 and perplexity is 184.73330015117023
At time: 890.2381126880646 and batch: 150, loss is 5.241229600906372 and perplexity is 188.902234019964
At time: 892.6961386203766 and batch: 200, loss is 5.249026355743408 and perplexity is 190.3808150033414
At time: 895.1437995433807 and batch: 250, loss is 5.211736688613891 and perplexity is 183.41231183360065
At time: 897.596696138382 and batch: 300, loss is 5.208687019348145 and perplexity is 182.85381698838037
At time: 900.0415518283844 and batch: 350, loss is 5.163532915115357 and perplexity is 174.78085203988806
At time: 902.482391834259 and batch: 400, loss is 5.1756323719024655 and perplexity is 176.90845084517676
At time: 904.923810005188 and batch: 450, loss is 5.150512752532959 and perplexity is 172.51992767157728
At time: 907.3737490177155 and batch: 500, loss is 5.1619391345977785 and perplexity is 174.5025115887738
At time: 909.8245656490326 and batch: 550, loss is 5.180436420440674 and perplexity is 177.76037232748305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1176905314127605 and perplexity of 166.94935975183373
Finished 32 epochs...
Completing Train Step...
At time: 913.7650191783905 and batch: 50, loss is 5.247303247451782 and perplexity is 190.05305071022545
At time: 916.2349219322205 and batch: 100, loss is 5.218607006072998 and perplexity is 184.67675121820548
At time: 918.6960422992706 and batch: 150, loss is 5.240983896255493 and perplexity is 188.85582556412396
At time: 921.1674273014069 and batch: 200, loss is 5.248818893432617 and perplexity is 190.34132225630046
At time: 923.6226377487183 and batch: 250, loss is 5.2116078281402585 and perplexity is 183.388678758945
At time: 926.0714480876923 and batch: 300, loss is 5.208411417007446 and perplexity is 182.8034289922565
At time: 928.5189995765686 and batch: 350, loss is 5.163629770278931 and perplexity is 174.79778128773125
At time: 930.9683842658997 and batch: 400, loss is 5.175852222442627 and perplexity is 176.94734853933704
At time: 933.413702249527 and batch: 450, loss is 5.1506968593597415 and perplexity is 172.55169269200655
At time: 935.8603708744049 and batch: 500, loss is 5.162125225067139 and perplexity is 174.53498786472892
At time: 938.3051853179932 and batch: 550, loss is 5.180322799682617 and perplexity is 177.74017620659785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.11761474609375 and perplexity of 166.9367079207639
Finished 33 epochs...
Completing Train Step...
At time: 942.2455153465271 and batch: 50, loss is 5.247129936218261 and perplexity is 190.02011523569874
At time: 944.6870381832123 and batch: 100, loss is 5.218393230438233 and perplexity is 184.6372760480526
At time: 947.1290435791016 and batch: 150, loss is 5.2408311080932615 and perplexity is 188.82697283384326
At time: 949.5706224441528 and batch: 200, loss is 5.248616819381714 and perplexity is 190.3028631001875
At time: 952.0395011901855 and batch: 250, loss is 5.211437835693359 and perplexity is 183.3575067182899
At time: 954.4840626716614 and batch: 300, loss is 5.208279371261597 and perplexity is 182.77929217074873
At time: 956.9325451850891 and batch: 350, loss is 5.16364200592041 and perplexity is 174.79992006379914
At time: 959.3797361850739 and batch: 400, loss is 5.175931186676025 and perplexity is 176.96132160274516
At time: 961.8232793807983 and batch: 450, loss is 5.150791826248169 and perplexity is 172.56808016747593
At time: 964.2668209075928 and batch: 500, loss is 5.1621488761901855 and perplexity is 174.53911586201855
At time: 966.709263086319 and batch: 550, loss is 5.180284337997437 and perplexity is 177.73334015136055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117591857910156 and perplexity of 166.93288708647063
Finished 34 epochs...
Completing Train Step...
At time: 970.6746325492859 and batch: 50, loss is 5.247001619338989 and perplexity is 189.99573401180746
At time: 973.1427388191223 and batch: 100, loss is 5.218225450515747 and perplexity is 184.606300218823
At time: 975.5855989456177 and batch: 150, loss is 5.240661859512329 and perplexity is 188.7950168409789
At time: 978.0288400650024 and batch: 200, loss is 5.248459453582764 and perplexity is 190.27291829429922
At time: 980.4795899391174 and batch: 250, loss is 5.21129093170166 and perplexity is 183.33057274704734
At time: 982.9253740310669 and batch: 300, loss is 5.208181657791138 and perplexity is 182.76143304433552
At time: 985.3733286857605 and batch: 350, loss is 5.163606309890747 and perplexity is 174.79368051203176
At time: 987.82133436203 and batch: 400, loss is 5.1760127735137935 and perplexity is 176.97575990636182
At time: 990.2696328163147 and batch: 450, loss is 5.150902853012085 and perplexity is 172.58724090662986
At time: 992.7171809673309 and batch: 500, loss is 5.162196016311645 and perplexity is 174.54734385107244
At time: 995.1627652645111 and batch: 550, loss is 5.18021936416626 and perplexity is 177.72179251047442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117560831705729 and perplexity of 166.92770787293637
Finished 35 epochs...
Completing Train Step...
At time: 999.1231887340546 and batch: 50, loss is 5.246859846115112 and perplexity is 189.96879961340707
At time: 1001.567403793335 and batch: 100, loss is 5.218072443008423 and perplexity is 184.57805622981655
At time: 1004.0121068954468 and batch: 150, loss is 5.240525979995727 and perplexity is 188.7693652081589
At time: 1006.4578511714935 and batch: 200, loss is 5.248317708969116 and perplexity is 190.24595004435528
At time: 1008.9027986526489 and batch: 250, loss is 5.211157131195068 and perplexity is 183.30604466451157
At time: 1011.3459017276764 and batch: 300, loss is 5.208071212768555 and perplexity is 182.74124906836605
At time: 1013.8158602714539 and batch: 350, loss is 5.163570880889893 and perplexity is 174.78748785627593
At time: 1016.2606112957001 and batch: 400, loss is 5.176087636947631 and perplexity is 176.98900941540015
At time: 1018.7042829990387 and batch: 450, loss is 5.150984430313111 and perplexity is 172.60132068222168
At time: 1021.1504554748535 and batch: 500, loss is 5.162228393554687 and perplexity is 174.5529953043354
At time: 1023.5950741767883 and batch: 550, loss is 5.180158243179322 and perplexity is 177.71093031107327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117521667480469 and perplexity of 166.9211704066013
Finished 36 epochs...
Completing Train Step...
At time: 1027.5326018333435 and batch: 50, loss is 5.246730041503906 and perplexity is 189.9441423875775
At time: 1030.0128989219666 and batch: 100, loss is 5.217924976348877 and perplexity is 184.5508391272958
At time: 1032.4545843601227 and batch: 150, loss is 5.240398530960083 and perplexity is 188.74530826765343
At time: 1034.8968486785889 and batch: 200, loss is 5.2481852054595945 and perplexity is 190.2207434583195
At time: 1037.3475015163422 and batch: 250, loss is 5.211029663085937 and perplexity is 183.28268047873183
At time: 1039.8025760650635 and batch: 300, loss is 5.207971506118774 and perplexity is 182.72302945896786
At time: 1042.2461626529694 and batch: 350, loss is 5.163528413772583 and perplexity is 174.7800652931334
At time: 1044.688954114914 and batch: 400, loss is 5.176139430999756 and perplexity is 176.99817663078085
At time: 1047.1321914196014 and batch: 450, loss is 5.151075668334961 and perplexity is 172.61706920371054
At time: 1049.5784969329834 and batch: 500, loss is 5.1622899055480955 and perplexity is 174.56373273726902
At time: 1052.0208506584167 and batch: 550, loss is 5.180080137252808 and perplexity is 177.6970505762615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117474365234375 and perplexity of 166.9132748470607
Finished 37 epochs...
Completing Train Step...
At time: 1055.949348449707 and batch: 50, loss is 5.246603870391846 and perplexity is 189.92017843571412
At time: 1058.419472694397 and batch: 100, loss is 5.21778974533081 and perplexity is 184.52588381683952
At time: 1060.8645987510681 and batch: 150, loss is 5.240277042388916 and perplexity is 188.7223792626718
At time: 1063.3253531455994 and batch: 200, loss is 5.248034830093384 and perplexity is 190.19214109496045
At time: 1065.7696838378906 and batch: 250, loss is 5.210885963439941 and perplexity is 183.2563447146952
At time: 1068.2143070697784 and batch: 300, loss is 5.2078446102142335 and perplexity is 182.69984412595727
At time: 1070.6591272354126 and batch: 350, loss is 5.163498411178589 and perplexity is 174.77482151645998
At time: 1073.1375920772552 and batch: 400, loss is 5.176191806793213 and perplexity is 177.00744729349935
At time: 1075.5818829536438 and batch: 450, loss is 5.151140537261963 and perplexity is 172.62826705096418
At time: 1078.028332233429 and batch: 500, loss is 5.162291069030761 and perplexity is 174.56393583926427
At time: 1080.47434425354 and batch: 550, loss is 5.180048723220825 and perplexity is 177.6914684831099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117559814453125 and perplexity of 166.92753806537723
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1084.423893213272 and batch: 50, loss is 5.2465425300598145 and perplexity is 189.9085290262024
At time: 1086.869654417038 and batch: 100, loss is 5.2176531791687015 and perplexity is 184.5006855457317
At time: 1089.3152124881744 and batch: 150, loss is 5.240027494430542 and perplexity is 188.6752898540046
At time: 1091.7615649700165 and batch: 200, loss is 5.247865142822266 and perplexity is 190.15987064756948
At time: 1094.206618309021 and batch: 250, loss is 5.210496110916138 and perplexity is 183.18491569050246
At time: 1096.6503219604492 and batch: 300, loss is 5.207337608337403 and perplexity is 182.60723843970075
At time: 1099.1000607013702 and batch: 350, loss is 5.162917976379394 and perplexity is 174.6734055635503
At time: 1101.5648090839386 and batch: 400, loss is 5.175714559555054 and perplexity is 176.92299113298267
At time: 1104.0093779563904 and batch: 450, loss is 5.1506220245361325 and perplexity is 172.5387802996748
At time: 1106.4579424858093 and batch: 500, loss is 5.161758890151978 and perplexity is 174.4710613147354
At time: 1108.9016227722168 and batch: 550, loss is 5.179757194519043 and perplexity is 177.63967387016032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117743937174479 and perplexity of 166.95827604764838
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 1112.8436636924744 and batch: 50, loss is 5.246478643417358 and perplexity is 189.89639679545695
At time: 1115.3147385120392 and batch: 100, loss is 5.217541494369507 and perplexity is 184.48008077435657
At time: 1117.759429693222 and batch: 150, loss is 5.239943332672119 and perplexity is 188.65941127803336
At time: 1120.2034533023834 and batch: 200, loss is 5.247773456573486 and perplexity is 190.14243640161385
At time: 1122.6463661193848 and batch: 250, loss is 5.210403623580933 and perplexity is 183.1679741892498
At time: 1125.0915684700012 and batch: 300, loss is 5.207251815795899 and perplexity is 182.5915727726263
At time: 1127.5360956192017 and batch: 350, loss is 5.162715187072754 and perplexity is 174.63798725609593
At time: 1129.9809415340424 and batch: 400, loss is 5.175453729629517 and perplexity is 176.876850340091
At time: 1132.426589012146 and batch: 450, loss is 5.150482559204102 and perplexity is 172.51471879930375
At time: 1134.871788740158 and batch: 500, loss is 5.161633310317993 and perplexity is 174.44915264349277
At time: 1137.3421375751495 and batch: 550, loss is 5.179715642929077 and perplexity is 177.63229281261835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117758178710938 and perplexity of 166.9606538069552
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1141.3048827648163 and batch: 50, loss is 5.246488285064697 and perplexity is 189.89822771837237
At time: 1143.748963356018 and batch: 100, loss is 5.217523183822632 and perplexity is 184.4767028741157
At time: 1146.1943407058716 and batch: 150, loss is 5.239948949813843 and perplexity is 188.66047100766025
At time: 1148.6397542953491 and batch: 200, loss is 5.247751655578614 and perplexity is 190.13829115251826
At time: 1151.0827553272247 and batch: 250, loss is 5.2103925132751465 and perplexity is 183.16593914835124
At time: 1153.527203321457 and batch: 300, loss is 5.207236099243164 and perplexity is 182.58870308509478
At time: 1155.9787793159485 and batch: 350, loss is 5.162682971954346 and perplexity is 174.63236136327777
At time: 1158.4228467941284 and batch: 400, loss is 5.175423164367675 and perplexity is 176.87144413546812
At time: 1160.8677997589111 and batch: 450, loss is 5.150457286834717 and perplexity is 172.51035899869743
At time: 1163.3146123886108 and batch: 500, loss is 5.161608037948608 and perplexity is 174.4447439557775
At time: 1165.7605888843536 and batch: 550, loss is 5.179705438613891 and perplexity is 177.63048020596355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.117760721842448 and perplexity of 166.96107841039472
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1169.6934518814087 and batch: 50, loss is 5.2464885234832765 and perplexity is 189.8982729936434
At time: 1172.1548347473145 and batch: 100, loss is 5.217520227432251 and perplexity is 184.47615748977208
At time: 1174.5996701717377 and batch: 150, loss is 5.239948711395264 and perplexity is 188.6604260275042
At time: 1177.0443851947784 and batch: 200, loss is 5.247748565673828 and perplexity is 190.13770364421023
At time: 1179.486162185669 and batch: 250, loss is 5.210388736724854 and perplexity is 183.16524741427628
At time: 1181.9297926425934 and batch: 300, loss is 5.2072330093383785 and perplexity is 182.58813890425898
At time: 1184.370799779892 and batch: 350, loss is 5.162676572799683 and perplexity is 174.63124386736374
At time: 1186.815025806427 and batch: 400, loss is 5.175416784286499 and perplexity is 176.87031568489658
At time: 1189.2586963176727 and batch: 450, loss is 5.150452327728272 and perplexity is 172.50950350358545
At time: 1191.703509569168 and batch: 500, loss is 5.161603021621704 and perplexity is 174.44386888610995
At time: 1194.1459200382233 and batch: 550, loss is 5.1797035026550295 and perplexity is 177.63013632099415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1177622477213545 and perplexity of 166.96133317297688
Annealing...
Model not improving. Stopping early with 166.9132748470607loss at 41 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe57da976a0>
SETTINGS FOR THIS RUN
{'lr': 19.244521822480802, 'tune_wordvecs': True, 'anneal': 2.505864489080415, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.23345668211504944}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.885186195373535 and batch: 50, loss is 7.545741376876831 and perplexity is 1892.6653957641838
At time: 5.309155702590942 and batch: 100, loss is 6.405187759399414 and perplexity is 604.9753778290342
At time: 7.745455503463745 and batch: 150, loss is 6.1226589965820315 and perplexity is 456.0757876001314
At time: 10.189225196838379 and batch: 200, loss is 6.068079175949097 and perplexity is 431.8503758058808
At time: 12.618232011795044 and batch: 250, loss is 6.028858613967896 and perplexity is 415.24080874000504
At time: 15.048391342163086 and batch: 300, loss is 6.062539577484131 and perplexity is 429.4647120374119
At time: 17.4845929145813 and batch: 350, loss is 6.062681121826172 and perplexity is 429.52550463983
At time: 19.9194598197937 and batch: 400, loss is 6.103973417282105 and perplexity is 447.6328733290319
At time: 22.352877378463745 and batch: 450, loss is 6.114115552902222 and perplexity is 452.1959270793292
At time: 24.792279720306396 and batch: 500, loss is 6.130474042892456 and perplexity is 459.6540047560805
At time: 27.221953630447388 and batch: 550, loss is 6.138979597091675 and perplexity is 463.58029075521586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.679875183105469 and perplexity of 292.91286716632516
Finished 1 epochs...
Completing Train Step...
At time: 31.190566062927246 and batch: 50, loss is 6.014059705734253 and perplexity is 409.1409451037051
At time: 33.630709648132324 and batch: 100, loss is 5.995130453109741 and perplexity is 401.4690534614536
At time: 36.04613494873047 and batch: 150, loss is 6.021382474899292 and perplexity is 412.14798629774333
At time: 38.46370840072632 and batch: 200, loss is 6.02845419883728 and perplexity is 415.0729130261733
At time: 40.878308057785034 and batch: 250, loss is 6.001665744781494 and perplexity is 404.10136290918285
At time: 43.309266805648804 and batch: 300, loss is 6.039892511367798 and perplexity is 419.8479035842781
At time: 45.75055456161499 and batch: 350, loss is 6.024739656448364 and perplexity is 413.53396710568336
At time: 48.19219994544983 and batch: 400, loss is 6.011189908981323 and perplexity is 407.96847692531935
At time: 50.63327169418335 and batch: 450, loss is 5.988811798095703 and perplexity is 398.94030656687124
At time: 53.07636213302612 and batch: 500, loss is 5.98712685585022 and perplexity is 398.2686811748058
At time: 55.51782202720642 and batch: 550, loss is 5.969720134735107 and perplexity is 391.39611724246254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.707709248860677 and perplexity of 301.18034822512084
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.459338665008545 and batch: 50, loss is 5.942015037536621 and perplexity is 380.70128454037535
At time: 61.90138483047485 and batch: 100, loss is 5.848076343536377 and perplexity is 346.5670628690326
At time: 64.3435685634613 and batch: 150, loss is 5.807870903015137 and perplexity is 332.9095738203944
At time: 66.78589200973511 and batch: 200, loss is 5.771135835647583 and perplexity is 320.90201767528873
At time: 69.23402810096741 and batch: 250, loss is 5.697552995681763 and perplexity is 298.13696520052804
At time: 71.71247148513794 and batch: 300, loss is 5.67173882484436 and perplexity is 290.53929236152817
At time: 74.15219211578369 and batch: 350, loss is 5.601150207519531 and perplexity is 270.73763286606254
At time: 76.59242391586304 and batch: 400, loss is 5.588240652084351 and perplexity is 267.2649937279767
At time: 79.0338704586029 and batch: 450, loss is 5.56783613204956 and perplexity is 261.8668404617165
At time: 81.49180245399475 and batch: 500, loss is 5.56683084487915 and perplexity is 261.60372136393016
At time: 83.93570685386658 and batch: 550, loss is 5.570897598266601 and perplexity is 262.6697653837923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3939697265625 and perplexity of 220.07529252730225
Finished 3 epochs...
Completing Train Step...
At time: 87.8763325214386 and batch: 50, loss is 5.631892356872559 and perplexity is 279.1899450678884
At time: 90.31930756568909 and batch: 100, loss is 5.599494962692261 and perplexity is 270.28986648344187
At time: 92.76164078712463 and batch: 150, loss is 5.609064893722534 and perplexity is 272.88893849783403
At time: 95.20541477203369 and batch: 200, loss is 5.60558777809143 and perplexity is 271.9417198529548
At time: 97.64830160140991 and batch: 250, loss is 5.5534020614624025 and perplexity is 258.11418420757616
At time: 100.08839774131775 and batch: 300, loss is 5.57043378829956 and perplexity is 262.54796477691826
At time: 102.53427243232727 and batch: 350, loss is 5.537713661193847 and perplexity is 254.0963844032134
At time: 104.97919011116028 and batch: 400, loss is 5.555560846328735 and perplexity is 258.67199908675445
At time: 107.42607641220093 and batch: 450, loss is 5.523490772247315 and perplexity is 250.50797898353153
At time: 109.87162017822266 and batch: 500, loss is 5.5236043357849125 and perplexity is 250.53642917124276
At time: 112.31471300125122 and batch: 550, loss is 5.513610582351685 and perplexity is 248.04509947453627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.340352884928385 and perplexity of 208.586304266273
Finished 4 epochs...
Completing Train Step...
At time: 116.2524631023407 and batch: 50, loss is 5.571522188186646 and perplexity is 262.8338775176086
At time: 118.71503710746765 and batch: 100, loss is 5.560993690490722 and perplexity is 260.0811481244126
At time: 121.15826225280762 and batch: 150, loss is 5.5743450260162355 and perplexity is 263.57686310050025
At time: 123.62069511413574 and batch: 200, loss is 5.574102783203125 and perplexity is 263.51302123264105
At time: 126.06947827339172 and batch: 250, loss is 5.525870742797852 and perplexity is 251.10489063050227
At time: 128.51413226127625 and batch: 300, loss is 5.532425384521485 and perplexity is 252.75619917945565
At time: 130.96362781524658 and batch: 350, loss is 5.495001697540284 and perplexity is 243.47193946374418
At time: 133.4168541431427 and batch: 400, loss is 5.507242822647095 and perplexity is 246.47062613930086
At time: 135.86685919761658 and batch: 450, loss is 5.478401126861573 and perplexity is 239.46352932466922
At time: 138.31924295425415 and batch: 500, loss is 5.477521848678589 and perplexity is 239.2530668088226
At time: 140.7655806541443 and batch: 550, loss is 5.473411149978638 and perplexity is 238.2715882016219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.318952433268229 and perplexity of 204.16988834356553
Finished 5 epochs...
Completing Train Step...
At time: 144.76144075393677 and batch: 50, loss is 5.524852266311646 and perplexity is 250.84927639446377
At time: 147.22827172279358 and batch: 100, loss is 5.507059364318848 and perplexity is 246.42541319773946
At time: 149.6821415424347 and batch: 150, loss is 5.523441038131714 and perplexity is 250.4955205005542
At time: 152.12615084648132 and batch: 200, loss is 5.52429443359375 and perplexity is 250.70938348287956
At time: 154.5756824016571 and batch: 250, loss is 5.476366987228394 and perplexity is 238.97692215022585
At time: 157.01985216140747 and batch: 300, loss is 5.497102699279785 and perplexity is 243.9840121765412
At time: 159.4617908000946 and batch: 350, loss is 5.460838575363159 and perplexity is 235.29465395763407
At time: 161.90525197982788 and batch: 400, loss is 5.477864494323731 and perplexity is 239.33505987673306
At time: 164.35340070724487 and batch: 450, loss is 5.4500378894805905 and perplexity is 232.7669851607489
At time: 166.80410933494568 and batch: 500, loss is 5.445990343093872 and perplexity is 231.82675408907375
At time: 169.2551941871643 and batch: 550, loss is 5.433770742416382 and perplexity is 229.01116147142702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.313645426432291 and perplexity of 203.08922742421578
Finished 6 epochs...
Completing Train Step...
At time: 173.2805380821228 and batch: 50, loss is 5.492957105636597 and perplexity is 242.97464726058607
At time: 175.7419991493225 and batch: 100, loss is 5.4861406803131105 and perplexity is 241.3240606631932
At time: 178.18536949157715 and batch: 150, loss is 5.495190572738648 and perplexity is 243.51792961766947
At time: 180.63898539543152 and batch: 200, loss is 5.496623573303222 and perplexity is 243.86714109866182
At time: 183.08888459205627 and batch: 250, loss is 5.450057010650635 and perplexity is 232.77143598040524
At time: 185.58774638175964 and batch: 300, loss is 5.470037813186646 and perplexity is 237.4691720575234
At time: 188.0406618118286 and batch: 350, loss is 5.429900312423706 and perplexity is 228.1265029122862
At time: 190.49180555343628 and batch: 400, loss is 5.448757495880127 and perplexity is 232.46914252114516
At time: 192.94562005996704 and batch: 450, loss is 5.414874372482299 and perplexity is 224.72431234768402
At time: 195.39307641983032 and batch: 500, loss is 5.415610265731812 and perplexity is 224.88974631554632
At time: 197.83811450004578 and batch: 550, loss is 5.4083449649810795 and perplexity is 223.26177568191594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.297016906738281 and perplexity of 199.7400770843333
Finished 7 epochs...
Completing Train Step...
At time: 201.7789261341095 and batch: 50, loss is 5.4660646438598635 and perplexity is 236.52753870015857
At time: 204.22889757156372 and batch: 100, loss is 5.454151401519775 and perplexity is 233.72644698249056
At time: 206.68455624580383 and batch: 150, loss is 5.4712115669250485 and perplexity is 237.74806603038368
At time: 209.1291468143463 and batch: 200, loss is 5.472886810302734 and perplexity is 238.14668570281668
At time: 211.57315969467163 and batch: 250, loss is 5.423751783370972 and perplexity is 226.7281637511895
At time: 214.02634572982788 and batch: 300, loss is 5.443529663085937 and perplexity is 231.25700390391134
At time: 216.47761011123657 and batch: 350, loss is 5.409418430328369 and perplexity is 223.50156814290688
At time: 218.93120098114014 and batch: 400, loss is 5.425745716094971 and perplexity is 227.18069546531186
At time: 221.37902331352234 and batch: 450, loss is 5.393204908370972 and perplexity is 219.90703928984152
At time: 223.829505443573 and batch: 500, loss is 5.390767345428467 and perplexity is 219.3716548218338
At time: 226.2757956981659 and batch: 550, loss is 5.377157707214355 and perplexity is 216.40631036838047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.283084615071615 and perplexity of 196.9765360051933
Finished 8 epochs...
Completing Train Step...
At time: 230.1874077320099 and batch: 50, loss is 5.436568050384522 and perplexity is 229.65267305261762
At time: 232.68092226982117 and batch: 100, loss is 5.416951446533203 and perplexity is 225.1915664781964
At time: 235.13041830062866 and batch: 150, loss is 5.440744562149048 and perplexity is 230.61382587928537
At time: 237.5741515159607 and batch: 200, loss is 5.4482887172698975 and perplexity is 232.36019149854258
At time: 240.02377200126648 and batch: 250, loss is 5.4030859375 and perplexity is 222.0907178797423
At time: 242.4675977230072 and batch: 300, loss is 5.422331142425537 and perplexity is 226.40629312371138
At time: 244.90999937057495 and batch: 350, loss is 5.382716808319092 and perplexity is 217.61268499991934
At time: 247.40205836296082 and batch: 400, loss is 5.403048849105835 and perplexity is 222.0824810444035
At time: 249.84825921058655 and batch: 450, loss is 5.367764253616333 and perplexity is 214.3830254275753
At time: 252.29687118530273 and batch: 500, loss is 5.364780349731445 and perplexity is 213.74428053576185
At time: 254.74351382255554 and batch: 550, loss is 5.34939211845398 and perplexity is 210.48031187306552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.275123596191406 and perplexity of 195.41462752374065
Finished 9 epochs...
Completing Train Step...
At time: 258.7653248310089 and batch: 50, loss is 5.404791717529297 and perplexity is 222.46987908186028
At time: 261.20808386802673 and batch: 100, loss is 5.39214940071106 and perplexity is 219.67504818113693
At time: 263.6481816768646 and batch: 150, loss is 5.4114038848876955 and perplexity is 223.9457611670068
At time: 266.09047770500183 and batch: 200, loss is 5.415743103027344 and perplexity is 224.91962204550146
At time: 268.54504561424255 and batch: 250, loss is 5.373960294723511 and perplexity is 215.7154751596335
At time: 270.99858140945435 and batch: 300, loss is 5.401397590637207 and perplexity is 221.71606807136425
At time: 273.517813205719 and batch: 350, loss is 5.363451595306397 and perplexity is 213.4604554858265
At time: 275.97416520118713 and batch: 400, loss is 5.382292165756225 and perplexity is 217.52029700897359
At time: 278.4333200454712 and batch: 450, loss is 5.347355031967163 and perplexity is 210.05198169493778
At time: 280.8937888145447 and batch: 500, loss is 5.343514423370362 and perplexity is 209.24680142910933
At time: 283.3449966907501 and batch: 550, loss is 5.324003448486328 and perplexity is 205.2037624154215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.273502604166667 and perplexity of 195.09811856955773
Finished 10 epochs...
Completing Train Step...
At time: 287.4010088443756 and batch: 50, loss is 5.3870894145965575 and perplexity is 218.56630297096353
At time: 289.9929506778717 and batch: 100, loss is 5.37956298828125 and perplexity is 216.92745486771332
At time: 292.4376571178436 and batch: 150, loss is 5.391916522979736 and perplexity is 219.6238967105385
At time: 294.88390922546387 and batch: 200, loss is 5.399165687561035 and perplexity is 221.22177111363973
At time: 297.3339648246765 and batch: 250, loss is 5.352247695922852 and perplexity is 211.08221368891492
At time: 299.7856411933899 and batch: 300, loss is 5.376795721054077 and perplexity is 216.3279884556098
At time: 302.2336947917938 and batch: 350, loss is 5.3375765895843506 and perplexity is 208.0080102099059
At time: 304.69044756889343 and batch: 400, loss is 5.360095958709717 and perplexity is 212.74536024035757
At time: 307.14210295677185 and batch: 450, loss is 5.325776443481446 and perplexity is 205.5679103800307
At time: 309.6388659477234 and batch: 500, loss is 5.321482210159302 and perplexity is 204.68704648049422
At time: 312.0894181728363 and batch: 550, loss is 5.306716051101684 and perplexity is 201.68681049000543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.272276306152344 and perplexity of 194.85901676915293
Finished 11 epochs...
Completing Train Step...
At time: 316.34597420692444 and batch: 50, loss is 5.3660571479797365 and perplexity is 214.01736315734004
At time: 318.7941846847534 and batch: 100, loss is 5.350102167129517 and perplexity is 210.62981621113497
At time: 321.2402572631836 and batch: 150, loss is 5.376973896026612 and perplexity is 216.3665361230241
At time: 323.6890721321106 and batch: 200, loss is 5.374255352020263 and perplexity is 215.77913297549154
At time: 326.1421320438385 and batch: 250, loss is 5.333003101348877 and perplexity is 207.05886014009639
At time: 328.59257769584656 and batch: 300, loss is 5.356896953582764 and perplexity is 212.0658741614058
At time: 331.04090428352356 and batch: 350, loss is 5.3173566341400145 and perplexity is 203.84433404246158
At time: 333.4923589229584 and batch: 400, loss is 5.341483793258667 and perplexity is 208.8223296917549
At time: 335.95077776908875 and batch: 450, loss is 5.304732141494751 and perplexity is 201.2870787360245
At time: 338.40608310699463 and batch: 500, loss is 5.299995050430298 and perplexity is 200.33581839624082
At time: 340.86346673965454 and batch: 550, loss is 5.281400661468506 and perplexity is 196.64511578411464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.250541178385417 and perplexity of 190.66942671510097
Finished 12 epochs...
Completing Train Step...
At time: 344.8990705013275 and batch: 50, loss is 5.346157970428467 and perplexity is 209.80068698414829
At time: 347.3517243862152 and batch: 100, loss is 5.336124591827392 and perplexity is 207.70620221094563
At time: 349.8025436401367 and batch: 150, loss is 5.345780220031738 and perplexity is 209.72144965831387
At time: 352.2523567676544 and batch: 200, loss is 5.357818822860718 and perplexity is 212.2614613147401
At time: 354.7011697292328 and batch: 250, loss is 5.305037908554077 and perplexity is 201.3486351046453
At time: 357.1498472690582 and batch: 300, loss is 5.324044532775879 and perplexity is 205.2121932393995
At time: 359.5950288772583 and batch: 350, loss is 5.287687158584594 and perplexity is 197.88521860471292
At time: 362.04111766815186 and batch: 400, loss is 5.315493507385254 and perplexity is 203.46489978671644
At time: 364.49196314811707 and batch: 450, loss is 5.282502346038818 and perplexity is 196.86187605279923
At time: 366.94443798065186 and batch: 500, loss is 5.281516876220703 and perplexity is 196.66797017549916
At time: 369.39767122268677 and batch: 550, loss is 5.262635555267334 and perplexity is 192.9894559959838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.24805653889974 and perplexity of 190.19626998413838
Finished 13 epochs...
Completing Train Step...
At time: 373.4033422470093 and batch: 50, loss is 5.318343095779419 and perplexity is 204.04551787215513
At time: 375.8516945838928 and batch: 100, loss is 5.305304613113403 and perplexity is 201.40234286537583
At time: 378.29891180992126 and batch: 150, loss is 5.328346700668335 and perplexity is 206.096952374826
At time: 380.745707988739 and batch: 200, loss is 5.329036149978638 and perplexity is 206.23909477085436
At time: 383.195942401886 and batch: 250, loss is 5.293001499176025 and perplexity is 198.93964736956514
At time: 385.647301197052 and batch: 300, loss is 5.308999004364014 and perplexity is 202.14777803545454
At time: 388.1006832122803 and batch: 350, loss is 5.277944431304932 and perplexity is 195.96663816622373
At time: 390.5521059036255 and batch: 400, loss is 5.302040500640869 and perplexity is 200.74601471321856
At time: 393.0048568248749 and batch: 450, loss is 5.263918914794922 and perplexity is 193.2372898490131
At time: 395.4462788105011 and batch: 500, loss is 5.263145875930786 and perplexity is 193.08796763733216
At time: 397.8891968727112 and batch: 550, loss is 5.247270975112915 and perplexity is 190.04691735273963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.237650044759115 and perplexity of 188.22725664736828
Finished 14 epochs...
Completing Train Step...
At time: 401.85433888435364 and batch: 50, loss is 5.313108367919922 and perplexity is 202.98018590931483
At time: 404.30320167541504 and batch: 100, loss is 5.302827701568604 and perplexity is 200.90410437824153
At time: 406.74657130241394 and batch: 150, loss is 5.31677770614624 and perplexity is 203.72635700451215
At time: 409.1899356842041 and batch: 200, loss is 5.317921562194824 and perplexity is 203.95952395952054
At time: 411.6366481781006 and batch: 250, loss is 5.283241939544678 and perplexity is 197.00752767273613
At time: 414.07996249198914 and batch: 300, loss is 5.295442972183228 and perplexity is 199.42594655028128
At time: 416.5210840702057 and batch: 350, loss is 5.261849145889283 and perplexity is 192.83774693845493
At time: 418.98945140838623 and batch: 400, loss is 5.288331718444824 and perplexity is 198.0128085888321
At time: 421.4400975704193 and batch: 450, loss is 5.2496179103851315 and perplexity is 190.49346897535665
At time: 423.8804383277893 and batch: 500, loss is 5.243777914047241 and perplexity is 189.38422994269231
At time: 426.3295843601227 and batch: 550, loss is 5.2287319564819335 and perplexity is 186.55609222827056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.221768188476562 and perplexity of 185.26147183183173
Finished 15 epochs...
Completing Train Step...
At time: 430.2757396697998 and batch: 50, loss is 5.287188968658447 and perplexity is 197.7866587350762
At time: 432.71999740600586 and batch: 100, loss is 5.276301231384277 and perplexity is 195.644890222432
At time: 435.1636686325073 and batch: 150, loss is 5.296650342941284 and perplexity is 199.66687302106416
At time: 437.61010003089905 and batch: 200, loss is 5.294516229629517 and perplexity is 199.2412156515129
At time: 440.0559594631195 and batch: 250, loss is 5.2641790580749515 and perplexity is 193.2875657706065
At time: 442.50370955467224 and batch: 300, loss is 5.279976043701172 and perplexity is 196.3651711127319
At time: 444.9448473453522 and batch: 350, loss is 5.251478900909424 and perplexity is 190.8483055874374
At time: 447.3876311779022 and batch: 400, loss is 5.275429105758667 and perplexity is 195.47433768257974
At time: 449.8291778564453 and batch: 450, loss is 5.234811401367187 and perplexity is 187.69370422990335
At time: 452.27808928489685 and batch: 500, loss is 5.231638116836548 and perplexity is 187.0990427158708
At time: 454.7200195789337 and batch: 550, loss is 5.215771360397339 and perplexity is 184.15381516829885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.22573496500651 and perplexity of 185.99782219330518
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 458.688668012619 and batch: 50, loss is 5.265712661743164 and perplexity is 193.5842197071321
At time: 461.1611359119415 and batch: 100, loss is 5.218283166885376 and perplexity is 184.6169553317666
At time: 463.6121895313263 and batch: 150, loss is 5.204601793289185 and perplexity is 182.1083415648266
At time: 466.06481862068176 and batch: 200, loss is 5.187170705795288 and perplexity is 178.96150123255316
At time: 468.5156855583191 and batch: 250, loss is 5.140169486999512 and perplexity is 170.74470487678076
At time: 470.9812169075012 and batch: 300, loss is 5.123578300476074 and perplexity is 167.93521843347855
At time: 473.4466826915741 and batch: 350, loss is 5.07283091545105 and perplexity is 159.62557482635856
At time: 475.8910827636719 and batch: 400, loss is 5.084995698928833 and perplexity is 161.5792442722767
At time: 478.3363380432129 and batch: 450, loss is 5.047666454315186 and perplexity is 155.6588034125479
At time: 480.8179259300232 and batch: 500, loss is 5.0512073802948 and perplexity is 156.2109567035582
At time: 483.26675510406494 and batch: 550, loss is 5.060377359390259 and perplexity is 157.6499958063822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.114329528808594 and perplexity of 166.38918442164305
Finished 17 epochs...
Completing Train Step...
At time: 487.2244963645935 and batch: 50, loss is 5.133372364044189 and perplexity is 169.588067480198
At time: 489.68735122680664 and batch: 100, loss is 5.111151037216186 and perplexity is 165.86115740715059
At time: 492.13322043418884 and batch: 150, loss is 5.121712532043457 and perplexity is 167.62218232145761
At time: 494.5765278339386 and batch: 200, loss is 5.1173312377929685 and perplexity is 166.8893866866615
At time: 497.01882815361023 and batch: 250, loss is 5.089510698318481 and perplexity is 162.31042385713118
At time: 499.463086605072 and batch: 300, loss is 5.087214097976685 and perplexity is 161.9380893975999
At time: 501.92732882499695 and batch: 350, loss is 5.048813591003418 and perplexity is 155.83746779342943
At time: 504.3796455860138 and batch: 400, loss is 5.075787105560303 and perplexity is 160.09815654839034
At time: 506.8417797088623 and batch: 450, loss is 5.044858675003052 and perplexity is 155.22236084850488
At time: 509.29738545417786 and batch: 500, loss is 5.04455906867981 and perplexity is 155.1758622136764
At time: 511.75331687927246 and batch: 550, loss is 5.043207845687866 and perplexity is 154.96632661739716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.098142496744791 and perplexity of 163.71751886194355
Finished 18 epochs...
Completing Train Step...
At time: 515.6801607608795 and batch: 50, loss is 5.107527523040772 and perplexity is 165.26124470462304
At time: 518.1725149154663 and batch: 100, loss is 5.088170404434204 and perplexity is 162.093025909795
At time: 520.6175127029419 and batch: 150, loss is 5.104987916946411 and perplexity is 164.84207872393674
At time: 523.0741560459137 and batch: 200, loss is 5.107466878890992 and perplexity is 165.25122288083188
At time: 525.5206654071808 and batch: 250, loss is 5.08528398513794 and perplexity is 161.62583205506326
At time: 527.9752850532532 and batch: 300, loss is 5.081492280960083 and perplexity is 161.01415509429512
At time: 530.4241163730621 and batch: 350, loss is 5.042171478271484 and perplexity is 154.80580775847926
At time: 532.8698189258575 and batch: 400, loss is 5.065896081924438 and perplexity is 158.52242753080847
At time: 535.315351486206 and batch: 450, loss is 5.036404876708985 and perplexity is 153.91567333753324
At time: 537.7608478069305 and batch: 500, loss is 5.033940114974976 and perplexity is 153.53677501448936
At time: 540.2216172218323 and batch: 550, loss is 5.028591737747193 and perplexity is 152.71779448403757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.09715830485026 and perplexity of 163.5564686720325
Finished 19 epochs...
Completing Train Step...
At time: 544.2632918357849 and batch: 50, loss is 5.094066877365112 and perplexity is 163.05162645302326
At time: 546.7445244789124 and batch: 100, loss is 5.076410493850708 and perplexity is 160.19799097907253
At time: 549.1873660087585 and batch: 150, loss is 5.09683928489685 and perplexity is 163.5042992170079
At time: 551.632116317749 and batch: 200, loss is 5.100422458648682 and perplexity is 164.0912144118775
At time: 554.0775637626648 and batch: 250, loss is 5.07831374168396 and perplexity is 160.50317778923787
At time: 556.5247604846954 and batch: 300, loss is 5.074780368804932 and perplexity is 159.9370609537602
At time: 558.990475654602 and batch: 350, loss is 5.034267978668213 and perplexity is 153.5871224016825
At time: 561.4529786109924 and batch: 400, loss is 5.059594297409058 and perplexity is 157.52659441010636
At time: 563.8967409133911 and batch: 450, loss is 5.03185037612915 and perplexity is 153.21625826627022
At time: 566.338541507721 and batch: 500, loss is 5.027651462554932 and perplexity is 152.57426521954554
At time: 568.7818109989166 and batch: 550, loss is 5.0210977554321286 and perplexity is 151.57760763905236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.093030802408854 and perplexity of 162.8827802300537
Finished 20 epochs...
Completing Train Step...
At time: 572.7569644451141 and batch: 50, loss is 5.083483438491822 and perplexity is 161.33507904096368
At time: 575.2070214748383 and batch: 100, loss is 5.066080007553101 and perplexity is 158.55158654941238
At time: 577.6600904464722 and batch: 150, loss is 5.088460178375244 and perplexity is 162.14000305077485
At time: 580.1071012020111 and batch: 200, loss is 5.093355569839478 and perplexity is 162.9356878429516
At time: 582.5582983493805 and batch: 250, loss is 5.073284273147583 and perplexity is 159.69795871592655
At time: 585.0057971477509 and batch: 300, loss is 5.069836664199829 and perplexity is 159.14833060033098
At time: 587.453587770462 and batch: 350, loss is 5.029283571243286 and perplexity is 152.82348632607378
At time: 589.9162480831146 and batch: 400, loss is 5.053985185623169 and perplexity is 156.64548356755
At time: 592.3746712207794 and batch: 450, loss is 5.026191720962524 and perplexity is 152.35170869570027
At time: 594.8299429416656 and batch: 500, loss is 5.019909248352051 and perplexity is 151.39756359219777
At time: 597.2851784229279 and batch: 550, loss is 5.012660131454468 and perplexity is 150.30403330885562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.096391805013021 and perplexity of 163.43115069965435
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 601.2406280040741 and batch: 50, loss is 5.06815390586853 and perplexity is 158.88074762298555
At time: 603.7323324680328 and batch: 100, loss is 5.038866958618164 and perplexity is 154.29509322214827
At time: 606.1794292926788 and batch: 150, loss is 5.049591903686523 and perplexity is 155.9588052842449
At time: 608.6359920501709 and batch: 200, loss is 5.041187887191772 and perplexity is 154.6536170058835
At time: 611.089870929718 and batch: 250, loss is 5.0155183219909665 and perplexity is 150.73424539563817
At time: 613.53395652771 and batch: 300, loss is 5.005278739929199 and perplexity is 149.19866498817595
At time: 615.9761278629303 and batch: 350, loss is 4.951267194747925 and perplexity is 141.3539734903352
At time: 618.4235107898712 and batch: 400, loss is 4.970413427352906 and perplexity is 144.0864443361035
At time: 620.8638815879822 and batch: 450, loss is 4.9488534736633305 and perplexity is 141.01319586065424
At time: 623.3098068237305 and batch: 500, loss is 4.950285320281982 and perplexity is 141.21524974890028
At time: 625.7544188499451 and batch: 550, loss is 4.960354194641114 and perplexity is 142.64431080087786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.039954121907552 and perplexity of 154.4629283988228
Finished 22 epochs...
Completing Train Step...
At time: 629.7100586891174 and batch: 50, loss is 5.026076793670654 and perplexity is 152.33420033252176
At time: 632.1595256328583 and batch: 100, loss is 5.00349609375 and perplexity is 148.93293348109177
At time: 634.6075403690338 and batch: 150, loss is 5.023195838928222 and perplexity is 151.89596396831487
At time: 637.0585091114044 and batch: 200, loss is 5.018818387985229 and perplexity is 151.23250003745508
At time: 639.5109074115753 and batch: 250, loss is 5.004351100921631 and perplexity is 149.0603266605904
At time: 641.9621539115906 and batch: 300, loss is 4.998451843261718 and perplexity is 148.18357003604888
At time: 644.4124047756195 and batch: 350, loss is 4.951144199371338 and perplexity is 141.33658867428164
At time: 646.8530564308167 and batch: 400, loss is 4.97405514717102 and perplexity is 144.61212340293352
At time: 649.3100428581238 and batch: 450, loss is 4.951772737503052 and perplexity is 141.42545203374914
At time: 651.7621881961823 and batch: 500, loss is 4.949921655654907 and perplexity is 141.163904094628
At time: 654.2009470462799 and batch: 550, loss is 4.953058223724366 and perplexity is 141.6073694047674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.036790466308593 and perplexity of 153.97503306390007
Finished 23 epochs...
Completing Train Step...
At time: 658.13694024086 and batch: 50, loss is 5.015182676315308 and perplexity is 150.6836605877584
At time: 660.6117806434631 and batch: 100, loss is 4.994848127365112 and perplexity is 147.65051960712972
At time: 663.0632059574127 and batch: 150, loss is 5.017272472381592 and perplexity is 150.9988879746615
At time: 665.5567996501923 and batch: 200, loss is 5.015167169570923 and perplexity is 150.68132399286714
At time: 668.0112707614899 and batch: 250, loss is 5.003335676193237 and perplexity is 148.90904393998335
At time: 670.4658315181732 and batch: 300, loss is 4.997560329437256 and perplexity is 148.05152120522865
At time: 672.9161047935486 and batch: 350, loss is 4.950578689575195 and perplexity is 141.25668404438784
At time: 675.3661222457886 and batch: 400, loss is 4.97329155921936 and perplexity is 144.50174147635704
At time: 677.8132350444794 and batch: 450, loss is 4.951061239242554 and perplexity is 141.32486385903607
At time: 680.2623901367188 and batch: 500, loss is 4.948204536437988 and perplexity is 140.92171683387895
At time: 682.711799621582 and batch: 550, loss is 4.948596086502075 and perplexity is 140.97690554500312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.037241617838542 and perplexity of 154.04451480785977
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 686.7001039981842 and batch: 50, loss is 5.009010820388794 and perplexity is 149.75652675536506
At time: 689.1466331481934 and batch: 100, loss is 4.985543937683105 and perplexity is 146.28312229243517
At time: 691.5947334766388 and batch: 150, loss is 5.000394372940064 and perplexity is 148.47170077934388
At time: 694.0424768924713 and batch: 200, loss is 4.992866458892823 and perplexity is 147.35821494862705
At time: 696.4912903308868 and batch: 250, loss is 4.97468168258667 and perplexity is 144.70275640920715
At time: 698.9389383792877 and batch: 300, loss is 4.9645898818969725 and perplexity is 143.24978889293476
At time: 701.3920814990997 and batch: 350, loss is 4.910769319534301 and perplexity is 135.7438046091525
At time: 703.8402178287506 and batch: 400, loss is 4.933879404067993 and perplexity is 138.91738502122135
At time: 706.2940573692322 and batch: 450, loss is 4.9158664989471434 and perplexity is 136.4374815314093
At time: 708.73983502388 and batch: 500, loss is 4.916572437286377 and perplexity is 136.5338319852692
At time: 711.1880805492401 and batch: 550, loss is 4.930053701400757 and perplexity is 138.38694371314517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.023559061686198 and perplexity of 151.951146060362
Finished 25 epochs...
Completing Train Step...
At time: 715.1793308258057 and batch: 50, loss is 4.993429479598999 and perplexity is 147.441204034963
At time: 717.6614773273468 and batch: 100, loss is 4.9708487415313725 and perplexity is 144.14918086230688
At time: 720.1271646022797 and batch: 150, loss is 4.988653383255005 and perplexity is 146.73868961312496
At time: 722.5786476135254 and batch: 200, loss is 4.985650339126587 and perplexity is 146.29868785588863
At time: 725.0284838676453 and batch: 250, loss is 4.969672842025757 and perplexity is 143.97977553313768
At time: 727.5266573429108 and batch: 300, loss is 4.9635114574432375 and perplexity is 143.09538808736644
At time: 729.9761667251587 and batch: 350, loss is 4.913341646194458 and perplexity is 136.09343150120134
At time: 732.4373295307159 and batch: 400, loss is 4.93783088684082 and perplexity is 139.46740064794403
At time: 734.8973774909973 and batch: 450, loss is 4.919265508651733 and perplexity is 136.902022898264
At time: 737.3659660816193 and batch: 500, loss is 4.917409191131592 and perplexity is 136.64812500507801
At time: 739.8175654411316 and batch: 550, loss is 4.926990566253662 and perplexity is 137.96369436786958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.021623229980468 and perplexity of 151.6572787447008
Finished 26 epochs...
Completing Train Step...
At time: 743.8541176319122 and batch: 50, loss is 4.987277584075928 and perplexity is 146.53694545596144
At time: 746.304007768631 and batch: 100, loss is 4.965268096923828 and perplexity is 143.3469760055106
At time: 748.762362241745 and batch: 150, loss is 4.984433431625366 and perplexity is 146.12076416551145
At time: 751.2113308906555 and batch: 200, loss is 4.98333984375 and perplexity is 145.9610556130658
At time: 753.6579976081848 and batch: 250, loss is 4.968285322189331 and perplexity is 143.78013927020726
At time: 756.1046795845032 and batch: 300, loss is 4.963656616210938 and perplexity is 143.11616114522405
At time: 758.5511548519135 and batch: 350, loss is 4.913493576049805 and perplexity is 136.1141097273432
At time: 761.0020792484283 and batch: 400, loss is 4.939100494384766 and perplexity is 139.64458196350864
At time: 763.4502568244934 and batch: 450, loss is 4.919760856628418 and perplexity is 136.96985383688283
At time: 765.9002087116241 and batch: 500, loss is 4.917147893905639 and perplexity is 136.61242389358642
At time: 768.3574640750885 and batch: 550, loss is 4.924594945907593 and perplexity is 137.6335813052842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.020668029785156 and perplexity of 151.51248484701227
Finished 27 epochs...
Completing Train Step...
At time: 772.3305666446686 and batch: 50, loss is 4.983550224304199 and perplexity is 145.99176621118033
At time: 774.8135259151459 and batch: 100, loss is 4.961602964401245 and perplexity is 142.82255197058936
At time: 777.255496263504 and batch: 150, loss is 4.981577072143555 and perplexity is 145.70398625243664
At time: 779.6978397369385 and batch: 200, loss is 4.981541843414306 and perplexity is 145.6988533765674
At time: 782.1443758010864 and batch: 250, loss is 4.966956949234008 and perplexity is 143.5892724209351
At time: 784.5879237651825 and batch: 300, loss is 4.963132095336914 and perplexity is 143.0411134150644
At time: 787.0389494895935 and batch: 350, loss is 4.912856330871582 and perplexity is 136.0273992980671
At time: 789.5309884548187 and batch: 400, loss is 4.938937511444092 and perplexity is 139.62182413351042
At time: 791.9821469783783 and batch: 450, loss is 4.919372816085815 and perplexity is 136.91671429129553
At time: 794.4364764690399 and batch: 500, loss is 4.916564197540283 and perplexity is 136.53270698579522
At time: 796.8812565803528 and batch: 550, loss is 4.9230038452148435 and perplexity is 137.41476654304628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0208887736002605 and perplexity of 151.54593398266243
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 800.8296630382538 and batch: 50, loss is 4.980794944763184 and perplexity is 145.59007172900027
At time: 803.2758619785309 and batch: 100, loss is 4.9571905517578125 and perplexity is 142.19374822750166
At time: 805.7211663722992 and batch: 150, loss is 4.975585861206055 and perplexity is 144.8336527156214
At time: 808.1644022464752 and batch: 200, loss is 4.971099996566773 and perplexity is 144.18540362023197
At time: 810.6070108413696 and batch: 250, loss is 4.954646997451782 and perplexity is 141.83253029037334
At time: 813.0486030578613 and batch: 300, loss is 4.947879524230957 and perplexity is 140.8759229978531
At time: 815.4900414943695 and batch: 350, loss is 4.8954832649230955 and perplexity is 133.68459608242242
At time: 817.9328649044037 and batch: 400, loss is 4.91983585357666 and perplexity is 136.98012654312774
At time: 820.3838763237 and batch: 450, loss is 4.903604564666748 and perplexity is 134.77470933498145
At time: 822.8343391418457 and batch: 500, loss is 4.90237642288208 and perplexity is 134.60928848385285
At time: 825.272613286972 and batch: 550, loss is 4.913071918487549 and perplexity is 136.05672828215717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.012322998046875 and perplexity of 150.2533693386695
Finished 29 epochs...
Completing Train Step...
At time: 829.1716763973236 and batch: 50, loss is 4.974866285324096 and perplexity is 144.72947139990774
At time: 831.6546635627747 and batch: 100, loss is 4.951500749588012 and perplexity is 141.3869912505882
At time: 834.1068065166473 and batch: 150, loss is 4.970882005691529 and perplexity is 144.1539759434972
At time: 836.557204246521 and batch: 200, loss is 4.968147192001343 and perplexity is 143.7602802641366
At time: 838.9993846416473 and batch: 250, loss is 4.953679285049438 and perplexity is 141.6953435811015
At time: 841.4400310516357 and batch: 300, loss is 4.948229455947876 and perplexity is 140.92522857775037
At time: 843.8811929225922 and batch: 350, loss is 4.896854104995728 and perplexity is 133.86798195145664
At time: 846.3299329280853 and batch: 400, loss is 4.922050523757934 and perplexity is 137.28382852053446
At time: 848.7832143306732 and batch: 450, loss is 4.904525051116943 and perplexity is 134.8988247432656
At time: 851.2600824832916 and batch: 500, loss is 4.902337274551392 and perplexity is 134.60401885806277
At time: 853.7071921825409 and batch: 550, loss is 4.9114443302154545 and perplexity is 135.83546405923764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.012351989746094 and perplexity of 150.257725502306
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 857.6341080665588 and batch: 50, loss is 4.971447191238403 and perplexity is 144.23547271545436
At time: 860.076587677002 and batch: 100, loss is 4.9480222225189205 and perplexity is 140.89602718526268
At time: 862.5288372039795 and batch: 150, loss is 4.966996669769287 and perplexity is 143.59497597696932
At time: 864.9877979755402 and batch: 200, loss is 4.963501672744751 and perplexity is 143.09398794898922
At time: 867.4311575889587 and batch: 250, loss is 4.948024892807007 and perplexity is 140.89640341874784
At time: 869.874121427536 and batch: 300, loss is 4.94198787689209 and perplexity is 140.0483719542583
At time: 872.3177993297577 and batch: 350, loss is 4.889715595245361 and perplexity is 132.91576679921235
At time: 874.7681639194489 and batch: 400, loss is 4.913983659744263 and perplexity is 136.18083338185826
At time: 877.2132210731506 and batch: 450, loss is 4.897515325546265 and perplexity is 133.9565274830183
At time: 879.6578161716461 and batch: 500, loss is 4.89764175415039 and perplexity is 133.97346449044002
At time: 882.1030566692352 and batch: 550, loss is 4.906002435684204 and perplexity is 135.0982694770921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.005249532063802 and perplexity of 149.19430727729153
Finished 31 epochs...
Completing Train Step...
At time: 886.0229642391205 and batch: 50, loss is 4.968279027938843 and perplexity is 143.7792342848436
At time: 888.4922723770142 and batch: 100, loss is 4.946451568603516 and perplexity is 140.67490198953521
At time: 890.9339666366577 and batch: 150, loss is 4.965894651412964 and perplexity is 143.4368188396073
At time: 893.3769402503967 and batch: 200, loss is 4.962590913772583 and perplexity is 142.96372314452546
At time: 895.8187899589539 and batch: 250, loss is 4.94758957862854 and perplexity is 140.8350825645317
At time: 898.2632875442505 and batch: 300, loss is 4.941992235183716 and perplexity is 140.04898232723514
At time: 900.7046844959259 and batch: 350, loss is 4.890079298019409 and perplexity is 132.9641174244072
At time: 903.1455881595612 and batch: 400, loss is 4.914721412658691 and perplexity is 136.28133825789908
At time: 905.5874049663544 and batch: 450, loss is 4.898280544281006 and perplexity is 134.05907275726778
At time: 908.0288414955139 and batch: 500, loss is 4.89757737159729 and perplexity is 133.96483921441006
At time: 910.4761457443237 and batch: 550, loss is 4.905237016677856 and perplexity is 134.99490225857303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.004971822102864 and perplexity of 149.15288028465594
Finished 32 epochs...
Completing Train Step...
At time: 914.3917953968048 and batch: 50, loss is 4.966962451934815 and perplexity is 143.59006255191412
At time: 916.8600628376007 and batch: 100, loss is 4.9454239463806156 and perplexity is 140.5304155854812
At time: 919.3151230812073 and batch: 150, loss is 4.965121622085571 and perplexity is 143.3259808180565
At time: 921.7577557563782 and batch: 200, loss is 4.962149829864502 and perplexity is 142.90067805191703
At time: 924.2007849216461 and batch: 250, loss is 4.947461585998536 and perplexity is 140.81705786545623
At time: 926.6443357467651 and batch: 300, loss is 4.942182006835938 and perplexity is 140.0755621759745
At time: 929.0907955169678 and batch: 350, loss is 4.890400524139404 and perplexity is 133.00683583271297
At time: 931.543185710907 and batch: 400, loss is 4.915143785476684 and perplexity is 136.33891194870398
At time: 933.988933801651 and batch: 450, loss is 4.898565378189087 and perplexity is 134.0972627655214
At time: 936.436847448349 and batch: 500, loss is 4.897535667419434 and perplexity is 133.9592524374258
At time: 938.885678768158 and batch: 550, loss is 4.904628419876099 and perplexity is 134.91276978811948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.005039978027344 and perplexity of 149.16304628353308
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 942.8601548671722 and batch: 50, loss is 4.965957813262939 and perplexity is 143.44587886056058
At time: 945.3078677654266 and batch: 100, loss is 4.944588060379028 and perplexity is 140.4129972593004
At time: 947.7547409534454 and batch: 150, loss is 4.96441164970398 and perplexity is 143.224259444067
At time: 950.2060008049011 and batch: 200, loss is 4.96114839553833 and perplexity is 142.75764403922136
At time: 952.6525211334229 and batch: 250, loss is 4.945033283233642 and perplexity is 140.4755262534008
At time: 955.0991816520691 and batch: 300, loss is 4.939726295471192 and perplexity is 139.73199904461416
At time: 957.5450720787048 and batch: 350, loss is 4.887289266586304 and perplexity is 132.59366039101351
At time: 959.9917254447937 and batch: 400, loss is 4.911807508468628 and perplexity is 135.88480550512136
At time: 962.4378709793091 and batch: 450, loss is 4.8940462970733645 and perplexity is 133.492633570858
At time: 964.8839616775513 and batch: 500, loss is 4.894887886047363 and perplexity is 133.60502678719337
At time: 967.3286941051483 and batch: 550, loss is 4.903495168685913 and perplexity is 134.75996632988947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.006802876790364 and perplexity of 149.42623755493568
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 971.2847332954407 and batch: 50, loss is 4.965752229690552 and perplexity is 143.4163917754742
At time: 973.7820508480072 and batch: 100, loss is 4.944565334320068 and perplexity is 140.40980626150545
At time: 976.2246923446655 and batch: 150, loss is 4.964316339492798 and perplexity is 143.21060936015954
At time: 978.6691098213196 and batch: 200, loss is 4.961031017303466 and perplexity is 142.74088838234474
At time: 981.1144194602966 and batch: 250, loss is 4.944316492080689 and perplexity is 140.37487071778423
At time: 983.5599975585938 and batch: 300, loss is 4.938990507125855 and perplexity is 139.62922368333972
At time: 986.0169806480408 and batch: 350, loss is 4.885935039520263 and perplexity is 132.4142199962697
At time: 988.4681446552277 and batch: 400, loss is 4.910345287322998 and perplexity is 135.6862570653817
At time: 990.9226694107056 and batch: 450, loss is 4.892656536102295 and perplexity is 133.30723957530526
At time: 993.3668353557587 and batch: 500, loss is 4.892906551361084 and perplexity is 133.3405725860132
At time: 995.809730052948 and batch: 550, loss is 4.902173757553101 and perplexity is 134.58201061235135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0064336140950525 and perplexity of 149.3710702059547
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 999.7471461296082 and batch: 50, loss is 4.964818706512451 and perplexity is 143.2825717214116
At time: 1002.1981320381165 and batch: 100, loss is 4.9438494682312015 and perplexity is 140.3093276115785
At time: 1004.6376447677612 and batch: 150, loss is 4.964100322723389 and perplexity is 143.17967680807573
At time: 1007.0805411338806 and batch: 200, loss is 4.961047658920288 and perplexity is 142.74326384127966
At time: 1009.5245845317841 and batch: 250, loss is 4.94436110496521 and perplexity is 140.38113338537798
At time: 1011.9684109687805 and batch: 300, loss is 4.939045219421387 and perplexity is 139.63686332768017
At time: 1014.4122622013092 and batch: 350, loss is 4.88568006515503 and perplexity is 132.38046206846423
At time: 1016.8540983200073 and batch: 400, loss is 4.909654264450073 and perplexity is 135.5925271466966
At time: 1019.2995412349701 and batch: 450, loss is 4.8921486759185795 and perplexity is 133.23955532464674
At time: 1021.7493281364441 and batch: 500, loss is 4.892450590133667 and perplexity is 133.27978831356117
At time: 1024.1970522403717 and batch: 550, loss is 4.90177399635315 and perplexity is 134.52822069857376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.006075541178386 and perplexity of 149.3175940459341
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1028.1629445552826 and batch: 50, loss is 4.9644661712646485 and perplexity is 143.23206846709567
At time: 1030.6399021148682 and batch: 100, loss is 4.943326768875122 and perplexity is 140.2360071803244
At time: 1033.100463628769 and batch: 150, loss is 4.9637072563171385 and perplexity is 143.1234087463316
At time: 1035.5837247371674 and batch: 200, loss is 4.960692653656006 and perplexity is 142.69259822498753
At time: 1038.0253522396088 and batch: 250, loss is 4.944067211151123 and perplexity is 140.33988230067774
At time: 1040.4677731990814 and batch: 300, loss is 4.938847522735596 and perplexity is 139.6092603111884
At time: 1042.9118859767914 and batch: 350, loss is 4.885447330474854 and perplexity is 132.34965612890747
At time: 1045.3531708717346 and batch: 400, loss is 4.909417581558228 and perplexity is 135.56043851282402
At time: 1047.8024718761444 and batch: 450, loss is 4.891983127593994 and perplexity is 133.2174995651916
At time: 1050.2546944618225 and batch: 500, loss is 4.892327795028686 and perplexity is 133.26342321276115
At time: 1052.7055125236511 and batch: 550, loss is 4.901634397506714 and perplexity is 134.5094420249223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.006119791666666 and perplexity of 149.3242015685715
Annealing...
Model not improving. Stopping early with 149.15288028465594loss at 36 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe57da976a0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -216.26841192058583, 'params': {'lr': 23.539469069257617, 'tune_wordvecs': True, 'anneal': 6.806922257196906, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.5657637377857802}}, {'best_accuracy': -150.9095121006645, 'params': {'lr': 19.036308676398342, 'tune_wordvecs': True, 'anneal': 2.612398573606364, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.3547617270083059}}, {'best_accuracy': -339.3026895331386, 'params': {'lr': 27.62545047311539, 'tune_wordvecs': True, 'anneal': 7.04416115551752, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.7579908248075645}}, {'best_accuracy': -138.47974651753466, 'params': {'lr': 20.1206014913586, 'tune_wordvecs': True, 'anneal': 6.1617772000621, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.09874894968015013}}, {'best_accuracy': -166.9132748470607, 'params': {'lr': 26.31165848231822, 'tune_wordvecs': True, 'anneal': 4.982039223058078, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.11989636450487262}}, {'best_accuracy': -149.15288028465594, 'params': {'lr': 19.244521822480802, 'tune_wordvecs': True, 'anneal': 2.505864489080415, 'wordvec_source': 'glove', 'num_layers': 1, 'seq_len': 50, 'data': 'wikitext', 'wordvec_dim': 200, 'batch_size': 80, 'dropout': 0.23345668211504944}}]
